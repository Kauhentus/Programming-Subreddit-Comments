I know you were looking for something stable, but Orchard 2 is a reimplementation of Orchard (a popular ASP.NET 5 CMS) in ASP.NET Core which is in relatively early development: https://github.com/OrchardCMS/Orchard2 I would keep an eye on it, as it's probably your best bet in the near future.
Same here. For the libraries that haven't been ported, I forked or rolled my own :-)
It's a good article. All of this information is out there, but not many people have put it all in one place like this.
I also want to add that if you are doing web heavy stuff, like gulp, Webpack, React, Angular, etc, I'd use .NET Core. The new project module is much more conducive to client-centric development.
Definitely. From what I understand the author is trying to get them accepted as part of the .NET framework itself so that we have a solid collection of hashes, rather than having to pull them from such nuget packages.
I wouldn't use basic auth because it requires some extra code to override the default authentication method (Windows). If you're in a domain and only using Windows/NTLM auth then you're fine, but otherwise you should probably use whatever forms-based auth is available in your project. This complicates things a little because you have to login and maintain your cookies so your access token gets sent with your actual request, but in my opinion it's easier than having to write a BasicAuthHttpModule even if it is quick and dirty. 
For asp core do you need to bring your own nginx? 
Seconded. And I work with .NET on Linux! It needs time to settle down.
Spot on - have an up vote!
You can either run Kestrel by itself (though not recommended), or behind IIS or nginx.
My Mistake! I suppose it is! Maybe we do have a django equivalent, and it just hasn't been shown the same love.
Thats for that it really helps. So lets say I'm starting a new web project in asp.net. Does everyone just roll their own ? Is there a lot of functionality built into the framework so doing so isn't a huge amount of work ? 
I'm building a string to point to a file based on the color. It's really straightforward but I can't do it all in one pass.
&gt; Does everyone just roll their own Hard to say that most people just roll their own, the platforms I mentioned above get a lot of use, but there is no "standard" or "starter" cms to go to. I, and a lot of others I know, have built their own very simple content management project (Nothing but put html in, get html out, no other fancy features) and just use that with a scaffold mvc bootstrap template, and I feel 50% of the way through any project I, or a client, needs. &gt; Is there a lot of functionality built into the framework so doing so isn't a huge amount of work ? I have worked with python, go, node, etc and .net/C# is my favorite because I feel like I have the most freedom AND tools at my disposal. I am sure there are plenty that will disagree, but that's just me. There is a TON out of the box you can use, and if you don't want to use it, you can easily override or ignore it altogether with no downsides. All that being said, time for me to backpedal! If you were to dive headfirst into Umbraco and learn it's ins and outs, you would have no trouble building whatever site you needed. There a lot of small things abstracted away from the core of C# that you won't be exposed to, but I am struggling to think of a situation where I would say "Oh you are using Umbraco? You can't do that." If going to back to your django roots but in the .NET world appeals to you, go with Umbraco (my favorite choice of the CMS' and open source, I was wrong before) and I don't think you will regret it at all. If you want to get into the nitty gritty, maybe do more than just webdev, and maybe contribute to some open source projects, start with a blank mvc project and go from there; I promise you it will be easier to get dangerous in the language that the others and not nearly as intimidating. No matter what you chose, I think you will enjoy the ecosystem! And it's an especially great time to get into it with core really reinvigorating interest and opening up new opportunities. 
Great find! I have been hoping I would come across something that would help me better understand it. Thanks.
See [this from the documentation](https://docs.asp.net/en/latest/fundamentals/servers.html#kestrel): &gt; Kestrel is designed to be run behind a proxy (for example IIS or Nginx) and should not be deployed directly facing the Internet.
That's not a very good advice. Right now ASP.NET Core works fine for 80% of people. If you have very specific needs, you might fall in the 20%, but even then there are workarounds. The benefits of Core negate the few workarounds you might have to implement.
A product I am lead on sells for about $10,000 to $25,000 per server. We try to strike a balance between painless and protected. We created our own licensing system that requires per-machine activation. The installer will generate a text file with cpu id, bios id, machine name, and fqdn. We then generate an xml license file which has those id's as well as the list of features to enable. The important part is that we sign the XML with the same strong key we use to sign our application. That way if the file is changed in any way it will become invalid by failing signature. They could still decompile our application and remove these checks (even with obfuscation I think). Additionally we don't do any sort of "phone home" stuff. So it's possible it has been pirated. But I think we send the message that we care about our software. If this scheme is too much for your app you could not require people to upload their machine id's. My biggest suggestion is to make it painless to pay for and price it appropriately.
A couple of suggestions from a Nuget sufferer on VS2015. Make sure you upgrade the Package manager to the latest version. (&gt;=3.4.0) Disable the package source that looks to the internet. ( make local feed or something and disable the official source in the settings of Nuget manager... 
Why not just create a nuget package. Might be easier to get into a project. Less steps etc. 
I'm a Nuget newb, but as someone who works with Java as well, I happen to be familiar with Maven. In Maven, there is the ability to specify a local repository so it does not search externally. Does Nuget have a similar feature and could you set an option to that effect to make it stop this behavior?
Any chance you've been RDPing into your dev machine? Windows can get a little batty when using RDP sessions with varying DPIs. Logging out and back in again will clear it out.
Hmm.. is there a way to get see Nuget's logs and/or increase its log level so you can see more of what it's doing? Just knowing what it's getting stuck on would be a big help. The other thing I like to do in these situations is fire up [Process Monitor](https://technet.microsoft.com/en-us/sysinternals/processmonitor) and just have it capture everything up to the point where the process gets stuck and then see what it's doing.
DLL hell, not even once.
[monodevelop](http://www.monodevelop.com/help/faq/#under-which-license-is-monodevelop-available) [xamarin](https://www.xamarin.com/licensing)
I like to ship files without any .dll, but that's a personal preference.
* That first link is for MonoDevelop, which is only available in *binary form* for Linux (unless someone out there is maintaining some unofficial builds). The binary form of MonoDevelop for Windows and OS X is distributed as "Xamarin Studio"... which used to likewise be free of licensing restriction, but apparently this has changed. * That second link is for the underlying Mono compiler and runtime, which is altogether separate from the MonoDevelop IDE.
ILMerge will fix that for you
I have no idea if resharper can run on a build server. If it can't though, 2 alternatives might be ndepend and doxygen/graphviz. Both work on a build server. Edit - fixed ndepend autocorrect.
You're misreading the sentence. Yes, the source code is up on GitHub, and I'm sure you could build your own binary if you're committed enough. However, the overwhelming majority of users would likely be looking to download pre-compiled binaries. And pre-compiled binaries, unencumbered by additional license restrictions, now seem to be available only for Linux. Previously you could download MonoDevelop / Xamarin Studio in binary form with no EULA issues. It now seems they've changed that for the two most popular platforms (Windows and OSX). Moreover, it *looks* like they're not offering pre-built binaries for those platforms even for money... they're steering people toward VS instead. I've heard no announcement, and really haven't even seen anyone in the community talking about it, so hence the "WTF?". Especially since Microsoft seems to be actively developing it and releasing new major versions (why bother doing that if they're trying to steer most users away from it?). Maybe there just aren't enough users on Windows or OSX for anyone to notice yet?
I also thought about this. I added a list of reasons of why I'd rather use single files in specific cases in the [repo](https://github.com/6A/mini): &gt;If I made a NuGet package everytime I wrote an helper class that I use in many of my projects, I'd have a lot of NuGet packages, and a whole lot of dependencies on each of 'em. &gt;Updating a NuGet package is much more complex than simply pushing a commit to GitHub. &gt;You can tinker how everything works easily. Don't like how that helper handles Exceptions, for example? No need to clone the repository, edit what you want, and recompile it. Just open the file, edit whatever you want, and you're done. Writing Windows softwares? No need to package your .exe with a ton of .dll files that aren't that useful. &gt;Oh, you don't like the fact that this package has a dependency to X? Well that's good, simply change how it uses this package however you like.
I definitely did misunderstand, my bad. Xamarin is all about targeting mobile platforms. The parts that make it something different from regular MonoDevelop are what Microsoft is interested in I believe. Those are the parts they've integrated with visual studio to increase value of visual studio. MonoDevelop was a vehicle to deliver Xamarin's offerings to developers and now that Microsoft owns them they want to use vs as that vehicle instead. MonoDevelop doesn't have a place in that. Which I suspect is why they don't focus on delivering it to windows. EDIT: I think the change most likely began way back when VS Express Edition came out. The Community Edition. By that point Xamarin was, I believe, mostly focused on delivering add-ons to a C#/.NET IDE and they were delivering them to both VS and MonoDevelop. I think over time the focus became mostly on VS because that's where so many developers are. 
III definitely need to do obsfucation so thanks for the link. Havent really researched that part of it yet but that looks exactly like what I need.
Personally I'd recommend you look into a RESTful api using asp.net webapi. There are tons of resources out there and you'll be able to interface with it from practically and language
Out of curiosity why not just use newtonsoft for json serialization/deserialization. It's already a nuget package and is extremely fast with features your simple one doesn't include.
Honestly, fuck the 'command pattern' with MVC
Interesting. Why do you think the command pattern is not good to use with MVC?
I don't know why /u/medurshkin thinks it's a bad idea, but I think it's not a great idea because it makes it less obvious what's going on to a new developer. If you inject a service into the controller, people know exactly which method is being called on which specific class. If you have a mediator instance, you have no idea which class will handle the command just by looking at the controller, so it requires a bit more digging. It doesn't seem to really provide any benefits in this situation either, other than reducing the number of dependencies you have in your controller (which, if you structure your controller properly, you shouldn't have many anyway). I think the mediator pattern is more useful for asynchronous event handling code where you want to write code for the case where, for example, "I got a random event from the queue, route it to the appropriate handler that can deal with the message"
Thanks for such a thorough comment! I wrote this post hoping that some people who read it won't go with TFS in future... 
Hey no problem, I've been in web Dev for years and sometimes you just have to ask!
Yes, exactly, yet there are still companies using these other choices. 
.tfignore has perplexing behavior, such as showing files and directories from the exclusion criteria in the change tracker anyway. It's not dependable, and adds odd behavior in the solution. TFS is generally just a terrible version control. I'd rather be using CVS to be completely frank.
Why not fair. You think only this one company is using old versions of TFS? Most of these problems still exists, even in the online version. 
Yes, I know - only in the new version though (or online).
ReSharper can run its inspections on TeamCity but not make the dependency graphs. NDepend would probably be your server-side solution for that.
Yes, there are benchmarks on the README. Please note that Json.NET does have way more features than my implementation. For the tests I did, my implementation was 3 to 10 times faster for deserializing, and over 40 times faster for serializing. Also, please note that the goal of mini is to provide many helpers, not just JSON. It's only a beginning. **Edit:** I do realize that what I just wrote sounds unreal. Try it out yourself if you want to, those are the results I got.
In the last two versions actually... If you haven't upgraded since 2012 you've got bigger problems. In fact, that seems to be your issue, that you're using outdated software. How is that TFS's fault? They've fixed these issues.
Here's one I know of: http://tattoocoder.com/asp-net-core-slack-community/
Just my two cents but if you're trying to monitor something critical, a service like binary canary is great as you can be sure it will always be monitoring. What happens when your monitor machine goes down and you don't know it
May not be the best solution tho, but it's fine for me
^ This. Very active channel.
I know that this is a "how do I do this" type question, but just FYI, there is a free service we use to monitor our deployed sites: http://uptimerobot.com/ - The alerts can be emailed or texted and in my experience it notifies both up and down statuses &lt; 2 minutes. Additionally, it has an API where you can access the data programmatically.
This is a pattern I'll typically use for checking HTTP responses from a C# application, this is using the RestSharp library btw. if (response.ErrorException == null &amp;&amp; Convert.ToInt32(response.StatusCode) &gt;= 200 &amp;&amp; (Convert.ToInt32(response.StatusCode) &lt;= 399)) return response.Content; const string message = "Error retrieving response. Check inner details for more info."; var applicationException = new ApplicationException(message, response.ErrorException); throw applicationException;
Just so you know, you were likely shadow-banned for breaking the rules on spam/self-promotion. https://www.reddit.com/wiki/selfpromotion &gt;You should submit from a variety of sources (a general rule of thumb is that 10% or less of your posting and conversation should link to your own content), The 10:1 rule is the common pitfall.
Understood - I just wanted to show other examples of work already done. It could give you some feature ideas!! :)
&gt; So you want to keep your controllers thin even as your application becomes more and more complicated. &gt; &gt; You’re concerned that your controllers are getting bloated and you’ve heard people talk about the command pattern as one possible solution. No, I just want my code to not be brittle. I couldn't care less about how 'fat' or 'skinny' the controllers are. What a pointless thing to measure! &gt; But you don’t know how the command pattern would fit into your app. How many commands should you have? Do you need separate commands every time you want to change a field in your database? All questions that don't even need to be asked if you can get over your arbitrary desire for 'skinny controllers.' &gt; One of the nice things about the command pattern is that you can pause thinking about your implementation, and focus on your user’s interactions. You can't just separate these two things. They guide each other. &gt; When you focus on creating commands like these, you can stay focused on the feature in question and defer all those niggling questions about service layers, fat controllers, where to put your code etc. &gt; &gt;That’s not to say you shouldn’t consider the architecture of your app, but if you focus on the feature first, you can prove the feature works before getting bogged down with other (more technical) concerns. Or maybe... I don't know... make the damn thing work right there in the controller -- which is testable -- and skip "all those niggling questions". Speaking of "niggling questions", what about those questions that the 'command pattern' brings up, you know, the ones mentioned at the beginning of the article? &gt;The good news is, implementing the command pattern is simple, especially if you use Jimmy Bogard’s MediatR to send commands from your ASP.NET MVC/WebAPI controllers. &gt; &gt;With MediatR you start by creating a simple C# class to represent your command. &gt; &gt;The bool type parameter indicates the response type meaning in this case running this command will return a bool. &gt; &gt;Then in your controller action you can “send” this command using MediatR. Hmmm... this almost sounds like... binding an incoming model and returning an action result... &gt;This ensures your controllers stay bloat-free; focused entirely on handling requests and forwarding them on to your business logic. &gt; &gt;So how do you actually handle this command? Simple, create a handler of course! I completely understand the desire to remove 'sufficiently complex' business logic from controller actions -- but *that is what 'services' are for.* How is having your controller action call a service layer method *not* the same thing? The example in this article is way too simple, anyways. Nothing is even being passed to the view. Where is the application logic? Are there no redirects? No validation errors? No other conditions? ## To me, the 'command pattern' as applied to MVC usually means 'I want to pretend I've abstracted everything out into nothing' and so that is why I say "fuck the 'command pattern' with MVC."
Yeah, any IT group more than 2 people will probably be using some type of automated system monitoring. If not, you probably don't want to work there anyways :)
what chat room name?
\##CSharp
Thank you so much. I didn't know that since I'm still newbie Reddit user :/ 
I agree. NDepend offers a quite nice dependency graph which can be configured pretty easily.
I see what you mean. Usually if you're a Microsoft shop you already have the msdn subscriptions that get you the professional/enterprise license. 
That's what I like. Short and pithy article. I totally agree with you Michał !
Thanks :) 
What a bullshit title. There was no big revealing, those are all information already available for quite some time.
Node only had two selling points over .NET: * No HttpContext performance overhead * Runs on Linux Meanwhile .NET beats out Node in the areas of * Mature libraries * Consistent API design * Asynchronous support at the language level * Static typing * Documentation * Tooling * Multi-threading .NET Core covers both of Node's advantages (at the cost of making the tooling and documentation as bad as Node). From a .NET developer's perspective, Node is like Ruby on Rails. A source of inspiration as to where .NET needs to go, but not good enough to actually leave .NET.
Thank you sir. That's the type of response I was looking for.
To add insult to this, they couldn't even be bothered to cite the source of the information. Yeah, I get that Scott Hunter said that, but *where* and *when*.
Thanks! Do you know if Doxygen can handle a solution that has both VB and C# projects? I can only gets docs of the latter to generate as it stands. 
No idea what this is. A little context helps!
I agree this is just a rehash and almost didn't post it but thought the summary may be good for some folks. Definitely not cool they didn't cite. 
In addition to what's already been mentioned, there are a bunch of chat rooms on [JabbR](https://jabbr.net/) and on Stack Overflow ([ASP.NET](https://chat.stackoverflow.com/rooms/43899/asp-net), [C#](https://chat.stackoverflow.com/rooms/7/c), [F#](https://chat.stackoverflow.com/rooms/51909/f)) that are also fairly active.
I'm glad Microsoft have got involved. For a long time, a lot of the more in-depth MSDN code examples have either been unnecessarily complex, or worse, have bugs.
Have you thought about the maintenance cycle of the code if this project takes off? For example, if a bug is found, how will you publish updates to your users and how will they be notified of updates? 
That's exactly a problem. Unless we use git submodules for example, there's no solution to this problem. Hopefully though, the files are short, not many bugs should be hidden in there. But that's my biggest concern as well.
Ok, so what you meant is .NET **Framework**. ".NET" is broader and emcompasses both .NET Framework and .NET Core.
Everybody else understands context. 
Looks like JabbR's SSL cert expired a few days ago lol
That...is...his...point!
I've tried numerous different settings. I'm pretty sure the data is in the session cookie as I can find it by pulling out the `HttpContext.Authentication.GetAuthenticateInfoAsync()`. So the data is obviously there and I can retrieve it. http://i.imgur.com/k5ztmbj.png The expiration sets but there doesn't appear to be a built in mechanism to invalidate the access_token when the expiration time hits, which means that once I'm authenticated with the OAuth server, my app believes I'm authed until the session cookie times out or I logout. I also noticed it is not saving the refresh_token despite me setting `SaveTokens = true`. There is a field there for it but it doesn't appear to populate. I've done a fair amount of looking through the source code but I'm not a professional developer so this all takes me 20x longer than most. Unfortunately, there doesn't seem to be a whole lot of .netCore discussion going on in either sub =)
 &gt;Or maybe... I don't know... make the damn thing work right there in the controller -- which is testable -- and skip "all those niggling questions". Speaking of "niggling questions", what about those questions that the 'command pattern' brings up, you know, the ones mentioned at the beginning of the article? Yeah if you jump through hoops, and don't care about unit testing. Separation of Concerns, that is what the mediator and command will give you. Granted you do loss some readability. &gt;To me, the 'command pattern' as applied to MVC usually means 'I want to pretend I've abstracted everything out into nothing' and so that is why I say "fuck the 'command pattern' with MVC."* Well that is just your personal opinion, and I would say it is wrong.
OK. Do you have the Windows dependencies as mentioned here; https://github.com/dotnet/core/blob/master/Documentation/prereqs.md#windows-dependencies 
Yup. (Got them from the VS installer).
Just for ref in NUnit this is known as a "TestCase" 
I remember joking just a few months before that Microsoft documentation had been open sourced on Stack Overflow for years already. Now the joke catches up with me.
Warning: This library is licensed MIT, but uses itextsharp under the hood which is licensed AGPL.
&gt; Yeah if you jump through hoops, and don't care about unit testing. Separation of Concerns, that is what the mediator and command will give you. Granted you do loss some readability. With ASP.NET Core it's really easy to test controllers; anything prior to Core and you just need to mock out the couple of places where you would directly use HttpContext-related things. But again, if you have anything 'sufficiently complex', yeah, it probably shouldn't be all just chilling right there in the controller action. Move it out into a service class or something, but for the love of god, don't act like you're "dispatching a command" or whatever bullshit. Is it that hard to call a method and leave it at that? &gt; Well that is just your personal opinion, and I would say it is wrong. Fair is fair :)
I did this recently a week ago and it seems we did the same instructions. There must be some deep setup issues concerning your update 3 because the same steps you did worked for me. There's no way this looks good though [24F8:20A0][2016-07-22T09:16:38]i000: Registry key not found. Key = 'SOFTWARE\Microsoft\InetStp\Components' [24F8:20A0][2016-07-22T09:16:38]i000: Setting string variable 'IISExpressVersion_x64' to value '10.0.1736' [24F8:20A0][2016-07-22T09:16:38]i000: Registry value not found. Key = 'SOFTWARE\Microsoft\IISExpress', Value = 'Version' [24F8:20A0][2016-07-22T09:16:38]i000: Registry key not found. Key = 'SOFTWARE\Microsoft\InetStp\Components' [24F8:20A0][2016-07-22T09:16:38]i000: Registry key not found. Key = 'SOFTWARE\Microsoft\InetStp\Components' [24F8:20A0][2016-07-22T09:16:38]i000: Registry key not found. Key = 'SOFTWARE\Microsoft\InetStp\Components'
You can't. If you target .NETCoreApp, it means you can only use assemblies that also support .NETCoreApp.
run the exe from the command line like so DotNetCore.1.0.0-VS2015Tools.Preview2.exe SKIP_VSU_CHECK=1 after the exe name. There's a known bug in the registry keys it checks to ensure Update 3 is installed.
YourPage.cs inherits from System.Web.UI.Page. If theres any private variables in System.Web.UI.Page, YourPage.cs cant access them. If theres any Protected variables in System.Web.UI.Page, YourPage.cs can access them. YourPage.cs.aspx act like a class (i have no idea how it works behind the scenes, does it generate the new class code from the *.aspx at compile time? Runtime? Or just pretending to be a class?). It inherits from YourPage.cs. So anything you set to private in YourPage.cs isn't meant to be available to YourPage.cs.aspx.
Private is only accessible in the class an nowhere else. Protected is like private, except classes that inherit from the first class can access the vairable. In your example, the aspx "inherits" from your class in the .cs-file. Which again inherits from System.Web.UI.Page. 
I recently switched to XUnit. It definitely has it's problems and I wouldn't recommend it in the general case. But for your specific needs it is a good fit.
Yup - here's the annoucement for the workaround - https://github.com/aspnet/Tooling#release-notes-and-known-issues.
Mostly strange design decisions. For example, you can't pass messages to asserts, making longer tests hard to understand. They really drank the unit testing koolaid. The way data driven tests work also have some unnecessary limitations, forcing me to duplicate logic across test classes instead of just shoving it into a static property in a base class.
Instead of targeting "Core" in project.json you need to target "Framework". This basically says to use the old .NET runtime but with the new MVC core. You are looking to change from `dnxcore50` to `net46` Look at this post http://stackoverflow.com/questions/31539341/project-json-definition-dnx451-vs-dotnet-4-51
[Here's a Discord server with a bunch of people.](https://discord.gg/35HuDSY)
But wouldn't you want the build to be marked as failed if these tests failed? You wouldn't want this build to be able to be released. Maybe marking it as partial success (which I personally hate) will prevent it from being released.
In addition to all the great features of Chroniton, it now also supports Cron expressions. When tackling this problem, had to choose to bring in another library or create a parser of my own. Generally, I don't like reinventing the wheel, but for Chroniton, I want to keep its footprint as small as possible with as few dependencies as possible. So, I decided to get into the wheel business. As much as I dislike parsing strings, it did present some interesting challenges. Firstly, how do you validate such a crazy string. Regex can't get you all the way there, but it can at least make it manageable. I ended up programmatically creating a regex that is just over 1000 characters. Next came the problem of creating an algorithm to process the multiple fields. Then came the tedious exercise of creating an implementation for each of the unique fields. All of the fields have at least one thing in common and several others have other things in common which led to some creative inheritance. All in all I feel pretty satisfied with what came out. This open source project has given me some fun riddles to work out. Now is when I get to expose my humility and ask the community (this time trolls are welcome) to take a look at this monster I created and see where cracks in the design might exist. So, if you've got some time and are curious to see my solution, I encourage you to take a look and try to break it. Thanks in advance.
Thanks for the pointer. But first I have to figure out how I can read and store things the best. 
I'd say the same for bin/obj folders. You should have a gitignore file for all of this. 
Eek! All the classes in Program.cs and public members. If you don't need it to be OS agnostic, I would PInvoke and use screen buffers for better performance or maybe even have an interface to support multiple implementations of the drawing code, the. You can even implement one using GDI, GTK#, etc depending on where you want to target. 
Thank you. It was pretty late when I wrote the news. I'll add it to the article.
It depends on the data. If the images never change, i would save them to a local database. If they change i would probably cache the images and then serve the images from the database or cache
Yes and Github has actually gitignores for Visual Studio built in
Instead of commenting out debug blocks as big chunks you should throw a debug toggle in and log debug output if that toggle is thrown.
The dotnot core documentation has some instructions on how to use nginx. https://docs.asp.net/en/latest/publishing/linuxproduction.html 
Template10
How about "less than 10000 lines"? If your classes start reaching 1000 lines, and it's not an API entry point or a domain entity, your classes are **too damn big** and probably have too many responsibilities and concerns.
&gt; The size of classes is a style choice, of course, and varies wildly with the both style and the purpose of the class. If a class is adhering to single-responsibility, and it should, then there's very little exception for it being more than a few hundred lines. Even in the exceptional cases the lengthy code/algorithm can be wrapped in its own class &amp; file and adhere to an interface. &gt; Yes, 10,000 lines is probably about the largest I would be happy going to (which is why I gave it as a maximum). But if the code is properly commented, then much of that isn't functional, of course. Files that are small and succinct tend to be self documenting and don't require excessive commenting except where something may not quite be clear. &gt; I'd find that little amount of code in a single non-lightweight class to be cramped, and I think that the design would suffer from having an overabundance of classes, leading to a great deal of unnecessary complexity. Can you elaborate how it is unnecessarily complex? Large files tend to become highly dependent on state, and become more and more side-effect bound. Thus the need for elaborate commenting. Additionally, how do you intend test such code without the separation of concerns? How does one do a pull request when there are potentially 100's of changes scattered across one file, rather than small concise changes across numerous files/classes with corresponding tests to cover those changes. 
In auto-generated controllers it is: var user = await _userManager.GetUserAsync(HttpContext.User); HttpContext is a member of ControllerBase. _userManager is given in constructor by Dependency Injection. Maybe it is what you are looking for. 
Template10, Prism, xUnit works well. There is a little problem with the mocking frameworks so there isn't really a good one that is working. PCLMock or hypermock maybe worth a shot. Winrtxamltoolkit works well also. 
&gt; Can you elaborate how it is unnecessarily complex? Take a small-to-medium-sized codebase, of say 5m functional lines of code. If you have ~10,000 actual lines (say 3,300 functional lines) per class, then that's 1,500 classes you have. Obviously, these will be further divided into their own projects. If you reduce that to ~300 functional lines of code per class, you now have ~16,600 classes to cope with. If you're splitting things out just to hit a (rather small) maximum number of functional lines, then you're forcing a bad design at the top level. It is always going to be a trade-off between class size and complexity. If you split the classes out, then you have simpler classes but a more complex top-level controller, and it becomes harder to know off the bat where you should look for a particular method. ~3,300 functional lines of code is big, certainly, and I'd be wanting to take a good long look at splitting it out. Which is why I stated it as a maximum. So, in essence, classes should be as big as they need to be and no bigger. There's nothing wrong with a class of less than ten lines, if it's appropriate. So the answer to the question "what's the maximum number of lines per class" is always the same general case ("as big as it needs to be, but never so big that it becomes difficult to understand or manage") and different in every specific case. But in any decently sized project, one will *always* find a few exceptions. "Here's a set of functional code which really all belongs together, and it's complex and it really *does* belong in this single controller class. Are we going to force it to be split up, to enforce a maximum lines-per-class rule, or are we going to go with a bigger class?" Those decisions have to be made on a case-by-case basis, and a project-by-project basis. &gt; Files that are small and succinct tend to be self documenting To a *very* limited degree, and if you're not doing anything really complex, then yes. But in 20 years of development, I have yet to meet *any* developer who used the phrase "self-commenting code" *and* could explain to me in simple, fast terms the code *which they themselves wrote* two years ago when I turn up at their desk trying to trace a bug and there's a customer with a show-stopping bug in production, losing $100,000 per hour. For one thing, to be *truly* self-commenting, your methods will have to be named things like: decimal convertCurrencyThrows_InvalidCurrencyException_IfCurrenyDoesNotExistOrRateIsNullInDatabaseReturnsMinusOneIfInvalidValueOtherExceptionsRaisedBackNoneHandledReturnedMinusOneIfInvalidValuePassedIn(string currencyIdFrom, string currencyIdTo, decimal value); which is plainly ridiculous. And even that doesn't tell me everything I need to know about the exception handling in that method.
&gt; The current build system is based .xproj / project.json files. Technically it's only based on project.json files. The xproj is just some glue necessary for Visual Studio to work - but for .NET Core it's not required at all. &gt; Both the projects need to be defined in the global.json file, so the .NET Core Tooling can correctly resolve both. This is not true. You can (preferably) also just define a folder where it will look for projects. &gt; "netstandard1.6" As a library author you should target the **lowest possible** netstandard version. Can you really not target a lower one?
Thanks! I'm already using xunit to test a uwp library. I just find resources a bit hard to find and the documentation a bit lacking, coming from nunit :) will checkout the other resources later on. Thanks once again! 
Run the installer from command line with SKIP_VSU_CHECK=1 as an argument to the exe 
Yes, a shame as I'd have been interested otherwise but I simply can't follow along.
Yes. I'm absolutely certain about that. The only files that are required are: - project.json - project.lock.json (is generated by `dotnet restore`) - A source file.
Well, as I see on nuget, RazorEngine has no version for .NET Core. That was my first motivation
What about [Microsoft.AspNetCore.Razor](https://www.nuget.org/packages/Microsoft.AspNetCore.Razor/)?
Okay, let me explain) This is not a separate parser based on Microsoft's Razor. Microsoft.AspNetCore.Razor - is an engine. And RazorLight - is a library that uses that engine to parse files and strings. It has an intuitive and convenient API that allows you to parse a string and files with some additional features included, like caching. (same as RazorEngine does, just for .NET Core).
The disadvantage of this method is that it requires additional tooling and manual steps right? It is not possible to automate it as part of a build script?
TLDR - Do the work in an IDE extension rather than gulp / grunt
Correct. However, once downloading the extension and manually building the files that you want to include, you'll have a configuration file that you can set to update the appropriate files on build or any other step. Additionally, the extension supports a globbing syntax as well if you already have your files within a specific location or they can be targeted as such, so that you wouldn't have to perform the manual selection (although you would still need to write a pattern to target the files). [The documentation for the extension](https://visualstudiogallery.msdn.microsoft.com/9ec27da7-e24b-4d56-8064-fd7e88ac1c40) goes into a bit more detail regarding some of the functionality that this post doesn't cover.
Good, fuck JavaScript 
If doing this you should probably add the minified versions to .gitignore and not check them in at all
Correct, but then you still end up with gulp/grunt HypeScript solution on .net core until the tooling is here. OOOOR the other solution might be to use iis/nginx module to minify &amp; bundle the code
Didn't they say in the stand up a few weeks ago that they were going away from gulp and back to a bundled solution? I was pretty disappointed to hear it.
I don't like the idea of it being an IDE extension. Hopefully squishit gets ported to .net core
Halle-f'n-lujah. 😀 
I am glad it helped you :-)
In my experience a Junior is someone who's typically fresh out of Uni and needs considerable amounts of training and supervision, typically &amp;amp;lt; 3 years of commercial experience. A mid level dev should be someone who's got a few or more years under their belt, has done a good amount of post tertiary study and should be able to design a solution with oversight typically only from an architect for larger projects. A senior dev should not need any supervision and should be a mentor in technical and process for less experienced team members.
Thanks for replying. If you don't mind, what makes a good junior developer? Like what should a junior developer know to be considered good.
It's hard to be specific but I make sure my juniors understand version control (we use git but a lot of .net shops use TFS), basic programing constructs like classes, loops, conditionals, and understanding how to combine them (like how to find something in an array). I work in JS land now so I also make sure they know promises. C# has these as asynchronous methods. It's also important in C# to know about background workers and maybe even some threading and what the difference between a worker and a thread is. Basically, I expect a junior to be familiar with the language and in the case of .NET the tools provided to you by the framework. If you're not familiar enough with these concepts I.E. You've done no programming outside of required uni courses, then you're an intern and we'll see if you can catch on to the concepts. 
I just go to PluralSight and learn the new concepts to keep myself up-to-date because the hard core programmers create those courses.
me too .. I watch things.. listen to podcasts.. I'm pretty good.. I just don't have the brain to ever be a genius programmer.. I try to make up for it with my overall knowledge of all the latest .net stuff and enthusiasm... really I need to find some sort of lead/management role at some point . that's where I belong. 
It's little to do with expertise and all to do with experience. Junior = At most very little professional experience as a developer. Intermediate = Has worked as a dev for a year or two. Senior = Longer.
This makes me want to learn about Dependency Injection haha. Thanks for replying!
Every company is going to have a different definition of a junior developer, but basically being able to understand programming concepts, without necessarily having a ton of experience. I.e. can you pass the fizz buzz test in a reasonable amount of time without google? If so, then you're probably qualified to be a junior dev. In case you've never heard of it, it's being able to write a program that goes from 1 to 100 and prints fizz for things divisible by 3, buzz for things divisible by 5 and fizzbuzz for things divisible by 3 and 5. Don't print out anything for other numbers. If you can handle that, you're good to go (as a Junior Dev). 
You're welcome! I see you have a pluralsight subscription, I recommend this course: https://app.pluralsight.com/library/courses/inversion-of-control/table-of-contents John Sonmez is an actual legend and this course is really good. It's fairly easy to digest and if you bump the speed up to about 1.4, he sounds like Jeff Goldblum. 
How long is a piece of string? Though I would argue it's going to be quite unlikely you'll find contracting work with no experience, or even if you're advertising yourself as a junior. Juniors are basically paid interns. You are soaking up the knowledge gained by working as part of the payment, hence Juniors are always on low salaries/pay. The sooner you stop being a junior (or thinking of yourself as one) the better. Hell if you are switched on, have personal experience (e.g. contribute to opensource projects or other hobby work that is of a good standard) then you'll be a "junior" for about a month before you can drop the arbitrary title and get a regular dev job.
Haha well thanks again 
I meant how much experience do you still need to be a contractor if you're still a junior developer
&gt; Regularly questions worth and ability of self, feels like an imposter the whole time. In the first instance that I came across this, I stopped to write this post with the intention to say this happens at every level, but then I kept going and found you had covered it already :)
If you want to be a contractor for a company that does consulting, that depends on the company but usually they want at least a few years of experience. I got my first consulting gig after 6.5 years, but I knew a guy that got hired at Accenture straight out of university, so your mileage may vary. Again, this is to be a contractor for a company - but if you want to do your own contracting work, then you can start at any time... it's just a matter of finding clients to do work for.
As I understood it they weren't going away from gulp but they didn't want to force developers to use gulp by putting it in the boilerplate project. So you can use gulp and it's still supported by Visual Studio but developers don't have to use it if they have other preferences.
&gt; basic programing constructs you hire them without any knowledge of that? That would be called an intern. I'd expect a junior to be comfortable in any one object oriented language. (they don't need to know advanced stuff like C# extension methods) Basic understanding of what it means to write readable/reusable code. (not design pattern the way to obfuscation and complexity) Be ready and prepared to ask a million questions. (I actually respect developers the more they ask, even if it's asinine or its to expand on a larger idea they have about technology) What I don't expect: 1. Deep understanding of database systems (most universities do not make it a required course) 2. knowing machine learning (this was a PhD class when i was in Uni ..though it is being offered at masters or late senior level elective) 3. TCP/IP/http stack. Beyond a very basic level due to their own self searching. 4. Poor communication skills. 
Intern: Has a basic ability to program and has dabbled with related tools, but not enough to be left to work unsupervised on even minor projects Junior: Able to write code and use related tools (git etc) although not necessarily the exact ones the company does. No professional experience, or less than a couple of years. Requires at least moderate supervision through a project, and is mentored heavily to improve/train them. Unlikely to be working alone on a major project, or given more than one or two minor projects at a time. Intermediate/Mid-Level. Anything from 2-7 years, depending on the company. Can mostly be left alone to get on with their work, but will benefit from some supervision and mentoring: particularly in the opening and closing phases of a project. Has enough experience to provide light mentoring to junior developers. Can be trusted (in terms of capability), to work on sections of major projects alone, or to handle several minor projects Senior: Generally 5-7 years and above, able to handle a project from start to finish with little or no managerial input other than perhaps initial requirements and periodically checking the vision is matching the output. Able to supervise and mentor junior and intermediate developers, alongside their own work, and handle multiple projects. Note that these things are only guidelines: companies can and will bend these to suit the candidate, and if you prove yourself capable of a higher level once hired, you can progress faster. They're just ballpark ways of vaguely separating experience levels. Of course, there are people who've been developing for 10 years who are basically worthless, and those who've got no commercial experience but have natural flair and enough "hobby" experience to produce good solutions And one final thing to note - you can be the best technical developer in the world, but if you can't handle a project, client, support workload etc, you'll never be as capable as a half decent intermediate developer. There's more to developing than being a code monkey.
Along with the stuff mentioned in the other replies, a Jr dev should learn: - how to become competent at debugging. Outside of the mechanics of breakpoints and F10, learning to question your assumptions (and which assumptions to question) will help you solve real problems. Being actually good at solving problems will help you stand out. - how to Google and quickly parse results. Seriously. Junior devs almost never encounter problems that haven't been written up on stackoverflow or someone's blog. Hit a wall? Google for a while, and then ask your mentor if the solution you found is appropriate. - how to ask for help. It's better to admit ignorance than to sit at your desk for days when you've run out of ideas.
so what you learn is it is best to jump around and find a place when you can be seen as the most senior person for the most pay.. downside is you might not learn as much if there is no one to brainstorm (my situation now) 
It has to do with the number of failures a developer has had to deal with and fix. In software, there are about 10,000 different things you can do wrong at any given time. A Junior developer may know to avoid 1,000. A mid-level developer might avoid 4,000. A senior developer will avoid 6,000 (which still leaves 4,000 errors to deal with...you never get to all of them).
When I'm hiring for junior devs I look for general interest more than anything. Is software something you love and dream about or was it the best opportunity going into college? This is what I look for. Also, learn SQL. I've used a dozen different languages professionally for the last 20 years. One of them always remains and that is SQL. 
I completely agree with you. I will use dependency injection in a few key places but I think using it all the time because you think you're supposed to is a sign of non-grey beard status. 90% of the places it's used there's only ever going to be one class injected. Most of what it does is make jumping into a new code base a lot more confusing.
I only ever meet people that feel this way on the internet. I know I'm a good developer and I know I'm not and never will be the best. I know that I'm plenty good enough and the work I do is valuable enough to deserve my paycheck though.
If that floats your boat, there's a JIRA course where the author sounds like the shop keeper from Frozen.
Isn't this the quintessential anemic model. What he's describing isn't even a command pattern; it's closer to a mediator pattern imo. All it's doing is taking a bag of properties, calling it a command, then handing it to an "operation" factory and calling it a dispatcher. Instead of the business logic living in a business entity or service, it is removed to its own class. It's a layer of indirection that promotes an anemic model. -1
Well I would strongly suggest you revaluate how you feel about DI in general. You're not wrong about some things - it can be misused, but I am a firm believer that misuse of a tool doesn't mean the tool itself is bad, just the person using it. It can seem like an unnecessary abstraction, but what you describe as "painful" only tends to come about when it's done poorly/badly. Asp.net core is a great example of DI used well and it's quite ingrained into the framework - so even if you don't believe me, at least think about the fact that some of Microsoft's most talented people make use of it. You're right that for some things you're unlikely to want to swap them out often, but then when it comes to Unit testing, it's a *lot* easier test and moq data. After all, industry standard or not, your business logic shouldn't be concerned about where your data comes from. That's not it's job. 
If you prefer a book I highly recommend "Dependency Injection in .NET" by Mark Seemann. https://www.manning.com/books/dependency-injection-in-dot-net He not only explains it, but gives good coaching on patterns and practices as well as an intro to a half dozen different DI frameworks.
Please note that within the command handler, you may e.g. inject multiple services and join them together in order to complete some specific task.
"Built-in" &gt;Step 1. Install-Package Microsoft.Extensions.Logging
I personally would like a middle ground. If I am constantly "innovating" i'd get burned out. A balance of creating new awesome things, and some drudgery to master the existing things would be ideal. 
Yeah this course was like a eureka moment for me. Suddenly lots of things I didn't understand and didn't know began to make sense. I was by no means an expert but it was enough that things just started to click for me.
yah.. for me all I ask is we use the Azure cloud.. that would allow so many interesting an better alternatives for things like reporting.. beats hardcoding HMTL in stored procedures and using dbmail
You could perhaps infer from the context that this is in regards to .net core seeing as DO is a Linux host but point taken. I am unfortunately unable to edit the headline, I will just have to hope everyone is able to piece together the clues.
He said "Node **only had** two selling points over .NET". Only had is past tense, as in it no longer does.
Does this mean we can setup event hubs without a message saying we can only use the old portal? Also please for gods sake port the nuget ServiceBus to mono so that we can actually use it on mono without a show stopping entry point exception from create await config timer or whatever the message is.
This is the old fashioned way. Right click the solution and select "publish". Deploy to the local file system, make sure the web.config is correct and deploy the files to the server. Make sure the directory in the server is configured in IIS. You can also select "Web deploy" and deploy directly to the server.
That makes it more of a mediator and says nothing of the model anemia
Thanks for your response! I think I should tell more about the application. It uses Windows Presentation Foundation. It'll be publicly available. The goal of this application is to make installing some popular programs easier. User should just search for a program and click one button. Now the communication looks like this: * User inserts the name of an app, then a WCF service is called to get more information about this app (description, download link, etc.) * WCF service executes a SELECT command to get some information from SQL database. * The service prepares the result for the WPF application, then it returns application info (as my own class object) to client.
F# guys are simply amazing. The amount of content they put out with fraction of C# team reasources is very impressive. I'm loving F# more and more with each realease.
I personally don't know anything about WCF but I have worked with APIs which this would be very easy to do in, especially if you're only doing get requests
I used to work at one of the big corps; keep in mind it's also on the east coast. Things are a lot different here than you flip-flop wearing, beach going, brogrammers on the west coast. 
I am not familiar with ISS. Do you mean IIS?
Have you looked at chocolatey?
Yup, typo on my end :)
Assuming you were to go down the Node route (sorry, not that familiar) would there be additional server set up/configuration to be done? Personally I would create a blank MVC project which creates the absolute minimum file structure, wire up the controllers/views and use a micro ORM such as Dapper or plain old ADO.NET for database interaction. Given it's a Windows server I would imagine the .NET framework is already there so it should be a breeze to get up and running with minimal dependencies and configuration which I am sure will save you time over the other option! 
In that case go with whichever you're more comfortable developing in. However, if you're looking for more exposure to C#/.NET, for such a small application it might be a worthwhile exercise.
Junior developers should not be left on their own. They should be task oriented, managed, and mentored. They know how to write basic portable code (the stuff that every language shares: primitive types, assignments, conditionals, control structures, loops, and basic oop) but have no experience solving actual problems and cannot own anything. They've also probably never have worked with source control or CI systems so you'll have to keep them from burning your house down. They are still learning how software is actually developed. They should be learning everything that an intermediate dev does on the daily. They should be *emulating* their more senior teammates (hopefully for the better) rather than focusing becoming their own developer, while expanding their knowledge on their own time so that by the time they're an intermediate, they're already teaching seniors new things. I think an ideal junior would be just a tiny bit cocky, wide-eyed, and idealistic. Why? Because that's what'll keep them from becoming complacent too early in their career; they'll always be looking for a better solution even if it never gets implemented. 1-3 years Intermediate developers begin to participate in the design process and can usually implement most if not all of a feature on their own. They should be comfortable in their environment (e.g. the .Net framework). I expect an intermediate to know their skill set well enough to explain to a rubber ducky and should be somewhat versed in community practice. They might be good enough to own a piece of a product but aren't quite ready to design their own or manage the SDLC. They should still be receiving mentorship from a senior and should be learning everything that a senior does on the daily. I think an ideal intermediate developer would keep their seniors on their toes. 2-5 years Senior developers have learned how to choose the right tool for the job. They can lead a team if needed (though I wouldn't expect all seniors to be good at it) and can definitely manage all stages of the SDLC, feature request all the way through deployment. They are a mentor and should be competent in their craft. Seniors should have the skill set to design an entire system if needed, but know that input from other team members is the way to go. They have generally encountered some flavor of just about any problem you could hope to run into so even if they don't have the answer, they'll know exactly what threads to pull on. Seniors also know how to delegate. Skills wise, I feel seniors should all have a firm command of OOP as well as FP, be well versed in design patterns, and understand that good Seniors are still up to date with current technologies and practices, but being dated doesn't necessarily make a bad senior so long as their still open to learning or deferring to other teammates. 5+ years, although highly debated. Some feel seniors should be 10+ years, but I find this to be very subjective. I'm a bit of a "if the shoe fits" kind of guy and at that length of service in this day and age, one is likely qualified for bigger things than simply being a senior dev. In reality, I think it really depends on the organization and the individual. For example, one of our senior devs just moved to amazon where he's now a level 1. I mean... a level 1 at some $120k a year, but a level 1 nonetheless. Plus, most of us have met intermediates that should be seniors and we've all met 15+ year Seniors that should probably still be intermediates or worse. Navigation properties are Satan.
"Feels like an imposter" You see right through me
If you find that you're constantly the smartest guy in the room, you need to find a new job.
You are too kind.
Why are navigation properties Satan?
Why do you say javascript is bad?
&gt;can you pass the fizz buzz test in a reasonable amount of time without google? If so, then you're probably qualified to be a junior dev. As a mid level, I would say that anybody who can't figure out fizz buzz without google isnt even a junior dev yet
Dapper replaced datatables for me some time ago.
Not the guy you're talking to, but I self-hosted a NancyFx app as a service for a really quick service we needed. 
Oh whoops, I read with google for some reason
Dapper repositories with query objects if how I've been doing it. 
dynamically typed, un-compiled, inherently asynchronous, lets you code any way you want and subsequently, there's no standard to follow leading to a rotten pot of spaghetti. 
Have you checked out [NancyFX](http://nancyfx.org/)? It's a C# library that can completely replace ASP.Net. It works similar to Node but is on .Net. You might want to look into it as it "sounds right" to you and is really easy to work with - you can use all the cool C#-SQL Server-integration tools/libraries you know.
Accenture aggressively trains the people they hire straight out of college in order to groom them for the skills that Accenture expects to be in demand. This approach is fairly unusual. The majority of small to mid-sized consulting companies will expect you to already have all (or most) of the necessary skills when you interview. For that reason most do not generally hire people with little to no job experience.
It is sharing the part to allow you to record and replay your app runtime states. But it doesn't require you to (re)write your app against TM.net framework as we don't have one, TM's.NET compiler instruments your app to allow for the recording of what is happening under the hood when the app runs. 
Migrations are amazing. And keep you from forgetting to dot your I's and cross your T's. Fortunately in my environment I'm lead developer which also happens to mean lead Network Admin and lead DBA (e.g., there are none) so there's no company fear to speak of :) 
Yep been there, point and load the gun so they can just pull the trigger... 
lol. I should be making nearly double what I make now in Topeka Kansas? Get real. This is the most basic/unrealistic calculator. 3 Inputs haha.
Mine was pretty close.
I am sorry this offended you so much. I simply thought it was a good idea to get this into the linux community. I figured if you want to vote for it you could vote for it and if not then you don't have to vote for it. 
Don't they already do that for wordpress? I haven't looked at their image too closely but I seem to remember it was running apache with nginx reverse proxy. I have spoken to a lot of people who would never have hosted with them if they didn't have that image. I don't agree that it would create a lot of support. They don't provide any sort of technical support for using their images. I imagine they already have a process for updating images with updates but I don't think one more image is going to make a massive amount of difference to a company their size. Are you by any chance the guy at DO that has to support the server images? *Just asking*
I know right. Let's just say I had a less than motivated afternoon.
I tested it on a side-project I did years ago of a custom captcha system. My goals were to make it difficult for a computer to correctly detect the edges of letters, but to make it easy for a human to read. I tested this app with this image: http://imgur.com/r0UumrE This is what the app thought it had: http://imgur.com/a/FMbV6 My captcha would use 2 words of 4 or 5 letters drawn in a dark gray, on a light gray, add dark gray flecks, then 3 circles one left half, one right, one whole, of random sizes/dimensions and direction, would be inverted creating solid breaks in the words that a human can easily parse. So, I guess my little side project is smarter than this app... Edit: I tested it with a few more randomly generated captcha's and most of them detected even less data than the one above.
cool, post your code so I test
I have an aspx page that has the code below. My page has a couple of changes, the captcha passed to it is sent via querystring but is also encrypted so when its rendered to the client, the URL has encrypted captcha (Salted AES 256bit) as part of the URL, that is then decrypted and set as the Captcha string. I removed that and replaced it with "Captcha Here" to simplify this. private static Random rand = new Random((int)DateTime.Now.Ticks); protected void Page_Load(object sender, EventArgs e) { Response.Clear(); String Captcha = "Captcha Here"; System.Web.UI.WebControls.Unit uWidth = new System.Web.UI.WebControls.Unit(285); System.Web.UI.WebControls.Unit uHeight = new System.Web.UI.WebControls.Unit(69); int iWidth = (int)uWidth.Value; int iHeight = (int)uHeight.Value; Image bmp = new Bitmap(iWidth, iHeight); Graphics g = Graphics.FromImage(bmp); g.SmoothingMode = SmoothingMode.HighQuality; Brush ForeBrush = new HatchBrush(HatchStyle.SmallConfetti, Color.LightGray, Color.Black); Brush BackBrush = new HatchBrush(HatchStyle.SmallConfetti, Color.DarkGray, Color.LightGray); g.FillRectangle(BackBrush, new Rectangle(0, 0, iWidth, iHeight)); Matrix Transform = new Matrix(); float fShear = new Random().Next(-5000, 5000); float fYShear = new Random().Next(-500, 500); fShear /= 10000; fYShear /= 10000; Transform.Shear(fShear, fYShear); g.MultiplyTransform(Transform); float fFontSize = 72f; Font testFont; SizeF StringSize; do { fFontSize--; testFont = new Font(FontFamily.GenericMonospace, fFontSize, FontStyle.Bold); StringSize = g.MeasureString(Captcha, testFont); } while (StringSize.Width &gt; iWidth || StringSize.Height &gt; iHeight); g.DrawString(Captcha, testFont, ForeBrush, new PointF(((iWidth - StringSize.Width) / 2), (iHeight - StringSize.Height) / 2)); if (bmp.Height &gt; 40) { DrawInvertedCircle(ref bmp, new Rectangle(0, 0, iWidth / 2, iHeight), true); DrawInvertedCircle(ref bmp, new Rectangle(iWidth / 2, 0, iWidth / 2, iHeight), true); } DrawInvertedCircle(ref bmp, new Rectangle(0, 0, iWidth, iHeight), true); Response.ContentType = "image/png"; MemoryStream ms = new MemoryStream(); bmp.Save(ms, ImageFormat.Png); ms.WriteTo(Response.OutputStream); Response.End(); } private void DrawInvertedCircle(ref Image img, Rectangle rect, Boolean RandomSize) { Image imgClone = new Bitmap(img.Width, img.Height); int iCircleWidth = rect.Width, iCircleHeight = rect.Height; int iTop = rect.Top, iLeft = rect.Left; if (RandomSize) { iCircleHeight = rand.Next(iCircleHeight / 2, iCircleHeight); iCircleWidth = rand.Next(iCircleWidth / 2, iCircleWidth); iLeft = rand.Next(rect.Width - iCircleWidth) + rect.Left; iTop = rand.Next(rect.Height - iCircleHeight) + rect.Top; } Graphics g = Graphics.FromImage(imgClone); g.DrawImage(img, 0, 0); Bitmap bmp = new Bitmap(imgClone); for (int w = 0; w &lt; bmp.Width; w++) { for (int h = 0; h &lt; bmp.Height; h++) { Color Inverted = InvertColor(bmp.GetPixel(w, h)); bmp.SetPixel(w, h, Inverted); } } Graphics gImg = Graphics.FromImage(img); GraphicsPath circlePath = new GraphicsPath(); circlePath.AddEllipse(iLeft, iTop, iCircleWidth, iCircleHeight); gImg.SetClip(circlePath); gImg.DrawImage(bmp, 0, 0); } public static Color InvertColor(Color aColor) { byte bRed = (byte)~(aColor.R); byte bGreen = (byte)~(aColor.G); byte bBlue = (byte)~(aColor.B); return Color.FromArgb(bRed, bGreen, bBlue); }
Sorry, I edited my post. I think you're right. I guess if DO wants to precan these images for customers then they might for dotnet as well. Feels icky to me, but who am I to tell you or DO what to do...
I've been saying this for a while, talking salaries shouldn't be taboo or private. It would be to our own advantage if we knew what we can get and where.
I've spent several hours today messing around with .net core just working out the new config and troubleshooting various issues. If a one click button to get it going immediately is "icky" then sign me up for some ick.
:) Best of luck. Ping me if you get stuck somewhere, if you want.
Have you tried the VS2015 default app? When I try "dontnet publish" it fails because it looks in root of "src/" folder but VS2015 created the project in "src/project/". "dotnet restore" works fine but "dotnet publish" doesn't which is really weird seeing as I am just using default project. Tried copying it up a level but that didn't work. EDIT: I mean the default asp.net core web app
I don't know how I feel about that generalization. If my employer knew for certain they were paying me more than most of the individuals behind Stack Overflow, I *sincerely* doubt it would benefit me in any way. When I felt as though I was underpaid, I was a huge supporter of salary transparency. While I continue to be interested in how it would benefit those that are being over-compensated, I find myself being a bit more reserved on this front as of late. There's a small part of me that believes we could all benefit from this sort of transparency, but I need someone to help me rationalize it from every point of view. 
Couldn't have answered it better myself. Also everything is a floating point!!! ARHGHGHG
I just meant if you can put your project up on GitHub, I can take a look. If you file an Issue I can also look, but you'll have to send me the link. I'm not on the dotnet team.
Ah OK. It seems to be a bug with package manager. I found I could build the project using the context menu but not package manager. I've got it all uploaded and gone through https://docs.asp.net/en/latest/publishing/linuxproduction.html but kestrel isnt listening on port 5000. I feel like those instructions are incomplete. Where are the kestrel docs?
The problem seems to be a permissions problem for supervisor. This is my config: [program:CoreTest] command=/usr/bin/dotnet /var/www/dotnet.dev/project/CoreTest.dll directory=/var/dotnet.dev/project/ autostart=true autorestart=true stderr_logfile=/var/log/dotnetcoretest.err.log stdout_logfile=/var/log/dotnetcoretest.out.log environment=ASPNETCORE_ENVIRONMENT=Production user=www-data stopsignal=INT The error supervisor gives is: supervisor: couldn't chdir to /var/dotnet.dev/project/: ENOENT supervisor: child process was not spawned ownership on project folder is user:www-data. Not sure why it cant chdir.
D'oh! You wouldn't believe how many times t checked that without seeing it. Thanks for pointing it out. It's all working now which is pretty cool. Now to start load testing :)
This is the worst answer. I have asked this question during interviews before this response typically results in a rejection of a candidate.
The curiosity is killing me? What did you deploy? Does it work now? :D
Nice article! I have to say, ES + Kibana is a great combination for visualizing data, stats monitoring etc. 
Just anecdotal evidence, but I've worked in private sector companies without any salary transparency: without fail, in each company people found out roughly how much each other earned and it was a major cause of tension and dissatisfaction. I've also worked in a private sector company with salary transparency, and a public sector organisation with fixed salary scales and full transparency. In neither of these were there any salary contentions I've ever been aware of. It's not a rationalising argument, but in terms of how I've personally seen things in each company it definitely seems that people are happier where salaries are transparent: even in the public sector organisation where the salaries themselves are lower. I feel that, often, people value fairness and perceived equality over the actual dollar headline figure. Within reason, of course.
I had an interesting discussion with an in-house recruiter on LinkedIn regarding this: I'd been talking about available roles with an ex-colleague, and we'd both come across a role with a company which we'd dismissed because there was no salary information. A fortnight or so later, I got an LinkedIn mail from the recruiter at the company the advert was for: I'd actually found a position, but mentioned that I'd skipped over their advert because of the lack of salary information. They'd been discussing it themselves within the company At the end of the day, there are hundreds of roles available, and applying (properly) to each can take a couple of hours. Just as companies will dismiss any CV/Resume with a blocker when they receive hundreds, I will dismiss any role which I'm not certain is suitable. When I have 200 roles to potentially apply for, it would take something specific to make me look into a particular position. To me, lack of salary information on an advert implies one of a few things, none good 1. You're going to offer 10% more than I currently make, rather than the actual value of the position 2. You don't actually know the value of the position (which leads to role creep, difficulties with quantifying results etc) 3. You're just going for the cheapest candidate possible, which doesn't bode well for your attitude to staff 4. You're banking on the "Whoever names a number first, loses" - asking the candidate how much they want, rather than offering whatever the position is worth Regardless, I find it suggests you're not treating your employees as adults and I'd generally rather avoid companies who do these things This may not be true, and I've heard a counter-argument that they're potentially looking for candidates within several ranges and don't want to scare off potential juniors by only advertising the top end salary. I don't think argument stands scrutiny, though - if you're potentially hiring either a junior or senior, or both, then advertise both positions separately with individual pay scales.
This sounds really good. My situation is a bit complicated because I'm trying to slowly become a dev without the degree. I'm an analyst at 49k in Montreal for a telco, around the same as my peers but I have started to make some simple applications for the company because I'm cheaper than their IT department. Unfortunately I know I pretty much maxed out the wage of my position.
ContinuousTests/MightyMoose is a free alternative, and is much lighter than NCrunch. https://github.com/continuoustests/ContinuousTests/
My client is not actually disney, they just service disney. My client only hires local to ATL, and doesn't use any offshore developers.
There probably is - if you have a better idea, let me know. But also, someone saw this post that needed a better job and I've already helped them land somewhere. Great guy too.
I still program in VB.NET, I find it more flexible than C# 
&gt; What if there's significant disparity between developer salaries of roughly "equivalent" skill within the same organization? (I realize that this may be stretching your definition of "within reason" beyond what you intended) That's exactly my point - you generally get a happier team with relatively equal salaries around the average, and unhappy teams where there's significant disparity. The team **will** find out about the disparity regardless. Your organisation's mistake isn't transparency, it's disparity: the lack of transparency is merely there to hide the disparity, no more and no less. &gt; On the opposite side of things, from the organization's perspective, would this not be a difficult sell? Should they be afraid of a "mass exodus" of the underpaid individuals? I ask, because if we can't convince them to implement it as a policy change at the organizational level, the only other alternative is for us, the developers, to assume this responsibility. I wouldn't recommend releasing all salary information if you currently have very unequal salaries, I would recommend leveling the playing field when it comes to the salaries you already pay, and then making the situation more transparency. In your situation, unless you change things, more transparency will just increase dissatisfaction because the company is basically then saying "We don't value you the same, and we don't care". But you're already losing out on that score because, regardless of how opaque you are, your team know about the disparities, and they don't like it. The problem is really one of paying different salaries for the same skill set, not of transparency. &gt; It's entirely possible that I'm going about this the wrong way; instead of asking: &gt; &gt; how would this benefit me? &gt; Should I instead rephrase my question as: &gt; &gt; how won't this hurt me? **It's already hurting you**: People aren't stupid or naive - I promise you that your employees know roughly how much each other are being paid, and those who are being paid less are already dissatisfied. Go find some data regarding your employee turnover rate/employment length, productivity, sick leave, and any other metric you can regarding your staff... graph each of those against salary and I suspect you'll see the same as I've seen elsewhere: those who are paid less, leave, take more sick days, aren't as productive, are less happy. Either because they're being paid below market rate and can get more elsewhere, or because they perceive themselves as being less valued than their peers who are doing the same work. How would it benefit you? Likely better staff retention, better morale, better productivity. The main question I'd ask of the organisation.... why are you paying some people less than others for the same job? What possible business reason do you have for doing that, other than to save a few thousand dollars a year while making 80% of your employees miserable? (Because it's not just the bottom half of earners who are unhappy, it's everyone but the top earners) If people are doing different jobs, give them different job titles and different pay. If they're doing the same job, pay them roughly the same (or, at least, the same base salary plus yearly increments for good performance). Then you can be transparent, and everyone's relatively happy. Want to earn more? Earn a promotion. I find a lot of salary disparity comes from one of two things 1. Unscrupulous companies being more concerned about saving $2000/employee by paying as little as possible. This causes entirely necessary dissatisfaction, and any company doing this deserves their high staff turnover... 2. Companies not sufficiently differentiating their roles/grades, so people have the same job titles with different roles and pay. This can cause entirely un-necessary dissatisfaction, which can mostly be removed just by being clearer and more transparent, and introducing further grading to the salary structure.
I'd rather not say the industry (it's tech, but not social media, silicon valley type tech) because it'll be a blatant identifier of the company (to those that are in the industry) but for the rest of the stuff... On an Agile team - which is important because I (and others), use it to our advantage. We have our tasks to do and when they are done, we are done. I benefit at being a more fluent developer than others on my team, so I knock my tasks out quicker than others, hence my 20 hours a week and my manager is smart enough not to "punish" someone for getting their work done before a sprint deadline. His motto always is, "get your stuff done, then go home or go learn something." We always deliver on schedule and have a pretty good track record in quality. For the PTO - a few years back, the company was having a hard time hiring developers because they clearly aren't the highest paying employer so they basically overhauled their benefits package and now give a ridiculous amount of PTO (35 days!). They also made an effort to actually let people use their PTO too (I've already taken 3 weeks off this year). The only stipulation is that if you take more than 2 weeks off in a row, you need management approval. I think this is pretty cool because they can hire high quality devs with lower wages (not that 80k is really that low, but I could easily be making 100k) because they allow so much "work/life balance". People still get their stuff done and as a result, the company is doing extremely well. Oh, let's not forget how they are totally cool with working from home, anywhere in the US (except not Hawaii due to the timezone...). They had lost some rockstars in the past because their spouses getting a new job in another state or whatnot, so developers had quit and just found a new gig in their new location. My employer is a perfect example of a new executive team coming in to a struggling company and turning it inside out to succeed. This was their way of "google-izing" the business environment.
The terms are really loose. Let's say junior is entry level and senior is the highest technical position (sometimes also called architect or lead). The spectrum is best defined by how independently they can take a business problem and turn it into a solution which fits with the requirements of the organization. A junior can handle well defined programming tasks. A senior developer can take a business problem / request interpret it, design solution and break their design into a set of programming tasks (for themselves and/or junior devs). Unless you're writing something that only needs to be run once, the biggest cost of software is maintenance. As a junior dev whatever task or technology you get try and build something that is better organized, has clearer naming, and is more consistent than the last thing you wrote. And never stop trying to do that. The best code is code you can look at 5 years later and still understand it. Also at home work on your own projects even if they never get done so you can practice being the senior dev. 
It sounds like the network firewall might be stripping Origin or Access-Control-Allow-Origin from the HTTP Header. I would Wireshark the external call both inside the network and outside to check whether or not the packet is being stripped.
Does this plugin actually work yet? It has always been really really crappy from the several times I've tried it
[removed]
You might want to reply to his comment, instead of replying to your own posting.
Most of the environmental differences are contained within config files in the asp.net realm. How does docker deal with connection strings to database servers?
Your right the config handles that well. Containers help more on the server configuration side including: installed Windows Features, IIS AppPool configuration, ACLs, other installed dependencies, etc.
What a weird response. You're essentially admitting that you don't learn anything new until *someone else* has told you to go learn it. I mean, sure, you don't need to learn about docker. I just hope that's not the attitude you have about **everything**.
Nothing. It's as useless to you as .msi files. You don't use those either right? /s Seriously, it's not going to affect your programming per se, but it will affect everything else downstream from you, including how fast your applications can be deployed, how scalable your servers will be, all the way down to how much money is leftover in the budget from server savings so maybe we can fund some more projects to do the next great thing we need. Oh, and by the way, if you need to do any application production support, you will need to know your way around containers in order to troubleshoot. So yeah, you're going to eventually need this if it takes off and if the Linux world has been any indicator, it will take off. 
I think you are right. The Access-Control-Allow-Origin is being stripped from the response header. Why would this happen at random? What firewall settings should I look at?
&gt;Quick-launch (Ctrl-Q) Ah yes, hidden :-).
Note, it looks like they've released the ability to run virtualised Linux servers *on* Windows, not run Windows *inside* a docker virtual machine. Anyway, the usual setup is to set the database server connection string to "Hostname=db" and Docker then puts `db` in the `/etc/hosts` file with the IP address set to whatever IP address Docker has assigned the virtual `db` machine. Here's a quick example: http://blog.hypriot.com/post/docker-compose-nodejs-haproxy/
Got a link to Scott Allens course?
So it's more of a devops thing than a developer thing?
Yes.
To those who say "Why do I need to know about server/instances, I'm a developer" you sound roughly like a professional driver who says "Who care how an engine works, I am paid to be behind the wheel." I am a dev by trade, but if you have ever had to manage your own server, spend 10 minutes understanding what docker does and you will see that it really does appear to be the future. Usually technologies like this are introduced and someone later comes along and does it way better, but by all accounts docker seems to leapfrogged all of that and perfected it first time out... it's just so clean.
Perhaps this pet project of mine is useful to you: https://github.com/Sebazzz/financial-app Basically it is an single page app manage your expenses and income, providing statistics etc. It is in Dutch, but for looking at the project structure it doesn't matter.
Each solution is a completely separate project from the other so we felt hosting them all in the same solution was not the right thing to do, the only common part is the shared functionality in Core. When we release, each project will effectively have its own copy of Core, so if one project changes Core, the others do not need to be updated and therefore do not need re-tested. Thats the main factor in our reasoning why they are not all hosted in the same solution.
&gt; each project will effectively have its own version of core Sure, until the next time you load up the application and realize that something changed in core which broke functionality. Really what you have is a "only the latest published has accurate code" scenario. I have experienced this - most times it was an "oh shit" moment when an unexpected app broke. Once, for a particularly stable app that didn't require many changes, it became a "we're screwed" moment because 2 years of core updates had not been applied or tested. If you can handle retesting multiple applications, the best solution is to put all of the projects into the same solution so that breaking changes to the core can be seen immediately. If retesting everything is not an option, then the second best solution is to make the core a NuGet package so that you can at least claim compatibility with particular versions of core.
The idea is that each project has a copy of the core dll. Not a direct reference. So Project A could be on Core 1.2 Project B on 1.4. Although I do get what you are saying, we should have it so all projects always have the latest version of core. No project should be out of date as there could be an issue with a Core version which isn't pushed to all the projects. Retesting all the applications is not going to be an option as far as I am aware. I think you are right regarding the Nuget solution. It seems like the only way we can get each of the projects to be bound to a particular version of Core. Thanks.
Step 1: don't. WCF stands for Wrangling Configuration Files. Or Windows Configuration Fuckup. Or Windows complication foundation. It's unfit for any purpose.
In one comment you said that you want to be able to avoid retesting things if they didn't need to move to a new version, but in this one you said they should always be on the latest. You might want to get your comments in line with one another so that people can help you most easily. That said, the options don't really change: Either a) move everything into the same solution and publish multiple artifacts from a single build, b) use a git submodule for your Core, or c) publish a NuGet package for your Core. Everything in a single solution is by far the best option, even if you have to jump through some interesting artifact management and deployment work. But if you're dead set against that, submodules can be a pain but after living with both submodules and NuGet packages for a while, I believe that submodules are less painful overall than dealing with topic branch development, build, and use of NuGet packages. EDIT - Oh, and stop putting binaries in your repo. Find a proper artifact management tool if you need one.
WebAPI matches my use-case so I use that, but ive heard good things about servicestack and protocol buffers. Even the old 1.1 remoting was less of a configuration nightmare.
We initially did have everything in one solution, but with 20 developers all working on different projects in the same solution it quickly became apparent that having a single solution with the changes from all teams wasn't going to be manageable. Especially since the releases of this software was going to be in different stages for each project. If we were to continue down the single solution route we would end up needing a branch per project to isolate any potential breaking changes. That's why we decided to go down the single-repo-per-project path. Of course I don't want to put binaries in the repo. That's the whole reason for this post!! 
If I can ask a qualifying question or two perhaps: Do you have a dedicated team who own and work on Core? Or does everyone have at it? Does Core have its own roadmap, moving forward on its own schedule, with projects picking up the latest and greatest once it is available? Or does Core get updated as and when needed by a specific project? Do any projects have their own variations on Core, perhaps for example by building their incarnation of it from a project-specific branch which merges to/from master from time to time?
Self-hosted api using WebApi or NancyFx.
For me, at least, your last paragraph is key. I suspect that, above all other things, you'll need to prioritize making sure that every project is on the same version of Core all the time, and if so then a single solution with Core in it is still going to be the best way to go, particularly from a versioning and compatibility management standpoint, and especially in the earlier days. That idea seems to freak you out, and I don't have enough context to claim to understand why, so if you can't stay in a single solution then a Core submodule is certainly a doable option so long as you can keep everyone as close to master as possible. Just know that even a submodule is going to add some strange friction, in that you'll be committing to two repositories quite frequently, and in some cases the commit on one will simply be to update the submodule reference of the other. As for NuGet, I wouldn't even remotely recommend that for your case based on what I know of it, at least not until Core has become so stable that it is basically never modified.
It kind of stinks about some of the features that are still going to be missing but EF 6 wasn't built overnight and I guess Core will also take some time. I like seeing where it's going though.
While not really "Deep" this is an excellent list of things that everyone should think about going into writing (or rewriting) a microservices architecture.
I'm not suspicious because they really had a good reason with .NET Core compatibility, and the driver versatility that they had in store. It was much easier to start over that to try and work with the existing code base. Really, EF 6 is very good and should keep us going until they get Core fully on track.
I agree to an extent. I don't mind Visual Studio other than database projects (absolutely ridiculous that you have to buy third party things to have basic integration like good source control for SSMS; it already uses the VS shell, why keep this lame project/solution concept?) and TFS, but it is lacking in features that come for free in other tools. Of course IMHO that can be said about a lot of things in the .NET world in general. Either it comes from the mothership, or it doesn't get used.
VS certainly has it's flaws sure. Regarding ReSharper a lot of the refactor functionality is already built in and you can basically roll your own with other free extensions. To be honest most of the stability issues I've had in VS stemmed from ReSharper.
Not only is is open source but it's under the Apache License 2.0, which is either equal to or less restrictive to the GPL. You're right on all counts but especially that EF Core is not EF 7. They had to step back and look at an even bigger picture that included .NET Core and many other types of data sources, not just SQL. They're looking at EF providers for XML and NoSQL now too. I'm super excited about the possibilities. Maybe it's because I started programming with BasicA back in the 80's that I understand how far we've come and where we're going. It's just incredible.
The problem is that they are always looking for excuses to "make huge changes in order to support new/improved features". EF Core may be a completely different library than EF 6, but don't forget that EF 6 was also a completely different library than EF 5. And while we're on the topic, there were breaking changes from EF 4 to EF 5 as well. What about EF 4? Well I just checked and it had breaking changes versus EF 1 (a.k.a. EF 3.5). So yea, I am rather concerned that EF Core 2, perhaps EF Core 1.1, isn't going to be compatible with EF Core 1.0. If they actually do pull is off it will be the first time ever in the history of the library.
I appreciate that. Especially considering how bad EF 6's current performance is when it comes to performing simple queries. But then I see that they've missed seemingly simple things like Group By and can't help but think they are rewriting it for the sake of rewriting it. Take the tooling for example. Aside from some shims for namespaces, I don't see why they need to rewrite that. Tooling can happily remain a Visual Studio feature for the time being.
My experiments are suggesting that even if each thread has its own object I can't write xml for them both at the same time. 
1. Because of project references. If a referenced project file changes it needs read again to make sure it's still a valid reference and to update metadata. I'm sure there is something they can do about it, but this one makes sense to me. 2. The 32-bit thing is quite annoying now. Their own add-in, diagnostic tools, crashes with out of memory errors when doing memory dumps. It's dumb. 3. Needing resharper is debatable. I haven't used it more than 6 years now. Am I less productive? Maybe. Do I still produce good code and products without it? Yep. Btw, Ctrl+, in vanilla VS does a good job.
When does it stop. When does EF cease being a research project and start being something that we can count on year after year? It's just as bad as the 90's when every version of VB had a different data access library.
1. In an ideal world, yes, but I only have that luxury for newer projects where a microservices approach can be applied. Some of our older projects are structured in such a way that they cannot easily be broken into smaller solutions, and well, "don't fix it if it ain't broken" applies here. From a management perspective, it's not worth restructuring the entire project (which is stable) just to get around some inefficiencies with Visual Studio, and I tend to agree with them. 2. I'm on Update 3. No difference as far as I can tell.
If you mean that you want to use two `XmlWriter` instances writing **to the same file at once**, then yes, that won't work. The file system is not thread-safe either. You need to synchronize shared access (to the file system).
It is soo slow.
What's the alternative to WCF for binary communication protocols with full duplex channels?
It might be TFS. We use them at work and they are super annoying; I'd much rather have the company pay for Red Gate or something so we can just use SSMS, but that's too much money for them to spend.
I feel like if you want a mature, stable ORM that isn't going to change anymore, just use NHibernate. EF Core is targeting a future state and if a re-architecture is the best way to get there, then it's okay to just bite the bullet and deal with the consequences.
Don't be a dumbass. You don't need to use app.config for WCF.
I'm glad someone has decided to take on the Visual Studio sacred cow. The things I don't like about Visual Studio: 1. Its fanboys. This is number 1 in my list because it is the thing I hate the most. Like all fandom its the cultish behaviour that is most wearying. No criticism is tolerated and all criticism is received like its a personal attack. To be fair Visual Studio fanboys differ in no way to Microsoft fanboys (they are often the same people) or - outside the MS space - Apple fanboys. 2. It's so bloated. Any other development tools I install take seconds, you download install and are up and running in no time. VS is like installing a f***ing OS. And then you have all the bits you don't want or need. I understand the VS team are trying to address this but I think VS is just too heavily tied to Windows and the registry to be hopeful of any radical change to this. 3. Startup time is too slow. This is likely a consequence of point 2. Sometimes I just want to quickly test a piece of code outside the project I'm using. It's less of a problem these days as I use the Mono REPL on my Macbook (the Mono REPL is excellent btw) but in the past it was a major PITA. I know there are options to use a simple text editor and you can argue that firing up an IDE will be slower but I've used several of the JetBrains IDEs and they are all much faster by comparison. 3. The assumption that anyone coding with C# is using Visual Studio. I'm fed up with reading books and tutorials that start with "fire up Visual Studio". This is a bad way to write a tutorial anyway - the code should be agnostic to tooling unless it's a tutorial or chapter on the tooling itself. 4. It's not cross platform. It's the only major IDE that ties you to a single operating system. I only work on OSX / Linux these days so don't use VS as much as I used to but it would be good to have the option. I currently use Xamarin Studio which is okay but lacks polish in certain areas and am looking forward to the date Rider is finally released.
I had the same experience.
For anyone not familiar with dependency injection, this is a fantastic article to get you started (whether you use Core or not). One of the most straightforward, easy to understand articles on DI I've read. 
I believe mosh's course on pluralsight actually does what you want in the later parts of the series (or so I have read), those courses are also on Pluralsight.
why is a post related to angular is posted in this subreddit? 
"Building a Web App with ASP.NET Core, MVC 6, EF Core, and Angular"
i'm using Stylet mainly, and it's a beautifully built framework, no magic strings like Caliburn, also, the guy behind Stylet is very friendly
Thanks
Stylet is good but If you want to support at least .net 4.0, I suggest you use Caliburn.Micro.
Well the down voting proves point 1 above :-)
Just off the back of this: If you're not familiar with DI, you're missing out on a fantastic design paradigm. Stop what you're doing and learn it now, it'll set you up well in the future.
To save a google https://github.com/canton7/Stylet
I'm a fan of angular (usually) and every time I look at angular2 I get overwhelmed. I feel like it's a completely different framework. It shouldn't be angular 2, it should have it's own name. What are others experiences with this? 
I haven't had much experience with AngularJS but currently working with Angular2 for a commercial project. It feels pretty good. I think the idea of naming them similarly is stupid. You should learn Angular2 with a fresh mind, not from the view of AngularJS.
Which specific part are you having trouble with?
I managed to create it but I can't figure it out how to make it available over the internet. When I'm executing the request on the app it doesn't return anything
any reason you need to go over the internet? are you on your own network? If so, you can just use your computer's network IP address (assuming firewalls are set up properly).
[ngrok](https://ngrok.com) could be of some help 
Not the longest guide, but covers exactly the things that I found to be such a hassle when I tried .NET core a week or so ago: the "This works in 4.6, why doesn't it work here" things that .NET core is so bad at explaining, such as compiling Razor and the new routing techniques. Bookmarked, for when I inevitably forget how to make Core compile razor views next time I start a Core project...
I would recommend my own, Tortuga Anchor/Sails. Advantages: * Support for IRevertableChangeTracking across object graphs so you always know if an object, or any of its children, have unsaved changes. * Support for IEditableObject so you can have dialogs that support a cancel button. (This stacks with IRevertableChangeTracking, giving you two levels of undo.) * A large collection of value converters that cover all of your basic scenarios such as [Zero, NotZero, True, False, Null, NotNull] to [Visible, Hidden, True, False] * Support for attribute based validation * Support for imperative validation Links: * https://github.com/docevaad/Anchor * https://github.com/Grauenwolf/Sails * https://www.nuget.org/packages/Tortuga.Anchor/ * https://www.nuget.org/packages/Tortuga.Sails/ 
Just make sure you aren't using EventToCommand. Last I checked, that causes massive memory leaks.
Shawn Wildermuth if you're reading this the gif on your homepage is really creepy, it isn't cool at all.
But other than certain parameters, it's literally just "open this URL in a new tab"? Like it sounds *too* simple from what I've looked at.
It does sound super easy, but that's how it's been in my experience. 
The worst thing was seeing them slowly reimplement things nHibernate got right years earlier. And adding features that other libraries did much better (migrations).
DbSet carries forward into EF Core with minimal changes. Some things _will_ require changing, but it's not as big as you might expect. 
Yep. This looks like something I could use on almost any project - but it'd be good to get some reassurance about the long term plan first. 
I could've sworn Code used to have more auto-complete support in a previous version. Maybe not. Thanks for the comments.
Bower has a hidden configuration file that you can modify to change the directory folder. The file is .bowerrc and should appear under bower.json when do show all files on the project.
Hmm... Never try it. I host asp on windows platform. It will more stable.
Refer to the "directory" option in [.bowerrc](https://bower.io/docs/config/). Note that by default the wwwroot folder contains content will be published with the application, and may be accessed via a static file/url request when the web application is running. You can also control what's published via `project.json`, example: { "publishOptions": { "include": [ "wwwroot", "Views" ], "exclude": [ "wwwroot/lib", "wwwroot/sass" ] } } It may be better to keep the bower content separate from your wwwroot (and out of version control), and use tasks to copy only what's needed into wwwroot. Information on using tasks in VS Code: https://code.visualstudio.com/docs/editor/tasks 
Not really an answer to your question, but I recommend using node for client side libraries and use gulp/grunt to compile and minify. Bower will remain popular, but more and more projects are switching to node for the tooling.
the projects could use some documentation, they look promising though.
Looks focused more on Mobile stuff than WPF
Pretty much. Would you happen to know any alternatives to RepotViewer? I hate the asp control. Is there a HTML5 version?
There's an [issue](https://github.com/reactiveui/ReactoveUI/issues/687) on this topic with a lot of examples, both mobile and desktop. Also, the [documentation](http://docs.reactiveui.net/en/fundamentals/history.html) is a good way to get you started.
Yeah, I noticed right after I submitted the link. Sorry about that.
I don't actually know the answer to that question unfortunately. 
Windows Server has been shipping Server Core (Windows Server without the GUI) since Windows Server 2008 first came out. 
I mean, you can really use anything so long as it can run on whatever Server OS you plan to target. What is the *dominate* view stack? Id be willing to bet it's new asp.net mvc they're working on. The Razor view engine precisely, plus angular 2. It seems to have many advantages over the current version of mvc. It will without a doubt be the stack most directly supported by MS, which is important for many people. However, there are loads of other view stacks which are worth looking into. Most are in their infancy now, but will be game-changers once mature and available on .net core.
&gt; Again, Snover highlights how important it was to make application development available to more businesses. “[In] Windows Server 2003, .NET really allowed that line of business app explosion…. Before that, you could write a VBScript program, but now you could write a real mission critical application because you were freed up from the minutiae of things like memory allocation.” Spotted the non-programmer in the room.
If you are properly segregating your code, allowing for portability and reusability, you can easily get hundreds of projects in a solution for even a moderately complex development project. The development environment should be able to support this without breaking a sweat.
Thanks. I was adding it with "bower init" but when I add with new file dialogue it comes in this format.
Can you be more specific? I can't imagine the choice of version control has any impact on it. 
For me it is Rethink DB, ASP.NET Core, RactiveJs, and TypeScript 2.0. 
It's like they forgot Visual Basic existed. And I'm not even getting into alternatives that worked on Windows.
I know that Angular is an option, but don't know how popular the combination of Angular+ASP.NET Core is. Does it play nicely with ASP.NET Core? Wouldn't you replace Express with ASP.NET core's Web API? Also, I often use sockets.io with my Angular front-end - don't know how this would work with ASP.NET - would you use Signal R?
How did you choose ReactiveJS vs. other options?
RactiveJs, not reactiveJs. I choose it because it is simply a view library with mustache like template. They are also developed by the Guardian newspaper - instead of a tech company.
&gt; n this release, we extended the CLR debugging APIs to enable the debugger to request more information and perform additional analysis when a `NullReferenceException` occurs. Using this information, a debugger will be able to determine which reference is `null` and provide this information to you, making your job easier. OMG. YES!
What's the difference?
Thousands? Now I now you are just trolling. 
I've given several reasons why having hundreds of projects in one solution is problematic. Your response so far has been limited to "nu uh" and "everybody else does it".
Finally
Depends on the goal really. .NET Core and ASP.NET Core haven't fully stabilized yet (and probably won't for a while) but they are the future. So if you are looking to learn something for the future and have the time to keep up with the ecosystem changes, .NET Core is where you want to start. On the other hand, ASP.NET MVC is battle-tested and production ready now. So if are working on an actual app that needs to be released soon or if you are looking for stability, then ASP.NET MVC is the way to go. 
You don't need to commit to new fancy front-end framework when you switch to ASP.NET core. They are doing ASP.NET with all the mature frameworks like ReactJs, AngularJs all the time, and it should not matter to just switch back-end and keep front-end.
Tooling for .net core is going to feel more familiar to a developer with your background than going for full blown VS, imo. Up to you if that's a positive or a negative. 
&gt; the desktop UI library path of choice has been kind of ambiguous for a while. Nah, MS wants you to use UWP. Oh, you don't want to avoid locking yourself to W10? Well feel free to use Winforms I guess. :( Seriously, this is great news for someone who (very very very late to the party here) just embraced WPF.
i appreciate the reply, I was feeling that going with learning Core would also help level the playing field for any potential job searches in the future, since not as many developers in the MS space are as familiar with it as the older MS frameworks. 
This is the best thing to happen all year.
I didn't even consider the job hunting benefits, but that's a good point. I also want to highlight that once you finish a full MVC app on core, about 80-90% of your code will look exactly the same as if you did it on the full framework. The environment, dependencies, build workflow, and packaging will probably be different, but all of the meat will be the same; going between the 2 will not be difficult once you become proficient in one. Controllers, views, models, etc will be practically interchangeable between frameworks.
&gt; RactiveJs, not reactiveJs. You have sent me down a rabbit hole. I am trying it out and digging it so far for it's simplicity. I have only been at it 15 minutes, but what would I notice if I used this instead of mustache? Edit: I see some other features, but simply for templating would it be the same?
Windows Forms is OK if you have discipline and follow the MVP pattern. Of course, what's more typical is that your peers hopelessly entangle the application logic with the forms.
This guy is probably incorrect but we'll wait and see. My guess is Angular 2 and React in #1 and #2 spots for front ends that talk to.net core web APIs. 
Does this only mean it will work during Visual Studio debugging? As opposed to logging the source in a production app. Regardless, the horribleness of this exception is one of the biggest things that made me fall in love with F#.
Can someone explain why they are still working on the .net framework? I thought they were shifting to .net core. Why are they creating a fragmentation? I was under the impression it was .net 1, 2, 3, 4 -&gt; .net core 1.0 and will move on from there.
Anybody have more detail on this? I just fired up a 4.6.2 project and introduced a null ref exception. I'm not seeing the identifier on what was null.
Top comment chain on that page says MSBuild still is affected. :(
JFC.
.net core 1.0 is still a toy, there needs to be real work done in the real framework until .net core is not a toy anymore (probably in 2-3 years).
Again PVS-Studio spam.
There's a reason MS called .Net Core ".NET Core 1.0" and not .NET X.XX. It is not the upgrade path for .NET applications, not yet. It has many things missing which may be OK if you start a brand new project and don't need what's missing. But at this point going from 4.X to Core 1.0 is not advised. Further down the line future Core versions may have sufficient features to allow such upgrades but not for now. Not to mention all the Windows-specific stuff (WPF...) doesn't even exists in Core.
Supporting customers. The vast majority of .NET developers will not move to Core any time soon. So they are continuing to invest and improve the full .NET framework as well. 
.NET Core is everything that is cross-platform, server-side stuff mostly (ASP.NET). .NET Framework is .NET Core on Windows + Windows specific stuff like WPF, WinForms.
For this kind of asynchronous activity, from the web application you would typically only insert / update related records in a database or queue. You would develop a separate batch process which would then read them and send the emails. The batch process might be something scheduled to run once a day or be a Windows Service.
aspnetcore + rethinkdb. You can use ***dotnet watch run*** to have similar experience with NodeJs development.
Thanks, This is a great solution.
You should probably just abandon the DNN code. Terrible framework.
MVP is different than those but it fits better with Winforms. Essentially you have a presenter that interacts with abstract UI elements of an interface and your form implements that interface. So, for instance, you might have an interface method called ShowError, and your form might implement that by showing a message box. So it prevents tight coupling.
Hi. I'm an intern developer using ASP.NET MVC, and I've been building an SSRS integration menu for the past month or so. ReportViewer is what I used. At a high level, I created a menu that displays the report names and descriptions that exist in the database. When clicked, it redirects to a generic .aspx page (with a ReportViewer) with the report name and parameters in the query string. The query string is then broken down and using properties of the viewer (like ReportServerURL and ReportPath) I am able to dynamically set the properties of the viewer. It's a fairly straightforward approach, but configuring Web Forms to play nice with the project was a fucking nightmare. Thankfully a senior associate figured it out, but not before a day of wrestling with the thing. Again, I'm just an intern. Feel free to ask any questions, and I'll try not to embarrass myself (if I haven't already).
That works for simple application. In bigger projects, having a clean separation between the access layer and the view layer is really important. I am currently working on an application where this separation is weak and the pain is real.
If something works, why change it? Not a very useful comment.
PM sent
It's easy to slice off all new work goes to regular MVC and all pure cms content loads still go to dnn. Then you can embed javascript as nessecary (strongly recommend using IntercoolerJS) to get/post to the mvc endpoints
Do me a favor and look to see if I won the lottery.
We are going to but the issue is this site was created via DNN. and we need to see how much of the work that was done for the site was actually done correctly.
Will you be able to review the .net info and let us know what would be the next step to get out of DNN?
https://docs.asp.net is the official site for the .NET Core docs, as shown on [this page](https://www.asp.net/learn) (click "Learn more" under .NET Core). These docs have always been there, I have several of the pages bookmarked. They're just gone now.
This. Quartz feels like coding in .Net 2.0 by comparison.
Can I have contact information so I can connect outside of reddit.
Yip. Yipyipyipyipyipyipyipyipyipyipyip...
-Absolutely- Yip FTFY
.Net core is for console/"cloud" workloads at the moment. Full framework has a much wider reach. I'd imagine these changes are already in, or will make their way over to .net core.
Seconded. It's a terrible framework.
I've worked with .NET since the original was in beta, and honestly for the future you should go with dotnet core. The old windows and asp.net are quite different in may subtle ways and very tied to windows and very much the past. I see no need for you to learn all that old baggage for a bit of stability today unless you have a critical application to deliver on a short timeline. I would go .net core + c# + asp.net/mvc + entity framework + postgres, on linux and use vscode as the editor.
Old school private void btnLong_Click(object sender, RoutedEventArgs e) { btnLong.IsEnabled = false; currentDateTime = DateTime.Now; Thread longRunningThread = new Thread(new ThreadStart(delegate () { Thread.Sleep(10000); Application.Current.Dispatcher.BeginInvoke(System.Windows.Threading.DispatcherPriority.Normal, new CalculateTimeElapsed(PostLongRunningTasks)); })); longRunningThread.Start(); } Modern private async void btnLong_Click(object sender, RoutedEventArgs e) { btnLong.IsEnabled = false; currentDateTime = DateTime.Now; await Task.Run(someAction); PostLongRunningTasks(); } 
I'd like to forget.
Thanks for your response! I really enjoy C# as a language more than Javascript, whcih is why I want to try out this stack over one of the other ones like J2EE or Rails. I'll test out the stack you recommend! 
Would love to hear about your experience. Please do share what's good and what's bad so we can learn as a community to help others coming from node and similar stacks. In fact was just thinking a complete soup to nuts blog post would be awesome. I think dotnet core is going to really be a great platform for the next decade.
Looks like you're adding/deleting cells as you go, which alters the state of the board before you've evaluated all the cells. You'll have to do it in two passes--the first to determine which cells live or die, and the second to create/destroy cells.
Without a constraint you basically can map Employee to anything (poor API design). From AutoMapper perspective you'll need maps configured somewhere and the configuration requires both UI models and DB DTOs as dependencies, thus breaking layer separation (UI - BL - DB). On another hand, you wanna be explicit about your UI models as Employee is explicit about the underlying table so why not have a few methods in the BL mapping a single Employee to a few different view models (different use cases in the requirements)? In this case you stay type safe with the layers cleanly separated, still using AutoMapper. There is nothing wrong with it as long as you map BL entities &lt;-&gt; UI models and BL entities &lt;-&gt; DB DTOs.
A very powerful tool in the programmer's arsenal. I've used it a couple of times to work around limitations in third party frameworks and libraries. Since you can insert yourself between any non-sealed class' public method calls, you can deal with deficiencies in those classes as well. For example, if a framework gives you a hook for object creation, but doesn't provide full IoC support, you can dispose your lifetime upon exiting the scope of your composition roots by having the lifetime injected into your aspect, and manually disposing the lifetime scope at the end of all other executions.
Been using Azure Webjobs. You can use it outside the context of azure/websites if you wish, but it's a fairly solid framework.
Using EF? Not going to happen\*. SQL Server has powerful and easy to use full text search capabilities, but EF has no way to expose it. You need to use something designed to work with raw SQL (e.g. Dapper, Tortuga Chain) or stored procedures. \*: Without ugly and unreliable hacks.
thanks for the suggestions. New to this so just want to clarify, I should use something like tortuga chain, which allows me to write my own sql statements or call stored procedures? then take the raw result,process it(the non-ef way)(eg. datatable?) then to json? did i get the flow right? 
Yes, it's always trade-off between time/quality/cost.
Just index your data into elasticsearch and you are pretty much done
Agreed. I just used elasticsearch in a project for the first time and it's amazing. 
This looks very interesting. Production quality and well used by some in the comment section. I wonder how I've missed this. This is everything I wished .NET web development on .NET Core would be like. When I go to .NET Core I think of getting quickly up to speed with Visual Studio Code and a quick, small SDK install. I expect Python-like prototyping / development. It can't feel clunky because I may end up with editors far below Visual Studio 2015-level IDE's, moving between both Linux and Windows. Thanks for the heads up!
Just hanging out / loitering in a public bathroom with a gay friend? Doesn't sound suspicious at all...
I like it, a great intro to AutoFac and DynamicProxy.
I just installed VS 2015 update 3 and I got this error message. Attempting to clean or build the solution gives this error: http://i.imgur.com/oLOvvXc.png
Executing raw SQL via EF can be done without ugly or unreliable hacks. See: https://msdn.microsoft.com/en-us/data/jj592907.aspx
But if you are already using EF for other things, I'd use it to call a stored procedure as you recommended in another comment. There's no need to add other frameworks just for this. That said, if my requirements called for anything beyond very basic search functionality, I would definitely look to use something like Lucerne.
P.S. Stored procedure support has been deprecated in EF Core. They may go back an implement it someday, but the for the time being you are forced to use raw SQL.
Is your NuGet plugin fully updated as well? Could be a first place to check.
I'd recommend lucene, the API is ugly and unintuitive, but it's fast and powerful.
Azure search has a free level. Amazon CloudSearch doesn't have a free level but their small instance is about half the price of the smallest paid Azure level. So it depends on what your usage requirements will be. Or you can use SQL full text search and write a small search provider that works with raw SQL or a micro-ORM like Dapper or PetaPoco.
Asp.Net Core is stable right now. The tooling around it and documentation is still being produced though. The problems I have run into with Core: 1. missing documentation in seemingly random places: https://docs.asp.net/en/latest/mobile/native-mobile-backend.html; https://docs.asp.net/en/latest/mvc/overview.html ... 2. how to add a reference to a class library 3. nobody seems to care about suggesting reasonable practices for working with some legacy code (for example our Sybase ASE database doesn't work with EF; thus we cannot use Asp.Net Identity to work with our existing user store and need to hand write some authentication code... since I am writing a prototype multi-tenant web api app for an angularjs front end, I spent a number of days trying to figure out the general design of what I am trying to do) To be clear, I am using the .Net Full Framework 4.6.1 (soon to be .2) with Asp.Net Core. I have nothing to say at the moment on the .Net Core Framework (outside Asp.Net Core).
wait... you mean you can have lucene within your .net application like a library then just publish it together. actually at this juncture i want it to be together and probably only split it out when my needs increase.
ah this is good insight, thanks 
Sweet! This sounds like what I'm looking for, will try it out tonight!
Yea, that's how Lucene works. I think Lucene/SOLR would match your use-case. Starting small, you have Lucene within your application. Growing up, you just need to split it out to SOLR server, code changes for this would be minimum. 
Ah, my bad. I was assuming it's part of an ASP. NET application. There is also Glimpse CLI, you can give this a try. 
Just as a warning to everyone, this is spam; didn't trust the URL so traced where it leads to and it's an Indian tech company. 
Ironic
I see it quite a lot, I don't think it's correlated to NuGet restores, though. I actually see it most often when trying to close a solution or close VS itself.
Use DYNDNS and IIS, it's a doddle to set up www.dyn.com 
Thank you, I found it and fixed it. 
My pleasure!
I finally found the time and changed the target framework to netstandard1.3.
Some quick research suggests that [mono may be viable](http://www.mono-project.com/docs/about-mono/compatibility/) **if** your project doesn't make use of any async/await calls. If you can VPN into the server, perhaps what you need can be achieved by creating a VPN tunnel between the Linux server and the Windows server?
Great to see MSpec on .Net CLI! Its natural language and reporting are hard to beat. Combined with F#s variable naming capabilities some very elegant cross platform testing and user accesible specificatiobs. Great work!
You will have a much easier time having them setup a windows 2012 server. IIS is included in the windows server software as a component you have to select to install. When setting it up you can set its 'role' to be either be an application server, web server (which will use IIS) or both and this will install the basic necessary components for hosting. This can be done during installation or after. &gt;Setting the role is the same as going to Control Panel &gt; Programs and Features &gt; Turn Windows Features on and Off , where you would then select the components needed for your development environment e.g. .Net Framework, Windows Process Activation Services (for WCF), IIS, IIS Hostable Web Core After your environment is setup for hosting you can create a website area under the default website or your own website for publishing directly through Visual Studio to the server. 
You could run ASP.NET Core in a docker container on ubuntu.
Sorry to be a buzzkill: http://www.zdnet.com/article/aws-microsoft-seen-rated-top-dogs-in-iaas-in-gartners-magic-quadrant/
huh? There are like 3 largely unrelated questions in here and you lump them all together in a way that seems rather confused. 1. Something about a VPN'd server. Not sure why this is even brought up. It's behind a VPN, end of story. You can bridge it out of the VPN, but then what's the point of using it at all and having it behind the VPN? 2. Do you need mono? Dunno, does your app target .NET Core or full .NET Framework? 3. Do you need Windows Server? Depends on the answer to #2. I wouldn't run an app on mono in Prod, but if you target .NET Core then it should work fine on Linux. If it runs on .NET Core, then throwing it on an Ubuntu box behind nginx would work fine.
As in I can ssh into my windows machine? 
Correct. You receive a cmd.exe prompt via SSH. I don't think you can change it to powershell or bash, but it's a start.
Great news, I was afraid that this library is dead.
Go into server manager, roles &amp; features and look under application server. Make sure you have .net 4.5 and ASP.net installed.
I tried typing http://localhost:82/Home/Index and it gave me a 404 saying it doesn't exist. (I'm using port 82 for this site and it worked fine with the default IIS website. I also have an Index action method with associated View in the Home controller). I've followed that link too. Thanks for responding.
I used the NEST wrapper which made it pretty damn easy to implement and it being free helped get the project off the ground. The only thing I had an issue with is when performing a search it would search multiple indexes even when I supplied the correct index name. Chances are that's just related to something I missed. 
Hi again, I've got it now. I just deleted my files from the application directory and copied them back in from my own PC. I guess my messing with the modules earlier broke my web.config. Thanks for the help.
Not sure what docs you were looking at but there is an Areas section under MVC &gt; Controllers on the asp.net core docs site. [.net core areas docs](https://docs.asp.net/en/latest/mvc/controllers/areas.html) As far as the error about two classes within the same namespace the common convention is to have your area controllers in a separate folder (thus a separate namespace as well) Using your classes as an example I would have MyApp.Website.Controllers.HomeController and MyApp.Website.Areas.Members.Controllers.HomeController If you do that and follow the directions in the docs you should be all set.
Thank you so much! This is so helpful!
Kind of hopes this means the Linux hyper v integration services might soon provide drivers. Id like to do some ml in a virtual machine running Linux on my windows box. But I can't even get a resolution above 1920x1080 let alone use the GPU for some powerful stuff
Thanks will have a look. 
Check out Visual Studio Dev Essentials, you'll get a 3 month free subscription to pluralsight. They offer very good in depth video courses concerning C#, .Net and IL. Edit: Pluralsight also has a learning path if you have no idea where to start, basically just an in order list of which courses to watch.
In MVC 5 you couldn't share class names regardless of namespaces, hopefully that's been corrected. But you will have better luck calling the Members/Home Controller MemberHomeController. The route doesn't change and it makes the code base a bit clearer as well. 
https://github.com/toddams/RazorLight#parse-strings I don't see how this is parsing anything. This is rendering the view using Razor. Sure, that happens behind the scenes in Razor, but that's only one part of what happens - and surely not the main point of this API.
So you think it would be better to say "Build templates from string/file" etc?
The string or the content of the file IS the template already, isn't it? You're just using Razor to render that template. Just take an ASP.NET MVC application. There you can also say `Render("NameOfTheView", model)`. This is no different.
I'm well aware what Razor does behind the scenes. But the string/file is still the template. The C# code that is generated is just a means to render the template. This is definitely not parsing. Your library just provides an API to render the templates using Razor. Rendering is the correct term here.
Anybody has a mirror?
For the end user this is **rendering**. You're not returning the parsed parts of the template, you're returning a finished rendered string. When you pass a library a C# file, and it outputs valid IL, do you call this parsing too? No, you would call it compilation - even when it does parsing behind the scenes. But that very parsing is just one step. The end result is the compilation. Same as with your library the end result is the rendered content. If your library would return a data structure that represents the parts of the template, then it would be parsing. But this is not happening here. You don't return a structure that contains the textual parts, the @model directive, the rendering instructions (e.g. `@myModel.Foo`). You return the rendered result. I really can't try any further to make this comprehensible to you. If you insist on using the wrong terminology, go ahead. I just tried to correct and teach you, but will no further.
Im a little confused....razor does not generate c#. It is fucking c#. Right?
Check out a few video's on mva.microsoft.com. It has some awesome resources on C# and .NET. You will also find some good resources on ASP.NET. Visual studio is pretty awesome so learning C# is actually fun. When I first started with C#, I used the Resharper visual studio extension. That helped me learn C# quicker but it is not necessary to learn C#.
The cool kids are using webpack nowadays, in case you missed it
Razor parses the templates (e.g. the cshtml) and creates C# code out of it. This C# code is then compiled and executed. For example you have this FubarView.cshtml: @model SomeClass &lt;h1&gt;@Model.Dummy&lt;/h1&gt; Then it will create (example) a class like this: class FubarView { private readonly SomeClass _model; public FubarView(SomeClass model) { _model = model; } public string Render() { var builder = new StringBuilder(); builder.Append("&lt;h1&gt;"; builder.Append(_model.Fubar); builder.Append("&lt;/h1"&gt;"); return builder.ToString(); } } This class is compiled and then executed at runtime.
You want shit quality and buggy software? Helios solutions is your choice!
As a Norwegian, reading your website gives me a bit of a /r/totallynotrobots vibe ... 
Nothing really new here Also I prefer constructor injection because well, makes unit testing your services much easier when you have defined test doubles.
And yet we still cant use nunit or xunit with it, what a joke
edit: Sorry, this turned into a rant that was not at all aimed at OP. Nice post! This approach serves up html/scripts on one server, which uses an API that exists on another server. My approach, [react-aspnet-boilerplate](https://github.com/pauldotknopf/react-aspnet-boilerplate) does the same thing, but uses a single server. Although this is a boilerplate, I really think people should stop using them. For one, you never really understand what libraries and techniques are being used unless you are confronted with the challenges yourself. Also, using React/Webpack/WebAPI/MVC is so simple, that I'd really suggest new projects just roll their own setup. The integration of client-side development to ASP.NET isn't really hard, and I hate seeing the heavy handed approaches used by ReactJs.NET and Microsoft's JavaScriptServices. I could solve all the problems these two projects are trying to solve in 15 min with only a few hand-crafted files and a few lines of code and very few dependencies. People really need to stop accepting the usage of sludge hammers just because. Now that .NET Core is done, I really hope the community slows down in this regard. With that said, this approach doesn't use any of the monolithic libraries though, which I like. I just really wanted to point out that you don't really need to separate endpoints. edit: Sorry, this turned into a rant that was not at all aimed at OP. Nice post!
No, that is absolutely true. I actually _dislike_ that I used two separate endpoints here but for the sake of a tutorial, I thought it to be simplest. 
Cool tutorial, do you expect to do a version for .net core? I'd imagine if someone were going to start a new project today with React (and all associated gizmos) front end that they'd use .net core for the back end rather than the older / much heavier 4.5.x.
Well, for some developers we've interviewed, upstream developers consult with downstream developers/users about what features and bug fixes are most important to them; because e.g. if the upstream makes drastic but unwanted changes, users may lose interest in upgrading and thus no longer get bug fixes. So communication between developers and users can influence the course of development, even though in principle it doesn't have to. In an ecosystem like CRAN where old package versions are less readily available to new users doing a fresh install, we think there may be a little more urgency to the discussion than in NuGet. But our interviews so far haven't been with NuGet developers, so we're interested to know how it's different in the .NET world.
Webpack? That's sooo last week.
Oh, I've had my eyes on your JsViewEngine project for a while! In general I think people worry too much about isomorphic stuff... it's a lot of extra work just to avoid the "loading flash", but if I ever needed to do it again I liked your approach a lot more than React.NET. Thanks for doing all the leg work on that implementation. Curious, whats the reason for using JavaScriptServices instead of something like Javascript.NET, or some of the other JS .NET libraries. Seems like some of those would be lighter weight, or is it just to avoid having to do the multiple-packaging thing (packaging a version for server side and a version for client side)?
I've only messed around with it. I ported a console Telegram bot that I had in 4.5.x to .net core, but haven't done any full on projects with it yet. Next up I plan to convert a smaller web API + angular front end project over.
Yes, if you are lokking for best resume writing service as per your need then should go for [essay republic](http://essayrepublic.com) as it the fimm that provide very good resume for your interview. It writes the resume with proper format so that interview can be impressed with it.
Server-side rendering certainly adds complexity to your applications and therefore entails a tradeoff. Like anything, it is a tool that should be applied judiciously. Due to the complexity involved, in my opinion, it doesn't _currently_ make a lot of sense for most applications (especially not for intranet applications), but it can be really valuable in some scenarios. Like, /u/theonlylawislove mentioned, it's important for mobile where bandwidth is limited. Additionally, it's really important for SEO - [Product Hunt](https://www.producthunt.com) is one such React application where server-side rendering makes a lot of sense. In the future, I can see the complexity involved with server-side rendering decreasing, so I imagine this trade off will become less and eventually (_hopefully_) seamless.
Regarding your edit: &gt; edit: Sorry, this turned into a rant that was not at all aimed at OP. No worries haha 😂 Your comment was actually the fist on this post of mine, and as I was reading it I was like, *"Oh no... The first comment sounds really negative..."* but when I got to the end, I realised your intent. Thanks!
I've tried them all and now just use npm scripts, a lot easier.
Please let someone proofread your articles. A friend, or a family member.
Hmm, so in CRAN if I update my library from 1.3.8 to 2.0.0 I can't then release 1.3.9? Is there anything stopping you just releasing a package onto the stream with version 1.3.9? And dependent libraries set themselves to depend on 1.* and so get bugfixes without the breaking changes.
If you're running on a Windows vm, it might take a while to spin up your ec2. Took us 10 to 15 minutes sometimes. I assume you're using dotnet core so you can use Linux, which works great to my knowledge. I've never used Azure but heard it's not bad. My work uses AWS so that's what I end up working with. If you haven't used your free trial on either, deploy to both in a dev sandbox and see which one you like better. Good luck! 
Depends on your usage (like all things) but usually the performance of your DI will only have a very minor impact on your application.
So, just use an adapter pattern?
I wish I could tell you why I configured it that way. I think this was the convention when it was aspnet5 and I just got used to it. I remember it was src and wwwroot at the same root level. I went through a nightmarish experience of upgrading from RC 1 to RC2 where my mvc app broke all over the place and I had not idea why. That's when I realized my understanding of aspnetcore was very shallow because I just jumped in to MVC (which is very familiar) without understanding the underlying framework. So now I am back to basic trying to explore more of this new stack.
If you just want simple triple des encrypting with a password, just use [ENCRYPTBYPASSPHRASE](https://msdn.microsoft.com/en-us/library/ms190357.aspx?f=255&amp;MSPPError=-2147217396) in your stored procedures that you call to read / write your data. You won't have to do anything in your C# code other than pass in the passphrase. If you're looking to get fancy, there are [lots of crypto libs on nuget](https://www.nuget.org/packages/EasyEncryption/) that are pretty [easy to use](https://github.com/polischuk/EasyEncryption/wiki/Usage-wiki#aes). With these, you'd encrypt in C# then just store the encrypted values in your database. All modern versions of MS SQL support [transparent encryption](https://msdn.microsoft.com/en-us/library/bb934049.aspx?f=255&amp;MSPPError=-2147217396) to secure your data files at rest, so if you're worried about someone popping your hosting company and stealing your database, you're OK (if you have it set up). Once you set it up, you literally have nothing else to do (no C# code changes at all).
I would use .Net's built-in `TripleDESCryptoServiceProvider`. This should get you pointed in right direction: http://www.codeproject.com/Articles/7580/Making-TripleDES-Simple-in-Visual-Basic-NET You'll want to protect your key/iv values. Some people use hashing to generate them from a phrase/string. Up to you. Just use separate values for different types of data you want to protect, so if one key/iv is compromised it wouldn't affect the others. 
Thank you friend, i'll take a look!
Neat. One minor error: Insertion sort and heapsort are both O(1) space, or both O(n), depending on how you're counting.
So, will i be able to set up transparent encryption even though i dont have direct control over the server? I'm doing this through a hosting company, I have the ability to create databases and manage them but no control over the database server...
Right-oh! Thanks, i'm on it!
No problem! In a recent project I worked on, encrypting the entire database was not possible but it was still a requirement that certain fields be encrypted. I used .NET's AES implementation (the NIST-compliant one, [AesCryptoServiceProvider\)](https://msdn.microsoft.com/en-us/library/system.security.cryptography.aescryptoserviceprovider\(v=vs.110\).aspx) to encrypt the data and then store the resulting byte array in the appropriate column in a table. This isn't ideal and adds an extra layer of complexity, namely key management and additional documentation on how to decrypt the data, but it is a possible solution. If you hit a wall let me know and I'll post a trimmed-down version of the code I used.
Ahh. You're right. I would agree with what you're saying - use built in SQL tooling if you can. And probably use AES instead. http://www.codeproject.com/Articles/769741/Csharp-AES-bits-Encryption-Library-with-Salt
Well, the only thing that makes it complex (unless your server is node.js) is the fact that you have to render JS views server side, meaning you are running a JS process somewhere service side to do it. Server side rendering is quite simple outside of that isomorphic requirement, I mean we've been doing that since literally the dawn of the internet. As for bandwidth limitations, I wouldn't say its BETTER from a bandwidth situation to do isomorphic. I mean you are downloading a completely rendered version with all the data AND still downloading the full JS app as well. Without isomorphic its just the JS app and the data you pull, so it's inherently less DATA, just more HTTP calls (for data loading). Whether those add enough overhead to make the isomorphic approach an actual TIME savings, is a different, but related question.
Edge, that's the one I was thinking of. Done a bit with that as a scripting engine for a few things and it worked ok. Looks like there are a few competitors in the space now . You're not a real man until you run your C# code in Node, through an Edge.js abstraction inside of a C# app (haha I have no idea why we tried that, I think just to see if you could). Haven't been bit by the webpack bug yet. It's one of those segregation of concerns issues... when I heard it does things with JS AND CSS I had a gut reaction to the idea of a tool that did CSS, JS, and HTML stuff all in one. :-)
Has anyone tried these tools and web load test projects in VS Ultimate? How do they compare?
DO NOT ENCRYPT PASSWORDS!!! Use proper hashing for that, that's way safer.
READ THE OP BEFORE COMMENTING!!! OP said the data he wanted to store was personal information (probably SSNs / etc). You have to encrypt that, you can't hash it and store the hash of it.
Cool. I actually need a load testing tool for some work I'm doing now. I'll definitely check it out.
Well. Why would Code help? As it runs perfectly fine on another server. Furthermore, I don't have logs as it won't even start, the only logs are the windows logs which only say "Service started" "Serviced has been cancelled"
You should log every single action your service does. Starting is one of them. Wrap your Main in a try/catch block and log it.
I did. And that's the thing. It doesn't even get to the point where the logging begins. 
This is likely the issue. Been there before. 
In your start method for the service add debugger.launch() that will force it to start up the visual studio debugger, you can step through your code then and find the culprit
If they are worth anything, the technical interview won't count for much. Unless they are doing something very specific and/or very ground breaking, they mainly want to find out if you are a freak who can't communicate well or work well in a team. My source is having done the hiring of "junior C# developers" for the last 15 years or so, among other things. 
Abstract and delegates arent used everywhere but I would say Interfaces are huge. If you are not comfortable with interfaces you are a junior, in my opinion. Abstract maybe not experience with but definitely know what it is, and why we use interfaces instead of abstract classes.
Maybe some basic oo stuff, explain inheritance, difference between abstract and interface, Doubt they'll ask you any specific c# questions 
Interfaces are absolutely important, and when used correctly are extremely powerful, especially with the advent of dependency injection. But I have been asked about all 3 (Abstracts, Delegates, Interfaces) and they are so rarely used in most code that it didn't even matter. And even by design, anything you could just google is not a good interview question. A better question for all of those, is how would you do X? And let them bring up those facets as solutions.
Well I imagine as a PROPER interviewer, when I ask about abstract, delegate, interfaces I'm not asking just for definitions but I will want to know how you used them before, ect. That being said, I wonder what kind of jobs do you work at where Interfaces are rarely used. In my past two jobs interfaces are vital and extremely common. I do like your idea of a question though. Show them a sample application to have arrays of different brands of cars and ask them to make a single list and process them ect. ect.
Most of the places where I have coded before (now I manager coders so I am now about as out of touch as it gets) most of our projects were small to medium sized with fast turn arounds. Larger applications we always inherited and was build as if it was a small application that was iterated to be massive. Largely visual, mostly POCO in/POCO out, much more JS heavy.
Unrelated, but interfaces are basically the same as Haskell Type classes, yes?
https://msdn.microsoft.com/en-us/library/ms173156.aspx Not sure... don't know haskell at all. But an interface is along the lines of a contract, and we know we can perform certain operations or get certain data on any class as long as they abide by that interface/contract. We can define an interface as having: interface IMammal&lt;T&gt; { int GetWeight(); int GetLegs(); void Walk(); } So I can apply that interface to other classes like: class Human : IMammal { int GetWeight() { return CalculateWeightOfHuman(this); } int GetLegs() { return 2; } void Walk() { MoveLeg(left); MoveLeg(right); } void Talk() { SayWords(); } } class Dog : IMammal { int GetWeight() { return CalculateWeightOfDog(this); } int GetLegs() { return 4; } void Walk() { MoveLeg(frontleft); MoveLeg(frontright); MoveLeg(backleft); MoveLeg(backright); } void Bark() { DogBarK(); } } So the interface assures us that if a class implements the interface "Mammal" it will have the functions GettWeight(); GetLegs(); Walk(); and they will all have the same signature. interfaces don't care HOW these functions are implemented, that is up to class to decide. But we can get an array of IMammals, and then tell them all to walk, and we won't care if it's a dog or a human: void MovemMammels(List&lt;IMammals&gt; mammals) { foreach(IMammal mammal in mammals) { mammal.Walk(); } } Disclaimer: Yes I know not all mammals walk, it's an example. Edit: And I forgot to mention that if it implements an interface, it doesn't have to ONLY have those properties or functions, it just has to implement them at a minimum, as shown with Bark() and Talk().
Downvote for the clickbait title. All I need to know? C'mon.
If you use Windows Server, then it will be easier for you. ASP.NET will work smoothly on Windows Server. I always use .net in windows server, though this newest .net Core can run on Ubuntu or Linux, but I don't care. It will save time to deploy it on windows server, I'm more familiar with this system
Why not try asphostportal? They have good reviews and my personal experience also good with them
Hi Liam, Please check your code again. From the error message, the problems is on your code
My list is shorter: 1. Don't be an asshole 2. Separate your code into distinct components. Doesn't have to be AoP, just logical and easy to understand. 3. Good variable names
1. It's installed 2. it has and all the .ddls are with it 3. it doesn't since it won't even start properly 4. I don't quite know if it does, how do I find out? 5. Gonna try that Bonus answer: I actually don't know. Company may need it, who knows ...
It's lose not loose :) Another thing - theoretically the random section could go on forever. It's highly unlikely, but if the random value kept choosing a square that is filled, rather than an unfilled one it would keep on going. Another way to do this could be to create an array of unfilled squares and then chose a random list item as the coordinate to use.
Are there an "Console.WriteLine"'s in the service? Comment them out and see how you go.
If you've got a lot of tasks then a service hosting a task scheduler (like quartz.net) is probably easier. A service can also maintain state, which may be important.
What sort of trouble did you have? I wrote a quick guide to creating one recently [here](http://flukus.github.io/2016/07/27/2016_07_27_Creating-a-Windows-Service/)
there aren't
Would it disable compile on VS project build? 
&gt; Be ready to say you don't know the answer to a question, SO important. When I interview a Junior, I'm often looking specifically to see that they aren't going to try to bullshit their way through. If you're a junior I **expect** that you don't know everything, and that's fine - if you knew everything you'd be senior and getting paid more! But I sure as hell don't want you to waste 3 weeks pretending that you do know what you're doing and making a mess of it.
If he is really a senior developer then he should understand the concept of a code first database and should have experience with using different ORM's that can apply this.
Assumption: Stand alone database - i.e. not an existing database the company uses. This will be a fresh new one. &gt; I feel like migrations would be easier to check-in and changes easier to do. Personally, I would never do code first again on any project other than something I'm just messing around with for fun or something that has a shelf life of say 6 months or so. It's just too much hassle managing migrations and getting people to remain disciplined compared to the benefits you get. You're always going to get that dev who adds something by mistake and instead of creating another migration to correct that mistake, he/she will modify that migration after checking it in and you'll be in a world of pain. 
What is "Code First"? What little I've found on the subject it sounds like you start with classes then generate your DB schema from them. It sounds like a nice way to get a project off the ground, but having worked with generated schemas in the past, probably isn't a good way to go if the project is meant to be around for a while.
&gt; I feel like migrations would be easier to check-in and changes easier to do, but I'm not sure if this is even true. Aren't you already doing this? Databases should be under source control too! Every change to the schema should be versioned.
I agree and have tried, but I've never learned this. I have no clue how to go about this and so far my research hasn't been to great. 2 year CC course and now working in a huge company, for a total of 4 months including my internship.
It doesn't matter, conceptually easy. I don't do .net and I watched a 10minute video. It's a pretty basic concept as long as you can get by the whole convention side of things.
Sounds like when you dev'd this tool it was in a shared database. He probably just wants a db with only tables relating to your tool so he can quickly understand the whole schema and not waist time. Probably just wants what you already have, working in isolation and not mixed with unrelated data. And really you should just ask. Email, or wait for a good opportunity to talk to them, or if you do morning standup, mention it as a block. 
Funny, I've had the same sort of issues with database first. Frankly I've come to the conclusion that ORM (specifically EF) is generally not worth it unless you're fully designed and documented before you begin coding. My shop simply isn't disciplined enough in our approach to not wind up fighting with the tool. Moving forward the plan is to implement a Repository to decouple the code and the database to give us wiggle room on either side. This seems like the most pragmatic approach to me.
Write the SQL to do it. I maintain a couple mission-critical PostgreSQL databases and instead of futzing around with code-based migration tools I just write the CREATE/ALTER/DROP statements by hand, it's really not much work and I'm not trying to figure out how to get a migration tool to output the DDL I want.
We use roundhouse for the project I'm working on currently. I usually do the db changes directly to my local copy to make sure the SQL is right, then we commit the file to a db migration project. Roundhouse runs through all the scripts when we do a deploy so all environments are all up to date. Also makes it super easy to db restores because you just restore and run roundhouse and you're back up and running.
Yep, experienced that check-in conflict today. Not cool. 
Yep, that's part of the process. In interviews I try to get any developer into a line of questioning they can't answer to 1) see that they can admit they don't know and 2) see what their process is when they don't (hint: "google it" is often an appropriate answer).
We recently switched to using a visual studio database project to manage our db. Despite some minor issues, I like it. It allows version control, it provides a local db for testing and it with it registering as a data-tier application it prevents last one wins scenarios. I build some infrastructure code to handle migrations. I can put them up as gists if it would be helpful to anyone here. The process was inspired by how stack exchange handles their dB's. 
This is not what the article says. Your TL;DR is probably your opinion. The author doesn't rule ASP.NET Core out for production apps. Here are the core statements: &gt; If you are looking for raw performance, ASP.NET Core would be my choice. &gt; If you are looking for something that is tested and has been running major websites for almost a decade, MVC 5 would be my choice. &gt; If you want to be on the leading edge of technology using nightly builds and you’re comfortable with updating and upgrading your code, then ASP.NET Core would be my choice. &gt; If you want the flexibility of developing and running your application on multiple operating systems, then ASP.NET Core would be my choice &gt; If you want a framework with thousands of packages and an active ecosystem that has curated its packages for the past 5 years, the MVC 5 would be my choice
No it's not. Server costs. $40 dollars a month including hosting on Compose.io
The only difference I see there is the multi-platform statement.
Just use topshelf and you don't need a Task Scheduler.
Incase you don't read my answer, Topshelf achieves all of these things.
Hmm, not quite. There are billions (trillions?) of LOC already written in .NET applications, and Microsoft would have been insane by stopping their cross-platform efforts and having everyone rewrite in HTML/JS/CSS. VSC was already released before .NET core was announced/released, so it's purpose is more than .NET, but HTML apps in general.
The problems one would solve in core and mvc 5 are not that different. The patterns are the same it's just a bit different set up. Core is definitely ready for some enterprise use on a case by case basis. The biggest challenge is 3rd party libraries not updating fast enough. Telerik has been slow for example. But redis had stuff ready for rc2 days after it was released. There's even an awesome boiler plate project for reactjs running on core with server-side rendering. 
Didn't VS Code come out of a garage project?
[No need for topshelf](http://flukus.github.io/2016/07/27/2016_07_27_Creating-a-Windows-Service/)
&gt;Microsoft clearly felt it was a better solution than their own. I... just, no. That's beyond naive.
Topshelf takes away the pain of DI and coding around stops etc.
Check out a tool called EFProf, it can help pick up on specific EF implementation issues. With lazy loading enabled it's really easy to shoot yourself in the foot with Select N+1s if you've missed an include. Next, I'd be checking the linq statements, to make sure they are appropriate. Use 'Select()' for projection to not return more data than you need. Use 'AsNoTracking()' for queries where you don't need to track the state on the returned entities. If the perf issues are still in the actual query execution, revisit your indexes. Typical the perf issues from using EF come from the overhead of the framework, not the SQL it's generating.
I might add: *If you need to use some sql db other than postgresql/sqlite, then ASP.NET Core is not yet your choice.* :(
Avoid code first like the plague. Stick all of your database scripts in source control, numbered so that they are run in the correct order. If using SQL Server, database projects (SSDT) are awesome.
Am I the only one who writes my own SQL and my own models?
Are those the only databases supported? No MySQL?
SSDT SQL Database project generating a DACPAC for migration. Only really works for SQL Server of course, but I haven't found a better solution. The compile time object reference checking is absolutely VITAL in a larger app. Version controllers SQL scripts lack that capability. Tools like RedGate supply some version control options but I don't think they give you that checking. The fact that you can write an sproc, etc., and it'll check immediately and catch that you mispelled that table, or that synonym isn't valid, or whatever, is fucking fantastic. Code first / migrations are... not well versed for larger projects. Over a certain size of project OR team, they become more trouble than they are worth.
It works fine with SQL Server, unless you've run into something I haven't. 
Until Oracle decides to release the client library for Core, yes. Or you can use [this one](https://github.com/SapientGuardian/mysql-connector-net-netstandard), which seems to mostly work, but definitely not official.
Hi Liam, OK, that's nice. :)
You could also forget about multi-platform until they support Core and target net452 and use any .net 452 library you want.
I am little surprised at the little support for code first development for databases. I was skeptical at first to this approach, but have come to like it. Maybe it because I have been developing applications for almost 20 years. But after taking a new position 8 months and using the code first approach, I don't have to worry what type of storage my data will be, because it is all defined in the model. I can define everything from the table name, fields, type, storage requirements, null able, and all relationships very quickly. Once that is done, I can focus on interacting with that model through interfaces and controllers, use tools like autofac to aggregate data and view models to display it. I have everything in one place, one project, version controlled with multiple programmers making changes to every piece. The biggest hurdle I would agree is having everyone disciplined enough to make sure the migrations are done correctly, and model changes are checked in promptly for everyone to use. Looking back how we did things 10-15 years ago, I really like the separation of each layer. I think it would hard to justify for small simple projects, with one or two developers, but working on large complex projects with many moving parts, having the control over changes is essential.
For an in the wild scenario, my company uses Entity Framework Code First to manage our database from code. To keep issues with migrations down, we restricted the team to 2 members (including myself) which are allowed to make database modifications which require a migration. Everyone is also firmly educated to not make any modifications to databases directly and to coordinate with us to get whatever fixes in you need. We tend to build a new feature which requires database changes at the beginning of the sprint, deploy those migration files to everyone's local dev environments and then they can build all the upper layers.
Heh. My name's not Ryan, maybe you've got me confused with another user? Regardless, the .NET world appreciates the hard work you and other authors have put into it.
I like the code first syntax, but I agree on not using migrations. The irony is that code first is actually the only legitimate option for badly designed preexisting databases.
That title needs more buzz words.
https://www.microsoft.com/en-us/download/details.aspx?id=49924 I've fallen in love with that little tool.
EF 6 isn't "optimized" so much as "somewhat less sucky than EF 5". For better performance use something like Chain or Dapper. (Dapper is better known, Chain requires less manual SQL writing. Both are significantly faster than EF.)
no MySQL indeed... yet (that was my problem)
Because legacy is a bitch.
Do you have lazy loading enabled? If so you might want to check your queries because you might be taking a huge performance hit when EF has to go load relationships for a long list since it does one at a time. I recommend you verify your queries and eager load the child entities you would be working with. 
"Almost a decade" o_0
It's back: http://stackify.com/15-lessons-learned-while-converting-from-asp-net-to-net-core/
Then it was exactly that.
It *sounds like* your customer is reporting virtual size, but the fact that you have a 21GB dump is confusing. With Server GC you'll tend to see much higher virtual memory usage, for example this program shows 17 GB of VM reserved on my machine (Surface Book with 4 processors/8 GB RAM): class Program { static void Main(string[] args) { Console.ReadKey(true); } } This is because Server GC uses 1 heap per core, and each heap allocates gen0/1 by using 4 GB segments when you have &lt;= 4 cores. It uses 1 GB segments when you have 16 cores, so at a bare minimum with 16 cores, you'll see 16 GB of virtual memory reserved. (These numbers are for 64-bit - [see here for more numbers.](https://msdn.microsoft.com/en-us/library/ee787088.aspx)) WinDBG's VMStat could help show if the memory is merely reserved or actually committed.
It's definitely 21 GB committed. It has over 40 GB reserved, and HeapStat shows 19 GB of stuff in Gen2. I just can't see what it is.
With Identity Server I'm a bit lost with the users management part. The samples I've seen don't show anything about that. Where should it be implemented? Is it the Identity server that should add some views to manage the users or should it be done somewhere else? Same thing about the access rights to the different services, is it the role of the identity server? Our needs implies that customers will have to have an "Admin" role that would allow them to manage their own users. They should be able to do it manually or automatically (through their HR management system for example). Obviously they should be able to see and manage only their own users, so there should be a kind of separation between users based on the customer's company. Moreover, we might have inside the same organization multiple admins that will take care of a smaller subset of the users.
But Azure AD would mean hosting the app on Azure as well, wouldn't it? Would it still be possible to have "on premise" installation? It's something I had thought of, but I'm always having a bit of trouble understanding what is it exactly that you can do with Azure AD and how it works. Would you happen to have something that explains a similar use case as mine with Azure AD?
IDSvr isn't responsible for user management at all, just authenticating them. The idea is your users are stored elsewhere, managed by a different system - any system you want. You just hook into it. &gt; Same thing about the access rights to the different services, is it the role of the identity server? Ish, You need to look into OpenId/Oauth2.0 to get your head around this - it revolves around the idea of scopes and claims, which you can specify on a user and client basis. You can use the EF persistence layer of IDSvr to store all that and there is an IDSvr Admin plugin if you want, or again just hook into some external system.
So. Yes. One option is to host completely in Azure. It would just be standard openId. But there is a sync option, so you could federate your internal AD accounts and sync them with Azure AD. So that would require an on premise active directory but also gives you the option to sync up with a cloud hosted solution. Guessing it may be more complicated to setup in practice but it does hit a lot of bullet points. Plus with multiple customers they could federate their IDs to your azure ad. I don't have any specific examples right now (on phone) but Microsoft does a really good job about documenting some use cases. I'll see if I can find something later. But their main azure site should list some resources. 
.Net Core isn't about creating the first cross platform framework. It's about making .NET cross platform. What's wrong with having more than one solution to a problem?
OK, that's where I'm lost. How can you have authentication and user management separated? Our use case is that we have customers that will maybe have an AD or something like that, and it might be indeed useful to be able to authenticate through that. But for most customers, they'll probably want to be able to simply manage their customers in our system. Meaning they should be able to add/remove/activate/deactivate the user and manage a bunch of information on them. With Identity server how do you do that? You have to develop your own users repository? Is there an already built solution I could use?
I'd recommend giving Stormpath another look. It has everything you're looking for: * Manage, authenticate, and authorize your users for multiple applications * Active Directory, LDAP, and SAML integrations * Social integrations (Facebook, LinkedIn, Google, Github) * Strong separation of user data in a multi-tenant scenario There are .NET SDKs that make it super easy to add Stormpath to a .NET project. (It's only [one line of code](https://docs.stormpath.com/dotnet/aspnet/latest/quickstart.html) for a simple ASP.NET app). To address your concerns: * On-premise support - Stormpath is a cloud-only service, so you can't deploy on-premise. However, you can set up a private deployment in your own Amazon container, which is close enough for a lot of our customers. * The above private deployment option also gives you more control over the geography. * Stormpath's policy is that you always own your data and shouldn't feel locked in. At any time, you can export all of your users and any other data. Disclosure: I built Stormpath's .NET SDKs :) if they break, you can blame me!
Okay so the point of IdentityServer is to give you an OpenID/Oauth endpoint that your applications can connect to. So take AD for example. That's great and you can connect into that to do some authentication really easily on a windows machine, but how do you do that on a website, or an Android app or whatever? That's where OpenID comes into it - there's all sorts of client libraries that let you connect into IDSvr as it's all a standard protocol. Now say you want to use multiple user systems - say AD and maybe Facebook and Google. You could write 3 login systems for every application, or you could go through your IDSvr and have a single login process that can handle the 3 of them. You build a simple class that IdentityServer will call to handle your authentication, that class can hook into your actual user system and validate the user or not. &gt; But for most customers, they'll probably want to be able to simply manage their customers in our system. Meaning they should be able to add/remove/activate/deactivate the user and manage a bunch of information on them. Okay so IdentityServer is not this, I don't know what to suggest for this component simply because I've never looked into it. AD is a good choice if that works, or roll your own. There are probably other user systems, possibly Azure AD B2C (though that also gives you the OpenID endpoint). Another thing worth noting, IDSvr can issue authentication tokens (such as JWT), which can be validated by other systems without ever having to connect to your user systems.
Stormpath and Auth0 are really amazing products. The amount of time I have saved by not having to build my own auth flow is amazing. Both are free for dev versions, and are very reasonably priced when you go live. I don't know how much your time is worth, but I bill out at 85 an hour, 50 bucks a month to stormpath is MORE than worth it for me; I have spent weeks tweaking/fixing auth or user mgmt issues before I went managed. Do what suits you best, but I am hard pressed to think of a situation where Stormpath or Auth0 would not meet your needs magnitudes better than what you could do yourself. 
nice article, thanks for sharing. Not sure if this was intentional or not(it gave me a chuckle): *"And configuring any of them is ass simple as scheduling a job"*
You're preaching someone who is convinced that buying a solution from people who it's their job to build a secure, scalable and complete system is definitely better than building something ourselves. Unfortunately it will not be my decision, and my management doesn't see all the benefits and only see the (limited in my opinion) risk of having their customer data not hosted in a solution that we don't "own" and built.
Thanks for the explanation, it's now a bit clearer. It definitely helps with part of our needs, we want our solution to be able to authenticate for some customers to their AD for example, or some customers asking us that users could connect with their LinkedIn account. But I still have to figure out the other part of the equation, the users management for customers who don't have AD, or don't want to use it for that, and putting everything together. Thanks for the help!
Saying it's just a demo isn't an excuse to use MD5. You don't have time consistent hash comparisons, so in addition to using a crap hash you also have a timing attack. Then you go onto use a constant salt, rather than an individual salt per entry. You're also assuming ASCII, and .net strings are unicode, so you're now going to barf on anyone that extended characters.
Ah, I am sorry they are pushing back so hard; I know how much it sucks to have to try and save someone from themselves. Best of luck.
Most people that suggest MD5 even for most examples usually don't really know much about cryptography or pay to much attention about it. OP if you read this at-least change it to SHA1 and say using MD5 is a very poor choice. TBH the entire site looks like it's just a massive collection of others peoples guides posted somewhere else.
There's two ways that you can approach this with Stormpath: * In your tables, have a foreign key to the Stormpath href of the Account. The Account href is unique so it works fine as a key. You do have to make a call to the Stormpath API to get the details. * In some cases, it makes sense to store data alongside the account directly in Stormpath using the Custom Data feature (unstructured JSON). You can definitely export your data from Stormpath anytime you want, so you could potentially do this in sync with your database backups.
If I understand your problem well enough, you're trying to render more table data after the page loads. there are two ways to do this. 1. Use pagination of the current view with a page query string. (default to 0) they will determin what section of the data to render. 2. use partial single page application techniques. I.e., create a web service that will hand you the data you want and use that to paginate/populate based on the query parameters you send it and update the page DYNAMICALLY. 
Before reading this, I had assumed it was the usual "we are the best, we do smart stuff, blah blah, here's your path to profit!", but no, it's actually well written and full of intelligent opinions. Thanks for sharing.
As already mentioned, if it ain't broke don't fix. I actually knocked out a back office app that used GridViews very recently because I knew I could do it in no time and if I hadn't the app would probably not have got built. Web Forms get a lot of criticism but they're the .NET answer to Rails in a lot of ways - you (may) have issues later on when you scale but for prototyping, ease of use and productivity they are better than a lot of the more celebrated alternatives.
Is there going to be a benefit to converting the site to MVC?
&gt; REST has a ton of momentum right now but its missing a great discoverability story Which is pretty ironic considering [HATEOAS](https://en.wikipedia.org/wiki/HATEOAS) is one of the key features of the original REST architecture. The difference between theory and practice.
With the newer versions of the framework you can run both forms and mvc in the same app. Use forms for what you have now and add mvc later as you learn it.
No
Hey bro, I have set up idsrver at work, if you go to thinktecture.com they have a user management tool that idsrver can work with. If you're using AD I have some working code that can get all your claims, auth and access tokens (auth token is who you are, access are used to secure apis)
You don't explain password security to someone completely green starting with bcrypt. There is a big difference between having all the right measures in place when you're developing something serious, and illustrating a complex combination of techniques to someone who hasn't touched a single one of them.
Did it this year, been a web developer for a few years. There is a lot of new azure stuff in the 487 exam, 480 is very easy and 486 is not too hard. You can pass by simply solving the exam dumps, as there are only like 8 case studies for each exam which are recycled. 
Examcollection.com sign up for a free account. You will then need a VCE player. What I did was download blue stacks android for PC then found the VCE A+ app apk and installed it.
Really enjoying this series and seeing a different take on ES than what we have implemented. Our events for example had an `[AggregateId]` attribute on the field that was the AggregateId. And aggregates have methods named `Handle(SomeEvent e)` rather than implementing `IHandle`. I kind of like having the interface, since you can just look at the class definition and see a list of event types it can handle.
;) fixed it. thanks :) Glad You liked it.
:(
Don't. It is completely worthless.
No you aren't -- I can assure you. =P
Personally, my employer requires proof of continuous development for reviews. A new exam every year (that they pay for) is easy proof and it looks great on my CV. I've learned a few nice tricks and best pracrices I might not have come across otherwise. And I got really good at CSS selectors! I've moved on to exams outside the MCSD now, recently the SQL exam taught me some great things that as a developer day to day I just had no experience of. However I do think that if you're just getting question dumps and memorising them (which it seems like a lot of people do) then you're not going to get any benefit out of it. Actually studying and understanding it had benefit I thought. 
This [guide](http://www.davepaquette.com/archive/2013/12/30/so-you-inherited-an-asp-net-web-forms-application.aspx) helped a lot when I converted our app from web forms to MVC. It's mostly just moving stuff from one project to the other, adjusting namespaces, and using visual studio's convert function. Then it's just as jstillwell said, you can use MVC features alongside web forms. If you're asking about converting the gridviews themselves, it's as jewdai said, you'll populate the grid yourself on the client and will have to recreate a lot of the benefits of the gridview in JavaScript or some other technology.
This. It's been about a decade since employers cared about Microsoft certifications for developers here in the States. You're better off working on your github portfolio these days.
Agreed. I just got a new position and they told me my github made me stand out. They liked that I wasn't sitting on my but while not working.
Why not do both. As an employer I can say that I'm impressed by a combination of format and no formal learning.
You can check for null outside the lock this will speed up getting the instance, just remember to check for null again inside the lock. You can also use Lazy&lt;T&gt;
Your solution is awful. It locks **every** time you access the `Instance` property. That's just unnecessary and slow.
I recently completed the MCSD certification. I'd definitely recommend that you do some preparation before hand. The exams cover a lot more topics than you think! You can check out the exam study guide on my blog - https://www.shanebart.com/?s=70-
http://csharpindepth.com/Articles/General/Singleton.aspx It's nice to write stuff, but this is much more detailed and has the correct implementation most people should be using. 
The HTTP/2 serverless cloud.
When MVC was first released we asked ourselves, "Should we stick with the tried and true WebForms or move into the unknown with MVC?". We went with WebForms. A few years later we re-wrote it in MVC.
[Optional dependencies](https://docs.npmjs.com/files/package.json#optionaldependencies) are a part of the spec with npm, and it's to be expected that in many cases they will be 'unmet' (hence optional). If they bring down VS then it's not an npm issue, it's a VS issue. 
They don't actually bring down VS, it's just the UI reports that not all packages loaded. Everything still works. It's a bug that needs to get fixed, but it's not a showstopper.
Maybe I should have said 'break' instead of 'bring down' - but the gist of my point was that it was a VS issue, and not npm. Doesn't much matter in this sub though - I said something negative about VS, and thus the downvoting begins...
Then build another connection in singleton? You can have multiple singletons of course, if they serve different purposes. 
The business versions of Windows come with Hyper V and you can load VirtualBox or VMware on any version. Why buy new hardware when you dev box has plenty of CPU to spare?
The article is about choosing that dev box, specifically one that does has good hardware support for virtualization. Exactly how much of it did you read?
Excellent article. Thank you. For those interested in a completely open source solution, I have another option. Recently at work, we needed a scheduling solution, and we decided to use Quartz.NET. After building out the project, I knew a better solution could be created and it served as the inspiration for my open source project. http://chroniton.net/ It is still being actively developed and does not have the full feature set that hangfire has, but I believe it's off to a very promising start. A dash board like the one in Hangfire is still a ways off, but serialization is coming in the short term, and all consuming code is fully testable through mocking, and all aspects of jobs and scheduling are fully customizable.
That's interesting then :)
Ding ding, we have a winner.
You say you're using MySql but your connection looks like it's trying to use a Sql Server LocalDb. So what are you using? 
connectionstrings.com is super helpful. Usually my first stop for stuff like this
I think you're referring to SQL Server 2016 which you can connect via the strings listed on this site https://www.connectionstrings.com/sql-server/
Thanks, I already looked there and I see no versions above 2012 
Too get the proper version you need to point it to the correct instance. Maybe I'm mistaken but you couldn't have two mysql versions running on lacaldb unless they were on different ports. I.e. I have sql 2012 and 2014 installed. When I need to get into the 2012 its localdb\sql2012 and when I need the 2014 its localdb\sql2014. The second part is what I named the instance. You should be able to find that by looking at how you're connecting to it with whatever db management software you're using
thanks, I got this working with "Data Source=(LocalDb)\MSSQLLocalDB and setting the User instance to false. After that I could update the database and everything works fine.
Your post history is just you, /u/gigilabs, promoting your blog. That's all I have to say about that. You're a self-promoting spammer. And I've reported you to the sub and admins alike. You could be writing the best articles ever: You're still just a self-promoting spammer. Deal with it.
Of course I share my articles. I write them to help others... do you think I write them just so that I can read them over and over?
I don't care why you write them. [Read the rules of this website.](https://www.reddit.com/wiki/selfpromotion) You're breaking them. Do you honestly think your good intentions mean anything? Do you honestly think rules don't apply to you?
How does that differ from this: public class Singleton { private Singleton() {} private static Singleton _instance = Singleton(); public static Singleton Instance =&gt; _instance; } Seems to me that you are just duplicating the lazy initialization for static variables that is built-into the .NET runtime.
Agreed, but what does that have to do with this post? Just read it already and stop assuming, you're embarrassing yourself.
16GB maximum, according to the spec sheet. Please RTFA, it's not a fit. 
I'm looking to get a homelab working with Azure Stack. That requires 16 cores and 128GB RAM minimum. That's not most people's dev boxes. The costs make me sad, especially with Open Stack being free.
Thanks for taking the time to write and share your blog post. Yeah, it's 2016 and SQL Injection wasn't just discovered yesterday, BUT that doesn't mean there isn't a ton of code out there that is still vulnerable or that there are still developers who lack basic security training! Your article is simple enough for beginners to go through step by step and see what the problem is, and how it can be avoided. 
&gt; Would you bitch about Scott Hanselmann sharing links to his own blog posts? Yes. I'd have more to say about it too: He'd be wasting his time doing a shoddy bit of marketing. Rules are rules, dude. I don't want to see every basic article re-written by every dotnet blogger posted to this subreddit over and over and over. All he's doing is rehashing tutorials he's taken himself. Peruse his blog if you like. Comparing this guy to Scott Hanselmann is ridiculous. 
What's the rrp of that? Looks awesome. 
You need to explain the problem better. I don't know what an "email lightbox popup" is, nor what is meant by "capturing an email". But even so, this is likely a problem with many different parts that you need to isolate and find help for individually.
Have been using Couchbase with .NET for 3 years or so now. It's great!
&gt; but at least the majority if Azure's stuff is named for what it actually is What's wrong with Glacier, Kinesis, RedShift, Route53, Elastic Beanstalk, SES, S3, ... ? :D
Glacier is the best named one IMO and S3 only makes sense if you know it as Super Simple Storage. It's nuts!
Agreed, fantastic article and well worth its length.
What's kind of interesting about AWS is that it took what was a competitive advantage of Amazon (compared to smaller online retailers) and instead of keeping it as a competitive advantage, they sold it as a service to other companies so that they could also have a similar competitive advantage, thereby creating a new market where one didn't exist before, and one where they had a head start. Would be kind of interesting if this were a regular pattern, resulting in continual renewal of companies and creation of new markets.
You can integrate MVC with SSRS through a Nugent package called MVC report viewer - https://github.com/ilich/MvcReportViewer/blob/master/README.md I've not used it to embed reports, but to allow downloading pdf and excel versions of reports. Quite straight forward to setup. I understand it can embed reports within a view as well. Parameters can be passed and reports downloaded through any action request.
Sounds like you have a terrible Microsoft Partner if you're having issues with support, they certainly aren't the greatest (and I can't think of many big software companies that are) but your partner holds the keys to the kingdom with any on-premises or Office 365 related support issues - get a better one.
I agree that the naming convention is much better on Azure than AWS. But AWS has working more working functionality despite the mix of links and ugliness that makes the mess of an enormous code base work. Personally, I love Azure more than AWS though it does need some more work. Azure storage is awesome by the way, use that instead of S3 :) 
Eh, my company is looking into starting do deploy our app in AWS or Azure... by my company I mean me. I'm liking AWS better and feel like it's gentler... but we're deploying a Java app on Tomcat with MySQL. RDS with MySQL kinda seals the deal.
You and me both :)
Amen, Brother|Sister.
Last I used it, it was more difficult to view and modify items on Azure storage using Azure portal. The AWS equivalent of blobs, tables, and queues made it easier to do some simpler tasks on the website, whereas for Azure I had to use commands or open Visual Studio to complete my desired action. Additionally, the new Azure portal site was missing a lot of features that used to exist on the older Azure management site. So for certain things, I had to switch between new portal and old site, but I think most of that has been fixed now.
Simple Storage Service
Can anyone confirm this?
Are you using navigation caching? I just tried the following, and it didn't show any `x:Bind`-related leaks: - Add some `x:Bind`s to a view (which explicitly disables caching) - Start a memory profiler session - Navigate to the view and back (to initialize any static stuff) - Take a snapshot - Navigate to the view and back - Take another snapshot No difference in bound objects, pages or ViewModels between the two snapshots.
You can automate deployment to almost any host, certainly AWS. I always set up the build server to deploy to test/staging automatically. 
CodeRush Roslyn maybe ?
Holy shit, I did it. Added this to my controller: TestModels someTest = new TestModels(); someTest.words = "Boogy!"; return View(someTest);
The Scroll Bar Map Mode is the best feature added to VS in a long time.
upon scanning over this document posted earlier today to this sub I think it is a reasonable overview: https://weblogs.asp.net/ricardoperes/moving-to-asp-net-mvc-core-1
There's this for mysql: https://github.com/bgrainger/MySqlConnector
Try this: https://github.com/bgrainger/MySqlConnector
Look at WebAPI. If your schema is json then it should just be a matter of adding a few routes in a controller and creating some c# objects that match your json. For an update you can either do an [HttpPost] or an [HttpPut]. So for example, you'd add the following routes to your controller: [HttpGet] [Route("doc/{docId}")] public IHttpActionResult GetDocument(string docId) { return Ok(DataLayer.GetDocument(docId)); } [HttpPost] [Route("doc")] public IHttpActionResult SaveNewDocument(DocumentModel doc) { return Ok(DataLayer.NewDocument(doc)); } [HttpPut] [Route("doc/{docId}")] public IHttpActionResult UpdateDocument(string docId, DocumentModel updateDoc) { return Ok(DataLayter.UpdateDocument(docId, updateDoc)); }
Thanks, that's the path I was considering. This is currently only an EDI specification doc, but am I able to push status updates over web api? If so what technology would I expect the client to have?
Take a look at [IdentityServer4](https://github.com/IdentityServer/IdentityServer4). Microsoft is going to use this as the standard for ASP.NET Core security. For more information, check out [this blog](https://leastprivilege.com/category/identityserver/)
Thanks for the tip. My fear is that they've had about 7 new identity server options in the last four years ... How long will this last? I'm basing a lot on Azure AD so if they change whatever its their problem to keep it up; I'll just SAML into that. Edit: I'm not trying to exaggerate so heres by list: - FIM - ADFS 2 - ADFS 3 - Microsoft Identity Manager - Microsoft Live auth - Azure AD Connect new since july - Azure AD Dirsync phases out next spring Okay so i did exaggerate a bit and some of these are rebrands/upgrades. Just a bit of a mess coming in fresh when the same things are rebranded often.
I completely agree. This is why I like this project, as it has lasted through Microsoft's last 3 rewrites and is really thoughtfully done.
Fiddler has request filtering (which can serve up a local file). I use that in a pinch.
If you want to push changes I would recommend going with signalR. Otherwise you would need to poll the endpoint. Of course signalR would fall back to polling on certain clients anyway. Edit: Didn't directly answer the question. The client would need to support SSE or web sockets.
Gotcha, pretty much as I assumed from my research. Thanks for your time, it's appreciated
Anybody have a good guide to 3rd-party serializers? Because I'm getting really frustrated with the built-in ones.
This looks great.
When label is empty it will have no width, therefore no underline will be shown. A couple of things to try: 1) set the width of the label to a static value on the page where you display what the user entered. 2) put some spaces in that section when rendering the filled-out form if the entry was left blank. causing it to have width. do not store whitespace in your database or whatever. 
There is already one linked in the sidebar of r/csharp.
Cool thanks! I'll give it a try.
Make a concise list of shit that bothers you or slows you down, prioritize it by how much it fucks with you, and then start fixing those issues in that order. You're a developer, you need to fix your own problems and streamline the fuck out of your workflow.
All of these are solvable. [this should help for gulp](https://msdn.microsoft.com/en-us/library/dd293582.aspx) [~~for deployment~~](https://technet.microsoft.com/en-us/magazine/ee851678.aspx) For local development woes maybe look at using a webpack server for your static files instead of gulp. Jspm I have no actual experience with, sorry. 
Thank you so much for your thoughtful replies! I went with a span around the label, that had a class set to display: inline-table; width: 100px; and that did the job! 
I know. I was just in a happy place with node and ftping the site. Ever since the higher ups dragged it into VS I've been taken out of my realm so I've been looking for tips. When I can't find a solution online as simple as 'if its in the project dir make it part of the project' I decided to come here. I'm trying, this is just part of it.
Thanks, I'll keep you in mind. gulp has no interface with the service fabric virtual machine, only visual studio and I dont see anything online or in the publish options to do a clean publish. Yes I use chrome to debug. Since I've lost the use of browsersync and gulp-watch I've been targeting only chrome for development. I quit using VS for JS. Now I have two IDEs up for the same project. Webstorm and VS. Just wasn't sure if that was normal. I'm wondering If long term, should I extract all of the .NET stuff like web api, signalr, etc and make it a micro service. Then keep the page separate. That way localhost debugging could work as long as the services are up.
Thanks for the reply. It's not a service fabric solution problem. It's just a visual studio thing. I cant install javascript packages through VS and VS does not acknowledge files added to the project dir eternally. From what I see online there is no solution for this. Some people just write their own software to add files to the project. But thank you for mentioning that I dont have to add jspm_packages to the site root. Turns out something was very wrong in our bundling process. This will be fixed and help me a ton, thanks. I though this was an issue with Aurelia. 
You should do that kind of logic on the controller
Sounds more like problems that have afflicted every user of VS and VS-driven-deployments since the dawn of time (or at least 5 years). The fact that there still isn't an easy way to drag and drop a whole directory of stuff, as the OP describes, into a solution is mind-boggling.
&gt; Why are CSS files I deleted still affecting pages? Probably a caching thing. &gt; Not to mention VS2015 is a horrible javascript editor. Try VSCode or Atom.
I didn't know Pub/Sub was seen as the Observer Pattern, I thought it was more like the Mediator Pattern.
Do it in explorer and click refresh. Done.
That... does not add it to the csproj..
Select files -&gt; right click -&gt; add to project
From where? The files don't even appear in the project explorer until you've added the files to the project. And the point that the grandparent made is that you can't add a directory. You have to manually create the directory in the project explorer, and then add the files in. If you have nested directories and more than a handful of files this quickly becomes completely untenable. Or else things have improved in the latest version and I haven't noticed yet...
Best practice is to add the authorize attribute globally, this means it's not on everything. And then you add the [AllowAnonymous] tag to those areas that don't need it
Powershell has it's place. But Bash on Windows is more interesting. Anyway, having powershell on linux will help to port existing code to linux from your windows setup, so this is a fantastic idea if only because of that.
It's great for managing Windows servers from different platforms. 
Was anyone crying for it? Probably not. Is it a logical step in Microsoft's path to being multiplatform? Undoubtedly.
I'm not familiar with dot net rocks but it seems the most irritating whining podcast I've ever heard, I think they should use quotations marks for the rocks part as they clearly mostly complain about it. They also seem to have no idea what they're talking about and mostly talked about what they didn't port; waste of time.
Well, sometime soon maybe. According to the known issues, fixed soon in a future release. Once New-PSSession is working, it should be able to execute remote commands, shouldn't it? &gt; New-PSSessionOption and New-PSTransportOption do work but are useless without New-PSSession. The underlying client remoting layer code for WSMan is missing. This will be fixed soon in a future release.
I think Mediator is more of an Adapter, but it's been a while.
What I really need is a tutorial NOW (Not On Windows). Nearly everything I've seen on Core references Windows, IIS and Visual Studio :-(
Cool, I look forward to it. I'm actually halfway through a blog post myself on a tutorial for a real world scenario on Linux :-)
If you're saying that you want to dynamically add server controls to the UI using a server variable, this isn't the way to go about it. You want to create the variable on the server and add it to a container.
You can have child templates to do this declaratively similarly to how you seem to want to do it.. Use just decompile or reflector to look at how a repeater's header and footer template work. You'll have to write a server control because it doesn't work with user controls, but it's pretty cool. https://msdn.microsoft.com/en-us/library/system.web.ui.webcontrols.repeater.headertemplate(v=vs.110).aspx The attributes on the property are what make it work declaratively and Visual Studio will give you intellisense. You can also look at the SqlDataSourceControl to see how you can add things declaratively that are not controls, like the parameters. Also, I'm not sure what you're trying to do, but you could use a placeholder control on the page in place of your variable and programmatically add your other control to it or hide and show it programmatically. What exactly is it you are trying to achieve?
Yeah I have used it, or well tried to use it. Because it seems like it could be easily implemented alongside a classic mvc project. However I wasn't to happy with it, the developer is a cool guy and very responsive but the project isn't really updated anymore. Also the CMS backend is pretty buggy and not really user friendly. We switched over to umbraco eventually after a lot of problems. And I'm very happy about it ever since.
Thank you so much !
Breaking into projects helps you avoid circular references. If you have done a few projects before, you may start out with a layout that has worked for you in the past. If you are just getting started, create a web project and refactor to individual projects as the layers of your app emerge. Typically I see layers for app logic, business logic and data access.
[Intro to Powershell on Linux](https://channel9.msdn.com/Blogs/hybrid-it-management/PowerShell-on-Linux-and-Open-Source) same reason their's bash, zshell, python, perl, etc.. another tool for administration. Same ps scripts that run on Windows can run on Linux and MacOS. No need to rewrite the script for another shell.
Yeah, although it might not seem like one at first and might be a bit intimidating. It's simple in the sense that it follows directly the MVC pattern. You have views for every page, a document type is considered a model and the pages you add are like a controller. Out of the box umbraco does all the routing but you can set up your own custom routing rules (I always work with explicit attribute routing for custom pages). You can customize users in the same way as documents, easily adding properties. Umbraco keeps everything open, it doesn't do a single thing for you, like adding default properties to pages. So it's very flexible. 
It definitely is separation of concerns. The actual naming convention used is pretty close to what I generally use. I don't know if it is considered a standard, but most everything I've worked on is named much the same. So the SolutionName.Data would imply to me that this is the layer of the solution that interacts with the database. It is THE ONLY layer that has that ability. The SolutionName.Business will have a reference to the SOlutionName.Data. I think of the business layer as the layer that makes decisions based on incoming variables or data being pulled. For instance, if the SolutionsName.Web needs to authenticate someone, the Web has a reference to the business layer. So the web passes that username/password to the BLL (Business Layer Logic). The BLL uses that to call into the DAL (Data Access Layer). The DAL may call a stored procedure in the database to see if that username/password combination works. The database returns 0 rows, and the DLL passes 0 rows up to the BLL. Based on the fact that the business received 0 rows, it will then pass up to the Web that authentication failed. Granted this is very simplistic. Imagine though that the BLL received 0 records. It may have logic that checks to see how many times this username has tried to authenticate. The BLL could say, hey, you have tried 5 times. Time to lock down the username. The description above is basically a 3 tiered approach to design. Some solutions may have SolutionName.Services (webservices possibly?) that the application makes all the calls to. The service layer has end points that handles all incoming/outgoing traffic. The website calls the service layer that then would call the business layer... to the data layer... to the database... and back up. I guess I'm missing something that might contain your Entities/Objects/Classes. That may be called SolutionName.Model. These are the objects that the DAL fills with the returned data, and passes it back to the BLL. Why have this separate? It is so that the BLL, DLL, Services, and possibly web can have a reference to it without gaining any access to another layer it does NOT need or should see. OK. Saturday night and I'm typing this shit up. HOpefully it helps and I didn't just confuse the holy fuck out of you. 
This solution structure is pretty common for larger ASP.NET MVC applications. Check out http://aspnetboilerplate.com -- it's a great starter template that is very well documented. The author explains his project structure and what each layer does here: http://aspnetboilerplate.com/Pages/Documents/NLayer-Architecture
There is the third part, and more are comming:) :http://indexoutofrange.com/Don't-do-it-now!-Part-3.-Hangfire-details-jobs/
Im a big fan of Jenkins for CI uses. WE have one problem with it though that we host it internally, so we cannot utilise the webhooks to build from a commit. Apart from that it's great. The Pipeline as code features are great as well.
BeepBeep! I'm a bot. Your comment seems sad to us, cheer up! Have a [kitten](https://img.buzzfeed.com/buzzfeed-static/static/2014-11/4/15/enhanced/webdr03/enhanced-31234-1415133567-2.jpg) P.S. This bot is powered by A.I. [sentiment analysis](https://www.youtube.com/watch?v=EJREoF9JHeU&amp;feature=youtu.be)
[X-POST /r/SQLserver] These projects are a bit dated, but I thought to share them with you anyway. SSRS Render Studio "In a single step, SsrsRenderStudio executes and outputs the rendered report to a supported destination programatically with no user intervention." https://ssrsrenderstudio.codeplex.com/ https://bitbucket.org/fixitchris/ssrs-render-studio/wiki/Home /c
Hey ben, would you mind elaborating just a bit here? Do you mean I could start building my team's .NET 4.5 asp.net webforms app using "dotnet build" (or whatever that new command is)?
They're just conventions like you said. Most projects I've worked on use similar names. The purpose of separate projects is to have a good separation of concerns -- this is to avoid the situation where 200+ line controller methods exist because the controller method is doing too much stuff. The idea many people follow is to only do request-serving logic inside controllers (i.e. returning a JSON object, redirecting to another route, etc.) and do unrelated business logic outside the controller (fetching data, saving data, validating data, etc.)
I have carefully tested it. And found solution. You should call Bindings.StopTracking(); at Unloaded event handler **explicitly**. It's because compiled bindings tracking code **doesn't** stop tracking automatically on page unload and it is a root cause of compiled bindings memory leaks.
Interesting 
I have seen compiled bindings generated code at /obj/ folder and unfortunately it doesn't call StopTracking() automatically. It even doesn't handle Unloaded event at all. If you don't call Bindings.StopTracking() explicitly, your memory will leak on PropertyChangedEventHandler objects. Seems like compiled bindings generated code doesn't use weak references properly and those bindings should be explicitly stopped (so removing any property changed event handlers) on page unload to prevent memory leaks. As far as I know old-fashioned runtime Binding did use weak references. Compiled bindings also have attempts on using weak references, but it fail at those attempts for some reason and memory anyway is leaked.
Have you filed a bug on MS Connect yet? 
Maybe some sort of infinite loop with your constructors 
Ah. I believe the NLayer Architecture is probably what I want to research on. Going through the supplied links, it's kind of weird. From a web-based standpoint, MVC kind of fits the bill of the NLayer architecture (as in Models has to do with data, Controller has do with logic/flow, and View having to do with the presentation of it all). I wonder if it would be appropriate for me to think of this in the same sort of way except assume it's on a grander scale?
Arvixe for inexpensive shared hosting our simply Azure. 
Which code specifically would he useful to include? I made sure to change the names in the controller too, and it works as a locally deployed app. Just not when I upload to the server.
Are you sure you deployed all of the files you updated?
I publish the app into a folder and can run the app from there locally no problems via the command line on windows. I then drop the containing file into where I have configured my server to listen to. This has worked with no issues before.
I guess my bigger point is you mentioned changing the name, and now it's not working. I'd that's the only thing you've changed, then it's more than likely the culprit. Maybe you didn't publish all the files, maybe something didn't publish fully, etc.
Capitalization is fine.
They are consistent.
Nah mate, just a large SaaS project.
Depending on how you have things set up IdentityServer might be something to look into. 
I have a large project, did the same migrated from RC1 -&gt; RTM. No problems what to ever
I have a userdatabase made with ASP.Net Identity with locally and social media registered users. I just need a web interface to manage them. 
We use id server 3, openid connect at work. Large scale high volume e commerce. Any questions?
Over time you'll end up extending and hacking it apart to do anything that's not out-of-the-box. I find that we're better off building our own (though be careful). Ignoring the OWIN bits, Identity is really not much more than a thin layer around a simplistic database schema. It uses Entity Framework to do most of the work which is relatively trivial. But implements a password hasher and a secure token generator/validator, which is less trivial.
I usually custom build the provider, but I use it and it works well. Reasons for writing your own provider? I often had to use a database that wasn't sql server and I hated the bajillion stored procs. Granted, at my current gig we are using it straight out of the box, thousands of accounts, with no issues.
All the same options you have on a typical network are available in Azure. You can call out from an Azure VM or hosted service to Valve's web API. I think your bigger issue, if you haven't touched Azure yet, is learning how to use Azure. It ranges from just spinning up a simple hosted service to setting up an entire domain-based network with all the VM's you would need to run a business. The later requires a lot of planning, learning, and trial and error if you're not familiar with network administration. In the end it really depends on what your requirements are. Why Azure at all? The people in [r/azure](https://www.reddit.com/r/azure) may also have some answers.
Sorry performance isn't the right word, although start up time is not great. Can be solved by forcing Azure App Service to stay awake I guess. Compile time is what I mean, and it's horrid. They have acknowledged this and it is explained in that github issue. 
Damn, really? I guess that's hopeful for me.
Don't roll your own. Microsoft has put millions into its security, which is baked into a solution that is easily expandable to meet your needs. You've paid money for that secure solution, use it!
Looks like there are extension methods defined on WebHostBuilder: [KestrelExtensions](http://apisof.net/catalog/Microsoft.AspNetCore.Hosting.WebHostBuilderKestrelExtensions) [WebHostBuilderExtensions](https://docs.asp.net/projects/api/en/latest/autoapi/Microsoft/AspNetCore/Hosting/WebHostBuilderExtensions/index.html) 
&gt; IdentityManager Installing IdentityManager I get ["Error: Sequence contains more than one element"](http://i.imgur.com/eWsdMkg.png)
&gt; Is that method (UseKestrel()) returning a WebHostBuilder, or the same WebHostBuilder, or am I just completely missing something about how this works? It's returning the same `WebHostBuilder`. That's how the **builder pattern** works.
Additionally ASP.NET Contrib, a more broader contribution: https://github.com/aspnet-contrib/
Thanks, that was my initial assumption but I couldn't find anything to back it up in the documentation
That publish to Azure button is just so friendly and inviting. 
I'm disappointed to see that they have not yet brought back the diagramming feature of SSDT that auto generates an ER Diagram from your .sqlproj files. It was extremely useful, and considering all of the improvements they've brought recently, its absence sticks out like a sore thumb. Also still starkly lacking is support for Azure SQL Data Warehouse, with its constrained feature support and language extensions (WITH DISTRIBUTION), and lack of support for primary keys/unique indexes.
They're two mostly different beasts, with a bit of overlap - with SSDT you can do a full fledged database project that compiles down to a deployment script, or do incremental scripting between source and target databases. It's got various type-checking / object-checking and linting-like features built in - i.e., if you're trying to access TableXYZ in your stored proc and it's not defined in the database project, then you're going to get an error - and this is without having deployed to any database. You can also use source control to manage all of this, have pre/post deployment scripts (good for loading those lookups, etc). This would definitely help facilitate CI. I'm sure you could script all of that out in SSMS, but it'd be a much more manual process. So, in a nutshell SSDT is more for the database developer. SSMS on the other hand is more aligned with the DBA, and has a lot of convenience features for quickly affecting the server and database objects (and scripting out those changes if you wish). There are of course tools that bolt on some of the functionality of SSDT to SSMS (think RedGate), but they tend to be kind of pricey. I think they (MS) also recently dropped support for version control in SSMS as well. Obviously that's not a complete breakdown of the features of either product, but you get the idea... 
I tried and I got "the name 'directory' does not exist in the current context" on the first step; when adding: .UseContentRoot(Directory.GetCurrentDirectory())
Editing the live config files will be fine. Just make sure you go the super paranoid route and create a copy of the file beforehand just in case.
You could create a web job. https://azure.microsoft.com/en-us/documentation/articles/web-sites-create-web-jobs/#CreateContinuous I think you can just take the loop logic out of your exe and run it continuously or on a schedule
I personally use asphostportal. Not so expensive but offer reliable service. 
The class Directory is in the namespace System.IO At the top of the file you need to add: using System.IO; Alternatively, you could change that line from .UseContentRoot(Directory.GetCurrentDirectory()) to .UseContentRoot(System.IO.Directory.GetCurrentDirectory())
Awesome, that works, I had a using for System, but not system.IO, surprised system.io wasn't anywhere in the document. I am not sure what exactly static files all entails, does that mean if my code has some JSON it is accessing through Angular it will be able to do that without issue? Or would this be no different than if I just opened up the files on my file system?
In the root of your .NET application, add a folder called wwwroot and inside, create some JSON files. public void Configure(IApplicationBuilder app) { app.UseStaticFiles(); } The above code tells your application to use the static file middleware. This middleware will serve static files in the wwwroot folder. (html, css, js, json, png, jpg, ect) For example: If you create a file in the wwwroot folder called test.json, run your project and navigate in a browser to localhost:5000/test.json, you should see the file. Same for html files and such.
ASP.NET Contrib targets prehistoric .NET 4.6.x, link in OP targets .NET Core While ASP.NET Contrib has more items in it, it's NOT compatible with .NET Core.
I'd like to see one like this, but targeting .NET Core's WebAPI
.NET Core is compiled into your application, so it will never be distributed through Windows Update.
Hey /u/sniper_fox, &gt; ASP.NET Contrib targets prehistoric .NET 4.6.x, link in OP targets .NET Core &gt; While ASP.NET Contrib has more items in it, it's NOT compatible with .NET Core. Did you actually.. uhm.. how to put it.. Did you actually looked at the link before spouting this nonsense? Directly when looking at the page I linked you can see brief descriptions of the repositories: &gt; OpenID Connect/OAuth2 server framework for OWIN/Katana and **ASP.NET Core** &gt; OAuth2 social authentication providers for **ASP.NET Core** &gt; OpenID 2.0 authentication middleware for **ASP.NET Core** &gt; **ASP.NET Core** samples demonstrating how to use the OpenID Connect server with MVC or JS apps &gt; Yeoman generator for **ASP.NET Core** OAuth2 social providers &gt; Hosting extensions for **ASP.NET Core** &gt; OAuth2 extensions for **ASP.NET Core** and OWIN/Katana Additionally you can see that a lot of projects are referencing `netstandard`: https://github.com/search?q=org%3Aaspnet-contrib+netstandard1.4&amp;ref=searchresults&amp;type=Code&amp;utf8=%E2%9C%93
Welcome to programming where no matter what you do someone will tell you you're doing it wrong. :) I wouldn't say VB.NET is not used anymore though C# has become the primary .NET language. You'll find more code examples using it. And there are probably more C# job opportunities but that varies by location. The good news is VB.NET and C# both hit the same frameworks, use the same data types and generally are about the same. You'll just type a bit more with VB.NET. I'd suggest you learn both. Get a dummies guide to C#, read it in a week and re-write something you've done in VB.NET in C#. The funny thing about it is it's all pretty much the same. Every language has variables, loops and conditional statements. And you can build a working solution using pretty much anything. Your mileage varies on how easy it is to support. 
Got it, I think that makes a lot of sense. I was attempting to take my knowledge and translate it over to C# but it seemed a bit overwhelming attempting to transition without any knowledge of C#. However, the idea of reading a book on C# first makes a lot of sense. Will give me an opportunity to make the connections in my head before attempting to put it into Visual Studio. Thanks a bunch for the thorough answer!
I'm largely starting to doubt the usefulness of tuples, especially with the incoming C# 7 changes - such as named values, or items if you like. At that point, you might as well create a new class.
I can't think of anything that would have really invalidated Dapper examples from ASP.NET classic or elsewhere. I'd suggest looking for async Web API examples for Dapper and go from there. Can you be more specific, what are you having trouble with? 
To start off, let me tip my hat to you because that is PRECISELY how I got started. I learned VBA while in the Air Force on a temporary assignment debriefing pilots (my main job was a crew chief on A10s). I wrote a lot of automation for spreadsheets that cut hours and and hours of work down to a few seconds and I was addicted. I transitioned from VBA to VB.Net and loved it at the time. I decided to go to school for CIS and ended up learning some C++, Java and C#. I feel in love with C# and Java and never looked back to VB. In fact, over time I started to genuinely hate VB as I become more exposed to other languages.... VB just began to look completely silly to me. VB isn't going away any time soon (unfortunately), but I wouldn't invest any time into learning it over any other language. Right now, it's a language that you'll almost only ever encounter in legacy systems. So there's value in it, but not nearly as much value in it for someone so early in their development career. I wouldn't call this the Latin of programming... personally I'd call it more of a... box of crayons on the wall kind of language lol. But that's just my opinion. C and C++ have a different purpose that are also not going away any time soon, but depending on what you're wanting to do for a living, these can be overkill. Where having an understanding of the concepts you'll need to learn in order to work with these languages is invaluable to any developer, they really shine in systems where low level performance is critical. For example, video games, graphics, embedded systems, etc. Now if you're wanting to work more on information systems such as business applications or web applications, then there are other languages that are far more productive with less of a learning curve. These are where the bulk of .Net jobs are and if you have competency with them, finding a job is almost as easy as saying hello to a recruiter. No seriously. C# is the .Net standard. It's not a hard language to learn, but coming from a VB/VBA background, you're at a slight and temporary disadvantage because VB is kind of its own beast (or lame duck) that doesn't really follow normal C-based conventions and has several features that are afterthoughts rather than being a first class citizen, leading to some really weird, unintuitive syntax and general shennaniganry (which is why I make fun of it and don't recommend it to anyone). C# is, in my opinion, the Zen of high level, statically typed, object oriented (and functional!) programming languages. Once you learn this or any other C-based language, just about every other language becomes fairly easy to learn. There's only a few concepts you need to learn before it starts to get super easy, and those concepts really aren't hard to learn at all. So anyway, let me back up. It was suggested below that you pick up C first. There's nothing fundamentally wrong with that except that if you're not coming from a background in software, it might scare you away. It almost definitely will scare you away, and so will C++. Now if you're technically minded and really care about micromanaging all the bits and bytes in memory, then you'll be fine but it's still going to take a long time to learn let alone build anything with, only to later find out that C# has eliminated the need for doing about 95% or more of everything you learned in C. Those languages will also mold your mind differently (for better or for worse depends on your end goal). So, personally, I'd say skip those. If you want to work in the Microsoft stack, enter with C#. The core concepts are universal: primitive types, arrays, control structures, and decision structures all of which you're probably familiar with by now if not by name. C and C++ are not memory managed, so you have to tack on a whole other paradigm of programming so that instead of focusing on your business concerns, you also have to deal with machine level concerns like memory and processor usage which can get really complex for some. Managed languages like C# in .Net eliminate that and simplify data structures so that you can focus on solving the business problem, making it far more productive. Plenty of C developers have something to say about that, but in the end it's just... aligned better with IS development imo. And VB is just a dead end.
Arvixe for simple sites. SmarterASP for slightly more mature sites. Azure for somewhat mature sites. Azure VMs for mature sites. Dedicated servers for more mature sites. 
Every time I see one of these articles about monkeying around with the GC, I can't help but wonder: a) if a few _milliseconds_ is a performance issue, then maybe the CLR is the wrong platform b) if you are creating so many objects that need to be disposed such that it causes a significant interruption, then maybe you should be looking at your data structures and algorithms c) if the .NET core team bent over backwards for its users on this issue
It's not about milliseconds in all cases, sometimes it can be seconds. Even if you don't have a lot of allocations you can have huge objects that require a lot of time to scan. Watch this great talk that touches this also a bit: https://www.infoq.com/presentations/csharp-systems-programming
I see it helping when using tryParse, I always hated initializing variables i knew would only be used for the purpose of the parse results.
I agree, it seems like a bit of a help there but my brain is still screaming at me saying it doesn't seem to be a good, clean, readable way to do things. I can be a bit weird about code though so maybe it's not as big a deal as it seems?
I'm not getting any info about Primitive&lt;&gt; and Span&lt;&gt; which he showed for marshaling byte * ... any links, tutorials?
Love the pattern matching and new tuples!
I like them. They're just a useful convenience, letting the compiler do all the legwork for you, creating the necessary types and writing all the boilerplate code to use those values. Or maybe you've been bitten by the existing [Tuple type](https://msdn.microsoft.com/en-us/library/system.tuple\(v=vs.110\).aspx) which is clumsy to use. After using other languages that *have* tuples, it's very nice being able to return multiple values *ad-hoc* without having to create a specific type for it. If later you feel the need to use those ad-hoc combinations elsewhere, then it absolutely makes sense to package them up into a new type. Anything that saves me time and effort is a good thing I believe. 
Hi, Thanks for the background tip. I have like a month to make it. The 2D drawings don't have to look fancy (it can just be sticks or circles). I'm just wondering if there is something specially made for showing moving objects in WPF (other than a canvas). If it's too much of a workaround to get something like that, I'll go for another project but I thought WPF had to have some kind of element specifically made for this stuff. what do you think about Storyboard Animation? 
Something that may ease your drawing portion of your work is [Win2D](https://github.com/Microsoft/Win2D) which is a C# library over direct draw. It's a bit more advanced but the documentation is fairly straight forward. The NuGet package is [here](https://www.nuget.org/packages/Win2D.uwp).
Seconded! I can finally have that Scala/Python feel with C#!
THIS is what's fucking awesome about the C# team. From the comments on the OP's article: &gt;Mads Torgersen - MSFT &gt; ... &gt; This, by the way, is very similar to, and in no small way inspired by, the philosophy of Scala, which holds that functional and object-oriented programming aren’t like oil and water, but can in fact be blended pretty well. Scala did it from the start, whereas we began in a mostly object-oriented place, and have been trending functional ever since. But hopefully programs aren’t necessarily written in the “object-oriented” or “functional” subset of C#, but instead just good code taking advantage of the full range of expressiveness. So, there's no language envy here. No purism. Just unadulterated pure development productivity-soundness "and we don't care if it tears a paradigm apart" awesomeness. So... big deal, yeah right. Haskell or OCaml, or better yet, ML or even Lisp did it better. Whatever. Those have never obtained nor have the developer community potential that C# now has. And make no mistake, they are world class. I really hope they incorporate this into .NET Core. There will be no stopping them on that stage. &lt;/rationally_exuberant&gt;
I do wonder why it's () and not {} for the tuples though.
I like the parenthesis better... Brackets feel more like a scope to me. 
tryParse's signature is the problem. Id rather it returned null for failed parse, instead of bool and out parameters. 
I'm not a fan of tuples. Custom types ares better. &gt; Custom-built transport type for every method: A lot of code overhead for a type whose purpose is just to temporarily group a few values IMO These transport types are not temporary. It's better to define them explicitly. I'm 100% confident my current dev team will misuse tuples. All of their methods that take 8+ parameters are going to start returning 8+ results. Next sprint it will return 9+ results. 
This wouldn't make it any shorter, or easier to write/read. Assuming tryParse would return nullable&lt;int&gt;, your code would be var maybeNumber = int.TryParse("42") if(maybeNumber.HasValue) DoSomethingWithValue(maybeNumber.Value) right now we have int number; if(int.TryParse("42", out number)) DoSomethingWithValue(number) and with this new change if(int.TryParse("42", var out number)) DoSomethingWithValue(number) 
I stand corrected. I didn't think of writing it that way, looks cleaner imho
Bad devs will always find ways to abuse a language. Tuples will be a very nice addition to the language when used correctly and a source of code smell when used improperly, but that's the case with many features in any language.
I haven't watched the video yet, but I'm willing to bet that's stuff from Midori, a variant of C# created by MS Research. 
I could also see myself being confused with tuples and anonymous types at first glance if it were {}, even though tuple doesn't look to support inline assignment 
Braces. Brackets are the square ones.
ohh... I always call them square brackets lol
Curly *braces*, square *brackets*, round *parens*, and "angle brackets" are an abomination they're greater-than and less-than signs god dammit!
 p.GetCoordinates(out var x, out var y); Makes me twitch just a little bit because I can't infer the type by looking at it. Doesn't mean I'm not going to use it judiciously. 
This is so strange. I really like most of the features mentioned. But I also find the syntax to be ugly. I mean, the syntax makes sense, it's just that it looks cludgey and I feel like code is going to start looking more like C then C#. One reason I love C# is because it just *looks* good in my IDE. All this extra syntactic sugar being thrown around looks like clutter. It is my opinion that the least amount 'syntactic features' per line of code the better. I prefer to look at a line and immediately understand everything that's going on. With this new stuff we have a whole lot of contextual information that needs to be parsed through in order to understand any result. It's the exact reason why I hate code like the line below: var myNinja = ninja.Clan == null ? Steal(ninja) : (Kill(ninja) ?? new Ninja()); This would not pass a code review in my eyes. But these new features lend themselves to clumsy looking lines of code just like the one above.
Can you show us anything of the code you already have? Either what you've written, or are working with? Giving this kind of advice is difficult without context
Is this a sharepoint app?
Ah, the configuration system has been replaced with [this](https://docs.asp.net/en/latest/fundamentals/configuration.html). They have an example of a connection string that's probably exactly what you want. Also, for general comment, .NET Core is ALL about dependency injection, embrace it and don't try to fight it (singletons, etc...) and you'll have a better time.
&gt; It's the exact reason why I hate code like the line below Then don't do it. This is perfectly readable: var myNinja = ninja.Clan == null ? Steal(ninja) : (Kill(ninja) ?? new Ninja()); 
You cannot with `var someVariable = someMethod()` either.
C# 7 works on .NET Core.
Looks fascinating. I'll have to give it a watch.
It sounded worse than I intended. My point was than many people honestly believe that any attempt to manage memory in C# is using the language wrong.
[removed]
&gt; but creating a type is too much overhead, It would be cool if they could shoehorn the anonymous types in there with the return value somehow. public whattypeisthis foo() { return new { first = 1, second = "aaa", third = DateTime.Now }; } 
Some of the advanced stuff is not yet standard .Net but you can play around with it in CoreFxLab : https://github.com/dotnet/corefxlab 
eh, care to explain the benefits of the library?
I personally think this is more readable/clean in comparison to doing if statements.
From the official Microsoft documentation: https://docs.microsoft.com/en-us/dotnet/articles/csharp/tutorials/console-webapiclient
You can (and probably should) do it with a MSI. But you won't get what you want. Updating a Windows Service requires administrator privileges, and UAC will block you as well. Chrome gets away with it because Chrome is not a service and it is a per-user application that doesn't touch Program Files, HKLM, or the GAC. For your MSI, remember you also can't replace DLL's or exe's while they are in use, so the MSI needs to be able to shutdown and restart your service's process. You'll also want to invest in being able to reliably roll-back. That aside, **DON'T DO THIS**. A Windows Service is not an app. It is usually something important and relied on. If you auto-update, admins will ***hate*** you. Services usually go through a trial period before they are rolled out, and they get rolled out when there won't be a service disruption. You also need to provide an update mechanism for instances that are deployed a firewall that blocks your auto-updater. If you want to take a look at an application auto-updater, Google open-sourced the Chrome updater: https://github.com/google/omaha. Be warned, an auto-updater is not a simple system, and creating one will be a huge, ongoing investment.
Just run Nginx, IIS or Apache with proxypass in front of your kestrel.
Thank you for the response. I appreciate it. We have a service that interacts with a web component a lot. Sometimes we need to make changes to it and do not want to worry about them being backwards compatible with older versions. If the service is running as local system or other admin user would it not be able to make changes to Program Files without a UAC prompt? I have checked out Omaha but it would be way overkill and still a huge investment for a smaller company. I've checked out Omaha but it's overkill and not maintainable for a smaller company.
HttpClient is a good base. But it tends to litter your code with lots of parameters and plumbing. For my app i have made a very lightweight wrapper around HttpClient to groom it better for the tasks it needs to perform in my app... I suggest you do the same. 
The simplest way to do it is you need 2 services. One will actually do your work and the other is your updater. The updater service can stop your main service and install a new package and re-register the service with Windows then start and the service back up. Your main service should also have similar logic to update the updater service. I've done this for a project a while ago and it worked well for me. 
Could you post some pseudo code?
Here's one article: https://msdn.microsoft.com/en-us/library/bb384398.aspx I learned mostly through maintaining an existing .net web forms project that had good structures. At first I wondered why the code seemed scattered and breakpoints jumped around. But I eventually learned to architect that way and managed others building on it. Its very much a standard for enterprise grade apps for a number of reasons. For one it gives things a basic structure that keep things organized over years of build and enhancements. You don't have to learn each programmers style or patterns for where they put things. If you're looking to tweak a data access interface piece its somewhere in yourapp.Data etc. This then lets you swap a data provider - say MS SQL to MySQL in one clean set of files and not a deep dig. Another rarer case but one that helped me is that in an extreme growth case - say you need to scale to millions of users - you can put each project on its own server or cluster of servers. This brings overhead but is an opportunity for large scaling an app. Not as important in todays cloud world but still a good option. Lets say your business tier is getting hammered with heavy report building computational work. Create a layer to serialize data between servers and give it it's own box. Far more reasons than those you'll find. But those stood out to me - imposing structure and enabling scaling.
While trying to find an alternative to RestSharp that works on .NET CORE, I found [Flurl](http://tmenier.github.io/Flurl/). I have yet to try it though.
It worked for Spanish.
Or, if you know exactly how many objects will be loaded per map and how much memory will be used, you can configure for that, and prevent the GC from running at all except when the map unloads and a new map loads. 
Basically you are going to want to get an access token from Facebook and then send it to Web api. If the token is valid then you find the user with the matching user ID from Facebook and then issue a bearer token from Web api. Here is an article that might help you along http://bitoftech.net/2014/08/11/asp-net-web-api-2-external-logins-social-logins-facebook-google-angularjs-app/. Pm me if you need more help, I have all the code written just crazy busy right now. 
This sounds like the best course of action for me right now. How would this work if I wanted to update multiple software with it? I'm thinking about what happens when you uninstall one of those components but not the other. Should the installer remove the update service as well? 
Potentially just using a Lazy&lt;T&gt;?
Here you go ... https://github.com/matthewblott/dapper-api
The azure portal is terrible. It's difficult to navigate, non intuitive and really laid out horribly. I honestly don't know what they were thinking.
In short: It's real time diagnostics and statistics of your code. Think google analytics for your .net code. If you need to monitor SLA's and response times, and at the same time want to know why certain calls are slow, it's a great tool. I evaluated AI against New Relic. For .Net, AI is a bit more proactive, but NR can monitor more stuff. In the end we went with NR because we could get more out of it for our application, but AI isn't bad at all.
What about migrations?
Application Insights can monitor anything that can call its API. You can run it in a Windows Forms application on your desktop, if you want, as long as the desktop can see the Internet.
The 'add-on' is the same thing if you're hosting you ASP.NET (not IIS) application in Azure or on-prem. Application Insights doesn't care where your app runs, it just exposes an API that any app can call. The libraries that get imported to your ASP.NET app automatically capture all kinds of core metrics from IIS and your application. &gt; I'm sure it's nice tech, but life is complicated enough as it is without this. What is your monitoring and application health solution for this project? How will you detect downtime, or trends that could result in poor performance? Assuming you're going to be using *something*, Application Insights is pretty darned easy to use.
I see no reason why you'd require the synchronization context. Why do you keep it?
You don't need AI per-se, but you absolutely positively do need *something* to monitor and analyze your application telemetry. AI, new relic, whatever catches your eye. Based on your brief description of the client it may actually be less headache to go with AI since it is a Microsoft offering (and free) which means you can always play the "this is the platform's default tool" card. Disclosure: I once worked on the AI team.
You will be fine without it, but it's actually pretty useful, and it doesn't get in the way so why not use it? It's not like you actually have to do anything to get it to work, just leave it there and let it do its magic.
You can use AI wherever. WPF application, console application, wherever. For example I use AI from an embedded raspberry pi with sensors.
Well what do you mean by "properly implementing security"?
And it is their second re-design I believe. Things aren't even fully migrated over. IIRC you have to view Subscription charges in the old classic portal.
Very cool, thanks for the info, I'll check into that.
C# 7's tuples appear to be very similar to Python 3's tuples. I like it!
Are you talking about getting it set up with HTTPS or some sort of authentication system or something? 
Pretty nice to have, not mandatory. I'm only using it for basic things at the moment like monitoring requests and load times, but it's super simple to set up. You could use the Windows Performance Monitor for some things but it's less intuitive. As a bonus, you don't have to host on Azure to use app insights. I moved my website from Azure to my own web server (Azure is great but I already had a machine I could use), and it's all working great.
I'm not sure what you mean by security. A lot of people mean identity/authentication, if that's the case I've used this before when working with angular and webapi. http://bitoftech.net/2014/06/09/angularjs-token-authentication-using-asp-net-web-api-2-owin-asp-net-identity/ The WEB Api part is just a token based system on top of Owin Identity provider so it's not just an angular solution. 
&gt; How to fuck your deployments (the fastest way) Don't do this on any project of scale. 
Yes. The backend data processing &amp; analysis is done in Azure, and you interact with the results via the Azure Portal. There is an option to download the raw data (in realtime!) but I believe that it isn't part of the free tier.
Why is that? 
And how many devs? How many lines of code? How many modules?
This is great for small projects and messing around at home. I use it all the time to deploy to Azure. I would weep if any business had this as their deployment process though.
9, tens of thousands, over 50 projects in the solution. 
Look at octopus deploy
No. [NuGet is open source](https://github.com/NuGet) under the Apache license, which permits binary redistribution.
Almost all of our deployments are automated via TFS2015 Release Management. We have the ability to publish debug builds via the one-click publish. Edit: I should be clear, I work for Microsoft as a .NET Consultant. I think one-click publish is good for quick debugging work but completely agree that it's a disaster to use it at scale and would never use it to deploy production code. An example of when and how I would do this would be - disabled a web node in ARR, manually publish a debug version with symbols to the node, start the remote debugger in that machine, and begin debugging. 
My customer has strange environmental limitations and often times it's impossible for us to debug certain issues in our development environment. It sucks but it's more of an organizational issue for them and we are not the only people that face it so there's not much we can do. We do use VMs but it's overkill for us to clone and redeploy nodes for simple app deployment. 
I was so disappointed when I saw the example with the magical strings used to reference the lastUpdate properly
They didn't even use ´nameof´...
Why should I use this over the Route/Httpx attributes?
I'd like to see performance numbers vs the built in attributes too
EF supports IQueryables and as such you can have expressions that will be eval'd in the DB as opposed to in memory. This holds true as long as you perform your `Where()` with your specification before you `ToList()`, etc. (`ToList()` forces the query to be evaluated)
Yes, but how is the specification pattern supported by ef. I'd think that somehow the specification needs to be translated to an `Expression&lt;&gt;`?
Thats gonna be...quite expensive.
Right. This is from memory as I'm not looking at my code right now, but the way I wrote this a long time ago was such that each `Specification` had a `SatisfiedBy` that returned an `Expression&lt;Func&lt;TYourEntity, bool&gt;&gt;` instead of a `bool`, and I wrote some `AndSpecification` that was used to join multiple specifications (using an `ExpressionVisitor`). The trick is to have each method that you're calling (i.e. `OlderThan` ) return an `AndSpecification` so that you can keep appending more conditions to the expression.
That indeed makes it possible, but comparing it to the peewee implementation, it is quite a lot of boilerplate. It still requires implementing both an `Expression&lt;&gt;` as well as a bool. What I'd really like is if a method could be annotated with a hybrid attribute. The compiler would expose both the method itself as well as an variant that returns `Expression&lt;&gt;`. EF would see those method calls/property references inside `where` expressions and inline the `Expression&lt;&gt;` for those that are annotated. Regular IEnumerable where expressions would still behave like they used to.
I understand the sentiment, but on the other hand, it's nice to have some of these features either for performance sensitive apps or for unifying with other languages (like go, JS, scala)
It depends. I just started a new project with Core and then abandoned it as it is something that needs shipping pretty quickly. Core is stable but you'll find yourself hunting for scarce documentation and not all the libraries planned are available yet.
Doesn't using nuget prevent you from having to manually share dll's across solutions?
Core runtime is ready, just not the tooling and secondary libraries. Welcome to the new Microsoft 
[removed]
I've used it. People are in for a massive disappointment. It looks nothing like the simulated videos. It has a low framerate, low resolution, very low field of view, dim transparent visuals, and chromatic aberration like this: https://i.ytimg.com/vi/n2vxfnr4mRI/maxresdefault.jpg They need to work on making more accurate simulated videos or they're going to end up in a No Man's Sky situation.
That big spike in .net seems suspicious to me, maybe it was indexing companies that have '.net' in their name or in their URL or something like that.
This reminds me I have stumbled upon a tutorial on core¨ https://www.codeschool.com/courses/try-asp-net-core Just in case someone wants to check it out
IMO, it's worth the time investment. I don't think I could go back to the old way of manually copying dlls into projects. Then if you update the dll, you have to go back to each project and re-add the dll. ugh. 
We just have a folder checked in to source control that has the latest versions. Once a version is updated, update source control, get latest, bam, done.
Did not know that, but you still have to package them though right? 
Yes, but it is VERY simple. You can script it easily, and build servers usually can produce the package, TeamCity does it, TFS can probably do as well... nuget pack foo.csproj
.NET is dying. Run as far as you can from it.
Yeah, I'd say it's a bug in their tracker. The C# line should be well correlated with the .NET line, something was up between 2014 and 2016.
Disregard him. Probably some unemployed UNIX neck beard troll.
lol
I won't say that it's dying, but the priorities on .NET and Core have been a little muddled for awhile now. The ecosystem is currently confusing at best :/
I have couple of comments. During unit test, you can use mock framework like moq instead of creating fakes because you have to maintain two dbcontext, one on ecommerce, another in unit test called test context. Also if u are going to create services calling entity framework, I recommend making it a generic type so it can handle any entity. I can see your concern on why you don't want repository pattern. Let's take a scenario where you have to add an employee to a database but the requirement also mentions to add that employee in a user table. Using your pattern with direct access to entity framework, u have to call one for employee and another for adding user which have to be implemented in the controller. I do not think that would be great in long run if u want other developers to follow the same business logic. 
We have a large team getting ready to launch a new platform on core running on Linux. We started on RC1, developed out a pretty extensive set of services, and are now conveting to 1.0 in prep for our production launch. I won't lie and say it's been simple, but once we got through some initial hurdles I have to admit I'm liking it. Performance is looking great, too.
&gt; using MVC6 .NET Core web API stuff. Please note that it's called "ASP.NET Core".
honestly the ProductService looks a bit like a Repository pattern anyway so I'm guessing he'd do it in a service.
Apologies. This was my first foray into the ASP.NET core world. Appreciate the feedback!
I do this for pushing nearly 1000000 lines of code and 400 libraries to multiple environments daily without issue 
Jon Skeet in the comments. That guy is everywhere! Which reminds me: http://meta.stackexchange.com/questions/9134/jon-skeet-facts
It would not surprise me at all. Tick Tock - new big .NET, then the core version later. It doesn't make sense to split the efforts long term, but things are seriously confusing right now.
A classic Ben A. Adams.
If you take out the spike, then there is no sudden drop-off. From start to end of the graph, all three technologies declined by about 50%. 
Whats today? SPA?
Basically it's a testing service to use when developing applications. If your application needs to make a POST request for example and is sending some JSON and you want to see what's going on, you can call the /POST endpoint of this service and it will display whatever you sent it in the "body" part of the response. It's kinda like [RequestBin](https://requestb.in/). GET Example: http://httpbincore.net/get?arg1=val1&amp;arg2=val2 Request Headers: http://httpbincore.net/headers
&gt; implying javascript
MVC Core
Wonder what would happen if he used the type safe enum pattern for this
Nice info. I've run into many situations where I could not wrap an IDisposable in a using. Best practices are great, until they aren't.
While it is written for Plastic SCM, it would be easily adapted to Git or any other version control
Do you have a working client (javascript for example) to retrieve authorized data from that API? If you have one, simply capture the HTTP request sent by that client and mimic it in Postman (most likely there's some Authorization header). For the default app, accessing the restricted pages may give you that hint.
What type of auth did you set up? Generally the default is basic auth. Postman always sends that credential header by default.
... am I the only one that gets angry when I read stuff like this? I mean what would happen to any of us if we released code that works like that? We'd be pilloried, of course. But when it's part of the core, we just accept it and make workarounds and write the same articles over and over and over. That seriously screws with the principle of least astonishment.
Seems like a weird benchmark... The jitter doesn't throw away unused results from ToString, skewing the benchmark?
`Find` is more of an Entity Framework query than LINQ. `Where` should be the one you're looking for - why doesn't it work?
Oh no, I think most of us are pissed too. (Except the ones who seemed to have imagined this was mentioned in the docs.)
Your last example should work. MyLog myLog = db.MyLogs.Where(x =&gt; x.ItemID == id).OrderByDescending(x =&gt; x.RecordTimestamp).FirstOrDefault(); The OrderByDescending orders the results by the field you want to order it by (in your case, the timestamp of the record since you want latest). FirstOrDefault() will give you one record, or a null, allowing you to handle the possibility of no record (or you can throw an exception by just using First()) Good luck.
28 occurrences of `await`, 0 of `ConfigureAwait`. Do you really **always** need the synchronization context?
For authorization, does your application use ASP.NET Identity (formerly Membership) cookie authentication with a login page? I'm using the Postman Interceptor extension [1] for Chrome which works in conjunction with Postman to pass along the cookies set by the Chrome browser. Install Postman Interceptor. Run your application in a browser and access a controller action that requires authentication. Login successfully and your authentication cookie should be set. Now go over and use Postman and the Interceptor add-on / browser extension should pass along the authentication cookie that was set in your browser when you logged in. [1] https://chrome.google.com/webstore/detail/postman-interceptor/aicmkgpgakddgnaphhhpliifpcfhicfo?hl=en
Yep, this is exactly what I just discovered and it did the trick.
"Best practices" are great ... once they're proven as such. Making a new coding technique a "best practice" simply in order to satisfy some urge to prove that XYZ new programming language is the best thing ever is just hubris. Not a best practice. I suspect we agree. Violently.
Can you point to where the HttpClient class explains how to reuse connections and keep alive where possible? I'm not seeing on the [HttpClient page](https://msdn.microsoft.com/en-us/library/system.net.http.httpclient\(v=vs.118\).aspx) nor in its remarks. Just curious if your snark is justified. 
I'm building a WMS/WMTS server, which is incredibly painful due to the lack of a decent native drawing library. People are doing real work in .net core.
I haven't done WPF for a while (stuck in Java-land at the moment ;_;) so I'm a little rusty, but IIRC... If you're using the MVVM pattern, then your view's DataContext should be set to an instance of your view-model.
I make no claim as to the thread safety of the object, I am just saying if you want connection reuse you should not dispose of the connections. The docs make no mention of reusing connections so it can be assumed it does not do so. Just because something is IDisposable it does not mean "You must inline dispose this every time you want to interact with its functionality" it means "this can be disposed of automatically inline when you want to get rid of it". In this case OP did not want to get rid of it, disposed of it, then was amazed that it did not reuse the connection.
Yeah this really annoys me too. For a start, I now feel like an idiot for not realising it - despite the fact that it's clearly the most logical way to use HttpClient according to all other norms within .NET. Then again, none of my projects I'm working on have any chance whatsoever of maxing out 4,000+ sockets within 4 minutes: that would involve every single employee within the company accessing the system within that 2 minutes, and considering most of the company work shifts that's quite unlikely
But they established the expectation with things like SqlConnection which does not actually create and destroy a SqlConnection. SqlConnection is meant to be used in a using block with a short lifetime, and the class manages the underlying connections in the optimal way without the user having to worry about it.
Great, what class has the .NET framework provided to do that for me? Because it's bad and unfinished if they provided the HTTPClient class without such a thing.
In the case of SqlConnection it is documented though. https://msdn.microsoft.com/en-us/library/8xx3tyca(v=vs.110).aspx For example Even the API docs https://msdn.microsoft.com/en-us/library/system.data.sqlclient.sqlconnection.open(v=vs.110).aspx "The SqlConnection draws an open connection from the connection pool if one is available. Otherwise, it establishes a new connection to an instance of SQL Server." These are the type of docs you should be looking for
I wrote almost exactly the same tutorial already [here](http://coderscoffeehouse.com/tech/2016/08/19/real-world-aspnetcore-linux-example.html).
When I replace just the **Find** with **Where** I get **Error CS0266 Cannot implicitly convert type 'System.Linq.IQueryable&lt;ReqMan.Models.MyLog&gt;' to 'ReqMan.Models.MyLog'. An explicit conversion exists (are you missing a cast?)**
Looks like adding .FirstOrDefault(); after changing where helps. [This]( http://stackoverflow.com/questions/16333557/an-explicit-conversion-exists-are-you-missing-a-cast) tipped me off. Hopefully I can get this working now. Ordering by timestamp is a good suggestion. I did put a datetime field on the table for this purpose but will use the primary key for now while I test. 
Oh, of course! That's pretty simple. `IQueryable` is kind of an expanded `IEnumerable`, and in your case that means you're trying to assign a "list of values" to a single `MyLog` variable. Assuming you know there is only one MyLog with the ID you're looking for, it's safe to just call `.First()` to get the first one (or maybe `FirstOrDefault()` if you don't know one exists). Otherwise, you need to change your variable to accept potentially multiple matches.
Meanwhile in the docs: https://msdn.microsoft.com/en-us/library/aa560610(v=bts.20).aspx Lingering is also documented in the socket class, and in the tcpclient class. It is fine if you did not know, but now you do: There are resources associated with a tcp socket which can linger at the OS layer, it also takes extra time to open a connection compared to reusing a connection to a server on both the HTTP level and the TCP level. Re-use connections where possible, the nature of the beast means that the exact way you wish to do so may depend on the application though. 
Don't write mock tests for controllers, that's the route of pure pain. Make your controllers as thin as possible, shoving everything they do, other than reading from HttpContext, into a "services" library. Then test that library as much as you can.
That's the documentation for BizTalk, not HttpClient. And it offers the wrong solution. 
Good point. I ran into this about a year ago. I dont have any docs on it. The documetation sucks and is misleading on this point. You can find more information on it if you search for asp.net controller best practices for using httpclient. Lots of people have the same problem putting this code in actions. Solution is dependency inject a singleton across all app lifetime 
&gt; That seems to be the standard. EF makes it super easy to persist data to SQL Server with minimal work on the developer's part. However, for enterprise level web apps this just seems like a disaster in the making. How's that? Is it a problem with EF or with ORM in general?
Why not use a database?
Here's the ASP.NET MVC path for preparing for the 70-486 exam - 60 hours. http://blog.pluralsight.com/asp-net-mvc-microsoft-exam-70-486
Did you ever try TypeScript? To be honest I'm migrating away from C# and into TS / Node. We're all confused about what is going on with .NET Core...
The form doesn't store long-term data. The form is just a few columns of checkboxes that filter content in the next panel down.
My main issue is that I don't like not knowing what queries are being used to gather the data since they're generated by the ORM. IMO it makes it just that much more difficult to troubleshoot problematic queries. I know EF allows for the use of stored procs, but when you get to that point, it seems like you're starting to defeat the purpose of using an ORM to begin with.
That's because you are foolishly looking at the current version of the documentation. If you look at v110 of the docs, it lists which methods are actually thread safe. https://msdn.microsoft.com/en-us/library/system.net.http.httpclient(v=vs.110).aspx 
Interesting. Does Peachpie allow for PHP code to call .net objects or does it only compile to .net?
True. I am also interested to see a setup with a database in use. What are people using with .net core these days anyway? Seems like ms sql server is the only decent option but thats not (yet) running on linux. (I know, it is, but not production ready...). 
It's a joke. Obviously I don't really think your are foolish just because you didn't know that hidden in an old version of the documentation was the information you were looking for. Though I suppose you could call Microsoft foolish, or worse, for losing that information between one version of the docs and the next.
I feel like I'm taking crazy pills. I keep reading about how EF sucks and NHibernate is more mature and better, but I'm on a project where we're using NHibernate and the random things that I can do with Linq to Entities but not Linq to NHibernate make me want to tear my damn hair out. Or is it just
Indeed graphs aren't accurate. I was searching for .NET jobs around that peak and it wasn't really any different than a few months prior. Also, the current graph indicates that Java has less jobs than .NET whereas in reality, it has over 10k more.
&gt; I prefer to stick with ADO.NET Check out Dapper.
We looked into using Postgres awhile ago. I remember drivers working, but migrations were a disaster (no PowerShell on Linux!)
Firstly, there is no MVC6 - it was renamed to MVC Core and they dropped the 6 because it's not really a successor, but a rewrite. &gt;First of all, .NET Core or .Net 4.6? Right now, .Net 4.6 in my opinion as the ecosystem of libraries have not yet caught up for .Net Core support. **Note**: You can run MVC Core against .Net 4.6 - it doesn't need to run against .Net Core. &gt;Is 4.6 more battle tested than Core right now though? Yes. &gt; Is it easy to migrate from one to the other? It depends. In our situation we're compiling against both .Net 4.6 and .Net Core at the same time to ensure we're not building anything which fails to compiles against .Net Core &gt;Do they both allow MVC dev? Yes. &gt;Entity Framework 6 seems like the go-to database architecture layer, with SQLServer in the background. Has this changed? EF is popular but there are other DB access technologies which are equally popular. It all depends on how "heavy" you want your ORM to be. &gt;Is Razor still the most trusted view engine? And what front end fraweworks work well with ASP.NET MVC? Most of the tutorials I've been using simply use Razor with some occasional jquery and Bootstrap for styling. In my nodejs dev, I typically created WebAPIs on the backend and made SPAs that gathered json from the API instead of views, so I'm not sure how well frontend frameworks play with MVC and the Razor view engine. Razor is great for generating HTML, but if you're building a client side heavy app, you'll typically not be using it. You'll write your app in Angular/React/Whatever, and then talk to your API. Here's where things get muddy. MVC Core supports both "MVC" and "WebAPI" style programming scenarios. Prior to this, there are two separate (although similar) frameworks, MVC and WebAPI. 
You can review the generated queries pretty simply
I'll do that.
[Here is a utility class](http://pastebin.com/cweVKvf6) slightly modified from the original file that I got from the interwebs somewhere. It creates extension methods on the controller that you can use in your tests to help in mocking out controller dependencies. This is using syntax from the [Moq](https://www.nuget.org/packages/Moq) mocking framework but you can modify it for another framework pretty easily I'm sure.
Really appreciate the in-depth response. It's good to know that I'm mostly on the right track. I wonder if I can wrap the database access layer and configure the layer at runtime so that I can start out a project with EF and swap it out without too much tech debt if EF doesn't grow with the app?
Knockout is a great framework for when you want to make "one part" of your site client side driven - not so much when you want an entire single page app. And also keep an eye on: https://github.com/aspnet/javascriptservices Driven by Steve Sanderson (who started Knockout). This is server side rendering of your JS application. It's still quite early days but very impressive stuff. 
*pfew* my old SarcasmsBuster2000 had it right already in its second try! I'll poke hanselman to see this updated, thanks. 
I have found it to be just that. If you find yourself with a question you can PM me. I work with everyday so should be able to help. Good luck. 
Looks good. If you are interested you could try to implement SignalR for a realtime communication between your browsers. Meaning you would have to refresh and comments and messages would just get updated automatically. 
Thanks, will do!
Man that `Startup.cs` is going to get bloated *quick*.
True. I think at some point when this starts happening you'd extract some of the code into helper classes. I.e. a DatabaseConfigurationHelper, ConfigurationBuilderHelper, etc. Haven't gotten there yet but that's what I plan to do once it becomes necessary.
2meta5me
WebClient or HttpClient. Create your API and inside use one of those two to create the connection to the other API.
Here you go: https://awesomelists.top/
would you have a link to an example? 
Yup!
Thanks I figured as much. Do you have any idea how I can deserialise and potentially reserialise the data in transit? I've discovered anything .Net is almost impossible to Google. 
There are few things that you should mention/show: - mention .NET Core is mainly for ASP.NET Core and web/server .NET, Mono will still reign in terms of mobile and gamedev - show .NET Core working on at least two different platforms, ie. Windows (both CLI and Visual Studio) and some Unix (Ubuntu or RHEL?) - mention that not everything is being ported to .NET Core (goodbye WebClient) and some approaches are being changed (ASP.NET Core is now actually a mix of WebAPI and MVC with all the concepts ported or inspired by WebForms in MVC being thrown away)
- Emphasize that you can use the full framework (4.5, etc) with aspnetcore so you can start aspnetcore development with legacy system. - Demonstrate dotnet watch 
Is it doing anything to the requests or just forwarding them on? If look at the IIS modules URL Rewrite and Application Request Routing and see if you can just set it up as a reverse proxy without writing any code.
Discuss tooling. I.e. dotnet. 
What country? Someone at my work is gonna be speaking about the same things 
Talk about built in DI and Middleware in Asp.Net Core. Talk about the lose of the AppDomain and ConfigurationManager (a lot of libraries are dependent on this). Maybe touch on NPM/Bower + Grunt/Gulp if you still have time. -All of my examples are specific to ASP.Net Core since there have already been so many good .Net Core suggestions.
&gt; mention .NET Core is mainly for ASP.NET Core and web/server .NET, Mono will still reign in terms of mobile and gamedev This is part of the reason that I strongly dislike the name ASP.NET Core. It seems to promote misunderstandings like this. * .NET Core is currently the [.NET Core foundational libraries](https://github.com/dotnet/corefx), the [.NET Core JIT Runtime](https://github.com/dotnet/coreclr), and the [.NET Core AOT Runtime](https://github.com/dotnet/corert) (think ngen and a few related tools) and can use any .NET Standard 1.6 library package or executable (for example ASP.NET Core); there are likely other tools specific to Core I am missing * Mono is currently the mono base libraries (close to the 4.5 profile; somewhere around .NET Standard 1.1), a JIT runtime, an AOT runtime (as far as I can tell all of this is simply called "mono"), and additional libraries (including a compiler, a reflection framework, a static analysis framework, several gui frameworks, a debugger, an editor, etc.) * .NET desktop framework is currently the [.NET Framework 4.6 library](http://referencesource.microsoft.com/), the .NET 4 JIT Runtime (CLR) and a bunch of [related tools](https://msdn.microsoft.com/en-us/library/d9kh6s92(v=vs.110\).aspx) and can use any .NET Standard 1.6 (as of 4.6.3; [more here](https://github.com/dotnet/corefx/blob/master/Documentation/architecture/net-platform-standard.md)) library package or executable * ASP.NET Core is a library that [targets the .NET Standard 1.6 api](https://github.com/aspnet/Mvc/blob/dev/src/Microsoft.AspNetCore.Mvc.Core/project.json#L61); in this respect it is similar to Powershell, Npgsql and Entity Framework Core and other libraries in the ever growing set of packages available on nuget.org There is no reason you couldn't build all kinds of applications against .NET Core. Console apps can be built (an ASP.NET Core web app is a console app). UWP apps are .NET Core. Additionally the lines between Mono and .NET Core will [continue to get more blurry](http://www.mono-project.com/docs/about-mono/dotnet-integration/).
Not new though, out existed in previous versions.
Are you in Pittsburgh
All of the `App_*` folders are essentially leftovers from an era where business logic, frontend code, and configuration were comingled into a single project. You need only look at the second word in each to understand the meaning... Data for your database, Start for application init scripts, Config for configuration and Code for business code. This is, as far as I know, the standard for .NET moving forward. For static files, it really depends on your personal preference. It seems that up until somewhat recently, .NET and NuGet packages have preferred web root folders like "Scripts" to store static assets. I, however, prefer putting everything under a single "content" folder so they are easily managed separately from any application code. 
I still use App_Start as the place where all of the startup config stuff lives. Bundle defs, routing, owin setup, handlers, global filters, etc. App_Data can just go die in a fire though. It used to be where code and files lived, and in some cases became a really horrible way to push actual code files onto the server. Both of those concepts are outdated now. Ideal scenario: code that deals with handling the details of how your site operates is in controllers, handlers, filters, etc. on the website. Code that handles your workflow is in a separate library and referenced by your site. And code that handles retrieving and updating data is in another library and referenced by your workflow. It really helps promote separation of concerns.
If you can get what you need through NuGet, do so. This includes jquery (which should be included by default) and your css framework of choice (bootstrap by default). You can change the structure to whatever you like, but .js files generally go into the `\scripts` folder, while css, images, and fonts go into a `\content` folder. There is a `\views\shared` folder which is where all of your common views will go. This includes layouts that act as templates for other views. I recommend creating a few of the template MVC projects and get familiar with how they do things. Lots of tutorials are available to help you get through the learning curve.
You can do this in ASP.NET Core really easily. I wrote [this](https://gitlab.com/jsedlak/brucke/blob/master/MyProxy/Startup.cs) in about 30 minutes.
I recommend a multiple projects structure. For example: MyProject.Commons MyProject.Business **MyProject.Entities** **MyProject.DataAccess** MyProject.Services **MyProject.Web** The bold ones are the important ones.
I am thinking App_Code. 
If you are just starting out, I would not recommend this. The last thing someone who is confused about where css should go is abstract the project into 6 projects. App_Data &lt;-- Ignore this. This is for any local data files. e.g. local DB. App_Start &lt;-- Startup and config scripts Controllers &lt;-- A MVC controller, a route action. Models &lt;-- Data objects and data access. scripts &lt;-- javascript for views Views &lt;-- cshtml views Content &lt;-- images and css, other statics
Thank you for your advice regarding mocking DbContext, I have now updated the article. ProductService will act as repository, and other product related services will access this. So you can say, i have embedded repository in services.
I disagree, to me separate projects make it clearer where things go. Much greater flexibility.
Considering that project.json file is going away in the next version or the tools, I'd skip the changes to the project/build.
We are talking about a guy who doesn't know where to put his CSS files. This is not the place for [Enterpise Fiz Buz](https://github.com/EnterpriseQualityCoding/FizzBuzzEnterpriseEdition).
Nice writeup, but not sure why you're injecting the db context directly into the controller and then just creating a repository instance with it. You should make your UserRepository class implement some `IUserRepository` interface and then you should just be able to inject the repository like so: public HomeController(IUserRepository userRepository) { _userRepository = userRepository; } Also keep in mind that DbContext already implements the repository pattern, so there's really no need to use your own repository implementation in the first place unless you have one of two situations: 1. Your application talks to multiple data sources 2. You expect to switch database engines every 5 minutes as opposed to once or twice 
I generally agree with this: My App_Data folder tends to hold things like, for example, SQLite databases containing session data: If I don't have that then, like you, I delete it. Although I'd point out that it might worth copying the .NET Core convention of naming your Static folder wwwroot with sub-folders eg css, images, and js. It's not a huge issue, but since .NET Core does that by default and appears to be likely to become the next "default" approach, it follows the concept of "Least Astonishment"
For my projects - App_Data is typically empty, although occasionally holds a SQLite database containing session data or other temporary data - eg SMS one-time authentication codes: things that aren't "real" data. This is just preference, though: for most projects, it's deleted - App_Start contains some initialisation scripts etc - typically things like Bundle Configurations. I tend not to put anything in here myself - Controllers: Fairly obvious, just controllers - Models: This has been known to get messy, as there are a few approaches. I personally try to keep this strictly to ViewModels, with Data Models being separate and part of their own structures - Scripts: Originally where your javascript files would go, although more people are using the same approach as .NET Core now, and using a static folder (eg wwwroot, content) with sub-folders for js/css/images etc. I prefer the latter approach, it's much neater. - Views: Views for your MVC My view structure is fairly simple - \Views\Shared contains _Layout.cshtml, _Error.cshtml and other shared views or partial views - \Views\&lt;ControllerName&gt; contains views associated with a specific controller I use the same convention for my ViewModels under the \Models folder. Data Models are typically contained within their own package-style folder structures, where separate from the ViewModel My wwwroot folder looks something like - \wwwroot - \css - \libraries //bootstrap etc lives here - \site //"my" css - \js - \libraries //jquery etc live here - \site //"my" javascript - \images - \libraries - \layout //images relating to the site design - \content //"real" images used in articles etc 
Why are you awaiting the `result`? The task has already been `await`ed when you called `Perform`, so the task would have RunToCompletion at the return point. Just return `result` since it has already been `await`ed
I've since changed this in the repository the holds the code. I'm still debating whether I update the write up as I wanted it to be a kind of beginners guide but I don't want to be telling people the wrong way to do things. I see a lot of posts doing generic repositories that then get extended but I try to avoid inheritance as much as possible so I'm also undecided as to whether I want to do that. I feel that it's very likely I do another shorter post explaining how to decouple projects better and make these changes in this post rather than modifying the existing one.
I used to take this approach, but now only split things out when necessary as it just added complexity.
I'd probably wait for the Azure bits to hit RTM before going full steam ahead with them.
&gt; In other words, you can’t build your app on Windows and then deploy it to a Mac. Isn’t that what the `-r` parameter to `dotnet publish` is supposed to do? `dotnet publish -c Release -r debian-x64` should work. Also for #9, Newtonsoft.Json is a third-party library, it’s not part of .NET Core.
Yeh OP is wrong on many things in his article. 1. You can publish your build on e.g. Windows and then deploy it to other systems. 2. Furthermore you can configure how the json serializes in your Startup.cs
At what point does authorization fail? Is it before, during or after your provider is called? Have you tried to create a dummy provider that just spits out all available roles and always returns true for IsUserInRole?
You can build on OSX and deploy to Linux 
Yes, I've done this a few times already (and was even doing it with the old DNX 18 months ago).
I like that last link - I was half expecting Mono to get annoyed that Microsoft are "moving in on their turf" rather than see it as a way to bring the two closer together I know that shouldn't be surprising, but I find too many open source projects get quite territorial and defensive, forgetting the reasons the project started in the first place (essentially: Open Source, Cross Platform .NET)
&gt;It's a WCF named pipe. If you are trying to connect remotely, it won't work because named pipes work within a machine boundary not between machines. For that you need a TCP binding. Google netNamedPipeBinding for info on the named pipe binding. So I'd have to develop an agent to run on each machine? That might be problematic. &gt;If you open a high port like 5000 on the machine, you could expose a NetTcpBinding endpoint to remotely connect. I ran netstat on one of the machines, and there doesn't appear to be an open tcp port for that service. &gt;If your client app resides on the machine with the monitored service, you should update your question with the client config. The server and client config need to have compatible endpoints setup for communication to be successful. The client and service run on the same machine. They usually have no problem talking to each other, but after about 4 hours, the service stops responding. It stays in the Running state, but the client can no longer use the services it enables. The client config is below (product name replaced with TEST): &lt;configuration&gt; &lt;system.serviceModel&gt; &lt;bindings&gt; &lt;netNamedPipeBinding&gt; &lt;binding name="LocalWebHostServiceEndpoint" closeTimeout="00:01:00" openTimeout="00:01:00" receiveTimeout="00:10:00" sendTimeout="00:01:00" transactionFlow="false" transferMode="Buffered" transactionProtocol="OleTransactions" hostNameComparisonMode="StrongWildcard" maxBufferPoolSize="524288" maxBufferSize="65536" maxConnections="10" maxReceivedMessageSize="65536"&gt; &lt;readerQuotas maxDepth="32" maxStringContentLength="8192" maxArrayLength="16384" maxBytesPerRead="4096" maxNameTableCharCount="16384" /&gt; &lt;security mode="Transport"&gt; &lt;transport protectionLevel="EncryptAndSign" /&gt; &lt;/security&gt; &lt;/binding&gt; &lt;/netNamedPipeBinding&gt; &lt;/bindings&gt; &lt;client&gt; &lt;endpoint address="net.pipe://localhost/localwebhostservice/service" binding="netNamedPipeBinding" bindingConfiguration="LocalWebHostServiceEndpoint" contract="LocalWebHostService.ILocalWebHostService" name="LocalWebHostServiceEndpoint"&gt; &lt;identity&gt; &lt;servicePrincipalName value="host/PT-TOWER.TEST.local" /&gt; &lt;/identity&gt; &lt;/endpoint&gt; &lt;/client&gt; &lt;/system.serviceModel&gt; &lt;/configuration&gt; Thank you for your help.
The Controllers/Models/Views system never made sense to me. You shouldn't have to go 3 places to implement one feature. I get rid of it and create folders for functional areas, e.g. Users. Then the controllers, models, and views that manipulate users go in there.
So If I wanted to make it where I could do this, as in have a console app I have a window to browse for the expected xls file and it spits it out properly in its own repo or same directory as the original doc stayed, either or. But was hoping for something within that realm.... However if your macro idea would work, can i wrap a excel macro somehow in a vb.net console app? Also, can you / would you be willing to help me formulate this? I am watching some .net beginner courses on pluralsight but am still very much a novice
Yeah a VBA Macro would be easiest way to get there. You can implement it directly in your data workbook or place it in a new workbook and let the user search for the data workbook. Its also possible in VB.NET/C# if you want to, I often use NetOffice for stuff like this, they have some samples on their site which should help you figure it out. 
&gt; NetOffice fascinating but a bit confused as to what exactly netoffice is... is this a framework / plugin / library like thing but for office?
If you are actually hitting the role provider, authentication is working properly. Is the GetRolesForUser returning anything? If you are going to always return true for the dummy provider, you should return an array of all roles for that method. This is the code for the attribute that authorizes the controller https://github.com/ASP-NET-MVC/aspnetwebstack/blob/master/src/System.Web.Http/AuthorizeAttribute.cs it may help to look there and see if the code is calling something you don't have set. That is a newer version, so using dotpeek or another decompile tool on the correct mvc version may provide more insights.
So, the server and client are 3rd party. We have no access to the source, and the vendor has all but refused to help us with this. No modification to either of them, aside from config file changes, is possible. Can I accomplish #2 just by modifying the server XML I posted? I don't think #3 would be possible without source access, and #1 is the last resort option.
Awesome! I'll try that asap.
If you don't have access to the source #2 is your best option. You can do it entirely in config. #3 is possible without code changes to the app. You can add a custom service behavior in the config and implement that behavior in a custom bin deployable DLL. The behavior would log service events or log errors. Here is an example using log4net https://pieterderycke.wordpress.com/2012/09/05/logging-all-unhandled-exception-in-wcf-with-log4net/. WCF also has its own logging system that uses system.diagnostics https://msdn.microsoft.com/en-us/library/ms733025(v=vs.110).aspx . You won't want to leave that on very long in production with a simple log file as it generates a large amount of data and can affect app performance. It is possible to leave it on if you setup a trace listener that rolls a log file, logs to memory out of process or some other means to avoid clogging up the filesystem. There are also flags to filter severity and log level. I would only go this level if you can't detect the error with the Service Behavior or if you want to find the root cause of the problem.
I have created this lib after many experiments with different approaches and many performance profiling sessions. I even have used C++ for XAML-based markup rendering for performance reasons. C++ indeed was much faster than C# in massive XAML tree manipulations, but anyway not fast enough for low-end Windows 10 Mobile phones. And finally I created this lib and just have released it to general public. **This lib is based on refactored real-world application code and already have been tested in a real-world app.** Ironically, low-level Direct2D approach (using great Win2D .NET wrapper for D2D) have proven to be extremely simple if compared to earlier XAML-based variants. I was fearful to use low-level Direct2D as a foundation for markup rendering engine, but in reality Direct2D-based code became much simpler and shorter. **Mostly because Direct2D itself is a great API and provide developer the rich and consistent experience!** And it not only feature-rich, but also extremely faster than XAML framework composition engine for this task. **Direct2D is just the right tool to solve custom markup rendering problem.**
Windows 8.1 universal app model is an ancestor of Windows 10 universal platform. It allowed to share most of source code between Windows 8.1 and Windows Phone 8.1, but not all as UI in Win8.1 and WP8.1 wasn't fully compatible. So, universal Windows 8.1 apps was separated into 3 projects - "shared" code, and 2 specific projects for Windows 8.1 and Windows Phone 8.1 As for today, "universal" Windows 8.1 app model is obsolete and have limited support from Microsoft. Windows Phone as a platform is dead and replaced with universal Windows 10 platform. Windows 10 also have replaced Windows 8.1 on desktop. So, universal Windows 8.1 is a legacy platform.
+1 for this response, I don't remember orderby working like this. I thought it took a queryable, an expression but could be wrong
Well now that is some interesting information. I haven't delved into WCF that much, but it looks like I'm going to be jumping in that rabbit hole now. I like the option of tapping into WCF's logging system. I'll leave the filter on warning to be safe. If I'm understanding this correctly, I can add something like the following to the server .config? &lt;configuration&gt; &lt;system.diagnostics&gt; &lt;sources&gt; &lt;source name="System.ServiceModel" switchValue="Warning" propagateActivity="true" &gt; &lt;listeners&gt; &lt;add name="xml"/&gt; &lt;/listeners&gt; &lt;/source&gt; &lt;source name="myUserTraceSource" switchValue="Warning, ActivityTracing"&gt; &lt;listeners&gt; &lt;add name="xml"/&gt; &lt;/listeners&gt; &lt;/source&gt; &lt;/sources&gt; &lt;sharedListeners&gt; &lt;add name="xml" type="System.Diagnostics.XmlWriterTraceListener" initializeData="C:\logs\Traces.svclog" /&gt; &lt;/sharedListeners&gt; &lt;/system.diagnostics&gt; &lt;system.serviceModel&gt; &lt;diagnostics wmiProviderEnabled="true"&gt; &lt;/diagnostics&gt; &lt;/system.serviceModel&gt; &lt;/configuration&gt; Then check c:\logs\traces.svclog for the log?
There are two trace sources in that config and one is set to ActivityTracing and warning. I'd make sure they are all set to warning. The svclog will be a binary dump if I remember correctly. You need the service trace viewer tool to look at it https://msdn.microsoft.com/en-us/library/ms732023(v=vs.110).aspx
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/csharp] [Is ASP.NET MVC dead? • x-post from \/r\/dotnet](https://np.reddit.com/r/csharp/comments/51jkdr/is_aspnet_mvc_dead_xpost_from_rdotnet/) [](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))* [](#bot)
I think once you have gone down the "JavaScript application" rabbit-hole, all you're gonna end up using .NET for is Web API. There are still plenty of applications that don't need that fanciness, and can get by just fine with full page refreshes of server-rendered HTML...but if you know a framework like Angular really well, why would you ever do it differently?
&gt; most new projects are still .NET 4.0 You do know that .NET 4.0 gets no further security updates anymore, right? That version is not supported anymore and you're a potential security risk that way. It also doesn't support the latest SSL and TLS versions, so potentially when you have to do web request to services you simply won't be able to access them at all.
OrderBy CAN work like this, it's just unusual. My recollection is it has something to do with whether its run against IEnumerable vs IQueryable. edit: I just checked a project I have this in, as op mentions, it's part of System.Linq.Dynamic, or the Linq Dynamic Query Library [(website)](http://weblogs.asp.net/scottgu/dynamic-linq-part-1-using-the-linq-dynamic-query-library) (though I think it's still related to IQueryable vs IEnumerable
I generally steer clear of most frameworks (apart from the obvious; jquery, bootstrap)...as next year another will come along. I've seen projects with a multitude of these in them and nobody can make any senae of how they work. Stay simple if you want maintainability. 
Absolutely not. Single Page Apps are great when you need an interactive rich UI. Everything else is usually simpler to build with Razor views.
I've been saying that since people were discussing how Web Forms is dead. I was like "So what, ASP.NET MVC is dead too" and they were all up in arms explaining that ASP.NET MVC is the future and so on. That being said there is still room for server generated HTML specifically where SEO matters. Sites like Wikipedia that are mainly for presenting data are good fit for good old server side code. However I expect that the so called isomorphic frameworks will take over this area too. For example you can render React on the server even today.
MVC is not dead. You use the tool for the job. Client-heavy apps are not what you always want.
I come from this reality as well. I believe big companies with battletested code always stay behind the hype curve until new tech are more than stable. On the contrary startup companies are always up for grabs on new crazy stuff . Once in a while a startup becomes more than that and starts getting behind the hype curve. It's like a maturation process for Software development :p
Indeed it isn't. ASP.NET MVC 5 is a best option for enterprise web development as it based on the full .NET stack supporting any enterprise scenario. If you're developing for enterprise and your backend is mostly Windows based, natural choice is to use ASP.NET MVC 5. For non enterprise purposes it isn't so obvious. ASP.NET Core and .NET Core aren't mature technologies and I don't know if they're production ready. Anyway, ASP.NET isn't dead and ASP.NET Core is actively developed. 
C'mon, ASP.NET MVC 5 and Core support WebAPI and any JS frameworks. And on server-side .NET is a much better option than Node.js