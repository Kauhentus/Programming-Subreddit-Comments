Yeah for an extra $40 it's a no brainier. Well worth the money for me. 
&gt; I was too intimidated by the gang of four book and that is why I took to reading Head First first. I can't speak for the Head First book, but I will say that the gang of four book teaches nothing about design patterns. Well almost nothing, they scratch the surface in the introduction that most people skip. In fact, you've already surpassed the gang of four book by actually implementing the design patterns instead of just talking about them in theoretical terms. So congratulations for that.
P.S. I should apologize for being so harsh. The fact that you've gone this far says a lot. The thing is, I've seen far too many people take the catalog approach to design patterns and never actually learn what they are. An important concept that the books often omit is "pattern languages". A pattern language is the set of design patterns that apply to a given context. Originally it was used in architecture, so there is a pattern language for houses, for bath rooms, for single-story office buildings, multi-story office buildings, etc. In programming your pattern languages would include server-side business applications, tile based games, side-scrolling games, CRUD/data entry applications, etc. For a given pattern language, you discover and name the patterns that you see repeated. Then when you have to add new features, you already have the vocabulary for describing them. *** If instead you get into the mindset that the only design patterns are the ones you found in your book, you never learn to recognize the patterns that you are actually creating and using. Which in turn makes it hard to write consistent code. I don't want that to happen to you. 
Oh no no no, don't apologise. I would have been harsher if you did something similar in something I was passionate about. I know that my code isn't great and the patterns are not exhaustive. I felt that there was no simple introduction to design patterns except for Head First and I wanted beginners like me to be able to understand them using C# instead of java. And thanks for the info on "pattern languages". I don't fully understand it right now and it seems to be a lot like a DSL but you have given me something new to read about. PS: I admit I felt a little intimidated by your first line, but it makes sense and I like to hear the other person's honest opinions so that I can give my side of the story and hopefully learn and clear any misunderstandings. Peace :)
Check: https://carlos.mendible.com/2017/03/21/step-by-step-running-aspnet-core-on-raspberry-pi/ then try to add dapper or ef to access a database... Hope it helps...
Knowing what design patterns are (themes that keep repeating themselves because certain problem domains lend themselves naturally to certain solutions that share similar concepts) and what they aren't (a golden hammer that will solve the problem in front of you no matter what the problem is) is pretty important!
I think the worst thing the GoF book did was name the patterns they discussed. I don't know how they could have written the book otherwise, but that alone led so many people down the wrong road.
Sorry I'm not used to VS, I don't find how to switch between font themes. I see I can change font colors so they look OK when I invert my screen colors but idk how to change between a set of colors. If you're asking why I invert colors at all, it's that I don't want to stare at white pages on the browser. I want everything to be dark.
The other mistake was not using real code so that they could talk about the limitations. For example, the visitor pattern has a hugely negative repercussions on extensibility. (Mitigated somewhat in later Java versions)
Yeah thank you /u/AbishekAditya for doing this. And thank you /u/grauenwolf for the replies. I'm learning lots from reading this. 
It may just be me, but i wouldn't consider those "Design Patterns". The reason being that those are patterns that in some cases are very specific to C#, and are not really about overall application design, but are recommendations of how to implement language features. Maybe that's being a bit pedantic of distinguishing between design and language features, but i wouldn't go to a Ruby shop and expect a property change notification pattern to be applicable, where the strategy pattern would.
Top man. Love this shit
Thanks. It means a lot
Visual Studio has a built-in dark theme. https://msdn.microsoft.com/en-us/library/hh923906.aspx
I would also highly recommend you look into the [SOLID design principles](https://en.m.wikipedia./wiki/SOLID_(object-oriented_design). I practically live by them today. 
A library for building json:api compliant web servers. Removes a significant amount of boilerplate while offering out-of-the-box features such as sorting, filtering and pagination.
Yes I invert via the OS. I can't stand the white background of webpages while reading docs for instance. 
Brave (maybe even foolhardy?) guy. Doing that as a French (or any EU country) citizen is an invitation for litigation.
This is great. Thanks.
looks great, how do you populate your database of movies ? 
When you say old Access database you're talking about a .MDB file right? That's possible.
Yes I am talking about MDB-Files. Thank you for your answer.
If you want a sql implementation, you might be able to do something like this in your dbContext: protected override void OnModelCreating(ModelBuilder modelBuilder) { modelBuilder.Entity&lt;MyEntity&gt;() .Property(p =&gt; p.CreatedAt) .HasDefaultValueSql("getutcdate()"); } But, this tightly couples you to SQL. Whereas using the SaveChanges method mentiond by /u/TheBearJew is independent of the EF provider you use.
Thanks [ThaBearJew](https://www.reddit.com/u/TheBearJew) It works via your method you provided!
Why not use odbc? 
Because i didnt know about it and i think if i would use it i would have to modify my code more in contrast to switching to ado, right? What are the advantages of odbc compared to ado?
You are not using the project variable, but returning _context.Project.ToListAsync(). Also you might want to consider using an actual viewmodel and not returning an entity directly.
you're missing a comma productName,productPrice,productDetails,productInStock) values('" + this.dataGrid1.SelectedItem + "','" + this.dataGrid1.SelectedItem + "','" + this.dataGrid1.SelectedItem + "','" + this.dataGrid1.SelectedItem + "'
Where exactly? I am not seeing it D:
An alternative would be to use `DateTimeOffset` instead of `DateTime`.
It's calling ToString() on the row object. You need a specific property of your class, it's not sentient 
I actually do have a viewmodel. Here is what I am trying to do and why I was messing around with the query. I have a viewmodel/controller/view for a table called projects. Everything works fine and I am able to add new add/edit/delete projects. In addition to that table I also need a couple of fields from a projectstatus table where the statusID field matches up. How do you go about putting together something like this that basically builds an HTMl table in the end but the data is coming from two different tables in SQL server?
what Restract's trying to say is, you're inserting the WPF control into the database. C# calls the ToString() function behind the scenes of all objects if there's string related functionality, like the query you're building there. So this '" + this.dataGrid1.SelectedItem + "','" + this.dataGrid1.SelectedItem" Is Actually this '" + this.dataGrid1.SelectedItem.ToString() + "','" + this.dataGrid1.SelectedItem.ToString()" What you want is the content of it. Somethinglike dataGrid1.SelectedValue or ((TypeofTheItem)dataGrid1.SelectedItem).PropertyYouWantToStore
And I would do this in my Query string? Where I write the inset into CartDatabase?
Sorry I've never used ASP before and Core is my first attempt. Do you know of any better explanations? I know I need to join them, just don't know how!
You're already giving EF sql, just do your join in sql and select out your viewmodel from there. `select * from Table1 inner join Table2 on Table1.Id = Table2.Table1Id` I've only every used LINQ to SQL, but the end result should be the same.
I guess it's a firewall thing. I'd expect it to block (unknown) incoming connections by default. 
Did that, still not working. Any other ideas?
does one machine reply to ping from another? if No you are not dealing with asp setup issue but with more low level network setup/config issue
What's the output of dot net run?
Stability isn't the problem - it's rock solid as far as I can tell. It is however an undocumented mess, and figuring out even simple things that you took for granted before can be really frustrating. Most of the examples online are also trivial and don't cover detailed, complex, real-life applications. I would definitely not use this for mission-critical projects yet. 
I don't think that by using odbc you'd be changing all that much. Back like... 20 years ago, we made the switch from Jet to odbc to ado, and the code changes were minimal if your code is structured well. You really want to move away from access mdb files if you can, they're not good long-run ways to keep data. There's sql server express, sqlite, heck even xml files/json files. Lots of choices.
Yeah, generally coffee shops, hotels, etc will block communication between machines on the local network. If this is part of a VM setup then it's something to keep in mind.
Glad you've solved it. Mac firewall turned off by default doesn't sound very safe though. :-)
That doesn't do anything. Most systems recognise the IP as its own and will simply loopback on the interface with that IP.
Thanks! I am looking into ClickOnce and it looks like just what I need.
nice project on .net
https://github.com/Squirrel/Squirrel.Windows
This. Common sense, everyday stuff.
Been using ClickOnce for years. Works as advertised. 
Yep. You need a working knowledge of JavaScript, vanilla is fine, jQuery if you must, but I expect you to know some things like how variable scope works. I don't care if you know angular or ember or anything, I just want to know that you have at least dabbled with JavaScript before. A little Css is good too. 
As a craftsman, it's ok to buy your own tools. Ncrunch and Resharper are worth their weight in dollar bills.
Knowing design patterns is mostly good to understand each other. You can vaguely describe your solution as you have it in your head and no one will understand, but if you say "Adapter pattern" everyone should immediately know what you mean.
Data actually does have weight. I calculated the 400mb install of Resharper to be roughly 1.04 × 10-25 grams [[source](https://www.quora.com/How-much-does-a-kilobyte-weigh)]
Sadly, a dollar bill weighs about a gram.
Some of us rather embrace Forms, WPF, UWP and WCF, while doing ASP.NET only when there isn't any way around it. :) So I would say a passable HTML/JavaScript is already good enough, as there are many companies that don't do web with C# and OP didn't specify to which market he/she is in.
They specifically said web
Opps, I missed it, you are right.
This is true but a downside of MS tech (at least traditionally) was reliance on expensive tools and OS licensing requirements. Obviously if you move to another stack this disappears.
&gt; And then, how much does it weigh if you include all the cache space it uses on your drive? As great as ReSharper is, I'm not paying *that* much.
Would the time saved be where the SQL Server lives or where the call was made from?
The date time would be set by SQL Server itself. Don't forget UTC.
👆🏻 I agree. There's way too many things to know in the JS space so I really just want a good working knowledge of the language. Honestly I'm not even that picky about like "make me an inheritance prototype chain!" interview type questions and stuff. If you're sharp, I trust you can pick up what you need on the job.
I think web forms is like the worst. I know there's still a lot of it out in the wild, but I would never choose to work for a company that uses it heavily. It hides so much of the underlying web stack that I feel like the devs who rely on it only barely understand what's going on.
Isn't all that stuff junior devs should know? I've known and implemented all that stuff and I only have 2 years working experience and no CS degree. During my interview for this job a year ago, I was tested on SQL, ORMs, classes vs interfaces, etc. I wasn't tested on design patterns but I feel that every dev should at least know the gist of at least a few of the Go4 design patterns. Either I disagree with you, or I should be looking to apply to some mid dev roles. 
This calc is for the actual mass of the energy itself
I was talking about Windows Forms, not ASP.NET Web Forms. Agree with you, other than we in consulting don't have that freedom of choice.
This right here, Not only do tools come and go, but most of them maybe take a day to really learn, and many are only useful in specific situations, and depending on how many of those situations you have been frustrated by depends on whether you cared to get the tools. Understanding good design patterns, understanding why they are used when, understanding the importance of design first development, what a unit test is means I am getting a developer I can trust to do more than just be a code monkey.
I would look for the ability to create an auth login from scratch without scaffolding. Can you do it without EF? Can you set up Identity Server? Continuous integration? How do you deploy? The tools don't matter as much as the ability to take ownership over the harder parts of the development lifecycle and the architectural pains. 
With a comment like that, I clicked just to watch the car crash.
Oh, in the form of magnetic energy or the energy used to store the bits in NAND gates and the like in SDDs? (Seriously, I don't care and I'm just messing around. This whole idea strikes me like the Dilbert "use semi-colons instead of colons" to save space idea. Lol...)
Love the shortcut cheatsheet, now just stop re-doing the shortcuts every iteration so I won't need a cheatsheet.
I'm a self taught web dev and I'm have some holes in my knowledge. What is the thought process of managers regarding people like myself who may have 80-90% of what you're looking for but haven't gotten around to learning the latest and greatest framework? Are you interested at all in training people in whatever stack you decide to implement?
* Some form of source control, preferably Git (TFS is way outdated IMHO and tends to show ignorance outside the Microsoft stack) * An ORM/Micro-ORM of some kind, EF being barely acceptable (if you don't use one at all and write raw ADO.NET code or have some half-assed way to call objects, or worst just call DataSets/Tables/Readers and loop through them to map things that's in my book a huge red flag because it shows total ignorance of a problem that was solved over a decade ago) * MVC or Web API (read this as "More than WebForms" since there's hardly any reason other than legacy support to use WebForms; if you're writing greenfield apps in WebForms, you have a problem) * Rudimentary Javascript, jQuery or similar. At least the basics, but if you're doing things by hand when a framework could do it better, you're showing that you don't understand frameworks. * What unit testing is and why it's good; not necessarily have to use it (I've found woefully few teams that do it) but at least understand why it's beneficial to have, even if you don't use it. * What continuous integration is/why it's good (same as above for testing but for CI) Also not tools but I'd expect knowledge of the SOLID principles and why they are a good thing, and why abstractions are often good. I've seen too many mid/senior people that write out hundreds of lines of code instead of putting things in a class because "it's easier to follow"
I like this list and agree with all of it. But why is EF just "barely acceptable"? In my company (large bank), where it takes an act of God to get approval to use non-vendor frameworks (e.g., micro ORM like dapper), we have to stick to EF, or just go back to doing raw ADO.Net (which is good to know, but I hate using it). I know Dapper and dabbled in it, and could learn it if I needed, but I would be lying to say I have experience in it. The other point is on JavaScript. "... but if you're doing things by hand when a framework could do it better, you're showing that you don't understand frameworks." I would agree, if we are talking about the big 2 or 3 frameworks like Angular or React. But with a new JS framework coming out every month, it's not feasible to stay on top of more than 3 or so. But I would guess you agree with that.
Grammar and punctuation issues make that post unreadable, for me.
Not to mention the general attitude of the thing - when the link to "repository pattern" is a LMGTFY link it is a pretty major turn off. Probably an attempt at humor, but in reality a douche move that makes me think he's an ass.
I think it's pretty clear that he didn't think this through and just wanted to rant. Which is frustrating.
there doesn't seem to be anything here
I tend to have a bad attitude whenever I come across any post that states "You're doing X wrong". It sets a bad tone right from the get go. Then the repository pattern came and went without a way to improve it. Then this gem... &gt;...neither is EF an abstraction for the storage mechanism it relies upon. It is instead a set of common APIs that let us access data in a uniform way. If you're going to write an article on EF you HAVE to recognize the fact that it's an ORM and you have to recognize the purpose of an ORM. And it is not "API's that let us access data in a uniform way"
Well, the wiki definition begins &gt; Object-relational mapping (ORM, O/RM, and O/R mapping tool) in computer science is a programming technique for converting data between incompatible type systems in object-oriented programming languages. There is nothing in there about abstracting away the complexities of the underlying data store. What do you personally think an ORM is?
&gt; Why would that be? Well two things. 1. You have basically locked yourself into using EF forever. There is no separation of concerns and likely no DI being used. You would be accessing EF specific components from your controllers. 2. Unit testing isn't going to go well... &gt;If you application changes, you either need to simply accept those prior decisions or modify the repository, violating the Open/Closed principle The Open/Closed principle allows for extension though. I don't think anyone expects that once a repository ends up in the wild you can't add methods to it. Again I think examples of code where you think the Repository pattern is wrong would be good (Because I'm not saying you are wrong, it's just that I haven't used it in the way you are describing it so I can't relate). The repository pattern is simply a way to to isolate the data access. It doesn't describe for example how it works with entity framework or anything like that. 
An ORM Maps a Relational database to Object. This isn't a personal belief I have, it's literally the entire purpose of it. It's also the letters ORM. It's not a set of API's to pull data. That's all on top of it. At it's core EF is a way to convert relational concepts to object models that we can use in code. In an article of how everyone is using EF wrong, not only does it not go into the reason for EF or ORMs in general. It distorts the purpose to be just a set of APIs to get the data. All that being said, it is most definitely an abstraction. The wiki definition begins... &gt;In software engineering and computer science, abstraction is a technique for arranging complexity of computer systems. It works by establishing a level of complexity on which a person interacts with the system, suppressing the more complex details below the current level. The abstraction is the mapping of relationship to objects. We just decorate a property here and there and the ORM creations and maps that relationship for us. It creates a relationship between two POCOs so we can then use linq to build the complex queries. We aren't telling linq how to interpret the expressions then translating them to joins with table and column names. We are telling EF how the objects interacts and it does all the heavy lifting for us. Edit: I forgot to mention that EF can actually build a database for you. Just by using POCOs and decorators EF will create a full relational database.
If you want the whole customer record, your repository has a GetCustomer method. If you want just the customer's name, your repository has a GetCustomerName method. If you want the Customer, his orders, and their products, you have a GetCustomerOrdersProducts. If you don't want them all at the same time, you have GetCustomerLazyOrdersProducts. Yes, you might have a lot of different methods, but that's because your business logic actually wants the results of all those different methods. No?
What do you gain by needing to declare all those? Cut the middle man and let your application do what it needs to.
I would suggest you look into EF 7's in-memory support. You can do all the mocking you could want with a simple .UseInMemoryDatabase.
Yes. That would be great if you have them. I'd appreciate it. 
No objection to the article itself. Just hoping he would participate in the community instead of just using it as a platform for self-promotion. &gt; Because reddit is a community, not a platform for self-promotion. and &gt; You should submit from a variety of sources (a general rule of thumb is that 10% or less of your posting and conversation should link to your own content), talk to people in the comments (and not just on your own links), and generally be a good member of the community.
This blog is great. Thanks for your contribution to Bechmarks.Net. This is the stuff that makes high-performance C# possible.
Comparing his posting history to yours, he's submitted more articles than you in the last six months even if you ignore the ones from his own website. Rather than bitching about what others are doing, concentrate on yourself.
&gt; You have basically locked yourself into using EF forever. I think even if you abstract your app from EF, you will use EF forever. &gt; There is no separation of concerns and likely no DI being used. People do not understand SOLID, it's a fact. You can inject DbContext with no issues. &gt; Unit testing isn't going to go well EF allow DI and unittesting without repository pattern. I have apps with no Interfaces at all but with DI and Unittests. 
I also have xps13 and it's terrible designed. - Fan intake on the bottom completely sealed if it sits on your lap. - Keyboard is not precise. Some hits are ignored. So you'll end up witing soethig lke tis. - Coilwhine IS real. - Touchscreen drivers conflict with Wacom drivers. - Touchscreen sometimes do not register a 1cm wide strip along the right side of the screen until you close the lid and open it again. It may be worth to wait until surface laptop hits the market.
Lots of anger but considering how the .Net Core has flip-flopped all over the place I guess it's understandable. I've kept a close eye on Core but haven't had the opportunity to make anything with it yet. I'm not sure if that's a blessing or a curse...
Unfortunately when it comes to technology you either adapt or die. The good news is that it sounds like you got a plan. 
Is it? I pulled the title from the title of the blog post. What would you rather I called it?
I supported the migration away from the old csproj file based projects and json was fine, everyone knows it. Dealing with huge projects in the old ecosystem sucked when dealing with big merges across teams. I even *like* the new csproj format. It makes sense even if the documentation on it sucks. However, the tooling is atrocious. If files get added in Visual Studio, for some reason they add Compile Include elements for those in the csproj file, which is completely unnecessary and msbuild complains about it. You have an externally referenced project in your csproj file, don't make a change to a file in that directory or VS takes a crap and the whole project structure collapses on itself. VS Code isn't any better. I'm actually excited to use VSCode but the support for csproj files and external libraries is awful. THis is a pretty common issue with .NET Projects and always has been: single solutions that share multiple projects. I'm not using NuGet for locally developing a library Microsoft, stop trying to make NuGet happen...it won't happen. There's a lot of potential there but right now its an absolute mess.
Almost every breaking change to the average user in the past year was related to project and configuration which were easily fixed by starting a new project and copying a few files over (if you can't be bothered to read the changes since the major releases all came with upgrade instructions).
I like the potential of VSCode and omnisharp but there's still a lot of outstanding issues. https://github.com/OmniSharp/omnisharp-vscode/issues/1156 
Log4net supports [configuration change](https://logging.apache.org/log4net/release/manual/configuration.html) by watching the (separate) config file. Not that any file change causes the ASP.NET application to restart so you'd have to place it in AppDate folder.
Heres scott hanselmans "explanation" in the thread: I can see why this is initially a scary WTF moment. Let me explain because it’s less freaky than it seems. You said .NET customers are going to need to interoperate. Totally agree. We can share netstandard libraries between ASP.NET Core 2.0 and EVERYWHERE. We can even reference many net461+ assemblies from ASP.NET Core 2.0 because of typeforwarding and netstandard20 You said WebApps may need to use: AD – Totally, this is a gap IF you want to call LDAP directly. You can certainly auth against Windows Auth NOW. We plan to have specifically the DirectoryServices namespace for Core 2.0 around summer timeframe Drawing – Totally, this is a gap. We plan to have this for Core 2.0 around summer timeframe. Until this, these netstandard options also exist ImageSharp, ImageResizer, Mono options, etc COM Automation – This has never been possible under Core 2.0, but you can certainly PInvoke if you want to hurt yourself. You could also local WebAPI to a net461+ process if you really want to hurt yourself. Sharing code with WFP Apps – YES. Totally possible with netstandard2.0. This is a weird change to make. Feels like it but think about it this way. WPF isn’t netstandard2.0, it knows it’s on net461+ and that’s OK. It’s optimized, but it can reference netstandard libs. ASP.NET Core 2.0 is optimize for Core 2.0 but it can reference shared libraries. Xamarin is the same. .NET Core is side by side and it’s moving fast. It’s WAY faster than .NET (Full) Framework can move which is good. By building ASP.NET Core 2.0 on top of .NET Core 2.0 (which, remember, is a SUPERSET of .NET Standard) that means stuff can be built faster than NetFx or even Netstandard. NetCore &gt; Net Standard &gt; NetFx when it comes to development speed and innovation. Point is, if you are doing new work, netstandard20. If you have older net461+ libraries, MOST of those can be referenced under ASP.NET Core 2.0. ASP.NET Core 1.1 which runs on .NET Framework will be fully supported for a year after we release 2.0. That workload is fully supported thru at least July of 2018. The remaining large gaps missing in .NET Core 2 are System.DirectoryServices and System.Drawing. We are working to have a Windows compat pack which would enable both of those on .NET Core on Windows this summer. What we need from you all is a clear list/understanding of WHY you think you need ASP.NET Core 2.0 to run on net461+. Be specific so we can close those gaps and let everyone be successful.
I haven't been following this too closely, but I don't see why they couldn't do things like how they initially rolled out `async`/`await`: as a NuGet package for .NET Framework 4.0 and then language-integrated in 4.5. Unless part of the issue is slightly different behavior in the .NET Framework / .NET Core components, which is gonna drive everyone *nuts* anyway.
I think you have stumbled into an area most others wouldn't have a serious issue. Your lack of understanding what the tool does makes you reliant on the tool and it being updated. If you had fundamental knowledge you would just do it manually without issue. Most of my posts are snotty but this one specifically is not meant to be. The lesson is tools are great but know your way around without them too. 
Along with your Go to Definition shortcut, I really like the Peek Definition with Alt + F12. This is a great list, I learned some new ones. Thanks!
You're welcome! Happy coding...
Looks like an IFTTT for the enterprise - kind of neat.
I've been burning unreasonable amounts of time dealing with all sorts of assembly mismatch issues. We have an ecosystem of apps - console, MVC 3, 4, 5, windows service, etc. - web apps running in IIS directly, Azure Web Apps, and Azure Cloud Services. Console apps on VM's and Azure Cloud Services. We of course have some shared libraries with lots of business logic and infrastructure functionality. Recently we stood up a couple new web apps using ASP.Net Core 1.1. Before long we saw value in adding things to our core shared libs - like common middleware and EF Core. This added dozens and dozens of System. packages to our shared libs. Shared libs weren't getting outputted in builds of non-.net core projects unless the executable projects also referenced the packages. Then a litany of assembly redirect errors. Every change now brings some works in this environment but not this one dll hell crap. What a nightmare. And now the upgrade path is closed to us as ASP.Net Core 2 will not be able to target the full Framework. Great.
Can someone eli5 this to me? I haven't looked at core since it first came out. Weren't dot net core libs already incompatible with full dot net? 
Fair enough that's correct. We are in the category that would need ASP.NET Core on the full framework due to legacy components, atleast for the near future.
How would you go about setting the UpdatedBy property in your SaveChanges method?
yep, that's what I meant when I said that 30% of my time always seems to be spent dealing with build / deploy/ assembly issues of some kind. If you think about it, that's an insane chunk of time just not coding. I go 'wtf' in my head way too many times when dealing with everything around c#, although I became pretty damn good debugging assembly issues on windows, though now that I had to deal with it on linux I just had very bad flashbacks and I just dont want to do this yet again.
The event aggregator pattern is exactly what you need. If you use the one you referenced from Microsoft, you will be trapped inside one process. If your application has only one process, then that is fine. I used this EventAggregator successfully already. But if your application spans multiple processes and even multiple technologies, you want to use a message queue like RabbitMQ, ZeroMQ, MsMQ, you name them.
And then people take me to task for not having jumped into Core yet, and still sticking to MVC 5. Sorry, but I don’t work at a big dev house where they can sacrifice untold man-hours dicking around to get things done… if I can’t be assured I can do things productively from square one, any greenfield project simply won’t get approved as a Core project; it will have to be spun up as an MVC 5 project no matter how nice Core might be. I’ll remain in a holding pattern until Core 2+ has been confirmed largely wrinkle-free, thanks. That, and the migration of most/all common packages. There’s far too many where I haven’t even seen *any* movement toward Core, much less Alpha status.
Thanks that's great to know I think one process should be fine , at least right now. I think even implementing my own version of the aggregator should be easy enough for the scenarios i need... famous last words but yeah we have weird policies on libraries. 
Yeah i will hopefully just use the microsoft one. 
Say that you have a .NET Framework 4.6.1 library, .NET Framework 4.6.1 targets .NET Standard 2.0. As long as this library uses the API surface defined by the .NET Standard 2.0 all is good, but if said library uses any other APIs not covered by the .NET Standard 2.0, your .NET Core 2.0 program (a web app, a console application, you name it) will fail. This happens because the .NET Core 2.0 runtime implements only the APIs defined in .NET Standard 2.0 (and below of course). This is always been a problem and that's why PlatformNotSupportedException exists. A pratical example of something that is not compatible between Core and "Full" is System.Runtime.Remoting, if a .NET Core App tries to reference and use a library that uses something in that namespace it will fail *at runtime*
I have not tried it myself, but when I was messing with certificates, [this stackoverflow question](http://stackoverflow.com/questions/35582396/how-to-use-a-client-certificate-to-authenticate-and-authorize-in-a-web-api) had what you are looking for I think. private static X509Certificate2 GetCert2(string hostname) { X509Store myX509Store = new X509Store(StoreName.My, StoreLocation.LocalMachine); myX509Store.Open(OpenFlags.ReadWrite); X509Certificate2 myCertificate = myX509Store.Certificates.OfType&lt;X509Certificate2&gt;().FirstOrDefault(cert =&gt; cert.GetNameInfo(X509NameType.SimpleName, false) == hostname); return myCertificate; }
Go away.
oh that's good! thanks!
I don't really have a dog in this fight as I'm just now looking into wtf dotnet core even is, but I will say that I like this note by Damien Edwards: "The potential for confusion goes both ways. E.g. we've had lots of feedback that the fact that "ASP.NET Core" can run on .NET Framework (which isn't .NET Core) is confusing. This change effectively makes ASP.NET Core part of the .NET Core stack meaning if you're using ASP.NET Core, you're using .NET Core."
Even then it's still *maybe*. They're filling in a lot of the issues in the upcoming months but some of the abstractions do nothing....
Depends on your configuration but more or less yes. The idea that dotnet is trying to move away from is future libraries will target the netstandard which makes it cross-platform compatible and should more-or-less work with any other netstandard on any platform. The .NET people *used* to say that if you wanted to target the .NET 4.6 frameworks then you should be targeting net461 but that will no longer be possible if you want to use .NET Core 2.0. The things you referenced (Configurations and Reflections) have changed but you can still do things you want to do. Remember, this is specific to ASP .NET Core, not .NET Core 2.0.
&gt; dotnet add package Package &gt; dotnet add reference MyProject
Shit like this isn't going to cut it in an enterprise environment. 
Perhaps create an issue on the MVC repo on Github?
Such a bad decision. Eventually .Net Core will supplant the current .Net stack but come on. This is nowhere near the time to pull that trigger.
If you are in an enterprise environment and are deploying with preview releases... that is what needs to stop in the enterprise environment. Not sure about the logic here. 
I would look into https://github.com/rebus-org/Rebus or MassTransit. However, I cannot recommend MassTransit as I found it to be both poorly documented and painful to configure.
It's true that if the check was performed at a low level then yes the overhead would be noticeable. However, if the check is performed in the constructor of the HttpStatusCodeResult class the overhead wouldn't be that much given that the status-description data is typically a pretty short string. Do I miss something?
Does anyone know of a code example that implements EF properly? Eg using db context as the unit of work and not wrapping it. Returning iquerable instead of ienumerable
This has been dropped. ASP.Net Core 2.0 packages will now support the full framework. 
They've back pedaled ASP.Net should continue to support .Net Framework. You were looking to swap platforms as well? Makes some sense to move away from .Net then I guess.
Why?
They are MVC, it's just that the controller action is declared inline in the view file.
What fractured English is this?
I'm Ron Burgundy?
Must be.
https://github.com/aspnet/Home/issues/2022
The problem as I see is that even if you stay on the classic runtime, this stuff is causing alot of noise regardless.
It's shit hit the fan type of issue. No more like cow hit the fan. But it was reversed from Microsoft. Seems that they do listen. 
I just cannot get it, why MS need .net core? .net was great before they start trying to conquer linux.
If you know ASP.NET MVC your, (Almost) already know ASP.NET Core since the concepts are, more or less, the same. There is some pluming that is different. You can properly skim their site [asp.net](https://docs.microsoft.com/en-us/aspnet/core/) to get a quick overview.
They are different and I would expect ASP.Net to continue being supported for the foreseeable future, but new development diminish. The major differences (setting aside the massive runtime differences and focusing on the development of web apps) is that ASP.Net Core builds on the ideas introduced by OWIN and even adopts the Startup.cs convention. They also have embraced Dependency Injection and it is available out-of-the-box in core. The question is whether or not you want the cool new features and if you want them bad enough to ride out the continually evolving APIs while core is being developed. Although, I'd say it is much more stable now and there is a lower risk of _everything_ breaking :)
I am just learning React because that's where the jobs are. It is the worst thing since... it's pretty much the worst thing. So. Much. Scaffolding.
I'm not an advanced web developer by any means, so somebody may point out that this is a silly question, but why not go directly to .NET Core and skip ASP.NET entirely?
The best tip I can give is to forget just about everything in Web Forms and start with a clean slate. When I transitioned, the removal of the page lifecycle stuff made the biggest impact on me. The MVC request pipeline and razor view compilation are completely different and much easier to wrap my head around. The tutorials are a great place to start, but I also like to just read a book front to back. I use the Kindle app on my tablet to read a lot of programming books on Amazon and it's great for me.
Most of the job opportunities in my area still require traditional .NET. The vast majority of the business world has not migrated to .NET Core
This poster has been advertising for his company all over technology subreddits with little or no content. I learned to just give the down vote and move along.
Let me guess, you're using redux? If you want to keep your sanity, use TypeScript/TSX on top of React, and use mobx for tracking state. That's about the best you're going to do in the front-end world.
My trust is gone
I made the same transition two years ago, using Microsoft Virtual Academy as a source of instruction, as it is free and a good starting point. I'm glad I made the change, but a point of frustration for me is that once you get past the hello world stage, you have to unlearn some things you were taught at the beginning, often hearing, "You're not supposed to do it this way, but we'll show it to you this way for our convenience..." A specific example is using ViewBags in the demo instead of View Models. You'll have your own frustration points, but keep on banging your head against the keyboard and you'll find a way through them.
These people are fucking unbelievable
Ironically? I joined a group to learn Ruby on Rails to expand my skillset and that helped "unlock" my understanding of how MVC works. When I jumped into things there was a Pluralsight series on MVC 4 by Scott Allen that Microsoft made free to anyone who wanted to watch them. Apparently that's no longer the case, but I can vouch for the series being a really good starting point to learning ASP.NET MVC. Honestly, once you learn MVC you'll never want to go back to WebForms if you can help it. It's a very different way of thinking and building apps, but once you understand its conventions and how the framework implements everything, it's just soooooo much better than WebForms. Sadly where I'm at I was placed on a team that supports/enhances an older WebForms app because for some reason the business hasn't settled on new requirements for the Web API project I was working on.
After playing around with MVC, it did not take long before the entire company made the transition. We went through the book called Pro ASP.NET MVC 5. It is a good book and is a good foundation for your transition. Having said that, think about the business needs. We started out doing it where the transition makes sense. For new projects or long overdue rewrites we started from scratch. For older WebForms code bases we decided to take the middle ground approach which is as follows. In case you have a lot of software that needs to be maintained and are written in WebForms (and too expensive to rewrite), it might be useful to have MVC running in the same project as your WebForms. This means that you can stop developing your WebForms aspect and continue to add features to the MVC portion. This is actually pretty cool, since you'll slowly be converting your solution to MVC as time progresses and you'll be learning as you go. No one was able to just read the book and instantly write the best software but it does feel a lot more natural. It just takes practice. Again, it depends on what your goal is. Good luck!
.NET Core seems to buggy for me. I've tried off and on for the last few months and gave up. When .NET Core works out its bugs and becomes more reliable, then I'll switch over.
At least they listened to us.
Reading the article, seems like it was a case of two lines of thinking within the team and the hysteria that erupted helped them decide the direction. Doesn't seem like a bad thing to me because it means you know they acknowledge they don't necessarily have all the answers and look to the community for guidance. If anything I'm relieved that's the case.
DotNet Core is great as long as the following can be satisfied: * you keep the project “simple”. * you do not need any package that has not yet been ported over (and there are many that haven’t). * you are not doing anything particularly esoteric (ties in with the first, but only peripherally). * you can survive with minimal to no documentation or guidance, and can deal well with the unknown. * you don’t mind having to deal with a target that is still very much in motion. DotNet Core is appropriate if you: * need something that is truly platform-independent, and can run on Linux, MacOS and Windows equally well. * need something that will be in use for the next 5-10 years or more (many/most public websites get refreshed every 2-5, but corporate tools can be in use for 20+ years). * will be building something that will be difficult to migrate from MVC 5 to Core. * need some shiny that only Core can provide. IMHO I will be moving to Core only with v2, as it will be much more stable and less likely to have breaking changes. Plus, documentation and online help. YMMV, good luck.
This guy fucks
If they're going to break compatibility JUST DO IT ALREADY. This is like dating a crazy person who is ready to get married one second and lighting your car on fire the next.
&gt; If they're going to break compatibility JUST DO IT ALREADY. This is like dating a crazy person who is ready to get married one second and lighting your car on fire the next. But the crazy ones are always so much fun when it really counts.
Awesome. Thanks for the details. What about things like Laravel built in authentication? Does ASP.NET have anything similar?
Yes, it does, but I felt like it was a bit clunky in certain things. For one, it uses GUIDs for UserId and RoleIds, so instead of integers, you'll have a userid like 055be6c0-27a6-4aa2-9dda-q85c7d97e6d3 and when you're trying to do lookups or quickly check your database, you can't quickly spot check it. In Laravel you'll say, oh User 1 or User 2 since it uses integers for their Ids. There were other weird things where certain versions of MVC only let you look up a user by username and to get their ID you actually had to go back to the database. In Laravel, the User object was really easy to access (Auth::user()) and contained all the stuff you defined in your User model...
That's what I am afraid of. I learned programming through PHP and Laravel. I don't want to make things harder on myself when I know I can easily do something in Laravel. 
It all depends on what you are comfortable with. I just found myself able to develop much quicker in Laravel/PHP than in ASP.NET MVC -- but that's just me. Also .NET Core is still pretty new and many users feel it's just not ready for a production level product. It's also a bit harder to find help from others since there aren't as many solutions posted on StackOverflow and what not -- but then again, it's all up to you. 
If that's the case, then someone should have told Hanselman because [his reply early in the GitHub thread](https://github.com/aspnet/Home/issues/2022#issuecomment-299536123) sure made it sound like it was not only intentional, but planned (and horribly communicated or discussed with the community). I'm glad they corrected this. I'm one of the lucky startup hippies (saw that phrase in one of the threads discussing this) who's fortunate enough not to have a lot of enterprise baggage, but further fragmenting the landscape like they were talking about is a bad move that would have slowed down the adoption of dotnet core/standard rather than sped it up. 
Or use RiotJS like I do. So much simpler :-)
MS didn't think their naming scheme was confusing enough so they slapped Core on the end instead of a 6.
To your point of rapid development cycle: ASP.NET Core supports watch feature - any newly saved changes will be reflected without needing to manually recompile the code (although I don't know how mature it is now). Here's the details: https://docs.microsoft.com/en-us/aspnet/core/tutorials/dotnet-watch
Might want to see if you can use Peachpie to compile and run the PHP on ASP.NET Core and then you may be able to make a more gradual transition that interops between the PHP and ASP.NET code as you transition? http://www.peachpie.io/
...isn't Portlet also a Java technology or am I confusing that with Facelets?
.NET Core is still missing a bunch of features. ASP.NET Core is as well (such as support for SignalR).
&gt; There is LINQ to SQL This is a minor nitpick, but Entity Framework uses LINQ to Entities rather than LINQ to SQL. In case someone wants to Google it later.
No idea. I'm pretty new to server side rendering. Portlet is just some lingo afaik, same as a page/view? Web portal is made up of portlets. I was just throwing out the last thing that made me scratch my head really. I'm just completely new to all of it, and picking up ASP.NET on an enterprise project. 
I know enough about krishan in training. It's pure garbage. I hope no one wastes their money on that shit.
Well to be fair I did try it in its early stage (beta or RC, couldn't remember exactly) but it was not super impressive back then. Plus now I mainly do SPA so small little changes on server code all the time is no longer the case. I put it there just for the awareness, but have no claim whether or not it's useful.
You can override the auth behavior of ASP.NET, so its up to you to use integers as id or use your own database schema, but of course that's probably not the right thing to advice someone that's just has begun to learn the languarge/framework.
That is what .Net Core &amp; .Net Standard is... Is this a joke? Can't tell if serious or stupid.
Links to a tumblr site which links to a blog site with broken code examples.
&gt;&gt; If you are doing data access, that should be in its own layer. &gt; &gt;There is a difference, I do not layer app by "layers", I split app by functionality instead. &gt; &gt;&gt; At the very least, each layer should be separated by interfaces. &gt; &gt;It's sounds like cargo cult for me. Don't make me wrong, what you describe it's fine, but from my experience it fits only for small projects. I introduce Interfaces only if I have more then one implementation of this Interface (but sometimes I use abstract classes, it's always depends). &gt; &gt;&gt; Also, the use of virtual solely to make the mocking framework work is evil. &gt; &gt;It depends... as usual. We have different experience and different opinions, it's fine. &gt; &gt; &gt; you cannot test the logic within that virtual method. No big deal if it's a one-liner wrapper kind of call, but any sort of if/else logic in there became relegated to integration testing. Enjoy spinning up that database for a simple "if this" test. &gt; &gt;In this case to not spin DB I can mock only DbContext (if we speak about EntityFramework). &gt; &gt;&gt; Lastly, if everything is virtual just to make mocking possible, the code is ultimately more complex than having the set of matching interfaces. &gt; &gt;Actually I don't know why it's more complex? Less code is always better in my opinion. Yeah, a CQRS approach doesn't have the layers I am talking about, it's a different organizational paradigm of the code. I rarely see it out in the wild though, 3-tier is still the norm in projects I've been involved with. On the contrary regarding interfaces, they are crucial for large projects. When you have separate teams working on functionality in their own code base that needs to integrate with code from other teams, the only sane way to orchestrate that is via an interface of some kind. I shouldn't care that you are using a MS SQL database with Entity Framework to do your data access to store a "widget", I just need to know you have something that returns me that widget so I can process it in my part of the system. Again, if you are doing CQRS, or microservices, that's a different paradigm altogether. However, those still need interfaces of some kind to provide a contract to consuming code. I stand by my argument against virtual properties just for the sake of testing. You are introducing complexity for the sake of testing. In OOP, it is okay to have portions of a class hidden from public view, especially if there is state being maintained in that object. Most of the tests I see being written at that level are needlessly complicated too, and usually have names like "SettingProperty1WhileNotSettingProperty2AndThenCallingThisMethodMakesProperty3ReturnFalse". It's a misguided attempt to test behavior, when really there is no guarantee that every consumer of your class is going to set those properties and call that method *exactly* like your test expects it to. The more you limit your public surface of a stateful object, the better, especially if you follow the open-closed principle. Tests aren't meant to enforce procedural concerns, that's the responsibility of your exposed components of the class. I see a lot of half baked "functional C#" being written today, with people really twisting the language in ways it wasn't meant to, particularly when it comes to state and side-effects.
Asp.Net Core does not have feature parity with Asp.Net 4. It's missing SignalR, EF, etc. Otherwise they're broadly similar. The biggest change is in startup.cs. If you know Asp.Net 4 then you pretty much know Asp.Net Core.
I think you are asking to many questions at once. If i understand correctly this i swhat you need to know: #1 how to handle clickevents #2 how to open dialogs #3 how to get dialog results #4 how to accociate a object with a UI item #4.5 how to store state within a specific UI item #5 how to handle collections of ui items Find answers to each and everyone of those and you will get what you need. Hope that helped, cause there is no one, or no easy way to answer your question.
No, I already have everything working except for 4 and 4.5. Number 4 is exactly what I was asking for. 
The simplest method for associating additional data to a control is to use the Tag property. Create a class to store whatever metadata you want to associate to the control and then set an instance of that class to the Tag property.
The most immediate application is helper functions. If you have a function which is only ever called by one other, and should NEVER be called outside of it, local functions will help enforce that.
Ildasm!!!! That's the name of the program I've been trying to remember for years. Thank you. 
I didn't realize how entertaining a github thread could be until a couple of days ago when I started reading that thread and the next thing I know it is 4:45 AM and im kicking myself, and had to wake up in two hours w hen already running on 2 hours of sleep from the night before.
Thank you so much. This was exactly what I was looking for, not sure how google did not point me in the right direction.
You are confusing dependency injection with dependency inversion. I just commented the way I did because dependency inversion (and the interface segregation principle) in Solid specifically refer to interfaces which you say you don't use. 
Why so harsh ? I would give it at least 6/10 
Hey, thanks - yes, I kicked Serilog off :-) That sounds like it matches my experience- Rabbit seems to do more of the heavy lifting that I think falls back upon client libraries in the case of Azure/Windows Service Bus. I must check out Rebus sometime, too. Thanks for the reminder!
Please note that using C# events is also using the observer pattern. You might want to note that.
Noted. Will change after exams
Does it still suck compared to EF 6?
It seems so. Still no m:n, still no way of running migrations from a library and still the god-awful tooling that's not compatible with VSTS release step. On top, the tooling only works with VS2017. I guess EF Core is repeating the same cycle as the original one: EF 1,2 and 4 was just crap but got better from there. So it's only 3 years until it's usable again.
Anyone read any of these yet? 
EF will probably always suck.
Waiting for final.net
True - I just had a thorough review of your code and you've laid down a wicked approach here which I will definitely be borrowing/stealing from in my next EF implementation. One issue I've recently seen in the wild is the abuse of eager loading via **Include()** and **ThenInclude()** where a single repository method **GetCustomerById()** loads the entire graph for the purpose of displaying just the name. Seeing stuff like that is what really brings your message home for me and how repositories can very quickly lead you into a design and performance rat hole. I just wish more people could get this message before they cut their next **CustomerRepository()**
My beef with EF is that it does too much, and hence is hard to implement. With a simpler design, we would have had a full featured EF Core for a long time now. Something like jOOQ would have been much easier to throw together Developers often know what SQL they want to end with anyway.
Reading this thread, I think some people really have the wrong end of the stick. Let me explain. Right now we have 3 ways to build a web app. * ASP.net Core * ASP.net Framework * ASP.net Core running on .net Framework That last one sounds stupid and is stupid. That's what this update was going to get rid of and everyone is going crazy about it. So why would people use it? It's so you can utilize the best of both worlds, the speed of .net core's web framework but running in the full framework CLR so you can use full framework libraries. But here's the thing. The only reason .net core is fast and is being developed at an extreme pace is because it has no baggage. They can basically innovate at ridiculous speeds because they don't have anything "legacy" to worry about. So they get to a point where they go "OK, I'm sorry but for us to keep innovating, we need to drop support for running this thing on .net framework". They are NOT dropping support for .net standard or throwing away the standard. What they are saying is that for ASP.net core to keep improving at the speeds they want, they need to basically make it for .net core only as they can't rely on the .net framework to keep pace with what they need. 
Ok then, as a matter of fact ASP.NET Core wins in terms of performance. 1) Good point. 2) VS has support for Python. Been using PyCharm though. 3) There's another thing. There're way more opportunities in my country for .NET rather than Python, but mostly in corporations or big companies. As a young dev, I'd like to work at startup-like companies, but those kind of companies normally don't use .NET. 
Honestly, I think all of the kerfuffle (well, ok, not all, but a LOT) would have been avoided if the team had been more forthcoming about it. Having the community learn about a change like this from a commit message is a pretty bad way to have handled it.
That I do agree with. And I also agree that now you have these people that are somewhat marooned on this weird combo of ASP.net core and Framework (Which totally sucks). But! I still think moving the ASP.net core packages to .net core only is the right thing to do. And I guess reading comments here and on HN about how that was a huge mistake... I couldn't understand it. 
One of the principle motivations for the .NET Core project was its decoupling from Microsoft / Windows dependencies so they were able to innovate at a greater pace. The complaint was other frameworks aren’t tied down with enterprise requirements. And now .NET Core is … tied down with enterprise requirements. What a shower.
Am I the only one who has had a miserable experience with .NET Core thus far? I've given it a go on several occasions and each time it's just one thing after another. Maybe my problem is trying to use it in Windows. Perhaps in Linux everything is smooth?
The next Xamarin step is a smarth move from Microsoft. Thanks for that tip. 
ASP.NET Core on .NET Framework is the only way to ship a web app that uses a working version of EF. EF Core is currently a glorified prototype due to the lack of some key features like lazy loading, many-to-many implicit entities and more. If your app can run with EF Core, then just go netcoreapp1.0... Otherwise you miss a lot because ASP.NET Core is much nicer than ASP.NET MVC 5.
that's kind of the idea, but obviously FF isn't xplat, so the actual underlying _thing_ is netstandard. netstandard is the cut down thing whereas core represents possibly multiple implementations of that standard. they have decided that netstandard can only move at the pace of the lowest common denominator (FF). because of that it slows the AspNetCore team down because they have to wait for FF to catch up and implement netstandard. So, they thought it would be better to just say f-it and depend on the core runtime specifically so they can move faster. which I would be totally cool with, but the indecisiveness prevents 3rd package authors from upgrading (so I expect very few people to actually support 2.0.0-preview1-final). If I have a package that depends on Microsoft.AspNetCore.* and netstandard, I'm not going to upgrade because I know these packages are all going to be changing back from netcoreapp to netstandard again. /endrant
Enterprise is the bread and butter of .NET framework. It will be stupid to abandon them.
Because continuity is important. There are millions of millions of code written in .NET. We would want to bring everybody along moving to the new platform with the least pain possible. 
Core is a right pain in the ass. Nothing seems to "just work".
Netstandard can move faster than .NET Framework. That would actually be a good thing, because it can drag .NET Framework along behind and prevent it from becoming abandon-ware.
[removed]
I don't know it maybe my misunderstanding of the entire thing but it seems like there are a couple underlying problems if they do divorce from the full framework completely. The first is obvious -- communication could you imagine the even bigger shitshow this would have been had someone not noticed this commit? Heck, most developers this would have/or so affect I'm willing to bet dont even know about this..even now.. Second, this would actually leave a lot of Microsoft's customers who have relied on the Microsoft without a true upgrade path. The truth of the matter is, there is a huge percentage of customers that rely on legacy libraries and code that because of this change would have no way to upgrade. It is not as if there is an ASP.NET 5 and ASP.NET Core is a separate project for those who want portability between say Windows and Linux..Asp.net Core is it.. And those who *already* upgraded their code to Core but had to still run on the full framework..not only have no true upgrade path but spent a significant amount of money and time rewriting their code. That is the type of thing that would make companies even on an enterprise level leave everything they can that is even relatively related to Microsoft. Backwards compatibility is absolutely essential in the business world..it is s big reason why a lot choose Microsoft..
Well, other than the cross platform ability (the entire point of .net core). 
Actually not exactly. When we start speak about Interfaces it's two meaning. 1. A contract of class or module 2. Implementation in specific language like C# or Java. And in this kind of languages we have a limitation (this difference between classes and interfaces, we need "implement" interfaces and "derive" from classes, and also we need mark our classes so compiler know what we implement or derive). For instance Go do not has such limitation. And when you pick a dynamic language like JS it's also bit different. So when I tell "DI" I mean dependency inversion. And I agree with you about C# and Java. And we need this interfaces because of limitations of thous languages (basically we have two options: interface or abstract class).
There are a million cross platform languages. Adding cross platform to .NET is worthwhile, but it's not a worthwhile thing on its own.
Machine learning in the cloud for virtual reality. 
Perl
For the many to many, can't you just create join entities?
If starting in the c# world, a lot of companies are looking for asp.net mvc. Along the same thread web api is a big one. Those can lead you into web development pretty easily as a lot of places now just use the Microsoft stack on the server side, and JS on the client side like react or Angular. In the desktop world, you can have a lot of fun by learning their universal app engine, and ever then expanding to xamarin. If you want too try everything, do what I did. In one solution, create a webapi project as your server, then a universal app, xamarin android and iOS apps to connect to it. A blank web project is all you need for angular or react. There you have a simulated system of multiple clients connecting to one so I. Granted it is a bit overwhelming. C# has a lot of cool stuff! Start with webapi and go from there!
I've been working with Core for the past 4 months. I'm switching back to MVC 5. Too much fuckery with their additions and changes. At the rate Core is going, MVC 5 will be supported til 2030!
[&gt; Not at all! The only thing the app is tied to is OpenID Connect \(and a limited amount of OIDC at that\). There is nothing in the template that is tied to B2C or any other Azure specific service. This means that you can easily switch to any OpenID Connect provider \(like IdentityServer4 or OpenIdIDict\). This setup allows for more choice, not less.](https://github.com/aspnet/Identity/issues/1187#issuecomment-301122981)
&gt; I thought core was a cut down multiplatform version of the full framework It *was*, that's why you could run asp.net core on the full framework, because the 1.0 release was basically a cut down version *of* the framework. They tagged everything in core as netstandard, which the full framework already had (Since core was based on it). *Now*, core has effectively caught up with the full framework - it's no longer *cut down*, it's more or less on par bar a few windows-specific things. So what happens now? The problem is that the full framework is slow to migrate, usually only doing it with major OS releases - so the choice was slower releases of .net core stuff (And thus netstandard) or leapfrog the full framework and let it catch up later. Had Microsoft stuck to their original plan, it wasn't going to be a case of "asp.net core doesn't run on the full framework", it was going to be "the full framework needs *updating* to netstandard 2.x to run asp.net core" - which *would* have happened eventually. Now we seem to be stuck with a slow moving .net core, it seems.
Yeah, but it's much easier to throw a tantrum...
https://github.com/aspnet/Identity/issues/1187#issuecomment-300623770
What's your point? Should I link to the follow up comments, or can you just read them yourself?
Do I need to link to the follow up comments or can you read them yourself?
I see this is going nowhere fast, have fun repeating my comments.
Have fun not being able to read the github thread to understand the issue.
...there's still nothing forcing you to use Azure here.
They simply didn't want to be bothered with backwards compatibility. They want everyone who uses the current stack to stay on the old stuff "forever" (which means until MS decides to stop officially supporting it which was going to be 1 more year). They don't want to do anymore feature work on the existing platforms. It's really easy to innovate when you're greenfield everything, but so far the adoption of .NET Core has been terribly low and not growing against the overwhelming force of NodeJS and React.
ASP.NET in general.
i would recommend the csharp digest and stackoverflow
Yeah, I'm learning vue.js for that reason. 
Is there a way to digest SO other than seeking answers to questions? Seriously, I haven't tried.
This is a bit of an odd comparison. Django is a product built on python that has many specific things that come along with it, like an admin, prebuilt tables, ORM, etc. Core is a platform, where you can built these things. A more fair comparison would be Django to Umbraco, or something like that. Node/Express is a lot more like asp.net core. That being said, nothing wrong with Django, like it a lot, but Node/Express is exactly the web platform I wanted that isn't asp.net. Still prefer .Net, but there are some niceties in Express that are pretty awesome.
[removed]
I mainly just read stuff by Troy Hunt. He's one of the few people I've read about who are using .NET in a production environment with a large number of users. As such, he usually has something interesting to say, like in this article: https://www.troyhunt.com/working-with-154-million-records-on/
[removed]
Thank you! I'm currently studying asp.net mvc in udemy. Do you recomend any other sources to learn web api, angular, etc?
I think the author does not really know what he's talking about.
Given that he's called it EF7 I'd say that's guaranteed
&gt; .NET Core is side by side and it’s moving fast. It’s WAY faster than .NET (Full) Framework can move which is good. By building ASP.NET Core 2.0 on top of .NET Core 2.0 (which, remember, is a SUPERSET of .NET Standard) that means stuff can be built faster than NetFx or even Netstandard. Jesus christ could they have chosen worse names for all this?
Yes. The previous name for .NET Core was .NET 5.0.
I know it will be eventually be ok, but most of the time, I think its decaf. http://i.imgur.com/FczhsKf.jpg
It was all versioned 1 higher than the current latest full .net. ASP.net core was asp.net 5, MVC core was MVC6, EF core was EF7, etc.
Feel however you want about it, what you're laying out is so far from good practice that it's not remotely reasonable to try and explain all the shortcomings of your current path. &gt; even posted part of my code on SE CodeReview a couple of months ago and no one mentioned anything too bad about it Go do you then. I don't care if you write good code or not or if you fail or succeed. Take advice from someone who's best suggestion is to change what you name a class and use more extension methods (half of what they say is flat out wrong/bad). Your code there displays another half dozen or more ways of doing things I would never recommend. Some highlights: Hidden dependencies: public AssetService AssetService { get; set; } = new AssetService(); Over-selection: select new { asset, customer, customerlocation, product, leftjoinedtaglocation }) Calling ToList twice in a row on the same list: var assets = await (from asset in db.vwAssets join customer in db.tblCustomers on asset.CustomerId equals customer.CustomerId join customerlocation in db.tblCustomerLocations on asset.CustomerLocationId equals customerlocation.CustomerLocationId join product in db.tblProducts on asset.ProductId equals product.ProductId join taglocation in db.tblTagLocations on asset.TagLocationId equals taglocation.TagLocationId into groupjoinTagLocation from leftjoinedtaglocation in groupjoinTagLocation.DefaultIfEmpty() where asset.AssetId == assetId select new { asset, customer, customerlocation, product, leftjoinedtaglocation }).ToListAsync(); return assets.ToList().Select(a =&gt; new Asset(a.asset, a.customer, a.customerlocation, a.product, null, a.leftjoinedtaglocation)).FirstOrDefault(); Unneeded async. That Task.Run, sheesh. (experienced, huh? not convincing me much - I hope for their sake no one's paying you to write code like this) public async Task&lt;List&lt;RepairFrequencyLength&gt;&gt; GetRepairFrequencyLengthList() { return await Task.Run(() =&gt; new List&lt;RepairFrequencyLength&gt;() { new RepairFrequencyLength(){ DisplayMember = "Day(s)", Value = "D" }, new RepairFrequencyLength(){ DisplayMember = "Months(s)", Value = "M" }, new RepairFrequencyLength(){ DisplayMember = "Year(s)", Value = "Y" }, }); } You really don't need an Asset to initialize an Asset? public Asset(vwAsset asset = null, tblCustomer customer = null, tblCustomerLocation customerLocation = null, tblProduct product = null, tblBrand brand = null, tblTagLocation tagLocation = null) { InitializeAsset(asset); InitializeCustomer(customer); InitializeCustomerLocation(customerLocation); InitializeProduct(product, brand); } private void InitializeAsset(vwAsset asset) { if (asset != null) { AssetId = asset.AssetId; Title = asset.Title; FullModelNumber = asset.FullModelNumber; RepairFrequencyValue = asset.RepairFrequencyValue; RepairFrequencyLength = asset.RepairFrequencyLength; CreatedDate = asset.CreatedDate; } } And that's mostly minor, sloppy coding that doesn't even come close to touching the architectural issues you're introducing. You've got a lot of fundamentals to learn before understanding how to resolve tension between competing concepts like do-not-repeat-yourself and single-responsibility-principle.
And a programming digest. I think newsletters are a great way to get your news and there is just not that many (if any) in C# community. And it's not a big secret (my profile bio tells you that I run it). I apologise if that gave you false impression and will be more explicit in the future – thanks for the feedback.
Glad to help! They are great for house chores :)
In this case, it's not that bad. * There has always been the .NET Framework (i.e. NetFx, DotNetFx, etc.). * A rewrite of most critical parts intended for OS portability became .NET Core (e.g. the core components of the .NET library) * As it became immediately clear that companies needed to bridge the gap in their library support, the .NET **Standard** facade was developed to "synchronize" compatibility between the two disparate environments. .NET Standard represents the absolute boundary of compatibility between .NET Framework and the new portable .NET Core library (and UWP, etc.) In many cases, .NET Standard references point back to the native .NET Framework implementations or the new .NET Core only versions. If I understand the problem here, and I haven't paid too much attention to it, the ASP.NET Core team wanted to drop support for .NET Standard because it meant they were tied to slower release cycles and more baggage. By dropping compatibility for all but the .NET Core environment, they would have been able to iterate much faster and bring more to the table at a significant loss to the majority of developers still working in the .NET Framework era.
I am one of those enterprise customers. It isn't that we aren't rooting for a clean break and separate evolution. I'd be happy to see ANC 2 target netcoreapp2 today if there was a commitment that it would target a sufficiently enhanced netstandard2x as part of a maintenance cycle before the previous version was EOLed. Let's face it, almost none of us (enterprise customers) are gong to hit the first 6 months of ANC 2 (probably not even the first year). We are big, we are slow and we have a lot of code to move. ANC is clearly the future of Asp.Net and it is wrong to even pretend it isn't. Changes like utf8 strings or Span substrings will be huge on some of our codebases (the applications I work on would use roughly half the memory we currently do if we switched to utf8, which means we can handle twice the client base on our current hardware because memory is our top restriction). I need: * sufficient reason to advise the company that a move to a new version is worthwhile; which must be contrasted against dev downtime while making the change * assurances that wherever we do go, we can contact someone for official support (if there was a problem at some point, we could throw money at it and make it go away) * knowledge that we aren't going to be moving to something which will leave us stranded at a point where a major rewrite is necessary (think asp =&gt; asp.net) or I have to fogbugz the problem away with a custom compiler I want: * ANC to grow without all the baggage in the full framework version (ThreadAbortEx all over the place, page lifecycle, webforms, postbacks, etc.) * ANC to be able to make breaking changes as long as there is forward warning and a migration path * ANC to be able to release features faster than I can reasonably switch to the new release (All this talk about being able to run multiple versions side by side doesn't matter if I would never have to do it because everything can be upgraded for each release due to them being multiple years apart)
Perhaps.. I think I just need more sample images, there is not much control I have over the learning process. The problem with [Custom Vision](https://customvision.ai/) - it is still in preview, and only allows about 1000 learning images, and bunch of other limits for prediction and learning. Hopefully they remove these restrictions after the preview. I would love to have an app that can recognize any bill and convert it's value into any currency.
Based on the names "Core" and "Standard," your description of Core vs. Standard sounds correct. However, the article seems to say the opposite: &gt; By building ASP.NET Core 2.0 on top of **.NET Core 2.0 (which, remember, is a SUPERSET of .NET Standard**) 
It seems very popular, and therefore easy to get up-votes, to just whine about how 'confusing' the naming is. But everyone who's doing the whining seems to not even have bothered learning what the different platforms are. It's actually incredibly simple. * We have .Net Framework, which is the classic full fat .Net. Along with this there are a bunch of classic tools we're used to seeing: ASP.Net, Entity Framework 6. * We have .Net Core, which is a slimmed down &amp; cross-platform .Net. Along with this we have rebuilt and more modern evolutions of popular tools: ASP.Net Core, Entity Framework Core. Note, anything named 'Core' carries this operability with .Net Core * To define what the common APIs are between the two major .Net versions, we have "The .Net Standard" which defines all the common APIs, and which the new 'Core' moniker tools are (wisely) targeted to, so (basically for free) they can be used on the classic .Net Framework. tl;dr `IDotnetStandard` `DotnetFramework : IDotnetStandard, IDotnetFrameworkLibraries` `DotnetCore : IDotnetStandard, IDotnetCoreLibraries`
This a great perspective, thanks for sharing it.
Thanks for sharing, this is handy.
This is great! I can drop my dependency on Auth0 now
I find the monthly [MSDN magazine](https://msdn.microsoft.com/en-us/magazine/ee310108.aspx#S2) helps in staying up to date as well.
This is not really ClickOnce. ClickOnce also does the initial installation. This also does not verify the integrity of the package, which makes it highly unsuitable for an update mechanism. 
I can't upvote you enough. Having written a deployment app like this before, the amount of incomplete or corrupted zip downloads was amazing. You need to serve the SHA1 or MD5 hash of the zip on the server and compute it after you download to ensure you got the complete and correct file. 
This does the initial "installation". It's portable and just downloads the EXE and supporting files to a subdirectory. Not much more to an installation than that right? There are no required prerequisites other than .NET 4.5.1 for my software. Can you explain this if I'm wrong? I'm a scrub a deployment. I didn't think about what would happen with corrupt downloads and it has not come up yet. Thanks! I will write a method for verifying the zip package was downloaded fully.
Thanks for the tip on computing the hash first. I appreciate it!
True, how about now? I added the disclaimer on top of the newsletter section.
&gt; The objects are either not inserted or are not visible. You should be checking that first. Debug and keep an eye on the grid's children. From there you'll have a better idea of what you're dealing with.
It is always great to expand your mind and learn/try something new. That said what you have is essentially how most apps were doing updates before clickonce etc. configuration complexity is not a valid reason for reinventing something. Not to mention the time it took you to conceptualize and write this ended up not saving time in the end. You shared though and I always appreciate that. 
Is the idea here that you can have a very high performance database on the same server your code is running on? Curious to hear from anyone out there that is really excited about this, maybe due to the performance benefits and your kajillion transactions per second.
Hi all trying to work with this EFcore and the toolings, I'm following a basic tuto from pluralsight (Kevin Dockx, build your first api with .net core) I'm supposed to do "Add-Migration myfirstmigration", but when I do, I've an error saying I dont have a parameterless constructor. &gt;No parameterless constructor was found on 'CityInfoContext'. Either add a parameterless constructor to 'CityInfoContext' or add an implementation of 'IDbContextFactory&lt;CityInfoContext&gt;' in the same assembly as 'CityInfoContext'. Now, that's something I would normally be able to do, except that MS went back from the package.json to csproj, meaning I can't find any ressources about this issue. Does anyone have any idea? (I've been to Dockx's github hoping to find a repo with the code, but he only did an initial commit with empty project -.-" ) Similarly, if anyone knows why they went back from package.json to csproj, I'd like to read about it. Thx
.net core is far from stable. Don't worry about that. I'd say you don't find "recent" ressources about WPF because crosoft believes it's mature and it hasn't changed much, more than because it's outdated or abandonned. However, you might wonder if, in the long run, it wouldn't be better to just go along with the web train. Not an expert though.
Jump to WPF train is maybe too late yeap, but UWP and Xamarin Forms trains just start their way from station.
I'm assuming that embedded means there is no server. The database is a bunch of local files accessed via some dlls that you can bundle with your application. More like SQLite than SQL Server. 
Yeah, stable. They went back from package.json to csproj two months ago. Half the libraries are still changing, sometimes overnight. The other half libraries aren't fully implemented yet. Most packages aren't working. There is little to no documentation to be found. I guess you mean it's stable as in "it doesn't randomly crash". I meant it's not stable as in "That thing is moving so fast there's no use to learn it yet because by the end of your lesson, they will have changed how it works". 
Sorry meant xamarinforms.
What a useless article.
Role based access control for asp.net stores the user role in a table which maps user to role. I haven't checked if the data is queried on request or at startup of your app, but a simple wrapper can be written to cache for performance. My guess is your company is doing policy based access control, which is resource driven instead of role driven. The policies can be defined in code for better performance and give you fine grain control. You typically see authorization logic creep into the data layer when it is data dependant instead of role based. Here is the .net core doc https://docs.microsoft.com/en-us/aspnet/core/security/authorization/policies. Here is an older blog post if you haven't made the .net core switch https://leastprivilege.com/2014/06/24/resourceaction-based-authorization-for-owin-and-mvc-and-web-api/.
I don't see me paying monthly fees for an IDE.
With a proper CI pipeline, I wouldn't say that necessarily is a bad thing. Makes your code much cleaner in the long run than magic strings. Properly typed permissions is one of the reasons we extended the ASP.NET Identity system as well. Makes code so much more maintainable. Refactoring tooling, unit tests, etc are all much nicer that way. But once again, without a proper CI, and if making changes to the code isn't easy, then yeah, it'll be a nightmare. We can push 10-15 changes a day to that code though and no one would blink an eye.
Fire up the SQL Server profiler and do a trace and see if you can find some slow queries that match the slow pages. 
When I've tried to replicate the queries in management studio they all seem to be pretty much instant. I know entity framework mangles the sql a bit but I wouldn't have thought it would make the difference between 50ms and 30 seconds
Hmmmmm if I can narrow it down to a specific page or routine I could perhaps fix it. But the whole thing seems sluggish. I'm happy to hire a programmer but that in itself is a can of worms
I've not found firebug to be overly helpful: http://i.imgur.com/lUFsJrL.png Although, again, I'm no expert
Everything is local that I can see, all stored on the same server. It uses jquery/jqueryui/bootstrap all stored locally
Yeah firebug looks like this on one of the worse pages: http://i.imgur.com/lUFsJrL.png
well, if everything is local - can you debug that request in visual studio?
Hmmmm ok, let me have a look
But doesn't work on Mac nor Linux. I should have written that clearer...
It works on MacOS and support for Linux under development.
Correct - it's a handful of C/C++ libraries with a C++/CLI wrapper integrating everything together. An API-compatible .NET Core version using interop in the background is due out later this year.
Use WinForms. You'll be done quicker and your app won't be a fat hog. Win32 is here to stay and WinForms is absolutely the most mature and stable kit for doing a desktop UI with .NET.
As others have said, GetWeeklyDeliverySchedule.aspx is your problem. Unless you can post code somewhere for people to review, you're going to have to debug the code yourself in VS.
I don't know enough about entity framework and how it handles the connection to the SQL DB, but if there are lots of queries and it's not using a transaction or it's opening and closing a connection with every SQL query that's executed, then that's not good and could add significant time to the operation.
Try https://dotnetfiddle.net/
I disagree with the maintain portion. Windows desktop apps tend to "just run" for long periods of time. Whereas with the Web, your app might not work on some or all browsers within a few years, especially if you wrote crappy javascript (which is easy to do). That said, I personally prefer the Web. However, WPF, UWP, and Xamarin.Forms are all just different flavors of XAML. So the import part here is to learn XAML. That said, I'd recommend UWP if you wanted to write a new desktop app for Windows at this point. If it isn't already, it's going to be Xamarin.Forms compatible from a XAML standpoint at some point in the future and therefore macos compatible. 
Most likely lazy loading from entity framework. For that page there is possibly an iqueryable object being looped on? 
Sorry, asp noob here, could you point me to a resource with an example for "use includes to force eager loading"
No, entity framework utterly DESTROYS the SQL for anything slightly complicated. You need to run a profiler session Also, take a look at the amount of calls. Eg, if you're loading the basic webpage metadata etc, how many times are you loading it? And if more than one, why? I've seen lots of codebases that will, eg, run a SQL call for the page title, another for the metatags, another for the H1 tags ..... Just get the page entity once and populate everything that needs it. Conversely, don't get the entire entity, all of its related objects and children if you only need it's title. This may not seem like a big deal, but if you're drawing a large-ish sites entire menu out like that there's going to be a problem. The only way to tell is sql profiling. Once SQL is ruled out, look into profiling the code itself. Try and identify timestamps for various events between the browsers request and the final page being ready. Also check the network tab of your browser Dev tools. If you're running a massive JS library and no cache system, it might not be the code or the db that's the problem. One of my clients a few years back had filled their site with 30Mb images, displayed as thumbnails. Some heavy pages took minutes to load because there was around 400Mb of data happening. 
Checkout this article: https://msdn.microsoft.com/en-us/library/jj574232(v=vs.113).aspx
You can use profiler, they are not that hard to run, and if I am not mistaken free/trial versions exist (like the one from JetBrains)
Attach dotTrace to the website https://www.jetbrains.com/profiler/ ; grab a sample during use; save it and have a look at it on your machine
30s? A timeout of some sort. Fire up Wireshark or `netsh trace`.
I would definitely attach and find where in the code the hang-up is happening. That is a huge time to load and I'm sure the issue will be revealed quickly
It's incredible what a CI pipeline does to your mindset
To offer another route, outsourcing user management is the one thing I will advocate as almost always being worth the cost. With almost all of the apps I ever route where we did a homegrown user management system, it took at least half of our time. We used Stormpath for a while, then switched to Auth0. [Auth0](https://auth0.com/) is pretty awesome, and very reasonably priced. For 60 bucks a month, we manage our app with around 3.5K customers without breaking a sweat about authN or authZ. Our internal management apps only costs us 15 a month. Doing the math, we were saving about 7K a month by having our team freed up for other things and not trying to solve user issues. We still have a few headaches every once in a while, it's not perfect, but it's a HUGE load off our backlog.
Have you seen/evaluated Orchard.Net? From http://www.orchardproject.net/mission &gt;Orchard is a free, open source, community-focused Content Management System built on the ASP.NET MVC platform. Orchard is built on a modern architecture that puts extensibility up-front, as its number one concern. All components in Orchard can be replaced or extended. Content is built from easily composable building blocks. Modules extend the system in a very decoupled fashion, where a commenting module for example can as easily apply to pages, blog posts, photos or products. A rich UI composition system completes the picture and ensures that you can get the exact presentation that you need for your content.
Gridviews themselves are pretty nasty and bloated. They come with a lot of functionality built in, but at a cost. If you can, swapping over to a repeater will reduce the overhead of the gridview quite a bit: http://stackoverflow.com/questions/139207/repeater-listview-datalist-datagrid-gridview-which-to-choose 
https://www.reddit.com/r/dotnet/comments/6bp2j6/speed_improvements_to_aspnet_website_where_to/dhoddcg/
With Entity Framework, make sure that your Dbcontext queries are executed only once by ensuring that they are materialized (should end with ToList (), ToArray(), First, etc.). If they are not materialized, what can happen is that every time you loop through that result, it may be generating a SQL query.
Yeah, the programmer used lots of gridviews, pretty much for everything. I'll read up on repeaters as that seems a lot better.
Right, this query takes 7 seconds to load on a blank test page: https://dotnetfiddle.net/JsBD2s Can anyone see anything obviously wrong with it?
Aside the SQL , there is a lot of string concatenation. Try to replace thoses with the [StringBuilder](https://msdn.microsoft.com/en-us/library/system.text.stringbuilder\(v=vs.110\).aspx) class or the [interpolated strings](https://docs.microsoft.com/en-us/dotnet/articles/csharp/language-reference/keywords/interpolated-strings) introduced in C# 6. Depending of the number of rows in your gridviews, it may account for a part of the problem. It's should not be dramatic, but you may gain half a second fairly easily. **Why should you do that ?** Because c# string are immutables, which means each time you use "+=", the compiler can't reuse the precedent variable, but has to recreate a new variable and then overwrite the reference, it's slow, especially with long string like html. 
Can we see some code from one or more of these? .... ProductionDAL ItemDAL SOPDAL CustomerDAL
productionDAL https://dotnetfiddle.net/EYZ5zT# ItemDAL https://dotnetfiddle.net/oClCHp SOPDAL https://dotnetfiddle.net/JsBD2s CustomerDAL https://dotnetfiddle.net/9LU7ya If there is anything specific missing, let me know.
Thanks for this, I see how to change the += to stringbuilders and will make a start.
Yes I was looking at this today. It looks quite good but I think the full version of mvc forum looks better.
Okay, each *DAL class is creating its own instance of a CS3Entities context. This is making your DB access shitty and slow. What you need to do is in your Web_UserControls_DeliveryScheduleWeekly control, create a single Using statement which instantiates your CS3Entities context for consumption by all of the *DAL classes. You will then inject the CS3Entities context into all of the DAL classes via the constructor (dependency injection) so they can consume this single instance of it, but to do this you will need to remove the *static* keyword from all of the class declarations. Like so: public class CustomerDal { private readonly CS3Entities _ctx; public CustomerDal(CS3Entities ctx) { _ctx = ctx; } public string GetCustomerName(string customerID) { return _ctx.Customers.FirstOrDefault(i =&gt; i.CustomerID == customerID).CustomerName.ToString(); } } Then you would consume your DAL classes as so: using (var ctx = new CS3Entities(ConnectionStrings.EFString))) { var customerDal = new CustomerDal(ctx); customerDal.GetCustomerName(customerId); } All of your other DAL classes should be inside your single using statement in order to consume the single instance of your database context. The main control code is a rats nest and exactly the reason why people love MVC so much (because it kind of forces you to write better code). So I would do a hell of a lot of rewriting using service layer, interfaces and all that, but the above should get you going and hopefully improve your app's speed. As someone else also stated, using .ToList() will force execution of the SQL and should speed things up too. I'm interested to know how you get on, so please let us know.
Thanks for this. I've got a lot of spaghetti to unravel!
No probs. This book has been invaluable to cracking the nut of writing maintainable ASP.net web applications, highly recommended: http://www.wrox.com/WileyCDA/WroxTitle/Professional-ASP-NET-Design-Patterns.productCd-0470292784.html
thank you, will comment further when I have had time to check out :)
[I wouldn't bother with the work to rewrite](https://www.joelonsoftware.com/2000/04/06/things-you-should-never-do-part-i/) in MVC if you just want to improve performance right now. The book I recommended is more about writing better backend code rather than better webforms themselves. It describes useful patterns and practices which makes it easier to keep your webforms code behind files doing what they're supposed to be doing, and that's presenting the data to the user and not doing any business logic whatsoever. Really, all your control should have is properties for the data to be passed to it for it to consume, and logic for presenting that data. But as you can see from the code, its doing all kinds of stuff. People hate webforms and run it down because it's easy to get in a mess with, but you can write elegant code with webforms if you use the patterns described in that book and are disciplined. 
From a quick glance EF likely is generating a crazily inefficient SQL from such a complicated query. In my experience for these cases we optimised by creating a Stored procedure with the exact sql we want and calling that, if you cannot get EF to generate efficient enough sql.
The maddening thing is, when I execute any of the SQL grabbed from SQL Profiler every query is near instant, even the more complicated ones. The bottleneck doesn't seem to be at the SQL server but at the ASP side of things.
Well, we were expecting to work with Openshift at my job. Openshift expects a project.json file to upload .netcore projects. You see how that is a problem? EFCore isn't simply "not ready", it also requires you to know exactly which was the latest stable version, cause the latest nuget packages released aren't actually working. I searched about an issue I faced, the answer was "Well it will fail for now, but hopefully it will be fixed in 2.0". That's from one of the world's biggest IT provider. That's not "stable".
I wouldn't count on it, depends on how the host configures the guest and which hypervisor you are using
Thanks for you comment! Such functionality will be possible when I implement ElasticSearch support. Then you only need Kibana to plot various reports or graphs and put them into Dashboards. 
And where is the cookie stored? In the browser? Then wouldn't you simply be vulnerable to client side privilege escalation attacks? That would be ... not good, depending on your potential impacts. 
It's encrypted. This is all part of the federated auth api although I use it for just the cookie, not federated.
Mvvm is an idea on how to develop applications. You do not need to use it. Neither do you not have to use it.
That's handy, thank you
Because Angular uses it an Angular is all the rage right now.
No matter how "correctly" you write javascript, it does not provide compile-time type checking.
What about in the VS performance profiler over that code block? the queries themselves may run fast but the EF code to generate the queries/transform it after the query might be slow.
You're absolutely right, MVVM is just another paradigm. For some apps you may find it's overkill to use the design pattern. That being said it's certainly the best pattern I've seen for WPF/UWP development but haters gonna hate, right?
Types provide intellisuggest, easy refactoring and it makes your code easier to navigate and understand. I personally can't code JS without it anymore. It's just not as enjoyable and I'm a JS developer in the end of the day. Types have made me more productive.
You avoid types in Angular? That's crazy! If you just use *any* for everything you lose some of the IDE intellisense. Also you make it difficult for people to know what objects your methods/libraries are expecting.
Download a evaluation version of dotTrace. https://www.jetbrains.com/profiler/?fromMenu It should give you a point in the right direction. I have a feeling your are executing a ton of sql queries. And when you're not doing .ToList() on the ef query you are actually not executing it until it's enumerated later. 
I wonder what specific components were subbed in though. Sub optimal scenes &amp; visual trees will still cause performance issues due to the rendering mode, but there could maybe (?) be some significant performance boosts, depending on the changes. Too bad they didn't release any performance comparisons... The best thing for WPF would be if they moved to Direct2D as that would have a huge impact, but the work would be far from trivial.
Ah, Code Lens... first feature I disable on a fresh install. Seriously, who the fuck thought that more line noise was a good idea?
Ah, ignore me, I see the "Trace Information" bit at the bottom of the page. I didn't know about that, very useful, thanks Infact, Trace.Warn("wibble"); can show me where the slowdown occurs better than commenting out lines blindly, brilliant
 Given your Orders Linq query if its not passing in a customer or orderid it could be your loading all the data for all sales orders and products on first load. If so you probably need to make sure some filter is applied first and/or implement paging so only a page of records i.e. 30-50 is shown first if the user really needs to see something. Otherwise make them select a customer/product/unpaid etc... then query. The size of your ViewState will also be a problem if all records are loaded. If so you can offloaded that to session state which might help.
I added the .ToList() and it made a big difference, from 7 seconds to 3. I'm sure there is more I can do here
Hard to say, that will be converted into an expression tree and this can be optimized and leveled, then into SQL. It could end up as the same sequence of operations i.e. no difference.
I think types is good when you create some libs which another people use, it helps understand libs. But for Angular application custom code in components... I think it's not necessary. But it's my own experience, I do not try to say it's good or bad approach.
I'm making good progress here, thanks for all your help was this: http://i.imgur.com/lUFsJrL.png now this: http://i.imgur.com/CdSIJ4b.png
from 63 secs to 1.6 secs, cool have you commented all the code out :-)
Ha! No, all functionality still in place. Changed a lot of IQuerables into Lists, instead of doing SQL lookups per line item, did a single lookup into a Dictionary and used that for the line item lookups, as well as rearranged some stuff. I've been tinkering for about 2 days but I think it's time well spent given the outcome! Especially since I know very little about ASP.NET
Yeah, I was really disappointed.
Worth while looking into correctly disposing of your database contexts if you haven't done that already. This will ensure the connection is closed down properly and the context is available for garbage collection. You can dispose manually when you've finished with the context or you can wrap the context in a [using statement.](http://stackoverflow.com/questions/212198/what-is-the-c-sharp-using-block-and-why-should-i-use-it)
&gt; It's like Microsoft only wants developers to use visual studio on a pc for development after opening .net core for cross platform. More like visual studio was initially released on windows in '97. its be completely overhauled and rebuild a bunch of times. Meanwhile Visual Studio for mac was just literately officially [released 9 days ago](https://www.visualstudio.com/en-us/news/releasenotes/vs2017-mac-relnotes). You can't possibly expect both IDEs to be anywhere near equal... Just install parallels and develop from there. You can get an [out-of-the-box development image](https://developer.microsoft.com/en-us/windows/downloads/virtual-machines)
Those having established backgrounds in statically typed languages like C# or Java, for example, likely find the transition to client-side development easier when working with TypeScript compared to base JavaScript. It's why I prefer TS if the option to choose between TS or JS is offered. However setting up the dev environment properly to use TS for all things frontend could take a while. A frontend buddy, with a .NET background, works at a React shop that has adopted TS completely. He noted the true power of TS appears only after you've mastered dealing with definition files.
Try Rider: https://www.jetbrains.com/rider/. It's a full, cross-platform IDE. These are the same guys who make Resharper, which I've only been using for a while but am converted! It's free under Early Access Programme at the moment but will cost at some point I suspect.
Don't do that. There's no reason to do that. Why would you want to do that?
Well I don't know if I feel like I can say it all here, but basically I have a class that performs operations on a specific type of db. Because the application is multitenant, I'd like to only have to write those utilities once even though the two model classes are slightly different
The short answer to your main question in the title is to check out "wilderminds-aspnetcore-snippets" extension for Visual Studio Code. I am a .NET developer during the day working on windows stack, but use Visual Studio Code to develop .NET core apps on Linux. Give it a look
Anyone know why .net source code has all its files lowercasewithoutacapitalletter.cs? I cant remember if .net core is the same or not.
.NET has its own SmtpClient. MailKit/MimeKit is the now Microsoft recommend email library for .NET though. It's very high quality, often updated, and the guy who wrote it works at Xamarin.
Yeah I use .NET's SmtpClient for sending and it works great, I just need[ed] one for reading emails sitting on a server. I'll definitely check out MailKit, though, thanks!
No worries. It's definitely the best option.
I use https://www.nuget.org/packages/AE.Net.Mail/ very simple
Why don't you link directly to the comment you mean? I assume it's: https://referencesource.microsoft.com/#mscorlib/system/runtime/versioning/resourceattributes.cs,68
I use Visual Studio Code on Mac exclusively to build my .NET Core applications with the dotnet CLI. You can do "dotnet new mvc" to create a new mvc project which shows the basic structure. Everything after that, just add new files manually in the file explorer following the same pattern.
I don't know, could be, I guess. It's not quite clear to me which one he's talking about. I thought because he was referring to 'snippet packages or command line tools' he was talking about the native mac visual studio
That's a strange comment but mainly as it says this further down the code case ResourceScope.None: // Public - implied
The best comments i've seen are from the Apollo moon lander software circa 1969 https://github.com/chrislgarry/Apollo-11/blob/dc4ea6735c464608d704fa183f3e3d08b013c42f/THE_LUNAR_LANDING.s#L245 CAF CODE500 # ASTRONAUT: PLEASE CRANK THE TC BANKCALL # SILLY THING AROUND CADR GOPERF1 TCF GOTOP00H # TERMINATE TCF P63SPOT3 # PROCEED SEE IF HE'S LYING P63SPOT4 TC BANKCALL # ENTER INITIALIZE LANDING RADAR TC POSTJUMP # OFF TO SEE THE WIZARD ...
Even though I do it sometimes, I was surprised to see multiple classes per file
If you don't mind, what was the final architecture of your production environment? Are you hosting the angular and asp.net bits on separate web servers or is the asp.net server using static files to serve up the angular app?
That wizard, he gets them *high*. He *takes them to the fucking moon*. Strong shit, that is.
I am learning about this too. I've been using this as reference: https://github.com/aspnet/JavaScriptServices 
Just buy NCrunch and thank me later It's not perfect but it's way ahead of Live Unit Testing in VS.
My opinion (YMMV): * If this is a project that will only ever be accessible over the local network (most enterprise apps within a business, etc.), go traditional MVC. Bandwidth is cheap, internal clients can easily handle full-flavoured content. Don’t go fat-free just to shave off a few Kb in transmission. * If this is for mobile or thin client consumption, also consider full-flavoured MVC-only content. Going fat-free means the client will get a massive dump of JS frameworks up front that the client device might not have the stamina to stomach. Plus, with mobile: data costs. Unless your site is massive, front loading an entire SPA will typically weigh more than MVC, especially if you are careful with MVC and have lots of repeatable items that can be cached by the client. The case *for* an SPA comes down to this: * If this is for self-education, and doesn’t actually *need* to be completed, polished or released; awesome. Go nuts. * If interactivity and responsiveness *after* initial load is important (think thick/heavy data sets to be displayed and quickly swapped out), then front-loading your calories is indeed important. This goes for both mobile as well as desktop. You can quick-load blank templates for that page without actually pushing anything back and forth (it gets pulled from the front-loaded content) and then show the content as it trickles in, which is very good for improved usability.
ActionMailerNext is, IMHO, one of the best/better packages for sending eMail on MVC. Multi-part, attachments, you name it. Granted, it only *sends* eMail, and only through standard SMTP (I have not seen an Exchange Web API version yet), but I’ve come to love it.
&gt; Universal Apps train (which uses a subset of WPF) but that limits the apps to running on Windows 8 and up. For anyone who wants to release just a single version of a program at a time, this is a deal breaker. Until Win8+ is 95+% of Windows systems, I just cannot justify dropping WPF in favour of Universal Apps.
MailKit, mimekit.
https://github.com/MarkPieszak/aspnetcore-angular2-universal
It'll either be a bunch of technical questions verbally or you'll be asked to build a basic page that pulls information from a database.
Server side performance is easy to poo poo until you have high usage or want to move into a cloud hosted solution. Then, suddenly, all the server side rendering makes for a whole lot of angst (especially if you're doing high levels of post backs). There's a balance to be struck that's not all SPA and not all server side. 
It was as recent as a few months ago. I know how to get close and after a few months of experience I believe I could get closer and probably get it to work smooth enough. The problem is I would have to own that configuration and every little change to core or angular cli would need to be integrated and God forbid you need a custom Web pack build routine.. It's a maintenance nightmare and I'd be working on it perpetually. The problem is that you are not managing one build,run,debug routine. You are managing two plus the interaction of both. It just bleeds complexity.
Understandable. Glad I can have this chat with you. I was about to embark on this journey myself. Thanks for warning me away. You guys use Windows Authentication in your project?
It was a path worth walking because the learning was intense but it was one I wouldn't use in production. By saying "windows auth" I assume you are referring to our application connection to the database correct? Yes we do you windows auth.
#Structure The general structure I use is an ASP.NET Core web app project, with the Angular app in a 'ui' subfolder. [Here is a gist](https://gist.github.com/anonymous/a84b7be7d5402a7296609eec47581a64) with the interesting files. ###Development/Debugging Launch `gulp serve` (or use Visual Studio Task Runner Explorer) and open a browser to http://localhost:4200/. The Angular CLI is configured to proxy `/api` to the ASP.NET Core web app on port 5000. Live reloads happen when angular files are changed, everything just seems to work pretty well here ###Builds/Deployment For builds/deployments, it depends on the scale of what you're deploying. If you want a simple single-server option, you can simply enable static content hosting in your ASP.NET Core app, and use the `ngbuild` npm script I have configured for the ui. That does a prod build with aot and writes output to the `wwwroot` directory. If you're doing a fancy/large-scale cloud deployment, you could build things, deploy them to aws cloud-front, etc... #Tasks Using the Visual Studio Task Runner Explorer, you can bind the `serve` task to run when the project is opened #Pi I haven't really tried to put anything on my Raspberry Pi, but I'd say you could just have ASP.NET Core host static content so you only have a single component to deploy when you need to update.
I might try this approach, but with dotnet core, you can just use static files and default file and then put your Angular dist folder in your wwwroot dir. The first solution with rewriting all 404s to call your index.html page is still necessary for html5 routing. 
Thank you, i have observed it.
Are you comfortable with your skill level? If so, calm down and wing it. If not. Well then you should have paid more attention in school ;-)
Pretty much. If only they had supported them to Windows 7 the coverage would have been pretty decent. But at this point it's either WPF or considering Electron.
If you've never had an interview for a software engineer position but were invited to interview. They probably have a pretty accurate understanding of how little you know. Just answer truthfully, don't bullshit. When they ask a question you don't know answer how you would approaching making it that you do know. Lastly if they ask you about websites and you don't mention Stack overflow that'd be silly. Hell probably just find some way to bring it up on your own. You might also want to discuss how you would determine which answers are credible and what you'd do more than just copy and paste 
&gt; and VS 2017 dotnet core .sln using WebApi2. There's no Web API 2 for .NET Core. You likely mean ASP.NET Core MVC.
A demo Steve Sanderson did a couple of days ago using asp.net core + react/angular4/knockout/aurelia/younameit :) https://channel9.msdn.com/Blogs/Seth-Juarez/Building-Single-Page-Applications-with-ASPNET-Core
My favorite one is Win32 API function [CharUpper](https://msdn.microsoft.com/en-us/library/windows/desktop/ms647474.aspx) *Return value* *There is no indication of success or failure.* **Failure is rare.** :-)
They've released pricing which will apply after it leaves early access. It's $140 USD per year. It'll have to be pretty amazing for me to consider ponying up that sort of cash when Visual Studio, VS for mac and VS Code are free.
I'm not nickel and diming, nor do I mean to imply you should always include JS in every web site. When I said balance between the two, I meant that in the grand scheme of things, not to make each and every website in the world a mix of JS and server side. I'm just saying don't rely on the cheap performance argument. I've been the guy whose been called in to fix the performance mess made by others way too many times in my career. Go with server side because it's the right thing to do for your project, not because performance is cheap.
Gotcha. We are looking into Redis for that. My problem is that the site and the API depend on shared objects. So distributed caching is a must. Or at least some mechanism to invalidate across services. 
I agree, honesty honesty. Many teams don't need or can teach the web parts. No team can tolerate a BS artist. 
It's the object cache that lives in application pool memory in the server. The core code and web application do CRUD ops on the core objects. So that has its own private API the core devs use to facilitate the app. I'm responsible for the public REST API and all integrations. Ideally I'd like to have this as a separate micro service that does nothing but the public API. The objects are streamlined versions of the core ones (to abstract away unneeded complexity for our client consumers). But they are all based on the same data store in the end. Right now it's all together because that is the easiest for objects that are tightly coupled like that. However, it would be nice to have a middle tier scalable shared cache or some other solution. Then I can take full reigns of the Public API as a separate application/service without having to bother with core code changes all the time. Just a thought experiment at this point and brainstorming possible solutions. 
Microsoft has just released draft Architecture Guides, including [Web Applications with ASP.NET] (https://www.microsoft.com/net/learn/architecture)
You may want to check out Julie Lerman's courses on EF on Pluralsight. Specifically "Domain-Driven Design Fundamentals."
In a lot of cases you don't need SPA. Make most of your page cache friendly and dumb and use AJAX for your heaviest/most interactive functionality.
We use this on my current project. Our base also has common fields like CreateBy (userId), CreateDate, UpdateDate, Status... fields that are all present in every entity. It saves a lot of duplication across entity definitions.
Keep your data objects and migrations in a separate library. Rarely do they belong in the web app. Look at the "--startup-project" arguement of the "dotnet ef" command. Everything else is really up to you.
Thank you!!
If Rick Strahl is struggling, it's bad news for average developer.
I believe that commands/queries should be declared and live *near* their handlers. This helps with your issue by using conventions. We go so far as to declare each Command and Handler class "pair" as nested classes within a single static class named for the feature.
As much as I like LINQ and EF, there are times when they get in the way of finding the best solution for the issue. In cases where I have a hard time figuring out exactly how to write a LINQ query to get what I want, I tend to take a step back and think of what SQL I'd write to get those results and go from there. In this case, here's what I'd write to get those exact results out of SQL SELECT OrderCount.CustomerId, OrderCount.OrderCount, TotalQuantity.TotalQuantity FROM ( SELECT CustomerId, COUNT(OrderID) OrderCount FROM Order GROUP BY CustomerId ) OrderCount INNER JOIN ( SELECT CustomerId, SUM(Quantity) TotalQuantity FROM Order INNER JOIN OrderItem ON Order.OrderID = OrderItem.OrderId GROUP BY CustomerId ) TotalQuantity ON OrderCount.CustomerId = TotalQuantity.CustomerId Without going into it, I personally would not trust a LINQ query passed on to entity framework to generate this SQL. What EF and LINQ are most likely going to do is to either A) pull all the entities into memory so it can do a count there, or B) hit the database a bunch of times either for each customer or for each order. Neither of these options are ideal IMHO, so the question is how to deal with it. The first option is to use that SQL statement, which is fine, but somewhat goes against using EF and LINQ. The second option, if you're willing to hit the database twice (which I wouldn't say is a huge issue), would be to simply split those two subqueries up into separate LINQ queries and do the "join" in memory. The second option would be a bit more preferable, since it'd create a cleaner more maintainable codebase. In interest of completeness, there's also a third option of adding the SQL query as a view in the DB, then just querying that view with LINQ/EF which would really be the easiest, but some people don't like doing that. There's also another issue within your code, which is passing an IQueryable to your viewmodel. LINQ has a tendency to wrap up a lot of what it does into IQueryables and IEnumerables that do not actually run your query when you construct your LINQ statement. It only processes the results when you try to iterate over the results later in your code. When you pass that to your ViewModel, there's a really good chance that the DbContext that the query is based on either has it's connection closed or is disposed already. Its much easier to use ToList() on the LINQ query, which will get the actual results right then and there. This will prevent errors from popping up in your ViewModel. Hope that helps.
Agree with what everyone is saying here but I'll add one more thing that's awkward. If the handler lives in a different spot from the request then navigating to "implementation" is kind of awkward.
I would look to chocolatey for managing dependencies on your workstation. Docker at the moment is more for application runtime management. I suppose you could continue down the path of proving this out, but I would suggest biting off smaller goals before dockerizing your local development environment. Here is an example of a chocolatey install script. https://gist.github.com/patrickhuber/9441dbd9c3dee3a09335 As for code sharing, use source control to sync code. 
Docker is all about maintaining a consistent application runtime environment and is not a great development machine environment. Yes you can run Windows Server Core, and _maybe_ you could install VS and wire up a VNC, but overall I would expect the experience to be pretty terrible and not save you any time.
Thank you very much kind Sir, you gave me lots of points to consider and study closer - cheers! ;)
I don't know anything about creating views in SQL Server, so I'll have to investigate further. Thanks for the tip!
I'm done with MVC views and routing. Angular sitting on top of ASP.NET Core that's really just serving static content and the API is light, fast and clean. I tend to think that giving developers two ways to do something is an invitation that ensures you'll see both patterns used in your app. In the case of MVC and Angular, there's a lot of functional overlap (view rendering, routing, etc...). I'll say that I don't think there's anything inherently wrong with using both, but I have a hard time imagining a scenario where I'm starting a new app from scratch and I would want to use both. I posted a comment [here](https://www.reddit.com/r/dotnet/comments/6c6bq1/noob_questions_about_spa/dhsui14/) with a link to my general setup and gist for the interesting files. That setup makes front-end and back-end development fun and easy.
The main app that I wrote (a few years ago) and now maintain is a combination of jquery/knockout, MVC and web API. It works, but the MVC side is complicated and really doesn't need to be there. Even though I know MVC backwards by now if I was building another app from scratch I would probably go down the Angular/React + Web API route. (Probably! Need to build a good proof of concept first :)
Since you didn't get many bites on recommendations, I'll try and help. When you say 'designing the fields (especially the geofences)' what makes the geofence special? It's essentially a polygon right? It would just a have a DbGeography / DbGeometry field and whatever other 'normal' fields you need. The Geog / Geom field can hold whatever shape type you want to throw at it, point / line / poly / multi-poly. I wouldn't beat yourself up over creating this via Code First... You could create the database tables in SQL Server and then when you generate your models, do Code First From Database and look at what gets created and copy that to your code first efforts or just roll with it. I doubt there would be much weirdness to it besides spatial indexes maybe. I know you acknowledged that you didn't like the SQL Server *only* resources you found, but your main task (comparing a point to a polygon and returning true or false based on if it intersects) is probably best accomplished in SQL using [STIntersects](https://docs.microsoft.com/en-us/sql/t-sql/spatial-geometry/stintersects-geometry-data-type) or the LINQ that wraps it. I don't know what your source of geometry for the geofences is, but it kind of sounds like you'll be creating them since you mention them needing to be compatible for insertion. If you wanted to avoid getting your hands dirty with SQL and become familiar with how it creates shapes from text, projections, etc, you'll probably need some software assistance with creating the geofences, like QGIS or ArcGIS. You can create them via point and click interfaces and then export them to SQL. What I would do is write a stored procedure that takes a point id or a set of lat/lon and compares against your geofences. Since they're mutually exclusive you just have to return the id of your intersecting geofence. You can then pull this stored procedure in with EF (or whatever) and treat it as you would any other stored procedure. [This thread has someone doing something similar.](https://forums.asp.net/t/1889036.aspx?stored+procedure) [Here's someone doing it with LINQ instead of a stored proc since you mentioned that.](http://stackoverflow.com/questions/21243403/how-to-use-dbgeography-filter-in-linq-with-entity-framework-5) Hope this helps. If you have any followup questions feel free to ask. 
I've never been a fan of MVC. I just don't like the idea of mixing server side and client side code together. For all greenfield development I use webapi and angular. It forces separation of concerns. 
Oh I see, you need to refactor something you've got running already. Yea, that'll be some work. Aww..that about page is out of date, its already leveraging react.js, its in nuget now. That sites just the result of a tutorial I went through and took it live to showcase in a round of interviews I was doing at the time.
I've been dealing with this by putting the message classes as inside the type of the handler class. So I have Foo with CRUD type operations so I've declared Foo.Create, Foo.Delete, Foo.Update etc. Then I can just easily navigate to implementations as they live in the same file. It's a bit more of a functional style too as the CRUD classes are nearly record types that live in/near the handling code in the same file in most functional type languages.
Yeah, I created a POC using the Angular template and a WebAPI project. The biggest hurdle already is the lack of an Identity library integrated like it is in MVC, I'm going to play around and see if I can't just rip the Identity components out of the MVC template and add them back into the API then fix up a login page in Angular. I've found one example of this online but it's from 2015 and using Angular 1
Hmm not too familiar with Signal, ill look into this. 
Loving this direction, too. I'm still developing on Windows, deploying to a mix of Windows and Linux - great having the choice and flexibility.
I love it! I use it all the time to quickly see references and to see why and who made the most recent changes to the block
More spam of shitty Indian companies.
I'm a newbie but I use OpenIddict's OAuth to issue JWTs. I have not yet used roles though. The JWT is compatible with the [Authorize] attribute but like I said I haven't used roles so not sure how decent that part is.
And of course if your app is authorized, you can forget all about StaticFiles module, or whatever it is :(
Yeah I was in the same boat, Moved away from MVC and went for the WebApi w/ Angular &amp; Typescript frontend. I have to admit, once you get over the learning curve it's a pleasure to work with. I never really enjoyed client side development until I used Typescript with Angular
Actually, I imagine most people would use WebAPI instead of MVC if they're using Angular or React. Of course, that means you have to set up ASP.NET Identity yourself as Visual Studio doesn't give you the automatic setup with a WebAPI only project.
Here is a near drop in replacement for `Random` which is using `RNGCryptoServiceProvider` by default, has a seeded cryptographic pseudorandom mode (AES CTR) suitable for reliable testing (significantly slower though) and an external buffer read mode that can be filled ahead of time for hot paths: https://gist.github.com/bbarry/4acb4f1b3a4201e3bfd6df188d2de18c The only line here I have a question about is #150, it is possible that: public override double NextDouble() =&gt; NextUInt64() * (1.0 / ulong.MaxValue); has some bias where a 31 bit version like this: public override double NextDouble() =&gt; (NextUInt32() &amp; 0x7FFFFFFF) * (1.0 / 0x80000000); wouldn't (or [a 31 bit version like the BCL one](http://referencesource.microsoft.com/#mscorlib/system/random.cs,94), though the standard implementation has the obvious bias issue of only a specific set of 2^31 values. I haven't gone to look for academic validation on the best way to generate uniform values in the range `[0,1)`. Add to this the bug where I am producing the range `[0,1]` instead (which the BCL version removes by replacing it with bias in choosing the random number `(Int32.MaxValue - 1) * (1.0 / Int32.MaxValue)` twice as often as any other number. Generating good samples for even relatively trivial floating point ranges is not at all straightforward and actually I find it surprisingly difficult to find online a proper algorithm for doing so. I suspect I can write this instead: const ulong MBIG = 0x8000000000000000; const ulong M1 = MBIG - 1; public double NextDouble() =&gt; (NextUInt64() &amp; M1) * (1.0 / MBIG); But I am not certain (I might need to shift MBIG a few times)... edit: decided that this one at the end is at least better because it represents the open range instead of the closed one. 
yep edit: fixed (untested)
That is the ultimate crux imo. I'm lazy/not smart so missing the out of the box Identity is painful. Been looking at OWIN and Auth0 but I didnt want to have to redo my current solution
@nmilosev @NavelBarricade The latest release contains this functionality https://github.com/tsolarin/readline/releases/tag/v1.2.0
EasyNetQ would be great! 
You mean ones not made by Microsoft that haven't been ported yet (like SignalR)?
I'm thinking more of C++/native libraries that don't already have a managed implementation
Open Whisper Systems Signal Protocol 
Try getting inspiration from useful libraries that have been ported into the rust ecosystem perhaps? https://crates.io/search?q=sys&amp;sort=downloads miniz, libz, libcurl, libgit2, sdl2 (isn't there already one?), freetype, sqlite3 in a single package that doesn't suck (why are all the packages on nuget broken or weird?), zeromq, plibsys
1. A fast, correct and bug free implementation of various unicode algorithms and formats. 2. A Perl6 Grammars+Regex feature port against the data structures for #1. 3. Various ADO.NET providers that correctly support async (MySql, Oracle, Sybase, Postgres) 4. https://github.com/BurntSushi/ripgrep api That is my "someday I'll think about doing something about this" list. 
Fast matrix and vector operations math library.
You want two apps and a shared dll with repositories for the work. Trust me it causes problems. I created two contexts one for migrations and one for normal operations and the code is a nightmare to maintain.
Pure C# port of OpenCV ! Massive job here.
First step is to set expectations with your manager/business, it's their job to then work with the client. Be clear: this will not be fixed on time, and you can't provide an estimate yet for the above reasons. Ask for help now - that includes possibly bringing in a consultant, don't try to be a hero.
Thank you for the suggestion. I will ask my manager to delay the deadline at least to 1 month starting from the day the working environment is set up. Without that, I cannot test anything. What do you mean by "bringing in a consultant"? I am in contact with a manager of the customer company...
They didn't use source control at all, or just didn't use git? IMHO, if the former you'll want to get that addressed before anything else. 
No. They used to make different copies (like copy-pasting folders) for different versions of the source code directory. They just gave me the last version... It takes so much because it needs to be tested inside a bank's building. The testing there isn't also done by me. I just give them the changed files and they'll say if it works or not... Not possible to test in those conditions. Already implemented GIT. How do I write unit tests if I don't have a good understanding of the app structure and functions? Thank you :)
I have another opportunity but for the moment I'll wait till the end of this. Then I will decide.
I strongly suggest avoiding giving any firm time until you know what you need to do. I wouldn't provide any firm time estimates until: * You have a working environment, and * You understand the codebase and have an idea of how to tackle the bugs. Given you're new, have no support and you're unfamiliar with the language, frameworks and possibly domain -- it could take you 1 month just to get things working. Once you have the above, you can then tell them that each bug will need to be estimated and costed individually. It's then up to your business and their client to prioritise and choose the most critical bugs to be fixed first. 
One of them cannot help with the setting up, the other one (the main one) almost never answers.
No source control at all... That's my opinion too. I didn't expect a situation without any instructions/code history...
Stay positive :)
this. If they don't know how code works, give them a date and they will hear an agreement.
Quit because it's too hard? He's a junior dev and this is actually a fantastic experience. Maintaining other people's code is the crux of being a good developer. Don't quit unless you are personally mistreated.
Working in an enterprise environment with messes of setup. I can surely understand points 5 and 6. Point 4. May or may not be true. You are a beginner at this language, with no flow charts or anything. Don't mentally block yourself by telling yourself these will all take a long time. Your brain will start to believe it. You will have to do your own analysis every time. Hopefully if you are lucky they are related or a refactoring could fix multiple. See if you can group them. You inherited a mess. If you joined 2 weeks ago, your boss should know you aren't responsible for what happened before you. It sounds like something is being heavily mismanaged. Do what you can. Finally for your config issues. I'm not sure exactly what you are working on but config generally happens at the app start if they are sane. If routes or other things need to be configured that has to happen at startup. See if you can just follow the startup routines. I would expect when development leaves for them to turn something over to other people in the company. Someone needs to at least know where to start and what dependencies are whether they are documented or not. I would also start documenting them for the support team / yourself when you need to do installs. That should fix the issues with production not being able to figure it out and help future installs.
It integrates into VS
Quit because there are so much better opportunities. (S)He's a junior dev and a willingness one, (s)he will easily find actually fantastic experiences. Maintaining other people's code can be done everywhere. I love refactoring and archeology, but its environment seems clunky and prone for failure. No team, no test, no docs, no time and no consideration. &gt; Don't quit unless you are personally mistreated. You can quit for a lot a reason: - better income - better advantages - better language (VB-&gt;C#) - better team - better enterprise - better project - better location - better climate - whatever 
That sounds great it really does but did you miss the part about a one month deadline ?!
I don't think that would solve anything. Without documentation, configuration instructions and previous version control, even an expert would need a lot of time.
This. On one hand I find it fantastic but on the other I want to save my patience/mental sanity :)
Good eye there ;)
1) Done 2) Will do as soon as I have a working env set up. 3) ToDo
One dev used a company laptop (also for home working). He factory resetted everything before leaving. The other dev, the main one, worked on his computer and has the environment still set up (but hardly answers)
Here are some classics: - You can do better anyways - There are plenty of fish in the sea - You deserve better - She/He wasn't marriage material Dumping a job is like dumping your SO. Not an easy choice, but if she/he is toxic sometimes it is better for each others. :)
Maybe you can find code snippets and docs in their mail box. People use to do this.
&gt; github/bitbucket Be careful with that. Even as close source, some enterprises wont allow to see their code-base online.
&gt; How do I write unit tests if I don't have a good understanding of the app structure and functions? Start by finding your units. likely, classes. For now, you can treat them like black boxes, and just write tests the public methods. You should have a rough idea of what's expected based on the name, inputs, outputs and so on. I'd also chime in about the timeline expectations: DO NOT, under any circumstances, commit to ANY timeline until you're comfortable with the code! Something about your story suggests your employers are totally clueless and you will be pressured into all-nighters and weekend work. Don't fall for it! Any hint of pressure like that this early in the game, and you should run for the hills. Good luck! 
Do a bunch of the documentation and groundwork, prepare questions and understanding, then bring the consultant. The consultant is to speed up your training and understanding,. It do the base grunt work.
I'm with you on this one. Quitting is easy, but maintaining code/fixing bugs is what most of the rest of the development world does on a regular basis. The learning opportunities are plentiful. I would have loved to start my career working on things like this. Unfortunately i started in an era where there was no internet, and the code was in straightup assembly - with no documentation :-) Thankfully we were used to that, and it was part of the learning process - though i was really thankful when we moved most of our codebase to C. Ahh memories.
Hey, from a technical perspective MEF is very similar do a DI framework, though it's main focus is to provide the infrastructure for plugins. You will find alot of functionality in a full blown DI container, which MEF will no give you. Examples are: * Configuration via code/configuration files * Better scoping (e.g. pooling) * Interception * Automatic factory support * ... Especially the lifestyle support and interception come in handy when developing LOB applications. 
Docker Tutorials [1-Hanselman](https://www.hanselman.com/blog/ExploringASPNETCoreWithDockerInBothLinuxAndWindowsContainers.aspx) [2-Kontena](http://blog.kontena.io/dot-net-core-and-sql-server-in-docker/) [3-Youtube Docker Windows Series](https://www.youtube.com/playlist?list=PL6tu16kXT9PrTeP07thlsrF8Sf9zHXmh5) Also check out the linux versions of the tutorials since most of that should apply to working on windows.
Umm yea, like the fact that we're looking to hire a junior dev and fully expect them to lean on me (an architect) for a long time to help them learn. Throwing a junior guy at this problem is just an indicator of a business who doesn't understand technology. Hell, just given the brief explanation, I'd probably quote 1-3k just to do an assessment to tell them whether or not I could finish it.
&gt; it became quite obvious that being able to asynchronously hand off to our queuing service greatly improved throughput. Unfortunately, at the time, Node.js didn’t provide an easy mechanism to do this, while .NET Core had great concurrency capabilities from day one. This just seems really strange. Everything in Node is asynchronous, so it's weird if the only way they could communicate with their queuing service required blocking I/O.
yeah im all for .net core but i was thinking the same thing...of course in my mind im imagining something like handing off to SQS maybe thats not the case for them?
pay me $50/hour and I'll un-fuck it for you :) kidding aside, fortunately since it's C# you've got pretty much the best IDE/debugger combination out there (in my opinion). Your description doesn't give me much to go off of in the way of offering any explicit solutions, but when I have no idea what the fuck I'm looking at, I look for "anchor points" - Does the bug occur during a particular workflow? Search the code for ANYTHING that might have to do with that workflow, learn what the pieces do. You can also just start at some endpoint and follow line-by-line, working backwards or forwards. If it's a particularly large codebase, I'd probably also install at least a trial version of Resharper, it adds lots of extra helpful tools for associating/remembering where variables are. VS2015 does a lot of the things resharper does, but I still personally keep it in my toolkit because it saves time in my workflow. As a sidenote, I don't think git will help you very much. Just gotta hard-mode reverse engineer it man. It's tough, but doable :) Also, don't look at it as "I don't know this language so it's hard". Programming languages, more or less, all accomplish the same things, just in different ways. If you're familiar with Java, JavaScript, C, C++ or a number of other general purpose programming languages, you can at least figure out for the most part what the initial developer was trying to do.
Even as a .NET Core fanboy I must say this (and the featured article at the microsoft site) is nothing but rather poor advertising. No details at all. Buzzwords everywhere. Not a single proove.
Totally. Everything in the article is very vague. &gt; Unfortunately, at the time, Node.js didn’t So, it seems now they have figured that out. :-) I wouldn't be surprised if they compared a single Node process to .NET Core on a multi core machine.
Bitbucket has free, private repos.
The other 2 guys' code you've inherited left for greener pastures. A senior person wouldn't feel good about the position you're in right now.
It's a little vague, but OP sounds like this is their first regular software job. This is a no-lose situation for OP short term (provided they get paid). If it's not successful, it was doomed anyway. If somehow OP pulls this off, then they saved a dead project. In other words, a resume building opportunity while they keep said resume out in circulation.
If you are two weeks in and don't have the code running locally yet then something is wrong. I guarantee this guy will answer the phone and agree to come in after hours to get you set up if the company offers him $400 for an hour or two of his time. Read the code all you want, if you can't run and interact with the software and debug you won't make any progress. Priority number one should be too get it running. 
I'm .net developer but when I saw 2000% increase, it's clear for me that problem not in framework they use but in their codebase.
I know, but here is not the problem. You are still hosting private sources outside. Even as close source, some enterprises wont allow it.
If (s)he achieves to save this broken project, and I hope for a happy ending, the company could/will keep throwing clunky offer like this one and think that it is acceptable. Nowadays as a developer, opportunities are everywhere.
We are talking about a company who doesn't even use source control and is in deep shit because two people left. I'm guessing they don't know what they are doing at all when it comes to IT and clearly aren't enforcing any logical policies.
You are right. But they could still cause trouble for this.
The whole reflection api.
showModalDialog won't work
Did you step through the code and you are 100% sure that result has a value and SignInManager isn't throwing a null exception?
Yes, I've broken down the process and stepped through everything. It's driving me nuts! The value I'm getting back is SignInStatus.Success which sounds nice but not when you'd like to authenticate on another level. 
In Statup.Auth.cs What are the following set as? // Enable the application to use a cookie to store information for the signed in user // and to use a cookie to temporarily store information about a user logging in with a third party login provider // Configure the sign in cookie app.UseCookieAuthentication(new CookieAuthenticationOptions { AuthenticationType = DefaultAuthenticationTypes.ApplicationCookie, LoginPath = new PathString("/Account/Login"), Provider = new CookieAuthenticationProvider { // Enables the application to validate the security stamp when the user logs in. // This is a security feature which is used when you change a password or add an external login to your account. OnValidateIdentity = SecurityStampValidator.OnValidateIdentity&lt;ApplicationUserManager, ApplicationUser&gt;( validateInterval: TimeSpan.FromMinutes(30), regenerateIdentity: (manager, user) =&gt; user.GenerateUserIdentityAsync(manager)) } }); app.UseExternalSignInCookie(DefaultAuthenticationTypes.ExternalCookie); // Enables the application to temporarily store user information when they are verifying the second factor in the two-factor authentication process. app.UseTwoFactorSignInCookie(DefaultAuthenticationTypes.TwoFactorCookie, TimeSpan.FromMinutes(5)); // Enables the application to remember the second login verification factor such as phone or email. // Once you check this option, your second step of verification during the login process will be remembered on the device where you logged in from. // This is similar to the RememberMe option when you log in. app.UseTwoFactorRememberBrowserCookie(DefaultAuthenticationTypes.TwoFactorRememberBrowserCookie); 
This is great! I'm pretty sure I'll be able to use this. 
pluralsight has good .Net videos
Thanks for the encouragement. :) The very best way to make such a change in a larger system is with baby steps. As you work on new areas, build the data access portions with EF in mind. Only create entities for operations you need. After a few such efforts, you'll notice this becomes easier. If you can do this for about 6 to 8 months, you'll begin to be able change out other pieces more casually. In 2 years, you should be switched more or less completely.
There is a lot out there. PluralSight videos are good as others mentioned. Try SoloLearn as well. https://www.sololearn.com/Course/CSharp/
Never seen sololearn. Thank you.
MEF - https://weblogs.asp.net/ricardoperes/using-mef-in-net-core - https://www.nuget.org/packages/Microsoft.Composition/
MEF involves loading the plugin, which is not the issue. I am properly handling the lifecycle of the assembly by loading each in it's own `AssemblyLoadContext`. My issue is how to distribute the published outputs to other users without having to distribute the entire framework alongside every single individual plugin. Even with MEF, in order to distribute the plugin, the plugin would have to ship with it's entire dependency tree, including many redundant dependencies.
I feel like at this point, Service Fabric is pretty much a no-brainer.
Disk space is cheap ¯\\\_(ツ)_/¯ You could also try just removing those files from the nupkg. It's just a zip file.
Newtonsoft JSON?
I did several implementations of it but in older versions. It was a huge pain to implement and they were very slow to move to MVC due to the complexity of it. Once our projects were done it worked pretty well. 
Exactly the same. Damnit. I thought for sure. I'm about to go through the entire process again. I'm definitely having one of these moments.... http://imgur.com/gallery/dzbQCj4
.NET Core is the stripped down, cross-platform version of .NET. You lose access to COM, Windows OS features, and many libraries that you might need. ASP.NET Core is a reimagined version of ASP.NET MVC/WebAPI. It gets rid of the legacy garbage from ASP.NET such as HttpContext, allowing you to get much better performance. ASP.NET Core can run on .NET Framework (i.e. the full version) or .NET Core. 
Do you need the best possible performance? Then running ASP.NET Core on the .NET Framework is probably the right way to go. It will give you full access to Windows Authentication while still getting the performance boost from ASP.NET Core. Otherwise stick to ASP.NET. It has much better documentation and you'll be able to easily troubleshoot problems. *** Avoid ASP.NET Core on .NET Core unless you need to run on Linux. Long term its probably the right way to go, but it isn't as mature as you'd expect from a .NET stack. 
.net core is the *runtime* Asp.net core is something that runs *on* the runtime. It can also run on the *full* framework* runtime. For all intents and purposes, asp.net core is a library/collection of libraries to help you build websites and HTTP API's on top of the .net core runtime. You can also build console apps on the .net core runtime. If you aren't sure which to use, break it down like this: Do you want to run your stuff on something other than Windows? If so, use asp.net core. Else, use regular Asp.net. *Except for v2.0 currently in preview but when that releases it'll also run on the full framework.
&gt; NuGet will throw the entire framework into the publish output &gt; Is there a way to distribute these plugins without having to ship every single dll Yeah, simple. Don't distribute the exact output of publishing. Just because publish puts a dll in it's output doesn't mean you have to include it in whatever sort of package/zip/download you distribute.
Interesting solution. Honestly, when I posted that clip from start auth.cs file I thought it was intended based on the default configuration.
I run Windows containers on EC2. Pretty cool stuff. 
if you have subscription at pluralsight then check https://www.pluralsight.com/authors/shawn-wildermuth He describes how to build modern web apps using asp.net core and angular
Yeah, 1.1 is almost identical. In my case, I went first with the pluralsight course while looking at asp.net core 1.1 documentation from time to time.
[Microsoft Virtual Academy](https://mva.microsoft.com/) is a good place to start.
Edge means the latest browser. If you look at IE developer tools they give you the compatibility options of 5, 7, 8, 9, 10, Edge. So Edge would be 11.
Was your cluster on prem or Azure hosted?
You may want to hold off of ASP.NET Core for the moment. .NET Core in general is in heavy flux at the moment. I suspect Brain Parasites... er, wait, that's Fluxx, not flux.
Check out my project https://github.com/dodyg/practical-aspnetcore
Yeah there isn't a whole lot of difference between the two. There's more APIs available in 1.1 but that's only good.
Node may be asynchronous, but it is not multi-threaded. It's not hard to imagine a multi-threaded app being able to serve more requests because it has more CPU's available. It's not so much that the code is faster, but that it leverages available hardware more efficiently. 
When you figure out a good resource, let me know. I'm stuck doing legacy repairs using webforms and (shudder) ASP Classic until I can convince my boss to start letting me use MVC.
Not cool :-/
I'm a big fan of Udemy. You can find courses for $10 pretty regularly on there and they tend to be very well done. https://www.udemy.com/user/moshfeghhamedani/ is an instructor who has done some good C# classes that I've been using to improve my skills. He has everything from beginning C# to ASP.Net Core.
Not unless the dev has programmed something in the page to do this (like using the querystring).
That change is unrelated to the framework version. It's the same for 1.0 and 1.1. The problem was that the tooling was always in a preview (even when the runtime and framework was released as RTM), and the preview tooling used project.json. Sometime later they updated their tooling and finally got rid of the abomination that project.json was.
Yeah and it's been in heavy flux for about 2 years
In another comment he makes it sound like he needs it for a new job at a large company, so it's highly unlikely that he'll need .NET Core at all.
Azure hosted.
Also just a heads up. Unless you need to run your stuff on Linux you can still use the new MVC and core stuff while targeting the full. Net framework
No, that's true. But there aren't many .Net core projects in production yet, anyway. And most that are are in 1.X since 2.0 is so new and requires rework to retarget. So it could be valuable to get experience with handling that migration since everyone will need to do it soon.
I just have to convince him to stop stepping over dollars (let me learn and train in MVC) to pick up dimes (continue using outdated methods since our clients are shortsighted)
Unfortunately you can't use the two together. I've been down this road, and I never got it to work. Anything online said the same.
Ugh.. I guess that means the library still has some issues if the primitives don't work correctly as documented. Thanks.
I went from DbUp to the database project so that I only need to write inserts by hand. With DbUp I had to write alters, also.
I would avoid any tools that add their own DSL on top of SQL, force you to execute migrations as compiled code, or lock you into a particular software library or framework. Stick with plain SQL scripts that are executed in sequence by some tool. RoundhousE is pretty good. But I'm partial to my own tool: https://github.com/jdaigle/Horton
Check out TPL Dataflow. It does pretty much exactly what you're trying to do here.
I use Red-gate sql source control. Not cheap, but it does the job very well. http://www.red-gate.com/products/sql-development/sql-source-control/
Liquibase for me
Remembering to run all scripts in all environments is hard. Multiple dev machines, staging, production... we have a setup now where change scripts are run as the application upgrades boot. This keeps all the devs in sync and ensures nothing gets forgotten. Having lived through both ways, this has a lot fewer headaches and confusion. 🙂
Use Mod (% in C#) and just loop through. Suppose you have an collection of your groups called Groups and Num_Players is the number of players then the following will assign a player index to : while (var i=0;i&lt;Num_Players; i++) { Groups[i%Groups.Count()].Add(i); } This is really just basic programming, not really a dotnet question...
I'd add FlywayDB to this list. Nice and simple runner for plain SQL upgrade scripts
Thanks for the response although I tried this prior to asking for assistance. I know the question really isn't language specific. It's more a overall programming questions. If you look at the results from your code it works fine for 2 groups: Group 1: 1, 17, 5, 13, 9 Group 3: 3, 15, 7, 11 But these groups are produced with incorrect results. Group 2: 2, 14, 6, 10 Group 4: 4, 16, 8, 12 When it should look more like this: Group 2: 2, 16, 6, 12 Group 4: 4, 14, 8, 10 player 2 should be with player 16 and player 6 should be with player 12. Likewise 4 should be with 14 and 8 should be with 10 thanks 
This looks very promising! Thanks for the link.
Are you using .net framework or .net core? If using .net framework, you'll need to first convert everything over to .net core in order to make the application cross platform.
Oh, I'm using .NET Core!
Sorry, should probably have mentioned I was using .NET Core. My mid 2011 Macbook Air is having a hard time with VMs, though. What would some alternatives be for developing web applications that I could work with on both OSX and Windows?
Mono works fine. No need to convert everything to .NET Core, especially since support is still lacking.
Haven't worked with Docker before, is it good?
It's a really nice way to contain a whole app's dependencies and make them portable. There should be loads of tutorials for both .NET Core on Mac and Docker. See what you can find :)
Sounds really nice, thanks!
Look into hmac
https://fullstackmark.com/post/10/user-authentication-with-angular-and-asp-net-core 
That would be great!
The way you guess at sounds just like the OAuth 2 password flow to me.
IdentityServer4 runs on .NET core. Pretty easy to use.
In regards to DbUp, I never tried it but I tend to check the commits on a project before actually using it. Sure it's OSS and I could contribute, etc. But seems like DbUp hasn't been updated since 8/16. Is it worth the time researching?
This is what I do. Pass in header of UTC Time then a pipe then HMAC of URL + UTC Time + Users API key. Server does the same HMAC using the passed in UTC Time (assuming it's close to correct time) and compares. If they match you're good, if not reject.
The [Prism](https://github.com/PrismLibrary/Prism) framework allows plugins to specify which other plugins (which includes the core application) they depend on. However, Prism is mainly aimed at writing GUI applications. I also think it hasn't been ported to .NET Core yet.
I don't think Microsoft will make the same mistake again. That said, there are fewer and fewer reasons to stick to the full framework.
Microsoft's engineers have also said the reason they were removing support for full framework was because it was holding them back from some new things they wanted to do. In other words, don't expect support for full framework in ASP.NET Core to last forever. We got a temporary reprieve, not a permanent exemption.
Oh, I see. Phuber commented on it below, but I agree, instead of just adding one player at a time, add the pair: while (var i=0;i&lt;Num_Players/Groups.Count(); i++) { Groups[i%Groups.Count()].Add(i); Groups[i%Groups.Count()].Add(Num_Players-i); } Then you'll have to handle the left overs if Num_Players/Groups.Count() != 0. 
This was linked by /u/FARTINATOR_ and is the best link resource I've seen to date https://github.com/robisim74/AngularSPAWebAPI
Perfect, thanks. Edit: so far, yeah, it seems that ASP.NET MVC is a statically typed implementation of Rails/Django. Quite easy to follow. 
There are several options: IndentityServer4, OpenIddict, IdentityServices or ASOS if you want to go wild.
* The DSL (i.e. "fluent" or "builder" syntax) is a leaky abstraction at best. It doesn't provide much over TSQL. And in some cases it makes terrible default choices that developers rarely pay attention to. * You're coupled to Entity Framework. That makes it harder to use in a heterogenous development environment. * The migrations rely on serialized model state to determine what changes are needed when scaffolding a new migration. But this plays poorly with teams of more than 1 or 2 developers and doesn't play nicely with feature branches.
Duplicating blog comment: ---- When working commercially you need to figure out how much your time is worth. Updates (many many years ago) used to be a day long job. Trying to manually script and change databases was just a complete nightmare. Changes of errors were high. Finally I got my company to purchase red gate's sql compare. While the up front cost was quite high, the time saved was massive. The new update process was: * point at 2 databases * click compare, * click "select all" * click generate script The output was an update script that worked every time with transactions that rolls back if it fails. Not sure what the price is currently (I don't personally pay it) - but assuming it's about $400 and companies time is worth at least $50-100 per hour. It's basically paid for itself in a couple of updates. For my use case - 8 hours down to 5 minutes it paid for itself in a single use. 
&gt; The DSL (i.e. "fluent" or "builder" syntax) is a leaky abstraction at best. It doesn't provide much over TSQL. And in some cases it makes terrible default choices that developers rarely pay attention to. Do you mean the options it picks to create the database and tables? You can customize them if you want, but really it wants you to define your models properly with attributes if you want to fine tune the field sizes and nullable and such. &gt;You're coupled to Entity Framework. Well sure, I don't know why you'd use EF Migrations w/o EF :D &gt;The migrations rely on serialized model state to determine what changes are needed when scaffolding a new migration. But this plays poorly with teams of more than 1 or 2 developers and doesn't play nicely with feature branches. We use it all the time in teams of 6+ including feature branches. What we don't do is have two different teams working on the same project both making DB changes at the same time. But that sounds like a recipe for disaster in any project.
As a rule of thumb, you should never assume a framework or compiler bug. I know they happen, but this is pretty fundamental stuff. 
Can you kindly explain to me what is IOC container? I am still very new in Dot Net... 
Life-cycle issue would be my guess too.
EDIT
OH HEY THANK YOU! I made that method async, and added the await, it finally works! THANKS A LOT! 
[Image](https://imgs.xkcd.com/comics/standards.png) [Mobile](https://m.xkcd.com/927/) **Title:** Standards **Title-text:** Fortunately, the charging one has been solved now that we've all standardized on mini\-USB\. Or is it micro\-USB? Shit\. [Comic Explanation](https://www.explainxkcd.com/wiki/index.php/927#Explanation) **Stats:** This comic has been referenced 4553 times, representing 2.8630% of referenced xkcds. --- ^[xkcd.com](https://www.xkcd.com) ^| ^[xkcd sub](https://www.reddit.com/r/xkcd/) ^| ^[Problems/Bugs?](https://www.reddit.com/r/xkcd_transcriber/) ^| ^[Statistics](http://xkcdref.info/statistics/) ^| ^[Stop Replying](https://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=ignore%20me&amp;message=ignore%20me) ^| ^[Delete](https://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=delete&amp;message=delete%20t1_di6hi3x)
DotNetOpenAuth. The ServiceStack OAuth2 library depends on it, which means that I can't move my project to .net core without losing Google Authentication. It's the only thing holding me back!
I will b
It is indeed confusing. If your team is not in a hurry, you could go for the .NET Standard 2.0 preview already: https://github.com/dotnet/standard/releases I don't know if you can use WCF in .NET Standard, I don't think they built it with that in mind. Correct me if I'm wrong.
Anyone else actually get this to work past a trivial example? We found the documentation is brutally incomplete and outdated at best.
I'm a VB.NET programmer who does mostly asp.net webforms, webAPIs, and some WPF apps here and there... should .NET Standard 2.0 have any relevance to me?
Then can you explain why I am getting the issue I have described? I have provided the code so it's easily reproducible. Just fill in the inner function with a loop that sleeps and logs periodically. Launch the wait with a button. For short tasks it works. For long running tasks, it doesn't. 
Well, for the time being, I'd get used to them. I've never had much of a reason to use them before now, but being able to import .NET 4.5+ features that aren't in .NET Standard is really handy. If you target your library at both .NET Standard and .NET Framework, you can import both regular and Core EF and then just use compiler directives to pick the correct one for each build. If there are enough differences, I'd even suggest duplicating entire classes and wrapping them in the `#IF` block so that you don't have to keep switching constantly in one file.
There are .NET Standard WCF client libraries (https://www.nuget.org/packages/System.ServiceModel.Http/). Servers will continue to need to use full framework.
First, I don't know about the ASP.NET Core. I can't imagine why you can't create ASP.NET Core VB.NET applications... ASP.NET Core, from what I remember, is defined the same as a console application (they both target netcoreapp anyways); it just has some extra modules you install that make it "ASP.NET Core". I can't imagine what would be preventing you from creating one in VB.NET. Hopefully there are some templates out there for VS 2017 or the `dotnet new` cli tool even! .NET Core is more akin to .NET Framework. It's a target for application code; for now, if you only care about Windows development, you don't gain a whole lot with .NET Core (and probably lose a bit more than you'd like). .NET Standard is the "building block" of the entire .NET stack; the standardized APIs that .NET Core and .NET Framework need to implement. .NET Framework builds off of it, .NET Core builds off of it, etc. I always love this graphic: https://msdnshared.blob.core.windows.net/media/2016/09/dotnet-tomorrow.png. .NET Standard *should* be what you strive to target for shared code. For example, my company heavily uses WCF services, with a bare-bones contract layer with no implementation. Ideally, the contracts should target .NET Standard so any project type can utilize them. The implementation of those contracts can be anything, but for us, they're all .NET Framework, as we know we'll be using Windows servers for awhile yet and a lot of our framework code base still is on .NET Framework. I personally think in an ideal world, .NET Core will replace .NET Framework. But, given the rate of change of enterprise code, and a lot of the legacy Windows behavior that .NET Framework supports, I can't imagine that happening very soon. For now, the both exist side by side. .NET Core can now branch away from .NET Framework, and create new APIs that might not ever see .NET Framework. And, if they do find their way into .NET Framework, you'll probably see those APIs standardized in future iterations of .NET Standard, to support both. Hopefully that makes sense! And apologies for the mini-blog post. **Edit** Swapped out image link.
.net core is the *runtime*, the thing that actually takes the IL you compiled and through one means or another makes it actually *do* something, communicating with the underlying OS as necessary. It's purpose was to be completely cross-platform, so it doesn't know of anything specific to Windows any more - that's the decoupling from the "full" framework, which contains all sorts of windows-specific functionality. The *netstandard* comprises of all the bits and pieces that the .net core runtime **and other runtimes** *like* the full framework can actually use. The net result is that if you make a library that's netstandard compliant, it should be compatible with many different runtimes and future runtimes. With the "old" way of doing this, via PCL libraries, if a new runtime (or even just a new version of a runtime) came out, you had to explicitly update your library to target this new PCL profile. Remember, there are other runtimes beyond the full framework and core. There's Mono, there's also Xamarin (which is sort of mono but a different incarnation), there's Unity and you get the idea. As for yourself: Yes, **all of this** is relevant to you as a VB programmer. You can use VB with any of this stuff. It's a bit of a second-class citizen in that all the docs and examples tend to be in C#, but .net core (and all of its supporting libraries) do support VB.net and even F# (Which is very much a third-class citizen). As a Vb citizen, you can consume netstandard packages. The language you use doesn't matter, it all gets compiled to IL in the end and it's a two-way street.
Yup. Do what the docs refer to as self-contained deployments and target the runtimes you want: - `dotnet publish -c Release -r linux-x64` - `dotnet publish -c Release -r osx.10.11-x64` - `dotnet publish -c Release -r win10-x64` See https://docs.microsoft.com/en-us/dotnet/core/deploying/deploy-with-cli for more information.
I think most people would suggest [Dapper](https://github.com/StackExchange/Dapper) Personally my company uses [Limebean](https://nick-lucas.github.io/LimeBean/) for this purpose They may be closer to raw SQL than you're imagining, though. Certainly Limebean is based around SQL shards, and removing the need for classic models. I don't have a lot of experience with Dapper.
Excellent! Thank you very much.
Thanks that worked. But I have another problem
I second Dapper. Use the abstract classes in System.Data.Common, such as DbConnection to decouple yourself from SQL Server. Dapper (and I'm sure many other micro-ORMs I'm less familiar with) will work with pretty much any classes that implement those abstractions.
You have the following options. 1) Make use of views and stored procedures -- SQL Serve, MySQL, PostgreSQL, Oracle support these -- this still gets you into the issue that you have to support the views and stored procedures in all the different version. However, if you don't go to crazy in your SQL you can copy the views and stored procedures from database to database. 2) Entity Framework supports FromSql (I am using core) EmployeeListModel.Employees = DBODbContext.Employee .FromSql("Select * from Employee WHERE Name like '%Test%'") .OrderBy("FirstName", "Desc") .Skip(25) .Take(50).ToList(); Since you are generating the SQL statement in code you have a lot of options. I make use of an helper class that I call like this SQL_Statement _SQLStatement = DBODbContext.GetSearchString&lt;DBOEntities.Employee&gt;( "Default", fixedSearchClause: "CompanyID='" + base.CompanyID + "'", search: EmployeeListModel.Search); I keep all my Loader class in a separate assembly -- which gives me the option to create an assembly per database. Plus class overriding so that you only override the Loader class if the sql syntax is different for a particular database. I keep very little SQL in my code it is -- I find it messy and hard to maintain -- even with the above methods I make extensive use of views and some stored procedures. 
linqtodb might be a good option
You are looking for a type of library called Micro ORMs
NHibernate, Dapper, etc. In twenty years I've never worked at a company that migrated away from the database without also migrating the code too. It's an abstraction that's not worth the cost. 
Came here to say Dapper.
If you are looking for something like Dapper but with good abstraction from concrete SQL dialect / ADO.NET connector take a look to the https://github.com/nreco/data , it offers abstract database queries (Query class) that may be composed dynamically and provides mechanism for encapsulating all db-specific SQL queries as application-level dataviews (they are accessed as read-only tables). 
Noted. But the features of that are not very compelling to us, so we're fine.
https://github.com/sapiens/SqlFu
FWIW there are far more differences than just 'subtle', they can be very substantial. Postgres for example has a fantastic amount of pluggable languages for handling pl/sql / non query based logic, for example it has the V8 JavaScript engine built in, far superior to TSQL. If you've just got a dumb backend the difference do narrow more but it really depends on what your require the server to do
These days I'm tending to prefer the JSWT model where you encrypt the session details into a cookie rather than needing to store and keep track of sessions at the back, basically the OAuth style
I agree that PetaPoco is a great solution. It also has a t4 template that comes with it to easily generate pocos from your db tables. Its helpers for building sql are quite handy too. PetaPoco is definitely my goto ORM/micro-ORM
NPoco would be a better option which is an active fork of petapoco. 
I was going to suggest OrmLite too. Nice orm as well as NPoco 
Thanks. I didn't know about this.
&gt; fixedSearchClause: "CompanyID='" + base.CompanyID + "'", That looks suspiciously like sql injection.
Just a thought - you could download one of the ASP.NET Boilerplate templates (https://aspnetboilerplate.com) and see how they structure multi-tenancy. In your case, each record center would be separate tenant.
Details such as expiration and/or roles in the cookie can sort of work if those things NEVER change. But this is often not the case, like if you're building an application where an admin might change a users roles, or disables a user account. If you never check a back end store on each request these changes aren't enforced. JSWT is also pretty heavy, not really a fan of it personally.
As a side note if you have some spare time I'd recommend installing VS2017 if you're going to be doing C# stuff in the future. Lots of features you're missing out on. You can install the Community edition for personal use for free. As for your main question, if there is no layer of abstraction between the form and the database that will definitely make things more complicated. Without specifics I can't really suggest a course of action.
This seems to be the way to go but if restrictions won't allow you could do something simple where the record center selection just allows anonymous similar to the login page and you just set a session flag then redirect to the main page... since they are not authenticated it will ask them to login if the were not already. You could add a check at the login page to check the flag and redirect to session selection if not set Edit: missed the fact you said login to the center and not just select it . 
&gt; Is there something I'm missing? Yeah, any useful details that might help someone identify your issue.
You detailed everything except the part that is failing. What import library are you using etc. Do you not have logging enabled ? This should be a simple thing. Worst case scenario wrap the import in a try catch and write the error to a file manually or the event log or any number of other ways 
Hi, I did work with it a little bit, you can check out my sample project here https://github.com/spetz/graphql-sample Basically, GraphQL is merely an abstraction over the data access layer which provides the specific query language for accessing the API resources and it takes a lot of work to implement it properly. I think that this repository is the only one available for the .NET Core.
We just expire and refresh the token if the backend has had any updates, no biggie 
How do you typically handle database deployment and schema changes with micro-ORMs?
 Sounds like your app might not have the needed permissions on the database. 
I use .sqlproj schemas, generate scripts, and deploy those scripts to the database. You could automate this and have them run on deployment. But I have too many apps for it to be feasible to have them run whenever the first app is deployed. My changes must be done carefully, as must most in a distributed system. Views are helpful.
Which seems fine if you're using SQL Server but I wonder what the options are for Postgres and MySQL/MariaDB.
Hm. Good question. I believe https://github.com/fluentmigrator/fluentmigrator is the go-to choice, though I've not used it myself.
Just some general advice; Implement logging and some try/catch blocks so you can isolate which parts of your code are failing in prod. You should also look at a library like ELMAH as well. It well help you find unhandled exceptions which I'm sure were being thrown in this case. Finally, you should look at how to avoid SQL injection attacks in .net. Forming out your SQL strings like that is not a good idea. 
Hi guys, There are 2 mature graphql .net libs for now: https://github.com/ckimes89/graphql-net and https://github.com/graphql-dotnet/graphql-dotnet I tried to use one of them based on NReco.Data (lightweight db access component) and component which is developed by Joemcbride (https://github.com/graphql-dotnet/graphql-dotnet). You feel free to check it here - https://github.com/nreco/data/tree/master/examples/SqliteDemo.GraphQLApi ready to use: fetch data (without subquery) based on database schema. BTW, mutation process (update data) can be implemented soon. Hopefully, it might help you.
Oh and do a basic understanding of the innerworking of the main databases solutions before learning the design patterns, it's more interesting in my opinion. Design patterns are to be use as inspirational material, not guides.
I guess focus on s.o.l.i.d, unit testing and profiling as a start Maybe also read the excellent docker docs from Microsoft particularly the architecture vision section: https://aka.ms/microservicesebook
You aren't going to tell the OP your concerns?
Thanks for the info. I hope I didn't come across as trying to be rude, I truly had never heard of these components. And you're correct, I don't manage/deal with lots of "things" I would need to consolidate logs. 
So much of our development these days is done by default in web apps, that I think we're suffering from golden hammer syndrome in this industry. That said, there is PLENTY of wiggle room for you to stretch in new directions. You could: - Firstly, you mentioned databases. If you don't have experience here, this is definitely a gap. If you haven't learned about databases yet, then I would work on that. Learn about normalization and database design, the differences between OLTP and OLAP, star schemas, and implement designs on both. Learn about Entity Framework and code first vs. model first design. Learn about stored procedures, practice writing them, and know when to use and not to use them. - Edit: Oh, and then there's no-SQL options like MongoDB, Hadoop/HBase, Cassandra, and Redis. Pick one, learn it, and then understand when to stay with a RDBMS vs. not. - Write a game in C# for mobile or desktop. Games development can be punishing, but it you start with a basic 2D style arcade game with patternized behaviors, you could work your way up from there and implement 3D graphics, use A* for unit movements to get away from hard-coded patterns, etc. Heck, we need another Terraria clone on Steam, right? :D - Implement a Windows service. Back the service with a finite state machine design to ensure it can never become "stuck". Build on that to implement a generic "sockets server" and implement a popular protocol; e.g. HTTP or maybe even invent your own. Combine this with the Windows service code and you could develop a robust server that's always on when Windows starts. Oh, be sure your state machine can handle threading. ::evil grin:: I mean, that doesn't even touch things like devops on Azure, embedded development/IoT, machine learning, and the 6 zillion other things you could learn about. All that aside, you could just go work on a Master's MS (or BS if you haven't already) degree as well. I've never regretted doing that either and beyond doing the type of exploration described above, it's one of the best ways to deepen your skills.
Just build apps in MVC or Web API until you feel confident. Stretch yourself a little each time. Don't mess with Core until you feel super confident, IMO - it's moving too much.
&gt;It is important to notice that Tuple is a structure, ie. value type and that it’s fields are public and mutable. Nope, Tuple is a reference type. The new ValueType introduced together with the new C# 7 tuple return syntax is a value type.
It sounds like you are already OK on web related .net and I would recommend staying in that area. Win forms is a step backwards IMO. If you really want to pick up more I would recommend some/one of the modern JavaScript frameworks (can use mvc still for backend) That said I am a big fan of the fundamentals so I would focus more on learning major design patterns .. visitor , composite etc. These are things that apply to your .net code no matter what stack you build on. Pluralsight has quite a few topics on patterns and the ones I watched previously were decent.
Sounds like you already are a .NET developer, 8 years is a considerable amount of experience. Don't worry about feeling like you don't know everything, no one does. And web development is still software development.
This is only available internally, we also utilize a 2 step authentication process. I do have try/catch in every logic block, so much easier to trouble shoot than general server puke errors
This is only available internally, we also utilize a 2 step authentication process. I do have try/catch in every logic block, so much easier to trouble shoot than general server puke errors 
I didn't see a try catch in the sample you provided? If you are catching at a higher level than that, it's too high. You should be doing a try on the execute part of this so you can capture the specific sql command error. I also hope you're logging and monitoring those errors some how so you can see them. 
The try/catch is around this, I just didn't copy it when I was providing the code. I'll remember that next time. We are logging errors. When I started here at this company they weren't doing any error handling, and that was a very dark time. I've only ever working on internal web applications, so I am interested in what you are suggesting to avoid SQL Injections... This is an example of the normal way I do it: Using excel_con As New OleDbConnection(connString) excel_con.Open() Dim sheet1 As String = excel_con.GetOleDbSchemaTable(OleDbSchemaGuid.Tables, Nothing).Rows(0)("TABLE_NAME").ToString() Dim dtExcelData As New DataTable() dtExcelData.Columns.AddRange(New DataColumn(17) {New DataColumn("Column", GetType(Integer)), New DataColumn("Column", GetType(Integer)), New DataColumn("Column", GetType(String)), New DataColumn("Column", GetType(Integer)), New DataColumn("Column", GetType(String))}) Using oda As New OleDbDataAdapter((Convert.ToString("SELECT * FROM [") &amp; sheet1) + "]", excel_con) oda.Fill(dtExcelData) End Using excel_con.Close() Using con As New SqlConnection(ConnStr) Using sqlBulkCopy As New SqlBulkCopy(con) 'Set the database table name sqlBulkCopy.DestinationTableName = "dbo.MyTable" sqlBulkCopy.ColumnMappings.Add("Column", "Column") sqlBulkCopy.ColumnMappings.Add("Column", "Column") sqlBulkCopy.ColumnMappings.Add("Column", "Column") sqlBulkCopy.ColumnMappings.Add("Column", "Column") sqlBulkCopy.ColumnMappings.Add("Column", "Column") con.Open() sqlBulkCopy.WriteToServer(dtExcelData) con.Close() End Using End Using
Have you read the **.NET Framework Design Guidelines** yet? That is probably the most important book for understanding why .NET was designed like it was.
Practice, lots of practice. Get with a group/community/knowledge people that are understanding and willing to help in a reasonable manner. Read books on .net and have patience with yourself. And seriously, practice. 
It all depends on what kind of developer you want to be. Most developers are business devs. Once a business dev understands the basics, which it sounds like you do, I think they are best served by understanding databases, data models, and the entire data layer in .Net. Regardless of the type of developer you are, the ability to unit and completeness test your own work will set you apart. Sounds crazy, but most developers really, really suck at testing. Never try to do the most, always try to do the best.
Deleting records does not reseed a primary key incrementing. In SQL u can truncate a table to reseed or execute a specific command to reseed. I'm not familiar with SQLite but I'd imagine it's similar. If needed EF context will let you execute raw SQL and you could manually do it. -- Why do you want to reseed though? This is not a trivial thing with relational data as references are added. ---- If I could be so bold. I PERSONALLY hate the lowercase public property in your context.
Yes, I'm using the "angular" spa template. It would have been nice if it accepted the --auth parameter and added the stuff. I would feel a bit silly spending time copying the stuff from the plain mvc template if there was a way to get it enabled with a parameter. The tip is good, its what I was thinking of doing :) 
It's a general tuple construction and (non-pattern) deconstruction syntax, not just return. 
You could use a migration to drop and recreate the table.
Yeah honestly I'm sure there are others out there but always out of date etc. I normally will just do like you mentioned start a normal project and copy paste bits. You can create your own clean template after you are done the 1 time. The good thing is you are looking for the auth which is almost trivial to wire up in a few lines. 
&gt; Why do you want to reseed though? Don't know it doesn't look right for me. Maybe because I don't know if the unused keys can be reused if I somehow manage to use the whole range (of a 64bit Integer?). The lowercase property was the leftover from the real name of my dataset which of course is CamelCased ;)
 If that is a realistic concern I have dealt with some fairly big data and haven't run into it yet but I would not feel qualified to speak in detail on how to go about planning for that. realistically it would probably not be actively happening from my site though . 
I think C# is going in the right direction virtually all of the time, but I have a problem with this formation: private static (string, string, int) GetNameBlogAndAge() { return (name: 'Nikola', blog: 'RubiksCode', age: 30); } Now, we have to manually parse the implementation to write the client code. This goes against every good-coding-practice gene in my body. Sure, some might suggest that the IDE will help you out, but not so in the case of DLLs. - What about in inheritance? Do developers need to figure out the naming convention of every other method implementation, too? - Or is there another language exception to cover that case? - Or delegates? Edit: A grammar or two.
I haven't used it personally, but this blog post might be useful to you http://asp.net-hacker.rocks/2017/05/29/graphql-and-aspnetcore.html
It's a private method. You'll probably be looking at it while writing the client code. Now if we were talking about a public method... yea I don't think I like that. 
Right. Actually you can't use 4.6.2 either, but there isn't much new in that either. 
&gt; but no so in the case of DLLs. If you think about this, there is no problem. There's no reason why this syntax: private static (string, string, int) GetNameBlogAndAge() { return (name: 'Nikola', blog: 'RubiksCode', age: 30); } Would compile to IL that is any different than what you'd get from this syntax: private static (string name, string blog, int age) GetNameBlogAndAge() { return (name: 'Nikola', blog: 'RubiksCode', age: 30); } The compiler will just use the implicit definition in the return statement to define the return type, 'lowering' the return statement, and then compile using the now-explicit return declaration. After compilation, both are indistinguishable.
You're not supposed to parse the implementation. If a method returns a ValueTuple, it's either private and you know the names of the properties, or you rely on documentation and automatically deconstruct it into a new ValueTuple where property names are known.
There is a difference-- I can see the names in version 2. With version 1, I can access the variable `name` in the tuple thus: var nameBlogAge = GetNameBlogAndAge(); Console.WriteLine("Name is: {0}", nameBlogAge.name); To do this, I need to know the name. To know the name, I gotta look at the code or rely on the IDE, which also led me to ask about virtual methods, where the names could be different.
The way I keep sane is by dividing C# features into "stuff for API/library design" and "stuff for internal use only". 
My suggestion is that you look into OAuth/ConnectID. You would create an "STS" which authenticates the actual user. Then every service that the user then has to access will be considered a resource. Each resource checks the token issued by the STS to see if the user has access. And you set up roles/claims to authorize the user allowed access or tasks on each resource. Think single sign in. This is a really brief description but check it out some more. 
I have a feeling you'd get a compiler error in that circumstance. The names of the members of the return type tuple are part of the definition of the return type - and the return type of the method can't change depending on how it runs, it has to be known at compile time. You can't have a method that returns `Tuple { name, blog, age }` or `Tuple { name, website, score }` depending on how it runs
What happens when you compile this code: public static void Math() { int count = GetCount(); var cache = count; ... } The type of `cache` is known at compile time, even though it's not explicitly stated - it's implied. How can the compiler do that? It can infer the type from how it's initialized. How about the following code? public static void Logic() { var thing = new { Name = "Antiduh", Age = 999 }; ... } Same situation. The type is implicitly defined, but it's strongly typed and the members of the type are 'Name' and 'Age', even though the compiler doesn't know that until it looks at how it's used. Alright, so what's the difference between the following two snippets? private static (string, string, int) GetNameBlogAndAge() { return (name: 'Nikola', blog: 'RubiksCode', age: 30); } private static (string name, string blog, int age) GetNameBlogAndAge() { return (name: 'Nikola', blog: 'RubiksCode', age: 30); } Nothing. They both return a tuple with properties `{'name', 'blog', 'age' }`. Once that method is compiled, the two implementations are indistinguishable. They will have the same IL. Any object browser will see exactly the same definition: `public static (string name, string blog, int age) GetNameBlogAndAge()`. 
I think you dropped a "not".
Oh? And how do I find out about their names? I have to look at the implementation. Your example does not support your argument. 
&gt; Oh? And how do I find out about their names? I have to look at the implementation. If you're looking at the source code, yeah, you the human won't know the return type without looking for return statements. From a code style choice, it's not great. However, the externally visible behavior of the declarations is identical. How do you know what the return type is for `IPAddress.Parse()`? You look at the documentation, or let the IDE tell you, or look in an object browser, all 3 of which will say `IPAddress`. How do you know what the return type is for (either) `GetNameBlogAndAge()`? You look at the documentation, or let the IDE tell you, or look in an object browser, all 3 of which will say `Tuple { string name, string blog, int age }`. I gotta be honest, this is a whole lot of hand wringing over what is ultimately nothing. Both syntaxes are identical to callers. The only complication is that, unaided, you'll have to find a return statement to find out what the property names are when looking through the source code. I really don't see what the problem is.
thanks, fixed. 
&gt; Names of properties are part of type information, because C# is a strongly-typed language (and this aspect preserves that facet of the language). This is not true for the new `ValueTuple`. The "name" information is not part of the type. It's actually an attribute attached to the method that returns the tuple - and during compile time the `name` will be compiled to `Item1`.
Yeah, but I am save now anyways... Setting it back via direct SQL command execution. Thanks to all for your help.
I can't remember for sure but isn't there an issue of using Thread.Sleep() in a Task as it blocks the thread, await Task.Delay() allows the Thread to be reused during the delay.
Guidelines are to not have tuples as your public api surface.
I am surprised recruiters know how to breathe.
It doesn't have to be private. The compiler adds attributes with the names, so it also works when linking against foreign DLLs.
That's not true. The tuple has fields called Item1, Item2 and Item3 in both cases, since it's the exact same tuple type. The names comes from metadata that the compiler emits.
The names are in the source code.
&gt; The names of the members of the return type tuple are part of the definition of the return type Yeah but not for tuples, which is why it's a bit more subtle.
Thanks
The best recruiters should know the difference broadly speaking, but they are diamonds in the rough. The majority of recruiters are playing a numbers game. Getting the word out far and wide is most important for them and they think it's going to be effective. I get Business Analyst opportunities sent to me on LinkedIn every month despite the fact I haven't had a BA role since I started my career nearly a decade ago and I'm now quite senior. From their POV they lose nothing by sending you an email about an opportunity for which you aren't a fit. I'd be more pissed off if I was the client and they handed me a bunch of candidates who weren't a fit. As a candidate, I fully expect the run around from recruiters, they are just the worst. 
They don't, that's why there's so much turnover.
&gt; recruiter is that specialized you probably aren't getting contacted unless it's one of the big companies and even that might be I might just say yes to all meetings in the future to waste their time.
From my experience recruiters were just some kids out of college who were repeating buzz words without knowing what they mean or how they relate to the job. That's when I started to pack my resume with buzz words even if they were redundant (spell out acronyms, etc.) - also helps with computerized word searches.
I haven't really looked at the server-side rendering part, do you know where I could find an article or documentation about how to control it? How to disable it? The thing I'm prototyping probably won't get indexed, but it does need to render properly. 
I suspect this pays in to the skills shortage, if you're paid by the positions you fill and you can't find a BE Dev, try the next best thing
They should have enough knowledge so that they can read a language or requirement and be able to beneficially (if just briefly) research it before they start talking crap. Ever see the ones who ask for 5 years experience in technologies made &lt; 2 years ago?
Advertising Rider is interesting given it will be a competitor to Visual Studio. Unless there's a deal in the making.. 
My vote is for elasticsearch. It's designed for this application. You get complex query building, first party libraries, and speed. Anyone have any arguments against elasticsearch? 
Microsoft cares more about getting people involved in their modern dev ecosystem, rather than not acknowledging competitors products - like Rider, iOS, Android, AWS, Google Cloud - all of which are mentioned on the chart.
Didn't see a way. I work for Microsoft, I'll see if I can find anything internal.
Always a goto: https://github.com/jbogard/ContosoUniversity
Kinda sad they didnt put Prism in there. 
I'm also interested. Maybe the DX guys will have some budget left to put it into print.
I think the newest version of .net will be popular for most of the points he raises, but we just need to see less dramatic changes before that can happen
I think the concept you are looking for is "bootstrapping" Imagine you have the following structure: * Data.Contracts (where all the interfaces live for your data layer) * Data (where all the concrete implementations live for you data layer) * Business.Contracts ( interfaces for your business layer) * Business (concrete implementations for your business layer) * UI (the UI layer, can be whatever, wpf, mvc, xamarin) so i think your question is, how do the business and Data dlls get their IOC containers going when the UI starts? (if that's not your question i did all this for nothing :p ) you create a "bootstrap" library for each level, so you'd add a: * Data.Bootstrap * Business.Bootstrap each of those bootstrap dll's have some static "IOCContainer" class with an initialize method, and when called, it handles all the IOC wiring up to map the contracts to the concrete implementations AND call the bootstrap initialize on the next level down. So, in your UI startup you will call Business.Bootstrap.IOCContainer.Initialize(); the Business.Bootstrap would do its IOC startup then it would call Data.Bootstrap.Initialize(); the reason you want a separate bootstrap library is because in a perfect world, the interfaces do not live in the same library as your concrete implementations, so your UI does not have a reference to Business, it only has a reference to Business.Contracts and there is no way business.contracts could startup the IOC container. Sometimes that complete separation does not happen and you just have Data, Business, and UI with contracts and implementations in the same library. in that case you could still use a bootstrap library (to prepare for future separation of contracts) or simply put the IOCContainer.Initialize in each library. 
What makes you think you are a bad .net developer? with 8 years experience I would focus on those things first. For most development, it is the core skills that translate across all types of development that makes the difference between a junior, or a hobbyist coder and a great software developer. That being said I would focus on things like the SOLID principals TDD and other design principals rather than learning any one technology in .NET. From my experience knowing the merits of these principals, and using them to various degrees will make you a better developer in any environment whether its .NET or something else and while technologies come and go, even if new design principals come and shake things up - if you are using something well known and with a good foundation your code will make sense and be easier to maintain for everyone including yourself.
My biggest problem with .Equals is when the string is null, as it will give you a runtime error compared to ==
Should recruiters be in the same open space as developers ? After a few weeks of curiosity, they get the basics. I work in a small company, so the HR is the CM too, and work on the same desk than the developers. They don't need to have proper coding knowledge, be exposed everyday to coders is enough to understand the ecosystem and the terminology. Otherwise, t's part of the job in my mind, if you meet clueless recruiters, you should make them aware that they need better training. 
I think it's worth mentioning that these cannot just be swapped out when you have overridden operators as they truly are different 
Yeah a little. And used a bucket load of ink trying to print.
Haven't tried Rider. Will try it as a I have the JB package for a bit longer. Azure Functions would be cool. Love those things. :-)
Like how ASP.NET Core is natively supported and integrated into OpenShift for example? I can't think of a better example of OSS cloud that you can run anywhere!
The purpose or intent is irrelevant if good is gained. I have not been forced or pushed in any way to use azure for a single project.
No, I mean the last month build event conversations where people get mad about naming and numbering .net versions and frameworks. I don't think the poster will help it anyway.
Ah! Thanks for the clarification. Wasn't sure.
&gt; if you have use-cases that aren't near the azure use-cases, you're going to end up being a second class citizen Examples? I can't think of any.
The article highlights a lot of things but I believe what I took from the article was while features and other stuff was looked at the fact is that azure had higher revenue made it #1. So awarding it #1 in revenue is not the same as saying it's better.
This article reads like it was written by Microsoft's marketing department. This looks like an example of native advertising. 
Embedded? I mean, you can run VMs in Azure so basically everything is a use case.
I've always wondered - *if* Microsoft went ahead with Core without open sourcing it (and kept it fully in-house), would we have even heard about it yet? Is the churn relatively normal for a MS project and it looks dangerous to us because this is the first time we can "see how the sausage is made"?
&gt; You can't think of any situations in which you'd want to run backends not on Azure? That's not what you said. You said &gt;if you have use-cases that aren't near the azure use-cases, you're going to end up being a second class citizen 
yes VS solution so basic example myproj.web myproj.service myproj.repository ill check out autofac, but yeah like you say, they all do pretty much the same thing
&gt; You can't think of any situations in which you'd want to run backends not on Azure? No, I can't think of any situations in which .Net makes you a second class citizen if you are not on Azure. Windows OS, sure. Azure, no.
`IisTrySkipIisCustomErrors` is one of those weird things that I've used over the years and have accepted as being an annoying requirement on most sites I deal with. I mean who really wants to show IIS errors in a production site, and why do you have to bake this flag into your code and not have it solely in the web.config (or better yet, the system-level machine.config)? Anyway, I like Rick Stahl's articles. A lot of them seem to target developers who haven't picked up on some of those 'subtle' nuances of developing sites in .NET. His articles are well-written and often saved as useful information for developers, new and seasoned alike.
Disagree. The Azure dashboard is stunning...so many services managed in one place. The developer experience with Visual Studio and the open source framework offerings integrate so nicely w/ Docker, .NET Core, etc. When you say that AWS is more stable, what do you mean? Both platforms offer 3-nines uptime, and both are based on mature techolgies (as opposed to the lagging Salesforce for instance). 
You should tell your friend to pick up some other languages. There isn't much in a language like VB to be an "expert" in. .Net Framework, sure. VB language, nope. Then they'll have relevant, marketable skills for years, if not decades, to come (assuming their expertise extends to the framework and not just the language). I have a friend some would consider a legitimate -expert- in .Net. He's been working in the platform for 15 years, a employed professional developer for near 20 years, and a hobbyist developer for near 30 years. He's spent a fair amount of time working in other platforms, but couldn't be considered an expert. He makes great money, at a company with great growth year after year. His raises average $15k a year. He's the CTO and lead engineer. He has applications handling millions of requests running on bare metal boxes, AWS, and Azure. He would say .Net is well suited for a large variety of application types and runtime environments. Not all, but the driving factor for platform choice is almost always going to be (or should be) what your team is good with. There are rare scenarios where a platform offers advantages unique to the platform, but they are rare, and even rarer are applications where those advantages are significant enough to outweigh where your team's strongest skills lie. He thinks it would be patently absurd to dump a stack as universal as .Net or Java after acquiring years and years of experience in it.. just because.. what? You didn't actually offer any tangible reason. I am my friend.
https://msdn.microsoft.com/en-us/library/system.string.isnullorwhitespace(v=vs.110).aspx
unfortunatly thats still not an ntier solution, its all in one project
If only the Azure portal wasn't so painfully bad. I'm a dotnet developer, Microsoft has been doing a ton of stuff right lately, but that portal is an abomination.
Maybe I explained it badly. The methods on the String/string object are fine, and are safe. It is the following use that I object to: string foo = null; if(foo.Equals("bar"))
i recently had to code this. use c# with epplus linq to excel extension.
Ah I misread it! 
Pick up ASP.Net Core (not ASP.NET MVC Core just as yet). It's a brand new architecture and absolutely marvelous. Check out my project https://github.com/dodyg/practical-aspnetcore.
Works fine for me, especially on mobile devices
Im forced into VB, but I did end up using Epplus, wow is that a life saver
If you read any hate in my comment, that's all on you. I like Microsoft just fine. Do you know what native advertising is?
Thanks I do now. I googled it. Apologies for the sarcasm in my post. I read your response as trolling. I too love what Microsoft are doing and I guess I've seen one troll remark too many. I deserved the down vote. 
To add to this question: Is a repository pattern overkill when using entity framework? Should there just be services that call entity framework since it's already using the repository pattern?
It sounds fine. What are the main concerns? The repository pattern gets a lot of flack sometimes due to things like web clustering and improper cache implementations but pulling those common queries out of code and into an injectable service is definitely better for maintenance. 
Works just fine on an old iPhone 6. Not even a 6s, but a 6. What are you running, Android?
Yeah android with Chrome. Cyanogen mod on xiaomi mi4c. Never had this issue before seem weird
My main issue is that this seems like a lot of files to keep up with just to allow for a .Select. I mean I could have the repository always include everything through the EF navigation. And then the service could have specific methods that would pare out the stuff I didn't want, but I don't know if I'm then losing some of the abstraction between the service and the repo.
ok i am also using iphone 6s was able to read it on it, not sure about android
It just comes down to value and long term strategy for the project. Number of files isn't something I concern myself with from a design point of view. Controller/API gets request and uses an injected service to call some method like CreateShipment the service has access to one or more repositories via injection to do the work. This allows you to use the same consistent process over and over even from different places. Edit: when I say repository it could very well be the db context. layers beyond that are up to requirements 
It's not superb, but it's not horrible. Application Gateway change application takes WAY too long though.
What about having a service layer using EF as the repository? Then it's easily testable and you're keeping DB logic out of controllers.
Highly recommend not falling down the repository wormhole. Cleaning up IFooRepo -&gt; IFooService -&gt; IFooController years later isn't enjoyable for anyone. Controller / Model sourced BL with shared logic pushed out to Services is perfectly fine to start. The code smells will bother you long before you need a repository, at which point you can look into using something like MediatR, Microservices, whatever instead. The whole "you can swap out the database implementation!" argument is cargo-cult nonsense that keeps getting passed around without any serious thought put into how or why. I've yet to see any company that touted that as a reason actually do it and even if they did I can't imagine a repository makes it any less complicated. Quit worrying about some extreme hypothetical situation and worry about making your application testable first, in which case there are far more applicable and OO patterns (like the mediator pattern) than a repository.
Isn't that essentially the same thing? To clarify what I'm suggesting, I like to create a separate project/DLL and only that DLL knows about Entity Framework. It implements some interface defined in my core project and IoC/DI wires it all up at runtime. No way can a database model pollute anything outside the DAL, then. 
Anything is possible with reflection, but it'll cost ya.
That's pretty much the project layout I would use for any new project, just with slightly different names. For example, I'd call it a DAL (Data Access Layer) instead of "Services", which is a little generic for my liking. As for the services layer itself, there's a lot of abstraction going on but it looks like it's all [just wrappers for CRUD operations](https://github.com/VolvoxCompany/Volvox.Reviews/blob/master/Volvox.Reviews/src/Volvox.Reviews.Service/Common/IEntityService.cs) - which *looks* clean on paper and works great for simple operations, but in my experience you don't get much benefit from this and the second you end up having to manage some related data, you end up making multiple calls - which will lead to issues. So take for example a scenario where you need to update two entities - for example, a customer placing an order for a product is going to: * Add an order to the system (i.e. create an "Order" entity) * Add the order to the customer's Orders table (Meaning we need to Amend either the Customer table, the CustomerOrders table or both, depending on your design) * Possibly other stuff, but we'll keep it simple. Under the current deisgn, you're going to need both your OrdersServce and your CustomerService. You're going to need to Add an order and then update the Customer - but if one of those operations fails, you'll end up with a dangling order or you'll end up with a customer that's got some null order or any other number of fun things. Even if it works, you've just made two completely separate database calls - halving your performance. Scale that up and it doesn't really work. Instead, if you created an interface with methods like AddCustomerOrder(int customerId, Order newOrder); you can do it in one call, making it nice and atomic. Does that make any sense? Still, I don't want you to feel like I'm shitting on the design you've linked, I can tell you for free that it's a good design with good practices and better than a lot of stuff I have personally worked on. 
Yes that makes a lot of sense thank you. I see exactly what you're saying because you can have both of the queries you need in AddCustomerOrder() then do the one database call using EF as Unit of Work.
It is an application developed elsewhere with no contact with the developers unfortunately. If it was developed in house it is easy to ask for some additional name attributes for the sake of better testing. 
[removed]
This is the approach I've used, more or less.
indeed. Should have specified String.Equals(String) to avoid any confusion.
Have you looked into log4net?
Thaaaaaat said, though, do you really want these things for logging? Or do you want them for metrics? Logging and metrics should be pretty different, and using logging for metrics will become overwhelming pretty quickly. 
We don't name our controls in xaml at my shop. Since very *very* few times in code do we have to refer to a control by its name, we just leave them off entirely.
Honestly I would not recommend going this route and here is why: * You are essentially adding another layer of "dependency management" by creating a new project to hold only the "bootstrapping" logic. * You are complicating the design and management of your overall application that you will most likely not benefit much from especially as your application grows to multiple "services" as you add more functionality. * Your testing will become much harder and you will have to try and figure out how to best manage circular references between projects especially if you break apart all of your "...Contracts" projects. Your goal should always be on the side of simplicity. I would start with the following: * Create a single "&lt;AppName&gt;.Contracts" project to hold you interfaces, common model classes, shared helper utilities * Change the default namespace to remove ".Contracts" from it * Create a separate folder for things like interfaces, configuration, utilities (this will help with organization and using of those classes) * Stay as close to the basic implementation of the IoC frameworks for how the load the dependencies given the framework that you are using. For example the default ASP.NET MVC Core has an already builtin IoC but it is very simple. Start with that and then move to something more complex only if you really need it such as StructureMap or Autofac. I can't really vouch for this but here is a sample project that someone has created using the AdventureWorks database and ASP.NET MVC that might give you a running start: [AdventureWorksSample!](https://github.com/mickeo4/ReferenceArchitecture) 
Never heard of it, but I'll look into it. Doubt management would sign off on adding a third party thing for this, however :/
this is a frequently debated topic. IMHO I think having a repository would be better then using DbContext everywhere. 
Have you considered not using the Onion architecture? I am not sure what you are working on, but its primarily used for large websites. I am working on a personal project now and using the Onion architecture. It's complete overkill and I am having the same problem you bring up. The only reason I'm using the onion architecture is because I am trying to learn it. That being said, I did exactly what you described when making the repository. Also be cautious of [anemic domain models](https://martinfowler.com/bliki/AnemicDomainModel.html)
If you're using asp net core I'm pretty sure you can use middleware for this. We're using a custom middleware in combination with serilog to do this at work. Not 100% sure of the exact setup but I can check it out for you at work tomorrow if you want?
In fact, I think our setup is based off this: https://www.google.co.uk/amp/blog.getseq.net/smart-logging-middleware-for-asp-net-core/amp/
i would suggest NewRelic over glimpse, due to security concerns and more robust functionality.
&gt; NewRelic NewRelic just like AppDynamics are APM(Application Performance Monitoring) tools that will work like a .NET Profiler to record code execution but will most of the time not give you the actual details such as request payload or real debugging information. They are more oriented towards watching your production application for metrics and helping with post mortem analysis instead of getting insight into what is your application doing when you are doing actual development. If the goal is for performance monitoring then APM tools are the way to go. I am curious what you think the issue is with a tool like glimpse when it comes to security? I just wanted to make sure that I didn't miss something very important since I've recommended the tool in the first place :)
Oh, it goes Left to Right, not Top to Bottom.
That's one source, but not the only one or even the most up to date. Here's another: https://github.com/dotnet/roslyn/blob/master/docs/Language%20Feature%20Status.md Fair warning, it doesn't always link directly to the proposal. And then if you really want to know what's going on, you also have to read through the discussions and design meeting notes. https://github.com/dotnet/roslyn/tree/master/docs/designNotes The InfoQ report are based on public information, but it can take several hours to track everything down and make sure nothing important is missed. If you are willing to spend that kind of time, then come work for us.
In cases like that I use a view on the database. I manually create the POCO class for the view I have extended the dbocontext to add in the views -- that way I can still regenerate the database tables without overwriting my view definitions. The upside one call to the database to load a record. The downside changes to the views have to manually applied to the poco class.
Do you bundle your core and Angular projects together in visual studio or do you have them completely separate?
Use Autofac :P
Pattern Matching with Generics! https://github.com/dotnet/csharplang/blob/master/proposals/generics-pattern-match.md
Are you suggesting you would put your Repository/Data Contracts, your Service contracts, your Business Engine Contracts and your web app contracts (all very distinct layers) in one "contracts" library, then have the web app map all those contracts across all layers to concrete classes? I'm not sure that would be a good idea because then by having a reference to the single contracts library when you are in your controller, you see (and have direct access to) the bottom layers, completely bypassing the middle layers (remember the questions is specifically about an n-tier application). You would also require your web app to have direct reference to all the concrete implementations dlls, when ideally(in an n-tier app) your web app should not have a reference to (or even know about the contracts for) your data layer. It's very possible i misunderstood the idea behind your proposed solution. If so, please elaborate a bit on how Neophyte should initialize the mappings for his bottom 2 layers when the UI layer fires up (either with built in IoC or one of the others you mentioned). 
SECTION | CONTENT :--|:-- Title | The S.O.L.I.D. Principles of OO and Agile Design - by Uncle Bob Martin Description | Source: Note: UB doesn't talk about all 5 SOLID principals, only a couple. If you want a more in-depth training on it, UB has that on his CleanCoders.com site. Very good vids there. This talk is still very entertaining and informative so sit back, relax, and enjoy! This presentation was given by Uncle Bob on Feb 4, 2015 at the Arlington Lakes Golf Club in Arlington Heights, IL. Schaumburg Microsoft .NET Technologies Meetup http://bit.ly/1hAO2ln Be sure to change the video to 1080p when watching it Recorded with my IPhone 6+ 1080p (60fps) Side Note: UB talks little about SR and DI toward the end, but it's more a talk around business and more top level tech talk around around decoupling code and history of coding (where the idea of decoupling stemmed from, how objects came into existence and why, and how programming languages were affected by it ended up providing polymorphism... and a lot of other interesting things around the importance of plug-in software....all indirectly related to the topic of SOLID itself as a whole. The S.O.L.I.D. Principles of OO and Agile Design What happens to software? Why does is rot over time? How does an Agile Development team prevent this rot, and prevent good designs from becoming legacy code? How can we be sure our designs are good in the first place? This talk presents the agile S.O.L.I.D. principles for designing object oriented class structures. These principles govern the structure and inter dependencies between classes in large object oriented systems. The principles include: The Open Closed Principle, The Liskov Substitution Principle, and the Dependency Inversion Principle, among others. About "Uncle Bob" Martin: Robert C. Martin has been a software professional since 1970. In the last 35 years, he has worked in various capacities on literally hundreds of software projects. He has authored "landmark" books on Agile Programming, Extreme Programming, UML, Object-Oriented Programming, and C++ Programming. He has published dozens of articles in various trade journals. Today, He is one of the software industry's leading authorities on Agile software development and is a regular speaker at international conferences and trade shows. He is a former editor of the C++ Report and currently writes a monthly Craftsman column for Software Development magazine. Mr. Martin is the founder, CEO, and president of Object Mentor Incorporated. Object Mentor is a sister company to Object Mentor International. Like OMI, Object Mentor is comprised of highly experienced software professionals who provide process improvement consulting, object-oriented software design consulting , training, and development services to major corporations around the world. CleanCoders.com twitter.com/unclebobmartin blog.cleancoder.com https://blog.8thlight.com/uncle-bob/archive.html Length | 1:12:12 **** ^(I am a bot, this is an auto-generated reply | )^[Info](https://www.reddit.com/u/video_descriptionbot) ^| ^[Feedback](https://www.reddit.com/message/compose/?to=video_descriptionbot&amp;subject=Feedback) ^| ^(Reply STOP to opt out permanently)
Personal opinion: Not really. There is a sub for tech position already at /r/jobbit. .Net is in really high demand in a lot of areas, the flood gates could be opened and this becomes largely about jobs and not the language. There was already another account spamming about offering there services that took forever to go away. However you asked first, so kudos. Have an upvote.
Is there any other explanation for this? There's no readme on github. What is a "sandbox that can run c#"? Is there something I'm missing?
&gt; The author Yeah, that Jonathan Allen guy doesn't sound too bright. ;-)
He got sneaky for part 3. You need to replace -7.1-a with -7.2.
&gt; C# 7.2 and 8.0 Roadmap &gt; by Jonathan Allen on Jun 07, 2017 Are you from the future? Does Microsoft rename .NET again?
https://www.reddit.com/r/dotnetjobs/
No, but they do have 3 more generations of Windows Phone, each incompatible with apps from the prior generation, before the CEO is locked up in an Alzheimer's care facility.
Thanks. I've come across the software engineering daily podcasts, but not the others. I'll sift through those and see if there are some that fit the topics I'm looking for. 
If the app uses EF then look for DbContext class or something that inherits from it. This would be your entry to the DB. On this object you have a Log property that when you attach an Action you can log all the queries that are being passed to the DB. That might help reconstructind the schema. 
There should be a web.config file in the app root that will tell you the database it's connecting to.
Can you profile the database and perform the action you're looking for to see what happens?
dropdownlists can hold any sort of object. The object just needs to have a tostring method. Create an structure to hold your product information, ensuring that you define the tostring method. Populate the dropdown list with objects of the structure you created. Create an event handler on the dropdownlist for selectedindexchanged. In the event handler you can get the selected object by checking SelectedItem. Alternately you could use what you already have. Create an event handler for the dropdownlist. In the event handler you can use the Text property to get the text for the selected item and then use that to load cost from the database.
Is it webforms or MVC? If webforms it should be pretty easy to figure out which events are firing and then figure out what is being called. Get used to using "Show definition" or "Peek definition" in Visual Studio.
All the suggestions here are good. You could fire up SQL Server Profiler and see what queries the app is running against the database, and infer the rest from there. 
https://www.codingblocks.net/
Why I don't like Hangfire: * GPL * Appears to be pushing for a subscription model when previously it was totally free. * Designed primary in the first place to ~~ab~~use the ASP.NET pipeline by forcing it's lifetime to not be terminated. This is a pretty hacky approach to having long running background processes in a web application, and is not at all officially supported by Microsoft/the ASP.NET team. Appears that they might be branching out from ASP.NET only now though. Reasons I don't like Quartz.NET: * Pretty much a 1:1 port from Quartz the Java library, and it shows. Badly. Does not feel very C#/.NET like. * Excessive XML over-configuration. * Has a ton of unnecessary dependencies (good luck getting it to run without Common.Logging which is a whole topic of evilness on its own). It's very likely to interfere with whatever version of some logging framework you are using. Personally I'd recommend https://github.com/fluentscheduler/FluentScheduler I've used it a few times and its very nice and simple. It's got a very nice to use API (hence it's name). Pros: * No XML/JSON support. If you want that, you have to write it yourself. * * No dependencies on a logging framework. Again, that should be a decision the developer of the application makes not the decision of a third party library you happen to be using. Sort of cons but probably not for most people: * No support for persisting jobs and their state in a DB. This is something you'd need to write yourself. * No CRON string support, but you don't need that often when it has a fluent API for building the dates. If you want to read more about this, I suggest you read the post I made on /r/csharp on exactly this topic, which is where I was recommended FluentScheduler. https://www.reddit.com/r/csharp/comments/4pve3y/alternatives_to_quartz_net_scheduler/ ***** ^^*Also ^^great ^^for ^^stopping ^^"senior" ^^developers ^^making ^^some ^^horrible ^^fucked ^^up ^^XML ^^blob ^^mess ^^to ^^describe ^^how ^^jobs ^^are ^^to ^^be ^^run. ^^Essentially ^^you've ^^removed ^^part ^^of ^^their ^^"job ^^security" ^^mentality ^^by ^^not ^^allowing ^^them ^^the ^^chance ^^of ^^making ^^an ^^XML ^^mess. Edit: And downvote immediately for providing a constructive look at the topic in the article. Fuck this sub. /r/csharp is the only decent .NET sub.
The hangfire paid model is for some advanced stuff mostly batches. If you don't need that the free version works awesomely. We use the free version in most projects and is a great product. 
This worked perfectly! Thanks!
Developers, like people, need to make a livelyhood.
?
I disagree on the use or abuse of the ASP.NET pipeline. It's my understanding that Hangfire uses its own thread pool, so it does not have any effect on the ASP.NET pipeline's use of it. Does this cause problems with servicing requests on the server? That's a big maybe. We've been using it for over a year on our sites and have noticed no trouble from it. We do try to keep our tasks short and sweet. They are primarily for pre-loading/warming cached data and have drastically reduced the load on our servers because of it. The other thing is that you can fully control where a task is run. By specifying the queue that a task is to be run from, you can assign this queue to one or more members of the hangfire "cluster'. This allows you to assign specific members of the farm to run specific jobs or types of jobs. If you are really concerned about it, you can simply just use a regular farm member and just configure your load balancer to ignore it for servicing regular web requests. I really like it because it allows us to have a scheduler solution that's baked in. We do some on-premise installs of our web service and it reduces the amount of configuration and installation overhead required to get it up and running. It's by far the easiest solution I've used. I agree that maybe running that extra thread pool maybe shouldn't be done but I'm not too hung up on it. 
Also, it's LGPL. GPL would have been a deal-breaker for me. 
let me know if anyone has answer for this? thank you
How is the VM configured? Do you have assigned physical CPUs or entirely virtual? Do you have dedicated RAM or completely shared? What VM product are you using? How is the disk array configured? Does the application make heavy use of the disk? What does the application itself do? Are you running a database server on the same machine and does the application use it? What other services are you running on the machine? Have you tried using Performance Monitor and other tools to see what's going on? Etc. etc... You have a pretty standard performance tuning issue here so if you don't know how to proceed at all at this point, then I would suggest you pick up a book or two on the subject and ramp up. If you know for a fact that the application pools are the major consumer of the CPU, then I guess you could start to tune those using a guide like this: https://technet.microsoft.com/en-us/library/cc745955.aspx Probably the worst thing you could do is to simply start guessing and "fixing" things based on your guesses. Figure out how to measure the problems up front, then change things one at a time to make and be able to measure improvements. I don't think anyone here is going to be able to give you magically correct recommendations though apart from "use a dedicated server if performance is a concern". (Seriously, VMs aren't normally going to be very scalable unless you're doing something quite small and specific with them and using some well designed caching.) Also, I noticed in one of your previous posts that you seem to be working on a search application of sorts. Hopefully you have some sort of caching and indexing going on in your architecture, because if you're re-searching everything on a per request basis, that doesn't scale. Sorry if that's too general, but your problem statement is pretty broad.
I'm not the one who configured it, but I know that VMWare is used. The CPUs are virtual. I believe it's dedicated and it's notable that the worker processes gobble up a lot of memory during load testing. The application is just a very simple record management system, it doesn't do anything crazy just queries a database and rendering results. No, the database is on another host. From just using resource monitor I can see that the IIS worker process is using using all the CPU and memory. I will check out the guide, thank you. That application is separate to this :)
Thanks guys. Checking that out!
I am curious if it will be possible in the future to yield return from async Task&lt;&gt; function.
Are you just executing the .sql files as scripts or are they creating objects which are then invoked via code/ORM (and thus they are migration scripts)? If you're just executing .sql files, you could keep them in their own project, embed them in your assembly, whatever. You're just reading the contents of the script. If they're migration scripts, they should live with your DB Migration utility of choice so it can execute them as part of your CI pipeline. 
Should be possible. You should probably return `Task&lt;IAsyncEnumerable&lt;T&gt;&gt;` or something similar. A generator function is already a state machine, as is an a sync function, so it should be relatively trivial to combine them. 😜
We have a data tier project in our application which is essentially a RoundhousE folder structure and a batch file to run it. All the procs, views, indexes and up scripts reside in there and are stored in Git. For multiple databases, we just have a project per database and pull them into the solution as required. Very simple and works really well.
I've previously worked in two positions using Xamarin, the first 100% of time and the second 70% of time. I get a LinkedIn message related to it every few weeks.
As a freelancer I have plenty of work for xamarin based applications. 
Been using hangfire for long running (30+ min) weekly scheduled process that runs via a .Net Core console app deployed as a webjob. Works great. 
Are you a junior or senior developer? I am 5 years away from coding professionally and before that I had coded only for 1 full year (yep, I did make the mistake -imho- of "following the immediate money" and became a sales/marketing professional). Have been watching a cool Xamarin course on Udemy, just for fun, but actually thinking about resetting my career. I live in Brazil and I have a passive income other than my actual sales/marketing job, so 2k USD would be more than fine for a new beginning in my career. Do you think it is realistic to get that money as a junior Xamarin freelance? If less, how much? 1k at least? Thanks in advance!
Sometimes I get an offer or two to build an App using Xamarin, but I have been much more lucky with Java/Kotlin and Swift, sometimes even porting Xamarin Apps to their native counterparts. Also, if you can develop Restful Web Services in a popular stack you can improve your chances.
It would need to be AsynEnum&lt;&gt; directly; it doesn't make sense to have an outer task here, as the enumerator needs to be awaited for each element already. 
Yeah, deploying with the new certificate and name would not normally causes issues. I think it depends on the installation zone. We deploy ClickOnce over the internet (public URL), and so the Smart screen filter kicks in. It doesn't occur when deploying via the local intranet/domain. https://www.howtogeek.com/123938/htg-explains-how-the-smartscreen-filter-works-in-windows-8/
I'm part of a small .net consultancy agency. I don't do Xamarin myself, but the ones who do have been getting a steady supply of projects. It's nowhere near e.g. Angular, but there is a market for it with customer driven companies (most notably press and magazines, for us at least)
Around 1000 unique installations (different machines) before we noticed a drop in calls with users getting the popup. This was all on the ClickOnce version of the software. The EV certificate should give you decent reputation from the get go - at least I'd hope so. We just had a standard code signing certificate when we did this. 
&gt; 3) Explain the new features in version 4 of MVC (MVC4)? A little bit dated I would think, especially since MVC 5 has been out for almost [4 years!](https://en.wikipedia.org/wiki/ASP.NET_MVC#Release_history) *exclamation point as I thought it had only been two and now I feel old. 
Not really. In what concerns mobile OSes I tend to focus on Android, and for portable applications, I tend to use C++ instead in a MVVM way, because I am quite comfortable with it and it has first class treatment across all platforms from their vendors. I think Xamarin is more relevant for those of us actually targeting both platforms at the same time, but be aware that it still requires platform specific knowledge and is yet another layer to debug. Having said this, I have been thinking to have a go at it instead of C++, just to see how the whole experience feels like.
This sounds like a nightmarish path to maddness. I'm glad I'm not dealing with it. What advantages are you getting from using this system? Perhaps the long term fix is to switch away from ClickOnce and start distributing a msi? Has your team discussed it, and if you decided to stick with ClickOnce, I'm curious why. 
We don't use click-once for deployment but we use the same hardware dongle with an EV Cert (from DigiCert). One thing I've noticed is that you have to be on the physical console of the box to sign it with the dongle. When you RDP in it doesn't detect the dongle. You should be aware of that if you are trying to use a virtual machine for your application signing. EDIT: We eventually had to install VNC Server on the PC with the dongle so people could VNC in to the box. When using VNC the dongle is detected.
This is sort of unrelated but don't you also have issues with users that are not using internet explorer? I thought Click-Once required IE to work natively without some sort of plug-in. For intranet applications you can control this but not out on the web. Click-Once always seemed like a solution for intranet applications to me not really for public distribution. Most public windows apps I see these days are just an msi download and updates are handled directly through the application.
If you're using mvc you can use attributes on your view model and the validation will be enforced 
In your Model/View Model class: using System.ComponentModel.DataAnnotations; ... [StringLength(50, ErrorMessage = "Cannot exceed 50 characters. ")] public string YourProperty { get; set; } 
&gt; And small corps are all trending towards ionic. As an addendum to what you are saying, when the Fragmented podcast visited Google IO 2017, they only discussed React Native as alternative for portable code across mobile OS, ignoring everything else, even Flutter as presented by Google at IO. http://fragmentedpodcast.com/episodes/84/
Is this an asp:Textbox? You can try writing maxlength in lowercase.. I can't remember for sure if this property is case sensitive. Create a pre-render handler for it and set maxlength using the attributes. Its been a while, something like : yourTextbox.Attributes["maxlength "] = 50 
Do you have a "build action" in the properties window for the files? 
Yeah, non-IE users get a bootstrap application, which effectively launches the app.application file and installs.
That was the promise that we bought the EV certificate in - automatic reputation to bypass the Smartscreen filter. But only 1000 unique installations? Hmmm, that's not unachievable with bodies and lots of VMs around the world.
Thanks everyone for their informative comments!
Yup. I was soo aggravated. Our first thought was to put it on a VM as well.
This is why I'm still using an old instance of vs2010 for building old Install Projects. Tried click once. Never again.
I guess you won't be doing any UWP then, since many APIs are C++ only, like accelerated 2D (hence why Win2D exists). How do you get around doing those low level stuff on Windows that C# still can't do? On my part of the world there are lots of C++ programmers, doing IoT, embedded specially car software, high performance trading and integration with medical devices (with WPF UIs).
I'm surprised that Windows 7 drivers don't work on Windows Server 2008 R2... they're essentially the same kernel (as opposed to Server 2008 which is the same kernel as Vista).
If you use a GPL project (whether it's source or just linking against a compiled library/assembly) you are supposed to release all of your source as well (as GPL). This is why it's considered viral. If you use an LGPL project, linking against a compiled library has no restrictions on your code. You have to provide the source for the library if requested (no big deal). So using pre-built (or even building your own copy) LGPL libraries is safe for a commercial application, but using GPL is _never_ safe for a commercial app. 
Kinda surprised nobody yet suggested calling MS on that. Shouldn't they have some sort of process exactly for stuff like this (I.e. not having to build/migrating trust)?
So, did you give them a call already or not? Because for obvious reasons I'd think this isn't a flow they would want to be publically known about on (or at automatically accessible through) the web.
No, we haven't called them, but it's documented fairly clearly on their website, regarding the application reputation: https://blogs.msdn.microsoft.com/ie/2011/03/22/smartscreen-application-reputation-building-reputation/ It seems clear that that's what's the they intended. They just assumed that new developers or companies would (a) live with zero reputation until it gets built; or (b) buy an EV certificate to build it immediately. Given they haven't published exact figures or calculations on what it takes to earn reputation (ie install count, unique machines, unique IPs, geographical distribution etc) indicates they don't want to give companies advice on how to artificially build reputation.
No, you can easily check the source code at their .NET Github repos. https://github.com/dotnet/coreclr/blob/master/Documentation/botr/ryujit-overview.md Your C# code or virtual machines on Azure don't work without the components I listed, even if you don't touch them directly. Hence why "C++ is simple to explain, there just aren't C++ programmers any more. That's not to say you can't find any, you can find Cobol programmers. But outside of game development you'll rarely find anyone who has anything but utter despisement of C++ " doesn't make sense, given how much from Windows infrastructure, UWP and .NET relies on C++ code. As mentioned, maybe when .NET Native becomes usable outside UWP apps or C# 8 gets released with the low level features on the roadmap (taken from Midori), Microsoft can switch to use more C# on those layers. Currently they are actually still moving from C to C++, so those C++ developers must come from somewhere.
Looks cool, can anybody share some experience? 
Awesome, thanks! 
No worries, glad to point you to other solutions!
That's not true about the thread utilization. Orleans uses its own scheduler that assigns a thread per CPU and allocates Actor activations on it. The point it that the system ensures full CPU utilization across Actor instances. This is not Orleans specific, BTW. This is quite basic of how an actor system works. There are actor systems (like Akka) that provides greater customization on the scheduler you use and allow several strategies of scheduling. Orleans point was to simplify the system so they prefer reducing configuration options and utilize the strategy which works best in most common cases (that's a design decision of simplicity vs. configurability). So the point of this is a simple programming model that uses known semantics (async/await) but actually allows you to use it as a distributed model and not just a single machine one. It's quite cool, actually. 
&gt; Dependency Injection is currently supported only on the server side within Orleans. The server side is almost the entirety of Orleans. The client side is just a class you use to talk to an Orleans cluster, it's the equivalent of `HttpClient`. Again, you've turned a trivial thing into a huge downside because you don't have much experience with Orleans. If you had, you'd know it's a non-issue - even more so with Microdot because it completely removes the need to use Orleans' client (which I admit is a bit clunky, they're working on it). &gt; I pose that it is you that does not fully understand Orleans as it's documentation specifically says to not use most of the TPL Their documentation is trying to steer you in the right direction. It's true that to achieve maximum performance you don't want any threads competing with Orleans' threads (thereby causing excessive context switches). But they don't block it or prevent it in any way, and it's totally possible to use the TPL in any way you want. It's just not natural or even needed when using Orleans. I will admit that not all of Orleans' documentation is up to date, and there may be some omissions. But writing documentation is really hard, as I'm finding out attempting to write some for Microdot. &gt; Just because `Parallel` is synchronous in nature does not mean I can't await it. I can use `await Task.Run(&lt;Parallel usage here&gt;)`. You can do the **exact same** on Orleans. There, I solved your problem. &gt; I didn't say anything about system interrupts My bad, that was a joke. It was about the old days of DOS and how non-NT Windows destructively overwrote certain interrupts to take over functions of the system. For example, how Windows 95 took over the Ctrl+Alt+Del function set by DOS (soft reboot) and overwrote it with its own (popping up a Task Manager GUI).
&gt; Dependency Injection is currently supported only on the server side within Orleans. In v1.5.0, which is in beta, the client fully supports DI. I'll update the docs once 1.5.0 final is released.
Working link: http://github.com/dodyg/practical-aspnetcore
That doesn't change my entire premise that numbers wise that is an extreme minority of Software developers who build stuff in c++ that aren't game developers 
I'm new to Orleans so I went ahead and read the research paper on it, which is on their website. Everything you've mentioned as problems are described therein, with reasoning.
This is great! Wish I had seen it 2 months ago before I started my last project.
I guess you need to experience other industries beyond deploying stuff on Azure, if you think that Google, Facebook, Microsoft, DropBox software, car industry, high performance trading and computing, biology research, medical devices, car infotainment systems, GPS devices, big data databases, life sciences robot arms controls, IoT, Arduino.... are a extreme minority of software developers. But what do I know.
Was this training event a sales pitch or actual hands-on training? Could you tell us a bit more about it? 
In threads like this I always have to recommend Mosh Hamedami 3 part series: Become Full-stack .NET Developer. It helped me a lot, course covers a lot of parts about building modern website, from thinking and structuring the problem, general full stack programming with MVC to arhitecture and testing. There are also tons of other great tutorials like other people mentioned. If you can afford it definitely go for pluralsight.com, MVA and youtube tutorials are great but honestly pluralsight offers higher quality content, especially for .NET developers. If you already dont know, Visual studio online offers free 3 months subscription for pluralsight.
Did you look at the job seeker interest that's a flat 0 vs the growth in c# interest? Once again those c++ numbers would be heavily skewed by gaming. Also c# has existed for what, 1/5th as long? 
The Service Fabric Team have recently began open sourcing SF : https://blogs.msdn.microsoft.com/azureservicefabric/2017/03/24/service-fabric-net-sdk-goes-open-source/
Am I missing something if I let my DI Container solve this for me by registering my implementation as a single instance?
You can solve the locking issue by checking, locking, checking again, and then initalizing if necassary.
It is quite useful, thanks! There are few minor error like *ApplicationException* in [Developer Exception Page](https://github.com/dodyg/practical-aspnetcore/blob/master/diagnostics-2/src/Program.cs) and [Custom Global Exception Page - 2](https://github.com/dodyg/practical-aspnetcore/blob/master/diagnostics-4/src/Program.cs) I often replace '\n' with '&lt;br /&gt;' in context.Response.WriteAsync() 
Thanks for the feedback.
You're not missing anything. That's how it should be done. 
Yes, that is right.
Might as well read the original: [Implementing the Singleton Pattern in C#](http://csharpindepth.com/Articles/General/Singleton.aspx) 
My first thought too... this is pretty much a straight copy from C# in Depth (in fact, it has less information). That's not a problem of course, but I feel like the author should at least credit this somewhere.
Yep agreed, I haven't self managed a singleton in years. 
Wow, that is bad. Where is double check lock aka "real standard implementation"? Because non-thread safe solution won't pass even on interview. Where is the least verbose version using static field initializer? And no it is not the same as using static constructor. Where is the industry-standard variant of using DI container to do singletoning for you?
I prefer non-modal displays for the @Html.ActionLink associated with the Telerik MVC Grid column that I use to display the details associated with the specified grid row, all the while using Bootstrap 3.3.7. I also like them to open in a separate browser tab/window so that the user does not lose the grid and can have multiple details open at a time. Example of the column binding: c.Bound(q =&gt; q.Id) .Title("Details") .IncludeInMenu(false) .Filterable(false) .Sortable(false) .Groupable(false) .Template(@&lt;text&gt; @Html.ActionLink("Details", "Details", "MyController", new {id = @item.Id}, new { target = "_blank", @class = "btn btn-primary"}) &lt;/text&gt;); The controller action looks like this: [HttpGet] public async Task&lt;ActionResult&gt; Details(long? id) Inside the action, you'll need to retrieve the details of the specified record by its passed-in id and return the view along with the view model that goes with it: Widget w = await db.Widgets.FindAsync(id); // some error checking omitted for brevity sake // Transform db entity into an easier to display view model (preformatted dates, money fields, etc.) WidgetDetailsViewModel viewModel = GetWidgetDetailViewModel(w); return View(viewModel); 
There are normally 3 ways of printing to a thermal receipt printer (on Windows): Microsoft's PointOfService framework, [ESC/POS Commands](https://en.wikipedia.org/wiki/ESC/P) and Windows Printing. Microsoft's PointOfService framework only supports devices that have an [OPOS](https://en.wikipedia.org/wiki/OPOS) driver. These drivers are normally only made for the more expensive variety of thermal printers (Epson, Star, etc.). This option is most likely out. This leaves ESC/POS commands and Windows printing. Almost all thermal printers support ESC/POS and a very cursory search for your printer indicates that yours is one of them. My guess is the test app that came with the printer prints in this way. 
##ESC/P ESC/P, short for Epson Standard Code for Printers and sometimes styled Escape/P, is a printer control language developed by Epson to control computer printers. It was mainly used in dot matrix printers and some inkjet printers, and is still widely used in many receipt printers. During the era of dot matrix printers, it was also used by other manufacturers (e.g., NEC), sometimes in modified form. At the time, it was a popular mechanism to add formatting to printed text, and was widely supported in software. *** ##OPOS OPOS, full name OLE for Retail POS, a platform specific implementation of UnifiedPOS, is a point of sale device standard for Microsoft Windows operating systems that was initiated by Microsoft, NCR, Epson, and Fujitsu-ICL and is managed by the Association for Retail Technology Standards. The OPOS API was first published in January 1996. The standard uses component object model and, because of that, all languages that support COM controls (i.e. Visual C++, Visual Basic, and C#) can be used to write applications. The OPOS standard specifies two levels for an OPOS control, the control object which presents an abstract hardware interface to a family of devices such as receipt printer and the service object which handles the interface between the control object and the actual physical device such as a specific model of receipt printer. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/dotnet/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^] ^Downvote ^to ^remove ^| ^v0.2
He mentions that in the article he just doesn't show the code
What about Microsoft Cognitive services OCR service? Works great
I never said it doesn't have its niches. But if I did IoT I would be using c#
Can you explain when one should use just a plain static based implementation using lock vs. using DI? 
It's 4 years and 2 versions ago, and I'm not aware of anything supported in 2013 that's not supported in 2015. That said 2013 is available on my.visualstudio.com along with everything else dev related.
If you've instrumented DI throughout your code, then use DI. It's simpler to manage in large scales, you can mock the singleton you've configured making unit tests easy, dependencies are clear from just looking at the constructor (assuming constructor injection), etc. If your codebase doesn't have DI, or if you're writing a library that you don't know in what environment it will be used, then you might need to use the other singleton patterns. Although I recommend to think twice, because while you might want only a single instance, you have to think first - a single instance per what? Per component, per AppDomain, per process, per machine, per group of machines, per data center, per region, globally, universally, multi-universally? When you do think about it a bit more, you realize you actually have to define the relationship of other components to your so-called singleton. It might be tempting to use implicit definitions by making it a static and being able to access it from anywhere in your code, but it's rarely the correct solution.
&gt; my.visualstudio.com That worked! I could get the web installer via that site. Thanks very much. EDIT: I understand why they have changed the way they do things. I just wish they would redirect links to this site instead of a 404.
No you said it was like COBOL, without anyone wanting to use C++ any more, when the reality is that the world still builds on C++, even your C# code depends on C++, being newly written every single day at Microsoft, to run. Like for example, the XAML engine that was re-written from C# into C++ for UWP, or the SQL Server and Document Engine that talk back to those LINQ queries. 
&gt; you have to think first - a single instance per what? Per component, per AppDomain, per process, per machine, per group of machines, per data center, per region, globally, universally, multi-universally? This is a great example of when DI containers can add more complication than not. I would *strongly* suggest caution when your "singleton" is really "singleton within a specific scope" or really almost anything else more complicated than "just one please." Wrangling your DI container to handle complicated scopes - if it's even capable of doing so - has often ended up in hard to maintain, hard to understand code that's also very strongly tied to a specific DI container implementation. Experience has taught me that using a Factory or Provider type object is usually a better route here.
I submitted the previous post prematurely by accident. I can't seem to find any good links regarding printing via ESC/POS commands. Most of articles are outdated and have dead links. I have some code for a test project I did which prints this way, you can PM me for more details on it if you want. You can also check out [this library](https://github.com/yukimizake/ThermalDotNet). As for Windows printing, if the printer driver supports it, you can just print to it as you would an ordinary printer via WPF's PrintDialog. Hope that helps.
Can't help but feel like I am old school in this... but making your data layer a singleton just seems like a smart idea from every angle. I always made them a static class for a long time, and now I always make it a lazy instance. I rarely, if ever, make any other classes a singleton. It should be more performat and memory efficient, based on all the profiling I do every time this debate comes up, and the code seems simpler compared to making a new instance every time. Never saw the need for using a DI container for this either when it is done with an extra 4 or 5 lines of code. I would enjoy hearing a calm, rational response to why someone else prefers not using a singleton on classes that are used for communication with outside systems; I think I am missing something around that approach.
Hmm. I don't have a lot of routing experience myself other than the basics but I do know that there are ways of tracing the internal routing decisions that are made. So, I'd google something like "how to trace MVC routes", turn on tracing, and then use that to try to figure out why the desired route isn't triggering.
SignalR is a common one and probably the easiest to use. 
I've ran into a couple of oddities with route matching in MVC.. Not sure exactly what you're issue is.. could be that '+' symbol in the url. I'd suggest using [RouteDebugger](https://www.nuget.org/packages/routedebugger/) to help understand what's going on with route matching in your app.
That's exactly what I do, essentially. As stated in the last example in the article, lazy loading seems a bit more efficient from the profile, but it is essentially the same thing behind the scenes.
It could be that the "+" is being interpreted as a space, like it is in the query portion of a URL. What happens if you use %2B instead of "+" in the URL (i.e., localhost:3677/info/Category_TestCat/ID_TestItem%2B/ShowItem.html)?
Honestly with how their site and links are setup I have always been fairly surprised it semi redirects appropriately half the time 
I was aghast about the missing double check lock as well. It was screaming at me from a mile away.
The only thing I'll have to add here is that singletons aren't inherently bad. The problem is they make it too easy to be lazy and screw up. This post points that out but I really can't stress that enough. I've had a much better time using a DI container, specifically because it forces you to think out your design pattern. There's 100% absolute valid cases where a singleton is needed but if you move responsibility of object creation to a single place, it forces you to think about "oh this is definitely how I should/shouldn't do something". We had an old code-base that was essentially just a giant singleton for everything. Testing that and making changes because a nightmare. The instance was a singleton and then had a bunch of static objects attached to it. Changing one thing literally changed everything. This wasn't because the developers were bad, it was just "easy". You get in the habit of adding here and taking away there and before you know it you have this massive object that is a headache to change. 
Is it better to use in memory cache or use something like Redis?
The typical use case for a distributed cache (can be Redis in your case), is when you have multiple web servers behind a load balancer and your users are not stickied to a single​ server for the duration of their session. So they can bounce around servers and still have access to the cached data because it's not stored on the web server.
This is probably a dumb question but what's the use case for this? 
Also, it moves your cache out of proc. If you reset your app (upgrade, scheduled maintenance, etc) a redis cache won't be cleared, in memory will be. Depending on how expensive the objects stored in cache are, this may be preferable. 
Yea you should use your DI container to create a singleton of your DI container mate.
A static site just showing portfolio images and descriptions doesn't need to be a dynamic asp.net web app. Consider something like https://wyam.io/ that just outputs plain HTML + CSS and then you can host it anywhere (like GitHub or GitLab pages).
Something I have never seen mentioned, which makes me kind of sad, is that there is an automatic cache dependency that is created when two methods call each other and are part of a cached entry, if the inner entry gets invalidated then the callers entries will also be removed from the cache. For instance in asp.net views, if your cached tag renders a partial that is itself using caching, then the partial invalidation will clear any other cached fragment that rendered it. If we didn't have this mechanism then some entries would not be refreshed as their result would be held by a higher level entry.
I agree with the other comment, If it's just static HTML + CSS I would not go to the length of creating your own asp.net website. However if you are, I know aws, azure, and Google cloud have free tiers to host an asp.net website. I currently set up mine on azure with an automated deploy through trigger from github. It is pretty slick, I would recommend. My site: http://benjaminjanderson.com Github: https://github.com/benjanderson/benjanderson-new 
Yeah, these dumbed down unrealistic blogs are part of the problem for less experienced developers beleiving they know caching. Many so called senior devs have no clue how to design for scale and reliability. Having an in-memory cache is not necessarily something you really want as others abobe have called out. Load balanced setups can round robin or simply move you over to a new host, new processes could be spawned as well in case of resource exhaustion, crash, etc. Ever had a problem of a load balancer sending a request to a cold cache, ruining an SLA? Yeah cache needs a warmup before accepting requests in an ideal situation.(depends on desired SLA's if any). Redis is nice, except it sucks with auth really. For one there is no Kerberos which is still a desired thing at a bunch of places on premise. Appfabric was discontinued in favor of Redis, which makes me wonder what Kerberos shops did for distributed caching with asp.net full or core?
Does not sound like to me that its an IIS config issue. What makes you think IIS can be configured to use that much less CPU at this scale? Perhaps start with web.config? Make sure to not run debug mode ! That will mess u up badly on the perf side, but I doubt its the one pegging the CPU. Try with instrumenting. Check out PerfView and isolate hot code. Also check SQL Profiler trace to see how much data is passimg thru. Channel 9 has videos perfvie on how to use it. You might as well want to take 3 consecutive memory dumps and compare the stacks for the highest CPU consuming threads. Start with DebugView then elevate to Windbg when its getting more serious. Look at the code, if you dont have it then use ILSpy and see what is it doing. Do you see something looping through many things returning small parts of it? I am not trying to imply that you won't be able to tweak IIS settings for your app as you will, but it sounds like you should first go after the CPU/memory usage, optimize the app itself then once happy with it review IIS for less drastic improvements. Are you serializing large amounts of data( XML/JSON), perhaps compressing and returning all of it for no good reason? etc you got the point i hope.. Also, if you need to scale furtther then go after every sync blocking code and switch to async. For anything IO bound so those operations wont block your worker threads. 
For static sites, no need for asp.net - hard code it by hand and host it on aws S3. It's pretty easy - tons of tutorials on it and it's super cheap (like 5 cents a month if you have moderate traffic).
Look at https://appharbor.com/ (free plan). It's like "Heroku for .Net"
Maybe Monox? * http://monox.mono-software.com/
Oh okay that makes sense I was thinking of why you'd want to cache like a getcustomer(123) api call. But the drop down use case makes a lot of sense. Thanks for the replies everyone! 
&gt; We are working on .NET Core 2.0 Preview 2 which will be available soon. woo. Great announcement. wait... what are you announcing again, since the preview 2 isn't actually available yet? &gt; Visual Studio 2017 (15.3) Preview 2 was recently released which includes .NET Core 1.0 and 1.1 Oh right. A visual studio preview. eh. 
Thanks mate, I'll check it out tonight!
thanks much appreciated 
The ones that support it will either be configured out of the box to use it (since no one is going to write custom code for every junky printer out there, most of them just write their firmware to support ESC/POS), or have instructions on how to configure it (generally with DIP switches on the printer somewhere). It's been more than a few years since I've messed with any of them though. In 2003 we began moving to OPOS, and only support Epson printers via OPOS now. 
Is that because Visual Studio masks the complexity?
I have a smallish project that I am currently rewriting into .Net core w/ Angular and Typescript frontend. https://github.com/tidusjar/Ombi/tree/DotNetCore If you have any questions, i'd be glad to assist 
not sure why they didn't release vs prevew with 2.0 preview anyweays.. I'm still learning the new npm world.. Dumb Question. How do you F5 run form Visual Studio a non .NET app (a simple HTML website) 
This looks fantastic! Over the past few years I've had to work on a number of projects that involve third-party proprietary API's, and more often than not these API's are not built for handling a lot of traffic, so response caching has been a necessity. Tie this in with other caches you might have set up (i.e. output caching, model caching) and you're in for a world of hurt.
Glad you like it! That's actually one of the reasons I built Cashew. :)
https://mva.microsoft.com Just a ton of stuff there, covering a wide range of topics.
I've used this: https://fullcalendar.io/ and had the calendar pull the events through a webservice.
For anyone checking this thread to find good .NET YouTubers (not necessarily for keeping up to date), here are two I recommend: * Jeremy Clark: I liked his task /await videos, so I ended up watching his other material on delegates, lambdas, and LINQ. He gives clear explanations at a measured pace, and each video set has an accompanying project on GitHub. https://www.youtube.com/user/jeremybytes/playlists * Jamie King: I came across his EF videos and then started watching a bunch of other stuff (not just .NET) because he's a good presenter. https://www.youtube.com/user/1kingja/playlists 
Actual link: https://blogs.msdn.microsoft.com/dotnet/2017/06/12/the-week-in-net-on-net-with-mattias-karlsson-on-cake-topshelf/
Maybe something like [Draw.io](http://draw.io)?
Probably means the dev builds apps or works on projects within a Windows dominant ecosystem. .NET framework contains libraries built mainly for use natively on Windows systems. (Or they use Visual Studio to code)
[What's new in dotnet 4.7](https://docs.microsoft.com/en-us/dotnet/framework/whats-new/index#v47)
The way URLs map to controllers is configurable. What you've described is just the default behaviour. You have two configuration options: First, use the static Routes property on the [RouteTable](https://msdn.microsoft.com/en-us/library/system.web.routing.routetable) class. This is done at application start-up in a Global.asax.cs file. If you create a new MVC app in VS it will generate this code for you so you can do that to see an example. An example route for your scenario could be configured like this: RouteTable.Routes.MapRoute( "ProblemRoute", // Route name "Problem/{name}", // Pattern new { controller = "Problem", action = "Index" }); // Default values More info on routing: https://www.asp.net/mvc/overview/controllers-and-routing Second, use the [RouteAttribute](https://msdn.microsoft.com/en-us/library/system.web.mvc.routeattribute.aspx) class. You can put this attribute directly on your actions. An example for your scenario could be: [Route("Problem/{name}")] More info on attribute routing: https://blogs.msdn.microsoft.com/webdev/2013/10/17/attribute-routing-in-asp-net-mvc-5/ Either approach will map a request for Problem/A0465 to the following action: public class ProblemController : Controller { public ActionResult Index(string name) { // "name" parameter will have value "A0465". } }
Yep, that's right. I did a few ninja edits on my post so check it now and let me know if anything needs further clarification.
I'm working on a personal project in ASP.NET with Code First EF if that would interest you. Dive into some custom identity stuff as well. https://git.teknik.io/Teknikode/Teknik 
What is the context? Do they talk about their work or their products? .Net to me is the execution environment. So anything that compiles to MSIL works in .NET...
Yes, something like that, but also something that can be integrated in a WPF or HTML5 app.
Really Google cloud has a free plan? I see the try it for free but is it free as long as you are under a certain limit?
These are my youtube subscriptions: - NDC Conferences (https://www.youtube.com/channel/UCTdw38Cw6jcm0atBPA39a0Q) - Scott Hanselman (https://www.youtube.com/channel/UCL-fHOdarou-CR2XUmK48Og) - Microsoft Visual Studio (https://www.youtube.com/channel/UChqrDOwARrxdJF-ykAptc7w) - SSW TV | Videos for developers, by developers (https://www.youtube.com/channel/UCBFgwtV9lIIhvoNh0xoQ7Pg) - On.NET (https://www.youtube.com/channel/UCvtT19MZW8dq5Wwfu6B0oxw) - Microsoft Developers Network (https://www.youtube.com/channel/UC04_E5cXyKb_nkjLWjuHIXw) Hope that's a good start. There's heaps of good videos on Vimeo too and some local User Group Channels. There's also heaps of great SQL Server related channels too. Good luck.
Everything here is deployed via SCCM. Citrix is all done manually. I can write something to automate it, but was hoping I could use some standard tool.
In my experience, yes. When I have worked 'in .NET' I use Visual Studio.
yo, i'v been watchin this stuff for a while&gt; they have promsed to bump the numbers with slim and make inplace changes cool stuff tho
C# provides a way to avoid this in production, assuming you're using a pooled service architecture. `GC.RegisterForFullGCNotification` allows you to find out when the GC is about to run. Your service can respond to the notification by unregistering itself from the pool, so that requests don't get sent to it, and the rest of your pool temporarily handles the load. This way, you don't get a 5 second GC pause in the middle of servicing someone's query, slowing down their experience. Once you've unregistered, you sit on `GC.WaitFor..()` methods to find out when GC is done, then re-register the service with your pool and start handling queries again.
That reminds me. Hanselman periodically presents to the .NET user group in my area, and they've started putting the monthly presentations on YouTube (linked below). Since user groups tend to like cutting edge material, you might try searching for other .NET user group channels. https://www.youtube.com/channel/UCHZpoCG07ki8M3jjtTeqj2Q
If you have a heap of 150 GB state (i.e. social network graph or trading exchange state for the whole day), you can not easily expect to have similar dataset teleported to a nearby box on "gc approach". There is a whole class of tasks, HFT and the like where you need to have 100s m of objects and need to have locality of reference. Furthermore, hosting boxes in HFT exchange is expensive. Anyway, shifting between boxes every 2 minute while servicing 200,000 re/sec is not an option 
NFX.Slim works 3x-5x times faster in BATCH mode. You probably used it in "anytype"/nonbatch mode. For replacing cache, you can expect to serve 250K tran/sec on a hefty objects (100 fields+). You can also expect to serve 5M+ tran/sec if you serve pre-redered JSON to clients. This optimization was added a month ago. The network will be the bottleneck 
I had trouble getting past a "Slim type registry count mismatch" exception when using Batch mode. I need to revisit it sometime.
I think some of the consensus is around tooling. Some people feel that the tools for Core are not "there" yet. For me, my enterprise/flagship apps are done in the full framework while my satellite/microservices are all in Core. I can't speak to building a massive app in Core, but for my smaller apps I love to use Core and it's flexibility for deployments.
you need two serializer instances - one for send another for receive, as it establishes channel state - the serlz instance now "remembers" what has went through it. look at the batching unit test - it has the snippet. batching obviates to alloc typereg on every call, so this gives a HUGE boost in some cases, in general it speeds-up things nicely
Really? Not even on VS2017? One more thing: Are you talking about ASP.NET Core or .NET Core?
Definitely recommend the attribute-based routing approach. Additionally, look up the "FromBody" attribute - which goes on the arguments to the controller methods. It allows you to specify which pieces of your request are not part of your query string (the parts like route?name=value).
Any deployment software can do that, Octopus, Jenkins, Bamboo to name a few. In the interim, why not just spin up a PS script to do the work for you as a post build step. 
Others have already hinted at or mentioned that Asp.net core and .net core are not the same thing. Asp.net core will run on the full .net framework, standard framework, or core framework. Recently, it was announced that asp.net core 2.0 would not support the full framework but the asp team reversed its stance and support will continue to be included. Concerning package support: unless you are using libraries that targeted web forms or asp.net specifically, you will find that most packages will just work if using the full .net framework. The team I work on recently started using Asp.net core on the full .net 4.6 framework and here is what we've experienced so far: Things that went well: * All .net 4.6 packages/libraries we built worked without issue and were installable through nuget * Entity Framework 6 continues to work * The new [TestServer](https://docs.microsoft.com/en-us/aspnet/core/testing/integration-testing) has proven to be amazingly useful * .Net and front-end package management is much improved * We were able to piggy back on the built-in IoC container without completely replacing it (you can replace it if you want) * Globalization now has helper classes that make localization and globalization easier * Getting to run on IIS was simple other than requiring us to install an additional handler There were only a couple of things that were difficult or took a bit of time * Getting the project to build with Jenkins (we now have it working after lots of trial) * Handling errors in Razor * Figuring out how to use resources (resx) files for translations * Our Sonarqube integration is broken We are scheduled to release multiple apps (web and service) in the next few months. Time will tell if we run into any additional problems. 
&gt; But it doesn't seem to contain any info on the user? Your ApplicationUser class derives from the IdentityUser class, so it inherits the Email, UserName and other properties from there.
The whole god damn point of tooling is to do that shit for you. Why the hell do you care about generating it by hand?
So you think (breaking) changes may still happen on the ASP.NET Core tooling until Standard 2 is out?
I've heard globalization is not an issue on ASP.NET Core.
Thanks, will take a look.
i am interested in caching can you pls explain more details for fencing entfrm orm to db
Can't wait for nullable ref types.
Really that it shares little similarity with ASP.NET? That seems odd.
Thank you! Based on yours and others responses, I'll be using AutoMapper to map the entity objects to DTOs and have the API methods return those DTOs. I have another quick question that maybe you cold also help me out with. Is there a simple way to create object classes in the apps that are calling the API? So say I have an app called CompanyApp that uses the web API. The API returns the DTO UsersModel.cs in JSON. Is there a way I can quickly/easily create a model class in CompanyApp that reflects the JSON returned from the API? Or would I have to create all of the model classes manually - and if the API DTO/entity changes, I'd also have to manually update all of the model classes in CompanyApp? Sorry if that's confusing.
I think you mean non-nullable ref types? Yeah, it sounds awesome.
Since: &gt; all reference types will become non-nullable by default it is somewhat correct to identify nullable reference types as the "new" thing, which is what the article does.
True, good point.
Oh, so you're saying there isn't a template for Core Web API project using Identity, but it still works if done manually, right
If you create a Web API project in ASP.NET Core, and click the authentication button, the 'Individual Accounts' radio button is not selectable, since that feature isn't in .net core yet. But if you create a .net core MVC web application (with identity/individual accounts), and add in the web api controllers to your web app project, it will work that way.
Well now that they've literally taken `Nothing`,, there's only a handful of useful VB features left to mine.
The browser has cached the css file. Ctrl+F5 should reload the entire page
Thanks, I'm felling every time more confident about eating Core on a big project. Surely will describe the experience here on this subreddit when I do :)
Hey, care to expand on that? My team is looking at trying to put together SSO that allows social providers like Google/Facebook/etc. We're all relatively junior programmers, so it's hard for us to explain why we'd need to pay for Auth0.
And type aliases and enums inheritation.
Nice! Some of that stuff looks really useful. I still hold out hope that some day we'll be able to create interfaces that can define more than just public member properties/functions. And default implementations being added, gives me some hope. I'm also really excited about that feature, in particular.
I'd love to see records...
Yeah, it started looking that way the more I read that article. I'd want to play around with it a bit first though.
Default interface implementations seem so wrong. I think if they don't want to break compilation when something new is added to an interface, they should make the not implemented methods throw NotImplementedException. In a sense that would be a default implementation, but it'd be the *only* possible default implementation.
Something tells me you've never worked on a project that supported more than 3-4 users at a time. 
As with many C# features, it's just sugar :)
Yeah that makes sense... have u ever used golang, python or nodejs toolset? Well i have and its sweet. Dotnet.exe was just as sweet and now its dead. And my userbase has been in the millions for the last few jobs...
There is a cool [attribute called remote validation](https://docs.microsoft.com/en-us/aspnet/core/mvc/models/validation) supplied it ASP.Net core that can handle the unique username example if java script is enabled: public class User { [Remote(action: "VerifyEmail", controller: "Users")] public string Email { get; set; } } As for when javascript is not enabled, I came up with the following [attribute](https://github.com/JohnPicchi/Authentication/blob/master/src/Core/Authentication.PresentationModels/Validation/UniqueAccount.cs) that works out nicely, and [here](https://github.com/JohnPicchi/Authentication/blob/master/src/Core/Authentication.PresentationModels/EditModels/RegisterEditModel.cs) is it being used. Using this attribute in addition to a global filter, that checks model state on POSTs and redirects them back to the same page if the model sate is invalid, works out quite well. For exceptions, I would personally use a middleware implementation instead of a filter. If an exception is thrown in the middleware pipeline it needs to be handled and this filter implementation would not catch it. 
You could run into an issue when storing TimeZoneIds (e.g. a user can configure his time zone) when testing or running on Windows and Linux. On Windows the TimeZoneId would be *Eastern Standard Time* while on Linux it would be *America/New_York* (IANA). See CoreFX issue: [TimeZoneInfo should have consistent Ids across Windows and Linux](https://github.com/dotnet/corefx/issues/2538)
Your idea will work too, and I believe that Unity employs a tactic like this and Stack Overflow do too (they have a good write-up concerning their tag engine and the GC pressure induced latency spikes their users were experiencing). But the approach relies on good programmer behaviour. In every instance, the programmer must pass the struct by reference. If they don't, the struct is copied (and the payment for that copy is commensurate with the size of the struct). Unlike C++, there is no default constructor allowed on a struct in C#. So you can't prohibit copying of a struct after program initialisation by throwing an exception in the constructor (which would lead to quick detection of the copy-by-value behaviour if doing so were allowed). Unlike C++, you also can't remove things like the notion of copying (by explicitly removing a copy-constructor). With the code-generation approach, the structure is 4 bytes in size. I can't conceive of a way that the programmer could inadvertently do any harm there, performance-wise or in terms of correctness. The extension methods may or may not explicitly add support for mutating the 'object' in the array, so there's no concern with accidentally copying a struct (instead of passing-by-reference) leading to cascading bugs due to the copy, and not the original, being updated. Whether they pass by reference or not, they can't really go wrong (assuming that the integer index value nested within the struct is readonly, which it should be). ----- You raise a fair point regarding strings, and other dynamic-in-length data. To that end, I would propose a rather roundabout way of doing things that I feel would still work well. Let's say you're working with JSON data that represents your business objects, and those business objects have a 'Description' field that can be some length. 1. Write a tool that, ahead of time, deduplicates the strings from the JSON file and stores them as binary. For this we'll use a byte array again, and this array will have 1 to 4 bytes padding each string (to indicate the UTF-8 byte length), followed immediately by the UTF-8 representation of that string (or whatever, really - it doesn't matter). The padding will be based on the maximum length of the string, which you can deduce at generation-time. Write this as a binary blob to a file somewhere (or perhaps a varbinary column in a database). This step is important because it means that your actual program's runtime won't have to carve up its heap into many small string objects while parsing the JSON representation of these strings. 2. For each JSON entry, update it so that it records an index for the appropriate description in the new byte array representing the strings. The index will point to the first byte of the padding. The index is now a fixed-width 1 to 4 byte integer, permitting the use of a fixed-size struct. 3. Load the binary blob at runtime in the real program. At this point we're just reading a stream of bytes from file to memory in the C# application. Lots of string data, but only a single byte array object representing it. 4. When one of our business objects needs this string, the process if as follows: Read the string index from the struct, go to that index, read the byte length of the string, and finally read that many bytes and so something with those bytes. You're adding additional explicit indirection in code, but you'd have implicit indirection when accessing a string object from some other object anyway. If you decide you want this data to span multiple byte arrays, you can use ArraySegment instead of just an integer index into what would have to be a single byte array. ----- At the end of the day, there are probably other workable solutions too. The above would work well for swathes of static and/or fixed-length data, but would likely need more work for dynamic-in-length data. That's where Pile would start to shine as a more generalised solution. For what it's worth, these ideas originated in the Java FIX (financial information exchange) space. They have the same constraints imposed by the JVM's non-deterministic garbage collector, and numerous YouTube videos suggest that ideas along these lines work really well.
What else did you expect?
Msbuild files can only build a single project (library, executable), which is pretty limited. At the same time, it's a hugely complex monster of piled-on legacy. Yes it can process .sln files to build multiple projects, but this is pretty primitive and the format sucks. 
Why not use Pile with strings - it yields 10M/sec throughput on a 6 core 3 ghz machine. and skips serialization completely. Also, look at SealedString class in NFX - can keep many read-only strings invisible to GC. Throughput is in 10s of millions/sec. As for using structs, one can not write business applications like this - the ones where there are many data models, and their generic intersections. Example from insurance risk assessment app: Document&lt;Policy&lt;Discharge&gt;&gt;&gt;, Document&lt;Policy&lt;Admission&gt;&gt;, Document&lt;Waiver&gt;.... generics create 100s of classes. Now, lets add Lists&lt;...&gt;, Dictionary&lt;...&gt;, Sets and other data structures bound to those data types from above. You can easily get 1000s of types to store in byte arrays. Now your boss comes and says: A must be in C, D must be in Z - and you need to refactor say 10 classes in 1 day. The whole point of Big Memory - is to handle all of that for you AUTOMATICALLY without requiring "super programmers". With suggested approach you can process 100Ks a second of business requests on a single box without having to care about low level at all. While its true that manual ways of doing the same thing (arrays, struct, marshalling etc..) could yield better performance, your bottleneck is usually a network adapter at a 100K req/sec churn rate 
is this how "Zero copy serializers" do it? It is basically a special new "format of data" which is not CLR native. Btw, there is a class of tasks where data just needs to be stored and shoveled - where this approach rules. However, as soon as you need to get rid of DTO - add methods, logic - this approach completely kills all native CLR benefits (polymorphism, interfaces etc...)
I'll take things I've never wanted for $500 Seriously Cmake is a bitch most of the time, why in the hell would I want this?
&gt; is this how "Zero copy serializers" do it? I think so, but I don't really know a lot about this topic.
&gt; But the approach relies on good programmer behaviour. I don't like that. I rarely trust myself; I certainly don't trust the knuckle-draggers that I work with.
&gt; For what it's worth, these ideas originated in the Java FIX (financial information exchange) space. Oh hey, I used to be a FIX programmer. I likedusing that format.
Honestly, I expected it to be a VS project template rather than a new language feature. Not that I really care either way.
Its language is horrible. It's not just you. Unfortunately it's also what a lot of people are used to. :-(
Am I missing something here. CMake is used to describe a build at a high level. When you run CMake it produces build files for the generator you specify, so "Visual Studio 15 2017" would make a 2017 .sln and .csproj files. You still need something to build that solution.
Well, being non-nullable is opt-in
Changing an interface is a breaking change to consumers. Theoretically, default implementations resolve this. I agree that it still feels wrong.
VS 2017 doesn't have solid tooling for .NET Standard or Core yet. I've run into inconsistent issues with IntelliSense, NuGet, and editing csproj files. Other developers in my company have had similar behavior on their machines. Don't get me wrong, I still use it and am pushing all of our development in that direction, but it's irritating and not what I would expect from the Visual Studio team.
&gt;What makes you think Microsoft is trying to kill MSBuild? IIRC MS has been trying to get away from MSBuild, possibly due to legacy cruft and a lack of good documentation. That's why they used npm and the `dotnet` command-line tool for .NET/ASP.NET Core.
It's not like MSBuild is a shining example of a good build system either. What would you prefer?
It's not like MSBuild is a shining example of a good build system either. What would you prefer?
Premake?
I hadn't heard of premake before, but I don't see how it's comparable in this case? Does it support building C# projects directly or does it just call MSBuild on a generated .csproj?
I don't think it works with C#. I was just listing it as a "good" build system. For .net, FAKE or PSAKE are better than MSBuild.
Or check out [Cake](http://cakebuild.net) if you like C#! It's an awesome orchestration system in the same vein as FAKE/PSake/etc and there's a pretty awesome community around it now as well 😃
Non-Mobile link: https://en.wikipedia.org/wiki/Premake *** ^HelperBot ^v1.1 ^/r/HelperBot_ ^I ^am ^a ^bot. ^Please ^message ^/u/swim1929 ^with ^any ^feedback ^and/or ^hate. ^Counter: ^80796
Right, because the project.json build process ended up not being flexible/featureful enough. Doesn't mean that MS doesn't want to move away from MSBuild, just that there's no good alternative (yet).
FAKE 4 had a bit more documentation. I don't know why they redid the website with so little documentation for FAKE 5. They should have left the version 4 site up until they had the content ready.
The difference there is that MSBuild actually compiles the projects. CMake doesn't to any compilation on its own. If you were running it in a continuous integration environment you'd have to call CMake to create the solution file and then something like MSBuild to actually compile the program.
Open your database in azure and look for pricing tier. You can adjust db size there. You could install sql server locally and attempt to connect to that to determine if it's a problem with something in azure or your code too. It looks like the number in the error is 1tb as bytes? Which is really large. I'd try a local database first to figure out which end the problem is on.
Do is you install the SDK over the previous one?
&gt; How do you change the size of the database? On the free tier you are limited to something like 32 MB. You can't change the size without paying for it. You can definitely have a database on the free account. I have had one for over a year.
To add to this, WPF has essentially replaced WinForms as the go-to GUI framework for .NET. To the point where UWP is just a slimmed down version of WPF that only works on the various Windows 10 devices (computers, tablets, phones).
If you go with WPF/Entity Framework as suggested here, perhaps take this tutorial to get your feet wet: https://msdn.microsoft.com/en-us/library/jj574514(v=vs.113).aspx
Thank you. First search result has all I needed.
Thanks, it clarified for me how to deal with my project.
Corrupt SDK maybe? https://social.microsoft.com/Forums/en-US/bf360e4d-2a1e-4b9e-86c3-80c815e2c965/abstract-method-with-nonzero-rva?forum=crmdevelopment 
Isn't there a big disclaimer in the documentation saying you shouldn't use System.Drawing from a Windows service application. That always put me off even though it appeared to work ok.
Yeah reinstalling the SDK ended up fixing it.
Do you know what an "RVA" is?
Relative Virtual Address. An abstract method had an implementation. Which can only be caused by a malformed dll via IL-hackery or a corrupt SDK/runtime.
That's not what I want. I have a controller action which allows the user to upload image already. But only to a wwwroot folder. if (file != null) { var imageName = asset.Id + ".jpg"; if (file.Length &gt; 0) { using (var fileStream = new FileStream(Path.Combine(uploadSavePath, imageName), FileMode.Create)) { file.CopyTo(fileStream); } asset.Picture= imageName; } } Which the path is taken here var uploadSavePath = Path.Combine(_hostingEnvironment.WebRootPath, "Uploads\\Pictures\\"); I want the image to be outside of the webrootpath, how do I do it..?
&gt; there might be multiple people trying to run the application at the same time How's that going to work? If each user installs a copy on their workstation will there be any shared data (i.e. user A updates/creates something and user B sees the updated/created values)? If there is shared data and you are dead set against using a database... you're in for a world of hurt. It is really hard to do shared persisted data correctly (ACID compliance). Maybe consider using a cloud data storage engine. SQL Azure is essentially a nice cloud version of SQL Server that you can access from any device connected to the internet. Might be pricey though. Google and Amazon also have nice offerings, though again might be pricey.
&gt; just drop a folder containing the .exe file and any external resources on a network drive at each customer's site Do you need one instance of the database serving all users of the application? That's the thing that would make a shared network drive a non-starter. I'm not aware of any engine that can support multiple clients off a simple shared file. &gt; code record-locking of the CSV files myself Do you have a working plan for this? Because I think you'll find it's also a non-starter over a shared network drive. I think your only option, if it's even plausible in your circumstances, is going to be a hosted (cloud) data store.
I wasn't planning on having the user's install anything. What I'm looking for is a portable .exe and associated data files in a shared network folder. Why? * rollout is dead simple, simply copy the folder containing the .exe, data file(s), possible resources needed to a client's file server. * Any machine on the client's network with access to the shared folder where the program resides can not run the application w/o having to install anything. I am not dead set against a DB, just against having to use a full database server. I want all data to somehow reside within the same folder as the .exe. I know I can do it with an MS Access database or CSV files, but I worry about the Access database getting corrupted or needing to be compact/restored. I don't want to have to code all the necessary file/record locking to use CSV files. Just curious if there's some way to have a data file sit in the application folder and a .dll or something that can handle the needed record-locking for me in case multiple people execute the program simultaneously. 