IIS Express. While not saying it is better and wishing we did use Kestrel, it is what my office uses. 
IIS Express is the reverse proxy in that scenario. This would be no different than using your same code on Linux and then using Nginx or Apache as a reverse proxy and offloading HTTPS there.
My point is that it’s the same code to do HTTPS with Kestrel on Linux, Mac, and Windows. If you’re not doing HTTPS with Kestrel, then yeah you you’ll need a different reverse proxy configuration for Mac/Linux if your using IIS Express/IIS or if you’re not using Kestrel and using WebListener.
Do you need anything from identity at all? Do you just want signin/signout? Just the cookie authentication pieces built in. https://docs.microsoft.com/en-us/aspnet/core/security/authentication/cookie?tabs=aspnetcore2x
Not only what @willhughes said, but with a JSON file, my hope is eventually to make most of the translation client-side instead of server-side and just set the appropriate cache headers on the message file.
I actually started with cookie auth. Do I just store the ID in the cookie and test it in the Details action? If the application stores stores roles am I back to using the native Identity functionality? There are there roles: guest, employees, and admins. 
Thanks, I was just curious how people handle updates on it. I have a WebAPI that has a controller for filtering profanity and I just have a plain text file that I load into memory... but every time I want to update the text file I have to copy it over then restart the IIS worker
You store whatever you need in the cookie as a claim, then you can check the current user and their ID claim against the ID of the record trying to be accessed. This is probably best done via resource authentication, because it's more testable, and portable between actions - https://docs.microsoft.com/en-us/aspnet/core/security/authorization/resourcebased?tabs=aspnetcore2x As for roles, there is a role claim, so when you are building your identity before handing it over to the cookie pieces add one or more ClaimTypes.Role claims. Then IsInRole() will work, and you can use the role based auth in your policies https://docs.microsoft.com/en-us/aspnet/core/security/authorization/roles
Could you elaborate on the nature of user the preferences and how they relate to the tasks? Have you written some code already that seems too complex? A little more detail would help to help.
A technique I use is to create the user record when he user logs in for the first time. On a mobile so formatting might be off. 1. User enters email and password. 2. Authenticate against identity using given user and password. 3. If it fails then authenticate against legacy system. 4. If it passes, create the user in identity using supplied user name and password (the request has the plain text). After a period of time remove legacy system. 
I'm not clear on a few things. 1. Once you issue a token to the client app, are you supposed to store it in some type of local storage for reuse? 2. For each full page request, does this flow have to be followed (regenerate a token) or can an individual token be reused. 3. What are the possibilities of the token to be intercepted/reused by malicious third party? Other than HTTPS, should other preventative measures be taken to protect the token?
This is exactly what I did, it worked great and my new identity system is much easier to use than rigging up some old system with the new framework
Adding a [FSW](https://msdn.microsoft.com/en-us/library/system.io.filesystemwatcher\(v=vs.110\).aspx) will let you get an event when your profanity file is updated. 
We did something similar as well when migrating from .NET 2.0 -&gt; 4.5. Attempt authenticate new system, then old system. If old system succeeded, we would create using new system and delete from old system (the old passwords were hashed but not salted so the legacy data could have provided an additional avenue of attack). For us, it also provided better security around the passwords as they will be hashed using more modern techniques on the newer Identity framework. 
1. Use [localStorage](https://developer.mozilla.org/en-US/docs/Web/API/Window/localStorage) 2. (Usually) there are two tokens. One is the actual auth token, the other is a [refresh token](https://auth0.com/docs/tokens/refresh-token/current) used to re-authenticate in case the actual auth token expires. 3. None? Use HTTPS and don't save it elsewhere is pretty much all.
As far as UI light apps go, Xamarin Forms is quite good at that. The api is close enough to java variant(on android at least) so you can often just use google docs for that. Although I have only used it to develop Android apps. No iOS.
Thanks for the reply. We don't use SPA type applications yet, but we are starting to look at it for some future type applications. These were just a few of the questions I had about the end-to-end use of the tokens that were not immediately clear in the linked article.
To add to /u/celluj34 answer: 3) The JWT spec has a `JTI` claim that you can (and should) include in every token. It's meant to be a unique identifier. This is handy if you need to invalidate a token. You can do this by blacklist server-side. So, if the user tells your API to log out or otherwise forget the token, your backend can blacklist that JTI value and your validation logic can reject it. Obviously, this does nothing for _detecting_ if a 3rd party intercepted the token, but it's a good practice to invalidate tokens whenever it's clear the user doesn't need them anymore. It's _especially_ good for refresh tokens since those are valid for a much longer time.
Sounds like what you're doing is upon the server not the C# app... So in apache: https://serverfault.com/questions/277664/reducing-apache-request-queuing There must be the same thing in IIS ;)
Hey mate. I decided to go with AWS! I'm a bit overwhelmed by all the different products though. Just wondering how you deployed your web API? Did you have to create a Lamda Serverless project? Or is there a way to deploy an existing project to Elastic Beanstalk? Still trying to figure out which each product is, and does. Any advice would be very much appreciated, thanks!
So, after reading yours and /u/celluj34 answers, would it be fair to say this is basically much like the token part of the OAuth authorization chain (authenticate, get a token to use on subsequent request) and that token should be treated as such?
Any token is simply a value that the server gives the client so that the client can give it back. The only real difference is that JWT is a signed payload that is verifiable by anyone holding the secret. The nice thing about that is that your backend doesn't need to remember every token it creates and other backends you have don't need to communicate tokens that were created somewhere else-- if they know the secret they can verify the integrity of the token. Also, JWT has expiration and some other you-really-ought-to-do-this type things baked into the spec-- the trick is to use all of what's in front of you. Keep in mind, JWT is an encoded and signed value. Its payload is not secret. Don't put anything it in that you don't want anyone to see.
As a long time .NET developer (10 plus years), I opted to go native on both platforms for my first app just to fully understand the platforms (you're still going to need to get your hands dirty with the iOS version, as you'll need a Mac and Xcode to debug and deploy). Several years, and 4 apps on each store later (plus a few more internal Android apps), and I still write native on both stores (and still do the majority of my business work in .NET). IMO, if you are going to pursue cross platform, do it in Xamarin (or another tool that compiles to native), and not one of the HTML / JS platforms like Cordova (unless you don't like performance). 
&gt; JWT is an encoded and signed value. Its payload is not secret. Yep! The important part is that it's *verifiable*, not that it's hidden.
&gt; Several years, and 4 apps on each store later (plus a few more internal Android apps), and I still write native on both stores Do you still go native simply because you've always done it that way and it's easy enough or there other reasons you've leaned in that direction? &gt; (and still do the majority of my business work in .NET). How are you achieving this?
I can't speak for Cordova, but my team has deployed a couple Xamarin mobile apps in 2016 and 2017 to both app stores. It's nice that you're writing in C# for Xamarin because if you have shared fuctionaly for .NET web apps, it makes it easier to extend. Xamarin Forms will compile to native code and you can use analysis tools to log exceptions or other meta data about the app while a user interacts. It used to be called Xamarin Insights, however I think they changed it recently as a part of a greater set of tools. You can also use what are called hybrid views to show your HTML pages as if they were in a browser if needed. Hope this gives you some ideas from the Xamarin side of things!
&gt; What are the possibilities of the token to be intercepted/reused by malicious third party? Other than HTTPS, should other preventative measures be taken to protect the token (eg. don't store in a cookie)? &gt; None? Use HTTPS and don't save it elsewhere is pretty much all. If you are putting it in localStorage, a XSS attack would be able to read your token. Whereas a cookie with an HttpOnly flag cannot be accessed by JavaScript (of course you will need to prevent CSRF attacks if you so).
I think his interface is probably something like: interface IGlobalizeStrings { string HELLO(); string GOODBYE(); string PLEASE_ENTER_PASSWORD(); ... } I've never done anything so insane, but I do recall reading about some performance and other strange issues when you add too many methods/properties to a class.
What does your globalization interface look like?
Auth0 is an awesome way to go. You can use them as a direct service for multitenant or have it backed my something like Active Directory
The original apps that I wrote relied heavily on UI, and the native map APIs (and this was before Xamarin Forms existed), so there was a very small amount of code that would have been reused between them. Now that I know both native platforms, and am used to working in them, there's no benefit to rewriting these in Xamarin. I do have one small (Android only / non distributed) app that needs to be rewritten, and I'll probably use Xamarin just to see if there's any benefit at this stage, but this app has a companion desktop app written in C#, so code reuse of some of the data layer might be a reality there. As for doing the majority of my work in .NET; my main business line is desktop development (WinForms, WPF, Windows Services, Libraries, etc). Apps are a sideline, although they represent almost 10% of my income now. 
I think you're over thinking this. Depending on your target audience and threat profile, I'd suggest looking at HTTP basic auth in IIS. 
I don't think I am. I just want to secure a webpage with a password.
I've done development with NativeScript but don't have an app yet on the store. It's a nice framework but the support can be lacking at times; Stack overflow can be a graveyard with the NativeScript tag and their Slack community can be unhelpful at times. But, I think you can iterate fast with it once you get a hang of it. React Native, while having a lot more support, imo feels too beta. A minor upgrade will break the crap out of your app and some of the docs/tuts out there are out of date; there are like 3 different ways to add a tab navigation bar and some of them are out of date (including the one on their site) wtf. My work's app is written in Xamarin but I never worked on it myself. It seems like an ok platform although I found the lack of live reloading kind of annoying but that may have changed in recent versions. There's a lot of options out there. You can go NativeScript, React Native, Xamarin. I'd avoid Cordova and Ionic and such just because of the performance hit. But if you're doing it alone I'd def recommend using a cross platform framework to iterate faster. And I'd recommend Google Firebase to get started. You can do everything with it just with the free stuff. Host a Node JS API and site with their hosting, use their NoSql db, etc. It's really a great platform for a hobby app to get started and then scale out.
Something like this might work for you. https://www.maxlaumeister.com/code/clientside-html-password/ It encrypts the html contents with a symmetric password, and then decrypts the page with JavaScript.
Even though /u/callmetom said http, I believe he referring to the technique's common name 'HTTP Basic authentication' and didn't mean to imply you wouldn't use it in conjunction with HTTPS. I agree basic auth (using ssl) would be the quickest/simplest way to protect the resource. 
&gt; single, static html webpage &gt; ASP script generate a webpage &gt; the script generates the rest of the webpage This doesn't sound like a static html page to me.
&gt;What would be the easiest way to convert a web app to use single sign-on? Single sign on means a number of different things depending on the environment and what authentication you intend to pull. Based on the context of your question though, the easiest way is to just pull the current windows identity by setting the authorization mode to Windows in the config. However, this may be a little tougher if the app is in the cloud rather than your domain. https://stackoverflow.com/questions/17224174/windows-authentication-for-asp-net-mvc-4-how-it-works-how-to-test-it &gt;That is, I need the user to be able to browse to the app page and the app would open a database connection using the user's credentials without having to type username/password. Please don't do this. I've found little reason for users to have DB access, and if your company is ever audited it will cause a headache. Have the app log in via a SQL user and have every connection use the same log in. &gt;Whatever the solution, would that still be applicable if the web app resides on an Azure server? Is your AD server in the cloud or on prem? Usually you can supply an LDAP connection string, although it looks like getting that set up on Azure is a bit more involved.
but it's not what I'm asking for is it??
A custom TT file that runs during build. It then finds the resx file, creates an interface with properties out of it, then creates a new class that implements that interface and getter just returns the resx property value.
interface with properties getter
Very similar public interface iGlobalization { string Label_Home; //property, not field } public class WrapGlobalization: iGlobalization { public string Label_Home { get { return ResourceFile.Label_Home; } } }
I am thinking about this path. My plan is to categorize the resx file and throw each of them into their own microservices.
I do .NET dev using my mac all the time but it is done using virtualization. Nobody I know of uses .NET Core so you're most most likely encountering a .net app (of some sort) written using Visual Studio on Windows. To do this, use test drive versions of tools: 1. Download VMWare Fusion Trial (which is good for 30 days) 2. From Microsoft, download test drive of Windows 10 or Windows Server 2016 and install virtual machine. 3. Install free version of Visual Studio 2017 Community Edition or Visual Studio 2017 Enterprise (evaluation) into the virtual machine. Did they tell you what type of app it is you're supposed to be running? There are multiple types of apps for .net - winforms, asp.net web forms, mvc, console, etc. etc. The dialect of the language is the same regardless, it is more how interaction with the user is coded which is unique to the various environments. My suggestion to you would be, once you have your environment set up, check out some of the free tutorials for .NET and VS on the web. Lastly, your app you're inheriting, if it is like 99% of all the other .NET apps, will require a database to be installed as well. On Windows, that would most likely be SQL Server. You can install a free dev version of that on your system as well. 
What's that?
When Microsoft teaches Xamarin they encourage you to still learn native technologies because they're necessary, don't code cross platform blind because you'll be guaranteed to fail hard a few times (I'm taking about production business apps, basic shit notwithstanding). The code reuse you should expect at a maximum is between 85-90% which is phenomenal, much less if you're using many platform specific features. You also get access to all (most) native C# and .NET functionality including everything Azure. It's a no brainer for the modern dev who wants to code once and use everywhere. Cordova doesn't compare at all in this context and shouldn't be considered, especially since you're already a .NET dev. It's worth checking out the old Winter of Xamarin content from Microsoft. The guy who ran it is a Microsoft Evangelist, and this is his repo. I did this basic primer event and it was fantastic. https://github.com/ovishesh/Winter-of-Xamarin
SQL Server allows you to authorize a machine to connect via the machine account. That way, you don't need to mess with service users (some companies require passwords to be cycled, and service user passwords suck to change, so machine accounts are used instead).
Good choice! There are a ton of products for hosting APIs and it's easy to get overwhelmed. EC2 - just a plain ol' VM that runs some flavor of linux or windows. You could go this route, but there's a lot of setup required as you just have a bare VM and need to install all the software you need. Elastic Beanstalk - analogous to Azure Web Apps/App Service. This is a wrapper around EC2 that makes it easy to deploy code. also includes monitoring, app configuration, etc out of the box. I recommend it as an easy way to get started. I use this in production and have had no issues. FWIW: My backend is actually written in Golang, not C#, but they have a tutorial here for .NET Core which you might find useful: http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/dotnet-core-tutorial.html Lambda - serverless computing, analogous to Azure Functions. abstracts VMs away from you. Basically, your API gets deployed and you don't worry about hardware at all, and each request to your app might be served on a different VM than the previous one. It's a very cool concept, but I wouldn't recommend it for C# development because management is a pain (using AWS' recommended approach, you have to deploy 1 lambda function per API endpoint, which becomes unwieldy). HOWEVER, if you're willing to write your API in Python, I've heard good things about this framework which greatly simplifies management (allows for an entire python app to be deployed to 1 lambda): https://github.com/Miserlou/Zappa tl;dr: go with Elastic Beanstalk for the least amount of headaches upfront ;)
Thanks! I'm still unsure about the charges around the different products (even with the calculator). Do you know of Elastic Beanstalk is more expensive since I'd imagine licensing comes into play? Also, any idea around using SQL Server vs Postgres? I can't seem to find any pricing difference, but that could be because I'm completely new to AWS and am struggling getting my head around things haha. Cheers.
1. Elastic Beanstalk has no additional charge on top of what you pay for EC2 VMs. EC2 is more expensive than Lambda, but still shouldn't cost you much. Also, if you run .NET Core, you won't have licensing concerns because should be able to deploy to Linux. 2. Yes. Trying to compare apples to apples here: the smallest SQL Server Web Edition instance (you don't want express, it has too many limitations for production use) on RDS costs $0.144/hour (~$108/month) while the cheapest PostgreSQL instance costs $0.018/hour (~$13/month) =&gt; 8x cheaper to use PostgreSQL on RDS.
Thanks so much for your help, you're awesomely helpful and quite clearly knowledgeable. Postgres it is! Re: EC2 costs. Any idea what I'd be looking at there? In trying to get an idea of a ongoing monthly cost so I can determine my pricing model (once I actually have a product haha). Thanks again!
No worries. I pay for a t1.micro instance and that costs me $0/month with the free tier. Normally it costs $13/month. Not sure what size VM you'll need for a C# API. I went with Golang specifically for the very low RAM usage which means my API uses 2mb of RAM when it's idling, and not much more under load, so I can get away with the cheapest instance for now.
Right, and mocking that interface takes a while because it has so many properties. I mean, it's not a terrible idea to do it that way, because then you don't have any magic strings / get compile-time checking / intellisense... but, it would be faster to mock an interface with a single method like "string GetLocalized(string key)", and maybe also generate a class with the keys like "public static LABEL_HOME = "LABEL_HOME";", so you could write Globalization.GetLocalized(GlobalizationKeys.LABEL_HOME); I mean, that would improve performance when generating the mock, since it only has to mock a single method. 10,000 times faster, probably. You could probably do a find/replace to swap it out in your code base. It feels crazy to do it to optimize unit tests, which is not a motivation I've often seen for a code refactor. But, if you are also motivated to stop using resx files, in favor of another data source, it would be much easier to implement IGlobalization { string GetLocalized(string key); } than an interface with 10K methos (practically, as a different TT file). You could also break your monolithic IGlobalization interface down into sub-interfaces, like IGlobalizationAccounting, IGlobalizationPurchasing or whatever (yet again a TT file change) and then have the existing interface inherit from those like IGlobalization : IGlobalizationAccounting, IGlobalizationPurchasing, so that IGlobalization would look the same. Then you can, going forward / with refactoring, have the classes using that interface only use the more-specific one that they need. Since the tests only need to mock that more specific interface, with fewer properties, it will speed up. But, I mean, blech.
React native is probably the best cross platform framework to use with legs.. if you have the time.
Hey thanks! Just accepted you answer. Sorry for the christmas delay and thanks again for helping out!
No worries!
&gt; Please don't do this. I've found little reason for users to have DB access, and if your company is ever audited it will cause a headache. Have the app log in via a SQL user and have every connection use the same log in. Letting every user access the database using the same user account also means that if a hacker finds a way to exploit your web service, they will have access to everything in your database. Restricting access to the database, based on the user who connects to the web service, provides an additional layer of protection against bugs and exploits, such as SQL injection.
I'd take a look at [TensorFlowSharp](https://github.com/migueldeicaza/TensorFlowSharp)
Is this an AI experiment in automatically generated articles? That's the only valid excuse. 
do what I did and just give up any hope of using tensorflow in anything besides python
My guess would be that you have to use Firebase API: [API Documentation](https://firebase.google.com/docs/reference/js/)
If you want anyone to use this you're going to have to tackle the elephant in the room of 'why this over Newtonsoft Json.NET'. I can't see anywhere on your website or github as to why we should this over the myriad of other options available. 
Just stepped in as a maintainer, will add more info to site and repo, One advantage is that it's leaner, just a few classes so you could easily embed it in your code. It's not a new project, it's 11 years old.
Hey for free just look at the SQL thats generated by the Entity Framework that hits your database then you might need to analyze that query and make indexes
The advantage of a js accessible JWT is that your application(s) that manage authentication and authorization can be separate (different domains/paths) from applications that offer the various secured services of the site. This comes at the cost of exposing the JWT to XSS vulnerabilities that happen to exist in the site. If you aren't going to realize that advantage, there is little reason to use a JWT over a secure HttpOnly asp.net auth cookie with claims. --- You can think of a JWT as something like a cross platform standard for the asp.net auth cookie. It is basically the same thing as the auth cookie or a SAML token. The advantages of a JWT over an auth cookie come down to the simple fact that it is well documented and has implementations in multiple languages. Over a SAML token: a JWT is smaller.
Full-text searching is a non-trivial problem. SQL Server has some full-text indexing / searching capabilities, but it will not be as powerful as a search solution like Elasticsearch. My two cents.
Try the [guide](https://docs.microsoft.com/en-us/dotnet/csharp/) to get up to speed on .net, then check out the [asp.net core tutorials](https://docs.microsoft.com/en-us/aspnet/core/tutorials/).
What snarfy said. These are high quality guides straight from Microsoft. Pluralsight is also a favorite around these parts.
Thanks!
A solution: 1. A resx file exists with every key defined as a valid identifier. 2. A TT file reads the resx and generates a big `ResxKey` enum. 3. there exists an interface: public interface IStringLocalizer { string Get(string name); } 4. calling convention (perhaps enforced by an analyzer): Home.Label = _strings.Get(nameof(ResxKey.LABEL_HOME)); `ResxKey.tt` file (untested): &lt;#@ template hostspecific="true" language="C#" #&gt; &lt;#@ output extension=".cs" #&gt; &lt;#@ assembly name="System.Windows.Forms" #&gt; &lt;#@ import namespace="System.Resources" #&gt; &lt;#@ import namespace="System.Collections" #&gt; &lt;# string comma = ""; ResXResourceReader rr = new ResXResourceReader(this.Host.ResolvePath("MyFile.resx")); #&gt; namespace MyAssembly.Globalization { public enum ResxKey { &lt;# foreach (DictionaryEntry entry in rr) { Write(comma); WriteLine(entry.Key); comma = ","; } rr.Dispose(); #&gt; } } --- You could make the interface use the enum, but then you would have to do `.ToString()` at runtime before looking it up in the file if you want (beware of order changes then; I don't think this is worth the potential headaches). It would be nice if T4 had some sort of `&lt;#@ watch #&gt;` directive that VS could use to trigger template generation upon file changes to get rid of some of the phantom intellisense errors this style solution brings to the table.
Thanks for the tip! We have a small dev team (one front-end, one back-end) and back-end is loaded up with other tasks right now. We checked the SQL that gets generated and it sure is ugly! Figured it would be beneficial for us to have a third set of eyes look at this without being interrupted with the day to day management we have to deal with.
Great feedback! Our e-commerce currently only handles a few hundred SKU's, and our search parameters are pretty trivial for now. ElasticSearch is something we will migrate over to sometime in 2018, but for today we just wanted to optimize what we have now.
I would be interested in taking a look. PM me when you get a chance. 
I'm interested. Sounds like a straight-forward-enough problem.
If you have not already done so research temporal databases it is a database design that keeps track of changes over time.
PM sent!
sure, send me an email to my reddit username + @gmail.com
For starters, I have some questions: - Which are the online solutions you have considered? - How do you intend to use the tracked data? - Shouldn't you add a `VersionId` alongside the previous and original version ids? I think that the approach you describe could work. I've made history tables before using a trigger. That involved 2 tables, one with the current data and another one with all the versions per record. It's fine for a basic audit trail, but less than ideal when the schema changes. I'm looking for a better way, so I find this question interesting. I've just read the following articles: - [Track Data Changes (SQL Server)](https://docs.microsoft.com/en-us/sql/relational-databases/track-changes/track-data-changes-sql-server) which describes two methods of change tracking in SQL Server 2017: `Change Data Capture` and `Change Tracking` - [Change Data Capture](https://www.red-gate.com/simple-talk/sql/learn-sql-server/introduction-to-change-data-capture-cdc-in-sql-server-2008/) which gives a pretty detailed description of how this feature works _Change Data Capture_ does track the changes but I don't like the structure of the change table. _Change Tracking_ does not capture the history and seems to be intended for synchronisation purposes. So I don't like either of those options. [Event sourcing](https://martinfowler.com/eaaDev/EventSourcing.html) is another approach that keeps track of who changed what and when. It turns things around: the application state is derived from the event log. But it also introduces new challenges. It's getting late. I'll be interested to see what other people post.
We use a combination of statsd and grafana for our akka.net services. There might be something like it for Orleans. It's pretty useful when you have metrics graphed for things like received messages or failures in an actor.
Actually, with the added columns to the table, Track Data Changes could work. As you say, though, the data format looked less than great. If I recall, you have to supply it a range of entries to query. Wasn't enthused about that. I didn't look too closely though. The other thing I looked at was the myriad people saying to use triggers. Still not clear on how efficient that would be. I also don't like having to clone the table, because I'd have to make model changes in two places. I could have the table point to itself, though, with an indexed bool column for active and inactive entries. The main reason I didn't want to do that was to avoid querying the whole history. Would an index prevent that? I a bit fuzzy on how those work.
The official Microsoft docs
This comes at a pretty high computation rate, and performance hit BTW
You need CQRS - it makes your database an audit log and generally avoids write locks. 
Strictly speaking, CQRS says nothing about the data storage. You're thinking more about event sourcing and/or command sourcing, which are usually used alongside CQRS (because they just make more sense together).
Our DBAs have added a stored procedure which (I don't have the specifics) does roughly: - Creates the *_audit suffix tables - Triggers on row updates - Tracks what changed in the row You can likely capture the SQL user that made the change, but that's far divorced from the application user. For that, you need to have last-updated-by ID column on the table and hydrate that property in EF before calling SaveChanges(). In my experience, it's pretty rare that the application users have logins to the database. If it's a cloud app, you usually have a single connection string which the webserver uses to talk to the database, which is a per-application account. Tracking which user of the application did something gets handled in the code layer.
Cost can be an issue for Auth0 (we've been looking into it). We currently use [IdentityServer3](https://github.com/IdentityServer) as a base for our stuff, looking at IS4 and possibly some more custom code on top of it.
Visual Studio for Windows running in a Win VM (via Parallels) is my daily grind. JetBrains Rider (if it's a .NET core application, which is unlikely) if you want something macOS native. You could use Xamerin (a.k.a. Visual Studio for macOS), but I've heard it's a bit rough.
An approach I’ve used in the past to great success is a manually built “history” table. First, create a “rowversion” column on the parent table. This is essentially a computer column that SQL automatically increments on INSERT/UPDATE. Next, create a “{table}History” table. This table should duplicate the columns of the parent table, with a few exceptions. First, the PK column of the parent table should just be a regular column in the history table. You’ll create a new PK column for the history table itself, since each row in the parent table may have multiple rows. Second, the “rowversion” column should be created as a binary(8) instead of “rowversion” since want to copy the computed value. Other audit columns are up to you: InsertTimestamp, Action, UserId, etc. How in terms of tracking the changes, you have two options. You could setup a trigger that inserts into the history table on each insert/update to the parent. But this means you might loose some application context like application users. Alternatively, using your ORM, setup hooks that will automatically insert into the table after the parent entity is written. If you design it right, you could make this work based on an entity interface or attribute to easily support multiple history-tracked tables with some shared code. What I like about this style of history table is the “rowversion” column: it’s a really simply way to order changes per parent PK. And “rowversion” also comes in handy for other, unrelated, things like MVCC and dynamic UI auto-refreshing.
How often does your data change? Caching might be an option to take some of the heavy lifting if your queries always return the same data. I'd be interested in taking a look just out of curiosity. Let me know if you'd like to talk and I'll pm you my linked in. 
Your db design is fine. Buy you really need a command pattern in your .net code. Every user action (granular and high level) becomes a command and the command base class handles policy like how and where to log the command. Do this instead of triggers. You could still write to those audit tables through your command logger or you could have one unified command log table.. depending on your logging needs. Or do both.. the master log table contains references to the other tables that store the actual before and after data. While the master log stores who did what and when.
We've had good success using this library - https://github.com/thepirat000/Audit.NET Also if you're using Entity Framework there's also https://www.nuget.org/packages/TrackerEnabledDbContext/
CQRS may involve a lot more than the OP bargained for. &gt; Suppose I want to track who changed contact information and what it used to be, who created the first entry in a series of edits, etc. 
Yes it is.
The command pattern is a great suggestion, not a necessity IMO. If you have dba's who can write procedures for you like /u/wuphonsreach described that could be a fine solution as well.
It's likely to be more secure, than whatever you can write yourself. I would think that it is easier to spot errors in your forms authentication config, than in your custom code.
&gt; Would an index prevent that? I a bit fuzzy on how those work. [Use the index Luke](http://use-the-index-luke.com/) is a great resource for brushing up your indexing skills. Depending on your query needs you might want to consider a separate table for the active state and another one for the audit. If you use code first you could slap an interface on the table entities to keep their properties in sync. In my experience 'active' bits can be error prone.
One thing that doesn’t sound secure, is “encrypting” your users passwords. They should be “hashed”, one way, not reversible, probably using bcrypt.
Pluralsight Here's the asp.net core fundamental course ASP.NET Core Fundamentals | Pluralsight https://www.pluralsight.com › courses › a... And here's the MVC 5 learning path to take you to the advanced level https://www.pluralsight.com/paths/mvc5
Depends on the exact algorithms used. I'm not familiar with the old ASP.NET stuff, but [this page](https://docs.microsoft.com/de-de/dotnet/api/system.web.configuration.formsauthpasswordformat?view=netframework-4.7) seems to indicate that Forms Auth is using unsalted single round hashes, which isn't at all acceptable for password hashing even if you don't use the broken MD5 or SHA1.
At the very least, if you can't use any 3rd party libraries because of company policy etc., use Rfc2898DeriveBytes (a.k.a. PBKDF2-SHA1) with a very high (10000+) number of rounds. Don't just use a normal 'hash' like MD5/SHA1/SHA2. Bcrypt/scrypt/PBKDF2-SHA2 are all better of course. Use them if you can! Just pointing out the minimum safe alternative that works anywhere.
Yes, you can use express for free. You do miss out on some features though such as SQL agent which is useful for scheduling automatic backups.
Not to mention CPU core and RAM limitations.
Yes, I forgot about that, 1 physical CPU (but multiple cores) and 1GB RAM
I found [this](https://stackoverflow.com/a/4936090/313253) SO answer helpful to get some insight in the security of the cookie. 
This actually sounds like a fun challenge. I'm up for it
&gt; 1GB RAM As someone who hasn't been on the sysadmin/ops side of any sort of database work. Is that really sufficient even for a small amount of users? That sounds so incredibly limited when you consider all the work a db engine has to do with joins and whatnot.
And size limitations on the database. But for basic use cases it will be enough.
I’d say the CPU is the more limiting factor in my experience. I wonder though if the RAM limit is a total memory limit or whether you can make use of paged disk space, I’m guessing not. That said if you’ve got a lot of data then you can probably afford to cough up for a proper license.
Amount of data does not equate to more money. I have a client ehobhas been in business for 20+ years with at least 10 to 12 years of data in their accounting and it is well into the several gigs worth of data. I don't have an exact number but last I checked it was around 12 gigs. That is a fair amount if data and they are broke even at their peak they weren't a rich company. 
I'd agree with this. Most basic databases will easily fit in 1gb ram but only using 1 CPU will hit performance with concurrent users. But as /u/angrathias said, by the time you start getting problems you can probably afford the license.
We normally use an Express edition for customers with less than 15 concurrent users and 5 gb of data. Also when you have more data and is limited on memory, the sql server will use the disks a lot more. So you can compensate some of it with SSD's
Yes you can definitely use Express as others have confirmed but I have to say you are at a great stage right now to not use SQL Server. All my projects use it and it's a great DB system but I regret it simply for costs. I'd look at PostgreSQL or Mariadb. Unless you are truly using features in SQL server save yourself the licensing headaches down the road.
Looking through its history seems to be a bot trying to promote a low-quality blog.
&gt;I a bit fuzzy Hmm, now I sound like outsourced help. Fixed that. Anyhow, *Use the Index, Luke* is a great resource. I'll be churning through that all today. I would love to use code-first. An interface would solve a lot, and to be honest I'll be doing most of the work for code-first anyhow, but the dev here before me needs a role in the project, and he doesn't know code-first (or relational database concepts.........), so he'll be in charge of managing the database via SQL. The owner is old-school and hasn't done development in 20+ years, and he trusts the other dev. I'm not bitter, though... not at all... I agree with you that I'd like to keep the active and historical records separate, but what sorts of errors have you run into with active bits? I'm mostly curious.
Agreed! If you want to ever scale, such as use more than 4 cores or have a database larger than 10 gigs, use something else
This is probably the best idea for my context. We have a small team and are locked into some design decisions where one guy is more or less going to do nothing but SQL (even though I'll be doing 90% of the work for code-first, anyway...).
Ooh, yes. Command pattern is probably the best solution here. I'm just not sure I can sell it to the lead, in part because it cuts him out of doing db design the way he's used to doing it. I'll give it a shot.
OP is talking about using it for multiple clients. It'd probably be best to purchase a hosting environment.
This is nice, still won't be using Git through VS much though...
The MVC5 learning path seems to be pretty old. Do you think its still relevant to watch? I'm also somebody who is looking to get into .NET
I know, and he is talking about multiple *small clients*, thats the reason to use sql express. Small clients with small dbs, no need to pay for anything. If one of the clients needs a multi Gbs DB, then he will have to charge for that, no matter if on premises, cloud or hosting. I even think that if he has a different instance for each client, assuming he is going to do the hosting, it will be completely legal. Most shared hosting solutions just offer sql express, so probably it's ok... It has been a while since i dont care about sql licenses. 
He is using EF and altough is supposed to be DB agnostic it clearly is not. Is a lot easier to use with MSSQL than with anything else. And if you start to specialize with something not "standard" you will end up paying the decision with a lot of developer hours too... As he said, its "small jobs", probably he shouldn't bother in something bigger than SQL Express, or wasting time in integrating other solutions. (I know it works, i have projects that use MySql and mongo and work fine, but the amount of hours needed to set them up and bug fixing are way bigger than for MSSQL)
What features does it not have that make you not want to use it?
The title made me want to see a punch line delivered...:'(
For sure, I'm not gonna deny MS SQL is easier to use, its when you outgrow it (which at some point you will whether its a feature limit or resource limit). So ya for small jobs sure - acceptable choice, I was just pointing out that back when I first started I wish I had more exposure and started some of my projects with free databases and the licensing down the road is painful. It's not fun to see that you can run your .NET core app on a slim Linux server for pennies and hour only to see that a SQL license will cost thousands - but you are right I guess I'm off topic from the original question asked.
If you have incremental version numbers the active bit is redundant in a way. On every insert you have to remember to set the previous record to inactive and the current to active. I made some silly mistakes when in a hurry. From what I gather, I'd say just start hacking on the trigger solution. Good luck :)
The UI is pretty terrible. That’s my biggest frustration with it. I always go back to command line cause it’s way faster for me.
The active bit is helpful even with version numbers, unless I'm missing something. Suppose I wanted to query all contacts at a particular company. I'll write it in LINQ-to-SQL since that's easier... from var contact in _db.contacts where contact.CompanyId == someCompanyId &amp;&amp; contact.Active select contact I'm not sure what I'd put in the `where` clause with version numbers, and nothing springs to mind for SQL, either. Was there some other method you had in mind for such a thing?
Just throwing this out there, I'd bet if you threw your code on here, people would be more than happy to take a look at it for free. 300ish lines isn't much.
Sadly I don't think either hits exactly what we need, but they're close. Thanks.
I have the same doubts from time to time, but when you think about it, a sql server web license (or whatever is called this year) is pretty good and is about 500$... If you are running a business, 500 is no money for such a good product. Of course, buying enterprise licenses becomes crazy confusing and expensive. But most of the times not even the MS people know what they are selling.
Cool concept, but it's probably a bit much for our project, and it would likely go over the heads of several people on my team. Thanks, though. I'll keep these in mind for the future.
It's still down, if you're keeping tabs on this. I'm guessing your article got too popular.
I feel the same. The ui adds an extra layer that makes it harder to understand what you are exactly doing. It's easier in the end to do it in the command line. Plus that knowledge isn't limited to the IDE. 
General ASP.NET MVC patterns apply. Go through the tutorial/intro and you’ll notice them emerging naturally.
Microsoft offers a [Windows 10 VM with Visual Studio 2017](https://developer.microsoft.com/en-us/windows/downloads/virtual-machines) in about 4 different VM formats. Free and Good for 3 months.
Why post this now? Update 5 has been out for the better part of a month.
RAM becomes important for speeding up data-intensive operations such as indexed queries and joins. On very large tables, the index alone could eat up a pretty good chunk of RAM, and of course, SQL Server's caching algorithms come into play. Perhaps 1GB is reasonable for most desktop-database applications, and maybe even truly lightweight web apps.
Or look into cloud services.
It's indeed helpful. Don't worry too much about that 'error prone' remark. Since you asked, there have been cases where I could avoid active bits it using an index, but then I just needed one column. The index would look like this: create unique index IX_CurrentStatus on DeviceTimeline (DeviceId, [Date] desc, StatusName) The LINQ query would look like this: from device in entities.Device let currentStatus = entities.DeviceTimeline .Where(s =&gt; s.DeviceId == device.Id) .OrderByDescending(s =&gt; s.Date) .Select(s =&gt; s.StatusName) .FirstOrDefault() // --cut-- // It's verbose but pretty fast since it only reaches the index to get the curent status. An extension method could help making it easier to read but I only needed it once.
This name is terrible. Might as well have called it MemeJSON.
How is it only "one way", though? If a user types "abc123" as their desired password and that encrypts to the database as "7F9S6G0A5D7F5", you technically have the key(s) to do that encryption... so in theory wouldn't you now also have the key to *decrypt* it?
Power users won't want to use the GUI, it's just too limited in comparison to what you can do with [`git`](https://git-scm.com/docs). But the GUI is still great for those who are not interested in using a shell and more power to them.
If you're going with RabbitMQ, I will suggest using MassTransit as a .NET wrapper. It will make this task easy
Hashing is basically calculating a number from some data. A lame sum hash of “abc123” would be sum of the ascii values for each character a:97, b:98, c:99, ‘1’:49, ‘2’:50, ‘3’:51, so sum(“abc123”) == 444. 444 is not encrypted, it’s hashed. Because there isn’t a decrypt function that takes 444 and produces “abc123” that also works with other summed strings. However you can easily make a database of all dictionary words and common passwords and what their sum hash values are. (called a rainbow table) “abc123” = 444 “abc124” = 445 “123456” = 309 “password” = 884 The same thing can, and has been, be done with md5. So always add “salt” right? (yes but...) Salt like “_lamesalt” added to the end of every password before hashing produces: “abc123” = 1390 “abc124” = 1391 “123456” = 1255 “password” = 1829 So maybe someone hasn’t pre-calculated words and your salt yet into a rainbow table. *But* it’s still very easy to make this table. In fact you don’t need to store the result in a table because computers are fast enough, something like $6 in cloud server time can calculate most md5 hashed passwords with a new salt. So thats why you use bcrypt, because bcrypt is deliberately slow. It is a hashing algorithm, not encryption despite it’s name. But it’s so slow to just do one hash that if you use a salt and bcrypt it will take too much time to generate a rainbow table. Better advice about this here: https://www.owasp.org/index.php/Password_Storage_Cheat_Sheet They recommend pbkdf2 instead of bcrypt.
I really wish VS would handle submodules properly. If I make any submodule file changes, they don't show in pending changes and I'm forced to use the command line to see what I've done.
So what I'm gathering is I'm never "scrambling" or encrypting a password, I'm just generating some kind of hash from it and then salting the hash, so if someone wanted to figure out a user's password they would need to have compromised/obtained 3 things: the pattern I used to create the hash, the salt I used, and a rainbow table to map that salted hash back to the "real" password. Am I on the right thought process here? If so, I'm confused what role bcrypt plays and why the "speed" matters... How is a custom-made pattern of hashing a password any less secure than using a pre-made library? How is a library like bcrypt making generating a rainbow table "slower?"
They don't even use proper Git terminology. There are actions like "Sync" and "Update"
Before I start, learn Core. It's going to be the new standard. But also learn MVC. And yes, it is still very relevant. For the learning path, there *will* be updated videos, the curators are usually really good at updating the learning paths. I'll clarify why I linked what I did. OP asked for .NET Core hence the first link, no brainer. **I'll preface the rest of this by stating that I like Core, and that support for what I deem to be industry standard packages is getting better by the day.** The issue that actual business is facing with Core is that it's not hugely supported **yet**, and many libraries that are deemed standard in business applications are still not past beta stages which of course takes time. Additionally, I've had many issues with NuGet packages that are meant to be working with Core but are seeing instability and general borkiness; Swagger immediately comes to mind. Azure feature integration just doesn't seem to be 100% either. On the face of it these aren't problems per se, unless of course you're working on anything even a year or two old in business (which has got to be almost everything ASP.NET MVC that's actually deployed out there). Core won't help you at all in isolation. If you're building from scratch today, absolutely go ahead and learn Core, but don't discount MVC if you want to be employable because most ASP.NET code out there today is MVC. I'll close with my personal perspective: I branched our farming web app and started the migration to Core, and let me tell you, what a cluster fuck. Deprecations alone blew the time scope to shit, and initial analysis told us that it was going to be in the vicinity of 300 hours just to get the app working again, excluding the planned upgrades we wanted to make to take advantage of what Core offers. And even if we did, initial benchmarking showed that our actual deployment to Azure may have been effected. Downtime for customers we're charging a significant amount of coin to is not really an option, neither is instability and adopting tech that's too new. We don't need to be on the bleeding edge, it's not what we've promised our customers. We need stability. And that precedent takes priority, immediately putting Core on the back burner (as part of a complete rewrite project vs an upgrade). For us: Risk vs Cost vs Reward = don't do it (yet). Ymmv of course
There is "sync" (which really is pull then push), but I don't remember seeing any "update" option.
You are correct! Good catch!
There are settings for this in IIS which I assume you are hosting your web services calls from. Fairly easy to set, just google 'iis throttle requests'.
There's a few different ways you can handle this. Take a look at RabbitMQ or SQS on Amazon AWS if you're interested in AWS. There's also a task backgrounder called Hangfire which you could setup background queues. 
I would argue that your client needs to restructure the database to pull out archives into a different database. I work with clinical data (that grows very fast and nothing can be destroyed) and several clinics decided not to pay for a license. What they have done with their sysops is to create new instances of sql and move all the archived information to these new instances or a new database. Keep in mind that the limitation in sql express is applied to a database (with the exception of the memory that is applied to the instance), meaning there are ways to circumvent these artificial restrictions.
Hi /u/solrflow, SQL express should be fine for a low transactional volume site. SQL Server is very efficient utilising resources and you can always optimise your queries to decrease the memory footprint. Alternatively, if you are using entity framework, you can reconfigure your context to connect to a PostgreSQL instance instead removing all the restrictions SQL Server imposes. I've been working for several years with SQL Server and deployed to numerous clients with SQL Server having no issues. The limiting factor of the ram is going to affect the responsiveness of the server based on the number of active concurrent connections. A conservative number of concurrent connections is 20 (assuming you are querying a massive table and inserting into new temp tables with indexes and whatnot) but you can easily have a lot more than that at any given point in time. The CPU will effectively limit the processing power to do data analysis and join data together. Small jobs don't tend to request elaborate reporting and data analysis, you don't need to create data cubes nor query every table at the same time. If you do, check the execution plan to optimise your query to filter data as early as possible to reduce the size of the final join, decreasing the overall ram and cpu usage.
I've never ran into limitations with the GUI and have always wondered what features people use that it doesn't offer. I've been a shell user since I ever started being a developer. But I'm not going to use another tool if I don't have to just for the sake of being a 'power user'
Do you use Visual Studio? Thats using MSbuild under the covers. Do you also run that from the developer command line to have more control and know what's going on? 
I take it most are using TFS? I've been considering making a switch to Git, because GitHub, and because I like how branches work so smoothly -- but these responses have me second-guessing that decision. Thoughts?
Does having the Git plugin enabled still cause performance problems? IIRC it was one of the things both NCrunch and Resharper recommend to disable as it causes lag when changing branches etc.
What if you still queue it to background workers or a different process but then use SignalR to update the user when it is completed?
Same thing?
If you have a small/fixed number of servers you can easily add a semaphore to control access to the resource.
Speed matters, because if it takes 0.0001 seconds to generate a hash, then a hacker could search 10000 passwords in a second, on one CPU. Thats 864 million different passwords in a day. Even more on a GPU. But if it takes 1 second to generate a hash. Then they can only search 86,400 passwords in a day. Much fewer. And if every user has a different salt, that means they would have to start all over for the next user in the database, instead of cracking the whole database in one go. The reason you shouldn’t use a custom algorithm is that the custom algorithm was not written by an expert mathematician trying to make it hard to cause collisions. In my crappy sum custom function, “abc” and “bca” both hash to 444. There are many other collisions too, which means it takes even less time to get to that account. MD5 and SHA were written and reviewed by expert mathematicians, and specifically designed to avoid collisions, yet years later people figured a way to find collisions. A hacker will look at a custom function and think of it as a fun game to simplify. Like maybe a clever dev spends 5 minutes of cpu time xor’ing a hash on itself, and a knowledgeable hacker would laugh and only spend 1 cpu instruction xor’ing it to do the same thing. Also, maybe the attacker can’t see the whole database. Maybe they see just one record because the code is logging the record in an error, and the web server is serving that error to the user when they post to some malformed endpoint. Or maybe someone json stringify’s the user object with the hashed password in it and puts the use object in the front end JS. There are more ways than proper connection creds that attackers get information. Let’s say an attacker finds the above bug, and it has their own user’s hashed password in it. If they figure out a collision, or many collisions, then they can log into other users accounts by guessing passwords that collide with the real one. If the custom function was written poorly enough, something as easy as “” could open every account. That’s why we just use pbkdf2() and then go work on other bugs. It’s more important to use the best-in-practice methods, and spend that time that would be used making custom functions to instead try pen testing your site.
Last time I checked, the history view doesn't visualize all branches, like `gitk --all` does.
interactive rebase
&gt; How is a custom-made pattern of hashing a password any less secure than using a pre-made library? Generally, it's because pre-made libraries (which includes the algorithm and implementation) are created by individuals that have a set of specific knowledge that helps them _reduce_ the chances of making mistakes that could make something insecure. To circle back a bit in your comment, I'll give you an example: &gt; I'm just generating some kind of hash from it and then salting the hash Notice that SmilingRob stated "the end of every password *before* hashing", but your understanding has implied that you think _salting the hash after generating the hash_ is what needs to be done. Obviously, I have no idea what implementation you have in mind for salting it by this description, but I'll state that mutating the hash without extreme care will likely be reverseable. If it is, then you could pre-reverse all of the hashes in the database, and then use a rainbow table as stated by SmilingRob to figure out most of the passwords in the database. In fact, what you're implying by your statements is _not salting_. Of course, to people who aren't actively in your code base, your claim to be "salting" the passwords might make them trust that you know what you're doing and you're doing it correctly. If later on it's discovered you were doing "salting" wrong, it would undermine everyone's trust in you (and the company you're representing). One simple mistake in understanding undermines the entire scheme. This is why you _must_ rely on experts to implement that part of your system. I'll also note that your usage of rainbow table is making me also think that you may have a misunderstanding on that as well. I'll further note that despite SmilingRob's explanation being accurate, it's very incomplete (entire books have been written on this, he's just giving you a cursory introduction). If you fully grokked his explanation immediately, I'm certain the result you could create would be riddled with holes anyway. &gt; How is a library like bcrypt making generating a rainbow table "slower?" Well, for one, a rainbow table isn't something that is useful if you have unique salts per user. A rainbow table is precomputed hashes (e.g. abc123 will always hash to "444" in SmilingRob's example, so you don't have to run the hashing algorithm on that ever again once you compute it once). A salt introduced into the process of hashing the password will make each hash unique, so you cannot precompute the hashes and have that help you. bcrypt has the concept of salting built-in (and done correctly), so as long as you're _using_ it correctly, the generation of rainbow tables isn't just made "slower," it's solved completely. Once you've solved the precomputation of hashes, now you have a different problem: well, if you just throw a bunch of computing power at brute forcing each users' password, you might be able to figure out a few of them. All of the traditional secure hashing algorithms are fast enough that it's feasible to pay a few bucks on Amazon/Azure and get enough power to crack them (as long as the passwords aren't too complex). However, if you add some complexity (_carefully_) to the algorithm with the intent on making it slower, you can raise the costs of breaking a password from dollars to hundreds of thousands of dollars _fairly_ easily. Note that a properly motivated attacker would still be able break passwords for individual users, but not super quickly nor cheaply. Again, bcrypt already has addressed this by making the computational cost a parameter to the algorithm. And it's been carefully crafted so that there aren't "shortcuts" or ways to slip through the complexity. Although, the algorithm doesn't require a lot of memory to perform, so a mass number of specialty devices could be built _relatively_ cheaply by certain individuals to reduce the time/costs of cracking password. That's why there's newer algorithms like scrypt and, most recently, Argon2, which are built to combat _that_ weakness. That said, if you use bcrypt, despite the fact that it's not perfect, you're doing billions of times better than simple hashing and you've made it so that only "special" attackers with specific means can compromise _individual_ users' passwords.
I'm not sure if unmanaged libraries can be used on Azure App Service but there's a number of wrappers for Google's Pdfium.
If you want a pure ASP.NET Core solution (no full framework), an approach you can take is using NodeServices + jsreport. Here's a simple tutorial: https://code.msdn.microsoft.com/How-to-export-HTML-to-PDF-c5afd0ce Note: Instead modifying HttpContext.Response like they did above, I simply did: &gt; return File(result, "application/octet-stream", "report.pdf"); Two things you need to do when deploying to an Azure App Service: 1. Before you deploy, you need to set 'Copy to Output Directory' to 'Copy Always' on the 'pdf.js' properties. 2. After deploying, Run 'npm install' in the App Service console.
Cherry pick is horrendous, it seems to want to take the head commit of a branch and apply it to my branch,I think, but I can never tell, and usually I don't want that commit anyway, so command line wins. Why not a dialog where I can specify the hash or range I want? Interactive rebase is another. Better merge conflict resolution, right now I have kdiff3 installed and configured as my mergetool and it's great, can just run a single command and get prompted for each conflict or have the 3 way diff tool open. Merge no commit... If I'm reviewing a pull request,I may want to see how it runs in the target branch, in case the feature branch is not completely updated (e.g. Due to another just completed pull request). I don't want to create another temporary branch, I can just merge no commit, compile and run, and not have to worry about my local branch being out of sync with remote. Also useful for merging configuration changes that aren't all in one file but are linked, although that's less common.
There are a few versions of iTextSharp for netstandard/netcore on nuget (e.g. [iTextSharp.LGPLv2.Core](https://www.nuget.org/packages/iTextSharp.LGPLv2.Core/)). I don't know how good they are.
I've never liked using Visual Studio with GIT plugin. In a really huge solution there is significant performance hit. It is much better (and convenient) to use free Source Tree from Atlassian.
Thanks - I had used Hangfire/RabbitMQ when on-premise in the past, and Azure / Azure WebJobs. Trying to keep track of a User's position in the queue is my next problem to solve, I guess. Any recommendations? My thoughts are to store an ID that maps to the users session and a redirect URI, which will redirect the User when it dequeues.
Thanks for the SignalR recommendation - I had planned on polling so this should work out well. Need to read some more on it.
Will look into, thanks!
I don't think Hangfire processes in a guaranteed sequence and I'd imagine other messaging type queues will have a similar restriction (I've used SQS, haven't used RabbitMQ). You'll probably end up putting a lot of effort in to displaying a x/y queue indicator to the user, it might not be worth it. It will be a lot simpler to just display a "queued / processing pending" message. Let's say you're sending text messages - if you insert a text message into the textmessages table, you could track the pending / complete status on that record. If the user refreshes the page for that textmessageid, you can show them the status. You could write a Hangfire job Id onto each text message record, but I'm not sure you can ask Hangfire what number in the queue that job is (as per the first point above). The costly alternative would be to have a Hangfire job which runs every X seconds to fetch the next job in the queue which isn't complete. If you order by ID and always take the first, that would make it possible to find out the position in the queue for any given text message which isn't complete. I don't love this approach though. It will get costly continuously calculating the size of the queue. 
Thanks for the thorough answer. I do believe it would be easier to display a job status. 
The goal of the tools is to simplify your life as developer. For some it might work, for others it might not. For me personally git command line is far easier to comprehend, plus it is easier to find documentation about. Secondly though comparing version control commands with ms build isn't quite the same. Still it also helps to understand what ms build does under the hood. 
You can try SkiaSharp https://github.com/mono/SkiaSharp
Here's one more pro argument: Git is becoming the near standard for VCS. That means there are both a lot of tools for it from command-line, GUIs, VS integration and more and more developers that know how to use it. 
iTextSharp is what I use.
I don't go against anyone's decision to use command line. And your reasons are good and valid. What I always question, especially with our community, and I'm not an exception, is our ability to disregard tools because we want to be the cool hacker who knows cmd line and not use the things that actually would improve our development, processes, guidelines, and regulations. I.e. It's really a bigger thing than just git, and I'm just playing devils advocate 
We use SignalR very heavily in our systems and it works well.
I need something I can use commercially. My understanding is I need a paid license if I'm doing commercial things with itextsharp
SQS has FIFO queues. Probably overkill, though. All the cloud queue services are roughly ordered, usually close enough 
Thanks I'll look into it
Thanks for the advice. It does seem like Core is the way forward but at the same time there are very few jobs advertised that require it which indicates that its not particularly widely used (yet?). Ok I'll learn MVC, then Core later, I think that ought to be the first step. I'm assuming that it'll be easy to pick up Core once I get proficient enough with MVC?
You can try [Syncfusion pdf library](https://www.syncfusion.com/products/file-formats/pdf) They have a community license too.
We've tried a bunch of different ones years ago including: * iTextSharp * Aspose * Amyuni * Syncfusion Of the ones we tried, Syncfusion was least bad. Our current design is basically to create a mail merge word document, merge with form data and then convert to pdf. Syncfusion does that ok. We strongly believe it has significant memory issues. To work around them, all interactions we do with these dlls are done via a separate console app that our web application proxies out to and then tells IIS to serve the resulting file.
Their library also requirees GDI+ and is thus unavailable in Azure App Service
From EF course: public interface IModificationHistory { DateTime DateModified { get; set; } DateTime DateCreated { get; set; } bool IsDirty { get; set; } } protected override void OnModelCreating(DbModelBuilder modelBuilder) { modelBuilder.Types().Configure(c =&gt; c.Ignore("IsDirty")); base.OnModelCreating(modelBuilder); } public override int SaveChanges() { foreach (var history in this.ChangeTracker.Entries() .Where(e =&gt; e.Entity is IModificationHistory &amp;&amp; (e.State == EntityState.Added || e.State == EntityState.Modified)) .Select(e =&gt; e.Entity as IModificationHistory)) { history.DateModified = DateTime.Now; if (history.DateCreated == DateTime.MinValue) history.DateCreated = DateTime.Now; } int result = base.SaveChanges(); foreach (var history in this.ChangeTracker.Entries() .Where(e =&gt; e.Entity is IModificationHistory) .Select(e =&gt; e.Entity as IModificationHistory)) { history.IsDirty = false; } return result; }
Generally I don't think one should do any significant background processing on a web site, of any kind. Parallel or not. Data transformations are fine. But anything that would take more than a second? No.
How else should I then do the process? On a separate console application? If it is on a different application, how should I get the data from the other application?
Yes, look into distributed computing. Queues, azure functions, lambdas etc
It is a logical progression. Don't get me wrong, there's no reason not to learn Core first, just don't at all discount MVC because that's what most of the ASP.NET code is in business at the moment (companies refusing to upgrade from aspx notwithstanding of course). Good luck
Web apps are inherently... faulty. The server should be free to recycle your process at any time, resetting all "variables" and state. They should be free to scale horizontally, users should be free to hit different instances across a cluster, etc. So yeah, a service that runs in the background. There are numerous choices. From Windows Services of the past, to Console apps of the present, to Azure Jobs, to Service Fabric.
Take a peek at this library https://github.com/NickStrupat/EntityFramework.Triggers full disclosure: I wrote it
Nice!! This is what I am more accustomed to seeing in Laravel so that's awesome. Even better your example shows exactly what I need! Appreciate the response..
If I put it in a separate application, can I still use parallel processing or is it also impacting the web server?
Let's be clear that when we say "processor", we mean a physical CPU chip, modernly with multiple CPU cores. Some of those might be pseudo-cores, more for hyperthreading than actually completely separate processing. What you can do is limit the number of cores and or processors being used by the web server, and use different one(s) for these other processes, with an interprocess communication technique of one or another kind to synchronize. This will keep the workloads separate. Look up "processor affinity".
For greenfield projects I'd stay away from MSSQL. You can use Express for free for commercial projects but you're severely limited by the RAM restrictions. The cost for licensing is pretty steep imho and since you say you're doing small scale stuff I'd guess like most small businesses the budgets are tight. Postgresql is likely a better option.
There is nothing inherently wrong with parallels or long running processes, but there is something inherently wrong with making your webforms site hang while it waits for the request to finish. Something I've grown accustomed to doing is having a separate server running Hangfire and sending long-running or fire-and-forget processes off to that to be taken care of.
Look into message queues. You can use stand alone implementations like rabbitMQ or the ones built into various cloud services (azure service bus).
Need to improve the presentation somehow. Just a wall of text.
&gt; Rools
Can you make the controller simpler? Why is it talking to a database? Maybe you need a service layer, that talks to a repository layer, which then talks to the DB. Then you might be able to test each of those classes more easily. 
I meant accessing the db via repository! Here is one of the controllers: ``` [Route("[controller]")] [Authorize(Roles = nameof(Role.Admin) + "," + nameof(Role.Agent))] public class AgentController : Controller { private readonly IEntryAgentRepository _repository; private readonly IAgentStudentService _agentStudentService; private readonly IAgentManager _agentManager; private readonly UserManager&lt;Agent&gt; _userManager; private readonly SignInManager&lt;Agent&gt; _signInManager; private readonly IAuthorizationService _authorizationService; private readonly IHostingEnvironment _environment; public AgentController(IEntryAgentRepository repository, IAgentStudentService agentStudentService, IAgentManager agentManager, UserManager&lt;Agent&gt; userManager, SignInManager&lt;Agent&gt; signInManager, IAuthorizationService authorizationService, IHostingEnvironment environment) { _repository = repository; _agentStudentService = agentStudentService; _agentManager = agentManager; _userManager = userManager; _authorizationService = authorizationService; _environment = environment; _signInManager = signInManager; } // GET: Agents public async Task&lt;IActionResult&gt; Index(string searchTerm = null, string applicationStatus = null, int studentPage = 1) { ViewBag.SearchTerm = searchTerm; ViewBag.ApplicationStatus = applicationStatus; var agent = await _userManager.GetUserAsync(User); //var identity = (ClaimsIdentity)User.Identity; //var claimsPrincipal = new ClaimsPrincipal(identity); var agentStudents = await _agentStudentService.GetAgentStudents(agent); var filteredStudents = await agentStudents.FilterByApplicationStatus(applicationStatus) .FilterByNameOrEmail(searchTerm) .GetLastTen() .ToListAsync(); var result = filteredStudents.ToStudentDetailsViewModels(_environment); return View(result); } // Get: Agent/SetupAccount [Route("SetupAccount")] public async Task&lt;IActionResult&gt; SetupAccount() { var userName = HttpContext.User.Identity.Name; var result = await _agentManager.GetSetupAccountViewModel(userName); return View(result); } // Post: Agent/SetupAccount [HttpPost] [Route("SetupAccount")] [ValidateAntiForgeryToken] public async Task&lt;IActionResult&gt; SetupAccount(AgentSetupAccountViewModel agentSetupAccountViewModel) { if (ModelState.IsValid) { var userName = HttpContext.User.Identity.Name; var agent = await _agentManager.Edit(userName, agentSetupAccountViewModel); // TODO: update email after verification // TODO: update username // https://stackoverflow.com/questions/36367140/aspnet-core-generate-and-change-email-address if (agent.Email != agentSetupAccountViewModel.Email) { agent.Email = agentSetupAccountViewModel.Email; agent.UserName = agentSetupAccountViewModel.Email; await _userManager.UpdateAsync(agent); await _signInManager.RefreshSignInAsync(agent); //await _signInManager.SignInAsync(agent, true); //var token = await _userManager.GenerateChangeEmailTokenAsync(agent, agentSetupAccountViewModel.Email); //await _userManager.ChangeEmailAsync(agent, agentSetupAccountViewModel.Email, token); } return RedirectToAction("RequestPayment", "Payment"); } return View(agentSetupAccountViewModel); } // GET: /Agent/Referral/ [Route("/Agent/Referral/")] public IActionResult Referral() { return View(); } // Get: Admin/Agent/List [Route("/Admin/Agent/List")] [Authorize(Roles = nameof(Role.Admin))] public async Task&lt;IActionResult&gt; List(string searchTerm = null, int agentPage = 1) { var pagedResult = await _repository.AgentData .GetAll() .FilterByNameOrEmail(searchTerm) .TakePageAsync(agentPage, EntryAgentsConfig.DefaultPageSize); var result = (await pagedResult.items.ToDetailsViewModel(_userManager)) .ToPagedListViewModel(agentPage, EntryAgentsConfig.DefaultPageSize, pagedResult.count); return View(result); } // Get: /Admin/Agent/Edit/first@gmail.com [Route("/Admin/Agent/Edit/{userName}")] [Authorize(Roles = nameof(Role.Admin))] public async Task&lt;IActionResult&gt; Edit(string userName) { var result = await _agentManager.GetSetupAccountViewModel(userName); return View("SetupAccount", result); } // Post: /Admin/Agent/Edit/first@gmail.com [HttpPost] [Route("/Admin/Agent/Edit/{userName}")] [ValidateAntiForgeryToken] [Authorize(Roles = nameof(Role.Admin))] public async Task&lt;IActionResult&gt; Edit(string userName, AgentSetupAccountViewModel agentSetupAccountViewModel) { IActionResult result = View("SetupAccount", agentSetupAccountViewModel); if (ModelState.IsValid) { await _agentManager.Edit(userName, agentSetupAccountViewModel); result = RedirectToAction("List", "Agent"); } return result; } // GET: /Admin/Agent/Details/first@gmail.com [Route("/Admin/Agent/Details/{username}")] [Authorize(Roles = nameof(Role.Admin))] public async Task&lt;IActionResult&gt; Details(string userName) { var agent = await _repository.AgentData.GetByUserName(userName); agent.Applications = await _repository.ApplicationData.GetAgentApplications(agent).ToListAsync(); var result = await agent.ToDetailsViewModel(_userManager); return View(result); } // GET: /Admin/Agent/ApproveDisapprove/first@gmail.com [Route("/Admin/Agent/ApproveDisapprove/{username}")] [Authorize(Roles = nameof(Role.Admin))] public async Task&lt;IActionResult&gt; ApproveDisapprove(string userName) { var agent = await _repository.AgentData.GetByUserName(userName); var isApproved = await _repository.AgentIdentity.ApproveDisapprove(agent); ViewResult result; if (isApproved) { result = View("ApproveSuccess", agent); } else { result = View("DisapproveSuccess", agent); } return result; } // GET: /Admin/Agent/BlockUnblock/first@gmail.com [Route("/Admin/Agent/BlockUnblock/{username}")] [Authorize(Roles = nameof(Role.Admin))] public async Task&lt;IActionResult&gt; BlockUnblock(string userName) { var agent = await _repository.AgentData.GetByUserName(userName); var isBlocked = await _repository.AgentIdentity.BlockUnblock(agent); ViewResult result; if (isBlocked) { result = View("BlockSuccess", agent); } else { result = View("UnblockSuccess", agent); } return result; } } ``` Guess can't be simpler... each action not more than 10 lines :)
Edited my post for "talking to the db" part :)
Why not have a set of controller services that do all that for you? Then you can test the controllers far easier. IMO controllers should do little to no work at all, makes testing them was easier.
Setup a base class for your tests that has all the mocks you need for that controller, then as you test each method, you can use the base class to create the controller. You also have 7 things injected into that controller...that's not ridiculous, but it is high if this is really supposed to be a simple controller, so maybe you should look at refactoring. Like maybe your controller doesn't need environment, and instead you inject that into services that do need it. Or instead of using both services and repositories in the controller, you only use services.
As you can see, all the work is done in that code in the services that got injected in the controller... still those services are doing the business logic, so they should be tested with the controller.
Do you have any example, cuz all the work is already done inside the services as you see in the controller snippet up there.
OK lemme add an example to clarify what I'm talking about: Yesterday, I added a referral system in which an agent who register will via another agent will get normal 60% from the comission. And the referral agent will get extra 10% from each student that the referred agent will add. Real life example: Agent James told Sam to take a url and register. Sam became an agent and added 3 students. Sam will get 60% of the commission. James will get extra 10% per student. To test this, I need to test Account, Agent and Student controllers in a way that uses the services all together in the testing process. Right?
Added an example down there :)
Added an example down there :)
I also think a layer of service classes that are injected into the controllers is the way to go here. Controllers have services injected into them. Services have repositories injected into them. This allows controller methods/endpoints to be very lightweight, essentially performing any required auth checks, passing query params or data to the service layer, and responding accordingly with data returned by the service layer. At my place of work, we actually don't unit test the controller methods. We integration test the endpoints with a few known success/failure cases. If there is complex logic it is hidden away in the service layer and we then easily unit test that logic in isolation. This is the approach we generally take when we test our projects and it works very well for us!
Mock the services. Test that the controller CALLS the service with the correct parameter(s). Separately test the services, which is a lot simpler in some ways because you don't have to worry about HttpContext or anything like that, you just pass in whatever data you want there (the service probably takes a dependency on a repository, possibly multiple repositories, but when possible I try to avoid that).
Nice way... guess I will add common behavior integration or automation (selenium) tests that covers my controllers. How do you write your integrated tests? What tools? (if you don't mind)
Injecting too many do'ers into your controller. Listen to the pain, it tells you something about the sickness. 
While I'd normally extract some of that code out into a service layer I don't see any particular problem with that code. It should be pretty easy to unit test. To answer your question, yes we 100% unit test controllers, services and factories. Usually we will skip over the data layer.
What's the problem of using DI as much as possible? Isn't it better than a bloated controller that does everything?
So I see a couple of tests here: 1a. When posting to "REGISTER AGENT", the controller calls the Agent Service to create the new agent, passing in whatever data was entered in the form (either a single viewmodel or individual fields). 1b. When registering an agent, the controller calls the account service to create the new account. (Don't forget to test the negative case here; like if the account creation fails, don't call the Agent Service, for example) 2. When posting to "REGISTER STUDENT" the controller calls the Student Service with the student data and the agent (and either the agent object has the reference it needs to figure out who gets the bonus 10%, or if this can change, then it's an extra parameter to the service call). As I mentioned above, you would also test the services to see that they're using the correct business rules.
Yeah it is a lot simpler, but it doesn't go with the real life example... cuz if I test adding the records in my services, it means I don't trust the EF-Core ! And actually, most of the services are depending on: * Repository. * View model. That is got from the get action into the post action. * HTTPContext. If user is an Admin, he can carry on with different behavior and so on. 
So just checking only that the agent service is being called in the right params. Then testing the agent service in those params. Right? Guess I understood testing controllers in a wrong way which test the controller itself for doing all what it is expected :)
Not for unit tests. For integration tests maybe
&gt;What's the problem of using DI as much as possible? Using DI as much as possible is not a goal worth pursuing. &gt;Isn't it better than a bloated controller that does everything? Didn't say you should do everything in the controller. Said you've got some domain modelling and architectural issues. You have a repository, two services, and three managers needed to process the needs of one controller for working with Agents. That's 3 different levels of abstraction injected into your controller. Among those you have an `IAgentManager`, `UserManager&lt;Agent&gt;` and `SignInManager&lt;Agent&gt;`, which all seem to manage some aspect of handling `Agent` in the system, and that's on top of the `IEntryAgentRepository`, which seems to also work with `Agent` and several other aggregates directly with the database bypassing all of your other abstractions. Just from the look of this controller, your architecture has an enterprise grade middle management issue that you've already found it necessary to short cut with an injected repository. Prime candidate for a refactor. 
Thanks for the feedback, and yes, I have problems in my architecture. Would encapsulation be a solution? Wouldn't that break the very first principle of the SOLID... SIP (Single Responsibility Principle).
Yeah, we used to test our services only by calling our controllers and that's really just a pain, because if the controller is doing something to the data it can be hard to get all the possible scenarios you need for your services. Like a common scenario is something where a controller is doing a form validation and not calling the service if the form is invalid... well, that's fine, but what if down the road somebody calls the service in a different scenario and hasn't made those checks, it's important to know how the service is going to behave. Testing each layer separately really gives you very good control of the tests, and at the end of the day is really what you want anyway... your controller's job isn't to care about business rules, it doesn't need to know what goes on in a given service, other than at a basic level your service might return a success / failure code and possibly some data which the controller will then return to the view.
No, controllers are ABSOLUTELY NOT the core of your application in an MVC application. By definition, MVC is the UI layer of your application, and should serve only one purpose: providing an interface to your application code by translating between the internal application state and the UI state. Be careful about buying into code metrics like code coverage. They tend to become exercises in chasing a number with little benefit. Interface layers (UI layers, DB layers, external API call layers, etc.) are NOT unit test worthy. By definition, these are dependent on external dependencies (A web server context, a DB, another API, etc.) and if you just mock those, you are just going to be testing very straightforward glue code that has very low utility to test. If you find yourself needing to test those layers because you've shoved a bunch of NON-glue code into them, this is why people are telling you to extract that out into a service layer. What they are really trying to get at is a concept like Onion architecture, where you have your application in a bunch of class libraries that your interface layers like DB layers, MVC, etc. translate back and forth to. The basic code smell for this layer is "depends on as few things as possible", i.e., just core framework stuff and a handful of libraries. If this layer depends on things from an interface library (i.e., System.Web, etc.) then you have leaked your interface concerns down into your app, and you shouldn't do that. This makes the actual application code MUCH easier to test, and leaves the integration / interface layers as thin as possible with very little utility to test as they are JUST glue code translating between some external contract and your internal application state. It can still make sense to not be this complicated with a project and have it all shoved into the controller method (with very few or no testing actually), but for ANYTHING non-trivial this falls down quickly.
Hey I have a pragmatic approach: I don't mock, except for external data source services not under my control. In my test, I host a kestrel server as I do on the app, and send request to it. If there is too many dependencies (like DB) I wrap everything in a docker-compose that I require people to run during dev and testing. My tests are then fast to write, with more coverage per LoC in tests. Tests are normally fast, if they are not, make sure to have a CI which run tests at each commit, so in dev you run only the tests important to you, the CI will run the rest. 
You can do whatever you want with the separate application.
So, throttle isn't the only thing you're looking at. Because if you throttle, you impact users. And now suddenly you have to change the way your UI works so the user isn't just sitting at a spinning icon forever until he times out. You'll want to insert work to be done into some sort of queuing infrastructure. And then work them one at a time. But you'll need to track state of outstanding work, and think about what you want the user experience to be. Should he now have a UI that shows him his outstanding tasks, and their estimated time to completion? What?
So basically, since controllers are part of the UI layer, it's better to integrate test it, right?
I would then argue - what is the controller doing? A controller pretty perfectly follows the service design pattern. curious as to what you think? 
Liked the Mechanize.NET tool, thanks! Can you please point to some tutorials which show your approach?
Sure! We use NUnit as our test runner and framework, but you can use any testing framework. We use [TestHost](https://www.nuget.org/packages/Microsoft.AspNetCore.TestHost) to spin up an instance of the server to make requests against. We have a separate IntegrationTesting environment set up that can be pointed to testing databases without affecting dev databases. Once the server is spun up, you can create a client that points to the TestHost using [this guide](https://docs.microsoft.com/en-us/aspnet/core/testing/integration-testing). Those are the basics. We also use two other libraries to clean up/speed up the tests a bit. [Respawn](https://github.com/jbogard/Respawn) allows us to clear out data before each test. [Refit](https://github.com/paulcbetts/refit) lets us generate a strongly typed client from the HttpClient generated by the TestHost.CreateClient method. You don't technically need either of these, but over time you may find them useful!
In apps I build, controllers contain essentially zero business logic and they are only potentially responsible for assembling HTTP responses. I typically don't make unit tests for them.
Yes. Interface / integration layers (DB layer repositories, external API call classes, etc.), if kept very thin and just handling contract translation and cross cutting concerns (auth mostly) or context situations (i.e., HttpContext, expectations of running in a web framework like System.Web, etc.) make those layers kind of pointless to unit test. You end up testing GLUE code, or reverifying that the framework does what it claims it does. That's THEIR unit test's job, not yours. In theory this is where mocks come in which helps with the external parts... but the TDD movement keeps fooling people into thinking this deserves a test that you then have to keep up: public Thing Get(ExternalThing thing) { return new Thing { Id = thing.Id }; } I'd argue THOSE unit tests are pointless. They become definitional, where your unit test code ends up being almost identical to the method code, so you've just written the same code twice, and breaking the test doesn't really TELL you anything useful, just that you didn't copy your change over. A practice should have utility here, and that no longer passes my smell test for usefulness. So my argument is, if you keep these layers as thin as possible, the tests around them end up being more effort than they are worth, and better covered by normal integration / QA smoke testing. But then, I'd say that my feelings on unit testing are that most people attempt to cover WAY too much. If you break things down fairly well architecturally, an awful lot becomes glue code translation layers. I only really test legitimate business logic stuff, not glue code.
One rule I remember being taught in computer science at school 30 years ago was the importance of good English.
This will be my way to go too... thumbs up!
Its a typo.. Tools ☹️
If you have a service that another services relies on is one use case. It's kind of hard to do that with controllers acting as your services. 
just my 2 cents, If it works - it looks a little smoother (IMO) foreach (DictionaryEntry entry in rr) { Write(comma); WriteLine(entry.Key); comma = ","; } // -&gt; Write(string.Join($",{Environment.NewLine}", rr.Select(x=&gt;x.Key)));
Tools* and not Tools... Wish i could edit..
service layer/pattern is for exposing functionality to an external consumer, isnt it? a service calling a service - does that fit? genuinely curious by the way, Ive done it both ways and am always trying to circle in on the best arch
Honestly, I don't bother testing controllers at all beyond some superficial manual testing. They are so thin that any mistakes are going to show up as compiler errors.
Forget about SOLID; it's just a bunch of bullshit. The Single Responsibility Principle in particular is a major anti-pattern. Consider `System.Boolean`. If we applied SRP to it then you would need: * A class to hold true/false * A class for parsing strings into Booleans * A class for formatting Booleans as strings * A class for handling comparisons and equality * A class for handling conversions to and from numeric types If you actually followed SRP you would end up one method per class or pretty close to it. And that's just ridiculous. You might as well abandon OOP at that point and just use simple structs and static functions. *** Instead of SRP what you really want is "High Cohesion". Which is to say that things which are closed related to the data are in the class and the things that are loosely or unrelated to the data are not in the class. High cohesion is found at the balance point near the middle, halfway between the "god object" anti-pattern and the "SRP" anti-pattern. 
I'll have services calling other services. I really just do what makes sense for my use case. I've tried with some projects to avoid injecting services in my services, but I always end up with some duplicate code.
It seems the question you are really asking is "how do I unit test this" which I generally use as a metric to determine that my class is too complicated. Yes this does mean you have interfaces for everything everywhere, but the result is that your code is really isolated in what it does. I would recommend using a base controller that exposes an IFooUser that you can then pass in to a set of controller services. You can then test this base controller, and mock all invocations of your services. Do you really care what the user is? You only care at the point where you need, for example, the username. If you give me some time (it's new years and I'm half cut) I can send you an example of how you could layer this to be more testable? Or some kind soul in the meantime could :) Happy new year!
Tools* and not Rools... Wish i could edit..
I would just do this: &lt;Button x:Name="Button1" Click="btn_SelectButtonClick" Visibility="Collapsed" BorderBrush="Transparent"&gt; &lt;Button.Content&gt; &lt;Image Source="{Binding ButtonImageSource}"/&gt; &lt;/Button.Content&gt; &lt;/Button&gt; Then set the image source to whatever in the code-behind or ViewModel as a public property with the name `ButtonImageSource`.
I think we need a little clarification on exactly what you are trying to do. Firstly, you should not be setting properties through the style unless you want to override the entire default style. Instead you should set the &lt;Button.Template&gt; directly. However, in this case, you should not be messing with the template at all. It looks like you are just trying to set the content of a button to an image (or maybe image with text). You can do this very simply: &lt;Button&gt; &lt;Button.Resources&gt; &lt;BitmapImage x:Key="ButtonImage" UriSource="[path to image]" /&gt; &lt;/Button.Resources&gt; &lt;Image Source="{StaticResource ButtonImage}" /&gt; &lt;/Button&gt; Note that the BitmapImage in Button.Resources can be placed in any resource dictionary. Usually this is a global resource dictionary. Also note that the Image object inside of the button default it to the Button.Content dependency property. If you want to have text and an image, you could do something like this: &lt;Button&gt; &lt;Button.Resources&gt; &lt;BitmapImage x:Key="ButtonImage" UriSource="[path to image]" /&gt; &lt;/Button.Resources&gt; &lt;StackPanel Orientation="Horizontal"&gt; &lt;Image Source="{StaticResource ButtonImage}" /&gt; &lt;TextBlock Text="Some Text" /&gt; &lt;/StackPanel&gt; &lt;/Button&gt;
I will appreciate that... Happy New Year!
Jsreport works quite well https://jsreport.net/learn/dotnet
Yeah... Not sure what language version is allowed in T4. The half dozen of them I've used are all C#2 in solutions that build in VS2008.
I try to push that duplicated code into base classes, but it doesn't always make sense.
Thanks wasabiiii - this is the crux of it. Currently, they just crash and that's not a good experience. In most cases, there will be no queue, so now I need to figure out what the best experience for a user would be during the surges. The saving grace is wait time should be low, even at peak, but 30 seconds, without a good experience, can be brutal. That's why I was thinking of a virtual waiting room, but trying to track the position in queue was tough for me to figure out if we went this route, versus just saying statuses. Will have to tinker a bit with this.
Looks great! Planning a project for a non-profit and this looks like a great fit. Just a quick note: link to Elastic Eail should be elasticemail.com, you have elasticmail.com.
Happy to hear you liked it. Just fixed the Elastic Email URL.
Typical: https://github.com/btcpayserver/btcpayserver/blob/master/BTCPayServer.Tests/UnitTest1.cs Here I use: * Self hosted server via my ServerTester class. * A client to my API (user.Bitpay) * Sometimes I resolve a controller through the service provider of my server and manually call methods on it. (Plan to just use Mechanize.NET in te future instead of that) This basically test my API, the .NET client of my API, and some UI controllers. (Soon my razor views with Mechanize) ServerTester is not overly complicated. I don't remember if I did in this project, but if you need to mock, just register the mocks when building the web host , and use TryAddSingleton instead of AddSingleton in your startup.cs for service you want to be mockable. 
I don't unit test controllers at all. As others have said, I unit test the services they use. Integration tests cover the controllers. Really, a controller is just there to define endpoints and direct requests to use the services you've written.
This looks great! May end up using this for an upcoming project.
Looks great, but what is the point of splitting Commands and Queries into 2 different nuget packages? 
This is inspired by MediatR. :) Aside from the enabling developers define handlers way of interfaces like other MediatR/other CQRS frameworks, I have added support for attribute registration. What it does is you can mark methods with [CommandHandler], [QueryHandler], [EventHandler] attributes, then the library will take care of routing to the handlers. This could help developers organize code in a way can reduce class count in a project. :) I am not aware it there are other libraries that already supports this.
Thanks! :) From my understanding of the CQRS pattern, commands and queries should be split. So that is what I did. :) Also, by separating them into two, developers will not need to package any command handling components to their Query modules, or any query handling components to their Domain/Command modules. This allows us to reduce dll size by around 30kb, though I am not sure if saving 30kb of disk space will have a big impact to a project. What do you think? :)
Oh, and I also added abstractions for hosted handlers. Let me know what you think about them. :) https://github.com/jeyjeyemem/Xer.Cqrs/tree/master/Src/Xer.Cqrs.CommandStack/Hosted https://github.com/jeyjeyemem/Xer.Cqrs/tree/master/Src/Xer.Cqrs.EventStack/Hosted
How about Brighter?
&gt; Do you have a demo server anywhere? I have setup one now, and you can find it here ... https://home.gaiasoul.com To understand what you're allowed to do there though (which obviously is significantly restricted, compared to what you could do if you downloaded it yourself), you can see this video - https://www.youtube.com/watch?v=k73k8awOBPQ Have a nice one :) &gt; It kind of sounds like a security nightmare. If you can somehow hack my server, I would appreciate it in fact, as long as you tell me how you did it. So please, feel free to go for it. Here is the documentation for the authorisation/authentication logic of Phosphorus Five, if you'd like to study it, to have a go at it. * https://github.com/polterguy/phosphorusfive/tree/master/plugins/extras/p5.auth * https://github.com/polterguy/phosphorusfive/tree/master/plugins/extras/p5.io.authorization
I see commands. I see queries. Then I see ... events. Why events? Events are not only built into the framework but the language itself. I probably wouldn't use your events over native events, but everything else looks good. 
Lots has been covered already, but here's my $.02 worth: * You already know you have too many interfaces being passed in to the constructor. This is a code smell (meaning the code is hard to test, harder to maintain and work out what is going on). * _authorizationService is never used - quick easy fix. * You appear to be doing at least 2 things in this controller. How can I easily tell? Your route prefixes. Everything /Agent/* should be in one, everything /Admin/Agent/* should be in another controller. This would halve the number of services etc needed for each controller I suspect and make the code much easier to maintain. 
So true. 
For the interfaces, I will find out how to make a service which deals with those various aspects of the agent... though it would break the "separation of concerns" rule cuz: * one of the services is dealing with finding out which role the agent has and behave accordingly (agentStudentService). * agentManager service is Creating and Updating the agent object based on the ViewModel. * SignInManager and UserMananger are there cuz I just use one method out of each, and it doesn't seem rational to wrap up those methods in one of the other services. For the admin part, I have other controllers which touch other objects like StudentController or PaymentRequestController and each one of them has an admin part... I already have an AdminController which has a dashboard and lots of actions! Actually, I took those actions from the AdminController into each controller that has its concerns.
Would be interesting to hear from JetBrains about why this isn’t enabled by default. Like he said, there must be some potential downsides or the setting wouldn’t exist.
Yeah, it was only a suggestion. Personally I'd have a AgentAdminController (and StudentAdminController etc).
This is pretty slick. Good job! &gt; Please have in mind, the "server" is actually just an old discarded Windows laptop, which I have converted into a "server", which runs out of my living room, on a 5MB internet connection Have you looked into hosting this as an Azure App Service? They have a free tier. From my understanding they also have built-in support for MySQL running along side the app. It may have all the bits to run your app, though I admittedly I did not look at all of the requirements. 
It’s free forever, though there are limitations : https://medium.com/@zaab_it/azure-app-service-plan-tiers-f07d5e22297a , though for low volume websites it’s not really a big deal in my experience . Scaling up to a good plan is very fast .... so you can bounce from a free plan to paid plan if you are want the benefits afforded by those plans (more CPU, more ram, an SLA, etc ). Give it a shot , it might work well for you .
&gt; Restricting access to the database, based on the user who connects to the web service, provides an additional layer of protection against bugs and exploits, such as SQL injection. Using accounts corresponding to each user means that you have to grant users the same SQL modification permissions *outside* the application as they have inside it. They then have to be responsible for keeping tables in a consistent and valid state, versus letting the application handle it. That's a recipe for invalid data. You're also having to cache their username and password in memory on the web server, which is not a good idea (or in the cookie, which is even worse). Using individual SQL users also means that you will have thousands or millions of SQL accounts to manage, instead of one. Good luck ever moving that database to a new server.
Yes you're right, 202 is fine, however the application will need to handle instances where the action requested couldn't be processed. An example would be if you have a list of items, and you click the "delete" button, which sends a DELETE request to the API for processing - this would return a 202 and at that point you could remove the item from the list, or mark it as deleted somehow. However, if the DELETE actually fails after being processed (for example the user wasn't authorized to delete the item), you need a way to let the user know that this item didn't actually get deleted successfully.
https://www.amazon.com/gp/aw/d/0996128107/ref=mp_s_a_1_1?ie=UTF8&amp;qid=1514861977&amp;sr=8-1&amp;pi=AC_SX236_SY340_FMwebp_QL65&amp;keywords=effective+engineer Check that one out my dude. It's great.
The problem isn't `ConfigureAwait`, it's `Result()` and `Wait()`. [Stephen Cleary discusses this in a much more approachable (and knowledgeable) way than I ever could.](https://blog.stephencleary.com/2012/07/dont-block-on-async-code.html) You should *rarely* ever need to synchronously wait on an asynchronous task--usually just at the entry point of your application. What `ConfigureAwait(false)` does is prevent the current execution context from being captured and passed to the continuation. If your method knows for certain that neither it nor anything it calls needs that context, go ahead and stick a `ConfigureAwait(false)` there (technically, you only have to do this once per method; everything else after won't have the previous context to capture, but I like to make it explicit anyhow). If your method does need the current context (say that it's writing back to the response stream, which requires the current `HttpContext`), then access to the execution context in ASP.NET is synchronized so that only one task is running at a time on it. That's how you can run into deadlocks if you're synchronously waiting on something that in turn is waiting on something else.
Just a note that asp.net core has no sync context so it's not necessary to call configureawait. It's a good practice in lib code that is distributed, however.
What `ConfigureAwait(false)` does is create an awaiter that does not run the continuation of the awaited task on the previous synchronization context. Queuing the continuation on the synchronization context is much slower than just running it in parallel on *any* available thread, in addition to the potential deadlocks. So, arguably you *always* want the effect of `ConfigureAwait(false)`, *unless* the code after the `await` requires to be run on the previous synchronization context. In ASP.NET, that mostly means access to `HttpContext.Current` and the request's culture, in WinForms/WPF it's required for UI interaction. Of course if you have no synchronization context you also cannot safely mutate shared state without additional synchronization, but you shouldn't be doing that anyway in async code. In general purpose libraries you cannot rely on there even being a synchronization context (ASP.NET Core doesn't have one any more for example), so they should be designed to not require one and thus they might as well take advantage of `ConfigureAwait(false)` even when they are used in an environment that does use one. After the synchronization context is ignored with `ConfigureAwait(false)`, it does not flow to subsequent awaits within the same logical context (so same method or nested method calls). Thus you technically only need to use `ConfigureAwait(false)` on the first `await` in an entry point. There is a small caveat to this, as `ConfigureAwait(false)` does not trigger if the awaited task is already completed. So if you have the logical equivalent of `await Task.FromResult("foo").ConfigureAwait(false)`, the synchronization context will still flow to the rest of the code.
Correctly Building Asynchronous Libraries in .NET: https://www.infoq.com/articles/Async-API-Design/
I've read his articles several times, and I think I follow most of his recommendations, but it's a cargo cult thing for me: I do it, but I don't REALLY understand why. I think my biggest issue is I don't really understand what a "context" is here. I sort of get it from an ASP.NET request standpoint, as the continuation needs to run in the "context" of the original request (but not necessarily on the original thread somehow?)... so it seems like a top level controller method, etc. should NEVER use ConfigureAwait(false) because it needs to continue to the same request context. I SORT of get why the deadlock happens... seems that a given "context" can only handle one continuation at a time (but it's NOT a thread..?) and it is already blocking waiting for the first call to complete, so any subsequent chained awaits can never complete on the same context. And thus for any library where you DON'T control the caller (i.e., a NuGet package, etc.) you should be defensive and use ConfigureAwait(false) everywhere (which I do)... unless you explicitly need to continue on the original context (example of when you'd want to actually do that from a library? Seems you'd NEVER want that?). So when is Result() and Wait() legitimate? What's Task.Run() in comparison and why is it preferable then as a sync wrapper in regards captured contexts? Sorry, know I have a lot of questions here that go beyond my original "what to do in case X" ones, but clearly I don't fully understand what all is wrapped up in a captured context, and I'm sort of shotgunning questions to clear up my understanding.
&gt; In ASP.NET, that mostly means access to HttpContext.Current Note that in ASP.NET Core there isn't a synchronization context to deal with so in theory it doesn't matter if you use `ConfigureAwait(false)`. I would still use it in non-UI classes (e.g. anything not named Controller) as they may be reused in other places where it does matter. 
Great link, thanks (I wonder if I've read this before...). So this validates my practice on LIBRARY code at least of using CnnfigureAwait(false) everywhere... and while it might not STRICTLY be needed, it seems like it would actually have a performance benefit for legitimately doing it everywhere.
That was a good explanation that validates most of my understanding at this point... So two followups: 1. So for MY application code, in an n-teired application, when SHOULD I use ConfigureAwait(false), and when SHOULDN'T I? Specifically, it seems I SHOULDN'T in the controller methods because I WANT access to HttpContext in the continuation... but it seems like maybe I SHOULD at all other layers (I never leak Http related details below the web layer as I see that as an anti-pattern). 2. ASP.NET Core has no contexts... is this a result of OWIN by chance? Moving away from a global "HttpContext" in favor of pipeline?
It actually didn't appear even once in all the lists I checked to make this ultimate list. I wonder why because it seems not bad.
&gt;Question, why await at all if you use ConfigureAwait(false)? &gt; &gt;I mean, if you don't need the context of the calling method, why not just put all the work in the task and call it a day? The awaited task might be a task that will only complete when a new block is found on the Bitcoin blockchain, which only happens once every 10 minutes on average. The caller might start your method on a thread that can be doing other interesting work in the 10ish minutes between blocks. The rest of the work after the await might not care about any particular contextual baggage that the original call might have come in with.
&gt;(technically, you only have to do this once per method; everything else after won't have the previous context to capture, but I like to make it explicit anyhow) If this first `await ConfigureAwait (false)` hits a task that's already completed, then won't subsequent awaits on not-completed tasks without `ConfigureAwait(false)` still capture and restore the sync context?
I use it all of the time; they work very well for "standard" .net web apps (other runtimes like Node are supported too). These days they also have a Linux host but I have not used it yet. If you are making use of something that's part of Windows, like MSMQ, then this will not work for you, since that is not supported; but I don't think this is a problem for most people. They also have a feature called "webjobs," where you can run background processes as standard .net Console apps. So if for example you have a queue, you can write a simple program to pick things up from the queue and process. This is extremely convenient, because without something like this you'd inevitably end up needing to rent a VM or something. Webjobs can be scheduled and/or run continuously, and the runtime takes care of restarting the job on failure. Webjobs are the basis for "Azure Functions" (Microsoft's answer to AWS Lambda), if you've heard of those before. One caveat for the free plan though..... App Services have a feature called 'Always On.' When 'Always On' is not enabled (as is always the case for the free plan)m your app gets unloaded from memory after it has not received any requests. This includes webjobs.... so a scheduled task may not run if your app has been unloaded. My work around for when I work with a free plan is to write a 1 line program that 'pings' my site every few minutes to keep it in memory. 
&gt; I think my biggest issue is I don't really understand what a "context" is here. It really depends on what your framework does on the `SynchronizationContext`. I work with a WPF application, so any handler for an event that's raised by a UI element will start with a `SynchronizationContext` that posts messages to the thread that the handler originally started executing on, which is also the only thread that's allowed to update the state of the UI element (and pretty much always all the other elements it's related to). So for example, suppose that I have a `TextBox`, and I want to listen to `TextChanged` events to provide auto-complete suggestions as the user types, asynchronously. My code might look something like this (a bit of hand-wavy pseudocode): private async void OnTextBox_TextChanged(object sender, TextChangedEventArgs args) { string newText = ((TextBox)sender).Text; CancelPreviousSuggestionRequest(); string[] suggestions = await SuggestionProvider.GetSuggestionsAsync(newText); if (!myOwnRequestWasCanceled) { SuggestionsDropDown.PopulateFrom(suggestions); } } That `SuggestionsDropDown` object can only be updated on its own thread, and the ambient `SynchronizationContext` handles making sure that requests get posted to that thread. If I try to `ConfigureAwait(false)`, then `SuggestionsDropDown` will only be populated if `SuggestionProvider.GetSuggestionsAsync` executes synchronously, and so the method happens to never leave the original thread it started on. &gt; And thus for any library where you DON'T control the caller (i.e., a NuGet package, etc.) you should be defensive and use ConfigureAwait(false) everywhere (which I do)... unless you explicitly need to continue on the original context (example of when you'd want to actually do that from a library? Seems you'd NEVER want that?). In my experience, there's hardly ever such a thing as "NEVER". In my WPF example, this code could be part of a library that automatically hooks up the given `TextBox` with whatever `SuggestionsDropDown` happens to be, and has its own `SuggestionsProvider` to do the fetching. In that case, it's just as incorrect for the library to `ConfigureAwait(false)` as it would be to do so in the application code. &gt; So when is Result() and Wait() legitimate? In general, you should favor `GetAwaiter().GetResult()` over either of these. `Task&lt;T&gt;.Result` and `Task.Wait()` wrap any thrown exceptions in `AggregateExecption`, whereas `GetAwaiter().GetResult()` will rethrow the original exception. But that's a bit of a nitpick. Consider [`System.Web.HttpClient`](https://msdn.microsoft.com/en-us/library/system.net.http.httpclient.aspx). Nearly all methods return `Task`. If you want to take an existing synchronous application and throw in some kind of `System.Web.HttpClient` work in the middle of a synchronous process, where lots of stuff down the call stack depends on waiting for that method to complete synchronously, then you kinda have two choices: redesign all the stuff down the call stack to be async (which can be expensive, and this same decision point can show up in lots of places as you go), or just synchronously wait for the asynchronous method to complete. This ties back to your original question... if you choose to go down the "synchronously wait" route, you should be **really** tactical about where you do it. If you happen to synchronously wait *on the UI thread* (which is most likely where you will need to do this kind of thing) for an asynchronous task with a continuation that also needs to run *on the UI thread*, then you've created a deadlock. That deadlock situation happens *a lot less* when you make sure to tag `ConfigureAwait(false)` on all `await` statements where it is not incorrect to do so. If you're `await`ing some `Task` that came back from a library, then you're really hoping that they followed this guidance, or else you're going to have to hack around it (maybe `await Task.Run(() =&gt; DoTheThingAsync(abc)).ConfigureAwait(false)`) _______ Going back to the very first thing I said, it really depends on what your framework does on the `SynchronizationContext`. I don't know how ASP.NET's usage of `SynchronizationContext` differs from WPF's; namely, I don't know which things need to be posted to the context and which things can be done independently of any context. The specific rules of ASP.NET would drive the answers to your questions.
Commandline applications don't have a synchronization context anyway. Typically, it's used by UI applications (like old ASP.NET or WPF), where --when on the UI thread-- you must keep the rest of method on the same thread after an await.
You may be `await`ing IO work that you don't own. It's a lot simpler to use `await` over `ContinueWith`.
If you're writing ASP.NET Core, you can ignore calling `ConfigureAwait` completely. If you're not, then you need `ConfigureAwait(true)` (the default) in your controllers _only_. Everywhere else you need `ConfigureAwait(false)`. ASP.NET Core losing contexts has nothing to do with OWIN, it was just a decision they made to make everyone's lives simpler. They worked around the requirement in the library, rather than pushing it up to "user land". It wasn't something that could be done in the old ASP.NET because it came out before `async await` was even a thing, and it would have been a huge undertaking. (As is my understanding, someone actually on the team can probably correct me if I'm wrong.)
Re: Result() and Wait() being appropriate - Effectively never. Those will be where your deadlocks are really coming from, not the presence or lack of ConfigureAwait(false). Use await instead of Result() and use When() instead of Wait(). Re: Task.Run() being appropriate - for your use case. Effectively never. That queues a work item to the thread pool and while that may make something compete sooner on your workstation or a lightly-loaded server, it competes with other ASP.net threads for system resources and will reduce net performance.
Really good, thorough but concise answer to a lot of the details about captured contexts here. Seriously, good job. That took real effort. ASP.NET uses the synchronization context in much the same way that WPF does only there's a whole pool of threads that can own their own contexts. You've got a pool of threads waiting for requests to come in from the web server. When the web server gets the request, a scheduler decides which of the thread pool threads will handle that request and sets up the context for that thread from the request. Then it calls a handler method on that thread that eventually calls your controller method. When the original handler returns, the context gets disposed and the thread is returned to the pool to handle another request. It's pretty easy to see why awaiting a `ConfigureAwait(false)` in an ASP.NET controller method could cause you problems. Control will return the caller and the web server will see the request as completed and recycle the thread. There are times when this might actually be what you want to do but it would be poor form to achieve that behavior this way.
So if dot Net core doesn't have a sync context at all - even dot Net core MVC - should we be using configureawait(false) absolutely everywhere?
&gt; propriate - Effectively never. Those will be where your deadlocks are really coming from, not so much the presence or lack of ConfigureAwait(false). Use await instead of Result() and use When() instead of Wait(). Use ConfigureAwait only to signal lack of need for any execution context, and do so from the perspective of maximizing performance and never from a perspective of working around a deadlock (though it may happen to do so.) &gt; &gt; Re: Task.Run() being appropriate - for your use case, effectively never. That queues a work item to the thread pool and while that may make something complete sooner on Yes, as I mentioned in another comment thread, this isn't something I've ever encountered or considered before, but it makes total sense. It just lends more weight to my defensive approach of marking all calls that shouldn't capture the context.
&gt; If you are making use of something that's part of Windows That wouldn't be a problem for me, in fact, I have a feature request over here, which I even haven't been able to implement, due to not having a Windows system to test my code on - https://github.com/polterguy/phosphorusfive/issues I must confess though, that it sounds like I'd have to create lots of code, which would only be applicable for Azure according to your description. Can you run standard Apache on it for instance ...? &gt; My work around for when I work with a free plan is to write a 1 line program that 'pings' my site every few minutes Hehe, creative :)
If there is no synchronization context, `ConfigureAwait` has no effect, it's effectively always `ConfigureAwait(false)`. Whether you still want to add `ConfigureAwait` calls in ASP.NET Core code for clarity is up to you. However, if you're writing library code, you should always use `ConfigureAwait(false)` because you don't know if the environments it will be used in might have one.
First, big thanks for this, great explanation. &gt; redesign all the stuff down the call stack to be async (which can be expensive, and this same decision point can show up in lots of places as you go), or just synchronously wait for the asynchronous method to complete. Yeah, this is the interesting situation I end up hitting a lot. My understanding is due to the overhead of async, it's better to write sync code up until the point where the async "virus" pops up, and then undertake to transform the call stack like this... which can be quite tedious obviously. Now purist comments on that statement aside, AT SOME POINT I usually end up writing SOME sort of sync wrapper for async pieces somewhere in a lot of apps, usually because there's some framework method or library consideration that is sync that I can't change... In that regard ConfigureAwait(false) is then a defensive measure. After all this talk I feel more justified in saying the following: 1. IN GENERAL all library code awaits should have ConfigureAwait(false) on them, except for the exceedingly rare case where said library needs the original context because it deals with HttpContext or the UI thread items, etc. (at least for me its exceedingly rare). Core only comes into consideration on Core only libs, but .NET Standard libs still should follow this. 2. IN GENERAL top level UI / Controller method awaits should NEVER use ConfigureAwait(false). 3. IN GENERAL all other app code awaits should have ConfigureAwait(false) on it, assuming that nothing beyond the UI / web layer makes direct use of the context (HttpContext, etc.), UNLESS in ASP.NET CORE in which case this can be ignored because the issue is solved in the framework at this point there. 4. ConfigureAwait() seems like a poor name for what this is doing...
&gt;Is that true of ALL of core, or just ASP.NET? For instance, what about a command line app? Everything here so far is about WPF / WinForms type UI threads, but does any of this apply to console apps? Console applications don't use one normally (Core or not), but UWP does. Also, 3rd party code can implement their own `SynchronizationContext` and set that. The Avalonia UI framework does this, for example. So you cannot rely on there being no synchronization context when writing Core code.
Awesome thanks for the info
So without saying "never", how do you call async code from a sync location? ;-) I've read a lot of Steven's articles, including this one which is great BTW (https://msdn.microsoft.com/en-us/magazine/mt238404.aspx). I'm willing to accept the answer is just "it depends" and you have to choose one of these strategies. It seems GetAwaiter().GetResult() is just a slightly better Wait() and Resullt() block that still relies on either (a) there being no sync context or (b) the called code using ConfigureAwait(false) correctly. Task.Run seems to decouple the context through a hard-line "go run on a new thread without a context" approach, which works, but obviously doesn't scale well in an ASP.NET world... Though I've seen it quite a bit. I'd assume this is something you plain wouldn't want to do in library code as the caller would have no idea you are allocating your own threads on the calls... ConfigureAwait(false) lets the LIBRARY protect their callers using it in any of these ways (at least from dead locks, and obviously as long as you don't need to continue on the same context which is probably the vast majority of library code), and it seems to offer the EASIEST consuming method for callers needing to wrap over it in a sync way (except exposing two versions of the method, which has other fun pieces, and obviously isn't always possible if your dependencies don't also expose sync methods). But of all the options I've seen... ASSUMING ConfigureAwait(false) and using GetAwaiter().GetResult() (just a cleaner Result() and Wait() really) actually seems to be the "best" in terms of ease of use as the library has stated it's needs correctly and thus it is safe. The issue ends up being BAD libraries that DON'T follow this then, meaning the CALLING code has to break the context inheritance instead somehow, and the "hacks" like Task.Run() come up as ways to force the Task to start somewhere without a context. As that seems an unavoidable necessity that I'll run into AT SOME POINT, what's the BEST ways to do that given that I'm primarily an ASP.NET developer not on Core yet? It's obvious to me that the CALLER (probably) can't do something like ThingAsync().ConfigureAwait(false).GetAwaiter().GetResult(), because that caller needs it to actually continue on the current context when it comes back (I'm assuming here caller is actually ASP.NET controllers, Application_Start handlers, UI thread handlers, etc.). But I'm kind of coming up blank on other options. When you LEGITIMATELY need to break that context from a caller, Task.Run seems like the only option? Is there another way to do it without breaking your OWN continuation? Could you maybe wrap the method in you're own async method to do it like this, and would that be preferable because it avoids the Task.Run()? public SomeResult Get() { return Wrapper().GetAwaiter().GetResult(); } public async Task&lt;SomeResult&gt; Wrapper() { return await RealCall().ConfigureAwait(false); }
Does this mean that Wait(), Result() and GetAwaiter().GetResult() are safe and valid in a console application because even if a library failed to use ConfigureAwait(false), there's no deadlock possibility?
&gt; always wondered what features people use that it doesn't offer. Mostly lots of optional operation parameters, stashing, configuration params, and recursive flags for working with super-repos/submodules and such. The core functionality is definitely covered by the GUI. &gt; But I'm not going to use another tool if I don't have to just for the sake of being a 'power user' This is an intelligent position to hold. These are just tools, a means to an end.
Where do I start with this? &gt; In that case, it's just as incorrect for the library to _ConfigureAwait(false)_ as it would be to do so in the application code. That depends entirely on what that library code is doing in the exact method where they call ConfigureAwait and even then only until the next await in that method or the method returns (its Task.) &gt; In general, you should favor _GetAwaiter().GetResult()_ over either of these. _Task&amp;lt;T&amp;gt;.Result_ and _Task.Wait() GetResult is not going to make things any better for code in trouble than Result or Wait. &gt; If you're _await_ing some _Task_ that came back from a library, then you're really hoping that they followed this guidance, or else you're going to have to hack around it (maybe _await Task.Run(() =&amp;gt; DoTheThingAsync(abc)).ConfigureAwait(false)_) God, no, please don't. Okay, question time to see what you understand: what code is affected by the ConfigureAwait you threw into the end of that Task.Run() call that I quoted just now?
&gt; how do you call async code from a sync location? ;-) The short answer is that you don’t.
Sounds good.
&gt;Is there another way to do it without breaking your OWN continuation? If you really have no other choice, I suppose you could use a helper method like this: public void WaitWithoutSyncContext(Func&lt;Task&gt; asyncMethod) { var prevContext = SynchronizationContext.Current; bool isDefaultCtx = prevContext == null || prevContext.GetType() == typeof(SynchronizationContext); // default impl just queues on thread pool anyway if (!isDefaultCtx) SynchronizationContext.SetSynchronizationContext(null); try { asyncMethod().GetAwaiter().GetResult(); } finally { if (!isDefaultCtx) SynchronizationContext.SetSynchronizationContext(prevContext); } }
Ok, can you think of a reason why this would be TERRIBLE functionality for a browser to allow? 
Ah, so even if ASP.NET Core doesn't NORMALLY have a sync context, it may behoove us to continue following the same rules because of this possibility?
This is why I was wondering if the context's elimination in Core was related to OWIN being the whole story in Core rather than it being an addition on TOP of the original pipeline of traditional ASP.NET. I.e., that you had to capture before because HttpContext was this global blob that needed captured, but in an OWIN situation that context is PASSED instead. It's an implementation detail I maybe don't need to be completely clear on, but I'm curious.
So for a completely contrived example that we hopefully would both be familiar with, lets take HttpClient, which only exposes async versions of its calls, and lets for the sake of argument say it's the only library that does what it does and you've been tasked to use it in an ASP.NET application where taking the time to rewrite the whole thing to be async top to bottom just isn't an option right now. This seems like the simplest case where I'd want to wrap the async calls in a sync wrapper. Edit: Other alternatives I can think of off the top of my head are class constructors, inside locks, and inside framework defined sync methods like Application_Start, or MVC action filters, etc., but this example seems to serve as the most basic generic use case I see.
The OWIN thing was just a guess on HOW they achieved the elimination, not a CAUSE of the elimination. My thought around that being HttpContext and such are static globals in the old framework, but that context is passed down the OWIN pipeline in Core, thus eliminating the need to "capture" it, since it's already captured as part of the current middleware call stack. This is more a curiosity thing to see if I understand what the "context" is here...
No problem, I'm learning here, I'll be flexible :-) So are you saying it's not possible to call an async method from sync code? Because *that* seems like its generalizing *too* much, as it seems there ARE ways to do it, they just all have caveats on when they are ok and when they aren't... Or that there is no one-stop answer to how to do it because implementations below the hood of the dependency MIGHT make it impossible (i.e. because it requires continuation on the original context)? Or that my wrapper example above is like "a thread you completely control" and my thought of breaking the inheritance of the context in that way would work, assuming no such continuation restrictions?
This is interesting, I've seen code like this somewhere before... I'm interested to see other's comments and warnings about it's use :-) Functionally would this be different than my wrapper example? Obviously it's a nicer, I'm just wondering if it's the same in concept so I know I'm not missing some nuance of what I'd reading.
Yea, that's a fair statement.
&gt; So are you saying it's not possible to call an async method from sync code? No, I'm saying that if you want any chance at reasoning about and relying on behaviour of the system then you need to control the thread of execution in which you perform blocking calls like Result and Wait. The problem is that most people call Result and Wait in all sorts of situations where they don't control the thread of execution and it gets them in trouble far too often. You're in ASP.net, as I recall, and therefore you do not control the thread of execution and should leave those calls to those who do.
This is the sort of stuff I use queues for. I'd usually have some sort of permanent data storage where I store the jobs, for instance, if you're doing SQL, a Job table. If you're using some NoSQL thing, some sort of collection of items that you can queue for the jobs for the user, based on their status. Second, expose a WebAPI capable of initiating a job. This web API would post a new message to a queue of some fashion to initiate the job, and return the status of the job (new). Next, a WebAPI to return the job status for a given Job ID. This WebAPI would simply look up the job information in the data storage. Third, a worker service of some fashion which processes items from this queue and does the work. It updates the data storage with the new job status as it moves through stages. And finally, when it's done, it can mark it finished. At each step, as the worker updates the job status, it also publishes messages back to the client over the same message queuing framework, which indicate that the data for the job has been changed. These messages make their way to the web server, where they are picked up by a SignalR Hub of some fashion, and the client is notified of a change. The client can then refresh the job from the GET WebAPI, updating the UI. Or the SignalR message can contain the new status. That's what's usually involved.
[removed]
Are interfaces *really* duck typing? I've we taken them as a *contract* the I give you what you request, and you'll give me backs result. Later down the road i don't care who's doing the calculation, I just care that I get my output. Languages like golang follow more what is duck typing IMO [Protocols and interfaces can provide some of the benefits of duck typing, but duck typing is distinct in that no explicit interface is defined](https://en.m.wikipedia.org/wiki/Duck_typing)
Non-Mobile link: https://en.wikipedia.org/wiki/Duck_typing *** ^HelperBot ^v1.1 ^/r/HelperBot_ ^I ^am ^a ^bot. ^Please ^message ^/u/swim1929 ^with ^any ^feedback ^and/or ^hate. ^Counter: ^134120
**Duck typing** In computer programming, duck typing is an application of the duck test in type safety. It requires that type checking be deferred to runtime, and is implemented by means of dynamic typing or reflection. Duck typing is concerned with establishing the suitability of an object for some purpose, using the principle, "If it walks like a duck and it quacks like a duck, then it must be a duck." With normal typing, suitability is assumed to be determined by an object's type only. In duck typing, an object's suitability is determined by the presence of certain methods and properties (with appropriate meaning), rather than the actual type of the object. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/dotnet/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
I don't think he ever implied that interfaces are the same as duck typing. He says that async/await uses duck typing _instead_ of requiring explicit interfaces.
this is sort of a gray area. as you suggest, it is not an explicit interface, but it is still an interface nonetheless, and enforced by the compiler. The wiki text bot above says of duck typing: &gt; It requires that type checking be deferred to runtime, and is implemented by means of dynamic typing or reflection. The type checking for async/await is enforced at compile time.
Fair point about compiler vs runtime checking. I would argue it is conceptually still "duck typing" but enforced by the compiler, so I think the wiki definition is too strict. 
[removed]
We ended up just doing a shared folder on the file server. Visual studio can still see it as a full nuget feed, and you can use AD to control permissions on the folder if needed. Now that I think of it, you could actually host the feed as a folder in source control, though that would add an update step.
Thanks... I will finish my exam tomorrow and look at the example... Thanks again!1
Docker run on root privilege- enough to choose vm over containers
Perhaps the first solution in this stackoverflow post helps you. https://stackoverflow.com/questions/5750659/print-images-c-net
That scale ok? I always worried once you got past a trivial number of packages and versions that it having to open each package up and reading the metadata would just grind to a halt.
Use the [built-in Identity framework](https://www.asp.net/identity). I am going to make this next part bold because its very important. **Do not attempt to rebuild this wheel as you probably will not be able to make it as secure as the prepackaged stuff provided for you already.** 
I work in a small company with three developers, so I don't have that data yet. We have a few packages with a few versions each, but they don't update often.
Can somebody explain why there need to be two containers? Wouldn't it be possible to live without the build container?
It's pretty trivial to setup a nuget server I would imagine once you hit scaling problems you just go ahead and set one up.
Don't have a list, but those I know of off the top of my head are: - `await X` - foreach - LINQ (e.g `where x &gt; 5` gets translated to `.Where(x =&gt; x &gt; 5)`) - Collection initializers (i.e. `new FakeCollection&lt;int&gt;() { 1 }` gets translated to `var collection = new ...; collection.Add(1);` - Deconstruct - A new one made for tuples. A `Deconstruct` method with all out parameters allows deconstructing an instance just like tuple (i.e. `var (key, value) = new KeyValuePair&lt;TKey, TValue&gt;(...);`). Most of these can also be extension methods.
I would say that in this case the compile time vs runtime refers to the compiler itself. An `Interface` is something the compiler is aware of in its compile time, that's why the new types need to be distributed in order for the compiler feature to work. For example C# tuples need the various `ValueTuple` types which were distributed as a package until .NET Framework 4.7. Duck typing is where the checking is deferred to runtime of the compiler itself, which is the actual application's compile time. And it's safe because the compiler doesn't crash if there's a type mismatch, it just doesn't produce the intended code.
I’m not familiar with docker, but I think there is only one container being used. The docker file resembles a makefile to me. I think the ‘build’ entity is the dotnet core app, and the ‘final’ entity is the actual container, which references ‘build’. Again, hope I’m not wrong because this is just how I understood it. Don’t actually know... lol.
The build container is only used to build, it is thrown away afterward.
It definitely is, though the implementation needs to make sense. You even have such an example in the .NET Framework, `Task.Yield`. It returns `YieldAwaitable` that has a `GetAwaiter` method that returns `YieldAwaiter`. It does sit on the `Task` class as a static method but it doesn't actually have anything to do with tasks.
I complain about this fucking retarded style of "article" that gets posted here ALL the time but nothing changes, I don't think the mods are even alive.
I find it unsettling that these `in` parameters trigger a **silent** copy a reference type if you call a mutating method within it, and it feels flawed. 
&gt;Containers are way more smaller. :P
thanks!
thank you!
Build environment isolation, to avoid "it builds on my machine" problems.
Ok. That makes sense.
Don't, you salt them, then hash them. Then you use the same salt when users gives you their passwords, and salt them, and hash them, for comparing against the stored hashed value ... I do that here, if you'd like to have a look at some actual code that does just that - https://github.com/polterguy/phosphorusfive/blob/master/plugins/extras/p5.auth/helpers/AuthenticationHelper.cs#L68 The "salt" is simply some randomly generated piece of string. I ask my users to provide one during installation ...
A path of the list of "duck-typed" methods can be found in Roslyn compiler sources: [WellKnownMember.cs](https://github.com/dotnet/roslyn/blob/master/src/Compilers/Core/Portable/WellKnownMember.cs) By implementing these methods, you can change behaviour of C# language constructions such as: * lock-statement - by implement own Monitor.Enter/Exit methods * new() constraint - Activator.CreateInstance ([good article about it on MSDN](https://blogs.msdn.microsoft.com/seteplia/2017/02/01/dissecting-the-new-constraint-in-c-a-perfect-example-of-a-leaky-abstraction/) * async/await - AsyncTaskMethodBuilder/AsyncVoidMethodBuilder classes * value tuples - you can define your own System.ValueTuple&lt;,,&gt; structure, e.g. w/o referencing nuget package on pre-4.7.1 frameworks. * maybe other, just try
It happens to me too, but not only cshtml, with all kind of files. From time to time I have to close and reopen vs for Mac... That’s why if possible I tend to use VS Code.
Nice write up. Found an error though. Last sentence of the section "ASP.NET Core Metapackage – Microsoft.AspNetCore.All" is duplicated.
&gt; what should you do if you're working on an existing application which is sync all the way down, and you want to slowly move to taking advantage of async? Fix it one call stack at a time. If you are able to make breaking changes, start internally on stuff that does actual IO work: * db calls (only postgres and mssql actually support async when last I checked a year ago) * network requests (`HttpClient` and cousins) * filesystem requests * invoking a secondary `Process` 1. get 0 errors, 0 warnings in your build 2. find 1 method with one of the above IO things 3. Duplicate that method and do that thing async 4. mark the original method obsolete 5. bubble the obsolete out of the codebase (it makes a warning where you use it)
 That would not work from a web page.
fixed, thanks!
You should never have to unit test your controller (unless you have business logic in your controller, which if you do... stop doing that). Think of a controller like an air traffic controller, it just tells you what you need to to go do. So instead, you unit test the classes the controller might be using. So in this case you would be unit testing your repository methods for Get, Add, etc. With all of that said... yes, unit testing is going to seem repetitive and tedious, and the ones you have created seem correct in what they are doing.
I'm sorry but if you are asking candidates questions core *metapackages* you should re-evaluate your interview process. You're literally not testing their skills in any way, while also picking a very niche subject that I doubt anyone new to .NET Core will have even seen. Also, why is the question "Talk about DI in ASP.NET" rather than just "Talk about DI"? Just because there now happens to be a container now doesn't somehow mean DI only applies to ASP.NET. I honestly think I'd be tempted to leave the interview early if these are the sorts of "questions" I'm being asked (I mean, the title says it's interview questions but really it's a series of statements copy pasted from places with the odd question interjecting). It seems to be a total mixture of questions I'd expect anyone with more than a years experience to understand (DI, logging, configuration) and yet at the same time it's trying to project itself as something you'd be asking someone with more experience. 
I find it helpful to use an automocking container. https://code.google.com/archive/p/moq-contrib/wikis/Automocking.wiki I haven't built up a whole class around it in my recent usage, just use it directly. I always use MockBehavior.Strict so that it will provide mocks for everything but only allow you to use the ones that you have setup. public class SiteApipBaseTest { protected AutoMockContainer container = new AutoMockContainer(new MockRepository(MockBehavior.Strict)); } //Add things to the container var cc = container.GetMock&lt;ICultureConverter&gt;(); cc.Setup((m) =&gt; m.ConvertFromString(langParam)) .Returns(cultureEnumValue) .Verifiable() ; //Resolve an object from the container instead of using a constructor. var controller = container.Create&lt;ConstructionController&gt;(); 
“Besides being cross-platform Kestrel is really fast and also production ready.” Don’t they (M$) recommend running behind a reverse proxy as it’s not production ready? 
While I like to use Moq, I have started to question whether a unit test should ever test that a specific function call is called from the method. It smells like testing of implementation, rather then focusing the test on behavior. Most systems (should) have an object (or set of arguments) that you pass in, combined with either an object that gets spit back out, or that puts the system into a particular state (object is now stored in database). I don't care how the object gets massaged or persisted into our chosen object store, only that it does.
Is your problem that the tests are too slow? If so, there's not much you can change about these tests, I think. [UI tests are simply the slowest tests you can make. ](https://martinfowler.com/bliki/TestPyramid.html). You could probably test a lot of this functionality with integration or even unit tests. And only run the UI tests when committing. In my experience UI tests also run faster when using a JS framework like Protractor. But that could've just been my limited experience. As a side note, you might also want to look into page objects. They're a nice OOP way of cleaning up UI tests.
If you don't test that the controller calls a service to SaveFoo, then how will you know that it got persisted? If you don't mock the service, now you're in a bad place because you can't say whether the controller test failed because of changes to it or because of changes to the service.
It’s not that they are too slow - I expect them to be slow. But it always had to repeat the login process every time I run a test, which is just really time consuming. It would just make things easier to avoid running through the same arguments to grab the cookie and continue on. I explored it for this use case. We do have unit tests for the processing functions, but we also need to verify it processes as expected in Vue (since it’s an SPA). We had issues trying to get the API data directly from a JS testing framework. This has worked out well (and is more of a natural push for the non-JS devs). Can you tell me more about page objects? I have vaguely heard of them, but I’d love to know more and kind of their use case. Always up for keeping it clean!
It is literally stated to NOT be production-ready and to use a more full-featured reverse-proxy in front of it (nginx, iis, etc)
Ah, didn't realize it was about having to log in every time. If you want your tests to be independent then I think you should keep the login as is. What happens if one of your tests has a side effect that changes a token or some state in the browser. Then your tests are suddenly broken. Though maybe you could save the JWT and injecting it to the browser. Though that feels a bit hacky. Page objects are a way of making a test easier to read and reusing code. Instead of typing .QuerySelector(".input").SetValue(username) you could have a login page object which has a method EnterLogin(username, password). The object then does the selecting and entering on the page. This way, you don't have all those ugly selectors in your test, they become cleaner, easier to add new ones and shorter. A login test would just look like this: ` var loginPage = new LoginPage(browser); ` `loginPage.NavigateTo();` `var resultPage = loginPage.EnterLoginDetails("user", "pass").Submit();` `// Some asserts about the resulting page` You should be able to find plenty of examples on Google about it
Oh nice! This is a really solid example. I was really struggling with the in-depth amount of selectors I needed to move around the page. Thanks for that.. And you’re right. I guess it’s not a bad thing to have to rerun through some of the UI to keep it consistent. Thanks again for the feedback! I really appreciate it!
With version 2.0 Kestrel is production ready, even though it's still recommended to go behind a proxy. https://docs.microsoft.com/en-us/aspnet/core/fundamentals/servers/kestrel?tabs=aspnetcore2x - You can use Kestrel by itself or with a reverse proxy server, such as IIS, Nginx, or Apache. A reverse proxy server receives HTTP requests from the Internet and forwards them to Kestrel after some preliminary handling.
I was always wondering if there is a real performance gap between Ubuntu and Alpine when running .NET apps
I mean, if you want to test how HttpActionResult works and returns what status code it should than be my guest. Even when I do TDD going that far is just a plain waste of time. The only time you should be testing your controllers is with regression tests. It's 100% pointless to do so with unit tests when your unit tests should be where your business logic is and if you have all of that covered, you're good to go.
That's not the punch line I was expecting
I wasn't saying that he's claiming interfaces and the same as duck typing. But I don't see actual duck typing in this. If you notice, the classes that he implements, however are using interfaces. And so im saying that this post is wrong to say that async/await use duck typing. 
I find that test driven development works best when pairing. One developer writes the test, the other makes it pass. When I am doing solo work, I write my tests after (and always for JavaScript tests). That being said, unit tests will be valuable when you start maintaining the site or adding new features, especially if another developer is touching the code. 
Honestly, I think strict TDD of more of a teaching exercise than a development practice. Once you get into the mindset of nit writing untestable code, it is usually more efficient to write the tests after you've discovered the edge cases. 
I was in a similar situation a year and a half ago. (Python and node to c#) First off, Be honest about your skill set and don’t BS around your knowledge gap. Second, are your senior or junior on that project? If you are junior than good! You hopefully have support from above. If senior, than hopefully you have support from below! VS is an amazing IDE. It does literally everything in the MS world. It can publish to azure, it can interact with a DB, literally anything you can want to do. That being said, it is going to take months to get proficient. I didn’t really have a problem learning C#. If you k ow OO and logic, than it should be a breeze. I did/do have a problem with all the different frameworks. Starting from scratch is a pita since there are a number of ways to do something. The VS templates are pretty good starting point. Hopefully you will have an existing codebase that you can learn from. Good luck!
Unit testing controllers is a waste of time because it should all be interactions with other layers. And unfortunately, unit testing interactions is often more harmful than helpful. Tests are practically required to test implementation details, and they’re incredibly fragile for it. You’re better off using integration tests to cover the controllers. 
Thank you very much! About to take off on a flight home so I have to make this short :) I’m coming into a Systems Analyst position, neither junior nor senior, they just told me Systems Analyst lol. Taking off!
The first thing you need to find out is if you're working with ASP.NET MVC or ASP.NET WebForms or both. If you don't know, then probably assume MVC (WebForms is archaic and in many ways generally pretty bad, but if you're doing more legacy than greenfield, you might be doing WebForms.) They are wildly different, so if possible, you want to avoid studying the wrong one. I learned on the fly, and one of the things I did was just to watch several different video beginner courses at 1.5 speed and let the repetition seep in. Once you dig into real code, you'll be surprised by how much you've picked up. Pluralsight is really good for that sort of thing, but I'm sure there are plenty of resources that are just as good. If you're not into paying for things, I would secondarily recommend Microsoft Virtual Academy. I find it to be a little underwhelming and buggy, but it is free. One more thing: If you're planning on installing Visual Studio, plan ahead. You may well be downloading between 10 and 40gb (YMMV a lot depending on components). It's not something you're going to do on battery power, and it's not something you're going to do if you're running out of disk space. You need to allow yourself half a day at least. I hope what I'm saying isn't too obvious and unhelpful, but that's pretty much what I did a year ago, and it worked out well for me. Good luck!
this actually helped me,to know how it works.googled up and read about salt &amp; hash. Thanks
If it is ASP.NET Core, you can have a look at this learning path from Microsoft Virtual Academy: https://mva.microsoft.com/learning-path/aspnet-core-6 Also, [Pluralsight](https://www.pluralsight.com/) offers many courses - though those are not free like the ones from MVA
Your response was super helpful, thank you very much :) I’ll be digging into this as soon as I get home. I currently have no idea exactly what they are using but I’m fairly certain it is ASP.NET MVC. As for visual studio, I believe I have 2015 installed fully (that monster 40GB installed while I slept).
I’ll check those out when I get off my flight, thanks :) 
&gt; ASP.NET MVC That's going to make it much easier. It's basically the same as PHP in design principles. (WebForms would be sheer pain to you, and honestly not worth learning.)
The object being awaited doesn't need to be a Task or implement any interface, it just needs to have a method called GetAwaiter that returns the correct type. That is the "duck typey" aspect. You may argue it's not true duck typing since it is evaluated by the compiler, but it's the same principal.
*Dependency Injection comes as a part of ASP.NET Core Framework and everything is built around it.* That's sentence isn't really correct. You can do dependency injection in c# (any version manually and with other languages) . I think the author is trying to say is that there is a DI container(a simple one) built in so you don't have to necessarily use autofac or other DI frameworks.
Your questions have been answered pretty sufficiently by the other posts here. I'd just add to remember that a large benefit to TDD methodology is to force you to consider the specs and functionality as fully as possible before you even start coding. The tests ensure that the code adheres to the details that were agreed on, now and with future builds. All the details.
NP :) PS, be careful, this is amongst one of the most severe places you can put a bug ... ;)
I am not sure why you are stuck on the notion that only business logic is fit for for tests. Yes testing third party details is probably not right. But on the other hand in the context of a controller there are things that are relevant to verify. That subsequent method calls are provided the correct data, is a prime candidate since it has a high value. The moment we have that automated test of that nature in place we know exactly what is happening in our system. An even better thing to test are the controller's decisions if, for example a repository fails to retrieve data. Without a source of truth how do we knoe? Well if we bang out a test for it, we know. Where it get's murky is where we have interesting things that are implemented by a third party. Like authorization, I only want to verify that I cannot interact with the resource if I am not authenticated. The rest is a third party detail. I am interested in reading some of those tutorials if you can source them. I have a feeling we are not communicating clearly and I might be misunderstanding you. I mean how can anyone credible that advocates TDD give the advice to not write a test first?
I would pay particular attention to C# the language and really take a couple of in-depth courses on PluralSight or the like. If they are using VB.NET, then OK learn that too, but still learn C#. You will want to have that under your belt. Just know that C# OOP is not like PHP OOP. Ever hard of a partial class in PHP? I'm no PHP expert, but I suspect not. Also, you have things like master pages, user conttrols, etc. etc. etc. It's fairly overwhelming to be honest, so focus your learning on one area at a time. Be sure you understand exactly what technologies they're using with C# because "ASP.NET" includes WebForms, MVC, jQuery, and possibly something like Angular JS, or maybe Angular 2+ on the front end. If they are using WebForms, be aware that's an older (almost legacy) technology, and it comes with an ecosystem of 3rd party commercial (not FOSS) packages and you may need to learn one or more of them for their environment. Services-wise, if they're using .NET for services development, then you may need to deal with SOAP Web Services (actually legacy at this point) include WSE*, WCF (which can publish services via REST or SOAP), Discovery, and ASP.NET MVC pushing JSON using the Newtonsoft library or something similar. They may also, if they're cutting edge, be dealing with .NET Core as well. Be aware that .NET Core development is a distinct set of APIs from traditional .NET Framework. Do not assume that your code will automatically work for both. Finally, in most .NET shops, you'll need to deal with SQL Server or another RDBMS. Hopefully you have some analogous experience there you can apply, but again, take it seriously. If you do need to pick up SQL Server and haven't dealt with it before, but perhaps have deal with Oracle, mysql/InnoDB, or maybe even postgres, then just know that it is different. Sure SELECT statements are the same, but it's a serious product with a lot of features too. it's not the tinkertoy that many in the FOSS community like to say it is. Oh... almost forgot: Besides SQL Server or another database, you will likely need to learn a .NET ORM as well. This will probably be Entity Framework, but there are others so pinpoint which they're using and put that on your learning plan too. Anyway, I would start with what they will expect you to work with first, get proficient with courses, practice, etc. There is no substitute for spending the extra time to invest in yourself, so unless they offer to train you up on all of this on their time (ha!), be sure to pay your dues there on **your** time so you can get productive ASAP. This isn't a matter of whether or not you think you're a genius. It's all about tenacity and discipline, so strap in. You can do it. Just my $0.02 USD. Good luck!
Well sure they should be mostly interactions with other parts of you application. On the other hand if that does not work then you have a broken system. I am not sure I am following you on the fragile part. What is fragile about validating how a mock is called? I assume you already use something like moq to create mocks? In a well written controller where all dependencies are injected I find integration test to not be that useful. I can mock all depencies and test it that way a lot quicker than setting up integration tests. In a legacy setting I agree with you, it's much easier writing integration tests but OP asks for advice on TDD in a new project.
Don’t bother learning webforms unless you are working with an existing project that uses it. Core is bleeding edge but might not be production ready depending on your use cases. 
you usally can skip unity &amp; c++ If you can also skip xamarin and the mobile emulators it gets down to around 5-10GB
But when it maps to the DTO why is IEnumerable&lt;string&gt; Modes not ["0", "1"] and instead ["dev", "preprod"]?
Firstly you should check what technologies you'll use at work. It's probably ASP.NET MVC 5 or ASP.NET Core, but they're not the same. When you know, I would suggest buying a book to have the list of things you need to learn, read documentation and watch some tutorials (I enjoyed Pluralsight or some Mosh Hamedani videos on YouTube and Udemy, he can explain things in a VERY easy way). Good luck.
That’s encouraging; having taken a glance over the MVC starter project this morning I came to the same conclusion re: the similarity to what I was working on in PHP (Laravel)
Yeah I was a user of the old Xamarin Studio, I am one of the few that were running ASP.NET on Linux pre .NET Core (detailed on my blog [here](https://coderscoffeehouse.com/tech/2016/01/19/aspnet-linux-setup.html)). Don't worry about shameless plugs - I do them all the time. I had actually already seen your tutorial and had a look at your IDE, impressive!
Haha, I didn't want to presume that /u/rghvb19888 isn't a secret black hat or anything :)
Pluralsight.com is a very good resource for all kinds of .net centric tutorials and has a number of course paths that will get you up and running pretty quickly. It has other, non-.net stuff as well but the .net stuff is just excellent. 
&gt; This is awful advice. I'd hope I'd never need to log into anything you have built. Why ...?
I don't think the other poster's answer is correct. What's inside of the get/set methods of Test.Modes has nothing to do with what AutoMapper does to TestDto.Mode. The default behavior for AutoMapper when trying to convert an Enum to a string will give you the names (Dev, PreProd...), which is what you are seeing. You will need to setup an explicit mapping for the Modes property using AutoMapper's "ForMember" method if you want the values as strings ("0", "1"..). You could also change TestDto.Modes to IEnumerable&lt;int&gt; if you don't care about them being actual strings and just want the values.
If you sign up on Dev Essentials with a Microsoft account you should be able to get a 6 months trial of Pluralsight, amongst some other MS related trials like Azure. https://www.visualstudio.com/dev-essentials/
For many reasons: 1) Its just confusing for a user, they may confuse it with their password. 2) Users are notorious for using similar passwords, and salts are supposed to be unique. If your service has a lot of users then I'm willing to bet a good percent of them have "salt" as their salt or "1234" 3) There is literally no reason to ask a user to provide a salt when you can generate a much more secure salt very easily. It isn't something they need to worry about. 4) As someone who knows about hashing and salting passwords, I would never sign up for a website that asks me to provide the salt. That just screams insecure to me, if you aren't following best security practices on sign up then where else are you falling short? I suppose my question would be, why do you ask your users for salt when it is very easy to generate one yourself? 
&gt; ... when you can generate a much more secure salt very easily. This one I agree 100% with, the rest I agree _"50%"_ with, since it depends upon how you display the _"provide new salt textarea"_ element ... &gt; As someone who knows about hashing and salting passwords, I would never sign up for a website that asks me to provide the salt. I think there is some confusion here. What I do (and said) was **during installation**. If you need to install a server-side web application during signup on another guy's website, I would argue there is something very wrong with the website you're visiting ... The step of providing a salt occurs during **installation** of the web app itself, and can only be done by a root user.
Thx ...
You need to know both, but I would recommend the Database programming first, because you can use those concepts in MVC, or any other .Net environment.
Just want to throw in a bit about .NET Core. Even if you aren't primarily using .NET Core, it may be a good idea to learn it, as well as the differences between .NET Core, .NET Framework, and where the .NET Standard fits in. To start, Core and Framework are **runtimes**, where the Standard is an API standard that the runtimes target. Then, if you write class libraries, if you can make them target the .NET Standard as you can more easily reference .NET Standard across both .NET Core as well as .NET Framework (IIRC it compiles the library using the runtime of choice if that runtime supports the standard you picked.) Then if you do ever move to ASP.NET Core or .NET Core in general, you should have less to migrate as your libs should work out of the box (as long as you didn't somehow bring something platform dependent into them.) As long as your company doesn't take issue with this, anyways.
Here is a free ASP.Net MVC course from Microsoft: Introduction to ASP.Net MVC It's a few years old, but it will give you the basics and from there you can find other resources for some of the newer stuff like ASP.Net Core. mva.microsoft.com/en-us/training-courses/introduction-to-asp-net-mvc-8322?l=nKZwZ8Zy_3504984382
Both, really - very often they are listed on the same job listing. But, MVC is a more pervasive part of the newer ecosystem, with more going into it all the time. I'd say that if you have other SQL database experience, you can pick up the .NET/C# stuff more easily than MVC and related bits. Source: 15 year .NET dev, currently interviewing for new work. Already deeply SQL experienced, but currently studying MVC.
&gt; What I do (and said) was **during installation**. Salts are primarily meant to solve for rainbow table attacks. They can only effectively do this if they are unique to each user. If you're salting using the same value for every user then you actually haven't solved the problem. Honestly not trying to be snotty, I just know that it's a common misconception that salts do not have to be unique and I think it's better for everyone to correct at every opportunity. 
For Web APIs I always create integration tests, no unit tests of single controllers. My tests launch the Web API self-hosted (that's pretty fast), I inject fake domain repos and services and write tests against the Web API client (the same client that users are going to use). That way I feel pretty confident that the client works (calling the right urls, with the right headers, authentication, handling of errors, versioning,...), that objects are correctly serialized / deserialized and that the controller does what it's supposed to. Occasionally I also add a few tests that use the HttpClient class (instead of my Web API's client) to transmit invalid json documents with missing fields, wrong data types,... or to create GET requests with invalid query params. 
Does the shared library make the api call? Otherwise check to see if the 2 projects use different classes to make the api calls
4.6+ can *use* TLS 1.2, but only 4.6.2 uses TLS 1.2 by default. 
oh shit bro damn, I retargeted the entire solution to 4.7 (thanks, Target Migration Tool extension!) and pcap'ed a local test session and it's still hitting the API over tls 1.0; I wonder if the devs somehow explicitly set tls 1.0 in their use of the shared library (that project began many moons ago)
Not that it matters, but are you sure it was 2015? I thought only 2017 was available now.
Positive, I need to update this laptop lol. I have 2017 on my workstation.
I don't understand why ...? Do you have a reference for this ...?
No one is really a .NET dev. C#, F#, etc. dev but not a .net one specifically. But game dev, desktop, and mobile are all paths with decent jobs. But it depends on where you live as to the job market.
they use the same class(es) from the shared project but I will need to figure out their usage of Autofac (a dependency injector) so I can figure out whether they're explicitly setting tls 1.0 somewhere in the web project
ASP.NET is just a web framework. You use any .NET language along with the framework libraries. 
Do you mean no one is a .net dev but not a ASP.net one? And if so why not? seems weird that u have to do web stuff why not just be really into desktop and mobile stuff and learn VR with that?
This https://www.pluralsight.com/blog/it-ops/learning-path-web-application-development-with-asp-net-mvc5
I mean no one really identifies as a .net dev. They identify by their language of choice. .Net is a set of libraries, not even a framework. You would be more likely to see UWP, ASP.Net, etc. if you were looking for info on a specific platform/framework. Or if you wanted a more general sense of what is out there look for info on the languages. Don't search for .net itself. Also I stated that desktop, mobile, and game dev were all viable options to look into. But look at your local job market as every location is different.
Ive worked for 5+ years as a C# sql guy with no real asp.net ability. So plenty of jobs like that. I can get by but its not pretty. But as craig said no one calls themselves a .net dev. 
ASP.net is really a framework (or better a design pattern) in which you use C# to code. Although the front end will use JS frameworks as well, which you will need to learn.
Using a unique salt effectively creates a unique hash function for every password, in the sense that var hash = hash_alg( random_text_1 + password_text ) Would be a distinct function from var hash = hash_alg( random_text_2 + password_text) And would provide two different output values for the password. **But why?** The general idea is that users are bad at passwords and will often use a password that another user has chosen (e.g. - multiple users may settle on 'pa$$word1' after the application prevents them from choosing 'password'). If you use the same salt for every user, and 100 users have chosen the same password, then an attacker who has cracked one password can compare that to your other hashes to see which other users they have also compromised. [This stackexchange answer is a more thorough explanation of the principle.](https://security.stackexchange.com/questions/211/how-to-securely-hash-passwords/31846#31846) Just go through the sections that include the word 'salt' if you want to focus on why random unique salts are useful.
&gt; Just go through the sections that include the word 'salt' if you want to focus on why random unique salts are useful Thx :)
I'm a noob, but I thought .NET was essentially a software framework which housed CRL and CFL.
Figured it out - the devs had &lt;httpRuntime targetFramework="4.5" /&gt; in web.config for the web app; changed this to 4.6 and it's right as rain. 
Figured it out - the devs had &lt;httpRuntime targetFramework="4.5" /&gt; in web.config for the web app; changed this to 4.6 and it's right as rain. 
Desktop apps, Xamarin apps, Console apps, Portable Class Libraries - all .Net development with no ASP.NET. You could easily mix and match between them.
.net + three.js should count as VR. 
Now we're getting into the "naming stuff is hard" part of development. And when it comes to the term "framework", it's pretty much been a buzzword from years past that lost its meaning and can mean 100 different things to 100 different people. It's like "cloud". What you may consider the cloud and what I do may differ but we both know that it's made up of mostly [Linux servers because of a drawing someone created](https://i.redd.it/l4bu591x5syy.jpg). In Apple and Microsoft land when they say framework, it's really just a set of libraries bundled together. However by bundling them together and saying it's .Net Framework 3.5 or whatever they can version them as a packaged whole. They can contain extra stuff other than just dlls, like xml, ui objects, etc. And they can package in the run time environment and all the other stuff needed for the apps to run in a managed sandbox. With all of that together it is a framework based on the definition that they use. It's not an incorrect definition but it's not the one that I use when I say that ".net is not a framework but a set of libraries". I tend towards a different view of frameworks that can be summed up by the phrase: "a library is something you call and a framework is something that calls you". So ASP.Net, WinForms, etc. would fall into the category of a framework based on how I use the term. You could potentially make the argument that the CLR would fall under that but I view that like the JVM, it's my environment/runtime. It's at a much lower level. And the .net libraries are just that, libraries. So depending on how you define a "framework", .net can either fall into that category or not. I just classify it in the not category. Edit: The phrase that I use for defining a framework comes from Martin Fowler and is referred to as the [Hollywood Principle](https://martinfowler.com/bliki/InversionOfControl.html). He shortens it to "don't call us, we'll call you" when describing what makes a framework.
Utter bullshit. I've been a .Net developer for 15 years and have met hundreds of other developers who called themselves .Net developers. Google ".net developer," and wake up.
Need to set `ServicePointManager.SecurityProtocol = SecurityProtocolType.Tls12` For 4.5
ASP isn't a language. It is a framework and runtime to construct, present, and handle the assets of a website, using Dotnet languages. I'm going to the depart from what I see commonly expressed here, and add that I've been programming in a variety of languages and libraries using Dotnet since it was released, and have much experience with it as a Dotnet generalist. I do market myself as a Dotnet dev. Weirdly, in the most recent job hunt, I found a surprising number of jobs here in LA requiring VB.net, often in conjunction with C#. I'd say I'm seeing more of those as a percentage of the jobs I'm seeing than 3 or 4 years ago. I don't know if this is just old projects opening up positions, or actually more recent ones that were started in VB, but I can't imagine a good technical reason for doing so. 
Im looking for help with finding some good C# .NET projects that will help me apply my C# and SQL knowledge, Any help is greatly appreciated.
Im looking for help with finding some good C# .NET projects that will help me apply my C# and SQL knowledge, Any help is greatly appreciated.
Perhaps consider contributing to open source projects? You can find a good list of projects looking for help at [up-for-grabs.net](http://up-for-grabs.net/#/)
I agree (without the condescending tone)
UrhoSharp supports HoloLens as far as I know. Windows Mixed Reality seems to have .NET APIs. Xamarin supports Apple's ARKit, and should fully support any future AR/VR stuff that comes out of Apple. Unity will obviously have the broadest device support, but as mentioned in another comment, it should also count as .NET.
EF is the serialisation layer, I normally I wrap the model with an interface and then mock the repository interface. Probably worth doing a google Search because there is not a one size fits all approach to data layers
If he is accustomed to PHP there is probably a chance he'll just need ADO.NET and not an ORM
Depends how powerful your machines are. If you are using a $2.5/month VM it sure makes difference. Using Debian instead of Ubuntu already gives you 45 MB more RAM the last time I've checked. In addition to that your security improves simply by using less programs.
Maybe look for backend developer jobs. It has some asp.net, but without the frontend (html, javascript, css). Asp.Net is still C#, it's just a framework in the same way that EF or ADO.NET is. A bit more complex, but you don't need to know it perfectly, start from the basics, deep dive as much as you need for the job, keep up with the changes. You can't run away from frameworks and just use the languages, otherwise you'll have to reinvent the wheel again and build your own libraries/frameworks. 
ASP.NET is in fact multiple frameworks. Webforms, WCF webservices, MVC, and WebAPI to name 4.
If you want to avoid Unity *and* stay platform independent, then there is [Xamarin](https://developer.xamarin.com/api/). And [Urho Sharp](https://developer.xamarin.com/guides/cross-platform/urho/introduction/) claims to support a number of VR and AR devices
I've looked into the example you sent... first of all, thanks for your time :) TBH, I didn't understand the main point of moving all the code from a place (controller) into another place (service). Basically, I need to mock all those dependencies to test that service, right? Also, can I test the methods that uses the repository in this approach? or do I need to use the TestHost which will give me a non-relational db... basically, non practical in-memory db :(
Honestly, I never really made the leap except for toy programs. I gave up and went to focusing on just server-side technology. (And Silverlight, may it rest in peace.)
Not an error actually. Session doesn't exist. The code doesn't recognize Session and sees it as a variable. Also, the code I used isn't finished yet. It's just that if the code doesn't even know what Session is, I can't proceed.
Make sure you have `using System.Web;`
Active directory? There's ton of info online for that? Azure is cloud right? Maybe I don't understand. You might need a breather, and then come back to it in 30 minutes.
&gt; I thought you had to have a developer license to get old versions? Or did you download from elsewhere? I installed it on this laptop back in 2015 and just recently pulled it out of storage because I've had to do a lot of traveling. &gt; Tangentially, don't sign up for a coding bootcamp unless you have a solid base in programming or are just good with math. They're not worth it. Afterwards, you'll end up without a decent job, only this time with $10k in debt. I've been a PHP dev for 15 years and mixed other languages (hobby game dev), so I'm not too worried there. As for paying for bootcamps, I can't afford it so I'm relegated to the free online variety at present. Cheers :)
That looks good, I'll pick that up after I finish doing my C# refresher.
Thanks for that, I'm currently working my way through https://mva.microsoft.com/en-us/training-courses/aspnet-core-beginner-18153
Thanks for the advice :) One of the analysts I spoke to specifically mentioned brushing up on .NET Core so that's definitely on my to-do list.
You are more than welcome mate. The point of moving is to separate concerns. Controllers at the most base level are there for routing. Why should they be concerned with database access and all that other good stuff? A controller should ideally be a defined entry point into the codebase. It defines its http method, its route, and what it accepts, modelbinds and validates. After that IMO it should delegate on responsibility as thats quite a lot as it stands. The point in removing out the responsibility into a service is that the controller is then super simple to test. You can check attributes (http verb, route, auth), check it invokes the service as expected, and ensure it returns the correct view, if applicable. No this doesnt make testing the service any simpler, and yes you will have to mock the world in order to get it done, but each method call from your sample was quite concise and shouldnt be too hard to test. What unit testing framework are you using? I'm a fan of NUnit, but most have a "setup" method you can call to handle the mock initialisation, so your actual test just has to setup the relevant mocks the method you want to test uses. Do you have a sample of what your tests look like? Below is an example of a test I would write (NUnit and Moq) - note: this is incredibly verbose, which im in favour of as it gives you nicer messages from your test engine failing as oppossed to "Test X failed" [TestFixture] public class MyTest { private MyClassToTest _myClass; private Mock&lt;IDependency1&gt; _dependency1; private Mock&lt;IDependency2&gt; _dependency2; private Mock&lt;IDependency3&gt; _dependency3; [SetUp] public void Setup() { _dependency1 = new Mock&lt;IDependency1&gt;(); _dependency2 = new Mock&lt;IDependency2&gt;(); _dependency3 = new Mock&lt;IDependency3&gt;(); _myClass = new MyClassToTest( _dependency1.Object, _dependency2.Object, _dependency3.Object ); } [Test] public void GetStuff_Returns_Correct_DbObject() { var dbObject = new DbObject(); var primaryKey = 123123123; _dependency1.Setup(d =&gt; d.GetThingFromDb(It.IsAny&lt;int&gt;())).Returns(dbObject); var result = _myClass.GetStuff(primaryKey); Assert.AreSame(dbObject, result); _dependency1.Verify(d =&gt; d.GetThingFromDb(It.IsAny&lt;int&gt;()), Times.Once); _dependency1.Verify(d =&gt; d.GetThingFromDb(primaryKey), "incorrect primary key passed to dependency1"); } } 
Looks pretty awesome, and that's a nice arch diagram; hate overlapping lines. Keep us updated!
Ok, let's go with something really basic. Let's say I want to store `public string Foo` as a property for my users, and then I want to retrieve that information and display it in the console for some reason. What is it I need to set do?
Free courses on plural sight
That namespace doesn't exist in ASP.Net Core IIRC.
There are a small handful of languages that work with .net. C# is the main one and happens to be an excellent language to learn and is very enjoyable to work in. So learn C# which will also teach you programming basics. Sticking to .net, your main framework to make web apps is ASP.net. Good news! A new framework called Asp.net core is on the up and up right now so it's a great time to get in on the ground floor where your lack of experience won't be a negative because it only came out recently. Whether you stick to .NET or not, webpages uses HTML, css and javascript to display the front end of the site (what the user sees). You'll want to get familiar with those three. For the backend, that's asp.net (core). It will handle routing, connecting to your database, hosting files, your business logic... so most or all of the functionality of the site. If you decide to move to another language or framework, you will still use some combination of HTML, css and js for the front and then whatever backend framework you choose for your language (django or flask for python, spring for java etc).
This is great. Microsoft has really been upping their game as of late. Thank you MS, I look forward to using this on my next side project.
Me too bro, I am starting to love MS like the way I first fell in love with it :-)
It works really well, been using it for quite a while. It also includes monthly credits for build agents.
They've had this for years. 
Ah yes the build agents.. they're ..ok..but pricey and the free trial is very build time limited...if you like build agents CI/CD gitlab has a much better free teir
Yes, I like gitlab as well, but their build agents only work with .NET Core (via docker).
I'm not AD expert and haven't had to work with it directly for years now, so take this with a grain of salt: I'd never use AD to store that information if I could avoid it. I would only use it for authentication; verifying a username and password and nothing else. After that, you could store all the other user properties in your own database just like you would for any other application. Just think of Azure AD as your dumb authentication endpoint. There are sample projects here showing how to set that up for pretty much ever scenario: https://docs.microsoft.com/en-us/azure/active-directory/develop/active-directory-code-samples. Keep in mind too that Azure AD is *not* the same as on-prem, full AD. It's basically a tool to make it easier to SSO against your on-prem AD without having to open it up to the outside world. This may not matter to you if you're just using it for authentication, but if you end up getting pushed to use more features of it I highly recommend reading through this: https://docs.microsoft.com/en-us/azure/active-directory/active-directory-whatis.
Also Circle CI 
I use a local build agent instead of a hosted one. It removes the cost and more importantly allows me to use a private nuget server that is ip restricted.
Check meetups in your local area. There are tons of .NET user groups. They often have talks on different areas of .NET. Some of these talks may be over your head at first, but it is great exposure to the concepts. It is also a great networking opportunity. You can talk to developers already in the industry; see how they started and maybe network yourself into a mentor-ship or future position once you get more up to speed with the framework.
Besides price, why would I choose to use this over github? Also, bitbucket offers free private repos, why would I use this over that? Genuinely curious what the differences are.. What problems are being solved?
Yep and I've been loving using it for years
The build and release system is really quite nice. If you're using Azure it integrates *extremely* well. I've set up build and release pipelines for numerous applications, internal services, etc and have been very pleased with the simplicity of it all. Also it has ARM template deployment options as well, so you can have continuous deployment of your _infrastructure_ along with the applications. It's a very robust set of tools and also integrates seamlessly with their work tracker. If you're just looking for source control, github or bitbucket are fine choices. The only added benefit would be single sign on through Windows with Azure Active Directory so you can manage groups/permissions at an AD level and never have to authenticate to use git on windows. However, if you have applications deploying to Azure VSTS offers numerous benefits over other version control and CI services.
Thank you for the detailed response! I appreciate the information.
You mentioned that it will be MVC you're using. Your application entry point will generally be Global.asax.cs at the root of the web app (if not it may also be program.cs/startup.cs). On app start the main things going on are route registration, dependency injection bindings, logging, and registering filters. Once routes are registered then it's just Controllers, models, and views for the web layer.
This. The tutorial you are watching uses asp.net mvc 4 and you are attempting to re-create this application in .net core. You'll need to follow this documentation to achieve the same effect as in the tutorial, though the syntax will not be the same.
Yeah.. but posting it here is similar to posting that 'Google does emails' links to product page... Borderline spam. or maybe it was meant for /r/programingcirclejerk
It's news to me.
Can you do public git repos on VSTS?
Also, if you are using c# the online code browser/editor is more advanced. You can actually jump to definition, etc. 
It's not just git repos. They're git repos hosted within Team Foundation Server/Visual Studio Team Services which has build and release tools.
I have to say, this is the first time I've ever seen anyone brave enough to also put node and Java along with ASP.NET in the same diagram and architecture. 
This is anecdotal but Microsoft uses it internally for almost everything.
i would consider using jira over TFS gor task tracking. TFS tasks works well for project execution by devs and QA, but Jira covers SDLC end to end from requirement discovery, development, documentation as well as product sustainment.
We don't really have a choice at this point. Anything is better than what we are currently using though. At my last job, we used Jira and it was nice...but our deployments, builds, code and version control are all already managed by TFS, so it just makes sense to use it. 
Beyond free courses on pluralsight, checkout the Microsoft Virtual Academy. There will be quality started courses on there as well. Just look for things like into to C# to get used to the language basics and then start looking into intro to asp.net courses I'd say. HTML and CSS is more straightforward but JavaScript can have a learning curve just like C#. 
I did something like that also, but with Chromium as "renderer". You realised it also with setting the window as child of the desktop, right?
Do y'all use git with them or team foundation services?
You are calling an asynchronous method from a synchronous one. It's probably terminating the processing before your save changes code can execute. Try making the calling method asynchronous as well and throw an await at the call site.
You are calling an asynchronous method from a synchronous method. The request is probably dying before your save changes completes. Make the controller action asynchronous as well and throw an await at the call site.
Since 2015! https://blogs.msdn.microsoft.com/devops/2015/10/01/announcing-git-lfs-on-all-vso-git-repos/ (VSO is the former name for VSTS)
I wouldn't. I hate JIRA's task tracking. Five priority levels? That's great until you have six tasks to rank. I'm not saying TFS is good, only JIRA is worse. None understand that if task A blocks task B, A's priority needs to jump ahead of B.
Which is the only reason I use GitHub.
in your example, i would argue that the priority *shouldnt* change. priority should be an indicator of importance to the project, without regard to predecessors and successors. a task being blocked by another doesnt increase either priority, it just indicates order of execution, which is handled during sprint planning. If we DID have order of execution be dictated solely by priority, we would need a dozen priorities, adding more and shifting numbers around to fit the current picture as you finish off the highest priorites. Its not a scalable approach. FYI, adding additional priorities is not a trivial task for TFS either, as it requires an admin to edit the work item template, an equivalent action in Jira. If no tool supports your process, perhaps a review of the process is required.
WebForms
Interesting. Thanks for the tip.
&gt; we would need a dozen priorities No, you need a numeric priority. A double so that you can always slot task 25.5 between tasks 25 and 26. High-Med-Low is useless when it comes to choosing what to work on next when you've got dozens or hundreds of tasks in the queue. (Though they can be useful in determining an initial number.) 
again, this is a planning process problem. A huge list of tasks should be grouped by milestones or prioritized epics so you can plan holistically.
If you are using Azure App Services you can setup automatic CI/CD from a github repo with almost zero work. Works great for test environments or branches where you deploy on commit.
If you only have one product with a well defined delivery schedule and no production support issues, sure. Feel free to do everything by milestones. But if you are an internal shop managing dozens of production applications that mode doesn't work. You need a way to prioritize tasks across the entire company, especially when multiple teams are involved. This becomes even more pressing when you are heavily invested in micro-services. Priority inversions, where a high-priority task is blocked by a low-priority task, can be especially hard to see when they run across multiple projects. The most efficient company I ever worked for had one task queue shared by everyone. Unless you were on a strategic feature team, every day you would scan down the list for the highest priority ticket that you felt capable of doing. Everyone was largely self-managed, working from the same backlog. There was still a planning process to set priorities. And we had rules such as the age or number of affected users would automatically increase the priority of bugs. But mostly it was an agile process with very little ceremony. 
The problem with that is its expensive for personal projects and we've got plenty of in-house CI servers for professional work. 
Yeh, and I'm OK with that. 
Pro tip: dont use aspnet controls, or postbacks, ever. Ever ever ever. Create your controls in raw html and use javascript to make it dynamic. Send data to the server through ajax. Its 100x simplier, predictable, and faster than using the built in asp controls. 
Pretty much what this person said. Do that, don’t look back. This does mean you’ll need to learn some JavaScript, but it isn’t that bad, really. If you want to keep it simple, check out JQuery. If you want to be one of the cool kids, then look at React or Vue.js to accomplish this.
The priority score was actually formula based: High: 400 pts. Med: 300, etc. This could be set by any engineering manager. Other people could make recommendations: but they only could rate things 50/25/10 or something like that. Enough to bump tickets, but not override an engineering manager. Each day (or was it week? can't remember) added +1 so that old stuff would bubble to the top over time. If I recall correctly, due dates also factored into the equation. Some people also got a few points they could add to items in order to tweak the order. All this meant that lots of people got to have input into the process, but no one person has to make the final decision. So we had design meetings, but not priority/planning meetings. (Other than strategic features needed for regulatory compliance.) 
My sympathies
If you are using EF.Core you can use their in [memory provider](https://docs.microsoft.com/en-us/ef/core/providers/in-memory/) and everything is just normal code then. No need to abstract the abstract 
I thought we were talking specifically about one project, but if you want to discuss project portfolio within a operations environment we can go to that level. I agree with most of your points. There needs to be a way for multiple teams to process a large queue of hundreds of tasks that addresses the highest value items first. But having everything based on a single "priority" field does not give you enough insight into the driving factors at a holistic level, preventing the business from plan strategically. There are many fields at your disposal that gives planners the necessary information to plan hundreds of tasks with multiple indicators of priority (such as impact, severity, system), and having it all bundled into a single bucket would lose all holistic analytical value. I do not know of any ticketing or task management system that has a double precision priority value by default. Unexpected work due to a bug is always tough to plan against. In PMI you add it as a predecessor. In Scrum and Agile you create another task that is higher on the list, bumping other work down. But neither system actually affects the priority, its beyond priority, its a strict order of execution. If you MUST have a priority attached, then it can only be the exact same priority as the originating task, as increasing or decreasing it would disrespectful of the other tasks in the queue. This is why there are additional ordering mechanisms in every process outside of "priority" in every process, since its not the end-all way to order your work.
&gt; There are many fields at your disposal that gives planners the necessary information to plan hundreds of tasks with multiple indicators of priority (such as impact, severity, system, due date, staleness), and having it all bundled into a single bucket would lose all holistic analytical value. You can still have all of those, and in fact I recommend it. At one place I worked each of those contributed to the final priority score. Took awhile to get the formula right for our company, but it did allow us to quickly prioritize literally hundreds of tasks. And eliminating confusion about what to work on next made it well worth the effort. &gt; I do not know of any ticketing or task management system that has a double precision priority value by default. TFS does. At another company we used an integer 0-499 in ClearQuest+MS Project. (That's how I stumbled onto the idea.) &gt; God forbid you need to re-priotize groups of tasks with 5-digit priority precision. Oh that's trivial. Just run a renumbering script once a month. I recommend numbering by tens so you don't get into decimals too quickly. This is assuming you are directly setting priority scores and not using formulas. 
&gt; I have made a system that was entirely priority driven than automatically bumped up priorities as they grew stale, and it simply didn't work because it did not respect other tasks How big was the age effect? For us each priority level was worth 100 points, while age was 1 pt. per day (week?). So only really old stuff could actually jump a priority level, at which point it really should be fixed or discarded. 
Honestly, I worked on the team responsible for building VSTS: The high level management goal is to mine the private repositories for intellectual property. They are not doing this for you, it is a last ditch survival effort by upper management within Microsoft. 
&gt; 2018 &gt; Using Unity IoC
I haven't used it, but it looks nice. If I understand the site correctly, it looks like a RAD tool for web apps a la Microsoft Access. I've used similar tools in the past, and they're fine for internal line of business applications. They can be used for external apps I suppose, but they're normally too data-centric for non-power users anyway. Previously, I used a tool like this for a company that didn't want to invest a ton of money in maintaining such a system and they were perfectly happy to have the bulk of the code be generated. That was on a product called CodeCharge produced by YesSoftware. There have been many other such products in the past, but that's just a comparison point for you. Anyway, it looks like this is FOSS too, with the source available at https://github.com/volkanceylan/Serenity Also, wasn't Microsoft's Lightswitch a similar product? I never used it, and now it's at end of life IIRC, but that's another comparison point as well. When you're using a tool like this, you need to be aware of the trade-offs. Yes, you will get to a working system very quickly, and if that's good enough, then great. However, if you need precise control over look and feel, caching, database transactions, reuse of existing libraries, etc. then you have to be aware that you will be working against the tool at some point. Customization of the templates, or hooking into their event model is typically possible, but then you're learning development technology that will only be useful for Serenity, or whatever tool you've chosen. Not only that, but your customizations will likely need to be adapted as they tool in question undergoes revisions. All that said, a typical developer simply cannot compete with the velocity a tool like this offers. In the example I mentioned above for a client, once I had integrated the data architectures for them, I was able to take about 15 full featured MS Access applications and create full-on .NET web app replacements for them in about 3 months. And seriously, it was not hard. The hardest part of it was showing them how to work with the tool in a way that went along with its intended uses. Once they saw the possible productivity, the temptation was to use it for things for which it shouldn't be used at all (i.e. applications which are not data driven internal LOB apps). Hope that helps!
That wouldn't surprise me. I haven't ever tried to investigate Stack Overflow infrastructure (articles, blogs, etc.), but if they keep even a small degree of their own servers, I've never found build agents to produce a significant load. I use one in my on-prem infrastructure, because there's this really weird build dependency we have in my org for C# projects. I also use it for "gold image" publishing to SMB for certain projects. Thanks for sharing the information :)
Remind me, what are some of the Azure Service Bus guarantees? Will there ever be a failure delivering your message? You suggested that your design could survive a failure in either store, but is that desirable? What happens if they get out of sync? Let me know how it goes. 
Thanks for sharing! Does it have deferred domain events?
So in general, one of the benefits of CQRS is the ability to scale up the read side independently of the write side. I think this use case might be a bit overkill in that regard, as caching the user data would give you a comparable amount of throughput. Also Event Sourcing is a really great approach when you're trying to understand the state of the system at every point in time. I'm not sure how much this is needed for user registration, so again it might be a little over kill, but you didn't outline all of your use case. As far as what you've outlined, I would personally lean against a typical relational database for a CQRS/ES solution. Both are far more suited for a document store, such as MongoDB, Couchbase, or RavenDB. The ES side is a bit of a different discussion as to why, but on the query side, your focus should be optimizing read performance. Of course you could structure your relation DB that way, but the tendency is to have a normalized data model with separate tables and joins that will have an impact on read performance.
Hi Bro, Thanks for pointing getting out of sync bro! I need a transaction compensation strategy here. I am planning to use NServiceBus' azure transport for topic based messaging. It supports a retry policy and dead letter queueing. This will help on partition tolerance management. Saga implementation might also be a good idea!
Hi, I am building a system with a goal of simulating a million users management for plain research and knowledge acquisition. I am also using a service bus and event store because I plan to propagate the user records on other microservices that I intend to write here is the general overview of what I am trying to build: https://i.redd.it/or1b5a0dha801.png The reason why I initially went for the SQL Azure on the read side is to have normalized view, but then I've realized that using a NoSql would eliminate the locks from the writes that the materialized views would produce on updating the materialized views.
Very nice insightful article bro! Keep it up and I hope you share more in our community.
Hey thanks for the input. I have a lot of small simple SaaS type apps to make. I recently made the move from MVC style apps to API/Angular5 style apps and the amount of extra code I have to write has hit me hard. I am obviously slower because it's new to me but also the amount of repeating of models and multiple layers just makes everything really time consuming. I guess I am just looking for something that can give the same effect in much less time. Thanks for the keyword "RAD tool". I had heard it before but it didn't click. Just did a bit of googling and seems like there are quite a few options available
Does MongoDB support real batch operations yet? The major advantage of using a queueing system for writes is that you can insert a batch of a hundred, a thousand, sometimes ten thousand records at one time with nearly the same cost as inserting a single record. I can't stress enough the performance difference between using a bulk insert (or bulk insert+merge) over individual insert/update operations. We're not talking just a percentage improvement, but rather orders of magnitude. Older version of MongoDB didn't support this and your "batch operations" were really just a series of single writes with the associated pre-operation overhead. 
&gt; The major advantage of using a queueing system for writes is that you can insert a batch of a hundred, a thousand, sometimes ten thousand records at one time with nearly the same cost as inserting a single record. &gt; &gt; There's nothing to write in batch because this is a green field project. The million users would register via public registration pages. Are you aware of the asynchronicity of a messaging topic? Making a huge chunk of data pass by the service bus would kill performance and throughput of the bus.
Oh.. I'm felling pretty silly now mate, cheers for having a look! 
How would you batch user registrations when those are out of your control? What happens to the user login after message submission but before write? The technique you are talking about is great for eventual consistency, not for write consistent domain (such as user registration)
Are you replacing an existing user registration system that has pain points that ES solves?
What in particular are you having trouble with? The examples from [the github project](https://github.com/StackExchange/Dapper) are pretty clear IMO. 
Nope, just purely for research purposes
Here are some video tutorials: https://vimeo.com/68877072 - Intro to Dapper (Part 1 or a 3 Part Series) https://www.youtube.com/watch?v=FyaCtcJwpJk - Forget the big mapper, switch to Dapper - Matthew D. Groves https://www.youtube.com/watch?v=9oDBK0irzIA - Dapper: A Micro-ORM for the Discerning .NET Developer
Stackoverflow is using Dapper bro!
There are scenarios where I need the current aggregate to be persisted before executing side effects with domain event handlers. For example, when a user registers, I want to send an email to him (side effect) only if the whole registration succeeded and not otherwise. This way an email is never sent if the user is not registered, leaving the application in a consistent state. 
When a user hits my api to perform an action on something, I will typically take the uid stored in the jwt and use that the find their record in the user profile table. So quite frequently. Maybe I just answered my own question, I could put the incrementing id in the jwt.
Can't you publish events only after successful persistence? That way, email will be sent if no exceptions occurred during commit. Does your registration process involve multiple steps?
Scrap that thought, I can't alter the jwt.
Yes. I have seen approaches where two interfaces are provided, IDomainEvent&lt;T&gt; and IDeferredDomainEventHandler&lt;T&gt;, and you can implement the one that suits your needs, having the best of two worlds. I was just curious about your approach :)
Are you running in to performance issues now that actually need addressing? Do you have an [index](https://use-the-index-luke.com/) on your firebase uid column in your user table? If you're not already running in to performance problems, I wouldn't worry about making any changes. If you think the queries could be faster but don't have an index, add the index to that column.
No performance problems at the moment, just looking ahead. Will do, thanks.
Are you using MSSQL? If so (or even if it's a similar RDBMS), this is considered a bad idea, assuming that a.) you have a clustered index and b.) that index is in this column. Values like this will immediately cause index fragmentation, which will kill the performance (especially for reads) of the entire table. For cases like this, it's best to use a surrogate key that is narrow, unique and ascending (or find a better natural key). 
Such frameworks are actually good to "finish the job"... but I wouldn't recommend it for the long run, why? Well, once they will stop the project esp. if not many people use it, you will be done! No more updates and as a dev it's like you don't know like others. For what it is worth, I tend to learn the basic languages (C#, Javascript, HTML... etc) and the promising frameworks which has a big momentum (like ASP.NET, Bootstrap, Angular) cuz those are unlikely to get obsolete for the next two or five years ;)
&gt;simulating a million users I'd be making sure I had a plan for how I was going to do that first. Its a nice goal, but you've got to have some resources at your disposal to be able to actually test with that many users. &gt;users management What is user management? Is it just registering new users? Is it just collecting basic info? Are you going to be aggregating some sort of usage data across different domains? Without any of that information its tough to say what will be the best path forward. You've only given the indication from your initial post that you're going to register a new user. Okay, unless you're going to be registering millions of new users a minute for the lifetime of this app, then frankly I think that a CQRS structure is probably over kill. Also, user registration is somewhat of a one and done use case, which again, means Event Sourcing is probably overkill. &gt;The reason why I initially went for the SQL Azure on the read side is to have normalized view The purpose of normalization is to minimize disc space and maximizing consistency of data by only having one place to write it. CQRS simply has a very different approach. &gt;I've realized that using a NoSql would eliminate the locks from the writes that the materialized views would produce on updating the materialized views. It's not really about the locks, as every database engine, SQL or NoSQL, needs to ensure that what it's writing gets written correctly. Whether those locks effect your reads, or how much atomicity is when writing is where you get into nuances. On the query side of your CQRS service, your data store should be structured and optimized for reads to get quick responses. Therefore, that's going to favor denormalization over normalization and in memory data sets over ones persisted to disc. Document DBs can handle this up to a certain scale, but beyond that, a Key-Value store like Redis or Riak are much better suited. Effectively the query side of CQRS is a glorified caching mechanism given a much more prominent place in the application's architecture.
The write consistent domain in OP's design is the Event Store. Mongo in this instance is just storing materialized views, which are eventually consistent.
Unless you're hitting the table a LOT or there are a lot of rows in the table, you won't see a performance impact. You will have massive index fragmentation though due to using a random string PK.
No problem. I'm working with Angular 5 now too, and the only advice I have there is to be sure you're using ng cli. The amount of code it saves you in creating modules, components, etc. is ridiculous. It even creates a baseline unit test file for each file. As part of ng cli, you also get 'ng test', which runs the Karma test runner which actually runs all of your tests in a browser. That said, ng cli is nothing more than a scaffolding tool like the ones in Ruby on Rails. Like Serenity, it will generate a starting point for you, but then you take it from there 100%. With a tool like Serenity, you could do the same, but you start with a complete working application that you can actually put in front of a user. Within its niche, it will always be faster than using a scaffolding tool.
Don't they have examples on their github?
&gt; Now if you want to continute to extend this code, you probably need to consider the input arguments for each method to be as part of your Cache Key (In addition to the CachePrefix that is being passed in to the attribute). That’s a pretty big leap. Do you have an example of how to genetically generate the cache key from any argument list? Maybe a follow up article :) (BTW, hi Aram)
I think you've missed the point. The values of this primary key column will not be generated in a sequential manner as an integer would and would therefore cause fragmentation when new records are created. Table rows are stored physically in the order of the clustered index column (assuming the PK column). Let's say, you insert a record with a PK starting with "a", then you insret another record with a PK starting with a "c". On the disk, those would become records 1 and 2 respectively (assuming 0% fill-factor and neglecting pages). Now, let's say you need to insert a new record with the PK starting with "b"; in this case, record with PK "c" would have to be moved to make room for "b". This is an oversimplification of what happens but you get the idea of why using non-sequential values as the PK, or using a PK with greatly varying values, is not very good. In MS SQL, this effect is mitigated by fill-factors (essentially, leaving gaps for records that need to go in between others) and the fact that records are read physically in batches (or pages) instead of invididual records. This is still an over-simplification, but feel free to read the MS documentation. 
I would recommend you build two versions, one using a basic read/write pattern from a single table, and a CQRS and compare the two based on performance, scalability, complexity and amount of code required. You mention having the materialized view for lock contention. MSSQL has two main types of isolation levels "read committed" and "snapshot". The default being "read committed" and in the case of a user registration system, you would only lock the row of the user trying to update a password. A user can't/shouldn't log in during this time and if they do, the lock contention helps to make sure they are always logging in with the latest password. A properly designed table with properly designed indexes is key here. Having one million users log in and query from the same MSSQL table isn't an issue. Having 500,000 update their password while having the other 500,000 won't be an issue. Your biggest issue in this case is having a properly configured web server behind a load balancer as IIS bases the number of worker threads on the number of CPU cores. Snapshot does away with all of those locking rules and just gives you a snapshot of what the database looked like when you started the query. More DB resources are used, but shifts some responsibility to your data access layer. EF, at some point, used to build a where clause of your old data to help make sure the data you had was not already updated. 
To me, Radzen seems like yet another RAD tool (see what they did there? RADzen?) that can generate against the metadata from a service as well as from a database. It's another take on the same idea. Nice idea though.
That one is OK but still faces some challenges like locking across quorums and latency related locks on geographical replication. PS now have in-memory tables in SQL Azure (locking don't exists there. it also implements eventual consistency. 
Hi Tyler, Sure coming up :), it won't be as generic as you expect though...
[This article](http://www.itprotoday.com/microsoft-sql-server/sql-design-how-choose-primary-key) discusses some of the tradeoffs. To quote: &gt; Brevity: Because the SQL Server query processor uses the primary key index for lookups and comparisons, choose a brief primary key—one column, if possible. You use primary key columns for joins (combining data from two or more tables based on common values in join columns), for query retrieval, and for grouping or sorting a query result set. The briefer the index entries are, the faster SQL Server can perform the lookups and comparisons. For example, you can use the primary key column for query retrieval (SELECT * FROM SalesPerson WHERE EmpNo = 101) and for data modification (UPDATE SalesPerson SET EmpName = "Joe Buchanan" WHERE empno = 101). In addition, you can use the primary key column to group or sort a query result set (SELECT * FROM Customer ORDER BY CustID). And this: &gt; Data Type: Integer (number) data types are the best choice for primary key, followed by fixed-length character data types. SQL Server processes number data type values faster than character data type values because it converts characters to ASCII equivalent values before processing, which is an extra step. &gt; &gt;Fixed-length character data types are better than variable-length character data types because SQL Server must decompress variable-length character data before processing the data. This extra step consumes valuable processing power.
This is the correct answer. We were using GUIDs as PK and clustering before and it caused really bad fragmentation. Allowing the GUID to be PK but creating a separate auto-incrementing int ID column for clustering gave us a huge boost in performance.
How many user profiles? Can you cache them in memory?
No problems because I haven't dived too deep into it. It's just that having an end-to-end reference project would be nice. I'll start with the docs and figure everything else out.
I wonder if you can use an ordinal collation in order to speed up character-based indexes. The default, which is case insensitive, obviously requires more steps to handle things like `'a' = 'A'`. 
[This](http://archives.miloush.net/michkap/archive/2005/01/23/358946.html) might help understand how SQL Server would deal with it.
Thanks for explaining that. Sorry, I meant previously that I'm keeping my incrementing PK and just adding a unique index to the other field.
You might just need to upgrade to the latest version of the nuget package for all your projects instead of downgrading. In the nuget package manager there is a consolidation option that lets you easily check what version of each package you have for each project. It's a good idea to have your packages at the same version unless you specifically need not to. 
Can you explain the need to use Node/Java/AspNetCore as backend APIs? is there specific tech underlying those backends that warrant this? Also what will the microservices be built in? bare bones or are you going to use akka/orleans? very nice interesting diagram though. I fucking love CosmosDb
I have this occasionally a problem when I right click add view. Copy the code, delete the view, make a new view and paste the code 
I have a sample project on Github using Dapper with the Unit of Work pattern if it's of any use ... https://github.com/matthewblott/dapper-unitofwork
Interesting. Can you tell me a typical case where you improve performance by writing directly to IL?
Yeah same, I had no idea they did this. 
I wish I could have convinced our "DBA" of this, but he hates using GUID or UUID, or even basic auto-incrementing integers. So, we have a bunch of PKs as strings. Granted they aren't random strings, but still. I feel this route would be the way to go either way.
You can also do stuff you can’t otherwise do, such as modify a value type inside its box directly. 
also as an event store, wouldnt TableStorage be cheaper and more suited as an event store than CosmosDb? CosmosDb seems overkill.
That's interesting. Think this could somehow affect unit testing in .NET?
Check out these: * [Infrastructure](https://nickcraver.com/blog/2016/02/17/stack-overflow-the-architecture-2016-edition/) * [Build and Deployment](https://nickcraver.com/blog/2016/05/03/stack-overflow-how-we-do-deployment-2016-edition/) They are really insightful I think.
Sorry for the thread necromancy, but I'm curious here. Is your objection against code first aimed at the method itself or at people using it poorly? With good indexing, it would seem to me that code first saves a good bit of time, and there's nothing preventing you from intercepting certain queries to run hand-written SQL in cases that need better performance.
Actually, it started to be usable about a year ago. It was one of most horrible apps I worked with, so slow that I was sometimes able to make tea while waiting for page to load. Fortunately, it's fixed now. Edit: and I can even recommend it now ;)
Around a month or two ago, I was looking this up, and the general consensus seemed to be "No".
Do you remember why they said no?
Because the ecosystem is still changing. Consider this RTM for hobby projects and alpha for anything "Enterprise".
Not yet. Once the documentation settles down, issues in EF Core and cryptography are resolved the ecosystem can catch-up. Perhaps 2.1 or 2.2 ?
You are really concerned about locking, in the context of a user management system, your locks are minimal. You want locks across your quorums because a user can't change their password twice at the same time. In-memory tables have locks, they are just different. Just like how c# you can lock on an object because two threads can't write to the same data without consequences. In the Memory-Optimized tables it just kills one off like it does with deadlocks. But again, you wouldn't issue two change password requests at the same time in a user management system. Context is key to locks in all aspects of programming. *Experience. Dealing with a 400gb SQL database in an Availability Group and replicated to 4 different locations in the US and to another 500gb DB. 
What have you had trouble finding documentation on? I've only had one obscure issue to work through that I couldn't find documentation on. I just poked around in the source code and figured it out in an hour or so. Not a deal breaker.
Sorry for the lacking answer. I think it basically amounted to the fact that it's still changing quite quickly right now, and isn't properly battle-tested.
 I highly disagree with EF Core not being production ready... because I have a large application using it perfectly fine. Does it have a couple of (would be very nice) features missing? Sure. A lot of the things missing however aren't used in most scenarios - hence why they're not a huge priority, and anything that isn't fully supported... you can run manual sql for, which is what you would do with any other provider such as dapper. There are no breaking bugs that would make me not want to use it.
 My thoughts exactly, there's been way more example projects for getting v2 setup compared to previous, and aside from the start-up code, there has been nearly no changes. Not sure what you would need to 'hunt' a solution for :s
Bugs aren't the problem. Efficiency is the problem.
My biggest gripe so far is that EF Reverse POCO Generator doesn't support core yet. But that's not on the EF team. Looking forward to using .Net core for future web dev at work. Hope EF doesn't become a problem.
I would love to see the source for his "benchmark". I don't believe that at all. Try to run some EF Core queries on a real life database with dozens of related tables with thousands or millions of rows and take a look at the queries that are generated. They're awful. Most of the time, EF will generate multiple queries when you have multiple includes in your seemingly simple statement.
Scroll down, it's literally on the post lol: https://github.com/StephanyBatista/benchmarkef the application I have running on production has ~800 tables and the main table has nearly 18 million records (which sure, isn't a ton compared to some, but hey). Runs perfectly for me.
Sorry for the long reply delay. Didn't want to work on this over the weekend. If I were implementing Identity to manage my roles/claims/user information, I suppose the way to do it would be to configure the sign-in method to use information I get back from AAD to look up the user and then sign him/her in via Identity? Seems easy enough to set up.
&gt; erate multiple queries when I have to say EF has always created nasty terrible queries for me in these cases. I've been using it since it was fresh and new, sometime around 2008 if I recall.
Looks very interesting.. I'm definitely going to try some of them out tomorrow! Thanks for the link!
Oh boy have fun
Never mind, actually. The client has decided to call it a day and give up. Sigh. Thanks for reading anyway. Would've been a fun experiment to see it come back! 
How many visitors do you have to your site, anyhow? Are we talking thousands of people for whom your site is broken, or is it just a few dozen? If it's just you and a few testing buddies, tell them to refresh their caches.
ASP.NET at least got a little better first class treatment than some areas. Desktop, obviously, just isn't really a thing, but I was kind of surprised to see CONSOLE apps really aren't fully fleshed out yet. "It's coming" seems to be the mantra for a lot of Core still... Looking at you Microsoft.Extensions.Hosting... Hurry up with the [generic host](https://jmezach.github.io/2017/10/29/having-fun-with-the-.net-core-generic-host/). :-)
I made a DNN site about that time, for a client. They never updated, and just a couple years ago, they just made a newer site altogether. I did look at the update prospect for them, but it was extensive. A series of updates needed to be done in a certain order, and there were MANY manual steps as some updates have breaking changes. And, DNN will sell you the update service, if you really want to. I'm not sure there's any real economy in not scratching and starting over, particularly if the site isn't terribly extensive.
Ive only built some prototype apps with it, but each of them have had pretty major query gen bugs, or massive perf bugs (sub queries for each rows for example). Maybe you got very lucky? 
Which issues?
It's called ef-scaffold and it works fine for me?
That might be the same as what I'm using currently. I'm just not big on the interface. Reverse Poco has way more templating options. Luckily it's just a like-to-have.
The MS documentation is really fantastic. I didn't find anything that wasn't well documented there. That's all up to speed with 2.0 and there's even documentation on what changed, so you can upgrade code examples from StackOverflow
I can't believe I've never heard of this tool.... it's pretty slick
I would not say that dont use ViewBag but yes all are for purpose. ViewBag ViewData can be use to pass small amount of info but for others u have modal.
Unless you can install software *on* the server and then expose a search over something like HTTP (since the expensive calculation could then be done on-server), this is probably the best you're going to get. I'm not sure what you're storing in the database (just a single column with the file path?) but you may be able to optimize further, e.g. copy the filename to a separate column and index that as well so you can search filenames with less data to scan through.
I'll add a "yes" to the pile. I have 4 AspNetCore apps in production now and haven't had any issues that weren't of my own doing. In fact, I have a 5th in the pipeline as well. It has been a joy to work with, to be honest.
Can you give an example of something on docs.microsoft.com that isn't updated for 2.0? Microsoft has done a good job of making sure most business cases are the current release.
Thanks for your insight. I found a crew who would get her back online, but she'd decided to pack it in. Oh well. But she is near retirement, so I guess she figured it's not worth her effort. Sigh.
Since .NET Core 2.0 is really more of truly version 3. Yes. As we know with Microsoft and development tech, they get it right on the third try. 
I was thinking more of the five to ten pages of Google search results filled with links from StackExchange and other such sites that you have to wade through before hitting anything even remotely MSDN related. MSDN comes up early if you have a highly precise question that hits the sweet spot for its ranking, but I have rarely ever seen it on the first or second page of a colloquially-written search for the solution to a problem. And IME that is how 99+% of quests for answers will proceed. The vast majority of resources out there are for Core pre-v2, and will most likely not specify that, either. Most will just be “a DotNet Core” answer with nothing to signify what version it is really meant for. And that is what will really screw up most people that are trying to diverge from cookie-cutter instructions.
So the issue for you is the search ranking of the docs, that is fair. But to be frank, you can't fault Microsoft for there being more blog posts or Stack questions referencing older versions of a framework. What occurs when you put 2.0 at the end of your search? Coming from someone who actually writes and updates articles on docs, I haven't had many scenarios that aren't updated. Just my two cents.
I never took Microsoft to task; it would be patently ridiculous to do so. Please don’t be putting words in my mouth. My issue is the copious amounts of documentation out there *that makes no mention of what version it is supposed to be for*, so even your suggestion of putting “2.0” at the end would include many pre-v2 articles simply because they don’t specify a version (thanks to Google’s habit of returning “near matches” when not many exact ones exist). I ran into this problem myself when dealing with MVC 5 — even though I clearly specified MVC 5, I often got reams of results that included MVC 2 &amp; 3 as well because their version was not specified on the page. Searches for solutions under Core v2 will be *that much worse off* for the next while *precisely because* of the twin issues of no version specified on the page and an excess of pre-v2 content.
Issuing actual SQL for the delete i.e. TRUNCATE TABLE ServerIndex is probably going to be faster than using RemoveRange() like you are. Could you not scan the folder once then setup a FileSystemWatcher to monitor for the incremental changes?
Wow thanks for that link! I don’t have extra cash and that 3 month pluralsight access is a godsend. Great post :)
SIGIL is likely a much better option for anyone. 
Run time "auto" mapping, reflection/dynamic programming 
Not as in affect it to make it not work right.
We've been using EF Core and I can't really say it's causing us any issues. There's some stuff missing that I really hope they get in there, I hate how they handle inheritance, but the overall changes are fantastic in comparison to regular EF. I haven't noticed any issues with bad SQL generation, but TBH none of our queries are that complicated.
*uses WordPress site to market ASP.NET application tutorial*
When I need to scan drives/directories/files quickly, I call methods inside kernel32.dll - particularly FindFirstFileW() and FindNextFile(). This is much faster than using System.IO, although not quite as easy to work with. It's not the best example, but look at the 3.5 second solution here: https://stackoverflow.com/questions/26321366/fastest-way-to-get-directory-data-in-net otherwise Google for other examples if you're interested.
https://developer.mozilla.org/en-US/docs/Web/HTML/Element/input/checkbox
I've tried that and my app blows up on me. I can upload a screenshot.
Define "blows up on you"?
Glad you like it ! Please spread the word (a tweet goes a long way: @codetrack4net) so others can enjoy it too :-) Happy profiling !
Kaboom!
I think this is kind of a unnecessary complexity for something that is already simple. If you want to take the ORM out of your ORM, why not use use raw ADO.NET to return a dataset/datatable. You can then just have JSON.NET serialize the datatable, or if you're using webAPI it will do it for you.
how tf would anyone do this sounds like some IoT stuff 
What are the reasons for this? Cost reduction? 
You make the unit tests and UI for the interface, not actually do something with a device :p
You really should add migrations to this, even if it's 'database first'. You should use that scaffold-dbcontext only to get your models initially created, and then use migrations to update your database from code after that. If you don't do this approach, you have to re-run the scaffolding tool every time you make a change to your database, which is a huge PITA.
Thanks Paul, glad that you like it
You didn't mention how do the files get moved? via request? or manually in the server? If via request, then you can modify the path in the db on that request. If manually, then why not making a custom file manager that helps the process of efficient indexing? Not an appropriate solution? then FileSystemWatcher might be a solution, but it might get expensive, so a C++ solution might be optimized to run as a service: https://github.com/simongeilfus/Watchdog https://github.com/emcrisostomo/fswatch Also, you didn't mention the purpose, exact matches or non-exact? if you use hashes or any data-structures that do, you will lose the ability to search for non-exact matches... what I mean is if you use a db, then you can add a spell corrector like this: https://github.com/wolfgarbe/symspell ========== Aside from C# solutions... you can ask Madhavan Lakshminarayanan who made Snowbird app (a very fast application to find files that I use personally, and it doesn't index anything). https://lmadhavan.com Don't forget to share the answer if you find something.
I don’t know ‘hook’ but Asp.Net middleware is something between webservers and controllers (mvc, api or other middleware). In middleware you can do whathever you want. You can manipulate the requests or return responses. For example you can create a middleware that log requests and response in a centralized way
I’ve been looking into FileSystemWatcher but I’m not sure about using that on such a large server, it’s totally new to me so it may be fine but I’d have to test it out. Indexing the files in a database I already know is fairly easy on the CPU. I’ve toyed with the idea of having people just use the intranet site (which is the search client) to upload files, but we have Office 365 so at that point I might as well make a push to switch to OneDrive instead of using an in-house file server. 
Could you explain that bit about HTTP a bit more? I have full control over everything here, but I’m a complete newcomer at web development (experienced programmer otherwise). I’m working on redoing our intranet so the search client here is just a Razor page I scaffolded up with Entity. The database indexing process I’m running now is on the same machine as the server. 
Yep. We've got one 2.0 app in production, and a few 1.x services. All on Azure as it happens. We've got some large services and a couple more apps being written in 2.0 at the moment all going fine. It's be nice if EF Core supported M-M and lazy loading to dust off the couple of annoyances I find but they're not deal breakers for us right now.
If it were me, I'd take a look into the options of O365 Sharepoint and/or OneDrive for business. Ultimately my decision would be based on how important it is to access these files through windows explorer. There's an option for onedrive which downloads placeholders for the files but doesn't actually fetch the contents until the user requests it. So if you're syncing a massive directory to their local PC, it's going to pull down references for the filenames so you can still search but it won't have to take up all that disk space. Of course if you just manage everything through the web interface for onedrive, nothing needs to get downloaded at all. Same for sharepoint - I'm pretty sure there's a way to map your sharepoint site to a network drive on the PC so you can access the files through Windows Explorer, but you could then run searches against the index which is automatically built in sharepoint. 
I see. Sounds like you were already doing a form of my HTTP suggestion and it was still too slow, so the database index is probably your best bet.
That's a good point, I guess I'm not understanding the scepticism with don't core in general. Seems like people just want to hold on to their old stack for comfort. Now that we're past the 1.0 RTM project.json fiasco I don't see any red flags.
There has to be something else you got billed for if it only logged 100 times, that cost report has to be very specific. Anyway, the other thing serverless functions are said to be good for is scaling, no matter how many concurrent users you get, they get the same latency, so you won't get that with a VPS. There was a link a day ago on reddit about someone benchmarking Azure functions vs AWS Lambda, and Lambda ends up being more consistent, so you might want to look into AWS. On AWS you also can add budget alerts to warn you when something goes above an expected consumption. 
If you dig into what you actually got charged for can you reply and let me know? We're looking into this as well as a cost saving mechanism.
I call bullshit. There's no way he paid 56 pounds for less than 100 calls.
Don't fall for the circlejerk. If you have a UI heavy application that would benefit from being an SPA, use Angular, React, etc. But if Razor fits your application just fine that that's great. Use it. 
They are completely different technologies so no, not really the same thing IMO. In WP you basically can use hooks to add to a list of function calls when a particular event happens. In dot net middleware are components registered into the pipeline, components designed to analyze, process, modify http requests and responses. 
That would be ideal, I can’t write anything that’s going to outperform Microsoft indexing on Windows, but it would be too radical a change for the people who are using this system. Windows explorer integration is a must, I mean even so much as changing the drive mappings would make people upset. So for now it looks like database indexing of the paths might be the way to go. I guess at least I feel better that my initial reaction to approaching this wasn’t totally wacky haha. 
Can someone explain to me the difference between using this approach to create the token as opposed to using openiddict or indentity server4? 
The tweaks to Razor and combining the new Pages approach with vanilla MVC can be pretty slick, but I still prefer pairing a WebAPI back end with Angular^TS as separate projects. PWA... I'm procrastinating as much as possible there hehe.
I'm using razor to serve pages built with vue.js - I needed the extensibility of .net core (my application uses dynamic plugins with their own custom views / routes) but the vue.js calls a service API for actual data (separate to the UI server).
The .net core app is only supplying routing and the page content itself, because plugins can include their own custom routes or page content. There was no easy way to dynamically load components into vue page after it's already compiled, so this way the base MVC app supplies the page template and then the contents can be filled in the middle depending on which route you visit, and then there's multiple vue components in each page depending on what the plugin contains. 
We have a lot of old developers who are conformable with MVC. We could move to SPA, but the older devs would become less productive. I think both solutions fit nice. SPA is basically just for bragging rights.
It's a winner! Just moving the page processing off of the server entirely makes a huge difference. If one uses Razor at all with a SPA, it's probably best to limit it in the extreme. It's completely unnecessary now. Oh, and Angular 5 is out now and it's a fairly painless upgrade. See their wizard to get your to-do's for that: https://angular-update-guide.firebaseapp.com/ 
Also wondering this.
SPAs are all the rage but increasing your reliance on JavaScript is a terrible pain. Not to mention that there are lots of situations where SPAs are totally out of the question; like any time your website needs to be crawlable by search engines. That said SPAs are cool as hell. But they aren't everything. And I'd bet money the project would get done faster with MVC/Razor Pages and be more scalable because of reduced JS.
Not disagreeing with you at all, but FYI - you can do server side rendering for SEO with angular 5, react, and Vue.
Less fat, more control. That might not be a good thing when dealing with auth though...
I still don't know why they wasted all that manpower rebuilding MVC. I hold out hope that they will junk all that crap and put some manpower into building a proper web development toolchain that shows the future of web assembly driven apps being built from C# and fully deployed into the browser as a single packaged bytecode assembly.
SPA may also be easier to hire for. You can find front end Angular / React guys on the market regardless of what back end they were working with and bring them on to your .NET team.
The benchmark was bunk. It was .NET 4.6 on Azure vs .NET Core on AWS. Of course AWS won.
It is not a case of JWT Middleware vs OpenIddict/IDS4, but rather JWT Middleware + OpenIddict/IDS4. You will need to have some sort of OpenID Connect authorization server such as OpenIddict or IDS4 (or a commercial offering like Auth0, Okta, etc) which will be used to authorize the users and issue an id_token + access_token. These tokens can be stored in local storage in the browser. Your Angular app will then need to pass the access_token in the Authorization header with requests being made to your ASP.NET Core API. Inside the ASP.NET Core API you can use JWT middleware to authorize the request by ensuring the access_token is valid. This is assuming that the access_token is a JWT, since OpenIddict, for example, can generate access tokens as either a JWT or it's own proprietary format - in which case you need to use their OAuth introspection middleware instead of the JWT middleware
You need to be careful about how you create the function app. Azure functions can be billed by invocation (i.e. a consumption plan), or alternatively, you can connect it to an App Service plan. In the latter case you effectively have a dedicated VM which is always-on, and therefore you will pay a much higher rate - in line with the costs you have mentioned. You can read more about this [over here](https://docs.microsoft.com/en-us/azure/azure-functions/functions-scale)
If you are not limited to Azure, then consider AWS Gateway + AWS Lambda. It can effectively host a full ASP.NET Core Web API as-is. See [this blog post](https://www.jerriepelser.com/blog/aspnet-core-aws-lambda-serverless-application/) for more info
Take a look at this: https://imgur.com/a/dEnFJ Thanks!
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/8fjxQre.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20dsgeknc) 
The consumption plan really is pennies. 
Server side rendering is always and will always be king. 
&gt; Don't fall for the circlejerk This. As a seasoned developer who got his start building web apps somewhere around 98, I find the modern web dev culture around frameworks and languages fascinating. 
It is a property of the model. I might just make it a dropdown box and call it a day. :)
How so? The system we're working on moving from ASP.NET is already loading pages 3x faster and we haven't even bothered with caching yet, whereas the existing implementation does.
No matter what your front end shows, your back end always has to verify a users authorization to edit something. Do you have separate actions for saving each field or a single action with a form view model? For users who aren't authorized to edit certain fields you should ignore those fields in your view model, or deny them access to the action for that field using the AuthorizeAttribute. On the front end you should still hide or disable the fields a user doesn't have access to, but the back end is what prevents them from using the inspector to change things they shouldn't be able to.
Razor is my weapon of choice. My favorite flavor of JS is vanilla.
Suppose you have products that go in categories and you don't want the user to have to reload every time they navigate deeper as they search for products. SEO might say you should set up your URL this way: /products/{category}/{manufacturerName}/{productID} If you used client-side routing, it might end up like this: /products/#{category}/{manufacturerName}/{productID} Then the SEO folks at your company might complain about that pound sign. That's not the bad part though. Some search engine bots don't load JavaScript and instead look for static links. If the links to all the individual products on the ` /products/#{category}/{manufacturerName}` page are loaded from a model client-side, the bots won't pick that up. Googlebot and Bingbot both seem to be able to handle at least some JavaScript, so you may still be pretty safe for most of the important SEO bots. I haven't tested it. https://stackoverflow.com/questions/1785083/how-do-web-crawlers-handle-javascript
Why would you have a # in your route when you don't have to? You can have Angular Universal or React/Vue with Next.js handle server side rendering with regular routes on an express server. Googlebot and Bingbot both now claim to read js. Are they 100% accurate? Who cares, you should be server side rendering your SEO pages anyways.
Neat, I didn't know you about the # thing in Angular5.
I'm actually using the actions created from adding scaffolding. Looking at the signature for the Create action I'm noticing the binding... public async Task&lt;IActionResult&gt; Create([Bind("Id,Title,Description,ReportCategoryType,IsUrgent,DateSubmitted,DateCompleted,Status,StatusDescription")] PulseRequest pulseRequest) Am I correct to assume that if I were to remove, say, StatusDescription from that signature that the action would fail if someone tried to send StatusDescription along for the ride?
I think you will get the best answer on how to handle this with IdentityServer4 by asking the question to their support. Generally speaking, however, there are a few ways: 1. You can store the permissions a user has in the scope of the access token. You can think of a scope as a permission, e.g. whether a user can read emails, in which case the scope may be "read:email" 2. You can add custom claims to the access token. So for example you may add roles claim which contains the roles of the token bearer. Be careful however that loading the token with all the possible scopes / roles can make it quite big. 3. Many systems will implement the role based-permission in the actual resource server (i.e. the API), as you mentioned the previous system have done. That is also a valid approach. But like I said, this may be best asked of the IDS4 people. Dominick and Brock are two very clever guys and should be able to give you better guidance. Same goes for Kevin who runs OpenIddict. He should also be able to assist better
Razor, with limited amounts of JavaScript only for the areas that actually do need it. And yes, we still do use jQuery as well.
Sr. Web Dev here... I’m a huge fan of razor. I build really great user experiences and use minimal JavaScript. If I have a view like a dashboard, I will include a js framework when it makes sense. But I only use it only that page, like a “mini SPA”. This has been rock solid, and razor is still updated and more and more features are added. I create a lot of useful, reusable components in razor/bootstrap. This allows me to put together things very quickly. Using a full SPA framework really doesn’t make sense if 95% of your views are forms and static content. 
Is not an issue but it is. Server side rendering uses node + .net so the server response will be slow. And for SEO speed is probably in the top 3 things you need. And if you have some kind of js error it will blow up very easily. Or if the page uses a setInterval it can freeze your server. It’s not without its problems. Also the html it renders is not exactly perfect... is better than nothing, but not better than a regular html page for SEO.
I’m not talking about the spa templates included in .net core- those are shit. I’m talking about stand-alone angular/React/projects hosted with node and express.
The `#` thing hasn't been a real issue for years now, considering the HTML5 History API has existed and been supported by all major browsers (IE was the last to implement it, in IE10) for about half a decade. The rest of your comment is covered by the other thing /u/itshotinjuly mentioned, serverside rendering. SSR means the data _is_ loaded and rendered serverside, and delivered to the client as static HTML, and then "hydrated" with JS on the client to be interactive. This improves loading speed (since you now don't have to wait for the JS to load to do anything else), but also SEO, for obvious reasons. 
&gt; hosted with node and express In /r/dotnet
Sounds like you are referring to overposting. Great article on it here: https://andrewlock.net/preventing-mass-assignment-or-over-posting-in-asp-net-core/ There are several approaches, simplest would be BindAttribute on the action method. Hope you are enjoying ASP.NET Core!
And? 2 separate projects. API is .net core. Front end is a js framework hosted with node and express...
If you host a .net core api doesn’t make sense to use express and host it differently because then you have to support two platforms without a real reason for it. And anyways, what difference it makes? Server side rendering with node is the same crap as with .net + node, slow and very prone to errors.
There’s no difference when you don’t know what you’re talking about...
I can attest to this I've moved to full time React contracting. It's going well.
Clearly I don’t. I just have been working with asp.net for the last 12 years and the last 2 years also with node and express. Also i currently work developing a nodejs crawler for SEO. I am sure you know a lot more about SEO, .NET and node than me. Enlight me!!!
Razor Pages for the initial bootstrap, passing some config from options to javascript (recaptcha site key etc.). Also debug/status pages. Rest is react app without server rendering (we don't need it) using json rpc for communication with server (fits our use case better than REST).
Probably because the web assembly apps can still benefit from the MVC pattern. It also means migrating existing MVC apps to .net core will be less painful and therefore earlier adoption will be more likely.
ASP.NET DevExpress controls (as well as WinForms, VCL, WPF, etc.) require license only for development. When a project is ready and deployed (or distributed to end-users), end-users do NOT need any additional DevExpress license. The only exception is the Report Server. It requires end-user licenses. See https://www.devexpress.com/Support/licensingfaq.xml for details. 
Try [DevExpress Reporting] (https://www.devexpress.com/Subscriptions/Reporting/demos.xml) tool. One of the best product on the market, an experienced support. A one-year license subscription does not require a renew if you do not need a newer versions of the tool. 
This is all well and fine, but after using Refit for the past few years I've got little reason to move away from it and it's fantastic level of support.
&gt; Fascinating More like infuriating ^^
Well that's the plan.... I'm interested to see what it turns out to be in real life. 
Was going to say the same thing.
You have the option with how you want routing to be generated with react-router v4. If you want hashes, use a HashRouter, etc. Not sure why anyone would want the hash there, but at least that decision is given to the developer.
And you can do that with SPA's now. Angular 5, react, and vue all support server-side rendering.
What are you doing in r/dotnet?! Get out imposter! haha
This doesn't necessarily have to be a ASP.NET solution right? Why not add a hidden class to the input tags and then use JavaScript to dictate when to allow those inputs for editing? &lt;input type="text" class="hidden" title="Admin Comments" value=""&gt; &lt;script&gt; // put JavaScript here that controls Admin Comments behavior..... &lt;/script&gt;
How dare you leave off webforms
`Mutex` is disposable, so you should dispose of it, however that will not actually release mutex if you own it, for that you need to call `ReleaseMutex`. This is how we use mutex to guard against multiple runs: https://gist.github.com/Kukkimonsuta/4d1e2e7ace5ad0c54876e4f449ea5312 
This is what I eventually figured out :) Also yes I am thoroughly enjoying it! I’ve been working with PHP for a solid 15 years and this is just such a pleasant experience. I really can’t wait to see what I can do later on.
It definitely needed to be an ASP.NET solution because if a person were to look through the code in the inspector they could send whatever they wanted through to the create/edit actions. It turns out it’s as simple as just removing fields from the binding in the function signature. That way even if I know the field name I can’t bind values to that property at all.
I migrated a .net core app from app service app to Linux free vm for a year.. should save €550 for the year.. azure functions seems like a trap...
&gt; You can't trust the client One of the golden rules, nice and concise. At my job I always explain it as "always assume the person using the page is a jerk". When that fails I point them towards some quarantined web apps one of their first PHP devs made back before 2010 whose database had been completely ravaged by bots.
You have a bazillion years of experience, but your responses say otherwise... so I dunno what to tell you.
What exactly? That server side rendering doesn’t work right at all? Maybe you are the one that don’t know what is talking about... as I said enlight me. Refute with a single argument your points. Just saying “you just don’t know” is easy as fuck and doesn’t prove any point at all. Have you tried to server side render a page where someone has forgot a setInterval? Does it render even close to the speed of asp.net? No Does it render 100% clean html? No. Does it make sense to host an express app + a .net core app exposing 2 points of failure with 2 different servers when the api project is already a website project in .net core? No. Double maintenance for no gain. As I said, reply with a real argument or stfu.
Look at option 3 : Use two different models in that link. 
What does your routing look like? I think your may need to add a parameter name to your route, or your routedata for that parameter is misspelled.
return RedirectToAction("Edit", new { ID= contacts.ContactNumber });
Too be fair, switching pages in a SPA will almost always be faster than server rendered pages. So maybe that's what they mean?
I created it via the Azure tools in Visual Studio which only allows you to create an app service one... guess that is how they conned me out my money, scumbags.
In my junior dev opinion, I would try to avoid using roles-based permissions. The principles would be mostly the same for evaluating claims, though.
For work? C# all the way. ASP.NET MVC and Web API are used for most new development, although often times the new applications have to integrate with older Web Forms or WCF applications. I think on .NET 4.5 or 4.6. Most of the JavaScript is still plain old jQuery, etc. We use TFS for version control 
I agree. It's not like you have the option to specilize in one as a professional since different projects/companies almost always requires more than 1 framwork.
We are using c# asp.net core 1.1 but targeting 4.61 for backend and angular 2/4/5 for front-end. Tfs for version control (git), ci (cake), cd. Jira for ticket management. Insurance enterprise apps. 
If you interviewed for the role, and were honest about your experience, then they think you can learn quickly enough. Obviously if you fibbed on your cv or in the interview then it's different. Most projects are much more backend c# than they are MVC. You'll likely start in the back and move forwards as you learn. 
This isn't helpful for you, really, but I don't really care one way or another. Our front-end guy is *really* great at that, and I'm great at back-end, and we rarely have to do stuff within the other's wheelhouse.
Nope.
Do you actually need the full type information, or can you just serialize the type's qualified name and compare that?
Oh I see. For that project I'm involved in both back end and front end.
For what it's worth, I really liked AngularJS with TypeScript. I know the ecosystem has changed a bit, but as far as I know, a lot of the logic is similar. You're just programming components more frequently.
Dot net core 2.0 for Web api backend with react frontend. Sql server cloud with dapper or azure cosmos for noSql needs. Vsts (visualstudio online) using git, Vsts build server releasing docker images to azure registry and hosting in kubernetes (linux) environment (azure acs, with planned migration to azure aks) 
Speed is almost solely an end result of fewest packets transfered. Aka latency. Apparent speed is lag time of processing any client side scripting plus latency. The fastest sites optimize both sides. Building a SPA doesn't inherently solve either of those. On the second part, it could make it worse since they ensure handwritten Javascript will be very frequently used. You can't say using a SPA is faster. 
Depends entirely on the client. Most .Net projects will be MVC on the .Net Framework Have one big site I work on that is mostly old mvc using the &lt;%% &gt; syntax instead of razor. I've made and aspx view with &lt;%:Html.Partial(Model.RazorView) %&gt; so that I can leverage all the masterpage driven layout and work with cshtml/razor instead. 
Ditto here, use VSTS for tasks and delegation as well.
`Type.GUID` is meant for COM, and probably won't be unique for many interfaces. It sort of works for classes without a defined COM GUID, but I wouldn't rely on it. I'm pretty sure it's just computed from the name anyway, and may or may not be stable.
Fair enough. However, diving in deep into your argument to try to invalidate the obvious still doesn't invalidate the obvious. :D
They haven't been fully released, but you can install them using dotnet new --install Microsoft.DotNet.Web.Spa.ProjectTemplates::2.0.0-preview1-final You will get the cli references, and fetch-data.component now has constructor(http: HttpClient, instead of Http :)
Were you pissed because you think the release schedule is too aggressive, i.e. too many major releases too quickly? If they don't horribly break backward compatibility with every major release, I'm fine with it. The TypeScript aspect and modular structure encouraged in Angular apps make those breaking changes and refactoring a much easier proposition than having to do the same with some of the older JavaScript libraries and without TypeScript IMO.
It seems you could use something akin to type.AssemblyQualifiedName + Type.GetType(name). Maybe this would be useful: https://github.com/dotnet/orleans/blob/master/src/Orleans.Core/Utils/RuntimeTypeNameFormatter.cs - it gives you something like AssemblyQualifiedName but without the assembly version included
I do think it's too aggressive and if everything was perfectly backwards compatible it wouldn't be too big a problem. But following the team I don't know if I can believe I won't encounter issues. Plus being back end focused I prefer very stable releases. Jumping whole versions like that supposedly signifies huge change. I'd rather stick to angular 4 then 4.1 ect. Ect. Also it will be annoying if they keep adding huge new features every six month. It just doesn't sound stable
Azure, .NET 4 with a mix of MVC and Web APIs, SQL, Vue JS (gulp, typescript), SASS, Sitecore (pushing for headless, but I doubt we get it).
Currently on my project I'm using C# ASP.NET MVC, with razor pages. Nothing too fancy. Planning to try out Angular for the next project if possible. But my boss is working on some legacy systems that's on using VB.NET with web forms. As for a VC.. It's basically nonexistent until I used it for my project although I had to setup a remote repo locally cause of restrictions.. Still trying to convince the people I'm working with of it's importance and usefulness, and that if I can get a server I can setup something simple.. Still not getting through but I'm having hope 😅
C#/F# .NET 4.7 but planning on moving to core once a few depencies catch up. A lot of newer services are in Go, and a small amount of Node. React for new FE work, slowly migrating from jQuery. Github for VC. All running on AWS. 
Azure, AspNetCore 2, Angular, SQL Server, Dapper. VS 2017 for all backend code and VS Code for all client-side code. GitHub for VCS and YouTrack for issue tracking. Edit: Also Serilog and Seq.
&gt; Too be fair, switching pages in a SPA will almost always be faster than server rendered pages. That's definitely true. I don't know what he's on about: SPAs definitely are faster for the user as long as they're not using a potato to run the browser. The below business about "fewest packets transferred" is true, but I think they forget that not using a SPA means a page reload, which at the very least is going to result in a bunch of 304's + whatever data changed. With a SPA, you only need to get the data that changed. It's not really a contest if you're doing things smart. 
I also think this approach - normal web pages with sprinkles of JavaScript to add interactivity where needed - can work in most cases. I am a bit sad that the Ajax helpers went the way of the dodo in ASP.NET Core MVC. Those were really great to do this.
.net core 2, service Stack plus ormlite, and vue on the front end. 
The numbering system is arbitrary, it has been a long standing debate in engineering for a long time. In mechanical engineering smart part numbers have declined in popularity. What's the value of smart version numbers when you are just looking at change logs and release notes anyway?
Visual Studio Code Angular 5 and Typescript Node.js for development MuleSoft for production Subversion for source control Previous project was plain old javascript, css, and html with bootstrap and jQuery. It was a very small project though.
How has the overall backend development gone with netcoreapp2.0 in VS? I’ve been having troubles with intellisense, debugger attaching, unit test running, etc. Just kind of buggy it seems. VS Code seems a little better 
C# MVC5, Razor views, little bit of jQuery, SQL Server backend. All self-hosted in a datacenter. Visual SVN server for a repo and versioning. I'd love to push into Core and Angular, but so much of our application is touching older, external components that it's really not worth the effort currently to change that right away. Such is the life of the solo dev: "make it work" trumps "make it work *better*"
`Guid.NewGuid().ToString("N");`
C#, MVC/WebAPI, still running .NET 4.6.2 but considering .NET Core, front end is Vue on the latest project (we looked at Angular 2 and React) older stuff was Bootstrap, storage is Azure SQL with ElasticSearch for better search / performance in places. Version control is GitHub combined with ZenHub for Kanban boards and better issue pipelines. (Based on anecdotes, we're about 3-4 years ahead of others in the same city. Makes it difficult to find new hires.)
Sometimes the link is the access. e.g. imgur, youtube unlisted video, password reset link, etc.
How hard is branching for releases? Just curious how u guys do it if there is hot fix etc? 
ASP.Net Core 2.0, EF Core and SQL Server. It’s a new project and I am still feeling my way a bit around between traditional MVC and Razor Pages. I think I’ll end up with a mixture to be honest. Right now it’s MVC since that’s what I’m most comfortable with though and am a solo dev. 
That's probably several months away from being considered. We started our current project in earnest last week. Considering we have no senior devs (not by choice), I'd be all ears if you could give me a primer on that tbh. We're deploying to Azure, so that's about all I have for you right now haha.
I have to agree. The number system is annoying, and I would prefer whole-number jumps to signify a qualitative shift rather than incremental improvement in kind. If we judge by that, the latest version is 2.3. The big shift happened between 1 &amp; 2, and as far as I can tell, the rest are all about the same.
EF vs. Dapper: opinions? I saw a benchmark recently that EF outperformed it now, but I don't know if that holds up for typical implementations.
True. But imagine working as a consultant, getting in touch with different technologies with each different project you're involved in. That's not fun nor efficient in my opinion.
I'm actually a little surprised to see so many comments saying they're using ASP.NET Core 2, I honestly thought it was too new still, but I'm also very very early in my career so I'm still learning a ton. I work at a very small place, one of four developers and I'm the only one working on building a website from the ground up. I'm using VS2017 for the IDE, VSTS for project management (barely), and TFS for the source control though I wish it was Git at times. I do like how TFS manages branches though, easier to move between two versions of code in my opinion. Im using C#, MVC 5, Razor pages, plain old JavaScript and jQuery so far. Azure and Azure databases too. We JUST started using cloud services. I would really like to learn Angular and Core since it seems to be the way to go right now, I feel a little dated already. Our older code runs on VB.NET and some even older tech that desperately needs to be updated.
I agree, they have some sort of weird issue where they were jumping whole numbers because internal dependencies were on farther versions or something. I don't know why it matters because the overall framework versioning is what's important.
C# 7.2 ASP.NET Core 2.0 PostgreSQL 10 with pgAdmin 3 Angular 5.2, React 16.2, MVC Core VS2017, VS Code Docker CE --- Public Website: MVC WebApp after Login: Angular or React
Strings are a terrible idea for identifiers and keys, so use something that's not predictable can be auto-generated, but can easily be parsed to another, indexable, not case or culture sensitive data type.....guid is ideal As with most things posted in this sub, there's already a built-in solution to this problem, no new project dependency required. 
For most of our services: .NET Core 2.0 / .NET 4.7 MVC jQuery Redis Dapper SQL Server For our customer support app: .NET Core 2.0 React / Redux (consumes our other services' APIs) 
I hear ya, I've only just recently started delving into front-end frameworks. I like React so far, and I've heard good things about Vue (namely that it's easy to pick-up and get something meaningful coded in a short amount of time).
Just because it's a new major release doesn't mean it's actually that big of a difference. The point of semver is that a new major release means breaking changes, but doesn't necessarily have to fit some subjective notion of a "big" release. Just because they release two major versions a year doesn't mean they're making breaking changes more aggressively than any other framework. I can sympathize with the fact that this is a bit confusing and not immediately intuitive (for non-programmers especially), but how often are you talking about your tech stack with non-technical staff, anyway?
.NET Core 2 (C#) with Ember
Have you looked at using linq expressions to build lambdas on the fly? 
Nice work. However, I'd probably recommend going with numeric IDs and then using something like [Hashids](http://hashids.org/) to generate the identifiers that go in your URLs.
So validate two pieces of info. Anything that needs to be secure needs validation. Security by obfuscation is bad news. For password reset for example I send out an email with l link that contains a guid. The link expires in 24 hours. When they click the link they're asked for their email. If the customer doesn't want that, the link becomes whatever?id=&lt;guid&gt;&amp;email=&lt;email&gt; On highly secure systems I'll do it just like banks do and ask questions. 
GUIDs are long though. If you need to generate a unique reference that may have to be transcribed over the phone like an order reference or account reference and you want to hide low volume something like this could be useful. You don't have to use them as the PK in the database you just need to be able to look up the correct record when needed.
Honestly, I mainly love the simplicity of Dapper. The apps I'm writing generally don't suffer the types of performance hits that a new ORM or data access layer would fix anyway (it is usually bad schema design or query writing on my part). I am also not a fan of the "generated" database designs that EF promotes, but that's just me and I know I'll probably need to give in at some point. Haha.
+1 for linq. If you haven't learned it yet you should. It makes dealing with collections much easier. 
I mean it sounds like you agree that they are using it incorrectly then.
So far: Python, Flask, Vue, SQL Server, Alchemy, GitHub, Sentry, Trello... FTW
Care to expand the reply bit? I've been looking at Python and Flask more recently and it offers exactly what I'm asking from .NET Core/WebAPI, a barebones approach.
There little dotnet core in the wild, AFAIK most people are still using MVC and Web API on the full framework. While they are similar, there are also a number of important differences between the two
I completely agree that linq is amazing. But I was referring specifically to expressions. They allow you to manually build lambdas. Just wanted to be clear. More info [here](https://docs.microsoft.com/en-us/dotnet/csharp/programming-guide/concepts/expression-trees/)
So what is YouTube doing wrong with their unlisted video function? What should they be asking everyone who visits an unlisted video? Look, I get your point. For things that need a higher level of security, relying on a random key alone is insufficient. But at the same time, not everything is high security, multi-factor required. Sometimes a random value like this is exactly what's called for. Pooping on the library because it doesn't 100% solve your high security scenario (even though it could provide a portion of the solution) isn't fair.
ah! ok, I found something http://www.daniellewis.me.uk/2014/02/21/asp-net-web-api-from-scratch-part-1/
Oh sure I have my own code to do this stuff and I understand what you mean about everything as a package. but worth pointing out that there are legitimate use cases for randomised strings. We might both know that but someone reading this may not :)
You could try wrapping the type around a string which is serialized. The getter and setter for the string would be getting the type's fully qualified name and for the set, finding it in the assembly. The type would then have [jsonignore] or however you do it in the framework you are using.
I just think this solves a problem that doesn't exist. If you're working on an app that doesn't require high security just use a guid. The only reason why I said anything to begin with is this gives the illusion of security which can be dangerous if someone pops this in their app thinking its a secure solution. 
It's not so drastically different that an employer is going to say, "Ooh, you have lots of experience, but it's in .NET *Core*... I don't know if we can have that. Better luck next time, kid." If they *did* say that, I'd guess you dodged a bullet from an employer who thinks you have to know the exact framework they're using to be of any use.
In Houston, I typically see everything advertised as wanting a full-stack developer. Very few jobs are strictly one side or the other. When you actually get to the job, though, it's a different story. Your team may stratify into people working on what they're best at. It's what happened with me, at least.
I'm having a hard time envisioning the context. Is there something preventing `var test = typeof(String) == "YourObject".GetType();` from working?
You mean "lesser known .NET features", though. Nothing to do with C# as such.
That would be quite difficult for sure.
 Ya, I believe it's a carry-on from the performance of EF core 1, and then just people echo'ing at this point, as some members of this sub absolutely hate it unfortunately. I doubt most people that say "super un-optimized queries" has even used it, and those that have, are what you said, didn't write it correctly and expected EF to magically fix it.
I love lambdas on the fly, but OP also be wary of this. Take a moment and decide who is responsible for the sorting and filtering of your data: the endpoint or the consuming code? It's a blurry path to make that decision; usually it's made on a case-by-case basis because no two endpoints are exactly alike. Things like data size, scalability, and who has access to the endpoint all factor in making this decision.
~~~I don't know much about MVC, but it looks like you're trying to assign a non-true/false value to the checkbox. If you didn't assign any value, perhaps it doesn't like `null` as a value.~~~ I think you tried to assign it a nullable boolean value. Make it non-nullable.
I remember seeing a basic one for a dotnet core web api on the Microsoft website. I’ve not followed it through but a glance it looks like what you might be after (or at least part of it). Link: https://docs.microsoft.com/en-us/aspnet/core/tutorials/web-api-vsc
It's not even like .net core is that much different. It's basically the same concepts with cleaned up startup and middleware. Controllers, views, services, repositories, etc is all pretty much the same outside of the new razor syntax (which you can still use the old). The only thing that was a little confusingly different for me was how to set up many-to-many relationships with eager loading in ef core because lazy-loaded virtual properties aren't a thing *yet*.
Yeah, it’s pretty great... until you need to debug. 
 A delegate is simply a function pointer. You set up a delegate to point to function A. You call delegate, function A gets called. An EventHandler can be thought of as a list of delegates. When you call the event handler, all delegates added to it will be fired. For example: Print print = new Print(PrintToConsole); print(); // Will print to console only PrintToStuff += PrintToConsole; PrintToStuff += PrintToLog; PrintToStuff(); // will print to both console, and log public delegate void Print(); public event Print PrintToStuff; static void PrintToConsole() { Console.WriteLine("Printed to Console!"); } static void PrintToLog() { Log.Log("Printed to Log!"); } A command (in terms of wpf) is syntax sugar for an event. Mostly used in MVVM, instead of binding the click event of say a button, you can bind to the Command of it. This allows you to bind the logic into a ViewModel, instead of directly onto the view. On top of commands, you can also bind to things such as CanExecute, to determine if the property should be readonly, or Executed to change output. It just creates a nice way of writing clean code.
How to implement this safely without controllers using iqueryable?
A delegate defines a signature/a type which can be instantiated, where in any function having the matching signature can be added to the delegate by either passing the function to the delegate constructor or using += operator. When a delegate is invoked all the functions attached to it are executed. An event is simply an instance of a delegate. So if you want to define an event there should be a delegate defined first. When an event ( like a button click event or a menu item selection event) is raised the corresponding delegate is called which in turn executes all the functions added to it. A command roughly is also an event where a piece of code is executed when an event is raised but with additional functionality (CanExecute and Execute methods). The main purpose of commands in wpf is that you can bind a single piece of code to different actions. For example file save can be done using Ctrl + S or from Save option in File menu. Both will execute the same piece of code. What I have described above is a high-level idea. Delegates and events in C # are involved concepts. So I suggest you spend more time understanding those. I cannot think of a UI application which does not use events and delegates. 
I did the same here and it ended up being just a couple of lines. I used the `app.UseOpenIdConnect(...)` middleware. Worked great for my primary AD tenant. Unfortunately it doesn't seem like I can have multiple AD providers like I could in the full framework. Which AD method are you using?
They had one segment of iffyness. They have had 3 other releases that have been fine. I don't think one time should make someone completely untrusted.
It's still really recent. And I didn't say completely untrusted. But I don't like it. And this decision showcases the same mentality that led to the one segment of iffyness. Obviously only time will tell but yes I really don't like their versioning system. It is counterintuitive and misleading.
Definitely depends on what you're talking about. If you're talking Windows apps vs. web apps, I'm sure the frameworks are quite different. If you're talking web app development on Core vs. the older frameworks, you're probably doing something something similar.
I've been rolling with ASP.NET Core 2.0 and, at least for the purposes of my projects, making do with Bootstrap/jquery and ordinary JS for front end stuff. VS Team Services for version control and it couldn't be more painless. However at some point sooner or later I will need a front-end JS framework. How do I choose between Angular/React/Vue or whatever else? If it at all helps, I don't particularly enjoy coding in JS. Can anyone recommend one of the three for someone not particularly into JS? Does that even matter? I dunno. I have been considering Angular with Typescript currently
I did this last project with linq dynamic library which made that pretty easy but it's just getting it into a nice flexible re-usable design that is going to be a bit of work and I figured this is the sort of thing that someone else must have done already I am just having trouble finding it. If I cant find it then I will of course just keep working on my own code but I reckon there must be something out there 
It becomes a pain in the ass to google "Angular 12 how to do X" and you see a ton of results for Angular 3-15 and you have no idea if everything still works the same or not
archive.org seems to have older versions, not sure what exactly is in this zip https://web.archive.org/web/20150213211038/http://referencesource.microsoft.com/DotNetReferenceSource.zip
ConditionalAttribute is very much a C# feature. Other .NET compilers do not necessarily honor it. The same goes for the CallerMemberNameAttribute and friends.
I have always felt the term ".net standard library" is misleading. There is no ".net standard library". You cannot find a link on the internet that allows you to download some code called "the .net standard library". The .net standard is a document that describes how some code is to be written. There are several libraries that adhere to this standard. A popular one is the .net framework. Another one is .net core. These libraries are not the standard, and the standard is not a library. So your diagram you should just read ".net standard".
I'm guessing ILSpy could help out here 🤞
Is this library any different than Task.WhenAll()?
Yes. It uses `Task.WhenAll` under the hood, but while `Task.WhenAll` is useful for working with `Task []` and `Task &lt;&gt;[]`, this library lets you await one `Task &lt;int&gt;` and one `Task &lt;string&gt;` (up to 16 different types) at the same time without temporary variables. It uses `ValueTuple`s to return multiple values.
Can't you just decompile the libraries using ILSpy or DotPeek? I had to do this before for the System.Drawing.
Couldn't you just find a nuget package/library that sorts in a similar manner? 
What a nice bite-sized article. Thanks!
Does this methodology apply to non CORE MVC/WebApi? 
dotPeek works better imho
Thanks for the help everyone! Turns out the sorting code I was looking for is still included in the latest source. They all seem to use mscorlib which has maintained backwards compatibility
This library just wraps `Task.WhenAll` with the additional gimmick of destructuring to a tuple. It basically means you don't have to `await` a second time (or access `.Result`) on your tasks to get the result. 
I can't really see the reason to have this on the .net reddit.
Asp.net is part of the .net framework yes :) Jquery and Javascript is not. Would fit more on Coding, Javascript, WebDev subreddits. You didn't include 1 line of .net code in your code samples. Why didn't you write this in C# if you wanted to post it on Dotnet subreddit? The time you used to make this post, makes people think it's spam. To be honest my first reply was made a bit to quick and should have given more feedback where the post belongs and why not on the dotnet subreddit :) 
Thanks, I'm starting to understand. I will continue to learn
Thank you very much, your example is quiet understable.
Always keep in mind that all the frameworks, patterns you find in .Net or wpf heavily emphasize code testability and maintainability. 
It depends. I use ILSpy, dotPeek and Telerik's JustDecompile interchangeably, depending on which one produces tidier output, becuase they all produce slightly different results depending on the code being decompiled.
Even if this weren't video spam, this isn't the right sub. This is /r/dotnet, not just /r/csharp.
Find something that currently annoys you, and try and fix it! For example for me I may start working on a more power user friendly steam library manager, as that is something that always slows me down!
project.json is dead
&gt;The Secret Manager tool does not encrypt the stored secrets and should not be treated as a trusted store. It is for development purposes only. The keys and values are stored in a JSON configuration file in the user profile directory. https://docs.microsoft.com/en-us/aspnet/core/security/app-secrets?tabs=visual-studio https://docs.microsoft.com/en-us/aspnet/core/security/key-vault-configuration?tabs=aspnetcore2x
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/freelance] [I got my first freelance web dev job and I'm very excited! I need help estimating hours!](https://www.reddit.com/r/freelance/comments/7pzm85/i_got_my_first_freelance_web_dev_job_and_im_very/) - [/r/webdev] [I got my first freelance job and I'm very excited! But I need help estimating hours.](https://www.reddit.com/r/webdev/comments/7pzfhz/i_got_my_first_freelance_job_and_im_very_excited/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
404, please delete when you repost to fix a link
Depends how well you know all of these things, docker is fairly easy to learn, but oauth2/openidconnect can be a bit of a pain initially. Do you know what kind of clients they will want connected? If you're going to have to have all kinds of grants set up to work correctly, it's going to take more time than if you need a simple login screen with just implicit. Most of those things are plug and play and could be set up in a day, but oauth2 modules should be thought through before just creating it.
As a general rule - Estimate how long it will take you to implement the functionality then times that by 3. Coding a solution is the easy part, refining the code, adding tests and on boarding feedback can take up a lot more time than you would think.
I would say at least 2 weeks to cover yourself against possible bugs, and also to allocate enough time for proper design and testing.
I would go for 2 weeks as well.
I've been doing this for 20 years and I still suck at estimating.
Congratulations! In general, estimating is hard, and estimating accurately at the beginning of a project is even harder. Trying to give an accurate estimate at the beginning of a project is pretty much like playing darts and just hoping. However, you can generally do an okay job by estimating a large number of small tasks and summing them up. Generally speaking, the sum of the estimates will be more accurate than one over-arching estimate. So, take each of these tasks and try to break them into as many small, and manageable chunks as you can. Then put an estimate on each of the tasks. If the task is over say 4-8 hours, you are going to want to explore it in a little more depth to try and break it down farther because there's probably something you're overlooking and you're glossing over some technical details that probably should be explored more. You want to, generally, be able to make tangible progress each day and not carry over a partially complete task, IMO. Once you have all of your estimates, you can then apply something like an "uncertainty" amount to each part. For example, perhaps coding up the user identity is really simple since you can use the built-in EF Identity and its authentication management. So you estimate 2 hours to scaffold out the classes you need and the end point(s) you'll need. Then, you're fairly confident, so you might say that you're 90% confident you can succeed on that estimate. Your final number would then be 2 * 1.1 to help represent the 10% uncertainty which brings you up to 2.2. From here, you can round up to whatever time-slice you bill in (2.25, 2.5, or 3 would be common ones). Continue like this for each task that you need to do, breaking down large tasks into smaller chunks to get more accurate estimates. Apply your uncertainty to them to represent how confident with the estimate you are, and then sum all of these individual estimates up. It's more work up front, but the time and effort is worth it because you will have a better understanding of your problem domain and a better understanding of what areas you're truly not sure about. Furthermore, it's far, far easier to fix a mistake in your architecture at this stage of the game than after code is already in the editor. Try and find the thorns now rather than later. Good luck with your first job, and just do your best! Don't over-promise, and fight feature creep as much as you can! Always be diligent in letting the client know the time trade offs involved when they, inevitably, ask for something beyond the initial scope. It's very natural to just agree with whatever the client wants because it seems easy and would just be another couple of hours, but unfortunately that slips your deadline and usually you, as the worker, are the one in trouble and not the client who kept asking for more features. :)
Estimate high, look like a rock star when you come in under. 
Despite what you're being told, nobody can estimate the job except for you. It takes a long time to learn to estimate coding work with any sense of accuracy. I'm really good at estimating for myself, am fairly good at estimating for members of my team as well but it took about 10 years to get good at it (been doing this for 25). Here's my general method: * Break the task down into sub-tasks. Web pages. API implementations. Don't forget your framework setup! Don't forget distribution and "finishing" work. * Order-of-magnitude estimate each sub-task, round up to the next: Hour, Day, Week, or Month. * If you came up with Month, your sub-task is too big. Split it up. :) * Have any unknowns? ("I've never used framework x!") Look around, you probably do. That's okay. Double estimates around these. * Have any unknown unknowns? ("How could I know the client wouldn't allow C++ Runtime?!? The only place I can call this is WinAPI!") These are the scary ones, and will blow your schedule to hell. Can't plan for these. Good luck. Now you've got your pile of work. Start working. ... but wait there's more. The important bit. Count your time while you do this task. *BE HONEST*. If you had to work nights and weekends, count that. You don't have to bill the client for it! Seriously. Don't leave off time because you were somewhat distracted. You'll always be distracted at times. Count that time too. Count the time in the zone. Count the time you spent trying to figure out why the debugger won't attach and had to google for 2 hours. Programmers with piss-poor introspection and dishonesty have the most fucked up estimation skills. I promise, if you're honest about it you'll be really good at time estimation for the rest of your programming career.
This sounds good, and can be, but that's basically what I have done all my career because I'm afraid of disappointing and the result is that people expect me to be done faster and get frustrated when the estimate turns out to be accurate.
Thank you for the thoughtful reply. I will keep these things in mind. When you say count the time honestly, but don't bill the client for it. Do you mean to count the extra time for myself so I know better next time? If I estimate 40 spend 80 because. Next time I say 80?(minus now having experience with the same thing) Or are you saying next time I say 40 hours, but say it will take two weeks(80hrs) while only billing 40? I have wondered what is considered billing time and what is not. Like having to read hours of documentation and then getting stuck on something small right afterward for 6 hours. Is it supposed to be billed? I feel terrible billing someone for my stupidity.
Would be fun to run .net javascript engine inside mono webassembly.
I'm a c# programmer and I was able to complete a project in 48 hours using xml as the database. I devised my own means of authentication and authorization using servers. If you have any free work or project. We can connect to work togethere. Thanks!
I've decided that its kindof a dumb thing and impossible. If you build things custom all the time for people, that's like saying to estimate how long it will take to cook a different food st a different temperature everytime with the same stove. You'll get close because you have the same stove, and the same cookware, and you know some similar properties of different foods so you can eventually get a little closer. But there will still be times where your wayyyy off. 
With amazon lambda, or azure logic apps! 
Try Orleans/akka.net/hangfire.io
Thank you, that is a very methodical approach! I will give it a try!
Was going to ask how this lib was different from hangfire which is pretty nice. 
Just await on the tasks when you need to use their return value. and don't bother with the temp variables.
Yes it is. I love hangfire. It offers a good selection of backends and includes a dashboard so you can tell what's going on.
Same problem space but this is a purely open source project. For example the redis backend doesn't cost money :)
A few differences. Firstly, working open source redis backend (which is the reason I made it). This library is simpler. You can grok the code in less than an hour. Take that as you will. I like my dependencies to be focused.
You are going to really like what I am releasing very soon. ;)
Very curious, why convert that way?
This is what has kept me out of web development for years. I’m so tired of needing to download 15 different tools, transpilers, package managers, libraries, and node modules (which will be considered obsolete in a year) just to feel like I’m keeping up with the cool kids. I feel so old, but I miss when you could just pop in a CD, install Visual Studio, and bam, you had everything you needed to get real work done. 
It’s because the dope created an App Service Plan instead of the consumption plan and wasn’t paying attention, so he’s blaming Microsoft. 
You can still do this. I use VS as my primary editor and I mostly ignore all the frameworks. Give it a try...VS Community is free and quite an awesome tool.
I do use VS extensively, I just meant I’m sick of trying to play catch up with the latest stack of the week. I might revisit razor for some things I’ve been wanting to do lately. 
Core itself still isn't a mature enough platform for what I'd deem to be mission critical web applications, add pages on top and it quickly becomes cumbersome for anything a little more complex than trivial. Not a fan though Ymmv
I just redid a simple marketing site with them. I think they are great for simple pages but I would do anything complicated with them.
Why VueJs ?
If its simple mvc model binding and some css, absolutely. Once I start typing too much jQuery I question my front end....
Are you using as SPA or only to run JS in a page?
Are you using bundling on the back end? I've used Razor to serve up my SPA through .Net Framework and the Razor engine to do other things, but to date Core's functionality has been a little sparse as compared to ye old System.Web.Optimization.
lol I was going to say we are doing the opposite. 
We are moving to Vue because our global components use them and they are more plug and play than Angular and React. Some of our applications use Angular, some still use old in house framework, some React and some Vue. Vue worked best to share components which can be used by any framework. My team's application uses Vue after talking to other teams (Angular [not 2+], React and Vue). The only downside we see is the lack of testing support but have talked to someone very involved in the Vue community and they are going to work with us on it. It will also be the easiest to refactor over time instead of all at once. I'm sure people will disagree, but for our very large, very active site with multiple teams working on it, it is the best for us. 
We use them currently, but are moving to Vue. They are very capable of doing complicated stuff, but it is better to use a framework, especially if it is a SPAA. 
I'm using tag helpers using templates from cshtml files.
After using vuejs for a few months Id recommend it over react and angular in most scenarios. The components are pluggable, the code is clean, its easy for large teams to maintain. 
I'm overseeing the conversion of a crappy Knockout website into Razor. Knockout is great, buts it's completely overkill for our use case. We don't even have an SPA. It's just a regular website with lots of page reloads and some Ajax sprinkled around for search results. We're already seeing snappier perceived response times because of the fewer client side operations.
.NET core SignalR library no longer uses JQuery. You may want to explore. https://github.com/aspnet/SignalR
*Oh the horror*. It's not "hip". Here we go again...
Agreed. Further, I wouldn't call this a "non-short circuited Boolean evaluation" because it's not. It's a bit-wise operation, and therefore not exclusively used for Boolean evaluations. While it is convenient that it can work as a Boolean operation, I wouldn't advise using it as such because to most developers it's going to look like a typo
Will do. The client has all their legacy systems in WebForms and new systems in MVC 5, so if I was going to Core I'd have to see how well I can set that up on IIS.
Such is life of a web development. Reinventing wheels annually.
I think they just merged in netstandard 1.6 (core 2.0) support last week
"var" keyword is much better in this case, more readable, and more to the point... my opinion :)
A visual guide and not an ounce of code?
Oh wow, I apologise! I've never noticed that it was a logical AND bit-wise operator. I thought it was only bit-wise and when operating on Boolean it just happened to return a Boolean value after bit-wise evaluation. With that in mind then, I would argue then that because it's a seldom used operator it would be best to avoid it because a developer may think it's a typo and change it without realizing they may be removing a necessary side-effect. But thank you for sharing it, regardless, because now I know more about the language I primarily work with!
Neato! I'm sure this will be loads of fun to use.
Full SPA from scratch. Asp net core Webapi for backend and Vuejs for front end. 
Do you use any library to JWT auth? I'm trying implement Google SSO login and JWT, but the request with authentication header doesn't work, the identity is null.
The claimType is just a string. You can make your own claim. That said, I see a System.Security.ClaimTypes.Role. 
[This is a good start](https://jonhilton.net/2017/10/11/secure-your-asp.net-core-2.0-api-part-1---issuing-a-jwt/)
Check the jwt tokens for your claims, if it's not in there then identity server isn't sending them. Also, depending on your setup, id tokens will have different claims than resource tokens.
Looks great - looking forward to the next part!
Is this a joke comment? You can't be insulting someone on grammar and write your comment that poorly.
Also, you have the added benefit of a weekend in case you managed to screw the pooch on estimating
Mystake
Please stop spamming your YouTube channel.
Thank you for your feedback.
Even debugging dynamically created Lambdas is great (Using the Expression.X-Syntax). You can follow the whole route of whats happening and even see tiny pieces form a whole. (Mouseover-shows the lambda Syntax )
I'd say - return IEnumerable from Service-Layer. `public IEnumerable&lt;Data&gt; GetAll() =&gt; db.Data.AsEnumerable();` Then you can just return that. If you're using AspNetCore you can do fancy stuff like: `Response.OnComplete(data =&gt; (data as DbContext).Dispose(), db);`
Its probably a false positive for what are called googeldorks. For example: https://www.exploit-db.com/ghdb/4558/ if you were to search google for the following 'inurl:/_layouts/mobile/view.aspx?List=' It would bring back a list of _vulnerable_ websites. Basically google thinks you're a bot looking for low hanging website "hacks"
[removed]
If you'd like actual code behind implementing a RESTful api then check out this fantastic Pluralsight course. https://www.pluralsight.com/courses/asp-dot-net-core-restful-api-building 
Gridview? Web Forms? Uhhh… sure. I’ll be right over, just as soon as a I crank up my Model T Ford.
Decent write up, but I did find it lacking in some areas (for both Angular and dotnet side of things). I noticed it wasn't using code-first entity framework * Shudders *. Switching to code-first would save some time and improve quality-of-life. Also, not sure why the id for the employee is included in the api endpoint`PutEmployee()`, the id can be inferred from the body of the request, which is a Employee object. Therefore the `id != employee.EmployeeID` seems redundant, and doesn't offer much. Response markup of the API is returning XML, I'd suggest changing that to JSON, personally. This can be configured during API startup. +1 for using Angular CLI. -1 for using Angular 5 but writing your services with the old httpclient instead of using the new HttpClient in Angular/common/http which provides Type Inference against your type-script models, automatically. Changing to this httpclient would clean up all the ` .map((data : Response)` function calls in the services, and put all those 204 http status responses to good use, too. Just my 2 cents, good write up still. 
Do you think angular and SPA in general fits better with web api or with mvc pattern?
thanks a lot for taking your valuable time to review my article, I'll consider your suggestions for upcoming articles. 'id != employee.EmployeeID' in PutEmployee() was a typo.
That's basically why we chose Vue as well. We're doing a conversion of a very large and complicated code base into something more modern. The old app was monolithic, the new app is more modular with separate APIs for separate concerns.
Lmao, I laughed way too hard at this. But its pretty much true. I understand that there are still WebForms, WinForms, WFP, Silverlight, Windows Mobile, etc. Codebases and systems out there. But man does this sub does not need or care about those topics. And that goes for the other 90% of Use .net Core 2.0 with JWT, EF, Web API, SPA, with Agile, and deploying to Amazon/Azure. Articles too
Yup, because I can use Visual Studio, TypeScript, and built in minifiers for bundles all without having to use another NPM or Node tool and about five or six less other build tools and 1 less setup process. All while having the same gains, performance, and usage with about .1% of the headache 
I'd your really haxor, you'll just read straight from the datastream into the response stream and get a ton of performance and maybe non of the sanity. 
You're not wrong, Walter, you're just an asshole.
This is great! I was wanting to deploy one of my Core 2.0 libraries as a service, via Lambda.
Btw, I used to think the same way. Unsurprisingly its turtles all the way down. If you think its worth the time and effort into learning it from scratch, you eventually wind up looking down the barrel of C++
Mvc, razor, ADO.NET, vanilla JS, and CSS
Been waiting for this.
Been waiting for this. Anyone know which flavor of 2.0 we are getting? (2.0.5 is current) 
Any reason you're not using strongly-typed views? Also I wouldn't encourage the use of lower-case property names, e.g. `username` and `created_at`, rather `UserName` and `CreatedAt`.
Based on [this comment by raRaRa at end of page](https://github.com/aws/aws-lambda-dotnet/issues/149) it's version 2.03 "Project is referencing version 2.0.5 of Microsoft.AspNetCore.All which is newer than **2.0.3, the latest version available in the Lambda Runtime** environment." OS info, based on code deployed to Lambda (retrieving System.Runtime.InteropServices.RuntimeInformation.OSDescription), show it's running on: Linux 4.9.75-25.55.amzn1.x86_64 #1 SMP Fri Jan 5 23:50:27 UTC 2018 
Task.WaitAll doesn't unwrap the tasks, so you still have to use the .Result property manually. It's not a huge difference, but this can help you clean up code without sacrificing performance. 
use https://jwt.io/ to decode your jwt's and see what claims are attached to it :)
Have you tried this solution https://youtrack.jetbrains.com/issue/RSRP-461744#comment=27-2242455? 
Any idea if they are committed to saying remotely current? Or is 2.0.3 what we get for a couple years?
Ah. Vendor lock-in through culture is very powerful.
Ha. Gimmick feels like strong wording, but yes, that's the value proposition. The other thing that I think people underestimate is that it avoids a couple of common problems like awaiting sequentially and early exits without awaiting all tasks.
Absolutely, something we fight against with every decision. Additional pain-points on AWS has always been .Net not being treated as a first-class citizen; more like an afterthought.
&gt; like awaiting sequentially and early exits without awaiting all tasks. For that you can just use `Task.WhenAll` and `Task.WhenAny`. For that you don't need your library, and you are very well aware of it. So no, your library does not avoid those problems. The existing .NET methods do it, you just wrap those.
How are the cold start times? It was too slow before to use for any kind of client-interactive APIs.
Isn't all code just wrapping the existing .NET methods? Anyway, I found it valuable and time consuming to put together these methods and I like what they do to the code that uses them, but there's certainly no reason to change if you're already happy with using the underlying methods directly or if your use-case is outside of what this library offers. I chaff at it being described as a "gimmick", but I certainly understand that I'm not curing cancer here.
Your library **only** adds the destructuring. It does not add functionality to the problems you described. The idea is honestly neat, but I'd probably find it a lot more difficult to read and maintain.
Not sure. AWS Lambda support for .NET Core 2.0 came out 5 months after it was released by MS, so hopefully this time lag will not get any worse. I would not expect interim updates for 2.0.4, 2.0.5 etc, but would hope to see a 2.1 update.
Once upon a time they were just going to support the Microsoft "Long Term Stable" releases. 2.0 is their first departure from that. Honestly what I'd really like to see a beta service that always has the latest release.
To chime in with a small tangent, but nothing is wrong with Azure. Its what I use for hosting for my contracting business. love it actually. However, for the longest time I was one of the people who had to keep crowing about Azure. 2.0 is a huge improvement and a majority of my clients were on AWS. I would have to either get an exception to use Azure, or downgrade to use Lambda. Or in worst case, they would spin up a VM on AWS just for my service. So its not something wrong with Azure, but AWS is a great product that runs pretty deep for a lot of companies. 
Knowing Microsoft, when Amazon announced they would only support LTS versions, MS probably decided to stall the LTS release of .NET Core 2.0 to delay Amazon. I think your suggestion of latest release as beta is a good idea. Amazon could release it as unsupported/beta, and it would allow people to develop against the latest version. As a bonus to Amazon, it would give them a heads-up if there were any issues, giving them a chance to fix problems before the next major release.
The article says use the AWS Toolkit for **Visual Studio 2017.** How would one go about using Visual Studio Code on Mac or Linux (I use both of these, I don’t have a Windows machine to try this out) Also, if the article says .NET Core 2.0 , does this imply ASP.NET 2.0 well? 
7.38 seconds (n=1) But you can always query your API every 10 mins or so to keep it warm. If you have reasonable site traffic, this shouldn't be necessary.
As a heads up to other readers, the solution you suggest (locking the entire dictionary) runs counter to the point of the Concurrent concept itself. If you are doing that amount of locking that Concurrent becomes little more than window dressing saving the coder from writing extra lock statements but not getting the "concurrentness" from the collection. This is likely fine in smaller implementations but if you are dealing with tons of treads and consumers of that data locking the whole thing may not be desirable. 
No, you get a million lambda + REST API gateway calls free per month for the first 12 months. Thereafter you still get 1M free lambda calls/month, but API gateway calls cost $3.50 per 1M calls. Hence sending a keep alive every 10 mins (via API gateway) would cost you approx $0.02/month (after your 12 months trial is up). 
It makes it easy to install .NET Core command line apps, distributed via NuGet.
Expensive azure also in my view implements price gouging especially on app sevice.. I mean I like azure yes but... For example how awkward they made sorting LetsEncrypt out on appsevice because they want you to but an expensive one from them. Consider managed dreamhost ... Or netlify have it as one button. Custom domain ? Bump that service plan up Ssl you say on a custom domain..bump that plan up again. I was making a service for a charity.. Dumped it and used Linux...yes benefits of pass lost but 500 USD saved. 
If you find yourself iterating through every element in a Dictionary, you probably have bigger things to worry about than random access performance during the iteration, and what he suggested with ConcurrentDictionary.ToArray() really is the quickest and least impactful sequencing method for that type of collection, so it's still good advice. Plus, if you're managing tons of data with high I/O throughput requirements to the point that ConcurrentDictionary isn't able to cut it for what you need it to do, the next step up is leveraging a proper relational database, which is going to offer much more sophisticated locking controls.
I am open to other providers.. and these answers are really great to help me determine the viability of other platforms!
Be careful there, in some applications a RDBMS is not fast enough to keep up with the application. For example in near-real-time automation where the controller may be responsible for 1000s of individual units at once. For some application your solution would be fine, for others not so much. That is the challenge with performance type concerns its very situational specific and the second you think you know exactly what people will need to do with X you will be humbled. All that said, most dotnet devs do not worry about near real-time systems or similar problems so take what I say with a grain of salt. 
Neither the linked article, or the original article that the linked article copied, reveals what is "next" for the CLR (outside of Span&lt;T&gt;, which is already out).
IDictionary&lt;K,V&gt; just models any dictionary-like structure and doesn’t say anything about its synchronization across threads or lack thereof, so I think it’s natural that it implements it. 
It's not THAT long for tl;dr, jeez
It is sad to see install-module and friends not getting more love. I really liked that infrastructure, but I don't think it was adopted by many. We have that system, Chocolatey, this, and a mish-mash of other things.
I agree about `IDictionary`, but unfortunately (IMO) it inherits from `ICollection`. `ICollection` has `CopyTo` that copies the collection contents to an array parameter. I think accepting a fixed sized array does in fact say *something* about the type's synchronization because you need to to know its size beforehand to create the array and hope the collection doesn't grow in the meantime.
When the user submits the form on your site containing reCaptcha, look for the g-recaptcha-response POST parameter on the server-side. Then send your own http post request to this url: https://www.google.com/recaptcha/api/siteverify The parameters are: * secret - Required. The shared key between your site and reCAPTCHA. * response - Required. The user response token provided by reCAPTCHA, verifying the user on your site. * remoteip - Optional. The user's IP address. Then you should get back something like this from Google which you can check for success. { "success": true|false, "challenge_ts": timestamp, // timestamp of the challenge load (ISO format yyyy-MM-dd'T'HH:mm:ssZZ) "hostname": string, // the hostname of the site where the reCAPTCHA was solved "error-codes": [...] // optional } See here for more info: https://developers.google.com/recaptcha/docs/verify
You should always validate the recatpcha on the server when the form is posted. I recently integrated recaptcha with unobtrusive validation in an old site. I only validate that there is something in the response textbox before submitting the form. 
excuse me, what?
If you have no constraints on price, then use whatever you are most comfortable with and find easiest to develop with. There's no issues mixing your controls, so don't feel compelled to convert everything else to telerik if you use their grid. 
I honestly don't know what I just read.
We had a project that used Telerik controls in a windows forms application but it was slow that we decided to rewrite it with devexpress controls. I was not involved in that project and don't know the details but I maintain a project uses Infragistics controls and it very good. Nice performance and easy to use API, highly recommend
As Said in r/csharp, this article is about extension methods, and msdn clearly specifies that extension methods are outside the bounds of thread-safety. 
I believe you are correct in the work being done on the UI thread. I've tried to create async tasks and I'm having no luck but I am new to mvvm and async tasks so I'm probably doing something wrong. An easier example of how I need help could be updating a button. I have a button labeled "Connect" and when it is clicked it has a delegate command run a method called "Connect Device". ConnectDeviceCommand = new DelegateCommand(ConnectDevice, canExecute); The ConnectDevice method does 3 main things: connect to the bluetooth device, update the button text and read data "infinitely". A visual example is below. Connect(); ButtonText = "Connected"; //This is updating a property in the view model ReadForever(); I need the button text to update before entering the ReadForever() method. 
Try something like this: public async void ConnectDevice() { var connected=await Connect(); if (connected) { ButtonText="Connected"; } ReadForever(); } public async Task&lt;bool&gt; Connect() { return Task.Run(() =&gt; { // Do some work // return true if connected false if not } } public async void ReadForever() { while (true) { var data=await Task.Run(() =&gt; GetDataFromBluetoothDevice()); // Do something with the data on UI thread } }
Telerik and devexpress are some big ones with regular updates that I know of. Most control suites offer demo's or trial versions, best to just try and see what you like best.
Second that DevExpress recommendation. Their WinForms controls are really good and their technical support is wonderful. You actually get to talk to people who know what you are trying to do and how to accomplish it.
Run the bluetooth on a background thread and make the INotifyPropertyChanged calls inside Dispatcher.Invoke.
I have implemented it like this for testing purposes: private async void ConnectDevice(object obj) { var testing = await test(); if (testing) { ButtonText = "Connecting..."; } ReadInfinite(); } public async Task&lt;bool&gt; test() { await Task.Delay(100); return true; } ReadInfinite acts as normal and doesn't send data to the UI thread. The point of this is to see if the button changed to "Connecting..." at all before it went into the infinite loop. When doing this, the button text never changed. Am I implementing this wrong?
How is ButtonText defined? What UI framework are you using? The next step is to understand how the binding between ButtonText and the actual UI Button's Content property is connected. Are you using WinForms, WPF, or ASP.NET? Are you using MVVM or anything like that? Can you provide a little more color as to the structure of your program?
My ButtonText is a property in the viewmodel. private string buttonText = "Connect"; public string ButtonText { get { return buttonText; } set { buttonText = value; NotifyPropertyChanged(); } } I am using a wpf with MVVM and the button is bound in my xaml: &lt;Button Command="{Binding ConnectDeviceCommand}" Content="{Binding ButtonText}" x:Name="connectButton"/&gt; Rather than a window, my view extends UserControl: public partial class EviseView : UserControl { public EviseView() { InitializeComponent(); } } I am not setting the data context in my EviseView because I have a data template in App.xaml that does it for me (I'm using a docking manager plugin from the company Syncfusion and have to implement it this way for mvvm to work with docks). &lt;DataTemplate DataType="{x:Type local:EviseViewModel}"&gt; &lt;Grid&gt; &lt;local:EviseView /&gt; &lt;/Grid&gt; &lt;/DataTemplate&gt; Thanks for the quick response! Edit: I know the updating works because I can get the button text to change. Just not before the method it is in finishes executing.
That looks like it should work. What does your implementation of ReadInfinite look like? Or, can you comment out ReadInfinite and see if the button text updates? Or replace it with some testing code like: Task.Run(async () =&gt; { while(true) { await Task.Delay(TimeSpan.FromSeconds(3); Console.WriteLine("Pretending To Receive Data!"); } });
A bit tangential to your question, but before you actually do this, please read this article: https://www.troyhunt.com/everything-you-ever-wanted-to-know/. Doing secret questions right is pretty hard. 
That works! Thank you! Will I implement this a similar way to update a property in a viewmodel with data that I am reading?
If you have navigation property for resource tags (and i suggest using it if you don`t) you need something like this: where query.All(q =&gt; r.Tags.Contains(q)) But be sure that you don`t run into problem with lazy loading of Tags.
Yes... kind of. Essentially what you want to do is have the code that (I assume) polls for new data via the Bluetooth connection run within a background thread. That means something like: // Code here runs on the UI thread await Task.Run(() =&gt; { // code that polls bluetooth device here is running in the background } // Code here runs on the UI thread } What you're trying to do is definitely possible here, though you may want to explore Observables as they may be a natural fit with your use case... Either way, I'd suggest you read something like [this blog that talks about async/await](http://blog.stephencleary.com/2012/02/async-and-await.html) for some insights about how this all works
Perfect! I appreciate all of your help!
&gt; Would replacing all standard controls with their Telerik counterpart slow things down? Not necessarily. The worst side-effect that you will see is just longer download/load times depending on how much data is being populated in your views. In my experience the only things affected were download size, and my software is designed to repair broken/excessively slow computers, so I think you'll be fine.
I hate Telerik controls. We’ve used them for years and our team can’t move away from them fast enough. Slow, bulky, updates break code, expensive, poor support, etc. For grids, we use ASP Repeater and Datatables/jQuery. 
I see what you mean now! I was confused on the terminology. Thanks for your help!
Any case anyone cares, this is caused by not having the file type specified in the manifest. Trying to figure out an alternative since the app doesn't actually handle jpgs...
About the only thing I could think of - Have you checked that all files and strings have the same case? Perhaps one file system is case sensitive.
I don't think you can somehow delay the account creation. I can only think of a dirty way to handle this. Do create an account, but do not let the user login unless you have the user's security questions filled up. (You can also trick the user by not sending a "user account added" confirmation email, till you have the security questions added to his account)
Yeah, they're definitely the same case. It works when I add the file type to the manifest. Permissions issue with the API. I just wish I had a way around it because VLC has no business being associated with image files.
But if you have an admin and every user already have an email. Why not simply implement password reset via email? What purpose would the "security question" have? 
Can you elaborate? That doesn't make much sense to me. I just ran your snippet and it works fine (tried both in LinkPad, as a standalone program whithout a manifest, and a Console app in visual studio)
Did you read https://joonasw.net/view/aspnet-core-2-azure-ad-authentication ? Using OpenIdConnect, authentication is getting pretty simple. You can freely add SQL DB authorization for your purpose. We were using it in previous ASP.NET versions (MVC 4), main problem was with authenticating API requests, but it shouldn't be a worry in ASP.NET Core.
Put a broker/collection/some kind of facade or adapter between your bluetooth integration and your view/viewmodel make it implement IObservableCollection and IPropertyNotifyChanged. You might just be able to use ObservableCollection&lt;T&gt;. Bind to that collection on your view and update at will in the viewmodel or in a separate thread. 
It's caused by permissions restrictions in UWP apps. In order to access a file type on removable storage, you have to explicitly add the file type association to the package appx manifest. My problem is that the app I'm working on (VLC) doesn't (and shouldn't) have this file type association. There's probably away around this using a folder picker, but it's not worth going to that kind of trouble. The code runs absolutely fine from my local disks. It fails on both my PC and Xbox when using a USB drive though.
Ah! That makes alot more sense! Thank you. Its like Android plataforms. But the file association shouldn't matter... maybe theres a way to ask the user to change it? (Like on windows 10 that asks the user if he wants to use the new app or keep using the old one) Anyway I never used UWP (aside from a really simple toy apps) so I probably can't help more :(
It's not really for dotnet reddit, since nav is using C/AL. There's interoperability with .net, you can import and invoke some objects from clr, but main lang is c/al. 
You wouldn't want to delay the creation of the account because you need to have somewhere to store the details you're collecting, at the time of collection. I'd create the user database record and another linking record, which would have fields pertaining to an enumeration of the various options for secret question. Let's say you have ten options (which you as the dev choose, don't allow custom questions), of which they **must** choose 3 and provide validated answers to (use Regex to ensure no special characters, no one character answers, whatever else you want). You'd have 3 columns in the table specifying the enum choice, and another 3 with the answers to those. Upon sign up, those would be all be populated, though until the account is validated by email conformal or whatever I'd potentially have a Boolean flagging that it wasn't yet valid, meaning no access to the overarching system until it's validates. Any accounts not yet validated by end of day would be cleaned up (deleted) in overnight processing if you're worried about erroneous data. Upon needing to use the secret questions (typically password reset etc, when the user has selected to do so, but you could trigger it once a month or something at the time of forced password change if you'd like), you'd query the database for all of the secret question/answer pairs that exist, and choose a random one to ask at that time. Give them another option to answer if the answer doesn't match, or deny access all together pending a phone call to your support team if you're requirement is to be particularly heavy handed in your security procedures. Perhaps you'd implement automatic secret question query to the user upon 3 failed login attempts. And then, if a single secret question failure occurs you could implement an account lockout period of xx days/hours/minutes.
Your solution was correct and very helpful, thank you! So for my sake I also would like to know what I am doing wrong here: var resources = db.Resources; foreach (var q in query) { resources = from r in resources join t in db.Tags on r.id equals t.Resources.id where t.TagName == q select r; } return resources; This seems logical in my head but doesn't work, any ideas as to why?
Maybe I'm just missing something, but I've had a lot of problems using DevExpress controls, particularly grids. They can do some cool things, but there are a billion properties, and what the properties do isn't always intuitive. If you don't have somebody experienced in using DevExpress, it seems to be a very steep learning curve, and I find it difficult to find relevant support topics in many cases. Generally I use customized DataGridViews and for my purposes, they're usually sufficient. My only other experience with third-party is ComponentOne TrueDBGrids, and I'd probably go for DevExpress first.
You might want to check this youtube playlist: https://www.youtube.com/watch?v=J_avx1yHVYk&amp;list=PL4gnRyDtzf6i0PC-1zlirbZMJvYC7Y-bI
I was assigned to that team to help porting a few forms and I say that its harder to use than telerik or infragistics, most of your code will be implementing events; want to change cell color ? implement an event.
Now add GitHub Issues integration :^) But really, good job. Finding a way to organize what needs to get done is the first step towards success.
Put the code on github or other repo
What UI framework? Looks slick 😄
Yeah, need more details, OP!
Ok, now you all have prompted another question. First I should point out that my project is current written as a VB.net WinForms application pulling data from an SQL server. Whatever modifications/updates I make to the project will all be handled by me and need to be done within 4-6 months. I'm not new to development, but I've also never used WPF or UWP just because they weren't around when I originally built this project. This project serves as an internal 'CRM' for the company I work for and it needs to be around for many more years and will just continue to be updated as necessary, so if WinForms is 'dieing' then now is as good a time as any to move on to the next technology. I need to be able to access local and network resources from my program and it also needs to be able to use Crystal Reports, since I have several hundred reports that I don't plan to touch this year. I have a small number of users that are still on Windows 7, but they can/will be upgraded this year to Windows 10. Given all that, can anyone help me choose between WPF and UWP?
Looks like WPF.
I'm a bit confused as to why the `Concat` methods needs tens of overloads. Why wouldn't a single, generic method be enough ? struct SelectEnumerable&lt;T&gt; : IStructEnumerable&lt;T&gt; { public ConcatEnumerator&lt;T, SelectEnumerable&lt;T&gt;,TOther&gt; Concat(TOther other) where TOther : struct, IStructEnumerable&lt;T&gt; =&gt; new ConcatEnumerator&lt;T, SelectEnumerable&lt;T&gt;,TOther&gt;(this, other); } Using the `Foo&lt;T&gt;(T t) where T : IBar` pattern instead of `Foo(IBar t)` should get rid of boxing through generic code specialization, shouldn't it ? 
Isn't it easier using SignalR?
WinForms is legacy, but it's not dying / dead. The odds of support for it actually going away anytime soon are just about non existent. UWP is super restrictive, so unless you need something that it provides I'd stick with WPF. You can also host WPF controls in your WinForms app, so rebuilding it in chunks is a realistic strategy (as opposed to rewriting all of it).
This looks like an interesting idea. Do you know if there's benchmarks on normal LINQ vs LinqAF?
This is really nice. I need to pull my finger out and actually use some of the coding knowledge I've picked up. 
I see more potential at compile time rewriting the LINQ expressions where possible.
https://stackoverflow.com/a/44153734
Indeed!
WinPhormsFramework
Integration with Github and VSTS would be excellent! (as an open source maintainer, good luck!)
Isn't allocation of some sort unavoidable due to the fact that you're returing interfaces. Structs get boxed when cast to interfaces. https://stackoverflow.com/questions/3032750/structs-interfaces-and-boxing 
I saw that page but thought that was code for the server end on not accepting SSL. Thanks a bunch! Got some more testing to do!
Nothing wrong with Winforms.
I use it for prototyping.
Wow I can't believe you just went there lol. God I love backronyms.
JUST DOT IT! MAKE YOUR DREAMS COME TRUE! JUST DO IT!
I know it’s not related, but share your wallpaper, please.
LinqAF is supposed to stand for "Linq: Allocation Free", but all I can think of in my head is "Linq as fuck".
But he's not returning interfaces. That's the point of the library.
Whow, that must include a lot of custom GDI drawing code then.
I'm retarded. You're right.
Same here lol 
Sort of. ;)
You might like this album I put together: https://imgur.com/a/AiuYf