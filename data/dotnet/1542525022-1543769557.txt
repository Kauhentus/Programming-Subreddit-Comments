Got a link on how that works? It would be interesting to see how to handle passing back progress to the ui and handling errors?
Woot!
&gt; I have a colleague that everytime forgets if it's ArgumentNullException or NullReferenceException he's supposed to throw. You should never throw a `NullReferenceException` yourself...
Some people write **huge** methods, and usually you remove debug symbols from the deployed versions (so no line numbers in stacktrace).
Yea, agree. I just gave you a tip and got downvoted. Can't critize VS on this apparently. Just use what you are comfortable with, you are not going to miss anything from VS in Rider. Atleast that's my experience. 
You can use `IProgress&lt;T&gt;`/`Progress&lt;T&gt;` to report progress back to the UI. Error handling can be done with a simple try-catch block around the await.
&gt;This is my go to link that I'm spattered all over the internet :) Is there a general rule of when to use \`ConfigureAwait(false)\` in WPF? I'm guessing if the rest of the method has some UI stuff? Could I just put \`Task.Delay(1).ConfigureAwait(false)\` at the start of any method I want to run on a ThreadPool thread ? :P
The only thing that needs thread affinity is the bit where you update the UI, so configureAwait(false) all the things and dispatch only the UI related code. Task.Delay(1) may do it, but there's not much point. If your task takes more than (near to) zero it will probably run in a thread. If it doesn't then you won't notice the difference in the UI because the scheduler is smart as fck, plus you save the context switch overhead.
I have a two year old Dell at work, 16GB ram and dual-CPU Xeon for 32 logical cores. It always felt slow until I put an SSD in it. HUGE difference.
Second this, it is practically essential nowadays.
1. Open Resource Monitor 2. Recreate the situations that lead to the slow down. 3. Look at Resource Monitor to see if it is CPU, Memory or Disk that is the bottleneck.
3rd World. Hard life here.
Cheers, ill have a look at it... I think im past the winforms days but its kind of interesting.
The point I was trying to make and perhaps didn't explicitly say it is that programming internships are far and few between these days. It's been that way for at least the last 5 to 10 years. Reason is, software automation (and therefore programming) is no longer a novelty these days as it once was. Free is not better when it comes to companies needing systems and solutions that actually work. They are starting to understand that GOOD programmers ONLY come from experience as they've been burned in the past by free/cheap/junior labor. If you notice, must programming jobs posted these days are for mid level to senior positions. A company would rather spend 150k a year on a programmer that can build quality solutions based on experience rather than 30k a year (or free on an intern) to build stuff that won't scale or be maintainable. Those to factors only come with experience. The stuff in textbooks don't work.
Exactly! :)
`IProgress&lt;T&gt;` is a simple interface that could be used in any environment. The default `Progress&lt;T&gt;` implementation is built around `SynchronizationContext`, so it'll work in any common GUI framework.
He's probably talking about the vertical tabs on the left sidebar at the bottom. Those are XP style.
In order for the user to interact with your app, your app has to run what's called a message pump. When the user performs an interaction a message is sent to the app, and your app has to handle it in order to for .NET to translate it into a winforms event your code can use. The message pump is started by calling Application.Run. There are also other pumps run for dialogs when you call things like MessageBox.Show (since those don't return until the dialog box is closed; similarly Application.Run won't return until you call Application.Exit which shuts down all your UI). Application.Run eventually ends up calling events in your code based on these messages, such as button click events and so forth. If you perform a long-running operation your window will not be responsive as you have to return control back to Application.Run's message pump to have a responsive application. There are several ways of approaching this: Application.DoEvents() runs the message pump for a little bit, so you could call it when doing a long-running operation, but it's generally frowned on and not the best solution anyway. You could use Thread to spin off a new thread so the message pump can keep running on the UI thread. This is the old way to do it. However then you have multi-threaded considerations. Most notable you have to call .Invoke on a control to run code back on the UI thread if you want to manipulate controls from another thread (for example, for a long running operation you may want to display progress somehow). Task is recommended to replace Thread when possible with the newer .NET versions, IIRC. Task actually tries to make things easier on you. In a WinForms app when you've called Application.Run Task gets clued in that that thread is your UI thread; when you use async/await code the code will resume running on the UI thread after await finishes, but while await is waiting it'll drop back to the message loop. This allows you to write async code that looks almost exactly like synchronous code without worrying about needing to Invoke. The downside is that any long-running functions MUST be async/await to not cause your app to stop responding, since any sync code runs on the UI thread. Though you can usually use await Task.Run to forcibly split off such code onto a new thread. Task also intentionally has no .Abort method like Thread does. Aborting a thread is not recommended as you have no idea what the thread is doing, and aborting may leave your program in an unexpected state easily. Instead, using CancellationTokens and having the Task check at periodic safe intervals to see if it should abort itself is the way to go (you can still do this with Threads if you want).
That was a version with async completion of the task.
If VS is already shit with small projects, then I don't want to see what's it's gonna be with big projects.
Never seen anything like that. It does seem you're missing an ampersand character before currency. Perhaps that's part of the problem?
[removed]
`&amp;curren;` http://graphemica.com/%C2%A4
I think it's just the way HTML works. "\&amp;curren;" means [this symbol](http://graphemica.com/%C2%A4) Even though you didn't put the ";" there, the browser just thinks that you meant this and puts the symbol there. So you should print "\&amp;amp;" everytime you mean to print "&amp;" in a HTML or XML file.
 string baseURL = "https://api.sandbox.amadeus.com/v1.2/flights/low-fare-search?apikey=...&amp;"; var parameters = new[] { "origin="+origin, destination="+destination, "departure_date="+polazak.ToString(), "return_date="+return.ToString(), "adults="+adults, "currency="+currrency}; string queryPart = string.Join("&amp;", parameters);
Looks like a encoding issue. Your &amp;curren gets converted to symbol
VS is not shit with big projects ...
It looks like you're missing a quote before "destination"
\&amp;amp; - don't forget the ";" There may be some nicer ways your web framework provides, but essentially, in the resulting HTML, each ampersand should be written as \&amp;amp;
I might try changing it to string interpolation just for giggles. If that didn't work I'd just change currency to currencyCode and move on with my life
do you mean to have 3 rs in your currency? +currrency?
post the part where you output it to the screen
That worked. But I need help again - C# says I am getting a "bad request 400" when I make my request, yet when I copy paste the same URL I generated with my code into my browser, I get the expected response without an error.
THIS. This is why I asked how you're outputting to the page below
You should be using the [Uri](https://docs.microsoft.com/en-us/dotnet/api/system.uri?view=netframework-4.7.2) class and construct your Urls that way. Properly escaping a Uri can get tricky. 
You are looking for eager loading? &amp;#x200B; [https://docs.microsoft.com/en-us/ef/core/querying/related-data](https://docs.microsoft.com/en-us/ef/core/querying/related-data)
https://docs.microsoft.com/en-us/ef/core/querying/related-data
I am using the following command to get the service server's response: var response = httpClient.GetStringAsync(new Uri(queryURL)).Result; The string I generate as argument for the URL is valid - I get the proper response when I copypaste it into a browser. Yet when I run the code, I get 400: Bad Request error. Do you have an idea of what the problem might be?
Not quite. Eager loading still requires that the developer use an Include in the code. var invoices = ctx.Invoices .Include(x =&gt; x.LineItems) .ToList(); What I would like is to be able to simply do a var invoices = ctx.Invoices.ToList(); and have EF configured so that it knows to load the line items as well.
You want the final URL to be "http://127.0.0.1/?abc=def&amp;ghi=jkl". The proper way to print this in HTML is "http://127.0.0.1/?abc=def&amp;amp;ghi=jkl". This is because the ampersand character ( &amp; ) is considered a special character in HTML and XML. In programming terminology, this is called escaping (in general). If you copy the escaped URL and paste it into browser address, you skip the step (performed by browser HTML parser) of replacing &amp;amp;amp; to &amp;amp; , and the final URL you visit is therefore malformed in undesired way. I'd suggest that you read something about escaping in HTML, URL and C# (or the language you're using) to get the basics and understand root of the problem. It is absolutely necessary for you to know this, otherwise your websites will have very strangely behaving bugs, including some very, very nasty security bugs (SQL injection and XSS to name a few).
This has nothing to do with automatic semantic versioning. The versioning still happens **manually** by yourself. You just shift the location **where** you define the version yourself.
Not built in. What you would need to do is create a method that returns the list of invoices and call the include in there. Then everything else just calls that method. Maybe make a class like InvoiceService and inject that into other classes to use. Rather then repeating the same query everywhere.
Right. I was hoping that I could use this in other IQueryables. So, for example, if I needed to do ctx.Orders.Include(x =&gt; x.Invoices).ToList(); That I wouldn't need to remember to include the line items. AFAIK, you can't return an IQueryable from a function and use it in a new IQueryable, right? Like, ctx.Orders.Include(() =&gt; getInvoices()).ToList();
I would start with [The Art of Unit Testing, Second Edition with examples in C#](https://www.manning.com/books/the-art-of-unit-testing-second-edition); this will get you started with unit test. There are few courses on Plurasight and Microsfot Virtual Academy (Free) that should be helpful. If you want to apply testing to legacy code (any code without unit tests) you might want to check out this \~1hr tutorial by Sandro Mancuso I found it very motivating and helping [Testing and Refactoring Legacy Code](https://www.youtube.com/watch?v=_NnElPO5BU0) &amp;#x200B; &amp;#x200B;
How would changing it to currencyCode fix it? Wouldn't changing it to currencyCode just result in ¤cyCode? Do you feel like the best way to solve problems is to just try changing things? How do you rate your ability as a programmer?
Sometimes its easier to read and write just to write it out the long way.
You'd either need to make a service / repo for each or some master service. But once you start needing lots of different queries it can get complicated and bloated fast. The code you have provided there doesn't make any sense. That method expects you to select a property of the class. I would honestly just include what you need with each query it's so much simpler. Plus it makes it very clear exactly what data you are loading. And if you are calling it in loads of places extract it to a service. And yes you can use IQueryables in other calls. Just not the way you provided. You could return that IQueryable and then run more linq functions against it.
Ah, cool. Sorry I misread you. If you want just a nice non-server db, then just use sqllite. Postgres is a nice choice though if you want something free/FOSS server based. They're both awesome in their own spaces. SQL Server is a PITA for personal projects though because of potential licensing issues if you happen to monetize something.
Right. I was hoping to use EF as my repository.
I've used this for an example project I built. It's really nice as it goes through the MVC pipeline. One problem I ran into was when I integrated azure key vault. Without a hack, if you use hosted agents on azure devops, integration tests start failing. Hosted agents don't have access to azure resources so until there is support or you use a private agent, integration tests don't work when using key vault. 
If you are putting it in an HTML document it needs to be escaped. &amp; has special meaning in HTML so it must be escaped to &amp;amp;
You can add a member to your DbContext class just like any other class, something like: public IQueryable&lt;Invoice&gt; InvoicesWithLineItems =&gt; this.Invoices.Include(x =&gt; x.LineItems) You can even make .Invoices private and change the name, and make this the new .Invoices, but then IIRC you need to add a line to OnMigrating or whatever it's called to override the table name back to Invoices.
EF Core now has lazy loading feature as of 2.1. You can enable it and it will always pull in related data by default. https://docs.microsoft.com/en-us/ef/core/querying/related-data#lazy-loading
&gt; You would have more luck getting your foot in the door in a junior position (if you can find one) than trying to aimlessly pursue an internship. The problem is that most of the entry level positions require a completed degree. All I have left to complete my degree is to complete an internship (or the first 400 hours of an entry level job would work). Still, I've had 2 companies screen me out during the phone interview for entry-level jobs because I don't technically have a completed degree. At least one of those said a degree was "preferred", yet they still gave that reason for screening me out. I only recently changed up my strategy to focus on applying for internships (because getting so many rejections humbled me). If I widen my search to the entire country, I have plenty of internships related to my skillset that I can apply for. Most of them don't really seem to get many applicants either (according to LinkedIn). I think this is a better strategy than applying for entry level positions and hoping to be the best candidate. 
The rules for encoding values in a URI go beyond just ampersand. And there's lots of routines out there that you can pick from to do this (i.e. **do not** do this yourself). Here's a stackoverflow question that should highlight the best approach (Uri.EscapeUriString): https://stackoverflow.com/questions/575440/url-encoding-using-c-sharp/7427556#7427556 (check out the answers by Siarhei Kuchuk and Simon Tewsi)
This would be better: string baseURL = "https://api.sandbox.amadeus.com/v1.2/flights/low-fare-search?"; var parameters = new[] { "apikey="+Uri.EscapeUriString(apikey), "origin="+Uri.EscapeUriString(origin), "destination="+Uri.EscapeUriString(destination), "departure_date="+Uri.EscapeUriString(polazak.ToString()), "return_date="+Uri.EscapeUriString(return.ToString()), "adults="+Uri.EscapeUriString(adults), "currency="+Uri.EscapeUriString(currrency) }; string queryPart = string.Join("&amp;", parameters); Even if you are certain that some of the values would not need escaping I personally think it is better to be super explicit about escaping. If down the road a maintenance change introduces a value that needs escaping (eg. destination "Turks &amp; Caicos") things will still work as expected. /note: I moved apikey into the array
* 4GB RAM is way too little for such a feature-rich IDE like VS. You could also have slow or ancient RAM for all anyone knows. * HDD's are slow. If you only have 4GB RAM there is a good chance things are being put in the page file which is HDD-bound. * You say you have an i5, but what i5? There are 9 generations of i5's. Thats like saying "I have a Ford, why do I get bad gas mileage?" It doesn't mean anything without specifics. * What are you working on? ASP.NET? WPF? WinForms? Unity? Some of these will be way more resource intensive than others. * Do you have a GPU or is your whole system bottlenecked by your CPU and RAM? * VS2019 might possibly perform better than 2017, but probably only on newer hardware. If anything, it will just get slower on old hardware, not faster. Imagine running Windows 10 on a Pentium 4 and then blaming Windows for not running fast. * What OS are you on? What other bloaty software to you have running alongside VS? Again, it isn't fair to complain about VS eating memory on a 4GB system if Windows and 15 other programs are eating the majority of the RAM that VS would have used.
Is this in .Net Core? If so you can (and _should_) use the `QueryHelpers.AddQueryString` to add query string parameters to your base url.
I’m not sure why you’re encountering this error, but I’d sure love to understand why you’re wrapping the call to serialize the request into JSON with `Task.Run()`
Fine, change it to curr or crrncy. You're right, the other way would have the same problem. I just wanted to avoid a nonsense word for the variable. My ability is fine. You might not want to put apiKey in a query string. Authorization belongs in headers, where it won't be trivially stolen. If a browser is doing this, it's outside of your control. You literally can do nothing to change an incorrect browser behavior, so you just work around it and move on. 
If a browser is deciding to interpret a correct query param as an html character I don't think any way you put it in the querystring is going to matter. All you can do is change the name to avoid that behavior
I've not used azure - is the keystore something that can't be abstracted out or similar?
There were a lot of changes to HttpClient internals in 2.1 - specifically to support the new SocketsHttpHandler. Try using `AppContext.SetSwitch("System.Net.Http.UseSocketsHttpHandler", false);` to go back to the 2.0 functionality. If that solves the problem, look through [https://github.com/dotnet/corefx/issues/30166](https://github.com/dotnet/corefx/issues/30166) and submit a related issue.
My laptop from 2016 has an SSD drive and 16 GB of RAM.
If you have methods/classes that use AKV I suggest using an If statement to determine if you're in Prod when getting your key/values. You can also store those values in the build and provide them as arguments if necessary. Not perfect but may help.
&gt; My ability is fine. Are you familiar with the Dunning-Kruger effect?
Well it can, but would be nice to not have to.
Hey man I just saw this--sorry I had notifications turned off on my phone for some reason. I felt like the Unit Testing course covers mocking/Moq fairly well, so the second course might be redundant. However, Mosh does **not** cover DI frameworks very much in the course. He explains IoC and *defines* DI very well, but his examples use "poor man's DI" (i.e., manually injecting the dependencies yourself). If you want a tutorial on Ninject/Unity/etc., you may have to find a second course for that.
Just use lazy loading.
Ha yeah, honestly not a clue why that’s there. Cheers!
Holy crap dude thank you, I feel like I was reading through my own journey of the vue templates except this story ends with a useful, up-to-date template. Can't wait to give it a try.
Thanks for the reply- I'm hoping it'll help others where I spent hours searching. If you have any questions, please feel free to reach out. I'm going to expand on linking .NET to Vue in the coming weeks in some more writing/repos - keep an eye out. Thanks,
A Comodo Authenticode cert costs $85 / yr. I would ask the client to pay for it.
I also had a very similar journey and I ended up with upgrading Mark Pieszak's template.
&gt;Dunning-Kruger effect \**pins the first wikipedia paragraph onto the companys board\**
&gt;For that sweet, sweet multithreading but that would not make sense inside a controller, because the controller would have its own thread/poolthread, so using [Task.Run](https://Task.Run) to defer a threadpool job to the threadpool would just be wasted energy. &amp;#x200B; The only reasonable application (that i see) here would be in a Desktop-Application, that would have to serialize a **large and deeply nested** JSON and still allow the user to interact with the app. */for real*
I'm working on a NuGet package right now. As for the Wiki, I'll do it very soon. :)
Sure!
You're welcome!
Excellent, I'm starting a vue spa this week and I've literally been dealing with the same pain of getting a good up to update starting point.. **THANK YOU**.. As for Microsofts lack of support I find it baffling.. Vue is super popular and maybe more importantly loved by devs(the loved part an order of magnitude more than angular). [Shame](https://gfycat.com/vigilantradiantbettong)
Back in time there was a special person inside Java team just to create and fix project dependencies 
If you have a VSTO Add-in, you can create a Windows Installer that will put it in the Program Files directory. If it's in the Program Files directory, Office will trust it. &amp;#x200B; [https://docs.microsoft.com/en-us/visualstudio/vsto/deploying-an-office-solution-by-using-windows-installer?view=vs-2017#ApplySecurity](https://docs.microsoft.com/en-us/visualstudio/vsto/deploying-an-office-solution-by-using-windows-installer?view=vs-2017#ApplySecurity)
What are your thoughts on this "industry path" making it easier for solo devs to create more complex projects? For me personally, I'm creating an app that is more complex than a to-do-list (orders, invoicing, pick-packing, label/barcode generation) and my hope is that these poorly documented plugins will allow me to get it done. I'm not yet good enough to tell when I'm on the wrong track...
As it stands today, this toolset does not help you manage complexity in any way, shape or form. Au contraire, it greatly increases the number of ways your entire codebase can go to shit: missing or incompatible dependencies which can take ages to sort out, programming yourself into a proverbial corner with all the latest and greatest tools, getting stuck on bugs. Do not follow my advice blindly, but I always recommend starting with the simplest approach possible. Simple ASP.NET application, server-side rendering, no bells and whistles. Unit test the hell out of your business logic. Do not mix concerns. Don’t do stupid and keep it simple. 
Annual price of VS + ReSharper is way above the price of PC that can handle them.
Try Rider. It's blazing fast.
[removed]
I'm in the same boat, in many projects as well you'll have 2 different teams working on each part so I just really just don't get the purpose of these templates beyond making SSR easier. Perhaps for these types of larger solutions, they aren't the right answer... and your approach is the best one - I really don't know :)
Yeah, I haven't had the need for SSR yet with the 3 SPA projects I have going on using Vue and Core 2.1 - not sure that I will. In that case I'd probably lean more to using Vue's SSR approach or something like Nuxt if I needed to extend an SPA. I really am using .Net core for the performance and my decade+ familiarity with it and ef / mssql.
[https://weblog.west-wind.com/posts/2018/Jul/31/Web-Assembly-and-Blazor-Reassembling-the-Web](https://weblog.west-wind.com/posts/2018/Jul/31/Web-Assembly-and-Blazor-Reassembling-the-Web)
This is essentially what I recommend in the article - specifically, my "template" uses the React template, guts the ClientApp folder, install the VueCliMiddleware package, and makes a few adjustments for HMR. Then, you scaffold a new ClientApp with vue-cli and go from there. Pretty simple. &amp;#x200B; The purpose of my article was to provide guidance on the above, but also to encourage discussion about the state of the client-side in .NET. It seems like there's a growing dependency on front-end JS frameworks to fill a void that Razor pages/Jquery used to. Steve Sanderson's Blazor project looks promising, and would be the ideal solution. Until something like that arrives, though, I don't think it's out of line to question the mediocre state of the JavaScriptServices docs, the confusing dropping of support for Vue, and the generally difficult-to-navigate (particularly for newer developers) system of configuring a SPA/JavaScript client with Core 2.1. &amp;#x200B; I've received a little bit of blowback for suggesting that Corporate Allegiances \*might\* have had anything to do with Microsoft supporting React/Angular over Vue. To be sure, the theory is pure speculation on my part. Even if it is true, it's completely understandable on Microsoft's part--but they should just say so, instead of framing Vue as being a second or third-tier framework. The larger purpose of this speculation isn't to knock Microsoft, the SpaServices/JavaScriptServices teams, or Steve Sanderson--on the contrary, I'm a huge fan of their work. I'm only trying to bring more attention to the state of affairs with the client and to (hopefully) see some more love for Vue in the future--it deserves it. &amp;#x200B; Hoping my article helps some fellow Vue/.NET developers navigate the front end. &amp;#x200B; Thanks for reading and commenting,
I thought the /s would be implied
Regarding | Is it not cleaner/easier to separate your SPA and ASP.NET Core (typically API) applications In this article [Multi-page .NET Core with Vue.js, Typescript, Vuex, Vue router, Bulma, Sass and Webpack 4](https://www.reddit.com/r/csharp/comments/9x0ro2/multipage_net_core_with_vuejs_typescript_vuex_vue/) a few users mentioned that they use a "normal" page but load a few parts using a spa framework. 
Why wouldn't you just save yourself the headache and run two separate projects?
4GB RAM and you want to be taken seriously? That is crazy. &amp;#x200B; Currently I have at least 50 tabs open/VS2017/Outlook and other misc stuff and I am using 13.1GBs of memory. My system was just updated to 32GBs, because I often run Android Studio and Visual Studio together when I'm testing some changes in API and android apps, and 16GBs would cause problems. Smart phones come with 4GBs these days. 4GBs IS LOW, 16GBs is probably what an average dev machine has.
Or small.
My team currently runs MVC 5 where we use a single razor view to serve our built dist files. It works, but it's a major pain in the ass dealing with webpack build configuration, issues with compilation sizes in CI, and the like. Upgrading from Webpack 2.7 to 4x was a nightmare, as well. &amp;#x200B; What I outline in my article is essentially a middle ground between both of these options. We don't use a razor view- instead, the webpack and babel configurations are completely self-contained within vue-cli and work ootb. Vue-cli builds into wwwroot/dist, and then uses the VueCliMiddleware NuGet package to serve those build files directly to IIS via \`npm run serve\` on port 8080. I'm not 100% on this, but I believe the VueCliMiddleware is using a Kestrel reverse proxy to cast port 8080's built development server with HMR to IIS's default development port/server (hence the need to turn off SSL and https). With SSL/Https turned off, IIS accepts the middleware's instructions, and HMR functions flawlessly within IIS, even with AD Windows Auth (something that was elusive, if not impossible for us using MVC 5). &amp;#x200B; I'm still new to a lot of this, and development in general, but I'm hoping that some of my trial and error, as well as the "template" I linked in my article, will help developer using a similar stack navigate some of the common, frustrating issues encountered when configuring not just a SPA, but any JavaScriptServices-related client-side setup.
Thanks for commenting - this is more or less what I outline in the "template" I linked within the article. The solution is segregated between a DAL and Presentation Layer. The Presentation Layer forgoes most of the default "template" configuration within VS, replacing it instead with vue-cli's project scaffolding and the JavaScriptServices/SpaServices/VueCliMiddleware NuGet packages to allow webpack HMR/builds to function correctly. &amp;#x200B; The larger issue I'm hoping to direct attention is not just that Vue should be officially supported by .NET Core, but that there are major issues in general regarding .NET Core's growing dependency on third-party JS frameworks on the client-side. Where Razor pages/Jquery used to fill that void, a new one has emerged. Steve Sanderson's Blazor project (still in the nascent stages) looks like it could be the answer to this issue, but it seems that it is still a ways off. In the meantime, additional documentation, support, and guidance for those trying to configure not only SPA's in .NET, but any JS-related client, would be really helpful. Anyone that has gone down this road already, particularly new developers, will likely be able to relate to the headaches one can encounter trying to find a viable setup.
I downloaded it. It works. I'm a. Net core / framework web dev so I'm still fumbling around building installable apps and how to store the data. 
They should absolutely be separated. The main reason? The front end will likely be rewritten in another frontend framework in a year or 2. Tying them together with the javascriptservices packages is very foolish.
&gt; The templates have built in magic behind them that takes what I consider 90% of the difficulty behind JS framework development - webpack bullshit and packaging - and makes it automatic. What do you think the vue/angular/react cli's do? The .net core spa templates are also half broken. For example, for 2+ years, the .net core Angular template was outputting invalid HTML when it used SSR. The Spa services packages are basically a hackathon project. It's pretty silly to be using them outside of just messing around.
Why should anyone care about that today? The CLI's have bee around for like 2 years.
The bug is in your understanding of TimeSpan.Parse. Read the manual: &gt; The s parameter contains a time interval specification in the form: &gt; &gt; [ws][-]{ d | [d.]hh:mm[:ss[.ff]] }[ws] &gt; &gt; Elements in square brackets ([ and ]) are optional. One selection from the list of alternatives enclosed in braces ({ and }) and separated by vertical bars (|) is required. The following table describes each element. &gt; &gt; ff Optional fractional seconds, consisting of **one to seven decimal digits**.
Yep, it should throw an exception, truncate the value, or at least behave consistently. Microsoft's own UnitTests expect it to handle 8 digits, and round `1:1:.00000001` to `01:01:01.0000001` (+10ns to +100ns). I know because that's the test that failed when I submitted a patch. I'd say their UnitTest was created around the result it was already giving rather than any sort of expected value though.
It's behaving like it's doing Split on '.' and then Int32.Parse on the second part.
That's what it does in a more convoluted way. The token in the parser had the fractional part stored as an integer value and another with the number of leading zeros 
That makes sense, in your opinion, what if you have 5-10 foreign keys on the item, would you really go check for 5-10 items before trying to add it to your database?
If you want to prettify you're EF errors for logging you might want to check the `DbEntityValidationException` class which is the one that gets thrown by EF when Constraint errors happen. Usually I clean it up like this. try { // Db Stuff here } catch (DbEntityValidationException ex) { foreach (var ve in ex.ValidationErrors) { Console.WriteLine("- Property: \"{0}\", Value: \"{1}\", Error: \"{2}\"", ve.PropertyName, ex.Entry.CurrentValues.GetValue&lt;object&gt;(ve.PropertyName), ve.ErrorMessage); } } Preferrably putting the printing to a logger than a Console Write code and making it a method than writing it inline on every Db Try catch before re-throwing the Exception with better formatting.
I had the same journey when upgrading the Vue template and changing it to use TypeScript. But then I started to realize that I didn't really need much from ASP.NET Core, so it would make more sense to create the Vue application as a pure JavaScript/TypeScript SPA hosted as static files, an keep the backing API completely separate (and in ASP.NET Core MVC). Why go though the hassle of trying to align multiple build, dependency management, bundling, minification (and routing) systems inside the same application? It makes for tighter coupling and makes rewriting the UI (to Blazor :) harder down the road.
Imagine searching that at work when you encounter a problem on "Brazzer's" code hoping for an StackOverflow result to pop up.
The only problem is that my website replicates entry level business software, so the main users are those that are used to consumer websites and have no experience with business software. So I have to keep a balance of keeping it nice and pretty while also just making it work. At the moment it's built on a mess of a PHP Symfony setup which I want to move away from, and we're also moving our main business software (non-web) to .NET as well. I think after reading this thread my next stop will be just going with Razor Pages and if I really need a SPA (I originally wanted it for virtual pick-packing with no hand scanners) I can just add it separately.
Maybe. For those 5-10 items, what is the user's recourse if their specified value does not exist? Can they issue a subsequent call to create their desired records, or are most of these more like relatively fixed sets, like a collection of statuses? If the latter, can you move the definition of those statuses into an enum or dictionary in your app layer? If the former, that the user can specify subsequent calls to create their desired relational records, maybe the API itself can definite a more complex object input type, and then you can use a fetch-or-create style operation on each child record to reduce chattiness.
&gt; i personally try not to mock during unit testing. How? Manually implemented fakes/stubs? Or do you only do integration testing?
I think you should give a step back because I think you might not be doing it the right way. How is it possible that someone chooses or assigns a value that doesn’t exist? Your application should not allow it, you should either validate the input or provide a set of possible options to choose from instead of freely allowing the user to type in the key value (for example a combobox) Or maybe you could elaborate more in what you are trying to acomplish. 
I think this is the plan.
I'm using VS 2017 and I'm able to run 3 instances at a time without any noticeable performance decrease. I have 8gb of ram. So I would say that your machine is just slow.
At work, we have a Word add-in, and are still updating it using a Visual Studio 2017 Installer Project and it sits in Program Files (x86)! This works great with the latest versions of Office 365 and Office 2019. I think you can rule out a few of your possibilities at least.
Your post has been removed. Self promotion posts are not allowed.
Why do you keep posting the same questions in lots of sub-reddits at the same time?
This is spam.
it is called crossposting, but in this case it is shitposting
It seems totally pointless, they're self links with no external links, it's like the worse attempt at karma whoring/spamming ever!
&gt;What is your target system? UWP is pretty \*specific\*. You may also consider using old WinForms or WPF. But how do you want to design the interface between the gui and the console app? Sorry about the answer's delay. &amp;#x200B; 1 - My goal is to write a local application interface using HTML 5, Javascript and CSS to communicate with my class libraries projects. I intent those technologies because I already dominate them. 2 - My project is a "off-line application" that interacts only with the users machine, using local files. Maybe approaches such Web or Web Services may not apply to my case. &amp;#x200B; 3 - My concern with performance is because my application access various local files and write logs. &amp;#x200B; Thanks for patient and help! 
Stop spamming this flame war crap.
&gt;Not sure it this is a business or personal app? Lots of factors go into choosing w Oh my god! **You got the point!** Xamarin , web assembly and Blazor are exactly what I was looking! I want avoid Xamarin because I intent to use only HTML 5, Javascript and CSS 3. I superficially read about Xamarin and if I'm not wrong, it uses XAML not HTML. Web Assemply seems what I'm looking for: " (...) [web standard](https://en.wikipedia.org/wiki/Web_standards) that defines a binary format and a corresponding [assembly](https://en.wikipedia.org/wiki/Assembly_language)\-like text format for executable [code](https://en.wikipedia.org/wiki/Computer_code) in [Web](https://en.wikipedia.org/wiki/World_Wide_Web) pages. It is meant to enable executing code nearly as quickly as running native [machine code](https://en.wikipedia.org/wiki/Machine_code)." I Do not know If this approach can call a local dll. **It is possible?** Blazor uses Razor, maybe it is not what I'm looking for. &amp;#x200B; I will study Web Assembly, thanks! &amp;#x200B;
**Web standards** Web standards are the formal, non-proprietary standards and other technical specifications that define and describe aspects of the World Wide Web. In recent years, the term has been more frequently associated with the trend of endorsing a set of standardized best practices for building web sites, and a philosophy of web design and development that includes those methods. *** **Assembly language** An assembly (or assembler) language, often abbreviated asm, is any low-level programming language in which there is a very strong correspondence between the program's statements and the architecture's machine code instructions.Each assembly language is specific to a particular computer architecture and operating system. In contrast, most high-level programming languages are generally portable across multiple architectures but require interpreting or compiling. Assembly language may also be called symbolic machine code.Assembly language usually has one statement per machine instruction, but assembler directives, macros and symbolic labels of program and memory locations are often also supported. Assembly code is converted into executable machine code by a utility program referred to as an assembler. *** **Computer code** Computer code or program code is the set of instructions forming a computer program which is executed by a computer. It is one of two components of the software which runs on computer hardware, the other being the data. Computers can only directly execute the machine code instructions which are part of their instruction set. Because these instructions are difficult for humans to read, and writing good programs in machine code or other low-level programming languages is a time-consuming task, most programmers write in the source code of a high-level programming language. *** **World Wide Web** The World Wide Web (WWW), also called the Web, is an information space where documents and other web resources are identified by Uniform Resource Locators (URLs), interlinked by hypertext links, and accessible via the Internet. English scientist Tim Berners-Lee invented the World Wide Web in 1989. He wrote the first web browser in 1990 while employed at CERN near Geneva, Switzerland. The browser was released outside CERN in 1991, first to other research institutions starting in January 1991 and to the general public on the Internet in August 1991. *** **Machine code** Machine code is a computer program written in machine language instructions that can be executed directly by a computer's central processing unit (CPU). Each instruction causes the CPU to perform a very specific task, such as a load, a jump, or an ALU operation on a unit of data in a CPU register or memory. Machine code is a strictly numerical language which is intended to run as fast as possible, and may be regarded as the lowest-level representation of a compiled or assembled computer program or as a primitive and hardware-dependent programming language. While it is possible to write programs directly in machine code, it is tedious and error prone to manage individual bits and calculate numerical addresses and constants manually. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/dotnet/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
I saw blazor and I think that Is not what I am looking for, cause uses Razor and I want use only HTML, CSS and Javascript to call local dll. Thanks for answer!
Well... It was announced 12 days ago. 😁
&gt; .N I loved the detailed text. Thanks so much. I will look CefSharp, I intent to write the user interface in HTML/CSS/JAvascip and call local dlls.
Thanks neilg, but maybe If I convert it to web API my user's machine will need to be online (internet connection). I want to run my application off-line. Am I correct? 
&gt; The closest 2n boundary Why?
Limits you to a fixed set of sane defaults &amp; just habit from aligning things to blocks.
Web Assembly is not intended to be written directly by the programmers. And it has the same restrictions as Javascript (obviously). So it can't access a local dll.
No, because the web API runs through the local loopback device. No internet connection required 
In this case, can I use a Electro or UWP Project to call my local Web API? I Think that the best way maybe UWP, cause Electro's backend is node.js and I do not know node.
As I said before, a web service is applicable to your case. But the question is whether your users access the app through an arbitrary browser (e.g. http://localhost:8080) or through a standalone program. However Electron.NET seems to be a good choice, as you can use your library directly. I don't see how the performance may suffer from accessing or writing files, unless you use a massive amount of files. But even in this case, the front end has nothing to do with it and should handle reaction delays. 
Okay, the offline requirement changes a lot. Technically you can build a local web app, but it might not be as practical as a standard app. Do you have anything against XAML? Why do you want to use a web language to build a desktop app?
I also know this person
This is to do with populating the comboboxes from a database. I'm currently using SQLServer. The second combobox would get it's data from the child table while the first combobo would be populated from the parent table in the one to many relationship.
Minor optimizations. Memory is [packed](https://en.wikipedia.org/wiki/Data_structure_alignment), your 5 byte use likely turns into 
If you look at the string in isolation, sure. But usually you have more data than the string.
1) Nothing really. They have very different use cases and comparing them directly makes little sense. 2) Both are hugely overcomplicated and attempting to accomplish things the Web was never designed for. 3) Wasm allows IL to be run in the browser this will be a huge leap for .net in the future. 4) Most things have telemetry in them to allow developers to identify bugs and feature usage. These questions are stupid and a waste of everyone's times. You should find a hobby.
Other way around. Your next piece of data might start at a new register, leaving 4 bytes forever unused.
Why didn't you post it to let the rest of us know? Pretty selfish of you. 
Yes, this is our approach; CRUD in EF Core, calling programmability objects and handwritten queries (for presentation nuances) in Dapper.
Google knows all
I like UWP :) 
on Windows, .NET Framework 4.7.2. And Yes.
Yeah, I mean... this has got to be the easiest thing to google ever.
It's really sad that we don't have a good deployment strategy yet. Squirrel is the best solution, if you hit **exactly** the use case it was built for (install into the user profile in the folder it chooses for you, no multi-version, no customization, ...). If you need the slightest deviation from that, it's go fork yourself. ClickOnce and all those are a PITA, and making installers woth WIX is not a happy place either.
Nice thing about ClickOnce is that it is built into the OS.
Why would you start a new project in a legacy technology?
There is also open source Material design UI library for WPF. http://materialdesigninxaml.net
I do to but I hate UI development.
Windows installers have always been a major pain point, imo. I haven't done a desktop app in a good 5 years, but up until then I'd used just about every notable installer building tooling there is for Windows and every one was a beast to learn and wrangle into doing what you want once you get outside the most basic scenario.
&gt; Your next piece of data might start at a new register, leaving 4 bytes forever unused due to packing. You're talking about two completely different things. What you set the database column's max length to is completely irrelevant to word size alignment in the client application's memory.
I've asked this question about WinForms going back 10 years, but they just keep maintaining it. As long as they do, it's not a bad option I think.
How come? 
Essentially .NET Framework will be supported as long as the operating systems it runs on. That said, it applies if you update to latest versions. Earlier versions may lose support, like 4.6.1 already has. Not what you wanted to hear, to convince your team to use Core eh?
No, EOL is like Friday I think.
This is cool, but am I the only one who hates the look of material design. Just find it so... wrong... especially the way they do shadows. 
VS 2017 runs quite well on my Dell Latitude E6520 from 2011, Intel Core i7 Sandy Bridge.
Unfortunately the article says nothing about data access: one of the most important design decisions a developer has to make. Hopefully in a future revision this will be included. Most authors of desktop apps think they have to decide on a local data access approach or an Internet based architecture. Local data access gives the user direct access to the database via sqlclient or similar. The great thing about local it that it is blazing fast. The bad thing: it's local only. Remote users are out of luck unless they have VPN. Internet based, such as WCF,REST, or WebAPI is much more available but generally runs at about one tenth the speed of LAN connectivity. The vast majority of users of desktop applications connect to a data source that is (or can be) on their Local Area Network. However developers who want their API's to be as flexible and available as possible often only expose them as Internet based services such as WCF or REST. Exposing services via an Internet based API is often the right choice but it leaves the majority of users accessing data at a fraction of the speed of a local connection. With a bit of planning and a utility like [AdaptiveClient](https://github.com/leaderanalytics/AdaptiveClient), developers can have the best of both worlds with almost no additional effort. AdaptiveClient is a very lightweight utility that detects if a user has LAN connectivity. AdaptiveClient instructs the dependency injection container (Autofac) to inject the correct service implementation based on the users connection characteristics. For example a user with LAN connectivity may need an implementation of `IOrderProcessor` to submit an order. Because the user is local, AdaptiveClient will inject an implementation that gives the user fast, in-process connectivity to the database. A different user who has only Internet connectivity will receive an implementation of `IOrderProcessor` that is wrapped with REST or WCF calls. The heavy lifting involved with registering and resolving services is done entirely by AdaptiveClient and Autofac. The developer need only write the service and make an entry in a config file. 
&gt;It's maintained, Yeap, actually .Net Framework 4.8 contains a surprising large amount of fixes for Windows Forms.
I mean, there are better options. Winforms is supported, but it's still technically legacy. WPF and UWP are the better options.
Yeah, unfortunately there was no reason for it here. I'll put it down to lack of sleep :p
Yes
I suppose in some environments you would have departments branched so that some would focus on functionality, and some would function on UI. Mine is pretty full stack which can be a pain at times, but most of the ui is pretty simple. 
I second this. I didn't know of this new release until this new post. Thank you!
For me, it's one of the best and consistent themes. I use it on frontend and I'm probably going to use it on Desktop applications too. But if you dislike it, then that's perfectly fine, I see a lot of people hate it because of shadows.
&gt;We have a base on model building extension that sets all strings to 250 chars, unless otherwise set in context or via attribute. Good idea.
Check his/her history, this guy/girl posts this flame war crap all the time. Like Are you afraid, do you hate or does it bother you.....
Your post has been removed. Please read the rules in the sidebar.
Your post has been removed. Self promotion posts are not allowed.
An abstract class *must* be inherited. A static class is sealed and *cannot* be inherited. Can't be both.
That feature was removed: https://www.c-sharpcorner.com/UploadFile/7ca517/primary-constructor-is-removed-from-C-Sharp-6-0/
Yea, not a great time. Big project, set timeline. For smaller apps though where we can afford experimentation, definitely.
What's your definition of legacy, technically?
Have a Reddit silver
My bank account still has PTSD from cosmodb
It seems like they removed it, because they might use it for record types [https://github.com/dotnet/roslyn/issues/6997](https://github.com/dotnet/roslyn/issues/6997)
Hi, got this working.... not sure how to use data from the controllers... can u give an example using SampleDataController? 
How do you suggest working on this project if i want to use api's in the controller? run it with iis in VS2017? i take it 'npm run serve' won't be of any use on its own? how do i get the client side to build on save, to catch vue/js errors? thanks, instructions were great btw 
Why is that? Any usage metrics/data to attach to that? I’m sincerely curious. I work primarily with mongo and sql server, but have been curious to see what cosmodb is about? Did you set up billing alerts? 
[Hmm](https://www.reddit.com/r/dotnet/comments/9veo84/announcing_mlnet_07_machine_learning_net/)
Nice job; I’ll be checking this out for sure. One nitpick though ... is it really a POCO if you have to use inheritance and an attribute ?
I'm mostly exaggerating. I don't remember the exact cost but it was definitely absurd for just having the server idling in a dev environment. I'll see if I can find it
https://docs.microsoft.com/en-us/previous-versions/sql/sql-server-2008-r2/ms189087(v=sql.105) Maybe my previous job had this setup. Looks like it's configured to what you mentioned by default. 
Thanks for reading and commenting - hoping the article helped some developers in similar situations. Regarding your questions, it's kind of a mixed bag. If you follow the instructions in the article completely, you should be able to build the project with HMR through IIS. The template linked in the project comes preconfigured to allow Vue/Js debugging with Webpack's HMR all within IIS/Visual Studio. The VueCliMiddleware that's included in the project's nuget restore allows communication between vue-cli's 'npm run serve' and IIS's root port. An important caveat (included in the article) is that SSL and HTTPS both need to be turned off for this to function correctly - it's the only workaround I've found thus far that allows HMR within IIS - I'd be open to other suggestions, if anyone has any. You can absolutely run 'npm run serve' through vue-cli or just your command line and the project will run fine, with HMR, but you'll lose native debugging features within IIS/VS. In summary, just stick to the steps laid out in the article and make sure that SSL/HTTPS are turned off (this should already be the case by default using my template, but you may need to double-check in the solution's properties). If all of that doesn't function correctly, let me know, and I'll see if I can provide some more guidance. 
Really wish it was truly elastic. It’d be a great Serverless solution if so. 
Yes, but that emulator didn't understand both query dialects the hosted version does understand the last time I checked.
 public static class Foo { } .class public **abstract** auto ansi **sealed** beforefieldinit Foo extends [mscorlib]System.Object { } // end of class Foo If you look at the IL generated for the above class definition, you'll see that it is both sealed and abstract. * sealed prevents you from declaring an abstract class * abstract prevents you from creating instances of a static class Hope this helps. 
You can check the link https://stackoverflow.com/questions/30452104/mvc-5-identity-2-0-lockout-doesnt-work
Thanks for the feedback. I considered including a decision on data access, but at the time I thought it wasn't unique enough to desktop applications so decided against it. On further thought, I think there are a lot of points specialized to desktop apps that I can write, including your input. I didn't know about AdapativeClient, seems very useful. Either way, for some reason this post was removed by a moderator as a self-promotion post. Strange since I'm not trying to sell anything or promote anything...
Not sure why you where downvoted, I agree that all new projects should be started with .NET Core. Even our company's Windows services are made with .NET Core. Desktop apps are not currently supported but that will change when .NET Core 3.0 is released. You can even use some non .NET Standard libraries in .NET Core if you put proper PackageTargetFallback in your csproj.
[removed]
Your post has been removed. Self promotion posts are not allowed.
From my experience of a relatively small project it was INSANELY expensive. I ended up canning the project in the end as a result and rewrite using anything but CosmosDB ... Which is a shame as it was pretty easy to use.
Kind of. 4.8 doesn't bring much, and even that took a long time since 4.7.2. Microsoft have stated that they are going to continue supporting it for a very long time (security updates, new encryption standards, etc.) but you will not see it really getting much love other than that after 4.8 is released. They are now recommending new and ongoing development be moved to .NET Core 3.0 when it is released. One of the biggest factors is that .NET Standard 2.1 will no longer support Framework, so all the new features it brings (Span&lt;T&gt;, for example) won't be usable in Framework. As library developers start using Standard 2.1, it will mean more and more libraries also stop supporting Framework. [You can read more about it here](https://blogs.msdn.microsoft.com/dotnet/2018/11/05/announcing-net-standard-2-1/), where MS are actively telling developers to move to Core 3.0 instead of Framework, as Framework is basically becoming legacy (they're not adding new stuff because of the time/effort of ensuring that doesn't break all the existing applications that depend on Framework) &amp;#x200B;
Never do work what your database engine is designed to do. You do exactly that. So specify the lengths of the fields / sizes that they should have and let the DB handle how it's stored. This gives the benefit that if you store a value that shouldn't fit *according to domain rules* your database will let you know. Now you have to program these rules yourself. There's no benefit of doing this yourself, as your 250char requirement might lead to more pages used, and thus slower overall performance. I also find it odd you define decimals to a scale of 2 for a finance system. I'd assume at least 4. 
You can setup a local cosmosdb free to play around with. It's actually quite easy to setup with your .net core application with documentdb. I'm not at work atm and just laying in bed else I would link you to some resources I've used. I started work on a new cloud services platform and i tried cosmos locally. I liked it a lot but unfortunately I ended up using SQL server because it made more sense working with the kind of data out customers work with (medical field, lots of relationships). (CosmosDB Emulator)[https://docs.microsoft.com/en-us/azure/cosmos-db/local-emulator]
&gt; What is a good practice in this case? Specify the length. EF otherwise will use maxlengths on string variables, like 4000 for unicode/utf8 strings. &gt; How do you usually handle this case? see above &gt; Besides string, do you have any precaution with other types? precaution? Rule of thumb: always specify what you *know* are characteristics of your field. If you don't, the framework will assume things based on the .NET type. a property of type string has no length max, so it assumes the max. A decimal in .NET has no precision/scale, so it will use whatever is specified in the value. &gt; I'm worried about space, don't how if not specifying a max length size implies in a bigger db. Shoul I be? No. Specify the characteristics of your entity model. that's what you're doing. The DB then follows from that (and code too, but EF with code first forces you to do that by hand). Size is something you should worry about if you have a massive amount of data that doesn't fit in memory. Unless you're amazon or google, chances are your database fits in memory of a big server so it definitely fits on disk and is reasonably fast to consume. Be more worried you define the right indices / FKs / PKs / UCs and the right business logic. 
I see, I must have misread. Thanks!
I have a cool web app that I built on the CosmosDB graph API that I was thinking about hosting someday, but if it is as expensive as everyone says then that'll never happen. That sucks.
Azure's floor is way overkill for 90% of my use cases.
Shit, I'd love if we upgraded from VS.Net 2012. I only see core being used when forced to because old framework gets retired.
We're using it solely where we want to run things on AWS Lambda/ECS/Batch on Linux. Beyond that we're in no hurry to migrate nor do we currently see and major benefits to doing so. 
All new apps are built on core, and we are slowly migrating legacy apps away from framework. No need to stick with the framework since it's eol at 4.8 anyways. 
We're all in with Core over here. Any new apps are created in Core and we're working on migrating any existing apps if we have time, but not in a rush to do so.
Ah... Its not. Most of our apps are still .NET framework. We are supposed to write any new apis in core however. 
As a contractor I generally try to only take gigs doing the latest stuff. But sometimes I have to take gigs using the old stuff. In those cases I usually try to convince pl to use the new stuff. Been like this for years. Love .Net Core btw.
I second this. Like it or not, core is the future and is generally a better way to build modern apps. If. Your company is not willing to em race that, I'd look elsewhere, especially if you're committed to the .NET ecosystem. Five years from now nobody will be doing anything but maintenance work on framework code. 
As developers, we naturally want to always work with the latest technology. It's exciting, often means doing away with boring boilerplate code we don't enjoy writing, keeps our resumes up to date, and helps with that constant nagging worry that we're falling behind the trends. As *professional* developers, we have to do what's right for the business. This means that any potential migration has to be thought of as a business case: here's the benefits, here's what it will cost, and here are the risks. If there's no practical reason to migrate that outweighs those costs and risks, then there's no business case, so don't migrate. Managers must think this way if developers don't, which is why developers often consider them to be very conservative. (I'm both a manager and a developer, so I get to have that conflict internally!) There isn't really a problem here per se, but there is an ideal compromise between these concerns: don't incorporate new stuff into production code just because it is new, but do make time for devs to explore the new stuff -- either pure research or in non-essential side projects. This way devs are at least fully aware of what the possible benefits are, so that they can make the best recommendations to the business -- that's the case to make to your manager -- and as a side benefit, get to stay motivated and keep their resumes shiny.
All new apps are core. New libraries are standard.
All new projects are done in Core and work is on-going to refactor older stuff to use Core. Sounds like you need to move to a different position.
Oh. I make sure Target manifest is set to false, I always want it to pull and use the version of a nuget package that I coded and tested against
Thank you very much for availability to answer my question. This community (dotnet) is helping alot in my C# study. 
You're welcome :)
I think it's harder to justify transitioning exiting systems over. For my team, we are deprecating our legacy app completely, so we were able to go for an entirely new stack (.Net Core, React, etc.). In our case it was easy to justify since were didn't need to make any business cases for migrating everything over. In my experience, that'll almost never make sense from a business perspective and it's really hard to convince people otherwise. Your best best if to try to find a different team thats working with the things that are interesting to you, or unfortunately maybe even switching to a different company. 
Have you checked this? https://www.nuget.org/packages/Microsoft.AspNetCore.HttpOverrides/ 
Like many of the replies here... we are writing new apps in Core, with no intention currently to migrate older existing apps. We are, however, making .NET Core and its principles a big part of our tech strategy (DevOps, Cloud-first apps, etc).
We are a SharePoint (ugh) consulting firm but we recently did a project where we used .Net core to build a modular solution for reading sensor data, storing it somewhere and giving the user the ability to configure/view pretty much everything. I really liked using it, but as we mainly do SharePoint, I doubt that we will use it again for the foreseeable future.
Our company went all in. Just being able to run it in linux machines drastically lowered our azure bill.
I work for a very large company that you have heard of and we are authorized to develop on core. Last i heard the server people were creatong an install package and we can deploy as soon as its set up (it may already be setup) 
tl;dr It sounds like lockout is dependant on a configured date and AccessFailed has nothing to do with it.
Easy: We are not. :D We're building a GIS application on top of MapInfo and they moved to .NET from a pure COM interface like just a few years ago. ;) The Industry™. Seriously though, I think they were just unlucky. A big transition to .NET and WPF, all that, and then Boom! pretty much the same year! So it's mostly because of we're depending on major components that are not there yet.
Good advice but I would also add a step before this to change to PackageReferences from packages.config
Oh yeah, I forgot that `PackageReference` is not mandatory when changing to new style csproj. I would do the switch in the same step as changing csproj style.
We're aiming to write all new stuff in Core unless there's a good reason not to. Currently no plans to migrate anything existing, though.
Yep, that's pretty much how we're handling it. When .NET Core went to 2.1, I asked my boss if my new dev could be .NET Core. The workflow and productivity has been intensely better for me, which makes my boss and I happy campers. Some stuff still has to be .NET Framework only because of our other Microsoft systems.
Microsoft has definitely established that .NET Core is definitely the future for them. Benefits are really rather project-specific. As you've said, performance is probably not that important for you. Deploying to Linux? Maybe, maybe not. You have to ask where do you want to be in ten years? Still using .NET Framework, long after MS has stopped making any improvements? Will yours customers be in the cloud? Will Linux-based containers in the cloud be much cheaper? You wrote that rebuilding everything for .NET Core would be "a huge task". To help you get an idea of where you stand, Microsoft has made a pretty good tool called the ".NET Portability Analyzer". You can choose which projects and which targets you'd like to analyze and it produces a report. You'll have to decide for yourself what the benefits are (and then perhaps convince your team/boss), but you can use this tool to pretty quickly find out what the costs would be. Maybe some of your code can be retargeted very easily. You could make the goal to write new code as .NET Standard/Core and re-target to .NET Standard as you go. The new software can still interact with the older libraries. You'll have to use at least .NET Framework 4.6.2, but then it's pretty smooth sailing.
To be honest even before core I've been upgrading stuff as I've gone along. Technically it's in our team's coding standards to upgrade to latest version of Framework, but only a couple of us do that. The basics of upgrading packaged to latest Framework then netstandard is pretty pain-free. Even going from Framework 2.x to 4.7.2 is often a doddle. Upgrading Framework apps to Core like web apps etc doesn't happen as often, but it does happen if someone has to do some really fundamental work on apps because coding Core is just much nicer anyway. We have like 200 repos though. We're not going to upgrade everything to Core else by the time we do it Core 3 will be out. 
You can make a cost argument - operating Windows servers are usually x2 more expensive than Linux due to licensing costs, even if you compare cloud offerings. The argument shouldn't be do we need to rewrite everything to core, it should be whether it makes sense that new initiatives are built on core and integrated with existing projects. This may involve some level of refactoring to improve existing architecture but not a rewrite. Then there's also the argument about cloud readiness. If your code is very tightly tied to windows specific API your architecture probably have things in it that make it not cloud friendly, so you can't leverage the new cheap, agile hosting methods. It would be a huge mistake to ignore the operational aspects as they represent a big part of the overall cost of the project - if you can't leverage new cheaper hosting platforms, eventually you will get a disruptor come into your area of business that is able to do the same thing, better and at lower costs and your company will be struggling to stay relevant. 
You can also multi-target as an alternative.
We (the development team) are not giving the business a choice. Our platform is built on classic ASP. Most of the development team has joined within the past year, and nobody has been with the company for more than two years. Fed up with the decision to rewrite being pushed back (despite it being the reason I joined), we jointly decided to over-quote the time required for the last change request, and rewrite one of the less business-critical applications in Core. We figure they can't sack the entire team for doing the job we were employed for. 
Immo Landwerth/terrajobst of Microsoft recommends upgrading to net472 https://twitter.com/terrajobst/status/1031999730320986112?lang=en I've had problems with especially `System.Net.Http` and `System.Threading.Tasks.Extensions 4.4.0+` and `system.IO.Compression`. https://github.com/dotnet/corefx/issues/32276 https://github.com/dotnet/corefx/issues/17522#issuecomment-338418610
We began our big new project in core. Existing projects are being migrated to a class library targeting dot net standard 2.0. legacy and core both consume the class library. Legacy must continue to work, and core is slowly built out to perform the same functions as legacy. Once we have a core replacement we stop using legacy for that service. So legacy -&gt; class library -&gt; core replacement -&gt; remove legacy
We will use .net core for new projects, but we are never going to migrate our legacy code.
Do you use App Service on Linux or is it something else?
And here I am just trying to introduce LINQ to our codebase. Scary new world for the seniors, I tell you what.
Of course, and that's a factor you should take into account when doing the analysis. The point is that it has to be a decision which takes into account the benefits versus the cost and risks at the business level. Or you can just be like every other dev shop in the world, and advertise that you need a .NET Core architect with 10 years of experience and a specialization in neural networks and quantum computing, expecting all candidates lie that they have *15* years of experience with that shit, then hire the one with the nicest haircut and put them to work creating accounting systems in VB6.
And here I am just trying to introduce LINQ to our codebase. Brave new world for the seniors, I tell you what.
&gt; So, in practice there's no real reason for us to migrate. Sure, Core seems to perform better but most of our websites don't have a lot of users or traffic since they're private websites for other companies, so performance usually is a non-issue. And other than that, I can't make any real argument for why we should switch, other than 'it's new and shiny'. So in ten years time when you or your successors are dealing with an outdated codebase that had no effort applied to keep it up to date?
4.6.2 does not play well with netstandard2. There are a lot of issues that happened already. Even the team at Microsoft took the blame for it because they tried to make life easier but it backfired. If you want to consume netstandard2 from 4.6.2 apps, either upgrade to 4.7.2 or use multi-targeting on yout netstandard libs. 
We use app service for our publicly hosted sites and apis and service fabric clusters for the business logic part.
I think that's where Continuous Improvement comes into picture. You are right, the business is never going to pay for unnecessary and expensive rewrite. But this is where we devs need to stand up. We need to keep upgrading the technology stack without impacting the business deliverables. It cannot happen in a day and its a journey. But we cannot not do it...
The selling point for NET Core is cross platform. Running on Linux hosts, or even containers. It really doesn't make sense for anyone to completely rewrite a whole system just for Core unless they specifically need to move hosting to Linux. A more common path is a gradual transition where new services are hosted on Linux with NET Core while the rest of the old platform stays on full framework until you've entirely replaced it with services. If you don't need the scale of services and your hosting bills for Windows aren't killing you, there's really no reason to change. 
to be fair some people go too into linq and are just dicks about it. Code readability while drunk is the real test. If I am drunk and it takes me a bit to understand exactly what the code is doing then just don't use linq like that :) Ballmer for life.
and jquery.
you simply make NEW projects core and just keep plugging away on the old. Its not THAT old unless you have that webforms crap or winforms. Even then why would you bother?
Well said. It’s all trade offs
Unless i can get debugging to work through vs2017 i can't see the use anymore... unless i'm missing something otherwise you can just create a separate asp.net core project and a separate client with vue cli, and just treat them as separate projects... What I want to be able to do is hit f5, and have HMR.... I just get "The vue-cli server did not start listening for requests within the timeout period of 50 seconds." even after multiple retries
Until .NET Core gets the EF 6 tooling, including designers (not planned for 3.0), and we get all the third party libraries to actually care about Core, it is going to be .NET Framework for the foreseeable future.
All of our current code is on .NET Framework. New apps will be written to target Core but there is no business case to migrate existing .NET Framework apps. We have our own servers so the cloud isn't a selling point (for now anyways)
So far - we're not, at least not beyond some very early sandbox POCs. **But**, my boss does want us to be on .Net Core. Mostly because he knows that the full framework is reaching the end of new development, and even more importantly, we can run core on Linux, which will cut our licensing costs dramatically
PackageReferences in older versions of .net framework still feel very broken to me. I tried to convert a project over to it and if you have any requirements to stay on 4.6.2 or lower then I would avoid the crap out of it. It caused all sorts of issues especially when package changes required binding redirects. 
Lucky for the next person looking at my code (which is going to be future me), I comment my code nicely as to the purpose of my queries. Thank you, past me.
The way the question is worded is a little confusing, if Projects C and D were just two projects in the same solution you'd just reference and make normal CLR function calls. I *assume* you mean you want D completely isolated (new solution, project, and application space). As to your questions: - How do I map 'api/X' and 'api/Y' to two different applications? One of two ways: Either full reverse proxy (nginx, HAProxy, etc) or the use of IIS's URL Rewrite module. In both cases you set up the different paths as full applications with their own unique hostnames (e.g. y.example.com, x.example.com, etc) and then dynamically redirect requests for e.g. example.com/api/X to the desired host. This is transparent to external parties. - How do I handle internal coms between project C and D? There's infinite answers, WebAPI with Machine.Config keys, filesystem files, localhost, a Queue system (e.g. Apache Kafka), database records, etc etc. Hope that's helpful. 
This is the same approach we are taking. All new apps are .net core of course, as well. Large corporation here too, for comparison, so it's no small feat, but leadership is on board.
In my team, most developers are not really web developers. When they search a solution to an [Asp.Net](https://As.Net) problem, they often end up with solutions that bypass MVC conventions, like dealing with HTTPContext through System.Web.Http instead of the OWin context. In MVC Core, there are less way to do things and most of them rely on the same tooling. It seems to me moving to [Dot.Net](https://Dot.Net) Core is a good time to tighten up the tooling and coding practices. We don't depend on internal libraries and migrating existing projects is out of question. We are slowly rolling out new projects using [dot.net](https://dot.net) core and everything is going smoothly.
We have a hard dependency on SignalR, so we waited until Core 2.1 and then made the switch. It took maybe a week to convert the projects, test everything, and get it into production. There have been minor hiccups but it was rather smooth. Our platform isn't massive, but it's not trivial either. It started in ASP.NET MVC with a host of console/library projects included, so it wasn't that far away from Core.
No!
Aren't static classes abstract and sealed at the same time?
They are in IL, as explained in the other comment. In C#, those modifiers are not compatible.
We have been doing net standard for quite some time now and we really like the way it is going. In fact, we are doing a new initiative where we are targeting Windows and Linux (with the idea the software should build and run on Win/Mac/Linux for any kind of developer). It works well, and is fast. We love it. 
I laughed... Then cried a bit.
Yes but in the context of this thread we would be upgrading to 4.7.2 anyway There are other side effects - older packages that load resources e.g. JS and css files don’t load correctly. 
Yea we didn't even get that far in our testing before rolling back. But even with upgrading to 4.7.2 it seems like an added risk for just developer and build time performance benefits as the PackageReference method isn't required even with .net 4.7.2 or the new project format. It seems like the best course of action is to wait for packages to catch up to supporting the new techniques and upgrade down the road but to develop new projects and application with PackageReferences from the beginning. 
I must say, it's very unprofessional to- wait - what? CLASSIC? As in vbscript or jscript? Jeet swesus, man - it's forgivable to strap a bomb to the server. Though interestingly, with classic, we were able to do server-side jscript a long time ago ('90s). Node comes out, and everybody soils themselves in amazement.
Interesting. We've had issues with System.Threading.Tasks and System.Net.Http, but have solved those with BindingRedirects (which I found to be annoying and balky, but seemed to work). Thanks for the tip.
I can only speak for our solutions, which are non-trivial, but we really have had more-or-less smooth sailing (barring some bizarreness with binding redirects). However, I'll take the suggestion for 4.7.2 to heart ... hopefully that would remove the rather large binding-redirect section in some of our applications involving system assemblies. Thanks for the tip.
Kind of relevant: Do you want to keep competent developers or do you want them to get bored and quit?
It is now a lot more consistant on pricing than it used to be. It has changed from per collection pricing to per database pricing : https://feedback.azure.com/forums/263030-azure-cosmos-db/suggestions/17893705-allow-database-level-pricing https://docs.microsoft.com/en-us/azure/cosmos-db/request-units. 
We are pretty much doing the same thing except our step 2 involved adding integration tests / unit tests to each project to assure it works as expected. After having worked with core for the previous year, I actually hate .net 4.7 now. So I've been working pretty hard at getting us to the next stage and after 5 months we are about 80% of the way there. 
Have you experienced problems migrating that unit tests have caught? If yes, what type of problems did they catch?
&gt; OP themselves says they can't give any reason to migrate other than it's new, that pretty much says it I don't think that's true though. In the long run, .NET core will cost less to run because you can run it on cheaper Linux machines. And you can sell a straight-up cost saving to the management. 
I realise this wasn't exactly your point but I can give some good reasons as to why a company should upgrade if its possible (in order of importance): * .net core is the future, incremental upgrades to your technology are a lot easier than trying to do it 10 years down the track. * You can start now and implement an upgrade path that will minimise the impact on current development. * If all of your dependencies work in .net standard then it's really shouldn't be that hard. * If you want to keep good developers upgrading to core is important. * If you want to hire good developers upgrading to core is important. * It is simpler so general development time will decrease. * Cheaper hosting options are available in core. * .Net core is fundamentally a hell of a lot better. Microsoft has focused on cleaning out the crap that has evolved over time in .Net 4.# That said, if you have a static application that doesn't need much future development I could understand why you would not upgrade. &amp;#x200B; n.b. Another way to look at it is, it's just a part of refactoring, why even mention it to management? On the sly convert your projects to .net standard as you go. Then a year later you can put forward the argument to move the remaining projects to core. &amp;#x200B;
You poor bastard.
Sounds like a team I would like to work with.
The more you learn new stuff, the easier it gets to learn new stuff and to be honest its getting so much better. I look back at .net 1.0 and cringe at how bad it was.
I would also like to add that the node devserver proxy support is quite handy when working with .net core and vuejs separately. You can even proxy websocket connections between your vuejs app typically at localhost:8080 and aspnet core site at localhost:54321 during development.
You just said there is no business case for migrating everything over. Then suggested the developer quit because they are not. I think you just highlighted the first business case for migrating :).
Oh yeah! Upgrading to 4.7.2 is the best thing for sure. The suggestion is more useful for library authors or in case you really can’t/don’t have time/resources to upgrade but still want to start migrating your small libs to netstandard. 
It's an async call, use await. Why do you not want to use await? Using .Result on something that's async is really bad.
Because then wouldn't I need to make the method async and then any method that calls it needs to still do something like `.Result` or also be turned into Async and then before you know it half the web app needs to be changed?
Finally someone with common sense. 
So you're going to replace the machines in the factory just because there's a new version available, not because they're old/worn out?
Why's that bad? Did the bits in the webforms code suddenly start to rot and fall out? No. It still works like it did when it was written. It's as if MS rewrites windows in Rust because C is outdated. 
&gt; Using .Result on something that's async is really bad. Bad as in "I'm a rules &amp; processes kinda guy ane it breaks the point of async in the first place and you should burn in hell for butchering the framework", or bad as in "you're fucking shit up, seriously, stop doing that, things are no longer predictable"? Cause I can live with the first one.
10 years ago, visual studio 2008 was new and shiny, .NET 3.5 was just released and Linq was brand new. No-one can tell what the landscape is in 10 years time. What does 'keeping it up to date' mean, anyway? Keeping it running? MS will support .NET framework in windows till the end of windows. As long as it runs on windows, it runs forever. 
Correct, lol
Bite the bullet and convert everything to async/await. There's nothing to be afraid of, and any modern app should be using it anyway.
Bookmarked :)
.NET Core !... that sounds fancy ... i'm still trying to convince the boss that XSLT is not the best language for HTML / databinding :D 
Yup
You should just use synchronous api in that case, \`.Result\` and \`Wait()\` are not meant to achieve this. Also, if you are using AspNetCore, you are killing your application with synchronous calls.
You should. But if you don’t want to, then you can make a single async method which returns something, call the async call you need to call with an await method().ConfigureAwait(false) inside your new async method and then return a value, then from your current method call your new method and .Result on that. 
Both have their uses. EF is great for making simple queries but stored procedures allow more control of how the data is actually fetched. 
By the time you get an Angular 2 book shipped, Angular 6 is out.
I like not having to write SQL code.
Can query based ef calls provide the same control?
That's interesting as I didn't know .NET Core had such an adverse effect on synchronous calls. It is not AspNetCore thankfully, just ASP.Net MVC Framework 4.6. It seems that the auth0 client only has Async methods, else I would have used a synchronous one. Not sure how to get around that. 
it seems a bit weired to compare ef with stored procedures. A comparision of ef and sql in gener makes more sense imho
Read the thread. Everyone who has commented so far has told you to use async/await, and I provided three links that describe in detail the risks that come with using `.Result`. In the real world we are concerned with solving problems in such a way as to minimize side effects, not with fitting tasks into an arbitrary time-box provided by your boss. You asked the question, now you're arguing against several more experienced people because their answers go against what you want to do.
I understand that await/async is better in general but you cannot simply change a whole legacy web app because of 1 small use case. This feature isn't even going to be used by the general public (it's an admin button to remove a user from auth0). It is completely unreasonable to make such a dramatic change every time some new technology arrives. I am debating this because people are dodging my question of how to use cancellation tokens properly/check for timeouts in a synchronous manner. None of this has helped explain how to use cancellation tokens effectively.
Some, but SPs will always have the advantage that all the data manipulation they do never leaves the database. For a lot of things you do that’s not going to matter, but especially if you’ve got a large database and you’re moving around large amounts of data (such as for an ETL process) it’s always going to be more efficient to keep all the data on the database server rather than pulling it back to your web server/desktop app/whatever is running EF, manipulate it in C#, and then send it back to the database. Of course a lot of the more modern design patterns exist specifically to make this less of an issue by focusing on pre-generating the data you know you’re going to need as the source data comes in rather than loading raw data and then manipulating it (ie: as you read the file to load the data in an ETL process go ahead and do whatever computation and transformation is required before you ever insert the data in the database, rather than loading the raw data and then later using an SP or pulling it back out to transform it as a separate operation). So... it completely depends. 
EF for CRUD operations and simple joins. Stored Procs for speed, joins on larger datasets, multiple result sets, pivot tables, and in some more database driven solutions dynamic SQL. 
Past a certain level of complexity, you'll find yourself fighting with EF to get it to generate the SQL you want in your queries. EF does have its benefits though, especially if you're going code first first because then you can get strong typing over your database models without having to worry about keeping them in sync with the database. I find that if you find yourself writing very complex queries in EF it's better to either split them into multiple queries or adjust your data model to make them simple queries. At the end of the day, the 'right' choice between the two is highly dependent on the requirements of your application and your personal preferences. Also, it's an absolutely valid approach to code mostly in EF and augment this with some complex views or stored procedures
I just finished writing my book on all the cool things you can do on Angular 4! by time it makes it back from the publisher we'll be on 12!
 class Program { static void Main(string[] args) { SomeSynchronousWork(); Console.ReadLine(); } static void SomeSynchronousWork() { var cts = new CancellationTokenSource(millisecondsDelay: 3000); Console.WriteLine($"Doing async thing for a maximum of 3 seconds"); try { Task.Run(() =&gt; PretendDeleteAsync(4000)).Wait(cts.Token); Console.WriteLine($"Async thing done."); } catch (OperationCanceledException) { Console.WriteLine($"Async thing timed out."); } } static async Task PretendDeleteAsync(int time) { Thread.Sleep(time); } }
That's perfect! So I can use async without needing to drastically change anything. And I should have used `CancellationTokenSource` rather than CancellationToken` :) Thank you. This is what I was after. Much appreciated.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/visualstudio] [Did anyone use the project template 'ASP.Net WebAPI Application with OWIN'?](https://www.reddit.com/r/VisualStudio/comments/9zf5cg/did_anyone_use_the_project_template_aspnet_webapi/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
Note that this is still bad code; the async task will continue to run after the main thread believes it has timed out, which will leave you in a state of not knowing whether the call was ultimately successful.
Yes! EF has a learning curve. But I find now that it saves me so much development/maintenance time on most of my database interactions. I do see a lot of people try it, and give up too soon. Once you get past the tipping point, it is pretty great. Note: You can have EF work with stored procedures, so it isn't an either/or question. Basically, the SP does the SQL logic, and EF can do the parameter mapping in, and the model mapping out for the results. Still a time saver most of the time.
Oh yeah definitely, it's great for rapid development especially when developers are more comfortable with writing code and not so much T-SQL. Great point about executing sprocs with EF, we tend to do that as well instead of using the SqlCommand approach that way our data access layer is more consistent. 
Yeah, I use EF for any single record read/write, but for a list of records I typically put that into a view. I can write my view to be much more performant than EF can.
Neither. Use a nosql database like mongo or couchbase. No need to normalize all of your data into tables when all you really want is an object graph. 
It's impossible to say that stuff from that template is useless without knowing what you're trying to do. It's like telling a woodcutter his saw is useless without knowing what kinds of wood he's gonna cut through.
I've done something similar, but for a complete reverse proxy. You can just take the headers part out of [this SO answer](https://stackoverflow.com/questions/52393880/asp-net-core-2-act-as-reverse-proxy-usering-rewrite-middleware/52414657#52414657) 
I don't have to put my EF queries through a pull request with the DBAs.
That seems like a bad situation. It's the same database in either case
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/csharp] [WPF App Deployment](https://www.reddit.com/r/csharp/comments/9zfx6m/wpf_app_deployment/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
The problem is that you seldom only want an object graph. Once you have collected a lot of data you generally want to do some analytics on it. That's where RDBMSs shine and document databases really stink. 
Why don't you use Click Once? It's built into Visual Studio and is designed for easy deployment and updates of things like WPF apps. You seriously just publish to a file location, then the application automatically checks for updates there when it starts.
It's usually because it's a change to the database rather than a change to the client code. With client code you add no artefacts to the database. My business doesn't make the distinction that /u/herple_derpskin does, but I've been at companies where it's not just a pull request, it's a weekly meeting attended by info-sec, dba's, product owner, etc :/ 
This reminds me of one of the temporary hacks we have in place. Due to the complexity of some of the existing code (and, the Core version of Microsoft.VisualBasic is still missing some functions) we're having to leave some of the code in ASP. However: because ASP allows VBScript and JScript to be mixed, we are able to use a standard JScript JSON library to hide the functionality behind REST services. VBScript takes the input, uses the library to parse it into an object, processes it and then uses the library a second time to stringify the result.
You can use EF to call an SP and materialize records from the result. I assume by "query based" you really mean LINQ though.
You, sir/madam, are a genius :)
There is a fine line between genius and madness.
There is - I've worked with people before who've been scared by things as tame as recursive methods, though: I'm my opinion, coding only gets interesting when you push your luck.
If I am a DBA and the database is my responsibility then I will never let anyone touch the database. They have to tell me what and why before doing so. And don't let me starting ranting about how many duplicate columns and shitillion useless indices in this CRM database I have to maintain.
You are right. However, every time when you create a new web-service project you have to write too much boilerplate code. Especially if you use microservice architecture. The template that I mentioned do it for you. As I can see there are about 1k downloads per week so I want to get some sort of feed back. Did you use it already? 
I mean [this](https://marketplace.visualstudio.com/items?itemName=sergey-tregub.asp-net-web-api-owin-template) template
I think maintenance is often overlooked when talking about ef. In my experience keeping track of changes etc. can be a nightmare when dealing with views and sp's. 
I haven't yet used the ASP.NET template, just the .Net Core version of it. I'd advice you to use that instead of traditional ASP.NET. &amp;#x200B; Basically, the .Net Core template is much cleaner as it has OWIN controllers, views and models embedded directly into the .Net Core framework so they don't clutter your solution unnecessarily. You can override stuff if you need to though, so extensibility isn't a problem despite it being "hidden". Never mind the fact that .Net Core will be the future of .Net and will eventually replace everything in the traditional .Net framework. The docs for .Net Core are also much better written.
I haven't yet used the ASP.NET template, just the .Net Core version of it. I'd advice you to use that instead of traditional ASP.NET. &amp;#x200B; Basically, the .Net Core template is much cleaner as it has OWIN controllers, views and models embedded directly into the .Net Core framework so they don't clutter your solution unnecessarily. You can override stuff if you need to though, so extensibility isn't a problem despite it being "hidden". Never mind the fact that .Net Core will be the future of .Net and will eventually replace everything in the traditional .Net framework. The docs for .Net Core are also much better written.
Thank you was wondering if it had anything to do with the .NET version. 
Ok, thank you for your opinion 
I think the key is deciding on your strategy and sticking with it. If you do code first, ef migrations makes this a breeze. If you do database first, there are generation tools that also make this easy. I usually go code first, but there are tools to make this not a nightmare on both directions. But... if you mix strategies... good luck.
I use squirrel for my application and I host the installer on S3. Squirrel has some quirks but I've been happy with it. I've always heard click once was shit so I didn't even explore it. 
We're slowly moving away from EF and using dapper. Greater control over the actual query being made.
Yep 
You can execute raw SQL in EF.
Mostly we refactored / cleaned up the libraries at the same time. e.g. seperating business logic from integration logic and moving each external integration into its own project. Being consistent with dependency injection. Removing duplicate code. A couple of times we had to change libraries, so in that case they would have caught migration problems. But generally the tests were more to assure we didnt break anything rather than directly testing the migration. These kinds of wholesale changes are risky.
Lol, I am totally with you :) I would hire someone who's sole job was to slap any developer who added a column named after a reserved word. We have a process, but it's within our team. Our DBs are all closed to other teams, but open through APIs, so we maintain control. Honestly though I've pushed for at least a consultant DBA for a bit. We've got a bunch of talented people, but I legit miss being able to go and discuss a weird query with people who know it inside and out.
This just seems like a single specific scenario where you can cause an issue. The MS link still suggests the .Result way as *an* answer, just not the best answer. This whole "you must change it all to await" just comes across a bit doomsday-y. I'd liken it to the process of filling up a car. You guys are telling me I should NEVER EVER remove the cap off my petrol tank, because it allows access to a dangerous liquid and I obviously can't be trusted to do it properly myself. Instead, I should always allow the experienced employee at the shop do it for me. Sure, there is a certain amount of risk associated with filling up my own car. But if I'm aware of those risks and and don't act like a complete moron, then it is actually still relatively safe. 
And if it becomes slow we will use Hadoop, and horizontal scale a few more EC2 instances
Sure, but Dapper is 10,000 times smaller. And usually way faster. Reducing dependencies is good. I had an EF statement generating 7K line sql query at work. Reduced it to 30 lines using Dapper's multi reader. Plus, it's just easy af to learn and use. EF is an ecosystem. Dapper is intuitive and simple and just sits on top of sql libs we already know. 
EF Core gives Dapper a run for it's money when it comes to speed. Even the behemoth that is Stack overflow uses EF Core. It fantastic for basic CRUD stuff.
So here is the thing.. A software is not a factory machine which comes with expiry date.. But to your point I will keep it oiling and replace worn-out parts of my machine so that it gives me the best possible performance any given time.. 
You shouldn't be running analytics queries against your production transaction data anyway....
Your approach with Wait() should work, but I think that you will get exception in case of cancellation. The problem that I was referring to with AspNetCore is called "thread pool starvation" if you are interested. Basically sync calls can block all available thread pool threads making it impossible to serve new requests. 
It's extra hefty initial load time.
Tried Entity Framework a few years ago and within hours my "entity model" seemed to get out of sync with the database and I then spent hours trying to work out how to get the thing working; i decided not to. Since then I have built my own ADO abstractions which takes away loads of the work and can write a full CRUD operation for a model in minutes. There are loads of better developers than I on here so their opinions will carry more weight, but having a reasonable understanding of what is going on under the hood seems in terms of SQL seems more preferable to abstracting out and creating another layer of issues to deal with. I would have thought that knowledge of SQL would be one of the first things that any developer should be aware of, yet I get the impression that this may not be the case.
Can you use .GetAwaiter().GetResult() instead of .Result? 
I wrote a helper method to hide the looping
Something I always keep in mind on this debate and applies to any ORM for the matter, it's how easy will be for your application to change database engine, I know that it might never happen but if it does, you only change some configurations on your ORM and that's it, instead with SPs at best, you run your create scripts once, at worst the syntax isn't compatible and you have to check and change every single one.
Does that mean that Dapper isn’t good with more complex statements? I’ve been using EF for a year and a half at work and have heard of Dapper but never gotten to use it or look into it. I don’t want to turn this in to a “pros and cons of dapper” thread but I am interested in learning it if you have any resources or anything (other than the docs of course) 
It's still great. The main reason to use EF is for easy CRUD action and change management. SO uses both EF and Dapper. They use EF for the quick and easy stuff and Dapper for the more complex stuff.
What most others said(EF good for most stuff but more complex stuff in sps) but I prefer Table Valued Functions over stored procedures.
Not a Black Friday deal, but Digital Ocean and Linode are sponsors of the Developer Tea podcast. They have some promo codes you can put in for some free credits. Digital Ocean: https://www.do.co/tea/ - $100 free credit Linode: https://promo.linode.com/developertea/ - $20 free credit I'm gonna recommend the Developer Tea podcast too. Really cool stuff about development work and psychology. The format is short and concise so it's easy to get into. https://spec.fm/podcasts/developer-tea/
I sent you a PM. I have an Azure VM setup with a control panel and everything else you would need to host a website. I can host your site for free. I’m using my $150/month credit from my MSDN Subscription on this one VM.
They have a free tier for the web aspect of that set up. You can get a good idea for your specific needs here: [https://azure.microsoft.com/en-us/pricing/calculator/](https://azure.microsoft.com/en-us/pricing/calculator/)
the actual question isn't worded this way.
Digital Ocean guides are pretty well renowned for setting up and configuring Linux web systems. That gives me a lot of confidence in this podcast. Thanks for the recommendation.
Just want to chime in here and say Linode has been awesome. I use them for small personal vm and they've had fantastic uptime and have upgraded my account a few times for free. Their management UI is nice too.
(I think) he’s saying that you’d move the data to another place and then run the analytics so you’re not affecting production. 
Aws free tier
I've seen this argument made (that EF makes CRUD operations really easy)... but if you have to _sometimes_ use EF and _sometimes_ write raw SQL, wouldn't it just make sense to go raw SQL in stored procs all the time? CRUD operations aren't hard to write in pure SQL either, and since you'll already have the framework and pipelines available, it shouldn't take much more time at all than just making calls via EF. That way, too, it's easier for new devs to come in and understand; they don't have to ask the question, "Is this too complicated for EF?"
Correct.
In 10 years of software development I've never changed databases or even planned on doing so. Even the thought of having to worrying about changing databases in the future is a waste of timeet alone designing for it 
not aware of any deals, but I'm quite happy with my A2Hosting. Leagues better than my previous one. 
I m into the same stack u describe in the post and personaly i m looking to Host my aspnetcore app whit angular as a frontend on docker, that way u can get a hosting whit Digital Ocean strarting droplet for 5$ a month hope it can helps! I think u can dockerize the SQL image on the same server u Host the app. 
Yes, absolutely agree :)
We only use stored procs for dealing with our hierarchical tables because we need to use recursive CTEs for that. There are some nasty ef queries out there but so long as you dont try to read the ToString version its fine... 
Or use something like NSwag to generate the service for you?
Wrong sub bro. 😉
ORM's shine with object graphs - aggregates - moreso than with single objects and simple CRUD. You appreciate that ORM when you need to perform an operation and you need to load, say, and Account with it's dozen or two other related objects, a handful of which all end up modified. How often are your modifying operations really just CRUD within a single table? It's common in tutorials and demos, but not so much in the real world.
We're simply throwing out all the old code and take the time to rewrite everything from scratch. In the process we throw out all the old shit we don't want to keep. In my company we were kind of overdue to update both frontend and backend so now we're simply taking some time to do it
I don't use either but my preference for avoiding stored procedures is to keep my codebase database agnostic.
Azure free tiers will suffice in most of you needs and SQL is around 5 a month for me
Wait what
How do you handle things like refactor/rename when using stored procedures? One of the things I like about ef is that I can be positive that my mappings match my column names.
if you really have to use Synchronous method than use it like this: public void MyAuth() { var cts = new CancellationTokenSource(3000); // Timeout after 3000ms try { _client.Users.DeleteAsync(auth0UserId, cts .Token).GetAwaiter().GetResult(); // OR if the library is really bad designed, and the previous action leads to a deadlock // Task.Run(() =&gt; _client.Users.DeleteAsync(auth0UserId, cts .Token)).GetAwaiter().GetResult(); } catch (OperationCancelledException ocex) { /* Handle the timeout */ } // Do all the other stuff after deleting the user } &amp;#x200B;
^ this No stored procedures for me. If I have performance problems with an SQL query generated by a library like Entity Framework, I optimize by writing my own SQL query. But I don't want to take a dependency on a database. I'd rather take a dependency on a library or on SQL itself.
Maybe it depends on the type of project. I work at a startup with a microservice architecture. We have a system that's always changing so we use microservice to make changing bits at a time more manageable. And part of that change is to be move around between different data store technologies whenever we need to. So far, we store our data in NoSQL data stores, but the concept is the same. We have to think about being able to switch. We do that by having a well defined service layer between our controllers and our data. If I were using EF, I'd do this by being able to switch EF providers easily.
My last commercial project used squirrel with success. We have single exe it always update without problem. Several times release management team on client side does not upgrade version but squirrel apps itselfs are really user friendly if you do it "correctly" (depends on your business needs). You can always start app from double clicking installer and don't have to worry about current version. I've implemented solution based on squirrel docs that checked before login screen for an update and it one is there it updates automatically and restart. There are no prompts - it's done automatically. No problems yet - about a year on production. More than 20 updates deployed. Everything works in Intranet.
In startup.cs under spa.UseVueCli(npmScript: "serve", port: 8080); I added this hack Thread.Sleep(3000); string text = File.ReadAllText(".\\ClientApp\\src\\views\\About.vue"); text = text.Replace("page", "page"); File.WriteAllText(".\\ClientApp\\src\\views\\About.vue", text); Basically, if you make a change to a file, the page loads in iis... ive tried stuff with postbuild events but couldnt get it to work.. this hack enables the page to load in iis without the timeouts... the number of seconds to sleep varies, anything less than 3 seconds not working for me... if you find a better workaround please let me know
App was on .net framework 4.6.2. Please take your time to read docs carefully cause it's written there - it shows net 4.5 but it did not matter.
1. Does all needed files are located in installation folder after install? 2. Is there any error in eventlog?
Check out cachemanager.net
I was wondering how one might start a project in .net core with TDD and get the necessary stuff inside Startup and/or Program. This seems to be more about unit testing a controller method. 
I just released this exact type of tool today (no CRUD yet). [Here.](https://www.pro.coravel.net/) I'm hoping to get CRUD in there probably in the new year. Pretty busy 😉
TDD helps you to define how your implementation will look, like the services you will need to use inside the `Startup.cs` file, the example in the post is really simple, that's the reason we are just "testing a controller", in a more complex application we should make end to end tests, and test services.
Agreed. My co-workers and I implemented a caching library using System.Runtime.Caching and made it extendible to implement any other forms of caching (i.e. Redis). It's worked out great for all our legacy APIs and various websites that will never see the light of day with Core. 
I recommend not using it.
100% this. OP, why do you consider this a negative? 
That's a fairly ridiculous list of criticisms. &gt; We just made a shiny new framework to leave this junk in the past, why are we adding support for it BACK IN??? How is this going to attract any newcomers and advertise .NET Core as a modern framework? How does this affect you at all? Do you take some moral objection with supporting desktop apps or do you think you'll be required to carry some burden in your web app because the runtime now supports WinForms? &gt; In 2018 code for simple things like accessing a URL or DB should be one line and absolute minimal. You can one-line DB access with any of a dozen different micro-ORMs, use dynamic and you don't have to define anything. Is there some pressing need for this to be part of .Net Core? Doesn't that seem a little absolutely contradictory to your what is essentially your first gripe of adding to much into the framework? Http client facilities have been much improved, and helpers like Flurl also make usage very simple. It's also a bit of a silly goal. Accessing DB's and calling URL's happens in a wide variety of contexts and constraints. We need options. Sometimes I just need to call a view or sproc, others I want a deep object graph with change tracking. HTTP calls pivot on a large number of points - do you not have a wide variety of timeout, retry, and error handling policies? &gt; 3) Monoliths are dying Type safety is more useful when systems span projects and repositories imo. We publish common type definitions in a few languages to help prevent simple mistakes the compilers can easily prevent. And nothing in .Net is preventing you from architecting micro services. Micro services also sound great until you choke on your compute costs because your service density is garbage.
Re-writing perfectly good apps on a different platform is a recipe for disaster. I’ve seen it blow up twice, with huge schedule overruns + quality issues + performance hits. Difficult to make a business case for it, unless you are free to spend gobs of other people’s money with zero net return.
&gt; We just made a shiny new framework to leave this junk in the past, why are we adding support for it BACK IN??? From my perspective, .NET Core is basically useless without it. 
&gt; But as we move to Microservices or other, our apps become a lot smaller. Can you be more specific on exactly how that works? Perhaps post some code? *Everything should be made as simple as possible, but not simpler.* -- Albert Einstein 
I can't help but feel people are trading just one set of problems for a new set these days. I'm curious to see how it plays out over time. Just some thoughts: I haven't had the need to create micro services yet, which seem like a good idea for breaking up a monolith I can't help but feel you are just multiplying your deployment and testing hassles. The thing about serverless is it seems all pretty proprietary to me - you're tying yourself into one of the large providers like azure functions or aws lambda for a reduction in cost. I use Node/Babel/Vue for front end stuff and while there is less code (sometimes, there is some overhead in managing module exports/imports) now you need to worry more about package security than when you import a handful of nuget packages. That and when you look at the whole orchestration of it (which the net effect is it works pretty well) - it looks pretty crazy coming from a more traditional perspective. I have 3 backends in .net core 2.1 that I'm working with right now - I have contemplating rewriting one of them in node/express for the fun of it but when I start to look deeper at what is involved and the tradeoffs it doesn't seem as advantageous. 
There is a reason nodejs is slow. It's because it dumbifies everything and makes all the decisions for you
&gt; .NET Core supports WinForms as a negative is not embarrassing at all Yes. Yes it is.
&gt; Microsoft finally went in the right direction, True, but in typical Microsoft fashion they did it 5 years too late. When they originally released .NET they promised that they would have jitters for other OSes... "soon". Had they done it then C# could have cornered the market and everyone would be coding in C#... much less chance of surge for Python let alone Node.js. Those platforms gained popularity and that helped push the move to open (and open source) platforms. The Node.js\IO.js split and then reunification ended the debate once and for all... developers want open platforms that no single company controls and no organization can then dictate the health or breadth of their careers. MS jumped on the bandwagon, but alas... as I said, too late. At this point, most are saying "why bother" sticking with or going back to dotnet when the alternatives are so good now. It's sad really... I love C# and dotnet. But, like many others, I have moved on.
Someone wanting to move from .NET to Node.js just... Fuck. It makes me so sad.
&gt; In 2018 code for simple things like accessing a URL or DB should be one line and absolute minimal. Node.js does this, C# looks comical in comparison. It's simply too often that C# implementations need a fair bit more code than the other languages, particularly with interfaces etc. "For every complex problem, there is a solution that is simple, elegant, and *wrong*." Accessing a URL or DB in 1 line and without interfaces is fine *in a scripting language*. The reason it takes slightly more than that in C# is because you want to use proper testing patterns and depending injection. Now, you can mock out all access to a URL in JavaScript such that you don't need interfaces, but that's because JavaScript allows willy-nilly global fuckery, which has its own problems.
Core has been shown to perform better than Node. I'll admit JS can be nice to quickly run code, but I'd rather work with devs that have type safety forced on them than optional type safety that you get with JS tools. It's just too easy to write bad JS or Python compared to bad C#.
Explain 
I don't think this is a very useful article. Kudos to the author for writing it, but the only value in it is the file with the example controller unit tests. He doesn't do TDD here. He doesn't demonstrate which steps to take first, how to get the program to compile to show you your first "fail" before you attempt implementations to get it to "pass".
Yep, at least .Net isn't Java - tons of ceremony there. You can be pretty lazy in .Net if you want to, but I've found that my code has become a lot more readable and faster to write since I started using a bit of SOLID and things like IoC/DI and mapper. But I don't go around using every design pattern and chaining inheritance just because I can! I have been dealing with a similar thing with JWTs (newer tech) - I've put together an implementation and by time all was said and done beyond the most simple examples you see out there (hey, we need refresh tokens, revocation, long lived sessions, worry about xss) - I looked at my .net cookie implementation and JWTs really didn't seem all that great anymore. So unless I *need* JWTs for some specific case (like maybe more what they were designed for transfering authorization claims and such,) I'm just going to go back to cookies/sessions/redis instead of chasing after the new shiny and trying to shoehorn it in for an existing technology.
&gt;Since .NET Standard 2.1 will not be part of .NET Framework I personally wouldn't rely on tools that were originally designed for .NET Core. If those tools (caching for example) moved to .NET Standard 2.1 then you can't update anymore. But for the majority of cases, changing from .NET Framework to .NET Core 3.0 when it's released will be pretty seamless, so I wouldn't exclude yourself from libraries that currently work just because those libraries *might* move to 2.1 Standard and you *might* go against Microsoft's recommendation of moving Framework projects to Core 3.0
They're putting winforms 'back in', as you put it, because they are essentially making Framework EoL and are encouraging everyone to move their existing projects to Core. I don't see any downside to this. 
I would differ - avoiding being in a half-state of disarray (i.e. caught between .NET Framework and.NET Core) would be horrendous. Can't go back, can't go forward. I would try to avoid that like the plague. If the option of migrating is available I would do that sooner than later.
They are spending precious time on **windows specific** technologies that they could be spending on things that will help them compete. None of the stuff they have been talking about of late will work cross-platform. None of it helps them in mobile. None of it expands their capabilities in the cloud. Or ML and AI. Or any number of areas that are all in heavy movement. Therefore, MS will be playing catch-up again on those. They shouldn't write another single line of code that is MS Windows specific. Period. (Or another line of code for their platform that is closed source.)
But what about Deno? Luckily now with .net core there is at least a possibility of a future - even if .net isn't the dominant platform (it never really was, Java always eclipsed it.) There also seems to be a fair number of people moving from Node to things like Go and even .net core. I remember when ruby on rails had a lot of popularity in the early 2000's and was subsequently discarded because of it's terrible performance.
&gt; Accessing a URL or DB in 1 line and without interfaces is fine *in a scripting language*. And you can pretty much do it in DNC as well. But as you mentioned, you shouldn't.
&gt; but I can't help but feel you are just multiplying your deployment and testing hassles. Let's be real. This guy's not testing. &amp;#x200B;
&gt; Luckily now with .net core there is at least a possibility of a future Again, I think it is just too little too late. They'll hang around, kind of like Java... through sheer mass. But new platforms are taking over for newer workloads and slowly, bit-by-bit over time Java and dotnet will shrink. There are still COBOL systems out there you know. Do you want to be working on them? I don't and I was good at COBOL. &gt; I remember when ruby on rails had a lot of popularity in the early 2000's and was subsequently discarded because of it's terrible performance. That was because of ASP.NET MVC believe it or not. The MS community was loosing devs at around 10,000 per month back then (the internal numbers indicated at least). ASP.NET MVC came along - shout out to Phil Haack, miss you buddy - and stopped the bleeding for a good long while. But Node came along and the bleeding started again and got worse as the cloud and things like docker gained steam. (Node also was the final death nail for Rails.) It just became possible for more and more systems to no longer have to "work" on windows server. By the time MS reacted (with cross-platform .NET core) it was, in my opinion, too late. Linux has won in the server space hands down and they have absolutely ZERO game in that market. Don't get me wrong. I still have hope for MS and think that, at least lately, they have been doing a lot of interesting and smart things (largely because of the new CEO). I'll always have an attachment to them on some level, I spent over 4 years there as a consultant on more projects than I could count after all. I just don't know if the damage done is reparable at this point. They certainly aren't an industry leader any more (neither is Oracle for that matter) - industry leader being defined as a business that can drive the development community and technology in their chosen direction. I will say that I think (outside of DevDiv) they are putting the emphasis in the right places. Office 365 and Azure. That's their future. The other thing they should be aggressively working on is a cloud first Active Directory offering (no not Azure AD, full cloud as in cloud only). Windows OS will eventually be free and so that's not how they will generate money. Anyway, I wish them luck but it is a much more competitive landscape out there now... and largely due to their missed opportunities or belated responses.
My two cents as I’ve written caching service using .Net standard 1.x packages for .Net Framework 4.7.1, don’t do it if you are not going to port your application entirely to .NET core in near future. Using .Net standard libraries with legacy code or even just Framework 4.7.1 caused me days of work just figuring out which system.* libraries are supported with .NET Standard 1.x or 2.x . I would rather create two packages using same code but different libraries, one using .NET Standard lib for your .Net core applications and another using Framework 4.x lib for .NET Framework applications. On the side note, I did get few benefits out of using Microsoft.Extensions.Caching, mainly I could write a generic and thread safe code to insert and read data from cache and easy extensibility if later you want to change your caching strategy. (I.e changing from file system to redis or aws)
Ya, bad code can be written in ANY language.
Interesting, insightful. I feel very similarly which is why I started diving deeper into node last year. Maybe you're just further along on the 7 stages. Luckily there seems to be enough job opportunities for c# and javascript where I live (Denver) that it's worth continuing on and see what happens.
Right, because building on and deploying directly from developers’ workstations is the kosher way of doing things.
&gt;Oh, and it runs faster, and these machines start up faster. Also, the devs will be more motivated. But here's the rub. Most PHB's don't care so much about "the long run" as they do about hitting *this quarter's* targets. Others here are also suggesting that ignoring upgrading/migrating might well become a problem in 10 years time. There's probably merit in that except that those same PHB's will almost certainly have moved onwards and upwards to better and/or different things by then. So, unless it's coming down as an edict from high above, there's really no motivation for those PHB's to throw their weight behind any kind of migration project. Sad, but true.
&gt; We need to keep upgrading the technology stack without impacting the business deliverables. If you have some magical process that consistently allows this, please share. More often than not, business deliverable deadlines aren't even long enough to allow the actual work of implementing the business deliverable, let alone the time required within that time-frame to start to try to learn, understand and migrate to an entirely new technology stack. I do agree that some small things, like refactoring a few functions or a few classes, can and often should be done "under the radar" so to speak, but mammoth efforts like technology stack migrations simply cannot be done without at least *some* buy-in from those above. It's simply to big an undertaking to try to do that kind of thing "under the radar".
Thats true and on reflection i came off harsh. You can still be good working on old tech. You don’t need to constantly be chasing the new and shiny to be a great developer.
If management can replace those competent developers with incompetent developers (but who are *just competent enough* to keep a large legacy project limping along and are also likely to be a bit cheaper than the outgoing competent developers), I suspect they really won't care. &amp;#x200B;
Thanks for the feedback, in this tutorial I'm assuming the reader already knows what TDD is and how it works but not necessarily is a .NET developer. I will try to cover things in more details in my future work.
Have you tried running the [portability analyzer](https://docs.microsoft.com/en-us/dotnet/standard/analyzers/portability-analyzer) on the binaries? It really might not be as bad as you expect &amp;#x200B; &amp;#x200B;
Certainly don't have any magical process. But I can give you my experience.. Ours is a fairly big solution developed and maintained by only three devs and one QA... We have a CI/CD pipeline and we release almost every second day.. We follow github flow and our master is always in releasable state.. The business always come up with their new requirements and need them 'yesterday'.. While we try to keep business happy we still ensure that our technology stack is not outdated.. One such example is migration from Angular JS to Angular 2+.. We started planning the migration almost a year back and after an year we have almost completed the migration.. In the meantime Angular came up with several upgrades (2,4,5,6 and now 7).. We tired to upgrade as soon as we can to the new version.. Was it easy? Certainly not...it required lot of planning and we encountered many issues.. But today as the migration is near we can confidently say that we are not on an outdated technology stack.. Now we have similar plans to migrate our backend from .net framework to .net core.. Again the idea is same.. Hopefully we should be able to do that in next few months.. All this upgrade is happening without letting the business know.. Why? For starters they don't care.. Secondly, if you tell them they will come back and say this is not a priority for them.. We let the business run their business and let them tell us "what" needs to be built but we do not let them interfere in "how" we achieve it.. Hope it calrifies.. 
The thing is I need to host my UI controls inside my non-.net code and I don't know if that is possible. Currently I'm using winforms interop for that and a helper COM visible class. ComVisible and related attributes are present in .net core, but I've read IDispatch is not supported. Maybe I should try out a small prototype to see if I can get it to work without IDispatch...
I was hoping for a relaxed answer, but I haven't got that. &amp;#x200B; What message does it send to modern developers when instead of improving the criticisms of .NET Core, time is spent adding support for technologies .NET is frequently criticised for? When people who worked with .NET years ago say why they left the framework, they quote WebForms etc. Why on earth are we bringing such a dated technology back into the mix? .NET Core should encourage modern approaches!
I'm very relieved to read this.
It's not a case of porting though, it's a case of choosing what technology to build new apps in (i.e. one which wouldn't require writing a wrapper function)
Why do you think I posted this question? I love .NET, and the lack of confident answers on here is concerning.
But why use dependency injection when you don't need it?
1. Answered in Wixred 2. Show this please? One line with no surrounding clutter. 3. I agree with this, but it's an advantage for them.
Generally you end up with more apps, each having less code.
Microsoft earns most of their money from companies. Companies they want to switch over as well by being able to switch to .Net Core when such legacy technologies are supported.
I understand why you've been voted down, but I wish people would address these obvious flaws instead of pretending they don't exist.
- IOC in ASP.NET Core are built in. You can just use it with zero ceremony. - Show them the TechEmpower benchmark. ASP.NET Core is a speed deamon. - Node.JS? Seriously? Have you guys checked the gazillions of npm packages on your folder? - If you are moving to Node.js, you are gonna use TypeScript. Guess who made that language? - You will also probably using Visual Studio Code. Guess who made that piece of software? So yeah, move to ASP.NET Core. It's awesome.
As far as I can remember, Mono struggled with adoption around issues like WinForms support (System.Drawing gives you GDI access). VB6 also been notoriously hard to kill and I'm sure MS wishes they could've brought that legacy crowd along for the .NET ride a bit better. Honestly these moves look like MS trying to learn from their mistakes. Not all their decisions are made to target you, they have a big audience with diverse needs and legacy support is very relevant (and Core stack may be free but Azure isn't + they can still sell support contracts to enterprise clients ...).
We'll probably keep that in C# yes, this is really for new apps going forwards. &amp;#x200B; We've already written a new microservice system in .NET Core which works good so far, but the development time was far higher than teams which used other languages for similar projects (hence as to why these questions cropped up). Platform stability overtime may give some incitement answers, but the .NET language and framework frequently felt overkill at times, especially for the smallest apps. We also had to switch languages for anything which was more appropriate as serverless.
If you seem to be the only one who recognizes these "obvious" flaws, maybe that should tell you something.. Everyone else is just stupid, right?
&gt;This guy's not testing. If pointing out flaws in the framework correlates to "must not write tests or use TDD", then we really are in trouble. Testing Microservices is extremely easy, especially with Docker. I'm not sure what this has to do with anything to be honest?
Why? I’ve had a lot of success with it. It’s the perfect solution for business environments where the app is stored on a local server.
Initial velocity in a language which is not statically typed (JavaScript, Python) is much faster. You trade this speed for the challenges that come from a lack of type checking as an application ages, but depending on your context that may or may not be an issue.
Have you ever actually converted a "monolithic" app to microservices? Its disturbing that proponents of microservices offer two choices: microservices and (god forbid) monolithic. For the record a well written n-tier application is anything but monolithic. Have you ever written a full enterprise application in a weakly typed language? I think if you ever undertake this task you will find that comparing .net to node makes no sense at all. 
If you ask any developer if they rather program in a strongly typed language or not they would choose the former.
Oh that’s right. Microsoft built Click Once, but then said “don’t use it guys, because having to use it from Visual Studio makes it bad practice, but we’re just going to leave it here anyway.” Not every situation calls for it, but it’s a perfect solution for others. I’ve used it plenty when working as a business internal dev. It’s fast and easy. 
1) Yes, as have many others - it's 2018. 2) N-tier was great, but you're still coupled. Having separate apps deployed separately, tested separately with their own databases etc is hugely powerful. 3) You wouldn't in a monolith, it would be awful. But when everything is broken down and sits independently, typing simply isn't such an issue.
Have you looked at the name of the subreddit we're in? And I'm not the only one, the ones bold enough to raise the same concerns have been voted down - which again is concerning.
Stack selection has a lot more to do with how well it fits with your business environment than the merits of technology at face value. A company that is Agile and public facing, and that is moving to microservices and adopting the cloud, will find that duck typed languages like JavaScript and Python make a lot of sense; you trade type safety for well defined apis and leave implementation details to the teams that implement them. In my shop we've got .NET (my team), JavaScript, Python, Go and Clojure, all working together transparently over restful interfaces. This works because teams are small (2-5 devs) and we support what we build. A company that is more B2B oriented or tends to have a lot of back office/line of business apps will prefer a more enterprise oriented, strongly typed stack like .NET or Java, and this makes sense because business requirements aren't going to abruptly change the way they do in the context described above. It really isn't productive to compare stacks without context; you'll get a lot of dogmatic arguments made from the poster's context which are probably 100% true from their perspective.
Sorry pal I don't mean to be rude but there are a lot of us who have been down these roads. You are not convincing.
What are you going to use the VM for? Development, or just running? I ask because Visual Studio 2017 does not support LTSC: https://docs.microsoft.com/en-us/visualstudio/productinfo/vs2017-system-requirements-vs &gt;Windows 10 version 1507 or higher: Home, Professional, Education, and Enterprise (LTSC and S are not supported) If you only plan on using the VM for a local deployment environment, will you be deploying on LTSC? 
&gt; but I wish people would address these obvious flaws instead of pretending they don't exist. It is sad because there are a lot of truly dedicated brilliant people at Microsoft. I worked with countless numbers of them. However, sometimes the tactics of the company or the various departments get in the way of real innovation. It is one of the reasons that I believe the ASP.NET MVC was a bit of a miracle at the company. It was way-outside-the-box thinking that would normally have gotten shot down at the company at that time (the Balmer years).
Unsupported or doesn’t work? I have never in my 20+ years of development contacted Microsoft for Visual Studio support. 
I've only ever spent time on Server when setting up and once-in-a-while debugging on our staging environments. I've found it a nightmare due to all of the added security restrictions requiring a lot more steps to do basic things like setting up proxies when necessary, but depending on what sort of development you do this could be not a problem for you. I haven't tried doing development on it. At work, our mac users just use Windows 10 with Parallels, and I personally use Windows 10 on multiple machines for multiple development environments, and have not found it any more bloated than other versions of Windows-- in fact, with enough configuration turning off all unwanted features, I've found it less bloated than 7 was in many ways.
Just in case OP wants to continue pursuing development on LTSC: Wanted to point out that thankfully, Visual Studio is not the only .NET (Framework / Core) IDE these days. If open to other options, JetBrains' Rider does not list LTSC as unsupported. [https://rider-support.jetbrains.com/hc/en-us/articles/207876035-Rider-system-requirements](https://rider-support.jetbrains.com/hc/en-us/articles/207876035-Rider-system-requirements)
This company seems so immature. I mean yeah Node.js and Python might be quick solution to do something quickly but Node.js is still JS, and JS is still very horrible and shouldn't be used beyond frontend. Python is much better but still not suited for mid-huge projects at all. I have no idea what is happening to your company's mind to be fair.
&gt; Windows 10 version 1507 or higher: Home, Professional, Education, and Enterprise (LTSC and S are not supported) I think this mean LTSC isn't a supported platform for VS2017.
&gt; to reduce spam. &gt; &gt; Parent commenter can delete this message to hide from others. How about reducing spam by just sending a PM, instead of this post?
Good for you. I'm not saying that dotnet is dead (or for that matter will be any time soon). I still love C#. But unlike a lot of people in this industry I am not bound to any language, ecosystem, or operating system. I've used so many over my (very) long career that even the concept of only have one platform to work in seems silly. The upside for you is that as you pick up other languages you will find that you will even become a better C# dev. It is surprising how learning to think the "Python" way or the "Ruby" way or even the JavaScript way can actually make you more innovative and capable in C#. My advice is just keep those skills sharp, be open to anything, and most importantly have fun and never stop learning.
Web API is , as the name suggests, an API or a service interface for the clients to talk to. It's the modern way to create applications that use a rich client framework like angularjs for example. It's a complete separation of concerns, separating client and server business. And it adheres to the REST principles. MVC is just the older of doing such stuff.
``` var content = await _httpClient.GetStringAsync("https://www.google.com/"); ```
I have a full dev environment on a remote Windows Server 2016 and it works quite well. The only thing it is missing that exists in Windows 10 is Windows Hypervisor Service (I think this the name, it is what makes Android emulators run fast on Windows). I have VS 2017, SQL server plus management tools and a lot of other stuff. It is difficult to tell you the footprint as I have a lot of stuff installed.
MVC is a simpler way to build a web app, but it provides less flexibility and scalability compared to Web API. MVC will handle everything from the request coming into the controller through HTML generation using Razor templates. Assuming you want a UI, and you're not just building a data API service, this is probably a good choice if you don't need superb scalability and don't want to dig into one of the major front end frameworks like Angular, React, or something like Blazor. Web API is the right choice if you don't need a UI (just a data API service) but will provide both greater flexibility and scalability if you do add a UI, at the cost of complexity. It will allow you to host and scale your backend API separately from your UI. The primary benefit of this is that your HTML and JavaScript can be hosted as a "static" website on a separate CDN, and your .NET app itself supplies the data and backing business logic to that static site. The latter can technically be achieved with MVC, you'll just be skipping the view steps of MVC and you can of course just return serialized data to your client. MVC does come with some more dependencies. Web API is obviously a greater investment if you want a UI, and might be overkill if you're just looking to play around or don't know if you'll need to scale out.
&gt; I'm not the only one, the ones bold enough to raise the same concerns have been voted down There's about 35 different users who have commented on this post. The only one who's clearly agreeing with you is [here](https://old.reddit.com/r/dotnet/comments/9ztef6/why_my_company_is_quickly_moving_away_from_net/eac1n2c/). There's a few posts that are neutral and neither really agree or disagree. The other 30 people disagree with you. You're basically the climate denier or flat earther here.
Dependency Injection is a *pattern*. I often don't use DI containers. Before it was called "Dependency Injection", it was just called, "programming to interfaces, not implementations", and it was a concept nearly as old as computer science itself. "Dependency Injection" just makes it a more explicit pattern of having a composition root where you set up all the interfaces to implementation mapping. Do don't use dependency injection when you don't need it, but don't ignore the possibility that you should still be programming to interfaces rather than implementations, even in a simple service. When engineers are developing a car engine, do they only test it while it's inside a car hooked up to a real transmission? No, they put it on a stand hooked up to a fake transmission that can measure its power. They test the transmission in isolation, hooked up to an *interface* that is pretending to be an output shaft connected to a diff connected to wheels connected to the road. Being able to test your pieces in isolation means building interfaces that can be substituted with different implementations. C# has the actual concept of interfaces, whereas JavaScript uses Duck Typing, but the design pattern is the same.
&gt; 2) Do you have an example of a one line DB or URL hit without clutter around it? These are extremely common and far less simple than some other languages, I'm not sure how anyone could seriously suggest otherwise. dynamic account = new SqlConnection("connString") .Query&lt;dynamic&gt;(@" SELECT Name, Address, Country FROM Account WHERE Id = @Id", new { Id = Id }).FirstOrDefault(); There's Dapper. Now, how is that "far less simple" than other languages? I also disagree that one line DB access is "extremely common" - particularly if you suggest that class definitions count as additional lines. Do your objects not have behavior? And if class definitions don't count as additional lines, practically every single DB access tech in .Net can one-line it.
&gt; What message does it send to modern developers when instead of improving the criticisms of .NET Core, time is spent adding support for technologies .NET Framework was frequently criticised for? What message does it send when Python developers release a new module to support XML rather than focusing on the big, bright JSON future? It's a complete false dilemma. It's *nice* to have a simple GUI framework you can count on being there for simple stuff, and it's not taking away from the development of the Core bits. WebForms was a hack of a technology to make web programming feel more familiar to VB/RAD developers. It was obsoleted by XHR and JSON becoming well-supported in all browsers and the web world moving on. WinForms is a simple GUI toolkit that's easy to use from a RAD tool or code up by hand, if you choose. .NET Core needed something simple and cross-platform to fill that niche, and WinForms was as good of a choice as any other.
In theory these days especially with how .NET runs, you can get away with it. Only big thing that jumps out is IE runs in security mode by default, if you are using Azure DevOps, logging in under VS2017 can be a pain. But there could be some differences in running under Win Server and Win10...
&gt; They are spending precious time False dilemma. They have enough engineers to do two unrelated things at once. WinForms support is *following* the Core advances, likely by completely separate people, not slowing it down. &gt; None of it expands their capabilities in the cloud. Or ML and AI. Or any number of areas that are all in heavy movement. Therefore, MS will be playing catch-up again on those. Only if you ignore the fact that they're accelerating rapidly in those very areas. MS is not a single engineer working on one thing at a time.
&gt; Do you have an example of a one line DB or URL hit without clutter around it? $response = Invoke-WebRequest "http://localhost:666/13/13/42" $jobj = Invoke-RestMethod -ContentType "application/json" -Body @{"foo"="bar";"id"=123} -Method PUT Oh, wait, that's PowerShell, a *scripting language*, where 1-liner shit is worthwhile. "But I can do so much more in 1-liner code in a dynamic language with loose typing" is an argument in CS that is older than *you*. And it lost to "but real software benefits from strong typing" every time.
Take that coder that writes bad code in C# and take away type checking or variable scoping. You end up with great stuff like if (varaibleThatWasMistyped) { CodeThatDoesntActuallyWork(); } That suddenly breaks everything when you fix the typo in the variable name.
Hah, you think they put things in different functions?
If it's legacy, hopefully you don't have to update it to take advantage of the new fancy stuff in the first place.
You're right. I should have made it `#include &lt;3 pages of code that doesn't work&gt;`
Try telling that to my IT staff. I’m using 1607 LTSB and they can’t figure out why that’s bad. 
I don't think MVC and Web API differences had anything to do with one being more heavily tied to IIS. MVC was designed to return views (HTML)... and web API was designed to return REST style responses. They have blended the two together into one controller now (so a controller or its actions can be setup to return view or REST responses).
MVC could always return JSON too, that was never a big issue. The self-hosting part was fairly importing (aka no IIS reliance). It's historically from the WCF Rest Services already.
MVC could return JSON, but it wasn't REST friendly. Web API's big sell was that it was REST friendly responses.
I'm not exactly sure what you mean. There are two desktop applications that I work on. Both were developed with other programming languages, but new features are generally implemented in .net. For smaller updates or to integrate the .net code we do still change the old code from time to time. Our goal is to slowly work towards 100% c#, since porting everything at once is simple not feasible neither is not changing development platforms. So in the meantime we have an application that is partly .net framework (currently) and partly build with other languages. But this in not visible to the user. This approach works quite well, since we are moving away from the old platforms and keep adding value for users simultaneously. But I do hope we can host .net core components like we do with .net framework now. From what I've seen I'm hopeful, but will wait with testing until a preview of .net core 3 is released.
If you set out to write code to replace type checking, via guard clauses or what have you, then yes it will get tiresome fast. 
No idea what you're going for here, did you read the thread?
Exactly, that's quite a lot for a simple query - and we haven't even called .Dispose() on the instance we created yet (which if we do it the correct way is another set of {} for the using).
This is an interesting point, can you elaborate with examples? In what cases would a compiler save you from writing more code?
*"I can't help but feel people are trading just one set of problems for a new set these days."* 5 years ago I'd have probably agreed with you, but once you succeed with a highly decoupled architecture you'll never think that way again. You'll still have other problems sure, but all the things that weren't previously possible now are.
&gt;Visual Studio 2017 does not support LTSC It lacks official support from Microsoft, but it's running fine.
Obviously you get bad developers in all languages, not relevant to the thread. 
That's ok mate but what's to convince? Companies have been successful with these approaches (including mine) for years now. I'm here to find answers to support .NET Core as a good candidate for them.
This is (one of the first) good answers - appreciate the information.
&gt;You can't run a business based on what you fear random slashdotters may use to concern-troll. Every business in the world runs in fear of how their target market perceives them. Hopefully the target market for .NET Core isn't just us .NET loyalists? &amp;#x200B; *and it's not taking away from the development of the Core bits.* Where is the development then? Where in .NET Core 3 are the features to keep us competitive?
Adding ef6 compatibility is a great thing imo. At my company we have both Oracle and SQL server db's. For the projects that use Oracle, you HAVE to use ef6 to connect to them because Oracle hasn't published the ef core tools yet. So what I usually end up doing is having an interactive application written with .net core talk to a web api based on .net 4.x and talks to the DB via ef6. It's entirely useless to have that extra middle web api for an intranet app, but that's the easiest way to do it because of shit support for Oracle and .net core / ef core. I've also heard a lot of complaints about ef core performance, so it's no surprise that they want to maybe be ng back what a pot of people are familiar with. I also personally like dealing wit the a database first model instead of doing the "code first" approach that is the recommended approach now. Yes it's possibly to still scaffold out your models from a DB via the command line, but that's tedious when you make small changes in the early stages of development. 
There's a lot of good responses on here but, I do just want to throw out one thing. For code clutter, I've found in writing some stuff for .NET Core and try to move away from past thinking, code clutter only happens if you let it. You can easily just not use IoC if you think it's too extra, there's NancyFx if you want the express/flask experience in .NET and you can avoid all the extra stuff. It's just there to give better guarantees on what you can and can't do, if you're using interfaces just for the hell of it (or "tagging" objects) then you need to step back for a moment. I've worked with Java, .NET, Python and JS (Node) professionally for about 6 years now, it really is just trading one set of problems for something new. I do like python and JS for small things but building more complex backend services has its own scaling issues when the codebase gets large. What I'd suggest (which is what we do internally) is finding a small part of your monolith codebase and writing a microservice version in one of the languages you're considering. If it works and works well, maybe it is time to move to something else you all feel more comfortable with. Not every language has a framework that's going to fit your needs but I personally find .NET Core has enough in place through MS or third party frameworks to let you either write tiny simple things or big ass monoliths with ease. As a side note, the porting of WinForms/WPF isn't that bad, it shows how easy backwards compatibility can be and in bringing some of those developers over, we'll see more small libraries for the simple things. The more devs the better even if at face value it seems kind of shitty.
Another good post, points noted.
It may not have been "REST friendly", but it could easily fully support RESTful services. No problem there.
People have spent time showing you various simple ways of executing a query - how about you show what you consider to be a simple one-line database query for others to critique?
Note that OP asked about dotnet core specifically. In which case MVC is not the old way of doing anything as it refers to all types of HTTP controllers, not just HTML or RESTful api ones.
They could have easily updated MVC to make it more rest friendly. The main thing was indeed the removal of the dependence of system.web assembly and switch to owin. 
The whole point of the ORM is lost with dapper. The point is to move your business logic into managed code where it can be factored and strongly typed. Sure, there are tradeoffs with speed and granular control of SQL. For line-of-business apps the trade-off is easily worth it.
Changing databases is pretty rare after an app is built or installed and running. However developers of shrink wrap apps may want to give their customers the option of running on a variety of platforms. This is actually pretty easy with EF. I wrote a utility called [AdaptiveClient](https://github.com/leaderanalytics/AdaptiveClient) that makes the process very simple. AdaptiveClient also allows an application to seamlessly choose a transport protocol. Users on a local area network can use a fast, in-process client while users connecting remotely can use WCF or REST. The entire process is handled seamlessly and transparently by AdaptiveClient and Autofac. Demo app is [here](https://github.com/leaderanalytics/AdaptiveClient.EntityFramework.Zamagon).
I haven't heard much of TDD lately. I think many of the TDD people have grown weary of being under the burden of their own dogma. 
&gt; False dilemma. They have enough engineers to do two unrelated things at once. Well, that's wrong. Every company has limited resources and therefore limited time to focus on the things that will best position them and their products - and by extension their customers. That's why .NET 1.0 didn't have generics. That's why it took until .NET 3.0 to provide Lambda and LINQ. Decisions have to be made on where and when to spend the time and meet timelines. Saying they have "enough" is dismissive because while they do have "a lot" of developers compared to many other companies... it is still finite. Hard choices ALWAYS have to be made no matter how many people you have. (In fact, I would argue it gets harder to make the right choices when you have more developers rather than less. With fewer of us around the choices tend to get made for you.) &gt; WinForms support is following the Core advances, But that's neither here nor there. It doesn't matter. The question is whether WinForms should be worked on **at all**. What competitive benefit will it provide to Microsoft and by extension the .NET community? Secondly, are those benefits MORE important than something else they WON'T accomplish otherwise? That's the fundamental decision matrix. &gt; Only if you ignore the fact that they're accelerating rapidly in those very areas. Sure and I wasn't ignoring it. What I was doing was indicating that they are **WAY** behind. So much so as to not be significant players in those spaces. So, they are investing (rightfully so) in those areas because otherwise they will completely lose the development community when it comes to those workloads. &gt; MS is not a single engineer working on one thing at a time. Oy... dude. **I WAS THERE**, I know. I've worked with and built large high-performing development teams at lots of organizations, including MS, and there are always difficult resourcing choices. Always. Stop trying to trivialize what I'm saying by dismissing it as though I'm a moron.
Just do that once: ``` readonly static HttpClient _httpClient = new HttpClient(); ```
Why does core still have both mvc and webapi project types? Do you think it's just legacy clutter / convenience or is there some other fundamental difference?
You mean the entire idea of type safety and variable scopes?
You can do microservices in a strongly-typed language. They are orthogonal concepts.
Yes, I understand the Mythical Man Month quite well. WinForms support is a library implemented on top of the Core, and can be done by a completely separate team than the people working on the Core. &gt; What competitive benefit will it provide to Microsoft and by extension the .NET community? Do you seriously not think WinForms support in .NET Core is important to a substantial part of their customer base, such as corporate IT developers? MS wants *those* developers to transition to .NET Core, so they can justify focusing on Core and de-emphasizing the old framework rather than spending resources maintaining .NET Framework for corporate IT developers in perpetuity.
I think he means a service layer - a set of functions that your web api or mvc can call. This is how I do it too, works great.
It just adds different files as a starting project. If you choose MVC it will add some Razor files, if you choose Web API it leaves those files away. It's the same framework in both cases, and nothing prevents you from adding that stuff afterwards. Personally I've never liked the start projects and just start from scratch.
TL;DR - LTSB is a hot mess, I'd only seriously consider Windows 10 mainline or Windows Server For LTSB issues: The OS shows Linux Subsystem for Windows in Add/Remove programs, but it doesn't work. If you look at some forum threads it seems that it wasn't supposed to be included, but somehow the option shows up, but it doesn't work because LTSB has no store app so no way to install a distro (along with some other missing internals). If you end up diving into Docker at any point and have any need to run Windows containers, Docker will be stuck in the painful place it was back in 2017, port forwarding pain, performance pain, DNS pain. The only positive I can see for LTSB is that there is less bloat like Candy Gem Saga of Nonsense and the other junk apps that get installed with the latest versions of Windows 10, but there are only like 6-10 or so of those junk apps and you can just uninstall them after you install the OS. Personally, .NET development is moving really quick these days, Visual Studio has an update each month, .NET Core releases quickly, Docker and Kubernetes. They're all moving very fast and I'd be shocked to learn that any of the tools you'd be using for development were getting any serious testing on LTSB. I think you'd be more likely to find some serious bug/problem on LTSB than on the current mainline Windows 10. 
Use ApiController to benefit from RESTful things like the new Jain safe error messages. 
1. Install vagrant. 2. Browse the vagrant marketplace for community-customised images, should be able to find a server 2016 image with SQL installed for under 10gb 3. ...net? 4. Profit
I am using Windows 1809 with Parallels for a couple of years now. This have been working perfect for me. I use vagrant to setup a complete .NET development environment. You can check out https://github.com/Baune8D/vagrant-vs2017-devbox This will use one of my prebuilt Windows 1809 packer boxes and install VS2017 + alot of other stuff i find essential. You should be able to customize it to your needs easily as it installs almost everything through Chocolatey. I love this setup because its easy to set up a new box if Windows decides to screw with you :D
&gt;The only positive I can see for LTSB is that there is less bloat like Candy Gem Saga of Nonsense and the other junk apps that get installed with the latest versions of Windows 10, but there are only like 6-10 or so of those junk apps and you can just uninstall them after you install the OS. The main point of LTSB is to avoid the frequenty, forced and buggy feature updates.
Overall very good articles! 👏👏
Google a basic node example. No one who isn't deluded would contest this point, .NET has its strengths elsewhere.
Are you actually listening to yourself? Go post on 'Climate Deniers' about how global warming exists, and see how many people agree with you? &amp;#x200B; Honestly... basic basic logic. Try reading the thread without getting upset for once.
Yes, hence us having made one in .NET
Thanks for this, I appreciate that you waited til the entire series was done to post, good stuff
Thank you for this. Do you have example code for Dynamic Role Based Authorization using ActionFilterAttribute? I'm still learning and I'm interested in seeing a working example. 
&gt; and can be done by a completely separate team than the people working on the Core. But should it? What else could that team be doing that might be more beneficial to the platform? This isn't about Core versus other this is about the entire dotNet platform\ecosystem and what areas could be moved forward to allow dotNet to compete better (especially cross-platform). I honestly mean no offense at this but it is clear you've never been in a position where making decisions on strategy and direction were a part of your role (or if you were you struggled immensely at it). You seem to have a very narrow view of what their options are and what might be beneficial. &gt; Do you seriously not think WinForms support in .NET Core is important No, not at all if it only supports Windows (which is not only the current plan but the ONLY plan as far as they have announced). Windows only functionality is **absolutely useless** at this point in our industry. The dotNet framework is struggling not because of lack of support for functionality on Windows... but because of lack of support on OTHER platforms and in other areas such as ML, better cloud support - especially serverless workloads, etc.
I have Implemented in one of my Application but don't have a solution available separately. I'll Implement &amp; share on Github in the next couple of days including Creating Users, Roles and Assigning Roles to Users. Please share your email so that I'll ping you after sharing. Thanks.
You're trying to make a point, you show how you think database access should look. Microservices aren't necessarily a phase, but stubborn instance that they're the one true way to do development is.
PM'd. Thank you. 
[https://blogs.msdn.microsoft.com/dotnet/2018/05/07/net-core-3-and-support-for-windows-desktop-applications/](https://blogs.msdn.microsoft.com/dotnet/2018/05/07/net-core-3-and-support-for-windows-desktop-applications/)
Awesome, I gotta learn how to use azure, since yeah, it's mostly messing around (making small online apps and projects to show on interviews) Thanks!!
Why aren't you asking the person making this outlandish claim? Strong claims require strong evidence; nobody else should waste time on answering such a claim since it hasn't met even a cursory level of credibility. 
Where did you hear this, CNN? .NET Core is the future. 
&gt; You seem to have a very narrow view of what their options are and what might be beneficial. *My* view is narrow? You're the one claiming it's useless. &gt; Windows only functionality is absolutely useless at this point in our industry. It's useful to Windows, which is still an MS line of business. &gt; I honestly mean no offense at this but it is clear you've never been in a position where making decisions on strategy and direction were a part of your role (or if you were you struggled immensely at it). "No offense", followed immediately by a direct insult. Classy. Great way to convince people of the merits of your argument. In my experience, strategic decisions often involve teams talking to each other and cost-sharing in one form or another. WinForms support in .NET Core is strategic, and *you have zero evidence that it's taking anything away from Core development*. &gt; But should it? What else could that team be doing that might be more beneficial to the platform? WinForms in Core, even Windows-only, allows for a faster deprecation of .NET Framework and therefore more focus on Core. Making Core the focus on Windows and transitioning Windows developers to Core strengthens the cross-platform viability of Core.
I assume by ".NET 5" you're referring to the full desktop .Net Framework. If so, that's true. The full framework has been around for 15 years and has many API's exposed for it. Lots of those have not been updated to be compatible with .Net Core. Active Directory and GDI+ are a couple big, notable API surfaces not in .Net Core. Linq2Sql is another many businesses looking to update older .Net apps will hit. It's also not always clear-cut. GDI+ is mostly exposed through System.Drawing in the full framework. Mono's version can be used in .Net Core, but it's not a drop-in replacement - a legacy library that referenced full framework System.Drawing cannot use Mono's drawing lib without updates. The question is, do you need any of the missing API's? For new projects, you probably don't. .Net Core has it's own advantages. For example, internal runtime improvements from new types like Span &amp; Buffer will never make it to the full desktop framework.
Good overview but I feel you’ve glossed over the implementation of the roles authroization too much. 
Interesting, I'll give this a shot tonight. Thanks for the comment. The timeout error is definitely extremely tedious, though it isn't limited to vue-cli. Angular-cli is plagued it as well from what I've remember.
I'm pretty sure AD works now. I just used it in a Razor Pages site. I'm 99% sure it had no dependency on the full framework.
That is entirely in the context of desktop applications.
Yes, I also think. But I'll update my article with full Implementation within the next couple of days.
&gt; It's the modern way The modern way nowadays is to use .NET Core, and not ASP.NET Web API.
Well obviously, because they already recommended similar action for everything else [2 years ago](https://blogs.msdn.microsoft.com/dotnet/2016/02/10/porting-to-net-core/)
The library author of, let's say, package X, could choose to make the next version compatible with .NET Standard 2.1 and not 2.0 anymore. So as of version X it would not be available for .NET Framework anymore. The questioner of this thread would no longer have access to any security fixes etc. Yes, the author of the package could multi-target - but if they have fundamental changes whereby they required APIs from 2.1 (memory management APIs, etc.) then there's no guarantee that an author would multi-target. That's totally up to the author. Given that ASP .NET Core 3.0 will **not** support targeting .NET Framework at all - any packages that are "for" .NET Core will have no real reason (other than not requiring the newer APIs) to backport to .NET Standard 2.0 - since .NET Core 3.0 will implement .NET Standard 2.1. Should we trust MS packages to multi-target (since they **will** be moving to 2.1)? Given what's happening with .NET Core as a whole - **no**. I wouldn't trust that they are somehow **obligated** to do the same with .NET Standard. Given the nature of what APIs are in 2.1 (Span, Memory, etc.) I would actually be surprised if most packages don't move to 2.1. Again, multi-targeting would be up to the authors. In this case, MS - who has a proven track record of breaking off with newer .NET Core related tools.
Whoever told you that was tolling you.
.NET 5 was a considered name for .NET Core IIRC. .NET 5 IS .NET Core. 
I usually prefer to do integration tests so I use [this method](https://visualstudiomagazine.com/articles/2017/07/01/testserver.aspx). Basically it creates asp.net core stack in memory. Then I create `class TestStartup : Startup` to use in testing, then I make all necessary configuration changes as virtual Startup methods and override them in TestStartup. You can override your middleware configuration in test to test the middleware.
A Unit Test really shouldn't be concerned with such a high level. You'd want to test if the string produced by the middleware is the correct string, not testing whether HttpContext works or not. That's more the reserve of an Integration Test. What you're proposing essentially tests whether the middleware produces a string *and* the .NET Core web context works *and* whether HTTP Headers are produced. That's too much. You're testing .NET at that point and *not* just your code. A Unit Test should assume Integration works, it should only be concerned about what's being passed into the integration. "Does this bit of code produce this string?" - Unit Test "Does that string go through the middleware pipeline gifted to me by .NET, does the server then send that string as a HTTP Header in the Response?" - Definite Integration Test. Integration Tests are perhaps better performed with specific tooling such as Selenium. Setting up your own HTTP Context and Request/Response lifecycles is too much to ask for a Unit Test to be honest. It's too error prone to be of much use. Not that you can't do it, you certainly can. But we tend to mock HttpContext in Unit Tests for that reason, you'd be emulating the browser/server infrastructure, a whole lot of moving parts. Definitely interested in seeing what others suggest though!
ASP.NET Core has a standard way of handling this via [TestServer](https://visualstudiomagazine.com/articles/2017/07/01/testserver.aspx). Selenium is a higher level, more for how a browser would render a website/clicking between pages/etc.
Even better!
Fun fact: ASP .NET Core [was once called ASP .NET 5](https://www.hanselman.com/blog/ASPNET5IsDeadIntroducingASPNETCore10AndNETCore10.aspx). That's when I realized how early end-of-life (or at least maintenance mode) actually seemed to be for .NET Framework.
Forgive me if I am wrong here, but when I have this line of code: var context = new DefaultHttpContext(); I can set the next line as a breakpoint and I can look at context and it generates an entire context for me, so I think I have it mocked out, maybe? I think I just need to figure out how to apply the changes i have in the startup.cs to it as well as populate parts manually to see if the data is correct. Also, I saw what /u/Trout_Tickler posted and maybe I will try to read through that first, just hope I am not over-complicating things.
Thanks for the warning of the journey I was about to go down. I'll check out the article.
I've been where you are right now with Unit Testing. It took many months of frustration to appreciate what Unit Testing was all about, especially in a Web context. I'd hear "Don't test .NET, only test *your* code" and I sat there frustrated thinking "How the blazes can I test my code works without testing .NET!? That makes no sense!" If you wrote a method, then you can test that method. Each method should do one thing. Thus lending itself nicely to a Unit Test. Test that one method. In your case, you have a method producing a string. In your mind you have a method *being used* to create a HTTP Header. That's where I think you're getting hung up. .NET creates the Header for you. You just need to make sure your method, with a set of given parameters produces the string you're expecting those parameters to create. Nothing more. An Integration Test is where you'll be looking at the whole view. It's no longer *just a String* in an Integration Test, it's a string being used for useful things like HTTP Headers! The Unit Test assures you that a string is created and correct, then if you go on to modify that method at a later date, your Unit Test will inform you that your assumptions about that string's correctness are either still valid, or have changed. That's the beauty of the Unit Test. Not that it creates a whole journey, that method is but one small (yet important) part of the journey. The Integration Test will tell you if the journey has the correct destination.
Thanks for the explanation.
Correct, DirectoryServices is available for Core.
Yeah I figure they renamed .NET 5 to .NET Core because the initial version was going to be more bare-bones and would NOT be a replacement for 4.x, which 5 would suggest. Then of course the ASP.NET rename naturally followed.
[removed]
Let me reiterate. If something is included in .NET Standard it won't move anywhere. Check [.NET Standard versioning rules](https://docs.microsoft.com/en-us/dotnet/standard/net-standard): &gt; * Additive: .NET Standard versions are logically concentric circles: higher versions incorporate all APIs from previous versions. There are no breaking changes between versions. &gt; * Immutable: Once shipped, .NET Standard versions are frozen. New APIs first become available in specific .NET implementations, such as .NET Core. If the .NET Standard review board believes the new APIs should be available for all .NET implementations, they're added in a new .NET Standard version. 
&gt; My view is narrow? Yes. Your understanding of how large organizations and large teams works seems to be somewhat limited. Of course, you took it to mean something else which is just constructing a nice strawman so you feel better, but is neither here nor there. &gt; It's useful to Windows, And what use is that in today's age? &gt; "No offense", followed immediately by a direct insult. Because it was not meant as an insult but an informed judgement on your tone and comprehension of the subject. If I had intended it as an insult I could have been more vulgar in my word choice and not included the no offense. One adds qualifiers, like no offense, as an indication that they understand that it could be construed as offensive or an insult but is not **intended** to be so. &gt; Great way to convince people of the merits of your argument. I don't have to. I already have. You are the one resisting and not making a valid case. &gt; WinForms support in .NET Core is strategic, It is LEGACY. It is there for legacy reasons... the question is what about it's future? That would be strategic. To be strategic is to think forward, there is no strategy to the past... that's just history. &gt; and you have zero evidence that it's taking anything away from Core development. WRONG! Firstly, this isn't just about Core. This is about the entire dotNet ecosystem and the health of the development community that MS stewards. Second, I have already indicated that they have finite resources and even shown some difficult decisions they had to make in the past due to those finite resources. If you've ever read Eric Lippert's blog you would understand what I'm talking about. Lots of times he answers questions like "why didn't you guys do this" and his answer nearly always includes "because we didn't have time". So, to repeat... **MS does not have unlimited resources." &gt; WinForms in Core, even Windows-only, allows for a faster deprecation of .NET Framework and therefore more focus on Core.... I'm not sure at what point in this conversation you became so myopically focused on Core. But this is only partly about core. I'm talking about DevDiv and their focus as a whole.
Ah, I'm out of date. AD's not something I use day to day personally.
Hi! Looks like this fix the problem. Thanks a lot for your help!!!
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/csharp] [Cyber Monday Deal - Complete ASP.NET Core 2.1 Course on Udemy](https://www.reddit.com/r/csharp/comments/a0mb5x/cyber_monday_deal_complete_aspnet_core_21_course/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
Just a heads up - Udemy is always running "deals" that are around $10. I think they were all $8, three weeks ago. $10 is still solid for much of their content, but just remember to never pay full price there. 
What’s the scope? Just to a single class?
Depends on the content too. Do your homework, look of the course teacher, and check reviews. But yeah, you’re right. 
Just one point, you dont test methods, you test small bits of functionality or one 'thing'. It may seem trivial, but its important.
Yeah just the one class but code to be shared between 2, maybe more
I'm prefixing an existing string with the machine name so it will require the original string as input. But still a simple static helper. This is the quickest and easiest solution I think. I would have done this straight away if I wasn't doing TDD. For TDD I was taking the micro test approach and thinking "what do I have, and what do I need". Which lead me down this route of I could have something simple like a helper, or something with a bit more forethought like a factory or even put a decorator around the original class. So far a nice easy helper class is what I'm going for and I can refactor later ;) . Thanks for the reply!
What do you mean by “code to be shared between two”? Two classes will use this functionality? Without knowing all of the details, I’d say if a single class needs this functionality, I would just make it a private method in that class. If two classes need it, I’d still keep it as two sets of private methods. Once you get to three classes, sure then write a tiny class that is injected in, or (I’m not sure what C# offers) you could maybe use AOP. It seems kinda much to inject in a class just to prepend a string. Maybe you could have some static helper class that has that method? I’d only do that work once you actually end up building out those other classes, unless you 100% know you will build them out.
Ha, I like your answer. Going from VB.net to C# for some new work this is exactly how it looks on the surface to me.
You’re doing the right thing.
Few years behind my original thought to make the switch back in 2008... Better late than never, right?
Yeah, I have worked on (and still occasionally work on) projects that have a mix of VB.Net and C# in the same solution. I often copy a chunk of VB code and paste it into C# or vise versa. Then it's just a matter of editing the syntax, line by line. Obviously almost every line has to be changed, but the changes are trivial. The concepts and the flow of the code is almost identical. So much of .Net programming is just farting around with object properties and that translates across completely.
Win10 PRO has hyper-v included
Pluralsight is 33% off right now too....
Don't look back
Microsoft creates a `&lt;PackageName&gt;.Abstractions` package that contains most of the main package's interfaces (in the same namespace as the main package). This allows other projects to reference just the abstractions package with minimal references. See [https://github.com/aspnet/Extensions/tree/master/src/Configuration/Config.Abstractions](https://github.com/aspnet/Extensions/tree/master/src/Configuration/Config.Abstractions) as an example.
As funny as it may seem, I actually used online code translators for a few weeks. Took me a little while to memorize how things were done in C#.
I used this method as well
Don't forget also to rename Dim to var, and you are mostly done. 
How is that 56 lines ? Could you show an example ? It might be that your class is doing more than one thing if there is 10 parameters passed to your constructor. Maybe you should try refactoring your class into smaller classes.
The reviews are deeply flawed on Udemy. Students usually review the course within the first few lessons during the easy stuff that's being taught. Then they hit the more difficult lessons and quite often it's not explained well or not in depth enough and students leave. But they've already left the 5 star review! Never ever ever leave a review until you have made it all the way through the course, or have almost completed it. This is one of my only complaints about Udemy. They should only allow reviews once the course has been completed by the student, or 2/3 of the way through the course. 
Have fun enjoying your life!
Ohhh good call!
\&gt; Every interface should be in the same folder than their implementation classes. &amp;#x200B; WTF does this mean? I don't understand the grammar chosen. &amp;#x200B; Are you saying to put your interfaces alongside your implementations? This is typically a poor idea. Having a contracts namespace separate from your concrete implementations is typically going to fair better.
There is a book called "Dependency Injection in .Net" and it explains how to do DI well. It also shows many ways of doing it poorly. The advice in that book is that if you have more than about 5 injections, that class might be doing too much. You can often encapsulate part of that logic into another class that can then be injected and handle some of the work in the "big" class. &amp;#x200B; With that said, I don't understand where you're getting 56
Are these the "contracts" ?
For majority of my interfaces (the interfaces that designate my boundary), I keep them in my Core project which handles my Core logic, stored in a folder by their function. My other projects will reference these interfaces to build off of it. For interfaces that are shared due to common functionality such as an IRule that a sole project depends on, I will keep it only in that project and it will not be exposed to the others. Due to Visual Studio handling the updating of the interface or just clicking through to get to it, this is why I group by function.
You made something in the project called `closeDescriptionPopup` and what you're trying to run can't see or find it, so it errors out as undefined.
This might be something net core would create automatically? Because I didn't write that or the function it's looking for hence why I think something might be missing. 
We can't see what you're running, so that's all the help I can really give. Do a quick search for `closeDescriptionPopup` in your solution to see if it comes up anywhere else. Otherwise yeah, you might be missing a file or something.
That’s debatable. You’re looking at two different packaging schemes - by layer and by feature. He’s advocating to do it by feature, you’re advocating for by layer. By feature is fine and honestly often preferable, because if you do it by layer then you end up with poor encapsulation...i.e. public everything.
You might be right because I added a boot strap reference in my wwwroot files. I'll just revert it and try again. 
This free Microsoft video does a good job of explaining why you'd want to use DI using an example: https://www.youtube.com/watch?v=QtDTfn8YxXg And while it is using Autofac (rather than .Net Core's built in DI) and there's an annoying part in the middle where you're stuck looking at their backs, it is worth a watching to get a good idea of when DI would be useful. 
Awesome thank you so much! I’ll check this out! I just need a basic understanding and I feel like the rest should come naturally after working with it but I could be way wrong.
First, understanding IoC as a concept is import. With DI you are handing control of object instantiation and object lifetime control over to a dependency container. It is responsible for figuring out your code’s dependency tree and creating objects. The injection of class dependencies is nothing more than this: the container is initializing a class, figuring out what arguments a constructer requires, building those classes, and then “injecting” them in the original class. You only have to worry about registering anything you want to be “injectable”. In other words the container needs to know what it can build and how it maps to and interface or type. There are multiple benefits. Testing is made much easier by having code built with injection in mind. Object lifetime management is also handled for you, you can have a true Singleton instance without actually coding anything different in your classes.
A basic understand is useful, but also understanding why the alternative sucks is helpful. The former you can definitely learn, the latter can kind of be learned but also has to be lived (i.e. do it wrong once to see why it sucks). 
I just don't see a point in this. The main problem with primitive obsession is that interesting behavior is not part of objects, but is thrown around the code base. When you turn a primitive into an object, the code working with this then gravitates towards it. It becomes obvious and trivial to refactor the code to be part of business logic of this object. Stuff like email having a parsing logic. Or some other type having specific equality logic. Etc.. In this light, I don't see how this library makes sense.
That video was absolutely amazing. It makes a lot of sense now. It can do a lot of cool things. When he showed the Stage 3 of how you can use the ContainerBuild and Container, it blew me away. Awesome recommendation and thanks for the help! I guess I just need to learn about how it works with .NET Core now and not Autofac but it seems like they are pretty similar from what I've seen so far. Thanks again!
So that small pieces of an object can't be confused. So that a `LastName` can't be assigned to a `FirstName` variable, but you can `.Format()` them. So that a `Meter` can't be assigned to a `Kilogram` variable, but you can multiply them by `2.0` easily.
Funny, as the .NET Frameworks time is pretty much over. .NET 4.8 will likely be the last version released.
&gt;So that a LastName can't be assigned to a FirstName variable And that is good why? How many errors in software are cause by mixing up variables like this? &gt; So that a Meter can't be assigned to a Kilogram variable But when I multiple Meter and Kilogram, do I get Meter\*Kilogram? F# has this system. It is great. But this is far from it. &amp;#x200B; In both cases, the ideas are good. But it doesn't go far enough to make it useful. The need to define your own classes and write more complex type definitions is not outweighted by advantages.
You are welcome. For some reason MySQL team isn't addressing this issue. So until then We can use pomelo ef package.
Like very long methods, very long constructors like this are normally a good indicator that this class can/should be refactored into smaller classes. You might find that the behaviour can be more easily tested this way also. For example (and this is hard to review with a very small snippet of code and no context so bear with me), does the EmailController need a reference to IGenerateCalendarEvents? Or can the events be passed into the controller method or be part of the ICalService?
A quick review on mobile suggests that it is nearly/mostly all server-side work with virtually no client-side stuff. Can someone who has taken the course confirm? I have been looking for a Udemy Core 2.1 course that isn’t infested with angular/react/vue distractions and actually focuses on *only* the Core 2.1 stuff. If this is client-side-JS-free, then it looks like I missed it by a country mile when I was looking.
This is why if two courses have the same or similar score, but one has been scored only 200 times to the other’s 12,000 times, I will always consider the latter over the former. It’s no different than judging an eBay seller - a seller with 99.5% on only 200 sales is a lot more questionable than a 98.2% seller with 12,000 sales. Considering that there will always be chronically biased users, a larger voting cohort always garners more accurate statistics.
Article makes no sense.... grammatically.
My friend let me tell you one thing that here, I am not talked about Latest .NET Framework or stated title that "Infor on Latest .NET Framework". I talked about whole .NET Framework, it's importance in todays world and how it becomes future of all enterprises of all around the world.
I don't like primitive obsession, but sadly I don't think this helps much. We're wrapping primitives in new primitives?
What do you mean by that? I guess it depends on what we mean by "rich domain models".
From what I know, rich domain models are a way to avoid primitive obsession. What this means is for example, all emails are strings, but not all strings are emails. So you create an email class that contains the logic regarding emails. By using these 'rich domain models' you are avoiding primitive obsession. As OP was saying, now we can't assign a string to an Email without passing some validation in creating the model.
You know a whole ton already :), you are absolutely right in that there is much more to gain other than removing potential bugs when assigning variables. Thanks for explaining, I agree with you.
I find a similar approach very useful to model Value Object.
The code is interesting. I'd at least parameterise the location the files are written. Not everyone builds to ./bin/debug. Then there's some stuff that doesn't seem to make sense. One example is ```method.Name.Contains('_') ``` is a criteria for a member being ignored. I don't know what the point of that is. I'm with /u/Euphoricus here and I have a few others concerns. The GC pressure will be significantly higher with your code in fast moving scenarios where many of these types are being generated. It's a great effort though. You've seen a need, written the code, published it and ask for comments. That's a nice combination of effort and bravery, so well done :)
&gt;Article makes no sense.... grammatically Can you please share at which point you find "Make No Sense Grammatically " at all. Specify it so that I can help you out.
Disagreed completely. You’re underestimating the usefulness of tools that guide developers towards correct code. You’ve obviously never made a mistake in your life, but it can (and does) happen to mix strings, especially when they’re all just that, strings. I’m not vouching for this particular library but your mindset is worrying. Anything that helps correctness and related habits should be welcomed and improved upon.
I know this is an ad, but I'd think twice about using the consulting services of a company that believes this. It seems very dated. It talks about PHP vs ASP.NET but no mention of Node? SQL and Azure doc db, but none of the many OS competitors like PostGreSQL, Redis, ElasticSearch, etc, etc. .Net Core gets one tiny mention, the IOT section is made up as is the Safety and Security section. No mention of containerisation in Safety and Security? 
There’s aggregates, entities, value objects, etc. Value Objects are just as important for a rich domain model. Value Objects should carry conforming values at all times. Entities should focus on the policies implemented by their business rules, of which VOs might as well be part of their input. Where value object validation happens is another matter (a service? a constructor?). Do not underestimate the usefulness of static typing. It’s a way to let the compiler enforce rules for you.
I think that page you're debugging might be something internal to Chromium's error page: [https://github.com/chromium/chromium/blob/b0432d14fe6b6e429c62fe438bd985cb79c6c505/components/neterror/resources/neterror.html#L45](https://github.com/chromium/chromium/blob/b0432d14fe6b6e429c62fe438bd985cb79c6c505/components/neterror/resources/neterror.html#L45)
I'm pretty sure you don't need interfaces to mock out stuff in C#. Not on a real computer to check that, personally I dont mock out service layers or repositories so Im not 100%.
Use orher containers if you need more advanced features.
Like the idea but probably won't be using it too much. Mainly because my company doesn't let us use external dependencies without review first, but also because having everything as a separate struct usually takes things too far. For those who don't understand why this is useful, we had serous bugs(and they still occasionally appear) because the initial developers of our software used strings to represent file paths. Which was fine until we had multiple inputs and they wrote things like path1 == path2 which is not correct because one of them might have the path in lower case and the other in upper.
 &gt; How many errors in software are cause by mixing up variables like this? Did you heard about Mars Climate Orbiter [1]? [1] https://en.wikipedia.org/wiki/Mars_Climate_Orbiter 
**Mars Climate Orbiter** The Mars Climate Orbiter (formerly the Mars Surveyor '98 Orbiter) was a 338-kilogram (745 lb) robotic space probe launched by NASA on December 11, 1998 to study the Martian climate, Martian atmosphere, and surface changes and to act as the communications relay in the Mars Surveyor '98 program for Mars Polar Lander. However, on September 23, 1999, communication with the spacecraft was lost as the spacecraft went into orbital insertion, due to ground-based computer software which produced output in non-SI units of pound-force seconds (lbf·s) instead of the SI units of newton-seconds (N·s) specified in the contract between NASA and Lockheed. The spacecraft encountered Mars on a trajectory that brought it too close to the planet, causing it to pass through the upper atmosphere and disintegrate. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/dotnet/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
&gt; Microsoft Corporation’s [**.NET Framework**](https://www.kunshtech.com/Blog/Post/2018/6/6/microsoft-asp-net-framework-for-web-and-mobile-app-development) is the archetype of technological evolution that is flying above all the disruptive entrants in the IT industry since the beginning. ASP.NET has an extended support for a virtual machine. Thus, the application is written in C# can be executed. The recent updates and additional features prove that this is one is continuing to be in-game for a long time. The very first paragraph, for example. &gt; Most of the time, the choices we are making for business needs, definitely seem winning. Yes, much winning
Can you explain how would using this library fix this problem?
What is IOverdueStudents, IDBCallsSessionDataDTO, ICalService,IGenerateCalendarEventsForControllers?
If you needed them you would know. Short answer yes, if you need more advanced features such as interceptors, decorators etc.
I'm not sure that this should be considered a tool that guides developers towards correct code. It is obviously an implementation with loads of implications and those implications have a cost. In my mind this library doesn't solve the problem fully and since the costs it brings I would try to find another solution if I were interested in the problems it tries to solve. 
The thing is you cannot achieve the same mocking functionality without using interfaces. You can get close by marking every method virtual, which BTW if you forget to do, it gets silently called from the tested method despite your mock being configured in the strict mode.
Maybe there is some valid use for this, but the examples in your readme look extremely unnecessarily convoluted. I could see *maybe* doing this for something very specific like an ISBN number. But if I saw this used for first name, last name, etc, I would probably get arrested for assault. 
Hi, I like the initiative. Pdf is awful to work with. Here's a suggestion: GetPage(1) is potentially unintuitive (1 based index), I'd either expose indexer which is more idiomatic for a developer, or my prefered way a Get propery which gives you IEnumerable&lt;Page&gt;. I'll look more into this later.
First, I think it's important to distinguish if you're publishing an app, or a library/libraries as packages. For libraries, I'd prefer the model where you have a single core project, and each implementation has it's own project, so you constrain the dependencies required for any given user. For example... You don't want to force the consumer of a Redis implementation to import a SQL Server dependency if they're only using your Redis implementations. For an app, I used to try to decompose large solutions into major functional areas or tiers, and try to leverage the internal facet. Maybe we hit the wrong levels of abstraction, but this usually led to a lot of unnecessary pain, and problems with things like package and framework dependency version stratification. The idea of forcing encapsulation by internal key word within an app team seems antithetical to the idea of a team that all has access to the same codebase. Now, I prefer the pattern of simply having a core project, which should contain basically all implementations, and then separate packages for each desired runtime... A web API project, a command line project, etc. These take care of their deployment, runtime, packaging, and configuration source concerns... And reference the core project. This also eases problems with managing transitive dependencies (System.Net.Http anyone?) Anything else just feels like too much overhead, and generally leads to pedantic disagreements.
An important thing to consider to add onto this, your container should sit as close to Main as possible and not bleed into your code base. I'd imagine in most situations you would run into the limitations of Microsoft's DI container long before the cost of switching container is a significant concern. 
PDF may have custom page numbering (e.g. title and toc does not count towards page numbering). So if you have `IReadOnlyList&lt;Page&gt;` to access list of all pages plus you give a method `GetPageByNumber` that returns page by semantic number used in the document. 
There is. Type all properties you need into the class you want to inject into -&gt; right click where the constructor ought to be(I like to use the keyboard shortcut for that) -&gt; Quick Actions and Refactorings(downArrow then Enter) -&gt; Generate constructor(instant enter) -&gt; then leave all properties selected(enter).
As I said, I’m not vouching for this particular library, but the attitude of bashing stuff is wrong. Moreover this is why the open source dotnet ecosystem is lacking compared to other technologies. Loads of implications? Give me a break. Could it be improved? I’d tend to agree, but no one shared an improvement so far.
The needing to create empty classes in the example struck me as odd. I would expect that creating a \`Scarp.Primitive.String&lt;LastNameTag&gt;\` would result in the constructor searching the Code DOM for \`LastNameTag\` and if it's not found, then creating an empty class and adding it to the Code DOM at runtime.
This is a very good point and something I'd completely failed to account for. It looks like I will have to change pages quite radically for the next release in light of this. 
Trying to understand here. If I were to come up against that problem, I would either use a library to handle paths comparison, which I know can be painfully complex if you need to handle special filenames for example. I would wrap a path property in a Path class or something like that which would handle the correct equality check. What is the benefit of using a solution such as OP's library versus a good old class? How would you handle the equality check with OP's library? I struggle to see the structure of a program using this typing system.
&gt;but the attitude of bashing stuff is wrong My intention is not to bash stuff. My intention is to compare complexity created and complexity removed. And this specific library has lots of (weird) complexity and doesn't remove nearly enough of real complexity. Creating simple struct/class might take more code, but is much more flexible and solves more problems. &gt; Could it be improved? I’d tend to agree I do not agree. I believe I'm highly experienced in C# and solution like this is limit of what C#'s type system can do. Yeah, maybe adding interface to Tags to allow for checking on parsing. Or extension methods that work on specific types with specific tags. But then, the complexity and "unconventionality" of this solution makes it less optimal than creating your own plain classes.
I guess I have typically found a good componentization of applications because I've never run into problems by separating problem domains into individual projects. It creates cleaner code in the long run because the outside world __only__ gets access to the parts that are intended to be exposed. In one giant core project it's __very__ easy to abuse inheritance and call upon objects and methods that aren't intended for general usage.
Cool project :) If you're into functional, or "railway oriented" programming: F# has that kind of assignment protection baked into the language as "Units of Measure". It also has a standard Result&lt;T. E&gt; class, and exhaustive pattern matching. It's also got a better type syntax and modelling concepts (disciminated unions), that make rich domain models a dream to model and work with while retaining minimalism and the utmost tersness, if you're into that sorta thing :)
I wonder if this is becoming one of those "it depends on the app" kind of things. What I'd you didn't think about overlying architecture but instead just started an app really small and let it grow organically, while still applying SOLID principles, keeping things decoupled AS NEEDED. Where would you end up? I definitely like the approach where you start with a single project, [keep things organized with folders and namespaces](https://makingloops.com/how-to-organize-your-projects/), and then organically add projects where you need to want to isolate a library to limit third party dependencies.
I've found myself unpleasantly in several of those types of arguments.. I think this is a good strategy to start with and maintain for the long time. Basic core project, organized with folders and namespaces, and then separate projects for the different deployment frontends. Make the decision of adding a new project very deliberate and thought out.
All of my dotnet core projects these days are rest apis. I create a single project per web service (I'm not doing microservices) and in it have a controller layer, service layer and then ef. I don't use a generic repository or UoW, I feel ef serves that purpose closely enough. The service layer is always an interface and I run my tests of of that. I use dtos between layers and for validation coming into controllers. My controllers are pretty thin, mainly handling mapping dtos (automapper) and catching exceptions / logging and returning data if needed. I have been storing files in folder by type (dto, service, controller etc) and have thought about storing by feature but most projects are pretty manageable. I've seen example projects on github where all of the various layers are in separate dlls etc, but since it's just me or maybe one other person working on a project ever, that seems like overkill. By having things split up in service layers my build times are really fast (like 2 seconds fast) since I'm usually only changing a handful of files at a time. I do like to put the backend api and frontend site in the same git repository as two child folders (api &amp; spa) so that changes are easier to track and there is less time spent dealing with git (even though I *love* git.) When I get core functionality that is duplicated between projects I do tend to put it into a nuget package to make it easier to share - but it needs to be significant for me to introduce that overhead.
&gt;This layer contains classes for accessing external resources such as file systems, web services, smtp, and so on. These classes should be based on interfaces defined within the application layer. Can anyone explain the reasoning behind this? That is, why are the interfaces for these classes defined in the application layer, but the actual implementations which access external resources kept in another layer (infrastructure). More specifically, what are the benefits of doing this instead of having both the interfaces and the implementations within the infrastructure layer
OK, I'm out of bed and can answer some things a bit more now. The code generator project was so I didn't have to copy/paste everything, it's not actually part of the "library", it's how I _built_ the library. If I remember correctly, there are some property getter/setter functions that contain `'_'` that I didn't want to copy over. Similarly, the ./bin/debug output location is just how this code is organized, not for users in their own projects. And no GC pressure since everything is a struct. I remember Joe Duffy's blog series on the Midori project and he mentioned that being a serious impediment to high performance. Not to mention the much newer `struct Span&lt;T&gt;` showing up now.
Thanks, that was the example I was literally too lazy to look up!
I'm very open to suggestions for improvement, just get in touch with me on the Interwebz in the easiest way for you.
[Apparently not.](https://sharplab.io/#v2:EYLgtghgzgLgpgJwDQxASwDYB8ACAGAAhwEYBuAWACgcBmIgJgIDEB7FggbyoJ6LpMIBBABQBKTt15ScAdgIAiCMADG8ggGpJU7VPkATOADN5FSlIC+WgldpFihAELC0AOxgEAHkgKv3AT3EuMx0iOQASRRUOD3M1TWCQnQiDQw4/WNMLKSpzIA=)
The idea is that you can replace your infrastructure layer with something else which implements the same interafaces. E.g. One flavour for the file system, one for AWS Storage, one for Azure Storage, etc.
Here's how I understand this. Someone please correct me if I'm wrong. The reason they define the interfaces in the Application and Common projects is because of Dependency Inversion. These 2 projects use the interfaces directly so it makes sense to also define them in the same project. i.e. They are the ones defining the contract that other projects must implement to use them correctly. The Infrastructure project is where we make the concrete implementations of the interfaces so the whole application actually does something. Now, if we wanted to change how the INotificationService.SendAsync method worked, we could easily create a concretion in the Infrastructure project WITHOUT modifying the Application project at all. e.g. (Sorry for the bad example) We could make a new notification service that only sends if the message contains the words "Error". Well we don't want to modify the current NotificationService so we make a new ErrorNotificationService and implement the method to check for "Error" in the message. Then we can update our application to use this new implementation without modifying the Application project.
&gt; Maybe a simple string.Equals(path1, path2, StringComparison.OrdinalIgnoreCase) would be enough? I'm not sure what /u/Harag_ was doing with those paths, but if he's trying to see if two variables point to the same file it's vastly more complicated. Even ignoring symbolic links, you have to watch out for relative/absolute differences (e.g. `c:\users\me\doc\file.txt` == `doc\file.txt` == `\users\me\doc\file.txt` == `~\doc\file.txt` (in powershell at least)). Plus separator normalization (`/` works just like `\` in Windows APIs)
Hmm, so I guess that makes sense, since both of the $ symbols are interpreted as separate calls to the string.Format method. But, this does seem like a missed opportunity for some compile time optimisation? Or, am I missing a reason why this is actually really hard to do at compile time? I've not seen that 'ShapLab' tool before, so thanks very much for posting that!
If I were using microservices, I'd probably split the problem domain up into them. I.e. a service for authentication, a service for backend administration, a service for clientside app, a service for maintenance tasks. Or split up to help with resource scaling. My projects are small enough to just put all that functionality in a single service, with different endpoints (they're all sharing the same storage, i.e. a sql database.) 
Thank you, as you say there's a lack of good free tools for it which motivated me to start this using the Pdfbox code. A contributor from PdfPig made a wrapper for a C++ lib here https://www.nuget.org/packages/Docnet.Core/ which provides a useful alternative while PdfPig is still in its early stages. The real challenge as you say is the PDF spec which is huge and not really viable to approach for free. For fonts alone you need to support: Type 1 Compact Font Format TrueType Type0 Complex fonts Type 1 and TrueType Possibly OpenType Each of which is sufficient to form the basis of its own library (in fact I hope to make the API for fonts good enough to package separately as PDFBox do with fontbox I believe). This is why each release of PdfPig so far has been fairly sparse on functionality, each feature requires a massive amount of code, but hopefully by releasing early and often something helpful will come of it even if I end up abandoning it. 
I may actually have a need for a .NET Standard PDF library. Right now I am using a .NET Framework one (iTextSharp). Sorry to hear you don't support PDF creation, that is something I would need. 
The reason why the interfaces are inside the application project and the concrete implementations of those interfaces are inside the infrastructure project is simply to achieve dependency inversion. If you place both the interfaces and concrete implementations of those interfaces inside the infrastructure project, then your application necessarily must have a direct reference to the infrastructure project. Why is a direct reference from application to infrastructure bad? Because by having that direct reference, you're removing a constraint from the application that it must communicate with infrastructure via an interface. Sure, you still *can* use the interface, but since you have a direct reference to the concrete implementations, you no longer *must* use the interface. With a direct reference, you're relying purely on developer discipline rather than an actually enforceable constraint. Why specifically is it important that the application owns the interface? The idea is that you're completely flipping the responsibilities around. Instead of having your application be built around the details of your data access mechanism, you're building a data access mechanism to specifically suit the needs of your application. The details of specifically how that data access mechanism provides data is merely an unnecessary detail to your application. If you don't think about data access as a detail, and instead you think of a SQL database as a SQL database while designing your application, you're going to be very tempted to let those details leak into your application's design. It's almost impossible not to. Why is this bad? Because you're going to necessarily make it more difficult to plug in a different data access technology. "Plug in" is the key phrase here. You want to be able to swap out infrastructure without your application caring or even knowing. Swapping SQL for a DocumentDB is a common example of what's possible with this discipline, but I find that example to be rather unrelatable and contrived. Consider something smaller like swapping out one Geolocation API for another. If your app defines its own interface for accessing Geolocation data, then it's entirely possible to swap out Google Maps for Open Street Maps or anything else. It's more likely that your application will want to swap out some smaller piece of infrastructure like the example I just gave than something massive like completely eradicating a SQL server, but the same disciplined approach to solution architecture is what enables either/or.
You can check out this library by a contributor to PdfPig: https://www.nuget.org/packages/Docnet.Core/ I'm not sure what the licence is like for that but it seems like it may meet your needs. 
I haven't yet found a need for a 3rd party DI library.
Do you need a complete source code?
It would result in a compiler error as a "pound-force seconds" type number would be attempted to be assigned to "newton-seconds" type variable.
The Mars Climate Orbiter example cited by others might make a good case to build an example around.
The Mars Climate Orbiter example cited by others might make a good case to build an example around.
Code DOM allows you to generate dotnet code and thwn export to c# which you then compile and load the new assembly. 
CMYK support is also not very common while required by print centers. I don't know how much work that is. Most people seems to make PDFs now from HTML via headless Chrome, which I fully understand. It just abstracts the pain. (But again, no CMYK support there) &amp;#x200B; &amp;#x200B;
This is the normal price. I've taken this course, and a much much better course would be this one: https://www.udemy.com/build-an-app-with-aspnet-core-and-angular-from-scratch Just ignore the Angular sections
This one is incredible. The one that finally allowed me to get it, just ignore the Angular sections, the REST api stuff is great: https://www.udemy.com/build-an-app-with-aspnet-core-and-angular-from-scratch
:)
I haven't even looked into colors yet (I intend to, but after the first release which supports creation) but that's another example of where what should be fairly trivial is a massively complicated process. There are different color spaces and they can be mixed and match and though I've only scrolled past it to get to other sections, it looks like a meaty portion of the spec. 
Performance matters. You should check benchmarks.
This has been working for me, after many attempts I settled for this. Usually, the type of projects I work with consist in a solution with one or many web applications, either On Premise or for Azure, and sometimes one or many desktop clients for doing specific tasks. These projects usually have several bound contexts, not just the one. So, supposing the name of my project is MyProject, I create the following structure. * MyProject.Domain.dll - common definitions for entity, value objects, repositories, domain events, domain services, etc. * MyProject.Domain.Common.dll - usually commonly used value objects between bound contexts (for example, Address, ContactInfo, UserName, etc). * MyProject.Domain.MyFirstBoundContext.dll - the domain entities for a bound context, as per DDD. * MyProject.Domain.MyFirstBoundContext.Queries.dll - directly perform database queries (I create a Query, QueryHandler and QueryResult class for each query, the QueryResult being usually a DTO, or if the query returns many items, QueryResult contains a collection of an internal Item class which itself is the DTO). * MyProject.Domain.MyFirstBoundContext.Commands.dll - execute the DDD logic, and it heavily relies on \*.MyfirstBoundContext.dll (I create a Command and CommandHandler per each application action, and a generic CommandResult to indicate success/failure. CommandHandler also executes any event risen by the domain). The CommandHandler.ExecuteAsync method should \*never\* return. * \[repeat for each bound context\] - if it applies, I create several domain libraries, one per each bound context. * MyProject.Infrastructure.Database.dll - defines the database, in my case I'm fond of NHibernate, so all NHibernate stuff goes here. All the repository classes from the domain are implemented in this library. Additionally, in the \*.Queries libraries, I usually define abstract QueryHandler interfaces, and I implement them here as well. * MyProject.Infrastructure.Client.Database.dll - when doing web apps that also require access from desktop clients, I usually need a local database that will eventually sync with the server database. * MyProject.Infrastructure.Security.dll - usually encryption stuff, licensing, authorization, etc. * MyProject.Infrastructure.Services.dll - any other services I might need. For example, sending emails, connecting to Office 365 services, logging, etc. * MyProject.Application.dll - common classes for a web site. A base Controller usually goes here, and here I define common objects (most of my cloud-oriented projects involve one or more web apps, so anything to be reused goes here). The base controller has a CommandBus, from where to send commands, and I add any mappings for dependency injection here. This library usually has a dependency with [ASP.NET](https://ASP.NET) Core libraries. * MyProject.Application.Automation.dll - sometimes I need to create PowerShell commands, so they go here. * MyProject.Application.Services.dll - common application services, usually background tasks, Azure webjobs and Azure functions. Sometimes, when the project requires to work on web apps and desktop clients, I add two other libraries: MyProject.Application.Cloud.Services.dll (for webjobs and functions) and [MyProject.Application.Web.Services](https://MyProject.Application.Web.Services).dll (for web-specific services). But it doesn't happen quite often. * MyProject.Application.MyFirstWebApp.exe - the web application itself. Most of the times, I end up with many web applications. * \[repeat for each web application\]. * MyProject.Application.MyFirstWebApp.Api.exe - when I need APIs isolated from the app, for example when we're planning to introduce mobile apps later on. * \[repeat for each api\] * MyProject.Application.Client - when I need to provide desktop apps as well, here goes the common stuff. * MyProject.Application.Client.MyFirstDesktopApp.exe - usually a WPF application. * \[repeat for each desktop application\]. Most of these stuff goes with an associated \*.Tests.dll for unit testing purposes. In the past, we used to have utilities for doing one-time configurations or rarely used stuff we don't want to expose in the UI. Now we put all that stuff into PowerShell commands, hence the \*.Automation.dll library. My two cents.
I was just really grateful that someone read more than the README.md!
It's interesting. I do tend to poke around if the repo isn't huge just to get a feel for it :)
This is okay if you have clearly defined domains. I don't have any personal projects, but every job I've had, this has never been the case as I'm walking into a situation where there is a pre-existing application. The only time I get to truly draw lines in the sand so to speak is if I'm hand-rolling a custom API for a client to access my company's software. Other than that, there is too much overlap unfortunately.
To me it seems like something that would be very low on the priority list when working on a project. I have my preferences as everyone does surely, but to really dig one's heels on this particular issue seems ridiculous to me. I'd rather argue about tabs vs spaces. (not saying you were digging your heels in, just that it's ridiculous that anyone thinks this is such a priority as to cause a huge argument about it)
This looks like some JavaScript from one of your CSHTML files probably.
Interfaces are only needed if you want to be able to class A use class B without being specifically aware of class B, or perhaps the the assembly class A is in can't depend directly on class B's assembly so you need an interface to act as a bridge in some other assembly. Or something like that. ASP.NET Core uses interfaces extensively for this case. You can define a lot of ASP.NET Core code without having to depend on ASP.NET Core since the interfaces exist. This is useful when writing class libraries since you don't need those dependencies. In your case unless you need that I would forego the interfaces. I made a hosted service without one and am not having problems.
I am not talking about hyper-v. Microsoft released a different package for Win 10 called the Windows Hypervisor Platform (https://blogs.msdn.microsoft.com/visualstudio/2018/05/08/hyper-v-android-emulator-support/) which is not available on Server 2016.
The way it currently works requires (at runtime) two string.format calls, and one string.concat. If the compiler identified that $"{a} " + $"{b}" was equivalent to $"{a} {b}" then the runtime overhead is reduced to a single string.format. 
Also your variable names are no longer case insensitive.
Ah, I see now. That makes sense 
Yeah, it's a bit of a weird one. I've been happily splitting interpolated strings over multiple lines without thinking too much about it. Turns out to be (slightly) inefficient. 
So how do you instantiate your classes without using Interfaces? Just New them up?
I agree. The argument I recently got into was more about the absurdity of the commenter getting bent out of shape because I was suggesting one way of organizing projects, and this particular asshole had just read some alternative advice that he thought was holy gospel. 
Here's a bit from my startup.cs: public void ConfigureServices(IServiceCollection services) { services.AddMvc().SetCompatibilityVersion(CompatibilityVersion.Version_2_1); services.AddSingleton&lt;Scheduler&gt;(); services.AddHostedService&lt;HostedServiceWatchdog&lt;Scheduler&gt;&gt;(); } You can see a couple things being instantiated there. ASP.NET Core will automatically call the constructors and do any DI it needs to for arguments. `Scheduler` in particular I have inheriting from `IHostedService` (an existing interface from Microsoft) with no explicit constructor. However I am only using `IHostedService` there since `HostedServiceWatchdog` requires it; otherwise I wouldn't need it. `HostedServiceWatchdog` I got from StackOverflow. It allows me to have a hosted service that works with DI (by also being a singleton; otherwise it won't get DIed). `Scheduler` singleton gets DIed into `HostedServiceWatchdog`'s constructor and the `IHostedService` method calls are proxied to it. You can also call the constructor yourself (I think these .Add functions have a variant that allows you to pass in the object to add) but it's usually not needed in my experience.
What sort of problems have you had? I've been investigating simple injector recently.
A structure I got used to at my first job and is very easy to jump straight into.. One thing to mention UI and Logic are hosted seperately, so cors setup is required. Project.APP - simple MVC to return SPA written in angularjs - UI client code Project.API - consists of endpoints - middleware - dtos - domain to dto conversions Project.SL - business logic - services Project.DAL - data access (EF generally) - repository pattern - CQRS 
[https://codeopinion.com/fat-controller-cqrs-diet/](https://codeopinion.com/fat-controller-cqrs-diet/) In this tutorial author uses MusicStore example from Microsoft, it should be enough example for your case
Jeez that post is eerily similar to what I'm about to do... Thank you!
Good, have fun and I hope you'll learn a lot :)
Typically, you'll want to use an IoC Container (Inversion of Control) that is created in the main of your application. Then you register each service you need to use. The registration would look something like this pseudo code serviceProvider .Add&lt;INotificationService, NotificationService&gt;() .Add&lt;IDateTime, DateTime&gt;(); Then (and this is where I sort of get lost because I'm trying to add this to a legacy application...) you initialize an "App" class, or something, that starts the program and that class gets injected with your required services. Some IoC containers that I can think of off the top of my head are: Ninject, Structure Map, and Autofac. [Here's a quick link to the structure map site for an example](http://structuremap.github.io/quickstart/) I'm still learning how to use these IoC containers myself and get a little lost of how to make sure all of my classes get the injection correctly. I know for sure the most important thing is to NOT pass the IoC Container instance around to other classes. This is an anti-pattern. I believe the term is something like a "Static service look up" or something (Sorry I'm forgetting a bunch). Services should only be injected from the IoC container.
So if I have 100 HTTP request calls in 4 batches with 25 requests each, few milliseconds apart. * The first 25 request will go and have a thread for them. Since they wait for the response but have an await, the request thread will not be blocked, being able to handle more request. * Second, Third and Fourth batch of requests will come and might use the same thread as the previous batch. They also will be non-blocked. * Now I will have 4\* (25 \* BatchNumber) of Task waiting for completion, so by the second batch I have 200 Threads? or at least 200 tasks that are being monitored for completion? would this start using a lot of CPU? especially since the method, everyone is waiting for, takes more time?
Command Query is a nice, composable architecture, but setting up the framework for it in a way that is testable end up adding so much code that it's overkill for most solutions 
Can I see a repo implementation of yours? I do the same but I expose dbsets from an interface in the repo layer. I'm wondering how I can unit test the service layer.
Me either.
You're closest to my own opinion, so I'll just add a few points. If your only reason for using other projects is organization, then there's really little difference between multiple projects and one project with folders / namespaces. It avoids some possible head aches for sure. My guiding principles usually come down to dependency management here though. You hint at it with the runtime / framework dependent project being separate here. In the SMALLEST projects that don't even really need unit tests, etc., I'd probably roll it all together in one project, but in more typical projects I'd at least break the line between WebAPI / framework dependent runtime project and my "application" / "domain" project, etc. At the very least it then means your business logic tests can work without having to actually reference all the framework stuff. In times where you end up having to add new UIs, or transfer to the new shiny webapi framework, etc. though having the actual APPLICATION in its own project that is NOT dependent on the big framework pieces makes that portable. I.e., when I see System.Web, MVC, etc. leaked down below that top layer, it almost always means someones accessing HttpContext, Session, or other static, framework dependent shit in lower levels they shouldn't. That is an abstraction I always try and enforce in these situations. That said, in larger projects there are other benefits to further breaking things up as a project grows. 1. There's almost always a "core" set of things needed in just about every other project. As such, I usually end up with a MyProject.Core that is referenced by everything and has a MINIMAL set of framework / library dependencies (ideally none). 2. Having ACCESS to stuff almost always has ended up with someone USING something they shouldn't. Stuff like, "Oh, EF is referenced in this project, I guess its ok to access the DB from anywhere instead of these specific repository classes", etc. Projects CAN help to delineate these situations for developers. This is usually where I like splitting off a DAL project for my DB access. 3. Similarly, since you mentioned it, HttpClient / System.Net.Http I like to isolate into an ApiClients project. Fuck that assembly. 4. And actually, this "break and isolate dependencies" is sort of my guiding light. If I'm using a third party library that isn't cross cutting (logging, json processing possibly, etc. as examples of cross cutting that might leak around everywhere) I might actually isolate the dependency and abstract the usage. Isolating dependency chains is a really big reason I like to break things apart AS LONG AS YOU DON'T GO NUTS. Like everything, there's a balance of what should be isolated (frameworks, a lot of third party libraries) and what shouldn't. 5. Right now the size of what I'm working on usually just has one "Business" project that has services, providers, and other "stuff" that is my primary logic layer. Breaking this up further as needed. 6. Or I might organize by MODULE instead of by functional layer. This would just be a project per functional unit. More useful when you have a series of similar "module structures". 7. There ARE other reasons to break apart. I'm told projects actually have a certain maximum number of classes they can contain? I know one project that had to split a monolithic business logic project in two over this. In truly large projects, you might want to avoid recompiles where you can. Seen projects with 20 / 30+ projects like this that would take minutes to compile the first time, and seconds after that as you changed only specific projects, etc. Distribution of shared libraries, obviously would make you create boundaries. Separate versioning / distribution concerns for some projects, etc. But that's probably more than OP is looking for. In a way this is sort of like asking "how should I organize my clothes?". Are we talking for shipment from a factory? As a service sending out uniforms or outfits? A personal closet? A department store? The organizational structure really depends on what you need, but is largely opinion based with some basic commonalities because there are usually only a certain number of types of clothes to worry about, and scales to think about.
Ive been wondering the same lately. I remember some kind of stat a few years ago that webform still represents 90% of current apps. What is it today? I still do webforms today and no MVC. We actually rewrote code from MVC to webforms. 
&gt; had just read some alternative advice that he thought was holy gospel Welcome to software development. That is like 90% of the arguments we get into usually, at least at the beginning. Its annoying especially with inexperienced developers who just discovered thing X but skipped over the "why I do X in these cases" right to "thou shalt always do X". I kind of blame the terminology we flipantly use. The "pillars of OOP" which are just useful tools it allows that you should use very sparingly. The "SOLID PRINCIPALS" which again are just concepts to consider when appropriate and not religious rules. "Test DRIVEN development". Etc. We really like to give our TOOLS and PATTERNS these very important sounding names that drive all number of new developers to become religious fanatics over applying them, when that was never the intention for any of them.
I work in cms web development and every platform we use are/have been moving towards MVC from WebForms. 
Perhaps in 2003...
`await Thread.sleep(1000)` Should be `await Task.Delay(1000);` Or its going to block the thread
[removed]
You won't have 200 threads. But you will have some things operating on threads from the pool. The number of threads is not determined by the number of tasks... but tasks do uses allocated threads from the pool. That being said, each async method adds a very small amount of overhead (aka allocating the object to track the task state, and requesting a thread from the pool when available, etc.), but it is smaller than you think. The other thing to consider... if your thread is blocked waiting for some network related response (as in your example) - aka waiting for hundreds of http request in turn, the operation would take MUCH longer and your CPU would be under utilized in a sense. Instead, let the framework parallelize those operations for a faster response. But if your only concern was CPU utilization and not how responsive an operation is... then async may not be the best candidate... but this would only apply to some very small edge cases IMO. &amp;#x200B;
Changed, thank you
When using async await code, a thread exists and only when instructions are being executed. Whenever code would block, it instead returns a task containing the wait handle for the thing that would block, and the rest of the method as a continuation. How many threads get created to execute a task's continuation depends on the synchronization context that was running when the task was created. Asp.Net's sync context uses a single thread, period. So does winforms and wpf. Console apps use the thread pool.
Was looking if someone uses command and query objects in their apps. Once you get into it, I find it a very useful approach to model and implement user's intent. Also it provides us flexibility to even choose different technology/approach for each command/query in case that need arises, without affecting the rest of the application. e.g. one query implemented with EF, another query implemented with Dapper and plain SQL, other query implements a read model with a cache, etc. Commands are pretty much similar. Some examples ([1](https://github.com/bojanv91/ContosoUniversity/blob/master/src/ContosoUniversity.Services/Features/Students/EnrollStudent.cs), [2](https://github.com/bojanv91/ContosoUniversity/blob/master/src/ContosoUniversity.Services/Features/Students/QueryStudents.cs), [3](https://github.com/bojanv91/ContosoUniversity/blob/master/src/ContosoUniversity.API/Controllers/StudentsController.cs)).
So how does the statement that the await won't block a thread works when is at the controller level? &amp;#x200B; Also if no extra threads are created by the use of await/async, it would also not have an impact if I repeat the same pattern of downstream await calls like 15 times in the stack and start hitting this?. &amp;#x200B; I'm asking because when I profiled my app the highest consumer was Thread. Start telling me that creating new threads was the problem is I'm trying to understand how that happened. &amp;#x200B;
Between the two it would be MVC. Most places still doing anything with WebForms are either supporting legacy apps they have yet to upgrade to something new, or they're way behind the times as far as modern development. MVC provides a better separation of concerns by keeping parts of your code in more logical places. You have models to represent objects or requests, the controller handles the requests, and the view handles how everything is displayed. Something isn't displaying right? Likely an issue with the view. Database isn't being called correctly? Likely an issue with a controller or a component it's using. That said, Web API is even more modern for developing web apps. It handles the M and C parts, and the V is whatever frontend framework you want to use, including ASP.NET MVC. It provides an even greater separation of concerns, and can allow your backend API to be consumed by anything capable of making RESTful calls over HTTP.
I don't tend to use a repository layer. My service layer contains the ef context which is registered with inmemorydb when in development or testing configuration. It's not perfect isolation for testing.
Async await goes all the way up to the top of the stack. The function calling your function is async, and so is the function calling that. All the way to the thread root. From there, the thread was created by your sync context, or the thing that created and set your sync context. If you're worried about threads being created, then find out what's creating those threads. You still haven't told us what your synchronization context is, although from context it sounds like you're in asp.net. And how are you benchmarking? Are you warming everything up first? If the thread pool is being used by your sync context, then you're going to see the thread creation time in your results. There would be no point in counting that sort of cost if it's a once per process thing, for a process that normally lives for days, weeks, or months.
So response time I'm not to worried about as of know, The app I'm working have a more depth async/await stack. That has 100% CPU utilization once I start hitting it with 1 request per second . If I remove the async and await, since I don't need anything from those methods. The utilization dropped to 1%. The functionality still remains the same without any impact. &amp;#x200B;
ASP.Net Core MVC FTW! WebForms carried loads of overhead in ViewState and Session and backed you into the full-page postback corner way too often. It also didn't quite flow with Domain Driven design, it was more module/component designed. MVC reduced huge amounts of the overhead and enabled more fluid usage of domain driven approaches with Areas. Bringing in lightweight two-way binding libraries for fluid AJAX (knockout, vue) to MVC is really nice on the maintenance and UX concerns. MVC Core now brings the services injection which eases the burden if you're trying to standup an external DI container (ala Castle, ...)
If your code is truly async there should not be any issues. By truly async I mean IO calls, network calls, reading files from disk etc. If you have ANY CPU bound code that you are running using Task.Run(), then you are going to see CPU spikes and high usage as the requests increase. With truly async code, [there is no thread](https://blog.stephencleary.com/2013/11/there-is-no-thread.html)
Nice my team is actually working on a new project based on the Northwind Traders one you posted. It's our first foray into CQRS. Really digging it so far. (Before we had traditional 3 tier)
What code are we talking about here? The code posted above? Task.Delay returns instantly. If you don't await on it, it'll do nothing, so your code will run as fast as possible. If you await on it, it'll take 1 second per request.
Interfaces make for good [seams](http://www.informit.com/articles/article.aspx?p=359417&amp;seqNum=2) for when it comes time to mock out collaborators when writing isolated unit tests. It's possible to pull off the same with other abstraction patterns in C#, but interfaces usually work best in my experience. If I open a project and don't see interfaces, I almost immediately know that it either doesn't have high-quality tests or that the tests that are present are likely very fragile.
Find and do some code katas - it's a good way to learn a language without getting distracted by the details of an "important" project.
In an attempt to clarify the benefit of async and await vs sync code, see use this example: Make a call to an api endpoint. Somewhere in the stack of calls in this endpoint, an async method is called to access a database. Await is used at this point, and the thread in use is freed up back to the thread pool until the database operation completes. This means that the thread is available to do other work while the async work happens. When using sync code, the thread is blocked during this time and can not go do other work. Now, because there is an async call somewhere in the stack, the stack as a whole needs to return a Task of some kind from each method (at least every method on top of the async method that does an await). This is true even if these other methods aren't necessarily async methods (return a Task, but are not async and do not use await).
I’m having ViewState flashbacks. Anyway, Webforms was an amazing technical achievement. It was a valiant attempt to synchronize desktop and web development into a single coherent model. It totally failed. And we were left with a vastly overly abstracted web framework that left everyone with idea as to what was happening. ViewState is what kept it all together. MVC was a much simpler web framework. It was approachable, coherent. It matched a lot of how other system frameworks behave. Unfortunately it was still built on asp.net...that monstrosity built to contain WebForms. Asp.net MVC core is suppose to fix that. TLDR: use Asp.net MVC Core unless you can’t. 
Having an await at the controller level for the SendMessage method, would make me wait for the response and take 1 second per request like you said. And if I change the Thread.Delay for a IO bound operation (going to a db) and add 4 more methods between SendToProperChannel and DoWorkInProperChannel that basically do a waterfall of await/async. &amp;#x200B; Would this change the CPU consumption? &amp;#x200B; Also thinking on the non blocking behavior of the async await, and using [asp.net](https://asp.net) core which is the thread I wont block? the [asp.net](https://asp.net) core thread at the controller level? &amp;#x200B; Thanks for answering my questions , I really appreciate it &amp;#x200B; &amp;#x200B;
The advantage of async await is that it handles that thread pool for you. And it probably does it much better then you can do on your own. For example, I have an application that parses through a file and queues up 100,000+ tasks to complete for async calls on each record I parsed out of that file (each record needs to make potentially multiple database calls etc etc). I don't have to worry about queuing up a thread and having it get work and maintaining the thread etc. I let the .net thread pool handle all of that for me. All I have to do is await my list of tasks to get the results back out List&lt;Task&lt;string&gt;&gt; myTasks = new List&lt;Task&lt;string&gt;&gt;(); //insert tasks to run into myTasks here var results = await Task.WhenAll(myTasks); //results is now an IEnumerable&lt;string&gt; //as it got and unwrapped all my results for me &gt;In other lenguages I have seen the repercussions of thread context change As for context change, there is some penalty there. In the full .Net Framework (read non .net core versions), there is a synchronization context during async calls that essentially hooks you back up to your previous thread when the task completes. This is good for certain applications that have say, a main UI thread that does all the GUI stuff etc. But it is bad when you don't **need** to return to the same thread you started on. That's when you call await someAsyncFunction().ConfigureAwait(false); which basically tells it that you don't care which thread picks the work back up after the async call. In .net core there is no syncronization context, so you don't need to make those `ConfigureAwait(false)` calls. &gt;you don't need to create an async invocation if you need them to return the response for the await statement. Not really. I'm not 100% on all the internals, but here's how I understand it and someone else feel free to jump in. When you make a call to an async method like this : `someAsyncMethod()`, that returns a `Task` object (or it should, don't ever return void in an async mehtod, it's bad, and makes bad things happen, and lets the terrorists win). Now your thread continues on its merry way because we never told it to await that task. Well, since we didn't await that task, it was never picked up by the scheduler. Your method completes, the Task object falls out of scope, the garbage collector throws it away, and your Task never gets run. Await basically tells the scheduler, "Ok, this is ready to be run now, go do it and bug me when you are done!". Now there is a small possibility the rest of your code (after calling the async method, but before the Task falls out of scope) takes so damn long that the scheduler actually sees the task and runs it anyway, but you don't want to count on that. &gt;Even if you are awaiting method results from another class unless you don't want to block the main thread you should also not wait for it. I'm not quite sure what you are getting at here, but you should always await your Tasks if you want them to run and not have the possibility that they may or may not have been executed. Don't think of async as threads. Think of them as tasks that a pool of threads are constantly working on. Don't worry about the scheduler and let .net handle it for you. **NOW**... One thing I did notice with the code you posted, and I think is what you are actually asking about, is "should you await and return single line calls". The answer depends a bit on what you are doing, but generally, no, you don't await them, but you don't mark the thread as async, you just have it return a task. Then you don't have the issues with context switching you are thinking of. Lets take one of your functions as an example. private async Task SendToProperChannel(){ await DoWorkInProperChannel(); } Okay, so your method is really just calling another asynchronous method and nothing else. Well you don't need to mark this as async, you just need to return the task from the async method you are actually calling.. Simply doing this will accomplish the exact same thing, without the (albeit very minimal) overhead of awaiting a call for no reason (AT THAT STEP! Remember, await your damn tasks!) private Task SendToProperChannel(){ return DoWorkInProperChannel(); } Voila! Same exact results, one less context switch!
Everything is debatable. What do you gain by having an interface sitting next to your concrete class? Why not just use the concrete class directly since you haven't actually abstracted anything?
Never look back. 
…Aaaand there will always be someone who prefers a horse-and-buggy over a modern vehicle. *shrug* Not my monkey, not my circus. &gt;webform still represents 90% of current apps. These days? Less than 20% of DotNet sites, to be sure. And 0% new development, at least for anyone with more than two functional neurons to rub together.
Damn, excellent explanation, thank you very much, this clears a lot. So there is a possibility that a GC happens on an unawaited Task and might never be executed? Great solution to avoid the waterfall async/await also. I really appreciate it. 
WebForms was a great idea and almost a necessary evil when there were a lot of issues with browser compatibility and there were some things you just couldn't do in the browser with JavaScript and/or CSS tricks. Think back to the IE 5 / IE 6 days. WebForms would try to determine the browser's capabilities by inspecting the agent headers and serve up HTML that would look good in the browser requesting content. Web standards were nowhere close to where they are today. Rendering between browsers is so close out of the box now. The little differences that exist can often be solved in CSS or polyfills. The heavyweight server-side processing we used to need in WebForms is obsolete. MVC and more recently WebAPI and single page applications reduce the burden on the server and puts more responsibility on the browser. It removes the complexity of trying to keep server state and client state in sync, especially when JavaScript and AJAX are involved. Overall this generally allows for a more stable, feature rich user experience in the browser and better response times from the server. There are some things that are overly complicated to implement in WebForms that are outright simple in MVC once you learn how to work with model state properly. It's also more consistent with web development in other technology stacks. Disclaimer: This is from my own experience. I've used WebForms since [ASP.NET](https://ASP.NET) 1.0 way back in 2002, MVC since version 4, and WebAPI since version 2.
Async shouldn't have that much of an impact on cpu usage. There's a slight overhead because of the code generated by the compiler but we're talking nanoseconds here. If one request per second is maxing your CPU there's something fishy going on.
I love Seq once I discovered it, it's so easy to implement and the different apps you can install with it are great. I had it alerting me in email for the time being on exceptions while running in QA so I could easily find errors that were happening. Unfortunately my company decided they didn't want to spend $8,000 on a solution that literally pays for itself.
also "private async Task" vs "private async void" is much preferred. https://msdn.microsoft.com/en-us/magazine/jj991977.aspx?f=255&amp;MSPPError=-2147217396
&gt; don't ever return void in an async mehtod Except in some very specific cases, like event handlers.
How can it block the thread if it's not even going to compile?
Amazon has ton of documentation for their products. Its really upto you which works better for you. RDS is really good for any database. Amzon gives you local network access so the other services can access the database easily n you can restrict access remotely. Have a look at lightsail(https://aws.amazon.com/lightsail/) its more like a droplet as you have in digitalocean n fairly easy to configure with VS as well. Regarding migrations if its initial migration you can easily run it through your program start so the migration will automatically take place during the first time execution of the app.
Totally agree with this. My only comment on using principles to justify decisions are that often with software development there are "many ways to skin a cat". And using principles that have come from legends in the field can help cut that decision time down. For me it always comes down to just starting and having a good feedback cycle which allows future change easier. This might just start as a single core project with directories, as that's still a form of separation and easy to change later on. If I do something that I think is harder to change later on then that's when I need to stop and think about the architecture.
:)
[removed]
Webforms is a dead technology and old MVC where the server creates a HTML view is also more or less dead except for small or internal systems. The current industry practice is to create actual APIs that return json, as this better supports a combination of different clients eg web+mobile apps. 
Sorry I was unclear. Yes i mean the same code will be used by multiple classes. right now that's only 2 but it could be more (DRY principles etc, etc). I went with the option of a simple static helper class. something like the below was quick, easy and testable. if it gets more complicated then I will probably look at a design pattern which fits. `public static string LocalizeName(string name) =&gt; $"{Environment.MachineName}_{name}";`
first make sure you get both aws SDK for .NET https://aws.amazon.com/sdk-for-net/ and visual studio toolkit https://aws.amazon.com/visualstudio/ deploying is easy with a wizard through visual studio, the database though you have multiple ways of doing it, either from RDS or custom EC2 instance or cloudformation can create the rds instance for you I think, depends on your exact requirements
You could get Application Insights to do this, which is a hell of a lot cheaper. Obviously if Cloud is prohibiting, that can be a problem. I know some people have had good experience with the ELK stack as well which can do a lot of this.
We started using your project in October, and since came to love it ;) Already helped me speeding up some tracking. The signal editing however still has a lot of room for improvement. There are other things as well, like I have no idea (and the docs don't show anything), how to reuse shared signals defined in the global workspace (like when you exit all workspaces) in other WS.
I understand the sentiment but I think we'll be waiting a long time! 
We’ve got serilog logging to seq and appinsights. AppInsights is really powerful but I’d pick Seq any day for the ease of use and UI.
True, that's the only time it is okay
Yup one interface and several implementations 
Don't forget to take a peak at Sentry as well. We've been using it for the last 6months (self hosted) and couldn't be happier.
On a project using DevExpress Grid controls in an mvc application right now. This hot garbage sends and entire copy of the grid js object as a request parameter on every callback... If the documentation wasn't trash, I'd probably find that there was good reason for it.
&gt; As for context change, there is some penalty there. In the full .Net Framework (read non .net core versions), there is a synchronization context during async calls that essentially hooks you back up to your previous thread when the task completes. This is only true for asp.net, winforms, wpf and other frameworks that set a synchronization context. The logic for awaiting tasks is exactly the same between full framework and .net core(i.e. try to continue on the same sync context unless you use `configureAwait(false)`). But, asp.net core does not set a synchronization context anymore so there the effect is the same if you use configureAwait or not. So it is not a difference between the frameworks, but of the libraries that use them. e.g. a console application will not continue on the same thread in both the .net framework and in the .net core framework.(unless you set SynchronizationContext.Current)
Yeah I simplified that slightly just to not confuse the issue further. Not everything has a SynchronizationContext in the full .net framework, but nothing has a SynchronizationContext in .net Core so, generally speaking, it holds true.
what's with the insults??
It is, but my company is currently still afraid of any kind of cloud solution. It's frustrating.
Ah shit, been there. Our newish CTO and Head of Dev are fully pushing it though which is lovely.
Nothing prevents you from using both. Application Insights can have its logs continuously exported to a Blob Storage. You don't have live monitoring but if you download them periodically, you can import those within Elastic Search no problems.
Helpful article, but I don't know if you want to be training your models on the build agent. 
Very true. Especially in hosted agents. I included it to show it’s there but I agree. Just because you can, doesn’t mean you should. 
We have a model that takes about 30 minutes to train. Currently we do it locally and then upload the model to blob storage. I would rather have it packaged up and running on a schedule. If you deployed the console app from your example as another container, would that be possible?
Absolutely. The output or package of the build/deploy pipelines is entirely up to you and a containerized console app is definitely an option. In terms of scheduling the build/deploy process should be doable, so I can see your proposed workflow being possible with Azure DevOps. 
Raymond Chen is a legend. This is a very rare low quality post from him, as this is not a trick at all, nor is it particularly nifty.
So what happens is when you call an async function, it runs normally until it hits an await, then it returns. If you never awaited the initial function call, or never pass in the function to Task.Run or whatever, it will never resume running from that await, since that Task variable is not known to the async/await system since you never awaited it or gave it to Task.Run or whatever.
I remember using their grid control in WebForms and Jesus, the performance was awful. Using one of their controls in the rows made a 400 line grid takes 3 minutes to load. Using the native control took a few seconds.
I'm guessing he's an automated spam bot and is doing a terrible job.
I don't even see the point in using a control when DataTables.js is so damn lean.
I don't either. There's probably some crazy edge cases for third party controls, but DataTabels.js should probably work in most cases.
Also why couldn't you just have, for every feature, a few exposed public interfaces which provide cohesion between packages without having them rely on concrete implementations? Why would other packages need access to your internal interfaces?
Change is hard and people will justify bad decisions like this all day long, so that they don't have to spend the effort to learn something new.
&gt; what's with the insults?? An insult is if I said your mother was a hamster and your father smelt of elderberries. What I did was mock your framework choices as being dangerously and foolishly obsolete, and akin to operating from a position of ignorance. At some point Web Forms will become every bit as quaint and anachronistic as Classic ASP, and because you failed to keep pace with technological change your own skills will be similarly obsolete. I did Web Forms myself for quite some time. Once I switched over to MVC 5, in just a fraction of the time I could do twice the work with half the code in much less time, and leverage the built-in tools to eliminate a majority of the errors and bugs I ran into under WF. MVC is so stunningly superior to WF in almost every way (short of rapid prototyping… I *will* give you that) that to go back to WF is like abandoning cars for a horse and buggy. It is a *massive leap* backward.
Scott Hanselman's introduction of ASP .NET MVC 3 was very inspiring and enjoyable for me. I cannot find a working link for it now, but he made fun of the marketing that used the three circles from Model, View, and Controller, and had me hooked.
Is it this one: https://channel9.msdn.com/Shows/HanselminutesOn9/Hanselminutes-on-9-ASPNET-MVC-3-and-NEW-ASPNET-Futures-with-Phil-Haack-and-Morgan-the-Intern?
Your UI where you accept input should be the first place that is checking for valid data and informing the user before they submit if something is not unique but needs to be. When that is submitted, it's usually simplest to catch the exception raised by unique constraint violations, rather then attempting to check your database just prior to saving the data. It also sounds like there's maybe a question about setting up custom error pages in ASP lurking in your post..
Plus, people will fixate on the “if it isn’t broke, don’t fix it” mentality, not realizing that it’s not a matter of whether it is broken or not, but whether it has become exceedingly - and almost laughably - obsolete and anachronistic compared against much better and useful options.
Ah, I blanked over the API part. What I do personally is that every POST/PUT/modifying operation is a Command and they all return a CommandResult/CommandResult&lt;T&gt; which contains a list of errors that occurred as well as an overall suggested HTTP status return code (I don't exactly like having the services determine that but the reality is that code knows best what went wrong and what to say to the client - otherwise the web app layer needs to map some part of the errors to an http response and that worse imho). My controllers all inherit a base class with a CommandResult() helper that takes the CommandResult object, sets the response code, and writes the appropriate output. This might all be overkill for you. It could be as simple as a little middleware that catches the specific exception(s) raised for a constraint violation from anywhere and returns a 400. You lose some context around where the error occurred but if you don't need it.. simpler than wrapping all the EF uses in try/catch (you'd probably look for a way to abstract that though if it's repetitive).
I have used this a couple of times now in a web context. It's quite easy to setup, in fact I have done the example in the article of converting a Windows Service that consumed RabbitMQ messaged to a background task in the web process. I'm still not sure if it's a good idea long term. The downsides I've thought of are: * If the background host errors out, I think you need to restart the app pool to start it again (better make sure your error handling is up to scratch) * If the web process crashes, your background host won't run until the web process starts back up. This is sort of obvious and in my case it doesn't matter as the background job and web process are functionally coupled * When you deploy you need to warm boot the process to kick off the background host, otherwise it won't start until the web process is hit for the first time. I think there's a few ways of doing this, but I just ping a 200 endpoint after deploying which is easy enough. Interested to hear what others think. It is certainly an easier thing to manage than installing a separate Windows service IMO.
All Udemy courses are always 95% off.
Hey, that's a shame! But, $8k really is the very top end of the Seq pricing options (enterprise-level, unlimited user, site licensing); sounds like someone could have pitched the wrong tier, internally? Paid/supported licenses start much cheaper than that... :-) Cheers!
Thanks! That's great to hear :-) Appreciate the feedback on signal editing - we're keen to keep improving release-by-release, we'll be back around to taking a look at this part of the interface soon. Workspaces are new in Seq 5, so the docs are still a little sparse; you should be able to find shared signals from anywhere by searching in the little box above the signal list. If you still need a hand with it, please do send us mail/screenshots/more detail, we'll help figure it out.
Ah, gotcha. Guess that's pretty tough to work around.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/csharp] [Offering my help migrating your projects to .NET Core \/ .NET Standard \/ new CSProjs for free!](https://www.reddit.com/r/csharp/comments/a1ajen/offering_my_help_migrating_your_projects_to_net/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
Stored Procedures can run right on the database which is useful for complex operation. You can reduce your app to high-level operations and write a stored procedure for each, to optimize for performance. However you're probably going to end up strongly tying yourself to a database type. Which can be OK. But when you are in an org that makes it near impossible to request a SQL Server database, sometimes it's nice to spin up a SQLite database file locally for testing. No stored procedures there. EF makes it stupid easy to switch databases though. Plus all your data is modelled in code and it's stupid easy to use. It's a no contest, EF every time. Too useful. If the limitations of using stored procedures were not relevant and I needed a performance boost using stored procedure might be better in those situations, of course.
Sir, Here's the Updated Article with Complete Source Code + Video Demo [https://codinginfinite.com/dynamic-role-based-authorization-asp-net-core-assign-database/](https://codinginfinite.com/dynamic-role-based-authorization-asp-net-core-assign-database/)
This is really kind of you! If you'd like we'd appreciate if you took a look at fido2-net-lib. We're already on.net standard but not sure if you setup source link correctly. https://github.com/abergs/fido2-net-lib Also, if you link your PR's in this thread I'm sure many people will learn from your work. Have a nice day mate!
Nice - needs some mild tweaks for nicer consumption - but overall should be a quick task. i have some questions about projects inside the solution tho... how is this library meant to be consumed? Currently a sample deployment of this package will produce 2 packages - fido2 and Chaos.NaCL... You also have another libraries ("CBOR") source code entirely pasted inside this repository? Any particular reason why you did that instead of a regular packagereference?
I haven't gotten around to updating my datatableproxy project, if you want to take a look. Http://GitHub.com/tdietrich513/datatableproxy
You bring up a ton of stuff I want to do but can't seem to find the time to learn, so I'm super interested whether you want to chip in or provide some recommendations. &amp;#x200B; [https://github.com/jpdillingham/Soulseek.NET](https://github.com/jpdillingham/Soulseek.NET)
This one I'll do for sure. I love deleting obsolete lines - old csprojs have lots of them - ☺️☺️
This one will require some focus towards a unified build entry point. I could tell you stories for ages about this - so much options to choose from. I will take a look at it on the weekend and see what fits best. I didn't quite the part about azure devops, though. Are you moving away from it or away from Travis?
But that's the thing, none of these things I mention were ever meant to be RULES. They all originated from someone who had a specific problem, even a common one, and suggested a specific approach. But failing to realize that and instead elevating these things from "a solution" to "the solution" causes all manner of headaches. You choose the tool to fit the situation, not change the situation to fit the tool. Those things are fine tools to know and apply when you have a situation they apply to though of course. Just had too many "we must do X because its a best practice" people lately, and I see so many newbies latch onto the same mentality because its easier than "it depends", which just perpetuates the issue.
For the sake of future maintainers, don't trade your own home-grown pet project ORM for some one else's. Linq2db is *not* a mature library. &gt; the benefits of migrating it to C# There's very little beyond preferring the syntax. The upside is that converting can be nearly automatic.
Doesn't C# offer some sort of automated unit testing or something that VB does not?
It's not my project, but the one I follow closely: [https://github.com/haf/DotNetZip.Semverd](https://github.com/haf/DotNetZip.Semverd) ([https://www.nuget.org/packages/DotNetZip/](https://www.nuget.org/packages/DotNetZip/)). It's a very popular NuGet package and a lot of people were asking for .NET Standard support, which was done in code by contributors. Unfortunately it uses Albacore for builds since before .NET Standard and new csprojs existed and a lot of people who were trying to help to build and publish .NET Standard package are just not familiar enough with this Ruby tooling and there are some issues with trying to make it work. Repository owner mentioned he is not opposed to moving to new Microsoft tooling as long as somebody can do it, and I think this is a great candidate for converting to new csproj format (thus removing dependency on Ruby tooling). I am sure a lot of people will be thankful if it finally happens.
You can just catch the error globally which is not ideal IMO. The alternative is checking the database for an existing record matching your constraint and returning a nice error. You run into this same problem when deleting records and with foreign key references. Everything should have validation. 
Google: ".net standard" First result: https://docs.microsoft.com/en-us/dotnet/standard/net-standard First line: "The .NET Standard is a formal specification of .NET APIs that are intended to be available on all .NET implementations" tl;dr: .NET Framework is an implementation of the standard
Sorry, I didn't get what it meant when I looked. Thanks.
There's always exceptions to the rule, General message i was trying to get across though is that unless your project is massive the idea of replacing a DI container shouldn't be terrifying, even in your case that's potentially a very simple refactor. I recently found [Scrutor](https://github.com/khellang/Scrutor), which deals with both decoration and assembly scanning. I haven't needed any more advanced lifetime scopes yet so that pretty much covers everything i want.
&gt; General message i was trying to get across though is that unless your project is massive the idea of replacing a DI container shouldn't be terrifying Even then, it shouldn't be scary unless you've let it creep all over the place. e.g. you have autofac's `KeyFilter` all over, but in case you've already lost the don't let it bleed battle. &gt; even in your case that's potentially a very simple refactor. Very simple. Since its contained to just one class knowing about the container. I'll check out scrutor since that might be what I'm actually after. 
Hi Anders! Congrats on getting all of this up! I've written a blog post about how to setup Source Link correctly for your package. It's a bit dated (end-August 2018) but I hope it can help out! https://blog.nuget.org/20180827/Introducing-Source-Code-Link-for-NuGet-packages.html If you want more info, we have [some guidance](https://docs.microsoft.com/dotnet/standard/library-guidance/sourcelink?WT.mc_id=social-reddit-marouill) on how to use it. Let me know if you have issues!
Hi! Cloud Developer Advocate from Microsoft here! Let me know if you have any feedback on the conversion process. I'd love to get those feedback to the .NET Team and make sure they know about it so we can make it better. I'm here to help and this is a great initiative! Keep me posted on the project you do PR! I'd love to see how many you can do! Keep on meditating. 😜
I've never heard of that. In fact you can unit test C# code using VB and vice versa. One they are compiled the language doesn't really matter.
It would be nice if VS had an option to upgrade to the new project format automatically! 
Where I can find the calendar of this tutorial?
&gt; dated &gt; (end-August 2018) Ah, the pleasures of being a modern developer :)
I've used this with reasonable success. https://github.com/hvanbakel/CsprojToVs2017 You do end up with a fair bit of unnecessary stuff in the project that you can rip out, but it's good for an automated first pass.
It's not likely possible. Our conversion of our mvc &amp; web api applications resulted in pretty much a total rewrite. The best we could hope for is the possibility of only having to update references and method calls from one thing to another. In other areas though, the loss of an existing class method or just the difference of "how you do it" it core resulted in much more re-work than i would feel comfortable automating. 
I'm kind of surprised this // Resulting list is { 4, 2, 3 } var list3 = new List&lt;int&gt;(list) { [0] = 4 }; Does a prepend rather than overwriting the first element with the collection initialiser.
I'd love to hear more. Can you give a few examples of where that logic lives? Services, utility classes, somewhere else? Just thinking of a typical sign in POST action, it seems like there would be a decent amount of work to do (service/repository call, validation, session, routing to next action, etc). I'm always interested in seeing how other folks approach these things.
I work on modernizing legacy Web Forms projects. One of my greatest annoyances is that a Web Forms website project can't reference the new .csproj format. I know web forms is end-of-life, but realistically we still need 5-10 years of support before client-side blazor becomes mature and we can replatform these sites. Is it possible that we could patch Web Forms to allow integration with the new .csproj format?
I have had the same issue with the package. I am actually happy to help. This one would even benefit my shop.
In C# parlance, Net standard is an interface. Net Framework is an Implementation.
Cool! [https://code.videolan.org/videolan/LibVLCSharp](https://code.videolan.org/videolan/LibVLCSharp) It is multi-targeted already, but we could use a hand on scripting, CI/CD, SemVer and SourceLink. There are existing issues with some info for most of those things. PRs welcome :) 
Wait... You can't reference a .NET Standard project from a Web Forms project by using the latest version of .NET Framework? I don't have those things installed on my machine but I'll confirm it with the team. I know that .NET Core 3.0 that is upcoming will fix a lot of issues with the older stacks that we have. I just want to make sure they thought of that. Can you confirm my previous statement?
You'd laugh how quick things are moving. I'm doing a conference tomorrow. I had to change the slides tonight because things just went from Private Preview to Public Preview. So... yeah. Azure is updated at an insane pace. There's hundreds of team each contributing and changing things left and right. We have [processes](https://support.microsoft.com/en-ca/help/18486/lifecycle-faq-azure) in place to ensure we keep things compatible for a while. While it's still updating quickly, I feel like things aren't changing as fast as the node.js world. 😀
What about... [an RSS feed of the MP4 HD version of the video?](https://channel9.msdn.com/Shows/Visual-Studio-Toolbox/feed/mp4high) I don't think there's a calendar but there's definitely a few RSS feeds available. Look at the [Visual Studio Toolbox](https://channel9.msdn.com/Shows/Visual-Studio-Toolbox?WT.mc_id=social-reddit-marouill) main page and at the top right you can subscribe. :)
And we wished it was as simple. There are so many things that can end up in a csproj file that it would be impossible to write a tool that would get it right 100% of the time. I normally follow [Nate McMaster's guide](https://natemcmaster.com/blog/2017/03/09/vs2015-to-vs2017-upgrade/) to old projects. The tools mentioned by /u/shrodes is still highly recommended but there's no "One Click Upgrade" button.
Hell yeah that's quite a project to help with ;)
I have already converted my projects, but I'm interested to learn what your preferred layout of the csproj files is, eg order of elements, treatment of conditionals. Because you've done this way more than I have, I'm sure you've developed a consistent, easy recipe to follow? Bonus points if you can document your style guide ;-) And great work. This reminds me of the guy who does logo design for free for open source projects.
I think it only works if a fallback Target (usually in form of a PCL profile tfm) is given.
I hereby declare my own *Yegor3219 Standard* that requires Console.WriteLine and File.ReadAllLines methods to be implemented. Now we have a new standard which .NET Framework just happens to comply with. Same with .NET Standard.
I would! I've read through your excellent guide and feel that I can create my own project with the help of your guide. But to get a complete overview of the solution it's always good to have the complete source code available. So if it's not to much trouble I would love to see you upload the complete source code.
I've created an issue for your project for tracking, where we can go into more detail: https://github.com/MeikTranel/MigrationSupport/issues/1
Here's the Complete Source Code [https://github.com/CodingInfinite/Dynamic-Role-Based-Authorization-DotNet-Core](https://github.com/CodingInfinite/Dynamic-Role-Based-Authorization-DotNet-Core)
Define mature. Linq2db has over 1000 Github stars, 50 contributors, released 1.0 over 5 years ago and the last commit was 5 days ago. That seems pretty mature to me even if it doesn't have a big corporate organisation behind it.
No. Thanks for trying to find it though. It was a presentation he did at what was probably an MS Build conference at some point. If I come across I'll share.
I understand some of those words. Maybe what you could do is actually a short writeup on all of these and what they actually mean and do. Many of us don't even understand what it is we're missing.
The work is in progress but if you offer to have a look, why refuse it? :smiley: Here you go: * https://github.com/xenko3d/xenko (contributor) * https://github.com/Kryptos-FR/markdig.wpf (maintainer)
Wow that first one is quite a job. I can look at it, but especially when it comes to 3D common native dependencies are quite something to deal with - i've added it to my general inspection board https://github.com/MeikTranel/MigrationSupport/projects/1 but lord almighty no promises here :D As for the second one: Can you write an issue detailing your wishes here https://github.com/MeikTranel/MigrationSupport ? BTW: 1. Windows dependencies doesn't mean we can't produce netstandard projects - you can make them work quite nicely even. 2. Full Framework project files have the same possibilities when it comes to clean project files. The only nut i haven't cracked yet is projects with old service reference entries. They rely heavily on Visual Studio interaction and are generally weird to deal with.
Anyone using it in large scale application(s) for recurring jobs? Did you have any problems with keeping the application alive?
Oren Novotny might have something cooked up that could help? Do you have any sample projects for me to take a look at?
Great, will comment there project details! 👍
I'm using in large scale. The app process about 1,2M jobs per day. We have about 15 recurring jobs running along the day. For me the performance is Ok, some jobs spend hours to finish because our business is very large. The app runs hangfire as a windows service. 
I will check it out, thanks! Are you able to use your own domain?
Thank you for offering your help! My Project is not OSS or in need of a conversion. But I was wondering if you had experience with targeting specific .NET Core Runtimes? On my CI/CD server I publish my applications with this command: dotnet publish -c Release -o ..\publish It outputs a runtimes directory with the following sub-directories: - publish/runtimes/ - debian.8-x64/ - fedora.23-x64/ - fedora.24-x64/ - opensuse.13.2-x64/ - opensuse.42.1-x64/ - osx/ - osx.10.10-x64/ - rhel.7-x64/ - ubuntu.14.04-x64/ - ubuntu.16.04-x64/ - ubuntu.16.10-x64/ - unix/ - win/ I only plan to publish this application on Windows 2016 Server so the OSX/Linux/Unix runtimes folders are not necessary. In several cases these runtimes folders contain known vulnerable *dlls* which lead to false positive OSA scans. Is there a way to configure the project to only output the *win* runtime folder? I have cross-posted to [stackoverflow](https://stackoverflow.com/q/53507229/2375884) if you want to get some points there!
Hell yeah you can. You can do this both using the cli command and the project file itself which is preferable. Why don't make documentation request issue in the migrationsupport repo I linked in the OP?
Will do thanks!
After reading an article just below that one, seems like it's not possible: "Prerequisites To complete this how-to: Make sure that your App Service app is not in FREE tier. " 
I'm running it in an ASP.NET MVC. With like 20 recurring jobs, processing 2k jobs per day
I was being facetious with the first one. It's quite an undertaking I did not really expect you look at it. &amp;#x200B; I'll take some time to think what I want with the second one. Thanks for the reply anyway.
[removed]
We are using it in production with &gt; 3000 types of jobs. I cannot recommend it enough. It is fast to setup a very open to customization. If you have lots of jobs the dashboard is a bit limited and you may have to build something that fits your needs.
I m using in a very large scale app. Integrating Hangfire is a breeze. All the variations of Hangfire jobs are awesome. Some of our jobs are event based (Fire and Forget) and over 6000 recurring jobs every 10 minutes thats about 864K jobs in a day. Fire and forget jobs gets added to the queue and gets processed fairly quickly. If anyone wants to use background jobs Hangfire is the best. 
Many years ago I used Quartz on the Java side of things. But in recent memory we simply use Windows Scheduler for all of our scheduling needs, kicking off executables with command line parameters. In what scenarios would you ditch Windows Scheduler in favor of something like Hangfire?
I'd rather wait for dotnet core 3.0 early access build
Set to application pool to never suspend and `AlwaysRunning` - this keeps your application pool online. If you configure a warmup sequence, it will always come online after IIS is restarted.
`.GetAwaiter().GetResult()` will work with Task without a return value.
Yes, this works for me.
If you are running your .net core apps in a docker container, it is worth giving good old cron a try. I wrote up on how to do it on my site [https://www.alexhyett.com/scheduled-dotnet-core-console/](https://www.alexhyett.com/scheduled-dotnet-core-console/) 
When will the RFC be submitted to the WWW consortium?
When this project started, Typescript support on the client-side libraries it's using wasn't that common. If you would like to suggest ideas on how Typescript can improve the project, we can discuss it on the github repo forum.
https://stackoverflow.com/questions/51873269/asp-net-core-2-1-custom-roleprovider-with-windows-authentication
I haven't heard anything here about execution plan cache, and the quality of the SQL EF craps out. With stored procedures, the execution plan is cached once. I'm not sure EF registers as non adhoc, meaning SQL may store each iteration of that query due to lack of parameters. It would be interesting to see the MSSql guys work with the EF guys to put out a SQL Server optimized ORM, since they're like...the same company.
&gt;but realistically we still need 5-10 years of support before client-side blazor becomes mature and we can replatform these sites. Why wait for Blazor? It might not even become a real product. MVC/Razor Pages on DNC is the current future.
Nice, will give it a read! 👍
I've noticed this project is not among the issues in the new repository, should I create it?
I don't know, it looks people doesn't know what linq2db is.
Linq2db generated pocos are c#, but you can use it in vb.net and linq.
Ya know, I glanced at the github link a bit too quickly and mixed it up with another ORM (that also claims to be the fastest and recently added some basic LINQ support) that the author has been promoting around reddit the last month or so. That said, I'd put Linq2Db kind of in the middle. Not as immature as the lib I thought it was, but still not something I'd be likely to choose for a work project.
You check for the entity in the db on the api side. var exists = await _context.MyEntities.AnyAsync(e =&gt; e.UniqueId == request.Id); if (exists) { throw DuplicateException(); } then attempt to save it.
Why do you need 299 certs on one server?
When you don't wanna pay for Windows license to run a core app just to get scheduler firing 
Because we load dynamic websites out of one database. 
Razor pages is great but it is not a single language full stack solution.
All my code is proprietary, sorry. But here are steps to reproduce. 1) Create a ASP.NET Website project (classic webforms). 2) Add a aspx and aspx.cs. 3) Add a .NET Standard Class Library targeting .NET Framework (same version as website project). 4) Try to import types from the class library. It won't work. 5) Add a legacy .NET Framework class library (old .csproj) with the same types. 6) Try to import types from the class library. It will work.
Correct.
Not commenting on the maturity of Linq2Db specifically, but just because an author claims a "mature" version doesn't necessarily mean software is mature.
I've listed it as a task in my board.
A couple days before April 1st
Is there an easy way to fill a collection with a bunch of new objects? I’m writing a for loop for this all the time. 
Do multi hostheader certs work?
How are your objects being created?
No argument constructor. Basically creating lists of objects which are later accessed by index for modification only created once, just wondering if there was shorthand for it. 
Could/would a virtual appliance with SSL offload work in your case? Azure App Gateway is probably a bad example because it can only handle 40 certs at a time. But you could potentially put something like that in front of your app/server (even multiple of if there are hard limits like App Gateway) and route requests to your underlying app. Of course this might add some overhead cost wise so weigh it up. A reverse proxy could also work, something like nginx maybe. Not sure if that has any cert limits though, but it would work in a very similar way to the gateway option. Programmatically pulling from the DB based on request isn't that viable, because SSL handshakes need to be insanely fast. If you felt like getting into some real low level code, then you could totally do this, but reality is you'd almost be writing a web server or a reverse proxy by the time you're done, so probably not much point. 
Thanks, this is pretty much how I did it. I was secretly hoping EF would have something that would just check the unique fields for me automatically (based on the mappings), but it's good enough 
[removed]
Yea, but "sometimes". I don't prefer that. You are making your way harder to handle.
Depending on what sort of tasks you're running it might be better to use Azure Logic Apps. It will be cheaper and you can connect with SQL, Storage etc.
You could start by reading up on the [HttpWebRequest Class](https://docs.microsoft.com/en-us/dotnet/api/system.net.httpwebrequest?view=netframework-4.7.2) then look at the many examples of it's use on SO. 
[removed]
Quartz.NET is essentially the gold standard for recurring task scheduling. It’s free, relatively simple to implement, powerful enough to adapt to your inevitable scope changes, and especially simple to set up a RAM store out of the box for a single weekly task. 
I have no idea what you mean by "richer"
it's basicaly a web platform with various domains, websites, etc. i want it to check if the website is online or if it returns some type of error. my boss (I'm still an intern, obviously) said he wants me to make a new console application to run the method that is responsible for doing that(present in the web application), when the task scheduler tells it to. i've been searching a little bit more and it seems i need to use some httpclient get method to get the web application, but i actually have no idea how i can do that, and, honestly, i don't know where to begin. my thecnical english might not be the best, but i hope you understood what's the problem i'm facing
Do we need to worry about securing it?
no... t's basicaly a console application that should be able to call a method from an web app using httpclient
You're getting the technical jargon mixed up. From your other comment, it seems that you just want to check if a website is up or not. It depends on what you want "up" to mean, but if you just want to see if it's not returning \`OK\` (202, IIRC), then [this](https://stackoverflow.com/questions/186894/test-if-a-website-is-alive-from-a-c-sharp-application) should work. But you can literally just Google \`c# check if website is up\` and that'll get you plenty of results.
the method that checks the websites is already ready, now i just want to be able to call it from a console application. but yeah, it's highly probable that i'm getting my technical jargon mixed up... xD
https://up-for-grabs.net/
I know you can, but I never tried it. Have a couple of friends running it on Thunderbolt drive without any issues. 
Volvox Learning Community is always looking for contributors to our projects. Currently we’re developing a Discord bot using ASP.Net Core. If you’re interested let me know!
no. it's just a simple fuction inside a controller. at least in portuguese that is also called a method
I mean, yeah, it's a method. But you didn't describe your setup. So it didn't make sense. Context is important. You have to tell us all the details. If it's a method in a controller, then it should just be an end point that returns HTML, right? Or is it a private method?
those e-mails are dummy temporary emails, for tests purposes
If those are real email addresses, you should remove them from the pasted code. As for calling it... I mean, I guess you could try adding a reference to the web project and maybe instantiate the controller then call the method? But I don't know if that'll work and that's super hacky. Typically that code wouldn't be in the controller. It would be in a core library or something. You could move that code to a core library and call from the core library or copy and paste it into your project. It looks like you're grabbing `HostingList` from a service. Is that in a separate library or is it in the web project?
don't worry, those emails are temporary emails, but thanks :) it is inside the web project. I made a service so i could interact with the database. HostingList grabs all the hosts from the db. i guess i can ask my boss f he accepts that aproach! i'll keep you posted. thanks man! :D
It sounds like you are trying to automatically visit a list of URLs and gauge the http response for success or failure? You may just be able to write a powershell script. 
Not really, no. I’m pretty sure that C++ support in VS is actually the C++ compiler. IIRC CoreRT goes C# -&gt; C++ -&gt; Binary. So if you don’t have a C++ compiler I don’t think there’s much you can do...
Mingw comes with everything required to compile c++, is there no way to point CoreRT to this gcc?
Pretty sure CoreRT uses either LLVM/Clang, not GCC. I personally haven’t tried though.
Unless I'm losing the plot, the logo for the tutorial has C++ in it. Otherwise nice tutorial from a fellow SMS / communication company.
I've seen more Dapper than EF, but that's from a small sample size.
I did some performance testing with linq2sql, ef 6, ef core and linq2db and linq2db was so much faster then the others. I couldn't believe the performance difference and I tried everything to get the others to perform faster.
Hi - hoping this will be of use for someone. This is a Windows app for polling TFS for build notifications. System tray notifications pop up whenever a build is actioned (started, stopped, passed, failed etc.) and includes a website dashboard for monitoring of multiple builds. Web based configuration pages for choosing which connections/projects/builds to monitor. Works with on-prem and VS online instances of TFS. We used to use Catlight for this before it went to a paid model. This is a free alternative (might not be as polished as Catlight but hopefully might be helpful in some way!). Link to installer is on the readme on the Github page. Contributors welcome. Thanks!
Anyone know if all these tools work with non-windows, non-mssql server setup?
Good job guys! 
Thank fck. MS annoy me daily, but that's because I use their stuff daily. Apple? Been their and ran away as soon as I could. Sort out your QC and you can also be the best public company too.
If you mean the Dashboard, then yes and no. If you do not set up any authentication, then you can only reach the dashboard from the local machine. [Here](http://docs.hangfire.io/en/latest/configuration/using-dashboard.html#configuring-authorization) are the docs for that. Right above it it also states &gt; By default Hangfire allows access to Dashboard pages only for local requests. In order to give appropriate rights for production use, please see the Configuring Authorization section.
Also running it as a windows service. Running ~35k jobs/day. Currently about 9 recurring, where a part of them run every minute (although we do have it setup so jobs cannot run when a previous instance of it is running). Although it's worth mentioning that SQL server is about the only first class citizen if it comes to job storage. The storage modules for SQLite, RavenDb, DBLite and MemoryStorage seem to only work occasionally (at least on my end).
Most, if not all, are unix or platform agnostic.
Doesn't this already exist? https://docs.microsoft.com/en-us/previous-versions/ms181725(v=vs.140)#monitor-build-progress-by-using-the-build-notifications-application
I'm on mobile so can't really look, but I think they didn't maintain that util and that it only works for the pre 2015 builds.
Looks like cheap outsourced devs pays.
To be fair, Microsoft didn't surpass Apple, Apple dropped below Microsoft.
Oh, that's probably true. It's been a while since I used it.
Whew for a moment there back in 2012 I thought I might have to actually build large distributed systems in MacOS using objective C.
Where it belongs.
No doubt. I was irritated to learn that Apple was the first company to hit a market cap of $1T earlier this year. It's nice to see them fall back to earth somewhat. But them dipping below Microsoft on a correction is not the same thing as Microsoft hitting the record numbers Apple hit before.
...And is still ridiculously overvalued.
btw apple and msft are two boats in the same ocean i.e. the correction that affects one also affects the other.
no surprise, have you seen their power bi pricing?
Did you read the linked article? It specifically addressed the reasons Apple has dropped over 20% since its high while Microsoft has remained relatively flat. Apple was always (and still is) overpriced. It was (and still is) due for a larger correction than the rest of the market.
It’s funny you say that as MS also has their fair share of terrible releases. I still love them both
Yep that’s right, as far as I know there’s nothing like this built in to TFS (other than email alerting etc.)
Dodged a cannonball.
I use swashbuckle with SwaggerCodeGen to generate typescript client. It was pain to set it up and new version of a generator has tons of breaking changes so we are stuck or need to do some refactoring. I have no experience with NSwag though. AFAIK there should be some native support in ASP.NET Core soon (probably based on NSwag) so I will migrate when that happens
We used nswag and it was slow as hell! Generating a typescript client took 2mins! 
Nuclear cannonball
https://www.hangfire.io/
Lol yes. I have been through my Apple periods. Way back I had a Quadra 610 which was great. No idea what happened to it. And obviously as soon as the iPhone 3 came out I got one and a Macbook 13". Loved both of those too. I actually learned C++ on the Quadra. My first project was a 2d vector graphics engine which was pretty cool :)
[Update] I figured out why it was crashing. When I was trying to install my dependencies I was still using bower. I have stopped using that and starting using libman. Json for my depen. And correctly routed the destination to my root file. So if anyone sucks like me here's the solution lmao 
&gt;cheat ii.o. codes一-ff🌌gg H 还， 如4－一-,有60 &gt;enjoy ' r
Even Apple programmers didn't want to carry on doing that which is why the invented Swift.
Woah, thanks for catching that. Idk how that happened! 
I've used IKVM in the past and it was okayish. Not a great experience, but I was able to get done what I needed.
Yes I should be more clear: "... the same *market* correction..." I don't call Apple's decline a correction. A correction implies no change in fundamentals whereas Apple is seeing a core change in their business. Apple is a fashion company and their main product is losing its glamor appeal. Microsoft is a technology company and they have built a viable product with Azure. 
&gt; IKVM thanks! Was it slow?
You have a pull request my mate ;)
Performance was fine, but I recall it being a pain to set up. This was a while ago so it may be better now. I'd be interested to know how you get on anyway.
Do a find all on the old namespaces and project names and change any relevant results you find.
I'd look at using something like fiddler to capture the raw http conversation and recreating it using httpclient. 
My experience would be to use WebClient in C# because it's the easiest to automate. Plus fiddler makes headless web scraping and form submission a breeze. Selenium is a really nice method for recording and re-executing web sessions, but I haven't gone about automating it. It should be ok but I think the automation method is usually Java.
The things I struggled with where: 1. How to pass parameters to the Lock. 2. How to use different URLs for signup and login 3. Adding claims to the session. That's all I can recall right now. Arguably 2 and 3 are OWIN not Auth0 issues, but have them in the quickstart could have made things go a lot quicker. &amp;#x200B;
Sweet! It looks good, I'll pull it in soon. Want to set up travisci to build and push the assembly while i'm tinkering here. 
We aren't fanboys here...
[Meanwhile on r/Linux](https://i.imgur.com/edL85vq.jpg)
Can you do this form submission like this if you have to login though? 
Gotcha. So WebClient can be used to login to this third party app, go to the form page, and then do the submission for you? 
[removed]
I don't have a project to suggest (right now) but this is a fantastic idea. Thanks for offering up your help to the community. 
What is that supposed to mean?
Yes. If a browser can do it it comes down to sending the right data. If it's cookie based login, you can simply set the cookie in the header. If it's a session based login you can make one request to login, extract the session key and pass that in the real request. Grab fiddler and you can see everything that is being sent and received from the site. If its https you might have to install the fiddler cert but that's easy as 3.14 
How is AAPL overpriced? It makes more money than any of the major tech companies but has by far the lowest PE ratio. 
This does sound like a good approach. Thanks for the suggestion!
Price is reflected not just in how much money you make today, but also outlook for the future. And by all accounts Apple is stagnating as a technology company. Each new hardware release is more underwhelming than the last. Apple is riding the coattails of it's hay day, but I'd say their long term outlook at the moment is questionable. I don't think they'll disappear, but I without some fresh innovation they won't hold their current place in the market either.
IKVM is no longer supported by the creator. Not sure if it works with newer versions of Java. There newer (commercial) interop frameworks
I will warn you against the idea of scraping the HTTP transactions for replay. It is fundamentally brittle, it will not run any client side code to evaluate UX. It is subject to break if any client side contracts change, and will fail if there is something like a nonce or other such security mechanism in play. Most importantly, it is philosophically opposed to your requirements - you have no direct DB or API access, so you shouldn't try to pretend you do. You situation is ideally suited to using Selenium WevDriver. C# library support is great for it. I've also used a layer on top of it called Coypu which wraps and handles some of the complexity of Selenium. This should remove the risk of the volatility of changes that you technically should be unaware of. You'll get even better stability if you follow a pattern like using page objects, which should encapsulate all of the XPath or element IDs for a given page or form. From there, you can further reduce fragility by establishing a business operations layer between your tests and the page objects... Which should expose operations and data as business model objects that are not bound to HTML or page objects. If you treat your tests like an application itself, with the browser replacing the database, and selenium being your ORM, you'll get a maintainable, reusable set of tests that should provide value and incur limited maintenance.
I could definitely see that happening, thank you for this insight. Selenium was my first thought as probably the most stable approach. Seems like this is about as close as it would get to actually emulating a browser. 
Yes, it can do anything a web browser can do.
[This guy](https://www.youtube.com/watch?v=d_AP3SGMxxM).
Is there any particular reason you want to learn Razor? Is server-side rendering a must? If not, I suggest you to invest time in learning some of Javascript frameworks (Angular, React, Vue) and do the front-end part with that. There is a large community in all of 3 frameworks so you can't miss with that. I also did Razor development with MVC till 2012, but nowadays I use [ASP.NET](https://ASP.NET) Core only as back-end (REST API).
We're users of a partly but increasingly open-source programming langauge. It's owner may be Microsoft but we're not fans of Microsoft per-say. To me the way Microsoft handles it's programming products is so wildly different from how it does hardware and Windows that I'm a fan of one without liking the other, or the paper owner. Also, Microsoft thriving is good but them surpassing Apple isn't of relevance to us. I think this mistake is often made in the Tesla subs as well, where brand loyalty sometimes results in ignorance.
While not mvc5, I recommend www.learnrazorpages.com if you want to brush up on the latest razor.
As mentioned that is what they use where I am interviewing. I have experience with all 3 frameworks you mention but that doesn't help me to start. If I get the position and get them to where they trust me, then maybe I can steer them toward a modern Javascript framework 
You can also look into using Puppeteer using Node.js
Sorry I have completely missed that part. I've read [https://www.amazon.com/Pro-ASP-NET-Experts-Voice-ASP-Net/dp/1430265299/ref=sr\_1\_4?ie=UTF8&amp;qid=1543701625&amp;sr=8-4&amp;keywords=asp.net+mvc+5](https://www.amazon.com/Pro-ASP-NET-Experts-Voice-ASP-Net/dp/1430265299/ref=sr_1_4?ie=UTF8&amp;qid=1543701625&amp;sr=8-4&amp;keywords=asp.net+mvc+5) when MVC was in version 3. Book was very good (for MVC 3) and was highly recommended. Be sure to learn MVC 5 if it's focus on Razor, IIRC Razor was more html-tag oriented in version 5 (smth like custom HTML tags). &amp;#x200B;
Thanks I'd seen that. Since you recommend it I will try a free Kindle sample. The full book is $42 even on Kindle (!!) but if it gets me the job it could be worth it. By the way I may also try this out after signing up for a free trial using my LinkedIn account: https://www.linkedin.com/learning/asp-dot-net-mvc-5-essential-training/using-razor-syntax
I also have a PluralSight subscription. Their courses are pretty much high quality stuff. I think you can enroll for a free trial for X hours a trainings (can't remember how much exactly). This is the newest Razor training (2 hours 42 minutes), I think it will cover all the basics for the interview: [https://app.pluralsight.com/library/courses/razor-pages-aspdotnet-core-getting-started/table-of-contents](https://app.pluralsight.com/library/courses/razor-pages-aspdotnet-core-getting-started/table-of-contents)
Unfortunately Node.js is not an option for us. I personally want to use the best tool for the job but management is pretty opposed to anything outside the .NET ecosystem. 
Undecided, I've used travis a lot in the past and this is my first repo with Azure; I may decide to go back, or perhaps do both.
Your welcome.. This kind of reverse engineering is extremely fun IMHO
Do they have a jar file?
Who are "We" and "Us"? Who do you represent and who appointed you as spokesperson? You don't speak for me and you don't speak for thousands who earn their living not only using Microsoft languages and but also their many technology platforms. Microsoft's continued role as a technology leader is very relevant.
why bother? aspnet core + razor is pretty great for rapid development. It is super easy. Anything you could ever need help with is easily available on google AND won't be crazy outdated a month or two after being answered. The JS frameworks are a year or two away from really being worth using but core 3 is like 6 months out and has some great things coming. &amp;#x200B; I am of course more of a backend guy and I do use the API extensively.
As another used said, httpclient may be a better version than WebClient. Use your discretion based on your research.
Looks like there is a relatively recent issue about this on the.net core repo https://github.com/dotnet/core/issues/766 Couple of workarounds in there and I guess someone forked ikvm and continues to work on it.
I love .Net, but Core is a train wreck, imo. It may be fast, but EFCore can't do what I need it to. Additionally, it requires lots of cycles on things like this where roughly 100 people in the world have tried integrating certain packages. We're back to 4.7.2. Life is easier.
I wouldn't spend too much time worrying about the razor, it's very simple. The basics is use of a view model, using the @ symbol to insert razor rendered elements and knowing the html helpers. However if they also use mvc5 and you're not up to date on it then I would look in to that far more extensively. Route configurations, Action methods Action filters Model validation Authentication Annotations Authorization decorators Controller Vs apiController As an aside, if they're using mvc they're likely also using Entity Framework, which is a whole set of tutorials on its own. If you haven't already, sign up for a free trial on www.pluralsight.com and watch a bunch of tuts on there. You're gonna be very busy til Thursday. Good luck!
Because it's not sexy and it's not what the cool kids use today, duh!
sure as hell is super practical. I use the scaffolding for tons of random tools I have to make for internal use.
Well they'd be hiring me to the front-end work so they told me Razor, HTML, Javascript and CSS. I do want to nail the Razor aspect since the other technologies are second nature to me having been a web developer since 1999, but not having worked in a MS shop in 4 years. When I work with MS last I was a full stack developer, and I do intend to brush up on the facets of MVC as well, and in particular since I would be a front end developer, want to be sure of the mechanism for the Controller to expose data to the View. Thanks for the suggestions, and thanks for wishing me luck!
I've found that Shawn Wildermuth has pretty great courses on Pluralsight. My second is Scott Allen, but Shawn is much better at presenting best practices.
Is this a subsystem of a larger business system, or a standalone task management system? If the latter, can I ask why using something like JIRA would not meet the requirements? Surely it will be cheaper and saner than trying to maintain a bespoke system for a long time.
Also wondering this. There are THOUSANDS of task management systems. Any particular reason you want to build another one? JIRA Microsoft Teams LeanKit Trello Pep All are good.
I have the mvc 5 version of this book on my kindle, it is how I learned, did the trick. It's nice that razor support is coming to VsCode - but I've largely moved onto Vue and .Net for my API.
The company has a variety of existing software and databases, long-term plans would be to combine these, but for now I would like to provide a working stand-alone solution. I worked on a project a 8 or more years back which used JIRA and I was happy with it then, but the majority of opinions I have seen on JIRA in the last few years where not positive. Also it is not open source and/or free as far as I know. I know there are many existing solutions, I prefer to find something that I can implement as a base framework on Azure, to extend with an API for Xamarin or other software providers later. It's also my self-learning path for .Net Core, so I was not looking for Java based solutions etc. &amp;#x200B; &amp;#x200B;
There is a thing, I don't know if there is a name for it, where you search and read the readme's and search some more. You find the project description that meets your needs but dig down and you find Java, PHP, Python etc. etc. I like C#, call me weird, but I want something I can work with and need to skill up on areas I have not kept up with; .Net Core, some MVC and entity framework. I could spend days searching thru github and other collections, but I just wanted to hear from people using something current and/or leading-edge in .Net Core that provide some or all of my previously mentioned requirements. Preferably without PHP. &amp;#x200B; &amp;#x200B;
Yes
Whatever you go with this isn't something you should be writing yourself. Unless you have nothing better to do for the business. It's a solved problem.
It's ASP.NET Core, but try this: https://education.launchcode.org/skills-back-end-csharp/
You can create a .net dll using a Xamarin Binding Library project. Create a new Xamarin Binding Library project, it will create folded named JAR, add the file to this folder using VS. Mark it has an Embedded JAR file under properties. Build the project and in the bin folder you will see (yourProjectName).dll We use this for android apps and works well. It basically embeds the jar file in the dll and then creates wrapper functions and classes that call the jar file. Let me know if you need any help.
Though updates to the UI like elements changing can cause problems. Or if a ui is used with animations, as an element may not be visible if when you click something else until the animation is done.
If you're referring to me, I've been a web developer for almost 20 years so I'm not one of the "cool kids" by any means
You could use selenium. A framework to make tests using browsers. 
I don't understand all the hate for Jira. I've used it on big and small projects with few and many developers and I've never had any issues with it whatsoever. Jira is probably my favourite software ever made.
I'm referring to the general trend of the last 5 or so years of chasing the latest hot front end JavaScript framework.
Yeah there's even a name for it: "Javascript fatigue". And I've even coined the term "React fatigue", because even within the framework, best practices are constantly changing.
I'm starting to think I can get by with videos for preparation for the interview, and then maybe if I get the position, splurge on this book and maybe read it before my first day, which always takes a couple/three weeks to come about, maybe longer now due to the impending holidays
As a full stack dev, I just don't see why any of this is necessary. We still produce perfectly acceptable web applications with mvc and jQuery. The churn in web dev is dumb. 
This is certainly true, and certain HTML structures can be a nightmare to find a good XPath query through. Element IDs become a valuable commodity - hopefully the app developer is willing to at least add these if requested.
Generally, I've found negative JIRA experiences because of how the company configured it... Made it far too complicated or far too restrictive for workflows. If you configure it in collaboration with the folks actually using it, and leave it flexible, then it should be fine. Executive demands need to be kept one layer out of direct JIRA management. Couple of things to point out: it has an API that you can integrate other systems with when you get to that point. And yes, it is not free. But neither is an engineer's time to develop, deploy, test, and maintain a custom solution.
I see what you're saying, you'd like to also use it as a way to improve your skills. I dig it. Unfortunately I don't know of any base frameworks, but to be honest, task management software is mostly just a glorified ToDo list. And guess what the most common example code is ;) I say just start from the bottom up and have fun with it with .net core, aspnetcore, maybe even a new frontend JS framework so you can skill-up there first. Start very small, a list of tasks with titles, due dates, and descriptions. Then add user you can assign them to, and then teams.. Could take ya awhile though!
Well the good thing is with azure i am able to connect my own free azureDevops account with your pipelines in my fork, which is really handy right as i'm testing out certain tweaks to your pipeline ;)
You really don't even need jQuery anymore, the issues it was meant to solve have largely gone away, so that you can just use the plain old Javascript/DOM api, or "functional" features from ES5, and new API's for Ajax. 
Well, I understand the logic behind that decision. Given that someone had created a Python version of Puppeteer - Pyppeteer, I figured that someone might have ported it to .NET. [Cha-ching](https://github.com/kblok/puppeteer-sharp). Hopefully this helps.
&gt; Also it is not open source and/or free as far as I know. Building your own isn't free either, unless you aren't paid. It will be far cheaper for the company to buy an existing solution than have you work for 6 months+ building one with 1/10 of the features.
Honestly? Most of the WinForm code I've seen in the wild doesn't remotely try to abstract the presentation tier. WinForms has its roots in old Win32 event loop programming long before that abstraction pattern came into vogue. One of the big reasons WPF came into existence was to allow that type of thing.
You can try model-view-presenter. Although it is not as elegant as MVVM...