Hmm a bit unfair. Rails opinionated way makes it the best framework out there for getting something to production quickly with a minimum of fuss.
Yep, all good points. We are definitely staying away from fancy tricks. I know you have to sometimes use those for business, performance, or whatever reasons, and this is a problem with the bleeding edge. However, if you can work around that, Core really is quite nice. And while the documentation is hardly ideal, there is a lot of activity out there, and I like the excitement!
Haha yeah that was my underlying point. Different people have different domains though so it's always good to have another tool in the tool belt.
It could also be read as .Net standard 2 being defined by whatever they manage to get into core instead of .Net standard 2 being defined and then trying to meet it with core.
Here's the thing, I am no Luddite (https://github.com/dodyg/practical-aspnetcore) and I have been using RactiveJs for about 3 years now, on the TS bandwagon since the beginning, etc. I simply do not trust the culture of full JavaScript framework. Everybody is so gungho about 'progress' and nobody really care about backward compatibility, etc. We are spending so much time and money to write and rewrite systems that essentially does the same thing.
That's true. Thing is, 2.0 isn't finalized yet :) So you're dealing with nightly builds which change potentially every day. Also, if you port now, it depends on what you're porting whether your users will use it now. Chances are they'll wait for RTM before they'll use netstandard20, so porting now is likely not that beneficial. But if your code works after the port today, you don't have to do it again within 6 months, true.
15 seconds. But then my VS is fully with loaded with Xamarin and Azure SDKs as well as Resharper.
That is impressive. What are your specs?
Once I've installed the Azure SDK on this weak machine and it fucked up my VS even more hahaha. And never could get 100% rid of of it (I'm pretty sure the uninstallers don't uninstall anything significant). It only did go away when I've reset Windows.
They don't provide PC or a budget/reimbursement? Sounds like a shifty outfit... you might better start shopping your resume.
Unless you're a consultant you need to get out of there.
10 Microsoft seconds. Microsoft seconds have no defined length.
This is a good point, I neglected to mention some of the solutions that Couchbase can use to solve similar problems. Perhaps I need a part 4, thank you.
Maybe 3-5 minutes to open a solution with a bunch of projects, including a website with over 1k pages, resharper. Maybe 1.5M loc total. i7, ssd, tons of RAM. Just opening vs or a smaller solution takes seconds.
Did you try this uninstaller? https://github.com/tsasioglu/Total-Uninstaller Got rid of all the uneccessary stuff for me.
If I'm understanding this right, you're trying to convert your model to json? Then all you need to do is `return Json(model);` Here's an example. https://hastebin.com/abufipajoc.cs
Who cares how many files it is? Zip the folder and that's your deployment package. 
I'm mostly backing away from Javascript frameworks. I guess I'm just not finding many use cases where I can't get the job done quickly and easily with JQuery, an API delivering JSON to the frontend, and a bit of regular Javascript. Perhaps I'm just not doing complicated enough things to be worthwhile, but I guess my question is: Why do you need a JS Framework? I see a lot of people rushing to learn one, integrate it etc... only to find they've spent far longer on that than they would've done on implementing the original task. The best exhibit for this: Go to any of the "Getting started" pages for a framework, and pick a random example, then see if you can implement it as/more simply. So yeah, I think JQuery, as more of a utility library, takes out enough of the fiddly bits and heavy lifting that it makes Javascript into what I wanted it to be 20 years ago - and I don't need much more
I forgot to mention, but it is a home office job.
Thanks.
&gt; (Startup time to an empty template shouldn't be more than 2 mins on a decent machine, loading from last project depends on the project dependencies) What to do mean "to an empty template"? Would it be loading VS and then creating a new project?
Nope. I'll give it a shot.
Can you not use a dependency property instead of injecting through the constructor?
I don't think this is best handled during serialization. You might try to define an interface and return different types that implement it based on your projections (maybe a factory). Anyway, serialization should only be responsible for serializing the type on hand. My two cents...
Isn't there also just IEquatable (no generic)?
I totally understand why they theoretically should be in an interface, but having default implementations for all objects makes it a lot easier to use external code written by potentially not very clever people. Imagine having to write your own hash code function for every class you use from an external library because its author didn't bother implementing it for common types (e.g. ones you may want to put in a map/set).
Yes. Edit: To add to this, I believe VS only loads an empty template on first install. Every other instance being loaded will attempt to load your last project unless you always close your solutions first before you exit VS.
Right.
If the author never makes mistakes, yes. But we all have to use code written by people who make (a significant amount of) mistakes at some point.
Ah I see what you mean, now!
In practice 99% of all hash code usage goes through `EqualityComparer&lt;T&gt;.Default` anyway, so having the default implementation sit in a built-in "ReferenceEqualityComparer" wouldn't be any less convenient than having it directly in `System.Object`.
That was really interesting but there is a mistake: &gt; Using this implementation, two anonymous types return the same hash code if and only if all the fields are equal. Not true. If you have an anonymous object consisting of 3 int32 values, it would be impossible to represent all possible combinations uniquely using a single Int32 hash code. 
I often use arbitrary objects as keys in a hash table. Having to invent surrogate keys would be a pain in the ass.
No, because all objects already have a non-generic Equals method.
Yeah, and also grab at least a hybrid HDD.
It's no different from regular .NET projects.
Try this: http://xunit.github.io/docs/getting-started-dotnet-core
Ah, that makes more sense 
Have you considered implementing an `IOutputFormatter`? The method `WriteAsync(OutputFormatterWriteContext context)` gives you access to your services via `context.HttpContext.RequestServices.GetService&lt;T&gt;()`. But I agree with /u/gblosser that this is probably not the right place for this kind of work. 
Check out NSwag which can generate Web API controller stubs from a Swagger spec: http://nswag.org
I agree with you. Considering this is the dotnet reddit, I'm assuming those who frequent here already grasp dotnet and have a familiarity with C# or F#. In that case, TypeScript likely is a more natural transition from dotnet languages compared to directly jumping to JavaScript.
It's just a 4 core AMD 3.2, 16 gb ram. 6 dedicated to the windows VM... make sure you make your virtual disk at least 150gb though as it's a pain to resize and VS is a heavy bugger. 
https://github.com/mzrimsek/dotnet-core-postgresql-boilerplate/tree/master/Test.Unit Nunit is my preferred testing framework. This is for an older version of dotnet core but the idea should still hold up. It's just like testing in regular .Net
I didn't think it was any different from full .NET?
Agreed! 
its not
aka monodevelop
If you're running 1.0 RC4, you can use `dotnet new xunit` and it will generate a basic project and test file. Then you can add a project reference to your test project by using `dotnet add &lt;test project file here&gt; reference &lt;path to csproj file of project to test&gt;`. Then testing from there is the same. If you don't know xUnit, head over to http://xunit.github.io.
Is your project set to debug?
How do you check that in VS Code?
Have you tried the embedded mssql dB
I'll probably forego EF then and just use LiteDB. Thanks!
About the only cautions I'd advise is that it is still a young DB, so there are sharp corners and occasional bugs. Also, it has a very low bus factor (1).
How have you been liking EF Core so far? I keep hearing it's not mature enough for big projects, but I do still plan on using it in a small-medium sized app i'm working on, just haven't gotten to that layer yet. Is it as bad as everyone is saying?
Have you thought about maybe using Dapper and sqlite? I get it that you would have to write your own SQL queries but is that really hard to do.
I'm not sure yet...it's my first time using EF, so I don't really know. It seems nice and easy to use, which is what ORMs are there for, anyways.
I worked for a few weeks with Angular2 in TS (before .NET Core), and while the concepts were okay (I like that, contrary to many other frameworks, it's possible to, mostly, separate the view into external template source files), I found the stack extremely cumbersome to work with. Here are a few of my gripes: * You need a comprehensive suite of tools, just to setup a proper build chain. Grunt/gulp or similar, TS compiler, TSLint, definitelyTyped, even down to choosing the right module loader wasn't exactly straight forward. Sure, once you set everything up, it's not that difficult. Except when everything changes in an update, and you either need to relearn or ditch a tool for something else. + its a lot of setup for every new project * Using any 3rd party JS lib that hasn't got typedefinitions was a pain. For someone new to typescript, I didn't have a clue where to start, and something that is supposed to be as easy as plugging a .js file turned into another research project, on how to properly adapt a JS lib with typedefinitions (which ALSO needs to be maintained when the js lib is updated) * Angular2 got updated heavily when I was working with it (don't know how stable it is now, but it was literally every few days a new version came out, and something changed.) * Even though the separation of views into templates were okay, in comparison to many alternatives, it will break intellisense in VS and it's not, to me, the most intuitive way to write MVVM/MVC interfaces. I came from a background in knockoutJS, which I personally find a lot more intuitive, much simplere to work with and though it can introduce a bit of logic in your view, I find it easier to read and develop the views.
&gt; use typescript. My god does it make life easier.. I disagree so much. I found it made everything much more difficult and cumbersome - though I do like the language itself.
&gt; it mixed HTML and Javascript This is one of the things that keep me from jumping on ReactJS. "Raised" with the very clear notion of separation views and logic, only just accepting the concept of hooking and binding for the model, this mish-mash really turns me off. I found KnockoutJS to be the best middleground in that regard. It's not as advanced as react and angular, but I found it much more stable, and much more "old-fashioned" in it's core concepts.
I agree with this completely. I keep wanting to try building larger projects with either angular2, react or some other hip framework, if nothing else, just to see what the fuss is about. But I keep thinking to myself - why is it better than this perfectly readable and simple Razor view with the odd bit of jquery? I'll never go back to a MVC 3 project and think - let's toss MVC and use something else. But if I go back to a MVC 3 project, where I happened to have tried some JS framework, I probably moved on from right after, I'm probably gonna have trouble maintaining/developing on it in the beginning, and want to switch to whatever I'm currently working with.
This will be a shameless plug as I am the principal developer but another potential solution is [Couchbase Lite](https://developer.couchbase.com/mobile). 1.x is still not compatible with PCL or .NET Core but the upcoming 2.0 (developer builds coming very soon, hopefully by the end of this month depending on how 1.4 QA goes) will be a .NET Standard 1.4 library and we will conduct testing on Xamarin Android, Xamarin iOS, .NET Core on Ubuntu, .NET Core on macOS, .NET Core on Windows, and UWP. It is a document based database with built in sync functionality to other devices via Sync Gateway and Couchbase Server. As a bonus, since these are prereleases for a brand new major version bump we are currently accepting comments and requests for the API. If you are curious about what it currently looks like it is all open source: https://github.com/couchbase/couchbase-lite-net/tree/feature/api-v2/src/Couchbase.Lite (2.0 API, 1.x is on master but after 1.4 this will switch) In addition Sync Gateway and Couchbase Server are also open source (all Apache 2.0 licensed) so I encourage you to take a look.
If you are writing proper unit tests, you will have another implementation. A mock.
Literally just added this before I left earlier and it did it. It sure how I lost it. Thanks!
Have you looked at SQL Server LocalDB? https://blogs.msdn.microsoft.com/sqlexpress/2011/07/12/introducing-localdb-an-improved-sql-express/
TIL: &gt; If you noticed, the implementation of the GetHashCode method now includes...: &gt; if (HashHelpers.s_UseRandomizedStringHashing) return string.InternalMarvin32HashString (this, this.Length, 0L); &gt; It appears that in .NET 4.5, you can calculate a hash code for [strings] for each [AppDomain]. Thus, by setting the [attribute value](https://msdn.microsoft.com/en-us/library/jj152924.aspx) to 1, you can get to the hash code to be calculated basing on the domain in which the method is called. Thus, the same [strings] in different [AppDomains] will have different hash codes. Turns out this setting was adding to [combat a DoS attack](https://www.cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2011-3414) centered around [POSTing a bunch of form values that all hashed the same](https://events.ccc.de/congress/2011/Fahrplan/attachments/2007_28C3_Effective_DoS_on_web_application_platforms.pdf) and thus generated a ton of collisions as ASP.NET constructed the Request.Forms NameValueCollection object. Really interesting! Also, hi /u/DevartSoftware. I use Review Assistant and Code Compare at work.
Once authenticated with the web api, how are you planning on passing the identity information? You said you don't want to "solely rely on token based auth" but your options are either token or cookie. You wouldn't want to use the latter which leaves you with token based.
Get started with unit testing. By passing the dependencies explicit (ideally as an interface) you can easier mock them, allowing you to actually test the unit. If you instantiate those classes inside your unit, then you can't unit test anymore - you can't test the unit in isolation, you always **have to** test a group of units. And if one of those is performing server calls, then you're having a lot of trouble.
That sounds fine if I'm understanding you correctly. As long as you only use JWTs on the api. If you had said you were planning on supporting cookies as well as JWTs on the api then you'd have an issue as you'd need to protect your controllers against CSRF attacks which would be a bit of a nightmare for your api consumers. I'd keep the api completely token based and any shared identity code can just sit in your core project.
I found this one but it's far from be completed: https://github.com/lucabriguglia/Bloeng.
Performance would be one reason. They are both very low level calls, and having to check a type implements an interface before calling them would add overhead. On top of that calling members on an interface is slower than a virtual method.
This may be the 64 thousand dollar answer. Thank you and I take a look at that book.
Dependency Injection is a set of software design principles and patterns that enable us to develop loosely coupled code. We can say that main goal of dependency injection is to decouple the code. So that it will be easy to maintain. 
I'll probably do a small project with it as well at some point. There are just so many things about that concept that puts me off. 
Did you try rewriting it as an interface? 
Could the Web Api not been used as a core/ backend, while the MVC app is being used as a proxy? 
This is why I work in consulting. New sites = new visual studio. Danged visual studio 2005 with visual source safe was hell in 2014.
The error message you are receiving typically occurs because you segued from your language declaration straight into your code, instead of closing the language declaration and opening up a new section for code itself. Example: https://support.microsoft.com/en-us/help/246828/prb-error-message-asp-0221-invalid-command-directive Unless you post the actual beginning of the file (the first few lines, at the very least), we’re grasping at straws here.
I'm terminating the directive line properly. &lt;%@ Page Language="C#" AutoEventWireup="true" CodeBehind="Login.aspx.cs" Inherits="User_Login_CS.Login" %&gt;
And is this truly the very first line in the file, or has the file been contaminated by a UTF-8 BOM?
Opened in Notepad++. Converted to UTF-8, saved as UTF-8, restarted IIS and the issue is still there.
Unfortunately I am a touch stumped. Taking everything at face value this should not be happening. My only viable remaining suggestion (at this time) is to do the programmer’s version of Tech Support’s “Have you turned it off and back on again?” -- Take both the *.aspx as well as *.aspx.cs files, copy-paste their contents into temporary Notepad++ files (don’t bother saving), delete the original files out of VS, close and restart VS, do a Clean on the whole project and then recreate (Add) the file from scratch and copy-paste the code from Notepad++ back into the newly generated files (overwriting anything that a general Add would include). And then do a general Rebuild (rather than a full Publish or Debug) on the Project or Solution.
A clue seems to be the wording of the error message: &gt;**Active Server Pages** error 0221 Notice that it doesn't say ASP.NET. It seems like IIS thinks your application is an ASP Classic application. Which is strange, because ASP Classic has different file extensions from ASP.NET (.asp and .aspx, respectively). This leads me to the conclusion that somebody erroneously configured IIS to map .aspx to ASP Classic.
I'm not sure. If you look at the link I visited you would think that would have been resolved (if that is the issue).
I wish, on my case we always get a system from customer's IT, so that everyone uses the same tools.
For a Console Client I usually do it myself, using Dependency Injection to get the token at the controller initialisation, that way I can be sure that the authentication process is always done. Further configuration will allow you to tune the persistence of the token. This solution need to be adapted for a web client. If you use Swagger, you can use Swagger Codegen in order to generate a client for many technologies (If you want a .Net Core client, you should use [Microsoft Autorest](https://github.com/Azure/autorest) as the codegen's generated code use a 3rd party library which hasn't be converted yet. Swagger doesn't work well with a bearer token but there is some solid workaround online. It took me 1 day to get a grasp on this whole problem with those tools and implement a working solution. Further work my be needed according to your coding standards but I can assure you that this is a viable path.
You still need to install LocalDB on your friend's machine. https://msdn.microsoft.com/en-us/library/hh510202.aspx
I just translated it, didn't notice it was in German. I also just installed SQL Server 2014 localDB on my friend's machine and it still doesn't seem to work.
Look at line 3 in your screenshot. Your file has a .asp extension, not .aspx Therefore it is being interpreted as classic ASP
Thank you for the answer. The thing is 2016 doesn't seem to work under Windows 7. And I installed 2014, but that didn't change anything. I think you're right though, since I just put a try-catch block around the connection and got something about a 'connection timeout during handshake confirmation'. edit: I just installed SQL Server Express 2014 and now instead of the crash I'm getting a 'cannot be opened because it is version 852, this server supports 782 and earlier'. So I guess I can't run this on Win7 at all?
A better clue lies in line 3 of the screenshot.
Writing tests is so much easier if you use interfaces.
Now I regret choosing to just do the queries manually.
Thanks, I feel like maybe my greatest issue is that I'm coming into this in the middle of what seems to be a large transition where many libraries and the dependencies are stuck in legacy instead of core.
At a guess, to keep compile time quick. At least `@model` tells you when you use fields on your `Model` object that don't exist.
What did you find more difficult about it? Personally I find it's so nice to define a type/interface once and then have automatic typings provided all through your code base. But I'm a typed-language kinda guy, so...
If moving to core is ever in your pipeline, then identity server is the way to go. I went to sdd conference in 2015 and dominick and brock were already looking into the core pre release, and as such they are the people that Microsoft shared their daily builds with to help with their core development 
I like your suggestion and I'll try migrating to SQLite on a branch. I guess there won't be much difference in speed, right? I only have a few thousand entries per table max.
&gt; If you're main focus is security and speed then I would also wait until the core version of identity server (and even kestrel itself) is a bit more tried and tested. Yea, that's why I'm waiting til at least v2. I'll probably just stick with Identity since it's what I know (the most about) today. None of my projects get much traction as it is anyways.
Yeah I hear you there - that's been my biggest pain point with the language thus far as well. Although it is getting better - a lot of npm packages now include typescript definitions, instead of being distributed in separate packages under @types. Also (I've never done it, so I'm not sure here) can't you just import a package as any, allowing you to use the entire package with no typing support? Obviously you'd only want to do this if the package didn't have typings, but wouldn't that be the way to go? On the compiling - I've never noticed it. I just use either ts-loader or awesome-typescript-loader with webpack; I've never once had to run tsc manually.
I get this, but I think it's worthy, because it could save a lot of pain.
http://cakebuild.net/api/Cake.Common.Tools.ILRepack/ILRepackAliases/6250E7E6
Your unit tests should be testing the code independently of the database, imagine if you had to switch database technology you don't want your unit tests to be tied to the DB. When testing the code you mock the database so that it only returns what you're expecting. That way you're not testing the DB but the code. If you need to test the DB do that separately from the code that accesses the DB. You can set up scripts that create a new empty DB instance in a sandbox and you perform your tests in that sandbox.
That's where integration tests are different from unit tests though, which is what OP is asking about.
When you are testing services, that access DB directly - DB mocking becomes somehow very complex. When you are using repository pattern - you should also unit-test repository...which is using database...so I need to mock database...right... I am using in-memory DB instead. I am asking about this: http://stackoverflow.com/questions/29275604/best-practice-for-selenium-tests-that-have-add-update-delete-functionality I am not about selenium, but things in general. I am using HttpClient for end to end integration tests.
Apologies I didn't read the question closely enough.
As integration tests, they are not going to be independent in the same way that unit tests will be. If don't have too many databases, you can make a back up and restore it before/after each test run. This can be hard to maintain if your database is still evolving. In the past, I have used EnsureXYZ methods with a bunch of asserts to make sure that things I rely on for several tests are there and have the expected properties. For example ensure a product definition exists before doing a series of CRUD operations using the definition. 
You don't. Your integration test(s) will not be able to test one of the letters of CRUD at a time if you want to stick to the boundaries of your service and not break encapsulation. Also with integration level testing, you test the _role_ that the system under test performs and how it integrates with other roles. For this question your tests are playing the role of the client application. As you've said this is a CRUD service (which is a smell, in my opinion. What _function_ does it provide if all it does is CRUD? There's no domain in CRUD.) then there are several scenarios that are worthwhile testing, if you don't already have confidence from the unit tests. You will need to test: * **R**eviewing a record before **C**reating it returns nothing, and then when **R**eviewing that record after **C**reating it, you get results. * After **C**reating, then **R**eviewing to prove creation (and remembering the values at this point), that after **U**pdating and then **R**eviewing again, the values are different to the previous review as you'd expect. * After **C**reating, then **R**eviewing to prove creation, when **D**eleting, the record is no longer available. 
despite what /u/liamht says. Core is production ready and has been since 1.0. They are just flipping the tooling bits here and there and from the sound of things the core API's are not going to change drastically (besides SignalR Core aka Sockets). Kestrel's hosting model allows it to be reverse proxied by any other web server which is how you should be hosting for now (And thats how it is hosted in Azure, behind IIS) That being said, if you are doing Core (on full framework or not), you will need to go IdentityServer4.
What about MVC5 and Web Api 2? Is the default auth enough or should I move to IdentityServer 3?
You can use tokens issued by an IdentityServer 4 authority against an MVC 5 / WebApi 2 application and similarly you can do vice versa with MVC Core and IdentityServer 3. IDS3 and IDS4 implement the openid-connect specification, any openid-connect provider can be used as an authority to do authentication against any other party that relies on openid-connect based authentication. They implement the same protocol. But if your choice is between 2 and 3, definately 3. I don't think that 2 is even maintained anymore. 
Written by Maoni Stephens Thanks.
Some of this is possible by setting up initial state using an initialization phase in whatever testing framework you are using: * Need to delete an entity with ID 12345? OK, make sure that row exists in the DB * Need to retrieve an entity 12345 and check that FirstName is 'Fred'? OK, make sure that data with that entity ID exists in the DB etc. You'll notice that the two tests I described are in conflict-- if the first test passes then the second test will fail. So you have two possible solutions: 1) Give every test some kind of setup phase that directly sets up state as the test will expect it to appear. This can include directly manipulating the environment-- for example directly adding rows to a DB where the identity of the row is specified. Remember you are testing your code, not whether or not you can write to the DB.. 2) Give all tests a shared setup phase to create initial state, and make the tests *ordered*. That is, your DELETE integration tests will only run after the GET integration test has run (as described above).
[removed]
&gt; Will there be a .NET 5.0? Yes. &gt; Has anyone heard about a new version of .NET coming in the future? No. 
whataloadofcrap
Zend framework is what sent me fleeing to the holy land of .net.
That's a good question. They're putting a lot of effort into .NET Core now instead. I wouldn't be surprised if they just allow you to use WinForms, WPF, and UWP in .NET Core 3 and call it good.
My brain said Zend, my fingers said Zen, I fixed it, then I reworded what I said and ended up with Zen again. *facepalm* ^Oh ^well, ^at ^least ^I ^didn't ^type ^Zenyatta, ^seeing ^as ^how ^I ^was ^just ^playing ^Overwatch.
I just went through the exact same sequence LOL
I doubt core will ever support winforms. It's a future technology and the hole point was dropping dead weight.
yeah, that sort of thing is what .net standard is for
&gt; MS has opened up to 3rd party GCs What? How did you draw that conclusion from the article?
I hope it's more stable than 2015 update 3...
No. And it will never. Visual Studio heavily requires on WPF (which is based on DirectX) and COM. Both not available on Linux.
And here's me still plodding along on 2012 at work - I should probably pester the finance department for an upgrade
I think that's fine for many things nowadays - but a lot of the "Tried and True" assemblies you now take for granted have roots in the alt.NET "movement". As noted in the article - MVC, REST etc - they weren't a natural part of .NET, and quite a big part of them being adopted by .NET was entirely because people were mixing things up in this way. I do agree that ending up with one "done right" option by the time you're finished is good, but it also shuts down innovation to stick with that and only that set of options.
If your work doesn't have MSDN, just switch jobs and never look back
This is one reason why .NET is dying - most companies are only hiring the devs who will work on a completely zero-cost dev environment (nodejs or java). Having MSDN license for every employee is a massive cost most companies don't want to be on the hook for.
If you update core versions it's better to update them all at the same time. Is that what you are talking about? 
The first step is naming lots of other things with "Visual Studio" so that they dilute the value and meaning of Visual Studio. The majority of "Visual Studio" users will be using something other than the DevEnv IDE and this will justify ceasing production. :P Maybe VS will survive; .Net managed to survive despite all sorts of things being branded as ".Net Enterprise Servers" 
If they share a masterpage, you can put it on the masterpage. 
But that's not Visual Studio. It's an alternative product. That's fine, just not the same.
I figured out what was causing the slow down. It was Xamarin. I am now at 1.5 seconds for VS to open.
I wouldn't say never. MS ported MSSQL to linux, if you would have told me that 2 years ago I would have told you that you were insane.
As far as I can tell the short and skinny of this is use locks and/or Interlocked.Read/Write when working with variables that will be shared by multiple threads? Am I missing something?
That's what I do, lol. Throws some dnx error after the update. This is fixed in the 2017 RC, so I have hope.
RCs are still missing Code Contract support 😫
No it was a giant dumpster fire of a framework. It just was the absolute wrong way to handle web. For instance treating gets and posts the same way was just a nightmare, the way page events were handled, viewstate. I could go on and on about the issues with the framework. In my opinion everything that they could have done wrong they did. It's not feeling dated that's the problem it's the fundamental design decisions of the framework that were the real issue. 
Porting MSSQL to Linux is incredibly more easy because MS SQL itself has a tiny operating system already.
but afaik it relied on many win32 system calls to enhance performance which is critical for a DBMS
Companies don't buy new employees new computers, they reassign old ones to them.
&gt; Am I missing something? You obviously didn't scroll/read to the additional points in the article.
The most common multithreading mistake I've seen is using raw Threads instead of higher order facilities like the ThreadPool, the TPL, and async/await (for asynchronous I/O). Unless you know why these abstractions won't work for you, you should look into using them.
Using a Thread is no better or worse than the ThreadPool, the TPL, and async/await (for asynchronous I/O). Less convenient maybe, but with all four you still have the exact same problems dealing with shared memory. 
Who even uses F#?
This sort of worked.. The issue is now one of the dialogs throws an error but only does so on Page2.aspx. Error: Uncaught Error: cannot call methods on dialog prior to initialization; attempted to call method 'open'
This was actually pretty interesting. Didn't expect it to be that detailed from its title. Thanks
Sad that it took me to read your comment to go back to article to read additional points
Whoa.
This is wrong. Tasks and threads are different things.
I'm still hesitant to install VS 2017 RC on my PC. Last update was Feb 7th while the .NET Core SDK 1.0 rc4 was released on Feb 16th. There is so little documentation around the CLI usage, things as simple as adding projects to a solution from the command line are documented via GitHub issues. Not to mention all the subtle CLI behaviors like `dotnet run -p [PATH]` doesn't switch the current working directory from the folder you invoke it from to the folder the csproj is located at.
These higher level abstractions deal with many of the dangers that come with working with raw threads. It's exceptionally rare for someone to be justified in working with raw threads in .NET 4.5+. Something as simple as efficiently running parallel operations is best done with a Parallel.ForEach over your own threadpool implementation that tries to efficiently manage all the threads.
while I agree in most cases that reinventing is a bad idea, we should consider the authors context. as a game developer he may not have access to tpl. also there are GC and performance considerations when using these. My guess is this is from a networking lib.
You're correct but tasks do serve a different purpose and therefore are applied in different ways in common scenarios. Comparing the two like you did will just lead to confusion, something many people already have. So long as you have concurrent processes you'll have synchronization problems but how you solve them changes between tasks and threads. Even if that change is subtle it ought to be considered.
This is the first time I've heard about you guys. It looks pretty cool. My first immediate question is do you have any plans to hold conferences in Birmingham?
[removed]
Sorry about that, I did miss the last bit regarding Thread.MemoryBarrier(); for counters in loops. However, the first bit ends with "use a lock", the second bit ends with "use a lock", and the part after that admits it's unlikely you will ever run into the problem.
September is also [around when the much-delayed .Net Standard 2.0 should come out](https://github.com/dotnet/core/blob/master/roadmap.md), which should be milsetone for .Net core and for the cross-platform story. It will be interesting to see what this release brings to the table.
Does anyone else find TypeScript static analysis to be clunky in vs 2015? I'm using Angular 2 with Typescript inside a .Net Core .xproj. I have a defined .tsconfig that uses es2015. It seems like I get red squiggles way more often than I should and it takes a compile or sln reload before they go away.
An instance per customer can be easier to scale to many customers if that is a requirement. Do you need to install any services or is it just an IIS web app?
I'm working on a SaaS implementation of a legacy server software myself. We're using Azure Active Directory for authentication. This may be a viable option if you are targeting organizations. I have high hopes for AAD since it allows organizations to manage access and permissions for SaaS applications in one place. For separating each tenant's data we use the new Row Level Security feature in Azure SQL/SQL Server 2016. Our code rarely needs to reference the current tenant instead we filter queries based on the current user's claims automatically. In fact during development we can run the same code on LocalDB without multi-tenancy. We opted for having a shared infrastructure to be able to keep the subscription fees low and be able to be more agile. We have invested heavily in automatic builds, tests and deployments. Any cloud services such as Azure AD, Azure Storage and SendGrid (email service) has been abstracted away in the code so when we do get a client where an on premises deployment is appropriate we can implement the necessary interfaces for that. It doesn't have to be one or the other but I strongly believe that shared infrastructure should be the default. On premises deployments can be good for isolating custom functionality for specific clients until it becomes mature and there is a greater demand for it.
If you copy the json, you can go into Visual Studio and use Paste JSON as Classes from the Edit menu. Edit-&gt;Paste Special-&gt;Paste JSON as Classes 
There's [Azure AD Business to Customer](https://azure.microsoft.com/en-us/services/active-directory-b2c/) but I have no experience with it. And yes we use App Service for hosting the web app and use wen jobs for background processing.
Woah I never noticed that, thanks a bunch. Learned something new! :)
...again? * https://www.reddit.com/r/dotnet/comments/5vc966/stringbuilder_the_past_and_the_future/ * https://www.reddit.com/r/csharp/comments/5ugymb/stringbuilder_the_past_and_the_future/
[removed]
I have one: http://www.tiobe.com/tiobe-index/ Shows C# up And another: https://www.indeed.com/jobtrends/q-Java-q--PHP-q--Perl-q--.Net-q--Python.html?relative=1 Showing .NET with java being dominant Or this: https://github.com/blog/2047-language-trends-on-github C# going up. Your turn. 
you need to look at the query it's generating, and then look at the execution plan. likely missing indexes.
I definitely agree that this is the way to go. If the code is more complex than "get some records", I typically just write the SQL out by hand. You'll find that EF (and most other ORMs) generate really sub-optimal SQL. As for tools, I suggest SentryOne Plan Explorer for viewing the Execution Plan (it's much nicer than SSMS, and free!) (https://www.sentryone.com/plan-explorer). If you need to share the execution plan with us, use https://www.brentozar.com/pastetheplan/.
Is your test database also on your local machine?
I'd have to check, but I think we only have a handful of additional indexes besides the ones automatically created by EF on primary and foreign key properties. I'm sure there's room for improvement there, but I've always been shaky on parsing execution plans. I wouldn't have expected poor indexing to cause the current behavior though -- normal execution time on the first batch, never finishing the second batch. Nor does it make sense that it would work in one app environment and not the other (using the same database server), since the only parameter involved did not change and the database's execution plan should remain the same.
&gt;there are no network connectivity issues either (the production servers are chugging along doing all of their other work just fine) Have you specifically debugged the network between the app server and the database server, or between the app server and anything else? You said it runs fine between your local machine and the database server; what about between the app server and the database on your local machine? You can use Wireshark (on either end) to view at the transport level the data going between the two servers. Is anything actually being transmitted while the database server is waiting?
Is the context identical when running from your local machine and in production? If it works fine on your machine against the same db, that feels less like a code issue and more of a config one. Have you confirmed connection strings and connection properties are identical? 
What would be the advantage of running the app in a Docker container instead of Azure App Services? A lot of examples I've seen on the Azure/Docker are using the Docker container for the automated build/deployment, but deploy on Azure App Services. The obvious one would be that once you have your app running in the container you can easily deploy it to another host, but is there something else I missed?
Thanks, that did it!
Could also be caused by a transaction that is not being committed.
This. Is your production enviroment in the cloud? Some cloud providers have a cap/max network speed on their smaller instance types. Just talking from personal experience with cloud providers and ASYNC_NETWORK_IO waits.
I've had a lot of problems with TS in VS 2015, especially with Resharper. VS 2017 is a huge improvement - maybe give it a shot. It's only at release candidate status for now, but it seems pretty solid. It also has a huge amount of improvements - they retrofitted a lot of stuff from VS Code into it, making the native JS / TS editors a lot better than in 2015. Also, I've found it doesn't upgrade projects to a newer version, so you _should_ be able to go backwards and forwards between 2015 and 2017 (haven't actually tried, but it's an educated assumption)
Saaskit from memory looked bad, it basically fires up an app domain per tenant. So much memory wasted. Just build an abstraction on tenant in your middleware, and share the application/servers. A lot of saas companys use hostname with a wild card ssl cert to differentiate each tenant, and build out underlying middleware for database access etc off this abstraction. Sharing storage will depend on the size of the app, and storage choosen. NoSql solutions can scale very large so you may get away with one storage system, but we found with sql server, we eventually moved away from a single shared database to scale out, now we have an sql db per client, with a bunch of clients on each mirror pair to scale out. You may be able to get away with a single db even with sql depending on your app and amount of data. I have worked at a large SaaS company for many years, we have shared web servers from the start using middleware to manage vendors, the idea of separate infrastructure seems crazy from a scale, ops and cost perspective. Most buyers these days in enterprise are a lot more comfortable in the shared SaaS model. We rarely run into security or compliance blockers when compared to 5 or 10 years ago. Even with bluechip and government clients. 
What tutorial course from Mosh did you watch? On pluralsight there is his course : Become a Full-stack .NET Developer , a 3 part series where he builds real web application. It is probably one of best course s you can take for making real web applications in MVC5. It covers: - Making real world application using best practices - Explains the thinking pattern in solving real world patterns - How to build simple API - How to build right architecture for your application, refactoring - Unit and integration testing His series helped me a lot when I started working with ASP.NET MVC. Link for first part of series: https://app.pluralsight.com/library/courses/full-stack-dot-net-developer-fundamentals/table-of-contents
agreed
The context and connection strings are the same, yes.
I'd say if it was really important that both operations occur in the specified order, it should be written like this: static void Main(string[] args) { // do not inline; it is imperative that both operations run in this order due to side effects var first = FirstLogicalAction(); Thread.MemoryBarrier(); // do not inline var second = FirstLogicalAction(); Thread.MemoryBarrier(); if(first || second) Console.WriteLine("The alternative is true"); Console.ReadLine(); } More likely you have bigger problems.
Thanks, I had a feeling that vs 2017 was going to be better with Typescript since they had a focus on tooling. We have plans to move to 2017 in March when it's released since we use dotcore. I'm really excited for the upgrades!
Sorry I don't know much about Azure and don't know what app services is. I work on an AWS shop. 
It's not in a specific cloud service like AWS or Azure, but it is at a remote hosting environment. So... technically, yes, it is. :P
Never do this. Never Ever, Ever Do this. For any reason, in any code base, at any time. To anyone or anything. If you want both sets of code to execute you should do it outside the scope of the function. The only time you should use a function in an if statement is if it doesnt matter if it executes or not and the logic behind it makes sense. for example using .Any() would make sense. 
My mistake, corrected what I was trying to say.
If you do not want short circuiting it is usually because of the side effects, which is horrible in its own way.
I just tried the production app server + my local machine's database... it didn't work either. I haven't done any network analysis yet. I'll be trying that soon, though.
Yeah, after watching his C# courses I saw he has MVC 5 course, I just didn't know will I have a "live" web app after finishing it. Would you mind checking last section of that course to see will there be a finished, reacting web app after deployment? Thanks!
Actually I wrote that in original post. I already have pluralsight's subscription and I'm looking for similar courses like Shawn's because he codes in Core, and I would like to code in MVC 5.
I watched his C# courses on udemy. I saw the course you mentioned, I just thought it's oriented on full-stack, meanwhile I plan to get a job and work on back-end only, but I guess I will watch it regardless. When you started, you jumped straight from C# to this course? Thanks!
The RC has been great for me. Our large code base wouldn't even run in 2015 but in 2017RC I have been working just fine for the last 2 weeks. ReSharper on the other hand ... I so wish I could find some other extensions that offer the same navigation stuff as ReSharper.
The memory barrier is not needed. The compiler will not re-arrange the order in which the methods are called.
It absolutely can. https://github.com/dotnet/csharplang/blob/6027ad5a4ab013f4fb42f5edd2d667d649fe1bd8/spec/basic-concepts.md#execution-order Nothing there prevents the second method from being called before the first unless there is a data dependency in those methods. An example program: class Program { private static int _A, _B, _C; static void Thread1() { var first = FirstLogicalAction(); var second = SecondLogicalAction(); if (first || second) _C++; } static bool FirstLogicalAction() { _A++; return true; } static bool SecondLogicalAction() { _B++; return true; } static void Thread2() { if (_B &gt; _A) Console.Write("!!!"); } static void Main(string[] args) { Thread threadOne = new Thread(Thread1); Thread threadTwo = new Thread(Thread2); threadOne.Start(); threadTwo.Start(); threadOne.Join(); threadTwo.Join(); Console.ReadKey(); } } Nothing in the specification prevents this program from writing !!! to the console. I think no current implementation/arch would do so (ARM might) but it is not prevented by the specification. more about the C# memory model: https://msdn.microsoft.com/en-us/magazine/jj883956.aspx 
Yeah the column names will be dynamic, and you're right, this is an internal corporate thing. Doesn't matter if it's a good idea, it's what the bosses want. Myself, or someone experienced in SQL will be the person adding credentials and generating reports from the data for whatever department wants it. They won't be anywhere near the connection themselves. They can just request a portal for them using whatever data they already have. Thanks for you help, I really appreciate it. 
I love Plan Explorer. The UI is a rather clumsy, but the information it provides more than makes up for it.
What's your timeline? If you want a .NET CMS with multi-tenancy, plugins and themes, have your looked at Orchard? It does full tenant isolation with separate tables (or separate databases). Lombiq offers several free and commercial modules (https://dotnest.com/knowledge-base/topics/lombiq-hosting-suite) to help with the scaling and to facilitate tenant management. Orchard 2, which is the port to Core, is also well under development. 
Remember that you are the expert. If you think "this isn't a good idea", put it in writing in an email and cover yourself. Whilst is may be a business requirement, there are always alternatives.
Try looking at what omnidb does. https://github.com/OmniDB/OmniDB
 Does your course cover or discuss Autofac.Multitenant all?
Shameless plug: https://github.com/jdaigle/Horton
Seriously?
 Here's a use case: How do each of these migration systems handle updating a view? For instance, I make a git branch called BranchA and I adjust a view called MySpecialView. My friend makes a git branch called BranchB and also adjusts MySpecialView. Now we both merge back into master. What happens with each of these migrators during my next migration run?
Gotcha. Yes, this is important. This is where EF falls very very short (amongst other problems) Redgate toolbelt and SQL Server Data Tools both can do it this way. I haven't checked other tools.
And the dispose and close in the finally 😎
In Fluent Migrator (which we use) each migration is assigned a version number. Our strategy is that when you make a migration, you assign its number based on a YYYYMMDDHHmm time stamp, so the chance of collisions is extremely low. Whoever's migration was created last would run last, so assuming you both used ALTER VIEW, the last migration would "win". 
Fair point, I suppose I can at least tidy that up :p
See my multiple comments about the "view" scenario. EF doesn't really handle this situation great (among many others) Edit: Many others include but not limited to: - You have to run "add-migration Whatever --nochanges" to update the internal Db model when you merge two branches of code together. I can't even tell you how annoying this is. Deploy fails, you have to go fix it by hand, then it works. I'm 83% sure you can't run this command automatically either which is even more annoying. - Two people modifying the same object can overwrite each other. Why would you voluntarily do this to yourself - There is no mechanism to protect against data loss that I'm aware. Which is fine... but... other options offer this feature - EF can be really really dumb about how it handles renaming tables, columns, etc. At least you can write custom SQL to do this. Although if someone else modified the same object in a different branch, you have to go clean that up after you merge code together. With other migrators + git, this would just magically work without issues. - I'm sure there are other things that are stupid about EF migrations but I still have some limited experience with it
https://github.com/chucknorris/roundhouse is another that can handle this scenario (and it is, admittedly, far more popular than my pet project). Also https://flywaydb.org, which is pretty popular in the Java world.
Yeah, I get your point. In your personal opinion, would an accredited "course" have more value in a resume, or something not matter at all?
Do you have any good recommendations of where I can learn .NET? Like, any sources to learn where I could actually learn at a "entry job level"?
I swear by SSDT. I can't understand why anyone would go back to incremental updates.
For me if I were interviewing you, probably not a huge difference: most people interviewing don't really know which courses/institutions are worthwhile. I'd want to dig into your knowledge and what matters to me is what you know, not how you know it (unless, like, you killed someone and stole their memories, that's pretty unacceptable) I suspect it's actually more useful for getting you past the automatic resume keyword-scanner, and the HR guy who's been told "Find us a C#/.NET developer", rather than for the interview itself, but at the same time, you can get past both of those just by listing it in your known languages. It may also make you more confident in an interview, though, and in the early stages of work: it's nice to be given work and be able to do it, rather than having to ask how. A bit of extra competence goes a long way. Basically, if you feel like your knowledge of .NET isn't great then I'd consider it worthwhile just for your own development, or you have some spare time and cash then it won't do any harm in your job search and *may* be useful. But if you need the money for food, skip it.
Interesting way of doing it. One thing that is nice is to see WHEN each script ran. Helps with "versioning" the database.
Pick a project that you would find interesting to work on. A lot of .net shops are doing web development, so maybe build a simple website that talks to a database.
True, but it's still worlds better than Liquibase where every script is a one off.
EF has some interesting problems when it comes to loading nested Includes. I can elaborate another time but the gist is that it ends up retrieving something like the cross product of rows which scales exponentially with depth. Try separating your single query into individual queries and doing an in-memory join. Something like this: List&lt;Thing&gt; things = context.Things.AsNoTracking().Where(VERY_LARGE_WHERE_CLAUSE) ~~.Include(t =&gt; t.CollectionA.Select(ca =&gt; ca.CollectionB.Select(cb =&gt; cb.SomeObj))~~ .Include(t =&gt; t.OtherObj.AnotherObj.EtcObj) .Include(t =&gt; t.OtherObj.YetAnotherObj) .OrderBy(t =&gt; t.Id) .Skip(currentIndex).Take(batchSize) .ToList(); var thingIds = things.Select(t =&gt; t.ID); List&lt;CollectionA&gt; collectionAs = context.CollectionAs.Where(c =&gt; thingIds.Contains(c.ThingId)).ToList(); var collectionAIds = collectionAs.Select(c =&gt; c.Id); List&lt;CollectionB&gt; collectionBs = context.CollectionBs.Where(c =&gt; collectionAIds.Contains(c.CollectionAId)).ToList(); var collectionBIds = collectionBs.Select(c =&gt; c.Id); List&lt;SomeObj&gt; someObjs = context.SomeObjs.Where(s =&gt; collectionBIds.Contains(s.CollectionBId)).ToList(); // Some more optimized version of the below foreach(var collectionB in collectionBs) { collectionB.SomeObj = someObjs.FirstOrDefault(s =&gt; s.CollectionBId == collectionB.Id); } foreach(var collectionA in collectionAs) { collectionB.CollectionA = collectionAs.FirstOrDefault(s =&gt; s.CollectionAId == collectionA.Id); } foreach(var thing in things) { thing.CollectionA = collectionAs.FirstOrDefault(s =&gt; s.ThingId == thing.Id); } This is going to seem crazy and counter-intuitive but please try it.
I would really like to see .net core identity best practices tutorial (with the possible addition of IdentityServer4). I'm talking a tutorial that uses both MVC and API and implements auth for both. If it were my preference, it would be JWT from the API that is also validated in the MVC app.
Absolutely recommend pluralsight for .NET . They have trial and you can also get free 3 month licence from https://www.visualstudio.com/vso/
https://gist.github.com/shirhatti/c8246ac51e9f293b157e
yep. Found microsoft virtual academy and a Lynda course and a EdX course and several books recommendations. I hope someone can give me some hints which are good and have exercises and which are not good
https://docs.microsoft.com/en-us/aspnet/core/publishing/linuxproduction
There should be tons of tuts out there for this. Do you know anything about MVC? I'm not trying to be rude, just want to gauge your knowledge as you said you are new. Basically, all you need to do is make a controller action that returns the new data. Then in JS you call that endpoint to get the updated data. You can get more complicated and set up pushes from the server if you like or just use plain old polling. If you have any more questions just ask. I have some seed projects on github that I could show you as an example also.
[ Microsoft virtual academy](http://microsoftvirtualacademy.com)
just to give you some pointers, I am also a noob and did something like this a month ago. I got my table through ajax, based on input value and returned it as a partial view which updates the div. I basically wrapped the entire table(the result) with a Ajax.BeginForm that called my controller for the update and whatever business validation necessary and my controller returns a partial view(with the updated values). If you want to go the razor route(i found js a little daunting though slowly getting the hang of it), I suggest looking at ajax.begin form examples, razor table examples(if you scaffold a model you can get re-usable code, the values you want to change become HTML.Editorfor etc) and then merge both. might not be good design, but so far it seems to be working fine.
Thanks I'll look at the ajax.begin and razor table examples. 
I pretty much only know the basics. I was able to go through the code first example at asp.net and then create my own project from scratch. The current project I'm working is the first time I'm trying to get ajax to work with asp.net. The examples I have been seeing are for a single record pulled from the database. I'm currently pulling all my records from the database and then creating a table by looping through the records and stuffing them into rows. I'm just not sure if I should be creating multiple forms Like a begin form { ... } for each record and then some type of onchange javascript to submit the form in the background using ajax. I suspect asp.net might have some magical tableview or something that is designed to work with ajax but I really don't know. At this point I'm thinking I might have to just try to get a single tutorial working that allows me to work with one record at a time. Then I'll have to see how to have multiple forms on one page. If I can do those two things, I'll have to try and figure out how to merge the two.
&gt; kendo grid looks cool, I'll check into it. Thanks 
It's outdated
You could use a watchdog instead. I can recommend [supervisord](http://supervisord.org/). Supervisord gets started on init and keeps your servers up.
Interesting write-up, I'd not heard of some of those tools. I've built something similar to DbUp for internal projects that does a few extra bits and pieces. I've never been keen too on automated tools that do too much magic under the covers and sometimes it is really important to have control over the flow of updates and data transformations. Sql scripts aren't that difficult to write and it seems needless to abstract them unless you need to target multiple db types.
Do you specifically want Entity Framework or could another ORM suffice? For example, you could use LINQ-to-SQL to do exactly what you want but the technology is older than EF. https://weblogs.asp.net/scottgu/using-linq-to-sql-part-1
I was under the impression that EF used Linq. I'll look through the site you linked to and see if there is any ajax related info there. Thanks.
Most likely your file path is incorrect on the azure service/cloud whatever of azure.
How are you deploying to Azure? Can you confirm the static files are indeed being deployed? I've had trouble in the past with the default settings not publishing files outside of wwwroot when using the Visual Studio publish feature.
My advice would be not to bother programming a workflow engine from scratch and using any free or commercial one out there. When it comes to your requirements, I think https://workflowengine.io/ has everything you need.
Look into IntercoolerJS. It matches beautifully with ASP.NET MVC
I make it a point to stay as far away from PowerShell as I can. Having said that… Have you tried the PowerShell ISE? It should be on your computer already. https://msdn.microsoft.com/en-us/powershell/scripting/core-powershell/ise/how-to-debug-scripts-in-windows-powershell-ise
Thanks for the info. Sounds like EF but without the horribleness. Still not a huge fan of the "scripts only" mentality. I actually think if I was setting up a new project I would use Fluent Migrator for one-off scripts and SSDT for schema. Sounds like a good combination.
Yes, but it does not provide any useful debug tools in this case, so far as I can tell.
Pure guess, try calling system diagnostics debug. Break () in a PowerShell script? Then load up web client symbols/reference source to set a breakpoint? I have no idea if this is possible..
I'm skimming the article and it has sections on setting breakpoints and managing debug sessions. What is it you need that this is not providing?
@jakery2 mentioned the powershell ise which has debug capabilities. Visual studio also has a powershell tool extension that may provide more debug info: https://marketplace.visualstudio.com/items?itemName=AdamRDriscoll.PowerShellToolsforVisualStudio2015 Vs community is free and supports extensions like this
&gt; I make it a point to stay as far away from PowerShell as I can. Why?
But that is another program. The calls to Thread.MemoryBarrier(); in the original program are unnecessary, unless the method is required to be thread safe.
This is what I was thinking that I'd want/need to do I just don't know how to do it. I was considering getting the git source for PowerShell and trying to load it into VS and step though it that way but I think I'm in over my head.
I've had some luck with DotPeek and debugging framework code. There are a few decompilers on the market if you need to look at the underlying source.
I've been working on my interface all day! I've had the most success with ajax.beginform. I basically have each row of my table it's own form. I have a submit button for each row. If I modify data then click submit or if I change a field and hit enter key my database gets updated! Now I just need to figure out how to submit the form using javascript and onchange event or somethinglike that. My initial tries were not successful but it was pretty much the end of my day so I had to call it quits. I'll be back at it tomorrow. Thanks! 
None of the PowerShell tools/utilities in VSC seem to work for me. Like the debugging engine crashes every launch and gives a very generic error message (on mobile currently or I'd provide it). Did you encounter these issues by chance? I'm up to date with the most recent version of VSC, too.
I've put a global.json file into my project and used RC3 and RC4 explicitly, I'm not understanding why it still requires this god damn expired project.json file. Its been 45 days since I started my journey. I was happily doing the tutorials on v1.0 and once I knew enough I took the plunge into 1.1 and its been hell the whole way through.
THE DEED HAS BEEN DONE. THANK THE ALMIGHTY UNIVERSE AND ALL ITS WONDERFUL CODE.. 7 DAYS OF STRUGGLING.. all gone.. I've done it guys.. IVE DONE IT. I've defeated ASP.NET CORE in all its glory. From development to staging to production.. to learning the ins and outs of ASP.NET Core with DOCKER and all its BS relating to json and csproj. .. WE"VE ALL BEENTHERE.. when you get that 'WOW' everything makes sense moment.. its my turn.. 7 whole god damn days of testing.. THAnks Baddie for helping out. Though the link you provided added 0 value to me, it was your sentence regarding RC4 that saved the day. I gave up on running .NET Core through Docker with the latest dotnet version. I scavenged Docker Repo for all the builds and went through atleast 20+ containers and I hit 1 jackpot. So ya, im happy. BTW i hate this transitional period of unorganized chaoness from 1.0 to 1.1 as I learn ASP.NET Core, things are moving in a pace that is wonderful to exerpieced devs but for newbies like myself its a hellhole. Its been my 45th day and I was pretty close to giving up but here i am.. HERE I AMM
Oops, forgot to provide the solution: If anyone running on ASP.NET Core 1.1 and .NET Core 1.1 via Visual Studio 2017 and want to deploy it to a linux distro, I recommend not installing the one listed in the [Microsoft .NET Core installation](https://www.microsoft.com/net/core/#linuxubuntu) page, because the one they have there is using a non MSBUILD version (even though it says .NET Core 1.1 MSLEADING, get it?.. maybe not) Instead I recommend just launching a Linux server with Docker. Use Docker and force it to run any .NET Core 1.1 with MSBUILD, in my case (as of the time of this writing) the "DOTNET_SDK_VERSION 1.0.0-rc4-004771" was my saving grace. This post, this thread.. won't matter in a couple of weeks since project.json is being phased out like yesterdays newspaper, but it matters to me because this bs has plagued me for a week. So Id like to share my agony, pain and **solution** with you all even if you dont care!
Good job, take the rest of the week off.
[removed]
Interesting. What kind of snags can I hit if I manage JS-libs with NuGet?
Aah, I see - thanks!!
The most interesting is the *[ThreadStatic]* one. I understand WHY it's only initialized once, but given the usage of a ThreadStatic variable, it would make sense to have some buildin support for initializing once pr thread.
I'm not trying to sound cocky, but that's not the case where I am. From start, companies ask you what exact position you would like to work on, since they don't want to force anyone doing the job he doesn't like.
I'm not doubting what you saying, but only reason why I would prefer mvc5 over core is because almost all companies in my city use mvc5 without planing to shich to core in near future. I could learn it for myself later, but since I'm trying to get internship, I would prefer mvc5 for now.
Wonderful tool !
I wrote a blog post on this [here](http://coderscoffeehouse.com/tech/2016/08/19/real-world-aspnetcore-linux-example.html).
[removed]
Only problem I ever had attaching database is the permissions on the folder. Go to Services -&gt; "YourSqlExpress" and find what account it use to connect. If you're lazy change the account to your admin account on your pc and just give access right to that account on the folder and the database (Do not do that on a server).
Only some people enjoy the kind of churn that has been happening. My experience told to me to wait until everything settles down... With any luck .Net Standard 2 will be settled enough :)
I continue to await support for Visual Studio 2017.
That makes sense, but just know that they are literally the exact same outside of the configuration.
Yep, looks like you can set it up as a debug symbol server http://confluence.jetbrains.com/plugins/servlet/mobile#content/view/53336814
Yes. I have two asp.net core applications - one simply to host angular client app, another is Rest Web Api.
It depends on the level of security you need. Machine-to-machine authentication can be tricky since you can't have any ui interaction. A flow you could look at is: C (client) asks S (server) for its public key S returns its public key C uses public key to encrypt an API Key previously exchanged and sends it to S S decrypts received payload with its private key and validates received API key If the api key is valid, it generates an JWT and returns it to C C signs its requests with BEARER {received JWT} S validates incoming token against issuer, expire time If the token is valid, the request is forwarded to the proper controller. I recently implemented this flow at my company. The server can wrap everything in a middleware (asp.net core) or an httpmodule (asp.net). The client can wrap the whole exchange with a HttpDelegatingHandler ad-hoc, to be passed to the constructor of your HttpClient.
I wouldn't think I need this level of security. But at the same time, I don't want to half ass a solution and leave any predecessors up shit creek like I've experienced with other projects. Would /u/SikhGamer's suggestion be a viable solution? Or do I need to implement a system like you've suggested? I'm going to implement token based user authentication as well. But just want to ensure that only the apps that I've given access to, can access the API.
systemd is from 15.10 onwards. I am running 14.04 hence why supervisor is needed. It's becoming a real pain though having to restart server all the time
You can use thinktecture identityserver to implement oauth authentication for your web api server.
Resharper always seems to score favourably in lists like this but I always disliked it. It just seemed to be a bit of a resource hog and the IDE always seemed laggy when it was enabled. I'd rather do without it if it means VS is snappier to use.
Really? That's very unusual for Linux. I stopped using Core and just went with MVC5 under Mono, I just found Core to be a moving target that needs time to settle down.
FYI: The MAX_PATH issue was fixed in Windows 10 (about time!) either in the anniversary update or an insider build shortly after.
Well, for me it's not about migrating to Windows, but with staying on it as a dev platform. I'm less likely to drop Win10 because it's a lot easier to develop in a language I really like (C#) while using linux commands to deploy and otherwise deal with my servers. I'm currently trying out Rider, but it's still not release quality so I'd still recommend VS for production work for .net coding.
Totally agree 100%! I think that's the big bet MS is making when investing in WSL and I think it's brilliant.
Not really, I rather use Powershell and native Windows applications.
WSL isn't about bringing .NET apps from Linux to Windows though? How many .NET apps run on Linux but not Windows? For .NET devs, WSL means access to full tooling ecosystem, common dev/build environment (also helped by PS on Linux), and easier Windows to Linux path for .NET, not the other way around
Would be my choice. Easy to manage, existing middleware for ASP.NET Core and you don't have to mind about securing your token server.
&gt;WSL isn't about bringing .NET apps from Linux to Windows though? How many .NET apps run on Linux but not Windows? I agree, that's why I think the headline is kinda funny to see here. :) &gt;For .NET devs, WSL means access to full tooling ecosystem, common dev/build environment (also helped by PS on Linux), and easier Windows to Linux path for .NET, not the other way around I agree x2 :)
You make a good point. I ditched Windows a couple of years ago because I got frustrated whenever I stepped outside the MS ecosystem - things that ran smoothly on Linux and OSX just didn't cut it on Windows. If WSL was available I might not have invested in a Mac.
You can use claims for that. Just add a claim for each role to the identity. Edit: or a single claim with list of roles. 
A nice idea, but aren't HTML Helpers being removed from MVC? I noted recently that they aren't present in .NET Core, and presume they'll disappear from the main .NET releases before too long
A nice writeup - thanks. It's nice to see how much the release process has improved. I'm considering moving my .NET Core projects to Linux hosting purely on cost Developing on Linux is still a long way behind, though - Visual Studio code is nice for quick edits or small utilities, but just can't keep up with development of a major project
I wonder how the performance compares against PHP 7. Also: conditional classes, what the hell php?
Per application, I agree with /u/SikhGamer's comment above Per user, once you have the API session you can just authenticate using the API: pass in the username and password (or whatever you're using) and authenticate that and return an authorisation. My typical API request includes all 3 of organisation/application key, application session, and user session: a valid combination of all three is required for every request (other than user authentication requests, which require organisation/application key, application session, and user authentication details). That makes it much easier to revoke an organisation or application, an instance of an application, or an individual user, as required. Whether you need this many levels depends on your own setup: if it's an internal API, you probably don't need all three even if being used by a third party
Wordpress is one of the reasons why PHP get's so much hate. It's a horrible hack.
Fair point about the objects. 
As stupid as I think Peachpie is, the benchmark results are super impressive.
Maybe I'm being daft, but where are the benchmarks?
If I can still use all existing Wordpress plugins + develop custom stuff using C# and Visual Studio (or Code) than yay! I'll have to give this a whirl.
This is my question as well.
&gt; the reasons why PHP get's so much hate. It's a horrible hack. It's not really a "hack". It just drags around a lot of legacy code and Automattic refuse to break backwards compatibility.
In the header, under "[benchmarks](http://www.peachpie.io/benchmarks)".
Thanks, I didn't see them either but I was on mobile.
Ah thanks. On mobile so it was a collapsed menu
http://i3.kym-cdn.com/photos/images/newsfeed/000/976/824/913.gif
There are no reasons why it's desirable to run wordpress...
Identity Server would be perfect for an SSO type solution (one identity + many apps). Its JWT tokens can span multiple application, and as /u/fakir420 accurately says you can manage roles via claims. *it's
Isn't a WP speed test, but tried the TechEmpower plaintext benchmark using Peachpie: Requests/sec: 305,612.35 https://github.com/benaadams/PeachpieBenchmarks
Do you prefer DotNetNuke?
OP here! I work for PhishMe and I am looking for a world class developer with experience in C#, .NET, REST/SOAP APIs. Do you want to build software that's distributed to MILLIONS of endpoints around the globe? Come join my team! We need someone that is highly skilled with experience in a Agile/Scrum/Kanban! They can work remotely for an amazing company! Plus we have unlimited time off, 401K, extremely competitive pay, plus annual trips! 
[removed]
Not saying they should. It would be a massive undertaking to update the architecture, and create a migration path for existing developers to the new platform. 
I run an installation from 2010 that begs to differ. It cannot be upgraded easily.
I thought php was the reason WordPress got so much hate. 
First impression: The .org site looks nice and modern. &gt; Over the past 10 years the platform has evolved Sounds like a legacy project to me with possible tech debt build up over 10 years. &gt; Cofoundry uses a code-first development approach which allows us to optimize both the developer and content editing experience Sounds to me its optimized for developers... (I like that, but my users sometimes wont) &gt; Sponsored by "uber" its not clear that its not the other uber. I still might use it. I'm managing another cms which is php based, but that client has the option to a lot of azure credit. Inline content editing is a killer feature I found with clients. I see that .net core is on the road map. Linux hosting will be easier if some other db options are supported as well.
OP and lead developer on Cofoundry here, happy to answer questions :-)
Thanks for your comments. The platform has actually been rewritten a couple of times over that 10 year history and in the process of open sourcing it we've been keeping an eye on what's going on with .net core and intend to move there soon. We very much want this to be a platform built for the future rather than the past, but everything we've learnt in that time has shaped our vision for how Cofoundry should work. Yes we get confused all the time with that taxi company, but we felt it was important to show that this is more than pet project and has some backing to push to forward as a sustainable project.
Downloaded and will be trying it out soon. Looks sleek and it seems like it may be possible to build simple business web apps. @Jim, do you recommend trying to use it build a typical business application? How well will a DDD approach slot into it? Or is it meant to be more of a Wordpress/Laravel competitor?
The article does point to [Using OpenSSL to Create Certificates](http://www.blinkingcaret.com/2017/02/01/using-openssl-to-create-certificates/) which explains in detail how you can generate your own certificates and how from a certificate and private key you can generate a .pfx file using OpenSSL. It's in the two paragraphs above the one you quoted.
Well *typical business application* covers a lot of ground, but yes I definitely think it's worth evaluating it for that purpose. The key point is that Cofoundry functionality is there if you need it, but if you need to use a strong DDD approach and include a lot of custom data access then there's nothing to get in the way of you doing that. We don't see ourselves as a Wordpress competitor in that we don't have an easily composable GUI system for creating a site with templates and such, instead we're a bit lower level than that and enable much more freedom to build the site exactly how you want.
[removed]
I already like it better than WordPress! Full disclosure: I hate WordPress
I don't think that is possible. In Kestrel you can't even have rules for serving requests for a particular domain: "When you configure Kestrel to listen on a port, it handles all traffic for that port regardless of host header." https://docs.microsoft.com/en-us/aspnet/core/fundamentals/servers/kestrel However, you can do that with Nginx or IIS working as reverse proxys to your web app 
Beware. We tried this in production for about a week, and suddenly it failed very hard. It's pretty difficult to debug, since there's really not much information. In the end, we just had to remove it. Not only did we remove it from use, we are reconsidering using the feature all together. Not sure if you're using the script from CodePlex, but we also found the following: https://github.com/Microsoft/sql-server-samples/tree/master/samples/applications/aspnet-session-state They seem to be updated samples (one with retry), and are different than the outdated codeplex (2014) script.
1. Did you eventually use the regular SQLServer provider (the one you use aspnet_regsql.exe to create the schema for) or something else for session state management? 2. If something else, what? TELLLLLLLL MEEEEEEEEEEEEEEEEEEEEEE
If you want to dispute these motivations then take them up with the article author, as that's where they're from.
Thanks for the feedback so far. I'm tech director over at Uber Digital (not the taxi guys ha)... the point about 10 years was just to stress that Cofoundry is born out of real world experiences with brands large and small... it's been bumped and knocked around through corporate IT teams and many of the developers that have worked with us at Uber... all the learning from this has gone into what we hope you guys will love to use in the future :)
&gt;Using ALL existing plugins right this second? Probably not. My point was just that it will be one of the requirements to make this something more than a cool curiosity. Speaking of which, are you guys going to refactor some Wordpress mechanisms to make this worl? For example the plugin loading on Worpress looks for PHP files and executes them, whereas is all pluging are now compiled it seems you'd need a mechanism to dynamically load them from the libraries.
Thanks.A little busy these days. It looks like something worth keeping an eye on. 
Great article!
I switched from a fairly standard php cms to a php cms that was technically more sound. But my users werent happy and couldnt manage the site how they wanted it. So if I see a cms with nice features for developers I'm afraid that my users wont be serviced as much as I would like. But that problem is described on their site: &gt; CMS administration interfaces are often built to serve both developers and content editors which leads them to be complicated and full of jargon. So I will try it out
Your method ended up being what I went with. For some reason though, even with the collections/properties already manually set, it was still trying to lazily-load them in from the context. I had to disable lazy loading entirely for the duration of the process to keep that from happening. Still, with this in place and a lower page size of ~2000, things are much faster now! Thank you!
After taking a look, I definitely like it better than SSMS's offering!
Well... I'm told I'm no longer young at 30, but I regularly use .Net (Core at the moment) and have done so almost since the beginning (when I was younger, but maybe not hip). I've used it for big projects and small projects, in corporate settings and just my hobby stuff. I don't agree that it's awkward setting up for small projects - I find MVC eminent for the task - and I think that it's affinity for enterprise only makes it viable in the long run as well. I don't know the scene in NYC, but from a technology and career perspective, I'd say go for it.
&gt; awkward and opinionated Create, Read, Update and Delete... what else do you want to do with data?
Thanks for your perspective. Why do you think its so unpopular among startups then? The thing that's giving me a headache with it right now is the user account/authentication system. The default individual user account template gives me a hundred things I don't need and that I'm having trouble untangling, but I'm thinking that building the pieces I want from scratch isn't the "ASP.NET way".
&gt; unpopular among startups Because up until recently it's been Windows-only for the most part (Mono is technically awesome, but not fun to use) and startups don't want to pay the "Microsoft tax". StackOverflow is a notable exception, which is heavily based on .Net and the MS stack.
Cofoundry can certainly plug into an existing application, I think one potential issue might be if you already have user accounts set up for your application and needed to integrate with our auth. It's doable but it would depend on what you have already. In terms of content editing I'd recommend checking out our [sample projects](https://github.com/cofoundry-cms/cofoundry/wiki/Sample-Projects) which is a really quick way to get a feel for how templates and content editing work. It's also worth mentioning that you can also inject in your own custom admin modules if you need to get really custom, but unfortunately we don't have a sample or documentation for that just yet, but it's coming!
That would be a good one to cross post to /r/linux 
Hmm null pointer exceptions aren't good! If a lot of data comes in it can indeed be choking on it. In normal circumstances I can't get it to keel over, only when I record a lot of data. I think you don't have it anymore, but in case you do: do you have a stacktrace for me? TIA! 
*compiler
You will absolutely be able to use existing WP plugins. We haven't modified this mechanism yet (obviously), but we can do it very cleanly without changing the WP code by passing on a virtual PHP file, while the rest can be in C#, for example. 
&gt; and startups don't want to pay the "Microsoft tax" Actually it's Bizspark, we get it in 2012. Ang you get everything from MS for free and additional resources in Azure for 3 years. So it's great. And if you do not have mony to pay for this products in 3 year... you know what does it means.
To be fair, startups tend to like things like Linux, Ruby, Python, PHP, MySQL, and PostgreSQL because they cost $0 in licenses. Your typical ASP.NET deployment uses Windows Server and MSSQL, neither of which are typically free. This is why .NET Core is a huge step forward. While Mono had somewhat spotty support for newer .NET versions, .NET Core works with C# 6 (soon to be 7) right out of the box. I've heard the PostgreSQL driver now has excellent Entity Framework Core support too.
I assume you mean JWT. Check out https://github.com/openiddict
[removed]
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/linux] [Resources for learning about securing a Linux VM(.Net Core hosting)(x-post from \/r\/dotnet)](https://np.reddit.com/r/linux/comments/5x0cve/resources_for_learning_about_securing_a_linux/) [](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))* [](#bot)
STIGs are a good resource to use. 
Just wanted to say thank you for putting the time and effort into contributing to open source.
How do you think Microsoft are doing on patching .NET and .NET Core? Can they can do better? Take the official survey to provide feedback to the team. (From Barry Dorrans https://twitter.com/blowdart/status/837102992222371841)
I want to hop in here and reiterate what he said and hoping to get more details; I would be very excited if I can stop managing my own (crappy) CMS project for my sites and have one thats shared by the community. Unfortunately every .NET CMS takes way too much work to maintain, and takes over the application. If I can reiterate what he asked and hope for more details... I like /u/Sebazzz91 just want an interface to store HTML, edit said HTML, and an easy API to get it out. I would like to manage my own pages and just use a razor tag for it to render in the content as it was entered. You mentioned user accounts above, mine are all managed though Stormpath or Auth0, I have no trouble if CMS access is manage elsewhere. How would you go about integrating this in an existing application and only having those features? I understand why your sample projects and instructions cover more broad implementations, but I would be curious about this. 
Not sure what you consider cutting edge? Angular 2.x just came out. .NET Core is also brand new and is still going through major changes, so it is not something you want to launch your production system. Regarding pricing, you can setup solution for free and use it for free as well. Not sure where you got your facts, please tell me what eCommerce product out there you consider more cutting edge besides starter kits.
Ok sure, well there's two parts to this. Let's address content editing first: If you don't want to use the built in page templating (which still uses razor) then you could instead create a [custom entity](https://github.com/cofoundry-cms/cofoundry/wiki/Custom-Entities) type called *Content* to manage content with a single Html property. You can use our data access to get hold of custom entity content in your controller, this can be done by the integer id but you'd probably want to use the *slug* string identifier in this case. Your auth scenario sounds similar to projects where we've had to integrate SSO. We don't have any plugin packages for this yet, but it's not too difficult to do auth yourself and then use our services dynamically create a Cofoundry user and then log them into the system. We'd be interested in making this a bit simpler in the future for well known providers, it's just that our use cases have always required integration with quite specific SSO systems. If this sounds like I'm on the right track I'd be happy to throw together a quick sample for you to try out.
How does it differ from Umbraco?
Best to put this kind of feedback in their github issues. That way you can actually have some influence
Apparently none of my formatting is showing :S
Ok, but for non-dev projects I dont want to manage an update trough visual studio, and have another repo for just hosting a site. I like the simplicity of php based cms's. You can just backup the db+data directories, and thats it. In a case of disaster recovery you only have to get the same version of the cms, copy it to the webserver, hook up the db and restore the data. In that case I only need an ftp client and access to my backups. I can do that any moment I have an internet connection. My Visual Studio is open for more than 10 hours a day (unless I'm on holiday, which is the time that stuff breaks...). It'll probably take the same amount of time for an update, but I just dont want to have those dependencies, especially if I don't have them now...
Reddit uses Markdown for its formatting. In Markdown, you use 4 spaces at the beginning of a line (block) or surround it with backticks (inline, same key as ~ on a US keyboard) for code. This is a code block `This is inline` code.
And that's why I don't use preview software.
I generally agree, but the issue I described is affecting the runtime (RTM) and not the SDK (actually in preview/rc).
I was thinking about it, but I wanted to get some feedback from the sub before opening the issue.
Today the prime medium of doing business is internet. Any businessman who wants to excel in his field must have a smartphone app or a website. Offshore ASP.NET Web Development allows your business to reach to your prospects throughout the world. You can take your business to next level at a minimal cost. 
Seems more of an aws problem to me...
It's been a while, but we released our first preview yesterday. The release is detailed on [our blog](https://www.cofoundry.org/blog/5/looking-for-feedback-on-cofoundry-a-new-net-cms). Core support is in the pipeline but we wanted to get a preview out to get some feedback on the concept first.
Heh until a while ago I actually felt that WebForms had their place, in "thick" internal apps that would have been desktop apps in the olden days (e.g. like a CRM or similar system). But it's so rare to find any webforms apps that are written in any sort of decent style (e.g. Model-View-Presenter or MVVM) versus those that just stuff 100 lines of code in an event handler or widget that it soured me on them entirely.
I prefer 2017 especially if you plan on doing anything with .Net core. The only real snags I've seen were minor enough to work around. (Stuff like Monogame templates missing from 2017 and Unity tools not being released yet for 2017, but since my bread winner is web dev while game dev is for funsies, not that big a deal)
Thanks! What do you do when you need to know what tag helpers are available and Intellisense won't give you any clues?
Yeah, looks like it changes from 2012. Additional 2c, it's not necessary to buy VS licence, it's possible to use up to 5 instances of VS Community Edition for development.
Yeah, I'll be surprised if they fix the tag helper Intellisense issue in a week.. Hope they do though!
1. Reverted back to SQL Session, yes. /sadface I was really really hoping that in-memory session state would have worked well, but it's really finicky and it took my site down. I don't have the resources to look much into why that was occurring, I'm already very very understaffed. 2. I might try using Redis Session Provider, seems to be the industry standard [ala Azure](https://github.com/Microsoft/azure-docs/blob/master/articles/redis-cache/cache-aspnet-session-state-provider.md)
You'll enjoy this: It turns out that the reason nothing appears in the state table unless cookieless=true in our app is because they're not even using ASP Session State, it's all stored in cookies (which reverts to storing state in ASP Session State if cookieless=true) This project has been blocked for like a month because the lead dev insisted that shared ASP Session State is necessary to make clustering our web app work properly, and I had no particular reason to doubt him. :D
For vital business stuff, it's often a good idea to lag a version behind unless you have a specific reason to upgrade. Especially if you use a lot of plugins and third party toolings. If nothing else, I like to leave it 6 months between versions: let the hobbyist teenagers deal with any teething trouble for me.
What is the class on your codebehind file? That is what you should be inheriting. You'll want to include the full namespace also. If the Codebehind file has this class: Namespace Custom.Pages Public Class MyMasterPage End Class End Namespace The markup should have: Inherits="Custom.Pages.MyMasterPage" Also one thing to think about is if yours is a web application or a web site project. In a webapp you want to use the "CodeBehind" attribute and in a web site project you want to use the "CodeFile" attribute. One final note. If you are not using a code behind page (your server code is in script tags on the markup page) then you do not need the CodeBehind/CodeFile and Inherits attributes in your page declaration at all. 
This is awesome, thank you! I would love a bare bones sample that could show this. I really appreciate the fast and in depth response. I may play around with it in the meantime while waiting on the sample.
No need to remove them from your model. Just include the columns you want in your select. The framework will generate the query and only pull back the columns you ask for.
If you target netcoreapp1.0 and have Net Standard Library 1.6.0 as a direct dependency on your Lambda then it should override the other libraries usage of 1.6.1. I do this for ASP.NET Core libraries and whatnot. It seems weird to me that AWS won't support major updates but that's their decision.
We're less than a week out until the final release, so you might as well wait until next Tuesday and install the full version.
I was not sure about "VS is not included", so I've reread bizspark faq. Actually "BizSpark startups receive five Visual Studio Enterprise with MSDN subscriptions, each with a $150 monthly Azure credit." So 5 developers will get access to almost ALL MS software including Office, and enough Azure credit. Also after 1 year this developers can use almost all installed software and also " If your Startup has servers in production, your Startup can get licenses to continue to use up to 4 Windows Servers (Standard Edition) and 2 SQL Servers (Standard Edition)."
You likely can't use EF Core 1.1 either. Have to use 1.0 in that case.
Don't pick other.. &gt; Based on your responses, it seems you don't match the profile we are currently looking to learn more about. Thank you very much for your interest in reaching out to us. Doesn't mads already have a plugin for this? https://github.com/madskristensen/PackageSecurity 
Add this nuget feed address to your visual studio to allow it to restore Episerver's nuget packages: http://nuget.episerver.com/feed/packages.svc/ Episerver (nowdays) releases everything through their nuget feed so it's quite developer friendly. If the project is set up correctly it should compile after all packages are restored. After that just update the connection string to make it point to your database copy, set the site up in IIS and try it out. 
Whoever commented may be shadow banned 
Get Resharper. You can thank me later.
30 projects is pretty insane for a web app. at that level you would think theres be web services involved to abstract layers and keep the architecture into manageable systems. since youre new to mvc, i implore you to not consider much of the code you see as standard practice in mvc. a solution that big would indicate several devs' involvement, half of which likely not following mvc standards (with mvvm, solid principles, tdd, etc). take everything you see with a grain of salt! if it were me, i would start by mapping out the projects, inputs and outputs, and dependancies.
Pretty cool.
I think he's talking about using resharper for the code navigation primarily
perhaps, it does add some extra MVC nav features.
The new .NET Core versioning works like this: 1. Libraries are implemented in as low a version as possible 2. Applications choose as high a version as they can for maximum compatibility with the libraries they use. 3. Platforms implement as high a version as possible so that they're maximally compatible with applications that use them. Amazon is breaking this contract by providing a platform with a lower-than-current version. 
4\. ???? and then what (;
If you are new to ASP.NET MVC, I would recommend this book: http://www.apress.com/us/book/9781430265290
/u/abeerantcheese isnt referring to migrations. he's referring to columns that require data for other integrating systems to work, but are ommitted from the dbmodel, so they are left as nulls or defaults during row inserts.
Cause they support 1.0 and no further, while your app requires 1.1. I dont like analogies, but your way of continuing it makes no sense. 1.0 =&gt; 1.1 is a minor change, not a patch. So thats like the iphone 7 is saying I need apple car 1.1, while your car has 1.0. They try to connect and both fail. Just like your app on aws.
Take a look at HMAC authentication: http://bitoftech.net/2014/12/15/secure-asp-net-web-api-using-api-key-authentication-hmac-authentication/
Resharper is also very useful if you like a slow and unstable environment. Just my experience, so please feel free to give it a try, but the moment your IDE starts freezing, crashing or responding slowly, the first step is to disable reshaper. Plus many of the most used features of it have been added to VS over the years.
Check out: http://docs.identityserver.io/en/release/index.html https://github.com/IdentityServer/IdentityServer4.Samples
With the state of things it's more likely that a lot of stuff is still broken. Especially regarding .NET Core.
Ah. So instead of adding continuous parameters like `/api/asset/id/name/value`, you just add query strings? That makes sense to me. Is that pretty standard in web API development? I haven't worked with them much and just want to make sure I'm not going to make the next guy after me mad for doing it like that or whatever.
check out odata it standardizes a query language to use for these types of concerns. Even if you don't use odata complaint endpoints, you might get some good ideas 
Ah, I finally see some of the faces behind the tool I spend most of my day waiting on. (I guess by rights we need some Jetbrains faces in there too)
Thanks for this. Simple examples that get the point across quickly without having to read a blog post.
A really useful post that points out the excellent [securityheaders.io](http://securityheaders.io/), which is particularly handy for pointing out flaws and grading sites.
I don't know if it's standard or not but it makes it very flexible for the Api caller to apply as many or as few filters as they would like. As long as you keep that in mind it will help mold how you add me parameters
are you able to use json?
Thank you all.
*Skims but doesn't read the article* Huh, I can't believe only four people worked on VS. Neat.
Have you tried dapper? I enjoy working with it.
NopCommerce Umbraco IdentityServer Serenity 
I haven't had any issues with nested objects when using ADO.NET. At some point you have to translate your OOP domain objects to a relational domain, there's no way around it. I find that ORMs simply mask this translation with a leaky abstraction, and in the process degrade performance and make the code far more complex.
Yeaps, did come across that combo. Not sure if it can handle high volume transactions, will give that a go. thanks for reply !
nopCommerce tops the list. Im also planning to get in touch with Kentico for discount. Their pricing is insanely high even for small businesses. Cheers
Reading Wikipedia it says it's derived from Debian so use that release [here](https://www.microsoft.com/net/core#linuxdebian) and definitely not the one for Red Hat!
Always good to hear from the experienced. Content isn't a lot to administer, site revolves around ecommerce with minimal content like banner, forms, organization info. Content doesn't get added or edited that often either. Nopcommerce appears to be right candidate, but keeping options open to explore others. Client also wants to have ecommerce integrated into their own mobile app for iOS and Android. Can this be achieved using Nopcommerce ? As a solo dev, I'm looking at pair programming on this project, shoot me a DM with your portfolio with tech stack they're built on. Cheers ! 
Running into nopCommerce a lot, will give it a go. Thanks for recommendation. P.S: umbraco is confusing af to get started and ecommerce options are limited. Cheers ! 
I'd say it's pretty standard (and OData takes that kind of thing and runs with it). If it helps, think of it like the `/asset/id` format is a way to *identify a resource* while with the parameters you're pretty much creating a basic, structured search engine. And if you're thinking from a REST perspective, search engines aren't RESTful so don't feel held back :)
There are plenty of cases where EF is a good tool, but it's knowing when where and how to implement it that is important. Not every business application is so mission critical that shaving off every millisecond possible is necessary, although as devs there is that part of us that wants things done the best possible way, it's important to think big picture. EF provides a degree of simplicity when it comes to managing object models and handling of the actual objects that we otherwise have to spend a lot of time and code on. When you're working on business apps that have a myriad of different db objects that need to be worked with and you need to turn around new features quickly and uniformly, hooking a ORM over the top can be the best solution, provided you're working with a well design data layer etc to begin with. That said, as soon as performance becomes a part of the discussion, EF gets dropped, because that's not what it's for, but to say it serves no purpose is a niave outlook at the real world needs of software development. 
I also recommend this combo. Last year I built an [eCommerce](http://www.leftyguitars.be) with Umbraco &amp; Merchello and it was a great experience because both technologies are very developer friendly (you can build it almost exactly the way you want). After launching it, the site had little to no problems.
Why 'dapper' specifically? Considering there are other microORMs with same/more features which are faster... (edit) ah, reddit. Downmodded because you state something against what some group believe. 
If you're writing code using ADO.NET objects, you're actually rebuilding a layer which is already included in an ORM. All the overhead of entity management, change tracking, SQL generation, resultset materialization etc. etc. etc., you have to write that by hand. The thing is that you have to do that without errors _and_ have to maintain it too, _and_ as it costs time, your project will be done later. You also have to make sure you avoid every pitfall there is in this area. I can assure you, unless you have done all that before and have spent a couple of years in this field, you'll fail and will write a layer around ADO.NET that is outperformed by the average (micro)ORM, and will contain more bugs and will make your project be delived later as you have to spend time on what's already written by others, better. Don't think because you are a star programmer you are better than all of us ORM devs, and you won't make the same mistakes every ORM dev will make eventually: you will make the same mistakes, simply because they're often non-obvious. 
How is it you need high volume transactions but can't afford Kentico? Not trying to be a douche, but I'm genuinely curious.
I think XLSIO by Syncfusion is pretty robust : https://www.syncfusion.com/products/file-formats/xlsio But I feel like the price is a little too high for what it is. 
Not sure if you want to roll your own solution but a .xls file is just a zipped up .xml file with some metadata files. 
I'm curious about what type of issues you've had with EPPlus. I use it quite frequently as well, but haven't experienced any issues.
ClosedXML. It's just a nice wrapper for the OpenXML framework. https://www.nuget.org/packages/ClosedXML/
[removed]
Cheers for that - will check it out...
Even using OpenXML, Excel files are a bitch to work with in dotnet. I can't imagine trying to roll my own solution.
If you have Excel installed on the target machine - you can use [COM interop](https://msdn.microsoft.com/ru-ru/library/ms173186%28v=vs.80%29.aspx). Otherwise I use ClosedXML in production and am pretty happy with it. 
Not OP, but I've had adding a column or row mess up all the formulas that i had added to the sheet.
I can safely warn you off of any Aspose solution. I've only personally bought and use the PDF generation library, but based on my experience there - be prepared for frustration. Their API is erratic, sporadic, odd, and clocks in around 2 WTFp/h (wtf's per hour). Maybe their Excel library is better, but I doubt it. Some pros are that they have code examples for most scenarios, but if you run into a problem not covered by a vague forum thread, you're going to have a bad time. Just my .02 cents on Aspose. May their servers be infested with the fleas of a thousand camels.
+1 for closedxml; simple to use
I actually built a class that would do all kinds of interop stuff for me in excel a few years back. The com references were a pain (and I recently found that Word seems to have none of these issues), but my solution was simply to name every object and release it at the end. THis means anytime you are using a . to reference a property, you name it first then you can release it. You never use two periods or you create a reference you can't release. It was a pain at first but once I got the hang of it it became really easy. The other upside was building things like pivot tables, being able to move around and easily insert values into cells without having to create all kinds of arrays and stuff. I agree that interop is the wrong tool for many jobs, but for really complex stuff, I found it to be very handy. The big win was being able to create a 20-30 tab spreadsheet, most of which had extremely complicated pivot tables, in a very short amount of time. These are reports that had previously taken one employee a full month to complete, each month. That was his whole job.
NPOI https://www.nuget.org/packages/NPOI/
I dont think its the same. The reverse poco generator uses a T4 template (more about t4: https://www.hanselman.com/blog/T4TextTemplateTransformationToolkitCodeGenerationBestKeptVisualStudioSecret.aspx) 
Nobody's mentioned GemBox. Fantastic paid library we use at work.
Umbraco And uCommerce
I've been using NPOI and it's been working great. The only thing I wish I could do is charts.
Some advice not related to your issue. - Try to avoid using includes, use joins when possible to prevent complex unions. Or profile your queries. - Use C# naming conventions on you DbSets. - Why ToArray? All the round trips... - There is no reason to use a view here. - Have you tried to use a profiler? 
^ this. Don't reinvent the wheel. In a lot of cases, our databases are already designed and built by database architects. We then use LLBLGenPro to reverse engineer these tables, views, and stored procedures into entities or typed views. We use EF for a lot of multi-entity updates (for transaction support and change tracking, etc.). For the SELECTs, we use Dapper to map a query result to the entities built by LLBLGenPro. Win Win.
What were the factors that led you to gembox over other solutions? 
Thanks!
Recently I had to write a generic API Controller which would wrap around our Generic Repository pattern and implement simple Get/Post/Delete web methods automatically (by using the Repository interface). When I implemented "Get", I implemented two methods: GetById which accepts an Id and GetWhere which accepts a lambda expression as a string. I then used [System.Linq.Dynamic](https://github.com/kahanu/System.Linq.Dynamic) to deserialize the string back into the lambda func. So now, our clients can make calls to the web service and get "assets" using intuitive lambda expressions, like x =&gt; x.DateCreated &gt; DateTime.Now.AddDays(-7) &amp;&amp; x.GroupId == 56 We found this has been very robust and flexible. 
&gt; Try to avoid using includes, use joins when possible to prevent complex unions. Includes are joins.
We had been using it since before I started, so I don't know. I can say is a pleasure to work with, though, and I've never had problems with their API or discoverability.
I'll provide some advice. I spent 2 years searching for a good installer builder solution for my team. We were happy when Visual Studio supported installer projects, but those are gone now. I've tried GUI programs to build installers, both free and paid, and many of them are extremely cumbersome to use, and often overly verbose. In all my years of searching, I found only one solution that was satisfactory: WiX. It may take a little time to get used to it, but it is well worth learning. VS 2015 Community supports extensions - just make sure you get Wix 3.10 installed. 
And if it's any help, here's an example file for the installer I use: &lt;?xml version="1.0" encoding="UTF-8"?&gt; &lt;!-- When the version number changes, regenerate the ProductCode. The UpgradeCode is constant between all versions - it's how we understand that all parts are from the same lineage, they all have the same UpgradeCode. --&gt; &lt;?define Version = "4.0.0" ?&gt; &lt;?define ProductCode = "{575B4D91-E26C-40EC-AA83-D0F798D00B8D}" ?&gt; &lt;Wix xmlns="http://schemas.microsoft.com/wix/2006/wi" xmlns:netfx="http://schemas.microsoft.com/wix/NetFxExtension" &gt; &lt;Product Name="SomeProgram" Id="$(var.ProductCode)" Language="1033" Version="$(var.Version)" Manufacturer="XYZ Corporation" UpgradeCode="11eab7a7-1efd-4eb0-a561-065e49422254"&gt; &lt;Package InstallerVersion="200" Compressed="yes" InstallScope="perMachine" /&gt; &lt;MajorUpgrade DowngradeErrorMessage="A newer version of [ProductName] is already installed." /&gt; &lt;MediaTemplate CompressionLevel="medium" EmbedCab="yes" /&gt; &lt;PropertyRef Id="NETFRAMEWORK45"/&gt; &lt;Condition Message="This application requires .NET Framework 4.6.1. Please install the .NET Framework then run this installer again."&gt; &lt;!-- For some reason, this little trick checks for dotnet 4.6.1 instead of dotnet 4.5. --&gt; &lt;![CDATA[Installed OR (NETFRAMEWORK45 &gt;= "#394254")]]&gt; &lt;/Condition&gt; &lt;Feature Id="F.Core" Title="SomeProgram Core" Level="1" Display="expand" ConfigurableDirectory="INSTALLFOLDER" &gt; &lt;ComponentRef Id="C.SomeProgramCore" /&gt; &lt;ComponentRef Id="C.SomeProgramCorePdb"/&gt; &lt;Feature Id="F.Gui" Title="SomeProgram Gui" Level="1" ConfigurableDirectory="INSTALLFOLDER" &gt; &lt;ComponentRef Id="C.SomeProgramGui"/&gt; &lt;ComponentRef Id="C.SomeProgramGuiPdb"/&gt; &lt;ComponentRef Id="C.ProgramMenu.GuiShortcut"/&gt; &lt;ComponentRef Id="C.Desktop.GuiShortcut"/&gt; &lt;/Feature&gt; &lt;/Feature&gt; &lt;!-- ====== User Interface ====== --&gt; &lt;!-- Removes the license agreement from Mondo See: http://www.howdoicode.net/2011/09/wix-how-to-hide-license-agreement.html --&gt; &lt;UI Id='Mondo'&gt; &lt;UIRef Id="WixUI_Mondo" /&gt; &lt;UIRef Id="WixUI_ErrorProgressText" /&gt; &lt;Publish Dialog="WelcomeDlg" Control="Next" Event="NewDialog" Value="SetupTypeDlg" Order="3"&gt;1&lt;/Publish&gt; &lt;Publish Dialog="SetupTypeDlg" Control="Back" Event="NewDialog" Value="WelcomeDlg" Order="3"&gt;1&lt;/Publish&gt; &lt;/UI&gt; &lt;Icon Id="I.AppIcon" SourceFile="$(var.SomeProgram.Gui.ProjectDir)\Resources\SomeProgram icon.ico" /&gt; &lt;Property Id='ARPPRODUCTICON'&gt;I.AppIcon&lt;/Property&gt; &lt;!-- Tell the UI what variable to look at to know where our install path is to be. --&gt; &lt;Property Id="WIXUI_INSTALLDIR" Value="INSTALLFOLDER" /&gt; &lt;/Product&gt; &lt;!-- Define our source and destination paths --&gt; &lt;Fragment&gt; &lt;!-- http://stackoverflow.com/questions/1641094 --&gt; &lt;Directory Id="TARGETDIR" Name="SourceDir"&gt; &lt;!-- Then we want to install everything into Program Files\XYZ Corp\SomeProgram --&gt; &lt;Directory Id="ProgramFilesFolder"&gt; &lt;Directory Id="CompanyFolder" Name="XYZ Corp" &gt; &lt;Directory Id="INSTALLFOLDER" Name="SomeProgram" /&gt; &lt;/Directory&gt; &lt;/Directory&gt; &lt;Directory Id="DesktopFolder" Name="Desktop" /&gt; &lt;Directory Id="ProgramMenuFolder" Name="Programs"&gt; &lt;Directory Id="ProgramMenuDir" Name="SomeProgram" /&gt; &lt;/Directory&gt; &lt;/Directory&gt; &lt;/Fragment&gt; &lt;!-- Define our components --&gt; &lt;Fragment&gt; &lt;!-- ====== Core Api ====== --&gt; &lt;Component Id="C.SomeProgramCore" Guid="BA2DD35F-39C3-4922-9611-AFCA54F28057" Directory="INSTALLFOLDER"&gt; &lt;File Id="Fi.SomeProgramCore" Source="$(var.SomeProgram.Core.TargetPath)" KeyPath="yes"&gt; &lt;netfx:NativeImage Id="NI.Fi.SomeProgramCore" Platform="all" Priority="0" AppBaseDirectory="INSTALLFOLDER"/&gt; &lt;/File&gt; &lt;File Id="FI.SomeProgramCoreDocs" Source="$(var.SomeProgram.Core.TargetDir)$(var.SomeProgram.Core.TargetName).xml" /&gt; &lt;/Component&gt; &lt;Component Id="C.SomeProgramCorePdb" Guid="{2F323A59-A44D-43C9-9502-562744D7C2C4}" Directory="INSTALLFOLDER"&gt; &lt;File Id="Fi.CorePDB" Source="$(var.SomeProgram.Core.TargetDir)$(var.SomeProgram.Core.TargetName).pdb" KeyPath="yes"/&gt; &lt;/Component&gt; &lt;!-- ====== Gui ====== --&gt; &lt;Component Id="C.SomeProgramGui" Guid="3316FC25-8E26-48F2-B01D-64640C2C6CD6" Directory="INSTALLFOLDER"&gt; &lt;File Id="Fi.SomeProgramGui" Source="$(var.SomeProgram.Gui.TargetPath)" KeyPath="yes"&gt; &lt;netfx:NativeImage Id="NI.Fi.SomeProgramGui" Platform="all" Priority="0" AppBaseDirectory="INSTALLFOLDER"/&gt; &lt;/File&gt; &lt;File Id ="Fi.AntiduhUi" Source="$(var.Antiduh.Ui.TargetPath)"&gt; &lt;netfx:NativeImage Id="NI.Fi.AntiduhUi" Platform="all" Priority="0" AppBaseDirectory="INSTALLFOLDER"/&gt; &lt;/File&gt; &lt;File Id="Fi.TreeViewAdv" Source="$(var.TreeViewAdv.TargetPath)"&gt; &lt;netfx:NativeImage Id="NI.Fi.TreeViewAdv" Platform="all" Priority="0" AppBaseDirectory="INSTALLFOLDER"/&gt; &lt;/File&gt; &lt;File Id="Fi.PsTaskDialog" Source="$(var.PSTaskDialog.TargetPath)"&gt; &lt;netfx:NativeImage Id="NI.Fi.PsTaskDialog" Platform="all" Priority="0" AppBaseDirectory="INSTALLFOLDER"/&gt; &lt;/File&gt; &lt;File Id="Fi.DockPanelSuite" Source="$(var.DockPanelSuite.TargetPath)"&gt; &lt;netfx:NativeImage Id="NI.Fi.DockPanelSuite" Platform="all" Priority="0" AppBaseDirectory="INSTALLFOLDER"/&gt; &lt;/File&gt; &lt;File Id="Fi.XPTable" Source="$(var.XPTable.TargetPath)"&gt; &lt;netfx:NativeImage Id="NI.Fi.XPTable" Platform="all" Priority="0" AppBaseDirectory="INSTALLFOLDER"/&gt; &lt;/File&gt; &lt;/Component&gt; &lt;Component Id="C.ProgramMenu.GuiShortcut" Guid="FF97312F-2D79-4DD2-A4BA-05553A321747" Directory="ProgramMenuDir"&gt; &lt;Shortcut Id="SC.ProgramMenu.Gui" Name="SomeProgram" Target="[#Fi.SomeProgramGui]"/&gt; &lt;RemoveFolder Id="ProgramMenuDir" On="uninstall" /&gt; &lt;RegistryValue Root="HKCU" Key="Software\[ProductName]" Name="ProgramMenu.GuiShortcut" Type="integer" Value="1" KeyPath="yes" /&gt; &lt;/Component&gt; &lt;Component Id="C.Desktop.GuiShortcut" Guid="{6BE4D3E7-FA7B-49F4-9CED-01409F1C1547}" Directory="DesktopFolder" &gt; &lt;Shortcut Id="Sh.Desktop.Gui" Name="SomeProgram" Target="[#Fi.SomeProgramGui]" /&gt; &lt;RegistryValue Root="HKCU" Key="Software\[ProductName]" Name="Desktop.GuiShortcut" Type="integer" Value="1" KeyPath="yes" /&gt; &lt;/Component&gt; &lt;Component Id="C.SomeProgramGuiPdb" Guid="{A9A2C31A-FF3B-443B-BF34-1BE5AC553D8F}" Directory="INSTALLFOLDER"&gt; &lt;File Id="Fi.GuiPdb" Source="$(var.SomeProgram.Gui.TargetDir)$(var.SomeProgram.Gui.TargetName).pdb" KeyPath="yes"/&gt; &lt;/Component&gt; &lt;/Fragment&gt; &lt;/Wix&gt; 
Really, I guess I haven't tried to do one in my new VS. I just upgraded like 6 months ago. 
I'd suggest not doing the zip file thing. Windows Installer and the database format that it uses may be a bit odd to get started with but it provides a lot of important install/repair/uninstall functionality. Learn WiX and create a [custom WiX bootstrapper](http://www.wrightfully.com/part-1-of-writing-your-own-net-based-installer-with-wix-overview/).
Reddit ;)
Yes, it can definitely be integrated. Either through direct calls to the database housing the information (which Nop does a decent job of segregating everything), or open up a few web services and consuming it that way. Fortunately NOP is open-source, so mimicking functionality in native apps should be pretty easy since you can see exactly what they're doing. I work at a marketing and advertising company, so everything I've worked on is fairly public. The two biggest NOP sites I've done were for [Huffy](https://www.huffybikes.com/) and [Vitro Sample Order](http://vitrosampleorder.dev.pipitonegroup.com/) (which is still in development). I've done a few Kentico-based sites, mostly properties for a company called PPG. And am working on a fairly complex client/internal site in Kentico for a company called NEP. The past few years I've done more CMS work than I thought. I am a .NET developer by trade, who has focused on full-stack dev for the most part. I've also been dabbling in mobile dev with Ionic/Angular/Cordova and am now starting with React. Does your client have an existing web or mobile app? And is the store going to house a lot of products? NopCommerce is definitely great for ecommerce, but it's got a pretty steep learning curve in the beginning (just because of how massive it is). 
One include is a join. Two includes may be a cross-join.
Titles cannot be altered on reddit.
Highly unlikely given that EF Core only supports a small subset of what EF can do in terms of SQL generation.
What is EF good far? If I'm doing rapid prototyping, I want an active record style library. If I'm loading sample data, bulk insert. If I want production quality and ease of use, Tortuga Chain. If I just was to call stored procs quickly, Dapper. 
Stored procs and multiple result sets. Aside from LLBLGen Pro, all of the 'object graph' style ORMs produce utter garbage for nested objects. You can easily end up cross joining five or six tables, resulting in tens of thousands of rows being returned to populate a couple dozen objects.
nopCommerce is great. It has so many features out of the box and is quite extensible.
Can you host Core apps in IIS? I thought all you got was a reverse proxy in IIS so it could forward requests to Kestrel.
Because you're probably not wanting to introduce 4 different DB access methodologies to any single project, and in many cases the tooling offered by EF is head and shoulders above many other solutions right now. There are other ORM options, and certainly cases where you *shouldn't* use EF, but there are plenty of places where the reduced friction it introduces just makes sense. Abstracting away the majority of the DB logic and SQL may not always be the best solution, but if you're simply building CRUD apps, there are few other options that offer the depth of both change tracking and ease of implementation that EF offers. Also, you're missing Code First, which allows for the reduction of devops overheads in small teams (provided they put in the early infrastructure) and can mean you don't need developers screwing around making DTOs and middle layer objects because they don't know how to make the database reflect their object model. Like I said, there is always a specific tool that will probably do the job efficiently, but some times you don't need just a hammer or a screwdriver, you need the whole toolbox and you don't want or need the level of fine grain control where you need to pick and refine every tool in the box. I personally probably lean on Dapper more than EF these days, but EF is still core in many apps I work on, because it provides a simple framework for dealing with a lot of real world, production problems when turnaround is important and the amount of time spent scaffolding most other solutions just isn't worth it.
Now at 13:00 CET we'll have the first experimental #stream about the check of a project by PVS-Studio. Join!
As I read it, he switched ASP.NET MVC with ASP.NET core, but not EF6 with EFcore, so things run on .NET full, with EF6 and ASP.NET core. 
Sounds like php to me.
Why doesn't PVS-Studio detect: var file = new File(); if(x) { file = GetFile(); } else { file = GetOtherFile(); } as "The 'x' variable is assigned values twice successively. Perhaps this is a mistake."
What time is it released for today ?- is this the new version ? I dont think so since is stil a release candidate https://www.visualstudio.com/vs/visual-studio-2017-rc/
Thanks!
[removed]
If anyone's having trouble downloading the installers, I've popped them on my Dropbox: Edit: links removed due to DMCA. If you're having trouble downloading form the main site, try disabling your ad blocker as that's what's reportedly preventing it. EDIT: (Thanks to /u/SikhGamer) &gt; For offline:- &gt; `vs_community.exe --layout c:\path_to_somewhere --lang en-US`
You are talking about dacpac right? I always wanted to start with dacpac as opposed to manually concatenation of series of migration scripts, but never got to try it. Sounds like it was a good decision.
For offline:- `vs_community.exe --layout c:\path_to_somewhere --lang en-US`
X-Post referenced from [/r/netsec](http://np.reddit.com/r/netsec) by /u/lowleveldesign [How to securely sign .NET assemblies? (with some insights into the signature binary form)](http://np.reddit.com/r/netsec/comments/5xznw6/how_to_securely_sign_net_assemblies_with_some/) ***** ^^I ^^am ^^a ^^bot. ^^I ^^delete ^^my ^^negative ^^comments. ^^[Contact](https://www.reddit.com/message/compose/?to=OriginalPostSearcher) ^^| ^^[Code](https://github.com/papernotes/Reddit-OriginalPostSearcher) ^^| ^^[FAQ](https://github.com/papernotes/Reddit-OriginalPostSearcher#faq)
Thanks! I've added your comment to the links.
All hail CSProj.
1) You are using databases so you should know about databases. 2) Use a stored procedure. 
There's something special about opening a new version of Visual Studio for the first time. Congrats Microsoft and thank you for an awesome product!
I'm happy it worked out for you 
Apparently the download pages don't work if you're using an adblocker. Which is probably why people are having download issues.
TL;DR: it's so fast without R#!
That kind of sucks. It was nice having a build system that was human writable, shame that the effort needed to make VS (and presumably some other MS tooling) integrate nicely with project.json outweighed the benefit.
not sure what exactly youre looking for and not sure exactly what we can tell you based off of arbitrary table names. You would join clients to Clients2Company and join that to Company etc...
In fairness, project.json wasn't a build system, it was a project structure and as cool as it was, it wasn't as powerful as MSbuild could be. Now we get the best of both worlds, the *new* csproj format is a delight.
Since I upgraded a project to VS 2017, its build time has nearly doubled. I changed nothing other than migrating to csproj. It's really irritating.
It may depend on which Adblocker. I know uBlock Origin on Chrome was blocking the download until I disabled it on the download page.
Click the link in this post. It's literally called "What’s new in Visual Studio 2017". Yes there's stuff for Xamarin devs, yes Resharper is compatible as an EAP and if you want the full breakdown, start here: https://www.visualstudio.com/en-us/news/releasenotes/vs2017-relnotes
Thanks that seems more informative. The page itself seemed way too generic.
It could do. I have UO on firefox. Either way, not sure if it's the page's fault or the blocklist's fault. Going to guess the latter.
I'm sorry for the poor explanation. If I were to instantiate a copy of this dbcontext under the var "context", I'm looking for the syntax needed to join so many tables together using that context. So to get all clients I would do something like: context.Clients.ToList(); To get a specific client with their companies(here's where I fall short). I know how to go one level deep with related entities but getting that second level and onward is stumping me. How do I add the Company class who's foreign key resides in the Clients2Company class to this query: context.Clients.Where(c =&gt; c.id == "9999").Include(c =&gt; c.Clients2Company).ToList();
Couple Options .Include(c =&gt; c.Clients2Company.Select(cat =&gt; cat.C.Select(c =&gt; c.Company2Episode))) and continuing on like so. Not very readable. method syntax: (i dont know the exact syntax here, i normally use the below (query) syntax this.context.Clients .Join( this.context.Clients2Company, c =&gt; c.Id, ca =&gt; ca.ClientId, (c, ca) =&gt; new { properties }) .Join( this.context.Company, ....) query (?) syntax (i actually dont know the name of this syntax) (from client in context.Clients join clients2Company in context.clients2Companies on client.id equals client2Company.clientId join company in context.companies on clients2Company.companyId equals company.id join company2Episode in context.company2Episodes on company.id equals company2Episode.companyId join ... select new { }).ToListAsync() 
Please stop spamming your website or we will need to ban you from the subreddit
I've used every version and I think 2010 was one of the best updates, apart from this release.
Search the Feedback Hub. There are many solutions for problems similar to this one.
The json project files weren't what I would have considered human writable. Better than the fat csproj no doubt, but it still had a long way to go.
That's what I got with the new console app template. I just changed netcoreapp1.1 with net452.
I'm not going to bother with this discussion any further because you clearly have no idea what you're talking about and the people that do already have enough information to understand my points.
I've been pushing off some projects until this day. Time to get to work
You have stack traces and stuff for that. You make a point but it would be so open to abuse. Over the years I have come to the conclusion that every time you relax the rules, some one will do something you didn't expect. Strongly typed languages are strongly typed for a reason. If we relax this constraint, we give up that guarantee.
I am not, no.
ReSharper's continuous unit test feature is a good alternative. Not the same but still good. 
XML is no easier or harder to process than JSON. And if you want something nicer, there is a library for working with csproj files. And MSBuild is open source. So what exactly is your complaint?
Mostly that MSBuild is one of the reasons I left Microsoft related tools and opted for open source tools (this was nearly a decade ago and I have no regrets). Ultimately, I'll just shelf any ambitions of getting familiar with Microsoft development for a while. Looks like .net core is still pretty unstable and I'd rather focus on practical tools and solutions. 
Has anybody had any success building their migrated projects using a hosted agent on VSTS? The version of dotnet on the hosted agent fails to build it because it expects a project.json file (which no longer exists), and the VS build step also fails, claiming that my .csproj files aren't supported by MSBuild. Would love it if somebody could point me in the direction of a working build definition.
I really hope so. I can honestly understand wanting to support both or at the very least preserve their existing msbuild customers but I think it was a bad decision to fully commit to project.json and an even worse decision to completely reverse course and go back to csproj. These sorts of sweeping decisions in combination with making major changes to project file format without a major version change do not instill confidence for organizations which rely heavily on tools that just work. 
I'm always shocked that no one considers the worst point of having a json file: You can't inline document. Does no one ever document their shit anymore? The project.json file was **complex**. If you ever needed something slightly more complex (e.g. include excluded files, pin a nuget version) there was no way to document it in the place where it matters.
We had some bad dependencies. Ended up blowing all the references/packages out and redoing it. Working great now. 
The steps in [this article](https://blogs.msdn.microsoft.com/webdev/2017/02/14/building-single-page-applications-on-asp-net-core-with-javascriptservices/) worked for me. Tried it last week. All I did after creating the project (using "dotnet new angular") was change the TargetFramework in the csproj file to netcoreapp1.1 and update the nuget packages.
So selecting mobile development in the installer adds 20GB to the install size... Thanks, Android.
XML and the MSBuild array of tools/commands are pretty simple to write, but you don't get cool points for hating xml over json. I wonder what we'll say about json vs &lt;new format&gt; in 10 years.
Started using this 2 weeks ago for our company's .NET Core projects. I love it.
&gt; but is it really that bad? From the standpoint of someone who has to determine whether or not to allocate resources for particular platforms or tools, yes. If Microsoft is going to change the toolchain every few months or even once every year that's a pain point. It negatively impacts business justification for investing in that ecosystem. Again, it's great for those who are already entrenched and I recognize that Microsoft is trying to break into OSS. I'm just saying that this change has put my organization off from committing to supporting .net in a greater capacity. Existing .NET solutions are already languishing in favor of web based alternatives specifically because of licensing and tools related issues.
Thanks!
As far as I'm aware, there is no way to restore them from previous versions of Visual Studio. Even if there were a way most of the tools I use, I assume you're experience will be similar, even have different names in the newer visual studio. 
AFAIK, yup. You have two options - Stick with 1.0.3 and 1.1.0 and use VS2015 - Upgrade to 1.0.4 or 1.1.1 and use VS2017
Fucking LOL. "Where was the setting for that?" "Not that tab. " "Not that tab." "Not that tab. " "Shit"
So you tried to open the list of server names?
Never attempted this before myself, but here are some resources on it. http://www.strathweb.com/2013/01/asynchronously-streaming-video-with-asp-net-web-api/ https://forums.asp.net/t/1823255.aspx?Live+video+broadcasting+using+webcam+in+c+website
I use the master page to load the common CSS and JS files, as well as the 'common look' such as the pages headers and menu. Now, if you have a good page layout design and you're very comfortable with CSS and HTML you can take advantage of the contentplaceholders in your master page (since you can have as many contentplace holders as you want) and add great functionality to your pages
Have not tried but I wonder if Application Request Routing would work with video streaming. (A quick Google search for "Application Request Routing video streaming" seems to suggest it's possible) In theory you wouldn't have to write any C# code, would just use ARR and URL Rewrite to make your server rebroadcast the content.
They never fully committed to project.json. When .net core runtime went RTM, they had already said about the switch back to csproj. That's why they kept the SDK in preview until now.
Asynchron is for the weak!
&gt; there is a library for working with csproj files What the library? please.
Gulp and Grunt support are built in as is npm. So long as you know how to configure tasks for those and have internet connectivity on your TFS environment you can do that.
Microsoft really, *really* needs to get their shit together regarding UI blocking -- it is just fucking embarrassing that I'm 2017 I have to sit and watch a waiting wheel every time my IDE does anything in the background.
It's sad that they invented `async` literally to deal with this exact situation (that's why you have to `ConfigureAwait(false)` all over the place), but VS isn't written for .NET.
I don't even want to imagine the quality of work that $15 an hour gets me. 
I'm always happy to see a .NET Core success story. But this kind of comparison is fairly meaningless without a much more in-depth look at the "before" and "after" solutions. The author was in the thread on /r/programming, hopefully we'll learn a bit more about this case in the semi-near future.
Authentication for API clients? Most APIs I use are secured with OAuth 2.0. We do so at edX, and use JWTs for access tokens. 
We hand out certificates and authenticate that way (the java guy does)... is this old fashion ? 
I'm skeptical.
There is no one "correct structure", but yours sounds like a good starting point. For a simpler project, this is probably fine. However, I do want to call out some information I read recently in Gary Hall's book "Adaptive Code via C#". In it, he refers to your design as the "Entourage anti-pattern". He describes that while it's good to separate concerns in such a way, by having dependencies on direct implementation projects, whom also have their own dependencies on direct implementations, the separation into different projects doesn't really buy you anything since you can't execute the code in the first project without bringing along its dependencies and their "entourage". To solve this, he proposes the "[Stairway Pattern](http://stackoverflow.com/questions/29259414/stairway-pattern-implementation)". This is basically following a design similar to what you have, but rather than having each project depend on the implementation projects directly, he abstracts the interfaces of each project into their own projects, which dependent callers then link to. So in your example, you'd have something like: ProjectName.Web -&gt; ProjectName.ServicesInterfaces ProjectName.Web -&gt; ProjectName.Models ProjectName.Services -&gt; ProjectName.Datainterfaces ProjectName.Services -&gt; ProjectName.Models ProjectName.ServicesInterfaces -&gt; ProjectName.Models projectName.DataInterfaces -&gt; ProjectName.Models ProjectName.Services implements ProjectName.ServicesInterface (and thus has a dependency on it) ProjectName.Data implements ProjectName.DataInterface (and thus has a dependency on it) The important distinction here is that the Interface projects have **no** dependencies on implementations (only POCO model data). These projects should only contain interfaces for the projects they support; there should be no implementations in these projects. This buys you a couple things: * It stops high level code from bringing along its dependencies "entourage". Since the interface projects have no dependencies of their own, the high level code is completely cut off from any other implementation code. Which brings us to the next benefit * The higher level code has no namespace access to actual implementations, which enforces **at the compilation level** that code in that project uses only interfaces for its dependencies. The obvious benefits of doing so in general are for testability, maintainability, etc. but having this enforced at the compiler level means no developer writing code in that project can "cheat" and new up impl objects that they should be retreiving via dependency injection. Unfortunately, even using the dependency graph I laid out above for your example, this wouldn't work. The issue is that at the end of the day, **something** has to actually new up real instances of real implementations to start injecting them into constructors for the code to actually do anything. Given the current layout, this responsibility would fall to "ProjectName.Web", since that's the startup assembly. But with a small tweak, that wouldn't have to be the case. If you were to rename ProjectName.Web to something like ProjectName.Core or ProjectName.Bootstrap, then separate out the actual Web/Console work into a new ProjectName.Web assembly, you can consolidate all the bootstrap work of newing up objects into ProjectName.Core and then ProjectName.Web can keep its dependency list contained to just ServiceInterfaces and Models. So your final graph would be something like: ProjectName.Core -&gt; **EVERYTHING** (only for instantiating objects and building the dependency graph, NO actual 'work' should happen here!) ProjectName.Web -&gt; ProjectName.ServicesInterfaces ProjectName.Web -&gt; ProjectName.Models ProjectName.Services -&gt; ProjectName.Datainterfaces ProjectName.Services -&gt; ProjectName.Models ProjectName.ServicesInterfaces -&gt; ProjectName.Models projectName.DataInterfaces -&gt; ProjectName.Models ProjectName.Services implements ProjectName.ServicesInterface (and thus has a dependency on it) ProjectName.Data implements ProjectName.DataInterface (and thus has a dependency on it) By doing this, when you do a full build of your application, you'll still have all the dependencies compiled together for the app to run, but if you do partial builds of specific projects (say, for running tests against just classes in that project) you no longer pull in the entire "entourage" of dependencies along with it. This is definitely a more advanced pattern and is probably overkill for smaller projects, but for large projects which require lots of eyes and lots of unit tests, it's definitely not a bad way to lay things out. 
It might be an old "sql team vs dev div" issue
People are downvoting you, but I agree with the sentiment. Performance increase like that seems unreal. However, Raygun is large and appears to be trustworthy. There's a thread in /r/programming about it and the guy posted some light details. 
Well, it IS a preview version. A preview version of improved 2017 builds.
There's so many different ones I've used. For the simplest sites I won't break it at all, but that's rare. One commonality once I break it, I immediately consider the MVC site as simply a client UI to the app. If I wanted to expose the app as services, I'd use WebAPI as a client to it as well, but if you design the core app right, both can access the same code pretty interchagably, so I start with one of two projects: 1. Root.Web.UI 2. Root.Web.API Then how I break it up depends on what the team decides... but a couple examples: N-tiered 1. Root.Business - Main interface to your app. Exposes service classes that wrap all business logic and processing. 2. Root.DAL or Root.Data - DB access (repositories, command/query, hard coded providers, but only if we're using that. If we are doing ORM I'll forgo this and use the ORM directly by preference). 3. Root.Core or Root.Domain - Core models, and other shared pieces, most everything will reference this. Limit dependencies at this level to the absolute minimum. I used to use .Domain but thats kind of a misnomer since really the entire app from Business down is the "Domain", so I use Core more often now. 4. (Optional) Root.ServiceX - I like segregating external service references to other projects. Especially with WCF/SOAP stuff. If you look at Onion Architecture these all have equivalents there, and I've found that in general that breaks things down nicely. Part of what they do there is define interfaces for functionality down in the application core, but the implementations in higher layers, and rely on DI to map them at run time in the app. This seems counter intuitive, but the couple of times I've done it, it was actually fairly nice...
Hi Ben! Did they use your special number? What kind of magic did you provide this time? 😁
First off, it's important to know what version of TFS are you using? In 2017 and VSTS, there are a lot of options. Gulp and Grunt have built in tasks. NPM is built in. If you can run a tool from the CLI, it can be set up in TFS Build. I have a build running webpack. I just installed webpack, then call it with a Command Line build step. Check out [Donovan Brown](http://donovanbrown.com/) and his [Channel 9 Videos](https://channel9.msdn.com/Events/Speakers/donovan-brown) for a lot of good information on TFS/VSTS Build and Release.
Preview channel would be a better way to phrase it.
/u/JenMSFT? 'Preview Channel' is a much clearer name. Not quite Windows... But [DEVELOPERS, DEVELOPERS, DEVELOPERS, DEVELOPERS](https://www.youtube.com/watch?v=KMU0tzLwhbE)
At present, we don't actually have a default cake-addin icon, but it is something that is being worked on, so keep an eye out for it landing.
I have been having some issues with VS 2017 as well. One where I got that same error because Bitbucket extension was messing with the startup. Sometime's I would have no intellisense throughout my projects that would prevent me from building, but would be fixed if I manually did a restore in cmd or restart VS. It's the growing pains of upgrade I guess....
I can't get Git working properly. Other than that I'm OK so far.
That makes sense. In an ideal world, that process is self-service. With OAuth, access tokens last on the order of hours. The credentials used to obtain the access tokens can be long-lived and reset by the user. 
I am having trouble too but I think it's because of the plugins. Maybe it's worth trying to remove them?
I've had problem with intellisense putting red squiggle everywhere with stuff like "object" and "string" were not defined. Build still works though. The problem comes and goes, not sure what's causing it.
Yeah. Although I've "only" had it crash or freeze four times (not a single crash/freeze for two years with 2015). Mostly been while typing (I remember one freeze was caused by my first attempt with inline out declarations - as soon as I finished typing `out `). Intellisense seems to constantly turn comatose at random moments, refusing to see types in referenced assemblies (and even sometimes in the same assembly, namespace and folder). Syntax highlighting and checking tends to keep error squigglies and lists of already fixed errors until I restart VS, even if the project will build fine. Any kind of file move or rename is almost sure to cause issues (and sometimes give an error dialog). Sometimes projects in the solution fail to load at all when opening it, and NuGet actually isn't as smooth sailing as it was with the xproj format... etc. Clean install, no add-ons yet except what's included out of the box.
I've tried with and without plug-ins to no avail on my install.
I wanted to migrate my CallJson extension from 2015 to 2017 and all I got was a non-visible extension and a corrupt installer that complains about missing SqlServer dll...that I don't use! It seems that there are some issues with PC's where the 2017RC was installed. Is it your case?
Is this on a new project/solution or an existing project/solution?
Actually a large part of VS is written in .NET these days.
No crashes, much faster. But when ever I open a solution, it loads for 5 secs and then stops. Opening it a second time opens it. Worthwhile trade-off for me ;)
Mine crashes several minutes into trying to analyze a node_modules folder. That's really the only issue I've had so far. I'd love to know a way to make it just ignore that folder.
I had something like that when opening old projects, changing the Dot net version to the most recent 4.51? (not in VS so can't remember actual number) fixed them for me. 
nope, i did a system refresh in win 10 today and put VS 2017 community, loads great!
Yeah I got a few I reported.
It's crashtastic for me as well.
Both. Gave up on migrating an existing project, after spending a couple of hours getting NuGet and Publish to behave on a multi-target project (ASP.NET Core 1.0), and then still having all kinds of issues. Since this isn't a huge project, I decided to start from scratch and move over functionality class by class, and clean it up in the process. Didn't get far before it started acting up again. Not related to the specific project either - a few changes in the ASP.NET MVC template causes havoc too. Haven't checked non-ASP.NET projects yet.
Who knows - maybe that's the issue. :-) I also never installed RC.
I have this weird error and its repeatable. Whenever I change the target of a project from 1.0 to 1.1 or back again, the debug folder appends to itself. And if I just open the project properties and save it does it again. so my debug folder ends up being this long folder in folder thing "/bin/Debug/netcoreapp1.1/netcoreapp1.1/netcoreapp1.1" 
The VS 2017 installer tries to skip a bunch of stuff unless you explicitly add it to make the install shorter and smaller. I know the Github integration for example is available in the installer, but I forgot to check it.
&gt;living in a cave Its ok, I' still haven't installed 2017. You might be able to right-click on the project node to get an option to edit the csproj 
Meh I tried VS2017 once and opened an existing project and it spiraled into error hell. I keep it on my home computer because I enjoy playing with the unity it came with but work-dev pc stays on 2015 for at least another year. Every day I see another "VS2017-dis Broke" post and don't need any of the new features that damned bad. (I understand his particular issue is from .netcore update, but I feel the same way about .netcore at this point too, for the same reasons :O)
I have not had any issues at all. Before that, the RC also worked flawlessly. However, the installer is a bit problematic for me because it runs different processes for the various components, which Symantec Endpoint Protection blocks.
I think it's exactly related to VS2017. They decide to abandon project.json and add csproj *in a bugfix update* the week VS2017 ships? That's too much of a coincidence for me.
&gt; They decide to abandon project.json and add csproj in a bugfix update the week VS2017 ships? They abandoned it way back in May last year. This isn't new.
1. I went to C:\users\&lt;yourusername&gt;\AppData\Local\Microsoft\VisualStudio and deleted the folder whos name starts with 15.0 but has other letters and numbers on it. "f135b672" is what was on mine but yours will be different. I also deleted the folder that stands by itself with the name of just the other letters and numbers. 2. I then went to C:\users\&lt;yourusername&gt;\AppData\Roaming\Microsoft\VisualStudio and did the same thing there. I didn't see the folder with the letters and numbers by itself though. 3. Start visual studio and don't install the prugins. 
2.0 comes out later this year.... not sure why 2.0 matters though
I couldn't open the Extensions &amp; Updates modal. Just threw me a COM exception dialogue, class not found I believe. Unfortunately I can't tell you the specifics because repairing the install blew up entirely. "The product failed to install the listed workloads and components due to one or more package failures." Then proceeds to list *every* workload I chose. Every individual component I installed. "PackageAction=Repair;ReturnCode=1706" "Try the installation again using a valid copy of the installation package" YOU DOWNLOADED THE PACKAGES! I WATCHED IT HAPPEN! HOW'S ABOUT YOU DON'T GRAB INVALID PACKAGES? The RC worked beautifully, although lacked some features supposedly found in the final release. I uninstalled the RC prior to final release everything seemed to go wrong from there. I wouldn't mind so much if MSDN Subscriptions weren't so pricey.
I too get the crash at loading projects, happens almost every other load.
.NET Standard 2.0 is the future.
[From a Microsoft employee on the Visual Studio team](https://www.reddit.com/r/programming/comments/5y0zu5/visual_studio_2017_released/demymfe/?context=1) You can create your own off line folder with the full installation (about 20GB) by using the command: vs_community.exe --layout c:\path_to_offline_folder --lang en-US Replace `vs_community.exe` with whichever version you downloaded. 
If you're not going to install xamarin you might be better off just using the web installer (even if you're installing on multiple machines), it's the majority of the download.
This week there was a very good article on the differences, you should check it out http://www.natemcmaster.com/blog/2017/01/19/project-json-to-csproj/
Consider plugging in on ASP.NET Bundling.
VS2017 has NEVER contained the project.json support, since the early RCs. It's always been the plan to migrate solutions to csproj. This was a well communicated and, now in retrospect, good decision; unless you've been ignoring Dev updates when using the *pre-release software*, and pretending it's finished.
There is a module for the Bundle Transformer - https://bundletransformer.codeplex.com/wikipage?title=Bundle%20Transformer%3a%20Sass%20and%20SCSS
Good luck ever proving you have a 100% download without an ISO to hash against!
MSBuild isn't even close to legacy, and the bigger issue was being able to include .NET core projects in the same solution as non .NET core. They built the JSON format so that it'd look like npm, but C# doesn't work that way, and to be honest npm isn't exactly a model that's worthy of emulation. C# solutions are basically intended to have multiple projects and an inability to reference properly is just fundamentally broken. The new csproj format isn't the same as the old one, it's been somewhat improved, but it's obviously going to be more complicated than the JSON format that was missing hide huge amounts of functionality. 
Odd how?
Projects: * **Busey**.Core * **Busety**.RabbitMq.Test.Reviever * **Buse**.RabbitMq.Test.Contracts Also solution and repository are named after one core project.
Why would anyone need to prove that really?
Are they still common? Have they ever been common?
BIDS?
business intelligence something studio i think... visual studio plugin or some weird stuff
The runtime was released as 1.0. The tools were still in preview releases.
Maybe he's from the Linus community? A subset of them have been obsessed about verifiable downloads for decades. That's why they spend so much time talking about securely signing official binaries.
It's Linux. And have you not seen the tools leaked from the CIA? One of the specific things they do is utilize programs that appear to be correct but contain custom code to do their dirty work. Also, it is not just the CIA doing this, it is a common tactic known as a Trojan Horse. If you have the hashes of the software as released from a "known good point" you can validate that the software is not a Trojan nor corrupted.
The problem for me is that after finished, the offline downloader of the installer doesn't give any confirmation message. Maybe I'm missing something but I don't see a way to tell what components are already present on the installer. I can see them on the folder but that's a lot of files to check. 
Sure. I got that. But maybe I am too used to other Open Source projects. I figured they would release 1.2 and replace the tooling in there. At no point does (did?) their website distinguish "tooling" vs "non-tooling" when you go to download .NET Core. You downloaded .NET Core. I never expected the tooling to be replaced in a bugfix update to an LTS release. Is it just me or is it completely insane to expect that a bugfix update to an LTS release will change the command line completely? If I had deployed the LTS release to production, I would be a little pissed right now. Or is this Situation Normal: All Fouled Up for Microsoft projects? 
Well I never said that their concerns were not justified.
Ah, I see what you mean. Yeah, it does seem odd that a bug fix was how the tooling was update. I should note, however, I didn't realize the change was made via a bugfix. If that is the case, that is surprising. 
Yup, that was my entire point (for which I got massive downvotes)/ Look at the [releases](https://github.com/dotnet/core/releases): &gt; .NET Core 1.0.4 &gt; &gt; This is the release for .NET Core 1.0.4 and .NET Core 1.0.1 SDK &gt; .NET Core 1.0.3 &gt; &gt; This is the release for .NET Core 1.0.3 and .NET Core 1.0.0 SDK - Preview 2 (build 003154). So if you want to go from 1.0.3 to 1.0.4, prepare to completely migrate from project.json to msbuild. Imagine if this update included security vulnerabilities...
I just added a small change to do this. Basically all you would do is specify a custom queue name and bind it to a fan out or topic exchange to route the messages to multiple queues with handlers listening
I'm also using Bonobo, didn't think VS2015 needed any extensions to use it though.
This sounds suspiciously like a product not shipping with all the dependencies it needs. Which is, of course, the Wrong Thing To Do.
Try a projects.json with the same level of functionality as a csproj file. See how that looks. Project.json is simpler than csproj which is why it's easier, not because it's JSON. 
With https?
And this, my friend, is why we should just agree to disagree. You're obviously missing a few steps, if you claim item groups have nothing to do with XML vs JSON. It has EVERYTHING to do with it. XML is intrinsically filled with overhead BECAUSE it's TAG based. You HAVE to declare tags, with names and often attributes, and opening and closing characters. JSON is JUST "Key":"Value". That makes the structure and readability INFINITELY better in JSON. Period. If you are not willing to agree to this very simple fact, then we're just done. There's nothing more to talk about. The whole point about this config situation is exactly that it's a CONFIG file, which means it's not SUPPOSED to be high information density. It's supposed to be easy to read, write and structure at a glance. JSON has a leg-up here. Period. &gt; You could probably put all that into a JSON file, but it's going to be ugly as fuck And this, again, is where we disagree. Regardless of what functionality you need to put into the JSON file, it will always be key-value structured. In XML it will ALWAYS need a slew of tags, with attributes and other declarative overhead. Always. I mean. For God's sake: &lt;Project ToolsVersion="15.0" xmlns="http://schemas.microsoft.com/developer/msbuild/2003"&gt; &lt;ItemGroup&gt; &lt;PackageReference Include="Microsoft.NETCore.App"&gt; &lt;Version&gt;1.0.1&lt;/Version&gt; &lt;/PackageReference&gt; &lt;/ItemGroup&gt; &lt;/Project&gt; vs { "dependencies": { "Microsoft.AspNetCore.Diagnostics": "1.0.0" } } Look at all that overhead. And that get's progressively worse with each extra tag and/or section. Not to mention sections are itemgroups with no clear indication of what's inside. JSON tells you. Right there. No extraneous and redundant tags. "This is where dependencies are". "This is where tools are". "This is where build scripts are". You have no clue when reading a csproj what the different property groups and itemgroups contain until you read the contents of the sections. This get's worse in XML over time and with increasing complexity, BECAUSE of all that overhead, whereas in JSON, you don't have to scan the contents or the data within the sections, you simply look at the very first key to find your section. Everything in csproj as of right now that does things that the project.json "can't" would have it's own section, and thus wouldn't affect readability to the same degree that XML would. And once again all that "functionality" you speak of that csproj has that project.json doesn't? It's all about the build tools. It's not the format's limitations, it's the build tools.
Oh I totally agree with you. I feel that is a better approach but my attempt here is to use model-binding. Which (as far as I know) deals with me setting a lot of stuff in the UI to allow the two way binding. Now if in the UI I don't use "binditem.questions.find(function...." and use just "item.questions.find(function..." (basically switch between binditem and item) then it does a one way binding and shows the value in the UI, but when tryupdatemodel() it of course doesn't include my nested collection in my model. My temporary solution (possibly permanent because web forms is lacking I believe) is to let tryupdatemodel make my model the best can for the base properties and then I do the sloppy formview.findcontrol("controlHoldingMyNestedCollection") and setting my nested collection's items and properties myself. But that then gets away from what I was trying to achieve which is get remove the find control lines for a large form and implement a new feature from .net 4.5 for web forms which just isn't doing the full job lol. Thanks for the code snippet though, love the alternative! May be better to forget about model binding in this case. 
Here's the idea of code that I'm trying to fix for reference. Public Class Form Public Property FirstName As String Public Property LastName As String Public Property Email As String Public Property Phone As String Public Property Questions As New List(Of QuestionModel) Public Sub New() Questions.AddQuestion("Question1") Questions.AddQuestion("Question2") Questions.AddQuestion("Question3") End Sub End Class Public Class QuestionModel Public Property QuestionType As String Public Property AnswerValue As String End Class &lt;Extension()&gt; Public Sub AddQuestion(ByRef Questions As List(Of QuestionModel), QuestionType As String, Optional AnswerValue As String = "") Dim Question As New QuestionModel With {.QuestionType = QuestionType, .AnswerValue = AnswerValue} Questions.AddQuestion(Question) End Sub &lt;asp:FormView ID="fvRegistration" runat="server" ItemType="Form" DataKeyNames="FormID" SelectMethod="fvRegistration_GetItem" UpdateMethod="fvRegistration_UpdateItem" InsertMethod="fvRegistration_InsertItem"&gt; &lt;EditItemTemplate&gt; &lt;asp:TextBox ID="txtFirstName" runat="server" Text='&lt;%# BindItem.FirstName %&gt;' /&gt; &lt;asp:TextBox ID="txtLastName" runat="server" Text='&lt;%# BindItem.LastName %&gt;' /&gt; &lt;asp:TextBox ID="txtPhone" runat="server" Text='&lt;%# BindItem.Phone %&gt;' /&gt; &lt;asp:TextBox ID="txtEmail" runat="server" Text='&lt;%# BindItem.Email %&gt;' /&gt; &lt;asp:DropDownList ID="ddlQuestion1" runat="server" SelectedValue='&lt;%# Item.Questions.Find(Function(question) question.QuestionType = "Question1").AnswerValue %&gt;'&gt; &lt;%--List Item Answers--%&gt; &lt;/asp:DropDownList&gt; &lt;asp:DropDownList ID="ddlQuestion2" runat="server" SelectedValue='&lt;%# Item.Questions.Find(Function(question) question.QuestionType = "Question2").AnswerValue %&gt;'&gt; &lt;%--List Item Answers--%&gt; &lt;/asp:DropDownList&gt; &lt;asp:DropDownList ID="ddlQuestion3" runat="server" SelectedValue='&lt;%# Item.Questions.Find(Function(question) question.QuestionType = "Question3").AnswerValue %&gt;'&gt; &lt;%--List Item Answers--%&gt; &lt;/asp:DropDownList&gt; &lt;/EditItemTemplate&gt; &lt;/asp:FormView&gt;
Why don't you move your data access into a web api? Then your web app could call the api when it needs data from the database and you could do the same for the mobile app.
Thank you, I'm not used to web.api but I will look into it.
What database are you using?
You can do it with MVC, just return a JsonResult instead (by calling Json() on the data to return). 
Look into Web API/MVC and Entity Framework as you have your database already built. Once you have your API parts returning JSON you can easily consume that from your mobile app.
Someone on the announcement thread mentioned using ISO's for automating deployments, which sounds to me like they don't trust developers to install software on their own machines (which in turn sounds like a shitty work environment).
What level of government? I work for a large local county as a software developer and we use MVC 4-5. I personally think Core is a little early yet to be doing serious developing on for shipping production software. As far as recommendations for learning, starting with 4/5 and moving to Core in the future would be best to establish the baseline, imo. Where .NET has been and then where it's going. For a frame of reference, most of our existing web apps are MVC 2, 3 and 4. If they are passing these onto the government entity to maintain, it might be best to start there. Purely my opinion from the government side.
*public static T AddIfValid&lt;T&gt;(this ICollection&lt;T&gt; collection, T item) where T : ICanValidate* I understand this is just an example, but the use of generics is somewhat contrived .... public static ICanValidate AddIfValid&lt;ICanValidate&gt;(this ICollection&lt;ICanValidate&gt; collection, ICanValidate item)
Totally forgot to check the EAP channel. Thanks 
Just using MVC in government is an uphill battle. They all want to use the easiest, cheapest solution (web forms) but bitch when everything looks like crap.
I faced similar confusion when I was getting into ASP.NET development from Windows Desktop (WPF/Windows Forms). From my research, I'll say ASP.NET Core isn't much in demand yet, so for jobs (government or not), you're better to go with ASP.NET MVC 5. I dd same.
I'm sorry - what!? If you want to add a field to that dependency, you don't have to handle an index!? Why? It becomes (real world example): "dependencies": { "Microsoft.NETCore.App": { "version": "1.0.1", "type": "platform" } } Your example with XML is, firstly, pseudo XML, as in the actual csproj implementation, dependencies first and foremost aren't called &lt;Dependencies&gt; or &lt;Dependency&gt; (Even THAT would be a step up, because then you would be able to scan for those tags). No, they're nested into an &lt;ItemGroup&gt; which is so arbitrary it hurts my eyes - and is used for multiple different config sections in csproj. Then a &lt;PackageReference&gt; and the attribute for the name of the package to reference isn't even name="" but Include="". Then of course moar tags for version and so on. Oh and you conveniently forgot to close &lt;Dependencies&gt; (but you're forgiven, it's hard to keep track of all those tags and overhead :)) you also conveniently left out hintpath in the XML, and I have no clue WHY you chose to put the name: property within the dependency, when you would obviously use the name in place of "dependency[1]" (seriously, what's with the index?). Your pseudo-code (to be fair:) &lt;Dependencies&gt; &lt;Dependency name="myDependency" version="value" hintpath="value" /&gt; &lt;/Dependencies&gt; dependencies:{"myDependency":{version:"value",hintpath:"value"}} And this is a case of pseudo-best-case-XML vs ACTUAL implementation of JSON. Liberal use of attributes makes all XML shorter - but it's not how it's done in actual implementations. JSON IS simpler BECAUSE of the format (duh). No doubt you could have made csproj much easier to read and write, if they hadn't used all those arbitrary tags and weird tagnames - and insisted on nesting tags instead of using attributes for values. But hey. That's how they chose to implement it. Maybe because of best practices? Regardless, arguing that JSON isn't simpler because of the format is very disingenuous.
And you didn't have to sign anything like an NDA about it?
Actually, if you REALLY needed to document this IN-PLACE, you could make a simple "__comment":"Known incompatibility with v.xx" (but this could obviously create other issues) You're right, there are times where comments MIGHT be nice. But if you ask me, stuff like that should be commented/documented elsewhere (preferably where the JSON is consumed if possible, as the code should/would have knowledge of it beforehand). Besides, you could also take a look at why the author [removed comments](https://plus.google.com/+DouglasCrockfordEsq/posts/RK8qyGVaGSr)
&gt; Actually, if you REALLY needed to document this IN-PLACE, you could make a simple "__comment":"Known incompatibility with v.xx" (but this could obviously create other issues) Now I get compilation errors because it cannot resolve the NuGet package "__comment". This is a hack, no solution. A poor one. &gt; You're right, there are times where comments MIGHT be nice. But if you ask me, stuff like that should be commented/documented elsewhere (preferably where the JSON is consumed if possible, as the code should/would have knowledge of it beforehand). So... I should document in the source code of the framework why in my application I want to pin a specific version of a package? Because that's where the project.json file was used. This makes absolutely no sense. It should be documented right where it's defined and changed - not somewhere else. &gt; Besides, you could also take a look at why the author [removed comments](https://plus.google.com/+DouglasCrockfordEsq/posts/RK8qyGVaGSr) Your link does not work. Besides that, Douglas never intended JSON to be used as a configuration format, especially not for something so complex as the project.json file. It was supposed to be a easy to write and understand serialization format.
Its about the bundle, not the container format. And mounting ISO's is build in win 10. They finally did that and now they don't release iso's anymore???
You could try this: - Configure everything according to this: http://stackoverflow.com/questions/31689374/how-to-make-razor-view-engine-to-use-c-sharp-6-0 - Additionally explicit install the `Microsoft.Net.Compilers` package in version 2.0.1.
So the vast majority of C# projects are single projects with limited dependencies and no build targets. Bullshit.
I would wait until .NET Standard 2 is out for consulting projects. ASP.NET Core is amazing but better train your developers with mini projects and get accustomed to its amazing capabilities before using it on big customers.
+1 to getting ReSharper 2017.1 EAP. (RTM is expected end of March) In addition, check out these two JetBrains blog posts that explain the state of support for latest C# and for .NET Core unit testing: [State of the union: ReSharper C# 7 and VB.NET 15 support](https://blog.jetbrains.com/dotnet/2017/02/17/state-union-resharper-c-7-vb-net-15-support/), [ReSharper, .NET Core and unit testing](https://blog.jetbrains.com/dotnet/2017/02/21/resharper-net-core-unit-testing/)
&gt; get-childitem -include *.csproj -recurse | foreach ($) {Invoke-Expression -Command "dotnet restore $($.fullname)"} $ : The term '$' is not recognized as the name of a cmdlet, function, script file, or operable program. Check the spelling of the name, or if a path was included, verify that the path is correct and try again. What am I missing here?
My guess is they are supposed to be $_
&gt; $_ Markdown ate the code.
Call me crazy, but don't `dotnet restore` and `dotnet build` automatically recursively search and do their thing? Or do you have to have an `.sln` with the new bits?
I think you will need .sln with that. I tried the commands at root and it simply says "the current working directory does not contain a project or solution file".
I would recommend creating an MVC Core project but targeting .NET 4.X; it's the best of both worlds! There's no reason to target Core unless you need cross-platform capabilities.
Update: -"Insert Comment" was removed from Code Editor context menu. It inserts an XML comment template so your comments show up when you highlight your methods. I was able to restore this by customizing the commands in that menu. 
I know this may not be a popular opinion, but I would advocate to go for .Net Core. A couple things may me advise for this given your situation * This seems to be a smaller project, the bloat of full MVC5 project has it's very valid place for larger projects, but this doesn't seem to be it. * The tooling is where .Net Core is kind of behind, but with a barebones approach, this shouldn't be a big deal for you. And as someone who is getting into this space, that means life only gets better * I have had a good experience with core, especially around the flexibility of deployment. Forcing yourself to code to the .Net Standard also opens up a lot of code reuse. Thats my 2 cents, but honestly no matter what you do you will be in good shape. The common question of full .Net vs Core really comes down to personal preference 99% of the time.
Microservices are services with a 'micro' prefix.
Agree, I can't see any compelling reason to target 4x if you're using Core.
**EDIT: Figured it out, sorta, I should have read the README closer. I had an outdated .NET Core SDK. I'm able to run `dotnet restore` now, `dotnet run` gives me a list of errors but I'll look into it. Leaving this here in case others see it and find it helpful.** Forgive for I'm fairly new to .NET Core, but I'm trying to run these projects in Linux and I'm getting errors due to lack of project.json files: (trusty)ryan@localhost:~/Projects/practicalAspDotNetCore/practical-aspnetcore/hello-world-with-reload$ dotnet restore warn : The folder '/home/ryan/Projects/practicalAspDotNetCore/practical-aspnetcore/hello-world-with-reload' does not contain a project to restore. (trusty)ryan@localhost:~/Projects/practicalAspDotNetCore/practical-aspnetcore/hello-world-with-reload$ dotnet watch run No executable found matching command "dotnet-watch" (trusty)ryan@localhost:~/Projects/practicalAspDotNetCore/practical-aspnetcore/hello-world-with-reload$ dotnet run The current project is not valid because of the following errors: /home/ryan/Projects/practicalAspDotNetCore/practical-aspnetcore/hello-world-with-reload(1,0): error DOTNET1017: Project file does not exist '/home/ryan/Projects/practicalAspDotNetCore /practical-aspnetcore/hello-world-with-reload/project.json'. Running dotnet --info gives me this, if it helps: (trusty)ryan@localhost:~/Projects/practicalAspDotNetCore/practical-aspnetcore/hello-world-with-reload$ dotnet --info .NET Command Line Tools (1.0.0-preview2-1-003177) Product Information: Version: 1.0.0-preview2-1-003177 Commit SHA-1 hash: a2df9c2576 Runtime Environment: OS Name: ubuntu OS Version: 14.04 OS Platform: Linux RID: ubuntu.14.04-x64 Is there a known way to run these projects that are using csproj files on Linux, or has this made them Windows only? (Edits are from me messing with markdown)
Unless you have, or foresee, a particular reason to use Core, I'd stick with 4.x They're just as easy to use, but Core is still not quite as full-fledged as "regular" ASP.NET Having used both, there really isn't that much difference between the two: You can make the switch very easily in future, and migrating the project is easy enough and only getting easier: especially for MVC/WebAPI stuff. Although I'd note that MVC 5's advantage you list: "Has MVC architecture in place for you to build your project" doesn't really apply - creating an MVC project in Core is simple too. Quite simply, it doesn't matter. If you want to learn Core, use Core. If you want a little more stability (in terms of the projects you use, rather than whether your program crashes) and variety in the available libraries/toolings etc, go for MVC 5
A couple of weeks ago I'd have said go with MVC5. I was an early adopter of Core and had to give up as I found it a moving target. However, it does now seem to be settling down and the tooling has improved - including for us Mac users. I'm now finally looking at porting my stuff over to Core. A couple of caveats ... Others have suggested MVC5 bloat as a reason to go with Core but the agile nature of Core can be oversold. If you're writing a serious business app with all the bells and whistles (controllers, authentication, db access, IoC etc) by the time you've added all the libraries you'll need you won't notice much difference. Core is good for keeping your app size down if you're doing a SPA or simple API. Web Forms are not supported with Core - if you want to run Web Forms alongside MVC then stick with MVC5.
I remember using the delete datasource files fix for path length. I never heard of anyone using the datasource files for anything.
So independently built utility classes? 
This works, thank you! For added clarity for others, you should also change `&lt;ItemGroup Condition=" '$(TargetFramework)' == 'net461' "&gt;` to use `netcoreapp1.1` as well
Thanks. The condition can be removed altogether in most samples. edit: I just removed all the conditionals 
These days it seems Core by default is paired with Angular 2. Nice to see it paired with the React platform once in a while. 
Sounds like you got it sorted but note you're in preview 2 tooling, not the 1.0 final.
It has issues opening solution files from VSS. I have had it hang on me a number of times when running projects. 
If you're doing a REST API, go with Core. Web API seems ok with Core from my dabbling.
You likely need to update your build server to use the updated version of MSBuild which can be found in C:\Program Files (x86)\MSBuild\14.0\Bin
There is very little content or information included in this post.
Well so I wouldn't be opposed to using 4.6. Do I need to use a new version of MSBuild? 
I think if you have 4.6.1 then you can run applications that target 4.5.2 without an issue. Have you tried running it? 
I think the idea is to physically separate logical pieces out. Instead of having a massive API that has shopping cart, account management, reporting, payments, and so on and so forth, all of these are separated out. You have a separate API for each of these "services". Since they're one logical piece and they support one logical business function, they're a "microservice". I'm closely watching this [repo][1] with how things are structured. A prominent software architect, Juval Lowy of iDesign, has previously said that a micro service is a single class' functionality. Although now he's saying that a microservice should be a single function! I think that the .Net community is still trying to find out how microservices are defined and used in .NET. Other language communities (Javascript, Python) seem to have it figured out. I'm just having problems visioning the infrastructure concerns of using microservices (service discovery, communication, external vs internal, etc.) [1]:https://github.com/dotnet/eShopOnContainers 
So we are using Teamcity and it seems to fail at pointing at the version, or finding the version. Do I need to spoof 4.5.2 into point at the v4 framework ? 
I agree 100%. I just hope that some way we can get C# to compile to WebAssembly then it can be the best of *all* the platforms.
That could be a Teamcity issue then? You certainly won't be able to install 4.5.2 unless you remove 4.6.1. You might want to see if there is a patch for Teamcity to fix it.
Going to try this - http://www.reddit.com/r/dotnet/comments/5z92n8/so_our_it_department_pushed_out_461_to_our_build/dewh0no
I can't ever remember if it's parentheses or brackets first. I just do it like I do on StackOverflow since it's all Markdown anyways. ;)
The nuget support for .net core cross platform isn't great at the moment. ...but realistically, your alternatives are JVM based or C++. For cross platform library support, I would venture that a JVM based solution is probably a better fit at this point. 
Been using it since RC and grabbed the completed version when it released. I've had no troubles.
Why is Node.js not great? Edit: I guess it's cause you want a binary.
You can run. But you need the exact version of the Targeting package to be able to build it. Funny thing is that the 4.5[.*] packages refuse to install on Windows Server 2016, so I "xcopy deployed" (aka ctrl+c, ctrl+v) the folders I had on my pc. Restarted the TC agent and got it to see the packages!
For smaller projects, yes, go all the way ASP.NET MVC Core.
&gt;The nuget support for .net core cross platform isn't great at the moment. This bothered me a lot for a while, not wanting to "limit" myself to a "dying" framework (net462/standard net) or at the very least not the "new" and evolving netcore and netstandard. I've gotten used to the fallback now though, for the time being, until everyone catches up or new alternatives are developed, it seems to work fine. I have yet to find a situation where using the fallback framework is a hindrance (though, to be fair, I don't do a lot of cross platform projects)
I've been dabbling in Go and Node but after almost completely moving to the JVM recently I've decided to stick to C# .NET Core is very promising and they're still working on Native complication (a dev for it got mad at me on twitter for thinking they weren't) and C# is evolving forward to get a lot of functional features. Nuget and msbuild are kind of ugly but they will improve. They have to. The JVM has even more baggage than dotnet and newer languages like Kotlin are neat but still need to live in that ecosystem. Elixir is still too much of a toy without a big enough ecosystem for libraries for me. If .NET Core didn't exist I would have jumped ship already but Microsoft is really stepping up their commitment to open source and cross platform. It's working. It's just slow as they're trying to drag a lot of the old full framework ecosystem with it. Net standard 2.0 will fix a lot of that and so enterprises can join the cross platform party much easier. You should always try new things and fresh ideas but C# is still my core language/platform as long as the current direction holds and delivers.
I am so looking forward to Silverlight The Next Generation.
Those points are the only concerns I can think of: - UWP lacks features that WPF has - Xamarin.Forms (Android) performance and its long build times. - The eco-system needs a little bit more support/involvement by the community. But other than that I agree. The .NET framework is awesome and in a good spot right now. This will even get better with .NET Standard 2.0. 
&gt; Native complication Complication is right. I shudder to think of all the annotations I would need to account for my liberal use of reflection. 
We tried to port my .NET ORM to Java, but we gave up in the end because we couldn't get past the type erasure problem. 
Haha oops. I imagine it is really complicated. I remember they use an xml file to assist in the native usage of reflection. I think things that emit code at runtime are the complicated things while reflection is mostly solved since it's known at compile time. Of course I don't know for sure.
... And I've also just proven that it's possible to reference other projects within a solution. kbyethxforplaying
MVC5 isn't really much harder than webforms. Although webforms is kept on for .NET full, the principles of how it works is different than everything else on the web. It is easy to get a webform up and running, but the details needed to make it work in all cases might be as complex as MVC5's routing stuff and the like. I'd advice to go for MVC5 and do the tutorials on Microsoft's site. When you understand the basics of how MVC5 works and how a browser interacts with the website, you can then decide to stay with MVC5 or move to asp.net core (which is still not mature enough, mind that)
Changing MSBuild is a big fucking deal, a much bigger deal than throwing away the project.json format.
I think C# wipes the floor with Java in terms of being a nice language, but I don't think JVM tuning is a particularly good reason to avoid the Java family of languages. If you want stability and a rich ecosystem, JVM is really the way to go. If you can tolerate some growing pains and a less mature ecosystem, I would go with .NET Core.
Nice copy paste.
Eh, just consider it a submission of 3 links at once :) I know though that the first link has already been posted ~1 month ago to /r/dotnet.
For me: * Ajdust namespace (and refactoring namespace) * Adjust extensions methods (inlcude their namespace) * Better generate constructor (alt+ins) * Better create new file (ctrl+alt+ins) * Better resharper configurations (disable code analysis per folder etc) 
Not enough in my experience. The features that are similar aren't implemented as nicely. 
Aren't the binaries from Microsoft signed? 
Yup sorry flipped it, I think I found the issue. Looked like we used the wrong .net installer for the situation. We installed the all systems 4.5.2 instead of the 4.5.2 developer pack 
Refactoring Essentials is a great free ReSharper alternative btw.
Com'on, is it really *that* bad that I've posted this?
Haskell fits most of what you need pretty well. Though how "easy to become productive" in it probably depends a lot on your background. 
I have no idea. I've been using ReSharper for too long now.
What /u/Otis_Inf has said is pretty correct, there is nothing foundational you will get better at by doing webforms first. This would be like saying you should learn to type on a typewriter before using a keyboard so you get better typing foundations; it just doesn't work that way. The sooner you get up to speed with MVC the better, it is the accepted format for how projects are conceptually separated on almost all platforms, not just .Net. To the point of it being "easier" I would also disgaree really, webforms doesn't add any niceties I can think of over MVC. My transition from MVC to webforms was extremely painless.
Anyone know a free alternative just for generating constructor from fields? Writing dependency injection boilerplate by hand is a pain.
&gt; Java/Scala/Kotlin and more: all run on the JVM, tends to require tuning of the JVM to really "get right" - tend to shy away from these options because of that I don't know about "require"... do you have any comparisons that indicate the JVM requires more fiddling to get to the same performance as .NET VM?
I would love an example of this. Because from my experience, LINQ queries that are 10 lines are *way* easier to read than if you compressed it to two lines.
MVC will be more relevant than Webform simply because MVC as a paradigm isn't tied to C#. The syntax will definitely be different, but the concepts you'll learn by doing ASP.NET MVC will transfer over to, say, learning Spring in Java (although the documentation is a bit over the head, even for me), Ruby on Rails or even Angular.
Note you need RC4 or RTM for this to work.
Everything you just mentioned is in VS 2017. Some of them need to be turned on (e.g. Code Analysis and some of the hints), but it is all there.
Does VS use the angular-cli to compile the angular 2 code? Cuz you really want the angular-cli to build the app in order to get all the optimizations they provide.
Before: var order = 1; foreach (var job in from job in serverQueue.Value.Where(x=&gt;x.JobType==jobType.description) let isRunning = Servers.Single(x =&gt; x.Name == serverQueue.Key).Databases.Single(x =&gt; x.Name == job.Database).JobRunning[job.JobType] where !isRunning select job) { redisDb.StringSet("QueueSlot:" + jobType.description + ":" + serverQueue.Key + "$" + job.Database, order); order++; } After: var order = 1; foreach (var x1 in serverQueue.Value) { if (x1.JobType == jobType.description) { bool isRunning = Servers.Single(x =&gt; x.Name == serverQueue.Key).Databases.Single(x =&gt; x.Name == x1.Database).JobRunning[x1.JobType]; if (!isRunning) { redisDb.StringSet("QueueSlot:" + jobType.description + ":" + serverQueue.Key + "$" + x1.Database, order); order++; } } } 
That's the opposite of what you said and is what I would expect. It took a few lines and expanded out to *more* lines to make it more readable.
I abuse this
Please tell me they've fixed the shitty [file / symbol navigator](http://imgur.com/a/2Cw77) as well. It's ALWAYS the 4th or 5th hit I want. Never the first 
Now that ReSharper uses Roslyn, perhaps Microsoft should buy ReSharper off JetBrains. 
I agree. I use the same technique.
newbie here. Can someone tell me what the real world applications are for something like this? Is it simply just so that you can have a bunch of things subscribed to this thing to listen for events? Can someone point me to a link to understand more about this?
You're thinking of CodeRush.
They updated the tools for 2015? Why is there no download for them on the official site? I'm still running preview 2 tools...
Anyone who selects text using the mouse or shift + arrows is an animal.
I'd also note: C#7 *does not* work in Resharper currently.
Quick background: the Mono people implemented most of ReSharper into MonoDevelop, and it's been backported to Visual Studio as the extension "refactoring essentials" the above poster mentions.
It'll slow down Visual Studio so much that you'll have a compelling argument to management for a new dev box.
Ctrl-click to implantation of method. "refactoring" tools. I'm still waiting for a good move tool that foes a namespace update and update all references. (NetBeans does it for java and coming from that and not having it drives me nuts)
No it's not. I found this useful. 
Or it will outright cause visual studio to crash. For an IDE that is maligned for being overweight it seems crazy to add even more weight to it. To each their own. I kicked it to the curb two years ago and haven't looked back.
You're just grabbing on to anything in every reply, that you feel you can counter, even though in the same reply, I already explain that it's not even necessary. This time, you're just plain wrong. Didn't change MSBuild? First of all, the very fact that the new csproj is a subset of the old means it HAS changed. Second of all, if it hadn't been changed, I should be able to migrate the old csproj to the new, right?!? Well, you can't. Not with all projects. There are, in fact, still plenty of old VS 2015 csproj based projects that you can't migrate, including UWP, WPF, Xamarin and ASP.NET 4. You can still work with them, sure, but that's besides this specific point. MSBuild HAS been changed to accommodate the new, leaner csproj.
Doesn't Visual Studio have unused detection nowadays? 
And I keep telling you it's not legacy. That's the whole god damned point. If you included everything in project.json it wouldn't be nice and easy to read. It's missing a whole bunch of crap. 
Well, *angular* and its dependencies are managed via *npm*. Actually, there's a command `npm install` at step 4. Could you please be a bit more specific?
Thank you for your advice, I think I will follow it!
Thanks for the heads-up! I think you are right. Will give MVC a look over Webforms. Thinking also of coding a ASP.NET Web Api and using an Angular front end to handle text inputs and the like.
It could ask me. A setting: What kind of results do you want to see in the navigation field - User-coded classes? - Generated classes? - Files? - Methods? - Fields? I'd just tick off files and be happy. 
1- In my experience of Government work... you'll be lucky to even be able to use MVC and ASP.NET 4.5/4.6. Pushing to try to use Core would be a total waste of time trying to force the bureaucracy and red tape through. A lot of government employees are long tenure and WebForms is more likely to be used. Moving to MVC isn't too big a challenge because it works on the same installed base and you can just start new projects in MVC and run them together: but persuading IT/Governance departments to use Core is a bit messy. They (probably rightfully) argue that there's little to gain from the switch. Even if working for a 3rd party agency, they're not going to complain if you can comfortably use either, but they'll be *looking for* competency in regular ASP.NET. Government and large companies are interested in the proven, comfortable technology: if you want to use the latest and greatest tech, go for the start-ups and agile little companies (who, in 10 years time, will be the ones stuck in their ways with what used to be the latest and greatest, as new start-ups come along) 2- Learn regular .NET - honestly the difference between regular .NET and Core isn't that huge, and right now using Core tends to be easier if you have a solid grounding in .NET anyway. Core has some interesting prospects, but unless you have a specific reason to use it, don't.
Wow...mind is blown, didn't know that feature
MSBuild is not the old platform. MSBuild is how .Net does CI and how they plan on doing it moving forward. That's why they did what they did. If you think making the new files work with MSBuild is easy, write the code. MSbuild is open source. If it's just a parser, write it.
I have argued for this for a while, but honestly it no longer makes (Intelli)sense. Yes, there are a few critical day-to-day features that are missing (see this thread), but I am now seriously considering whether to renew my subscription once it's up.
Instead of executing a query every second look into "Detecting Changes with SqlDependency" and "Query Notifications in SQL Server" SQL server will let you know when a change happened.
Ya, I've copied in angular-cli projects but have had a hell of a time getting the VS build/debug pipeline to integrate with the angular-cli build/debug pipeline.
Thanks. So it's the Result Execution step that's taking all that time which is where it finds and render the view. I've looked into it and rendering the view includes compiling the view. In this step collections get enumerated which can results in queries and delays which isn't necessarily what I am observing. It seems that the rendering and compiling is much slower than the rest of the process. I guess to know what happens precisely at that stage I would have to dig in into the code.
If it does, it's definitely not as readily visible and helpful. Resharper highlights unused code, and offers you all kinds of file, project, and solution-wide options for fixing things. Like getting rid of all unnecessary using statements, unused variables, etc. If this exists in VS2016 I would love to know where.
&gt; Looks like this approach is working for me. Thanks Guys. That's how I'd suggest as the fastest way if you just want to grab a small amount of data occasionally If you're relying on pulling more data than that, though, there are probably better options
MSBuild IS the old platform (in the context we're talking at least). It was the build platform before .NET Core was conceived, ASP.NET Core was supposed to be the "new .NET" and they figured now was as good a time as any to change things up, so they made the project.json. It wasn't until RC versions that they really realized the intense issues with MSBuild compatibility and the work involved in legacy migrations for other project types. MSBuild was before, project.json was supposed to be the new. That makes it the OLD and the NEW build platforms. Of course, now that they went BACK to MSBuild, it's not the old or new *anymore*, but like I said. In the context of xml/json it was the old. &gt; If you think making the new files work with MSBuild is easy, write the code. MSbuild is open source. If it's just a parser, write it. a) I never actually said it was easy. I said it was a shame they didn't invest resources into it. b) It's not *just* a parser (obviously) - I mention this multiple places as well : &gt; Building a parser and changing things around in MSBuild (and yeah, I've also said "just a parser", and I apologize, that's oversimplifying the work involved). MSBuild isn't exactly simple (which is another argument for change). But changing MSBuild through the github isn't possible. That's not how it works. You should consider the repo as much more of a vSafe - they're not going to make large sweeping changes through their github repo. It's not for trying new things. And incidentally, there are [two](https://github.com/Microsoft/msbuild/issues/16) [issues](https://github.com/Microsoft/msbuild/issues/613) on the repo right now, starting from all the way back to march 2015 and still active as of 7 days ago to improve and modernize MSBuild to either support JSON directly or, like I talked about in one of my first comments, build an agnostic, provider-ish config consumption system. If you think this is just some stupid retarded idea I have in my head, and only I in the whole world wants this and thinks it a good idea, you're just plain wrong. David Fowler even said he thought project.json was his favourite feature of ASP.NET Core when it was conceived.
What does this give you that the precompile publishing option doesn't?
This article (https://stacktoheap.com/blog/2013/01/19/precompiling-razor-views-in-asp-dot-net-mvc-3/) does a good job of explaining it, I think.
its more the principle; why should I spend my own money because my company doesn't care to buy it to help us?
I use the same all over. 
I can see that, but you can do more outside work with your own licence as well.
App Engine isn't a FaaS offering though: it's just a standard web app runtime (a la Azure App Service)
I don't have access to the SQL Server this is running on. I only have read only access to it. 
Right, I get the benefits of precompile. What I'm saying is, there's a precompile option built into VS that you can use when you publish. No extra packages needed. 
The controller still allows dependency injection so you would pass requests on to a service/repository like you normally would in a Web API controller. 
My favorite Resharper features are contextual selection (ctrl + w) and navigation fuzzy matching (jump between files with ease and speed). There are others but these contribute the most to my efficiency. One downside of Resharper is that it does impact performance on large code bases.
I just rolled out a static site on App Engine two days agowhich was using the python runtime. I also looked at the docs for the Node runtime as I had originally thought the site I was putting up was an Express app (I didn't develop it), both only required a yaml file to run. Not docker. You shouldn't need a docker file for this. Containerized apps can be deployed to Google's Container cluster which uses kubernetes to do load balancing and spreading out deployments across servers in the cluster, restarting apps if they die, and rolling deployments so that if you have three instances of the app running, it takes down one at a time and bring it up with the updated app. 
What does NancyFX bring to the already minimalistic asp.net core? 
I use azure for messing about as I have free credit. But if I want to something semi serious (I.e. Do it properly) I stick my dot eat core stuff in a container and shove it on heroku. They have a free tier and their entry level paid plan is only $7. Also, 12 factor!
Sorry, I meant https://appharbor.com/
All right, thank you.
[Amazon Lightsail](https://amazonlightsail.com/). Fixed costs (unless you use a ton of bandwidth) starting as low as $5/mo. $5 a month: 512 MB Memory, 1 Core Processor, 20 GB SSD Disk, 1 TB Transfer $10 a month: 1 GB Memory, 1 Core Processor, 30 GB SSD Disk, 2 TB Transfer The server itself is Linux (but .NET Core works fine) and you can hook up to an Amazon RDS database or your on-prem database running MS SQL Server if you'd like.
Don't go for webforms. It's terrible and it's going away.
http://stackoverflow.com/questions/30691024/updating-existing-data-in-ef-6-throws-exception-entity-of-the-same-type-al That should help
If you do .net at work, you might have an msdn subscription. That comes with monthly azure credit that most developers overlook. It's a great way to fund weekend projects.
Wouldn't SQL Server require an (expensive) license?
I prefer a single up front cost, rather than a PAYG. I'm not gonna be busting any usage limits with this thing, and if it did ever happen to get a Reddit hug of death I'd honestly rather it went offline rather than pay more money, it's not worth anything to me
The lack of an included SQL server kills that for me - I don't have access to a SQL server instance 
That looks like it could be perfect, thanks!
ask for it. or find a reason you need it - visual studio update, etc. microsoft stack developers should have access to msdn at work.
TL;DR: Oracle intends to support ODP.NET, Managed Driver on .NET Core for Windows and Oracle Linux. Oracle will evaluate support for other Linux distros. Oracle will not certify on versions early than .NET Core 2.0. Will not support Entity Framework Core or ASP.NET Core on first release. No plans to certify ODP.NET, Unmanaged Driver on .NET Core.
Honestly, I'm not sure about the licensing for SQL Server for Linux and you could still use the express edition for the hobby project. For the production stuff, I'd go with SQL Azure.
We actually use it for quite a while but it has its disadvantages. For instance, manually building a migration script is a pain if you do not want to install the RedGate tooling on your build server.
What does swagger and services being self-describing have do with anything here? Or did I miss something
Came across this post last night, I enjoyed it so much that I had gone ahead and created my own account and started tinkering with ZEIT myself. I go a tad more in-depth than Scott does, just touch on some random points and things I ran into: https://blog.nandotech.com/post/2017-03-16-zeit-now-deploying-dotnet-core-in-a-docker-bow/
Your example for using the library in GitHub registers the formats again instead of writes log entries. Then use it: L.Register("INFO"); L.Register("ERROR", "A {0} happened: {1}"); fyi
It would be nice if the examples matched. That is, if the second example used the formats that were registered in the first example.
The first one was "FATAL" by mistake. Thanks again.
Sorry to be picky, but now I think your example gets an error because the Log call doesn't match the format (it needs another argument).
I know but i do not know how to turn on a more specific error code yet i will research a way to pass the error code further up
Your post and the comments others have left led me to this review: https://joshtronic.com/2017/02/14/five-dollar-showdown-linode-vs-digitalocean-vs-lightsaild-vs-vultr/ I didn't know these options were available, but thanks to your post and the replies, now I know! Hope it helps :) Edit: Vultr offers a $2.5/mo plan: https://www.vultr.com/pricing/. This might suit your needs?
I more meant step up in scale rather than difficulty: $10/mo just for a SQL server add on means it's already hitting the top end of my sensible pricing while on the free hosting tier: at which point it's lacking in features due to having to choose either a web or background worker, not both. It looks good at the $49/mo level, but that's more than I need now. So yeah, I feel like it'd be a good setup for a revenue generating project but probably isn't right ATM 
It looks good for Linux hosting, but that's no good for .NET - Core was my "nice to have", but being able to host 4.5 or 4.6 is more important 
Azure free tiers
I'm not sure what I did wrong with Azure free tiers, but I repeatedly got charged when I wasn't expecting to: I may have to revisit, though, as it sounds like I missed something important or chose the wrong option somewhere.
I think the wording in my title might be confusing. I have a database setup that I have full control over. There is another database that has information in it that I have readonly access to. I was referring to this database as a **second** database. So I'm not trying to query a database every second. I'm just trying to query from a second database. I'm actually probably only going to be querying it once a day and just pulling about 50 to 100 records from a view.
Yeah I only read their QuickStart at first, and that doesn't mention it - hence me wasting half an hour before realising what was wrong :)
I see. But for hobby projects you could use something like sqlite maybe?
I'm more comfortable with a "proper" database - SQLite is great but I've run into a few issues in write-heavy applications, and the lack of any type safety just makes it a bit of a faff in my experience. I prefer the more structured environment. I do use SQLite sometimes, particularly for small databases, where I want something with a little more grunt than flat file, but part of the reason I undertake hobby projects is to practice or develop skills for work (a large part of the reason I've migrated from Python/PHP/MySQL at all). MySQL would be an option if I had to, but I'd rather try to use MSSQL if I can. Worst case I guess I'll get a windows VPS and install SQL Server Express, since the 10GB limit isn't likely to be an issue and I only need to connect locally.
Hey, thanks for sharing your project! Just looking at FileWriter.cs here: https://github.com/tallesl/net-L/blob/3.0.1/Library/FileWriter.cs#L44 If there are concurrent writes happening on the file, and `content` is longer than the buffer used internally by `FileStream` (or, I think, if another process manages to write between your `Seek` call and actual write), then you can still get interleaved/corrupted data in the file here. Might not happen often, but if a lot of apps are using the library to do a lot of writes, eventually it'll be hit. I spent a while trying to find a way around this that didn't require a `Mutex`, and stumbled upon `FILE_APPEND_DATA`. It'll guarantee atomic appends to the file even if it's used concurrently, though you have to jump through some hoops to do it. I wrote up what I found here: https://nblumhardt.com/2016/08/atomic-shared-log-file-writes/ - hope it's useful, and good luck with it!
Without support of other linux distros this oracle ado.net connector seems useless, isn't it?.. For Windows-only we already can target net462 and use existing connector. And finally, it's hard to understand why this connector is possible only for .net core 2.0. Just remember old joke: using oracle is almost bad as using java :)))) 
On a plaintext benchmark you will see quite a difference. But the overhead is constant per request it won't be that much on normal requests. Core and asp.net core are still faster. So kestrel+iis will be much faster than normal asp.net+iis. Ref: https://github.com/aspnet/IISIntegration/issues/220
Jesus, I've been extra sloppy on writing that. Fixed (this time I actually ran the code), thank you for being *picky* :)
my bad........sorry...... ignore my comments.... you are on the right path. second as 2 not as a time period....... you got to love ambiguous words.
What micro ORM did you find to be faster than Dapper? I'd like to test them.
see examples here: https://github.com/pythonnet/pythonnet/wiki/Projects-using-pythonnet
&gt; emember: We just got stable RTM tooling 2 weeks it's just a tooling; nothing is changed in netcore runtime/corefx libraries, MVC Core and EF Core. Tooling cannot affect the ability to create ado.net connector :) Netstandard2.0 will bring more classes from full .NET Framework, and that's all. I don't believe that Oracle connector needs something very special in comparing to connector to SQL Server or MySQL. It is clear that Oracle tries to shirk from creating netcore-compatible connector as longer as possible; many people asked about it 2 years ago on Oracle's forums - so demand is proven. 
Nice! And you learned a little about exceptions. Good luck!
yea thank you so very much for that snippet! i think it will be useful. although i THINK i might have sinned. I threw a try at the top of my main sub and a catch at the bottom just to see what breaks. it compiles but i feel dirty for doing it.
Yeah that's generally a bad idea but I think it's fine for you. Exceptions are an important part of .NET (and other programming languages). With .NET it's sometimes hard to know If a certain method will throw an exception but you'll learn over time. If an exception goes uncaught, your program will crash. There are also different kinds of exceptions like OleDbException, FileNotFoundException, OutOfMemoryException, etc. You can use multiple catch statements to handle different types of exceptions. If you're serious about programming it's something you should learn about sooner rather than later. There's plenty online describing different exception handling techniques.
They do not share anything. They both work on the same ObjectNumber but on seperate urls. The ObjectNumber is given by the first call. Can you give a example how to do both calls in parallel. 
Identity 2.0 will easily fulfill your needs without having to retrain and rewrite any kind of interface. Use the built in asp.net user db tables it creates when you start a new MVC5 project and user management will also come built into the site.
You can use Parallel.ForEach https://msdn.microsoft.com/en-us/library/dd460720(v=vs.110).aspx
/u/ben_a_adams ??
How often do you really spend time outside of work though to validate a license? (Not like I'm not considering getting a personal CodeRush because I'm loving it so much at work...)
I've learned to put my README examples into my source as unit tests since this sort of thing happens all too often. Still sometimes a challenge to remember to sync the two when you're editing the README though.
Agreed. Before Community came around, I'd suggest putting up the money. Now though, don't bother.
So, I just did "pip install pythonnet" (on my Anaconda installation), then I was able to "import clr" and "import System" (on IPython prompt!), and do stuff with the classes there. This is some sweet stuff. With IronPython you have to "give up" much of the python ecosystem hygiene, like IPython (and tons of third party packages).
~~Diagnostic Tools is not in Community edition, but if you have never heard of it, you probably don't need it.~~ I stand corrected. As /u/SikhGamer writes below, Diagnostic Tools is available. What I was referring to is IntelliTrace. 
I just checked, and you're right. I stand corrected. The last time I used Community (2013?), it did not have Diagnostic Tools, which pushed me to use my Enterprise licence from work on my personal computer (this is legal, BTW). I missed a few other "Enterprise" features from Community edition at the time. I'll have to try Community again.
But no code lenses 😥 
No, MSDN doesn't work that way. The licenses are mostly for Dev use only and the ones in production are for your use only.
MSDN comes with Dev licenses for a lot of products which can be very useful. Also community is not legal for non open source companies beyond a certain size.
IronPython is good when you need very deep integration with CLR. But the burden of new CPython versions and rich C-API is what keeping alternative Python implementations so much behind. And Jim Hugunin left from MS to Google. Interop is much more light-weight and easier to maintain.
Ha ha ha. No way. That is comedy gold. 
Or open it with 7zip and edit+save from there. 
You can't use those keys in any production setting. And when you become a partner and get internal use licenses, you can't use those for any revenue generating purposes, I.e. host your​ product for people to use. Additionally, if you have 1 msdn sub, that is one developer only. If you have more than one using visual studio, your not licensed properly.
I don't know if BizSpark is still going or not. If you're making less than 1 million a year, you'll get msdn universal for free for 3 years as a startup. 
It has never been a production license. 
That only works if you publish directly from VS; projects that use an external build/CI server need another solution.
Holy crap. 999 parameters? Bad design dude, bad design.
What page are you looking at?
[This one](https://www.visualstudio.com/vs/compare/) Check subscriber benefits.
That looks pretty much ideal, thanks - only potential problem is the limited DB space, since the first app I intend to put on there does quite a lot of logging, but I think I can handle that with the 1GB/€8 plan - I guess if I bust 1GB I need to look at a VPS with MSSQL/Express anyway rather than a shared host. That'll do great to get me started though, thanks :)
https://www.youtube.com/watch?v=i_XwYPBbP_s
&gt;[**02 Intermediate ASP.Net Core 1.0 - Authentication [26:30]**](http://youtu.be/i_XwYPBbP_s) &gt;&gt;Find out how to use dependency injection (DI) to register and resolve application services. &gt; [*^RG ^Edu*](https://www.youtube.com/channel/UCqIRlOkiw6K7Cz8Xc51DNOg) ^in ^Education &gt;*^3,488 ^views ^since ^Dec ^2016* [^bot ^info](/r/youtubefactsbot/wiki/index)
I want my log timestamp format to include milliseconds. How can I change it from HH:mm:ss to HH:mm:ss.fff
https://github.com/lukes/ISO-3166-Countries-with-Regional-Codes You could use something like this to save you writing it all out
Ah, it must depend on what capabilities the extension uses, as mine flat out refused to install until I migrated them properly.
Does it work on 2017 as well? Or is it already included? I haven't really had time to play with the community version at home.
Generally speaking I think it would benefit from a better title which makes it clear it's an introduction to a future post, or even just something like "The justification behind moving to Azure Functions from Windows Services". When posting here on Reddit, I think a brief summary on what your link is would be helpful for setting expectations as well, but it's only really necessary if the title is ambiguous. 
Would you write the summary in the description or did I miss some other field when posting the link?
You didn't miss anything, you only get a 'title' when posting a link. For link based threads, I personally jump into the comments first and take a look for something from the OP but I understand not everyone will do this.
Thanks. Will start adding comments when posting links for sure :)
Thanks for the tips, I'll try that. Never thought of AJAX until now actually!
Thanks a lot!
Depending on the concurrent connections Azure would be cheap. 
Awesome - look forward to it!
Hmm, right now you can't. That's a good thing to allow the library user specify, took note to change it in the next release.
I've been down this road, and I've concluded that I've essentially reinvented the function. An ICommand with a single execute method is just about equivalent to an Action delegate, except now you have to have each one of your functions in a separate class. In any non-trivial application, your Commands are going to be in some way or other dependent on each other, and you're going to end up with huge constructors where you pull in all the dependent commands. You might conclude that a lot of the time, most of these commands are somehow related. Wouldn't it be nice if you could just put several related methods in a single class?... The only time where I found a system like this useful, is if you want to let the user invoke arbitrary commands through some interface. This is often used in UIs to make toolbars and such. In this case usually the ICommand has other properties like Name and Enabled, and you have a list of available ICommands somewhere, which represent the actions that the user can invoke. In general, you should suspect yourself if you start thinking too hard about abstracting basic principles. More often than not, you're just noticing a pattern that already exists, and are trying to explicitly describe it. You should aim to use the constructs in your language to model things in your domain, rather than meta-modelling a language inside your language. If that makes any sense.
Look into maria db options. Both Amazon and Microsoft have services to support this. Depending on the amount of traffic amazon will be considerably cheaper for a total hosting service. I would recommend using something not sql server unless you have to or you already have a server running. It's a beast that is super overkill for a small project.
Agree. I've just opened a Digital Ocean account - they seem to be going after the non enterprise customers who Azure and Amazon seem less interested in.
What are the requirements? Read only? Read/write? How many concurrent users in read? How many in write? What are you writing? Are you accepting payment online? Are you using a product or are you building custom? Each of those answers will change my response. When you go cloud, you can go "standard" with databases and websites but it can cost more than using the right tool for the job. 
.Net Core, Asp .Net Core and Entity Framework Core
Hi Dermot, Assuming that you are using something like VSTS for source control you can use the Build/Release features they are on par if not better than the Octopus Deploy/Team City features as they provide stronger integration and reporting options. Happy to go into detail if this interests you? Alternatively looking into any Build/Release talks on MS Virtual Academy or MSDN Channel 9 will give you the same outcome :) Full disclosure i work for Microsoft so i am fairly biased towards our stack :P 
Forget web forms. It's starting to become an ancient technology and unsupported in a sense. We use it at my work and I'm ready to venture of to MVC. But I'm in the same boat as you, so many directions and no one to direct me. But as I said MVC is my next stop of progression. 
You may want to check out firebase. They have recently added functions and have free plan.
So in pull the data, now how to do the recursive full name build. Everything I try ends in an infinite loop or terrible performance. 
Could someone explain/link me to an explanation of the benefits of async/await in ASP.NET MVC? Isn't the pipeline, well, a pipeline? So it can't continue with the request until your filter is done. The only potential benefit I can see is to do with releasing threads, but isn't this crazy premature optimization for that?
Ok, so form your history it looks like you are trying really hard with this. Here are some pointers: * That while has no break (exit) condition * An elegant way to do this (for now) is to use a second list of location in which you put all the new locations * If dealing with this in a recursive matter is must you need to build a method, something like 'BuildLocationName' that calls herself. * Yo get the data already 'prepared' you could use something whatr is called a 'Store Pocedure'. It looks like you are tying to learn here, so re-iterating through conditionals and linq extension methos, whould be in great help. 
That is the thing, you are literally doing almost no work to garantee higher throughput. Simply add the async and return task T and you're done, so you might as well do so. Imagine a heavy enterprise system that has a lot of IO operations that may take a while. If the system is not asynchronous then your thread pool for handling requests may be blocked by operations that they really do not have to waste time on. They could be handling other requests instead of being blocked waiting for some PowerShell script executing or a long running SQL statement for a report. In other words, if it takes 0.2 seconds to handle a normal request A, and some blocking request B has to wait additionl for 1 seconds for IO then you could have made B asynchronous and performed 5 instances of A before returning back to finishing B. Instead B blocks, and the A instances just have to wait for their turn. It is simply a good practice and greener if you'd like.
Each location eventually has a ParentID of 0, so that would end the while statement. I can get a second list of location, or more specifically, I can make an easily navigable list by using the SQL query above. A right join on location to location gives me a list of locations with the parents. It should then be possible to iterate through them to determine the full name. Just has a Big O problem. I have 2 issues in that approach (not saying I am doing it right) Using a separate method: How do I use the raw SQL item to execute the query, iterate through the rows and develop my dictionary or data structure for this? AKA: Run the SQL i listed above, make a dictionary and iterate through that to build full names and print the list Option 2: What is the proper way to do this, and how do I use a stored procedure to make this data set. Never done that and cant find a good primer on a self join table. 
&gt; For sure, its certainly a balance trying to keep classes lean but also not have them bleed in scope. This is something I have wrestled with a bit; I'm sure more than a few other folks have as well. The situation I've ran into a lot in increasingly larger systems is you create one larger class (Foo) to house some logic where refactoring down further would almost seem pretentious and then a separate process now needs to use a tiny piece of that Foo logic but it requires setting up and tearing down the whole class just to use maybe one method (e.g. I make the user create/update commands but now I need to write an import to borrow part of the validation policy verify users.). Now if I want to refactor that Foo class I need to re-tool original controller, write the new one, and update the existing unit tests. I think structuring an application it's all about managing dependencies, and minimizing what each piece of code actually needs to have access to in order to do its job. If you have a piece of logic "Bar" in the class Foo, which is presumably used in some other method of Foo for something, and you also want to use it from somewhere else without creating a whole Foo, then you should look at what Foo is providing to Bar, and how Bar depends on the environment that Foo sets up. Is this a part that could be extracted into a lower layer, with less dependencies? Could you place Bar into a different class, and then have Foo take that class as a dependency? In my experience, the best way to structure an application is by separating its different aspects in groups (or layers, however you might call it) of classes along its dependency graph. As an example, here's what one of my usual applications might look like (bottom up): http://i.imgur.com/XMMqD9p.png The idea is that each layer should only depend on classes in itself and on layers above it (and avoid cycles). Depending on your situation, you might take your Bar, and extract it into a class that lives in a layer with less dependencies. Then you can take a dependency on that class from Foo, and from wherever else it's needed. Within this, there's no need to go as far as one class per function. This kind of structure grows organically out of your dependencies and the infrastructure that each layer has to manage. Sometimes you don't need so many layers, sometimes you might need more. It depends on the complexity of your system. &gt; Had me .... then you lost me :). I guess if I'm understanding correctly, I should consider simple leveraging Action delegate and the constructs already provided in .NET versus trying to create my my own generic execution interface? I feel like that would result in even more constructor/parameter bloat when trying to adhere to DI and lead to a bit of confusion. If at all possible, would be really interested in a code example if you can provide one. What I'm saying is, if you take away the ICommand interface, *all of your code will still work*. You don't have any methods that take any ICommand and do something useful with it. Is there any place where you would want to substitute a CreateUserCommand with a DeleteUserCommand? If not, then you're not making any use of the abstraction, and there's no point to it. What you're noticing is the general pattern of what the language is allowing you to do. If you want to put code in a function and call it, just do that. Create classes that represent things, put methods in them that represent interactions, take dependencies on other classes, and just call the methods that you need to call. No need to abstract it away behind a "generic execution interface". *That's already what the language is!*
This looks similar (but different) to my CQS implementation, you can read about my approach and find links to the resources I was influenced by in the [Cofoundry CQS docs](https://github.com/cofoundry-cms/cofoundry/wiki/CQS). There's a bit of a sliding scale of complexity/features between different approaches that really depends on what your needs require. For me these were some of the things that were important: - A strong non-leaky boundary between domain and UI layers that allows for handlers to be optimized or reworked without creating side effects - Ability to apply functionality to all handlers like logging, validation and permissions checks - Ability to use DI to swap out handler implementations - An SRP approach to commands that makes intention clear Some of the downsides with this approach is: - API discoverability isn't as easy as with a single repository/service per entity. I've created wrapper repositories to get around this. - You end up with lots of query/command files. I find this easier to work with (one class per action) but it's not for everyone. - Code sharing is more difficult and relies on creating more shared classes to inject in. Good for modularity, but it does increase the amount of files/classes you have to work with. There's nothing wrong with your approach, although I assume you'd be using DI more in your composition but have left it out. Whether you want to make it more complicated depends on whether you need those extra features.
A database should accept connections only over a private network or VPN. You should not put the database in the cloud and accept connection from any IP in the internet. It's not safe. Does your application have a server back-end? If yes then store them together. 
I'm writing an application for a fitness centre i need the sql on the cloud for debugging purposes because my friend lives far away so I cannot be on site. Probably not alot of concurrent users, probably just 2-3 scanners. But I also want to use this service to build an app and to play around with ASP.net websites 
 &lt;action type="Redirect" url="http://{C:1}{URL}" /&gt;
I would use the word `Invoke` instead of `Execute` do draw parallels to the `Action` delegate type.
Why not ask on the [Serilog chat on Gitter](https://gitter.im/serilog/serilog)?
I would take a good look at Table Storage and blob storage. Unless the website have a login area, you don't even need a login page. Every membership can be handled by a local application (web or desktop). If you need it to be web, you can restrict the IP addresses accessing the application. From what I get, you don't need a database. 
Not really, there might be a way to hack it in but I broke my installation trying!
https://channel9.msdn.com/Series/aspnetmonsters/Episode-17-Structured-Logging-with-Serilog-in-ASPNET-Core
&gt; Is there any way to set up this globally for any error? For ASP.NET, it's pretty much the same as before: use [exception filters](https://docs.microsoft.com/en-us/aspnet/core/mvc/controllers/filters#exception-filters). For a console application, you can still use AppDomain.UnhandledException but will temporarily need to use [this](https://www.nuget.org/packages/System.AppDomain/2.0.11) package to get access to it. AppDomain is coming back, but looks like [it's not quite there yet](https://github.com/dotnet/corefx/pull/11275). &gt; Is there any example out of box The [wiki](https://github.com/serilog/serilog/wiki) is still the go-to for all of your setup questions. There are links on the right side to both a web app and console app. &gt; to log any exception that happens to azure blob storage(devops) Direct [link](https://github.com/serilog/serilog/wiki/Provided-Sinks) to all available sinks. There isn't going to be one for blob storage as that's intended for binaries, but you'll find DocumentDB and Table Storage in there, either of which will work nicely. And if this is for work (or unsecured personal play), there's always [Seq](https://getseq.net/) which works great.
Good question, I honestly don't know the answer. Hopefully someone on here can help you with that. Best answer is you can look at writing your own package for Core. As a developer myself, I want to one day be able to have to rely on other people's code for my projects if possible. 
What package do you need?
Do MVC 5 tutorials on PluralSight. But just know that .Net Core is the future. By the time you get the hang of MVC 5, the .net core tutorials will have matured (many tutorials today have obsolete information due to things changing and I can see that confusing a beginner). Also, anything .net core related should probably use VS2017.
We've been having the worst luck getting xproj and the new json stuff working in our CI environment. For one, I welcome the change back to the csproj files. 
The migration tool is very decent. CI is where a few dragons may be lurking.
Looks like the API is http/json so you could use the API directly rather than with their API. You should also contact them and ask about ETA on a dotnet core version so they know there's a demand for it.
Look into OWIN middleware. https://docs.microsoft.com/en-us/aspnet/aspnet/overview/owin-and-katana/owin-middleware-in-the-iis-integrated-pipeline
Google "Web API 2 pipeline" or "MVC 5 pipeline" and you should find what you're looking for. In Web API you want a `DelegatingHandler`. Can't remember what the same thing is called in MVC, but there is a similar concept (`MessageHandler` maybe?). You'll register those items in your startup class and they will run on every request. 
Are you looking for something more than is at the tutorial below? https://docs.microsoft.com/en-us/aspnet/core/tutorials/first-web-api
I think you pretty much nailed it with this and yes, I left a lot the larger composed objects out for the sake of brevity which seems to have added to some confusion. Certainly a fan of what you've got going with the CQS library though. I wanted to have something that was angling towards the path you've got setup (using a fixture to execute any generic commands, queries, etc.) but felt like it was an unnecessary layer for what we were trying to achieve. To your point, can make it more complicated if I need those extra features. 
Can we meddle with the GPIO pins?
This is the correct answer. And OP, please cache the source... I can't imagine countries changing often enough for you to want to read that from disk every time a client calls your API.
If you need it for work, my suggestioni is get yourself a Pluralsight account, there are plenty of asp net core courses available
If you are staying with WPF, with no plan to eventually migrate to UWP, the easiest would be probably to use effects: https://msdn.microsoft.com/en-us/library/ms743435%28v=vs.110%29.aspx 
I need to do it only once after starting the server. How can I then cache this the best ? 
Yeah, I've seen it mentioned many times. It's a bit pricey for what I can currently afford, though. :(
To make your work a little easier, .NET ships with a CSV parser, FileIO.TextFieldParser. You can read up on some examples here: https://msdn.microsoft.com/en-us/library/cakac7e6.aspx
WPF is fairly undocumented. I had to dump half a year worth of time into learning how to do things effectively in WPF. You'll want to design things in an MVVM way, which may help in your google searches. Read a lot, write a lot of code. Eventually you'll figure out what works and what doesn't. I would suggest staying away from third party libraries that imply they do the "MVVM" for you. They generally don't, and they make your code foreign to other WPF developers. No framework is required to write good WPF applications. As far as EF goes, if you're working alone to learn, I would suggest staying away from it. You will find that something smaller in scope is easier to grok. Something like Dapper will help you out more on your learning journey. If you're able to master normal ADO.NET and a light ORM like Dapper, you're more than able to effectively work with databases in .net. Once you're at that point, EF will make a lot more sense and you'll be able to decide whether or not you want to use it. Personally, I don't like it. It's very heavy and thus a large investment for any project. The important thing, however, is that you make your own choice, just like with any other tool.
You can use Microsoft Dev Essentials ([here](https://www.visualstudio.com/dev-essentials/)) to get a 3 month free trial if you haven't used it yet. That's enough time to get through at least a couple of courses.
Hopefully, but I haven't heard anything myself. I'm bummed because VS2015 Community accidentally got CodeLens for free with an update but now I don't have it in VS2017.
.NET's [Task.Run()](https://msdn.microsoft.com/en-us/library/hh195051\(v=vs.110\).aspx) is about as close as you can get to a "generic execution engine". It takes an Action and runs it asynchronously. If you want to run a function with parameters, you just write an Action that calls the original function with the parameters passed in. To do this with classes, each ICommand would have to receive its arguments through properties in each concrete implementation. It would also keep those fields around after invocation. This may or may not be desired. As it is, it's too generic to be properly useful. It could be useful, if you were to add something more than just Execute to the interface. If you add an identifying Name property, for example, your execution engine could log each invocation, or do a check if the user is allowed to invoke it. 
Nope, MS have confirmed that they are not going to be bringing the new CSPROJ stuff to VS2015
Do you have a link to where this was stated?
I would still suggest looking at Dapper. I don't really know Eloquent. Hopefully you're able to fuss around with multiple options. As far as the MVVM/WPF goes, expect to feel like that for literally months. The worst part about it is how little good WPF code there is out there publicly available. You will find lots of code that feels very amateur, and a large subset of that code will actually end up being bad and not useful to you. I just got finished with a project that has about 200k lines of MVVM WPF for a desktop application, and honestly at the start I was fairly novice. I had to fail repeatedly and throw things out constantly, and only now, on the other side of that project, do I feel confident and proficient in MVVM WPF. It's unfortunate that it is such a clusterfuck of a community because the technology itself is very good. Much better than UWP and Winforms, in my opinion. But the journey to get to that opinion was very arduous.
How awesome of them to completely screw up the project format and then leave an entire generation of Visual Studio with no workable solution.
Doing it as a query is easy, its using the query in ASP that I have an issue with
I'd do it by functionality because then different types of users can use the same controllers. Both admins and regular users would both need to update their profile or submit content, for example.
Thanks for that. Pretty hard to weed out such a crucial piece of information tho, I would've expected them to at least put it in **bold**
Visual Studio plus Visual Studio Code amounts to over 60%.
Try Rick Strahl article at CODE magazine [Getting Down to Business Building an ASP.NET Core API Service](http://www.codemag.com/Article/1701061)
They have a 30 day trial which should be plenty to get up to speed on EF, as at least get you started with the basics of WPF.
Have you looked at the [Pi#](http://pisharp.codeplex.com/) project?
If it prints something along the lines of &gt; System.String[] It means you're printing out an array object. To actually see what's inside array you need to print each item in a loop.
The code at the start is very similar to what you'd write in OrmLite and other micro-orms (I just finished watching a pluralsight course about micro-orms, well worth checking it out). I'm not sure how most of the micro-orms would react to a new field in the db - I like code first more than db first. Ormlite can just create the table for you from your code, not sure how it works on updates. 
I was totally surprised that it wasn't ever Visual Studio. That opened my mind to the possibility that the Microsoft bubble I've lived in for the last 15 years wasn't the biggest bubble out there. Which was also a surprise.
I have been loving my vs code experience with the angular cli. It's been fan fucking tastic. 
FYI, I just changed the name of this post, and posted the next chapter (with lots of code!): https://www.reddit.com/r/dotnet/comments/6106l5/migrating_a_topshelf_consumer_to_a_function/
Being sincere - there probably isn't a packaged solution out there for you. With that said, you will definitely want to look into creating a custom setup, which is something I do on a daily basis. My gut reaction would be to recommend a repository pattern utilizing Identity for user management. This means DI is a must, but I still prefer to roll my own mappings rather than rely on an automapper (that’s just me). Provided you focus on simplicity, reusability, DNR and making processes as clean as possible, it should be relatively simple to roll something like this. Remember a CMS is exactly that – a content management system. You're not dealing with much content here. Well, at the very least most of it is orders. If you're dealing with any content at all, it's more like a bit of an e-commerce system where someone can pick up product and then pay for it. If this site is going to be primarily eCommerce, and very little in-store, then I would probably recommend nopCommerce.
Technically Intellij's various front-ends together add up to 44.6 percent, although some users could have selected multiple, so VS is still probably the most popular.
I am :) Classic Asp, then Asp.net, then Mvc. Javascript from day 1 - how else could you do image hover effects? Never used anything but VS. Apart from Visual Interdev right at the start, and er, notepad to do my first .Net sites for some reason. Never worked anywhere that used anything else. I understand I am in a MS bubble.
Why not use .NET?
Nice, I have a very similar setup. However, I haven't touched WebStorm even though I have the license. Looks like I need to try it out!
I'd do your research first to make sure there isn't something off the shelf you can use. I.e. www.gloriafood.com Then you could also consider other options like Square which probably won't give you this out of the box, but you could integrate with a service like Weebly or even roll your own integration with Square so it manages the products etc. I think you've missed a large part of the complexity which isn't the number of transactions but rather the product catalog. How many items will be offered? How frequently do they change? Is stock level important? Will there be promotions like daily specials? Are some things sold only at certain times? Will people need to be able to customize orders like 'build a sandwich or salad' or will everything be offered / sold as is? If you aren't familiar with it, I'd take a look at Seamless or Grubhub or your regional equivalent. Maybe even place an order off a platform like that if you're not familiar to give you some food for thought (zing!). If your intention is to use a 'CMS' with eComm built in to manage the product side, then you might find your use case actually isn't suitable to a traditional CMS with eComm and you've got to roll your own. While not a cafe / coffee shop, take a look at https://order.sweetgreen.com/dupont/menu for my opinion of online ordering done well. It looks great, is highly functional and easy to use. Final piece of advice - you're going to have to think about hosting. I'd recommend you get familiar with a platform like AWS or Azure and start to utilize some of their service offerings for scaling, high availability, message processing and queues etc. 
You should add the choco package names. Do people still install this stuff by other means?
The microsoft blogs are quite good at this. They focus on the tech itself. https://docs.microsoft.com/en-us/aspnet/core/getting-started
Scott Hanselman has a link to some good videos for Asp.net Core. http://www.hanselman.com/blog/ThreeFREETrainingCoursesOnASPNETCoreFromMicrosoftVirtualAcademy.aspx
Got a couple questions Is Greenshot better than ShareX? Or is it just an alternative? Any advantages/disadvantages between these two? Is there any reason to use WebStorm instead of PhpStorm besides the slightly lower fee?
Here is a nice series of YouTube videos on ASP.NET Core I've used: https://www.youtube.com/playlist?list=PLYMOUCVo86jGwWoSoEkpgnCJ3IPXIQmIC He is up to 50 videos and still going.
I tried it few times, and it still has some problems. The main one I guess is the way it accesses data on your h(s)dd, which is quite slow...
I never used ShareX, so don't really know. I checked the website and it looks like these are just alternative products... I use WebStorm, because I don't do PHP. As far as I know PhpStorm has all the features of WebStorm (hence higher fee) 
I also work on the ssd. Maybe the problems were caused by the big solution (7GB and 80k files) I was trying to run git on. Will give it another try then :) 
Having used both, I prefer Greenshot for its simplicity. I don't need the extras of ShareX (e.g. video clips, additional upload sites), so Greenshot is sufficient. YMMV.
Heh, yeah that would be slightly different than my projects of late. The largest folder's combined size is 700mb and ~5k files :). To be honest my current setup is just using the shell for migrations and managing servers. Everything else is normal Windows apps, including the build which pushes it to a folder for release (and separate git repo). If you want to use the shell for everything you might experience pain points similar to the large solution one that I won't (at least, not unless I change my process).
I know there's a course on [Code School](https://www.codeschool.com/courses/try-asp-net-core).
Can you install Visual Studio with Chocolatey?
You can do JavaScript in Visual Studio. Heck, TypeScript (a popular JS variant that compiles to JavaScript) was created by Microsoft.
Why's that? If you do annotations and such maybe, otherwise I don't quite see what's more professional there.
No LINQPad? Must-have tool for me, these days...
I do since I bought an SSD for my os hard drive and choco only lets you specify install location with the paid version. 
Thanks for reply. nopCommerce seems like a good fit. It's a coffee/food shop in multiple locations with same exact menu. After reading your inputs i'd treat it as a eCommerce-ish rather than a CMS. If you'd like to discuss more, please DM your Skype. Cheers. 
Thanks for your inputs. Had a look at both reference sites. I foresee final product is a blend of both. You are right, complexities you mentioned are very valid. In this case, its a multi-location, same menu. There has to be room for minor customizations based on customer's preference like no cream, less sugar etc., Items are rarely added or removed with promotions once in a month or so. that's pretty much it. For hosting, I go with Azure, have got some experience there. If you'd like to discuss more, please DM your Skype. Cheers. 
No Postman?
No Beyond Compare?
I use the bash shell that comes with Git. It actually works pretty well.
This is extremely vague, nothing about what you have provided will really say that you should do X vs Y but I will give you my general approach that I change based on specific needs. Since you haven't provided anything too specific outside of ~10K transactions per day, take all of this with a grain of salt. More and more I am moving my platform to azure. I would start there personally. Pick a database you like most (MSSQL or DocumentDB) and get your ground work. I would separate out transactions from web, have simple microservices or functions to process transactions so it will be very easy to scale these without a headache. A .net core web app or Azure function would do this nicely. For your presentation layer this could completely depend on what you have to do design wise. Do you have comps? Do you know how many breakpoints? How often will content be updated? Product and pricing management? CMS like Umbraco is popular, Cofoundry is another CMS I have taken a liking to. However an ecommerce solution might be more appropriate if you only need to manage straight transactions. I think your first task should be figuring out what you want to build before trying to to find the tools to do it. Having done several projects like this over the years, it's the most important step. At this point you have promised someone you will build them a place to live, and you don't know if that needs to be a tent or a mansion. If you would like some advice I would be happy to help, but my first impression is that you have a ton of planning to do right now.
I wanna try it over the weekend and come back with my findings...
I prefer Clover and ShareX.
Sadly yes
I have also coded JavaScript in VS - but anyway VS isn't commonly used for web development. 
I thought Selenium Grid is going away. endtest.io is a suggested replacement.
Did you bother Googling? Here's the 5th result for "fuzzy string matching c#" - https://www.dotnetperls.com/levenshtein Edit: also, second result: http://stackoverflow.com/questions/8218553/fuzzy-text-matching-c-sharp And the wiki link from the answer in the SO post: http://en.wikipedia.org/wiki/Levenshtein_distance
This is a really good article it discusses four different algorithms Dice Coefficient Levenshtein Distance Longest Common Subsequence Double Metaphone and comes with a sample code. http://www.tsjensen.com/blog/post/2011/05/27/Four+Functions+For+Finding+Fuzzy+String+Matches+In+C+Extensions 
There is a .net library called Simmetrics, which is a port of a java package of the same name which gives you handy API calls enabling you to perform all the popular fuzzy string matching algos. 
(From a linux point of view). Think of each container as a lighterweight VM. Barring implementation details, they are VMs. They are just not running inside a VM, but they think they are. So each container thinks it's the only container on the machine. And it has to carry everything it needs along with it. I don't know how IIS/Windows work, but in Linux, the container won't even see the webserver. Remember, the container thinks it has the entire machine/OS to itself. The application container exposes ports and the webserver (which may be running in another container itself) connects the application container to the outside world.
Incidentally, we had a 3-day workshop with MS Azure engineers at our office, and they were not really promoting Docker as much as we were expecting from reading the blog posts. When asked why, they said they considered it a fad which they had to add to their catalogue for marketing reasons.
Lol. I mean windows docker is hot garbage, so it makes sense that they'd think that.
These are very different tools. How can I create a new file in VSCode and quickly output to console/debug/create wpf controls on the fly or query a database with only a connection string and objects like LinqPad? I'd love to use one less tool but I think LinqPad has the quick and dirty market all sewn up.
For the love of spongebob, stop saying "it's like a lightweight VM". It's not like a lightweight VM. VMs have completely separate resources, separated by hardware virtualization, e.g. ring -1. Containers share a kernel. VMs have complete copies of their entire OS. Containers only have copies of the blocks they need, separate from the base OS. A container is 1 to n processes. The cgroup in the kernel segregates the processes so that they sees different resources. I react to this because the "it's like a lightweight VM" puts you in the wrong frame of mind when you have to tackle the complexities of container networks and port management. Containers, sharing a kernel and an IP stack, share ports. Inside the container httpd may think its on 443, but really its listening on some random port, then a load balancer elsewhere is mapping public traffic to this random port. If I had a buck for the number of times I've had to walk back "it's like a lightweight VM" in order to explain containers...
I like RiotJS myself because it's simple - I went through the whole API in a couple of evenings. You can get working quickly and don't feel like it's an investment you have to keep. But most people will tell you to use React or Angular.
Were the project files in /mnt/c by any chance? There's a known IO perf issue with accessing files through the mount, something to do with caching. If you copy the files to a native WSL directory it should run a lot faster, although that's obviously not always feasible. Last I heard they're hoping to have a fix in fast ring by late spring.
If you're interested in hosting a web service, you could do it with .NET Core as a Web API and then your front-end could be Electron.
How do you feel it is clunky, and how does it only sort of solve those problems?
[Electron](https://electron.atom.io/)
but you'd need Edge JS to use c#
Dislaimer: I've spent about 30 seconds looking at this, so may be totally wrong. ____ Mono isn't related to .Net Core and won't interact with it, so you can ignore that. A quick google of `IdnMapping` and then `IdnMapping system.runtime` after seeing all results were for globalization, shows up this: https://github.com/dotnet/corefx/issues/13680 although it looks like it's for .Net Standard 2.0, it's possible it's already a change that was made to another version of the framework My working theory is your 'bug' is actually on the Windows PC, where a newer version of the assemblies is hanging around (and being resolved) from somewhere, and your project file is likely pointed at the older versions, or vice-versa it could be falling back on the old version as the new isn't available on the Mac Check your project file(s) is pointing at the latest versions of .Net Core, .Net Standard and any explicitly imported assemblies. If you need to find out what this should look like, create a new solution in VS17 or using `dotnet new`.
We use Knockout. Works really well with WebAPI and JSON.
So, what if this works on my other Mac? Edit: I looked a bit further and the only thing I can see different on that Mac is that it has a reference in Visual Studio for Mac of an older Mono framework, which I thought it may be falling back to. That doesn't appear to be the case, though as they're both running 4.8. I know you said it shouldn't matter, but it appears that YamlDotNot may be needing it, which I'm not sure which of my packages uses. In any case, that Mac seems to work with an identical setup. Very confusing.
[Neutronium](https://github.com/NeutroniumCore/Neutronium) allows you to do exactly that.
Glad you got it sorted!
The main new feature with this update is the introduction of Environment Variables into the CliArguments API. We use this at my workplace, and have found it to be really stable since 1.0 in January!
I had a coworker using them, but he was pulling down the entire dataset into Javascript and filtering client side, which obviously was utterly worthless for even moderate datasets. Since filtering, sorting, and paging involves posting all the filter/sort/page info to the server, is there a good workflow for that with jQuery datatables? Particularly since Entity Framework doesn't play nice with those dynamic filters unless you want to use the beast that is Dynamic LINQ.
That would be a fairly dynamic proc, which would mean the normal query plan optimization would *suck*. SQL is underrated by newbies, but it's composability really does suck.
&gt; You don't really get full isolation from the OS with docker The level of isolation is about as good as it gets without running a full VM. I struggle to think of a scenario where something on the host OS would fault in a way that would ruin your container without ruining the whole machine. &gt; it has dependencies on the underlying system that a VM does not have, and if you want to be certain your container will behave the same way you need a lot of constraints on the underlying OS. I think this is false. "A lot" of dependencies is really just the kernel. If you feel that is a lot, I don't know what you would think a little is. I am also not sure what constraints you have to have on the host OS. Every server and work machine I know of running docker has not had to change anything due to dependencies. &gt; The eventual end goal of containers is operating system as a service, but that's not what docker gives you. Really shouldn't think of it as a "system." It is just a service in a container, when we think system we think platform for other services, and that would be the host OS only. 
I like c#, LINQ, good documentation, the nicely designed API's and Visual Studio. I don't like EF6 first time context initialization time and debugging start up time.
The basic issue is that you don't actually have a separate kernel, or separate memory, or even separate disk. Absolutely everything is being emulated and quite often somewhat poorly. If you change docker versions or the host you're running on, you may or may not get the same results.
I have actually done those things, and it is that easy! The containers are built to be portable. Though there is potential for a different version of docker losing compatibility with a container (hasn't happened to me yet) you can move that container ANYWHERE as long as the kernel is the same. Changing hosts seemless is exactly what it was built for. It runs as a contained service. Your complaint seems to be that it is not a VM. I am very curious as to what you have been using docker for if not these precise things. 
Some new garbage collection stuff though https://blogs.msdn.microsoft.com/alphageek/2017/01/24/significant-garbage-collector-changes-in-net-4-6-2/
 angular 1 or 2?
How would you manage the app lifecycle then? Topshelf?
Thanks! DocFX was an interesting one. Their own docs are actually fairly complete, when you run the build it outputs the static site and (optionally) runs a HTTP server: `docfx --serve`. However their default template leaves a lot to be desired and has a lot of noise. I actually exported the template, deleted most of the content, and customised the site fairly heavily. You can see the whole site and build process [here](https://github.com/Nick-Lucas/EntryPoint/tree/master/docs-generation), so if you checkout and install DocFX you should just be able to run the build script and take a look at the workings of it. I don't run any commands manually, it's all scripted :) 
&gt;4.6.1 fully implements .Net Standard 2.0 while 4.6.2 doesn't What kind of logic lead to that?
I hate how after 2 years of it existing no one really knows the proper way of just writing some code that will run on it. Add .NET Standard to that too, the tooling is a mess. I'll give it another 2 years ffs.
Sorry meant vs code EDIT: &gt; can Vim really contend with the navigation, intellisense, and typing information that VS provides? Definitely not vim out of the box. I just dislike how core, although open source, is still married heavily to a certain IDE. Java and possibly Python are languages that can benefit from navigation, intellisense, and type info as you've mentioned, all of which I have been able to do without using basic text editors other than Eclipse/ VS. 
I have had great experiences with Aurelia it's small, modular and fast. I haven't given angular2 a fair shake yet but only because Aurelia makes it hard to want to use it further. 
Typescript isn't about syntax. It's about features: features that JavaScript doesn't have at all (compile time type safety and interfaces), and features that it doesn't have without creating a mess (modular components).
To be fair, it took them until earlier this month to get the final version of the .NET Core tooling out.
https://code.visualstudio.com/
https://code.visualstudio.com/
Wow. Where exactly is an interface compiled down to JavaScript? Where exactly does any IDE warn me that the method I just passed a string to only accepts numbers? Of course it stops once it's transpiled. That's the point! I don't need to argue with a JavaScript fanboy. You can harp all you want about ES2015, but it's still not supported anywhere, and once it is: *TypeScript still has more features* Or should I preface all of this with "You do realize..."?
Not really a replacement for VS - nice text editor though
The biggest issue we have is that there are too many tutorials these days that don't actually teach you anything useful. Sure, it's nice to know the 'basics' of adding paging/sorting/filtering where you have to do it column by column, but there's barely any resources that go further than that and helps developers build less "hey how about we sort/filter each colum manually :D" and more "here's this way better method than the simple tutorials show where you don't have to write repetitive code". I've had a beef with especially Microsoft as the biggest offenders on that for a while now... So here's some (in my opinion) helpful links that may help you on your way: https://github.com/danielpalme/GenericSearch http://stackoverflow.com/questions/15055718/dynamic-filtering-sorting-and-paging-on-generic-list-with-entity-framework - both answers should work, although the first one i've used personally in the past. https://github.com/amoerie/generic-datatables - use this as an example for datatables specifically. There's a lot of code to work through with them, and there's a lot you need to understand before you can really start adjusting them to your own needs, but basically you need to look at expression trees. Also to store the previous filters, you need to pass all the variables you get in your controller back into the views, and then expose them in your query parameters. Easiest method is to build the filtering/sorting/paging as a form with method GET with all its parameters as fields. 
About core Like 1. Request/Response pipeline 2. Performance 3. Cross platform / Linux support 4. Flexible platform - very un-microsoft 5. Good development team - we have reported two issues with .net core and the response was fast and professional Dislike 1. Keeping EF as showcase for database (prefer micro-orms) 2. Authentication / Authorization - has always been bad in asp.net and its not much better now 
.NET does simply not support loading multiple versions of the same assembly within a single AppDomain. So NuGet does the only sensible thing it can do. The alternative would be to simple refuse and break.
Is this a re-branding / re-packaging of the Orchard CMS stuff? 
Why do you use an IMultiValueConverter converter, and not just a normal IValueConverter for the entire object? Seems to me you don't really gain anything by doing it this way.
I choose that based on a guide I read about using MultiBinding in XAML. The guide used IMultiValueConverter so I did too. No reason other than that so I could look into IValueConverter.
Beyond excited for this. Orchard 1.0 was a great framework with a lot of smart people behind it that never seemed to get the attention/traction it deserved. By no means an easy framework to learn but once you got the hang of it, it's extremely powerful 
The problem with ports of JS libs such as this and Less (dotLess) is that they always leap behind. Usually it is better to run an existing up-to-date JS lib in a javascript engine to produce the desired result.
Hello, Sebastiaan! JSMin is not a JavaScript library. Original library is written in C - http://github.com/douglascrockford/JSMin 
Like : The move towards cross platform Dislike: The recurring feeling that they were trying to throw out the baby with the bathwater. I don't have much appreciation for all the command line stuff. I only drop down to command line when a UI doesn't work (git merge in VS 2015). 
How does this compare to regular Orchard? Can orchard do workflows as well, like SharePoint?
Yeah 100% and so much better than Umbraco imo
&gt; I also dislike how complicated the web frameworks are. It got much better with Asp.Net Core but they are still far from what Go has. Use [nancyfx](http://nancyfx.org/) if you need something simple then?
You want website B to make HTTP requests to website A? If I'm understanding this correctly, you want website B to make requests using something like the [HttpClient](https://msdn.microsoft.com/en-us/library/system.net.http.httpclient.aspx) class. If the target website is a bit more complex (single page app sort) and you actually have to run it in a browser with a Javascript engine, you may want to check Selenium or PhantomJS. They allow you to programmatically fetch a website and then you can then check the resulting DOM. 
Most business analysts I've met are at least familiar with SQL or DB interfaces once set up. I just wonder if this doesn't take you further from what you want to end up with than is necessary.
WCF is the swiss army knife of communication over [various protocols.](https://msdn.microsoft.com/en-us/library/ff649712.aspx) WebAPI is HTTP only. [Comparison Here](https://msdn.microsoft.com/en-us/library/jj823172\(v=vs.110\).aspx) Since you're building your UI in as an ASP.net, I'm assuming this is a website with simple CRUD ops and maybe some transform logic..stick with WebAPI. WCF can do HTTP but it's not worth the headache..
There is no up-to-date plugin usage. Java and Silverlight are dead in modern browsers for all intents and purposes, and Flash is on its way out as well and is already being disabled by default in some browsers. Most sensible method in my opinion is proxying the request through your webserver. Since you only need to download the initial page without any external resources, resource usage should be manageable.
&gt; WCF can do HTTP but it's not worth the headache If you're calling services on the same box, NetNamedPipeBinding is fast but needs some configuration. If you're crossing a machine boundary (e.g., to middle-tier servers), HTTP or TCP binding is probably the way to go. TCP goes by IP so a connecting to a load-balanced endpoint is out of the question or not worth the hassle (e.g. connecting to http://servers.domain.com load balanced to http://server1.domain.com and http://server2.domain.com). This leaves us with an HTTP binding: BasicHttpBinding, WSHttpBinding, etc. If you don't need all the security, Basic is fine. If you're a Microsoft shop and don't need interoperability, then a CustomBinding is best (HTTP transport with binary encoding). 
If you are using Azure Paas, there is zero support for Azure AD authentication against a hosted WCF service. Everything is built for WebAPI. I am hoping that WCF support will be added later since Azure is evolving quickly. But Azure AD is a passive Web based authentication method and it doesn't fit the multi protocol nature of WCF. On the up side, WCF uses proxies and it is easier to call and to use on the server side. It has limited rest api support so not so great when calling from browser using javascript. 
It's only a matter of time before Microsoft buys JetBrains. 
Wat.
I have load balanced WCF NetTcpBinding running in production and it wasn't difficult. 
Are the requests going to be per page or per domain? The later seems, from a UX perspective, inelegant. I am afraid given the restrictions of the modern web, and honestly the most streamlined solution, would be to do this on the serverside. Given that you are only checking the HTML, there would be no need to download images and other heavy bandwidth things. Your next sticking point, though, is that a lot of cross site problems come from a user's JS, not just the plain HTML. That's a whole other bag of worms to solve for if your tool is going to go for total coverage. Best of luck!
M'okay then. You may want to check that URL OP.
It's a blessing and a curse. :) WCF apps can be ridiculously flexible in how to communicate. Without even editing code. Especially nice if you don't actually maintain the codebase yourself. Also don't miss out on Microsoft's WCF configuration editor. It's a user interface that can help a lot.
Fun little article - thanks!
Everything is just a text editor next to Visual Studio. 
WCF doesn't need any xml configuration, you can do everything in code.
this looks real interesting!
It's not no actual bugs, just no bugs left open to the 2.0 release.
Fair Enuff
Web API - I would say this is a no brainer.
I'm not saying it is a good database. It is just a hyperboole and a preface to the next post: https://indexoutofrange.com/Want-unlimited-scale-and-performanceThis-is-where-to-start/
I also need some clarifications about WCF. &gt; Why now a days WCF is not getting any updates from microsoft. *Is that going to be deprecate in future??*
[removed]
The Titanic didn't have bugs, it had undefined use cases. 
I once worked on a project where bugs marked as "will not fix" were not counted in order to get to "zero bugs"
If they **want** to go, then let them. In **my** experience, that event has way too many people. It is like a zoo. More importantly, it is geared toward SharePoint more than anything else. Nothing against SharePoint, but I don't consider that real development. Don't get me wrong, many people make a great deal of money off of SharePoint and all of the 3rd party products being peddled off at Ignite. Besides that, you will get most keynotes being Azure sales talks. Its ok as a mini break for developers. But it is hectic.