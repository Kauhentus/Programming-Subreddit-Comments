I think for an entry level position any rep that is interviewing will take into consideration your experience (or lack thereof) so I wouldn't be too worried about the interview in terms of portraying your expertise on database topics. I am speaking from little experience as I have only been in the industry two years so take what I'm saying with a grain of salt. First, let them know what you know. No one expects you to know how to create a triple nested dynamic cursor. Let them know that you know what a cursor is, what a stored procedure is, what a file/filegroup is. This will give you some credibility in the fact that you're showing that you actually took something away from your classes and that you're someone that was serious about your education. Secondly, show an eagerness to learn. Say "Hey, I have minimal experience working with stored procedures or functions but I would love to learn the intricacies of both." Obviously, you don't have to be as upfront as that but do it in your own way. Given that you've created this post and asked for advice shows that you are serious about this and want to do the right thing so I am sure you do have the desire to learn more. Make sure that they know that. That one factor may be enough to give you an edge if there are any other candidates. Thirdly, show a little savvy. By this I mean that if you do have some experience with something where you picked up a piece of knowledge you otherwise wouldn't have then bring that up. For example, for a project you created a database with a table that was comprised of nothing but VARCHARs when some of the fields could have been INT and querying it was incredibly slow which taught you that you need to use more efficient data types when ever possible then say it. Obviously this is a lame example but if you have anything similar then use it. In addition to this throw in some knowledge that you gained about some of the things in your post. Mention the pros and cons of triggers (hint: avoid), why partitioning can be necessary. If you don't necessarily know the pros and cons then research it. Just make sure that if you bring it up you can back up your claims with enough knowledge. Fourthly, DON'T LIE. This is incredibly important. There are so many people that lie on their resumes and lie in interviews about their skills. Either they get lucky enough to make it through the interview only to get fired after they start or they called out in the interview when the interviewer asks a specific technical questions. Just be honest. Don't overstate or understate your skills. Tell him/her where you're at but say where you want to be in the future. To address your last statement about your trip: You need to evaluate how important getting this job is to your trip. If you would rather take the trip than take the job then feel free to bring it up in the interview. If you feel you could do without the trip then I would wait until after the interview then ask. If they say no then don't go. Yes, it sucks that you had this planned previously but it sucks for the employer as well. You are basically asking them to allow a new entry level hire to have 3 weeks vacation almost immediately. It's fair to say that they don't take too kindly to that. I would just evaluate your priorities. I hope some of this helped. Good luck!
I've been a DBA for 8 years, and like anything in IT, you don't need to know how to do everything. What you need to have is a good foundation of logic and the ability to look things up. Knowing the difference between the join types is virtually useless. I know roughly what they do, but I use that knowledge 0 times per year. Not to discourage you from learning this stuff - it's good to know, but if I was hiring a junior DBA I wouldn't even ask about this. The kinds of questions that matter to me when I'm interviewing a junior dba are: - How do you keep current with updates, changes, bugs, etc? I'm looking for mailing lists, forums, blogs, etc. - A query is running slowly. What do you do? The logical steps here would be something like check the server CPU and make sure it's not pegged, check the execution plan to see if anything is obviously slowing it down, etc. Just some series of steps that might eventually lead you to the right answer. - Have you played around with sql 2016 yet? What new features look interesting? - Pitch me a backup policy with a combination of fulls, diffs, logs, and explain why you chose each schedule. I don't care what the schedule is or if it would fit our environment, just that you know why you are picking these things. - Basic T-SQL questions - what are the differences between inner, outer, full joins, etc. If you're troubleshooting a query, you need to know what the query is doing. Basic stuff though - I wouldn't ask how to cross apply a function to force a query to run multi-threaded so you could use batch mode to look up data in a clustered columnstore index in a sql 2014 database. That sounds like a great question, but retaining that knowledge isn't useful. You just look it up when you need it. - What's the difference between a block, a deadlock, and how would you view/troubleshoot those? I was a bit all over the place here I suppose, but if you understand basic computer logic, you can solve any problem. If I were hiring someone that's what I would look for, and if you were interviewing, that's what you should stress. People have different amounts of knowledge, but the people who excel are the ones who can figure this stuff out.
Don't know if there is a standard for this, but I name FKs as tablename_id. So in your instance, I'd name the columns in my Posts table "sponsor_id" and "creator_id". That being said, it's probably a matter of personal preference, as long as the column name clearly identifies itself.
I like to use a random 32 digit numerical code that you then update in a new email to yourself with the latest version of code definitions. Then you simply search over your emails, decide which version is active, find the corresponding numerical code, and get the table name, and any other descriptions you stored in your email. But, you know, that's just one of many ways.
I also know very little of the hotel industry, but I'd agree that you'd have a user table with hotel guest info, and use the PK from that table into a booking table. As I think it's one person books x number of rooms for y nights at z hotel. I'd do it where every night each room would be 1 row.
I'm learning/working with SQL in toad now. I thought maybe I could get a better/faster understanding of the tables and data if I could use a GUI I am familiar with. 
Thanks!
Just thinking out loud here, I assume 1 booking applies to exactly 1 user ID and 1 hotel ID. If that's the case, you shouldn't have this table.
Our data warehouse does this kind of thing. We call it a link table.
You seem like the ideal person to be interviewed by. Out of curiosity, if you were interviewing me and I couldn't describe the difference between a block and a deadlock, but could explain a deadlock was able to answer everything else properly and then some, would I be cut from your shortlist or still considered? I've been in a position for a few years that would likely be considered a mix between a DBA and a developer and have pretty much learned everything on the job. It makes me curious how I'd fare at another company since I don't really have a way to gauge my skill level.
it's often called a junction table i'm a bit concerned that you have a "booking id" a 2-entity many-to-many association or junction table should have only 2 columns if you were thinking of booking_id as an autonumber, don't do it 
I have changed it now so there is a one to many relationship between bookings ad user, then a many to many with booking and hotel. :)
Not quite. Usually you have to add jar files. I'll have a look after downloading. Love IntelliJ, so if it's anything like it, it'll be great. Thanks! 
I know you've solved it at this point, but you should look into normalized data and their forms. Understanding that would answer this questions, and help you with future design. You also make no mention of it being a primary key, so that's something else to consider. Also, the pedantic side of me wants to let you know that you have some terminology issues even though we knew what you meant. What you were really asking is that is it ok to have a table with 3 columns also be the composite key. Good luck with the rest of it.
Do you have a column in the `words` table named `length` or are you trying to calculate the length of the word? Something like this will get you in the right direction, SELECT word FROM words WHERE LENGTH(word) &lt; 8 ORDER BY RANDOM() LIMIT 1;
i have a column named length but i mainly asked here to avoid using ORDER BY RANDOM() LIMIT 1; since i heard that it gets really slow really fast, and i cannot take half a second to generate new enemy...
True; there may be a [faster approach](https://stackoverflow.com/questions/1253561/sqlite-order-by-rand/1253577#1253577) if your `words` table has a numeric primary key column.
Honestly im still trying to figure out where exactly to take my career. Do you happen to know what kind of day to day activities a Database Analyst would perform?
Use SELECT DISTINCT in the subquery and you won't need the GROUP BY, it will make only unique rows to be selected.
I have it running now to test Thanks for the input! To clarify Having it in the sub query allows me to call the unique rows in the main query to still perform the AVG and SUM, but if the distinct was in the main query I would NOT be able to do the AVG and SUM?
Yes. The main query will "see" the subquery like a table, so as long as there is a group by in the main query it doesn't matter what is happening in the subquery.
Thank you for the explanation and link 
Yup. The HAVING count() &gt;=1 is the same thing as not even adding the "&gt;=1" in that case, because if it returned 0 in the count it wouldn't even appear in the SELECT in the first place. Also, for performance, distinct is the same thing as GROUP BY by every field in the select, which is pretty much what you are doing.
Yeah, Essentially removing duplicates while not actually being allowed to remove them from the table itself :(. Thanks for all the help !
I'd say get familiar with both. Analysts can benefit greatly in their career if they know databases as well (at least in my personal case). 
You don't have to choose, as database analysts often become database administrators. I'd choose based on the company, be open about exploring the DBA option (assuming it's an analyst role) down the line, and see whether you get a positive response.
Absolutely not! In fact, you can take this a step further, and automate it a bit by creating a vba macro in outlook that faxes the email to a data reader, which can then convert the email to values that you can then feed to a SSIS package, which saves the information to a database table, and query it anytime in the future!
Can't you use a CASE WHEN command checking if the CHARINDEX &gt; 0? Kinda like: CASE WHEN CHARINDEX('\', srv.srvname) &gt; 0 THEN &lt;code for Server\Instance&gt; ELSE &lt;code with only Server&gt; END
SELECT DISTINCT c.customer_num, c.customer FROM customer c INNER JOIN ORDERS o ON c.customer_num=o.customer_num INNER JOIN order_line ol ON o.order_num=ol.order_num WHERE ol.item_num="FD11"
Looking into your code, no. The CASE will be used only in the SELECT part.
Doesnt work, says syntax error, missing operator. Did you forget something?
Work with me here. Kinda tough typing this out without the database. Try this. .. SELECT DISTINCT c.customer_num, c.customer_name FROM customer c INNER JOIN ORDERS o ON c.customer_num=o.customer_num INNER JOIN order_line ol ON o.order_num=ol.order_num WHERE ol.item_num="FD11" 
Does this help you? https://docs.oracle.com/database/121/SQLRF/queries003.htm
It wouldn't affect the joins unless it was part of the join predicate.
That's pretty reassuring then. Based on the way some people talk, it's like everyone is expected to know everything since the dawn of the database.
Thanks, that's exactly what I needed. Will have to face that for future reference :)
&gt;Where do I get SQL IDE? [Here](http://lmgtfy.com/?q=SQL+IDE) Just open the file with notepad and you can see the SQL statements.
It isn't a query, it's an expression in SSRS such as: `Submits / Leads`
The ratio is calculated by the SSRS? If so can you post the properties of the ratio field? Also, your report has a source for sure, and that source is a query, even if it is just a "SELECT * FROM Table"
Pretty good rule of thumb is to pre-aggregate for subtotals, i.e go with Sum(Submits)/Sum(Leads) instead of going per-line metrics. YMMV and you might need to go for scope calculations or whatnot (I'n not in any way experienced in SSRS). 
You've really given me something to think about with regards to the trip. It's a once-in-a-lifetime thing, as in my mother's lifetime, with me, her, and my siblings. We're not likely to all do this again within the years my mom has left (she's in her 70s). I appreciate your other tips, too. Thanks for the help!
Set up the link through a VPN connection and forget about it. I couldn't live without this awesome and under-appreciated feature! (I do MSSQL&lt;-&gt;MySQL using an ODBC driver, works flawlessly)
I think you need to question how and why you're using linked servers. Are you consulting just with your IT team, or are you discussing with your development team too, and see if it will help their needs?
I am 99% against them. It's too easy to set them up insecurely - providing open access to server b just by logging onto server a and the "all other logins map to this" feature. Performance-wise, using linked servers because it's "easier" is like deciding to drive cross-country on a bumpy old gravel road at 2 miles per hour just because your house is next to it. Much better to spend the extra five minutes and hop on the interstate.
How big is the dataset that you're working with? Some of the tables that they would access across the link are literally millions of records. From what I understand, none of the performance tuning will occur on tables/views being retrieved from the other side of the link until the data is brought back to the primary server. 
This is something I didn't even consider. Instead of creating the integration package or using some kind of bulk export/import, we could have a local job that pulls the data across at night. Thanks. 
Table names with spaces and single quotes
&gt; I know in MSSQL I can just add Limit 1 to the end no, you cannot and that for sure wouldn't give you the highest price *for each item* try using the MAX() function with GROUP BY 
In that case the nightly scheduled Job suggest by another poster is a good way to go.
&gt;security [...] concerns. Yes and no. Do keep in mind that if you're a Windows Authentication-only shop, Active Directory and Kerberos will handle the security for you, but you will have to properly configure SPNs and Delegation, and the SQL Server Services must be running under a 'real' service account that has been assigned the SPNs and Delegation authority. Also, you can limit who can access the Linked Server(s) on the SQL Server side - it's not hard, and it does work. &gt;performance concerns This one is tougher. Our situation is a bit unique - we have two separate billing vendor platforms (long, complicated story - we're in the process of converting to just one), and thus we have two separate SQL Server data warehouses on two separate hosts. But, we're often asked to do consolidated reporting between these two systems, and to do it in an automated fashion. For really small datasets we've done some kludgy workarounds, but now that we're being asked for more and more we're going the Linked Server route. Initial testing shows little performance impact - mainly because we're not executing a remote query; we execute a stored procedure that loads the data into a temp table, and then we retrieve that temp table across the link - no local/remote joins, etc.
In Microsoft SQL Server, there is a description value available on every column in a table. If you want to add it programmatically its along these lines (change the sp_addExten.... to sp_updateExten.... as appropriate) EXECUTE sp_addextendedproperty N'MS_Description', 'Here Is a Description.', N'SCHEMA', N'dbo', N'Table', N'MyTableName', N'Column', N'MyColumnName' I like to do a little magic with sys.tables and sys.columns to auto generate the above if a column is used all over the place like updated time or active or what have you. Doesn't answer your question about a naming convention but you can add a 255 char description to reference later on some of the tricky columns.
I was surprised there wasn't a small code sample for the other JOINs since there was one for INNER.
The answer really depends on what your end goals are, are you designing this for ease of the application, or for ease of reporting? Did I read it correctly that you are going to set all null values to 0? If I were doing this I would set it up using a star schema, similar to option 2 but also very different. I wouldn't base my decision on eliminating the join, if indexes are set up properly it could out perform the full table option. So again, the answer is, it depends.
I prefer guids. I use them right up until the point where they wouldn't make sense. It wouldn't make sense for transactional scenarios where a unique ID is required but must also be human-readable and will be printed on receipts or pushed to countless web services for human consumption. Of course, you could use it anyway in that case but if you have to have a different unique field anyway then there's usually not much point. 
For what it is worth, we use linked servers extensively. There are performance issues at times but it can make you think more about other ways of writing the query. Security is an issue but like any other system if you configure it properly you should be fine. There is a point where it's better to use replication or some other method... Largely due to the amount of data you're needing to query. Some data types are not supported over a linked server, like XML. Also, it can get confusing to work with MSDTC and distributed transactions in general.
&gt; Just know that whenever a business manager's data is not correct, it's your fault. Part of the job is to absorb the finger pointing, step back, breathe, and educate. You can't win them all, but you'll be doing a lot of teaching and educating as a DBA. There is a natural clash that seems to happen since a DBA is logical to the point of hair-splitting where business managers are interested in corporate buzzwords like "dashboard" and "helicopter views of data". They like to say casually toss around the word "warehouse" and get mad if I try to explain what 3nf is. As a ex-accountant turned technical consultant, I cant disagree with this more. Accounting people are not DBAs and they really don't care how the data is stored (nor should they have to), they just want the data set that ties to the GL, Trial Balance or whatever other report is displayed in the system. As a DBA, you might not need to know all the data points and meanings, but your query results should tie out to the report displayed by the system, especially if that report is a Trial Balance which is used for financial reporting. 
*My* query results? That's exactly what an accountant would say!
LOL... Accountants don't write queries they consume data and (at best) write requirements (although they unusually suck at that). DB folks write queries.
http://iforce.co.nz/i/gai1ygi0.yvp.png 
there are subqueries in the select list. Enclose expressions (like z.formatted_name or the whole 'case' expression) in max() to avoid having multiple records issue.
In a distributed system, GUIDs are a simple way to get (almost) unique identifiers without having to synchronise. On a monolithic database server there is not a huge advantage to using them and a sequential key may be fine. If you ever do scans of sequential keys, then a sequence works best.
Oh this is the first sub-problem. I am "connecting" the table with itself. So this table will have 20,000 rows, and some rows need to be joined to each other based on whether they have a #X instructing it needs linked to another menu. Essentially these are all codes that will form one big IN clause. But the program can only have so many bytes per field so they broke them up in this manner. I need to connect that all back together into one field so I can then move on to the next step which will be parsing the newly created field. 
I don't know anything about pokemon either, but if Type is a foreign key to the Type table, then you have that backwards - each Type has many pokemon. If each pokemon had multiple types, then you would need to have a third table because it would be a many to many relationship. I would also suggest that you do not name your columns and tables with the exact word "Type" because that is a SQL keyword.
We use a lot of linked servers at my current work. So yes, as others have said there are security concerns. First of all, you're just passing SQL code, via whatever account you use to access SQL in the first place. OPENQUERY just passes code. So this is a valid query SELECT * FROM (linkserver, 'Select top 10 [id] from [table]') But this is also a valid query SELECT * FROM (linkserver, DROP TABLE [table]') An OPENQUERY will let you pass any code to a linked server. At that point you're only protected by permissions on the destination server side. But that's not the only security hassle. I built a super awesome query using linked server. The goal was to use Tableau to execute the query so we could rapidly get data that day without having to create a bunch of fixed data objects. Query ran perfectly. But as soon as I copied the data and pasted it into a custom SQL query in Tableau, it failed. SQL logs showed that an anonymous login was trying to authenticate to the linked server. The AD user/pass was hitting the first layer SQL server but wasn't getting passed to the linked server. We ended up having to pull DBAs off task to troubleshoot then request a change to security settings on the linked server. Which required a deployment anyway. But that's STILL not all. You can also pass this: SELECT * FROM OPENQUERY (linkserver, 'SELECT * FROM GiantTableWithMillionsOfRowsAndHundredsOfWideTextColumns) Now if you went directly to that linked server, like just logged directly on to it and passed that query from the giant table, you'd get a ways in like 20 minutes or so before SQL would be like "hey screw you buddy you're outta memory" But if you pass that to the linked server and your DBA isn't a paranoid rock star, it's just gonna chug and chug and chug ... on the other server. You'll just set it and forget it on your own session, and your own SQL will be like "la la la just waiting for a response" while it's red alerts and pegged processors and capped memory and temp disk, before spewing hugongous piles of data over your network. *** So linked servers are awesome because they bypass ETLs but you really need to consider how stupid your users/developers are before implementing them
There are a lot of things wrong with your current schema. First of all, it's one pokemon (pickachu) that can live in multiple regions. If thats the case, then the pokemon table should not have region in it since each row in that table can only have unique pokemon names. You can make a seperate table to link PokemonID to either (RegionID IF pokemon can live in any location(Pallet City) within a region (OR PokemonID to LocationID if Pokemon only exist in specific cities within a Region) Also, create table that uses LocationID as primary, and regionID as attribute column 2. PokemonType should be a INT datatype in Pokemon table. 3. Instead of creating columns for each type in Type table using BIT, I would create a lookup table which has columns(ID INT, Type String) and each individual type has a separate row. Lastly create a table that links PokemonID to TypeID. Hope this makes sense.
So it is a self join. 
Yes it is a self join but looking for creative ways to join it to itself besides a dense_rank () partition by column A order by B and then using a max rank function with a series of Case When
Absolutely this problem is made for another language not SQL but I am trying to hack a creative solution together as I am not farmilar enough with another language as this is just one step and then I'll need the database to do about 10 more. I have never had a reason to use recursive but this could be my chance to learn it. I will look at your link!
This exchange reminds me of microsoft's twitter chatbot. 
Do the classic response time versus throughput. For example, do one query that executes with nested loops and another that does a hash join.
I think the approach you're taking isn't necessarily the correct one. You need to think about things from the smallest picture, then move up. So in this case, we have Pokemon. Each Pokemon has a unique type set, such as Dragon/Fire/Water/etc. They can carry multiple sets. So a "Pokemon" table would contain column information such as Type and location. But wait. Can a Pokemon be found in multiple locations? What happens if certain Pokemon can only be obtained not by location, but by trading (such as Gengar)? You need to think one to many in the circumstances of Pokemon. They'll often be constrained in that way. I would also suggest thinking of adding another column, as it's been a while since I've played Pokemon...but region/location are Pokemon exclusive too depending on which version you're playing. I.E. Growlithe is in Red, Ponyta is in Blue (not sure if this is true or not, but you get the point).
&gt; But, it wouldn't work. What was the error message?
What software are you using for MySQL? The MySQL workbench should have a tool to export a table to CSV. Obligatory: I am an IT consultant. If you have any questions or are unsure, send me a PM. I'd be more than happy to remote in and take care of this for you... 99% of shit takes 5 minutes to do.
I guess I'm just really confused because I do put the semi-colon and that . . .&gt; thing comes right after it. No matter what command or input I type as soon as I press enter it goes back to . . .&gt; not sure if that's right or not. I'm not even sure how to check the table or see the example I just made so I can see if I have been inputting it correctly or not.
Select avg(star_rating) avgRating from table where trunc(download_at) between sysdate-30 and sysdate 
Oh sorry, for our enterprise replicate data is housed in Oracle, Teradata, and SqlServer so normally the flavor doesn't matter its more the thought process. I think I can make this solution work, just have to modify it with an additional case statement for when a A-Z character is present. Thanks a bunch! Ill report back 
COLUMN_VALUE is the default name given to a TABLE() clause "set" SQL&gt; select * from table( cast( multiset( select rownum-1 r from dual connect by level &lt; 5 ) as sys.odciNumberList ) ) 2 / COLUMN_VALUE ------------ 0 1 2 3 4 rows selected. breaking down my query - bit by bit - I started with: SQL&gt; with data 2 as 3 ( 4 select x, strt, stop, stop-strt+1 range 5 from ( 6 select x, substr(x,1,idx-1) strt, nvl(substr(x,idx+1),x) stop 7 from ( 8 select x, case instr( x, '-' ) when 0 then length(x)+1 else instr(x,'-') end idx 9 from t 10 ) 11 ) 12 ) 13 select * 14 from data 15 / X STRT STOP RANGE -------------------- ----- ----- ---------- 1 1 1 1 10-15 10 15 6 22 22 22 1 30-32 30 32 3 4 rows selected. that gave me the ranges easily accessible for each row, just sort of parsed the data to get the set I wanted to work with. Now, I need to join each row in that set with a set of rows whose cardinality equals the "RANGE" column. The first row needs to be joined to one row, the second to six and so on. That is where the TABLE() clause comes in. Notice that there is no join condition in my query - it is not necessary using this nested table, it will be "auto-joined", the database will run the query in the TABLE clause once for each row in the outer query and automagically join these rows to them. Using the query: select rownum-1 r from dual connect by level &lt;= range I can generate any number of rows - and since RANGE will be passed in for each row to this subquery, it'll automagically join to RANGE number of rows for each row. That is what lead to this query: SQL&gt; with data 2 as 3 ( 4 select x, strt, stop, stop-strt+1 range 5 from ( 6 select x, substr(x,1,idx-1) strt, nvl(substr(x,idx+1),x) stop 7 from ( 8 select x, case instr( x, '-' ) when 0 then length(x)+1 else instr(x,'-') end idx 9 from t 10 ) 11 ) 12 ) 13 select * 14 from data, table( cast( multiset( select rownum-1 r from dual connect by level &lt;= range ) as sys.odciNumberList ) ) 15 / X STRT STOP RANGE COLUMN_VALUE -------------------- ----- ----- ---------- ------------ 1 1 1 1 0 10-15 10 15 6 0 10-15 10 15 6 1 10-15 10 15 6 2 10-15 10 15 6 3 10-15 10 15 6 4 10-15 10 15 6 5 22 22 22 1 0 30-32 30 32 3 0 30-32 30 32 3 1 30-32 30 32 3 2 11 rows selected. I went from my original four rows to 11 (it goes to 11....). The first row was joined to one row, the second row from data was joined to six rows and so on the column_value is simply the result of "ROWNUM-1" being generated 
This is great, Oracle is something I have the least amount of exposure to and I am reading up on many things you identified here. I will finalize and work on my code tomorrow! I have to ask, what is your level of experience / title at your company? You no doubt have much more knowledge than me and I appreciate you helping me grow! 
1. Simplify your attempt as much as possible while still producing unwanted behavior: http://sqlfiddle.com/#!9/f53ab/1 2. Google the specific problem for helpful results: http://stackoverflow.com/questions/12126991/cast-from-varchar-to-int-mysql 3. Verify the fix: http://sqlfiddle.com/#!9/f53ab/2 repeat!
Correct me if I'm wrong... I would think that on an ordered index, using a GUID would have a huge performance issue as it would have to reorder all the ID's since the first character isn't guaranteed to be sequential. As a result, it would cause [page splitting](http://sqlmag.com/database-performance-tuning/clustered-indexes-based-upon-guids). This becomes a huge problem as the table scales out to millions of data as the cost of reordering the index becomes significant unless it is handled differently. Granted, if you wanted a PK that is absolutely distinct within the system, GUID would be an easy solution. But if you only need the ID to be distinct in the table, a sequential ID would be ideal. Architecturally, there is a different but pretty maintenance heavy solution to having ID's that are distinct system wide.
Your last one was almost right, you just need to indicate that it is a string by adding a single quote before and after the variable, like this: WHERE SupplierName= '"+ Request.QueryString["SupplierName"] + "'"
I've updated the query to include the single quote and a parameter, see below: "SELECT Date FROM Calendar " + " WHERE SupplierName= '" + Request.QueryString["SupplierName"] + "' AND Date &gt;= @firstDate AND Date &lt; @lastDate"; oCMD.Parameters.Add(new SqlParameter("@firstDate", firstDate)); oCMD.Parameters.Add(new SqlParameter("@lastDate", lastDate)); oCMD.Parameters.Add("@SupplierName", SqlDbType.NVarChar, 50).Value = Request.QueryString["SupplierName"]; However this is still not turning the dates red per supplier. I am getting an error that says "Cannot find table 0"
If it didn't matter then why is it a rule? 
Correct but if you want the median the function is median() instead of avg()
You need 3 things here and all if them would have platform specific answers. Please help us to help you. Read the sidebar then post your question again.
 SERV_ON_TM, (SELECT when using a subquery as a column in SELECT clause the result must also only return 1 value too.
This is where I would use a GUID too. They reduce readability of the tables hugely so best not if it is a single DB.
Please read the sidebar and tell us which platform. If your has a name length limit then abbreviate. If not then don't. It is actually faster to type out the long names than to do the mental gymnastics to abbreviate and expand the names.
i was there for 23 years to the day doing things like http://asktom.oracle.com/ (among other things...). Was an end user of Oracle, Sybase, Informix, DB2 and others for years before that. Sometimes I still dream in SQL :) It can be an extremely powerful language once you start 'thinking' in it (thinking sets, not thinking procedurally - it requires a mindset change sometimes)
Visual studio. New sql server database project. Import database. Now you have find/replace, refactor, show definition, go to table, compare data, compare database. Did I forget to say it is free now?
Order by 2 desc
SSIS has a cool 'fuzzy grouping' capability that might help you.
Dynamically adding columns to tables when new data arrives! Dodgy NVP databases, "Look Ma! I have revolutionised.the whole world's databases and I only needed 2 tables!" Making network links to databases in the same server in the same instance.
Yep there is a connect too https://connect.microsoft.com/SQLServer/feedback/details/288800/bcp-out-to-include-field-columns-names
Yes it is but we know where the manual is thanks.
 Select h.* (Select d.* From detail Where d.HeaderId = h.Id For xml auto, type ) as Details , t.* From Header h, Trailer t FOR XML AUTO
I thought it might turn into one of those "that database doesn't have a median aggregate function" for a while!
I will have to play around with that. That is quite interesting thanks for the information. 
Like /u/phunkygeeza said, you used both, you should use one or another. The parameter route is the safest one
I hate when people are pedantic on here and disagree with ideas that will work. It discourages people who are here to help others. I just wanted to point out there was a security hole in the suggested method.
i just learned how to use this earlier today. these sites are a great help http://www.regular-expressions.info/ http://regexr.com/ here is an example DECLARE vday1 VARCHAR2(20); vday2 VARCHAR2(20); BEGIN vday1 := TO_CHAR(SYSDATE, 'Day, HH24:mm'); vday2 := REGEXP_REPLACE(vday1, '\s+(, \d)', '\1'); DBMS_OUTPUT.PUT_LINE(vday1); DBMS_OUTPUT.PUT_LINE(vday2); END; / vday1 comes out as `Monday , 13:05` vday2 comes out as `Monday, 13:05` the regular expression finds a series of one or more spaces that are followed by a comma, then a space, then a decimal and replaces all that with what is inside the parenthesis, which is a comma, a space, then the decimal that was found.
No it tends to be a proprietary extension. It is very useful though where basic REPLACE, LOCATE and LIKE aren't enough.
[Some people, when confronted with a problem, think “I know, I'll use regular expressions.” Now they have two problems.](http://regex.info/blog/2006-09-15/247) I wouldn't call it "advanced" but it's definitely not beginner-level material. I try to avoid using them in SQL if possible as it *can* make the code less clear to a reader. Oddly enough, 10 minutes ago I encountered a situation where I legitimately do need a regex. Until someone adds a new record which makes it return incorrect results.
You may need to create empty databases on your new server first. I cant remember if phpmyadmin provides an option to export with the CREATE DATABASE option or not but if it does try re-exporting with that option enabled. If you have many databases it might be easiest to write your own create database SQL file and execute it. This would create all the empty databases and then you can run your import SQL afterward.
I don't know how phpmyadmin exports the data. Is it a single .sql file? What do you get when you run &gt; $ head -40 that_file.Sql Make sure the CREATE DATABASE lines are there. But something you can try is manually create all of the databases first. 
Prepared Statements would also help protect (assuming that the OP also removes the current string concatenation in the `WHERE` and replaces it with variables) against the SQL injection vulnerability that the current code is wide open to.
Do you have an index on the date field? If you cluster the table by the date, it will run *super* fast. are you using &gt; '2016-04-10' for the first query, and Between for the second? Show the execution plans for the different queries and see what it is doing differently. 
BrentOzar.com is good. 
What is a tomcat server? What is a Query execution plan and how can you see it? Why would I want to cluster my tables on a date field? Why is the uniqueidentifier data type often the wrong choice? What is fill factor? How do you change it? How do you trace a specific user or application? What is STAR schema and why do non-sql people always use that freakin word? What's the difference between an inner join and a left join? How can you tell who is running what queries on your server? What happens if my TempDB is too small? Why would I put a smaller, cheaper CPU in my Database Server? Mention the trip when they make you an offer - it is something you will have to negotiate.
Thanks for the help! Yeah, I've tried putting it in quotes, I posted here one of the many things I've tried. Nothing seems to help. But I'll take a look into those prepared statements, that seems to be the right way to do it! Thank you!
Interesting. Now, I'm wondering about the efficiency of matching GUIDs to GUIDs vs Int to Int. My initial thoughts would say that matching a 32/64 character string to string would have a greater cost than matching numeric to numeric. 
Select name, email From tbl Where status='bounced' Group by name, email On mobile
You want to select the distinct emails that have bounced, so: SELECT distinct email FROM [table] WHERE [delivery status] = 'bounced'
&gt; , it becomes increasingly more important to match a naming condition rather than an exact value as situations get more complicated. What is a better was to match naming conditions as business problems become more complex? I would like to put in the time to learn a better way. Thank you in advance. 
I know how to do this; does this mean I am advanced at SQL? If not, what would you suggest I learn? 
So Regex is not advanced. What do you mean by regex being proprietary extension - can you ellaborate?
I know regex replace and a few other regex functions. I am trying to see what people think about how advanced knowing pattern matching techniques are in SQL.
Pretty sure both the other answers are wrong. This should work. Select email from tablename group by email having max([delivery status]) = 'bounced' 
Thanks! Potential questions are always helpful.
sorry im a bit mixed up myself. but one question not including count, but includes sum,max and min. the question brifly states how i need sum of employee hours. emploee with min and max hours. in 1 querry
Which OS are you running? In general you can ssh into the server, become root, and see if one of the following will work: mysqld_safe &amp; or service mysqld start The commands vary slightly from OS to OS, FreeBSD has been known to use `safe_mysqld` instead of `mysqld_safe`. If the commands are found but MySQL refuses to start up, you may need to track down its log file and see what's going on.
List the combined total hours worked of all the academic staff per week, the maximum hours worked, minimum hours worked, the name and subject of the academic staff who had the maximum hours , the name and subject of the academic staff who had the minimum hours worked.
ive also got this question. which i need go count and display results. if this is easier to explain • List and total number of all full-time Academic Staff and their location
Different RDBMSs implement regex differently, at least syntactically. Stop trying to categorize things as "advanced" or not. I rarely if ever hear such talk around the office.
With a general question like yours, I think the best approach is to review the concepts, specifically [Relational Algebra.](http://db.grussell.org/index.html) I studied the topic *after* I started working with SQL, so you can certainly skip it, but it clarified a number of concepts, especially the notion of *selecting* data, optionally *joining* that data, and *projecting* the results.
This question doesn't make much sense to me. If you are listing all staff AND locations... what are you totaling? This gives you number of employees by location: select location, count(*) from employeeTable where employee_status='full time'
I am looking for a salary range as a result of my question.
That sounds like it could be similar to: Select Sum(hours) as total_hours, Max(hours) as Max_hours, Min(hours) as min_hours From Time t You can then use this to join back to time to get the users data Select Employee as Max_employee, Subject as Max_subj, Hours From time t Inner join ( query above) Maxtime On time.hours = Maxtime.hours Repeat for min The better way to do this with SQL server management studio 2012 + would be windowed functions. 
You want to know what salary range someone who knows regex usage in SQL can command? I would expect **anyone** who is capable of writing more than a `select *` from a single table to be able to learn how to use a regex in a query. It's not something you put on a job description as an independent bullet point, and you don't highlight it on your resume. It's one of those things that professionals who know what they're doing will just expect to see *when appropriate* (and will expect to *not* see when it's not called for). At best, the usage of a regex *when appropriate* even though another, more clumsy, solution might also exist indicates that you're deal with someone who's a bit smarter than the average bear - or maybe they're just more accustomed to regex and there's some other facet of SQL which they're less versed in, like `PIVOT`.
Thank you.
As is being largely said in this thread, regex use is a standard part of any programmer's toolkit, and although there are implementation differences across languages (including SQL) the skill is mostly transferrable. Writing queries that use regex doesn't necessarily make one an advanced SQL person, although those types of query won't usually be found in SQL intro courses. They are just using a particular type of function, of which there are a lot.
Okay. What are some advanced functions?
Wrong.
Care to elaborate?
SSIS and BIML are really built for this type of thing.
Try: SELECT CAST([tm] AS smalldatetime) AS 'tm' , SUM([totalCount]) FROM [TestDB].[dbo].[counts] WHERE [tm] BETWEEN '2014-09-15 06:53:04' AND '2014-09-15 06:53:14' GROUP BY CAST([tm] AS smalldatetime)
Ask the owner of the database for access to it, or a dump of the data you need. Prepare to be told "no." Asking for a list of email addresses to "get in touch with everyone" sounds like spam to a lot of people.
I hate to be the one to break it to you but if 1) you think salaries are linked in any way to being able to work with regex, or 2) you think being an expert at SQL is related to your knowledge of a single set of functions (e.g. regex), or 3) you are not even aware that regex is vendor-dependent then you are in the super-junior/noob/beginner category. Any book on SQL is going to be of benefit to you. SQL is much more than just querying. There's DDL, DML, TCL, data modelling, performance tuning, ETL, procedural extensions, administration, and so on. There's also many dialects of SQL, so that's another way to specialize. Oh, and regarding that link: that is 100% bogus. Any coder who lists regex as a skill ought to be kicked in the proverbial nuts. They are the same as people who list IDEs as a skill or even worse: LaTeX. No one in the industry gives a flying fuck whether you can generate documents that are properly typeset. 
Not every sentence needs an exclamation mark.
Your query returns the following result: tm totalCount 2014-09-15 06:53:00 9349 Not quite what I wanted and I'm not sure why it behaves differently from /u/paulsandwhich's answer but I thank you for taking the time to help. 
Your query returns the following result: tm totalCount 2014-09-15 06:53:04 510 2014-09-15 06:53:05 526 2014-09-15 06:53:06 516 2014-09-15 06:53:07 512 2014-09-15 06:53:08 3944 2014-09-15 06:53:09 592 2014-09-15 06:53:10 639 2014-09-15 06:53:11 659 2014-09-15 06:53:12 633 2014-09-15 06:53:13 537 Which is exactly what I wanted. Thank you very much /u/PaulSandwich for the help!
I meant naming conventions having column names that are used in every table like 'LastUpdateTime' would be one way Another would be that all of the location names follow the layout of [two letter state] _ [3 letter city] _ [2 digits of year built] so that you could search using regex "^MO_" to find all locations in Missouri. If you were looking in a database it would be a like statement with 'MO\_%' escape '\' to match "MO_" at the beginning of a string. This could also allow for ending in '_16' to find all of the locations built this year. regex "_16$" or sql like " '% @_16' escape '@' " The same goes for variable naming in your code. It is beneficial to be able to search the project for _ db _ for all variables being passed into/out of database transactions. tl;dr: when your livelihood is based in text files, you better get good at searching names and contents of the text.
It's all text, right? haha
&gt;What's "proper" in these circumstances? Whatever you prefer, or whatever your standards are (Team, Company, etc). Here's the 'secret' - the query you write is often not the one that is actually executed - most modern RDBMS' optimize and re-write your submitted query for the best performance. In other words, so long as you have the right keywords in the right order, the engine doesn't care if it's pretty to you or not.
It's all about personal preference, and consideration for your fellow analysts. For instance, I prefer leading comma select statements, and capitalize all reserved words/commands. Some people don't like my format, but being consistent, my format has "won out" in the dept and slowly became a best practice. I also like to add comments behind field lines, or above tables, to describe what is going on when I wrote the query: SELECT Field_1 , Field_2 , Field_3 --this is a comment about why the field is here FROM db.schema.Table T -- OT: Using this table to pull X results JOIN db.schema.OtherTable OT ON T.FieldJoin_1 = OT.FieldJoin_2 WHERE 1=1 -- Ensure "X" results are filtered by "Y" constraint AND &lt;condition&gt; = 1
All the RDBMS I know are case insensitive , so type like you want.
It depends - if the [database's collation](https://msdn.microsoft.com/en-us/library/ms144250%28v=sql.105%29.aspx) is set to case sensitive, then it'll puke if you don't match the column's case. Its not common but its out there.
This looks infinitely readable. I literally started learning SQL a week ago, and while I don't understand everything this does, I can reasonably follow the logic. I'm assuming this is a backend for a Business Intelligence/analytics script user interface of some sort? Or is this a standalone query? It's more complex than anything I've seen so far (tutorials and such). RTRIM selections as human readable naming conventions, case options indented separately with understandable results per option, and clean in-line comments. I love this. Your spacing and layout looks almost Pythonic, IMO. I'll try to do similarly.
AFAIK, that is for column data only... not for object identifiers (column names, table names, function names, etc.) but maybe it impacts them too. DB2 and Oracle are absolutely case sensitive and they will throw errors if you use "Quoted Identifiers" with incorrect casing. But if you leave the quotes out, everything is converted to upper case. SELECT 1 FROM DUAL AS "CaseSensitive"; SELECT 1 FROM DUAL AS CaseInsensitive; -- Returns column named CASEINSENSITIVE On our SQL Server (with default case insensitive collation, in case that matters...) the following query is valid: WITH TEST AS ( SELECT 1 AS [Case] ) SELECT [case] FROM TEST I don't have access to DB2 or Oracle ATM but I'm 99.9% that won't work there.
&gt; AFAIK, that is for column data only... not for object identifiers (column names, table names, function names, etc.) Not quite, at least not with SQL Server. If the database (or, god forbid, the Server) collation is set to be case sensitive, then object identifiers are also case sensitive, and, by default, the values in char/varchar/etc. columns are as well. However, commands - `SELECT`, `FROM`, etc. are not.
&gt; sql isn't case sensitive. Lots of languages aren't. Let me watch while you tell a C programmer that it don't matter ;) Actually, some DBMSs are case sensitive for the table/column names. Why you'd want a DBMS where tablename.field is different from TableName.Field is beyond me, but yes, it does. 
I was trying to point out that the syntax can be case sensitive on the column and table names. Connected to a MS SQL Server instance that has the following collation Latin1_General_CS_AS The following works. select top 10 AddonNumber from [dbo].[Addon]; This does not - notice the lowercase n in number select top 10 Addonnumber from [dbo].[Addon]; Msg 207, Level 16, State 1, Line 3 Invalid column name 'Addonnumber'. This also fails, the column is fixed but the table name is not right select top 10 AddonNumber from [dbo].[addon]; Msg 208, Level 16, State 1, Line 3 Invalid object name 'dbo.addon'. But this will work SeLecT TOp 10 AddonNumber FROM [dbo].[Addon]; So its not perfectly in line with the author's original question on syntax and formatting of the SQL key words. But it was to show that there are times when case will bite you. 
Question: Is this true for both [Quoted] and Unquoted column names? Does the behavior change at all between [Quoted] and Unquoted, as it does in DB2 and Oracle?
http://software.dell.com/products/toad-data-modeler/ Toad Data Modeler has a trialware with some size limitations, i think. It does have re-engineer functionality.
i think this is all semantics. from the business unit point of view, anything beyond them is a black box and in tht black box is business analysts, BI, etl, dba. the db folks do not write queries, it is the analysts in conjunction with BI, the db make sure that the data in the db is correctly loaded, but if the query is wrong, that's on the BA or BI. of course the reqs could have been wrong but i still put the blame on BA or BI for not providing the correct query during the building phase.
Doesn't appear to. in my examples I strictly did unquoted on the column, so I just retested. works: select top 10 [AddonNumber] from [dbo].[Addon]; Fails: select top 10 [Addonnumber] from [dbo].[Addon]; So it appears in SQL Server the [] won't provide insurance around case sensitivity.
Another approach that is more portable (if that matters) is to hash the natural key(s). 
Coding style is all about making a generally useful style. Just like the Constitution, it's not a suicide pact.
The fact that both you and her had jarring experiences reading each other's code is the strongest argument for a single coding standard. In a multi-developer environment the odds of having to debug someone else's code goes up dramatically. I once had to debug a rat's nest of subqueries where the dev had removed all spaces from a 300-line query and put it all in one line. I wanted to publicly crucify him as a message to other developers.
There's tons of stuff that can go into a database but a quick and dirty possible outline for what you're requesting could be accomplished with 3 tables, call them **Member**, **Reward**, **MemberReward**. **Member** - Contains a list of your members. **Reward** - Contains a list of all rewards available. **MemberReward** - Contains memberID (to relate to Member), rewardID (to relate to Reward), a column to record when the reward was awarded and a column to record the date the reward was redeemed.
I would go about it this way: [PERKS] * Name PRIMARY * Has_Amount (true/false) * Descr [MEMBERS] * ID PRIMARY * First * Last * Salutation * Address * Phone * Email * Other info. [MEMBER_PERK] * MEMBER\_ID (FOREIGN KEY MEMBERS.ID) ___ COMPOSITE PRIMARY * PERK_NAME (FOREIGN KEY PERKS.NAME) _/ * PERK_AMOUNT &lt;- Will always be -1 for 'Has_Amount = 'false' perks You'll probably want some kind of uniqueness constraint(s) on the columns of your 'MEMBERS' table, you have to balance the following problems: 1. Asking for too much information 2. Your system not accepting 'John Smith' (jr.) having the same address and phone as 'John Smith' (sr.) but being a different person 3. Avoiding any possibility of the same member being entered twice, possibly by a new employee not knowing what they're doing.
Same again, you tend to be changing/maintaining the bottom of the column list rather than the top.
Thanks, I took the test, and it seems I did pretty OK. Much obliged!
That may be by default (although last time I used DB2 on an S/390, it was case-sensitive), but most RDBMSs have the option to be case-sensitive. It might even be a [good idea to set some up that way](https://sqlstudies.com/2016/04/20/why-my-primary-test-environment-is-case-sensitive/)
That is why I wrote "RDBMS I know" , which is mainly MSSQL and a bit of MySQL and Postgre. I never touched DB2 and I saw very briefly Oracle. Good to know about it.
Awesome, glad I could help!
Personal preference is truly it. I am big on -- -- -- from table a inner join table b on a.-- = b.-- where a.= '1' etc. I always use the naming convention a, b,c,d for more or less flow. 
Can you post the query?
This is an Entity Relationship Diagram, ERD for short. If you're using MySQL Workbench as your interface tool of choice, you can actually build the tables and constraints directly from the ERD. There's a tutorial on how to do just this on MySQL Workbench's website, https://dev.mysql.com/doc/workbench/en/wb-getting-started-tutorial-creating-a-model.html .
http://imgur.com/a/rbnuG Here's a model I did real quick. 
A simplified answer: decide what is your precision is supposed to be at (e.g. if you would say hours, 23:13 might count as a whole 24 hour day, minutes - 23:59.13 might count as a whole 24 hours, etc), figure out the difference in the timestamps in the unit of measure you've decided on earlier, compare to the interval expressed in the same units of measure (e.g. minutes in 24 hours - 1440).
by the way, the numbers table is extremely useful, it's a good idea to create it as a permanent table in all your databases
Unfortunately with this particular database, I only have readonly rights.
exactly!
Here we go: SELECT * FROM actor WHERE id in( SELECT id FROM actor GROUP BY id HAVING COUNT(id) &gt; 1)
Came here to say this. EDMX otherwise known as 'ADO.net data model' can reverse and forward engineer databases. 
How about Insert b Select * From a Where not exists ( Select 1 from b where b.x = a.x And b.y = a.y ) 
Probably, but why put the extra work on SQL server to cast a DATETIME column to a DATETIME2? I don't think you're gaining anything. Since is already a DATETIME column, I wouldn't cast it at all.
Knowing what query or statement you are attempting to run would shed some light on this I suspect. Can you post that up?
So you're talking about when you right-click and select "Edit top 200 rows"? You're probably not going to find many here who use that function in SSMS too terribly often. Does running queries that you write work properly? I kind of suspect it's going to be something in SSMS itself, and might be version or instance specific. The first several Google searches seem to point to statistics and DAC (Data Access Components, I believe). Are you certain that the version of SSMS you're running is at least as recent as the DB you're accessing with it? Are you running one of the SQL 2016 release candidates?
Yeah this sounds to me like an SSMS back end issue. My original thought was that someone dropped a column from the information schema, but it could definitely be mismatched SSMS and SQL Server versions. 
This should work without write rights and without temp tables --your table with tbl as ( SELECT CustomerNo, ServiceNo, CAST(EffDate AS DATE) EffDate, CAST(ExpDate AS DATE) ExpDate FROM (VALUES (1, 1, '1/1/2016', '1/1/2017'), (1, 2, '1/2/2016', '1/2/2017'), (1, 3, '1/3/2016', '1/3/2017'), (2, 1, '1/1/2016', '1/4/2017'), (2, 2, '1/4/2016', '1/1/2017'), (3, 1, '1/5/2016', '1/5/2017') )t(CustomerNo, ServiceNo, EffDate, ExpDate) ) --get the interval of dates you want to have a result , interval as ( SELECT min(EffDate) from_date, max(ExpDate) to_date from tbl ) --generate a list of dates from beginning of interval to the end of it , dates as ( select from_date as date_ from interval union all select DATEADD(DAY, 1, date_) from dates where date_ &lt; (SELECT to_date FROM interval) ) --Here is the result select date_ , CAST(COUNT(*) AS float) / COUNT(DISTINCT CustomerNo) as Accounts_in_Force from dates join tbl on date_ &gt;= EffDate and date_ &lt; ExpDate group by date_ --Add the option so the recursive query to generate a list dates does not fail --Make sure the max is bigger than the days in your interval OPTION (MAXRECURSION 20000);
try SELECT COALESCE(a.FULLNAME, CONCAT(a.FNAME, CONCAT(' ', a.LNAME)),'') as OwnerName FROM OWNER a or SELECT COALESCE(a.FULLNAME, a.FNAME || ' ' || a.LNAME,'') as OwnerName FROM OWNER a
is this what you are looking for? select customer_id, count(distinct order_id) from tbl group by customer_id
&gt; select customer_id, count(distinct order_id) &gt; from tbl &gt; group by customer_id Bless you! I wasn't running distinct in the count.
I'd suggest you to use object_id(@table_name) instead of the query since it's able to recognize schema name in the text string. Otherwise, there's a possibility that you'd have a table with the same name, but in a different schema.
You can simply use SqlBak (http://sqlbak.com/) to make schedule SQL Server databases backups and send them to OneDrive for Business. Find more information here: http://sqlbak.com/blog/how-to-backup-sql-server-to-onedrive-for-business/
I like it. I added &lt;tab&gt; prefix and &lt;cr&gt; suffix: Declare @table_name varchar ( 255 ) = 'person_info' , @prefix varchar ( 255 ) = 'pi' Select stuff ( ( Select char(9) +', ' + @prefix + '.' + sc.name + char(10) From sys.columns sc (nolock) Inner Join sys.objects so (nolock) on so.object_id = sc.object_id Where so.name = @table_name and type = 'u' and sc.name != 'row_timestamp' for xml path ('') ) , 1, 2, '' ) 
You may want to become familiar with SQLPlus (which should have gone in your oracle directory when you installed the oracle client) and build your SQL there before putting it into a Crystal report. That way you know what's a SQL problem and what's a Crystal problem. I think /u/ibilali has provided your solution in that Oracle's string concatenation is more ANSI SQL compliant than MSSQL server's acceptance of using the + symbol. 
Thanks. I meant that I didn't see where you could enter your own SQL without Crystal using graphics to build the report. I guess I'm used to SSRS and Report Builder.
Except the OP asked for customers having only a single order (if I read that correctly). So equal to one rather than greater than one.
Thanks
That is a perfect case of SSIS package. Make one to import from a flat file (your CSV) into a SQL table, there you can make every specification you need and even schedule to run it and make it search a pre-defined folder, that way it will be fully automated. If you are not familiar with SSIS, take a quick look at google, it is much easier than it looks at start.
Sounds like you're really looking for BCP + A Format file. https://msdn.microsoft.com/en-us/library/ms162802.aspx BCP is the Bulk Import tool that comes with SQL Server. It's not the easiest beast in the world to get to work, but it can be done. The format file is just that a file that defines the mappings between SQL Columns and "columns" in the file you're importing. Another option (I know less about), is when you're done running the import from the Import Wizard it gives you the option to save it, that saved file is a DTS file that can be re-run. https://msdn.microsoft.com/en-us/library/ms188513.aspx 
&gt;Definitely mismatched SSMS and SQL Version. Sounds like it. Generally speaking, SSMS is backwards compatible, but not forwards compatible without running into issues like this. For example, I use SSMS 2016 with SQL Server versions 2005, 2008 R2, 2012, 2014 and 2016 - that's backwards compatible. But - and I have tried this - if I try to use SSMS 2005 (and it was called something else back then, I think) to connect to 2008 R2, 2012, etc. it might connect - but I always have issues trying to do anything, especially with the object explorer which makes heavy use of the sys objects.
Well, the easiest solution is to use the Split function and then use IN (SELECT * FROM dbo.Split(t.PriceDaysOfWeek). There's an ugly way of solving it within one query though: DECLARE @t TABLE ( SomeId int , PriceDaysOfWeek varchar(255), ShowDayOfWeek int) INSERT INTO @t VALUES (1, '1,2,3,4,5,6,7', 4 ), (1, '1,2,3,4,5,6,7', 4 ), (1, '1,2,3,4,5', 4 ), (1, '1,2,3,4,5', 4 ), (1, '6,7', 4 ), (1, '6,7', 4 ) DECLARE @delim varchar(16); SET @delim = ','; -- can be more than 1 symbol ;WITH L0 AS(SELECT 1 AS C UNION ALL SELECT 1 AS O), -- 2 rows L1 AS(SELECT 1 AS C FROM L0 AS A CROSS JOIN L0 AS B), -- 4 rows L2 AS(SELECT 1 AS C FROM L1 AS A CROSS JOIN L1 AS B), -- 16 rows L3 AS(SELECT 1 AS C FROM L2 AS A CROSS JOIN L2 AS B), -- 256 rows L4 AS(SELECT 1 AS C FROM L3 AS A CROSS JOIN L3 AS B), -- 65K rows Nums AS(SELECT ROW_NUMBER() OVER(ORDER BY (SELECT NULL)) AS N FROM L4), DNums AS ( SELECT TOP(255) -- probably should match the column length N - LEN(@delim) AS 'n' FROM Nums ) SELECT t.SomeId , t.PriceDaysOfWeek , t.ShowDayOfWeek FROM DNums x -- Cross join each string with numeric table CROSS JOIN @t t -- only return the numbers that equate to the position of a delimiter in the original string WHERE (SUBSTRING(t.PriceDaysOfWeek, n, LEN(@delim)) = @delim OR x.n = 1 - LEN(@delim)) -- and where column = one of the items in a delimited string AND t.ShowDayOfWeek = TRY_CAST( SUBSTRING(t.PriceDaysOfWeek + @delim , x.n + LEN(@delim) , CHARINDEX(@delim, t.PriceDaysOfWeek + @delim, x.n+LEN(@delim)) - x.n - LEN(@delim)) AS INT) 
Ouch, haven't thought that there couldn't be any 2-digit week days :( This is much more simple and effective of course.
What about primary/foreign/unique keys, indexes, triggers and such? I'd use snippet for this with some re-runnable create/drop logic that you can later execute as-is on test/production once you're done with development. 
Sqlzoo.com
How to turn off secure-file-priv?
Foo Bar
&gt; You're probably not going to find many here who use that function in SSMS too terribly often. Very true. I often forget that SSMS is capable of these things - until someone asks me a question about it.
What server are you using? 
Is it null or is there an empty string? if you change the first query to vchStat ='' as opposed to '''' , what happens? 
Too many quotes. '''' is an escaped single quote not an empty string. You want '' As an aside the original statement can take advantage of indexes, but the isnull expression can not.
Try this web site http://www.studybyyourself.com/seminar/sql/course/?lang=eng. For free. 
That is god awful! How do you *edit* that?
I actually don't have specifics much beyond that unfortunately since this question was just part of a questionnaire. I've definitely received questions in documents like this that didn't make sense or weren't possible in the past due to being written by someone who isn't actually familiar with the technology. I'm assuming they mean the symmetric and asymmetric keys you would use for encrypting and decrypting fields that contain sensitive data. We currently do that for a number of sites that have credit card transactions. My understanding is that those keys need to be generated on the server (at least in 2014). On top of that, I'm guessing we would need their password to set the keys up and access them via stored procedure or whatever other method required. Let me know if that helps at all or if there's any other info I can provide. Edit - I just received further information about their question and it has nothing to do with SQL encryption keys (different encryption all together). Sorry for wasting your time and thanks for your help. I won't delete this for now just because I'm still curious and want to see what people have to say.
There are two ways to build your dataset in Crystal: 1.) Using a command object (direct sql) where you could basically do exactly what you describe within the SQL itself, then reference the aliased column on your Crystal Report. 2.) add in the tables from your DB source, create the linkages, then create a formula to use isnull *(if isnull ({FULL_NAME}) then {FName} + ' ' + {LName} else {FULL_NAME} - then add the formula to your report instead of the column name...
It's pretty much useless as there is not conflict resolution possibility
If you are just using this interactively in SSMS or similar and want more information, try the built-in sp_help (i.e. EXEC sp_help 'MyObjectName') - it returns a lot more than just the columns in a table and also returns information about other object types. There is even a standard shortcut for it: highlight any object name in the code frame and press Alt+F1 and sp_help will be run for that object.
SELECT CONVERT(CHAR(3), YourIdField%1000)
yea, I used that before this. I copied the columns into excel, concatenated them with a ',', copied, transposed, copied again, then pasted back into SSMS, then smashed my face into the keyboard and cried. 
substring(lastname,1,1)+firstname+right(cast(ID as CHAR(8)),3) PS but that's not SARGable at all, so be careful using it in WHERE clauses
Er, for TDE it would be something like this, I guess CREATE CERTIFICATE TDE_Certificate FROM FILE = '&lt;full file path&gt;\&lt;Trusted_Certificate&gt;.cer' WITH PRIVATE KEY (FILE = '&lt;full file path&gt;\&lt;Trusted_Certificate_Private_Key&gt;.pvk', DECRYPTION BY PASSWORD = '&lt;Certificate password&gt;')
I'm the same, can't move for their ads at the moment - especially on Twitter.
To r/nosql with this fluff.
I would go this way. Might need some zero paying though.
You will find the new DROP IF EXISTS in SQL2016 helpful then. https://blogs.msdn.microsoft.com/sqlserverstorageengine/2015/11/03/drop-if-exists-new-thing-in-sql-server-2016/ 
The import wizard can save the SSIS package it generates for you, then you will find the flat file definition object in there. Make the package part of an ssis project and you can re-use the connection object as much as you want. 
 %TMP%\out.csv Might work if the variable expansion is supported. You could also use: C:\Users\Public\Documents\out.csv
I just drooled a little on my keyboard....
I'd never assume it would be haha. But I assume that at least the objects will be there, source and destination defined, and all the work will be to make the customization that you would do anyway.
You need to get a decent explanation on why the program works that way. It sounds like a lazy app developer to me....
I know i'm late to reply to this. I just want to put my 2 cents in. Will this Name + ID ever need to be unique? JSmith (John, Joe, Janice, Jane, Joyce, Jack). I know the chances of there being a match with the partial ID attached is low, but the possibility is there.
What RDBMS? Not sure why you would want this (terrible normalization); but for SQL Server you could do something like: ;WITH cte AS ( SELECT rn= ROW_NUMBER() OVER (PARTITION BY identifier), identifier, people, places, things FROM SourceTable ) INSERT INTO DestinationTable(identifier, people, places, things) SELECT identifier, CASE WHEN rn=1 THEN people ELSE NULL END, CASE WHEN rn=1 THEN places ELSE NULL END, CASE WHEN rn=1 THEN things ELSE NULL END FROM cte Edit - **This is more preferable unless there is a very good reason NOT to do this**. I'd just skip the NULL bit and filter the above CTE to only include one row per identifier: SELECT identifier, people, places, things FROM cte WHERE rn = 1 
After reading [Difference between these 2 statements?](https://www.reddit.com/r/SQL/comments/4k76gh/mssqldifference_between_these_2_statements/) I researched the difference between IsNull and Coalesce. This article does a great job of explaining what these two functions do.
I don't understand the question. 
 /* Select dbo.fn_eatsnakeeat(1,2) Select dbo.fn_eatsnakeeat(5,2) Select dbo.fn_eatsnakeeat(5,10) */ Create Function fn_eatsnakeeat ( @Criteria1 int , @Criteria2 int ) Returns int as Begin -- Declare the return variable here DECLARE @results int , @Modifier1 int = 0 , @Value int IF @Criteria1 = 5 Begin Set @Modifier1 = 10 End -- Separate If Statement IF @Criteria2 = 10 Begin Set @Value = 10 + @Modifier1 End Else Begin Set @Value = 15 + @Modifier1 End Set @results = @Modifier1 + @Value Return @results End GO 
You'd have to do something like: select id from us_holiday where date = (select birthday from birthday) http://sqlfiddle.com/#!15/85dbc4/5
$12 an hour. 
NYC and Industry doesn't matter. EDIT: I am open to any industry right now, as I have worked in the finance industry but am open to other industries.
So you are just trying to become an analyst in ANY industry? Well, my first piece of advice is to learn SOMETHING special. And what I mean by that is you need to pick up on how to analyze your data. You can't just go into an analyst position and not know your data. KNOW YOUR DATA. Find out how that data gets generated and why it gets generated, stored, and looked at and by whom. People love to talk about themselves and show off. Ask someone that is 20-30 years in the industry and ask them what this little piece of data is and ask the story about it. They will most likely go on and on about it. Rinse and repeat. Do that until you feel comfortable asking that person what they need. Ask, "if you could have any report that you wanted, what would that be?" Then focus on that report and try to generate it for them. They will feel like they "own" you and they will go to you specifically for their needs over someone else. Then you get to be valued by that person. Rinse and repeat. You will then be valued for customer service and along the way you will learn everything that they know and possibly more because you have access to all of the data. Good luck friend. 
I have worked in the financial services industry for an asset management company doing data analysis which involved web scraping to clean and upload internal CRM data. I have also used VBA to automate financial workbooks, fuzzy map folders for creation and file placement, and pattern parsing for text file conversions. I want to be an official SQL Developer though.
Use MERGE
[removed]
[removed]
A MERGE statement will help you but there are multiple ways it can be done. If you search for the term "Slowly changing dimension" this will help you find information on what you need. Here's a quick link: http://www.made2mentor.com/2013/08/how-to-load-slowly-changing-dimensions-using-t-sql-merge/
Well, according to your description, you are rbar-ing through 100bn record combinations in a tsql loop to do a substring match. Tsql is not meant for those kind of scenarios to begin with. So there are, most likely, dozens of other ways to achieve whatever your end goal is, the question is - what is that you getting out if your current solution? BTW, 100k records, although not too intimidating a size, can be a nasty multiplier in the nested loops scenarios. Could be a good time to consider bringing your table2 into 1st nf, maybe. 
I'd suggest reevaluating why you have multiple values in a single column (Table2.role). This would suggest to me that the database and/or process wasn't designed properly and it's making things more difficult than they need to be. 
I would just download SQL Server 2014 Enterprise. That's being very optimistic you will encounter such newly refreshed production servers. Most cases are 2008 R2 / 2012 and now trending is 2014. I tend to work on what I can expect to see when people ask me "How long until we're back up". I did just download 2016 Ent. and installed SmashSQL which I happen to be loving right now. 
Assuming that I understand your situation, I think this could help: http://pastebin.com/nfByjJDs
Is there a particular error message you're getting or is it just executing without the result you'd expect? I'm kinda confused with your query, it looks like you're inserting to a temp table from your table, then just joining the temp table back onto the same table? Are the old and new data sets not stored in different tables? 
Go for developer, it's the same as Enterprise just a different license. Perfect if you're a student learning and not actually putting anything into production.
[removed]
[removed]
[removed]
[removed]
[removed]
These two courses at Udemy seem to cover most advanced SQL functions: https://www.udemy.com/advanced-sql-tutorial/ https://www.udemy.com/sql-advanced/ Oh, and check this website too: http://sql-ex.ru/ It teaches SQL coding through exercises (available in English)
Thanks but 1. He uses Oracle 2. His accent is *too* rough I love Udemy but so much of their material is from non-native English speakers.
Oracle or Microsoft is largely, inter changeable. There are Differences but it'd not the language that matters, it's your ability to think in sets and know the functions of the language. And pl sql and tsql is very similar. 
okey thanks!
Developer has all the features so that is the one for experimentation, development etc.
[removed]
[removed]
Are you referring to [Amazon Aurora](https://aws.amazon.com/rds/aurora/details/), or [Microsoft Azure](https://azure.microsoft.com/en-us/)? I assume you meant the former. I have some MySQL instances running on Amazon RDS, and I've contemplated migrating to Aurora. They actually offer a lot of the same features, but my use simply won't ever scale (famous last words) to justify the (admittedly minimal) pricing difference available with Aurora.
I wouldn't think that CTE would require a course. Now, when you throw hierarchical into a CTE, that's a bit more involved. Even then, I don't think these would still need an online course. Just search for youtube videos and do some googling. There's plenty of blogs out there about how to do this stuff. 
Wow... I was convinced I wrote aurora.... Yes, that's what I meant, thanks. I also noticed that their DB instances begin at r3.large which is about $200/mo. Most of my MySQL databases are also for websites &amp; so I really wouldn't need that much either. Just curious if it was worth it for those who do scale to that size.
[removed]
I have no idea what this is.
Seems to be a spambot that went out of control, based on its post history.
Off topic but why would id not be automatically generated?
Maybe its not an identity column or IDENTITY_INSERT is on.
If you need to compare both sides, you can do this : ;WITH table_2_splitted as ( SELECT t2.guid, fn.Item FROM table2 t2 CROSS APPLY dbo.StringSplitt(t2.Role, ',')fn ) SELECT * FROM table_2_splitted t2 FULL OUTER JOIN table1 t1 ON t1.guid = t2.guid WHERE t2.item IS NULL OR t1.role IS NULL 
I use [enterprise architect](http://www.sparxsystems.com) - the cheapest version is $135, but there's a 30 day trial as well...
one way to do it: with data as ( SELECT computer , convert(date,lastcheck) as LastCheck_date , cast(DATEPART(hour,lastcheck) as varchar) as LastCheck_hour , cpu from heatmap where LastCheck &gt;= DATEDIFF(DAY, 7, GETDATE()) ) , get_max_val_per_hour as ( select computer, LastCheck_date, LastCheck_hour, MAX(cpu) as max_cpu from data d group by computer, LastCheck_date, LastCheck_hour ) select computer, LastCheck_date, LastCheck_hour, max_cpu , (SELECT count(*) FROM data d1 WHERE d.computer = d1.computer AND d.LastCheck_date = d1.LastCheck_date AND d.LastCheck_hour = d1.LastCheck_hour and d.max_cpu = d1.CPU) nr_of_occurrences from get_max_val_per_hour d 
Thanks. This gives me exactly what I wanted. Seeing the results, I am now thinking that the data would be better represented in ranges instead max since there may be only one occurrence of the max value but multiple occurrences in neighboring values (e.g. 90-100, 80-70, etc.). Would this be difficult to change it from max(cpu) to ranges in increments of 10?
You can't go to XLSX? It has no row limit. Think about how long ago 2003 is and then make plans to upgrade. Old XLS is literally the only format that is going to restrict your export. After that, your restrictions are mainly memory and hard drive space. 
Assuming this isn't an interview, you should know various ways to remove duplicate values from a table. 
Knowing how to keep duplicates out of a table is good too.
From a data perspective, if I were interviewing you I'd ask you how to construct a database to support a simple shopping cart application. What tables would you need for users, items, orders. This is all modeled in many example applications online. Think about how to add items to the card, then view, edit, and remove items.
&gt;COALESCE('['+[Object ID] + '] '... SQL actually has a native function, quotename, that does this work for you. COALESCE(quotename([Object ID]) + ...)
Don't sweat it, I've broken it to him many times on the other accounts he's had.
Your description of the problem is to vague. I think you need to start with a data model even if it is very small. You can start reading about Entity relationship modeling and Normal Forms specially 3NF. Basics if you need to do something with data
Instead of them entering new values into a table, you could have them call a proc with the values to be entered into that table. The PROC would do validation, then you'd have the proc update/insert into all necessary tables. IMHO triggers are nasty beasts that should be avoided.
Usually, some SQL (about 20-25%), but mostly other programming/scripting languages. Why do you want to know?
For technical knowledge level, I would not hire anyone entry-level that does not understand or know how to use the HAVING clause and also basing window functions like RANK. If they know how these work logically they can pick up the rest as they go. Though how they fit in with the rest of my team is more important.
This depends on what platform do you have, but it seems that you might find an analytical function called lag helpful: lag(operation_code) over (partition by location, prod_id order by timestamp) https://msdn.microsoft.com/en-us/library/hh231256.aspx 
Thanks! Will try momentarily. Looks like what I need. 
Yes, that's mostly true for developer jobs...but there are some jobs like that of a data analyst or a data warehouse developer, where SQL is much more heavily used (50-75%) and in those roles you rarely use other programming skills...maybe just a dash of shell scripting...and a good dose of understanding entity relationships, ERDs, etc.
Some more (' ') &lt;&gt; 
the only thing I'm getting from that is like 5 syntax errors in that snippet mixed together. Can you clean that up a little ?
when you look at the result set, is it a postive or negative value? If it is positive then the -1 is there to make the data set positive. Edit add: Also you dont have to keep putting dbo.column name. In your from clause put a an alias like FROM table A then when you refer to the columns you can put a.[itemno] etc.... I really would suggest http://www.sqlcourse.com/ and start learning some basic skills with select statement creations
Yep and that makes sense. It would be an item ledger entry as a negative (since it is a Sale coming out of stock) so to make the query show a positive that must be why. Thank you good sir! I opened the link. Should I work through the MS SQL or just the general SQL courses at the left side of the page?
What is a table aliases? Just a way to name a table so its less messy when entered into a query? How do you do that?
Awesome thanks so much!
Ok, I think I get it. I'll have to research a bit more for sure!
Reddit ate some of it, thinking it was markdown.
Def feel like a goof for missing that!
http://pragmaticworks.com/Training I've been attending these free live training sessions the past few weeks. 2 weeks ago we went over CTE's and Pivot, but I'm sure it will be revisited. This week we went over tabular models. Every week is something new and SQL related! I highly recommend it. 
 with `/* wtf? */` as the most common comment!
COALESCE is superior because it's not limited to 2 options and it will return the first non null result in the list, so it works just like is null for 2 options yet gives you more flexibility
http://ondras.zarovi.cz/sql/demo/?keyword=default isn't as good as vertabelo, but it's free
I've actually been having a frightful time with this and still haven't solved it. I wrote a merge which functions, but doesn't operate as expected. I'm burned out today but will check your idea out tomorrow. Much appreciation.
`COALESCE` can also bite you because it can return a different data type depending on what you pass in - it's based on data type precedence. `ISNULL` always returns the same data type that you're checking against. This can also bite you if you're checking a `char(5)` and want to return `No value` if `NULL` is found. One isn't necessarily superior to the other - pick the right one for the task at hand.
I was making the point that certain functions exist in 1 program that may not be available in others. ISNULL was just an example that exists in Microsoft SQL Server but does not exist in Oracle. The OP would have to learn to use NVL() instead.
You gained points by adding the [MS SQL] tag, but you forgot to format the code. You just need to add 4 spaces before the code to make it format like this: SELECT dbo.VOXTURNS_MONTH.[Item No], dbo.[KD Live$Item].Description, SUM(dbo.VOX_TURNS_MONTH.Qty * 4 * - 1) AS Projected_Annual_Usage Other posters already explained the math behind the multiply by -1, what alias is, and good link to improve your SQL skills, I'm just helping with the reddit formating :D
Not sure but if you run the sql configuration tool, you can switch to shared memory or network connections rather than named pipes.
Nice find! You're still better off in TCP mode if these are remote connections though. http://m.devproconnections.com/database-development/sql-server-performance-tip-favoring-tcpip-over-named-pipes 
Makes perfect sense. Thanks for all the help. EDIT: I also wanted to see if I could re-write your query using two joins, if you're not out of patience let me know what you think of this method? SELECT CLIENT.client_name AS Client , PURCHASE.item_name AS LastPurchase , PURCHASE.purchase_date AS DateOfLastPurchase FROM CLIENT INNER JOIN PURCHASE ON CLIENT.id = PURCHASE.client_id INNER JOIN ( SELECT MAX(PURCHASE.purchase_date) AS maxdate , CLIENT.id FROM PURCHASE INNER JOIN CLIENT ON CLIENT.id = PURCHASE.client_id GROUP BY CLIENT.id ) AS p_inner ON CLIENT.id = p_inner.id WHERE PURCHASE.purchase_date = p_inner.maxdate ;
I'd add columns for "allow/deny" and "order" along with day-of-week and time-of-day-start/end and IP. USERID|ORDER|ALLOW_DENY|M|T|W|TH|F|S|SU|START_TIME|END_TIME|IP START, END, and IP can be "*" or to allow/deny all. Make a unique index [userID,order] so you can't have 2 rules with the same order. For each login load the user's restrictions (I'd call it UserLoginRules since it's allow/deny not just deny) and go through them in order until you get a match and allow or deny login depending on the allow/deny column. Depending on your requirements you can have an implicit allow if nothing matches. Usually I'd have an implicit deny. After re-reading the question you may not just be talking about allowing login but what permissions are valid after the login. Instead of the allow/deny column you can specify the permissions that are allowed/denied by that rule (one column per permission if they are known ahead of time). Then you can use first match in order or you can take the union of all rules that match. You can have the permission columns be "allow"/"deny"/null where the first rule that has a non-null is used or say that any "deny" takes priority (or "allow" but usually "deny") . I'd recommend going by order rather than trying unions since it can be difficult for users to understand the results of unions of rules. 
I don't think it is possible since it's "by design", yet you can try other apps. Excel for one, can handle it easy enough.
Reply to this with the line but do the followingg: put it on a line by itself with a blank line before and after, then put exactly four blank spaces on the front of the line.
Free. One of the best I found http://www.studybyyourself.com/seminar/sql/course/?lang=eng. You can submit exercises online with feedback !
Sort of possible, but not with two result sets from the same query. Instead of having the result panes side by side, you can make the entire query windows (and therefore their nested result panes) side by side. Open two query windows and drag one until it's side by side with the other. Use the "docking" hover icons while dragging if you need to. Run each select statement in its own query window, with both being visible. Sorry, but without a plugin (extension to visual studio/SSMS) I don't think you can do precisely what you want regarding multiple result sets from a single query execution displaying in anything other than the default stacked view.
You will have 3 tables * [User] * [Rule] * [UserRule] Your [Rule] table will contain the rule definition with your add/deny column. [UserRule] is a join table that has a [User] ID and a [Rule] ID and little else if any thing. If you add a new rule it wont apply until you add a join entry to [UserRule]. Any modification to a rule will instantly apply to all users having that rule without having to modify anything else. This is arguably the best design. In a system we had user roles. A role was a set of permissions (and restrictions). We added yet another table that allowed us to individually add or remove a permission thus overriding that in a role for a particular user. This gave us very fine grained user control while keeping storage to a minimum.
If your using SQL server I would write something like the following: Select t.column1, t.column2 from t.table Where ALL in (@parameter) or column1 in (@parameter) - - use : for Oracle You can then specify ALL, 0, 1 in your multivalue parameter. Granted this may give you a data type error if column1 is an integer. Now that I think about it, why don't use just use the multivalue parameter? Anyways, phone, good luck. Edit* You don't need to declare your variables when using SSRS. Be sure your parameter name in your query matches your parameter name in SSRS. 
Can you be more specific with what you're doing? General rule of thumb in SSRS is this: Do as much of it in SQL a you can and as little of it as necessary in SSRS. Are you trying to join on a parameter? Sounds like you're doing something in Boolean and IIRC SSRS treats Boolean as "true/false" not 0/1. 
There are three ways you can determine who updates a given record: 1. You can use built-in auditing 2. You can program your own audit trail 3. You can ask your users 
You can use a code step to create a variable with the file name. Use the current date in ISO-8601 format without delimiters (YYYYMMDD) and pass that to your file output step.
Or he fetched the value from a sequence maybe?
Users have poor memories. Also, there was that one occasion where a user was not entirely truthful. 
Oh, absolutely. I wouldn't trust a user, either. I'm just giving you a complete list of options. I'm not saying they're all perfect.
Been thinking about that. Likely will be taking the plunge this July
1) trigger 2) change tracking 3) change data capture 4) database audit 5) revoke everyone's write access and fire the first five people who stop by your desk to complain
How can I build my own audit trail? Any links available online?
In the corresponding links, I don't see audit specification that captures table level changes. Closest thing I see is TRANSACTION_GROUP and that is also throwing error when I use this code. Error message: Invalid syntax near Transaction_group CREATE SERVER AUDIT SPECIFICATION [Job_Audit] FOR SERVER AUDIT [ServerAudit] ADD (TRANSACTION_GROUP) WITH (STATE = ON) GO
Triggers have huge overhead and Change Data Capture stores only the data that has been changed but not the user who modified them isnt it ? 
I should've mentioned it in my original post but I am not supposed to use third party tools.
Sorry, my programming vocabulary range isn't the biggest! For one parameter, I am trying to get 3 different results: active, discontinued, and both active and discontinued. The value of active is 1, and discontinued 0. I am able to get the results for the former two but not the third. I have tried both the multi select parameter and the standard dropdown. (Forget I brought up the other parameter, I have that one working)
I just created and enabled couple of audits using the built in SQL Server auditing and I didn't come across Extended Events yet. Am I doing something wrong? And while we are at it, is there any limitation that only one database audit specification per each server audit ? I was not able to create multiple audits for a single server audit. It gave me error saying that 'there is already a database audit specification exists for the server audit'
It's too much to paste here, I put it up here: http://pastebin.com/GEefBrp2 If you run it with @help = 1, hopefully that should explain everything. You mentioned wanting to see the step execution messages, using @step_info = 1 will do that. If the job is running, it will also show which step is currently being executed (I was very happy to get that working). One thing to note, it uses a SQL 2014 feature of creating a nonclustered index when declaring a table variable. If you're running an older version of SQL, you'll need to remove that INDEX line from the table variable declaration in order for the proc to work.
You can store whatever you want from a trigger. If the user is logged in the SQL Server then SUSER_SNAME() will return the user of that connection, and in the case of a trigger, the user that is changing that table, also don't forget to save GETDATE() to know exactly when the table was changed.
Damn Ola Hallengren, I really appreciate that. That does look like a lot of work. Can't wait to mess with it tomorrow. 
I think what you are mostly looking for is something like this: [Stairway to AlwaysOn](http://www.sqlservercentral.com/stairway/112556/) Probably one of the best sites for learning and all things SQL Server. Below are some extra resources related to the cluster and always on modes with other useful content provided by links throughout. [WSFC Quorum Modes and Voting Configuration (SQL Server)](https://msdn.microsoft.com/en-us/library/hh270280.aspx) [Availability Modes (Always On Availability Groups)](https://msdn.microsoft.com/en-us/library/ff877931.aspx) 
&gt; JOIN(Parameters!Status.Value,";") This will work if you have a split function in your stored procedure. &amp;nbsp; &gt; where Status in (select Split.value from dbo.Split(@Status,';')) ^^Mind ^^the ^^semicolon ^^[;] ^^in ^^the ^^JOIN() ^^and ^^in ^^dbo.fn_Split(). If you don't have a split function in your database, you find one on the internet. Like this one: http://stackoverflow.com/a/19885283.
&gt; They don't use triggers; the application is just written to do the update for everything. I have an application that does this too, but it's...spotty. The trouble with doing things this way is that you have to trust that the developers **always** update those fields on **every** update. At least in the case of the app I deal with, that doesn't happen. The other issue is that you don't know *which field* on the record the user changed - just that *something* changed. &gt;The audit table is a huge pain in the ass to use because we normally have about 1,000 concurrent users (central database for a school district) so the overhead is significant One could lessen that overhead by putting the audit writes into a queue or otherwise doing them asynchronously, rather than do them serially with the "regular" updates. If scalability *really* became an issue, you could put the audit trail in a different database or even another instance.
So your SSRS should be as simple as: WHERE Parameter IN (@Parameter) OR @Parmeter = 3 But keep in mind that @Parameters in SSRS are case sensitive, so if you name it @PARAMETER then you have to use that same case in the query itself or it won't work. Here is a very similar report that I have: [1](http://i.imgur.com/H1HzO4p.png), [2](http://i.imgur.com/4ha9pCn.png), [3](http://i.imgur.com/u9KyIfW.png). And the query looks like this: SELECT , [Paid Flag] FROM ( SELECT , CAST(CASE WHEN C.CampaignCode IS NOT NULL OR D.LSC = '96' OR D.LSC = '45' THEN 1 WHEN (C.CampaignCode IS NOT NULL AND D.LSC = '96') OR C.CampaignCode IS NULL AND A.KenshooID IS NOT NULL THEN 1 ELSE 0 END AS bit) AS 'Paid Flag' WHERE LeadCreatedDate BETWEEN @STARTDATE AND @ENDDATE+1 GROUP BY , CAST(CASE WHEN C.CampaignCode IS NOT NULL OR D.LSC = '96' OR D.LSC = '45' THEN 1 WHEN (C.CampaignCode IS NOT NULL AND D.LSC = '96') OR C.CampaignCode IS NULL AND A.KenshooID IS NOT NULL THEN 1 ELSE 0 END AS bit) ) A WHERE [Paid Flag] = @ISPAIDMEDIA OR @ISPAIDMEDIA = 3 You're basically tricking SQL and saying when the flag isn't equal to 1 or 0, but is equal to 3, then ignore the where statement all together. 
I think our apps are cousins.
Thanks for the gold!
reformatting so that people as well as computers can read it -- SELECT [Item No_] , SUM([Valued Quantity]) AS Qty , MONTH([Posting Date]) AS Month , YEAR([Posting Date]) AS Year , SUM([Cost Amount (Actual)]) AS Cost FROM dbo.[Kawartha Dairy Live$Value Entry] GROUP BY [Item No_] , [Item Ledger Entry Type] , MONTH([Posting Date]) , YEAR([Posting Date]) HAVING ([Item Ledger Entry Type] = 1) AND (YEAR([Posting Date])= YEAR(GETDATE())) AND (MONTH([Posting Date]) &gt; MONTH(GETDATE()) - 4) notice you have a column in your GROUP BY that isn't reflected in the SELECT list -- since each item is likely of only one type, i suggest you remove it from the GROUP BY 
&gt; I want it to GETDATE but instead of grabbing the last 4 months of data instead get the next 4 of the previous years. So for today's date, instead of getting all data from February 2016 forward, you'd want to get the data from June, July, August and September of 2015? HAVING ([Item Ledger Entry Type] = 1) AND (YEAR([Posting Date])= YEAR(GETDATE())) -1 AND (MONTH([Posting Date]) BETWEEN MONTH(GETDATE()) +1 AND MONTH(GETDATE()) +5)
This of course creates a problem on year boundaries. But then so did your previous version. Functional equivalent, last 4 months. HAVING ([Item Ledger Entry Type] = 1) AND [Posting Date] &gt; DATEADD( Month, -3, DATEADD( Month, datediff( month, '2015-01-01', getdate()), '2015-01-01')) The inner DATEDIFF gets you the number of months from Jan 1, 2015 (an arbitrarily selected first of the month) The inner DATEADD gets you the first of GETDATE()'s month (May 1st, 2016 today) The outer DATEADD gets you the first of 3 months prior to that (Feb 1st, 2016 today). Now to do something similar for the *next* 4 months of the previous year HAVING ([Item Ledger Entry Type] = 1) AND [Posting Date] BETWEEN DATEADD( Month, -11, DATEADD( Month, datediff( month, '2015-01-01', getdate()), '2015-01-01')) AND DATEADD( Month, -7, DATEADD( Month, datediff( month, '2015-01-01', getdate()), '2015-01-01')) Today that would get you between Jun 1, 2015 and Oct 1, 2015 And both versions cross year boundaries just fine. 
Can you throw out the full query you're using? I am pretty sure I got rid of some parenthesis and Microsoft Query may be throwing some back in and that may be causing your problem.
Thanks for the input. I will reference this in the future!
[Gotta be careful with `BETWEEN`](http://sqlblog.com/blogs/aaron_bertrand/archive/2011/10/19/what-do-between-and-the-devil-have-in-common.aspx)
Here's a comparison of the editions http://bit.ly/1NOnnsF Here's some of the best features Developer/Enterprise has over standard: -Online indexing -Table and index partitioning -In-Memory OLTP -Data mining query transformation A lot of shops can get by on Standard. Companies who have massive workloads and 100s of transactions per sec i.e. CRUD operations against the DBs are better off paying the premium for Enterprise.
is this because of the table name 'Talent/Group' ?
Yes. Anybody correct me if I'm wrong, but I beleive the "/" character is reserved for special purposes. Try renaming your table to something like TalentGroup or Talent_Group.
unfortunately this is to Band-Aid a programing flaw in a program from an outside vendor. They are unwilling to fix the issue, and we are unable to manipulate their code. 
Can't you just store them as a numeric or text values such that 'Thu Mar 24 15:07:42 EDT 2016' would be 20160424140742 and then you can simply simply compare that to the pre-calculated date from 30 days ago e.g DELETE FROM table WHERE table.date &lt; '20160426000000'
You can get current date in seconds from 2004 with select strftime('%s','now') or current date - 1 month SELECT strftime('%s','now','-1 month') so you can do something like delete from your_table where strftime('your_date_format', 'date_column') &lt; date('%s', 'now','-1 month'); the details on how to convert your string to nr of seconds you can find out yourself here is some documentation https://www.sqlite.org/lang_datefunc.html 
Round trips, to my understanding, are basically going back and forth to the server to get additional data. A sproc does not inherently force someone to do this, but having a database guy write it helps focus the developer on getting his data all at one time. So one example I heard about a long time ago is that the relative time for the server to validate the credentials, open the connection, calculate the plan, etc is vastly longer than just the time it takes to read the data. So for an analogy, say you have a shopping list. It takes you a lot of time to get it together, start the car, drive to the store. But once you're there you want to get all of the stuff on your list. It's much more efficient than going to the store, then going home, for each item. Back in the days of hard drives, the analogy was that if you do a lot of round trips, it's like driving back to the store for each item on your list, but the relative time means the store is located 100 miles away. So an actual real world example would be something like: Select name, nameid from customer where custid=1 Then, Select comment from comment where nameid =1 and commentid = 1 .... Select comment from comment where custid = 1 and commentid=2 ... etc. When someone is writing a webapp, it's often easier to go back and forth to the server using the code, because joins are sometimes not well understood by front-end guys. If instead you provide a proc that takes custid as an input, then returns all comments by that custid, then you basically only have one trip to the database. The front end guys can put the ouput of that proc into an array and loop through it at the front end, instead of essentially putting a loop of database calls that get one row at a time from the database. This chews up the server resources just to provide the overhead of creating a connection, validation, etc for each individual call. 
pretty sure this is the case. that's also quite a few joins ya got there.
Like by itself is not terrible. Like with wildcards (and like with preceding or inline ones in particular, eg '%foo' and 'bar%baz') however is terrible. OP is also casting values on the fly (not efficient) and doing 20+ joins against the same table (not inherently bad but questionable).
is it spool space? this is a monster of a query. If in Tera id work a bunch of volatile tables to split the load and collect some stats :D
Indeed. Leading with a "%" makes the argument "[non-sargable](https://en.wikipedia.org/wiki/Sargable)", because the engine can't rely on a predicate to make use of indexes. So OP is scanning the table 24 times as if it were a heap, since indexes are useless.
Replace getdate() with the date you are interested in (for example some variable or some column expression). Let's call this new expression &lt;your expression&gt; Then replace [Posting Date] &gt;= dateadd(month,-4,&lt;your expression&gt;) with ABS(datediff(month,&lt;yourexpression&gt;,[Posting Date])) &lt;= 4
At this stage you might just be better of using something like elastic search to run the search instead of trying to use the DB for search
you need to make sure you have the right indexes. try this: 1. Cluster wp_posts on ID 2. cluster wp_postmeta on post_id 3. cluster wp_icl_translations on element_id 4. create an index on wp_postmeta (meta_key, meta_value) 5. create an index on wp_icl_translations (element_id, language_code) 6. rewrite the bottom of the query - it is much harder to read than the rest 7. lose the not in('','','') - these are inherently slow. Come to think of it, this bit does nothing but hog resources, right? AND wp_posts.post_type IN ('post','page','attachment','_pods_pod','_pods_field','editors','sidebar' ) ) OR wp_posts.post_type NOT IN ('post','page','attachment','_pods_pod','_pods_field','editors','sidebar' ) ) 8. do all your functions (cast, row, concat) in a seperate query, moving the data into a new table, or something. Remember that your server has to hold every function in its head (in RAM). 9. make sure that TempDB has space allocated. People often forget to do this and leave it at the default 8MB. 10. You can rewrite the query so that you only join to wp_postmeta once, and replace all your 'and's with 'or's. 
Dear lord, that's a truly terrible query. Some if it makes me wonder if this is an attempt at SQL Injection. There's a ton of obfuscated code in here. Look, all of the JOIN predicates have been pushed down into the WHERE statement. That's functional but ugly. What's more interesting is the fact that most of those JOIN predicates appear to be searching for SQL keywords. There's HTML encoded single-quotes (&amp;#039;), some weird concatenation that produces the string value "qvppq1qvjzq0" and more. I almost suspect this could be part of a 2-pronged attack against a WordPress page. Some user leaves a comment or a post that has certain data in it and this query goes off and builds &amp; executes another query based on the data in the comment or post. Anyone else have similar thoughts? EDIT: Thought, the username it's looking for may contain the string 'KDXn' Double-Edit: Holy shit! Look what I found when I searched for 'wordpress 0x7176707071' on Google!! WTF is a "mujahidincyberarmy" and holy crap look at [all of these exploits](http://mujahidincyberarmy.blogspot.com/2015_07_01_archive.html).
YOUR SERVER IS COMPROMISED! 
Oracle doesn't know that there is (and will only ever be, for all time) one value. You could move the sum outside the division? That should do it.
Working on my format, one min
Roundabout way: I would set up a series of CASE statements to test how many spaces are in the name. If you can reliably say: one space = they have a first and last name, 2 spaces means they definitely have a first, last, middle, 3 spaces means first, middle, last, suffix - then you can parse it out with SUBSTRING in each CASE. But if they are going to have unexpected values past my basic example, what a mess.
But two spaces often means first, last, suffix. And then there are commas indicating last name first. There's no good algorithmic approach to handling names because there is no common naming standard for human beings. Anything you try to implement is only going to be a good guess.
Yup, you're right. My example assumes really clean data..that's almost never the case.
Fixed version of the script to address the issue mentioned by /u/jmremote (if it's what OP wants) if object_id('tempdb..#tmp_t') is not null drop table #tmp_t create table #tmp_t (eeid int ,code_1 char(1) ,month_id int ) insert into #tmp_t (eeid ,code_1 ,month_id) values (1234, 'A', 1) ,(1234, 'R', 2) ,(1234, 'R', 3) ,(1234, 'C', 4) ,(1234, 'C', 5) ,(1234, 'C', 6) ,(1234, 'C', 7) ,(1234, 'R', 8) ,(1234, 'R', 9) ,(1234, 'R', 10) ,(1234, 'R', 11) ,(1234, 'R', 12) ;WITH Lags AS ( SELECT eeid ,code_1 ,month_id ,CASE WHEN month_id - LAG(month_id) OVER (PARTITION BY eeid,code_1 ORDER BY month_id ASC) &gt; 1 OR code_1 &lt;&gt; LAG(code_1) OVER (PARTITION BY eeid ORDER BY month_id ASC) THEN 1 ELSE 0 END AS LagMark FROM #tmp_t ) , RunningLags AS ( SELECT eeid ,code_1 ,month_id , SUM(LagMark) OVER(PARTITION BY eeid ORDER BY month_id ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS RunningLag FROM Lags ) SELECT eeid ,code_1 ,month_id ,DENSE_RANK() OVER (PARTITION BY eeid ORDER BY RunningLag ASC, code_1 ASC) AS RK FROM RunningLags ORDER BY eeid ,month_id 
Weird, I can't get any CAST to work on that site, for MySQL 5.6. If I switch to one of the SQL Servers it works. I checked docs and MySQL 5.6 does support CAST. Dunno why it won't work on that site. Maybe a bug.
I had to do this years ago. Wished I'd kept the source. It was used to parse names in the Dun and Bradstreet data. Here was a favorite of mine: Fr. David O'Donnellan and Sister Mary Agnus
This is it
In Oracle: with data as ( select '0' val from dual union all select '00' val from dual union all select '00' val from dual union all select '000' val from dual union all select '000' val from dual union all select '1' val from dual union all select '10' val from dual union all select '02' val from dual union all select '003' val from dual union all select '089' val from dual union all select '000000' val from dual union all select '0000007' val from dual ) select * from data where regexp_like(val,'^[0]+$'); Gives: VAL 0 00 00 000 000 000000 
 WHERE CAST(id AS SIGNED) = 0; or WHERE CAST(id AS UNSIGNED) = 0; http://sqlfiddle.com/#!9/a12e9/2
So what if you have thousands of values? Seems like a lot of work to type them all in? (Sorry new to SQL myself).
A Common Table Expression could also solve for this.
 create global temporary table tabAtemp (result int) on commit preserve rows; insert into tabAtemp select col1 from tabA where date between x and y and rownum &lt; 1000000; select b.col1, b.col2, b.col3 from tabB as b inner join tabAtemp as a on b.col3=a.result; select c.col1, c.col2, c.col3 from tabC as c inner join tabAtemp as a on c.col3=a.result; select d.col1, d.col2, d.col3 from tabD as d inner join tabAtemp as a on d.col3=a.result; select e.col1, e.col2, e.col3 from tabE as e inner join tabAtemp as a on e.col3=a.result;
One of the problems in your sqlfiddle example is that the column is defined as an integer. Any zeros that are on the left side of the number will be removed from the value when it's stored as an integer. You might want to store the numerical values as text and then run a query something like this... http://sqlfiddle.com/#!9/3b18de/2/0 This gives you the rows that you want. I'm not sure how to trim out the non-zero characters that you want. 
 I used some of your points and I entered a hard limit. I still don't know why queries like these can run for 10 hours but I got a quick fix and you made someone happy. 
Sometimes I would rather do it with plain SQL without creating any new database objects (like temporary tables or stored procedures). There are probably a dozen ways to accomplish it, but here are two ways to consider. - set transaction read only; This will allow you to do consistent reads across multiple statements. Normally Oracle does *statement* level read consistency. But this command will allow you to have read consistency across your entire transaction. Just close the transaction at the end (commit or rollback). - if possible, combine the queries into a single query using a WITH clause for the first query and a UNION ALL set operator to combine the results This may or may not be the right approach depending on the complexity and similarities of the queries. But your example makes it look simple. Try something like this: WITH A as (select col1 as result from tabA where date between x and y and rownum &lt; 1000000) select 'tabB' tablename, col1, col2, col3 from tabB where col3 in (select result from A) UNION ALL select 'tabC' tablename, col1, col2, col3 from tabC where col3 in (select result from A) UNION ALL select 'tabD' tablename, col1, col2, col3 from tabD where col3 in (select result from A) UNION ALL select 'tabE' tablename, col1, col2, col3 from tabE where col3 in (select result from A) I added the column "tablename" in the results to help you keep the results straight in case you need that. It's untested code! I just typed it up on reddit so I apologize in advance for typos or syntax errors. But you get the idea, I hope. Feel free to ask for any clarification! 
Wow, we had very similar responses. Good to know I'm thinking in the same ballpark as you. Can you comment on whether using "set transaction read only" would be another possible solution? (Even though I believe a single SQL query to be the best solution whenever possible.)
So that works, but even after looking at the code and googling cast I still don't understand why it works. After googling it looks like cast converts a field to a particular type, but I don't see why =0 makes it work for everything... edit: is it because it mathematically equates to 0? If that's the case, then I guess it can't be used for 1's for example?
Yes, because the field is cast as an integer, and any number of '0's will evaluate to zero. This would only work with zeros as you suspected.
Your best bet is to use regex, which would give you a lot of flexibility.
It converts the string of characters to an integer. So on the left side '0000' or '00000000000' are both converted to 0, and 0 = 0. If it was something like '00006701200' it would convert to 6701200 qhich is not equal to 0. You are correct about it only working for 0.
If working in SQL I would create a CLR function in C# or VB and parse out the name there. String manipulation in SQL is limited and sometimes brittle.
It changes the value of the field in the column from whatever it is natively (probably a (n)varchar) to an integer. What this means is that all the *values* in the column (with whatever length constraints, etc.) become *(real) numbers*. So "000123" becomes "123", "00100" becomes "100", and "0", "00", "000" and "0000000" all become "0". (this is for mssql; ymmv)
besides me questioning you saving what looks to be a numeric as a string (ffs ppl datatypes are your friend), just do what /u/rbardy said
no, there is no function for that. You are dealing with freetext data entry, that you try to transform into something normalized (or usefull as I call it). There is no prebuild function, there is no let me google that for you answer. What ever logic you write is going to be wrong in some cases. The difference is, how large your error margin is going to be, and what you write depends on what your data looks like and what margin of error you can live with. //edit : since this is reddit.... I'm gonna spell it out. I am not saying that you did anything wrong or are lazy or whatever someone might read into that. You are very right in saying that you are not the first person to deal with that problem, and there is no real solution to this problem. You are facing a text input to a single field in some form, and you try to make sense of that. Good luck making sense of that. People are very creative in entering freetext to a single field. This is not a question on how you implement some logic to do it, that would just differ in performance. You are facing the problem of coming up with some sort of logic to do what you want to get done.
I wonder if this is faster than the value NOT LIKE '%[^0]%' method.
I'm offering SQL Performance Training for developers. E.g., online: http://winand.at/services/sql-performance-training-online Unfortunately, many people consider "performance" an advanced topic :/ However, there is no practical part in the course, instead I'm including a consulting block into the course itself: you train alone on your own system and can com back to me for questions. The training is based on my book SQL Performance Explained which is freely available online at http://use-the-index-luke.com/. Also http://modern-sql.com/ might be of interest to you: all the "advanced" new SQL stuff that they don't teach you at university (maybe because it's not "relational" anymore). Although I don't have it on my website, I have already done a training for LATERAL, WITH, WITH RECURSIVE and a little bit OVER. Let me know if you are interested.
I was just thinking that you do a fantastic job
The only thing I'd say is to avoid using a NOT IN subselect. They don't play nicely with Null values and are generally terribly for performance. I'd use a EXISTS instead. 
That's exactly what I came here to say.
vague question, vague answer :) one point to start is looking for core domain tables like products, clients etc. Then look for how those core tables are related to each other and what is the meaning of that relation for the business, basically what process do they capture. like orders, order details and payments
https://lagunita.stanford.edu/courses/DB/SQL/SelfPaced/courseware/ch-sql/seq-exercise-sql_movie_query_core/ and subsequent exercise pages 
Thank you for your reply, sadly I have tried this before but there are about 200 tables and even more external tables we connect to, I made a diagram once but the whole thing was such a mess that it was incredibly difficult to read with hundreds and hundreds of links going everywhere... :/ 
I'm a conversion programmer working in MSSQL and I can tell you from experience if this is for a customer and they are expecting any degree of accuracy and/or it will be your fault I'd there are mistakes just tell them it is programmatically impossible and there will be too many errors. We've even had a customer attempt to have their in-house data people parse it out and they gave us the parsed data and we had to point out all of the parsing errors. They finally gave up and let us simply use a "full name" field. As someone else has said, you can always try and get close;so long as you are okay with a certain degree of bad data.
I'm addition, if you arent too worried about a long-running query you may want to add... where prjid not like '%.00' Just in case there are some that already are there OR if you are writing it in a script that might be accidentally re-run. Makes me nervous doing update without any where clause.
1. Become familiar with the popular tables. Look at existing queries that others have written on the team, and take note of what tables are being used. Take a lot of notes and *keep them organized.* 2. Use the SQL Studio Object Explorer to inspect databases, tables, columns and primary keys. The most important attributes of a table are: its column names, their data types, and which column(s) make up the primary key. 3. SQL has nearly limitless capabilities for problem-solving. But there are many different techniques that you will have to master. I'm spit-balling here, but would suggest you learn how to use (in this order): WHERE, JOIN, GROUP BY (and the aggregate functions MIN, MAX, COUNT) and finally CTE's – or "Common Table Expressions" which allow you to combine several queries together. Being on the BI Team, you may eventually want to get into the windowing functions, but that is quite a bit down the road.
&gt; if this is for a customer That's basically what this is for. About 50,000 records. It is hard to believe people actually store data like this. I know it can't be perfect but I know I can get it right most of the time as well. The client will be made aware and there will be signoff. Worst part is most of the people I am parsing out is already in our system so not only do we need to parse it out, I need to find if the exist with us. 
I was just simplifying the idea and you are totally correct, but I think charindex would perform better in this scenario, so the WHERE could be: where charindex('.',prjid ,0) = 0
Noooo no no don't do name-based data comparisons/joins! John A Smith &lt;&gt; John A Smith, they could be two entirely different people... There is ZERO guarantee and you will get really messed up data. Dude, I feel for you, and I get that you want to do the best for them, but if you're doing what I think you're doing you'll regret doing this.
In addition to all of the other good advice here, I would add that you can often get a good handle on relationships by checking out stored procedures. You can also, depending on the quality of the code, throw up in your mouth a little bit or gaze in awe. If my experience serves as any indication, though, throwing up is more likely. A lot of databases start out being created by people who only speak SQL as a second language who, at some point down the road, realize they need to get some natives in on the action. 
Concepts behind complex joins, aggregations and the like (perhaps diving into set or relational theory) with practical examples. Essentially I want to move beyond syntax to understand how the database is interpreting and executing the query. I feel like I know enough to be dangerous right now and I want to avoid writing syntactically correct SQL that does something entirely different than what I think it will do. 
Notepad++ and regex searching a CSV to clean data before a load has saved me a lot of time. It's also nice to add single quotes inside only the third and fourth commas.
How is this data being populated? If it's from an ERP or other enterprise app there is probably a data dictionary or table structure available. 
So I came up with a really convoluted way using a staging table which inserts each column into a table with an ID, applies a sequence to find duplicates, and then re-insert them into another staging table without the duplicates and then concatenating the results... But you're right in that the case statement is WAY simpler. if exists (select 1 from INFORMATION_SCHEMA.TABLES where TABLE_SCHEMA = 'dbo' and TABLE_NAME = 'test') begin drop table dbo.test end; create table dbo.test(ID int,Col1 varchar(100),Col2 varchar(100),Col3 varchar(100)); insert into dbo.test select 1,'Tim' ,'Tim' ,'Cook' union all select 2,'John' ,'John' ,'Smith' union all select 3,'Jane' ,'Doe' ,'Smith' union all select 4,'Jim' ,'Smith','Smith' union all select 5,'Jimmy','John' ,'Doe'; Here's the important part: select ID ,Col1 +case when Col1 = Col2 then '' else Col2 end +case when Col1 = Col3 then '' when Col2 = Col3 then '' else Col3 end from dbo.test; Edit: OH, this is in MSSQL (t-sql) so you might have to alter the syntax a little for teradata.
I've been looking for that JOIN chart everywhere and I haven't been able to find it, do you have a link to a non-camera photo version?
Scratch that, found the stupid thing. http://imgur.com/2mlaF1M also this one is nice too https://s-media-cache-ak0.pinimg.com/originals/52/20/c4/5220c492bc4e1a8b9175aba77ed7d091.png 
How about starting with a book? There are plenty of them for all kinds of SQL languages: T-SQL, PL/SQL, or other languages. For T-SQL you can start with Training Kit (Exam 70-461) Querying Microsoft SQL Server 2012.
Thanks! I haven't seen the second one,good find. 
Woops thought we talked about SQL server sorry .... Thanks for the tip with special chars didn't need that but with remember if I do
Yes. It comes in handy all the time. [RegexBuddy](https://www.regexbuddy.com/) helps a lot when I'm trying to do something particularly complicated.
http://modern-sql.com/
Msg 207, Level 16, State 1, Line 3 Invalid column name 'db'. Msg 207, Level 16, State 1, Line 3 Invalid column name 'true'. Msg 207, Level 16, State 1, Line 4 Invalid column name 'false'.
1) SELECT TOP 1 WITH TIES id, dt FROM table ORDER BY ROW_NUMBER() OVER (PARTITION BY id ORDER BY dt ASC) 2) SELECT id, min(dt) FROM table GROUP BY id
Do you work in a fortune 500? What is your job title?
A simple group by can handle it: SELECT ID, MIN([Date]) as EarliestDate FROM [Table] GROUP BY ID
Get one of Itzik Ben-Gan's [books](http://tsql.solidq.com). He wrote /u/nvarscar's 70-461 suggestion among others.
Most every use case will involve listing an ip and all ports that belong to it (or vice versa). I'm exploring the option of a convenience feature to tie an internal and an external ip together under an arbitrary host name to easily view the full picture of a machine. I think I understand what you're suggesting (much thanks to /u/grumpypenguin for the keyword to Google). I'll do some more reading. Thanks! 
As a contract developer who works at new companies all the time, one of the first things I do is go into ssms and look for any database diagrams. If they don't exist, I'll create one. If various relationships are already defined, the tool will display them. Otherwise I just place the tables around as neatly as I can with what look like related tables close to each other. Next I print this out and tape the pages together. It can be quite large - six feet per side is typical. Finally I'll ask to meet with BAs and other developers in a conference room. With that diagram splayed across the table, we'll discuss what it all means. People are receptive to this as it can be fun and interesting for everybody - scribbling lines around and hunting for that elusive table with a foreign key to some other table. This has always been a particularly effective way for me to learn not only the data structure but the business too. Tldr; print out a data diagram and sit down with people who can explain it.
Why so glum bout the market?
I found the Joe's to Pro's SQL Queries book extremely helpful when I first started with TSQL 
What are the tables and how do they join? With what you described it would be something like SELECT STATE , COUNT(*) FROM ADDRESS ORDER BY COUNT(*) DESC, STATE ASC GROUP BY STATE it's possible you have to switch the order by clause and group by (I always forget which comes first and don't have a computer to test) Edit. There should be asterisk in the parentheses not sure why they aren't showing Edit 2. Thanks for the gold kind sir
I'll give this a try, thank you! EDIT - Yep, that did it. I had to make the changes you mentioned, final code looked like this: USE HandsOnOne; SELECT STATE, COUNT(*) FROM ADDRESS GROUP BY STATE ORDER BY COUNT(*) DESC, STATE ASC;
Do any customers or more addresses? You probably have some kind of customer ID field. If there are duplicate IDs in your address table then you'll need to get more complicated, if not, ignore me.
 You would just change the asterisk to distinct (name of the customer ID field) or a where clause designated a field that shows the default address
Assuming there isn't a day field or current flag. Guessing from what we've seen so far, OP is probably fine without but figured I'd mention it.
not sure if posgres supports this but if they do, learn window functions. 
The degree or certification will help you get an edge over another candidate without a degree or certifications; however, 90% of what you need to do day to day will be self taught and learned on the job. 
If you are looking to learn about writing sql queries any books by Itzik Ben-Gan. For learning how to administer and the basics the Microsoft Press Books are a good start. For how sql works I'd recommend books by Kalen Delaney, Paul Randall, Kimberly Tripp. There is a ton of good blogs out there with lots of information. If you are on Twitter check out #sqlhelp. Good luck. 
It shouldn't need escaping, if it's part of the actual string. INSERT INTO TABLE (FIELD) VALUES ('--aass') I know you said MSSQL and not MySQL but I'm certain it doesn't act any differently in this case. http://imgur.com/r8WJHNU.png
it wouldn't. 'abc' + '--123' + 'def' will concatinate into abc--123def on mssql without any problem.
That's perfect. Thank you!
As you have stated the problem --aass is a string to go into a varchar column. Right? As so it is only a constant in the sense that it is a literal constant. That is a predetermined sequence of characters that does not come from a variable or column. You declare a literal constant by putting the characters inside single quotes. There is a memory trick that I found helpful. SQL = ***S***ingle ***Q***uoted ***L***iteral. 
* What other services is he running that you are not? * Does he have ***anything*** open besides SSMS? [Smack him if he does] * Does he have a screen saver? DISABLE THAT. There is ***something*** that he is doing that slows it down that you are not. look at everything and trust nothing. One other question: Are you restoring from a network location? Could be a balky NIC or something in the path to the resource that is causing contention. Try putting the backup on an external SSD plugged into a 2.0 USB port directly. We used to have a big external that we kept for just stuff like this. They used to get pissed at me for running giant restores across the network that would take down e-mail and VOIP.
Where are you from, OP? 
I'm a programmer also in search of a career change. So databases was part of the job, SQL Server, MySQL, Oracle, Postgres, CouchDB, Mongo. But I focused normally on coding. Hot topics at the moment: - Real Time Aggregation - Data Visualization - Big Data - Not Only(NoSQL) I strongly believe PostgreSQL, makes all of the above possible. So I'm toying around with CitusDB Wikipedia example. I'm going to SPAM some of my ideas below, you might find it interesting. For Data Visualization I export it to csv and use spreedsheat graphs or search for a tool that can speak to Postgres. Personally I would go D3.js and access my data via [PostgREST](http://postgrest.com/) but it might be out of your scope. **EDIT** reddit was not reading my Markdown correctly so I moved it to a [gist](https://gist.github.com/Morabaraba/8b13d40dce4806dbc180b129ffcb76ed)
That's interesting. I have a little programming experience from home projects, and prep to teach computers at my school. Otherwise, most of my efforts have been in My SQL. 
Yes it is returning 1
Yikes. As if I didn't have enough to be nervous about with security.
&gt;I always put in a column with autonumbers for every table I make. Is this simply enough? Do you understand *why* you're using an autonumber in every table? IOW, was it a conscious decision to do so? Is there anything in those tables *other than* an autonumber that you could use for the same purpose? Do you actually use it for anything significant, especially for looking things up? Do you ever look up using other fields?
If your backup strategy is primarily dependent upon Dropbox, you're gonna have a bad time.
Index depends on how you are using your tables. Lets say you have a Client table and a Sells table, both have the index in their ID field. Now you make a view linking Client.ID with Sells.ClientID, in that case your index will work fine, but now you make a report where you have the same view but you filter it using Sells.Date to know when a Sell was made, it would be a good idea to create an index using the ClientID and Date field in the Sells table.
I guess it could be feasible if your full backups compress down to 300KB though...
OMG it was so simple!! I was just doing it the hard way! And just a quick question. What do I have to use in order for this process to be automated? I mean, what if someone adds a row to Table1 and I want it to automatically populate with the sum of it's corresponding pid from Table2?? Do I have to use a Trigger or something similar, I'm still new to all this stuff. Thank you so so so much for your help bro!
Just realised doing this will lose the ability to filter by Item No. since we would have to remove that. Could be wrong though but just a thought.
Do you have any knowledge in other programming languages? SQL is only the Database, where the information will be stored, the system where users will add/search/change the data will require another language. All in all, here is a safe bet to start with SQL: http://www.w3schools.com/sql/
Perfect! Thank you so much again!
I have been learning python for the past few months, and know enough to be dangerous. Would this be sufficient for building the access system? Or would it be better to go with a compiled language?
I'd say absolutely, yes you should learn it. SSMS 2012 (IIRC) and later has the ability to do Find/Replace with RegEx statements which is incredibly powerful. I had an issue today where I needed to remove the AS [column] column aliases from each line in a long SQL file. VERY tedious if I were to do it via text editing. Regex Find &amp; Replace did the trick, removed all the offending column aliases in just a few seconds.
So you want it grouped by both Item No and the Date?
&gt; lose the ability to filter by Item No. use a WHERE clause
Yeah if possible.
Ok, so group by basically can summarize the selection list? Trying to figure out what these all do! Yikes.
No I have not, what is that? 
It's a database Microsoft has been releasing since the 90s to help people learn SQL and Access. You should be able to download it pretty easily. You'll need to have SQL Server installed, and might want to go ahead and install SSMS also. This might be a little too heavy for your needs though.
http://sqlfiddle.com You can try a couple DBs, and there is a sample you can start from and update however you like. 
As noted already, some more friendly front-end would save your users from wanting to strangle you. Having said that, don't be afraid to write the meat of the functionality in stored procedures in your database. Then whichever front-end you choose simply needs to execute the right function to manage the data. Further, the business rules around managing the data would be in one place and properly preserved no matter which or how many front-ends are using them.
When doing large updates, and should your database support it, consider the MERGE command or using a Common Table Expression.
If all those partitions are in the same physical disk then it can even decrease the performance. The partitions (in different physical disks) improve performance because it allow multiple reads at the same time, which doesn't happen with logical partitions.
Your CIO is 1) micromanaging, which is a bad idea because it creates bigger problems when, 2) he's wrong. Not to mention he's kind of a prick to deliver design requirements *after* the server's been built. Additional partitions (or virtual disks, which I think you mean to be saying) aren't going to impact performance when it's all going to the same SAN controller anyways. TLDR; Tell your CTO to build the server himself.
I've typically done * O/S * DB * Logs * Temp * Backup The Swap and Program partitions - maybe we don't hit enough load for that to matter but I've not done those two... As to the why, as others have said it depends a bit on how the infrastructure is setup. If they map the drives to different LUNs then they can split up the IO traffic back to the SAN for better performance. If they all use the same path then splitting them up doesn't get you anything.
CIO was visiting our site in a "what can I do to help" role. We went over a million things this week and sort of brushed over the SQL Server. Really nice guy and just wants to help, I'm just looking for a source other than some passing comments if I'm going to rebuild this server.
That actually makes a lot of sense. Thanks for the input!
Yeah - I'd fire you on the spot with that kind of attitude. The CIO is right in this case and while someone else *should* be instructing OP on how to do this correctly the CIO just happened to be there. Performance is not the only reason to have partition standards. Even with SAN drives for TEMP partitions you still need to do testing on how many tempfiles to have for your tempdb to get optimum performance for the tempdb. You dont just "not do it" because SAN.
Op is virtualized. Probably on a san, so parallel read is probably happening here regardless of partitioning. 
But is he sure what exactly is running in parallel? Like, if tempDB + DB end in the same disk while OS and Swap is in parallel it won't make much of a difference. I'm not really sure how a SAN manages disk reading, I only worked with servers with regular disk arrays.
Thank you so much!
Not sure how I would feel about giving a 3rd party website access to my database and access to another 3rd party app. E
Can anyone ELI5 why one would choose NoSQL over SQL? My recollection is that people like Google have a need for it because they have such a massive amount of data. I read into it a long time ago, and felt like it just wasn't for me, it didn't fit my use case. And for some reason, I just want to hate on MongoDB. Why do I have this irrational hatred for MongoDB, does it suck?
Wow, that's pretty cool!
You're welcome. I did not know these things once too :)
I know this doesn't really help, but isn't rule of thumb to not "COUNT(*)"? Wouldn't you want to run your COUNT against whatever the index is? I guess it doesn't really matter unless you hit performance problems with the query...
&gt; Because it's cheap How much is your data worth to you?
You can look down upon these people all you wish, but at the end of the day what they do with their data and whether or not they want to spend the $$$ it'll cost for legit solutions, is completely on them. We install and support a CRM software that is MSSQL based. Out of the 600+ clients I've installed over the years maybe 30 have had anything remotely decent backup wise. The rest just don't want to spend anything extra on what they consider a what-if. We do our best with what we have at hand and if that's dropbox/google drive/carbonite/crashplan then so be it. Personally in the 7 years I've used dropbox I've never run into issues. Hell, convincing some of these companies to get a machine dedicated to hosting the MSSQL server can be a tooth pulling experience and some just end up installing it on a random workstation.
Did this from memory SELECT Table1.SSN, Table1.Last, Table1.First, MAX(Table1.[Deed Recording Date]) AS [Max Deed recording date], Table1.[Property Address1], Table1.[Property City Name], Table1.[Property St], Table1.[Property Zip], Table1.[Buyer 1 Last Name], Table1.[Buyer 1 First Name], Table1.[Seller 1 Last Name], Table1.[Seller 1 First Name], FROM Table1 WHERE ((Table1.Last)=Table1.[Buyer 1 Last Name]) And ((Table1.First)=Table1.[Buyer 1 First Name])) GROUP BY able1.SSN, Table1.Last, Table1.First, Table1.[Property Address1], Table1.[Property City Name], Table1.[Property St], Table1.[Property Zip], Table1.[Buyer 1 Last Name], Table1.[Buyer 1 First Name], Table1.[Seller 1 Last Name], Table1.[Seller 1 First Name];
1 - Yes VARCHAR is the best way to have an array. 2 - No, it is the worst way to do it. The best way is since you have a USERS table you can use its ID field to link to your GROUPS table and make 1 row to each member for each group, your GROUPS table should be like: GroupID | UserID :-- | :-- 1 | 1 1 | 2 1 | 54 1 | 25 2 | 2 2 | 5 3 | 1 That table indicates that Group 1 has users {1,2,54,25}, Group 2 has {5,2} and Group 3 has user {1} 
Ya, it's understandable. I've made choices for certain services vs security such as using Googles services for their integration with my Android phone and many websites I use. My personal backup scheme is a one machine using unraid that runs 24/7 as the main file server (nothing important is stored on my workstation), another machine starts and mirrors the contents of that main server every Monday and Friday. I then also have crashplan running on the machine that syncs with a friends server that lives a couple blocks away (he syncs his back to mine to act as offsite-ness). So I have 3 copies at least one of which isn't in this building but with a trusted source, local and encrypted.
thank you for the answers. i didn't realize how big an assumption being able to use an array would be for listing members. i've only taken a few database/sql courses, but i never actually get to do projects with backend work. let me recap and see if i understand. for GROUPS, don't have a 'members' field. instead, create a separate table (GROUPS2) that will account for members. GROUPS2 will have no primary key (as seen in your table), but will be JOINed alongside GROUPS and USERS for a query (example, listing names of all users in a group). GROUPS will then contain all info (name of the group, location, group id), pretty much everything minus member IDs. USERS |UserID(p)| Name| |:-----------|:------------|:------------| |1|Ashley| |2|Billy Bob| |3|Charlemagne| GROUPS |GroupID(p)|Name|Location| |:-----------|:------------|:------------| |1|ABC Group|America| |2|Babaganoosh|Bolivia| |3|Coco Coco Puffs|China| GROUPS2 |GroupID|UserID| |:-----------|:------------|:------------| |1|1| |1|2| |1|3| |2|1| |3|NULL| did i understand correctly? if so, thank you for your help. seemed simple enough in my mind, but obviously it's a task for a dba's mind. i'm getting even more excited about this project. it's an opportunity to exercise what i've learned in online courses.
rbardy gave me some insight. i just have no experience with backend/dba work (i'm front end) and so i'm quite clueless regarding what can be done and how it should be done, though i've taken some database courses and i understand schema and relational models. 
Why not use this simple tool https://sqlbackupandftp.com/blog/how-to-backup-sql-server-to-google-drive/
Free. One of the best I found http://www.studybyyourself.com/seminar/sql/course/?lang=eng. You have a basic course and an advanced one. Some exercises in the advanced course are kinda difficult. 
Here is a good website on varying Data Models per industry (generic examples) http://www.databaseanswers.org/data_models/
You seem to recommend this site a lot... as in it is basically the only thing your account does.
The whole idea with relational databases is that each table should have an 'ID' or unique key so that you have a common key that you can use to get related data from other tables. For example: Users and Companies. If you needed to track these two things, you'd have two tables: Users: ID as int ,CompanyID as int ,Name as varchar(10) So that's the user's ID, the ID of the company that user belongs to, and then the user's name (or more user-specific attributes). Now, if you have 10 or 10,000 users, they're all in that table so that you can query it to get the specific user(s) you're interested in. Next is the company table. Companies: ID as int ,Name as varchar(10) So you might have 50 companies in that table, each has it's own unique ID (the ID integer column), and if you look in the users table you'd find that the CompanyID for each user corresponds to the ID for one of the companies in this table. Later on, you could link them together. SELECT u.Name AS [Username] ,c.Name AS [CompanyName] FROM users u LEFT JOIN companies c ON u.CompanyID = c.ID
Great points. Also on the note of premature optimization: What if you're only dealing with a few hundred records? Probably not necessary to think too hard about indices, the performance increase will likely be negligible. :P
&gt; some just end up installing it on a random workstation. I laughed and screamed at the same time. :P
If your table/DB has snapshots, sure. If not, no. Always test your query with SELECT before running it with UPDATE!
I also recommend http://www.w3schools.com/sql/ and http://www.sqlcourse.com/. You cannot do exercises there though. If you find another web site for training SQL queries for free and which does not require any registration please let me know.
What database system? But, in truth, without backups, you're pretty much SOL. Don't use commands that you don't understand.
Check the execution plan with and without the USE Master, and see if there are differences. It is likely that a better plan is executed for some weird reason when your both tables are called from another database.
You're right, i never thought to think that comparing integer to integer is more efficient especially with a larger query. Thank you for this! I found the more complicated query on StackedOverflow from i assume is a more experienced developer as that person also have many updates on that comment. I was just curious as to why someone would use the more complicated query vs the simple ones like yours which seems to be to be the better option. 
pseudo code wise, I think you'd have to figure out how many rows were returned. So what I'd do if I had to do this (and i'm not an expert) Query for Count(rows) table x Query for Columns(table x) Create table Y and use a loop to add columns based on C|R Then, construct an insert statement actually looping through Select * from X could be a long query though, depending on table x's size. and if there's too many columns, Y might be in trouble. Perhaps another question? What is it you're trying to do? Pivot table? 
Do you know the name of the columns in table X or does it have to be generic for all possible numbers of columns and column names?
As long as you don't mind managing the columns, you can use UNPIVOT and PIVOT: with r as ( select 1 as a, 2 as b, 3 as c union select 4, 5, 6 union select 7, 8, 9 ), kv as ( select cast(k as varchar(10)) + cast(rn as varchar(10)) as col, v from ( select row_number() over (order by rand()) as rn, * from r ) p unpivot ( v for k in (a, b, c) ) unpvt ) select [a1] from ( select * from kv ) p pivot (max(v) for col in ( [a1] ) ) pvt
There are a few scenarios that it can understandably happen * you're under such pressure * you're so deep in the code working with deep business logic Then again, there are people who * like to obfuscate code * just make the code as complex as possible to show off If I saw this similarly in the previous company I worked at, I would ask a few questions about the context of the code: * What are they trying to account for? Is there some edge case scenario that I'm not seeing? * What is the business value they're trying to deliver with this code?
Something along this lines should work: UPDATE Client_Jobs SET JobManager='635' FROM Clients WHERE Clients.ID = Client_Jobs.Id You need to use the FROM to get which clients you want and the WHERE you filter what you need from your clients table.
How do I go through each client number without having to enter it? I have a list of them on a csv but I don't want to have to copy paste 900 times.
This seems to be exactly what happens. When running from the InventoryDb, the Execution Plan shows a missing index with a impact of 26.9027. When I switch back to the master db, that Missing Index message is not there. Thanks this gives me something to investigate.
You are probably benefiting from better cardinality estimation in the newer compatibility mode. Several things have been improved since SQL 2008.
You need to create a Client table and fill that table with the data from your CSV so you can do the JOIN, after that it will be automatic.
Use Excel and the concat function to open the CSV and create the where clause for your list
Base on your EDIT I think what you are looking for is: with data as ( select 1 trip_id, 0 distance_start, 2 distance_end, 10 fee_rate, '=' operator UNION SELECT 1, 2, 30, 0.5, '*' UNION select 2, 0, 2, 10, '=' UNION SELECT 2, 2, 10, 0.5, '*' ) select distinct trip_id , SUM(CASE operator WHEN '=' THEN fee_rate WHEN '*' THEN (distance_end - distance_start) * fee_rate --If you have other operators than add the necesary calculation here END) OVER (PARTITION BY trip_id) from data hope this helps
Yes. Just installed SSMS 2016 GA yesterday on my work laptop, Windows 7 x64.
After index creation the timing of the two queries appear to be nearly identical now. I am still investigating the compatibility level topic suggested by treebear901 to figure out how the query was able to run efficiently without the index. I tried this on other queries from other databases and there is definitely something happening something SQL Server is able to do something better from the Master DB. Thanks for your help and yes it is very unusual.
Ok, I've read this and the replies a couple times as its not very clear to me. It looks like you have a fee lookup table. I'm assuming you read the example like this: If Distance is between 0 and 2, the fee is 10$ If Distance is between 2 and 30, the fee is .5 * Distance DECLARE @Fees TABLE (distance_start INT, distance_end INT, fee_rate MONEY, operator CHAR(1)) INSERT INTO @Fees (distance_start, distance_end, fee_rate, operator) VALUES (0, 2, 10, '=') INSERT INTO @Fees (distance_start, distance_end, fee_rate, operator) VALUES (2, 30, .5, '*') DECLARE @Distance INT SET @Distance = 16 SELECT SUM( CASE WHEN operator = '=' THEN fee_rate WHEN operator = '*' THEN @Distance * fee_rate END ) FROM @Fees WHERE distance_start &lt;= @Distance AND distance_end &gt; @Distance 
So in order to determine which row is "a1", "a2", "a3", etc. you have to have an ordering mechanism. I just used rand() (ie random), but if you have a specific ordering column or columns, you'd use those instead.
Do you have backups of the system databases?
Yes, I should have mentioned, I have solid backups of the system DB's and all other DB's that were on the lost drive.
 DATENAME(WEEKDAY, dateadd(mm, DATEDIFF(MM, 0, getdate()), 0)) is the first day of the month and does not equte to datename(weekday,getdate()) That is why they use 0. they could use almost any integer value lower than the current date. 0 is just the simplest choice. They are just trying to get the first day of the month is all and that was the standard code to do so.
I forgot that rand() will return a float value between 0 and 1 by default... So this is indeed a non issue. Another way I have written this is SELECT ROW_NUMBER() OVER(ORDER BY(SELECT NULL)) Seems to have a similar effect.
Are they strings? You could do something kind of hackish but kind of clever! where convert(int, your_column) = 0 :)
Easiest way is to use the import/export wizard. Open up whatever that thing on the right is called, and right click on your db and hit Export data. Follow the prompts, it's simple.
That's not how rand() works, rand() simply generates a random number for each row and then orders the rows by those randomly generated numbers.
I'm not sure that is exactly correct either. rand() is only deterministic when a seed value is applied. In the case above, rand() would generate the same value for each row. That is kind of besides the point here though because it's passing a scalar value to ORDER BY. One example: SELECT RAND() , RAND(max_column_id_used) FROM sys.tables 
Did rails programmers really not know that you can create multi-column indices?
Great update, thanks.
Hi JustShat! Can you specify which type of meatdata you'd like?! I'm kind of a noob to programming in general. Best, Mango 
http://i.stack.imgur.com/1UKp7.png 
Unfortunately, IMO, all visual representations of joins are deceiving in one way or another. Personally, I'd try to deconstruct the query first to "inner joins" first then figure out the outer (or more complicated pieces of it). I would also recommend going after 'logical grain' of the result set (i.e. grain for the "Master Join Detail" would be Detail, "Master Left Join Detail" would be "Detail or Master/NULL Detail", "Master Left Join Master" would be "Leaf or Root/NULL", etc.)
Do people really use right inner joins? I just left inner joins and move the target table how I want it.
I do if I'm feeling really lazy and don't want to rearrange the table names. 
I typically get away with INNER and LEFT.
To be fair you want to be super familiar with is: SELECT * INTO name_your_backup_table FROM table_you_dont_wanna_destroy Because shit happens haha. I remoted into a place last night that had an uncommented delete just sitting there that could have really screwed the pooch. You should do it though man. SQL is pretty straightforward, and every business uses database.
with (nolock) 
MS SQL stores date/time columns. It is a combination vale where the integer part is the date in number of days while the fractional part is the time stored as the fraction of a day. Using the zero is part of a set of tricks. Your DATEDIFF(MM,0,GETDATE()) Gets you the number of months since the zero point in time. IIRC day 1 is 01/01/1900. Dates earlier than that are stored as negative numbers. 
can't relate, always use ctrl+e :( 
I'm with you fellow ctrl-e bud.
dirty reads everywhere
That is really interesting and good to know!
I would think something that is quantifiable and helpful to the company would get two birds stoned at once. Like "implement extended events tracking of database blocking". Benefits the company and extends your knowledge into a new tool. Or since SSIS is on your list, "implement ETL for sales department to inventory reporting system". That kinda stuff. Depends on what you think they'd want done, and what's reasonable for you to do. I also feel like I learn better when I have a goal in mind, just trying to learn a nebulous concept leaves me feeling overwhelmed.
If the tables are large, import/export wizard works great for this. If you have QA available from DEV as a linked server (or the other way around) you can do this easily with the SELECT INTO statement as well, if the tables are small. We have DEV setup as a linked server on our QA system, and vice versa, for this very reason. Super easy to transfer small bits of data from one to the other. This will create "yourtable" on the server, with schema matching "yourtable" on qaserver, and populate it with data from qaserver: SELECT * INTO yourtable FROM qaserver.database.dbo.yourtable 
This worries me. So if your writing in mssql studio you could do that creating either a query or a view? I guess never write delete and your ok? Haha. 
Can't do any real damage with a view, that's the whole point of them haha. Just comment out your DELETE before it is 100% tested and ready to go. Like: --DELETE FROM blahblah WHERE yaddayadda Then the DELETE statement can't fire off if you flub a keystroke. Same goes for UPDATE. Shoot, I've done worse damage with an update. Boom!...(900000 rows affected) Ahh. How long has it been since I worked on my resume?
Views are what I would use and honestly there wouldn't be much that could destruct. It would be making simple views that a user in AR could hook into and see what customers are overdue on accounts and slice that data. Or it would be a view that the Plant Manager could have to show capacity utilization by line for the next weeks production. Simple stuff. But I can total blow it up being simple.. That's why I don't get admin rights! Haha
Its being used as well. Ive only posted part of the code because the rest is irrelevant. &gt; Can you post the text of the error message? Quote on quote, I cannot say for sure but it seems to say something like it is missing a value when it gets to sda.Fill(data); If I change this: oledbcmd = "Select * From [CLIENTES$]"; Its works perfectly. That's why I know the issue is there. 
FTFY: When your big query runs for about 10 seconds without any error messages showing up.
Wow just watched some tutorials and definitely can change base table with Delete and Update. Why use Update?
OPTION (RECOMPILE) 
Instead of EXEC @UpdateStatements can you SELECT it? What error or result are you getting when you EXEC it?
ahhh /u/svtr ... the worst thing that ever happened to programming....
Change your query to this: update s set s.supplierid = item.supplierid from SupplierList s inner join item i on s.[id] = i.[id] where i.id in (select itemid from supplier list) x Change the fields you're joining on to .. whatever the correct fields are. http://stackoverflow.com/questions/9588423/sql-server-inner-join-when-updating
I've just been told that SQL Server 2012 is what is being used, so which is more beneficial?
TSA FTW
Why do I have to CAST the RAND() function?
Its not a question of whats more benificial. It's a question of what task you need to perform on SQL Server. It sounds like you need to be asking your manager these questions. Once you get a better idea of that, then you can start understanding what you need to learn. AFAIK, sql zoo is only going to teach you basic and generic ANSI-SQL - mostly DML and maybe some DDL. There's more to MSSQL than just learning SQL. Theres a whole rich world of database administration where you would need to understand the applicable underlying concepts of your environment. Btw... that sentence in my prior comment about going to a store was an analogy. Im not sure if you thought I was being literal. My point is that your trying to find the answer without understanding the problem. The breadth of MSSQL is too large for a newbie to try to learn it all and is even broken out to separate specialty tracts for veterans (as in DBA vs DB Dev). TL;DL: As in Hitchhiker's Guide To the Galaxy, the answer is 42, but you don't understand the question.
Look for sp_executesql , that will allow you to execute dynamic query WAY easily.
OJT
First microsoft access, then oracle sql plus and now sql in phpmyadmin
I used MS Access and decided to migrate all the macros to a SQL script in MS SQL. Also being a GIS guy I picked up Spatial and migrated spatial data build processes over. It helps to have a project to work on.
Your first 2 are not correct. First see if you can write a query that gets you aggregated results (show the count in your select) and then worry about filtering them. Hint: you can't filter your aggregate (your count) using a WHERE clause. 
Sqlzoo
What's that?
I hopped off in the deep end with SqlDbx and Sybase ASE. Basically just started learning how to write simple queries, then moved onto slightly more advanced stuff. I'll be honest though, I haven't really advanced much beyond simple queries and joins since my boss handles the more advanced stuff. I've been handling the more sysadmin side of things, web dev, and C# stuff. This works for us though, we try to complement each other's skills, we've built some pretty amazing things (or so we think).
On the job training. Which mostly went like this: Boss: new role is available working with the erp system. Interested? Me: sure! Boss: you now own and manage the sql database too. You've got a class in two weeks, otherwise don't mess it up. So I spent a lot of time researching and working with the dv database using Google, forums and sql books about managing databases, writing queries and tuning performance. I didn't have a dedicated person to learn from at work and don't consider myself a true dba, but I knew more than enough to get around and can hold my own when discussing databases. 
Ah man, I wish that happened to me! I'd love if a company invested in me a bit more. 
I'm not offended at all. Seems its you since you replied. The point is that I could use jet, ace, etc. but that isn't the problem. The issue is that the query is incorrect. Do you want me to change it to ace and see if then you can concentrate on the real issue? 
Usually, posting the code you've got so far helps people to point you in the right direction or where you might be getting tripped up.
On the job. Got hired as a sysadmin, and inherited a homegrown ERP written in PHP and MySQL. The app was a piece of shit, but it forced me to learn SQL. MySQL isn't the greatest RDBMS to learn on, however. It's very loose about SQL. Part of it is because it was MySQL 4.0. Part of it is because MySQL is just not a very good RDBMS that still ignores far more rules than it should. However, it taught me enough to get into another job, supporting apps that run on SQL Server, Informix, and DB2, all of which work much better. Simply put, SQL and programming in general is essentially impossible to learn until you have some reason to do something with it. With SQL, that means you need to have a database with data that you care about and reason to write queries against it. IMX, that's the only way to learn.
Oh, sorry. I will post an update.
IMX? What does that stand for. While I agree that "on the job" is the best way to get experience, having a fun project can help. I learned SQL using MySQL in college. After getting my degree in Economics I'm a MSSQL DBA. Go figure :\
Google
Its not that you don't care. You just don't know how to do it. I offer to change it to ace just so you could be happy but you refuse simply because you still would not know how to solve it. You are clearing evading the issue. Not sure if its a epenis issue or something trying to make a person look bad because that person doesn't know how to do something you yourself don't know how to do but hey...whatever helps you sleep at night.
This. The DB really matters here because they all have different ways of getting the Top,Limit,Rank Over, etc. You want to use those helpers because otherwise brute forcing a top 3 children for every parent is going to be a pain.
Microsoft has a ton of resources that are free. Most of them revolve around the adventure works database, that is a mock database people use for certification. If I was targeting a job as a sql dev or dba, I would get a cert if I had no experience to get you in the door and learn more on the job.
&gt; get two birds stoned at once. Ricky, is that you?
"How do I do..." then googling it. Except for joins, I had to read about them.
Started building reports with Access using VBA to generate them in Excel and email them out. Realized soon that would be a nightmare to maintain and figured surely it was a problem that someone else had already solved. Discovered SSRS and then started re-creating the reports I'd already made, looking at the SQL behind the queries I'd created using Access's GUI and then started writing the SQL myself.
I wouldnt need to do like a SUM(COUNT (date)) since date isn't an integer? 
&gt; IMX? What does that stand for. In my experience.
MS Access pass-through queries.
&gt; Baiting me by that "you don't know yourself" is not going to change me not helping someone like you. Well, if you think Im baiting you, then hook link because you are still replying. Im not baiting you or anyone, Im just looking for help. If you want to help out, great, Ill give you a thank you. If you don't and don't show any knowledge of any kind, Im just going to suppose you are a troll. See Im still the guy with the problem getting data off a excel using C#. Ive made my knowledge transparent. You are just the guy trying to make another person look bad without proving anything at all hiding behind a keyboard. &gt; Besides, after a decade dealing with data in any form or shape you can imagine, I can ensure you, that pulling data out of an excel worksheet is not that much of a challenge for me. Besides, after a decade of dealing with space material in any form or shape you can imagine, I can ensure you, that doing a spacewalk on pluto is not that much of a challenge for me. (Or you know, I could just be writing out random sentences too without proving anything) 
&gt; ahh almost. I almost gave you a hint, but no. A hint? http://lmgtfy.com/?q=c%23+get+data+from+excel using ace instead of jet is already listed in multiple results. I just choose jet because it came up. Like I mentioned, do you want me to change the code to ace? I have no issues with doing that honestly...the same thing would happen. That is the only "hint" you have given even remotely related. &gt; btw, its so blaringly staring you in the face you really should get it on your own It is. The SQL query is incorrect. I already know this. For now, noone knows how to correct it (better said, noone has proven how to correct it) 
Honestly, who started all of this? I tried to make a simple question and you were the one that replied: &gt; ahhh jet 4.0 ... the worst thing that ever happend to data.... I never acted like a asshole. At best I was just replying to your completely offtopic comment the same way you did: Like a asshole. Im sorry that you felt offended in some form for replying. If you don't want to take that a sincere apology, fine by me. You may leave knowing that you have your point of view and I have mine. If you take it as sincere and we can move on as adults, that's great and thank you for sharing the help. 
If you feel that way, so be it :) I guess you feel you have won so...congrats? 
Not really. Sooner or later, Ill find the answer. So its really delaying the inevitable. That's why Im guessing for you this is a win....???....don't know? I have no idea someone can be satisfied not proving themselves on a sunday and trolling but hey, whatever makes you happy. Congrats on winning, again.
personally, i'd wait to confirm whether the ERP has been tested w/ the latest SQL... I know some products that have required updates. Yes, in theory it shouldn't happen... but compatibility *is* a thing.
Well, it would take time for testing so we will just stick with 2014. Thanks.
Honestly, its kind of sad but hey Im glad you are happy like I said. Sometimes the simple things in life can make people the happiest and I guess being alone in front of a computer drinking laughing at someone who really doesn't care makes you happy. Also, congrats that you keep on replying and caring SO much. Do you suffer from depression that you need attention and alcohol? (Im just asking; I don't know who you are so this might be a issue for you) (And no, Im not being sarcastic, just to clear that up) 
I would check with the ERP vendor, actually. Chances are that they only have certification for 2016 somewhere on their product roadmap and don't actually have the product certified for 2016 yet, but you never know. I work for a software company. I know that we just certified the product to work with 2014 about six months after 2014 went GA. Also, it might depend on the timing of the implementation verses their product releases as well. If you don't even start implementing and testing the system for several months down the road, then you might end up with 2016. But, once you start installing (or have purchased the software), you're probably locked in to that version or you induce unnecessary risk to the implementation.
FYI, the way Microsoft licensing works is that you always license the latest release of a product, but you have downgrade rights within the same version. So you will likely end up licensing 2016, anyway. However, you can install 2014 and then just upgrade to 2016 when you are ready. 
I mean, downvoting is cool... [especially when someone is right](http://sqlfiddle.com/#!6/33b37/3) ;\ I did have the join condition messed up... so I guess there's that.
Important question, does that ERP support 2016 ? If yes and price is the same, go for 2016. According to what you've said, you don't need 2016 specifically. So play with the price bargain.
It says the issue in the error. You must group by menu.country since you're using the average aggregate. 
Short answer: Yes I may be able to give some advice, it may not be necessarily right for you but, for our setup: - store access DB on shared drive - create local shortcut to DB on shared drive - call access macros from C#, query tables, etc The tables are in the access database, on a shared drive, correct? How are you querying these tables? Edit: I may have all of this backwards, 
Now you're getting into political territory..... I'd tread very carefully there - it depends on the exact situation. If the first thing that happens to the owner is they get permission denied for some routine process of theirs you can end up with a lot more political headaches. A more *diplomatic* way of doing it would be to give full permissions to the table in the beginning and slowly paring them down. Another way would be to have insert/update triggers on the table doing write only to another, backup table 
We do not (as analysts) have any privileges other than creating temp tables. Basically currently if I needed to use the info that is in the access DB, I will either just build the entire job in access, or get up to the point where I can dump my SQL results into excel then import them into an access table THEN query the access DBs. If I could upload them to the main sql server, I would :)
We do not (as analysts) have any privileges other than creating temp tables.
Can't you make a Linked Server to the Access DB files? EDIT: yes, [you can](http://www.sqlservercentral.com/Forums/Topic565482-6-1.aspx)
If you do not want any data and don't care to run a truncate then I would add to that statement. where 1=2
Thanx for the link. So I will apply the ranking and then put the celing() function around the ranking divided by 2 as shown below: CEILING((RANK ( ) OVER ( [ partition_by_clause ] order_by_clause )) / 2.0) AS 'NUMBER' Is this correct?
Easily done... create table schema2.table1 as select col1 as diffcol1name, to_number(null) as newnumcol, col2 from schema1.table1 where 1 = 0
I think that will work. Give it a shot and let me know. 
isoliation level serializable is supposed to be able to concurrently execute transaction, while the outcome is supposed to be, like if the transaction were executed sequentially, if I understood that right. So how are the databases able to do that? what technique do they use to solve that problem?
I take the opposite approach to security - deny by default, grant where needed. [Principle of least priviledge](https://en.wikipedia.org/wiki/Principle_of_least_privilege) It's far easier to grant permissions that are needed than to give away the farm, then try to pare it all down later.
[Have fun](https://wiki.postgresql.org/wiki/Serializable)
what you're looking for is GROUP BY -- SELECT arrival , COUNT(*) AS visits FROM visits WHERE status = '1' AND arrival &gt;= '2016-06-01' AND arrival &lt; '2016-07-01' GROUP BY arrival note also how to specify a date range -- **pro tip**: don't use string functions on date datatypes
what have you tried so far?
The issue is that you're using approximate decimal values. The implementation of floating point numbers is going to vary from RDBMS to RDBMS, especially on proprietary systems. My guess -- and it's purely an untested guess -- is that MS SQL Server adheres to an implementation that best supports MS's C, C++, and C#, while PostgreSQL adheres to the IEEE standard. The standard says it's "precise to 15 digits," but in my experience, that's *extremely* optimistic for SQL Server, at least (I've not used double in PostgreSQL enough to form an opinion). I'd expect only 6-9 digits to be accurate, and if you're aggregating with a SUM(), then you can throw your accuracy out the window. You also have to remember that because your data was generated and saved in this format, that it's *already this inaccurate*. &gt; Should I be using Numeric() over double precision? That depends entirely on your data requirements, and your limitation is going to be SQL Server. First, no matter what, you can never store values larger than 10^38 - 1 or 10^-38 + 1. If those values don't work, you can't use NUMERIC. Second, do you need more than 38 decimal places of precision? While you can represent 10^38 - 1 or 10^-38 + 1, you can't represent both in the same field at the same time. For example, do you need to store, for example, 6.5e30 and 4.6e-10? NUMERIC can't do that. Find the largest numeric value and take the place value of the place value. So, if 6.5e10, then 10^10. Then find the smallest decimal value that you consider accurate -- remember that the leftmost digits are the least accurate in IEEE 754 -- and take the place value. So, if that's 4e-20, then 10^-20. If the difference between the exponents is over 38, then you can't use numeric/decimal: 10 - -20 = 30. Here, we're OK. We could use NUMERIC(38,25) to accurately store both 6.5e10 and 4e-20, which gives us 13 digits to the left of the decimal place, and 25 to the right. Third, how much data do you have? NUMERIC data types use more storage. In SQL Server, a NUMERIC(38) uses 17 bytes. A float(53) uses only 8 bytes. I said above that the limitation is SQL Server. That's because in PostgreSQL, you're limited to 1000 digits of precision when you specify a range. If you don't specify a range, however, PostgreSQL supports up to 131072 digits before the decimal point and up to 16383 digits after the decimal point. I would *avoid* that, however, since PostgreSQL's documentation says that calculations on such fields can be very slow.
Whoever wrote that quiz needs help also. Those instructions are horribly written. 
Don't post your homework please. If this is for work that you have been assigned to take a look at, please put the schema and data here: http://sqlfiddle.com/ Write down what you have tried and what problems you are experiencing.
The great thing about technical debates is that proof can be offered one way or another. If this is really a problem with max and min surely they could offer some examples?
Your colleagues are spreading superstitions and downright lies. Don't support their fear mongering. Demand proof!
Max and Min have always seemed reliable for date, datetime, and time in my experience. The tricky thing is comparisons and being sure you understand precision. They're also not always best for finding the order of things. There's more to this story that you need to get from them or it's just bunk.
Or, a slight variation on /u/r3pr0b8's excellent take. SELECT arrival , COUNT(*) AS visits FROM visits WHERE status = '1' AND MONTH( arrival) = 6 GROUP BY arrival 
&gt; if its possible for me to change the WHERE player_name from a preset text to a variable It sounds like you're asking about how to do this in code in a language outside of SQL (e.g. PHP, Python, Perl, etc.). If that's the case, you want to look into how to use placeholders and/or bind variables. You'll replace 'Zinedine Zidane' with a question mark (?) and will pass the variable name when you execute your query. The actual method of passing the variables will depend on the language. If you are looking to do this purely in SQL you can as well. The method might depend on which database engine you're using, but typically would see you replacing the hard coded string with an @variableName and then declaring/setting that variable prior to calling the SQL query. Also worth noting, you can remove your query from the WHERE clause by joining another table, cleaned up example (untested) below that I think will probably work: SELECT teams.team_name FROM Relations INNER JOIN teams ON Relations.team = teams.team_ID INNER JOIN players ON players.player_ID = Relations.player WHERE players.player_name = ? ; The "?" is if you're using this as a placeholder for a bind variable. Feel free to replace that with your hard coded string for testing. Also, you probably want to look into having more consistency as it relates to naming conventions (capitalization is inconsistent on your table names, for example). Functionally it's irrelevant, but from a readability standpoint it pays to be consistent.
I am aiming to do it in PHP. THanks for the answer though it really cleared up alot of questions that I had. 
If I had to guess, I would say they've seen it work poorly when dates were stored as strings (VARCHAR)... which is poor practice anyway. 
Thank you for your answer! I believe however that your first assumption is wrong - it is the other way around: for a given country, a city that exists in the Sakila database should exist in the World database. Ok, this query (at least half of it) will give me countries from Sakila where I have typos, but it still does not resolve the most important problem. I want to have eventually two colums: one with the cities from Sakila with typos (and that is done now) and one with the World with the correct names of the cities that match those with typos (so I can compare them afterwards) - that is the most difficult issue. Do you have any idea how can I match them together?
The unreliable function is coworker
Totally false. I (and a team of 3 others) manage a list of nearly 1000 instances, most of which have default names (because we rigorously stick to 1 instance per server). Among that, we have probably 50 Failover Cluster Instances and 20 Availability Groups. Of those 70 AlwaysOn deployments, I think every single one uses the default instance name and we've never had an issue in that regard. This is a stretch, but are the consultants perhaps confusing named instances with virtual network names or listener names?
manually -- been doing it that way for decades helps to have a good text editor, i use Ultraedit 
Great idea! I believe that the country from world has to exist only in the limited Sakila (the first part of your query) because we are interested only in the countries that contain typos in their cities. From the technical point of view - is it possible to somehow alias the first part of the query, e.g "sak_c" and add to the end of your query (before "order by") something like " AND EXIST ( SELECT * FROM world WHERE world.city=sak_c.city )? I'm sorry if that's the noob question, I'm still fresh to MySQL.
Very good point!
Pretty much -- --- -- Presumed: For a given country, a city that exists in the -- World database should exist in the Sakila database. -- -- The spelling in the World database is the more trusted source -- -- --- SELECT "In Sakila, Not in World" as rType , Sakila.country , Sakila.city FROM Sakila WHERE NOT EXISTS ( SELECT 1 -- No city with this spelling is found FROM World WHERE World.country = Sakila.country and World.city = Sakila.city ) UNION SELECT "In World, Not in Sakila" as rType , World.country , World.city FROM World WHERE NOT EXISTS ( SELECT 1 -- No city with this spelling is found FROM Sakila WHERE World.country = Sakila.country and World.city = Sakila.city ) AND EXISTS ( SELECT 1 FROM Sakila WHERE World.country = Sakila.country ) order by country, city
Ok, I was thinking about choosing not all the countries that are both in the World and Sakila, but only those which match the countries from Sakila with typos in the cities (those are the ones that your first SELECT finds - before UNION).
It's a misunderstanding. ***Data*** can be unreliable. The dates in SQL Server are stored as a number. Type 42370 into A1 in Excel. Format it as a date, and it reads "1/1/2016" SQL Server does the same thing. Time is handled by adding decimal for however specific things need to be.
that's what the second exists does to the second select. It limits the world to only those countries also listed in Sakila You should only get those cities with typos, the incorrect version out of the first select, and the correct version out of the second.
Because RAND() returns a float value random number. If you try to pass a number with a decimal in it as the second argument in SUBSTRING() (which is the location to start selecting the substring from) it will give you an error. The string index position is integer only, so you must cast it first.
Can you be a bit more specific? What are the issues with those topics you listed?
speaking of fewer mistakes, investigate the "leading comma convention"
There are 2 main approaches to this: circular logging/locking and snapshot isolation. This is one of the main practical differences you will find between using say SQLS and ORACLE, although each can be made to operate in the other approach. The differences in approach are subtle and have various pro's and con's, way beyond me explaining in this exact moment!
You mother fucker did you just use a right join?
A good manager should do this for you.. By taking feedback from his employees, knowing what areas they need growth and find ways to help you get better while also helping the department. Taking random training is not useful. The goals should be attainable, worthwhile and also broken out into steps over the course of the quarter or year. Deadlines should be defined for each step. Completing different steps and having regular check ins to review the goals will help assure they are met or maybe you need to pivot for another priority. This approach also makes eoy reviews less painful. 
Don't* use a [function](http://use-the-index-luke.com/sql/where-clause/functions/case-insensitive-search) like that in your where or join clauses (*too often or on large tables), SQL can't use an index to make the query faster that way.
I stuck in an edit to start things off.
Well let me rephrase that to "put *this* subquery in the select statement" then. But from the context it was pretty clear that the subquery has multiple rows as result.
Thank you for the info. Looks like all I needed was the paraentheses and bot null before. I'll make sure to ask my questions better in the future. 
&gt;If questions aren't welcome here, and they can't be tagged, perhaps it is time for a /r/SQLHelp ? You mean like /r/DatabaseHelp?
So MySQL? Blegh Google got me [this](http://www.xaprb.com/blog/2006/12/07/how-to-select-the-firstleastmax-row-per-group-in-sql/) and it seems solid. Even though he complains about compiler performance, I'd try this first. select type, variety, price from fruits where ( select count(*) from fruits as f where f.type = fruits.type and f.price &lt;= fruits.price ) &lt;= 2;
Ok I got everything now. Thank you very much for your help fellow redditor!
I took a lot of those answers with a grain of salt. If you read the comments, there's some conjecture. Plus, the test is divided into 6 sections, and you can still pass on your strengths. I did terrible on security, but I nailed "Install and Configure" and "High Availability", because I've done a lot of that. 
I did terrible on security, because it focused so much on giving Employee X the exact permissions they need and nothing more, but I nailed "Install and Configure" and "High Availability", because I've done a lot of that on the job. I like to call myself a "wartime" DBA. I was the the sole DBA at a small, fast moving company, so I didn't focus much on minutia. It was more important to tune everything and also train the developers on proper SQL coding. Before DBA, I was a SQL developer.
I've literally never heard this and I've been working with data for nearly 15 years. There are lots of urban legends surrounding SQL that may have once been true many decades ago but are no longer true. Examples include "a #temp table is written to disk and a @table variable works in memory", or, "heap tables load data faster because they don't have to sort any data" These statements were " " true " " to a certain degree way back in the 20th century or something but new advances have changed the reality. But old guard SQL guys will say this and the people they train will spread it. Similarly I think that the comments your coworkers made might have been true to a certain extent way back in 1987 but is no longer true today. *** One thing about datetime, look for the definition of how the time is calculated. Is it the SQL servers system time? Is it Grenwich Mean Time? Is it the users system time on their PC? Is this consistent? This can be a pitfall, as others have pointed out. If you have two tables that both have a ModifiedDate field, don't assume both fields obtain data the same way, check the source.
Yeah folks that are doing this should be aware that they're not doing themselves any favours or "getting away with" something. One day they'll have to explain to their boss why they couldn't complete something.
In MS SQL Server (T-SQL), you can use IsNull: SELECT Firstname ,Lastname ,IsNull(ThirdColumn, 0) That will take any null values and replace them with the second parameter. If you need to find the null values and then match an existing record by first and lastname that DOESN'T have a null value and then negate it (looks like what you're trying to do here), then I think you need to separate it out into two separate queries: First pull the ones that are null and then join on the same table by first/last name with the number value from the 3rd column multiplied by -1 and insert it into a temp table, then select the non-null list and UNION the two results together. You could also insert the null records into a temp table and then use UPDATE.
To compare a field to null you use "IS NULL", like: SELECT * FROM [Table] WHERE [Field] IS NULL That will return the rows with NULL fields
Probably a couple things worth investigating. First can you connect Sql Server Management Studio on the local machine using Windows Authentication? If not then yeah sql server can't query AD. It might need a new service account configured, or the existing account would need more permissions on the local machine. If you can login to SSMS it might be that the site fails because of IIS settings for the report server.
That is exactly what I am trying do. I like the double query idea. My mindset got stuck in a formula index method after getting help to do this in excel :/ which I then realized wouldn't work since I have way too much data. 
Any company without a BI function which uses a transactional database system with structured data entry. This can vary wildly really! 
Yeah, I can do it in anything. First tried excel but then realized quickly I had way to much data. So now I am trying to do it all in SQL. Previously I did separate queries replacing null with -1 then joining the tables back together and multiplying the -1 in and that gave me very bad results. So now I am trying something similar but with a union.
Not a knock on this...but genuinly curious, how is this different from something like Domo? 
order by IFNULL(nl_posts.id, 9999)
This is how I would do it, you could also use joins which might be quicker, not sure. SELECT DISTINCT Order FROM YourTable WHERE NOT (Order IN (SELECT Order FROM YourTable WHERE Material = 'MAT3'))
You can also use COALESCE, which is essentially: If (Column1 IS NOT NULL) THEN Columm1 ELSE Columm2*-1 END AS Column3 Syntax for that would be: COALESCE(Column1, Column2*-1) AS [Column3]
We're gonna build a firewall. And we're gonna make the hackers pay for it!
We'll all have enterprise edition, and it'll be yuuuuuuuge.
I'm a developer. When I get technical interviews I generally get asked to explain the different types of JOIN (INNER, LEFT, FULL OUTER) bonus points if you know what a CROSS join or RIGHT join do. The odds of using CROSS or RIGHT are rare. I also got asked what is the difference between COUNT(field a) vs COUNT(DISTINCT field a) The rest were developer questions like describe the 1st 2nd and 3rd normalized form, when would you use them and when would you ignore them?
Oh man, I don't know if I'll be able to answer those developer questions like you mentioned. I pretty much learned SQL by myself during this internship and I've been able to do a lot with it. But I wouldn't know the theoretical stuff. Any tips or resources where I can read on this stuff?
Thanks! Oh man some of this went over my head, but I think I got the gist. The job I'm applying to mainly requires me to know SQL and VBA (which I'd consider myself pretty good at it)
I wouldn't sweat it, it sounds like you're pulling data out of the DB for reports and what not. To which it sounds like you can do. My first SQL role they asked what my most advanced query to date was. Learning how to PIVOT would be powerful, but I have to look that up when I need to use it. (HINT you can avoid doing that by left joining to the same table many times) The other stuff you need to know if you're making tables and to a lesser extent if you're updating/inserting into the tables.
Chuckled hard.
My execution plans they, let me tell ya, they're amazing. I'll optimize all of the clusters. All of them. They'll be running so fast they'll make your head spin. And these people, they're saying, they're saying use the wrapper for 2016 for analytics and I say no. We're going right back to T-SQL and we're gonna do our reporting in Excel and it's going to be amazing. You'll see. Believe me. EDIT: This Gold kind stranger, let me tell ya, I just want to thank you for it. It's amazing. Just wonderful.
This will still show the others. It's just setting the order based on whether or not the price is null.
Foreign keys will be ok as long as they are documented. Undocumented foreign keys will not be allowed. Look for a boom in the market for ER Diagram tools.
Thanks I will give this a try and let you know how it works out !
Thanks I didn't even think about trying it this way! I love seeing everyone's thought processes on how to solve problems
Ahhh I get it now, thanks, working a version of this in. Using ISNULL instead, although IFNULL would work also, and targetting the meta_value field. `ORDER BY ISNULL(nl_postmeta.meta_value), nl_postmeta.meta_value ASC, nl_posts.id ASC`
Disk space usage will be... YUUGE.
The main difference is that IN is unable to validate NULL values. IN is essentially column = arg1 OR column = arg2 OR ... OR column = argN. So it really depends on whether you will actually store NULL values in the column you are checking. If you want to clean up the CASE statement you could do something like: CASE WHEN nl_postmeta.meta_value &lt;&gt; '' THEN 0 ELSE 1 END This way populated values get compared where empty strings would evaluate to 1. Since a NULL value compared to another value results in NULL, NULLs would also be set to 1. Any non empty string would evaluate to 0.
It's not just you.
Bernie, on the other hand, wants all workload to be evenly distributed across the schedulers. No more will memory and CPU be focused on the top 1% of queries. 
Give it a try and let us know?
Thanks for all the help. I really appreciate it. 
I tried this function and ended up confusing myself when I went into reporting on it. Let me try again and get back to you. 
I'm getting results in the field COUNT(*) of anywhere from 2-32. Some results have duplicates, however with a SELECT DISTINCT query, I can work around this. The ACCT_IDs that are giving me trouble are the ones that are tied to multiple GROUP_IDs. I need it to be a 1-1 match. How do I query for these?
Kind of. It came through with All negatives without actually pulling over the A.total amount when not null. So i have been playing with it to do that and only find it where null is in B.total amount, giving me all the inverses, then using union to bring it back in to a query, but it has been returning way too many results. But as I was typing this I realized I missed A VERY important where clause in reducing the numbers. lol 
When you work for a F50 company with big data, you get used to the sewer smells. 
I guess we already established the fact your primary keys are all fucked when you said you had to de-duplicate :)
I think I figured it out. I'll have to create a table SELECT DISTINCT Acct_ID, which will return the first match between Acct_ID and Group_ID. I'll then have to create an outer-join back to the original table, to see which Accounts didn't make the cut.
And we're gunna make management pay for it!
http://i.imgur.com/WTDnnwE.gif?noredirect
Just flip it around. SELECT GROUP_ID, COUNT(DISTINCT ACCT_ID) and then change your GROUP BY to match.
One can always migrate to MongoDB... (Just sayin', I'm Polish)
I heard she is more on the on-premise side in the cloud debate.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/analytics] [I built something code named "Bazooka" for an attribution analysis, and it works... but now I don't know what to do with it and could use help visualizing methodology. Millions of rows of test data inside. (XPost \/r\/SQL)](https://np.reddit.com/r/analytics/comments/4ndpew/i_built_something_code_named_bazooka_for_an/) [](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))* [](#bot)
It's a great thing!
Hibernate made one that looks just like this. Thanks for the answer!
ok, thanks! 
Are you sure that tool actually works? I decided to sign up for the free account. I specified an email address and password and created the account. It prompted me to click to log in. I did so, but yet it said the login failed even though it was the very credentials I'd just specified. I checked my email in case I had to verify the address, but there was no email like this (which actually means I can create an account with any email whatsoever?) I tried to login again, it failed. I noticed the URL was &lt;domain&gt;.datareportive.com/accounts/login so I removed the /accounts/login part to see if that helped, but lo and behold when I opened that page, I was signed in. Yet, the only option available, despite some pretty icons, was a big power off symbol. Clicking that, logically enough, logged me out. This web site seems like it is just broken. Now I want to delete my account, but all I can do with it is log out. EDIT: I logged it with them and got a reply already. Well, that's nice and makes me feel better about their support. The responder said: &gt; I’m so so sorry you had a bad first experience. I’ll be honest with you, we are still testing it so we are in early beta and we haven’t even advertised it yet which makes me curious how you found about it. &gt; Anyway we found a bug due to your activity and we’ve fixed it :). I’ve changed and set you up with a temporary password which you can change once you are logged in. &gt; The credentials are: &gt; [DELETED] &gt; Of course if you want to delete your account just write to me and I’ll do if for you right away. I understand completely. &gt; Anyway if you are interested in trying it out I’m also willing to give you an unlimited trail. 
I really enjoy using SQL Server, but the licensing model is complicated and confusing. Why do they have to ruin a good product with this bullshit? It drives people away from an otherwise decent RDBMS.
Thank you kindly, I appreciate that a lot.
I'd also like to add here that I did NOT say "Be a dick back" My response to the question of pointing out misinformation that you see posted on this subreddit was: &gt; The user who responded to you is not a moderator or any sort of MVP. Just a user expressing his opinion as you expressed yours. This subreddit is an open forum and if you have an optimized solution or counter argument them I encourage you to post it. Increases participation by experienced developers is the ideal way to better the community. 
On the first glance it looks ok. SqlFiddle can help if you don't have access to the relevant DB but know the structures: http://sqlfiddle.com/
Check to make sure your network card drivers are up to date. Certain models can have problems with TCP offloading and other driver specific features.
I ran that code and parsed it in SSMS (little check button the top or ctrl+f5 and it does not like the types of quotes you have around the literals. Change the ‘USD’ to 'USD' and it is syntactically correct.
Yeah, actually ended up just doing the &gt;= '2016-06-01' A "good to know," nonetheless. 
That's an awesome tool *bookmarked*
I changed the syntax to the following: Select * from finance.account_pivot a Inner join (select PARTY_ID, count(distinct group_id) from finance.account_pivot where currency_code = 'USD' group by PARTY_ID having count(distinct group_id) &gt; 1 ) b On a.PARTY_ID=b.PARTY_ID Where currency_code = 'USD' Order by a.PARTY_ID, a.GROUP_ID I am getting back the error, "[Teradata Database] [3706] Syntax error: All expressions in a derived table must have an explicit name." Anyone know the fix?
 SELECT * -- Please pick and choose your fields, FROM finance.account_pivot ActPiv INNER JOIN ( SELECT acct_id , COUNT(DISTINCT group_id) FROM finance.acount_pivot -- Supposed to be account_pivot? Missing a c WHERE currency_code = 'USD' -- don't ever use smart quotes GROUP BY acct_id -- never a number alwayss field name. HAVING COUNT( DISTINCT group_id) &gt; 1 ) AP_GrpCnt ON ActPiv.acct_id = AP_GrpCnt.acct_id WHERE currency_code = 'USD' -- don't ever use smart quotes ORDER BY acct_id, group_id
I think linked server queries will ramp up that counter too. Post all of your wait stats for better help.
in the subquery's SELECT list, give the COUNT a column alias (assuming you actually want to return the count -- see my earlier comment) and for next time, check the sidebar and then identify your platform
Because Oracle does it and it makes them lots of money, so Microsoft wants to do it too. At least SQL Server still includes everything into one package so you don't have to pay $23,500 extra per processor core for something like OLAP or database caching. But who knows how long that will last.
I guess it is not that relevant. Here is what I would do. Since you're a sysadmin, temporarily enable xp_cmdshell. Then construct an xp_cmdshell command for odbcconf.exe. You can find the syntax to both online. If memory serves, xp_cmdshell runs as the account running the SQL service, so it should have the local permissions needed. EDIT: I'm assuming this can't all be done from the linked server interface in management studio
My permissions are NT, no other options in my environment. I am connecting to a Teradata server - also using LDAP (NT ID)
Not your permissions just the ones for the link create a sql server user on both servers and have the that user for the link.
SQLzoo! Just go through each section.
Sort of depends on your engine how you'd limit it, but: select name, salary from Employee order by salary desc That gets you the name (presumably a column?) and the salary, and sorts it by the salary column in descending order. This will return all employees, so to limit to a top n employees, you need to either do a top or limit, depending on your choice of engine.
Make a user dsn to get going, then request a system version is made later
See connectionstrings.com
You don't need that subquery just join students into your query. You then want Having sum(b.Student_Fundraising) != 0
Just search this sub. De duplication has been answered a good few times lately, at least twice by myself.
The question you're asking is incomplete/illogical. By definition there can only be one highest salary (I know employees can have the same salary, wouldn't change the query) so a "list of employees with the highest salary" isn't a possible result. You need a range or something to specify the criteria for the list.
Sure just let me google that for you. http://dev.mysql.com/doc/refman/5.7/en/select-into.html
 Select order From order Minus Select order From order Where material='Mat3' I'm being deliberately pedantic here. It helps to know what you want from the query. Identifying orders, well sure - my query gives you the order identifiers, you probably wanted more.
If you're ok with it being a little postgres specific, https://pgexercises.com/ is great. The tasks are fairly general, so while the solutions sometimes have postgres specific details, I think it's probably still a great tool for learning SQL generally.
If it was me, it would probably be easier to do a delete of the existing records and an insert of all the new ones wrapped up in a transaction. The problem with this is that your primary key on the project skills table will change. You may not even need this however- your primary key could be a compound key of the skill and project.
Codecademy / Plurarsight / Joes2Pros all focus on T-SQL based training and I've used them all. I'd really focus on sub-queries (correlated vs non), CTE's, 2012/2014 date/time functions, basic joins and finally some performance tuning.
I even usually give them the aliases first_copy and second_copy to help me see it that way, but for some reason it still just won't click and self-joins take me way way way longer than anything else
I find it helps me to name them relevant to the task at hand. What is the purpose of the first v. second copy. Thinking through their purpose and being reminded of that purpose every time you try to use them might have you keep things clear in your head. 
You could use LIMIT to find the people with the highest salaries. Like to find the ten highest salaries: SELECT last_name FROM employees ORDER BY Salary DESC LIMIT 10;
Hey that's not a bad idea! I'm gonna try that. Maybe it is all in the names...
If there is only 1 level then that is as simple as joining any 2 tables. It is when N level recursion gets involved that it becomes tricky. I guess at one point, I worked through the concept and got it to click. Never had an issue since. Why not find some exercises on recursion and hierarchy modelling/flattening. Look for 'fill down/up hierarchy'
Agreed. Good naming can make it much easier to understand. SELECT Employee.ID, Employee.Name, Boss.ID, Boss.Name FROM dbo.Users Employee INNER JOIN dbo.Users Boss ON Employee.BossID = Boss.ID This is pretty clear what is happenining. Vs: SELECT A.ID, A.Name, B.ID, B.Name FROM dbo.Users A INNER JOIN dbo.Users B ON A.BossID = B.ID
I actually like them. :P. I use a self join to search back or/and forth in a table (with row_number). Join the table with itself on the key columns and you're good to go. What makes it so hard for you?
An example: https://www.reddit.com/r/SQL/comments/3lani7/selecting_a_row_and_2_rows_before_it/cv4oo5d?context=10000 (see attached image).
At my current company BI is an important thing but there's no one with it as their entire job. The Project Managers and Database Developers intersect to get the information that they need and do...whatever it is they do with it. Send it to people in suits, I assume. 
I was a BI developer for 4 years and just moved into another BI position with a fancier title that still has BI in the name. I basically build corporate dashboards using SQL and other tools to make fancy graphs and help make data more accessible to everyone from the board and C-level staff down to the minions in the field. I often end up automating the creation of reports that we used to get interns and our accounting team to create manually. I've always been a one-man BI team, but that might be because of specialized experience in my industry (healthcare) and SQL + web development knowledge. As Liam Neeson would say: I have a very particular set of skills.
Its a bit of umbrella term, essentially meaning data that provides the necessary information to support business decisions. BI used to be much more simple, at least from a consumers perspective. Transactional systems were using databases with reporting/dashboard layers on top (the BI presentation layer I guess); and yes, SQL/Excel etc could do all of this quite happily and the integration came from the supporting functions like SQL to the end report users. Now its changed, or certainly changing - particularly for us SQL Server folk and under BI now includes things like data science. It was always there, but now its a lot more consumer driven. Things like R are really coming to the forefront and data analytics will be an even bigger industry over the next few years. If you want specifics, predictive analysis is now much more complex than say plotting averages. At SQL Bits for example in the keynote it was even suggested that business analysts could be a thing of the past as we will have cloud based API's making, or at least supporting critical business decisions.
Did you update the client middleware installations of the services and systems connecting to the SQL Server, or are they still using the 2008R2 clients?
It sounds like that for each Scheduled Date, if they record it on a Friday at 2 PM through Monday morning (let's say at 8 AM for example), we want the Schedule Date "adjusted" to Monday morning. I think for this, you are going to need to have a table to hold your hours of operation. In this example, I am using a temp table, but you could create a permanent HoursOfOperation table and set the "IsOpen" flag to a 0 for weekends and holidays. In the two example dates selected, you can see that the one for Friday at 3 PM has been readjusted to Monday at 8 AM. --Create Hours Of Operation Table CREATE TABLE #HoursOfOperation ( OperationsDate DATETIME NOT NULL, DayOfWeekNumber INT NOT NULL, DayOfWeekName VARCHAR(50) NOT NULL, HourOfDay INT NOT NULL, IsOpen BIT NOT NULL, --This would be a 1 if it is during the week and not a holiday CONSTRAINT pk_HoursOfOperation PRIMARY KEY CLUSTERED(OperationsDate) ) --Load it with some data SET NOCOUNT ON DECLARE @d DATETIME SET @d = '20160101' WHILE @d &lt; '20500101' BEGIN INSERT INTO #HoursOfOperation SELECT @d, DATEPART(dw, @d), DATENAME(dw, @d), DATEPART(hour, @d), CASE WHEN DATEPART(dw, @d) BETWEEN 3 AND 5 THEN 1 --Tues through Th WHEN DATEPART(dw, @d) = 6 AND DATEPART(hour, @d) &lt; 14 THEN 1 --Friday, before 2 PM WHEN DATEPART(dw, @d) = 2 AND DATEPART(hour, @d) &gt;= 8 THEN 1 --Monday, after 8 AM ELSE 0 END AS IsOpen SELECT @d = DATEADD(hour, 1, @d) END SET NOCOUNT ON --Let's use two sample dates, and come up with an "Adjusted Scheduled Date" CREATE TABLE #SampleSchedules ( ScheduleDate DATETIME NOT NULL PRIMARY KEY ) INSERT INTO #SampleSchedules SELECT '2016-06-15T12:00:00' --does not need to be adjusted as this is on a Wednesday INSERT INTO #SampleSchedules SELECT '2016-06-17T15:00:00' --needs to be adjusted to Monday as this is Friday, after 2 PM --For each scheduled date, derive and Adjusted Scheduled Date SELECT ScheduleDate ,MIN(OperationsDate) AS AdjustedScheduleDate FROM #SampleSchedules s INNER JOIN #HoursOfOperation h ON s.ScheduleDate &lt;= h.OperationsDate AND h.IsOpen = 1 GROUP BY ScheduleDate DROP TABLE #HoursOfOperation DROP TABLE #SampleSchedules 
&gt; Fortunately, the marketing idiots have moved onto BIG DATA!!1!! I was once told that Big Data is like sex in high school. Everyone wants it and most kids don't know what to do when they get their hands on it.
That is a thing that you change on the report or application. 103.9 = 103.90, the difference is that the 2nd have 2 decimal places, in the database it will be stored without formation.
Bingo. Don't confuse how something is stored with how it is presented, lest you risk storing everything as `varchar`
The sub is fairly inactive. 
A BI team I worked at contained 1 BI Analyst/Reports Developer, 2 .NET Developers, 1 Statistician, 2 Business Analysts. Typical life cycle for a BI project would be something like * Gather requirements * Create data collection interface * Create ETL processes to retrieve data above and place into data warehouse * Create end user reporting tools * Help business users analyze data
I'll go against the rest of the comments I've seen here so far and say BI and its related terms have very little (if any) relation to the technology on which it's implemented. From CIO Magazine: &gt; Companies use BI to improve decision making, cut costs and identify new business opportunities. BI is more than just corporate reporting and more than a set of tools to coax data out of enterprise systems. CIOs use BI to identify inefficient business processes that are ripe for re-engineering. It's important from a career perspective to be adept at using BI technologies, but IMO it's more important to understand *why* BI is useful and *how* to apply it to business issues because that's what's important to enterprises. They couldn't give a rats ass if BI were implemented on green algae blooms in stagnant lakes, as long as it gave them the information they want. 
This is exactly what I needed! Thank you SOOO MUCH!
I might be missing something How about SELECT MIN(Date) as MinDate, MAX(Date) as MaxDate FROM table WHERE weeknum = 24
it was so simple :( Thanks for the quick response! 
&gt; YOU SHOULD FEEL BAD MAN! CURSORS ARE NOT FOR DML CODE. I'm not gonna restart a flamewar .... at least I hope I don't. There are no absolutes. Even read uncommitted has its place. Things that go against best practise can be the best solution to a given problem. They just usually are not. I'll leave it at that.
That is exactly how I feel deep down but I feel people do not understand the grey area. You have to be polarizing about some subjects because just like you will never hear a health board say "it's ok for someone pregnant to have a glass of wine a day" which it totally IS but because there will be someone who thinks ,"I saved up my glasses of wine for 10 days, it's time to party". Cursors are really tricky because many people think ,"Oh here is how you do for each loops in sql". They never think of set based solutions again.
I'm not at a computer to test, but my first thought is to use an ifnull to give the nulls an actual value. Then it should select them along with any other values according to your grouping.
I end up doing a mix of both reports that can be run on demand, as well as reports that are a one-off collection of data. I believe both fall under BI. When I say "automate reports" I'm referring to automating the collection of data that used to be done manually by the bean counters. Usually they start off as a one time report that an intern made, and then management decides they want it refreshes regularly and emailed to them, but doing so manually takes too long. So, it ends up in my list of projects and then I get to watch the accountants cry when I show them it takes 30 seconds to pull and calculate everything that that used to spend hours upon hours compiling and hand-keying previously. It's glorious. 
I do BI at a large Corp. It is a mix. You need strong command of IT skills such as SQL, some interpreted language, *nix, etc. On top of that you need a stronger understanding of how the business side works. Currently I build solutions for our accounting and finance organizations for a multi-billion dollar line in our income statement. This gets used for everything from accounting close (I built the process that generates their accruals) to gated project reporting (leveraging data from billing + provisioning + vendor invoice detail). I primarily use Oracle (we have two exadatas) a little Postgres (two netezza appliances), informatica, and Python. More importantly I spent the better part of a decade working on the business side and I understand how all of the systems work which allows me to work much closer with the business side to get them the solution they want.
Hi. I'm a Senior Business Intelligence Engineer for a major telecom. I've been in BI for about 5 years, and a data developer/analyst for 15. It helps to think of what the word "intelligence" in this sense means, so think of military intelligence. They analyze an enemy's strategies, an enemy's capabilities, the environment, the available tools and resources, the nearby civilian culture, political history and more. They also conduct some clandestine operations on behalf of senior leadership. At the end of the day, they make recommendations to senior leadership and military personnel so that the best decisions can be made. This is what BI does. We obtain every fact imaginable regarding employee activities, customer activities, sales, product/service categorization, operations and more. We identify how to best summarize this and share it with managers and executives. *** My day to day work is a hybrid of warehousing and architecture, with a healthy dollop of analysis. I identify the best solutions to quickly move lots of raw data. I merge and denormalie the raw data into scalable reporting objects. And I build platforms to report on that information 
I think you need one more variable - DateTime When Scheduled. It could just be a flag but I think housing the datetime would serve better. That would drive your logic of if this thing was created in a certain window, ignore it for a time. That said, the stuff the other folks have already said is all correct as well.
Please provide one rows worth of data so we can confirm that it's actually hex. It could be something else like a GUID. Please share the code you used. Which version of SQL are you using?
don't you think the APPLICATION USING the field will also need to be updated?
varbinary(max) is a BLOB already. Are you talking about base64 conversion (binary to text)?
FYI: Your VARBINARY() *is already a BLOB*. That "hex format" that you see, 0x89504E470 is just a *text representation* of the binary 1's and 0's that are stored in your BLOB. The second example you sent looks like base64 or some other encoding, that also represents binary data. Just like hex, it uses a set of printable characters that can be used to represent any combination of 1's and 0's. What you really want to do is convert between different representations. Do you have access to a scripting language? Because this is incredibly easy in Perl, PHP, Javascript/Node, etc. They all have built-in functions for this kind of work. If you *have* to use pure SQL, then your best bet is to try some of these hacks in the XML engine to convert your data in the query. http://stackoverflow.com/questions/5082345/base64-encoding-in-sql-server-2005-t-sql
No, it doesn't work like this. In order to achieve it, you would have to join some other table or use UNION ALL to add missing rows.
If you're using just strictly dates and nothing else you should be fine. DECLARE @tester1 table (ident int, testdate date) INSERT INTO @tester1 VALUES (1, '2001-01-01'), (1, '2010-12-31'), (1, '2016-06-14') SELECT ident, MIN(testdate), MAX(testdate) FROM @tester1 GROUP BY ident That gives you one row But if you want anything else you'll get multiple rows DECLARE @tester1 table (ident int, fruit varchar(6), testdate date) INSERT INTO @tester1 VALUES (1, 'apple', '2001-01-01'), (1, 'orange', '2010-12-31'), (1, 'cherry', '2016-06-14') SELECT ident, fruit, MIN(testdate), MAX(testdate) FROM @tester1 GROUP BY ident, fruit That gives you three My original code was superfluous. Just using MIN and MAX should be fine (I wrote it on my phone in bed, sorry). But this is why I said your sometimes get undesired results 
I had an idea but it really didn't pan out once I looked at it more closely. the pertinent points that still apply. You are neglecting to mention what DBMS you're working in which could complicate things. Since you're doing a trigger after update, I think the new value should actually be in Table, so far so good. You're not checking to see if the Table.AssetID actually changed, so you're always updating X and y. That seems inefficient, even though it shouldn't cause your problem. 
Seems like it is more the grouping levels than anything specific to the data type. 
and database analyst my first sql job was mostly creating reports with report builder. I had to go out of my way to use management studio and writing queries from scratch. And a huge part of it is understanding how a database works. I have had three different jobs working with different oracle and sql server databases and all of them maintained their data in different ways and for different reasons. Some have been reporting friendly while others were not. It's going to take some time to understand the data you are working with and knowing SQL is only going to get you so far with that. edit: I didn't mean to latch onto this post. I just realized I had more to say after responding with database analyst
Hello Everyone! I Managed to get it working using the bcp utility! Thanks everyone that replied:)
OK, so that people can read this...: --Okay so I have a massive join query that works just fine: SELECT msi_CustomerOrderItems.partID ,SUM(msi_CustomerOrderItems.numOfParts) AS 'numOfParts' ,msi_parts_substrate.substrateFileName AS 'thisFileName' ,msi_parts_substrate.substrateID ,msi_parts_substrate.priorityID ,msi_material_substrate.substrateGrainDirection AS 'grainDirection' ,msi_material_substrate.allowMirroring ,msi_material_substrate.substrateName ,msi_customerDivisonSettings.substrateFolderPath AS 'thisFolderPath' ,msi_parts_substrate.departmentID ,msi_CustomerOrder.customerDivisionID ,msi_parts.PrintNumber ,msi_customerDivison.divisionName ,msi_departments.departmentName FROM msi_CustomerBatchOrder INNER JOIN msi_customerBatchOrderAssoc ON msi_CustomerBatchOrder.customerBatchID = msi_customerBatchOrderAssoc.customerBatchID INNER JOIN msi_CustomerOrder ON msi_customerBatchOrderAssoc.CustomerOrderID = msi_CustomerOrder.CustomerOrderID INNER JOIN msi_CustomerOrderItemAssoc ON msi_CustomerOrder.CustomerOrderID = msi_CustomerOrderItemAssoc.CustomerOrderID AND msi_CustomerOrder.CustomerOrderID = msi_CustomerOrderItemAssoc.CustomerOrderID INNER JOIN msi_CustomerOrderItems ON msi_CustomerOrderItemAssoc.CustomerOrderItemID = msi_CustomerOrderItems.CustomerOrderItemID INNER JOIN msi_parts ON msi_CustomerOrderItems.partID = msi_parts.partID INNER JOIN msi_parts_substrateassoc ON msi_parts.partID = msi_parts_substrateassoc.partID INNER JOIN msi_parts_substrate ON msi_parts_substrateassoc.partSubstrateID = msi_parts_substrate.partSubstrateID INNER JOIN msi_material_substrate ON msi_parts_substrate.substrateID = msi_material_substrate.substrateID INNER JOIN msi_customerDivisonSettings ON msi_CustomerOrder.customerDivisionID = msi_customerDivisonSettings.customerDivisionID INNER JOIN msi_customerDivison ON msi_CustomerOrder.customerDivisionID = msi_customerDivison.customerDivisionID INNER JOIN msi_departments ON msi_parts_substrate.departmentID = msi_departments.departmentID GROUP BY msi_CustomerOrderItems.partID ,msi_parts_substrate.substrateFileName ,msi_parts_substrate.substrateID ,msi_parts_substrate.priorityID ,msi_material_substrate.substrateGrainDirection ,msi_material_substrate.allowMirroring ,msi_material_substrate.substrateName ,msi_customerDivisonSettings.substrateFolderPath ,msi_parts_substrate.departmentID ,msi_CustomerOrder.customerDivisionID ,msi_parts.PrintNumber ,msi_customerDivison.divisionName ,msi_departments.departmentName ORDER BY msi_parts_substrate.substrateID ,msi_CustomerOrder.customerDivisionID ,msi_parts.PrintNumber --Now, I need to get another field from another table. I want to return only the top value based on this query, that also works: SELECT TOP (1) msi_departments.departmentName ,msi_departments.departmentAbbr ,msi_parts_partFlowAssoc.partID ,msi_parts_partFlow.departmentID FROM msi_parts_partFlowAssoc INNER JOIN msi_parts_partFlow ON msi_parts_partFlowAssoc.partFlowID = msi_parts_partFlow.partFlowID INNER JOIN msi_departments ON msi_parts_partFlow.departmentID = msi_departments.departmentID WHERE (msi_parts_partFlowAssoc.partID = 1) AND (msi_parts_partFlowAssoc.partFlowID &gt; 1) AND (msi_parts_partFlow.departmentID NOT IN (2)) AND (msi_parts_partFlow.independentFlow = 0) ORDER BY msi_parts_partFlow.partFlowOrder --Now I am trying to link these together so that I can add columns to the first query. I did a bit of looking and found this and this works: SELECT c.partID ,a.partFlowID FROM msi_parts AS C OUTER APPLY ( SELECT TOP 1 * FROM msi_parts_partFlowAssoc WHERE partID = c.partID ) AS A --I thought that I could then just drop in the second query to replace the last select, changing the partID =1 &lt;--- was just a placeholder in the second query with partID = c.partID so I could join it for each row. It doesn't like it. This was my attempt to join queries 2 and 3 before ultimately combining it with 1. --THIS DOESN'T WORK: SELECT c.partID ,a.partFlowID FROM msi_parts AS C OUTER APPLY ( SELECT TOP (1) msi_departments.departmentName ,msi_departments.departmentAbbr ,msi_parts_partFlowAssoc.partID ,msi_parts_partFlow.departmentID FROM msi_parts_partFlowAssoc INNER JOIN msi_parts_partFlow ON msi_parts_partFlowAssoc.partFlowID = msi_parts_partFlow.partFlowID INNER JOIN msi_departments ON msi_parts_partFlow.departmentID = msi_departments.departmentID WHERE (msi_parts_partFlowAssoc.partID = c.partID) AND (msi_parts_partFlowAssoc.partFlowID &gt; 1) AND (msi_parts_partFlow.departmentID NOT IN (2)) AND (msi_parts_partFlow.independentFlow = 0) ORDER BY msi_parts_partFlow.partFlowOrder ) AS A --The alternative is to find some other way to joins two queries in some other fashion which is very hacky or to hit the server with a crazy amount of queries. --I would say that my SQL skills are mediocre. So feel free to let me have it. Any help is appreciated.
No problem. You might need to give a bit more information too. What errors are you getting? Which field do you want from the other table? You've mentioned you want one but have selected four.
I appreciate your help and I knew it was a lot to chew. I was getting an error about columns not being valid. I did find the solution and it was CROSS APPLY and it is a beast SELECT msi_CustomerOrderItems.partID, SUM(msi_CustomerOrderItems.numOfParts) AS numOfParts, msi_parts_substrate.substrateFileName AS thisFileName, msi_parts_substrate.substrateID, msi_parts_substrate.priorityID, msi_material_substrate.substrateGrainDirection AS grainDirection, msi_material_substrate.allowMirroring, msi_material_substrate.substrateName, msi_customerDivisonSettings.substrateFolderPath AS thisFolderPath, msi_parts_substrate.departmentID, msi_CustomerOrder.customerDivisionID, msi_parts.PrintNumber, msi_customerDivison.divisionName, msi_departments.departmentName, nextdepartment.departmentName, nextdepartment.departmentAbbr, nextdepartment.departmentID FROM msi_CustomerBatchOrder INNER JOIN msi_customerBatchOrderAssoc ON msi_CustomerBatchOrder.customerBatchID = msi_customerBatchOrderAssoc.customerBatchID INNER JOIN msi_CustomerOrder ON msi_customerBatchOrderAssoc.CustomerOrderID = msi_CustomerOrder.CustomerOrderID INNER JOIN msi_CustomerOrderItemAssoc ON msi_CustomerOrder.CustomerOrderID = msi_CustomerOrderItemAssoc.CustomerOrderID AND msi_CustomerOrder.CustomerOrderID = msi_CustomerOrderItemAssoc.CustomerOrderID INNER JOIN msi_CustomerOrderItems ON msi_CustomerOrderItemAssoc.CustomerOrderItemID = msi_CustomerOrderItems.CustomerOrderItemID INNER JOIN msi_parts ON msi_CustomerOrderItems.partID = msi_parts.partID INNER JOIN msi_parts_substrateassoc ON msi_parts.partID = msi_parts_substrateassoc.partID INNER JOIN msi_parts_substrate ON msi_parts_substrateassoc.partSubstrateID = msi_parts_substrate.partSubstrateID INNER JOIN msi_material_substrate ON msi_parts_substrate.substrateID = msi_material_substrate.substrateID INNER JOIN msi_customerDivisonSettings ON msi_CustomerOrder.customerDivisionID = msi_customerDivisonSettings.customerDivisionID INNER JOIN msi_customerDivison ON msi_CustomerOrder.customerDivisionID = msi_customerDivison.customerDivisionID INNER JOIN msi_departments ON msi_parts_substrate.departmentID = msi_departments.departmentID CROSS APPLY ( SELECT TOP (1) msi_departments.departmentName, msi_departments.departmentAbbr, msi_departments.departmentID FROM msi_parts_partFlowAssoc INNER JOIN msi_parts_partFlow ON msi_parts_partFlowAssoc.partFlowID = msi_parts_partFlow.partFlowID INNER JOIN msi_departments ON msi_parts_partFlow.departmentID = msi_departments.departmentID WHERE (msi_parts_partFlowAssoc.partID = 1) AND (msi_parts_partFlowAssoc.partFlowID &gt; 1) AND (msi_parts_partFlow.departmentID NOT IN (2)) AND (msi_parts_partFlow.independentFlow = 0) ) nextdepartment GROUP BY msi_CustomerOrderItems.partID, msi_parts_substrate.substrateFileName, msi_parts_substrate.substrateID, msi_parts_substrate.priorityID, msi_material_substrate.substrateGrainDirection, msi_material_substrate.allowMirroring, msi_material_substrate.substrateName, msi_customerDivisonSettings.substrateFolderPath, msi_parts_substrate.departmentID, msi_CustomerOrder.customerDivisionID, msi_parts.PrintNumber, msi_customerDivison.divisionName, msi_departments.departmentName, nextdepartment.departmentName, nextdepartment.departmentAbbr, nextdepartment.departmentID ORDER BY msi_CustomerOrderItems.partID 
If someone makes a physical copy of your house key and uses it, is that a problem with the lock on the door?
https://msdn.microsoft.com/en-us/library/jj590844.aspx 
If you like real-world datasets to play with, try the Health Canada Drug Product Database: http://www.hc-sc.gc.ca/dhp-mps/prodpharma/databasdon/dpd_bdpp_data_extract-eng.php It's a series of CSV files that you have to import and join by DRUG_MFR column to get relevant tables. Pretty good practice for real-world problems because the data is real.
&gt; And a huge part of it is understanding how a database works. I have had three different jobs working with different oracle and sql server databases and all of them maintained their data in different ways and for different reasons. Some have been reporting friendly while others were not. It's going to take some time to understand the data you are working with and knowing SQL is only going to get you so far with that. I think this is a big aspect of it. A lot of companies seem to think they can just 'hire' BI, but I find it's often based on a solid operational knowledge of that specific company. If you don't know what parts of the database are relevant for any specific request, you're not really providing much BI value.
Just came back because I realized that I never linked the query back up the ID (partID) generated by the first half of the query. The second half was still set to a static value partID=1. When I try to create an alias in the first half I get these errors: The multi-part identifier "msi_CustomerOrderItems.partID" could not be bound. That error shows up twice and the last is the same but with "NumOfParts" So I am back to being stuck.
&gt; multi-part identifier "msi_CustomerOrderItems.partID" could not be bound Okay, just got unstuck. Realized that I needed to create an alias for my msi_CustomerOrderItems table. Once I did that and changed all of the references to that alias I was good: SELECT a.partID, SUM(a.numOfParts) AS numOfParts, msi_parts_substrate.substrateFileName AS thisFileName, msi_parts_substrate.substrateID, msi_parts_substrate.priorityID, msi_material_substrate.substrateGrainDirection AS grainDirection, msi_material_substrate.allowMirroring, msi_material_substrate.substrateName, msi_customerDivisonSettings.substrateFolderPath AS thisFolderPath, msi_parts_substrate.departmentID, msi_CustomerOrder.customerDivisionID, msi_parts.PrintNumber, msi_customerDivison.divisionName, msi_departments.departmentName, nextdepartment.departmentName AS nextDepartmentName, nextdepartment.departmentAbbr AS nextDepartmentAbbr, nextdepartment.departmentID AS nextdepartmentID FROM msi_CustomerBatchOrder INNER JOIN msi_customerBatchOrderAssoc ON msi_CustomerBatchOrder.customerBatchID = msi_customerBatchOrderAssoc.customerBatchID INNER JOIN msi_CustomerOrder ON msi_customerBatchOrderAssoc.CustomerOrderID = msi_CustomerOrder.CustomerOrderID INNER JOIN msi_CustomerOrderItemAssoc ON msi_CustomerOrder.CustomerOrderID = msi_CustomerOrderItemAssoc.CustomerOrderID AND msi_CustomerOrder.CustomerOrderID = msi_CustomerOrderItemAssoc.CustomerOrderID INNER JOIN msi_CustomerOrderItems a ON msi_CustomerOrderItemAssoc.CustomerOrderItemID = a.CustomerOrderItemID INNER JOIN msi_parts ON a.partID = msi_parts.partID INNER JOIN msi_parts_substrateassoc ON msi_parts.partID = msi_parts_substrateassoc.partID INNER JOIN msi_parts_substrate ON msi_parts_substrateassoc.partSubstrateID = msi_parts_substrate.partSubstrateID INNER JOIN msi_material_substrate ON msi_parts_substrate.substrateID = msi_material_substrate.substrateID INNER JOIN msi_customerDivisonSettings ON msi_CustomerOrder.customerDivisionID = msi_customerDivisonSettings.customerDivisionID INNER JOIN msi_customerDivison ON msi_CustomerOrder.customerDivisionID = msi_customerDivison.customerDivisionID INNER JOIN msi_departments ON msi_parts_substrate.departmentID = msi_departments.departmentID CROSS APPLY ( SELECT TOP (1) msi_departments.departmentName, msi_departments.departmentAbbr, msi_departments.departmentID FROM msi_parts_partFlowAssoc INNER JOIN msi_parts_partFlow ON msi_parts_partFlowAssoc.partFlowID = msi_parts_partFlow.partFlowID INNER JOIN msi_departments ON msi_parts_partFlow.departmentID = msi_departments.departmentID WHERE (msi_parts_partFlowAssoc.partID = a.partID) AND (msi_parts_partFlowAssoc.partFlowID &gt; 1) AND (msi_parts_partFlow.departmentID NOT IN (2)) AND (msi_parts_partFlow.independentFlow = 0) ) nextdepartment GROUP BY a.partID, msi_parts_substrate.substrateFileName, msi_parts_substrate.substrateID, msi_parts_substrate.priorityID, msi_material_substrate.substrateGrainDirection, msi_material_substrate.allowMirroring, msi_material_substrate.substrateName, msi_customerDivisonSettings.substrateFolderPath, msi_parts_substrate.departmentID, msi_CustomerOrder.customerDivisionID, msi_parts.PrintNumber, msi_customerDivison.divisionName, msi_departments.departmentName, nextdepartment.departmentName, nextdepartment.departmentAbbr, nextdepartment.departmentID ORDER BY msi_parts_substrate.substrateID, msi_CustomerOrder.customerDivisionID, msi_parts.PrintNumber
Junior DBA 
Trying to find what the "out-of-the-box" security settings are regarding these catalog views, but I can't. You'll have to inspect them to see what security permissions are involved. That being said, I found [this document](https://technet.microsoft.com/en-us/library/ms189586(v=sql.110\).aspx) to provide a decent overview of all the different paths you can use for encryption. EDIT: Markdown sucks.
&gt;god when you think about it, 95% of what you can do in SQL is against best practice Most true statement ever written on this sub
If you happen to be located in south Florida, PM me your resume. My team is actually looking for an entry level data analyst. No other language required as long as you're willing to learn. 
Junior database developer
Look for titles with Jr/Junior or associate
That encryption is meant to protect the data on disk. So if someone gets a copy of your database files, or a backup, they can't access the data. If they can run queries against your database on the server your database is attached to...yeah they don't even need your encryption keys they can just go grab whatever tables they want right? :) That said, you can also prevent who can reach the sys.tables or any table for that matter by restricting users. So that way not only would they have to connect to the database, on its current server, but they'd have to be logged in "as you" to get things.
Typing this from the phone: Select members from (Select members, sum(case when plancode =3 then 1 else 0 end) count3, sum(case when plancode=1 then 1 else 0 end) count1 from table group by members ) Where count3 &gt; 0 and count1 =0;
Make sure you have an index on plancodes. select members from table t where plancodes = 3 and not exists (select null from table t2 where t2.members = t.members and t2.plancodes = 1) If you want distinct values, change to select distinct members ...
I thought about that, but remember I said that I am only mediocre. LOL. I have only used temp tables a couple of times and all of those times were prompted by what I found online. I don't have a grasp of how to create and how to join. I do appreciate the advice and I will try to find out more online. The huge queries are definitely a mess to work with now, can't image what they will look like when I have to come back to it some day. 
As the number of joins increases so does the number of possible join trees. with X tables the number of possible trees is (2X-2)!/(X-1)!. So the more joins the longer the optimizer will search for possible plans and become counterproductive. http://bit.ly/1UbBhaT Also see http://amzn.to/261nTYW 
Absolutely. Sqlite.org/cli.html on some neat things you can do with it from the cli. I know you may want to rush out and try ms SQL, mysql or postgres but focus on queries and polling the schema first. 
Ask yourself why you're clearing the plan cache. If you're just experiencing a problem with one or two plans, you can remove those individually without flushing the whole cache. It would be similar to a restart of SQL Server in terms of performance (although without such a big hit on the buffer cache) Everything submitted to the optimiser would need recompiling, so there will be an initial performance hit for each new query. Additionally, if you have unstable plans there is a chance that they would recompile to a non-optimal plan So the answer is really - it depends, but there are definitely ways around clearing the entire plan cache that you can and should use if you have performance concerns
I support this statement (partially)! In my opinion it would be best if everyone tries to learn new stuff at the most basic level possible, like using just the CLI or just a simple text editor for your first coding projects. **But** when it comes to interfaces for DBs in, my expirience as a strong visual person, it can slow your learning progress down if you cannot display the relations between tables in a graphic manner. Therefore I would suggest that you handle all your queries in a CLI like "Sqlite.org/cli.html" but use something more advanced to get the bigger picture once in a while (try [JetBrains DataGrip](https://www.jetbrains.com/datagrip/download/) which has 30d trial and can connect to many RDBMS). **Try to resist the temptations of advanced tools for some time!**
I've never heard of `sqldiff`, is it only showing you the differences in DDL (tables, columns, constraints etc) or is it meant to show you the difference in data too? If it's the former, then my guess would be that the two databases share a structure but one has more data in than the other. Also, does vacuuming your databases make any difference?
Thanks. I ran this command on each db, but the size difference remains: $sqlite3 1.db "VACUUM;" 
What's your reason for not advising SQL Server Express Edition or even LocalDB instead? Differences in T-SQL? 
For an entry level analyst, you can eventually find somebody to take a chance on you with just SQL. But good luck beating out the person who has SQL and knows Excel really well. I'll give you that becoming competent in Excel is not that difficult, but why would an employer not hire the person who is already really good at it?
It's the quickest way I can think of to get right into SQL coding. The CLI also teaches OP the fundamentals of what SQL really is, i.e., it's not graphical relationships (although that helps conceptually.) As for the differences between SQLite and the others, it's likely not relevant for OP at this point. Creating tables, loading data, update, joins, etc., is what OP needs to start. The "nice stuff" becomes more important later. 
Most databases will allocate space into their files, but won't release it once it is done with, instead retaining the free space as empty pages. Vaccum should shrink it though as far as I know, but then I'm no progress expert. (edit: looked this up and it should. try the verbose option in case it can't leg go for some reason?) If your databases are identical data and structure wise, there is no need to panic.
I don't see any issue with using them with different entity types. At the oil company I work for, for instance, our ERP system has a single entity table, with an entity type ID on it, and then an entity relationship table which has the parent/child IDs on it. I have a recursive CTE to build the entity hierarchy from Asset Team (organizational group) &gt; Field (place) &gt; Facility (property) &gt; Well (object) which I have saved as a snippet and comes in handy pretty regularly It's something like: ;WITH hrchy AS ( SELECT EntityID , EntityType , AssetTeamID = EntityID , AssetTeamCode = EntityCode , FieldID = CAST(NULL AS INT) , FieldCode = CAST(NULL AS VARCHAR(20)) , FacilityID = CAST(NULL AS INT) , FacilityCode = CAST(NULL AS VARCHAR(20)) , WellID = CAST(NULL AS INT) , WellCode = CAST(NULL AS VARCHAR(20)) FROM Entities e INNER JOIN EntityType et ON et.EntityTypeID = e.EntityTypeID AND et.EntityType = 'AssetTeam' UNION ALL SELECT e.EntityID , et.EntityType , h.AssetTeamID , h.AssetTeamCode , FieldID = CAST(ISNULL(h.FieldID, CASE et.EntityType WHEN 'Field' THEN e.EntityID END) AS INT) , FieldCode = CAST(ISNULL(h.FieldCode, CASE et.EntityType WHEN 'Field' THEN e.EntityCode END) AS VARCHAR(20)) [etc...] FROM hrchy h INNER JOIN EntityRelation r ON r.ParentID = h.EntityID INNER JOIN Entity e ON e.EntityID = r.ChildID INNER JOIN EntityType et ON et.EntityTypeID = e.EntityTypeID AND et.EntityType IN ('Field','Facility','Well') ) SELECT * INTO #hrchy FROM hrchy OPTION (MAXRECURSION 32767) In reality, there are more levels to the hierarchy and more attributes of each than that, but you get the idea. I then join #hrchy to the entity attribute detail tables as necessary. My main recommendation would be to avoid chaining your recursive CTE with a bunch of other CTEs. As tempting as it may be to do everything in a single SELECT, it's slow as hell. Load it into a temp table and then use it.
Okay, I just felt that the effort of downloading and installing SQL Server Express Edition with SQL Server Management Studio was trivial and you do get a nice editor to go with it. You don't have to use any of the graphical stuff (I hardly ever do). I've never used SQLite, so I don't know how nice the CLI is, but I'd want to at least have something like Notepad++ with it). On top of that you're being introduced to a toolset that is common in many professional environments. I mean, if you wanted to get right into the action with minimal effort, might as well use something web-based then. :)
I love CTEs and when I found them I had the urge to use them almost everywhere as well. They're useful and produce more readable code, allowing you to get a little more imperative with the order in which your query executes. They're also extremely useful for data analysis and are awesome to use in combination with SQL Server's "UPDATE FROM" syntax. The biggest downside to using them is that they will sometimes throw the query engine for a loop and produce inefficient execution plans. SQL Server is pretty good at performing well with them. Oracle 11g is a little further behind, IMO. Sometimes you can go overboard with CTEs and use them where they aren't really providing any clarity or readability, that's really the only time I would say "don't use them."
&gt; you're being introduced to a toolset that is common in many professional environments Definitely a good counter-argument, but I think OP just needs the basics at this point. As for Notepad++, that's what I use. My SQL requirements are pretty light and I use it to prep data for financial planning and budgeting model application development, I do nothing for production. 
The type of workload that would really suffer is workloads that have a lot of plan re-use. So applications where a lot of the queries are parameterized or relying on stored procedures and passing variables into them.
There's really no functional difference between a derived table and a cte (huge oversimplification but you get the idea). It's just a way of creating a virtual table on the fly. The intention with CTE's is you create a derived table that you can then simply re-use over and over throughout the query without having to retype the entire syntax. If you only use it once, why not just use a derived table since it's less code? That said if the data changes very rapidly then two derived tables have the potential to produce different results so a cte is better. In most cases I find it doesn't matter but I think the old saying applies. When all you have is a hammer, everything looks like a nail. Try to use the right tool for the job, try to be consistent so others can read and update your code, and always try to use the simplest method to reach the goal.
Self referencing tables should be used only when a hierarchy is composed of homogenous members. This is sometimes called a 'pig's ear'. They are especially useful for n level hierarchies. For fixed level heterogeneous hierarchies (Planet-Continent-Country-Region-etc) there should be distinct entitites for each level. You can mix levels e.g. 3 fixed hetero then n level homo if the logical model calls for it. The specific SQLS implementation of recursion via cte's should not be confused with using cte's in general.
&gt; Hope that's clear enough sorry, it isn't could you give examples?
My point is that you can use SQL Server Express for basic things without somehow spoiling his learning process, but I guess anything will work at this point. :) 
 With c as ( Select team1 as team From mytable Union all Select team2 From mytable ) Select team ,count(*) From c Group by team
Hih - what would you suggest to make it easy to understand? Thanks!
There is no magical number of joins. The optimal plan should naturally be generated by the optimiser and if it us not then statistics and indexes should be improved until it does. Once you start storing intermediate results you are taking huge numbers of possible plan permutation possibilities away from the optimiser.
That's an old thread to follow up on :) Just call specifically whatever your data is (for example, "code/identifier that has an optional block of letters followed by a block of numbers"); Otherwise, "8u33aaa" would be alphanumeric as well.
The update() function is normally to check if a column name is involved in the update. In this case I'd expect something like If update([AssetID]) Also triggers must join to the pseduo table *inserted* to limit the scope of your update to the rows involved in the update. This is all based on guesswork on which platform you are using. Please read the sidebar.
There's a couple ways you could do this. Here's a quick and dirty one with nested queries. SELECT team, SUM(cnt) AS total_count FROM ( SELECT team1 AS team, COUNT(*) AS cnt FROM my_table GROUP BY team1 UNION ALL SELECT team2 AS team, COUNT(*) AS cnt FROM my_table GROUP BY team2 ) AS t GROUP BY team; 
Oops my bad Select team ,count(*) From ( Select team1 as team From mytable Union all Select team2 From mytable ) c Group by team
you should store the data in a DATE type column. The way it appears on a report etc.. will be determined by NLS_FORMAT 
Oh totally. Don't get me wrong, I know well what it is like pulling from such databases, when their design is utterly beyond your control. The comment was more a theory/design note for anyone considering such a pattern. Put it this way, last time I bumped into someone putting this design at the heart of their product, I watched them burn their way through several million £ only to dump the whole concept. Later a proper designer replaced it with a clean normalised db in about a week.
&gt; There's really no functional difference between a derived table and a cte (huge oversimplification but you get the idea). It's just a way of creating a virtual table on the fly. The intention with CTE's is you create a derived table that you can then simply re-use over and over throughout the query without having to retype the entire syntax. SQL Server and Oracle behave *very* differently here. In Oracle, you can use it for performance gains; in SQL Server, it's just an improvement in readability. * Oracle will create a temp table out of a CTE and use that when the CTE is referenced. * SQL Server, a CTE is just syntactic sugar. It essentially replaces the CTE references in your query with the derived table like you said. Which means that if your CTE is fairly complex and you use it 4 times in a given query (like in a subquery), you're going to execute that complex query four times. IOW, there's **zero** performance improvement. In a situation like that, dumping your data into a temp table will likely (but not always) result in better overall performance.
Didn't know that about Oracle, I'm useless on that platform. But agreed on performance with the caveat. If the query is complex enough it's worth testing out a temptable or table variable instead to see if using and then re-using one of those will work better than the derived table\CTE option. I could see how straight forward reads result in the same IOps either way. And very computational windowing works better as a temp table.
I think what he was referring to is using aggregate functions on other kinds of data types. MAX() on a char field is decided by the collation. A fair point in general but it does not specifically apply here.
try /r/ETL If you had SQL Server I could help you.
I don't know if Oracle has anything built in but you are starting to get in the realm of reporting and scheduling. You'll most likely need to get a third party reporting tool. We use a third party program that allows us to schedule reports to burst out via email and it allows us to export via PDF, CSV, Excel, etc. Google stuff like "Oracle reporting" and "Oracle reporting tools". You'll find something.
Unless you have access to Oracle's reporting tools you will have to write a program to do this
&gt; Didn't know that about Oracle, I'm useless on that platform. Same here, I haven't touched it in close to 20 years. I learned that tidbit from a previous CTE discussion here on reddit.
ORDER BY Age Ascending, Height Descending 
I know BIRT can do it, both in the opensource and commercial versions. The commercial has a better excel emitter and web interface for scheduling. Soon I'll be switching jobs and I'll know for sure, but I assume MicroStrategy can do it too. 
Toad Data Point is what we use - there's a freeware edition. In addition to being a complete cross platform querying tool, it also supports extensive automation features, including executing queries and exporting the results to Excel, etc. and sending emails with attachments.
This is the exact solution I thought of when reading the top posted. +1 
Yup, that's me! It's actually supposed to be *Rex Anglorum et Saxonum* (King of the English and Saxons) but I dropped the *et*. Ælfred went by a [bunch of names](http://www.archontology.org/nations/uk/england/anglosaxon/01_kingstyle_0871.php). Some of which are rather long and grand. &gt;'rex et primicerius totius Albionis regni' (King of all parts of Albionis (Britannia)) His successors had even grander titles. &gt;Eadred was styled 'regis qui regimina regnorum Angulsaxna, Norþhymbra, Paganorum, Brettonumque' ('king of the Anglo-Saxons, Northumbrians, pagans, and Britons') in a charter of 946 (Sawyer 520), and again in 949-950. He was reduced to 'rex Anglorum' in 951, and raised back to 'king of the Anglo-Saxons, Northumbrians, pagans, and Britons' in 954. Good stuff.
&gt; Is it Possible to Have Select TOP Return Nulls? Yes, if they exist. What it appears you're asking &gt; Is it Possible to Have Select TOP the number of rows listed even if fewer matching rows exist in the select set? And the answer to that is no. You have to have at least *n* rows in your select set before TOP *n* is applied to actually get *n* rows back.
 SELECT A.* FROM @Sales A LEFT OUTER JOIN @Sales B ON B.CustomerID = A.CustomerID AND B.SKU &lt;&gt; A.Sku WHERE B.CustomerID is NULL
Do you mean in a single sale, or ever? 
I don't use MS Access enough to give you a specific solution, but I think you might want to use 'Format'. e.g. format(YourDateTimeValue, ShortDate) on both columns so that you are comparing shortdate to shortdate...
Sounds like you're doing a full replacement of the data (vs an incremental update). Regardless, the change to your table can be split into inserts (new data), deletes (records that are no longer needed), updates( changed attributes for known keys) and unchanged data. You'll have 2 options/paths here: if the data volume is small, you can forgo the analysis, truncate the destination and simply re-insert all the data. Even if the data volume is large but the 'unchanged data' constitutes a small fraction you can still use the same approach. 2nd approach would be useful when there's a relatively large amount of data transferred along with a lot of unchanged data. I would recommend bringing all of that data to the target server in a staging location using method #1 and then running comparison and relevant DML on the target server itself (detect records to delete, detect records to update, detect new data). The last 2 (update/insert) can also be done via a merge statement. 
 DECLARE @Items TABLE (CustName varchar(20), ProductID INT) INSERT INTO @Items (CustName, ProductID) VALUES ('John', 12) INSERT INTO @Items (CustName, ProductID) VALUES ('John', 15) INSERT INTO @Items (CustName, ProductID) VALUES ('John', 1) INSERT INTO @Items (CustName, ProductID) VALUES ('Mary', 8) INSERT INTO @Items (CustName, ProductID) VALUES ('Mary', 12) INSERT INTO @Items (CustName, ProductID) VALUES ('Ann', 1) SELECT *, ROW_NUMBER() OVER (PARTITION BY CustName ORDER BY ProductID) AS SequenceNumber FROM @Items
I do this exact task with an odbc connection and powershell. Powershell can handle executing the all and formatting it to xls or html and then triggering the email. You can then run a scheduled task as needed.
Okay, I had no idea that those functions existed, but that was exactly what I needed. Thank you so much!
There's a couple different ways to do it here: http://stackoverflow.com/questions/10019557/join-sql-server-tables-on-a-like-statement
Thanks for your help. Says Quarter is an invalid identifier?
Apparently I should actually read the question rather than skim it, didn't realize it's comparing substrings of both. Your solution above looks like it'd cover the request.
Apologies. In an attempt to be vague, I used `@SQLString` too generally. That is meant to represent the `@SQLString(11,12,13)K`. As for the order by, that is something I can have them look into. Thank you
I just tried it, and it is giving me an error with the ending modulus. I'm trying to decypher through it, but I am more of a programmer than a DBA.
Forgot another '+' after p.item, should be corrected in the original post now. Additionally I tested to verify this on one of my test databases and it does work. Went through about a quarter million rows in about 30 seconds. 
The best kinds of arguments!
No. The Table Data Import is a feature in the MySQL workbench. I guess I'm a little confused myself as to what you're trying to accomplish. When you say you want to import the Excel data to .sql, are you expecting insert statements in the sql file? Screen shots of the MySQL feature above. http://screencast.com/t/NPfFncgj http://screencast.com/t/XpAhtceS I used my workflow above when I got a crapload of data in Excel an needed to join some information between multiple spreadsheets. I just created tables in MySQL, populate them with the data in the excel sheets, and wrote my join statements.
Are @SQLString13J and @TableName13J variables declared somewhere in the code before this section? Or it's a typo?
I just want to take my excel data, which is currently saved as a .xls file, and export it to MySQL so I can effective queries from it.
I see. Well, doesn't seem anything obvious. Try splitting the segments into separate blocks (with GO statements).
Try [OmniDB](https://www.omnidb.com.br). It supports many RDBMS and can convert databases from one RDBMS to another.
Look into a window function - i.e. `ROW_NUMBER` - the `PARTITION BY` and `ORDER BY` clauses will do what you want.
I don't see how LIMIT would help you - there is FETCH and OFFSET which do what LIMIT does - but it wouldn't help you (that would give you the first N rows - not the first N rows BY something) select nbr, id, dt from ( select nbr, id, dt, row_number() over (partition by nbr order by dt ) rn from t ) where rn = 1 will do it. the analytic windowing function will break the data up by NBR's, and within each NBR order by DT from small to big and after it has ordered the data within a NBR, it will assign a sequential number 1, 2, 3, .... N. Just keep the "first one" NOTE: if you have duplicate DT values within a NBR value - this query is non-deterministic, running it twice on the same data can and will return different results. You typically want to ensure your partition key + order by columns are UNIQUE, if they are not - non-deterministic results will happen. You can add something like the primary key to the order by, or just ROWID to ensure uniqueness and hence deterministic behavior within a table...
did you actually test it? this will break the data up by NBR, so for each distinct NBR there will be a row_number with a value of 1. partition by NBR - break the data up by NBR order by DT - within a unique NBR value, sort the data from small to big. ROW_NUMBER() - assign a sequential number to each row in a unique number starting with 1 after sorting by DT SQL&gt; select * from t; NBR ID DT ---------- ---------- --------- 1 1 01-DEC-01 1 2 01-JAN-01 2 3 01-JAN-01 2 4 02-DEC-01 4 rows selected. SQL&gt; SQL&gt; select nbr, id, dt, row_number() over (partition by nbr order by dt ) rn 2 from t 3 / NBR ID DT RN ---------- ---------- --------- ---------- 1 2 01-JAN-01 1 1 1 01-DEC-01 2 2 3 01-JAN-01 1 2 4 02-DEC-01 2 4 rows selected. SQL&gt; select nbr, id, dt 2 from ( select nbr, id, dt, row_number() over (partition by nbr order by dt ) rn 3 from t ) 4 where rn = 1 5 / NBR ID DT ---------- ---------- --------- 1 2 01-JAN-01 2 3 01-JAN-01 2 rows selected. try it yourself and see....
To get the values based on the most recent date (assuming only on nbr on the same date), you could do this: SELECT nbr, MAX(ID) KEEP (DENSE_RANK FIRST ORDER BY dt ASC) as first_id, MAX(dt) KEEP (DENSE_RANK FIRST ORDER BY dt ASC) as first_dt FROM tablename GROUP BY nbr; *edit changed to ASC - missed "earliest" date at first
I would just add to a Date Dim table. Here's a good starting point: http://www.codeproject.com/Articles/647950/Create-and-Populate-Date-Dimension-for-Data-Wareho The idea would be to calculate all of these things in the future and then you can just join on the date field. 
I'm on my phone so can't easily change your code for you, but essentially the easiest way to fix this is to left join your table A to a subquery that has tables B and C inner joined. This should explain it: http://weblogs.sqlteam.com/jeffs/archive/2007/10/11/mixing-inner-outer-joins-sql.aspx
Thanks, I'll give it a try
Honestly, I don't know. I'm trying to learn it for free as a student. There are like dozens of different links and pages on Microsoft to download just the 2014 version. Here's the one I used: https://www.microsoft.com/en-us/download/details.aspx?id=42299 So I have express, I suppose? 
Do tables A and C have a joinable fields?
Alas, no they do not.
What is the character collation on the table or columns set to?
Yeah, those only come with Enterprise and Developer Edition. Luckily you can get Developer Edition for free: https://blogs.technet.microsoft.com/dataplatforminsider/2016/03/31/microsoft-sql-server-developer-edition-is-now-free/
 from dbo.TableA a left join( select b.ColumnA, b.ColumnB, b.CertainColumn from dbo.TableB b join dbo.TableC c on B.ColumnC = C.ColumnC AND C.ColumnD NOT IN ('A','B','C') )bplusc on bplusc.ColumnA = a.columnA and bplusc.ColumnB = b.columnB
Express edition only includes the database engine. You also have the Tools and Advanced Services (which includes Reporting Services) versions of Express Edition available. https://msdn.microsoft.com/en-us/library/cc645993(v=sql.120).aspx If you're wanting to learn, I suggest downloading the Evaluatioin Edition, which is a 180 day eval of Enterprise Edition. https://www.microsoft.com/en-us/evalcenter/evaluate-sql-server-2014 Lastly, I suggest checking out https://www.visualstudio.com/products/visual-studio-dev-essentials-vs which gives you free access to Visual Studio 2015 as well as SQL Server Developer Edition.
Hmm, I don't think I made any specific changes to the character collation, so likely whatever is default.
Eh, not that many versions: Express = free, fairly limited, can be used in production. Standard = run-of-the-mill, lacks advanced features. Web = Same as Standard, but without any BI features. Enterprise = super expensive, but highly scalable and lots of advanced functionality. And Developer is Enterprise Edition, it just can't be used in production. 
 FROM dbo.TableA as A (nolock) LEFT JOIN dbo.TableB as B (nolock) ON A.ColumnA = B.ColumnA AND A.ColumnB = B.ColumnB AND EXISTS (SELECT TOP 1 C.ColumnC FROM dbo.TableC as C WHERE B.ColumnC = C.ColumnC AND C.ColumnD NOT IN ('A','B','C')) 
The outer query runs first, correct?
It would run as part of the join conditions, if it works. Same logic as others have stated, left joining a to the subset of b and c joined together, or b where c exists.
Lol @ people reporting this. holy balls, just because someone says the word "trump" doesn't mean you need to get offended and try to get it removed. 
I'd skip the evaluation edition and go right for the 2014 dev edition since it's now free.
What exactly are you trying to do? I'm willing to bet you won't need those advanced services anyway.
Technically, TOP doesn't return the top rows from a table, just a result set. If you want to return the bottom most records, you would simply order by the PK desc and still use TOP/LIMIT.
In some databases, TOP fetches the first N results from the set, but not all. It's not in Oracle, for instance. LIMIT has better support. And as already stated, the bottom N records are nothing but the top N in reverse order. 
I_SALES as well as the columns contained in dPER are not contained in the group by or an aggregate function. [Here is an SO](http://stackoverflow.com/questions/1520608/ora-00979-not-a-group-by-expression)
No, no conditions.
 SELECT TOP 10 * FROM TableA ORDER BY tableKey DESC
This is the correct answer. Just flip the results upside down and then grab the top.
&gt;Technically, TOP doesn't return the top rows from a table, just a result set. To expand upon this: a table doesn't *really* have an ordering in the first place (we can argue about whether a clustered index makes the table truly sorted or not), and a result set doesn't have a guaranteed order **at all** without an `ORDER BY` clause in your query. &gt; you would simply order by the PK desc Only if the PK is the field(s) that you want to order by in the first place. If you want to order by a different field, you'd use that one.
I don't know if that's valid SQL, as I'm not familiar with MySQL - but generally in programming the pipe (|) character is used as an OR (in TSQL we just write out OR, but we're old fashioned that way). The IF logic is likely IF(statement, when true, when false). So, it'd be the same as WHERE name = 'Fra' OR 'nc' OR 'e'. I don't know why anyone'd throw the IF in there, as it would likely preclude the use of any indexes on the name column to solve the query. Maybe the query engine would optimize it out since 1 obviously equals 1, but if not that would make the predicate non-[sargable](https://en.wikipedia.org/wiki/Sargable). Again, I'm not too familiar with MySQL... so, just a guess.
Checking the obvious, are your compatibility levels the same? It could be someone bumped the Dev server up to a higher revision which allows syntax the prod would not.
That expression doesn't make a whole lot of sense. However, this one would: ('Fra'||if(1=1,'nc','777')||'e') The difference is the | versus ||. In ANSI SQL, the double-pipe (||) is the concatenation operator. So to break it down, we are building the string: 'Fra' Then concatenating it to the expression: if(1 = 1, 'nc', '777') -- 2nd arg is "then", 3rd arg is "else", since 1 = 1 evaluates as true, this expression returns the string 'nc' So now we have 'Fra' || 'nc' = 'Franc'. Once we evaluate the last concatenation, we see: ('Franc') || 'e' Therefore, we get the string 'France'. In an upside down world, we may have gotten 'Fra777e' instead of 'France' - but those are the only 2 possible outcomes of the expression.
If they're using it for concatenation, I frequently see `1=1` used as a quick way to "shut off" part of a `WHERE` clause by flipping one of the `1`s to `0`. For testing purposes only. For example: `WHERE country='France' or (1=1 and city='Paris')` So if you want everything in France or everything in a city named Paris, leave it at `1=1`. Flip to `1=0` to get *just* France. In the given example, they could "easily" test `France` vs. `Fra777`. Why would they need this? No idea. I have no clue why someone would do this in production code. If I saw it in a code review, I'd be upset.
It may be dependent on the 'sql_mode'. I.e., this will set double pipes to concat: set sql_mode=PIPES_AS_CONCAT; SELECT 'Fra'||if(1=1,'nc','777')||'e'; And output 'France'.. This, however - will just output 0: SELECT 'Fra'|if(1=1,'nc','777')|'e'; ...regardless of the PIPES_AS_CONCAT. And this: SELECT 1|2 AS T; ...will output 3 (also regardless of PIPES_AS_CONCAT). Maybe OP missed the double pipe when posting? Edit: Single pipe is a bitwise OR according to these [docs](http://dev.mysql.com/doc/refman/5.7/en/non-typed-operators.html), and double is OR dependent on mode modifications (and probably context as well).
I get that, to a very small degree.... At that point, why not just use a variable though...
Can this variable be in the 'string' that has been omitted? Because I don't see any issues with the code (other than rather strange idea of using EXCEPT to filter out top 9 rows in an undetermenistic query)
Yeah, I said PK because of the way the OP was worded regarding returning the top results from a table as that would (usually) be what determines the default order.
You can use the following logic to filter the column within the group: SUM(CASE WHEN Is_First_Ordercart = 'n' THEN 1 ELSE 0 END) as ReturningCustomers
Believe it or not, your original table is in 3rd NF and what you're doing is NOT normalization in the technical sense of the word. Splitting the way you've outlined does not change what the form is - it allows you to save some storage space and/or to set up referential integrity rule (i.e no extension other than defined in your extension table can be inserted into your file table). Also, extension table does not need "ID" - the extension itself could be the key and be used for the referential integrity. The point of using a surrogate key is to save space/introduce a simplified key verification/generation/referencing/stability to the world of business rules complexity and change by avoiding over-engineering (for example, a business key for an entity might be First, Last, DOB OR OrgName and Tax ID; Last names can change, etc.). This does not change the fact that synthetic IDs represent _new_, _additional_ pieces of data. Specifically, introducing extension_id into your table this way: file_id, file_name, extension_id, extension _This action_, although usually omitted from the design/thought process, creates a broken 3NF (since extension depends only on the extension_id) and now to bring the structure back to the 3NF you will need to split out that entity as a separate table.
being gibberish is the very sense of obfuscation so to thwart protection stuff against SQLi ;) yes single |
Very good explanation! :)
&gt;The data model can then evolve without affecting the code base as long as the SQL is modified accordingly to always return the same columns. &gt;So for this approach to work there has to be a way to externalize the SQL into 'configuration files' or what not Isn't this one of the main use cases for views and stored procedures? Present *those* to your application code, and if your schema changes you revise the SPs &amp; views correspondingly and your application code is none the wiser (assuming data *types* in the resultsets don't change). IOW, we already have a method for insulating the application from schema (data model) changes; [it's not clear to me what your proposal is bringing to the table](https://xkcd.com/927/)
If you are using 12c you can use the new identity feature https://oracle-base.com/articles/misc/autonumber-and-identity
Yeah I kind of forgot about 'views'. Didn't think this through. Back to the drawing board.
I came up with a solution. Got a little carried away by incorporating a pivot table but it runs quick and is ready for SSRS. Thanks for the help. Was cathartic just to type out the problem and then go through some feedback.
Shows how little I know. I guess I should learn SRS, been avoiding it for a while but keeps being the suggestion i get. Could you elaborate on the benefit of using sharepoint?
I use Sharepoint only because we already use it for other things. It just makes sense for us to integrate it. As someone else mentioned, it sounds like data-driven subscriptions **are** available in native mode, so I would go with that. SSRS is a nice tool. Not always the most user-friendly to work with, but very flexible and powerful. Plus, it's free with SQL Server.
To me, what you wrote is like "the sky is blue". The frontend datalayer sends a request,and gets back a response. That is an interface. If the fontend calls a stored procedure at least. Watever I do in the storage layer should be completly abstracted away from the frontend. I do not get the xml config you wrote about, since for me, this is about the separation between a backend datamodel, and the 1-many consumers of that datamodel. A good datamodel does not care how any application access it
Is there a reason why you can't directly call the sequence when inserting into SCHOOLS?
So, I can get the individual select statement to run when I am connected to the database SELECT to_char(stamp, 'MM/DD/YYYY') FROM actionerrorlog This yields the exact results I am looking for and when I run the psql command without the select statement psql -w -d mydatabase -h myhost -p myport -U myuser -c "\copy actionerrorlog to 'C:\Users\Administrator\pgdump\actionerrorlog.csv' WITH (NULL '', FORMAT CSV, DELIMITER ',', FORCE_QUOTE(tables here))" Works correctly, but when I put the two together it creates an issue that I cannot quite figure out 
Hello and welcome. You've arrived at ORM dilemma. Enjoy your stay :)
Good answers around here, so just a couple of extra points. In your example you use txt and doc as values of 'extension'. What are the possible valid values for extension? Is ttx allowed? How about doxc? By normalising you are defining the domain of allowable values. Now you are writing a report and you have a parameter for the extensions filter. What is the query to populate that list? Are you going to Select distinct extension from files Or Select extension from allowedextensions Now in your db you decide for each extension you want to group them up into categories, like JPG and GIF into 'image files' and DOC XLS into 'office files'. Where do you think the attribute for that goes? Don't think of joins as an overhead just because you have to write more code. If you find yourself repeating the same join paths then wrap them in a view.
I was taking from documentation COPY { table_name [ ( column_name [, ...] ) ] | ( query ) } TO { 'filename' | PROGRAM 'command' | STDOUT } [ [ WITH ] ( option [, ...] ) ] 
Tree structures don't mesh well with relational databases, and that's basically what you have here. Many DBMSes have non-standard facilities for dealing with this sort of thing. If you're okay with tying yourself to a particular DBMS, you might look into that. Otherwise, maybe have two columns, parent_post and parent_comment, NULL when not applicable. Alternatively, have posts and comments in the same table, with some sort of type indicator for whether a given row represents a post or comment.
the answer is that you will need the FK to link to a supertype table, of which the comments and posts are subtypes alternatively, comments and posts can share the same table, so then the FK would simply refer to its own single PK
Each comment should have a self referencing parent id. To format the records as a tree you should use recursion.
One way to do this in T-SQL would be to use ROW_NUMBER() to partition by the values you want, sort by [Number] and then select the first row from each partition to get the first record, complete with the ID. I'm typing this up on my tablet, so this is as best I can get from memory, but something like the following would work: WITH [ResultSet] AS ( SELECT b.[Id] , c.[Name] , b.[Cost] , b.[Number] , ROW_NUMBER() OVER (PARTITION BY c.[Name], b.[Cost], b.[Number] ORDER BY b.[Number]) AS [RowNumber] FROM [TicketCategories] c JOIN [BatchCategories] bc on bc.[TicketCategoryId] = c.[Id] JOIN [Batches] b on bc.[BatchId] = b.[Id] WHERE b.[CurrentQuantity] &gt; 0 ) SELECT [Id] , [Name] , [Cost] , [Number] FROM [ResultSet] WHERE [RowNumber] = 1 Hopefully, that should work off the bat if my memory hasn't failed me (and I've understood your problem correctly). Edit: removed ID from the PARTITION clause because it's silly. 
It would probably help of you explained what you wanted as output or describe the problem you are attempting to solve. Including the ID field when you're trying to find any aggregate is going to get you nowhere. If you want all IDs that have the minimum value, then you should likely use a subquery to determine the minimum value, then filter your result set by that. Edit: Posting your schema wouldn't hurt either.
You're correct - that introduces the same problem as OP's query. Removing the ID from the partition would have the result I was intending...
looks nice. how is this compared to undergraduate courses at MIT?
Comments will always belong to a post. You may as well have the PostId as an FK. Then have a parent_CommentId so you can form the comment tree. You can query them either level by level or by recursion. For instance you may wish to grab and display all top level comments for the first view that will be those with a null parent. When the user expands a top level comment you can either get the next level, or just recurse the whole subtree. When you do recursive sql you usually get a Level pseduo column which helps to group and indent your data.
This seems like it would be against the licensing terms.. so.. why not just buy it..?
Yeah. And dev licenses are pretty cheap.
x, y, z etc. are values from a column within the table I'm querying. Apologies I should have been more clear about that. 
Right but how do you know *which* values you're looking for? Sometimes loading those into a temp table and `JOIN`ing to it or using `EXISTS` with a sub-select can be faster than a ridiculously long `IN`
Besides what's on the wiki already, I really enjoyed Mode Analytics SQl tutorials. I personally got the most from that site than any other. I have the book SQL queries for mere mortals and it's also quite thorough without being specific on any one SQL platform. I find it to be more thorough than most sites and there are multiple examples. 
https://www.hackerrank.com/domains/sql/select 
http://bafflednerd.com/learn-sql-online/
CodeSchool.com offers a couple free intro classes. 
That would largely depend on what you want to learn. Reporting, analysis, dba ? Learning python and flask, has very little to do with sql. If you want to learn sql, learn sql. Not 2 other technologies that uses some sql.
Any resources that break it down for you?
Yes indeed. The list of values I have a very specific. Arguably the most important values I'm after are the columns I'm returning.
Check the Wiki (as mentioned on our sidebar :) for some resources ... https://www.reddit.com/r/SQL/wiki/index
Oh, I didn't realized that's what you were doing. I thought you were just trying to learn how to avoid the problem in the first place.
I had to learn on the job, so I already had problems to work on. W3schools helped me a lot though.
So what's the issue with this? Why are you expecting multiple rows?
Technically, yes. But... have you analysed the consequences? Is the site readonly?? (hopefully) are inserts or updates in any way dependent on the url or domain name?? Are you certain nothing will break? It could be done, I suppose, but thorough testing is quite advisable. 
Check if this works: SELECT Item , (SELECT TOP 1 Quantity FROM Table as B WHERE Item = A.Item ORDER BY Quantity DESC) as Quantity FROM Table as A GROUP BY Item
Didn't even realize. Thanks for the heads up
Thanks for all the replies, each one of them was useful in a different way. They've allowed me to achieve what I was trying to do and learn some new things, so really appreciated.
fyi, your query is a functional equivalent of select item, max( quantity) from table group by item
If there are other columns in the select, you need to add in the group by, even if in theory it wouldn't change the result, for example: SELECT Item, ItemDescription , (SELECT TOP 1 Quantity FROM Table as B WHERE Item = A.Item ORDER BY Quantity DESC) as Quantity FROM Table as A GROUP BY Item, ItemDescription
If you really want to use TOP (which in some cases is indeed needed, not this one though), you need to use window functions to get the row number within defined window (item), two options here: DECLARE @t TABLE (Item varchar(1), Quantity int) insert into @t values ('A',5),('C',2),('A',3),('C',4) SELECT TOP 1 WITH TIES Item, Quantity FROM @t ORDER BY ROW_NUMBER() OVER (PARTITION BY Item ORDER BY Quantity DESC) SELECT Item, Quantity FROM ( SELECT Item , Quantity , ROW_NUMBER() OVER (PARTITION BY Item ORDER BY Quantity DESC) as RN FROM @t ) t WHERE RN = 1
Thanks. Would love a code except and sorry for not tagging the platform. I am using MS SQL 2008
Do you have a test environment to attempt this in first?
I'll apply this to your query if I get chance, but this is the function you're looking for: https://msdn.microsoft.com/en-us/library/ms186734.aspx
I believe, the issue that you're trying to use variables in the dataset with multiple rows here: SELECT @CID=[CID], @DC_Code = SUBSTRING([Comments],13,LEN([comments])-13) FROM [Development].[dbo].[Issues] WHERE [IssueType] = 'Main' AND [Region] = @Region If that's the case, try to move all the consequent logic into that select statement. As for the logic, try implementing some split function splitting string into columns instead of that monstrous LEFT(RIGHT(... [example](http://social.technet.microsoft.com/wiki/contents/articles/26937.t-sql-splitting-a-string-into-multiple-columns.aspx) 
I would assume that you need to a) implement artificial unique column (ID) using row numbers to distinct rows from each other and then b) join the table to itself on T1.ID &lt;&gt; T2.ID AND T1.B=T2.B AND T1.C = value AND T2.D = value
I primarily need to make sure that I just get 1 from each quantity-I will be running an update on all quantities to make them 0, run another update to make just the first x quantity, and then delete the remaining 0's
&gt; You only select one row I didn't know I was doing that. If I put the SELECT...FROM...WHERE into SSMS (without the @Variables) I get a long list of about 300 records. I thought I had built a stored procedure that would return those 300 records along with the extra columns that the logic in the middle created. Can I salvage this or have I gone down the wrong path at square one?
&gt; Why are you expecting multiple rows? I built the Stored Proc around the SELECT...FROM...WHERE statement in the middle. When I started it returned multiple rows. I thought I had just written in extra columns. My apologies, this is the first time doing a stored procedure as something to return data. The majority of others were simple action queries. Edit: a word.
I'm not entirely sure what you're asking for here. Maybe you could provide some sample rows of data of the current setup and then of how you want the result to look?
I tried removing just the collation (setting it as ""), which actually ran for me. However, there were still weird characters. Then I tried switch to SQLNVARCHAR/SQLNCHAR and when I run it there are Chinese characters. I'll give you gold if you can help me get this solved :)
Open your file in a hex editor and you should see the first few bytes are odd looking chars. This is the BOM and describes your file encoding. What do you have there?
okay I think I understand what you're going for here. There are multiple ways to accomplish what I think you're trying to accomplish. Here's an easy, but manual way to go... /*Setup to match your scenario*/ CREATE TABLE #Parts (Date DATE,OrderID INT,Part1 INT,Part2 INT) INSERT INTO #Parts (Date,OrderID,Part1,Part2) VALUES ('2016-06-30',123,56,NULL), ('2016-06-30',124,56,57), ('2016-07-01',125,58,NULL) /* Potential solution for your scenario ....Here's the query I think you'll use */ SELECT Date,OrderID,Part1 AS PartsID, 1 AS PartsNumber FROM #Parts WHERE Part1 IS NOT NULL UNION ALL SELECT Date,OrderID,Part2 AS PartsID, 2 AS PartsNumber FROM #Parts WHERE Part2 IS NOT NULL UNION ALL ... SELECT Date,OrderID,Part49 AS PartsID, 49 AS PartsNumber FROM #Parts WHERE Part49 IS NOT NULL UNION ALL SELECT Date,OrderID,Part50 AS PartsID, 50 AS PartsNumber FROM #Parts WHERE Part50 IS NOT NULL You can also achieve the same results by using some dynamic SQL using some system tables, but the code above should work just fine, it's just not elegant. And, using your example above, I'm not sure where the second table comes into play. I assume that you're just not giving us the full picture.
&gt; SELECT Date,OrderID,Part1 AS PartsID, 1 AS PartsNumber FROM #Parts WHERE Part1 IS NOT NULL UNION ALL &gt; &gt; SELECT Date,OrderID,Part2 AS PartsID, 2 AS PartsNumber FROM #Parts WHERE Part2 IS NOT NULL UNION ALL Thanks, man I'll mess around with this and see what I can manage. I haven't really delved into dynamic SQL yet. This really just needs to work for a one time situation and then future insertions will be done according to the new schema so elegance isn't too much of a concern at the moment.
Hopefully that code is almost a copy/paste of what you need. I've had to do something like this recently for an audit trail table. Feel free to PM me if you have any questions. Good luck!
&gt; SELECT Date,OrderID,Part1 AS PartsID, 1 AS PartsNumber FROM #Parts WHERE Part1 IS NOT NULL UNION the second table comes into play because, for example the value of Part1 on the first row of the OrderID table is the same value as the PartsID in the second table. So piggybacking off your idea I'm thinking something like... SELECT t1.Date, t1.OrderID, t1.Part1 AS PartsID, 1 AS PartsNumber FROM table1 as t1, table2 as t2 WHERE t1.Part1 = t2.PartsID UNION ALL ... through 50 might be what I need. I'm not sure if that'll cause some duplication though.
This is giving the same # of rows as a select just on item, itemdescription, quantity and those items with repeat quantities are showing both. I've also tried using a max(quantity) with the same result. 
I guess I don't understand why you need the join. In your example, you're not pulling in any of the table2 attributes into table3. 
Can you post the query you are running? Use the MAX() one, it has a better performance, my variation is for when the value isn't a number, but both will give the same result.
Is this some kind of de-duplication exercise? The whole 'make them 0 then update then delete' seems unnecessary. Regardless, you hopefully have some kind of unique 'ID' to identify or at least sort the original records per Item group. If the number of duplicates is small, delete from the original table joining/using a correlated subquery with a condition like myUniqueEnoughID &gt; "select item, min( myUniqueEnoughID) from mytable m2 where m2.item = m1.item group by item). If the number of duplicates is a significant percentage of the table, save "select item, min( quantity)" to a temporary table, truncate the original and re-insert the saved results. 
Also, to find the parts not associated with an order, just change the WHERE EXISTS code I sent you to WHERE **NOT** EXISTS. Easy peasy SELECT Date ,OrderID ,Part1 AS PartsID ,1 AS PartsNumber FROM Table1 t1 WHERE Part1 IS NOT NULL AND NOT EXISTS ( SELECT 1 FROM Table2 t2 WHERE t1.Part1 = t2.PartID ) UNION ALL
Nothing beats videos. That being said, you should learn vanilla SQL first and foremost. Go to Lynda.com and take the course on SQL Essential Training. After that, download SQL Server Express (if T-SQL/SQL Server is whats most used in your area and industry) then look up the Querying SQL Server in course on Lynda.com. Also checkout Wise Owl (I think that's the name) over on YouTube.
Used max and it worked correctly after I removed some unnecessary columns from the select. The next step I am looking for is to be able to do an update using the same requirements- any idea if it is possible to use max as part of the where clause in an update? What I have now follows: SELECT li.[LOCATION] ,max([LOT]) ,[ITEM] FROM table as li inner join table2 as loc on li.[LOCATION]=loc.[LOCATION] where [WORK_ZONE] like '%-p-%' group by li.[LOCATION] , [ITEM] order by li.[LOCATION] GO 
Sorry, should have been more clear. It needs to be UPDATE li SET field =some quantity, this is the easy part FROM table as li inner join table2 as loc on li.[LOCATION]=loc.[LOCATION] WHERE [WORK_ZONE] like '%-p-%' and lot=max([LOT]) &lt;---hard part that I'm working on GROUP BY li.[LOCATION], [ITEM] When attempting to put in the max([lot]) into a select, I am getting "An aggregate may not appear in the WHERE clause unless it is in a subquery contained in a HAVING clause or a select list, and the column being aggregated is an outer reference." Edit: Was able to get it to work with a having, but it created some other issues. I think those are fixable though. Thanks everyone
Yeah, the "where" for group by and aggregation is the HAVING clause. Good luck with your code :)
Totally. Any problem that is reducible to a finite number of precisely defined and simple steps becomes "easy", at least in theory. 
Hello! Thanks for your response! I'm developing a 'Event management system', where the user can buy tickets for events. In this query I was looking for returning all available ticket batches for the user. As soon as one batch ends (when the CurrentQuantity column reaches 0) I need to automatically show the next batch. I want the query to result in something very similar to this page: http://www.seetickets.com/event/epica-powerwolf/o2-shepherds-bush-empire/984648/ However, in the system I'm developing we can have multiple categories (labels) attached to the same batch. For example, you could have 'LEVEL 1' and 'LEVEL 2' for the same batch.
Yes, the Id column is a identity.
Hello! Somehow this query is returning all possible records.
Change T1.ID &lt;&gt; T2.ID to T1.ID &gt; T2.ID
Are you trying to create a "first to reserve" or "first to complete transaction" type system? Actually, in either scenario I would not let any user actions directly change the available tickets. Instead, I would put the whole pool of pending transactions into their own table then order them oldest to newest. Then run a batch assigning them to the "reservation verified" table. If there aren't enough tickets remaining to fill their order ,return an error.
This is the correct solution. Also. please don't delete records from the table like you said you're planning on doing. From everything you've told me about this table it sounds like it was designed to have a 1 to many relationship. This seem like the table you're working from is an archive of all inventory/inventory changes. Especially since it is relational to another table that has an expiration date column. My guess is that somebody has created a view that shows what the live inventory is. I would check with the DBA first before trying to do anything.
1. Tickets are mostly unique. (e.g movie ticket/airline ticket) 2. Reservation process need to be done within short period such as 10 minutes (adjust this as you want) probably 30 minutes for air ticket. 3. Your ticket data need a status (Available, Reserving, Sold) with datetime stamp. 4. When the reservation process begin, only show available items + any "Reserving" items which are 10 minutes older than now (aka expired reserving). Once user choose particular ticket, lock it down by updating the table with "Reserving". This is first come first serve, so use conditional update/merge statement. Only this method will grantee user to get the seat/ticket they choose, I don't see any other way. 5. If reservation process is done/finished (paid) then update the reserved item with "sold". 6. If a reservation process took longer than 10 minutes, your client side need to let user know their reservation is going to be expired and they need to re-book it. Because those "reserving" tickets older than 10 minutes are expired and open for subsequence queries. 7. One IP/User should not be booking (reserving) a lot of items at the same time (prevent abuse). 
 SELECT posts.id , posts.title FROM job_saved INNER JOIN posts ON posts.id = job_saved.post_id WHERE job_saved.user_id = 42 
What do you have so far?
not to be that guy, but did you try googling "sql dashboard tools"?
I don't see anything wrong with your code, tbh? Why doesn't it seem to work? Maybe try something like this? SELECT * FROM POSTS P INNER JOIN SAVED_JOBS SJ ON P.P_ID = SJ.SAVED_JOB INNER JOIN USERS U ON SJ.EMAIL = U.EMAIL WHERE U.EMAIL LIKE 'ADMIN%'
i'v just done it :) it was select * from posts inner join saved_jobs on posts.P_ID=saved_jobs.saved_job where saved_jobs.email = 'admin'
You just turned you query around, am I right? If so, then it was all about '*' :)
Just add the toolqueue as a column in the select. You are already grouping by that field, so you can simply add it in, such as: &gt; SELECT "public".t_userthistory.toolqueue, count( "public".t_thistory.toolqueue) FROM "public".t_thistory where "public".t_userthistory.tstamp BETWEEN '2016-05-01' AND '2016-05-02' and t_userthistory.toolqueue in (0,4,5,6,9,21,24,25,27,28,37,41,42,48,49,55,56,75) GROUP BY "public".t_userthistory.toolqueue
I did. I am not knowleageable enought to tell if this query is DMBS-dependent, if its output could vary on different systems. Also, asking the question on a community such as this provides me not only with the answer to my question, but also with input from different points of view; tells me if I should be using this query regardless of its redundancies (or not), if it's good practice, etc.
it doesn't make a difference, you must've had a syntax error
there is a difference yes. Depending on the rbms you use. here is an example for mssql thats shows you a real difference, and a nasty one at that. The more metadata you give a cost based optimizer to work with, the better the execution plan will look like. CREATE TABLE #temp ( col1 INT NULL , col2 INT NOT NULL , col3 INT NULL CONSTRAINT chk CHECK (isnull(col3,0) &gt;-1) ) INSERT INTO #temp SELECT TOP 100000 i = o3.object_id ^ o2.object_id % 10 , i2 = o3.object_id ^ o2.object_id % 10 , i3 = o3.object_id ^ o2.object_id % 10 FROM sys.objects o CROSS JOIN sys.objects o2 CROSS JOIN sys.objects o3 CREATE NONCLUSTERED INDEX idx1 ON #temp(col1) CREATE NONCLUSTERED INDEX idx2 ON #temp(col2) CREATE NONCLUSTERED INDEX idx3 ON #temp(col3) SET STATISTICS PROFILE ON SELECT COUNT(1) FROM #temp WHERE ISNULL(col1,0)= 10 SELECT COUNT(1) FROM #temp WHERE ISNULL(col2,0)= 10 SELECT COUNT(1) FROM #temp WHERE ISNULL(col3,0)= 10 Query 1, nullable, no constraint SELECT COUNT(1) FROM #temp WHERE ISNULL(col1,0)= 10 |--Compute Scalar(DEFINE:([Expr1003]=CONVERT_IMPLICIT(int,[Expr1005],0))) |--Stream Aggregate(DEFINE:([Expr1005]=Count(*))) |--Index Scan(OBJECT:([tempdb].[dbo].[#temp]), WHERE:(isnull([tempdb].[dbo].[#temp].[col1],(0))=(10))) Query 2, NOT NULL, no constraint SELECT COUNT(1) FROM #temp WHERE ISNULL(col2,0)= 10 |--Compute Scalar(DEFINE:([Expr1003]=CONVERT_IMPLICIT(int,[Expr1004],0))) |--Stream Aggregate(DEFINE:([Expr1004]=Count(*))) |--Index Seek(OBJECT:([tempdb].[dbo].[#temp]), SEEK:([tempdb].[dbo].[#temp].[col2]=(10)) ORDERED FORWARD) Query 3, nullable, WITH constraint SELECT COUNT(1) FROM #temp WHERE ISNULL(col3,0)= 10 |--Compute Scalar(DEFINE:([Expr1003]=CONVERT_IMPLICIT(int,[Expr1005],0))) |--Stream Aggregate(DEFINE:([Expr1005]=Count(*))) |--Index Scan(OBJECT:([tempdb].[dbo].[#temp]), WHERE:(isnull([tempdb].[dbo].[#temp].[col3],(0))=(10))) In the second query, the isnull() function is optimized out, since the column is defined to not allow null values. In the 3rd query, altough a check constraint could prevent null values also, the column is still nullable, hence the function is applied and we end up with an index scan, instead of a seek. 
In Oracle it is not redundant: &gt; A check constraint lets you specify a condition that each row in the table must satisfy. To satisfy the constraint, each row in the table must make the condition either TRUE or unknown (due to a null). 
Maybe something like this? I start with 1/1 of the Year, add Week weeks, and add Day days. If you had the dates that some of these rows should equal, you could probably massage this to return what you want. declare @Dates TABLE (iYear INT, iWeek INT, iDayOfWeek INT) insert into @Dates (iYear, iWeek, iDayOfWeek) Values (2016, 17, 1) SELECT DATEADD(dw, iDayOfWeek, DATEADD(ww, iWeek, CAST('1/1/' + LTRIM(RTRIM(CAST(iYear AS VARCHAR(4)))) AS DATE))), * FROM @Dates
That.... makes me glad I live in the US.
This. With a small correction: you need to subtract days from resulting date by a number of the weekday on 2016-01-01. As well as we don't need to add first week because it is already first: declare @Dates TABLE (iYear INT, iWeek INT, iDayOfWeek INT) insert into @Dates (iYear, iWeek, iDayOfWeek) Values (2016, 5, 7) SELECT DATEADD(day,-DATEPART(dw,DATEFROMPARTS(iYear,1,1)),DATEADD(dw, iDayOfWeek, DATEADD(ww, iWeek-1, DATEFROMPARTS(iYear,1,1)))) , * FROM @Dates 
The only thing this script can be used for, is to re-map users and logins after restoring some of the databases on a different server, if users have different sids. Some years ago I used this script to fix missing mappings between users. --USE [DB_NAME] declare @name sysname declare cur cursor for SELECT su.name from master..syslogins sl inner join ( select * from sysusers where islogin = 1 and issqluser = 1 and hasdbaccess = 1 ) su on sl.name = su.name where sl.isntname = 0 and sl.denylogin = 0 and sl.hasaccess = 1 and sl.sid &lt;&gt; su.sid open cur fetch next from cur into @name while @@fetch_status = 0 begin exec ('sp_change_users_login ''Update_One'',''' + @name + ''',''' + @name + '''') fetch next from cur into @name end close cur deallocate cur And no. There's no need to run it twice.
ALTER procedure [dbo].[usp_rpt_NewHireMailingList] (@startdate date,@enddate date, @companyid int) AS SELECT employeeID, ei.firstName, ei.lastName, benefitClassName, ei.addressLine1, ei.addressLine2, ei.city , ei.stateCode, ei.zipCode, CONVERT(VARCHAR(10),hireDate,101) [Hire Date], CASE WHEN CONVERT(VARCHAR(10),reHireDate,101) = '01/01/1900' THEN '' ELSE CONVERT(VARCHAR(10),reHireDate,101) END [Rehire Date], CONVERT(VARCHAR(10),EI2.benefitEffectiveDate,101) [Effective Date] FROM Employee_DemographicInfo AS DI INNER JOIN view_EmployeeInformation AS EI ON DI.userID = EI.userID INNER JOIN Employee_EnrollmentInfo AS EI2 ON EI.userID = EI2.userID WHERE DI.insertDate &gt;= @startdate AND DI.insertDate &lt;= @enddate AND DI.companyid = @companyid
Thanks! I knew I was off a little bit but I couldn't think of a good way to correct it.
If there are have one-to-many relations between jo2 and jo1, by joining the first table, you would get multiple rows for each entry in jo2. The following query would load only those rows that are presented in jo1 without actually joining data from jo1. select * from _JO2 where _jo2.[item no number] IN ( SELECT [item no] FROM _jo1 )
Hello! First, I don't care too much about which ticket the user will get, only that the ticket needs to be from the selected batch (ticket value is the same in the whole batch). Ok, so in this case I wouldn't have a column (like CurrentQuantity) that could potentially be updated by multiple users? Considering that the user can buy multiple tickets from multiple batches how I can handle possible concurrency problems? SELECT b.Id, SUM(ob.Quantity) as CurrentQuantity, b.Quantity FROM OrderBatches ob JOIN Batches b ON ob.BatchId = b.Id WHERE b.Id = 1 GROUP BY b.Id, b.Quantity Based on the query below I can know how much tickets are left but it is possible that after the INSERT (inserting the reservation) the tickets already left because someone else bought it. Do you know how I can solve this issue? Yesterday, in face of this problem, I was thinking of using an optimistic lock in the CurrentQuantity column but I t hink that the user would be affected a lot because I would do multiple updates (in every batch that the user selected it) to update the CurrentQuantity and if someone else change it anything the page would refresh and the user would be notified that something changed.
 select * from the_first_table a join the_other_table b on a.site_number = b.site_number and (a.lat &lt;&gt; b.lat or a.long &lt;&gt; b.long)
perfect!
i would either add 'distinct' to the subquery (if the number of distinct [item no] in _jo1 is close to the 43k) or switch to the 'exists' if the number of _jo1 [item no] values is higher.
right now there are only two records for each store but as we start tracking records I can see wanted to keep historical data and the query only pulls the MAX for each store. Thanks for the reply, it will work for now but I don't think it will for long term.
Even better than using the wizard, use a SQL script to generate the date table for you. More reasons to have a date dimension (table): https://www.brentozar.com/archive/2014/12/simply-must-date-table-video/ And a script to create one (linked from the comments): http://www.made2mentor.com/2011/06/creating-a-date-tabledimension-for-sql-server-2008/
Looks like its just generating a bunch of random data. ^ is bitwise exclusive or, and % is modulus. 