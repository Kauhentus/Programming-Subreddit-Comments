use vsql and a .bat file to create a loop. &amp;#x200B;
SQL Server's full text search can parse PDF files with Adobe iFilter.... but it's basically like searching a text blob, as far as I'm aware. Personally, I've written PowerShell scripts to fetch/store PDFs in an SQL Server database and parsed them with iTextSharp 5's `iTextSharp.text.pdf.parser.PdfTextExtractor.GetTextFromPage()`, but it was nowhere near as complex as the parsing you're talking about. 
why don't you try it an let us know?
My work desktop requires admin access, and I usually need to request a ticket to IT to get it installed.
&gt; Nope. Just Pdf. Thanks Alabama... [These are the documents I suppose](https://labor.alabama.gov/wc/FeeSchedules.aspx)? Yeah, it's time to get your management to pressure ADoL to supply the mandatory data in a useful (i.e., computer readable) format. Even Excel would be a more usable format than a PDF. They should be contacting your congressmen or the governor's office for wasting your time and money as well as the time and money of everybody else in the state. This stuff looks mandated by law, so everybody's got to be dealing with it. 
that's your first problem to solve :) ask your boss to ask IT to change this
lag( tran_date) over (partition by cust_id order by tran_date desc)
That's pretty cool. How does it protect you from applying the wrong migration or to the wrong database? And does it have undo? Thanks! It practically wrote itself :)
http://sqlzoo.net/
In Linux, there are tools like `pdftotext` (http://poppler.freedesktop.org) that will convert your pdf into a text file you can then process. If it's made of images rather than embedded text, you could use `pdftoppm` or `convert` to convert it into a series of images and then do OCR on it. I'm sure similar tools are available on Windows, or through Cygwin.
Not this bullshit again...
What are your thoughts on using Python embedded in SQL to do that job ? I think SQL Server 2019 has that feature. I wasnt able to test it at work as we're still using 2017 and we also don't have that package configured / enabled (sorry if terminology is wrong I'm on the user side).
SQL is the language you use to insert your clean data You'll need another language (I'd write a C# tool) to parse the pdfs and use regular expressions to generate clean data Paste your data from the post in regex101 and see where you get
You don't write migrations. Imagine on your database, the live one, you have a table like: CREATE TABLE tbl ( i int ); Then in your visual studio solution you have that script and you change it to: CREATE TABLE tbl ( i int, j int ); Now you build a package, the result is a dacpac file. You then deploy it using a tool called sqlpackage, give it the dacpac file and the target database. It will analyze both and come up with the following script that needs to be applied: ALTER TABLE tbl ADD j int; It works really, really well (cough, calculated fields on tables do not work well, cough). Sometimes a table rebuild is necessary, for example when you add a column in the middle of a table, it will drop the existing table's constraints, create a new table under a different name, insert all rows from the old table into the new one, then drop the old table and rename the new one. If you want to undo, just take the previous version of the dacpac and apply that. You can extract a dacpac of the existing database too. 
Again? I'd love a link to the previous one.
&gt; If you want to undo, just take the previous version of the dacpac and apply that But don't you lose some of the data?
As a SQL Server developer/DBA, I'll give you kudos for trying. Microsoft tried the whole transpiling game one time, called it Entity Framework. And it's performance often sucks, because it writes bad SQL. While C#/Java/C/C++/etc are imperative languages that control the state of the program and say do task A like this, then task B like such, SQL is a declarative language. For instance, I write the code SELECT ColumnA FROM Table1 WHERE ColumnB = @Variable And the query engine takes the above query, looks at indexing, statistics, workloads, throws in some optimizer magic, and decides how exactly it is going to get that data out. &amp;#x200B; So now put another layer on top of that declarative SQL, and see what you get. It may work fine for simple selects of a few thousand rows/very simple joins, but the second you get a query requiring 5 inner joins, 3 outer joins, a Cross apply referencing two of the 5 inner joined tables, and a few windowing functions, and...I'd like to see how it would be more readable, and how the sql it transpiles into is clearer than, regular sql. 
That's not going to happen. We tried many times.
It seems like a waste of SQL Server resources to me. Python isn't really "embedded" in SQL Server the way you're thinking (BTW, Python was introduced w/ 2017 so you're clear there), SQL Server more or less "shells out" to run your R or Python script and the output is returned to SQL Server in a way that it's able to use it. Why can't you just write a Python script that's run directly from the host OS and then inserts the data into a database via one of the Python libraries for SQL Server?
&gt; There are other files in that domain website that I want in my database. &gt; I saw there was a 200ish page file in there. All the more reason to do this outside SQL Server.
1) hat is your CreatedDT right now, because you don't really need to cast is as date if it's already a date and if it's a string you should shoot whoever made the table. 2) Write dates properly. 3) You don't need to check the dates twice. WHERE o.CreatedDT as date BETWEEN '2018-10-29' AND '2018-11-04' AND (p.Priority &gt; 2 or p.AdID is not null) should do the trick.
Do you know why you're using `with (nolock)`and understand the risks of doing so? Unless you can answer "yes" and can explain why, take those out. It won't solve your logic problem but you should not be using that unless you know, understand and accept the risks that it presents.
You don't need a recursive CTE, just a regular CTE (the very first portion of the article you linked). All this does is essentially create a temporary view that only persists for the duration of your query. You can then reference this instead of pasting the entire query each time.
Thanks for the tip about Entity Framework, but it seems pretty different from what I'm aiming at. And also a bit of a mess :) I don't think it will be a walk in the park, but I believe it's doable. I'm aiming at zero-cost abstractions, such as functions, variables, implicit joins, etc. I'll also do my best to produce readable SQL, and so far my proof-of-concept produces queries very similar to those I would write. But I do appreciate the words of warning.
Well, how else would it work? By default, sqlpackage will abort when it detects a data loss, ie. dropping a column off a table. You can override that of course. But when the change results in altering a view, or a procedure, or whatever, it's all fine.
It seems like I might need a few CTEs in the same query, each referencing the former with an INNER JOIN
What do other rows of data look like? Do they all follow the same format?
no. mongo is not sql. 
Thanks. We can. We were just exploring options. 
Point it to this https://aws.amazon.com/textract/features/ and then store the result in your SQL or dynamoDB database 
Take NULL out of your column declarations and make sure the number of columns vs columns in select are equal. Also, what's the error message?
that semicolon is wrong
it's actually possible in MySQL with MyISAM tables, **but it's not a good idea** there is absolutely no reason to re-number the way you want but if you insist, please, knock yourself out -- https://dev.mysql.com/doc/refman/8.0/en/example-auto-increment.html
Nope. Mongo is a JSON filestore/database. But it is not a SQL Server.
NoSQL is a catch-all phrase for databases that are non-relational and don't use SQL as an interface. They are not a "new generation" or replacement for relational databases, but bring individual strengths that can make them better in specific situations.
its webscale tho
The more elegant solution is to break this up into multiple tables
The only issue with CTEs is that you can only use them in one statement within the batch. I'd guess temp tables are what you're looking for. 
Bold move, bringing up NoSQL here.
Why are you using a GROUP BY? I don't see where you are aggregating anything.
When in doubt, TO_CHAR()/TO_DATE(). 
I like using WITH clause. easy to read. It's sorta reminds me object oriented programming
The same as your WHERE clause. WHERE (c.isdesign = 0 OR c.isdesign IS NULL)
Why not set the column to not null in the table and set a default value of 0? If these truly mean the same thing, there is no reason to not have values for some records but not others. 
Or WHERE Coalesce(c.IsDesign, 0) but that doesn't score as high on the readability scale. 
Not to my knowledge. It's easier to just write a procedure or update that achieves what you need. 
Also... "SQL Server" refers to a Microsoft database product. If you're talking about all relational databases, use "relational database" or "SQL database".
Turing complete too, I hear.
&gt; NoSQL stands for Not Only First I've heard of that. Seems some people are actually using it that way. To most of us, and the original definition is "non sql"... https://en.wikipedia.org/wiki/NoSQL
Why not use isnull? Isnull(c.design, 0)
What DMBS?
Sorry: Oracle DB 18c v 18.3
Can I see your trigger logic?
What you're trying to do now is some correlated sub-query. Avoid those if possible. Try this. SELECT department, FROM I'm still typing hang on. 
I have to use correlated subqueries because that is the section that my course is on. I have not covered OVER and JOIN yet. 
I'm pretty new to SQL - can you elaborate?
Well that blows. 
CREATE OR REPLACE TRIGGER Payroll_trigger BEFORE INSERT ON Payroll FOR EACH ROW BEGIN :new.pay_rate := (SELECT pay_rate FROM Employees WHERE id = :new.emp_id); END; /
I got it to work. I was defeated by a lack of a comma. 
Nice. 
Not really sure what's wrong with it, but have you tried SELECTing INTO a variable and then setting the new field to that variable? Not sure if you'll get any different results, but might be worth a shot. 
You can do "Edit top 200" records, then click on the button "show SQL pane" and apply filters to it to get the data set within range of what you want to edit then click on the "execute SQL" button or Control R. You can even change top 200 to whatever amount you want. Not the best route but it works. 
I’m a noob with sql what do you mean by selecting into a variable ? I tried declaring a variable and setting it to the :new.emp_id but the results were the same. Trigger gets created but shows compilation error. Thanks for brain storming with me I’m super stumped 
I'm not great at triggers either, but I've never seen someone set a variable in Oracle like you are trying. It makes me think that might be throwing it off. 
&gt; click on the button "show S this is very close to what I was looking for, thanks! is there any way to bulk update multiple fields in this view?
[removed]
I think they had to back peddle when the hype died down
Which one? 
 \* Column Not Allowed Here on OrderStatus. The \* is popping up under it which I guess is the reason for it. Both match in terms of numbers. The Null is what I have matching the Orders.ReasonNum (Reason for cancelling an order).
There’s a chance I don’t understand correctly but there are 2 things I’d try- First, and easiest option. Select x,y,z from Table Left join tabledetails on table.eventid=tabledetails.eventid and aotype= Left join tabledetails2 on table.eventid=tabledetails2.eventid and aotype=2 And same for 2 more joins. This way you pull the details into the same query without sub queries in the select statement. If this is possible it will improve performance for sure The other option is to join to the details table once and then pivot the resulting data onto the value in the aotype column. This will be a bit complex to do the first time but learning the pivot function is good practice. I don’t know if this is really any better than option above though And sorry I’m on mobile so I can’t see what I’m replying to and had to remember your question as I typed. Hope I didn’t fuck up..
So, I've done that, and come up with this: \`SELECT TOP 5 at.EventTime, STUFF(';' + at.UserName+' ('+at.UserId+')',1,1,'') As UserDetails, at.NetworkData, at.EventId, ri.AoValue as RecId, rn.AoValue as RecName, mo.AoValue as RecMod, rd.AoValue as RecDesc FROM AuditObjects LEFT JOIN AuditObjectDetails ri ON at.EventId = ri.EventId LEFT JOIN AuditObjectDetails an ON at.EventId = rn.EventId LEFT JOIN AuditObjectDetails mo ON at.EventId = mo.EventId LEFT JOIN AuditObjectDetails rd ON at.EventId = rd.EventId WHERE rn.AoType = 'record-number' AND mo.AoType = 'mod' AND rd.AoType = 'record-desc'\` and then I can insert ri.AoValue = $userVariable as I please! &amp;#x200B; Thank you for helping me with that, that works as I would like it to! The query is still a little long-running, but I might just have to throw extra resources at the VM that hosts the database
Yep that is essentially what I was getting at. Should be a lot easier to read too. If that doesn’t perform well then either add more resources or look at indexes on the columns you join/filter on
You could try posting your problem. We're a pretty helpful bunch here.
Well, it could copy the deleted column into a backup table, and restore it later if the user asks. I don't know how useful that would be, but it shouldn't be difficult from a technical standpoint. Of course, there is a trade-off of storage space and amount of undo steps, but I think even one step of undo can make deployments a lot less nerve-wrecking. What do you think?
From my perspective, that's not necessary. If I want to drop the column, it means I don't need the data, right? If I wanted to retain it, I'd do so (for example write a PreDeployment script that would copy the data somewhere else). At my workplace, taking a backup prior to deploying a package is standard procedure, so there's always that, if needed. Overall, I don't have much experience with Flyway-like migrations that you're probably referring to, but there's nothing nerve-wrecking about my deployments ;)
Yes, all other rows share the same format and patterns just diffferent currency codes and fer amounts. Hope that helps
You need to put ReasonNum there then, instead of NULL. That is where you're declaring the columns to be inserted to based on data from the select statement. There's not a column named "NULL". Column names go in insert into argument, data values come from the select statement. 
Yes, that's another issue. Take out the semicolon between insert into and select. That's one operation. 
Chane then if to or. Currently you're evaluating the first condition, but only raising error if the second is also true. You want to raise the error when either are true. Your date can't be NULL and less than 1900 at the same time. 
If I try to remove then and replace it with or it says 'then was expected' 
Please don’t do this
Did you remove if as well? Should be if &gt; or &gt; then, not if &gt; or &gt; if &gt; then. 
Thank you !
Don't do it. COALESCE is not sargable.
Not sargable.
 if form.dataset.fieldbyname('NEXT_DELIVERY_DATE').isnull or form.dataset.fieldbyname('NEXT_DELIVERY_DATE').asstring &lt; '31/12/1900' then raise ('Incorrect next delivery date, please check and try again.')
Thats the exact thing i put in and it just gives the error 'then was expected' 
Put both the kunta = inside of parentheses and give it a shot. That way it would be: Where (kunta = or kunta = )
Omg, that's such an silly mistake, thank you!
What language is this?
SQL against software that is on an oracle database 
I'm very new to the job hence coming here for some help but even my manager cant work this one out as of yet 
No worries! I don't have experience in Oracle (only MSSQL), but that does not look like SQL syntax, it looks C++ to me. I googled and I cannot find "fieldbyname" operator for Oracle. I know you're pulling data from an Oracle DB into this function, but what language is the function itself from? 
why not just select into #TempTable at the bottom then select from your #TempTable on your way out. This way you aren’t constantly calling an ass ton of joins every select.
scroll down and see my comment.
I don't like this article as I don't really understand what problem its trying to address. It also does not say (unless I missed it) what sort of SQL engine this is targeting. You mention renaming objects as a destructive change that cannot be rolled back with ease? It's as simple as a single SP call in T-SQL land: https://docs.microsoft.com/en-us/sql/relational-databases/system-stored-procedures/sp-rename-transact-sql?view=sql-server-2017 &gt;Destructive migrations are migrations, that cannot be rolled back with ease. Fine, invent some new terminology. We don't need it but hey... Solutions area does not have 'Create rollback script prior to release'. This is a far far better solution than any of the other three you offer. This completely fails to address data changes as part of migration scripts. Appreciate the article title points to schema changes but ignoring that aspect of upgrades and migrations is ignoring half the picture completely. Again - I do not understand what issue this is trying to address. In a properly tested upgrade you also test the upgrade path by restoring prod to a lower instance and running the upgrade scripts. This article just screams 'im not testing my deployment process properly and having to fudge fixes after the fact and am trying to make my life easier by identifying the changes that are breaking stuff'. Not one of the 'solutions' are good, all of them are an indication something has gone very wrong. Stuff like 'fix if bug is small' is not something you should EVER have to do in prod ideally. If your article suggests 'fix if bug is small' as part of a deployment process to a production system then you really really need to re-evaluate your approach completely. &gt;Minimise the changes of such a release or phase out across multiple releases. It's not the job of a DBA to control the release cycles. If your release is that likely to fail then doing it in smaller pieces is only making it easier to identify the breaking change, it does not actually solve anything. &gt;Restore database from backup Absolute last resort only. This is not something you suggest as a solution as the only time you restore a backup over prod is when its gone so horribly wrong that all other process have failed. Correct me if I am wrong but this article in my opinion is redundant in a properly tested and controlled change release process and is only really relevant if you are a cowboy fixing bugs on prod due to improper testing and piss poor processes. Maybe I am missing something?
Overuse of brackets is better than underuse, especially when you're first learning. Heck, even later on when you're more proficient I'd prefer to see brackets if things are even slightly unclear. If it makes reading the code easier, then I'm all for it. The order of how AND/OR statements are processed isn't defined by how you write it, but by the how the database processes it. We know from context what you're trying to achieve, but the database may interpret it differently. Make it explicitly clear using brackets.
I would put those where conditions in joins because if one type is missing then its AoType is NULL and whole event is not in result set.
&gt; Our team's DBA has tried backing up and restoring a single table to the new server, but he said that process took 2.5 days Did he restore from a file share or network disk or did he do it from a local disk? That can really affect restore times depending on your network.
LPT: Anytime you think SQL is not working properly, you are wrong and it's probably something really stupid you aren't thinking of.
Try this: SELECT \* FROM Opisk WHERE kunta IN ('HELSINKI', 'LOVIISA') AND (op &lt; 90 OR op &gt;= 120)
May I ask the context? For example is this a class for the “most efficient” code or are you asking from a management perspective what I would like to see? 
Your instructors code is easier to understand at first glance. Your code on the other hand requires more thought to understand what is being achieved. The goal was code, in my opinion, is to ensure that you and others who happen upon your code later on can understand what is being accomplished. Nell specifically about subqueries, I don't like them, as they tend to run slower and are not as efficient as possible. Essentially the sub-query runs each time a row was returned. So in your situation your code is running potentially longer than your instructor's query. My personal practice is to use the WITH clause for my sub-query and use it in a join in the FROM statement. I would recommend that you examine the Explain Plan for each query to see the differences that happen during processing.
...very helpful LPT ;D
Context would be expectation of an entry-level SQL analyst, so I suppose management perspective. Alternatively, suppose someone is learning and he wants to avoid bad habits. 
Was it because he used the aliases, namely in the CASE? &gt;The goal was code, in my opinion, is to ensure that you and others who happen upon your code later on can understand what is being accomplished. This is exactly why I asked. Numerous times in the course, my code has looked different from his. Naturally, I understand what I've written but will another person easily understand it? 
Ask them to explain why they want `ALTER TRACE`. If they can't explain why, don't give it. If they can explain why, still don't give it. Most likely, [they want to run SQL Server Profiler](https://docs.microsoft.com/en-us/sql/tools/sql-server-profiler/permissions-required-to-run-sql-server-profiler?view=sql-server-2017). Which is a big pile of BS IMHO and you'll need to dig deeper as to why they think they need it. Because it'll hose up your production server's performance real good, on top of the security implications.
So personally ( I manage a team of analysts) I would like you to roll the max/min into a cite grouped by category. That’s my personal opinion though and others may not like that... if you like I could take a look at this and write a sample when I am home
I'll second that as it has happened to me before lol.
Thanks for the feedback, highly appreciated. Is the terminology or the meaning of `destructive` or `non-destructive` is wrong ? If yes, could you share the correct one's ? I agree on your point of testing and I have mentioned it in the post about pulling production code to staging or some other instance and testing. From my experience, even after testing, issues come up (rarely) and we have to revert back to the previous version. I've removed the `fix if bug is small` from the list of solutions and done some changes to the title.
You've got your answer from other comments, but I'd like to add that you should google up "sql operator precedence" or "sql logical operator precedence". This comes up a lot.
I agree with everything said here. Profiler is very resource intensive. The only time we run it is when it's run against a test database that only has one active connection, and the output is not be saved to the same server. Even then, we limit the profiler to run only for a few minutes at a time. It's useful to capture the exact queries and behavior going on, but it should never be run in production. There's a reason Microsoft has pushed extended events to replace profiler, even if our most common purposes (capturing the exact queries a given application function is sending) are about 1000 times easier to do with profiler (particularly with older editions of SQL Server). 
Thanks for the reply. I was finding those issues with enabling it in my research, but wanted insight from the experts here in /r/sql. They have zero control over what versions we run. (They're a 3rd party that just does our billing) We are at the mercy of the software company as to which versions of SQL we can use. Thankfully we have to upgrade to SQL 2016 in the next couple of months for the new release of our software. (holy crap! the licensing costs are ripping my 2019 budget apart! lol). I will tell them to find a different way to run their reports. Thanks again guys. 
I can see quite some weird things. &amp;#x200B; You have two "WHERE" keywords after each other. You have two "= 2017" bits - CONVERT(INT, CONVERT(VARCHAR(12),os.ReturnYearFiled)) = 2017 = 2017 - surely this is just CONVERT(INT, CONVERT(VARCHAR(12),os.ReturnYearFiled)) = 2017 &amp;#x200B; But the error you get suggests to me that this piece of code fails: &amp;#x200B; CONVERT(INT, CONVERT(VARCHAR(12),os.ReturnYearFiled)) &amp;#x200B; Check the content of the "os.ReturnYearFiled" - maybe it has some rows where the value is "SH". &amp;#x200B;
I'll take door number 3. Nested queries can be so hard to read and a pain in the ass to modify. My preferred method for readability, modularity and performance would be to use common table expressions ("with" clause) or temp tables. Which I'd choose depends on the database platform, but the pattern is similar: with dept_max_min as ( select department ,max(salary) as max_salary ,min(salary) as min_salary from employees group by department ) select emp.department ,emp.first_name ,emp.salary ,case when emp.salary = dmm.max_salary then 'Highest Salary' when emp.salary = dmm.min_salary then 'Lowest Salary' end as salary_in_department from employees emp join department_max_min dmm on emp.department = dmm.department 
Yep, CTEs are the way. You can re-use them and they make monsters so much easier to read, especially when you name them well and use good aliases. A small, cute, furry animal dies somewhere whenever someone aliases their tables "a","b","c", etc.
Not that I am aware of. If you want to mass change fields at once your best bet is learn some SQL.
I rarely do. At this point in my journey I do most of my changes with T-SQL. But at the start of my journey it was easier to make a handful of quick changes this way.
They might not be at the point where they learn about CTE's though
His code due to legibility and because he isn't doing 4 selects.
&gt;Is the terminology or the meaning of destructive or non-destructive is wrong ? If yes, could you share the correct one's ? Schema changes are not necessarily destructive. Non-schema data changes can be. Destructive is your opinion on the change. You consider schema changes harder to unroll therefore more likely to be an issue if it goes wrong. My opinion is that mass data changes are much harder to roll back without a backup. I can add a few columns in a minute, rename a procedure in seconds but reversing out an UPDATE that should have had a where clause? That's restoring backup territory, no way to avoid. I just don't see the point of the article - its all sticky tape for when its gone wrong. You should be aiming to avoid going wrong in the first place. Sure part of that is analyzing risks where it may go wrong but if schema changes are really catching you out that badly then your process is flawed. The entire approach and article premise is wrong IMO. Take 10 steps back and ask yourself 'why should I have to do this?'. If the answer is 'because of issues that would have been caught if the release process was properly tested' then your question should be 'what can I do to ensure this does not occur again'. The correct answer is NOT trying to identify where its likely to go wrong and get better at finding them. That's backwards. The correct answer is test your damn code - that includes the process of releasing code. Correct me if I am wrong - but if you properly test your release process then nothing in the article is relevant.
This is by far better approach, both from readability and performances. I would say that the golden rule about subqueries is don't use them. almost always there is a better option. 
Which is a better example of best practices? Neither is. SQL is optimized automatically and we hope the optimizer is smart enough to pull correlated query results from a cache. But what if it doesn't and the code executes as-is? You will end up figuring our max/min values 2 or 4 times per record. Look at something like u/doctorzoom has suggested for a better practice. &amp;#x200B; Why did he place it \[subquery\] in the FROM clause? Stepping back to basics, SQL queries return data sets and work on data sets (so, you are pulling data "FROM &lt;a data set&gt;"). Data sets can be stored into tables and that's how most of your queries will get data initially. The ability to do a query a data set ( FROM (query) ds\_alias) is convenient and powerful and sql optimizers are quite smart about running those efficiently as well.
There is absolutely no need for ALTER TRACE just to run a report. Unless their report tooling is built stupidly, or they're trying to poke at other things. Story time: In a past life, I was a developer/development DBA and while doing some testing for a new release, got a call about slow performance in the QA environment. Discovered that our lead tester was RDP'd into the server, running Profiler for 2 hours. Why? "To see if I can see anything". He was killing our performance for no valid reason. Within an hour, I'd had his access (RDP and SQL Server) revoked in that environment.
My guess is that the "WHERE WHERE" and " = 2017 = 2017" are copy/paste errors from SSMS to Reddit, since I think SSMS would catch the syntax error before it would complain about the CONVERT(). You're right -- there's probably some weird data in os.ReturnYearFiled that it's not happy about.
alright. I have read that NoSQL DBs like MongoDB scale horizontally well relative to SQL DB like MySQL. Would you know if this statement is true? And, how does it work?
I don't know what it is, but it's not SQL. Look rather like an ORM syntax that create a query to send it to the db.
Sam: Hey Dave, great job figuring out the answer to our problem, how'd you come about it? Dave: Well, that's easy Sam, I posted a question on Reddit, and the kind people there helped me out. Sam: Oh good idea, I'm gonna go find that thread and read it. Dave: \[internal monologue\] Oh shit, they're going to find out I go by "Surgical\_Dildos" online.
If you need to count each OR\_LOG record only once just use count distinct (specifically, count( distinct orl.LOG\_ID)).
Even when I don't strictly need to use brackets, I still use brackets because it helps break up the logical flow in a more structured way. And you don't have to guess if the AND gets executed first or the OR.
Lol. Idgaf what an external vendor thinks about my hilarious usernames. If the questions posted by surgical_dildos help someone in the future, then I'm glad to help. 
So I do not need to do anything in my GROUP BY clause? but instead change COUNT(orl.LOG\_ID) to COUNT(DISTINCT orl.LOG\_ID)?
this is correct fix
group by might be burying rows they might be missing code 1083, or have a not null start date
All of them have 1083. Even if I do \`right outer join\`, the code return incorrect results.
You should alias all of your column with the table they're from. Just a nice best practice to let the next guy (or Reddit) easily so what column come from each place. The records in your sales table may not be for customers that have sales in 2017 or sealed with a code of 1083
Thanks. Added the alias.
Hard to tell from your code, i'd try checking all the rows in your dbo.paid table and ensure that your filter predicates for those 200 rows all have those valid datapoints filled in (each row would need to have a return\_year, a Code). Try to remove the filters one by one and test, see what your results are. Good luck.
Make a few fake rows that show the problem by copying real rows and replacing the actual personal data with nonsense
I have copied `s.customer and s.return_year` from the `dbo.paid` in Excel and using `match` have found the 200 rows to be existent in the 2500 rows table with the `1083` code.
I will say it is mostly true, particularly for MongoDB, as I believe that is one of the features of that project. It isn't necessarily true for other NoSQL DBs, which may have other goals in mind. I don't know the internals for Mongo but I do have some experience with DynamoDB, which is also built with horizontal scaling in mind. Dynamo uses table partitioning to distribute the workload across compute and disk units and will automatically create new partitions as the table grows or as additional compute resource is provisioned to the table. This comes with its own set of complexities and issues to deal with (partition hotspots being the most common) but does allow for nearly infinite scaling. Mongo is a different type of NoSQL DB (document store rather than a key-value store), so it may take a slightly different approach to scaling than Dynamo, but I believe the basics are similar. You can do similar things with relational databases (SQL Server clusters come to mind), but they tend to not be automated (AFAIK) and require a fair bit of planning, configuration and are more difficult to execute.
But do they also have sales in 2017? Try to answer that by writing SQL. Challenge yourself to use less Excel. SQL Alissa are in demand and if you become skilled you'll be very employable.
Thank you!
Unless you are stuck on Sybase ASE. Fuck my life.
What's to guess? AND has a higher precedence than OR. [Oracle](https://docs.oracle.com/cd/B19306_01/server.102/b14200/conditions001.htm) [SQL Server](https://docs.microsoft.com/en-us/sql/t-sql/language-elements/operator-precedence-transact-sql?view=sql-server-2017) [Vertica](https://www.vertica.com/docs/8.1.x/HTML/index.htm) [Teradata](https://docs.teradata.com/reader/756LNiPSFdY~4JcCCcR5Cw/R0hHJn1U_zg54N96rpn61w) [MySQL](https://dev.mysql.com/doc/refman/8.0/en/operator-precedence.html) 
I think you're missing the point I was trying to make. Personally, I will always prioritize maintainability and readability. Which means if I can explicity state how something should be processed over implicit system behavior, I will choose to explicitly define it. So for example: 1. WHERE kunta ='HELSINKI' OR kunta ='LOVIISA' AND (op &lt;90 OR op &gt;120); 2. WHERE (kunta ='HELSINKI' OR kunta ='LOVIISA') AND (op &lt;90 OR op &gt;120); I will choose the second way of coding this. Even if all SQL engines on earth handle this correctly.
Hard to say without seeing the underlying data. My first hunch is with the where clause. Try only running one clause at a time, then add in others to see if you find anything. The other option would be to run the query without a where clause as a cte, then query the cte with the where clause.
Nested queries are the worst in terms of performance and readability, especially in Postgresql that has laterals, or better - window functions, distinct on, etc. Look how simple it can be: ``` ( SELECT DISTINCT ON (department) department, firstname, salary, "Highest Salary" as type FROM employees ORDER BY salary DESC UNION ALL SELECT DISTINCT ON (department) department, firstname, salary, "Lowest Salary" FROM employees ORDER BY salary ) ORDER BY departmnnt, type ``` Simply gets the first row for each department according to the ORDER specs.
Since you have so few rows, run the query like this to troubleshoot... select * from EDW.dbo.Starts os left outer join SQL.dbo.paid s on os.Id = s.customer and os.YearFile = s.return_year Once you get that worked out apply your filters and grouping.
But the latter is the only correct one of the two for the question.
Truth. This is gospel. 
The way I would do it is have everything in one flat table first and then try to normalize it. This way you will be able to understand the relationship and decide on the schema.
Yes, you can do when matched update, when not matched insert. Depending on the database, you can even specify when not matched on target insert, when not matched in source delete. Merges are pretty cool. 
What I mean is after src.sth = tgt.sth when not match can I follow up with and src.sth = false then insert like : “src.sth = tgt.sth when not match and src.sth = false then Insert” can I add AND condition after not match?
Oh, when not matched insert into ... Where src.sth = false. Is that what you mean?
If AoType is NULL, then a) something is wrong with that event, and b) I'm not looking for that info :) To clarify, putting the WHERE clause into the JOIN would look like: `LEFT JOIN AuditObjectDetails ri ON at.EventId = ri.EventId AND ri.AoType='record-id' I have changed to this, and it certainly helps with the readability, thanks!
I have a primary key set on EventId on the AuditObjects table, is that what you mean by indexes? If not, can you point me to some resources to learn more about what I should be indexing and how? :)
Without using where but just “AND” condition 
Is this a test. 
One of the best employable skills for someone working in SQL is writing reports, blows my mind sometimes how many openings I see for report writers. Also easy as hell to go into business for yourself doing it as a subcontractor. Anyway, dunno if you deal with that at all where you're at now, but it's easy enough to translate the skill-set. You're not locked into becoming a DBA w/ SQL skills, is my point.
I'd recommend becoming a DBA - it's a good starting point for a sysadmin or systems architect assuming you keep on learning
No it is not a test, I kind of need it on the project I’m working on. The requirement is quite complex but just need to know if it is plausible to do it in that syntax format.
I have no clue. 
Is there any difference in the results set when you change it from a left to a right join? If so, what is the change and why did that join cause it? If no change, it’s likely something else in your query. Like someone else said, could be grouping something wonky - you could try breaking the query out and running just a select * without grouping to see if that has any affect.
Indexes are extra data you store on a table for specific columns. It essentially sorts the table by that column so you can filter on it faster. Creating an index can be easy, it’s deciding what columns to index in order to optimize queries. I don’t have any specific resources for you I’d just google anything you have questions on, that’s how I learn with sql myself if I need help with a concept. I’m not an index expert by any means FYI. Also there is a chance you automatically get indexes on primary keys, but I’m not sure if that’s just how the database I work on is set up or if it’s most places
Report writing. Business or Supply Chain Analytics. Depending on the acceptable salary range you are looking at, you could transition directly to a position in any Supply Chain role and you should prosper. Supply Chain tends to be a data rich environment. There is also generally a LOT of money tied up in it. The ability to use that data to save the company money and/or shorten their data to decision time is worth a lot of money. 
Can confirm. Currently do reporting for supply chain and almost exclusively use SQL to do it, though I kinda fell into it and probably don’t get paid as much as I should.
Report writing is also something you'll do in almost any other development role, unless you spend your entire career working in companies with dedicated reports teams.
There is no point doing a left join from EDW.dbo.Starts if you add EDW.dbo.Starts to the WHERE clause. Its now run like an inner join. https://www.tech-recipes.com/rx/47637/inner-and-left-outer-join-with-where-clause-vs-on-clause/ 
sooo... t1, t2, t3, T1, T2, T3 in a case sensitive database?
I am a reporting Developer :)
Dude . . . I bet you're the kind of person who scrapes a fork on a dry plate just to watch everybody wince. :)
Basically anything is possible in SQL. Why are you limiting yourself to a merge statement? How about you just do an insert? Merge is a cool keyword, but they're not performant when you have tens of millions of rows. Also I dislike then as imo they're not as clean to read as an update followed by an insert with a select. I've used them, but I've developed a dislike for them.
Thanks for answer , sorry it was a late message .I will do this .
Thanks man , really , really appreciate your help .Hope you have a good day.
Not trying to be glib but if you don't think SQL queries are fun then working with databases may not be for you. Broadly speaking though there are three technical paths I see linked to being a DBA : 1. Business Intelligence. Kind of flavour of the month and likely to remain so for a while. This includes data analytics, reporting writing, data science, etc. This assumes heavy interest in SQL itself as well as other methods of working with data. You should be passionate about data to do this. This kind of role interfaces directly with end users more than other IT roles. 2. General infrastructure. Arguably in a relative decline but this includes general server administration as well as networks / security / storage management etc. Where I work I have "SQL DBA" in my job title but I'm also a Windows domain admin, I help look after our Exchange server, file server, some Linux servers, etc. This kind of role used to require good knowledge of hardware but not need any use of scripting/coding whereas that's kind of reversed now. A lot of these roles will be combined with "DevOps" in the future and even where this doesn't happen automation is the keyword here. 3. General developer. Very different work type to being an administrator but the jump is not as large as it might seem, especially if you enjoy writing code. Different kind of stress to being an admin - you shouldn't get calls at 2am but you may well be expected to work long hours to meet a sprint goal. Although ever increasing specialisation is a thing I'm finding that there is a convergence in what a lot of IT roles require. Developers (through things like Docker) are provisioning their own virtual networks now and Windows server admins are expected to do more via scripts/APIs than they would have 10 years ago. Everyone needs to understand security to some extent (although it's debatable whether anyone does) while fewer people need to know how to replace a hard disk in a physical server. &gt; All I can imagine is loads of stress and being called at 2am for emergencies. If this happens it's more a product of your environment/organisation. SQL engines tend to be really reliable and by themselves seldom go wrong. Where you do get serious problems they are very likely to be caused by other issues (e.g. running out of disk space, network outages, application level code weirdness) which tend to be avoidable (e.g. disk space) or out of your control (networks/application). If you have a healthy change control *culture* and monitor for things like disk usage growth then you avoid 95% of problems. I don't see DBA as being a particularly stressful role in and of itself but as I say that's more down to your team/organisation/management. 
I concur that BI is the flavor of the month, but for some reason it's not treated with a lot of respect money wise. I've been scouting better paying jobs for a year now, in several different countries. There seems to be a ceiling for anything BI related. Data scientist, data engineer, analyst... The only exceptions are some big data engineering roles with ridiculous requirements for experience (several years in several big data technologies, that are simply not feasible to implement if you have less than a few terabytes of data to process. And big data doesn't rely on SQL usually, and requires a slightly different set of skills: multiple distributed managers, cloud computing infrastructure, Java, Scala, Python. I could also go freelance, and demand as much as I think I'm worth, but that means giving up an environment free of all other stress other than stress related to learning and developing. Anyway, just my 2 cents
You should learn to read errors. It's a must-have skill without which it's gonna be close to impossible to be efficient. How do you do that? Read the error. It says something about errors converting "SH" to integer.
You can learn dev. I’d become a DBA sort of by chance, didn’t love or hate the job. It’s just not my type of work. I started learning Python, C++, Java and getting my SQL skills beyond queries. At the moment I still solve some tickets but leaning more and more to dev and design. It depends on your company, mine is ok with this kind of transitions as long as an employee stays at the company. Maybe you could talk with your manager? TL/DR: in-depth knowledge of DB systems helps becoming Dev 
As a developer, I think being a pure DBA sounds pretty boring, or perhaps I should say it's more of a specialization of a sysadmin job position. At my company, I'm a hybrid DBA and database developer. I do a shitload of SQL, both application development (SP's, views, tables, etc.), but also some reporting (both scheduled and ad-hoc queries). On top of that, I also do DBA work such as backup/restore, jobs, AAG, running scripts, chasing performance issues, etc. Our sysadmin department do the more physical server stuff such as installing the actual Windows+SQL Server, setting up external backup systems, monitoring the server, and more. We also overlap a bit on the AAG. They are the ones getting woken up at 2am if a disk is full, or the AAG fails to failover, so to speak. Are you a developer or sysadmin? If you don't like SQL and don't have an interest in being a DBA, steer away from this all together. 
https://www.w3schools.com/sql/ 
Google 'oreilly sql pdf'. Should come up with a few results. Decent books. ^*do not endorse piracy*
thank you very much
What’s your title? Supply chain analyst? Business analyst?
Start by [converting the datetime values to month and year](https://stackoverflow.com/questions/1781946/getting-only-month-and-year-from-sql-date). Then you can group by that
Some good things about being a DBA: low stress, autonomy, predictable, well compensated, if you like to be in charge and take your responsibility seriously then it can be satisfying to be the gate keeper for the companies data. If you don't like SQL, but enjoy coding maybe data engineering is more to your taste. Focusing purely on production DBA responsibilities won't really give you much development experience and you might be pigeonholed. I see more DBA positions being replaced by DevOps/infrastructure team members with more of a sysAdmin/networking bent, so personally I worry about the positions future growth. 
Have a user table with a user ID, first, and last name. Comments table with commentID, user ID, comment, and date. Likes table with commentID and user ID. You can put a foreign key contstraint on the user ID and comment ID when used outside their original tables. Then you can join those 3 tables on user ID or comment ID. Don't repeat the first name in each table. Just join the user table on the user ID.
To answer your question: [Query Stores](https://docs.microsoft.com/en-us/sql/relational-databases/performance/query-store-usage-scenarios?view=sql-server-2017) Although I'm fairly certain that won't be the issue. This seems more likely to be a blocking or autogrowth issue.
Try out: OPTIMIZE FOR UNKNOWN OPTION RECOMPILE You need to determine which bit is killing it - the import? The 'massage'? Or the insertion? Then break up or otherwise refine the code. You may find sticking an option recompile in there somewhere will fix it. As its a daily data load the additional overhead from recompiling the query plan each execution will be minimal. Also grab the plans from the plan cache so you can see whats causing the pain. If it is a cached plan as you say, you should be able to replicate by doing a small file then a big one immediately after which will either prove or disprove your theory.
My official title is data analyst.
I would say it's not blocking. The server is idle at this time, as each import process is dependent on the next, and there's a several minute gap between them. The server is only for this application. I've also moved the start time of the process 30 minutes back and forth, no change. I would also discount autogrowth, as a process that runs earlier imports 4gb of data each day, and the other processes bring in 20-30mb of their own data as well. It's just this process, on Thursdays, when the expected amount of data differs.
The insert is what's failing. There is a separate massage sproc, then this insert sproc. The insert is a direct "insert into production (columnlist) select columnlist from staging", and I verified it using Extended Events. Aren't I replicating it every week? small file, small file, big file fail, small file, small file... 
What I don't get is how JIT capabilities build into the Java are relevant for their database technology, and why are they even mentioned in the article? I was not able to find any source that they plan to implement this in their RDBMS and I wonder if this is a commercial decision related to their software-in-silicon processors( which provide similar benefits), or maybe it says more about the state of their codebase https://news.ycombinator.com/item?id=18442941 
Yeah you are replicating every week but you need to see both query plans to determine what its doing badly. Might be worth checking your autogrowth settings and seeing if if its having to grow the data file at insert time as thats very expensive. Failing all that, doing the insert in pieces - say a few K rows at a time.
So, /u/korrakage , how was your SQL Saturday experience?
Autogrow is set to several gb, and imports that run before and after this one are 30mb to 4gb in size, far more complicated, and have never timed out. Its just this rinky-dink one. It's been especially frustrating because it completes immediately if I rerun the exe the second it fails.
&gt;I wonder if I can use KEEPFIXED after the big file processes, to hold on to that plan indefinitely I would not recommend that over OPTION RECOMPILE. Plans are supposed to change due to statistics etc. 
Rank (row_number) partition by project and month order by date descending. Filter to rank =1
Alright, I'll give the recompile a try. Thanks for the responses.
Thanks for this perspective! 
just don't use nosql. I used mongo in my project, waisted loads of time and effort, and now I'm thinking how to get rid of it and move to rdbs.. :(
Hi, this sounds exactly like what I need! I'm really new to SQL so I'll have a look online but if you have a few minutes I'd really appreciate help on how you write this??
Hi, thank you for this :) but my issue is selecting just one of the status reports from a month when there are loads! preferably the first one of the month
&gt;Nevermind figured it out! thanks so much!
What kind of Sql are you using? Then type into Google, (Sql platform) sql rank syntax. 
You can do a Cartesian (cross) join then rank for each iss timestamp based the calculation of distance between iss position and city position. 
It's possible that your process is loading data in an expensive way from an existing table(s). It times out, because the query takes too long reading from disk, but MSSQL will keep that table and results cached in memory. Next time you run the process, same query plan, but everything is already in memory, so no issue. At any rate, you are going to need to do some logging/tracing to see what is happening, and what is taking so long the first time it runs. Once you figure out what is taking so long, you can start trying to figure out why that step is taking so long. Also, the following won't make you a better DBA/developer, and is kicking the problem down the road and covering it up with a bandaid, but sure does fix a lot of angry managers when you need to in a pinch: Could you increase the timeout? The 20mb file, is it a huge deal if you split it into 10 different 2mb files, and run the process 10 times? You should totally figure out what the issue is, for a best solution, and for being a better developer, but I do want to make sure you're aware of some firefighter options.
What is the data type for mynum? What does power(0.0,0) yield?
select power(0.0,0) = 1.0 select cast(power(0.0,0) as float = 1
To clarify, there's a staging table that's truncated first, then populated with the data from the file. Then it's messed with a little (formatting numbers, removing invalid rows). Then it does an INSERT INTO [prodtable] (fieldlist) SELECT (fieldlist) from [stagingtable]. I don't think SQL can cache that. I also do far more expensive operations on similar size tables and they're fine. This import is on several hundred customer servers, and this is the only one that has a file variance of this magnitude. The process has never failed on any other customer in the 8 years I've been here. That is why I'm guessing it's something to do with how SQL is looking at the data, not the code itself. Unfortunately, the file cannot be altered. I would look to promoting a change to the timeout, but that it completes in a minute if the exe is reran immediately, doesn't seem like that would be necessary/good practice. It seems to point to a plan being created that is for 2mb of data, so it fails, recreates a plan for 20mb of data, and then Friday, it completes, sees only 2mb of data, and recreates the plan, and then here we go again. I'm going to go with the other guy's idea of using RECOMPILE, which if I understand it correctly, will look at the tables involved, create a plan accordingly, then run the statement.
c.period = ‘201810’ or c.period = ‘201811’
Wouldn’t this give me both entries if the payment code ‘prt’ is in both periods? If I get a result for the 201811 period I do not need a result for the ‘201810’ period. Thanks btw 
I have heard similar sentiments expressed elsewhere. I have not implemented anything in mongo. What don't you like about it? Is it a performance issue? 
Joins are always 'conditional' so you should be fine writing it just like that. You'll need to think what happens when b.other_data_b is not 'some_text' and whether your "and" is a logical "and" ("this is true at the same as prior") or a colloquial exclusive "or" ("if not prior but this following instead").
&gt;You'll need to think what happens when b.other_data_b is not 'some_text' Good point. I'm not quite sure how to go about that, if it is another value than 'SOME_TEXT', I don't particularly care, I just need A.KEY1_A = B.KEY1_B to still be in effect. It definitely needs to be an AND but only when that case statement's criteria is matched....hmm
A kind of weird news about power(0,0). Oh well. Here you go: (sign(mynum) + 1) /2, though the 'why' question is hard to suppress.
I'm having a hard time understanding what you need to accomplish. Could you export me the DDL of all the tables you have currently? Sometimes when I'm having a hard time understanding what I need a historical function to do when replicating it to SQL, it's better to ask the question "what do I want it to do".
 SELECT tenant , MAX(c.period) AS latest FROM ... WHERE c.period IN (‘201810’,‘201811’) GROUP BY tenant
Why? Because case statements aren't the solution to every problem in SQL programming, despite what my boss thinks. I didn't know about the sign() function. This is just what I wanted. myflag = sign(mynum) will work in place of a case statement. Nice. &amp;#x200B;
A little late to the party here, but Toad for DB2 allows you to set it so you only pull so many records or all records in the Editor window. You can also create Snippets as SQL templates as well.
Off the top of my head id write it something like this: select A.* ,B.* FROM A JOIN B ON CASE WHEN B.OTHER_DATA_B = 'SOME_TEXT' THEN A.KEY1_A = B.KEY1_B AND A.KEY2_A = B.KEY2_B ELSE A.KEY1_A = B.KEY1_B END 
Just an FYI (because I was interested and looked it up), [Wikipedia](https://en.wikipedia.org/wiki/Zero_to_the_power_of_zero) defines 0^0 as "Zero to the power of zero... is a mathematical expression with no agreed-upon value. The most common possibilities are 1 or leaving the expression undefined, with justifications existing for each, depending on context."
Zero to the power of zero, denoted by 00, is a mathematical expression with no agreed-upon value. The most common possibilities are 1 or leaving the expression undefined, with justifications existing for each, depending on context. In algebra, combinatorics, or set theory, the generally agreed upon value is 00 = 1, whereas in mathematical analysis, the expression is generally left undefined. Computer programs also have differing ways of handling this expression.
Is this a millenial thing? Sign function, most likely contains quite a bit of logic inside (implicit conversions, conditionals, etc.) so it's not a clear-cut answer what would be better performing and, most likely, the performance difference if any is not significant. Readability-wise, "case" will express your intention/logic clearer than the sign/power, so I'm gonna side with your boss's opinion on this case.
Can you show an example of what you wish for the pattern above would become, delimited?
You are right. I think it added a couple seconds to my script. I will say that, if you had ever seen my boss's case statements, you would know how fully they can obscure intent. File under can but should not.
Thanks I’ll give a try - however right now I have multiple table columns after select and three left joins so not sure how to insert this. 
That doesn't work, as the CASE statement has to return only *a* column, not an expression (at least for Oracle).
Could I see the query?
Your logic is very stripped down, but try something like this: SELECT * FROM TableA a INNER JOIN TableB b ON a.KEY1_A = b.KEY1_B OR (b.OTHER_DATA_B = 'SOME_TEXT' AND a.KEY2_A = b.KEY2_B) &gt; I could solve this by separating the queries into 2 sets via a union with different where clauses, but I feel like I don't really need to do that. Two queries UNIONed or UNION ALLed together would have potentially different result sets to the above query, but it may also perform better (or it might be significantly worse). 
I'll add to this. Merges can be really nice if you need to UPDATE/INSERT/DELETE a small dataset pretty quickly/easily, but it's really not the best option if you're looking to do gigantic update/inserts/deletes. It really just works out better in the long-run to develop a procedure in T-SQL/PL-SQL that will perform the necessary update/insert/deletes.
Very good point, this likely wouldn't be able to properly grab the index. The columns selected will be the exact same between both UNION ALLed queries, just the where clause would differ.
do the case statement like: case when B.OTHER_DATA_B = 'something' then B.KEY2_B END = A.KEY2_A else TRUE end 
Glad to see you made it here. :) I'm also looking forward for the response(s), so... Thank you for the post.
I really only have the two tables right now. Was planning on getting this part hammered out then building the rest of the tables around a functional core. &amp;#x200B; There's the "counts" table that stores how much of each item we have on each shelf at a given time CREATE TABLE public.counts ( ct_datetime timestamp NULL, shelfid int4 NULL, sku varchar(20) NULL, qty numeric NULL ); and the inventory transactions table that contains positive values for received shipments of items and negative values for wasted or damaged items. &amp;#x200B; CREATE TABLE public.inv_transactions ( tr_datetime timestamp NULL, sku varchar(20) NULL, description varchar NULL, qty numeric NULL ); &amp;#x200B; I also have a view set up to aggregate the counts by sku and date &amp;#x200B; CREATE OR REPLACE VIEW public.vw_agg_counts AS SELECT c.ct_datetime, c.sku, sum(c.qty) AS qty FROM counts c GROUP BY c.sku, c.ct_datetime; &amp;#x200B; Every week, I count our stock. To save time, I only count the most important items every week. Less important items I'll count every other week or once a month. These counts determine how much product we re-order every week. &amp;#x200B; To know how much to re-order, we need to know how much we tend to use. To figure that out on a weekly basis, we take last week's count, add in anything we've received since last week, subtract anything that's been wasted, and then subtract this week's count. &amp;#x200B; Say we have 10 bottles of syrup one week, we receive 12 on that week's syrup truck, we knock 2 over and break them, then have 13 on hand at the time of the next count. &amp;#x200B; That'd be 10 + 12 - 2 - 13 = 7 bottles used that week. When going to re-order, we'll average those usage values for the past few weeks, tack on an extra safety percentage based on how much business we're expecting, and order accordingly. &amp;#x200B; I want to be able to, for each sku, take the counts, find the next count in the sequence, add the stock received in the interval to the first count, and subtract out the second count. &amp;#x200B; I've thought about doing this by week. The counts and transactions tables would each have a weeknum column. I would just sum all the transactions on weeknum, then join that sum to the counts on the weeknum, then add those columns and subtract the next count. &amp;#x200B; The problem is that I would have to count everything every week. Or at least I think that's what it would mean. &amp;#x200B; Without an easy identifier for each count interval, I'm at a loss for how to aggregate the transactions table to match the intervals between counts for each item. &amp;#x200B; I feel like I'm rambling. &amp;#x200B; TL;DR: For each record on the transactions table, I need to find the counts table record that's nearest in the past with matching SKU.
This code does not work as the OP requested. There could be rows where b.other_data_b = 'some_text, the key2 columns match, but the key1 columns do not match.
Are you doing this in MSSQL?
 from A join B on A.KEY1_A = B.KEY1_B and (B.OTHER_DATA_B = 'SOME_TEXT' and B.KEY2_B = A.KEY2_A or B.OTHER_DATA_B &lt;&gt; 'SOME_TEXT' )
I'm doing it in PostgreSQL.
Piggybacking off of this (which others suggested also), I was able to satisfy what I needed via altering it to: SELECT * FROM A JOIN B ON A.KEY1_A = B.KEY1_B AND CASE WHEN B.OTHER_DATA_B = 'SOME_TEXT' THEN B.KEY2_B ELSE A.KEY2_A END = A.KEY2_A
&gt; The columns selected will be the exact same between both UNION ALLed queries, just the where clause would differ. Not if you want to guarantee the result sets are identical. To avoid unnecessary duplication while simultaneously preserving any actual duplication in your result set, you'd have to write a UNION ALL query like this: SELECT * FROM TableA a INNER JOIN TableB b ON a.KEY1_A = b.KEY1_B WHERE NOT (b.OTHER_DATA_B = 'SOME_TEXT' AND a.KEY2_A = b.KEY2_B) UNION ALL SELECT * FROM TableA a INNER JOIN TableB b ON a.KEY2_A = b.KEY2_B WHERE b.OTHER_DATA_B = 'SOME_TEXT' AND a.KEY1_A &lt;&gt; b.KEY1_B That should be deterministically identical to the query in my first post. Each half of the UNION ALL has to remove records that would be in the other or they'll incorrectly duplicate while also allowing legitimate duplicates through. Yes, you may have a situation where you can just do this: SELECT * FROM TableA a INNER JOIN TableB b ON a.KEY1_A = b.KEY1_B UNION SELECT * FROM TableA a INNER JOIN TableB b ON a.KEY2_A = b.KEY2_B WHERE b.OTHER_DATA_B = 'SOME_TEXT' But it's going to depend on your data. Similarly, this will not be the same: SELECT * FROM TableA a INNER JOIN TableB b ON a.KEY1_A = b.KEY1_B UNION ALL SELECT * FROM TableA a INNER JOIN TableB b ON a.KEY2_A = b.KEY2_B WHERE b.OTHER_DATA_B = 'SOME_TEXT' Once you have one pair of records that satisfy both conditions, it will duplicate where it shouldn't. Again, your specific data may mean your results are the same, but if we're just looking at the relational component, the above two UNION queries aren't the same as the first one (they're not the same as each other, either). 
Awesome, I just made a workaround in one of my other comments but this is indeed a valid solution too! Thank you!
I disagree with that interpretation. He says quite clearly: &gt; I *always* need rows to pass through that satisfy the join condition of: &gt; &gt; A.KEY1_A = B.KEY1_B 
i would make separate select statements with your key 2 condition in the where clause. alias that select statement and join the aliased table on key 2. If you are doing a left join, then in the top level select statement you could use CASE + ISNULL statements to account for when the condition is not met for your aliased table because the joined ON value will be “null” in those cases.
Agreed on the requirement. Your join condition can evaluate to TRUE in some cases where A.KEY1_A &lt;&gt; B.KEY1_B, i.e. exactly those cases where the stuff in parentheses evaluates to TRUE.
So, you think it should be, what, this? ON a.KEY1_A = b.KEY1_B OR (a.KEY1_A = b.KEY1_B AND b.OTHER_DATA_B = 'SOME_TEXT' AND a.KEY2_A = b.KEY2_B) That's nonsense. It's identical to just `ON a.KEY1_A = b.KEY1_B`. Just say what you think the correct condition is. 
Run this and then tell me if it changes your mind: CREATE TABLE #TmpA (sex_c varchar(1)) INSERT INTO #TmpA (sex_c) SELECT '1' INSERT INTO #TmpA (sex_c) SELECT '1' INSERT INTO #TmpA (sex_c) SELECT '2' INSERT INTO #TmpA (sex_c) SELECT '2' INSERT INTO #TmpA (sex_c) SELECT '2' SELECT COUNT(CASE WHEN sex_c = '2' THEN NULL ELSE 1 END) FROM #TmpA DROP TABLE #TmpA 
It's D. You want the case when to be embedded in the count, or else you're attempting to return a value in a group by that isn't in a group or aggregate function.
Now I'm basically trying to join the transactions and the counts on sku and minimum distance between count date and transaction date. This doesn't work, but it feels like I might be barking up the right tree? with time_diffs as ( select t1.tr_datetime, c1.ct_datetime, extract(epoch from t1.tr_datetime - c1.ct_datetime) as diff from inv_transactions t1 join vw_agg_counts c1 on c1.sku = t1.sku) select t.tr_datetime, c.ct_datetime, t.sku, t.qty from inv_transactions t join vw_agg_counts c on c.sku = t.sku and extract(epoch from t.tr_datetime - c.ct_datetime) = (select min(td.diff) from time_diffs td where td.diff &gt;= 0) &amp;#x200B; I think this records for transactions with the minimum distance of time between their time and their item's most recent count. |tr\_datetime |ct\_datetime |sku |qty| |:-|:-|:-|:-| |2018-10-01 14:00:00|2018-10-01 13:00:00 |11071912 |\-4.0| |2018-10-01 14:00:00|2018-10-01 13:00:00|7360 |\-3.0| &amp;#x200B; My goal: &amp;#x200B; if these are the counts: &amp;#x200B; |ct\_datetime|sku|qty| |:-|:-|:-| |2018-11-29|6663|2| |2018-12-01|1234|16| |2018-12-03|6663|12| |2018-12-08|1234|17| &amp;#x200B; I want the query to return something like this by way of joining the counts table on the transactions table: |tr\_datetime|sku|qty|datetime of the nearest-in-the-past count for this sku| |:-|:-|:-|:-| |2018-11-30|6663|3|2018-11-29| |2018-12-02|6663|\-1|2018-11-29| |2018-12-04|6663|2|2018-12-03| |2018-12-03|1234|5|2018-12-01| |2018-12-04|1234|\-1|2018-12-01| |2018-12-07|1234|4|2018-12-01| |2018-12-09|1234|5|2018-12-08| |2018-12-11|1234|\-4|2018-12-08| &amp;#x200B; Then I'd sum the transactions by sku and "nearest-in-the-past datetime..." and go from there. 
Self-join is a more direct functional alternative to windowed functions - have you tried that?
The answer is almost certainly yes, but the details would depend on which type of DB you're using, which you haven't told us.
&gt;Could you write the code in a IF THEN type statement that would keep looping and/or not start until the specified time I could upload the night before and hit execute? Yes. The easiest way would be to run a loop until `Curent_Year = 2019`, sleeping for a minute inside of the loop... then have your query outside of the loop. This is going to vary drastically based on what flavor of SQL you're using Microsoft SQL Server has the SQL Agent that allows you to schedule things at specific times... not sure if other DBMS have something similar.
What type of database are you using? Assuming it's not MS Access, it should have a job scheduler that can handle this. Have you spoken to the DBA's? They should be able to get this scheduled.
I think I understand the general idea of what you want to do. Give the script below a look. Ultimately, it contains tables that can be used to house manual counts (by yourself), counts of a shipment, counts of wastes, and summarizes them with a series of views. A stored procedure automatically merges "USED\_CNT" data into a historical table for easy averaging, which can also be pulled from a view. A table also exists that allows for you to over/under order specific items by percentages. I commented it all out in the script. This was written for MSSQL Server, so it might take some finnicking to get it to work on PostgreSQL. https://www.mediafire.com/file/mmveh038cvkduvu/DRUNKNEMATODE.sql/file Let me know if this gets you any closer, or of any issues with it.
SSMS really isn't the best IDE, but you can sort of bend its capabilities to your will a bit. I know of no out-of-the-box way to make hyperlinks in SSMS, but try this: 1. Type a comment line above a section of your script that describes that section. 2. Select the section of code (but not the comment). 3. From the 'Edit' dropdown menu, choose 'Outlining', and then choose 'Hide Selection'. 4. If you do this with every section of code in your script, you'll end up with a poor man's Table of Contents, under which is an expandable, collapsible section of code. 5. BUT... these blocks won't persist after saving and closing the file. 6. BUT BUT BUT... they WILL in Visual Studio, which so far seems to be a decent replacement for SSMS. &amp;#x200B;
Hi thanks for this template, I ran it and the first one gave a count of 3, the second one gave a count of 2, and the third one gave a count of 3.
The first one should have given you two rows, 3 and NULL. The only way choice B works is if you do a GROUP BY, which will divide the results into two rows, one row for males and one for everything else. Option D is correct. You most definitely can use case statements inside an aggregate function. This will allow you to aggregate results over the entire table into one value, using the CASE statement to pick and choose what data to be used in the COUNT() function.
I am completely floored. This is so far above and beyond my expectations I don't even know what to say. It's gonna take me some time to work through this and fiddle around, so I'll definitely get back to you with questions if they come up. &amp;#x200B; Crazy phenomenal. I was expecting confused looks and a link to some tutorials at most. You're an absolute legend.
Thanks, I'm flattered 😊 Let me know if questions come up. I'm sure you can mold it into whatever you need it to be if something isn't just right. 
https://dev.mysql.com/doc/refman/5.7/en/spatial-analysis-functions.html Here's a similar question/answer: https://stackoverflow.com/questions/24370975/find-distance-between-two-points-using-latitude-and-longitude-in-mysql
I agree that the longer statement you've just written is equivalent to KEY1_A = KEY1_B, so it is not what I think it should be. Here's a correct join condition: https://old.reddit.com/r/SQL/comments/a5va3a/i_have_2_tables_both_have_2_keys_to_join_on_i/ebpqhem/ 
Use SQL Agent. If you can't do that, use waitfor WAITFOR TIME '00:00'; SELECT 'blah blah';
The easiest platform-independent way to do it is to use tho host's OS tools to launch your sql script. Microsoft on SQL server can do this, as well as Oracle on *nix OSes via Crontab. I do this all the time, which is why I have so much time to browse reddit. Set up a test script that echos "hello" for your proof of concept. And once it's working, then replace it with your script. Wash, rinse, repeat for everything you need scheduled. 
I was a DBA. The basic fundamentals of report writing can land you an analyst position. Currently a Clinical Analyst for #1 hospital in my state. All I do is reports and benchmarking. You can do it!!
Have you tried a lag?
This is the route I would also suggest. /u/sprodigy here is a sample on how that would look. &amp;#x200B; IF OBJECT\_ID('dbo.Expenses', 'U') IS NOT NULL DROP TABLE dbo.Expenses; CREATE TABLE dbo.Expenses ( ExpenseId INT NOT NULL IDENTITY PRIMARY KEY ,DEPARTMENT VARCHAR(10) ,AMOUNT DECIMAL(9,2) ,ACCOUNT\_DATE DATE ,ACCOUNT\_YEAR AS YEAR(ACCOUNT\_DATE) --I'm lazy ,ACCOUNT\_MONTH AS DATEPART(MONTH, ACCOUNT\_DATE) ) &amp;#x200B; INSERT INTO dbo.Expenses ( DEPARTMENT ,AMOUNT ,ACCOUNT\_DATE ) SELECT CASE ABS(CHECKSUM(NEWID())) % 5 WHEN 0 THEN 'Sales' WHEN 1 THEN 'Operations' WHEN 2 THEN 'IT' WHEN 3 THEN 'Reception' WHEN 4 THEN 'HR' ELSE 'Executive' END AS DEPARTMENT ,CONVERT(DECIMAL(9,2), ABS(CHECKSUM(NEWID())) % 1000000.0 / 100.00) AS AMOUNT ,DATEADD(MONTH, -(ABS(CHECKSUM(NEWID())) % 120), GETDATE()) AS ACCOUNT\_DATE FROM sys.objects AS O CROSS JOIN sys.objects AS O2 &amp;#x200B; &amp;#x200B; SELECT E.DEPARTMENT ,E.ACCOUNT\_YEAR ,E.ACCOUNT\_MONTH ,SUM(E2.AMOUNT) FROM dbo.Expenses AS E INNER JOIN dbo.Expenses AS E2 ON DATEADD(MONTH, E2.ACCOUNT\_MONTH, DATEADD(YEAR, E2.ACCOUNT\_YEAR, 0)) &lt;= DATEADD(MONTH, E.ACCOUNT\_MONTH, DATEADD(YEAR, E.ACCOUNT\_YEAR, 0)) AND E2.DEPARTMENT = E.DEPARTMENT WHERE E.DEPARTMENT = 'IT' GROUP BY E.DEPARTMENT ,E.ACCOUNT\_YEAR ,E.ACCOUNT\_MONTH ORDER BY E.ACCOUNT\_YEAR ,E.ACCOUNT\_MONTH ,E.DEPARTMENT
Hm. Yeah, that's still not how I interpret OP's post given what's they've written there. 
Just add the parts from his query above into their corresponding parts of the query you've already developed. Probably make the FROM here another left join.
D's the right answer of the choices, though if I were me, I prefer using sum() to count(). Feels better to say "sum up the number of 1s" than "count up the not-nulls because nulls aren't considered".
D. Or wrap it in a SUM() and do `THEN 1 ELSE 0`: SELECT sum(CASE when Sex_c = '2' then 1 else 0 end) as "Male", sum(CASE when Sex_c = '1' then 1 else 0 end) as "Female" FROM myTable;
I thought of one additional functionality that one of the views needs. As this ERD can handle infinite amounts of shipments/waste inputs between two "full inventory" counts, the days between the full inventory could vary greatly. In the previous view, \[dbo\].\[INV\_ORDER\_ESTIMATION\] all "full inventory" counts were considered the equal regardless of days between inventory counts. This needs a weighted averaging system to ensure that the inventory weights out if you were to input a transaction for 14 days, but then input a transaction for only 1 day. *(2 weeks would carry 14 times the weight of 1 day.)* Below modification to the \[dbo\].\[INV\_ORDER\_ESTIMATION\] view will correct the average estimations, converting them to weighted averages. https://www.mediafire.com/file/hnifxdz88u4kqf2/DRUNKNEMATODE\_WEIGHTED\_AVG\_FIX.sql/file 
I'm having a hard time visualizing this. Could you make up perhaps 3 records from both of the tables as well as what you \*want\* it to look like?
Have you tried a join?
[SQL Fiddle](http://sqlfiddle.com/) 
Thanks for that! :)
I don't know that I deserve all this TLC. Thank you so much for your time. I'm heading into work and have some Christmas preparation to do in the next couple of days, but I intend to spend every spare moment breaking this down and figuring it out. It can't be said enough how fantastic this is. I learn best by taking things apart and building them back up, so having something fully functional I can use with my own data will be such a tremendous learning tool. My database learning process so far has been scattershot and lacking in direction, so to have this many moving parts and concepts working in concert with data I already understand is inexpressibly helpful.
This is the best approach because we have to show the latest payment for a particular tenant. It may be the case, you can avoid the where condition from the query if you want to avoid the payment period like as SELECT Tenant , MAX(c.period) AS Latest\_Period FROM dbo.TenantPaymentMaster Group by Tenant
[SQL Zoo](https://sqlzoo.net/)
no. performance is the thing, that mongo advertises, while simplicity being other (start server, connect, insert data - no other setup/ddl etc). the biggest problem is, that mongo is suitable for only very specific applications (for example quick gathering of non-relevant data, like logging user visits or smth). in real world you won't run away from relations, concurrency, transactions, security, etc. for good, reliable system ACID principle is a must. P.S. if you have 32bit platform - mongo is no-go: you'll be limited to version 3.2.20 (or something) with max 2gb of data and no collation-based sorting (for non-English speakers) and max document size of 16mb
Ok awesome thanks I’ll try
Yeah I’ll get this in a bit when I’m at the computer
I don’t think this is a simple join. But then again, that’s why I’m posting. I’ll put what a before and after data set will look like when I get to a computer 
livesql.oracle.com
updated above
Updated above with a before and after data set
[https://docs.microsoft.com/en-us/sql/relational-databases/import-export/import-data-from-excel-to-sql?view=sql-server-2017](https://docs.microsoft.com/en-us/sql/relational-databases/import-export/import-data-from-excel-to-sql?view=sql-server-2017)
Select [your-columns] into [your-table] From [the-other-table] Where [you-know...]
Thank you everyone, I will look into this more and test out a few practice scripts. I apologize as I am still new to SQL, we are running SQL Server 2014 Management Studio.
I think this is what I want. I can insert rows from the same table back into the table as new lines with this, right? It has an auto increment key so it'll automatically add that. How would I tell it to insert all those columns/rows but change everything in one column to something else? Like from NJ to KS?
I like u/cadpas' answer the best, but if it's a particular set of data that you can't quite query, and you just want to insert, I really recommend notepad ++ and use regex to put in proper notation. Then do a mass insert in the format of: **INSERT** **INTO** **table\_name (column\_list)** **VALUES** **(value\_list\_1),** **(value\_list\_2),** **...** **(value\_list\_n);**
Then do it in Excel and import a CSV.
 SELECT a.Batch, a.Qty, a.Expire, a.Price FROM dbo.Stock a INNER JOIN (SELECT Batch, Max(Expire) as Expire FROM dbo.Stock GROUP BY Batch) AS b ON a.Batch = b.Batch and a.Expire=b.Expire
You can use group by here Select batch, sum(qty), max(expire) group by batch And then use a self join to get the price
Yeah, the only easy way I can think to do this is with a loop. Can I ask why you're storing the data like that? Having the set of ranges really complicates the counting process. 
if you use the select \[\] into \[\], you actually gonna create a new table with the data you provided. i think you could use something like that, using the CASE \[column\] WHEN \['NJ'\] THEN \['KS'\] statement at the column you'd like to &amp;#x200B; EDIT: Bad image uploading [https://ibb.co/hgnhS91](https://ibb.co/hgnhS91)
 &gt; Whereas non destructive migrations can be easily rolled back without any issues. I don't think you've made a particularly useful classification. So, sure, there are some changes that remove data. Some changes don't. If we're making a "non-destructive" change, like adding a column, we're doing it for a reason: some part of the system wants to start using it. If we make the change, then data starts appearing in that column, we can't easily roll-back anymore because what was a non-destructive migration to apply is now a destructive migration to roll-back. You advise to always have a rollback script, but don't offer much advice about handling data loss that the rollback must require. 
Implement the TPC-C benchmark. Build the database, build some code to exercise it per the database specification. Run it on your server. Run it on another server. Compare and explain your results. Tune, iterate.
You're doing too much all at once. Take the SUM () off of it, wrap this in a sub-query, and sum it up in the outer sub-query. 
I removed the `sum()` from the subquery and `sum` the outer query and still spats the same error.
You removed it from the FULL OUTER JOIN too?
Was I not supposed to?
Yes, what are you trying to accomplish? There might be an easier way to do it. 
I need to first `count` the *ReturnYearFiled* multiple by 10 the count and then subtract it from the `sum` of *r.grossrevenue* THEN addition of `sum` of *ancrevenue*. sum((r.grossrevenue - ((count(ReturnYearFiled) * 10))) + sum(ancrevenue)) commission
it's coming from a feed like that. I have no choice
Ahhh gotcha. Is there any downside to have just using a cursor/loop?
They are essentially the same thing, depends on a lot of factors, readability, what you're doing, why you're doing it, how often you need to refer back to a sub-query in later logic, etc. Eventually if you use either they will become so complicated and inefficient that you will end up using #tables, or real tables. It just depends on your own preference and style, and there are certain limitations when using a CTE in a sproc, etc.
[hackerrank.com](https://hackerrank.com) 
A subquery as an in-line view vs. a CTE will be treated as the same thing by the execution plan as long as you're keeping it simple. If you were in a situation where you need to reuse the inline view multiple times, defactoring into a CTE is better for code organization because you can reference it easily without all the copying/pasting.
They are essentially the same, unless you refer to it multiple times or need recursion. Here is an article I wrote about how WITH can help you structuring your code (queries) for better readability: [https://modern-sql.com/use-case/literate-sql](https://modern-sql.com/use-case/literate-sql) &amp;#x200B;
Try this: https://www.mediafire.com/file/1kk0czossmslpn6/EASY\_WINS.sql/file 
I want to ask, how _do_ you think about these things, normally? To me, the following seems to be as straightforward as they come: Your output granularity is {TableA_Date}. Your output has data that comes from TableA and TableB -&gt; it's a join. You'll be joining granularities {TableA_Date} and {TableB_ID} by date (or a function of it) so there's no shared granularities, so the join output will be at {TableA_Date, TableB_ID}. Ok, so you'll need to group it. From your example it does seem like you dont want a simple date comparison but rather you want to ignore the year part (otherwise 2018-01-01 wouldn't fall between ID=4 2017-01-01 and 2017-01-01). So far: select a.[date], sum( b.cost / (datediff( d, b.startDate, b.endDate)+1)) as TotalCost from TableA a join TableB b on datepart( m, a.[date])*100 + datepart( d, a.[date]) between datepart( m, b.[startDate])*100 + datepart( d, b.[startDate]) and datepart( m, b.[endDate])*100 + datepart( d, b.[endDate]) group by a.[date] Ok, it does seem like you want to capture that into your TableA, so you'll need to write an update statement next.
Have you tried converting it in the Where clause?
If the data you want to insert is originating in Excel, I always just right click on the database and SQL Server management Studio and select "import data". It then opens a wizard kind of window that won't shoot through which tabs of the Excel sheet you want to import and if you want to do any Transformations on the data types kind of like in ssis. Then you run it and all the data is imported. Sometimes Excel and SQL Server argue about the data types so there may be some minor manipulation there. I use it all the time when I'm not using this I'm using the insert values method that somebody described above. Hope this helps a little
I dont want the last date (as in MAX(expire)) but the last record added to the table.
Use sqlite? 
Thanks so much man!! Appreciate you going the extra distance.
No problem. I couldn't tell exactly what you were trying to do, but something tells me there may still be an easier way to do this. Not sure though. 
What didn't work about it when you tried casting/converting to date? Did you get an error?
I think ROW_NUMBER() is probably your best bet, just use a random function (in MSSQL it's RAND()) in the ORDER BY statement. 
Thanks for the replies. One of my colleagues said I should have used a Subquery instead of a CTE for a couple of reasons that didn't seem to make much sense to me. 
Some people strongly prefer one over the other. A lot of it comes down to what they know better and has higher readability for the individual. 
I received output, but the data was not correct. Values greater than today's date, lots of legitimate values filtered out. 
Could you provide an example of a date that was not converted correctly? 
I'm trying to do the following and it's not really jumbling up the partition by section: ROW_NUMBER() OVER (PARTITION BY group ORDER BY floor(random() * (10-1+1) + 1)::int) Can I do this in one statement or do I need sub queries?
The following sql creates a temp table, inserts two dates (as varchar) and then does a select where the varchar date is greater than today's date. And it retrieves the 2019 date. If your date doesn't have the periods in it try adding them and see if that works. &amp;#x200B; CREATE TABLE #temp\_table ( id INT PRIMARY KEY, vcdate VARCHAR(50) ); &amp;#x200B; INSERT INTO #temp\_table VALUES (1,'2018.01.01'), (2,'2019.01.01'); &amp;#x200B; SELECT \* FROM #temp\_table WHERE vcdate &gt; getDate();
Right click the table in SSMS' Object Explorer and select the Edit rows option. Copy/paste from Excel into the last row of the edit view in SSMS. You can do multiple rows. You just have to make sure the columns in the source match the column layout in the table.
i don't think postgres has a "local / in-memory" execution model. on the Microsoft side of things, you mentioned SQL CE, which is now shifting more to LocalDB... you might be able to use that on Linux using .Net Core and maybe something like EF for .Net Core. on the OSS side, SQLite is probably your best option... others exist, but i don't think i'd specifically recommend them.
I'm not sure what you're achieving by using all the logic in the order by statement. Will it work if you just do ORDER BY RANDOM(). (Sorry, I'm more SQL Server/Oracle native.)
Is there actually a benefit to writing joins this way or is this just as asinine and unreadable as it looks? Yes and yes. The benefit is making the order of joins semi-explicit. The un-readability of it you can see for yourself :) So, conceptually, a join is a dataset operation that takes 3 things: leftDataSet, rightDataSet and a condition. The beginning of the operation is indicated either by "from" or a "join" keyword, the beginning of the third parameter is indicated by "on" keyword. Since the joins return datasets, you can have another join as a "parameter" and you can figure out syntactically what individual operations are by treating your join/on keywords as a sort of opening/closing parentheses. I.e. if you put parentheses into "from A right join B inner join C on c.x=b.x on b.y = a.y " it would read " from (A right join ( B inner join C on c.x = b.x) on b.y = a.y) and the inner join will execute first, prior to the right join.
If you move the ON statements to be underneath their corresponding JOINs, you should get the same result set. It was probably just his preferred syntax. 
&gt; If you were in a situation where you need to reuse the inline view multiple times, defactoring into a CTE is better for code organization For organization maybe but for performance, no (in the OP's case anyway). SQL Server (unlike Oracle or Postgres) does not materialize CTEs in to temp tables, so if you're referencing the same CTE twice in a query, that subquery will be executed twice. In that situation, you're better off refactoring that CTE/subquery out into a temp table and then query against that.
so the logic in the ORDER BY was just doing random() for an integer 1-10
In SQL Server, they're almost functionally equivalent; CTEs are syntactic sugar that will be converted into subqueries at runtime. Run it both ways and look at the execution plan. If performance is a concern, you may be better off with a temp table. Test to verify, as always.
 BULK INSERT tablename FROM 'filepath' WITH ( FIELDTERMINATOR = ',' , ROWTERMINATOR = '\n' , FIRSTROW = 2 ) To bulk insert from a csv file. If it's tab-delimted, change FIELDTERMINATOR to ='\t'
Just glancing at it, I'm not sure of a way to incorporate that directly inline. Generally I just use ORDER BY RAND() and let it be for random orders. 
https://www.fakespot.com/product/sql-performance-explained
Any chance you get get the database fixed so it's using the proper data type? Those type conversions are going to hurt your performance and waste space.
Is there a row Id on the original table? The other solutions are thinking that the last added record it the most recent date hence the max. 
In Postgresql you could just `sum(distinct x)`
That's a valid point. I think there is no simple way to handle it. I would probably lose the data and get it again if it is a less critical column (Like user bio, hobbies etc..). It would depend on the business requirement. Another way I could think of is to copy the values to another table (backup table) and can be restored once the new version has been successfully deployed. Do you have any ideas that you can share ? Thanks for your feedback.
Sub-queries more readable? It's the definition of spaghetti code. With CTEs you have clearly defined queries in sequence. With sub-queries you start a query, show which fields you want, then start another query, then without finishing it you start another. This concept should die in the mind of any developer the very moment when he has to count closing parentheses or when a developer realizes he might need the result again in another part. There's no excuse for it unless you're held prisoner somewhere and forced to work with MySQL 5
SELECT ... INTO is typically used to create a new table. If the target table already exists, you'll get an error. You'd want: INSERT INTO ... SELECT ... 
I have tested my query both ways but I don't have enough data to really be able to tell. The setup is very bespoke so recreating it with a larger set of data isn't the easiest thing.
Could you give an example of what the table looks like currently and what you want it to look like? 
You can tell from the execution plans.
find the length of the existing column, and update the 200 below to match -- add a new temp column ALTER TABLE MyTable ADD COLUMN MyNewColumn varchar(200); -- update new column based on existing one UPDATE MyTable set MyNewColumn = CASE WHEN MyExistingColumn = '65321.00' THEN 'etc' WHEN MyExistingColumn = '79900.00' THEN 'etc2' ELSE MyExistingColumn END; You can now run this repeatedly until MyNewColumn looks right, then run a final update of MyExistingColumn, and then drop MyNewColumn 
I've tested multiple times and always found temp tables to be more performant than sub queries or CTEs. Just my experience though, YMMV
Why not just use OPTION (FORCE ORDER) if execution order was important? 
It depends on the type of SQL you're doing. If you're gonna be working with SQL Server, use Microsoft Virtual Academy for videos. Also, books by Itzik Ben Gan on Querying Microsoft SQL Server are highly regarded.
I think CTEs are more readable because of the order, instead of reverse order in subqueries. I think the only real reason would be for recursion.
How about looking at documentation? https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF
Pluralsight
How about you actually contribute to the question! I've tried several methods with no result. 
Well, because option( force order) doesnt change the result - it only prevents changing order of operations where order doesn't matter functionally. In a way, it is similar to A + B * C - depending on where you put parentheses, the result will be different. Similarly, (A left join B) right join C could give you different results from A left join (B right join C). Here's an example: with A as ( select * from (values ( 1, 11), ( 2, 22), ( 3, NULL) ) a (a_id, b_id) ), B as ( select * from (values (11, 111), (22, 222) ) b (b_id, c_id) ), C as ( select * from (values (111), (222), (333) ) c (c_id) ) select a.a_id, b.b_id, c.c_id from a left join b right join c on c.c_id = b.c_id on a.b_id = b.b_id 
For only 60 rows, I usually just have Excel create the individual `INSERT` statements via formula and then paste those into SSMS to execute.
I disagree... pretty strongly. As always - readability ultimately comes down to personal preference so it's a silly thing to argue about, but sub-queries are entirely self-contained...you can't string along. By all means, use whichever you prefer and find more readable yourself and do what you do - but don't write one off as better than the other; they are functionally equivalent, and readability is entirely a personal rating.
This is actually not true - see the example i've posted.
I'm not saying they aren't self contained, but you can't read a main query contiguously. You have to stop reading it, and switch your brain to reading the sub-query, or jump over code to reach what remains of the query, which isn't easily visible unless you use indentation for each sub-query. But that way you end up with a ton of indentation. Sub-queries are literally the equivalent of JS pure lambda functions callback hell. It's called hell for a reason, and pretty much everyone agrees that it's hell. I think the only people who prefer the sub-queries are the ones who got used with that either from since before CTEs, or too much usage of MySQL or influence from MySQL coders. With minor exceptions.
For small amounts of data, I like writing insert statements in Excel using the concatenate function and then pasting those into ssms.
Instead of having a case statement, if you know the values of the numbers then why not have another table with the numbers and meaning in 2 separate columns and join to it. This will mean that if new values are added then you can just add to the table and not work on adding to the case statement? 
Interesting. 
Here's an interesting example I've created to show how that syntax works: CREATE TABLE #TmpStudent (StudentID int, StudentName varchar(100)) CREATE TABLE #TmpClass (ClassID int, ClassName varchar(100), ActiveClassFlag bit) CREATE TABLE #TmpStudentClass (StudentClassID int, StudentID int, ClassID int) INSERT INTO #TmpStudent (StudentID, StudentName) VALUES (1,'John Doe'),(2,'Jane Smith'),(3,'Ford Prefect') INSERT INTO #TmpClass (ClassID, ClassName, ActiveClassFlag) VALUES (100,'Intro to SQL',1),(101,'Advanced Chemistry',0),(102,'Hiding 101: How Not to be Seen',1) INSERT INTO #TmpStudentClass (StudentClassID, StudentID, ClassID) VALUES (500,1,101),(501,1,102),(502,2,102) SELECT SC.StudentClassID, S.StudentID, S.StudentName, C.ClassID, C.ClassName, C.ActiveClassFlag FROM #TmpStudent S LEFT JOIN #TmpStudentClass SC INNER JOIN #TmpClass C ON C.ClassID = SC.ClassID AND C.ActiveClassFlag = 1 ON SC.StudentID = S.StudentID DROP TABLE #TmpStudent DROP TABLE #TmpClass DROP TABLE #TmpStudentClass Using this query, you're basically saying "give me a list of all of the students, and then show me all of the classes that they're enrolled in where the class is active." It first does the INNER JOIN between #TmpStudentClass and #TmpClass. Two out of the three rows in #TmpStudentClass will come out of this: StudentClassID 501 and 502. StudentClassID 503 will be excluded due to the join criteria on C.ActiveClassFlag = 1. It then does a LEFT join from #TmpStudent with the result of the above join. #TmpStudent has three rows, and two out of the three rows have one match from the earlier INNER JOIN, so three rows are returned: StudentClassID 501, StudentID 1, ClassID 102 StudentClassID 502, StudentID 2, ClassID 102 StudentClassID NULL, StudentID 3, ClassID NULL This join syntax works, but like /u/ichp said, it's difficult to read. I always do a double-take when I see joins like this, and it definitely takes me longer to troubleshoot.
For something this small, this would be my solution as well. It takes about 3 minutes to create all of the insert statements. You can test the first one to make sure it works the way you want before doing the whole bunch. And (for me at least) I'm treading on familiar ground rather than trying to learn a new method. I think this would be approaching my threshold. If it started getting very much more than this number, I'd look at something less brute-force-ish.
Haha I learn that way the best too, having something functional and actually understanding how the functions and process works. You can also look of plenty of guides, like w3schools too, that will help expand your knowledge on SQL. 
Lag isn't in 2008
Yep, you're right. Haven't used 2008 in quite a while, so didn't even think of that. 
Too fucking real 
You could make the same argument about a CTE in a select - you have to break your train of thought to go inspect the CTE. They are literally equivalent - and comes to personal preference. I just want to point out how dramatic you're being - calling a functionally equivalent value literally hell, simply because you don't like it as much. You say they are "literally the equivalent of xxx", but do you not realize that CTEs are *literally* the equivalent of sub-queries? Honestly I'd almost never use either one and would stick to #temp tables, I think both of them are equally difficult to work with and inefficient. But to argue that one is better than the other (or that one is "literally hell") is ignorant, and silly. They are functionally equivalent.
I've always been a big fan of making the db do as much work on the data as possible. Just give me a nice, clean recordset please. &gt;Exploit the power of SQL to write less code because “the best code is the code that was never written”. If it is not written there is no need to maintain it I would add: The database is probably going to crunch and clean your data faster than anything you can write. Because that's what it was built to do.
thanks for the help. just got back from party, will try now. 
hmm i cant upload photos. the data i am getting is created from multiple data sources and but the field i am looking at is from a new area that doesnt have correct mapping yet. I want to do an ad hock analysis and correct some of the wonky stuff in the table...I am use to doing mostly front end but i have to get better at sql and putting together packages... and suggestions for a work/learn as you go guide? 
Agreed.
sqlite it shall be. thank you for your effort! 
I’m working with my own SQL Server at work now, meaning I have 100% control over the data, relationships, structure, etc. I use tableau for front end dashboards/reports and have learned how much more powerful tableau can be when you let SQL handle the data transformations and slow tableau to viz it. 
Can you teach our Tableau team that? We've tried to tell them, but they just don't seem to get it.
I was referring to callback hell. Google it. It's a thing. And it's basically the same, with the same arguments for and against. &gt;CTEs are literally the equivalent of sub-queries? That was not the point. The point is that they're literally completely different syntactically, which is what matters. Also there are more databases that implement the SQL standard that is CTE, and they behave differently. Also you can't join a sub-query twice without copying the entire thing again, which will execute again. Like I said, it's literally like unassigned lambda functions in JS that create the callback hell. Which makes them even functionally different. Sub-queries have the ability to become lateral queries. CTEs have the ability to be recursive. They're only smile in their most elementary use case. It's like saying that general relativity is literally the same thing as classical mechanics.
Thank you for the thoughtful reply! Greatly appreciated! This is making sense now!
Thank you so much! This something that's bugged us for years but I have not had the time to dive into. This helps me understand this much better.
To further your point here is what happens if you move the join up. Just for quick reference to others. The results ARE different. WITH A AS ( SELECT * FROM (VALUES(1, 11), (2, 22), (3, NULL) ) a (a_id, b_id) ), B AS ( SELECT * FROM (VALUES(11, 111), (22, 222) ) b (b_id, c_id) ), C AS ( SELECT * FROM (VALUES(111), (222), (333) ) c (c_id) ) SELECT a.a_id ,b.b_id ,c.c_id FROM a LEFT JOIN b RIGHT JOIN c ON c.c_id = b.c_id ON a.b_id = b.b_id; WITH A AS ( SELECT * FROM (VALUES(1, 11), (2, 22), (3, NULL) ) a (a_id, b_id) ), B AS ( SELECT * FROM (VALUES(11, 111), (22, 222) ) b (b_id, c_id) ), C AS ( SELECT * FROM (VALUES(111), (222), (333) ) c (c_id) ) SELECT a.a_id ,b.b_id ,c.c_id FROM a LEFT JOIN b ON a.b_id = b.b_id RIGHT JOIN c ON c.c_id = b.c_id
I’ve noticed it depends on what type of users they are. If they’re used to excel/access as backends, then you get messy data in and complex formulas out. 
As a follow up, does this actually accomplish something you couldn't through a more standard join with additional logic? I guess more, is this something I should actually learn and use or is this more of a thought experiment?
I asked teh same thing to /u/ichp but, does this actually accomplish something you couldn't through a more standard join with additional logic? I guess more, is this something I should actually learn and use or is this more of a thought experiment?
very misleading article in that it seems to suggest that `GROUP_CONCAT`, `CONCAT_WS`, and 'ON DUPLICATE KEY UPDATE` are features of SQL (example -- "Group concat is a powerful operation in SQL databases") whereas if you tried to use them in anything other than MySQL, you're in for a surprise 
This is the only right answer. You have this powerful data engine that can do math and transforms much better than your personal algorithm at your fingertips; use it. Coders just like to code, I guess.
there are not a lot of stuff that you cannot do in SQL actually. its very versatile. I tend to do EVERYTHING in SQL, including complicated stuff like multiple iterations or recursive stuff. there are only a few thins I need to do on a procedural language. and of course everything is super fast.
Another superficial blog post, but the point is very valid. I used to be against writing more SQL than necessary and writing it in the application code (C#) instead, but lately I've changed my mind. Modern OO code is just so cumbersome to deal with, not really because it's hard to write, but all the meta-programming is so annoying - source control, thousands of files, endless inheritance and interface hierarchies, branches, nuget/DLL references, build, deployment, ugh! SQL development (all the stuff surrounding the actual code) is just so simple in comparison. A good example of letting SQL do the work is if your company/project has a website that allows you to search through items, products, houses for sale or whatever. Let the website just display raw what comes from the database, including stuff like price calculations, sorting, paging, business rules, autosuggest, filtering, etc. It makes it so easy to change all the rules without having to deploy anything other than perhaps a single stored procedure.
Try telling our developers this.... The number of times I have told them, let the database do the work with the data... But noooo, they always want to write something extra to process tbe data as its 'faster' than sql.... Every piece I have rewritten when they have performance issues has resulted in greatly improved run times (quicker). It's literally what the database is there for! 
Firstly, thank you for the gold. Secondly. No, please dont learn to use this - this would be one of my taboos for the production environment. The regular way of doing it _( from A left join (select B.column, C.columnC from B join C on b.some = c.another)t on a.columnA = t.columnA)_ accomplishes the same thing, is easier to read and is easier to convert to CTEs if the subqueries become too long and/or too involved.
Here’s the thing though. The database can only do so much. When it comes to complex dashboards, it’s probably best to let tableau do the data acrobatics. It’s much much easier and manageable to perform certain calculations in tableau. Especially when you start getting to LOD calculations. However my motto has always been, pre-aggregate your data to the most granular level you will be visualizing at on the database end first. No need to pull in line level data when you know you’ll only be reporting at the monthly level.
Usually, yes. But some of it is better done on the viz side, imo. Order by is a big one for me.
Ok, so I'm really trying to take my SQL to the next level. I've spent 80% of my day in SQL (MS SQL) for the last 3.5 years and I would (hope) to call myself at least an intermidate with the language. Recursive CTEs, Dynamic SQL, Query optimization to an extent, are things I'm comfortable with though most of my time is doing data analysis and ETL. Is there any resource you would recommend that would help me get to that next level where I'm truly understanding how the engine is interpreting the code, how to better optimize and how to start understanding and using more advance techniques? I've always leaned towards Brent Ozar but his information, though amazing) isn't really setup to educate on SQL as a whole but more peice mealing knowledge together by solving specific problems. Any insight would be amazing! 
See for me, I like to put out a nice flat dataset to be rolled up, with all of the columns you'll need to validate it with detail. That way to can deliver the detail behind the roll up and it's the same consistent data
Except if you’re working with millions of records. You’re better off validating your dashboard by writing queries against the source database. Plus if your dashboard is wrong, you will never know if you’re just exporting the detail records that make up the dashboard, because the detail is most likely wrong too. Hope this makes sense. 
That is true. Our guys don't seem to get that you do as much work to the data as you possibly can *before* it goes into the tableau server, and that you should send the minimum amount possible. Nope. They'd rather send 10,000,000 rows that need crunching and cleaning every freaking time somebody opens the dashboard. Because that's easier than spending an hour or three figuring out how to shrink it by 50% or more. Especially if that means sacrificing even the smallest bit of questionably useful data on the dashboard.
Yea, the hardest, important, and most overlooked part about BI is prepping and cleaning the data. I think people get such a hard on just before creating a dashboard that they forget to really prepare the dataset to make it as useful as possible.
I'm not a fan of OOP. Objects are fantastic, but the idea that *everything* is an object and the program only works because they interact with each other, ugh. I'm a procedural guy.. The bit about the meta parts is really spot on. Things get crazy really fast. And I think part of the problem is that too many people are afraid to admit it's gotten crazy and needs to be simplified because they're afraid they'll just be seen as "not smart enough". So they just go along with it.
No I get it - it makes sense in some cases to deliver the roll up. But in the case of the detail being wrong - it's not always wrong that you have to worry about. The logic for that data might only exist in that one place, so sometimes it makes sense to deliver the detail too - or possibly put it into a datastore so that logic can be consistently applied to all reporting from that data. You don't want multiple developers trying to recreate the wheel every time
Recursive CTE if you're just cranking out near copies.
It sounds like you picked up a very denormalized means of storing data and now you're trying to get it into something better. What if you put each of the transaction types into a separate table, instead of referring to them as HHH, etc. in one big table?
I think this is the correct approach but i am using the glue and gum method. Once i get it done, i can come back and fix it.
&gt;clean your data faster than anything you can write. Because that's what it was built to do. That's an invalid argument. I do a lot of the data crunching in procedural languages outside of the DB just because it's faster and with a lower memory footprint. Depends on really what you want to do. Trivial joining and aggregation - sure, SQL wins. Building graphs from edges and parsing them - not really an SQL thing, even if you could write it in pure SQL.
Tableau data engine (Hyper) is one of the fastest database engines out there. It's only slow at taking in data.
You could write your own group_concat function in postgresql in a few rows of SQL. But why would you? group_concat is useless as it forces a string output that, unless you want a string output, is useless. If you want row-level aggregation of data without loss, then functions like `array_agg` are a much better design.
The answer is simpler than you think: Don't write code that smells bad. If you know that it's wrong, don't write it like that. Doesn't matter if it's SQL, C#, or anything else. Going overkill is so bad, that it can be best represented as a satire: Meet [FizzBuzz Enterprise Edition](https://github.com/EnterpriseQualityCoding/FizzBuzzEnterpriseEdition) Don't be the person that does this.
Then here's an interesting problem for you. It's easy to do in any procedural language. Do it in SQL: ``` packstation (id) cell (cell_id, packstation_id, cell_size FLOAT) parcel (parcel_id, packstation_id, parcel_size FLOAT) ``` You know these automated post stations that dispatch parcels to customers? That's a packstation. A cell is a place where a single parcel would go in. A packstation has N cells, each of different size. Let's say, for simplicity, that size is unidimensional. Each packstation receives a daily delivery of parcels that need to be distributed in the cells. For simplicity, let's assume that all the cells are free. You can't put a parcel in a cell that's smaller than the parcel. Task is to write the code that assigns a parcel_id to a cell_id in the most optimal way (most parcels fit, maximize the number of free bigger cells).
Alas, that seems almost how modern C# is written when I look at the stuff my fellow developers code. So much abstraction and unnecessary inheritance and decoupling, which is rarely actually used for anything. Part of my struggle is also how C# has evolved with really exotic syntax, and since I haven't been following the latest stuff since about version 3.0, it's hard to read. All those =&gt; and Task.Whatever() =&gt; everywhere, and goddamn Entity Framework which is a huge black box in itself. Don't worry, I would never write FizzBuzz Enterprise Edition myself, I prefer more simple "functional code" so to speak.
way later than it should've been, but MSSQL 2017 has STRING_AGG()... and simple string concatenation has been around for too long to remember. what this article LACKS however, is any discussion on the use of formulas on SEARCH CONDITIONS, since they can often murder your performance.
Can't do DISTINCT with STRING_AGG unfortunately.
well.. your code section, and description of requirements are both pretty abysmal to read... but if i understand you, the intention is to match a parcel into the smallest cell that will fit it. this is known as the bin packing problem, and it's a well covered topic... https://gertjans.home.xs4all.nl/sql/binpacking/intro.html has some good articles that compare several algorithms/approaches, with 40k assignments in 30 seconds. 
DISTINCT is just an alias for GROUP BY... so, ya know, group by... or subquery for the DISTINCT
Sometimes stuff like that happens in math. Just because it makes some theory work out better or maybe it just doesn't have a strict definition so this flavor of SQL defines it as 1. 
you turn it on and it scales right up
You didn't mention which RDBMS, but assuming some version of SQL Server, yes you can find the MDF and LDF files and connect them to a new installation of SQL Server. Now... stop running Win 2k3 and set up AND TEST backups of your data. 
That's what makes STRING_AGG awkward to use in those cases. Have to subquery with a DISTINCT first.
It was great! I unfortunately didn't arrive until a bit past 11 but I spoke with the vendors who were friendly and helpful. Afterwards, attended some classes and spoke with the speakers along with some classmates before/after the sessions. Thank you for checking up on me and also for organizing past events!
I mean, SQL is a subset of programming. The possibilities are greater if you leave the scope of only SQL. 
True, that may be it since it's so specific. 
The mechanisms you describe are pretty primitive, relying on human intervention and interaction to achieve goals. What's the deployment plan? Sounds like it goes like this: We make the DBA write some scripts, then we deploy the new code, then we beat-up the DBA if they don't work or we need to role back. Processes that rely on humans are slow, unreliable, and don't scale. Are we down -- pissing off customers and turning away money -- when the DBA is trying to reason out how get data into the backup table? A better solution involves decoupling the app from the database. Some people do this with views or stored procedures. That can help, but still has some trouble. Table schema versioning is a more advanced approach. Row schema versioning is even more aggressive. Once we start considering more advanced solutions, we reveal better classifications. Are we pursuing [state-driven database deployments, or migration-driven deployments](https://enterprisecraftsmanship.com/2015/08/18/state-vs-migration-driven-database-delivery/), for example? Then, we can think about how finely we manage the migrations (row- or table-level), how we isolate deployments or versions, and so on. I think the implication that one column is somehow less critical than another is a fallacy. If the application is executing "SELECT Salary, Hobbies FROM Table3", then it will fail to run if *either* "Salary" (sounds important!) *or* "Hobbies" (sounds less critical!) don't exist, or are the wrong datatype.
&gt; Correct me if I am wrong - but if you properly test your release process then nothing in the article is relevant. Ideally, sure. But that's a panacea. Humans write software, so it might have problems. How do we avoid problems? We write tests. But humans write tests, so they too won't be perfect. And won't catch everything. And not every deployment can be tested -- management eventually thinks that getting something fixed now is better than thoroughly testing it. Sometimes, that's actually right ... it's not pointy-haired thinking. I completely agree with your points, tho. The article seems very naieve about practice and doesn't deliver much insight into improvement.
 &gt; build things that you can really see No programmer is building anything anyone can see. They work on castles made of air, surrounded by moats of abstraction and attacked by dragons of fancy. Seriously. Let's try a thought experiment: Let's get two programmers of equal experience and similar background in a room. Have A explain to B what they did last week with sufficient detail that B can walk in as an adequate substitute for A first thing next Monday morning. How long does that conversation last? Three days? A week? A month? Does making B more experienced than A help, or hinder, the estimate of time? 
Its actually not that hard. Of course it all depends on how you build your schema and the way you store the data. But if we assume you have a table woth available cells, a table with how the cells are currently occupied (a connector table), a table with parcels (1..N with previous table), a table with parcels that beed to be stacked, a table with rules how to stack parcels in cells. From here on its a game of joins, order bys, a formula that ranks the available parcels with available space by the rules, probably a rank function dependent on that formula... You end up with a query, or a series of queries or nested queries that spit out a list of parcels to cells that need to be placed. To me its much easier to do in sql by the way rather than procedural.
No. Create a new table with transactions then use a view to see if the sum of the amounts outstanding minus transactions equals zero. If that's not possible create a job with SQL agent.
No no, I get that it seems easy. Trust me, it's not. Try to actually solve it. This question was on stackoverflow with a 250 point bonus and nobody was able to provide an answer. Ranking doesn't solve it.
That's not SQL. That's T-SQL, a procedural language. And it's slow. 30 seconds for that is abysmally slow. And in JS it's around 4 lines of code.
SQL is bound by the language to an extent in that it is declarative. You are making a query to get data out of the database, update it or put it in. You largely don't get a say in how the database executes that query in the background, that is dictated by how the database implements the SQL language spec. With programming you get full control over how your program executes. This means you have more freedom to do what you like. That said, really getting in depth with SQL can be very interesting. Understanding how the query is executing and how to tune it can be instrumental when you get the ORM generating some terrible query which is killing your app or the whole DB. 
TBH withh SQL the thing I really miss from other programming is decent source / version control
This will be an unpopular opinion, since this is /r/SQL, but I heavily disagree with this sentiment. Placing the application's business logic in the context of the database is bad practice. 1. It creates high coupling between the application and it's storage mechanism. 2. It obfuscates how the application works. The database is supposed to focus on the thing it does well: persistence. It should store data in the most efficient manner possible, allowing the application to query data from it in an equally efficient but unobtrusive manner. When the database starts changing the data, which the application has asked it to persist, it causes nothing but confusion and logical inconsistency. Having consistent logic is the applications primary purpose for existing, and thus it should handle this to it's full extent. [Stored procedures are commonly considered bad practice for a reason.](https://blog.codinghorror.com/who-needs-stored-procedures-anyways/) The benefits are tiny compared to the downsides.
I'm a data engineer that is one of the contributors of ideas and solutions in a family of companies spanning several countries. I don't have the illusion that I'm the best or anything, but I can read my results and they look good. 80% of the code I write (and it's a lot) is sql. Usually complex stuff that uses the maximum that the database engine can offer. I even went beyond and modified a custom extension myself to make it faster for my use case. And I agree with you. Regular applications should live in the main language of the application. SQL is just to get data from data structures, not to compute logic. Because it allows unit tests, debugging, easier understanding of the data structure for the developer if it's a strongly typed language. When I said 80% in SQL is because around 10% of the data manipulation is in Python. Because even with GIL toothache hampers the use of shared resources and makes a lot of the used memory redundant, it's faster, and easier to read, because at it's heart it's procedural logic. Writing it in sql, though possible in most cases, is like fitting a square peg in a round hole. Plus python as a language is faster than database side procedural languages that extend SQL. It's only slow part is the start up time. And Python is slower than Java unless you're using numba.jit with native types. And Java is slower than C or Rust if you know what you're doing. Which makes procedural SQL extensions the slowest if there bunch, that are fast only on start up, and on data access because of the proximity to data.
Can you mount the windows drive using linux? If so, you could try to get the DB files into another machine.
SQL IS programming. There is a lot more to it than people think. Most videos on YouTube are beginner tutorials. You’ll never see a video of a Dba explaining how to optimize certain things specific to the business requirements. SQL is just another tool that specifically deals with data persistence, which is something all applications need one way or the other. Whether you actually use that specific tool depends on your requirements. 
Because the vast majority of developers will rarely need anything but the trivial SQL queries. Plus all you get is just data. Cool stuff in programming means a bit more than just data.
Heidi is a GUI. It doesn't manage a database or database files. No offense, but if this is a critical data loss issue, hand it over to someone who knows what they're doing. Watch and learn, but don't risk data for that.
Bucking the trend here and going to say the reason is stupidity. Analytics is an emerging niche skill that heavily depends on SQL skills as it applies to statistical programming. While true that languages such as R, or Python, or other things like Hadoop, blah, blah, blah, are all better, sleeker, faster, and more appropriate... The fact is that the world runs on databases and being able to work in SQL is a hugely in demand skill. At the end of the day it depends on where you want to take your career, but if you want to do anything remotely DBA/analytics related then I would think SQL is the single most important technology to learn aside from basic statistics. From there going to SAS, SPSS, R, Python, etc., is fairly simple. You can do anything in SQL. The idea that it's going anywhere in our lives is not realistic.
i do sql on macos at home. i installed sql inside a docker container and then connected with VScode. Still wish i had ssms, but oh well. You could also use vmware, parallels, or bootcamp to run windows on your mac. 
I've found practice databases on github. forgot the name, but its only a google away.
Yeah that’s more what I was implying. LOD functions are phenomenal. I also tend to manipulate the data in tableau directly to see what I actually want, then will take it back to the db to see what I can achieve through a better table or view to improve performance. I focus a lot on end user experience- no one wants to sit for 1-2 minutes and wait for data to load every time they change a filter. 
thanks for your reply, I didn't think you could use bootcamp on an external ssd reason for this is because my main MacBook has limited space. &amp;#x200B;
Sounds like you did it right!
That's not really your analyst's fault though. Management is telling them they need 20 filters that get used maybe once a quarter if that plus the ability to drill down with all the extra dimensions at that level. Makes me scream when they complain it's slow and they insisted on a bunch of garbage being added in. 
Per Wikipedia: &gt;A programming language is a formal language, which comprises a set of instructions used to produce various kinds of output. No specification is made about what other stages those instructions have to go through. SQL (at lest T-SQL) is Turing-complete, and it is a set of instructions to produce output. You're arguing semantics about *how* those instructions are interpreted; we could say that any language that isn't Assembly is "bound by the language" and it's not programming either. Lego Mindstorms, Scratch, LOGO - these are all programming too. &gt;You largely don't get a say in how the database executes that query in the background, that is dictated by how the database implements the SQL language spec. With programming you get full control over how your program executes Is this *really* true anymore? Between JIT compilers, bytecode, optimizing compilers, and speculative execution &amp; other tricks modern CPUs do, your code has been transmogrified at least twice before the output gets produced. SQL *is* programming, it's just a different kind of programming.
The whole article is dumb. The solution is a financial details table with a single amount column where the sign of each transaction matches its affect on the balance sheet. Add in a few dozen dimensions and have the table sit at the lowest level possible and now it's just a simple sum and group by for almost anything. Finding ending balance as of a particular date is as simple as adding a filter for &lt;= date and summing the amount. 
Have you tried Azure Data Studio? It's a lightweight version focusing on query-building. A bit like what VS code is compared to Visual Studio.
Look into last_value and first_value to get the price. The rest you should be able to get with a group by. You say you want the most recent transaction (not based on the date field) but do any of your fields tell us that?
download postgresql and postico
In my course "business and data analysis with SQL" I take you through how to install MySQL and use the example Sakila database installed with MySQL. To get you started, here is a half price voucher for my course. https://www.udemy.com/business-and-data-analysis-with-sql/?couponCode=WHAT_DATA24
In your experience, how does that scale?
DBeaver and forget you ever had a need for any other software for managing databases. It's the last program you'll ever look for. As for databases to play with - download and import the open data sets either from developer surveys on stackoverflow, github usage data (see ether example project for Mara - github.com/mara), NOAA weather stations measurements, NY cab rides, etc. There are so many open datasets that you can ticket with...
i have not, i’ll check that out! thanks. 
Ssms is free. I downloaded it from the ms website at home the other day. 
I agree with what you said about DBeaver, I like it a lot.. They should improve on the Themes though, the Dark theme's are not that great!
Are there courses explaining about the query execution process at a block level? (For ex, how search is done using indexes?)
Yes, I explain how the SQL execution plan works as well as index performance on tables throughout the course. 
i reject the premise of your question
And you can create a localdb on your machine for querying 
I concur, definitely try a view for this. 
Business objects, tableau, access, excel, vba. Don't even use sql that much now lol, but having that 2 years sql on my resume opened a lot of doors.
is it the visual studio clone? it maybe is good for querying but not overall functionality like management and if you are a developer. i hardly met anyone who only analyses data in t sql. most of the people who only do analysis on the data tend to be in the reporting end.
I've used it and liked it, but I'm quite used to VS Code and its exactly like that. I can see it being the future instead of SSMS.
&gt; Azure Data Studio offers a modern editor experience for managing data across multiple sources with fast intellisense, code snippets, source control integration, and an integrated terminal. Azure Data Studio is engineered with the data platform user in mind, with built-in charting of query result-sets and customizable dashboards. &gt; Azure Data Studio is complementary to SQL Server Management Studio with experiences around query editing and data development, while SQL Server Management Studio still offers the broadest range of administrative functions, and remains the flagship tool for platform management tasks. https://azure.microsoft.com/en-us/updates/azure-data-studio-is-now-available/ Im talking about this for the specific purpose of querying and pulling data. It sounds like it is not meant for the DBA type functionality and is designed for quick querying for the analyst, which is what makes me interested. 
I think I used this at the beginning of the year. I think it is similar to VS code and I liked the dark mode but I would switch to it once I see an extra benefit. Maybe with SQL 2019 they would release some exciting features.
Stored procedures are considered a bad practice by people who don't understand databases. For people like me, they are easier to understand and maintain than having the logic scattered (and often duplicated) across a dozen microservices because the developer couldn't figure out a simple table function. 
What features did you like about it? 
Azure Data Studio has a long way to go before it catches up to the features in SSMS. I suspect Microsoft created ADS for two primary reasons. The first is to be platform independent. The second is because the amount of technical debt buried in SSMS must be enormous and not one wants to get near that code. This is a major reason why Entity Framework was rewritten from the ground up. I would guess SSIS suffers from the same fate. &amp;#x200B; I think ADS can be a great product someday as long as Microsoft keeps pushing it forward and doesn't let a whole slew of summer interns work on it. I have both installed but I spend almost 100% of my time in SSMS. &amp;#x200B;
That is not the problem.
I have the beta Sql Ops Studio installed and have used it. I also use SSMS for SQL and VS Code as a daily driver text editor. I like Ops Studio. There is potential, especially with extensions, but it's hasn't gotten there yet. Whenever I open something up with it, I end up back in SSMS.
Just for writing Select statements it's nice but is missing so many features compared to SSMS it's gonna be a no go for many years
Hi, you can install all free software a download a practice database (which you can alter in any way). I describe it on my starting blog all with links to Microsoft download pages. http://www.straightforwardsql.com/database-playground/ 
Its nowhere near SSMS, and its a shame its tsql focused instead of sql agnostic as it would be great to use with postgres/oracle/mysql. Calling it "Azure" is a serious misbranding, imo. The promise of this application is that it has a lot of the benefits of what vscode has (lighter footprint, customization), but is specifically sql focused. 
The easiest way to think of a LEFT/RIGHT OUTER JOIN is that you're taking everything on "the LEFT/RIGHT side" of the join. In this case, you're taking everything from the \[product\] table and joining it to \[SpecialOfferProduct\] table. Because it's not a &lt;insert directional here&gt; outer join, you're going to get every record back from the \[product\] table regardless of if it has a match in the \[SpecialOfferProduct\] table. If it does have a match, then'll you see the match in the \[SpecialOfferProduct\].\[SpecialOfferID\] field that you currently don't have SELECT'ed. If the product ID does not have a match in the \[SpecialOfferProduct\] table, then the \[SpecialOfferProduct\].\[SpecialOfferID\] field will just be NULL as there is no data. This is why you can write your WHERE statement where you're looking for SpecialOfferID IS NULL. Essentially, you're saying "return all the records from \[product\] table that don't have a corresponding match *(based on \[product\_id\])* in the \[SpecialOfferProduct\] table". Hope that makes sense.
 EDIT:: How do I fix this to not look like complete garbage? &gt;Copy and paste your code into the address bar and then copy and paste it again. &gt; &gt;It'll remove all the special formatting.
So, your issue is you've got a query you believe should be returning results, but it's returning 0 records? Did I understand this correctly?
As an analyst I have to write queries and build datasets each day of the week. So I use real data and answer real questions. I can imagine that as an analyst you should have access to some sort of database. why not use that?
Why can't you use WHILE?
I think all I am trying to say is it's a declarative language with a specific purpose which is interacting with a database. It's not a general purpose programming language which is able to do anything from web serving to desktop apps. I'm definitely not saying it isn't programming
&gt; Because it's not a &lt;insert directional here&gt; outer join, you're going to get every record back from the \[product\] table regardless of if it has a match in the \[SpecialOfferProduct\] table. I believe this should read because it IS a left outer join etc.. &amp;#x200B; The other thing to note is that these aren't imaginary, just temporary, and the columns can be made use of until the query ends, which is helpful to understand when you're making subqueries. This query could be used to join to a table that holds customer ids or something. 
Yep, didn't catch that when glancing back over and saving. Good catch. 
On the first problem, when I try it with the AND it returns no results. When I try it with the OR the results are not the desired results because its returning records that have a cost lower than Heidi. On the second problem the query is returning nothing.
Can you provide more info on the table? When you say "career stats", what do you mean? Individual records or a summary?
Can I use WHILE in a basic loop? All the homework says is REDO that loop above (which I had made in the previous homework problem) in a basic loop structure.
Right! I did mean temporary but imaginary'' came up at the time because I wasn't seeing the columns with possible NULL values.
Your opinion is popular with me. It's far easier to scale a business logic layer, or a front end than it is to scale a database server. If you're doing string manipulation on data then you're storing it suboptimally. 
It's a pretty vague question, and I'm not entirely sure what answer they were wanting from you. But lets assume the "4 non-relational data sets" are just csv files. For any sort of detailed reporting, you would want to import them into flat staging tables, and then write some routines to load them from there into normalised tables, where you can query them effectively. 
Does invencopy table have a primary key? You don't need to use COST &gt;, but instead look up the primary key so that it's \[other\_field\] &gt;.
I added a primary key to it after reading your comment (itemno). This resulted in giving me all CH. Doesn't there need to be a cost parameter in the subquery so I can compare to cost of Heidi? SQL&gt; SELECT * FROM invencopy 2 WHERE itemclass IN 3 (SELECT itemclass 4 FROM invencopy 5 WHERE itemclass = 'CH') 6 OR 7 itemno &gt; 8 (SELECT itemno 9 FROM invencopy 10 WHERE itemname = 'HEIDI');
You need to add the cost part back into the WHERE statement below the HEIDI part to only get the primary keys you need. 
You need to add the cost part back into the statement below the HEIDI part to only return the itemno that you need. 
Excel VBA is a perfectly functional programming language... /s
I believe they were 4 tables - say customer, product, sales, order - but for some reason, its not inside of a relational database. Your answer would still apply? Can you also briefly explain what you mean by "writing up routines to tidy up any problem" / what are flat staging tables?
By Ops you mean Azure data studio right? Are there any use cases it is currently better than SSMS at or is it just totally inferior? 
By Ops Studio, I mean SQL Operations Studio, what Azure Data Studio was called in pre-release. The version I've used most recently is probably a few months old. There was a cool backup size chart I was able to pull out of it. There aren't really any other ways I can think of it was beneficial. It's so young compared to SSMS. I know it will functionally and usably overtake SSMS eventually, but not in the next few years.
I use SSMS for more management type tasks but my go to for writing queries and working with data is DataGrip. The auto complete is superior to everything else. If you spend hours everyday writing SQL and learn the shortcuts your productivity will skyrocket.
Honestly when I’m reviewing candidates (for developers/analysts), I don’t really look for certifications. I look for any projects they’ve completed and what platforms they’ve worked with. When I’m interviewing a DBA candidate, Certifications are more important for production DBA concepts/methodologies like Availability groups, Failover clusters, Disaster recovery, etc. The technical screening/interview will give me a very good idea of the skill level more than any cert will. And if you do get a cert, ensure that you keep practicing the concept of the cert long after you get it since you’ll be expected to hit the ground running in that area of expertise. If you have a cert in a concept and can’t do a technical/working interview for it, you’ll get some negative points.
Thank you. Not on my PC right now but do you mean adding in as a separate OR or right into the second sub query? 
I really like it. It very easily exports grids to Excel, its customization is fuller, it's readier at startup (my connections in folders right there), "script as alter" doesn't sometimes throw the whole procedure in quotes, and its Intellisense is faster and more thorough. Drawbacks: the output window (how many rows retrieved, etc.) is too big and gets in the way, haven't figured out how to make the font of the connection tree smaller, and the "command palette" isn't as easy as a tree when you don't know what you're looking for. Also, it doesn't have the same pure management features that SMSS has, so I just use Azure Data Studio for TSQL work.
Not knowing all the info I’d think you would be best to use a composite key with date and time stamp. Especially if it’s a large data set.
I believe he means by running cron jobs on that, which is a programmatical way to tell the server taking certain set of actions (cleaning data, apply mapping to the data, load it into destination tables, etc) The script of the cron jobs can be written in many language, for me it would be Python as I also plan to do some data science stuff in the future
there are 2 datasets actually, 1 generated from Google Firebase and another one generated from Google Form. As both of these 2 products do not generate Primary Key by default, I picked `timestamp` to use as a key instead
I actually think the correct answer is more foundational data cleaning and preparation question. If I were presented with 4 datasets that were non-relational, I would first examine each dataset to determine the characteristics of each. I'd be looking for data types, missing fields, things I could standardize. I'd then do all that cleanup in a program like R Studio or Excel, THEN load it into a table as suggested. Once that was done, I'd start looking for common data objects in each dataset - addresses, emails, things that can be used to identify individuals. I'd use those to build identifiers into each table.
Do you have a plan for when you do run into duplicate primary keys?
Basically just doing `count(distinct timestamp)` on it for a dashboard building/reporting purpose, nth more at the moment. On top of everything I could use `count(*)` as well but due to the nested data structure in Bigquery I think it is a safer option to go for `count(distinct timestamp)` instead
best design is last one you posted all the others lead to cumbersome, verbose, hard-to-maintain SQL, thgat probably won't perform well
The last option for structuring your data makes the most sense - one row per stock, per day. If you're stuck with your table design though you'll want to use an UNPIVOT query to transform on the fly to your third option and go from there. The exact method varies a little between DB platforms and you haven't told us which you're using. 
Awesome answer, and very clear, thank you!
[removed]
Thats something I was told early on and actually got me into my career now; certificates are for DBAs, not for general SQL users. For many reasons. I was a data analyst and wanted to become a DBA. The cert was the stepping stone.
You can’t rely on timestamp being unique in every case, so shouldn’t use it as a PK. Can’t you use a sequence or UUID for PK?
For every case indeed, it was more like a hack or quick fix. I just wanna know what sort of problem should I expect for this fix
Thank you for the input. I agree with what you said. Nonetheless, I'd like to discover the worthy certifications available for a SQL Developer.
You will at some point get duplicate values. If you get duplicate PK values you won’t be able to insert the dupes. 
The goal of a PK is for the values to have meaning and be used to identify a record. If you don't have a unique column, what about a concatenation of several columns? What is the data? From Google Forms, is it a survey or something?
I’ve been a Sr. dBA for a few years now and have yet to obtain a Cert. I just keep training and watching videos. Pluralsight, dba stack exchange and BrentOzar have been my best friends over the years. I’m considering going down the Data asciende path, but I specialize in ETL/Architecture and I really enjoy it.
No problem. Just remember that the first step is always just like any other profession or project - it starts with looking at the stuff and figuring out what you have. Every decision and piece of code you make will flow from that.
For sure, go ahead and get those carts if it’s something that interests you. Just don’t let them hold you back from applying for a job.
Why so? Is it because of the insert operation depends on the PK?
A primary key has to be unique. If it’s not unique, the inserts of the duplicate values will fail. 
Do you really need a primary key? My point is, if your source data doesn't have a unique PK, nor any obvious, true column to choose as the PK, I don't see why you should add one.
 I like your thinking, concatenation would do the trick. &gt; What is the data? From Google Forms, is it a survey or something? Yes, one of the dataset source is a survey. And Google Form would automatically capture the `timestamp` when the form is being filled. And I am currently using that data point as PK. &amp;#x200B;
Yes for sure. So the table has these columns: - Result ID (PK): Unique ID for each result - Fight ID (FK): ID for the fight (fight ID 1 could have anywhere from 2-4 Result ID’s attached to it depending how many people were in the fight - Fighter Name (FK): Name of the Fighter - Decision: W or L, depending on if they won or lost. So for example there could be: ResultID - 1 FightID - 1 Fighter name - Mario Decision - W ResultID - 2 FightID - 1 Fighter name - Luigi Decision - L The Career Stats query/view that I made is a list of their totals individual records. So Mario could have, say, 75 wins and 50 losses total and it would show in that query.
Were they asking about joins and normalization then? Also I’m not that great at sql just was wondering.
I think you're assuming that the data is already in a database, at which point joins and normalization would come into play. But it's easy to forget that most data doesn't exist in a database, or at least not a database a client controls. I see folks who have files they've downloaded from various systems they've used over years along with metadata they've tracked in Excel or quickbooks. This data can BECOME a database, sure, but it doesn't exist like that in nature. You have to clean it, establish the table definitions, them mount it in your chosen environment. THEN you start normalizing and joining based on the data you have.
Could you give examples of the information that could be in the tables? I'm having a hard time visualizing what you mean by stock 1 and stock 2. 
If you don't care about 100% accuracy of a `count(distinct` operation, and 99% would do just fine, have a look at HyperLogLog. It's a cardinality estimator for sets. Ultra fast and low memory and storage usage.
So they’re asking to create the database in general? To look at the information given and create a database out of it?
Timestamps contain more bytes (8 bytes) of data than is probably needed. This will eventually contribute to disk and processing constraints. I doubt this would be your problem. I really dislike PK timestamps due to the eventual collision and general awkwardness joining on datetimes. I can say the same for GUIDs (16 bytes). GUIDs will be inserted randomly into the clustered (physically sorted) index, causing file fragmentation issues. Because the data is in no particular order, there is not a quick way to seek to a particular record without additional indexing. Guids have their place, but most of the time it will be overkill for what it's worth. Integer sequences are 4 bytes, guaranteed to be unique on a single server, and will index nicely. 
It's hard to know without knowing more context, but I tend to work from the assumption that the customer's implementation is an un-technical as possible
You're not comparing apples to apples. Writing SQL code and writing say C# code is night and day different. Generally someone writes SQL code to retrieve, or store, data from / to a data store. The SQL code you write is usually part of a larger context or goal such as business intelligence, reporting, analytics etc. Traditional programming is much broader and can be virtually unlimited in scope and purpose. The first 20 years of my career was as a software developer, mostly C#. Yes, it was cool. The last six to seven years I've been an Azure data architect and business intelligence. I write a lot of SQL but it's almost always within the context of wanting to accomplish something much larger such as building a data model for a Power BI report. Therefore, you really cannot compare the two the way you are attempting to do so.
Thanks for the suggestion. I changed each table to the third option like you and another redditor mentioned. So far I've got everything working except for selecting/deselecting duplicates. Here's my query so far: SELECT name, value, COUNT(\*) AS duplicates FROM new_volume WHERE value != 0 AND date &gt; 20181208 GROUP BY name, value ORDER BY COUNT(*) DESC I want to try to add AND duplicates &lt; 2 in the where clause but it throws an error. I tried HAVING duplicates &lt; 2 but also fails. Any ideas? I'm using SQLite (DB Browser) because I didn't think it would make a difference but this needs to run on Microsoft SQL Server. so I'll download the MS version to test it 
Well thanks for the response! I’ll consider myself not too far off with my initial guess since the questions pretty vague lol.
cannot be less than 510 (a-&gt;b), cannot be 512 (you'd have a-&gt;c then). So 510 or 511 - both should be possible. 
A primary key can be composite.
 SELECT REGEXP_EXTRACT(column, '.*(MODEL.*)',1) You may have to play around with the index...
I will use CTEs for adhoc analysis only. sub-queries are easier to put into ETLs, so I generally avoid CTEs. YMMV. Both CTEs &amp; subqueries (aka in line tables) have horrible performance.
sqlplus, so your platform should be Oracle. Therefore, case-sensitive string comparisons. See if correcting for this that resolves your issue.
You were really close: To select the duplicates: SELECT name, value, COUNT(*) AS duplicates FROM new_volume WHERE value != 0 AND date &gt; 20181208 GROUP BY name, value HAVING COUNT(*) &gt; 1 The easier way to ignore duplicates is to use SELECT DISTINCT: SELECT DISTINCT name, value FROM new_volume This seems a little odd given your dataset - generally I'd imagine from your dataset that if a stock was x price, changed the next day, and returned to x that you'd want to count it as being x value twice?
Sorry for the ugly code. I'm lazy and on my phone. Off the top of my head, I'd try to chunk it out with some CTEs/temp tables/table variables. First get your +50% fighters. You could try to use your career stats query as it is, but I might try something like: Select F.Fighter From Fighter F Join ( SELECT Cast(Count(ResultId) AS FLOAT) 'Wins' FROM Results r1 Where r1.fighter = f.fighter AND Decision = 'W' ) w Join ( SELECT Cast(Count(ResultId) AS FLOAT) 'Total' FROM Results r2 Where r2.fighter = f.fighter ) t WHERE w.Wins/t.total &gt; .5 Stick that result set into your preferred temporary storage solution and then join it to your "OtherFighter" bit in the original query. You might want to try adapting that approach to the originals as well. It will make them easier to read, if nothing else, and would improve performance, though probably not by a ton of there aren't many records in Results to begin with.
[https://en.wikipedia.org/wiki/First\_normal\_form](https://en.wikipedia.org/wiki/First_normal_form)
sure, but it’s windows only.....not sure what your point was. 
&gt; But I would like to know what sort of potential problem would this hack cause in the long run? Timestamps are miserable as primary keys, although many of the real caveats come from having a default constraint of the current timestamp. 1. You're essentially creating a system rule that two things can never happen simultaneously. That's a terrible rule because it doesn't represent reality. In order to scale your applications will now need to be coded to automatically retry INSERTs when they fail due to primary key violations. 2. You can't insert multiple records with one statement. Ever. No INSERT INTO ... SELECT. No SELECT VALUES (),(),(). Everything is one row at a time. That's maybe fine for organically created data in an infrequently used system, but if you need to batch insert data or generate data on the fly, it's a huge pain in the ass. 3. As soon as you assign a datetime to something, someone will attach a meaning to it. There will *always* be a business rule that will require the data to be modified as soon as you apply a meaning to it. Like you'll have the datetime mean that it's when something specific has happened, and suddenly there will be a business rule that will require that that datetime change when something happens. Now you've got to look for duplicates before you can update, modify all values in all relevant tables, etc. Now you have a fixed number of records that can be created for a given day, and you will need to split hairs because the key is supposed to identify uniqueness and identify something like order of occurrence. 4. Foreign keys get really nasty because you've got to specify the entire value to the microsecond, which may cause problems if different programs have different precision with microseconds (hint: most do). There's no "close enough" anymore because they're all unique values. I would strongly, *strongly* recommend a simple auto-increment surrogate key. Just create plain INT or BIGINT with whatever auto-increment your RDBMS supports. That's the proper "hack" or "quick fix". **You are not going to run out of values with a BIGINT.** If you insert 1 billion records every second, it will take over 292 *years* of continuous inserts to exhaust the positive signed 64-bit number range, and *the keys alone* will require 64 exabytes (65,536 petabytes) of data storage. Just for the keys. If that's really still a problem, then consider using a UUID as your primary key (128-bit integer with ~122 bits of variance), although if you do that you need to consider what you want your clustering key to be. 
How did you answer the question to your interviewer? Did you get the job? Curious 
group by will do a select distinct for you...
That's what I mean - same thing, much shorter syntax.
Not really sure what they were looking for in this questions... But I would have answered with some kind of ETL process to bring all of the data into one database. 
Does nosql play a part in this potentially? I don't know much at all,haven't got that far in my learning, but isn't nosql for non-relational datasets?
It is, but you'd need to ask someone more conversant in NoSQL to get their take on it. I understand the tech, but I've been a relational guy all my life and I'm not sure if they'd approach the problem differently. I do know that if you wanted to do a graph, which is something that would rely on NoSQL, then you'd still need to go through the same initial steps to classify your data into nodes and vertices to define your relationships. 
Is it well normalized? Or is the data all over the place, duplicated, out of place in some tables, not making a ton of sense? 
I don't know if it's possibly, but I'm not sure why you'd want to create the foreign key before creating the parent/child tables. Seems a little like putting the cart before the horse. 
If you remove the AVG() function from your query, what result set is returned? 
I am really sorry I am going to remove this for now I think I may be making a mistake here somewhere. 
Alright. 
I'm not sure there would be any use for a primary key, unless you split the tables apart. 
That was my thought. It’s aggregate data for the unit columns. The SiteId and DateId are foreign keys, but I can’t think of any use to refer to a PK in the units table from anywhere else. 
Because SQLite doesn't support the ADD CONSTRAINT variant of the ALTER TABLE command, so a foreign key cannot be added after a table is created. Looking at [this Stack Overflow answer](https://stackoverflow.com/questions/17984116/sqlite-foreign-key), though, "SQLite will interpret the REFERENCES clauses only when needed, so there is no problem creating a reference to a table that does not yet exist." https://stackoverflow.com/questions/17984116/sqlite-foreign-key
Interesting. Didn't know that was a limitation. 
Interesting. Didn't know that was a limitation of SQLite. 
Ok thanks!
I am going to spend some more time on it, I might post again tomorrow if I can;t crack it.
Are you asking if the SQL instance is setup well with the database? (Dba) Or are you asking if it is setup well from a data perspective to its application? (SQL Developer)
If you want the count distinct to always work, just add in an index column that auto increments with each row
In my current case performance isn't an issue because there isn't enough data to make a difference. But I am trying to be forward thinking, in that in the future I will probably have to implement the same thing on a larger scale and this will matter. Would temp tables be the answer here for better performance?
1. see is the resource utilization is within limits, CPU, ram etc. 2. check the job run times. see which takes the most time and why so 3. a clear distinction between staging and mart tables 4. find out the large tables and see if data duplication is there, unused/duplicate indexes are there etc. 5. find out how much the data is used. in that way you can remove stuff that you don't need. 6. find the long-running queries and see why they are taking time 7. proper backup or disaster management is in place or not 8. see how the data is accessed by downstream users and if the objects they access are read optimized. basically writes are less frequent on those tables. 9. documentation on important stuff like architecture diagram. these are that comes to my mind.
disagree respectfully, my dude PRIMARY KEY ( SiteId , DateId )
That's not an identity though. I could see the use for a unique constraint on the table, just not an identity integer. 
normalized != good db
ah. sorry, sir. I didn't know it's a bad move to ask the experts on this subreddit. 
If they have a preprod environment, they are setup well. If they have preprod and load testing, they are setup really well. 
I have access but only at work. I wanted a solution where I can practise at home with example datasets.
thanks will check it out.
You need to ask two questions Does it efficiently meet the needs of the use case it was designed to solve? Are the optimizations that would make it better fit that use case, and are worth the effort it would take to implement them?
I'd use nice words like 'Data warehouse', 'data dictionary', 'etl', staging'
Before the new record gets inserted, the primary key aka timestamp is tallied up with the existing data. If there is already a timestamp with same value, the new record gets rejected as primary key violation. 
Work experience.
good one. As a Frontend Developer, it ain't easy for me.
I've built several similar tables where I included unique IDs and have never once found a use for them. Don't bother with them and use the space on indexing instead.
There's a big difference between cleaning data and transforming it into graphs. The fact that SQL doesn't build graphs doesn't make the argument that it is the right choice for cleaning the data invalid. The sense of the original argument is that it does a better job of doing its job than other tools. It's a matter of knowing and using the right tool for the job in question.
 SQL server and Oracle both have free versions of their engines you can download and run on your own machine. Oracle even has a web version called LiveSQL.Oracle.com that is self contained in the cloud. You can build a full stack application on your machine that connects a front-end to a middle tier and a DB for experience.
Could you create a trigger which would insert a stubbing record into the Teams table when there is an insert to Members and the team_id value doesn't already exist in Teams?
I am but a humble SQL user, so I don't need too many features. I just liked that I knew where everything was and the shortcuts for some things because I'm a VS Code user.
Your last point is the most important one. Putting everything data relalated on SQL is a bad rule of thumb.
You're allowing anyone to run arbitrary SQL from how you're bringing that value into your code. Always used prepared statements! http://php.net/manual/en/mysqli.prepare.php http://php.net/manual/en/security.database.sql-injection.php https://stackoverflow.com/questions/60174/how-can-i-prevent-sql-injection-in-php
I'm still new to this so I'm not the best with reading anything above an entry level of terms. If im only using GET to direct to three pages (mens, female and gear) does it matter if they can change that variable?
&gt; I probably should've clarified that a little better agreed
I can pass in `?page="female";DROP ALL TABLES;` https://xkcd.com/327/
If you really are just starting out, I recommend just skipping ahead to using Laravel. [https://laravel.com/](https://laravel.com/) It has countless problems that you have not even created yet solved and baked in. Here is a place that the Internet recommends to start for that: [https://laracasts.com/series/laravel-5-fundamentals](https://laracasts.com/series/laravel-5-fundamentals) &amp;#x200B; If you are not going to do that, and for some reason feel very strongly that you do not want to use a framework and instead code in raw PHP... then I recommend you read "the only proper PDO tutorial" which details the right way to access a MySQL database. [https://phpdelusions.net/pdo](https://phpdelusions.net/pdo) Then I would read about SQL injection [https://phpdelusions.net/sql\_injection](https://phpdelusions.net/sql_injection) Which should detail how to properly handle GET and POST content... &amp;#x200B; Generally, if you are trying to use "mysqli\_fetch\_assoc" that means that you are using the wrong generation of PHP code, one that is likely to go away eventually. You need to be using PDO to future proof your code. &amp;#x200B; HTH, \-FT
Yup that’s the plan, 99%accuracy is good enough for the use case 
Is name in an index or primary key? Is "filter_table" an actual table or a subquery? Can also just run both over a large data set and see which performs better, check out the execution plan.
1. name is an index. 2. filter_table is an actual table (containing only two rows). 3. I was curious why will the execution plan differ in the two cases ( assuming point and 1 and 2). 
I see, interesting 
I don't think in this context they will perform any differently. Though if name is not in an index in the filter table or their might be more in operators down the road, it could go either way and would be up for speculation per query. 
 Method 1 uses only one table. Method 2 can be slower if the related table has lots of locking contention or is remote. Comparing two SQL statements (without including the name of the RDBMS hosting them, or the underyling physical or logical storage, or ...) doesn't usually lead to any meaningful information. A SQL statement describes semantics of a query. The query optimizer and execution engine are allowed to do whatever they want to do as long as they satisfy the semantic meaning of the query. Seems like a possible answer to your question is that, if there is a meaningful difference, there's something really wrong with your RDBMS. 
Temp tables w/indexes will have better performance.
Good point. There's a lot more to a good database design than normalization, including scalability. 
Here is the complete table creating and insert logic in python if that helps at all thank you again: import sqlite3 con = sqlite3.connect('test.sqlite3') cur = con.cursor() cur.execute("""CREATE TABLE IF NOT EXISTS journal (id integer PRIMARY KEY, title text NOT NULL); """) cur.execute("""INSERT INTO journal (title) VALUES ('hey');""") cur.execute("""INSERT INTO journal (title) VALUES ('ho');""") cur.execute("""INSERT INTO journal (title) VALUES ('foo');""") con.commit() cur.execute("""CREATE TABLE IF NOT EXISTS review (id integer PRIMARY KEY, rating integer, journal_id integer NOT NULL, FOREIGN KEY (journal_id) REFERENCES journal (id));""") cur.execute("""INSERT INTO review (rating, journal_id) VALUES (1, 1);""") cur.execute("""INSERT INTO review (rating, journal_id) VALUES (2, 1);""") cur.execute("""INSERT INTO review (rating, journal_id) VALUES (3, 1);""") con.commit() cur.execute("SELECT journal.title, AVG(review.rating) FROM journal LEFT JOIN review ON journal.id = review.journal_id;") print(cur.fetchall()) con.close() 
dude, you forgot your GROUP BY clause you get only one row in the results when you do that
You'd be hard pressed to find any noticeable performance difference between these two queries with the examples you've given. Possible exceptions are if the filter table is constantly being updated There is one scenario I can think of though... Let's say you have 400M rows in table1, and you want to filter out 50000 names. If you are running the method1 query from middleware (ie, a website script, or client software), then it means you have to transmit the entire list of 4000 names to the sql server, each and every time you run the query. This means more network traffic (inevitably increasing response time), and the sql server potentially has more work to do in evaluating the query, especially if it does not support cached query plans Contrast this to method 2, where the query is identical each time (even when there are 4000 rows in filter_table). 
If you are working with Microsoft SQL Server, check out the First Responder Kit by Brent Ozar Unlimited. Run sp_Blitz, and you'll be able to get a decent starting point. 
&gt; its not inside of a relational database I don't know what this means. Are you saying the 4 tables are in a database, but the tables have not been normalised? Regardless I think my advice still stands. &gt; what are flat staging tables So to get the data into nice relational tables, you will need some sort of import process which works from the raw data. You could write this in scripting or software, which loops over the raw data, and then builds some clean relational dataset to insert. Alternatively just load all the ugly data into 'staging' tables (which are nothing more than tables holding each field) so you can query them from SQL. That way its much easier to just write your import process via a series of SQL queries (which cleaning up NULL or invalid values, tidy up any character encoding issues, de-duping, massaging data etc). Once thats all done, you can just run a few queries to insert from your staging tables into your clean tables and the job is done 
I should probably change my name to sql_noob. I have it working now, but after some googling I think I now understand that you generally have to use GROUP BY along with the aggregation functions like AVG(). Is that understanding correct? Thank yo . 
yes, that's correct, unless you want an overall average for the entire table if you want averages by X, you have to include X in both the SELECT clause and the GROUP BY clause
Makes sense thanks again
I think this is what you were getting at. I got some help from someone on Stack Overflow. ;with cteTestData /*this will not be necessary in the production query, as this CTE will be replaced with the actual source table*/ as ( SELECT 1 as ID, '2018-12-06 07:03:27.000' as ContactStartTime, (Left('00:04:28',2) * 3600 + substring('00:04:28', 4,2) * 60 + substring('00:04:28', 7,2)) as Duration UNION ALL SELECT 2 as ID, '2018-12-06 07:03:32.000' as ContactStartTime, (Left('00:14:28',2) * 3600 + substring('00:14:28', 4,2) * 60 + substring('00:14:28', 7,2)) as Duration UNION ALL SELECT 3 as ID, '2018-12-06 07:08:12.000' as ContactStartTime, (Left('00:10:03',2) * 3600 + substring('00:10:03', 4,2) * 60 + substring('00:10:03', 7,2)) as Duration UNION ALL SELECT 4 as ID, '2018-12-06 07:14:59.000' as ContactStartTime, (Left('00:02:58',2) * 3600 + substring('00:02:58', 4,2) * 60 + substring('00:02:58', 7,2)) as Duration UNION ALL SELECT 5 as ID, '2018-12-06 07:15:01.000' as ContactStartTime, (Left('00:15:47',2) * 3600 + substring('00:15:47', 4,2) * 60 + substring('00:15:47', 7,2)) as Duration UNION ALL SELECT 6 as ID, '2018-12-06 07:15:12.000' as ContactStartTime, (Left('00:08:18',2) * 3600 + substring('00:08:18', 4,2) * 60 + substring('00:08:18', 7,2)) as Duration UNION ALL SELECT 7 as ID, '2018-12-06 07:18:50.000' as ContactStartTime, (Left('00:10:22',2) * 3600 + substring('00:10:22', 4,2) * 60 + substring('00:10:22', 7,2)) as Duration UNION ALL SELECT 8 as ID, '2018-12-06 07:20:05.000' as ContactStartTime, (Left('00:03:11',2) * 3600 + substring('00:03:11', 4,2) * 60 + substring('00:03:11', 7,2)) as Duration UNION ALL SELECT 9 as ID, '2018-12-06 07:29:32.000' as ContactStartTime, (Left('00:32:53',2) * 3600 + substring('00:32:53', 4,2) * 60 + substring('00:32:53', 7,2)) as Duration UNION ALL SELECT 10 as ID, '2018-12-06 07:35:17.000' as ContactStartTime, (Left('00:07:37',2) * 3600 + substring('00:07:37', 4,2) * 60 + substring('00:07:37', 7,2)) as Duration ), cteIntervalNumbers as /*tally table to generate a number for each interval*/ ( SELECT num = 1 union all SELECT num = num + 1 FROM cteIntervalNumbers WHERE num &lt; 99 ) , cteTimes as /*CTE to calculate ContactEndTime, ContactStartInterval, ContactEndInterval*/ ( SELECT ID, ContactStartTime , Duration , DATEADD(second, Duration, ContactStartTime) AS ContactEndTime , DATEADD(minute, (DATEDIFF( minute, 0, ContactStartTime) / 15) * 15, 0) AS ContactStartInterval , DATEADD(minute, (DATEDIFF( minute, 0, DATEADD(second, Duration, ContactStartTime)) / 15) * 15 + 15, 0) AS ContactEndInterval FROM cteTestData /*this will be the source table*/ ) SELECT IntervalStart, count(DISTINCT ID) as Contacts, SUM(DurationInterval) as TalkTime FROM ( SELECT * , CASE /*all of the time exists in the interval; just find the difference in start and end, which will be the entire duration*/ WHEN ContactStartTime &gt;= IntervalStart AND ContactStartTime &lt; IntervalEnd AND ContactEndTime &gt;= IntervalStart AND ContactEndTime &lt; IntervalEnd THEN DATEDIFF(second, ContactStartTime, ContactEndTime) /*contact carries over into next interval; get time between the start time and the end of that interval*/ WHEN ContactStartTime &gt;= IntervalStart AND ContactStartTime &lt; IntervalEnd AND ContactEndTime &gt; IntervalEnd THEN DATEDIFF(second, ContactStartTime, IntervalEnd) /*this will get the elapsed time in the carry over intervals where the contact ends within that interval*/ WHEN ContactStartTime &lt; IntervalStart AND ContactEndTime &gt;= IntervalStart AND ContactEndTime &lt; IntervalEnd THEN DATEDIFF(second, IntervalStart, ContactEndTime) /*this is for all intervals where the contact neither starts nor ends i.e. where the full elapsed time of the interval is needed*/ ELSE DATEDIFF(second, IntervalStart, IntervalEnd) END AS DurationInterval FROM cteTimes d CROSS JOIN cteIntervalNumbers n CROSS APPLY /*this calcualtes the start and end time for each interval that the contact crossed*/ ( SELECT DATEADD(minute, (num - 1) * 15, ContactStartInterval) AS IntervalStart ,DATEADD(minute, num * 15, ContactStartInterval) AS IntervalEnd ) i /*only show intervals crossed by each contact*/ WHERE n.num &lt;= datediff(minute, ContactStartInterval, ContactEndInterval) / 15 /*how many intervals does the contact cross?*/ ) rawdata GROUP BY IntervalStart &amp;#x200B;
hey one other part to the question was "how would you ensure the client that the information is good" or "how would you verify the accuracy of your data?" - i wasn't sure how to answer this either. 
Well, you'd put your values from the second subquery in their separate fields and just select nulls into the columns of the first query. like this : ... (select null as "Sender First Name", ... A better question is does this bring you the output you were asked for and how did you get to your current solution?
The result of that is https://i.imgur.com/gIW20mH.png . This was the first time that I finally have a second column with Receiver First &amp; Last Name as the title lol. Now I'm just not sure how I would make it so the null values are not showing.
Can you not alias the JOIN twice, once for the sender and once for the receiver? That way you wouldn't need to UNION JOIN person AS sender ON sender.person_id = message.sender_id JOIN person AS receiver ON receiver.person_id = message.receiver_id Then add the fields for sender.first\_name, sender.last\_name, receiver.first\_name and receiver.last\_name.
Thank you so much. 
so many hours finally solved lol
No problem, good luck on the project :)
It's not that simple. You should read The Data Warehousing Toolkit from Ralph Kimball. At least first few chapters.
Purchased, Thank You.
You might be able to get MySQL for free. Good luck with the interview 
Thanks. I did try MySQL. I couldn’t figure out how to get my data sets uploaded. Any tips or tricks?
MySQL, PostGRE, MariaDB, MS SQL (Developer or Express), Oracle Express.
I’ve tried PostGRE, and I keep getting stuck with the Stack application, and I can’t get anything else to run. Will try MariaDB. Thank you 😊 
Glad you were able to solve it. Would you mind showing how you wrote query and the result?
I would say SQL Developer
I believe what you are looking for doesn’t exist. This would have to be manually documented by who ever did built the DW. What you can do is look at the ETL (Extract, Transform, Load) to see how the DW table was built. Building a DW is not a feature, it’s a process that takes design and an ETL to build. As far as keywords, data lineage would be the term to see how a field or table get modified over time.
Learn R.
Most Common databases have a free version for community/educational/personal use. So you can basically use any flavor. In my work experience MSSQL and MySQL are the most commonly used (MariaDB and PostgreSQL are pretty similar to MySQL with varying features). If you want something kind of "self contained" you can try "[DB Browser for SQLite](https://sqlitebrowser.org/)" but be warned it has some significant differences from common enterprise level databases. In terms of how to "load a dataset", it really depends on what format it's coming in on. Most database tool sets will have the ability to import flat files like CSVs, some use proprietary backups or depending on what you have going on it may just be a SQL file that create the tables and inserts the demo data. So... I'd base my decision on what database to use on what format their example comes in on and what database the company your applying for uses. You'd want to demonstrate competence in a technology they specifically use if possible. If you're in a crunch for time, Google is your friend, just look up what formatt they are using and how to import it into the respective database.
You want R or pandas (Python). 
imo ms sql server dev. &amp;#x200B;
one thing you can try is, transactional systems are written optimized (one clue would be heavy normalization) whereas mart should be read optimized for downstream users (denormalized).
thank you man. 
You could always downloading SQL Server Express and load your data sets in there through management studio. https://www.microsoft.com/en-us/sql-server/sql-server-editions-express https://docs.microsoft.com/en-us/sql/ssms/download-sql-server-management-studio-ssms?view=sql-server-2017
To answer the first question: If the client is bringing me the data to analyze, it's fundamentally not my problem and they shouldn't ask me to comment on quality (unless they're prepared for an answer they won't like). I can analyze gaps in the data, I can perform linear regression to try and fill missing values, but it doesn't matter whether or not the data is good - it's what we have to work with. To answer the second question: quality is determined by the consistency and frequency of the data collection, so I would ask the client about their data collection practices. How was this data generated, and over what period of time? What is the standard for data entry? Who handles it? Has there been a change in standards, data collection systems, or data entry personnel over time? 
LOAD DATA INFILE is the command you want. Also, Google.
You could get something basic by doing a LOT of work with functions and stored procedures but R does a lot of the basics already.
HeidiSQL
Query: SELECT sender.first_name AS "Sender First Name", sender.last_name AS "Sender Last Name", receiver.first_name AS "Receiver First Name", receiver.last_name AS "Receiver Last Name", message.message_id, message.message, message.send_datetime AS "Message Timestamp" FROM message JOIN person AS sender ON sender.person_id = message.sender_id JOIN person AS receiver ON receiver.person_id = message.receiver_id WHERE message.sender_id = 1; Result: https://i.imgur.com/PtqtOjS.png
You're awesome! Thanks for taking the time to reply!
No problem! Just curious, are you taking a similar course currently? 
Got it, thanks a lot.
Ok thank you. Just realized that temp tables can't be used in views, so i think a CTE or a sub query is the best I can get with what I am doing. 
Following code seems to work but it's definetly not the cleanest SQL I have written, there must be a better way: ```sql with test_data as ( select to_date('2018-01-01', 'YYYY-MM-DD') as date, 1 as user_id, 'Sarah' as user, 'black' as status_1, 'green' as status_2, 'Elizabeth' as owner union all select to_date('2018-01-06', 'YYYY-MM-DD') as date, 1 as user_id, 'Sarah' as user, 'red' as status_1, 'yellow' as status_2, 'Elizabeth' as owner union all select to_date('2018-02-02', 'YYYY-MM-DD') as date, 2 as user_id, 'Bob' as user, 'blue' as status_1, 'red' as status_2, 'Sarah' as owner union all select to_date('2018-03-19', 'YYYY-MM-DD') as date, 1 as user_id, 'Sarah' as user, 'yellow' as status_1, 'orange' as status_2, 'Elizabeth' as owner ), ranked_data as ( select *, /* generate row number for users based on date, most recent one has 1 etc. */ row_number() over (partition by user_id order by date desc) as user_date_rank from test_data ), user_with_most_recent_parent_rank as ( select user_data.user_id, /* Smallest date rank -&gt; most recent date / parent */ min(parent_data.user_date_rank) as most_recent_parent_date_rank from ranked_data as user_data left join ranked_data as parent_data on user_data.owner = parent_data.user /* Only older parents than current user */ and parent_data.date &lt;= user_data.date group by user_data.user_id ) select user_data.date, user_data.user_id, user_data.status_2 as child_new_status, parent_data.user_id as parent, parent_data.status_2 as parent_status from ranked_data as user_data join user_with_most_recent_parent_rank on user_data.user_id = user_with_most_recent_parent_rank.user_id left join ranked_data as parent_data on user_data.owner = parent_data.user and parent_data.user_date_rank = user_with_most_recent_parent_rank.most_recent_parent_date_rank where /* most recent users */ user_data.user_date_rank = 1 ``` date | user_id | child_new_status | parent | parent_status --- | --- | --- | --- | --- 2018-02-02 | 2 | red | 1 | yellow 2018-03-19 | 1 | orange | (null) | (null)
Whaf does your clothes table look like? Is it just a bunch of lines that all have size and color data?
Your second way to do it is the correct way. As you add more "tags", you're going to start adding more and more columns to that table, and indexing starts becoming problematic. Or you'll have a "tag type" and "tag value" column, which puts you into the neighborhood of an Entity-Attribute-Value, which is [can be problematic if not handled carefully](https://sqlblog.org/2009/11/19/what-is-so-bad-about-eav-anyway)
Why not use Analysis Services? You got there few Data Mining options. All part of SQL Server.
Hm, not sure that I get it. &gt; I want the query to retrieve the new status of the user, as well as the last status of the owner at the time of the status change. Because Sarah is owner and you want previous record where Sarah is user. What would you expect if in previous record Sarah was also owner? Anyway, for previous status I would use LAG function definitely.
Classic example of schema on write vs schema on read. Having a separate table for each attribute is easier to query and more explicit. Having a tags table doesn't require modifying the schema when you implement a new attribute. However, it makes it more difficult to query. Neither is correct or incorrect. If you are likely to add a lot of new attributes, then using a single table will probably be a goo solution. then, you can create views to separate them if needed. Otherwise, one table per attribute is likely a good solution. 
If you go the tags-table route, you can have three columns in the tags table. id, value, tag_type. 
also easiest on any ORM that may exist on top of the database
You just described the EAV pattern.
Very cool. I always call it key-value. Good to know there is an understood name. 
Don't over normalize your data. If all you're doing is moving one field into another table, and that new table will only be storing that one field and a key, there's no reason to move it. All that does is mandate a join that makes writing queries more annoying and adds to the time it takes them to run. If there is a good reason to normalize on those particular fields, they're unrelated so I'd use separate tables for them.
If you were the one that gave me gold, then thank you! That was very kind! I can't think of a scenario where I would need to use this weird JOIN syntax that I couldn't accomplish via more standard methods. I created the above scenario as a thought exercise more than anything -- you could get the same results in a much more readable query by doing this: SELECT TMP.StudentClassID, S.StudentID, S.StudentName, TMP.ClassID, TMP.ClassName, TMP.ActiveClassFlag FROM #TmpStudent S LEFT JOIN ( SELECT SC.StudentClassId, SC.StudentID, C.ClassID, C.ClassName, C.ActiveClassFlag FROM #TmpStudentClass SC INNER JOIN #TmpClass C ON C.ClassID = SC.ClassID AND C.ActiveClassFlag = 1 ) TMP ON S.StudentID = TMP.StudentID Glad it could help! :)
Here's another post with the same (ish) homework: https://www.reddit.com/r/SQL/comments/a4qa9z/carrysum_duration_through_time_intervals/ 
SELECT sum(cnt) FROM tablename GROUP BY str
well, that's not really going to produce anything useful -- you'll get a column of counts with no idea which `str` they belong to
define "next row" -- remember, there is no sequence to the rows in a relational database, it has to be based entirely on data values
SELECT sum(cnt) as Count, str FROM tablename GROUP BY str
Let's say yes. Although I would like to use this tags table for other similar 'tag-like' properties in other tables.
I've never wanted an ugly sweater until this very moment.
Thank you for that link, it was an excellent read. After digesting it, I'm actually fairly confident that my 'tags' approach is correct. We are not trying to pile data in there, it is serving as a generic string table, and I foresee it might have the bonus function of helping with localization.
Seriously. Where can we buy the sweater?
Should probably throw a LOWER in there. Might miss some results if SQL Clause's DB entries aren't formatted the same.
My thought was that in exchange for this normalization I get constant sized rows in most of my tables, integer lookups (if I need to find all widgets that have a particular property, it is an int instead of a string), and one place to localize strings (if that becomes relevant). &amp;#x200B; Does this still sound silly?
Apparently SQL Clause is bad developer. He told you he made a table that he is sorting twice in a query with no order by clause. SQL Clause is coming to town to ask "Why doesn't this work?" /pedantic rage 
He's sorting the table, not the query output. 
~That guy~
No. It isn't.
"squirrel" as in Microsoft Squirrel Server 
Should probably be using a code table too to prevent issues with data entry. 'nice' should be a number that corresponds to the 'nice' entry. unless of course you can trust the omnipotence of the list to always enter the correct string in all cases. For some reason I feel like 'christmas magic' isn't any more reliable than users.
&gt; constant sized rows Sounds silly &gt;integer lookups Looking up the integer value you need for your query will get really old, really fast. Performance gains, if any, will be negligible and cancelled out by the join you needed to bring the actual value you want to look at into your output. &gt;one place to localize strings Don't know what you mean by that. &gt;(if that becomes relevant) General Rule Of Programming: Solve the problem you have, not the problem you think you might have someday.
I actually looked up a custom sweater website and am considering this very thing lol
I messed around on a site called Customized Girl (they have mens stuff too) and came up with a decent version using their templates. I kinda want to see if I can get an actual knit sweater made though. Thinking about seeing if there's someone on etsy who would knit one. 
SQL Clause is ~~coming to town coming to r/SQL to ask "Why doesn't this work?" Fixed that for ya lol
Could be a derived table. Maybe he's using an old school method to get median.
Which is even more terrifying.
Thanks for the feedback! On the joins: the tags table changes infrequently and will generally be cached in application memory. Is it still not worth it?
A brave topicstarter a sql joke dares He checks duplicates and gets 'overflow' He runs select count(*) from who_cares He's lucky - In his result set there is no row
I wouldn’t do any certification off of your own dollar. All of SQL can be self taught and unless you are applying for a DBA or Senior BI role vast experience with SQL isn’t required. Most jobs will require you to know the basics and go from there and learn on the job 
Whether or not it's worth it is up to you. Personally, if all I'm doing is getting rid of one column that's unlikely to ever have extra info associated with it, I just leave it be. I just don't see the benefit for all the extra work it's going to cost.
probably built a clustered index and forgot one of the keys he needed
The problem is I'm trying to do a PHP code for that query, so I don't really know what column belongs to a specific word so I would have to look up on every column like so: Select idItem, count(idItem) FROM items WHERE nameItem LIKE '%hammer%' OR colorItem LIKE '%hammer%' AND nameItem LIKE '%red%' OR colorItem LIKE '%red%'; That's why I wanted to use Full Text Search, but I'm having trouble getting it to recognize things that aren't words or are too short, like "9.2 in". I wonder if there is a way to combine a full-text search with a LIKE query specifically for the measurementItem column.
Really? You're just going to use a varchar for behavior?
Got it figured out. Thank you so much for the help :). 
Thank you! I am curious about the knit sweater... if you find someone, I’ll buy one for both of us!
Thank you! I am curious about the knit sweater... if you find someone, I’ll buy one for both of us!
It's also my understanding that the client wants to identify the 'naughty' contacts. I know I didn't put that into the original Acceptance Criteria, but could you adjust the query to include these entries?
I've had to do something similar before to this. What I ended up doing is: Numbering the rows of the table, ensuring that it is ordered properly when I numbered it. So in your case, it looks like you would sort by id and then value. The ROW\_NUMBER() function would work for this. Throw that into a temporary table so that you now have 4 columns in the table. Now you can join the temporary table to itself, but join on rownumber = rownumber - 1 and id = id. This will get it linked up how you want. Now you can check to see if the trigger column is met for the next row since essentially you have it all in one line. 
Contacts could still be a view and the underlying base table uses better logic.
You can download and use SSMS for free, at least for a while
You can try this: https://www.sentryone.com/plan-explorer
This. Otherwise .... well.... reading the xml. I'd try plan-explorer first I gotta say. With 48MB, my guess is you got a lot of scalar function in there, so you might actually have a chance reading the xml.
Some are a bit incorrect. Like oracle doesnt have update with joins. It does it is just called merge. Or that oracle doesnt have information schema. It does, its just the all_XXX views. Or oracle doesnt have time and date types, yes it does its just part of date.
LOL, kevin?
I would just import tables and relink. Would probably have to create relationships and table validation rules and what not in Management Studio. Express should be fine, just make sure you’re getting periodic backups.
I have a chain of sprocs that look like this: 1. Global 001 2. Staging Controller 002 3. Blah blah 003 4. etc. So Global 001 executes and then based on a variety of variables the process can either execute sequentially from 001 all the way to 043, or it can run an incremental load such as 001, 015-043, or 001, 018, 019, 043. I could theoretically test each single sproc between 1 and 43 and try to optimize each one individually, but I am currently looking at things as a whole and the %'s are rather interesting. Some of the high %'s are for processes that should execute simultaneously, which is leading me to believe a table is locked or something.
I try to pronounce it without vowels, which sounds more like "SQIL"
look into not exists condition
 INNER JOIN NaughtyStatus ns ON customers.NaughtyOrNiceId = ns.Id Doesn't sound as catchy 
thank you!
I know some basic R, have done some regression analysis but not sure hat method and library is relevant here. If you have any suggestions please share. Thank You!!
 I know some basic R, have done some regression analysis but not sure hat method and library is relevant here. If you have any suggestions please share. Thank You!!
 I know some basic R, have done some regression analysis but not sure hat method and library is relevant here. If you have any suggestions please share. Thank You!!
&gt; I tried to build Decision Tree model in SSAS but couldn't deploy it to server as my company has not configured any server for Multi-dimensional models. Hence I am trying to find a way to create regression or some model in SQL Server.
Thanks! I think I got it because of your recommendation! I did test a little in my test environment but just posting the reply to see if you spot anything wrong here, although it does produce the expected behavior: INSERT INTO employee_type (employee_id, employee_category_id, id) ( SELECT DISTINCT ON (employee_id) employee_id, 2, nextval('employee_type_sequence') FROM employee_type et WHERE NOT EXISTS ( SELECT 1 FROM employee_type et_sub WHERE et_sub.employee_id = et.employee_id AND employee_category_id = 2 ) ) I'm using the DISTINCT clause because there are multiple records with other categories. Basically I'm just looking for an employee id once where the conditions apply. I assume this is the right way. Thanks again for your suggestion. I'm not a DB guy but more a general systems dude. Your aim has helped fire me in the right direction. Much appreciated.
It's very likely a bad idea. It's an anti-pattern, and you shouldn't go duplicating data across a relational database needlessly. IF this was purely a reporting database rather than an actual transactional database AND IF there were issues in the existing structure that caused performance issues that couldn't be solved by proper indexing (very unlikely that you're dealing with that volume and complexity of data), then I could just about maybe accept that duplicating data is an unfortunate but neccesary evil. If you are doing this because JOINs are syntactically scary, then yes it's a bad idea. 
Well, the correct model is completely subjective to your data. For regressions I usually just start with linear which comes stock in R - lm(). The statistics hole runs very deep though. lm_ &lt;- lm(y_var ~ x_var, data=dataframe) summary(lm_) predict(lm_) I found this website to be helpful for learning this stuff: http://r-statistics.co/Linear-Regression.html R's lm() documentation: https://stat.ethz.ch/R-manual/R-devel/library/stats/html/predict.lm.html
[http://sqlfiddle.com/](http://sqlfiddle.com/) &amp;#x200B; Knock yourself out.
I had a professor once who wouldn't let us say "Microsoft Sequel Server", because "Sequel" is a language. We had to say "S. Q. L. Server". I get his point, but what a turd.
It’s a bad idea. Having transactional data in duplicate locations like that is just inviting your data to get out of sync between the tables. 
ok i got it ... thanks! So basically JOIN is common &amp; natural in the sql world ... If I meet performance issue ... I should learn to use index and maybe VIEW contcept ...
&gt; So basically JOIN is common &amp; natural in the sql world by george, i think he's got it
Think I found it: https://www.xonot.com/product/sql-clause-sweatshirt/
Take a look at [window functions](https://docs.microsoft.com/en-us/sql/t-sql/queries/select-over-clause-transact-sql?view=sql-server-2017). 
The only certifications worth getting are the ones from the platforms vendors themselves - Oracle and Microsoft. And even then, a lot of folks don't bother.
Medium task answer is incorrect? At least in TSQL \-&gt; isnull(null,1.1) returns 1.1 \-&gt; coalesce(null,1.1) returns 1.1 but they mark the answer as 1 1.1 &amp;#x200B;
just make it a Bit column you pleb! Where Behavior_Nice = 1
[I've discussed certs before](https://jonshaulis.com/index.php/2018/07/25/why-and-who-should-become-certified/), but the consensus is that it's a nice to have but many people don't bother. In the end, it should be about you being the kind of person who continues to learn and grow and it's less about the paper certificate itself.
you forgot the vital sql keyword `FROM` also, `CLIENTS` should not be in single quotes also, `DATEADD` is not a valid function in MySQL
[removed]
Thanks for the idea, worked perfectly.
typo on no2 you typed SELECCT for starters
[removed]
[**Got same design sweater from here**](http://td2.todaydeal.club)
Thanks for coming back to comment. I thought maybe you hadn't even read it. 
thanks for noticing but i checked again and this is still not the issue. 
If you have the images, you could put them on an ftp server and have the link be thebimage attribute in the database
"Why isn't the code returning the same name as the actor's who's Id is 8." Well, because it's not what the code says it should do. &amp;nbsp; A question to ask yourself, i guess, is what were you trying to do with your current code? Understandably, the final goal is get to the ultimate answer, but what was the logical sequence of data set operations that you were trying to achieve? And while there are multiple ways to get the same result set via queries, an obvious question is - why not more or less directly translate the english to SQL?
I have something similar currently developed for my process.
While it's _possible_ to store binary data (including images) in most RDBMs using the appropriate data type (`varbinary` for example), it's generally discouraged in favor of storing a path or URL to the image instead. Which is what you've landed on.
Most databases have some kind of support for binary/BLOB stores... if push really came to shove, you can always base64-encode any arbitrary binary and shove it into a string/text field, assuming the text field allows that length. But general best practice is to not put BLOBs into a traditional DB because can lead to various performance issues (various issues w/ indexing, disk optimization, locks, etc), small thumbnails (&lt;1MBish) are usually not too bad. People typically instead store string URIs that point to where binary files are.
It is possible.
I agree, this does seem possible.
That begs the question, if its possible to store an image in a database, could you store something like an executable package?
If it's just a few logos you can store them in the db without any issues. If it's hundreds of gigabytes of even terabytes of images then you should consider storing them outside of the db.
Yes, and IIRC Mladen Prajdić did a demo at PASS Summit one year where he injected an executable into a `varbinary` field and then got SQL Server to execute it via `xp_cmdshell`
I'm happy with the solution I've landed on and just using URL's, this is more of an intellectual question relative to capability.
Thanks, I'll check that out.
You can do it, but don’t. That’s not what databases are designed for.
It's entirely possible but it's hard to come up with a valid use case for doing so. These are text-based RDBMSs, so everything is going to get reduced to a character representation and then it's up to you to have a process to reconstruct binary data back into something meaningful. Even in Oracle's own ERP, they store images outside of the database and resort to linking to the filesystem through their middleware product which is driven by Weblogic. It's simply a call to the web server and then the web server serves up the image from it's file system. And this method is not used just for storing and retrieving images, but for any file. PDFs, plain text notes, word/excel documents, and more. If a table stores blob data types and is transactional, meaning rows coming and going frequently, consider what that will do to your high water mark. The table could quickly become large and unruly, backing up gaps of empty space with each backup, costing valuable time and storage. I strictly feel that files (images, video, documents) should only be stored and retrieved from an application layer using a database on the back end for fast reference. Text-based RDBMSs are great at text, but not so good for storing other things. Even though you definitely can do it. 
The BLOB data type holds binary encoded data; you would need to use a SQL function or API or ETL tool to retrieve the BLOB, decompress it if required, and rename it to myimage.jpg or whatever. It's very easy. We have an asset management database with a *LOT* of images on it; this database was purchased from a supplier. Unfortunately we save so many images it takes multiple seconds to retrieve any of them, and a couple of times a year, the server runs out of space. And, the production environment can't be cloned easily to the test environment because that would cause the test server to run out of space. So, if it were my database, I would be storing the images in a data lake store / BLOB storage on Azure or AWS and putting URLs in the database instead. Much better (and much cheaper, like a few pennies / cents per terabyte).
I would start with something like this SELECT value FROM STRING_SPLIT(REPLACE(Column_Name, ' ', ','), ',') That should get you on the right track for the comma and space splitting part... You would then want to apply some additional logic for checking the length of those returned rows, you could maybe count the space between commas, and insert one every 50 chars.. 
thanks for the info, i will try using the union function to make it work.
&gt; Is this possible? Yes, you can. You can use `varbinary` or whatever your regional equivalent is called. Microsoft released a white paper about 8 to 10 years ago that showed that, if your files were under 1 MB in size, that you'd actually see significantly better performance using SQL Server to store the data in a `varbinary` instead of the alternative of the file system. In C#/SQL Server, you just store a byte array that you can get from ReadAllBytes() into the varbinary column, then read it back and save it with WriteAllBytes(). There are caveats of doing each method. 1. As I said above, Microsoft found for SQL Server that below 1 MB, it was better for performance to store in the database. Above that, the file system took over. This was somewhat before the ubiquity of SSDs, however, and also doesn't account for the fact that web servers can take advantage of the asynchronous sendfile() system call on Linux (because Microsoft). Basically, if the data files are on a Linux server's file system there's a good chance that the web daemon can offload the task to the OS. That won't happen with data files in a database; the web server has to manage it. I'm not sure if Windows servers can do it, either. 2. Do you need transaction support? Does your app require the ability to rollback if a transaction fails? Is it a *major problem* if the metadata for your file and the file itself are off by even a little bit? A database can handle that easily because transactions and ACID compliance are what it does. A filesystem can't without a bit of work. 3. Backups are more complex if you're storing data outside the database. If you're storing files outside the database, then when you back up the database, you also need to backup your data files. A complete data backup is more than just the database. This is somewhat more complicated. 4. On the flip size, if you're storing a *lot* of data files in the database, backups may take a lot longer. I've seen more than one application have an "attachments" database that is used exclusively for tables that contain BLOBs. 5. File access. If files are stored outside the database, then you need to be sure that nobody is moving, deleting, or modifying them randomly. In some cases, you might need to make sure nobody is *accessing* them outside the database (i.e., W2s, check stubs, etc.). You'll have to rely on OS security instead of database security. If you're using something like DB encryption, this may be an issue, too, since the file system typically doesn't support database encryption. 6. In rare cases, features might depend on it. If you were talking PDFs, then it is possible on some systems to install filters. For example, you can install Adobe iFilter on SQL Server and you'll be able to create Full Text indexes on PDF BLOBs. I'm not sure if this supports some of the other features noted below. Finally, some RDBMSs offer extensions to kind of do the work for you so they offer a hybrid approach. You can fetch and store the data like it's a column in the database with full transactional support, but it will be stored on the file system directly. [For example, SQL Server has the FILESTREAM feature](https://docs.microsoft.com/en-us/sql/relational-databases/blob/filestream-sql-server?view=sql-server-2017) and the newer [FileTables](https://docs.microsoft.com/en-us/sql/relational-databases/blob/filetables-sql-server?view=sql-server-2017) feature. I'm not advertising for SQL Server, it's just what I'm more familiar with. This is a fairly contentious issue. More opinions on the caveats can be found online (I do not endorse all these opinions and, in fact, disagree in some cases, but they are generally well reasoned and understandable): [Is it a bad practice to store large files (10 MB) in a database?](https://softwareengineering.stackexchange.com/q/150669/22593) [Storing Images in DB - Yea or Nay?](https://stackoverflow.com/q/3748/696808) [Storing images in SQL Server?](https://stackoverflow.com/q/5613898/696808) [Why is it recommended to store BLOBs in separate SQL Server tables? ](https://dba.stackexchange.com/q/174678/15011) 
&gt; These are text-based RDBMSs, so everything is going to get reduced to a character representation I have never heard of any RDBMS engine storing binary data as text. What engine does that?
which platform? they all have different string handling functions
The only caveat to file-based storage is that usually there's no specific per-file access control/security so Client A could (if they know the name of the file) retrieve Client B's logo. To minimize the possibility you can hash the names of the logo files or give them random hashes as names.
Within my application layer that isn't a possibility. It's really a simple little set up where there is a filter at the top of a visualization that contains a set of Client names, so a user can pick any client from the list to look at the results of our analytics, and the logo itself is rendered on the screen based on the client that you pick. I understand what you're saying though, but in my example a user can already see them, however we do have the ability to restrict which clients a user are allowed to see in the filter set.
It depends on the metrics. How much, what avg size? We use the SQL server Filestream/Filetable feature to manage a few million randomly sized binary data (most small, max size 45mb) but as it is stored in the filesystem and not inside the DB, the size does not matter at all. Just make sure to implement the best practices for Filestreams (especially separate them into different directories each ~200.000 objects)
You should do a left outer join; this way it won't lose the employee if there's no match. This is a medium pain to change in PSQuery because you have to choose left outer as the join when you add the new record in, so you'll have to remove the EMPLOYEE (or whatever) record that you're joining for the manager lookup and re-add EMPLOYEE as a left outer join.
It is mostly a defense from site-mining: say you mapped logos to clients by having a sequential ID in the name ("logo_ClientID.png") and a given user can only see 'Client #3' via your app and can retrieve "https:\\myserver.com\imgstore\logo_Client#3.png". They could, technically retrieve logos for 1,2,3,4, etc. and discover what other clients you have. You might not have this concern at all.
SQL Server: DECLARE @string NVARCHAR(100) = '1,2,3,4,5' SELECT \* from STRING\_SPLIT(@string, ',') WHERE RTRIM(value) &lt;&gt; '';
Ah, in my example we have everything locked down in an Azure front end. We do have this concern but the security in the application layer prevents what you're talking about.
It is. I have done it on several occations :)
In Oracle, a blob value is stored using a binary character set, which has no collation with the concept of lettercase if that's what you're pointing out. You can't store a PDF as a blob object and have it be a hash of uppercase and lowercase letters, for example. It is only text in the sense that "emojis" are text or "╔" is text. It will surely be "something" on your screen if you convert it, but it's mostly not going to be [a-z][A-Z][0-9].
It was me! And thank you for the recommendation I'm looking it up right now.
Thanks for your replies!
You can put any computer file into a database. Some things shouldn't be put there though. I used to work with a guy who said a previous shop he was at put an archived database into another db. I guess when it was needed, you'd extract it then mount the db in SQL server. It sounded nightmare like.
&gt; It is only text in the sense that "emojis" are text or "╔" is text. Right, and I'd say that is not &gt; a character representation or &gt; text-based
I wrote about this at: https://github.com/datacentric-group/datacentric/blob/master/database_design/patterns_antipatterns/blob_or_not.md PRs welcome!
&gt; We have an asset management database with a LOT of images on it; this database was purchased from a supplier. Unfortunately we save so many images it takes multiple seconds to retrieve any of them, and a couple of times a year, the server runs out of space. Yeah, this is what storing images in a database looks like... at it's best. Go with the URL and blob storage option for sure. 
&gt; in favor of storing a path or URL to the image instead This is what we do also, and a little tip for everyone: Store as little as possible of the path or URL. By that I mean, instead of storing: \\companydomain\fileserver01\production\productimages\2018\12\24\8cf8c560-8d82-4c2d-bd40-9fd4235d0877.jpg Just store for example: 2018\12\24\8cf8c560-8d82-4c2d-bd40-9fd4235d0877.jpg Or perhaps even just the filename, extension and date in separate columns. The most important thing is to not store the environment-specific paths (prod, dev, UAT, etc.) since that will result in all your paths pointing to production when you backup/restore the production database to a development environment of sorts. As for the domain name and fileserver name (or URL root path or similar), you won't have to change anything more than a few configuration strings somewhere in case the domain name or fileserver/DNS name changes.
That's just nuts.
Just use PowerShell: Invoke-Sqlcmd -Query "select 1 as col1, 2 as col2, 3 as col3" -ServerInstance "sql_server" | ForEach-Object { Invoke-WebRequest ("http://google.com/?col1={0}&amp;Col2={1}&amp;Col3={2}" -f $_.Col1, $_.Col2, $_.Col3) }
There is a database suitable for storing images, it's called filesystem.
My take, untested ;) ;with persons as ( select p.PersonId from Person p inner join Deed d on d.PersonId = p.PersonId and YEAR(d.date) = YEAR(GETDATE()) group by p.PersonId having sum(d.NiceOrNaughtyDeed) / COUNT(d.NiceOrNaughtyDeed) &gt; 0.5 -- adjust to weed out less nice children ) select p.FullName, c.Name, ci.Name, p.Address, p.ZipCode from persons ps inner join Person p on p.PersonId = ps.PersonId inner join Country c on c.CountryId = p.CountryId inner join City ci on ci.CountryId = c.CountryId inner join CountryChildStatus ccs on ccs.CountryId = p.CountryId where datediff(year, p.BirthDate, GETDATE()) &lt; ccs.ChildMaximumAge order by c.Name, ci.Name
Thank you!! This part: &gt;ForEach-Object does that mean "for each row", so it automatically "loops" through itself?
Jim Gray was one of the research scientists I worked with (for a couple of days, one time) when I worked at Microsoft. He went on to win a Turing Award. I went on to press buttons on my keyboard and click my mouse. Dr Gray wrote a paper called ["To BLOB or not to BLOB"](https://www.microsoft.com/en-us/research/publication/to-blob-or-not-to-blob-large-object-storage-in-a-database-or-a-filesystem/) which examines the issues surrounding your question. The paper is getting dated, but I don't think the advice it gives changes much.
The percentages are a lie. Don't use them to do performance analysis. The cost estimate percentages are based on the ESTIMATED execution plan, not the actual execution plan. So if everything goes right, specially on the row estimates, those percentages are close to accurate. When we do performance analysis and optimization however, things usually are not. What you are describing does sound a lot like c# code forced into SQL to me i gotta say thou. To me it sounds like it warrants a mayor rewrite. This is just my gut feeling right now thou, I don't know what that thing does, and how it does it. 
Don't do it. It is slower than the filesystem and clutters up the btree with tons of overflow pages.
I asked for an actual execution plan, not an estimated one though?
Powershell is not the most debug friendly thing on the planet. Powershell uses dynamic type casting piping results of one command to the next, so it can get REALLY hard keeping track on what exactly happens. On the other hand, I might just suck really badly at powershell, thats an other option. If you are not quite sure, use a c# script task. Much more typing, for sure, but sooooo much better on debugging and maintaining. Just my 2 cents
My process is 43 sprocs chained together and it has a "brain" so to speak which will decide whether to run everything, which includes populating its staging tables with new data, or to simply skip that step and then either a) execute a full run of all analytic calculations based off the data in the staging tables, or b) execute an incremental run of the calculations which need to be modified. The calculations themselves are assembled using dynamic SQL which scans mapping tables which are imported from CSV and the later sprocs in the chain are loops, so Sproc #36 might calculate a KPI relate to cost savings, but the way it calculates it for Client XYZ is determined by the values loaded for Client XYZ in the mapping tables. If we need to adjust that calculation we reload the mapping files, rerun the job, and it is "smart enough" to know that it only needs to delete the data in the final table relative to that specific KPI, then only rerun the sproc for that single KPI and only for that single client before dumping the results into the final table which is then consumed by Tableau. Once a month it truncates the finished table, repopulates its staging tables, and then recalculates everything. This total process takes about 40 minutes, but 39 of those minutes are populating the staging tables. The full run of calculations takes about a minute, and an incremental run of a single calculation takes less than a minute.
the percentages you get are still based on the estimated plan. Like I said, the cake is a lie ;)
Yep, thats pretty much C# forced into SQL. If something breaks in there, you are going to have a pretty bad time. Plan caching is going to be a real problem for you, unless you recompile everything, which then causes a lot of recompiles, which hurts your db, and my soul, and so one. Don't get me wrong, I'm not trying to badmouth the piece of art you got your hands on, I'm quite sure it actually is a piece of art.... but SQL does not shine at being used as a process orchestration language. 
Just to let everyone know, the company I work for did have the resource I needed. Along with some great dedicated time with a developer! Learned some stuff this week! &amp;#x200B; Thank you to everyone who took the time to respond with meaning ideas and resources. Great community you have here!
The sprocs that control the staging tables are pretty basic queries that join large dbo.objects together which are indexed, and then take those results and dump them out into a staging table which has a clustered index built around a `SegmentID [int]`. There isn't much code that determines whether or not it should or should not trigger this logic. So for example I have something like this: select *, 1 AS 'SegmendID' from table a join table b join table c join table d where [date1] between x and y And then another sproc like this: select *, 1 AS 'SegmendID' from table a join table b join table c join table d where [date2] between x and y These queries are expensive but narrow my data down. PS, I don't use between's in them, but just general example. Then once all the staging tables are good we assemble the calculations based on mapped values as I said, which the business owns/maintains. Nothing really can break, and the mapping tables might have values like this: | Client | StagingSource | Segment | | :--- | :--- | | XYZ | SpendStagingTable | 1 | | ABC | OrdersStagingTable | 4 | These are used as I mention within the dynamic loops to assemble queries such as: select sum(case when blah then 1 else 0 end) / sum(case when blah2 then 1 else 0 end) from &lt;dynamicsql&gt; where client = &lt;dynamicsql&gt; and &lt;more dynamic sql&gt; This allows for me to run hundreds or even thousands of custom calculations based on what multiple "owners" are entering into an Excel file, and the execution time for these calculations is very small. They could make a change, have the process run, see the results, request another change, etc., and it won't ever tax the system. Not sure how to better come up with a solution for the project I've been tasked.
Is there a defined number of child records or is it unknown how many keys that list column would contain?
&gt;Nothing really can break Famous last words. Everything can break, EVERYTHING. Look, I am not telling you what to do, I do not have domain knowledge, I can't speak to the problem you are tasked solving. All I can tell you is, that I've worked on probably similar things, and I've worked on dynamic SQL, and debugging somthing like that is HELL. You can't easily unit test SQL code, you can't easily trace dynamic SQL, you got performance issues left and right doing something like that... its not easy to maintain let me put it that way. If statements do not belong in SQL as a general rule of thumb. If statements belong to the middleware, in C#, Java, or whatever language your middleware is written in. Thats the natural order of things. That does not mean that you must not use IF in SQL, I'm just saying that high level languages are much much much better suited to do that. Again, I do not have domain knowledge so I'm not telling you what to do or not to do. 
I go with Base64 encoding the image. Easy peasy.
What I mean is that the "brain" of this process is totally self referential and in no way dependent on any outside objects other than the logging table we use for all processes. The only dependency is that it looks into that logging table to determine when the last time the database was refreshed and how long it has been since it refreshed itself. Everything else is self contained and fairly straight forward IF logic. If this date is greater than this date, then execute sproc 2, which is responsible for triggers sprocs 5 - 11, if not execute sproc 3 which is responsible for triggering 12, and 12 then looks at the mapping tables to see whether it needs to run a full load, or an incremental load. Now if the values in the mapping table are not real, the calc will fail. So that can go wrong, but we can just modify / reload the CSV files, rerun, and done. &gt;All I can tell you is, that I've worked on probably similar things, and I've worked on dynamic SQL, and debugging somthing like that is HELL. So with this in mind I have every query that is dynamically being generated being exported to a "query table", so debugging is a divided process. I can pull the query and look at it directly to see if it is functioning correctly, or if it isn't executing properly then it is an error in the CSV and how it is being loaded, which is debugged using another method I've designed. The sprocs that are calculations are template calculations, and the variables necessary to complete the calculation are what we load from the CSV. So one client might want to calculate cost savings usings fields A and B, but another client might want to use A1 and B1. No problem. Only error is when we add fieldN, which isn't a real field, so the query will error out and not add any data to the final table. &gt;If statements do not belong in SQL as a general rule of thumb. If statements belong to the middleware, in C#, Java, or whatever language your middleware is written in. What would be an appropriate solution here? This is all a database solution for reporting/analytics consumption in Tableau. The IF's are stupid simple and almost instant. &gt;Thats the natural order of things. I work in analytics and we constantly strive to redefine the natural order, and abuse technologies according to our deliverables. With respect, my only concerns are a) Execution time, and B) Ability to read the code and support it for a stranger -- which isn't easy here, but I document my code well and this program solves a very specific solution for a global business with hundreds of clients. Using middleware here would strike me as making the process more complex, and more difficult to support, no? &gt;I'm just saying that high level languages are much much much better suited to do that. Oh, I agree as a general rule of thumb. 
We are drifting into a rather philosophical discussion. I'll take SSIS as an example here, not as a silver bullet, but an example. SSIS is pretty good at designing ETL processes which I understand you pretty much are working on. It aint perfect, but it gives you a LOT of tools that you would have to dirty hack in pure SQL. SSIS is an orchestration tool to build such processes, and it comes a lot of capabilities, that you would have to dirty hack in pure SQL. I would advocate to use such a middleware layer, to do business logic. It usually is much more test-able, and better debug-able that running hundreds of dynamic sql scrips. well, we are getting into philosophy, we are not talking about your concrete thing, since I know nothing about it, and hence can't speak to it. 
Sorry to hammer you but I really haven't had this process reviewed, and no one I work with has the skillset to really review it properly so I'm legitimately curious about what you have to say. I purposely broke down the sprocs to the smallest possible logical decisions. Instead of having multiple IF's, I would break each one into a separate sproc, for example: Sproc1: `exec sproc02, exec sproc10, exec sproc40` Sproc2: `IF datediff(dd, field, field2) &gt; 29 then exec sproc03` Sproc3: `exec sproc04, exec sproc07, exec sproc08, exec sproc09` Sproc4: 'IF blah then exec sproc05` Sproc5: `exec sproc06` Then in the 1 sproc to rule them all that executes everything else I have it clearly documented what IF decisions are being made where, etc. So Sproc1 is just a bunch of simple execute statements. You can then go into the other sprocs to see more detail. Included in this documentation I have comments about how to add new sprocs to the chain, how to debug, etc. In the end all the various custom calculations, etc., get dumped out into a really generic table (example above) which is then connected to Tableau. This process is designed to supported over a hundred clients, each of which can have over a dozen calculations, some of which are the same across clients, and some of which are unique. New sprocs are either new segment's, or they are new calculations which are more like "templates" that are not complete queries because they are dependent on being supplied values from the mapping table which gets imported into the database using a separate ETL process.
Ironically I designed this process to be fully integrated and ultimately consumed by SSIS, but it isn't there yet. I may or may not be the developer who takes this and puts it into SSIS, but it was built from the ground up with that being the conceptual final home for the entire "meta" process. We use SSIS to import the CSV files to populate the mapping tables. In a perfect world I would be handing this entire process off at some point to an offshore team for them to do exactly this.
Your comment made me laugh :-)
Unknown. Technically, the maximum number of child records is known, but it seems rather silly to add 18 new columns
Good! :)
it's honestly best to just have another table
Look into [SUM with GROUP BY](https://www.w3resource.com/sql/aggregate-functions/sum-with-group-by.php) Something like this SELECT snimi, Opisk.opisknum, SUM(käyttunnit) FROM Propisk, Opisk, Proj WHERE Opisk.opisknum = Propisk.opisknum AND Proj.projnum = Propisk.projnum GROUP BY snimi, Opisk.opisknum ORDER BY snimi;
Thanks, this was it! I had to use format SUM (Propisk.käyttunnit) to get it to work properly. &amp;#x200B;
postgres, for example. Most likely tho, you're trying to invent a wheel.
Okay, breaking it down, you have this general form: SELECT d.col1 ,subquery1 ,d.col2 ,subquery2 ,d.col3 ,a.col1 FROM d, ss, s, l, a WHERE d.col3 = a.col2 or d.col3 IS NULL First thing, in your main select, you're bringing in 5 tables, but only joining 2 of them. Those joined tables are the only ones represented in your select, but the other 3 aren't. What this means is tables a and d give you their result, but the other 3 tables form a cartesian product and are joined in. I recommend rewriting that in ANSI format and removing the unneeded tables. FROM a LEFT OUTER JOIN d ON a.col2 = d.col3 Next, your subqueries don't seem to relate to the outer query in any way. As far as I can tell, they'll always evaluate to the same values for user and delegate each time you run (whatever is in rownum 1). The rownum = 1 is allowing that to evaluate, but it shouldn't really be there. A subquery should only return one value and forcing it to return row 1 always doesn't seem correct to me. Personally, I'm not a big fan of subqueries unless they're unavoidable and they can easily be represented in the main query. In any case, you should use ANSI joins there, leaving out the rownum and not representing table d in the FROM clause. In the WHERE clause, you put the restriction on d from the outer query. This is what relates your subquery to the outer query. ,(SELECT ss.name FROM uis.types ss INNER JOIN uis.history s ON ss.id = s.act_state WHERE s.studium = d.user) AS USER_STATE, --state of user Delegate state would be similar, except substitute in the d.delegate to get that status.
Thank you for putting so much effort into this detailed answer. Now I understand what I did a lot more.
Do you mean you've done 2 of the Stanford modules? Indexes and Transactions; Constraints and Triggers; Views and Authorization; Online Analytical Processing; and Recursion in SQL modules all teach you more of the SQL language. I found them very worthwhile.
I have done the videos on Indexes and Transactions as well tbh, I just didn't do the quizzes properly hence didn't mention in the post! I've started looking for jobs right now, and hence I felt I needed something more practical and less academic focused, and thus I didn't continue the course. Will you recommend me to continue with the course? Thank you for the reply! 
Sure thing! Glad I could help.
Thanks for the link. I'm pursuing a Masters of Information Technology with a concentration in Database design online at SNHU. My bachelors was in project management so I've been looking for some good material to step up my SQL knowledge 
I'm in your shoes, really, so all I can say is that I found the other modules worthwhile because I learned something new, that seemed practical, and got good practice applying it in the practice problems. I hope someone more expert can guide you on next steps!
Thanks for the link. I understand it’s self-paced. How much did you put in to complete the course?
Yep either add 18 attribute columns or create a M2M like you were talking about otherwise changing the middle of the list is going to be a nightmare 
Makes sense Thank you again! 
This course is really great! The emphasis of the course on quizzes is the best part, really makes you think how the stuff works internally and eventually very satisfying
Not sure about the exact metric, but a lot of time for sure. I kind of cheated though, when I just couldn't get around the quizzes I googled for the solution, but again that doesn't mean I copied straightaway all of them. Sometimes just the idea, sometimes the part that I couldn't understand but in the end made sure that I understood exactly what's happening Also I recommend the SQL course really, solving the quizzes is very satisfying!!
I won’t call you googling answers as cheating though because it’s part of the learning process and you will still need to understand google examples to “copy” it into the code you are writing. I know programmers and analysts google for rarely used functions at work. Keep in mind they use SQL or other programming to perform their job day in day out. Back to the question, did you spend weeks or months to complete the online course? Cheers
I think a month should be enough. ( 2 good hours each day ) Now this includes everything, making good self explanatory notes ( helps a lot when you come back to them later ), watching the videos, going through some other content of the same topic on the internet and solving quizzes. Although if you're going for the course, I'll suggest you to go through the relational algebra videos only thoroughly and not the quizzes if you want to save time. They're good no doubt, but in hindsight I feel going through the videos would've been enough ! 
I’d say to practice your skills a little now. I’ve used Kaggle to practice when I was first picking up sql and python and it helped me figure out what I didn’t know yet. Most of the data is pretty clean there, but I’ve still found it helpful.
Thanks! Can I ask you if you are working rn or a student? Asking because I will like to know what the main focus of people working on sql in the industry is. Like is it querying? if querying how important query optimisation is? And a few more things. Honestly I know it can differ a lot from company to company, but still eager to hear any opinions on that! 
Yes there are multiple methods I can think of off the top of my head but I would recommend a many to many table over all of them. 1) Store IDs as comma separated strings in a text field. This is bad as you have to parse the field to get the values out and will kill performance. 2) Add many columns. This is how you end up with tables with silly numbers of columns (had a 350 column table for a 3rd party product that I had to fix recently). 3)Use some sort of single table hierarchy - e.g. have a parent ID field that links back to the ID of another record in the same table. This is pretty daft though and the only instance where I have seen such a thing used was really bad to work with. You would also have to duplicate data. I'd suggest you set up a M2M table between them as its by far the best option.
&gt;Personally, I'm not a big fan of subqueries unless they're unavoidable Fully agree with this. I'd kick any code sent to me for review back if it had any sub queries in the select portion of the statement, its rare in SQL that there is a hard and fast rule for performance as it generally always *depends* but in my experience sub selects in the select portion of the query always are slower. Reason being is select comes way later in the order of execution than the joins and if you can do it in a sub query you can probably do it better in a join. Sub queries even in joins often kill performance aswell, if the query is remotely complex it tends to run the entire sub query before applying the join.
Thats a network error not a sql one. I'm not sure what testing the connection from the client will prove as the error already indicates where the issue is. https://stackoverflow.com/questions/29229109/test-database-connectivity Powershell will do it fine. A small C# app would also do it. But probably you could get away with a ping in CMD on a loop, you don't need to connect to the instance to prove a networking issue is going on. 
1/ wut? 2/ why?
Need for an older system, it counts every char.
Like person.name nvarchar(n) I need "Name spaces until n end"
Yeah I think this is simplistic at best and wrong in places. Stuff like sql server apparently not supporting filtered aggregates. Example given: &gt;avg(salary) filter (where dept_id = 1) Ok the syntax may be different but what is wrong with: &gt;avg(Case when dept_ID =1 then salary else 0) I was also quite confused about 'Permanent global temporary tables' Are they not just tables? This is not a thing in SQL Server. Not a criticism of the article but not exactly something that sounds like some functionality I want or need that I don't already have. Also &gt;Add table column at specific position You can do this in the SSMS designer apparently so its supported even in the GUI! Sure it might do a drop and re-create behind the scenes but saying it does not support this when the ability to do this has been cemented into the SSMS UI is wrong imo. And the bit on regular expressions atleast for SQL server is just wrong https://docs.microsoft.com/en-us/previous-versions/sql/sql-server-2008-r2/ms187489(v=sql.105) You can fully use regular expressions in queries, I have done so myself. Finally, the number of spelling mistakes in this really seals the deal. The internet would be better off without this web page as its incorrect, poorly written and misleading.
I'm working with SQL for just over 15 years, it's been my bread and butter in every company I've worked for. Thankfully as a language it has not changed a lot over the years, and I don't see it going away anytime soon. Over the course of this summer, I wrote a course on how I use SQL for business and data analysis. If you are interested in learning more, you can access discount here. Best of luck. https://www.udemy.com/business-and-data-analysis-with-sql/?couponCode=WHATDATA10
Will definitely check this out tonight. Thank you 
And I'm currently writing more new company content for the course. At 40 chapters it's just the tip of the iceberg. Let me know if you have any questions?
I have a few actually! Will it be better to ask the questions after checking out the course content tonight or I can ask anytime? Thanks again. 
which platform? you'd be surprised how different the string functions are 
Sorry, Ms sql
Ask anytime. My Facebook page for the course is www.facebook.com/whatdata/ I've posted up a number of free videos there and you can also ask questions in the group or message me directly. If you have quite a few questions, I can jump on Facebook live.
Oracle can do it. But as the others said this looks like a code smell.
When you say package I'll just assume you are on oracle. You could use a function which is no package. If you don't want create a function just for this you can put it in the CTE: [https://oracle-base.com/articles/12c/with-clause-enhancements-12cr1#functions](https://oracle-base.com/articles/12c/with-clause-enhancements-12cr1#functions) This saves you from having to solve it in pure SQL and might make your select more readable. 
Very delayed, but thank you!!
if you can think of an easy way to generate a string of *n* spaces, something like this -- SELECT CONCAT(person.name,LEFT(spaces,LEN(n - person.name)) another idea -- define a table using NCHAR(n) columns instead of NVARCHAR(n) and copy from one to the other
I have a vague, bad memory of something like this, from 15 years ago. That system was written in the late- or mid-1990s. One thing is that SQL likes to trim trailing spaces for VARCHAR types. This crops up in quirky and frustrating places, like JOIN and WHERE clauses, variables in stored procedures, etc. Have a look at SET ANSI_PADDING in BOL. You may be stuck using CHAR types. 
select top 3
Without any information about how the table is used (e.g. whether it is a master table or a transaction table, although it sounds like the latter), if you wrap these inserts in a transaction and are using a fairly recent version of SQL Server, you should be fine. I know that's vague... do you have any more details that might expose some "gotchas"?
1. validate that you have 9 digits 2. write a function/expression for your SIN calculation 3. ???? 4. profit
MSSQL?
It is not particularly elegant, but it works. The syntax is postgress, but it should give you an idea SELECT SUM(CASE WHEN sin*magic_num&gt;9 THEN substr((sin*magic_num)::VARCHAR, 1, 1)::INTEGER + substr((sin*magic_num)::VARCHAR, 2, 1)::INTEGER ELSE sin*magic_num END) AS NORM FROM ( SELECT substr('130692544', m, 1)::INTEGER AS SIN, substr('121212121', m, 1)::INTEGER AS magic_num FROM ( SELECT generate_Series(1, 9) AS m) a) t
I work as a site reliability engineer right now, but I’m moving into a data programming job soon. In my current experience, I’ve done a lot of complex queries and query tuning for performance, as well as some data monitoring and disaster recovery in python. Kaggle helped me get the position in data programming since it rounded me out a little more in the analysis part of data. Instead of pulling reports, I had to draw insights and build predictive models. Most SQL / data jobs you’ll apply for will ask you to do different things based on their needs. What’s helped me is becoming well rounded enough to speak to any area at a high level, while still being able to do a few things very well.
I work as a site reliability engineer right now, but I’m moving into a data programming job soon. In my current experience, I’ve done a lot of complex queries and query tuning for performance, as well as some data monitoring and disaster recovery in python. Kaggle helped me get the position in data programming since it rounded me out a little more in the analysis part of data. Instead of pulling reports, I had to draw insights and build predictive models. Most SQL / data jobs you’ll apply for will ask you to do different things based on their needs. What’s helped me is becoming well rounded enough to speak to any area at a high level, while still being able to do a few things very well.
Wow this makes so much sense. Also what helps is I'm a beginner in Python as well, like I am currently working on CSV files in Python and I like it. So yes Kaggle looks like the right place to begin Thanks! 
Yep, but it's not dynamic. You have to know n But I have a table with a lot of columns. I want to have them all filled
It's possible, but I kinda brand dead. You get columns length from system, but I can't join it right way
Yep, it'll loop through each row of the resultset
Yeah sorry should've specified. It's an MS SQL plugin into a freeware application 
can you create and populate a table with NCHAR(n) columns?
Overkill for this. Group by works fine.
What kind of concurrency problems are you worried about? Are you expecting the inserts to be performed in a specific order? Worried that one will block another and cause something to timeout/be rejected? Other? Are you submitting them as a single batch, or are they coming into SQL Server from separate processes simultaneously? SQL Server can handle tens of thousands of transactions per second, your three should not present a problem.
The source and destination tables are transaction tables of hourly records. I am importing data from three work center tables (with all the same fields) into one central table. These inserts will be part of a stored procedure run as an hourly job. This is the only way anything will be written to the table. We use SQL Server 2008R2. Do you think there are any potential issues based on this? Thanks!
&gt; Worried that one will block another and cause something to timeout/be rejected? This was my concern actually. &amp;#x200B; &gt;Are you submitting them as a single batch, or are they coming into SQL Server from separate processes simultaneously? My code is a stored procedure consisting of three subsequent inserts - nothing else. I am not submitting them in batches, nor are they in a transaction. What exactly do you mean by separate processes? Thank you. &amp;#x200B;
The titanic project alone helped me a ton. There’s a ton of documentation around how to do it so you won’t really get too stuck. Good luck on the search! 
It seems to have much better intellisense and query plans are rendered more nicely. What it does do it does well but it covers only a fraction of the functionality of SSMS when I last used it.
You should be able to update or insert to your table with a merge statement. I'm an Oracle guy, so I'm not sure if that can be used in mySQL or whatever you're using. The editor I use has an Excel workbook import wizard. I'm not sure if what you're using has that feature.
Quick question. Do you actually have to enroll in the course to access all of the content or do you just click explore course?
They'll be executed in series inside your stored proc. You shouldn't have any concurrency issues.
A) you should be fine B) You're migrating off SQL Server 2008R2 in the next 6 months, right? It goes EOL in July.
SQL is a language used to interact with a database. Oracle is a database, as are Postgresql, Mysql, MS sql server, Firebird, Db2, Sqlite and a lot of other programs. Each database is a bit different, meaning that a deep knowledge of how it works is a plus. They also have integrated languages that can be used to add logic element to the database (pl/sql for Oracle, T-sql for MS sql server, pl/pgsql for postgresql) and they are usually pretty different, altough they do the same thing in the end. If that enterprise uses exclusively Oracle, that's why they look for people used to it.
I second that recommendation. Not just for novices, but for experienced developers as well, to sharpen the rusted knife of data structures and algorithms. I aced some interviews thanks to it, but didn't land the jobs for other reasons (incompatible views on appropriate compensation)
Or just use Postgresql arrays. But still a many to many table would be better
Data denormalization is quite common and not only accepted, but recommended method of designing the schema, when you read data more often than write, which is most cases. As long as you do operations in a single transaction, there is no problem with this approach.
It's actually fine. Having a normal form might please the DBA OCD, and avoid errors when you aren't using transactions (you should), but it does make the database slow on reads. Following this approach without question is the primary reason why people turned to NoSQL. But all they had to do is ask why. If speed is what matters, and you gain it by this, then do it. There are plenty of other database design paradigms for various use cases, and no reason to stick to a single one.
I generally try to do as many staging tables at once as my processing power will allow, so more of a one package per system approach. Basically loading all of the system's tables in several parallel streams. For fact and dimension loads I generally do one package per table unless there is another table that must be updated every time the package is run (e.g. an audit table).
SSIS is the best tool for a Microsoft SQL database and Excel spreadsheet. 
There is no difference in functionality, it's just a personal preference, in my opinion AS is more readable because = makes it look like you're setting a parameter or equation.
That makes sense , honestly. 
As a general rule, I usually create one package per ETL task, where that would be: 1. Pull data from whatever source (CSV, API, etc.). 2. Load raw data to staging. 3. Do all your filtering and transforms 4. Load to production. This way one failure doesn't sabotage your whole 80 table process, but it does make sure that production isn't updated for that one source unless everything goes perfectly. If you need to link these processes because of dependencies, just have the last stage of one job call the next job.
Avoid SSIS! I'd rather just code it. Let's face it MS doesn't put much development into the product
I agree, but y'know in the place I'm in, it's Hobson's choice as my gran used to say.
SSIS to load staging tables from source... usually grouped by source *system/solution*... if multiple systems/solutions use the same database, multiple SSIS packages... rationale is to associate the SSIS package to a single set of changes, and a SME that knows the system and how the data/processes interrelate. from staging, sprocs to perform the rest... TSQL against local copies of data (aka NOT linked servers - create a local copy instead) will always be the fastest, and is usually just as easy to troubleshoot if designed well. Load the SSIS packages into the SSIS Catalog... create a local set of environment variables that map to global environment variables... check that logging is configured correctly... etc then create SQL agent jobs for the SSIS package (the sprocs can either be in the SSIS, or a separate job step in the same job, or a different job, just depends on the scope of dependencies). finally, use something like SQL Sentry to perform job scheduling / chaining so that they run as fast as possible (start-to-end) instead of timers that may have gaps, or worse yet overlap with consequences. (at work we ended up writing our own little set of sprocs/utilities to do this, but i'm a fan of using supported products).
SSIS does indeed get love... though it could use more. but for straight data flows, it's plenty fine... the issue is that they built a tool that promotes bad design (like key lookups/etc)... do that stuff in a sproc. but for control flow and basic data flow, it works great.
&gt; I am a Computer Science graduate(2018) who wants to be a data engineer in the future, currently I am not looking for specifically data engineer jobs, but any data related jobs that will get me started. ​I suggest you go through a book to learn Oracle SQL or Microsoft T-SQL, toward taking a certification exam by either of those companies. If you have a certification, that can help you get related jobs. 
Ha - you’ll find most SQL books are like this. Takes some time to go through but it’s fun. Have a good trip 
Yah, the plethora of learning resources out there are an auto-didactic’s wet dream, but the sheer amount of options can also be paralyzing, so I’m deciding to just dig in and focus on this text.
I like one package per destination table, because they are easier to debug and change. You don't wind up disabling a bunch of sequence containers or data flow tasks because you want to test a change to a single table. And you don't want to test execution on a package that pulls 50 tables if you are just changing one table. You can then group them together in master packages. That said, it only works well at scale if you have reusable templates. For very, very simple data copies (such as source into staging), I prefer writing code that you can give parameter values to. Instead of spending an hour writing a simple SSIS package, if you've created an .exe or a Python script that can move data from point A to point B simply by setting arguments for connection strings, pull method (incremental, rolling window, etc), then you are looking at 10 minutes for a staging "package" instead of an hour.
I've never touched a SQL book (I know the basics and some advanced stuff). &amp;#x200B; Before I Google it (which I'm going to anyway), were there any segments that you thought would be really useful? Stored procedures come to mind. I don't use them often. I usually write a Python/C# script that handles that. Using recursive comes to mind as well. I've only seen it used once and it's pretty intuitive but I, for the life of me, did not know it was available in SQL.
This is the book that our SQL expert recommends... FWIW. Lots of pretty basic techniques are covered here that are hard to find elsewhere for some reason. &amp;#x200B; \-FT
Here's an interesting 'other' I've seen: one single SSIS package, with all the configuration outside of the package. Control DB stores the connection strings, table names, etc. SELECT * from the source table, creates generic staging tables with 99 nvarchar(max) columns to dump it into, then calls a stored proc which moves it to a further staging table with real field names and data types. As far as I can tell this served two main purporses: firstly, the team could change data and sprocs without going through the change control process that changing the packages themselves required. More importantly, it made a lot of work for some very highly paid and poorly managed contractors.
Did they give you some kind of certificate that you completed the course? If so, the next step seems to be to just put that on your resume and look for entry level data engineer jobs.
i believe one of them is not Standard SQL hang on a sec... yup, i ran both methods through the [sql validator](https://developer.mimer.com/sql-validator-99/) and got this -- Validated SQL: select a = c1, c2 from t Validation result: The following feature outside Core SQL-99 is used: T031, "BOOLEAN data type" Validated SQL: select c1 as a, c2 from t Validation result: Conforms to Core SQL-99 
 select * from newtable a1 inner join ( select distinct(a.ID) from Table1 a where not exists (select * from Table 2 b where b.ID = a.ID) union select distinct(a.ID) from Table1 a where not exists (select * from Table 3 c where c.ID = a.ID) ) a2 on a1.id= a2.id
Oh well that is kick ass!
I second SSIS, you could use the import export tool, but if it's going to be recurring you best to create ssis package 
Here's a hacky way: DECLARE @string varchar(9) = '147028431' DECLARE @string2 varchar(9) = '121212121' DECLARE @goodday table (number int, n2 int,n3 int,n4 int,n5 int,n6 int,n7 int, n8 int, n9 int) DECLARE @mytable TABLE (SINN int,NUM int,OUT int) INSERT INTO @mytable (SINN,NUM,OUT) VALUES ( 1,1,1) INSERT INTO @mytable (SINN,NUM,OUT) VALUES ( 1,2,2) INSERT INTO @mytable (SINN,NUM,OUT) VALUES ( 2,1,2) INSERT INTO @mytable (SINN,NUM,OUT) VALUES ( 2,2,4) INSERT INTO @mytable (SINN,NUM,OUT) VALUES ( 3,1,3) INSERT INTO @mytable (SINN,NUM,OUT) VALUES ( 3,2,6) INSERT INTO @mytable (SINN,NUM,OUT) VALUES ( 4,1,4) INSERT INTO @mytable (SINN,NUM,OUT) VALUES ( 4,2,8) INSERT INTO @mytable (SINN,NUM,OUT) VALUES ( 5,1,5) INSERT INTO @mytable (SINN,NUM,OUT) VALUES ( 5,2,1) INSERT INTO @mytable (SINN,NUM,OUT) VALUES ( 6,1,6) INSERT INTO @mytable (SINN,NUM,OUT) VALUES ( 6,2,3) INSERT INTO @mytable (SINN,NUM,OUT) VALUES ( 7,1,7) INSERT INTO @mytable (SINN,NUM,OUT) VALUES ( 7,2,5) INSERT INTO @mytable (SINN,NUM,OUT) VALUES ( 8,1,8) INSERT INTO @mytable (SINN,NUM,OUT) VALUES ( 8,2,7) INSERT INTO @mytable (SINN,NUM,OUT) VALUES ( 9,1,9) INSERT INTO @mytable (SINN,NUM,OUT) VALUES ( 9,2,9) INSERT INTO @mytable (SINN,NUM,OUT) VALUES ( 0,1,0) INSERT INTO @mytable (SINN,NUM,OUT) VALUES ( 0,2,0) &amp;#x200B; &amp;#x200B; INSERT INTO @goodday (number, n2,n3,n4,n5,n6,n7,n8,n9) SELECT DISTINCT --CONCAT( CASE WHEN SUBSTRING(@string, 1, 1)=SINN AND SUBSTRING(@string2, 1, 1) = NUM THEN \[out\] end, CASE WHEN SUBSTRING(@string, 2, 1)=SINN AND SUBSTRING(@string2, 2, 1) = NUM THEN \[out\] END, CASE WHEN SUBSTRING(@string, 3, 1)=SINN AND SUBSTRING(@string2, 3, 1) = NUM THEN \[out\] END , CASE WHEN SUBSTRING(@string, 4, 1)=SINN AND SUBSTRING(@string2, 4, 1) = NUM THEN \[out\] END , CASE WHEN SUBSTRING(@string, 5, 1)=SINN AND SUBSTRING(@string2, 5, 1) = NUM THEN \[out\] END , CASE WHEN SUBSTRING(@string, 6, 1)=SINN AND SUBSTRING(@string2, 6, 1) = NUM THEN \[out\] END , CASE WHEN SUBSTRING(@string, 7, 1)=SINN AND SUBSTRING(@string2, 7, 1) = NUM THEN \[out\] END , CASE WHEN SUBSTRING(@string, 8, 1)=SINN AND SUBSTRING(@string2, 8, 1) = NUM THEN \[out\] END , CASE WHEN SUBSTRING(@string, 9, 1)=SINN AND SUBSTRING(@string2, 9, 1) = NUM THEN \[out\] END from @mytable &amp;#x200B; SELECT CONCAT(MAX(number),MAX(n2),MAX(n3),Max(n4),Max(n5),Max(n6),Max(n7),MAX(n8),Max(n9)) from @goodday
Could try in sys.comments
I read Oracle 12c by Joan Castel along the way I found that there was a lot of similarities with this book. I haven't read it yet, but I went through it for some solutions. 
This was one of my first book, a great beginners resource. Also check out his other book database design for mere mortals. Enjoy your journey! 
Thank you for sharing, i'll try that.
When you enter the website, there'll be 2 options to take the course Select the self paced one and then you can take any course any time you want! Cheers! 
Makes sense! :) When I started I wanted to go for certifications, but then later stayed away from them because people told me you'll learn more by doing personal projects and projects will add more value, which I still agree with. But I need a definite goal to aim for as you said and a structure, and rn I am already doing a course in Python which kind of involves building things, so rest of the time I can read a book and prepare for certification, which will be a good addition to my resume as well. Thanks for the suggestion ! 
They do. I actually do have one, but it doesn't add any value to be honest because it is not officially a certificate, they've clearly mentioned that in the certificate itself! 
I'm not on Facebook unfortunately Although I'll message you on Reddit after making a list of questions. Thanks again ! :) 
!RemindMe 1 day
I will be messaging you on [**2018-12-21 04:43:30 UTC**](http://www.wolframalpha.com/input/?i=2018-12-21 04:43:30 UTC To Local Time) to remind you of [**this link.**](https://www.reddit.com/r/SQL/comments/a7qxxg/let_the_journey_begin_going_to_make_a_study_of/) [**CLICK THIS LINK**](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[https://www.reddit.com/r/SQL/comments/a7qxxg/let_the_journey_begin_going_to_make_a_study_of/]%0A%0ARemindMe! 1 day) to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete Comment&amp;message=Delete! ____id____) _____ |[^(FAQs)](http://np.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/)|[^(Custom)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[LINK INSIDE SQUARE BRACKETS else default to FAQs]%0A%0ANOTE: Don't forget to add the time options after the command.%0A%0ARemindMe!)|[^(Your Reminders)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List Of Reminders&amp;message=MyReminders!)|[^(Feedback)](http://np.reddit.com/message/compose/?to=RemindMeBotWrangler&amp;subject=Feedback)|[^(Code)](https://github.com/SIlver--/remindmebot-reddit)|[^(Browser Extensions)](https://np.reddit.com/r/RemindMeBot/comments/4kldad/remindmebot_extensions/) |-|-|-|-|-|-|
I just want to take a moment and throw a little love to my other favorite tool, Powershell. With three lines you can load that Excel document (saved as CSV) into a separate SQL table then run any command(s) you want to update from the temp table to your target table. In PowerShell it would look like: Import-Module SQLServer $data = Import-Csv &lt;path to file&gt; Write-SqlTableData -ServerInstance &lt;server name&gt; -DatabaseName &lt;database name&gt; -SchemaName &lt;schema name&gt; -TableName &lt;temp table name&gt; force
&gt;They also have integrated languages that can be used to add logic element to the database Thanks for your explaination. What are ''logic elements''? In a database what are ''logic elements'' ?
Phrased only as an autodidact could :P
lol glad someone appreciated it. A colleague used that term to describe me and I’ve been using it every chance I get it. Love it.
thanks but lets say i may want to get top 5 events in each category how may i write this &amp;#x200B;
Ok thanks for the insight! So in the case of updating another table every time the package is run, could you do this through CDC or placing an execute package task at the end of the control flow instead? (This isn't a recommendation, just trying to get a feel for things).
Makes sense, I like the premise of developing in discreet units that have limited consequences if they fail. In the case of a failure, how do you mitigate the impact? Do you receive a notification so you can kick off the package again? I imagine that (e.g.) if a source doesn't get loaded into staging, the effects can balloon if that source is being used in several areas / joins in the warehouse.
Ok thanks that's helpful! So, (sorry if my questions are low-tier), in para 2 you say sprocs to perform the rest... what's the reason for the sprocs? Are they faster to do the compute on the database instead of in SSIS? How common is this approach (using SSIS more like an 'orchestration' tool)? Also in para 2 you say it's fastest to run against local copies... do you mean the staging tables? Local to what? Thanks!
Some points: 1. You don't need distinct when you use union. Union does that already. 2. Why do you even union when it's both times the same table? &amp;#8203; select distinct(a.ID) from Table1 a where (not exists (select * from Table 2 b where b.ID = a.ID)) or (not exists (select * from Table 3 c where c.ID = a.ID)) You could then just join table 4 and be done with it.
Having read horror stories about oracle and also being a SQL server person by trade I'd recommend SQL server over oracle. I am biased though by what I have only heard but not experienced where oracle is concerned. Apparently oracle has unit tests written in pure C that take 3 days to run. Considering C can be very fast when written correctly this is scary bad. I know that any language can be fast or slow but as C is lower level than say C# it should be easier to write faster code in C than C# which has many more layers of abstraction. If you go down the SQL Server route I'd recommend C# over Java. C# and Java are very similar in syntax but C# is better supported with SQL Server as both are made by microsoft. To be a decent DBA you need to have the skillset of a strong SQL developer + the admin/management side of databases. I'd consider SQL dev as a stepping stone towards being a DBA as any decent DBA could fit into a SQL dev job with ease but the opposite is less true. In order to skill up you really need to use SQL in earnest, that means getting a job with a heavy SQL focus and sticking that out for a couple of years even if its a bit lower level than you ideally want.
Yo , thanks a lot for the comment man! , So c# over java and more focus on sql cool.
Table based or scalar functions and stored procedure. They allow you to have logic elements in your db. For example, at my work place, we often need to round a datetime field to the last millisecond of a day. To do that, we have a function that we pass a day to (with any time) and it returns the normalized value. ALTER FUNCTION funDateTimeMax(@dtmIn DATETIME) RETURNS DATETIME AS BEGIN RETURN DATEADD(MILLISECOND, -3, DATEADD(DAY, 1, DATEADD(DAY, DATEDIFF(DAY, 0, @dtmIn ), 0) )); END; Then a call like: select funDateTimeMax(current_timestamp); gives you the current day, at 23:59:59.997 This is an example of a scalar function. &amp;#x200B;
Happy cake day btw!
Hahaha thanks!
Agreed but code works just as well for straight data flows as well. 
The whole point of learning SQL is so you don't have to go through the larger investment of having to learn Python or C##.
Are you going to repeat this process or is it a one off? If a one off for just 4,000 rows I personally would just use an Excel formula to create a load of update statements and then just run them. e.g. Add in column C ="UPDATE the_table SET field_1 = '" &amp; A1 &amp; "' WHERE field_2 = '"&amp; B1 "'"
In the described situation, if 79 of 80 tables load, then yeah, you are still going to have a situation where reporting, applications, etc. are incorrect. I have each job set up to alert on failure, and they run pretty early, so I have some time to investigate, as well as warn people that reporting might be "off" if I can't fix it quickly. Typically, re-running the job won't fix the issue. I have to look at the error log and see why it failed. If it was a "blip" in the network that caused a disconnect, then rerunning it will actually work. But usually it's that someone has changed something on the source side or not made source data available.
You're aliasing a subquery column. It's the same concept if you alias a calculation or even just a column then try to reference that alias. What you can do is calc the max sal in a CTE and join that in to your FROM clause so you can use the aliased value.
&gt; CTE I had to look up [CTE](https://www.postgresql.org/docs/9.1/queries-with.html). I just finished my first course on SQL and we didn't cover WITH. I think the only other way I could do this is to use a Window Function. 
But that's not the question you asked. You asked "how can I improve this query" and now you're saying "please do my homework for me" Show what you've done in an attempt to solve the problem and explain where you're stuck.
Just confirmed we are upgrading - thank you.
You can also create your subquery in your FROM statement and reference it that way, using a cross join. But I'm not sure if the performance is better/worse/the same as using a CTE.
&gt; Should i learn Oracle/ms sql , ? If geographical restrictions are a concern for you, meaning that you're unable to move from the area you're in - then you should definitely look into what's the most prominent jobs for your region. Specific database usage is usually regional - in that most regions are either geared more towards SQL Server or more towards Oracle. With a smattering of postgres and mySQL jobs peppered into the mix. As far as Oracle vs. SQL Server - previously I've been heavy on the Oracle-side, and only recently in my career did I land in a SQL Server shop. Both have their shortcomings and strengths, much like any database you use. Personally, I prefer developing on an Oracle database over SQL Server, but prefer administration in SQL Server much more. 
So what does the outputted number represent in this case? Would I take that number and divide it by 10 to verify?
Did you meet with Tableau? You met with Tableau didn't you?
I would check out Power BI. Fairly easy to get started and has all the tools you need to create reports and dashboards. Free to test out.
This sounds like a great way to have all of your clients' data leaked. There are people's full time job that the devote to several areas you are mentioning (devops/deployment, web development, data architecture etc). You should absolutely not put whatever you end exposed to the internet. Don't spend a cent on the project, you can do it all with free tools. Host it on your machine (localhost - your machine can access it but nobody else can). You can learn a framework in Python like Flask maybe in that time period. If you're starting out asking these questions though, a year would probably be a more realistic time frame.
I have no geographical restrictions , I'm planning to move to Mississauga ,CN . I wish i get a job there . Now,im gonna continue with SQL and have to learn c# or java. 
[removed]
1. Make your big head boss do some work. If he thinks it can be done he had better explain how he thinks it can be done. Meaning connecting you with all the right players to make this kind of thing happen. 2. Once you have an expert team make an outline from top to bottom of the budget including every line item you can think of including a time line, because time is money. 3. Once that is done and he approves the budget, then look at https://www.cloudera.com/products/pricing.html as a potential solution for the cloud database. 
Ignoring all the technical "how do I do this?" stuff, this project is doomed from the outset and you're being set up by your boss. You get that, right? You don't have the time to do this, let alone the expertise. &gt;Here's the kicker, I will need to pay out of pocket for the tools to create the staging website and be reimbursed Nope, stop right there. The company's going to pay for this. I don't care if it's in writing that they'll reimburse you. **Make the company pay for it directly**. &gt; I have been told this is because this is not an "approved" project by IT and there is no budget for it. OK boss, come back when there *is* budget for it. Unapproved and unbudgeted projects means that you will get zero support from anyone beyond your boss when you need something reviewed for validity, questions answered, etc. It will (and should) take a backseat to everything else that *is* approved, budgeted and necessary to keep the lights on. &gt;My boss (who knows very little of technology) says a proof of concept should be possible within two weeks (when he gets back from vacation) Yeah, you're boned. This is uncharted territory for everyone and he expects a working PoC in 2 weeks *during the holidays*? A lot of what you describe is doable with SSRS. You can even embed SSRS reports in a web application. PowerBI is also a good route to look at, especially if you're already in the Microsoft world. Do not go out and "set up a website" or anything like that. Just get PowerBI Desktop and build some dashboards there. How do you publish them outside the company? I _think_ you have to do that via Azure but I'm probably wrong. However, your next issue there is security - it'll be tied in with your company's Azure/Office365 stuff which is great, until you need to share with your external customers. And then it's pretty much gone. Your biggest concern here should be security, like /u/mkingsbu pointed out. There are ways you can publish this stuff for external users to access, but A) you have to manage the security *and make sure client A can't see client B's stuff (who's responsible for that?) and B) (depending on the product) will have to pay for licensing to do it. This will become an administrative headache and if you're a one-man-band, you *will* miss something at some point.
&gt; My boss was upset at the price. &gt; He wants me to recreate the vendor's product for our clients. This sounds like a totally reasonable, fiscally responsible thing for you to waste your time on and the company's money on. :/
I was just testing another number in the example I provided. Put in the example number from the link: 130 692 544 The end result will be: 160394584 Whatever you need to verify it, it will work, as long as you convert or put into somewhere as an INT.
Also, I was thinking you would create a table with those numbers. Ultimately you can track each number and create out, in text, exactly what each number represents. So you can tell the whole story in text, not just 9 numbers.
So your boss is asking you to write an RBAC-based front end reporting system from scratch that interacts with the Internet and also supports local protocols for LAN distribution? You just went from reporting analyst to full stack developer. Software like Tableau, Crystal Reports, Power BI, Qlikview, Kaboodle, Excel4apps, and others have spent years developing such a product. They have entire teams of developers, marketers, and salespeople. To even get a primitive version of something like that going seems very unrealistic to me. You wouldn't ask your mechanic to fabricate a Lamborghini, and ask the mechanic to slap it on his credit card while he's building it. Bless your boss's heart for coming up with such a low-hanging fruit solution of "let's build it for free!", because it's easier said than done. What a visionary. 
I added the SUM query, which adds up to 40.
With such a boss and company environment i would even consider to look for a position at another company asap!
Not this case in general, but yet to see when "Too expensive - we will build our own!" mentality was cheaper than initial product. 
This reminds me of the time my buddy's company was tired of paying for Salesforce so one day the CEO went, "Fuck it, we'll make our own CMS!" - and everyone good quit
Your boss is an idiot. Look into Tableau Online. It’s a pretty small footprint. What your boss is asking for requires a large team of developers, business analysts, DBAs, and Dev Op engineers. 
I'm a bit confused. Why does the following code work? SELECT pay_type, count(pay_type) FROM (SELECT first_name, last_name, salary, CASE WHEN salary &lt; 100000 THEN 'Underpaid' WHEN salary &gt;= 100000 THEN 'Overpaid' END as pay_type FROM employees ) as Q1 GROUP BY pay_type
Sounds like a vendor that would actually build the reports, not a reporting tool
&gt; I will need to pay out of pocket for the tools to create the staging website and be reimbursed. My favorite part.
This is how my team implemented OP’s problem. It’s been working very well and we have happy customers. /r/PowerBI GuyInACube has a fantastic channel on PowerBI. https://youtu.be/_I8EXGa2xLE
Here's a sneak peek of /r/PowerBI using the [top posts](https://np.reddit.com/r/PowerBI/top/?sort=top&amp;t=year) of the year! \#1: [80% vs 20% of the Effort](https://i.redd.it/4qg64sylbc421.jpg) | [16 comments](https://np.reddit.com/r/PowerBI/comments/a6asiu/80_vs_20_of_the_effort/) \#2: [\[Cross-Post\] Visual Vocabulary](https://i.redd.it/92d3tpl6g4z11.jpg) | [6 comments](https://np.reddit.com/r/PowerBI/comments/9y9gkd/crosspost_visual_vocabulary/) \#3: [Data is useless without labels. Compliments of XKCD - Thought you'd all appreciate this one](https://i.redd.it/6urmzted5qv11.png) | [3 comments](https://np.reddit.com/r/PowerBI/comments/9t9oi4/data_is_useless_without_labels_compliments_of/) ---- ^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| [^^Contact ^^me](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| [^^Info](https://np.reddit.com/r/sneakpeekbot/) ^^| [^^Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/8wfgsm/blacklist/)
I'm with this commenter. You said it yourself, you're not a web dev, and your boss has asked you to create something that a vendor is selling(!) in 2 weeks, in-house. It's unrealistic, and you need to tell him/her just how unrealistic it is. This is not something you want to involve yourself in.
You said it yourself, you're not a web dev, and your boss has asked you to create something that a vendor is selling(!), gave you 2 weeks, and it all has to be done in-house. It's unrealistic, and you need to tell him/her just how unrealistic it is. This is not something you want to involve yourself in. Even ignoring the data security aspect, this kind of project has far too large of a scale for someone who is also managing the entire data farm.
&gt; Is there a faster way to select all tables in the union code? no
Salesforce would be fairly easy to recreate if you were very narrow in what your capabilities are. The real strength to Salesforce is the ability to hire Salesforce developers to highly customize Salesforce and have it integrate with multiple other technologies (e.g. Cisco phones used in a call center.) If you were just using Salesforce for basic case management, or lead management, etc. then you could absolutely make your own with very little work and the only problem you'd have is if/when they want to keep making it more complex and robust. If you can boldly say that you won't ever need to do that (which is probably a lie) then there is really no reason to be using a tool like Salesforce in the first place.
 SELECT t.employee_id , t.first_name , t.last_name , t.salary , m.max_sal , m.max_sal - t.salary AS deviation FROM ( SELECT max(salary) AS max_sal FROM employees ) AS m CROSS JOIN employees AS t
&gt; Why can I not reference max_sal here like I referenced pay_type above? because you declared `max_sal` as an alias for `employees` instead of `max(sal)`
I had a professor who made us give him our resumes as an assignment and would dock points if people put SQL in a section with the header of "languages" instead if "skills" or something because the "L" was redundant. 
*Narrator: There was no reimbursement*
*Narrator: But Bob didn't know that, and Bob was about to have a very bad day.*
I disagree about it being overkill... You act like window functions require a big investment. In fact - window functions usually require less investment than a group by, certainly if you're just considering typing.
Ok, very roughly speaking, queries in SQL work on data sets and return data sets and these can be chained like so: DataSet1 -&gt; Query1 -&gt; DataSet2 -&gt; Query2 -&gt; DataSet2 QueryX can address address column names (aliases) from DataSetX. What you specify in the 'select' clause/list is a definition for the 'next' data set. This is why in your latest example you can address max_sal as your query is reading from the data set Q1. By contrast, in your TS you're trying to define something in the output result set and reference it within the same "query level". Does this make sense? 
You're fucked
/r/Whatcouldgowrong 
It looks like several others have thorough responses. OP... please update us what happens with this later.
Please go ahead and produce an example that works the same as this: https://www.reddit.com/r/SQL/comments/a70v7u/sum_count_in_sql_query/ebzga6j/ using windowed functions that is fewer characters than the linked code. Go ahead.... You made the claim. Prove it. I will be very impressed if you do. 
You're fucked dude. You're way out of your element, no offense. I would be a hard pass on this. You have to learn too much, do too much, without the proper tools, in too short a timeframe. Try to get out of this and go on with your regular stuff.
I commend you for your optimism and not immediately telling your boss that this solution - with its time, budgetary, and know-how constraints - is ridiculous. This kind of solutions is a huge undertaking for a team of people who already know exactly what they're doing. The QA for this type of solution alone would span multiple days, depending on how large your user base is going to be. &amp;#x200B; Tell your boss that using the vendor solution, while more expensive, is way better relative to the headache you're going to have trying to first learn all of the required knowledge and second to implement it, not to mention the monetary value of your time to implement this and the cost of licensing, etc. It would be a pretty big red flag if your boss doesn't care about those issues, and it's an even bigger flag that your boss things it's going to be that easy to copy the vendors solution with no project budget, **AND** have a PoC ready in 2 weeks (not to mention it's right before the holidays...) on top of your day to day duties. &amp;#x200B; Just my opinion, but I would tell your boss it's going to be way easier in the long run to go with the vendor. 
Okay, you caught me, in a table with 6 rows and 2 columns, you save what, about 10 characters using a group by. That's not the point I'm trying to make, but you're right, I wasn't clear enough. But you're also (I'm assuming) deliberately ignoring my real point to win an internet conversation. And a silly one - because adding even 1 column to your group by will probably make the window function less key strokes. As far as IO window functions are *at worst* as efficient as Group By, sometimes allowing greater parallelism, but to be fair they're usually negligibly comparable. But the scalability of window functions is incredibly higher. Group by gets the job done. I'm not trying to argue that. And I understand the value of beginning SQL lessons teaching group by (maybe-kinda). But it's not like window functions are difficult to understand - and larger scale scripts are incredibly over-complicated by group by, and usually far more complicated to understand in more involved scripts. And maybe manipulating the script to allow higher efficiency isn't exactly the original point of the thread - using window functions allows more freedom to improve your script over group by. I also never said my answer was the only answer. I just suggested somebody learning SQL take a look at window functions - something that will almost certainly improve OP's ability to use SQL. And for whatever reason you felt the need to say it over-complicates the issue. Which I still argue is a ridiculous claim, it's never over-complicated to learn a new technique. And I would certainly still use a window function to solve OP's problem over a group by. I think the extra 10 keystrokes are worth it to know and understand an improved tool.
I can provide insight with using sprocs. It is possible to do quite a bit of work using SSIS toolbox components for a package. For very simple mappings and transformations it makes sense to do it using the toolbox. For complex transformations, I would recommend doing it in sprocs. Your package will run faster, you will be able to identify errors more easily and if you have multiple developers on your team it will make the packages easier to maintain. &amp;#x200B; The package will be faster if you have the work being done entirely in the database. The SSIS overhead is not there and you avoid memory issues. For debugging if you are using a lot of the toolbox components a failure with the package will not readily identify the issue as a general error is provided and SSIS errors can sometimes be very vague. If you are dealing with 100K and more rows in a table that are being worked on and all those rows are going through multiple transforms using the toolbox, you will have memory issues when the package is run and the memory issues don't crop up right away (dependent on server load). When the work is being done on the database, the memory overhead is within the database which is already very optimized. &amp;#x200B; In terms of maintainability, the worst issue I have with SSIS is that at the end of the day, it's a big XML file. You can't really have multiple developers concurrently working on it and there is no easy way to identify changes between revisions. If your transformations are being done in SQL and have sprocs, functions, views, etc in the database for the package then all that is easily version controlled and if there are changes to be made then you are updating the sproc and in many cases do not need to touch the package at all. &amp;#x200B; The other big factor with maintainability is being able to easily understand what a package is doing and having the ability to make changes easily. If the toolbox is heavily used, each time someone opens a package, you will spend a few minutes and more having to understand what is going on and it's not always obvious. Making changes and testing becomes harder. If the team is very small this may not be an issue but if there are multiple developers that need to maintain these packages, it's very hard sometimes to have someone make the simplest of changes without spending hours. I have found that developers on the team who do not have strong SQL experience will naturally gravitate to using the toolbox heavily and ones that do will have a better intuition on when to use the toolbox fully. It's definitely worth spending time to improve SQL skills. &amp;#x200B; Anyway, I have spoken in a lot of generalities since it's hard to be specific without writing a small book on this but having worked on a large scale integration with multiple team members all the above sentiments were learned as we moved forward with the project. &amp;#x200B; &amp;#x200B;
Tableau is one of the most successful publicly traded companies this year. I don't mean to disparage OP but surely they have such a huge talent pool to fish from that they won't rely on people asking for SQL advice on Reddit.
I asked if the OP met with them, not if the OP worked for them. Also, I question the talent that works there because their product is a steaming pile of shit.
You don't even need the 'as' keyword to be standard. Validated SQL: select c1 a, c2 from t Validation result: Conforms to Core SQL-99 Personally I prefer the non standard '='. As I read left-to-right and like to setup multi-line edits.
They are two different tools with overlapping uses. Saying one is better than the other is like saying a hammer is better than a mallet. It depends and in this example group by is better suited. Windowed functions are a bit more tricky in terms of syntax for a beginner and group by is perfectly fine and suited for this. This was what I had in mind when I said it was overcompliating things. I use both fairly regularly but I use group by more often. &gt;larger scale scripts are incredibly over-complicated by group by, and usually far more complicated to understand in more involved scripts Umm thats something personal to you. Group by or windowed functions are not things I ever really struggle with. Sure if the group by is on a 20 line case statement or theres 10 layers of nested window functions I have to think a bit but thats due to the implementation not the tech. 
Are you seriously still trying to argue about this?? OP asked a question. I suggested he look at window functions. There was no need for you to ever make any comment. I defended why I don't think it's over-complicating it. And you choose to ignore all the facts I give you to focus on one preferential comment I made. Just stop. This is a forum for trying to help other people learn SQL. It's almost exclusively used for helping people with homework or prepare for interviews. I suggested OP learn a new tool. Just stop replying and go to a different sub-reddit - you obviously have no intention of coming here to help people learn or meaningfully discuss SQL.
Thanks for all your comments and suggestions. My assumptions were correct. The scope of this project is massive for one person, especially for a guy who needs to learn it from scratch. The best bet is for me to tell my boss it is a better ROI for him to pay a vendor to set up a program than to let me pick at this for the next how-ever many months/years. I appreciate all your candid and honest answers.
If you actually take a look at my post history I try to help a lot of people here and over at /r/sqlserver. Take a look at this comment I made earlier today where I even said I would jump on a webex with OP if they still needed help and I have time. https://www.reddit.com/r/SQLServer/comments/a7wwvc/help_me_install_sql_server_2017_on_my_laptop/ec6i7n1/ Please dont make assumptions about people like this, its rather rude and shallow. 
&gt; Are you seriously still trying to argue about this?? No I am debating some aspects of a technology I use on a daily basis. Anyways... &gt;As far as efficiency, window functions are at worst as efficient as Group By, sometimes allowing greater parallelism, but to be fair they're usually negligibly comparable. False. It depends on the situation. Your statement is definitive when its a variable outcome. You can see here https://stackoverflow.com/questions/32453293/sql-server-performance-comparison-between-over-partition-by-and-group-by in the plans that group by is producing a signficantly cleaner query plan. Appreciate that is one example but its quite clear that in some situations one will be better than the other. If your gonna be a dick atleast make sure you know what you are talking about. You've made incorrect statements and also assumptions on my presence here in this subreddit without knowing the facts about either! :)
I have absolutely no idea what you are asking, but taking a crack at it, it looks more like C++.
Your situation should be a joke about stereotypical bad clueless management. You'll never get something as generic and stable run in anything under a couple of months, and that's if you had the experience and knowledge to do such stuff. And the "pay with your open money" just put the nail in the coffin. I'd also mention the meme of the Rockstar developer and why it's bad to be in such a situation, but it's just blown away by this whole situation.
No, this is not Oracle SQL. It's also not any type of ANSI SQL, instead it's probably some general purpose programming language. Instead of `TO` ANSI SQL would use `BETWEEN` and instead of the `:` ANSI SQL would use an `IN`. 
&gt;ask your mechanic to fabricate a Lamborghini, and ask the mechanic to slap it on his credit card This is a great analogy. This is too expensive - you build me a sports car in house. Have it done next week, and I'll reimburse you. "Well, I can buy tires at Walmart, so where do I go from there?"
For mere mortals? It's written as if SQL is some goodly language that nobody can learn and that can perform miracles. It's just a tool that makes it easy to manipulate data. "Easy" is the word. It's not meant for speed. Any SQL query can be made faster by coding it in a way that optimal for that data structure, which requires skill. But SQL is really for doing the same, but in 10 lines of code that take 2 minutes to write, and is usually just a bit slower, if it's properly written. I haven't found any languages / platforms that are as easy to learn as SQL and a corresponding database engine. Maybe with the exception of html and css. SQL is awesome, because it's easy
look into joins
How big are the lists? I would just pull all the data and populate both lists at the start, unless you are leaving the second list read only from the start to force the user to select from the first dropdown first. Either way, I would filter the second list based on the initial selection using C# and Linq. Best practice in my eyes is to only hit the database when necessary. In this case it's not really necessary every time the user changes their selection, just on initial form load.
&gt; I usually write a Python/C# script that handles that. In-Line SQL in a large production application is a good way to risk both the security of your data as well as to cause a maintenance nightmare. The other option is using an ORM, which can often times throw up a blob of poorly produced SQL. If an application is going to interact with a database a level of abstraction should be built (aka Data Abstraction Layer). This allows for the database to securely execute the SQL given who is calling the query, limit what's returned, and allows for the DBA/DD to maintain the internals of the sprocs/functions/etc. without having to have an Application Developer do it. &gt; I've never touched a SQL book You should. You should generally strive to write ANSI SQL when possible. You should also understand how the particular database's SQL Engine will execute the SQL you write. Remember, SQL is different from your general purpose programming languages. SQL is declarative - You're telling the database WHAT you want, not HOW it should go about getting it. If you're just building small little apps on your own, then your approach is fine. But in an enterprise environment it's a disaster. 
Depends on whether proprietary software is more widely used in your area. It's a culture thing apparently. Proprietary software has one major flaw - the sunk cost fallacy. Companies will force software that's crappy for a situation just because they paid money for it. I found that it results quite often in either developers trying to fit a square peg into a round hole, or buying way more software and hardware than they would actually need. It makes the developer rigid on this platform. I'm an advocate of Postgresql for small to medium databases, because it's the most flexible and feature rich database engine out there. A couple of features (ex. Computed columns) make sql server and Oracle faster in some use cases (ex. Tableau live connection), but that's where free solutions shine : nobody will object if you implement another database engine just for that (Presto, Spark, Vertica, Exasol, ClickHouse, Brytlyt, etc), which are all the best tools for specific needs. Properly using a combination of these will make you faster, leaner, more flexible and cheaper than just relying on a single tech. As for the language to learn - it doesn't matter. Pick one. It's easy to switch. But if it had to be one: - Java and its family are all fast and well established languages in the data industry that you'll see at every step. Problem is that working with it is relatively hard, and Oracle is sadomizing it. Its future looks stormy. - C# is developing fast, now that it's open source. But it's not nearly as widespread as Java is, and is slightly more resource hungry though as fast as Java. I've never seen anyone use it for data manipulation though. Doesn't mean that you can't, it's just that the primary user base of this language pushed towards changes that they needed, and not towards speed when working with data. Which is why LINQ is hugely popular even if its performance is abyssmal. - Go. The new kid on the block. Same performance as above, but smaller community. It's biggest advantage is small and readable code. - Python. The goto language for data. Compact readable code, and the language itself is built with data in mind, with very short and easy manipulation that's unprecedented. It's built in types allow you already to operate on data of any size. Plus it has some very popular data libraries like Numpy, Pandas, Numba, that make Python as fast as C if used correctly, but still give you great versatility. You also don't have to wait for your program to compile, it just works. Its biggest downside is that it's single threaded (You have threads, but they're not parallel, so you really just use a single core per script). Also it's speed when debugging is abyssmal.
Impossible deadlines Unrealistic expectations No budget Clueless management Sounds great Y'all hiring? 
SQL? More like T-SQL. Santa should be ashamed for not specifying the question properly.
It would exactly look like your result set but if I'm understanding correctly you need to group by the parent and it will give you all children associated with it. something like this. Your result set doesn't make sense to me so it may not be the answer your looking for. Select parent, child, from table group by parent order by parent &amp;#x200B; Parent Child 1 A 1 C 2 B 2 D ....
Why we still have a feeling Bob will try to do this project after all? 
I use MS SQL, try with this code: ;with group\_all as ( select distinct row\_number() over (order by material, supplier, price, allocation, \[month\]) as rbr, material, supplier, price, allocation, \[month\] FROm test group by material, supplier, price, allocation, \[month\] having COUNT(distinct groupid) &gt; 1 ), new as ( select distinct max(groupid) as groupid FROM test t join group\_all g on g.material = t.material and g.supplier = t.supplier and g.price = t.price and g.allocation = t.allocation and g.\[month\] = t.\[month\] group by g.rbr ), for\_delete as ( select distinct groupid FROM test t join group\_all g on g.material = t.material and g.supplier = t.supplier and g.price = t.price and g.allocation = t.allocation and g.\[month\] = t.\[month\] where not exists (select n.groupid from new n where n.groupid = t.groupid) ) select \* from for\_delete &amp;#x200B;
I do take offense to calling me a dick, especially with how-dickish you've come off the whole time. If you're interest was really in helping/educating you would have started with providing examples why a window function is inferior in this case, instead of calling it overkill. But the whole point of my comment was to give him insight into window functions, because his question was already answered. I'm still not convinced by that entire SO thread - I have plenty of cases where using a window function over a Group By significantly improved run-time and IO stats. But I admit I never gave it much more thought that my anecdotal experience feeding my bias, and that I usually work in situations where I need extended window functionality over just selecting distinct sums. But certainly something to start considering with my shameless window function pitches. I sincerely appreciate you giving me a good reason to re-address and re-think some of my go-to solutions. 
I would if I were Bob, and I would significantly upgrade my resume... then in about 8 months right before everything blows up I'd find another job with a big raise and leave.
Good man. Do not get bullied into doing this, and NEVER BUY THE COMPANY TOOLS WITH YOUR OWN MONEY. Why wouldn't your boss use his own money, if it is that important? He makes more of it than you do. What a crazy world this is.
This is insane. You met with a vendor, that has X amount of employees, all with different specialties, and your boss wants you to recreate what they have spent Y amount of years developing... All while paying out of your own pocket? Hell no. 
Thank you!.gif - can’t even make a popular chart because tableau believe that it’s not professional looking! https://community.tableau.com/thread/118656#207740
I suspected it was proprietary trash based on the fact that it's a valid query (to the system that it came from) that starts a bracing with "\[" and ends it with "}"
Doing anything in Tableau is infuriating and they force you to learn their shitty philosophy rather than conforming to standards. It is extremely difficult to prepare data for Tableau consumption. I have become quite good at it, but I still hate them for making me do it. Never in my life did I think I would defend SSRS and hold it up as a shining example of competency and ease of use. Sure it's hard as fuck to make something look pretty in SSRS, but you know what isn't hard as fuck? Writing code that it can consume and visualize in an intuitive way that is semi-analogous to Excel. Tableau is like the Apple of the technology world. They gave me a mouse with one button and when I asked them how to right click they laughed and told me to go fuck myself.
I self taught myself Power BI, only to find out my company uses tableau when I switched roles: it was like being handed a rock and chisel and being told to write gone with the wind! Even now, I barely consider myself competent 
I have done some pretty interesting shit in Tableau relative to SQL. Like some really interesting shit. Having said that I fucking hate Tableau, however that hatred wouldn't prevent me from selling it to a client and making a lot of money building them a Tableau environment.
Select distinct t0.child,t1.child from t as t0 inner join t as t1 on t0.parent=t1.parent;
Looks like you are generating a tree from tables? Mind sharing your table structures?
This is the kind of situation that a recursive cte is generally used for - they run a query based on the result of your result set, and incorporate those values into your result set. Without an idea of your data structure, that's about as specific of an answer as I can give you. Note, recursive cte's are not supported on all rdbms platforms.
It would be great if you found a better answer. I did this with drop down lists for Oracle and had to use many triggers for project. In other words I kept my list as short as possible and wrote triggers for each instance in the lists.
this should work (not tested): with GrpCount as ( select group_id, count(*) as group_count from tbl group by group_id ), GrpToCompare as( select g1.group_id as delete_candidate_id, g2.group_id compare_group_id, g1.group_count from GrpCount g1 join GrpCount g2 on g2.group_id &gt; g2.group_id and g2.group_count = g1.group_count ) select g.delete_candidate_id from GrpToCompare g join tbl t_delete on t_delete.group_id = g.delete_candidate_id join tbl t_compare on t_compare.group_id = g.compare_group_id and t_compare.material = t_delete.material and t_compare.supplier = t_delete.supplier and isnull( t_compare.price, -1.0) = isnull( t_delete.price, -1.0) and isnull( t_compare.allocation, -1.0) = isnull( t_delete.allocation, -1.0) group by g.delete_candidate_id, g.compare_group_id having count(*) = g.group_count 
&gt; I will need to pay out of pocket for the tools to create the staging website What ze fuck? 
Believe it or not, discrete date points as a concept are very very different from contiguous date/time values. Anyway, if you find yourself in need of reporting per/drilling to hours/minutes with additional attributes you might find it helpful to create a separate hour or hour/minute dimension table. If, for example, you are dealing with time zones, you might find it more convenient to set up a date/hour dimension instead and keep smaller time fractions in-place in your fact table(s) instead.
Don't worry too much about which flavor of SQL for your first job. Whichever shop you land in you can specialize. Python and Java suit different needs. I don't use them, but from what I can tell python is used more for ETL focused positions and Java for architect positions.
This is the most amazing unrealistic requests I've seen in the 13 years I've been in BI. This isn't possible, you shouldn't pay out of pocket, you're being set up to fail. Run, run, run!
What's the granularity of the time component? Minute? Second? Less? Having a DateDim and a separate TimeDim (with 3600 rows for minute level) isn't a bad idea and is quite common.
This is like some kind of nightmare. I mean I can see where something like happens, but all of it together? 1) Two weeks? Is he high? 2) Never ever, NEVER EVER pay for a project for your company out of your pocket. 3) Clearly your boss either needs to shop around or learn how to negotiate. He wants you to find a mating pair of unicorns and expects you to have them mate, give birth, and have it tame enough for his 6 year old daughter to ride among the fluffy pink clouds that you'll be blowing out of your a** while driving a rally cross course like Ken Block and singing "I did it my way" like Frank Sinatra. Did I mention this would all happen in 2 weeks time? Yeah, seemed just as likely to me as that.
I'd just slap another column on your new table for "date" with the truncated datetime and call it a day. Then you can relate that new column back to your date table and perform joins quickly without having to do datemath each time. Databases can absolutely be over-normalized, and hard disk space is cheap - don't trip too much about the same data existing in a couple different places if it makes life easier for you or the people using the database.
Forgot to respond to this! Thanks! I ended up using a similar approach to your last LEFT JOIN =)
Yep
Just to reiterate what others have already said DO NOT under any circumstances pay one cent out of your own money for a concept. If your company has a process where it's easier to claim expenses end of month then get your boss to do this, and ask 'if this won't get the buy in now for a small amount of money for a POC why would a wireframe change that'? I would definitely just go with BI solution, you can highlight the things you've said above full teams are dedicated to building and managing these full suite software products and this doesn't seem like your value add as a business. Also may need to think about who else to partner with internally sounds like your boss is a nightmare. 
Someone needs to gold you ASAP 
are you saying you want to display sql reports on a website? Is this possible? I’m new to sql but I am a front end web dev
The date table does not save space. On the contrary, it makes things heavier. A date type typically needs 4 bytes to represent. So does the integer key used. The date table often has a negative impact on performance, if the query planner expects contiguous values. It's good only for OLAP software that it's going to filter by month, week or whatever. So just having a date, or even compete timestamp (date+time) is completely fine. Your 12 million record table will probably not even notice it, unless it's a very thin table running on a raspberry pi. That said, if you want time as a dimension for reporting, you could create it, on an hour or even minute resolution. It's unlikely that anyone will need anything more than hours however.
Mate, you need to elaborate more clearly. At least for me, I don't get it
This project needs more than someone, i think its better to hire some people in different fields to help you to build product.
Describe the table youre wanting to join to: DESCRIBE TABLENAME
Ah great. Will try. Thanks 
&gt; Does this make sense? Yes, I think so. In this case, the subquery is now my data source and the outer query can only obtain data that are contained in the subquery. Below, I cannot call salary in the outer query because it is not called in the subquery. I also cannot call salary and max(salary) because without grouping. SELECT max_salary FROM (SELECT max(salary) as max_salary FROM employees ) as Q1 Without using a CROSS JOIN (because the course hasn't covered it up to this point), I can use another way to call salary and max(salary) but it doesn't look any cleaner than my OP. SELECT salary, max_salary, max_salary - salary FROM (SELECT salary, (SELECT max(salary) as max_salary FROM employees) FROM employees ) as Q1
This is the cleanest example of my code should look. Thank you. 
If by your title you're trying to find the name of a column, and say you know roughly what it should be named but not entirely, try: &amp;#x200B; SELECT \* FROM INFORMATION\_SCHEMA.COLUMNS WHERE COLUMN\_NAME LIKE '%column\_name%' &amp;#x200B;
if cross join syntax is verboten for some obscure reason you can always join on "1=1". Just a thought. Anyway, you latest version will execute the same way as your very first and in both cases your sql engine should be able to detect non-correlated query and execute it only once. 