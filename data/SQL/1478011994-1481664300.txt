Actually, I typically perform the tasks of the DBA on our team and we know when to use it and when it's not ideal. Sometimes it makes sense like using it to do sliding window table partitioning. I read brentozar article (love that site btw) and it's only real argument as that it's annoying to parse and debug. If you really wanted to argue against it, it would be that people shouldn't be using it because it recompiles the query plan each time, but that can be circumvented properly using sp_executesql which can reuse the cache.
I actually understood what you meant. Caching with a database isn't often as simple as all that though - certainly not in any mildly active database. Different data in different tables is going to change at different rates, and cache invalidation is not by any means a simple issue. Regardless, implementing a full text index here as others have suggested would likely alleviate the immediate issue, and be the path of least resistance.
I see your point but you can create a small SQL script to generate your code for you. Something in the lines of (edit: changed the code to use a variable): DECLARE @sql NVARCHAR(MAX) = ''; SELECT @sql = @sql + CONCAT('UNION ALL SELECT SCHEMA_NAME(schema_id) AS SchemaName,name AS TableName FROM [', name, '].sys.tables WHERE OBJECTPROPERTY(OBJECT_ID,''TableHasPrimaryKey'') = 0 ORDER BY SchemaName, TableName; GO') FROM master.dbo.sysdatabases; PRINT @sql I'm not in front of sql server now to try it but you get the idea once you have the code, just create a view with it. 
SQL is also really nice to learn in that there is a natural progression to it. As you start using it you'll run into problems and think 'it'd be so much easier if the data was structured or structured like *this*', which leads very naturally to learning about the data modification aspects to it. Don't worry about cramming all that data modification stuff in now, you'll find it comes very naturally to you as you find problems that need those solutions.
SQL Server 14 -- build temp table with sample data DECLARE @Events TABLE (CustomerID varchar(50), Arrival DATETIME, Departure DATETIME) INSERT INTO @Events (CustomerID, Arrival, Departure) VALUES ('123456', '10/20/16', '10/23/16') -- CTE is just a table of numbers, 0 - ?? ;with Numbers (Number) as (select row_number() over (order by object_id) - 1 from sys.all_objects) select CustomerID, Arrival, Departure, DATEADD(dd, Number, Arrival) AS 'Detail_Date' FROM @Events INNER JOIN Numbers ON number &lt;= datediff(dd, Arrival, Departure)
&gt; really good habbits AVOID RBAR AT ALL COSTS. That's my only advice.
http://stackoverflow.com/questions/19092636/how-to-create-a-database-link-in-mysql-to-connect-to-oracle
 /* Select * from FN_GetAllDatesInRange('1/1/2016',GetDate()) */ create function FN_GetAllDatesInRange ( @Date_Start date , @Date_End date ) RETURNS @temp TABLE ([Date] date) Begin While(@Date_Start &lt;= @Date_End) Begin Insert @temp([Date]) values (@Date_Start) Set @Date_Start = DateAdd(d, 1, @Date_Start) End Return End 
A multi-statement TVF is definitely an overkill in this situation, they have rather poor performance. It can be easily done with a CTE, either as shown by /u/willhaney or using recursion: CREATE FUNCTION GetDatesInbetween ( @DateStart date , @DateEnd date ) RETURNS TABLE AS RETURN WITH datesBetween AS ( SELECT @DateStart AS [Date] UNION ALL SELECT DATEADD(DAY, 1, [Date]) FROM datesBetween WHERE [Date] &lt; @DateEnd ) SELECT [Date] FROM datesBetween Though in this case it's important that you might need the `MAXRECURSION` option, like: SELECT * FROM GetDatesInbetween('2016-01-01', GETDATE()) OPTION (MAXRECURSION 0) 
Yeah that's what I mean generally, I pull the data into sas using the proc sql function then do most of the rest with normal sas coding. Only other main thing I do is use sql to join datasets as I find it better than the sas way of joining tables/creating tables with a few fields from a bunch of different tables.
This syntax makes me unreasonably angry every time I see it. Microsoft needs to provide a real LISTAGG() type function.
Stop posting ads fuck head
Indeed, it is definitely overused, and should only be done if you really know what to expect. 
Thank you so much, I've bookmarked this article since I'm thinking I'll get more mentorship out of it than from in person analyst contacts. 
Excellent! This worked perfectly. Thanks a lot man. For anyone maybe checking this out later, here's the solution /u/switchbladeali gave me written out in SQL: SELECT atc.table_name, atc.column_name FROM all_tab_columns atc, all_tables at WHERE at.table_name = atc.table_name AND column_name LIKE '%SITE%' AND at.num_rows &gt;=1;
I'll occasionally use a giant IN list for ad-hoc queries... but for reporting, sql agent jobs... I'll use a table to hold the values as it makes them easier to change / track changes to: SELECT ct.* FROM table ct WHERE ct.CompanyNumber IN (SELECT CompanyNumber FROM tblImportantCoustomers) Or as a JOIN: SELECT ct.* FROM tblImportantCustomers t JOIN table ct ON ct.CompanyNumber = t.CompanyNumber I'm guessing the JOIN performs better as more values are entered, but maybe not. It might be able to create a hash match on even that many items... so they both probably perform just as well.
What values are in your IN? Are they a list of strings that you're inputting, or are they based on lookups from other tables?
Depends on how long it takes to rebuild the index. Ideally you disable the index and insert the data but that might not make sense if it takes multiple hours to build and a few mins to insert the data.
Is the list coming from another table? If so EXISTS might be worth looking at. It might even be worth putting your list into a table and indexing it. If both sides of the comparison are indexed you would probably see performance improveme ts.
Building the IN clause from the Excel list is the easy part. I wrote the code to do that with one click years ago. I'm just looking for something besides IN that I can use in the SQL.
Oh ok. If you are only concerned with resource management for the script, then my only recommendation is to break it up into sets of 300 or 500 at a time. Use UNION to pull the results together. There is no alternative to IN that I'm aware of and googling didn't show up any results for me either. I've only ever concerned myself with breaking up results like this when performing 3000+ rows of Update/Insert scripts. Sorry I couldn't be of more help. Edit: If you get an answer from someone on this, I would definitely be interested in finding out what the solution was!
What I do when I have to write custom reports, tables, views, stored procs, etc.... against a database / product is create a new DB with the company initials, so if the company I was working for was "Contoso Corp." and the product or DB name was WidgetFactory, the new DB would be CC_WidgetFactory or Contoso_WidgetFactory. Then all my custom tables and code would go into this database... makes it super easy to separate out my code from the software / product to avoid stepping on its toes or have an update / upgrade of the software fail due to my changes, or my changes being wiped out after an upgrade.
If you have time to post this that means the data can't be THAT critical. I would leave the index in place and batch up inserts into batches of 1000 or so and do the insert. You won't lock out the table (as bad :)) and you won't have to fiddle with an index.
Dunno if this works in MSSQL; with myFakeTempTable as ( select 'x' from dual union all select 'y' from dual......) SELECT fields FROM CompanyTable ct WHERE ct.CompanyNumber IN myFakeTempTable ; 
I am coming from SQL Server, where it is my current understanding that inserting into a table blocks users from selecting data from that table. Batching up inserts gives the table a little room to breathe in between inserts so that it can handle other request. Agreed that 3million is trivial, but the OP made it sound like they were expecting or were concerned about performance issues (assumed it was because the data needed to remain as accessible as possible).
instead of WHERE com.CompanyName IN ( 1, 2, 3 ) use JOIN CompanyNamesOfInterest coi ON coi.Name = com.CompanyName again, same effect... except that the table can be managed completely separately, such as when you've got lots of records
So SQL Server...
Okay, I think the table level lock happens in Teradata too. It is not a problem in Oracle. In Oracle, a select is never blocked and a select never blocks.
A little rusty with MySQL, but given that id is the ticket number, and your table has an entry for id with the timestamp for OPEN and an entry for id with the timestamp for CMPLT, I'd try something like this. select avg(timestampdiff(minute, o.logdate, i.logdate)*1.0) as AvgTicketCompletionMinutes from rlmain o inner join rlmain i on (o.id=i.id and i.status='CMPLT') where o.status='OPEN'
Something like this: select avg(datediff(b.logdate, a.logdate)) from rlmain a inner join rlmain b on b.id = a.id and b.status = 'CMPLT' and a.status = 'OPEN'
 DECLARE @minDate DATETIME2 = '10/1/2016'; -- Alternatively, automatically set with SELECT @minDate = MIN(Arrival) FROM dbo.Reservations DECLARE @maxDate DATETIME2 = getdate(); -- Alternatively, specify a date, or auto-set with SELECT @maxDate = MAX(Departure) FROM dbo.Reservations WITH DateRecords AS ( -- Produce a result set that contains 1 record for every date from @minDate to @maxDate SELECT @minDate as [Detail_Date] UNION ALL SELECT DATEADD(d, 1, [Detail_Date]) FROM DateRecords WHERE DATEADD(d, 1, [Detail_Date]) &lt;= @maxDate ) -- Now join to that result set as appropriate, repeating the dbo.Reservations record for each Detail_Date that's between Arrival and Departure SELECT * FROM dbo.Reservations LEFT JOIN DateRecords ON Arrival &gt;= Detail_Date AND Departure &lt;= Detail_Date
&gt; It works well for my use cases, but I've seen others in r/sql say that table variables are their #1 cause for performance pains fyi, that's because the cardinality estimator for a table variable is 1 row in SQL 2012 and 100 rows in 2014+. If you're going to have (tens of) thousands of rows in it, the resulting query plan will be suboptimal.
 ;with Numbers (Number) as (select row_number() over (order by object_id) - 1 from sys.all_objects) What a fascinating and bizarre way to generate a sequence of numbers..!
Thanks! If you know how many numbers you need, the change is minimal: ;with Numbers (Number) as (select TOP 500 row_number() over (order by object_id) - 1 from sys.all_objects)
&gt; If you have time to post this that means the data can't be THAT critical. That statement rubbed me the wrong way. It comes across as dismissive but doesn't add any value to the discussion on hand. You could have left that message out and still get the message across.
to be honest, you only remember this hack if you have had to do it a few times, if you remember it the typed way, or the not typed way, no real difference. You remember it after you had to google it 5 times pretty much, then you type it out by memory, cause this kind of hack takes a few repetitions dosn't it. 
concur. You can't really apply anything until you understand relationships between said tables. Lots of programmers that use dbs for coding end up in one shape or form learning about relational diagrams. They can be quite helpful. In the case that your company does not have them, see if you can find common structures, such as primary keys, foreign keys, functions etc. That might point you in a good direction.
Ha, might be easier to remember if I knew what the hell .value('.', 'nvarchar(max)') does.
What I suggested will work fine if the new partition exists prior to loading. If it's an interval generated partition, just load the 3 million rows with the index in place and call it a day.
Thanks! I tried * just to see what happens. I stopped it at 15 seconds. This seems like an improvement over the version I have, although I would always have to use top n to limit it. 
 WITH Numbers AS ( SELECT 1 as Number UNION ALL SELECT Number + 1 FROM Numbers WHERE Number &lt; 50 ) SELECT * FROM Numbers;
OK, then you'll need to lookup the exact syntax, but after you create the new partition, you'll run something like this: alter index &lt;index_name&gt; partition &lt;partition_name&gt; unusable; Then load data, then: alter index &lt;index_name&gt; partition &lt;partition_name&gt; rebuild; Like I said, you'll need to look up the exact syntax, but that'll handle exactly what you need.
Will do. thank you for your help again.
Thanks so much! This is much simpler.
Thanks! This is exactly it. 
In order to get the installer to run, you will need an X environment on your system and some of the basic x-server packages installed on the AWS instance. You can then ssh to the box with either the -X or -Y option to allow the installer graphics to be displayed on your own system. That said, why are you not leveraging RDS in AWS? From my experience, most don't run stand alone instances in AWS. RDS is a fantastic way to learn as well.
In EC2, you fully leverage your instance. In RDS, you have one hand tied behind your back the whole time. I prefer to learn from a full VM that allows me to configure all aspects of the database versus being turn key. I don't learn well that way sadly.
MySQL
Just did some looking, and from what I could find in my research there is no good way to pivot data in MySQL. Your best bet, if the transformation MUST happen at the database layer, is a block of CASE statements. Barring that, SQL Express is free and MSSQL offers the PIVOT keyword.
One way to do this is to add the space ' ' into the first part of the ISNULL. EMPLOYEE_FIRST + ' ' + ISNULL(EMPLOYEE_MIDDLE + ' ','') + EMPLOYEE_LAST If there is a middle name the space will be added to it, if there isn't a middle name, then it will instead insert the blank string ''. 
Do you know how to use the CASE statements or a good tutorial I can read?
Personally I would use a `CASE` statement: _assuming you're using 2012+ since it has a `CONCAT` function_: SELECT CASE WHEN EMPLOYEE_MIDDLE IS NOT NULL THEN CONCAT(EMPLOYEE_FIRST, ' ', EMPLOYEE_MIDDLE, ' ', EMPLOYEE_LAST) ELSE CONCAT(EMPLOYEE_FIRST, ' ',EMPLOYEE_LAST) END as EMPLOYEE_FULL_NAME ,EMPLOYEE_ID ,EMPLOYEE_DEPT FROM Employees Otherwise, if you're on SQL older than 2012, you should be able to just use the `COLUMN + ' ' + COLUMN` as in your original post.
I'm sorry I can't be more helpful (my work is all in MS SQL) but I'd check [here](http://www.artfulsoftware.com/infotree/qrytip.php?id=78) and [here](https://en.wikibooks.org/wiki/MySQL/Pivot_table) for help in doing a pivot in MySQL specifically.
Unpivot the original records (ie. go to EAV structure of id, version, attribute_name, attribute_value), then join on the ID and attribute name; detection of changes will be easier, and you'll be able to display the output you wanted as well.
Free. One of the best I found http://www.studybyyourself.com/seminar/sql/course/?lang=eng. You have a basic course and an advanced one. Some exercises in the advanced course are kinda difficult.
I have proven that pattern to be performant - WHEN you add "OPTION (RECOMPILE)" at the end (admittedly I'm passing in the query as text w/ params, not a sproc specifically)... it may not cache the exec plan, but the performance never suffers. The only time I ever see dyn SQL is for dynamic pivoting/unpivoting - which is the devil's design.
if you only want TX shown, just add LIMIT 1 edit: then maybe you could join it with another sql query with an ORDER BY ASC to get the min value and its state
Quick question from me: but why don't you use 2 tables: T_Customer and T_Company which would be somewhat easier to overlook and i guess the join wouldn't have been a problem for you. :)
If you want both in a single result set, use a window function so you only have to hit the table once, e.g: with CTE as ( select state, COUNT(*) AS ZipCount, ,ROW_NUMBER() OVER (ORDER BY COUNT(*) ASC) AS LowToHigh ,ROW_NUMBER() OVER (ORDER BY COUNT(*) DESC) AS HighToLow from zipcounty group by state ) SELECT * FROM CTE WHERE LowToHigh = 1 OR HighToLow = 1;
Do FIPS codes correspond to zip codes? If they do, then you can find the min and the max with these two queries: SELECT TOP 1 State, COUNT(CountyFIPS) Count GROUP BY State ORDER BY Count SELECT TOP 1 State, COUNT(CountyFIPS) Count GROUP BY State ORDER BY Count DESC
Yours worked. However we have not covered "CTE." But, I did figure it out: SQL Statement: select state from(SELECT State, COUNT(Countyfips) Count from zipcounty where state is not null GROUP BY State ORDER BY Count desc) where rownum=1; UNION select state from(SELECT State, COUNT(Countyfips) Count from zipcounty where state is not null GROUP BY State ORDER BY Count) where rownum=1; Result: 1-PR 2-TX Thanks everyone. 
Glad to hear your environment works for you! Hopefully you don't have too many issues with the installer. x-forwarding over ssh isn't hard to set up, but it isn't the fastest thing going. The main benefit is you don't need a desktop environment and VNC. Best of luck with the education!
Customers can be people or companies, so it was better to put them in the same table, I guess. I didn't set it up originally, so I'm not entirely sure of the logic.
&gt; The query looks something like this, ignoring the joins. it would help if we could actually see the real query the SUM() that requires your GROUP BY might be possible to achieve in a subquery, thus removing the need for the GROUP BY in the main query can't tell, though, from what you've posted
The thought has never occurred to me, but it should probably be possible to get this sum with a subquery. I'm going to try it out sometime. I could paste the full query here, but it wouldn't do much good. It's consists of 2800 characters, and I doubt anyone would bother trying to wrap their head around it. Thanks for the tip.
This is doing the trick, thank you very much!
I didn't know Oracle had dependencies on X... That's gross....
I am sorry, I meant your, @narayanis, worked.
Also interesting. I didn't use left join because I didn't want multiple hits, but I guess that could be avoided with MAX, as you suggested. Thanks. 
These are fantastic in helping me expand my learning, thank you for taking the time to share! 
Yes, a new table with client id and date sent; then do a query on this table returning the count of each client id for the last 30 days or whatever, then join this with the client table, if count is less than x, send email.
What about using a WITH clause to build a temp table, then joining it in to the main query? I don't know what impact that would have on the performance of the query, but I would imagine it would be faster than having the 2 redundant sub-queries.
`OPTION (RECOMPILE)` does not sanitize your inputs though, especially when some of your parameters are strings. Also when such a query is executed frequently, you'll be burning lots of CPU cycles unnecessary recompiling each time, even if parameters are unchanged.
This, report builder would do what you want.
Instead of a CTE, you could also do it as a subquery. Just SELECT t.* FROM (my query here) t WHERE HighToLow = 1 OR LowToHigh = 1. I just did it as a CTE for readability.
Depending on how fancy you want to make this thing. You can: 1. Use SSRS as others have suggested. 2. Create a basic web app in a language like C# and just open a connection to SQL using the built in SQL connection library 3. Create a SQL data connection in an Excel worksheet. Just some ideas
Yeah, **grep**. Free-form text is not really something that relational databases excel at querying. Maybe if you would stop shit-posting for a couple of hours, you could devote that time to learning a technology that would help you out.
If i wasnt on mobile i could send you some snippets. Lookup adodb.recordset, thats what i use all the time. Basically you have to give it your database's connection string, use the connection to execute your select statement (what you have above) and put that data in a recordset, then you unload the recordset to your sheet.
Will upsert do what you need? https://wiki.postgresql.org/wiki/UPSERT
I'll look into USING clause. Thank you!
Character values should be enclosed in single quotes: VALUES ('OGG' , 'Kahului' , 'Kahului' , 'United States', 20.89865, -156.430458) If your lat/long values are stored as a character data type, they'll need to be quoted also.
If they are double values do they need to be in quotes?
Really? If you're going to cheat on your homework at least Google the question first
If you can't figure this one out on your own, SQL is not for you. This is as easy as it gets.
Common misconception on how it works, the first number is the maximum length of the total number (# before decimal + # after decimal). So you're limited to 6 digits total (the first 6 in double(6,6)) and 6 digits can be after the decimal (the second 6). Which is why it's rounding your numbers to .999999... it can only have 6 digits and they have to be after the decimal, so it's truncating the numbers before the decimal and approximating the closest representation of the number that it can... 1 or -1 (.999999 / -.999999) If you change it to 9,6 it should work.
one table overall, with the PK consisting of two columns, the unique id plus one associated integer
Oh wow, you missed some database design 101 class, didn't you? This is a clear case of one-to-many relationship, and you implement it by having a master table of your IDs and then a detail table of the elements associated with each ID, wherein you have a foreign key to the ID in the master table. You could get away with just one table if all you really need is an ID-integer relationship and the IDs themselves don't have any properties. Let's say what you really want to represent are Students and their Grades. The Grade table will contain a reference (foreign key) to the Student table that will tell you which Student is a given grade for. [Here's an sqlfiddle with a working implementation](http://sqlfiddle.com/#!9/2a308/3). 
For free. For beginners. http://www.studybyyourself.com/seminar/sql/course/?lang=eng.
I haven't used SQLite, but typically indexes don't actually alter the table structure. You won't see additional columns or anything like that. They change how the data is accessed by the RDBMS. A quick Google search shows that you can try .indices tablename Or SELECT * FROM sqlite_master WHERE type = 'index';
To add on to this, MySQL has a GROUP_CONCAT() function that will allow you to return a comma-delimited list of values associated with each ID.
Khan academy? 
&gt;Need help resolving these SQL questions. Write a **SELECT** statement that returns four columns **from** the **Products** table: **product_code, product_name, list_price, and discount_percent**. Then, run this statement to make sure it works correctly. Add an **ORDER BY** clause to this statement that sorts the result set by **list price** in **desc**ending sequence. Then, run this statement again to make sure it works correctly. This is a good way to build and test a statement, one clause at a time. The question itself has the query almost done, if you really want to learn SQL you started on the wrong foot, try a bit harder or read the first chapters again, because it can't get any easier than that.
if OP would show both the query source code and the mysql error message, i could show you how useful it is
Not good. Salt and hash them. Or don't store them at all.
Quick research points toward using bcrypt as your hash method. 
Yes, that's a problem. Debatable how big the ramifications are. Some say SSNs are a poorly kept secret and are easy enough to find on dark web. From a company perspective it may be expensive (paying for customer's cyber security protection) and result in job loss. In past jobs we've used a CLR Stored Proc that used RC4 encryption. Found this article in a stack overflow article about encrypting SSNs. http://dotnetslackers.com/articles/sql/IntroductionToSQLServerEncryptionAndSymmetricKeyEncryptionTutorial.aspx
Get that select statement out of the select statement! That's RBAR! (Row by Agonizing Row)
Rather than using a qualified nested subselect inside of the SELECT statement, consider using a CTE to get the values you're going to need and then LEFT JOIN to the CTE. Probably going to need a windowed function like Row_Number() to get the top 1 CurrentProcessStepID per AssetID.
I know this isnt the best solution. but i ended up with this monstrosity. when ( select top 1 NextProcessStepID from dbo.itTransactionProcess where AssetID=a.AssetID order by TransactionID desc)=60 or ((select top 1 NextProcessStepID from dbo.itTransactionProcess where assetid=a.AssetID order by TransactionID desc)=NULL and (select top 1 CurrentProcessStepID from dbo.itTransactionProcess where assetid=a.AssetID order by TransactionID desc)=60) THEN 'Harvest for Parts' ELSE 'Recycle' 
We do all of our encryption/decryption at the application and store only encrypted values. SQL is not able to decrypt anything, so any unauthorized access would still need to break an encryption algorithm to get at any valuable data. 
Which database are you using? There are other options in addition to hashing for Oracle such as RLS(Row Level Security) and encrypted tablespaces(all of the data is encrypted on disk). In short row level security lets you define which columns or rows a given user can access Encrypted tablespace is the physical location of the data and will encrypt how it is stored but automatically will decrypt it upon read, so it doesn't really prevent a credentialed user from selecting the column.
Sigh... Why do academics insist on using an outdated ER notation (this looks like Chen)? This would be much easier to understand if done in "crows' foot" or UML.
no offense, but this is a terrible approach - you're selecting the same row 3 times in your subqueries. Just use isnull or coalesce in your sub-query. So if you always have at least one record returned, you can use coalesce( NextProcessStepID , CurrentProcessStepID, 0) in the subquery select. If there are cases where no records exist (which your original syntax accommodates, btw), then move the last piece to isnull( &lt;subquery&gt;, 0). 
Join with group by perhaps, example: select a.PatientID, min(b.VisitDate) as nextvisit from Operations a join Visits b on a.PatientID = b.PatientID where b.VisitDate &gt; a.OperationDate group by a.PatientID
wonderful... may i make a suggestion? next time, **DO NOT** put [MySQL] into your post title
I honestly hate this.
How do you want me to send you what I have? My teacher is saying we do not create separate tables for relationships. We just create FKs on the "N" side of the relationship and create a column to match the Primary Key of the "1" side. 
I'm really sorry
do not send it to me at all... post it here as CREATE TABLE statements many-to-many relationships will definitely require a table -- a so-called "junction" or "association" or "linking" table 
Yes I am creating tables for many to many relationships. I actually think I am done, I just am having trouble debugging. I don't know where to drop these FKs. http://pastebin.com/Pf3Pphht &gt;Msg 3726, Level 16, State 1, Line 8 Could not drop object 'AIRPORT' because it is referenced by a FOREIGN KEY constraint. Msg 2714, Level 16, State 6, Line 9 There is already an object named 'AIRPORT' in the database.
I like this idea. I often forget about aggregate functions for non-statistical information. Thanks!
Yep, I'm a big fan of that approach, and use it regularly. Super useful for returning multiple columns. Thanks!
It's the order that's important; you can't drop `AIRPORT` until you either drop the tables that reference it, or remove the foreign keys themselves. Instead of reordering your entire batch, one approach would be to drop all of the foreign key constraints at the top of the file: ALTER TABLE AIRPORT DROP CONSTRAINT FK_AIRPLANE GO ALTER TABLE SEAT DROP CONSTRAINT FK_SEAT GO ...etc. with one of these queries for each foreign key you have. Then the rest of your batch should work.
Yeah I keep getting the same errors. I am about to reply to the person who responded to me. Will you take a look at this code and see if you can debug it. 
So I have done that. http://pastebin.com/HLkwmrLf But now get a ton of the same errors. &gt;Msg 3728, Level 16, State 1, Line 8 'FK_AIRPLANE' is not a constraint. Msg 3727, Level 16, State 0, Line 8 Could not drop constraint. See previous errors. Msg 3728, Level 16, State 1, Line 10 'FK_SEAT' is not a constraint. Msg 3727, Level 16, State 0, Line 10 Could not drop constraint. See previous errors. Msg 3728, Level 16, State 1, Line 12 'FK_FLIGHT_LEG' is not a constraint. Msg 3727, Level 16, State 0, Line 12 Could not drop constraint. See previous errors. Msg 3728, Level 16, State 1, Line 14 'FK_LEG_INSTANCE' is not a constraint. Msg 3727, Level 16, State 0, Line 14 Could not drop constraint. See previous errors. Msg 3728, Level 16, State 1, Line 16 'FK_INSTANCE_OF' is not a constraint. Msg 3727, Level 16, State 0, Line 16 Could not drop constraint. See previous errors. Msg 3728, Level 16, State 1, Line 18 'FK_DEPARTS_ARRIVES' is not a constraint. Msg 3727, Level 16, State 0, Line 18 Could not drop constraint. See previous errors. Msg 3728, Level 16, State 1, Line 20 'FK_FARE' is not a constraint. Msg 3727, Level 16, State 0, Line 20 Could not drop constraint. See previous errors. Msg 4902, Level 16, State 1, Line 22 Cannot find the object "AIRPLANE_ARIPORT" because it does not exist or you do not have permissions. Msg 4902, Level 16, State 1, Line 24 Cannot find the object "AIRPLANE_ARIPORT" because it does not exist or you do not have permissions. Msg 3726, Level 16, State 1, Line 29 Could not drop object 'AIRPORT' because it is referenced by a FOREIGN KEY constraint. Msg 2714, Level 16, State 6, Line 30 There is already an object named 'AIRPORT' in the database.
Is this for SQL Server? Regardless, you could use a left outer join to the same table to get your result. SELECT a.customer_number, b.product FROM salestable a LEFT OUTER JOIN salestable b ON a.customer_number = b.customer_number AND b.product = 'Prod B' WHERE a.product = 'Prod A' Just check the b.product for a value (isnull or case statement) and if it's not null, then it's a Yes.
 select customer_number "Customer That Bought A", case when ProdB = 'Y' then 'Yes' else 'No' end as "Did They Also Buy B" from (select Q.customer_number, max(Q.ProdA) ProdA, max(Q.ProdB) ProdB from (select customer_number, case when product='Prod A' then 'Y' else '' end ProdA, case when product='Prod B' then 'Y' else '' end ProdB from your_table) Q group by Q.customer_number) where ProdA='Y'
This depends entirely on the database you're using. This isn't really a SQL question. Research encryption for "data at rest".
I like this answer but can you explain the second select statement?
This train has no brakes! 
Regex? 
Sure, if you knew what you were looking for. Most definitely not on the scale of cross-referencing batch a (55k) emails vs batch b (650k) emails. If you were looking for, let's say, hclinton@clintonprivateserver.com, or some variations of that, Regex would work.
When I talk about dynamic SQL, I always mean to say execute as a prepared statement via so_executesql, since exec() is just .... nope, never do that. Should have been more clear, but thank you for adding that.
Short answer: AzureSQL uses 90% of same syntax of T-SQL in normal SQL Server. You can view T-SQL Books On Line on each statement to see if applies to which versions if you are concerned. As such, for playground, just install 2016 Development Edition on your local PC to create a local database - free for development/personal use, and 90% of syntax will be the same - of which you can check the documentation where they differ if you are so concerned. Note that Azure SQL Database and Azure SQL Data Warehouse are two different technologies with their own limitations, but are conceptually built off the traditional SQL Server. For example Azure Data Warehouse you cannot do cross-database queries (unless some advanced features such as elastic database is enabled). Whatever your focus is, start focusing on T-SQL as you will definitely use that - and there is a myriad of learning materials on there out there....but I wouldn't worry too much as most people pick these things up quite quickly on the job.
aha, well, there's your error you can only use the VALUES keyword once INSERT INTO airports VALUES ( 'OGG' , 'Kahului' , 'Kahului' , 'United States' , 20.89865 , -156.430458 ) , ( 'YNG' , 'Youngstown Warren Rgnl' , 'Youngstown' , 'United States' , 41.260736 , -80.679097 ) , ( 'OGS' , 'Ogdensburg Intl' , 'Ogdensburg' , 'United States' , 44.681854 , -75.4655 ) , ( 'GEG' , 'Spokane Intl' , 'Spokane' , 'United States' , 47.619861 , -117.533833 ) , ( 'GFK' , 'Grand Forks Intl' , 'Grand Forks' , 'United States' , 47.949256 , -97.176111 ) , ...
So they're telling you to drop the actual foreign keys that table uses - I think you just copied the dummy name they used in their example. You need to go into the object explorer, find the actual foreign key name and reference that.
DTS was deprecated like ten years ago. ಠ_ಠ
Great advice, thank you will start from SQL Server and work my way up.
Your fastest option might be importing the spreadsheet into Access
it means they repeat a user's tablet can have multiple apps
That's a good point, perhaps something more like K-nearest neighbor or some bayesian logic? 
Take the quotes out of your select statements after as ;WITH cte AS (SELECT DISTINCT deviceType AS SubType FROM AssetDetails WHERE deviceType NOT LIKE '' ), cte2 AS ( SELECT SubType, sum(Issue) AS Issue FROM(SELECT AssetDetails.deviceType As SubType, COUNT(diag) As Issue FROM [DeviceTesting] LEFT OUTER JOIN AssetDetails ON DeviceTesting.serial = AssetDetails.serial WHERE report NOT LIKE '%No Fault Found%' AND report NOT LIKE '%Vendor%' GROUP BY deviceType) ), cte3 AS ( SELECT SubType, sum(NoIssue) AS NoIssue FROM(SELECT AssetDetails.deviceType As SubType, COUNT(diag) As NoIssue FROM [DeviceTesting] LEFT OUTER JOIN AssetDetails ON DeviceTesting.serial = AssetDetails.serial WHERE report LIKE '%No Fault Found%' AND report NOT LIKE '%Vendor%' GROUP BY deviceType) ) SELECT SubType, Issue, NoIssue FROM cte a LEFT JOIN cte2 b on a.SubType = a.SubType LEFT JOIN cte3 c on a.SubType = a.SubType OPTION (MAXRECURSION 0)
Thank you sooooo much!!!
From Excel &gt; SQL you can actually set your connection string to your Excel table directly from sql [Here](https://jwcooney.com/2011/10/28/sql-server-how-to-ad-hoc-query-an-excel-spreadsheet-using-openrowset/) its probably not what you want but its cool and I wanted to show it off. There is also an import export wizard in SSMS that is very straight forward and easy to use if you're trying to do a bulk import you can even save the ssis package if you're needing to do it multiple times. SQL to excel You can open up a sql connection from within excel and query that, you can also just run the query and set results to text on then save it as a .csv file and excel will handle it, you might have to change the delimiter depending on your data. There are also SSMS extensions that allow you to just right click on a result set and open it in an excel document. So if you just wanted the columns SELECT TOP 0 * FROM table and use the extention to open in excel and you're done.
Enjoy!
&gt; near "VALUES" at position 108 bingo... it was the second VALUES keyword 
I've had it fail in a while loop and I might as well not even have an error message in the first place
Depending on how you split things up, and how the data is used, you might get a performance boost by splitting things up on one machine. Maybe. But I doubt it would be worth the work. You're far better off talking the boss into maxing out the RAM. Moving to an SSD would also be a good, but due to the work involved, I'd be lobbying for a new server instead. Companies rarely do things that make sense though, so be prepared. A few years back, my team was trying to get our own SQL Server setup. It cost north of $2 million to run that team for a year, so spending $10k (and that's a high estimate) to make us way more efficient would seem like an easy call. But nope. Didn't happen. A couple months later they increased our headcount by 1 because we couldn't keep up with the workload. 
Right. Yeah makes sense - I'm just uncertain how dividing a database by business units - that's more of the actual proposal, the data is separated by the business unit the data represents - would necessarily enhance performance. It would certainly add complexity is my fear, but maybe the complexity wouldn't be that bad. A lot more database names would have to be specified in views mostly.
Generally I have the following drive setup: C: System D: Install and files E: SSIS / and automated packages or files F: Data files L: Log files X: Backup files T: Tempdb data files If I need the speed, then tempdb gets its own 2nd drive for the log files. If I have super busy DB's, they may get their own data file / log file drive depending on the type of processing. I have a DB that grows about 25-40GB a day, it has two drives split with log files and a single data file drive. Tempdb is split on four drives, two for logs and two for data. This allows for greater parallelism. I agree with splitting a database up by topic, but I don't agree for performance if you are doing nothing with the files and filegroupings. This can be accomplished by schema design too however. There's a lot of ways this onion can be cut up. For backups, when I take a backup of superlargedatabase1 or run checkdb on it, it's going to take a long ass time to do it and it's going to put it under a lot of pressure. If it's split between 4 db's, each db will have less time it will be under pressure, even though the total time will still be about the same as just running it once on superlargedatabase1. At least 3/4 of the instance can function semi normally during that duration. Does that help explain it a bit better? 
If you have additional pieces that require manual effort to change or name changing tables in those instances, you're going to have some major pain points. Views / synonyms can help alleviate that, but it gets really dicey really quick when it's mixed between multiple DB's. I have a database I have to move to an archive and restore a blank shell with monthly and append with a new name. There's a major checklist of things I have to update afterwards, I wish I could just use views and synonyms, but even then I'd have more views to update than reports. Royal pain in the ass.
If every business unit has the same schema, and they all share the same tables, and queries usually only look for data from one business unit at a time.... then splitting the data into smaller tables will help the queries run faster. But queries that pull data from more than one BU will have either no gain or, more likely, will run a bit slower. How busy the server is plays into it as well. Maybe you can do something with some new indexes instead. You'd get almost all the gain with none of the loss plus you don't have to rewrite a bunch of queries all at once or reconfigure anybody's access. 
If you are planing to do something like that. make sure that each partition/drive is actually different physical drives, make logical partitions and spread the files among them may even lower the performance of the DB.
This is very true, I did not get into it as it wasn't the question. But you'd need an anchor and a join to the Anchor within the cte, its use in my experience has been pretty limited to loading fake data and test scenarios. I also agree that this is much cleaner and likely to yield the same results. [Check this out](https://technet.microsoft.com/en-us/library/ms186243)
I think I agree with most of the consensus here, If the two databases are on the same logical drive you're likely not going to see a performance increase, the person might be saying this because a particular DB could be created on a different drive or this data could be added to a DB already on a different drive. Data partitioning would still be a better option IMO, any things like views or stored procs would need to be changed potentially to use 3 part names if the tables they're referencing is moved outside of its current DB; it still depends on if that is your bottleneck, if you're pegging the CPU or consuming all the RAM on the machine that is going to need to be resolved prior to any read/write performance increases.
I'm very familiar with the page you just linked :) A few real-world cases for CTEs: * Generating a sequence of numbers or dates * Business Organization Charts * Processing a manufacturing Bill of Materials These days I seem to be using them mostly for generating sequences.
First off, head back to Oracle and fill out that form. They really just want to get a bit of info from you before giving you the free downloads, and they really won't mind that you're a student. Then I'd say go on and follow the guide provided here: https://docs.oracle.com/cd/E17781_01/install.112/e18803/toc.htm In general, I find Oracle software to be pretty unpleasant - especially during installation and especially when installing on Windows. Good luck!
Thanks for your help. I tried an INNER JOIN like so: SELECT taughtby FROM Schedule INNER JOIN Enrollments ON student = 'Michael Moore' I still have duplicates but it removed one of the people I didn't need. Do I need to use DISTINCT somewhere?
this does not attempt to match Schedule rows with Enrollments rows it just filters out the Enrollments rows to thoise of a single student but those Enrollment rows **are joined to *every* Schedule row!!** your ON condition should match column(s) from Schedule to column(s) from Enrollments that's what a join is supposed to do without that "matching" condition, you're getting a filtered cross join maybe google some more on how joins work...
Are the strings in the duplicated rows identical? 
This a great solution! It is very clean and easy to understand. I don't care for using the CTEs but it was used by the closest example I could find. I'll be sure to make sure I understand what actual recursive queries are, since I misused the terminology. You don't know unless someone tells you, so I appreciate it.
This is called entity-attribute-value and it's generally considered a bad practice in relational databases. You're really using the wrong tool for the job. [Article on Simple Talk](https://www.simple-talk.com/sql/t-sql-programming/avoiding-the-eav-of-destruction/). [Getting data out sucks](https://dba.stackexchange.com/questions/68679/efficient-query-for-an-advanced-eav-model). You mention that you're using SSIS for ETL then go on to mention common variables that are also properties of connection managers in SSIS. I'd recommend using package configurations for the things you listed. This will help you out should you also have lower environments to test in like BETA, DEV, UAT, or TEST. SSIS essentially manages that stuff for you and triggers can be setup on the configuration table to capture all changes.
No problem, your using a notation I'm not use to so wondered where it is from. If your interested in a real db design have a look at zabbix https://github.com/zabbix/zabbix/tree/trunk/database the nice part about the codebase is that it is implemented in 5 sql db's
You mean I can't hash my whole hard drive into 32 bytes and get the contents back when I need them? I was so excited...
Can you input data? like, create small tables? If so... thank you!!! :)
I meam, is this a DB meant for you to analyze and work with, or is this a DB meant for an application? If it's the former there's literally no reason this shouldn't be varchar, unless you're some kind of masochist/the values in the DB are being used directly by an external application and the effort to cast them is resource prohibitive.
You can by using CREATE TABLE and INSERT in the Schema Panel
BINARY datatype is to store blob (files, images, music, etc) VARCHAR datatype is to store text Pick what suits your demand.
Right now we merely have schema (and thus naming conventions of tables) based on source import system. This helps readily identify - what the hell is this data, and where did it come from? We haven't actually partitioned anything by "theoretical Business Unit this may apply to, which may be several, especially for core central business data" -- which I think is more of a fake divide because "we have to divide this data up somehow." I was more concerned with the actual logic and tradeoff of performance vs. complexity. But yes the end BI system - the users don't know (other than common sense) where the data necessarily came from. Oh here's our email data ... that probably came from our email software, even though I can't see the actual schema name in the BI software.
You can download SQL Server express on your local machine, and the installation process isn't hard to go through. Plus, it's free. 
This
&gt; is using a JOIN the correct way to go about doing this. it's the only way
You mentioned an App, so I'll recommend Microsoft Access. That would allow you to download countless templates and sample databases.
Yes, which is the issue. I'm not an admin so I'm wondering if it's how the table is setup, but it appears that for this one row generation they are duplicating it. 
you need a LEFT OUTER JOIN with employees as the left table the "first 30 days" date comparison must be in the ON clause of the join, not in the WHERE clause you'll need GROUP BY because you want an aggregate function to count sales "more than one employee" would be a completely separate query
do your own homework
Install SQL Express along with the SQL Server Management studio. Pretty straightforward and will let you learn a lot more quickly.
 
 
I don't think you need the dateadd in the case, just do it in the select and alias it. Assuming 5 days- select dateadd(dd,5,datecolumn) as date_adjusted From database. I'm not sure what it's like doing this in excel though, someone else will probably have a ore sufficient answer Edit : so the date column data type is a string and not datetime..? I think you're going to want to look into the convert function, check this out. Hope it helps some http://stackoverflow.com/questions/10304373/how-convert-string-to-date-t-sql/10304533 That said you should be able to wrap the convert around the dateadd and have it all in one statement 
There are bound to be YouTube videos of people walking through the installation of SQL Express (it's the server) and SQL Server Management Studio (SSMS, it's the client that connects to the server).
&gt; From what I've read this shouldn't have any negative impact on performance and if anything will make it a tiny bit faster. Out of all the "it depends" questions in relational databases, this might be the "it depends"est of them all. Joins normally come with a performance cost, however minor it is, but that depends on stuff like indexing, storage, server memory, what queries you normally do, etc.
No problem, but I guess after you refreshed it with some coursed you should follow the tip of /u/Naeuvaseh and download sql server express and maybe do some throwaway projects
You are probably doing the SELECT, UPDATE and INSERT queries in the code of your apps, that way when both opens a connection and fill the Datasource for some instants they will have 2 different copies of your data and each do their own things. A solution would be to make stored procedures in your DB and call them instead of execute queries, that way only the DB will be reading the tables directly.
Not only this, but stored procedures should ALWAYS be you choice to interact with a database. It allows more control, not to mention a 'pre-compiled" set of instructions that increase efficiency. 
You seem to be using cross apply as a replacement for an inner join. You can, technically, but it is generally not a good idea.
The thing is, when I try to use a join to get both the date of the last record (the same way I use a join to get the date of the first record) I end up with an empty result. Thats the reason I write (in the comments): -- CROSS APPLY TO PASTE LAST -------- -- we cant join in this case a (record cant be both early and last) -- so we use cross apply -- one student has duplicate end_agreements so we use max aft_id instead of the max(per_slut) But I have properply made a mistake
I didn't go through all the code, but if you want to get the first and last record for groups, can you use row_number() partitioned by group, ordered descending in one query and ascending in another, with an inner join between the row_number result and group id?
It might be a big in your app or you might be experiencing race conditions, where there is a parallel process modifying the table in between your select and insert. Serializable isolation level or locking the whole table down will eliminate the latter, but I would expect perfomance impact with both. Anywho, doing an upsert by read/modify is an anti-pattern, you should switch to merge (ma version of upsert) at the very least, or redesign the data flow to avoid race condition altogether (by using queueing, for example)
I have, that was the first approach i'd taken but i couldn't work out a way to increment the number only when the optionGroupID changed and only resetting on ProductID, which is why i switched to dense_rank. If you can see a way to resolve this with row_number() i'd be very grateful. To be honest though i'm not entirely sure either is the best approach, but at this point my mind is going to mush.
Never worked with stored procedures. Can I call those in the app and give them some parameters? Because I need to select certain rows to get things work.
In this example, how would you evaluate the rank of the rows in Rank 2? Rank 2 contains both the lowest productoptionid (1) and a higher one (4) productid|productoptionid|optiongroupid|Rank :--|:--|:--|:-- 25|3|10|1 25|5|10|1 25|6|10|1 25|1|11|2 25|4|11|2 25|2|19|3 26|7|10|1 
 
Hey, thanks for the reply I get the following error: Msg 207, Level 16, State 1, Line 1 Invalid column name 'End_Time'. Msg 207, Level 16, State 1, Line 1 Invalid column name 'Current_Time'.
Great thanks for helping, that worked Really appreciate you taking the time to help
My pleasure. That is what happen when devs are bored hahaha
&gt;A .network_user_id, &gt;A . NAME first of all you are having spaces where should not be and that is in a lot of places and will for sure fail. 
That's.... not a fun problem to have. It means that *somewhere* in your table, there's a row that has a date in the nvarchar column that's formatted incorrectly. Run this query in SSMS: SELECT CAST(yourNvarCharCol AS DATETIME2), * FROM yourTable ORDER BY yourPrimaryKey When the error kicks out, click over to the results tab and take a look at the last record to process correctly. Go find the following record and correct the incorrectly formatted date value using an UPDATE statement. You may have to rinse and repeat several more times.
try this `to_timestamp('2016-11-01 11:59:99', 'YYYY-MM-DD HH:MI:SS')` or this `SELECT timestamp '2016-11-01 11:59:59'` a minute has 60 sec and you are trying to go to 99. o_0 
wowww i am an idiot. sorry and thank you for pointing that out &lt;3
i forgot to rename network_user_id to user_id for the sample dataset. The query actually works beatifully. It just took forever to run. Thanks for confirming the logic!
just found this sub. you guys look like a nice bunch
&gt; lastaft, where you are actually correlating your data, you should just use a join Can you explain what the difference is between antal and lastaft to help explain why one is considered correlating data and another isn't? To give OP a bigger picture, APPLY was originally implemented alongside functions in MSSQL as a way to *apply* a function or algorithm to a set of column values for every record. which would then typically return one value (scalar) or multiple (rows and columns). This can be helpful for maintenance reasons. Imagine you have a complex calculation with business logic that returns some metric that is used in many reports. Do you want every analyst to code up that algorithm or would you rather write it once and let everyone else use it? Even better, what happens when the business decides to change the algorithm? Now there are dozens of versions of this calculation out there and some of them are out of date. But if you write it once and simply have everyone reference that function, it will be much easier to keep things up to date.
&gt;There are child records on this table - indicated with the Parent Transaction ID column. I need to find the latest entry and exit dates in the child records. How do you pick between which entry date and which exit date? Or do you just pick the latest entry of both regardless of which child it came from? 
Latest regardless of which child
Stupid me. I meant to say I do need the PK as well.
Do you mean that - for each individual Recipient ID you need to find the corresponding top-most Parent transaction ID which holds the latest entry and exit dates (sorted as such)?
Again, I'm not going to pretend I've read all of your code (your calculating something per slut made me chuckle tho). Here's a rewrite of that segment with something I would expect to see in a cross apply: cross apply (select top 1 a3.aft_id as max_aftid from aft a3 where a3.CPRNR = a.CPRNR Order by a3.aft_is desc ) maxaft Ps. In this particular case, an analytical function would be better anyway, imo.
 SELECT person_id , MAX( CASE WHEN ordinal = 'Excellent' THEN 4 WHEN ordinal = 'Good' THEN 3 WHEN ordinal = 'Fair' THEN 2 WHEN ordinal = 'Poor' THEN 1 ELSE 0 END) AS result FROM ... GROUP BY person_id 
Is this actually a good book to buy and use for those who have little knowledge of SQL?
Seriously, I buy a used copy for anyone with even an inkling of interest. Tell them to get through joins and they can do plenty, I think that's only like 6 lessons. 
Great book / not a good price.
5 months later, how do you feel?
At a guess, select column, generate ROW_NUMBER, check MOD of the max to see if you need to return an average, else return the middle value. 
Thanks for sharing. I am operating on macOS Sierra, but that workflow you've described sounds like what I'm looking for :)
Then I'd suggest MySQL or Postgres, but that's about as much help as I can be!
Thank you! I'm going to look further into MySQL to start.
I'd recommend that you take a look at Postgres. Aside from the typical SQL work, you can also (with some setup) embed R or Python code as triggers, which is immensely helpful when it comes to any sort of serious mathematical work in SQL.
[Start here](http://postgresapp.com/), try [try psequel](http://www.psequel.com/) if you're not comfortable with the command line (psql) and you'll be up and running in no time. There are [plenty of tutorials](https://www.pgexercises.com/questions/basic/) around, too.
Try /r/learnsql
Send me your resume.
Thank you, I will check this out!
This. if you have appropriate permissions on the database, SQL Server management studio has an option to generate a relationship diagram. However, this only works if the primary key and foreign key constraints are properly setup. 
[LogParser](https://technet.microsoft.com/en-us/scriptcenter/dd919274.aspx) was a lifesaver for me when I needed to filter data out of some insanely large (multiple gigabytes) csv files. It lets you query directly against a CSV file (no importing into Excel). However, it is Windows only.
The only advantage is that sqlite is super easy to add to applications for simple dbs or for people who understand its limitations and know that they are well inside of them. It's really not that great for novices to use in direct interaction. I wouldn't recommend it here, especially if OP wants to see where he can grow to.
That will return an int for each ordinal and not the ordinal value itself
You should be able to do a select within your values or add the values as columns in your select. You can't do 2 separate inserts though. You can also do an insert then an update but better to do all at once. However the best way would be to create a stored procedure to pass the parameters to and the insert would be done there and avoid SQL injection. INSERT INTO tblTestResults (TestPersonnelID, TestDate, fPass, DownForce1, UpForce1, DownForce2, UpForce2)" &amp; _ "VALUES ((SELECT PersonnelID FROM (tblPersonnel) WHERE PersonnelInitials = ('" + a + "')" ),Date(), '-1', '" + c + "', '" + d + "', '" + e + "', '" + f + "')" , _ 
 INSERT INTO tblTestResults ( TestDate , fPass , DownForce1 , UpForce1 , DownForce2 , UpForce2 , TestPersonnelID ) SELECT Date() , '-1' , '" + c + "' , '" + d + "' , '" + e + "' , '" + f + "' , PersonnelID FROM tblPersonnel WHERE PersonnelInitials = ('" + a + "')
another CASE statement can take care of that, if it needs to be done in the sql
Can you elaborate? we open a connection with the common DB. How does your solution works?
Thank you. This helped put me on the right track. Can you help with this last part? Now I need to insert a serial# into a third table, and import the auto-incremented serialID into the TestResults table on that same line. My problem is the join. objRecordSet.Open _ "INSERT INTO tblEquipment (SerialNumber)" &amp; _ "VALUES ('" + serial + "')" , _ objConnection, adOpenStatic, adLockOptimistic objRecordSet2.Open _ "INSERT INTO tblTestResults (TestDate, fPass, DownForce1, TestPersonnelID, EquipmentID)" &amp; _ "SELECT Date(), '-1', '" + c + "', PersonnelID FROM (tblPersonnel) WHERE PersonnelInitials = ('" + user + "')" &amp; _ "RIGHT JOIN tblEquipment ON tblPersonnel.PersonnelID = tblEquipment.SerialNumber" &amp; _ "WHERE SerialNumber = ('" + serial + "')" , _ objConnection, adOpenStatic, adLockOptimistic Errors as: "Number of query values and destination fields are not the same." When I test the bare minimum code block as follows, I get a general 'syntax error in the join' window. objRecordSet.Open _ "SELECT ([tblPersonnel.PersonnelID] [tblEquipment.EquipmentID]" &amp; _ "FROM (tblPersonnel LEFT JOIN tblEquipment ON tblPersonnel.PersonnelID = tblEquipment.EquipmentID))" , _ objConnection, adOpenStatic, adLockOptimistic Been trying all the bracket syntax conventions I could dig up, but it's going nowhere. 
Ah, got it. Can you take copies of the dev database and restore them to the SQL server you're connected to? I'm not experienced with MySQL, so some of the developers on this subreddit may have other/better solutions.
 Wait I think I got it. I'll edit top post with solution. EDIT: I don't got it.
You don't even need a VM - just run a MySQL database on your local desktop. Also, http://sqitch.org/
Each developer should have their own environment. Changes should be tested locally then deployed to a central test server. All dev changes should be pulled down to Test periodically called a "build". This should be tested, peer checked and eventually pushed to live out of hours with a full rollback plan in place. Read up on versioning software, its tougher for databases than applications. You need to script all your entities and create a process that recreates it all from scratch (data and all)
Are you sure you need pgAgent? What do you need it for? 
Thanks for the response - for some reason, I never figured out how to use pandas in an ad hoc workflow. I've used it before to manipulate data, within a .py script, which allowed me multiple times to try and fix my commands till eventually getting it right. Felt that from that experience, I would not fare much better in trying to use that directly in a shell, let's say. I'll keep an open mind though - if you have any recommended reading specific to ad hoc work, let me know, and thanks for taking the time to share your thoughts!
I wish our group had put a better effort in to this years ago. I'm the lone DBA now. We have one script that will update a database to the latest version. It's thousands and thousands of lines. I've finally got approval to start fresh and maintain proper version scripts with our next build. We have very few foreign keys. No data dictionary. No diagrams. Oh, and our product runs on either SQL Server or Oracle, so there's all those headaches X 2. 
Now that you have Postgres up and running, just make a database, some tables and start learning some SQL tutorials. Good luck, it's fun :) 
Don't I, maybe for an older setup (Access '03)? I was getting this recurring join syntax error that looked something like &gt;----&gt;ment.EquipmentIDFROM (tblPersonnel LEFT J&lt;---- which stopped after I squared off EquipmentID from FROM. And I thought I had it figured out because that code block worked fine by itself then. I mean, you got me halfway convinced. I just figured out I was going down the wrong track using UNION ALL to combine the SELECT .... FROM tblPersonnel you wrote with that working SELECT/JOIN block I just posted here. Thought the ish was the brackets, but it was really that union needs column identity, right? But... then how am I supposed to concatenate: SELECT Date() , '-1' , '" + c + "' , '" + d + "' , '" + e + "' , '" + f + "' , PersonnelID FROM tblPersonnel WHERE PersonnelInitials = ('" + a + "') with: "SELECT [tblPersonnel.PersonnelID], [tblEquipment.EquipmentID]" &amp; _ "FROM (tblPersonnel RIGHT JOIN tblEquipment ON tblPersonnel.PersonnelID = tblEquipment.EquipmentID)" &amp; _ "WHERE tblEquipment.SerialNumber = ('" + serial + "') " , _ ^(or am I doing something mindblowingly stupid wrong) ^^(^[again])
&gt; how the Where clause in the main query knows to compare each TeamCaptain's HandicapScore against only all other members of the TeamCaptain's team SELECT Teams.TeamName , Bowlers.BowlerID , Bowlers.BowlerFirstName , Bowlers.BowlerLastName , Bowler_Scores.HandiCapScore FROM ( Bowlers INNER JOIN Teams ON Bowlers.BowlerID = Teams.CaptainID ) INNER JOIN Bowler_Scores ON Bowlers.BowlerID = Bowler_Scores.BowlerID WHERE Bowler_Scores.HandiCapScore &gt; All ( SELECT BS2.HandiCapScore FROM Bowlers AS B2 INNER JOIN Bowler_Scores AS BS2 ON B2.BowlerID = BS2.BowlerID WHERE B2.BowlerID &lt;&gt; Bowlers.BowlerID AND B2.TeamID = Bowlers.TeamID ) what you are asking is accomplished by the last two lines
pgAgent is a separate install from pgAdmin. You can [download it from here ](https://www.pgadmin.org/download/pgagent.php) and find the [docs here](https://www.pgadmin.org/docs/dev/pgagent.html). But as mentioned by other rather wrap your head around other pg concepts.
Thanks. After reading a bit more in the book and thinking about it, I think I realized what I was missing. I was considering the embedded select statement to be a static list rather than a list that changes for each team captain handicapscore in the first part of the main where clause.
the gist of the subquery is that it refers up to the main query using this -- &lt;&gt; Bowlers.BowlerID 
&gt; Make sure you're using an advanced DB like Postgres, DB/2, Oracle, MS SQL OP did mention MS SQL
make the bottom query part of the above one. Desired behavior is like INSERT INTO tblTestResults ( TestDate , fPass , DownForce1 , UpForce1 , DownForce2 , UpForce2 , TestPersonnelID , EquipmentID ) SELECT Date() , '-1' , '" + c + "' , '" + d + "' , '" + e + "' , '" + f + "' , PersonnelID FROM tblPersonnel WHERE PersonnelInitials = ('" + a + "') , tblPersonnel.PersonnelID , tblEquipment.EquipmentID FROM tblPersonnel RIGHT JOIN tblEquipment ON tblPersonnel.PersonnelID = tblEquipment.EquipmentID WHERE tblEquipment.SerialNumber = ('" + serial + "')
 INSERT INTO tblTestResults ( TestDate , fPass , DownForce1 , UpForce1 , DownForce2 , UpForce2 , TestPersonnelID , EquipmentID ) SELECT Date() , '-1' , '" + c + "' , '" + d + "' , '" + e + "' , '" + f + "' , tblPersonnel.PersonnelID , tblEquipment.EquipmentID FROM tblPersonnel LEFT OUTER JOIN tblEquipment ON tblEquipment.EquipmentID = tblPersonnel.PersonnelID AND tblEquipment.SerialNumber = ('" + serial + "') WHERE tblPersonnel.PersonnelInitials = ('" + a + "') 
Thanks so much for the detailed reply. My degree is a communications degree in new media and technology that I regret with a passion but I didn't have a clue what I was doing when I went to college. The main reason I'm interested is because I'm looking to move to a different state but find something similar to what I do now. I'd like to find a business or data analyst position and all seem to be looking for SQL experience. Being realistic though, a job like that would be a step up for me but I have the skills in the making with what I do now just not SQL. If I was staying at my current job, I know I wouldn't need a certification. Also, I live in Tennessee but am looking to move to Columbus, Ohio (used to live there a few years ago). Really I need to find a position that would allow me to learn and move into an analyst role. I hope I don't sound too clueless here. I really appreciate your response.
&gt; My degree is a communications degree in new media and technology that I regret with a passion but I didn't have a clue what I was doing when I went to college. I'm a firm believer that just having a college degree in anything in and of itself is valuable. Some of the best techs I've met have only high school degrees. Don't beat yourself up over that, you're already in a better position than most people. &gt;I'd like to find a business or data analyst position and all seem to be looking for SQL experience. Being realistic though, a job like that would be a step up for me but I have the skills in the making with what I do now just not SQL. If I was staying at my current job, I know I wouldn't need a certification. mmm...I can't give you great guidance as to how to get a BA job (business analyst) since personally I'm from a more tech background. However, in my experience, analysts don't have to be god tier at SQL. They just need to know enough to be dangerous. Usually if anything too crazy lands on their desk they turn to folks like me (production support) to help them out. HOWEVER, if you are an analyst, and you don't have to summon people like me, you'll be worth your weight in gold. Honestly, for an analyst role, the mere fact you can write a SQL query and are familiar with what it is, should count enough towards having "experience". In my 7 years in the professional world, as soon as an analyst needs a query that requires a "join" or more than 3 conditions in a where clause, its not unusual for them to come running to support team to bail them out, and we happily help them out, its part of the job. If I was you, i'd focus more on familiarizing yourself with DB technologies that are catching on in a big way. Hadoop, noSQL, etc. So if they come up in an interview, you're not clueless because HR folks love reducing everything to a check box. You're an analyst. Nobody is going to give a crap if you know how to properly index a table, create relational tables, etc. Because that's the sort of stuff a cert will teach you. They just want to know whether or not you can write a simple sql query, which it seems like you can already do. Like I said before, if you wanna get REAL good at SQL. I'm sure you can find an entry level position that will require you to use SQL a lot. However, that's more of a path for techies. For someone like yourself, take as many free online classes as you can so if during an interview they ask you to write a query, you'll knock it out of the park. Good luck, and let me know if you have any more questions. I'll be happy to help.
You do need some form of date field. I assume delivery.actual_delivery_time that is listed in the example might be it, but certainly try to find the full field list for each of those tables.
 
i've [already explained how to write that left outer join](https://www.reddit.com/r/SQL/comments/5c3xzh/how_would_you_have_written_this_query_mysql/d9th4g9/), you've got the tables backwards :D
May not be the solution you are looking for, but I would do a comparison as follows: SELECT OrderDate,ShippedDate FROM corp.orders WHERE OrderDate - ShippedDate &gt;=2; 
That was one of the variations I tried, it appeared to return zero results from the sample database. 
 DATEDIFF(d,OrderDate,ShippedDate) &gt;= 2 Wouldn't you need to specify what date part you want to find the difference of?
Did you add the 'd' prior to OrderDate in your original example? The [DATEDIFF](http://sql-plsql.blogspot.com/2010/07/datediff.html) function requires three arguments. You had: DATEDIFF(OrderDate,ShippedDate) &gt;= 2 And I had: DATEDIFF(d,OrderDate,ShippedDate) &gt;= 2
Shoot, no. Edit - SqlDev is now giving me an error that SQL wasn't properly ended using this. SELECT * FROM corp.orders DATEDIFF(d,OrderDate,ShippedDate) &gt;=2;
Still errors. "ORA-00904: "DATEDIFF": invalid identifier 00904. 00000 - "%s: invalid identifier" " I understand the logic behind the query; pull all from the orders table, then display where the difference between the order date and the ship date is greater than 2 days. Its just communicating that to Oracle. :/ 
Example please!
you wrote this -- `‘IT’` you want this -- `'IT'` 
Well, SSRS is mainly building and scheduling e-mail reports using T-SQL... There isn't much to SSRS in itself that would require special experience. I guess you can lookup how to make fancy chart/graph reports in Report Builder, lookup some scripts on how to get some metrics from the reportserver database like schedules and such and just how to make use of report parameters. If you'll be working on an Enterprise version of SQL, data driven subscription would be a good place to look into as well. SSIS can be a bit of a rabbit hole if you get into ETLs, but if anything, at a Jr. position ( Hopefully you're not the only person working with SQL at this place), you'll most likely just be learning their existing SSIS packages which is great. Good luck! 
Are you handling @ReportDate the same in both data sets? I.e are they both using =, are they both using &gt;. Are the date fields both the same format, not date for one and datetime for the other?
I am handling @ReportDate the same in both datasets, using &lt;=. Neither of these datasets are stored procedures, just dataset queries. Thus I'm not declaring the parameters, just referencing the @ReportDate. The first time I reference @reportDate in dataset1 I'm doing some error checking saying IF @ReportDate &gt; @Today BEGIN GOTO ErrorFutureDate; END; The first time I reference it in dataset2 I'm converting it ; WITH dte AS ( SELECT CAST(CONVERT(VARCHAR, @ReportDate, 101) AS DATETIME) AS day UNION ALL SELECT DATEADD(DAY, -1, day) FROM dte WHERE day &gt;= DATEADD(DAY, -5, @ReportDate) ) That might be my issue. When it comes to the actual where clause where I'm using the parameter as a filter they are handled the same, but this first spot where they're referenced is very different. 
I think you want to put that in the WHERE clause, so something like SELECT GROUP_CONCAT(name) from books where name like '%some_book_name%'
FYI, these frequently happen when you use a lot of Microsoft apps that have "smart quotes" enabled. Don't write SQL queries in Word, WordPad or Outlook. Stick to a purely text based tool like Notepad or SQL Studio.
Make sure to check out the changes for SSRS in SQL Server 2016. Interviewers like to see that you have put in the effort to learn and keep up with trends. 
&gt; CAST(CONVERT(VARCHAR, @ReportDate, 101) AS DATETIME) What is this nonsense? Are you converting a datetime to a string and back to a datetime again? Why???
Thanks. We've actually started doing that through wikispaces (on Atlassian). It's slowly coming together, but is a process in itself. Thanks
if your product runs on SQL server - I can't recommend using visual studio Database projects enough... 
You shouldn't need multiple VMs... just multiple databases - unless you are doing something that will change the server itself... The one problem we run into with this is - you may start to need to start taking a prune of your database or just a smaller subset of data as our production database is multiple terabytes... having one of those for every developer is... extreme... so we now have a custom SSIS script that creates a "mini" dev databse off our weekly full restore - and anyone who needs a fresh copy grabs that.
Well there's a small chance that code was part of the problem, depending on the regional settings of the server. But apparently you've already solved the issue. Mind sharing what you learned? Or are we just here to serve you??
&gt; Since =Today() is already in the correct format I didn't need to convert @ReportDate to a varchar 101 in my second dataset Wow, that's remarkably close to the problem that I identified, which you in turn ripped me for. Hmm.
On the server side, caching and snapshots, make people feel good if you know how to handle large or long running reports, different ways to execute subscriptions, email, file drop, etc.., on the sql side any familiarity to querying the execution log to find report errors and identifying long running reports, makes people feel like you super know what you're doing and you're super proactive. :D Some might be overkill but details.
Sorry, I was directing that towards OP to expand upon your response, ;) you seemed plenty familiar.
I just realized you weren't OP. Have an upvote :)
SQL queries for mere mortals. First time I'm venturing into programming
Is this for a reporting operation (like loading of dimension tables in a data warehouse) or an application? The answers that make the most sense will ultimately depend on the greater context of the workflow/platform. Also, design question, why does this data have to be duplicated into two tables?
It's an application. It's Microsoft Dynamics Navision 2013. The first table is the table that contains the products and their data, the second table is a related table that contains the Product Group and the Item Category registry. The way the application works currently, it allows for the buyer to register a new Product Group into a new category on the Product Table without first inputting it into the Product Group table. I can't fix Dynamics NAV, and I need to integrate this data into another application for our Cashiers system, which requires that the data is inputted into both tables. The integration job stops once it finds a item that is broken, and it has to be restarted after this item is fixed. It takes about an hour to run this job, and I am facing multiple registry errors due to the buyers not being well-instructed or careless. So it leaves me right now with the only choice of going through the database to find which groups where not created, so I can at least mitigate the amount of times I have to restart this integration job. Only problem is, I suck at SQL :(
Like this? &amp;nbsp; Declare @table1 Table ( ID int not null identity(1,1) , ItemCode varchar ( 50 ) not null , Category varchar ( 50 ) not null , ItemGroup varchar ( 50 ) not null ) Insert @table1 values ('D55-01-01', 'PERFUM', 'CH') , ('D55-01-02', 'PERFUM', 'ARMA') , ('D55-01-03', 'PERFUM', 'CK') , ('D55-02-01', 'CLOTH', 'ARMA') , ('D55-02-02', 'CLOTH', 'GUESS') , ('D55-02-03', 'CLOTH', 'CK') Select * From @table1 Declare @table2 Table ( ID int not null identity(1,1) , ItemCode varchar ( 50 ) not null , Category varchar ( 50 ) not null , ItemGroup varchar ( 50 ) not null ) Insert @table2 values ('D55-01-01', 'PERFUM', 'CH') , ('D55-01-02', 'PERFUM', 'ARMA') , ('D55-01-03', 'PERFUM', 'CK') -- , ('D55-02-01', 'CLOTH', 'ARMA') , ('D55-02-02', 'CLOTH', 'GUESS') -- , ('D55-02-03', 'CLOTH', 'CK') Select * From @table2 -- exists in table 1 but not in table 2 Select t1.* From @table1 t1 Left Outer Join @table2 t2 on t2.Category = t1.Category and t2.ItemGroup = t1.ItemGroup Where t2.ID is null
Exactly! Thank you for saving me a ton of time, my friend. I owe you dinner. I need to brush up on my SQL :(
First, thank you for the detailed reply! Please keep in mind this is more of a higher-level design discussion as opposed to an answer to "How do I insert records into a table where the combination of values doesn't exist yet?" It may not help you but it may help others in similar situations. Sadly, it's an ERP, to which I have no real knowledge of. Maybe this could somehow be implemented via that system but I wouldn't have the first idea of doing so. Ultimately, you want the second table (call this the **group** table...) to be in sync with the first (...and this the **product** table). So ask yourself what are the ways that the product table could get out of sync with the group table? Really, there's three ways I can think of: 1. A record is inserted into the product table that has a new unique pairing/tuple of (group, category) 2. A record in the product table has one of their values for either the group or category columns 3. A record is deleted from the product table so that a certain tuple of (group, category) no longer exists Ultimately, if you can find all the places those actions happen, just update that code to also insert unique (group, category) records into the **group** table. If not, you'll need to run a SQL Agent Job that runs every so often to keep these N'Sync. I would make this step an explicit SQL task within your SSIS package right before whatever operation you run for your cashier system. This lowers the time between when the appropriate tables are looked at and when the operation on those tables is conducted. This means less opportunity for either of the data in those two tables to change; though I would recommend you *stage* the data into a third area that the cashier system uses so as to not prevent the application from changing data into the **product** or **group** tables.
That makes sense. Should have clarified - we use VMs because the image already has our software installed on it. This allows us to manipulate data and test things via the front end as well as SSMS. Can you explain a bit about the "mini" database? Sounds cool.
Thanks. I guess I am not clear on quite what you mean for my Locations table (or maybe I was not clear enough in my example). The locations table simply has two columns. One uniquely identifies a company, the other lists a state where that company has resources. Company|State :--|:-- 1|IL 1|WI 1|MI 2|IL 2|OH 3|IN etc.|etc. Is that not all of the cross referencing I need there?
If you notice, you have IL listed twice. Once for company 1, and once for company 2. Because IL is listed twice, that considered to be duplicate data, and thus isn't in 3NF. With my suggestion of putting states into their own table, you can reference the primary key of the State table in your Locations table as a foreign key, and now company 1 and 2 will share a reference to only one record for IL, and thus your data is now not being duplicated anymore because of the new reference. In this case, it may not be necessary to go this far, however, if you ever need to track more information about a state, or even use State information with another table, all you would need is to reference the State table. Hope that makes sense. Cheers! 
&gt; I started out as a junior business intelligence developer. It was mostly an entry level position and started at 55k USD annually. I think it's on the lower end of the spectrum don't quote me on that. ~ */u/Error-451*
Well its a bit more than what i said but sure I can go over it... though it is probably over kill for most situations... 1st one thing you need to know is we keep a fairly fresh full copy of our database for our testers to be able to play in something as close to the live environment as possible - it is generally refreshed every time we do our full weekly backup (we don't refresh it with incrementals through the week... mostly just because idk...). Anyways when that "test" environment is created, we have a few scripts that run as part of restoring it - some are for cleaning the data so we don't have to worry about e-mailing a customer, or having valid cc info in a test database etc... but one of the last ones to run creates what I call the "mini" database for developement. Basically the idea is simple - reduce a 3.something terabyte database to something that I can put on my laptop and take home without sacrificing the ssd. It was a pretty straight forward process - developers mostly don't want or care about documents/pictures or other blob data stored in the database so we came up with ways to replace them with standard test versions so as a developer you still get a file and not an error, or a picture, or whatever the blob was... Next the biggest thing is historical data - you still want some historical data so we came up with queries to get a random 5-25% depending on how big the tables were... lastly we needed to make sure we still got all of the status tables and link tables and what not... In the end it is an SSIS package that runs every week it has a few steps. 1) run the scripts to create an empty database (generated from VS Database project) 2) Move data from freshly generated weekly backup to new dev database (again only a fraction of the data and we clean it as i said above). 3) Create a backup of dev database those backups can be used by/distributed to developers Sorry I guess that turned into a bit of a wall of text... honestly it sounds like more work than it was - it took about a week to create the package, and it takes maybe 5 minutes to edit it when we change stuff... The one thing about creating the package is - figure out what your biggest tables are and focus on them - then just bring everything in for the rest of your smaller tables once you get to a certain point.. (e.g your saving yourself 350gb by writing a custom query to trim that big 400gb table, but its not worth doing the same thing for the table with 10 rows in it.
So you just need to know who, what, when? You want an audit table. http://dba.stackexchange.com/questions/15186/what-is-an-audit-table http://stackoverflow.com/questions/201527/best-design-for-a-changelog-auditing-database-table
Thanks for the help all!
 SELECT date_trunc('month', "public"."invoice"."date") AS "date", SUM(CASE WHEN "public"."invoice"."facebook_tool_amount" &lt;&gt; 0 THEN 1 END) AS "facebook tool mrr", SUM(CASE WHEN "public"."invoice"."home_valuation_tool_amount" &lt;&gt; 0 THEN 1 END) AS "home valuation tool mrr", SUM(CASE WHEN "public"."invoice"."texting_amount" &lt;&gt; 0 THEN 1 END) AS "texting mrr", SUM(CASE WHEN "public"."invoice"."real_leads_amount" &lt;&gt; 0 THEN 1 END) AS "real leads mrr" FROM "public"."invoice" GROUP BY date_trunc('month', "public"."invoice"."date") ORDER BY date_trunc('month', "public"."invoice"."date") ASC
Well changing that part of the code didn't change anything, still have the same issue. I tried changing the name of the dataset from "Variance" to "Dataset2". I know this won't do anything but I'm grasping at straws at this point. I also changed the timeout time in seconds on dataset2 from 0 (the default) to 20 seconds. I updated the subscription time so it sent to me again and that appears to have done the trick. Well find out for sure when it runs again tomorrow. Edit: Found the real issue. Both datasets shared the same datasource which means the datasets run in parallel, not in the order which I see in report builder. Dataset2's data is dependent on a table which get's populated during the execution of dataset1. Dataset2 was finishing before dataset1, thus it wasn't picking up current days data. I updated a setting on the datasource to make it so that the datasets run 1 at a time in the order in which they're shown in report builder. The setting I changed was checking a check box in the datasource properties called "Use single transaction when processing the queries".
it seems your record is stuck (possibly, you are either old enough or hipster enough to know what happens when the needle keeps jumping back on a vinyl record) to implement a many-to-many relationship in a database, you need an association table... ... **which is what OP's Locations table already is**
Nevermind, brain lapse it was. declare @StartDate datetime , @EndDate datetime set @StartDate = dateadd (mm,datediff(mm,0,getdate())-2,0) set @EndDate = dateadd (mm,2,@StartDate) For anybody who needs this, or if anybody would like to suggest a better alternative.
[removed]
&gt;When dealing with any RDBMS, you should always use a primary key. This will ensure referential integrity and eliminate any duplicate data from populating in your database. This is super super misleading - creating a surrogate key on its own does nothing to protect against or eliminate duplicate records. I am not saying surrogate keys are bad - it can be easier to create foreign keys in other tables, and it makes updating a specific record easier...
Awesome, thanks. I'm still a little confused on `put_line`. Do I need to iterate over the result set and call it every time? And say I run the `SELECT` query. What do I need to do after that and before the `put_line` function?
&gt; Do I just create a numbered primary key? yes, i definitely would if any other tables are going to reference a result if not, i might not bother declaring a PK at all... (disclaimer: no experience with PostgreSQL... PK might be required)
Just commenting to find this again, I case someone else has some good ideas. This sound very much like something I could use at work. 
I'm not going to say this is "better" but it might be easier on the eyes, based on your preference for the DATEFROMPARTS() function: set @StartDate = DATEFROMPARTS( DATEPART(YY, getdate()), -- This year DATEPART(M, getdate()) - 2, --Two months ago 1 -- First day of month ); To me, it's a little bit more natural than DATEDIFF'ing against the date "0" but it's also not a big deal.. They're both quick &amp; correct ways to get the first day of some month.
I like this answer.
I think it would be helpful if you copy and pasted exactly what the assignment is. I have no idea what you're trying to do.
What do you want your output to look like? This: | userid | question | answer | | :--- | :--- | :--- | | 1 | q1 | a1 | | 2 | q2 | a2 |
It it helps my current query is: SELECT q.userid, question, answer, firstName,lastName FROM BSwift_MemberPlanQuestion AS [Q] INNER JOIN Bswift_Member AS [BM] ON q.userid = BM.userID
Do you only need to pivot on these two questions? Something like this would work. I changed the join to LEFT (outer) so you will be able to see if any Members have missing answers. SELECT BM.userid, Q1.answer as [Q1answer], Q2.answer as [Q2answer], BM.firstName, BM.lastName FROM Bswift_Member AS [BM] LEFT JOIN BSwift_MemberPlanQuestion AS [Q1] ON Q1.userid = BM.userID and Q1.question = 'Q1' LEFT JOIN BSwift_MemberPlanQuestion AS [Q2] ON Q2.userid = BM.userID AND Q2.question = 'Q2'
Create a table called ProductOpportunityCalculations that has two columns: product and calculationName. Then you include the table on a join (by product) and build a case statement with 6 whens, each one coded with the calculation to be performed for calculationName. EDIT: I see now that you're trying to produce 52 columns, each one representing a product. In that case, you'll need to look at pivoting your result set. You're going to have to put a little elbow grease into this one ;)
It would be: userid, Q1, Q2 1, a1, a2
That is really close! So in the data is the question table is the text of the actual questions. So I'm I thinking correctly that if I change Q1 to the text it would work? 
&gt; That is really close! So in the data is the question table is the text of the actual questions. So I'm I thinking correctly that if I change Q1 to the text it would work? Look down in each of the JOIN predicates. From: and Q1.question = 'Q1' To: and Q1.question = 'Your Question 1 Text' And from: and Q2.question = 'Q2' To: and Q2.question = 'Your Question 2 Text'
IDs would be however many people are in the population. And right now there are 11 questions
take as long as you want, LOL
you need a toppings table then `SELECT * FROM toppings` gives you "all pizza toppings for your pizza restaurant database" your Customer_Order table has the wrong PK -- remember, a PK can contain only unique values, so the way you've set it up, a customer can have only one order
 1. List all problems of existing tool 1. List all features that would be great to have in new tool -- severity, logging, auditing, and performance should really be handled by their own sub-systems but easily interfaced-with via stored procedures or SSIS packages. 1. In one big list, rank both problems and features from most important to least important 1. Brainstorm with other ETL developers or SQL gurus on how to solve each item of a top X of combined problems-features list multiple ways 1. Determine platform or pattern that solves the most amount of problems with least amount of effort using an easily repeatable and maintainable pattern or algorithm -- ie, pareto principle. 1. Sell to users/managers via stating the list of problems you created earlier, and how the solutions you and others came up with will save time in some fashion (less users trouble-shooting exceptions, less developer and business user time spent entering a new exception, less developer time spent coding up an integration with the system, less CPU time spent on churning the data...) Additional questions are... * How often are exceptions created? * How often are they updated? * Who needs to be notified? * What action needs to be taken? * How quickly does the action need to be taken? * Do I implement this change in every single process that loads data? Or do I implement this as something that runs every Y minutes?
Thank you so much. After giving up using "IN", I did it the following way and got the same result: select(case when substr(job_id,1,2)='IT' OR substr(job_id,1,2)='AD' OR substr(job_id,1,2)='AC' OR substr(job_id,1,2)='PU' then 'Administration' when substr(job_id,1,2)='SA' OR substr(job_id,1,2)='ST' OR substr(job_id,1,2)='SH' THEN 'Sales' ELSE 'Other' end) AS "Job Category", count(job_id) AS "Count of Emp" from employees group by case when substr(job_id,1,2)='IT' OR substr(job_id,1,2)='AD' OR substr(job_id,1,2)='AC' OR substr(job_id,1,2)='PU' then 'Administration' when substr(job_id,1,2)='SA' OR substr(job_id,1,2)='ST' OR substr(job_id,1,2)='SH' THEN 'Sales' ELSE 'Other' end; As far as "smart" quotes vs. "vertical" quotes, when I am in MS word and I press the single or double quote key immediately to the left of the enter key on the right side it marks it as smart/curly quotes, but the same key on MS note pad marks it as "vertical" quotes, just like typing here. Why? I have been writhing the codes in MS word and then copy and pasting to oracle sql command line and that is why I kept getting errors. I think I will use the MS note pad.
Make your current query a subquery and then put the "not relevant" condition on the outer query 
&gt; After giving up using "IN", why? did what i posted somehow not work? did you copy it exactly? &gt; I think I will use the MS note pad. smart move (pardon the pun)
I can only help you from an Oracle perspective which has the quirk that an empty varchar is null. You can try to trim your fields: select OrderId ,coalesce( ltrim(Part1Gen2) ,ltrim(Part2Gen2) ,ltrim(Part3Gen2) ,ltrim(Part1Gen1) ,ltrim(Part2Gen1) ,ltrim(Part3Gen1) ) Difficulty from your_table Edit: Typo
Oh, that's a lot easier. Create the table like I suggested with the "6 WHEN" CASE statement that contains all of the logic for final opportunity calculations, then let Excel do all the pivot work.
Thanks! I didn't consider using the trim features to sleuth out valid entries. I actually found a NULLIF function that ignores whitespace, so i'm back in business. Thank you for the suggestion!
&gt;All inputs for the programming problem are from STDIN and output to STDOUT. This is *the* most confusing way you could possibly run a SQL challenge.
I say keep it simple: WHERE ROLE_CODE NOT IN ('MAM', 'MAP', 'SUP', 'SHIP')
&gt; Oracle ... which has the quirk that an empty varchar is null. The most annoying aspect of Oracle &gt;:(
Could I run everything on my surface pro. Mind my still developing knowledge but I believe I also read the express edition was light weight and you could install a DB on the same machine as running SSMS. Didn't want to create a big resource hog. Another idea... could I install the DB on a USB flash drive?
If you can run express on the surface pro, you can probably run the developer edition on it, but I'm not 100% sure. You can always turn off additional services and features that are not in use. You also specify the ram used by SQL, CPU wise, it's going to use what you're cranking. You can install the software on your pro, and have the DB files on the flash drive. I have a lightweight small SSD laptop that has a few 200GB DB's on a flash drive. Just make sure you load / unload it properly or you will corrupt your databases. 
Here's the really cheap version : select ROLE_CODE, PARTICIPANT_ID, ROLECODE from (SELECT ROLE_CODE, PARTICIPANT_ID, DECODE(ROLE_CODE, 'MAM', 'MAM', 'MAP', 'Asset Provider', 'SUP', 'Supplier', 'SHIP', 'Shipper', 'Not Relevant') ROLECODE FROM PARTICIPANTROLES) Sel1 where sel1.ROLECODE &lt;&gt; 'Not Relevant'
&gt; such as when you run "ping" what's ping? is that a dialect of SQL?
http://pages.swcp.com/stocks/#historical%20data http://opendata.stackexchange.com/questions/4116/stock-market-historical-data 
And we have a winner. This is the exact sort of thing you would use SSIS for. Edit: Pretty sure you could also use SSIS to send all the pdf's to their appropriate destinations, as well, assuming that's the ultimate intent of generating the flat files. 
Microsoft must be feeling some heat. This seems more like an appeal to ISV's than to customers. But it sounds like a good change for everyone.
Be interested to know if this includes Sharepoint and SSRS Sharepoint integrated features.
my main development machine is a Surface 4 pro - I had to pay for the memory upgrade but that is all I needed... the surface is a beefy machine and I love the thing... any RDBMS is going to use all the memory it can take though - so I recommend if you use the device for anything other than database development - make sure you don't have the SQL service starting up on computer start up.
Please don't use mysql. You will be so limited. 
Can't think of anything major since they're equivalent... might be easier to use `space(0)` in situations where you're embedding a SQL statement, so you can use `space(0)` instead of `''''`: DECLARE @sqlString NVARCHAR(256) SET @sqlString = 'if '''' = space(0) print ''equal'' else print ''not equal'' ' EXEC sp_executesql @sqlString
ALWAYS. Never make the person maintaining your code count spaces. If there's a function that'll do it, use that function.
honestly it's just tough to figure out what is going on with this. you're making a way for people to block notifications? so those notifications would presumably be hidden in some later function? am i understanding this at all? does your notification table have a "is_blocked" column or is that being handled out of database some how?
I wasn't sure how to design that part of the database so I designed what I thought would mean ' the notification will only be added to the user's notification table if the profile that the notification is coming from is not in the user's block list'. Does that make sense? One of the requirements was to allow the user to deactivate notifications from certain groups, users, or events.
Yikes, How about a "knowledgeable" SQL interviewee?
Where is the documentation that SQL can do e-mails? Im unaware of this functionality and would like to look into it.
That's some complicated shit, I only know what I've taught myself, but I've done some kick ass joins and casts. 
 * Basic knowledge: What's the difference between a WHERE clause and a HAVING clause? * Intermediate knowledge: Tell me about two different methods to select all rows from table A where column 1 does not have a match in table B * Expert knowledge: Given a table consisting of invoices submitted by vendors, what method would you use to select the top three invoices, by invoice amount, per vendor?
Thanks for the reply. Sorry for how bare bones the ER diagram is. He was more concerned with the sql code so that was a mockup and I wrote up the cardinality and foreign and primary keys on a separate sheet of paper. Would you I would be better off creating the BlockedID and BlockedIDType tables or just using another programming language to make sure my database never gets populated with incorrect information? Like in another program query the database for a persons block preferences and then put the notification in their notification table if they are not in there. Sorry if that doesn't make sense.
Wouldn't storing notifications in the user table create A LOT of redundant data though? every notification would have a bunch of useless information tagged to it. Like the school you attend, your email address, and all the like.
No, it would all be in a single table. So say I (UserID 5) wanted to block you (UserID 1). The blocked table would have a value of (5,1, 'User') or something along those lines. So when you are retrieving user notifications, you join this table with the notification table using the keys and can filter out those records. If I remove my block on you, just delete the record from the blocked table. There are other methods you can use as well.
If you were going above and beyond you would have googled around for the answer. It's not hard. Considering the lack of effort, it's pretty clearly homework. 
you're hired!
: ^ ) http://i.imgur.com/3oRf4fz.png
I use it semi often. It really depends on the type of questions you need to ask. In the first query you do, the column c no longer exists for the window function to operate on since your GROUP BY was on b and you aggregated a sum. In the second, since your GROUP BY included c, the column exists for the window function even though it isn't in the projection. Your conclusion is correct in that window functions happen after GROUP BY and HAVING. Here's an example of some code where a GROUP BY and window function are utilized in the same query. CREATE TABLE sales ( store_num INTEGER , region INTEGER , revenue DECIMAL(10,2) ); INSERT INTO sales VALUES ( 1,2,10231.52); INSERT INTO sales VALUES ( 2,2,12412.12); INSERT INTO sales VALUES ( 3,2,41231.83); INSERT INTO sales VALUES ( 4,3,26547.65); INSERT INTO sales VALUES ( 5,3,54895.89); INSERT INTO sales VALUES ( 6,4,54548.78); SELECT region , sum(revenue) reg_rev , sum(revenue) * 100 / SUM(sum(revenue)) OVER () reg_pctg FROM sales GROUP BY region ORDER BY reg_pctg DESC; SELECT region , sum(revenue) reg_rev , sum(revenue) * 100 / SUM(sum(revenue)) OVER () reg_pctg FROM sales GROUP BY region HAVING sum(revenue) &lt; 75000 ORDER BY reg_pctg DESC; RowNum|region|reg_rev|reg_pctg :-:|:-:|--:|--: 1|3|81443,54|40,748706 2|2|63875,47|31,958861 3|4|54548,78|27,292431 RowNum|region|reg_rev|reg_pctg :-:|:-:|--:|--: 1|2|63875,47|53,937829 2|4|54548,78|46,062170
Exactly this. I used to put Excel expert on my CV until I got a job somewhere with real excel experts. I'm now far, far better than I was but still consider myself to be somewhat amateur... 
They're not joking. Googling is the main skill. 
Wow, that is a very nice answer, I learned quite a few things here. 
How do you get good at googling?
Learn a few commands beyond just putting in words, like +, -, "" etc
Ok I already know those three; anything else?
&gt; What's the difference between a WHERE clause and a HAVING clause? I've always wondered, is there any significance why the HAVING clause exists? I'm sure it exists for a reason but why not just make it so you can put HAVING conditions in the WHERE section.
&gt; How are aggregate and window functions calculated when used in the same query? Hmm...that's sort of a theoretical question. What do you mean "how" are they calculated? Interview questions are supposed to be a little more practical in nature to gauge how a candidate will solve a problem. I admit that I often have to try a few iterations of some functions before I get my desired results, and I've been working with SQL Server for 12 years.
Are you planning on scrubbing any sensitive data and PII out of the production data before putting it in the dev database?
Ah gotcha, that makes sense. Thanks.
It's funny, I don't consider myself an expert. But I've written pretty complex SQL statements for reports or for querying data. Typically used stored procs with temp tables. I learned everything from Google and others with no formal training. I'm fairly confident I could pull any information out of a database once i learned the tables. I'd say I'm no where near an expert but could probably handle almost any data analyst job. Now I would never do that job. I have a business and finance background. It pays a lot more to be a liaison who can speak the language and project manage than the actual doer.
Yeah, this is definitely how I feel ... I also think that I'm screwing myself over sometimes, but the more you actually know, the more you know that you DON'T know things too. I'm amazing at VBA/SQL/Excel compared to 99% of people, but I only know what I use and I know there are amazing things that those languages can do that I've never had a reason to learn. 
&gt; ask qualifying questions to understand the question better That's fair. I suppose my response would be the same as what I put above: "what do you mean by "how". Then we can go back and forth so you can gauge my knowledge. I actually enjoy those sorts of interviews. &gt;I personally feel that an expert should know when JOIN criteria, WHERE, ORDER BY, HAVING, GROUP BY, AGGREGATION, WINDOWING, and the actual projection happens and in what order. Definitely agree, but these ideas are little more well defined and more easily explained. 
&gt; , sum(revenue) \* 100 / **SUM(sum(revenue)) OVER ()** reg_pctg What is this wizardry? /runs off to play EDIT: Holy cow! **SUM(val) OVER ()** is cool. Thanks for sharing, this is a nice technique that would have saved me writing many (many!) CTEs over the past year.
If all you're using are INNER JOINs, why not just use WHERE for everything? The code is much easier to read and you can set up your constraints easier too.
Mass generating reports, moving those reports, emailing them. Data caching. Any time we have more than 40 active queries the DB team gets an automated email and the queries are logged (they can be accessed quicker than SolarWinds this way). Not a whole lot of stuff yet. I need to learn quite a bit more still.
Window functions are definitely very cool. The DBMS I typically use has a bunch of extensions to the ANSI specs. I would typically reference the aggregate SUM value by using its alias (reg_rev) in the window function but I didnt write it that way since most of the DBMS people use here don't allow that.
I use window functions a fair amount, but I've never thought to leave the OVER() clause completely blank, which essentially sets the window to your entire result set. In any case, you've helped me refactor a few queries. Not quite as many as I was hoping, but a few!
your real performance issue is probably the CAST... are you sure this is necessary? however, your subqueries can be made into joins SELECT d.* FROM ( SELECT MIN(CAST(txn_dttm AS DATE)) AS min_date , MAX(CAST(txn_dttm AS DATE)) AS max_date FROM dbo.transaction_fact ) AS m INNER JOIN dbo.datedim AS d ON d.greg_dt BETWEEN m.min_date AND m.max_date LEFT OUTER JOIN dbo.transaction_fact AS f ON CAST(f.txn_dttm AS DATE) = d.greg_dt WHERE CAST(f.txn_dttm AS DATE) IS NULL if performance remains an issue, perhaps try using functions on greg_dt instead, to make a straight compare to txn_dttm 
I've learned over my many years of conducting interviews that reading resume's is basically worthless as an indication of skill. It's good to show role history, length of work at jobs, role changes while at a company, etc. But as an indication of ability... nada, nothing, zip.
It's all relative. If you are in a smaller shop or otherwise similar bubble you can very well feel like an expert. Or maybe you've just got really good at the type of querying you do. I don't think everyone is intentionally dishonest.
SSIS package exec SQL task to extract and assign an object with all the possible values for each loop container to execute each instance of that object inside the for each loop, complete the task assigned &gt; dataflow to send records to flat file file task to rename email task to send the flat file via email or another file task to send the file to a share sound about right? 
Google "sql server levenshteins distance" and you'll find a guide to do it. (In case you didn't know, Levenshteins distance is the editing distance between two strings) Anyway - if you have access to SQL sever 2016 I'd suggest you look into doing it in R instead as it has a library exactly for this purpose. 
1 - Which RDBMS are you using? MSSQL, MySQL, Oracle ... 2 - What is the data type of your lecture.ends and lecture.starts fields? Datetime, string or something else.
I don't agree with this at all. "Expert" carries a certain connotation. It doesn't mean "good" or "very good", it means you've mastered its ins and outs. If you work at a small shop and you're the go-to SQL guy, but you don't know, for instance, how the server even searches for data, or how to peruse an execution plan...even if it's relative, them saying that shows a lack of perspective and actual understanding. If you just operate in your own sphere and don't try to break out of it...that's bad.
PSQL AND TIME
"which start before 10:00 or end after 18:00" is what you want "lecture.**ends** &lt; 10 OR lecture.**starts** &gt; 18" is what you wrote you have it backwards 
doest work with that I get this message: HINT: No operator matches the given name and argument type(s). You might need to add explicit type casts. 
Ah cool. We're actually in the process of migrating to 2016 at work. 
it was just a toy but was reasonably fast. can provide spelling suggestions for queries
oh damn that's cool though. so if i'm querying quickly i wouldn't have to nail the very specific spelling? i could go fast and then let the tool you made help me out after?
What flavor of SQL are we talkin here? There are lots of options. If your company uses MS SQL, then SSRS is your friend.
Mainly Oracle
Okay, food for thought.
Thats an awesome response man thank you. 
What is the data type used for your start and end? 
Ummm.. This post looks so unreadable. Don't know how to fix that.. I've tried
So is a BI solution what i should be looking into?
You want to be using Oracle APEX since you are using oracle it is likely already installed and you just need to talk to a DBA to get a listener to point to it. If you are interested PM me and I can give you a tour. Disclaimer, I am a full time APEX consultant and instructor but you literally just described the tools core objective. 
What makes you think you need an IDE to learn SQL? I recommend starting with the shell of whichever database server you're using. This will help you understand the underlying commands before obfuscating them with an IDE.
nfojones was on target with his/her reply. If you're having trouble even getting started, we can help point you in the right direction. If you're stuck somewhere, same thing. But right now, this request feels a bit too broad.
I never said I need an IDE, I was just asking if you knew of any that you would recommend. 
Download SQL Server Express. It's free. The Management Studio included in the package is a good GUI. It provides options to script tables, views, stored procedures, anything really. It also gives you templates for creating all the various objects in SQL that you'll need in any database instance. You can learn a lot of SQL code just by looking at the outputs it provides. I'm sure MySQL and others provide similar features but I've found MSSQL to be the most intuitive. 
Which DB are you using? If it's MySQL, then http://www.heidisql.com/ is really great. Very easy to use, and doesn't get in your way like I've found many others to. I'm also using it with Postgres, but it only partially supports it at this stage. It also support MS SQL, but I've never tried that. 
You will need to have a database running before using Datagrip. Postgresql, mysql and sql server express are some free examples. In Datagrip you then setup a new project under File -&gt; Project. You enter a name and there's your project. Within that project you will have to setup a new data source, connecting to the database that you have running. You can now start querying that data source. Let me know if you have any other issues 
Lean to cross read walls of text. Don't read it, just scan it for "things" related for what you want to find. That to me is being good at googling.
And result set is what I'm looking for
There's a name for this effect that's escaping me at the moment. It basically boils down to the fact that the more you learn about a domain of knowledge, the more aware you are that your understanding represents a small fraction of the possible knowledge you could have. This is why you generally see novices at programming saying "I'm an expert" and multi-decade veteran DBAs saying "I'm passable."
Well, obviously I would not have asked for a 'talk through' if I understood the Wikipedia page...
can't see anything wrong with your LIKE syntax may i suggest you look at your cursor logic
Full disclosure: I'm self-taught on what I do know. So I don't know any of the proper names for anything. May I ask what cursor logic is?
Can you be a bit more specific? I'll give an also generic answer: Making a SELECT 
I know we try to be beginner friendly but this is hilarious, I actually laughed.
and yet you didnt upvote smh
You guys are funny. Lets put it this way, is it possible for a database trigger to send data to my application ? If yes then how ?
Which part of the wiki are you having trouble with?
Databases don't typically push data. Your application would query the data from the database. You could set up a job to push out an extract but that's unlikely to be ideal if we're talking about a distributed application. Tell us more about the application.
There is a ProveIt test for that. Had an applicant list "fuel boy" at an airport as a previous job. I asked, "If you mix any two grades of aviation fuel what color does it turn?" He did not know. WRONG ANSWER! I told him to stay the hell away from ***my*** airplane. That is a Day One kind of thing. How to check for fuel contamination. Then remember that expert is a two part word. ***Ex*** is a "has been". ***Spurt*** is a drip under pressure.
A good friend of mine put this together using Oracle, Node, and the oracle-node driver. As I think he states it is a little hacked together, but it is pretty cool. https://jsao.io/2015/02/real-time-data-with-node-js-socket-io-and-oracle-database/
DBMS? SYBASE ASE, selecting your two dateadds I get an error on number and type of parameters I'm guessing that it's arguing about the integers. Query B has a positive 1, query A has both datediffs with negative 1s. That seems wrong. SELECT DATEADD( Day, -1, CONVERT( DATETIME, CONVERT( DATE, Getdate()))) -- Yesterday , DATEADD( Year, -1, DATEADD( Day, -1, CONVERT( DATETIME, CONVERT( DATE, Getdate())))) -- yesterday minus a year , DATEADD( day, 2, DATEADD( Year, -1, DATEADD( Day, -1, CONVERT( DATETIME, CONVERT( DATE, Getdate()))))) -- yesterday minus a year plus 2 days. GO
My take on this is that Microsoft is doing everything that it can to kill Oracle without buying them.
It worked. You're the best! 
Yes! Make a view so you never have to look at that again! 
i'm gonna have to go with column b on this it's all fuct up even the tables themselves have no consistent naming scheme, key will be called loc in some, locator in others, id in others still it's a clusterfuck
The mixed case? Yes, but I've learned to live with it. My job has our databases made up from a third party program that extracts data from what is basically a flat-file application, and we end up with A_Lot of MIXED_CASE column_names.
What's the issue here, that they mix all caps and camelcase? 
Yes, but have you seen a mixed camel case and underscore case in the SAME column?? I have this in a couple tables in our OLTP.
Or possibly a live not-animal.
Let me just add the parenthesis at the end. I ll correct it right now. ps : I assumed you are using MySQL.
I work in about 10 different databases (and the many apps that use them) and they were all created with different standards by different people and then modified hundreds of times by said different people using different standards over a period of 10+ years. Some of these databases have data that sync to each other and some stand alone. There isn't much consistency in the naming anywhere. Up until recently we didn't have management that gave a crap about standards so it was everybody for themselves. Now all those people that were supporting these databases have gone and it's just me supporting them as legacy systems. I feel like an archaeologist. I keep the Tylenol handy, the coffee warm, the music loud, and the duct tape sticky. It's all good because I enjoy a challenge. 
that's correct. UNIT is an integer. I left out the ', ' + in front of the second column to simplify the example. i will try this thanks!
I work with horribly designed ERP databases. This record could be anything from a valid ledger transaction, to a customer that just got some product shipped to address NULL. Just another day at the office :'(
Pink Unicorns. You never know. 
any help?
10,000+ with a 5 person team 
Wow! That's a hell of a lot. How do you manage them efficiently? It seems like a lot of databases per person to manage. What software do you use to monitor database backups and transaction log backups? I am the only DBA in the organisation I work for, while I look after more servers and instances, I do not look after as many databases.
Do you feel that is too many databases per person to look after? Or do you feel it is completely manageable? What software do you use to monitor everything?
Instances, probably 700-800. Servers, probably 300-400. Of that, production is probably 150 servers and 300-400 instances. All hosted in the US. DBA staff is three guys. Fortune 500 retailer.
Very carefully. A mix of custom scripts/jobs, reporting, and spotlight
5 DBAs. 150ish SQL Server instances with around 7,000 DBs 10 or so very large DB2 databases (our biggest data repository - everything builds off of these more or less) 5 very large Oracle DBs per environment (4 envs) A few MySQL DBs One Postgres DB that won't seem to die.. edit: Also number of databases is a pretty bad metric to measure this by.
&gt; only those classes which are in schedule sounds like you want an inner join here are you fully sure you want FULL outer joins?
To add a contrast, I support 1 production database. There's 1 DBA (me) and I also manage a team of 4 network/server support people. I support about 15 dev/test instances. 
What made me choke was the IS_VISIBLE coming back NULL. Having a binary that is NULLable makes no sense. The fact that some columns are all upper and the later ones are upper and lower just means, to me, that new columns were added the right way and that the old wackadoodle stuff did not got changed.
1) In your select statement, what is P? Table alias? I think that is incorrect. 2) for a select into, you have to restrict it to a single row. Otherwise you'll get an error at runtime. 3) you also have to update records corresponding to that single row.
Try replacing your multiplication with space padded zero prefix. for example, `*.9` with `* 0.9`
Okay, I have made some progress. However now I am getting my entire SalePrice column set to the same value. It seems each iteration of the loop writes the entire SalePrice column to be the same value. So the value that I end up with is that last x.p_standard_price. create or replace PROCEDURE ProductLineSale is current_price number(38); BEGIN for x in (select * from Product) LOOP if x.p_standard_price &gt; 400 then update product set SalePrice = x.p_standard_price * .9; elsif x.p_standard_price &lt; 400 then update product set SalePrice = x.p_standard_price * .85; end if; END LOOP; END; / Any help would be appreciated. Thanks!
Nobody is going to tell you how to do your homework. It seems like it's a pain because you haven't been paying attention to lectures. A quick google search on the syntax of the commands you are attempting to use might be a good start if you don't plan to read your textbook. Find out what an identity column is and how to use a where clause. 
Thank you everyone who commented. I found it very insightful and interesting to see how much variety there is in terms of servers, databases to people who support them ratio. :)
Using CONVERT with option 101 and then casting back to DATETIME is dangerous, since option 101 is explicitly 'mm/dd/yyyy' but the string-to-datetime cast will use local regional settings for interpretation. Also, why do you believe this only works for 2012 or newer? Select Cast(Cast(GetDate() as Date) as DateTime)
This is really a poor design, you should either specify start/end date right in the query: SELECT * FROM my_view WHERE start_date &gt;= @startdate AND end_date &lt; @enddate , or you should use a table-valued function instead and pass dates as parameters to it.
Yes, but the problem is I do not have the start date &amp; end date at design time. I am asking the user at the front end to input start date &amp; end date and based on that, I am showing results to user. 
Why not create a mainframe job (assuming z/OS)? Create a catalogued procedure that runs a SPUFI step containing your update statements. Make one of the parameters to your catproc the DB2 subsystem id. Then just build a JCL deck containing repeated calls to your catproc with different DB2 subsystem ids. Disclosure: I have not done this in decades.
&gt; I do not have the start date &amp; end date at design time This is programming 101. NOBODY has user-entered parameters at design time. Why can't you use a WHERE like /u/nvarscar said? Where exactly are you creating this temp table?
Based on this and your other responses in the other threads you've started with this same question, the only conclusion I can reach is that you're approaching this completely wrong. Post your code so people can point out what you need to fix.
The point is, I'm trying to avoid JCL. I simply can't stand it. But thanks!! Haha
I tried both ways. I have to find those that are not in schedule and those that exists only in schedule.. Both of them in their own columns.
Yes.. If I'm understanding your question properly... the `WHERE` clause can / will limit the rows returned from ALL tables, the one listed in the `FROM` clause, as well as any listed for any `JOIN`s Customers: CustomerName|CustomerNumber|Country :--|:--|:-- Widget Corp|001|USA Gadgets 'R Us|002|Canada Contoso|003|Mexico This will return all customers: SELECT * FROM Customers This will return ONLY the customers in Canada: SELECT * FROM Customers WHERE Country = 'Canada' This holds true for when you join a table as well. You will want to select the columns from `Project_T` where Location = 'X' then join that table to Department_T, THEN join that table to to the Employee_T table. Something like: SELECT [columns here] FROM Project_T INNER JOIN ... ON ... = ... INNER JOIN ... ON ... = ... WHERE Location = 'X'
Okay! This is the conclusion I was slowly coming to. Thank you so much for the confirmation.
That is essentially my message. I understand not being able to access the production system, but at a bare minimum I will need a sandbox and sample schema to develop in.
Not enough context. How is that you're getting data for your vizzes if you dont get to access the database? Published data sources, extracts, external excel/csvs, something else?
I'm not. :) They just plan to give me dev access to Tableau and to work off existing visualizations to create new customizations, which I think is doable, but they additionally want to validate data from Excel workbooks and their internal database which both leverage diffent "ETL" processes. So I'm basically saying that I need SQL as a critical function of the job.
Yeah, we have been triple checking the landing tables in Dev, UAT, and Prod and they all have the same types. It's possible the source data changed a bit (we don't own that) but again...no issues in 2 of 3 environments.
Does this error occur during validation or execution? 
Both... It comes up when we are validating packages as well as during normal scheduled runs.
This might be tangentially related: https://msdn.microsoft.com/en-us/library/ms173839.aspx
First, you are not updating rows where standard_price is equal to 400. Are you sure that this is intended? Second, why the loop? update product set SalePrice = case when p_standard_price &gt; 400 then p_standard_price * 0.9 when p_standard_price &lt; 400 then p_standard_price * 0.85 end ; You can still put this in a procedure if you really need to and it should be way more performant.
Delete the source components in each of them and recreate them. If nothing else works this is my go-to solution for SSIS problems. 
Actually, this workaround does the trick for me when all else fails, ssis is finicky sometimes. 
If these are exact copies, then it may be source data? I assume your live data is different from UAT and Dev. That would be where I looked first. Can you run a live job through dev to test?
This answer is excellent. I'll add two items for OP's benefit. First, to explicitly answer your question: it's OK if you aren't returning anything in your SELECT from the JOINS in the FROM clause. This is actually common. (Sometimes I'll return some data even if its ultimately unneeded or even redundant while testing, but in production remove them.) The other thing that is worth mentioning is that while ihaxr (correctly I would think since this is homework) assumed that there is a corresponding match on both sides of the INNER JOIN, in reality users suck at providing complete data and oftentimes developers suck at enforcing data completion. If I were assigning this as homework, I would have a situation where there was actually a missing record in the Employee_T table so the INNER JOIN wouldn't return all of the Projects from Project_T. And then ask why. This would provide an opportunity to discuss OUTER JOINS, and specifically a LEFT OUTER JOIN (where all of the projects would be returned even if there wasn't a match on either the Department or Employee).
It's a little odd for this, we are a reporting environment only. All 3 environments source data from production data. This is a data warehouse, consolidating multiple sources and systems into one place. All loads run daily and basically duplicate effort so that all warehousing development has production data to model from.
That looks very similar to running a select top 0 * from table. Interesting and didn't know this was an option in SQL. Nice Though, our source for the problem data is Oracle (I should have given more info off the bat), not sure of an Oracle equivalent, though now I'm seeing "describe [table]" could be an option.
I don't think VM's are a huge issue for SQL as long as it is set up correctly, in the past this was not always true. There are still additional headaches, but you have to make sure it's set up properly. http://dba.stackexchange.com/questions/1554/what-are-the-best-practices-for-running-sql-server-in-a-virtual-machine
Be careful with adding more cores to your SQL Server instance, you should verify you won't exceed any limits of the license you've paid for.
Good Point!
I'd almost argue that access isn't really designed to handle that large of a dataset. When you start to get datasets that big, it's best to upgrade to SQL Server. 
See below. All the temp table stuff is just setting up a test, the real magic is in the INSERT..SELECT statement. This is written on SQL Server so your mileage may vary depending on the RDBMS. IF OBJECT_ID('tempdb..#source') IS NOT NULL DROP TABLE #source; IF OBJECT_ID('tempdb..#dest') IS NOT NULL DROP TABLE #source; GO CREATE TABLE #source (Val1 VARCHAR(10), Val2 VARCHAR(10)); GO CREATE TABLE #dest (Val1 VARCHAR(10), Val2 VARCHAR(10)); GO INSERT #source (Val1, Val2) VALUES ('reddit!', 'woo!'); GO INSERT #dest (Val1, Val2) SELECT Val1, Val2 FROM #source GO 20 SELECT * FROM #dest; DROP TABLE #source; DROP TABLE #dest; GO
These may help. https://social.msdn.microsoft.com/Forums/en-US/28699c94-4d1b-43f2-bcfb-a95152ca226b/could-not-obtain-information-about-windows-nt-groupuser-error-code-0x534?forum=sqlservicebroker https://www.experts-exchange.com/questions/27833148/Event-ID-28005-in-SCOM-SQL-issue-I-think.html
I agree and hate Access, but depending on a variety of things it might be the best solution. I mean if all they're doing are simple little selects and at most someone is only spending 10% of their job working in the environment... then maybe not worth going to SQL. Honestly, as a consultant if I hear the word Access I say NO and feel that it really isn't appropriate as an enterprise solution for any reason... but there are a few uses that get a pass. 
I use the table for other things that I need to pull two results, this one report just needs one. 
Try this: SELECT ROW_NUMBER() OVER( ORDER BY FIRSTVALUECOLUMN ) FROM DB.SCHEMA.TABLE
no problem
It doesn't need to be consistent since for this either result is fine, I just want to guaranteed amount of fields so my macro work correctly. 
Can you just do a simple insert and make use of "GO x" immediately after your insert where x is the number of times you want to run the batch? In TSQL at least, you can repeat instructions using this syntax. 
In SQL, you can build indexes on the table to improve query performance. Indexes are essentially meta tables that can be used to quickly sort/join tables together. A quick google search shows that its possible to [create indexes in access](https://support.office.com/en-us/article/Create-and-use-an-index-to-improve-performance-0a8e2aa6-735c-4c3a-9dda-38c6c4f1a0ce) as well. If you can see that this will grow, i suggest putting it into a SQL server database.
You're adding the 13 into the row_number, you just keep it like 'ROW_NUMBER()'. https://msdn.microsoft.com/en-us/library/ms186734.aspx SELECT ROW_NUMBER() OVER(ORDER BY name ASC) AS Row#, name, recovery_model_desc FROM sys.databases WHERE database_id &lt; 5;
It will keep the dups, but it should create a column that is a unique identifier, it's an incremental number so each row has a unique number associated to it. The query I edited in my post if you run that gives an example of using it. It provides three columns, the row number being a unique identity. 
That data set is way too large for the Jet / Access / whatever jank-ass engine is running underneath of it.
I've seen people use Excel with that many rows so I don't see why Access couldn't handle it but yeh, I agree with you, Access sucks for all but the most basic things.
Here's how to pitch the purchase: "If our company is big enough to require, and properly utilize, an ERP, the cost of buying a server to host it is offset by the cost of operating without it. If we'd spend more money on the server and software than we're saving by using it, we didn't need it in the first place." Expect the people writing checks to understand nothing other than money. If all else fails, go ahead and try doubling up on a server until they start moaning about how damn slow everything is before reminding them that it's because of all the money you're "saving"!
Do a "numbers" table and a SELECT/JOIN on that, using the "x" to limit the numbers table. Since I'm not sure how much spoon-feeding you want, I've left it a little vague, but I would happily provide a full solution if needed. What SQL engine are you working with?
Numbers table is the way to go here (taken from stack overflow): ;WITH Pass0 as (select 1 as C union all select 1), --2 rows Pass1 as (select 1 as C from Pass0 as A, Pass0 as B),--4 rows Pass2 as (select 1 as C from Pass1 as A, Pass1 as B),--16 rows Pass3 as (select 1 as C from Pass2 as A, Pass2 as B),--256 rows Pass4 as (select 1 as C from Pass3 as A, Pass3 as B),--65536 rows Pass5 as (select 1 as C from Pass4 as A, Pass4 as B),--4,294,967,296 rows Tally as (select row_number() over(order by C) as Number from Pass5) INSERT Example (ExampleId) select Number from Tally where Number &lt;= @X
Oh I'm an expert at googling.
I've never tried it, but I'm kind of surprised MS Access can't handle that much data. I guess it depends on the nature of your queries, it is possible to make any database slow. Try [PostgreSQL](https://www.postgresql.org/). It's free, can definitely handle that much data on normal PC hardware and is generally all-round awesome. You should be able to export your table as a .csv and import it into PostgreSQL. Then you can build indexes and write your queries.
Not sure I know what you mean. If you'd be finding this information manually, how would you look it up given the data that you have? Can you transcribe that method into a "from/join" clause?
 SELECT a.albumid , a.title , g.genreid , g.name , g.description FROM album AS a INNER JOIN album_genre AS ag ON ag.albumid = a.albumid INNER JOIN genre AS g ON g.genreid = ag.genreid 
Select Q.title Z.name From ( Select A.* B.genreId From Album as a Inner join album_genre as b On a.albumid = b.albumid) as Q inner join genre as z On q.genreid = z.genreid. 
This is a clever solution. You can figure out the number of days then do the milliseconds by the hours, then multiply the number of days by 8.64e+7 and add the remainder. Again, these are huge numbers so I'm not sure what you'd want them for, but that's one way to go about doing it. &gt;DATEDIFF returns an integer, which simply isn't big enough to hold the result. The only way I can think of would be to work out how many days there are between the two dates, then do a comparison on only the time portions of the two dates to work out how many milliseconds difference there is, and add the two together (multiplying the number of days by the number of milliseconds in a day). – Alan Nov 4 '14 at 8:05 &gt;&gt;http://stackoverflow.com/questions/26730173/sql-server-datediff-function-resulted-in-an-overflow-when-using-millisecond 
Using second instead of millisecond and removing the divide by zero would functionally be the same? So... DATEDIFF(SECOND, ConnectedDateTimeUTC, TerminatedDateTimeUTC) instead of DATEDIFF(MILLISECOND, ConnectedDateTimeUTC, TerminatedDateTimeUTC) / 1000
This would have been a better post if you had provided some follow up to "Use views when they make sense, but don’t use them every time." How do developers decide when they make sense? Exposition or examples of this would be great.
My bosses boss who loves to break from her role as a senior manager and dabble in sql and writes atrocious code. She writes code this way because she doesn't understand fundamental database design. My boss has asked me to explain it and I finally think I have the analogy that really captivates it. She creates temp tables and joins them because she is like a soup kitchen cook who only knows how to make "chicken soup for 2" and she needs to make it for 100 people so she makes it 50 times. Cursors, temp tables, inserting into a result set to be finally returned because she doesn't understand that she can't structure her statements like she did 20 years ago with SQL2008+. To be a better SQL developer you have to take into consideration database design, the sql optimizer (how is sql going to interpret this), and ultimately how is the sql engine going to carry out the task. How to get around creating temp tables? Use inline queries or CTE's. They can be very efficient. If you must, you can also use table variables with a primary key although these aren't much faster but the clean up easier as they exist in the transaction only. Creating small heaps of datasets is not a workaround solution. It's terribly inefficient and is painful to rewrite these when they get out of hand. Also, We find these always having granularity issues too. Insert into #temp Select userid, 'User is fun!' as Description From someTable Where fieldA = 1 and FieldB = "Fun" Insert into #temp Select userid, 'User has Good Times!' as Description From someTable Where FieldB != "NoFun" and FieldC = "Good Times" So if there is a record #temp most likely contains a user who is Fun and who also has good times. Wonderful! How long has our fucking accounts payable been double paying users who like fun good times? Never change granularity. You're queries should always have a TRUE granularity, even with aggregates. 
I just typed up this big reply where I wrote about cursors and how he can loop through for each individual fruit, when I see this. The answer is always simpler than my initial thoughts lol.
You should learn to use the **UPDATE FROM** syntax, which is specific to SQL Server but wildly useful, especially because you can easily combine them with CTEs. Something like this: WITH AveragePrices AS ( SELECT cd_Fruit, AVG(vl_Price) as Price FROM Price GROUP BY cd_Fruit ) UPDATE Target SET Target.vl_Price = AveragePrices.Price FROM AveragePrices LEFT JOIN Fruit Target ON Target.cd_Fruit = AveragePrices.cd_Fruit 
Watch out for your "and" and "or" order of operations within the Where clause. I think you may be looking for: select * from friends_of_pickles where height_cm between 35 and 160 and (gender in ('trans', 'female') or species in ('cat', 'human')) order by height_cm asc;
This is a correlated query, do you understand what this actually does?
Also could you explain the (1,1) key identity opposed to prinary key?
really simplified: A primary key, just indicates that the field(s) will be unique and can be used to identify a single record. Declaring it as an Identity(1,1) tells it to be an auto incriminating integer starting at 1 and increasing by 1 for every record inserted. 
Thanks so much! This is why I love Reddit.
If you have excel 2016 you have power query built in. This feature handles 100k rows of data pretty easily and you can manipulate it in similar fashion to SQL before dumping on to a worksheet. 
Yes, this is what I wanted, which leads me to, again, see that I need to review my SQL knowledge because what I was trying didn't make sense (seriously) Thanks!
&gt; Even if you aren’t accessing columns from all the tables in the views which are being referenced, SQL Server still needs to query those columns. Not true. if you create your views and table constraints such as sql server can use join elimination. Basically if SQL Server knows for sure that by leaving a table out does not influence the result than it will not use the table improving the performance. This is not always possible to achieve but in many cases is.
If you use cluster index with your_date_column as the first column. than you do not need to create temp tables. Just make sure that every join uses the your_date_column and you use it as a filter. You can consider to partition you tables based on your_date_column. creating clustered indexes on your_date_column might have performance impact on your inserts if you have to insert "old" dates into your_date_column as it will trigger to insert rows in the middle of the table which will create index and table fragmentation.
you can create a function that returns a record CREATE FUNCTION &lt;Function_name&gt;(&lt;param input&gt;) RETURNS RECORD AS $$ DECLARE res RECORD; BEGIN --your query which sets the `res` variable RETURN res; END; $$ LANGUAGE plpgsql;
the point is that you are inserting data in a fact table for a theater that does not exists in the theater dimension. Your query looks ok on the select part and that should not heapen but make sure to list the columns of the fact table in the insert into. It might be that theater_id is being loaded by some other column basically change this &gt;INSERT INTO fact SELECT TIMEID, clientid, productionid, theatreid, Trowid, total to something like INSERT INTO fact (TIMEID, clientid, productionid, theatreid, Trowid, total) SELECT TIMEID, clientid, productionid, theatreid, Trowid, total
Posted from my phone, while half asleep... I deserve the ridicule!
Did you previously do `sudo apt-get update`? 
www.sql-exercises.com is quite good. No need to sign up. Several exercises available right away.
So far, Debian Stable provides only MySql 5.5. Looking at this [search result](https://packages.debian.org/search?keywords=mysql-server&amp;searchon=names&amp;suite=all&amp;section=all) you might be able to install 5.6 if you add jesse-backports to your sources.list file (EDIT: provided you are running Debian Stable aka Jessie).
&gt; have each price multiplied by the number of tickets sold at that price and then added for the total amount per eventcode SELECT ticket.eventcode , SUM(sub.price_subtotal) AS price_total , MIN(ticket.showdate) AS first_showdate , MAX(ticket.showdate) AS last_showdate FROM ( SELECT eventcode , COUNT(*) * price AS price_subtotal FROM ticket GROUP BY eventcode , price ) AS sub INNER JOIN ticket ON ticket.eventcode = sub.eventcode GROUP BY ticket.eventcode 
Thanks, I really appreciate the help! This is more complicated than anything that I learned in my class so I think I'm doing something wrong because there should be a simpler way to do it that I am not seeing. 
You need a transaction to cover all of the inserts, but you must have 3 separate insert statements.
What do you mean by a transaction? I'm really new to SQL/PHP and my class has kind of just left us to learn pretty much everything on our own.
You need to insert into each table individually. Can't do them all with chunk of code. If you can't get data to insert into after writing each query for one table at a time, then something asked might be wrong.
They don't have a "shared" primary key, as far as I know that doesn't exist. You have a foreign key pointing to another tables primary key, which just means you need to insert into that primary key table first. 
Like this? SELECT Count(words), words FROM test GROUP BY words I know this works in SQL Server. I don't have Oracle nearby to test, but I recall doing it the same way. You can modify it to have a where clause as well if you want to only count fish and dogs.
I think the OP wants two counts in one select. Also, I would make slight change to the WHERE clause: WHERE LOWER(words) like '%fish%' 
it works, thank you very much :)
Shouldn't this GROUP BY be for the employee_ID instead of Last_name? Otherwise it could be off if an employee has the same last name as another.
Thank you kindly. I will try your query when I get mysql software running in unbuntu (run thru virtualbox on windows) 
You could install mysql directly on windows instead. 
I installed sql workbench but I can't figure how to impore CSV sheets to Mysql workbench . I tried to create the tables in workbench itself but i can't access the tables for querying
Thanks!
Data Services doesnt have to be SAP related. It works just as well for anything you would find yourself doing with SSIS. But I am curious about this toad thing. Never heard of it. 
Thanks for the article, very informative! In the past I've looked at Actian Vectorwise as an RDBMS Which uses the GPU and more engines are under way so it seems, like MapD. Would Alenka be a replacement for MapD? 
that dangling comma ahead of the FROM keyword is gonna kill ya google "leading comma convention"
Having that information helps :) Your initial query is correct, with the exception of the missing relationships that the WHERE clause provides. You need to tell the database engine how to merge the tables together. Once you do that, you'll have the right query and you can format it in your report.
Hesitates but asks *Can I Get an example Ple^e^e^a^s^e^e^?* mainly of how id merge two tables -Is really bad at this
Going back to your original query, it appears you have an appointment table, an appointment service table, a person and a service. There's probably an appointment ID in both app_svc and appointment, so you can join those like this WHERE appointment.app_ID = app_svc.app_ID Next, your service table likely has a service ID on it which we can use to connect app_svc and service WHERE service.svc_ID = app_svc.svc_ID Combining those two together, we get WHERE appointment.app_ID = app_svc.app_ID and app_svc.svc_ID = service.svc_ID You can do similar to bring the person table in -- the person will have one or more appointments and you'll join those two tables together. What you're doing is telling the database how to connect a row in Table A to one or more rows in Table B (primary keys, foreign keys -- topics you should have discussed in your class by now).
I'm just glad you were able to get it figured out :)
I'd think you could find some financial analyst positions that would be a good hopping off point. 
Hey, I was just in the same boat. I just graduated in June. I was hired about a month ago as a user acquisition analyst for a tech company. Before employment I spent a good month or two learning about big data, practicing excel and becoming competent at SQL. Sharpen your excel skill most importantly and SQL can be learned/taught on the side. Look for analyst positions that's a great way to pivot into data science. My company's data team is extremely helpful in teaching me so also look for an employer who promotes personal growth if you want to take that route.
The Visual Basic Automation side of Excel would be a good thing to put on a resume. My company utilizes Tableau a lot these days also, not sure if there's a free version of that available or not. 
No problem, keep your head up work hard and don't get discouraged. You'll get the job you want. Also focus on pivot tables in excel, understand how sql relates to pivoting data.
Anyone can share their feedback here and ask questions.
 INSERT INTO newtable ( Name , Res , Qty ) SELECT Name , Res1, Qty1 FROM oldtable UNION ALL SELECT Name , Res2, Qty2 FROM oldtable UNION ALL SELECT Name , Res3, Qty3 FROM oldtable UNION ALL ... SELECT Name , Res23, Qty23 FROM oldtable 
Oh brilliant, thank you! TIL you can combine INSERT INTO and SELECT statements. Thanks again!
This is a pretty common problem, google "Recursive queries Oracle". But here's a similar query to start with SELECT LPAD (' ', 4 * (LEVEL - 1)) || first_name || ' ' || last_name AS name FROM employees START WITH manager_id IS NULL CONNECT BY manager_id = PRIOR employee_id;
Which table has a foreign key in a many to one relationship?
How many majors can a student have? One, Two, or Lots? The answer will help guide how you would want to build it.
Well technically a student can have lots, but perhaps in this case the answer is only one? if its the former, my foreign key with a many to one relationship would be many major_degree to one stu_id unless its the latter
If you want to get into tech, and you don't want to go get a CS degree, these days, a code bootcamp might be a solid decision. Usually I advise against going the codecamp route to people who are deciding between that and a college degree, but judging by what you've said, you seem like the type to identify and fill the holes in your knowledge that usually arise from a crash-course introduction to CS. Anecdotally, I have a friend that did a 3-month camp and got a job with Facebook doing big data analytics about 2 months after graduating. There are quite a few companies out there happy to hire codecamp grads these days, but not as many as those looking for CS degrees- sadly, this is no longer a field comprised of self-taught hackers, and institutional education is a must practically everywhere, unless you take the pivoting route suggested by others. Most of the camps out there focus on Ruby or Python these days (don't ask me why), but I would highly recommend going for C# or Java if you can, as these platforms have just about the most cozy ecosystem for analytics in my experience. Don't try to get certs, though, they don't look great to most employers for various reasons.
Many students can be [blahblah] majors.
So since in the sample data, since the values under major_degree aren't unique (multiples of the same ones) we need a new column.. Specialization cannot be unique bc there is multiple specials for one major? Also i just lost power, on mobile now so i guess ill be taking a break 
OK, take a breath. The questions in the attached do not require a SQL master... they start with the very basics of SQL syntax. Start with the assignment #3... to get those answers you need to understand just SELECT, FROM, JOIN, WHERE, and ORDER BY clauses. Read about each one and go forward from there. Assignment #4 will add some string functions, calculated columns, and aggregates. A bit trickier but take each question on it's own. If you don't have access to a database to play with, there are websites online you can test SQL against. 
OK, well when you get back :) So here's a new set of sample data for majors just to make it easier... MAJOR SPECIALTY Business Finance Business IT Marketing IT Marketing Graphic Arts Math Math So in this table, neither Major nor specialty by themselves are unique. If specialty was truly unique, then it could be your primary key. But in the real world, this is likely to not be the case. SO, there are three options to solve the problem... 1- Use a "composite key" where the two columns together are the primary key ... the combination of the two must be unique. However, this then means the student table must have two columns to make the foreign key... THIS IS GENERALLY A BAD SOLUTION 2- Create a third column that merges the first two... "Business-IT" as a sample value. This then only requires a single FK column in the student table. This is also generally frowned on but not as badly as the first option. 3- Create a third column that is simply a unique numeric key for each row, commonly creatively called "ID". Then the foreign key in the student row would be "Major_ID". This is the most common solution for this scenario. EDIT: Formatting
Ah, this is a great, simplified answer. I do have access to a database. I'll get going on assmt 3. My exam isn't for two more weeks but ill get back to you afterwards. 
Now apply the same thought process to advisors.
literally just adding an advisor_id and making it the PK?
Short answer: yes (along with the FK column of course in each student record) Slightly Longer: if the advisor names are unique, then no need for an ID... real world of course how long until 2 Dr. Tom Jones are hired and your application crashes? Usually the day you were planning on going on vacation...
Oh, and don't forget about the possibility of having the requirement being to allow a second major, or the requirement of having multiple majors. 
How about just casting CUSTOMER_NUMBER to an integer and using BETWEEN? or... SUBSTRING(CUSTOMER_NUMBER,1,3) = '000' Or do you have to skip customer numbers in the middle of the range?
Ugh, Oracle. Anyways, couldn't you do something like: WHERE CUSTOM_NUMBER IN (0, 1, 2, 3, 999) OR CUSTOMER_NUMBER IN (1000, 1001, 1002, 1999) For best performance, you should be storing all of those CUSTOMER_NUMBERS in a secondary table that you simply JOIN to.
You'll need the server application and the client to interface with it. What OS are you on? If you're on windows, try using Microsoft SQL Server Express (because it's free). You'll also want to install Microsoft SQL Server Management Studio (also free). If you're on Linux, I assume you know your way around them as you have a masters in CS. Most distros can install mysql/mariadb using your package manager. You can interface with them directly through your shell. 
&gt; Data Services doesnt have to be SAP related. It works just as well for anything you would find yourself doing with SSIS. But I am curious about this toad thing. Never heard of it. Nice try wherescape salesguy
I'd love to help if I can how much about SQL do you know? As well do you guys have a database to put these tables on or are they paper only.
These are all pretty simple but can be overwhelming if you are just reading the entire problem all at once. Break down the requirements into pieces, WHERE &amp; AND are your keys for what to include in the SELECT and WHERE clause. Just take your time and you will get it!
We do have a database to connect to and put the tables in. 
1- What database are you using? [See sidebar] 2- Why firstName as a Primary Key? 3- Can you post more detail on the errors you are getting and when you are getting them?
I'm going to have a hard time using the precise terms here but you're going to need to create a set of each column using sample data: For example if you start with a first name of Gary, John, and Bob, a last name of Smith, Jones, and Wilson, a city of Detroit, Chicago, and New York, and a state of MI, IL, and NY then there are n number of possible combinations, e.g. Gary Smith Detroit MI, Gary Smith Detroit IL, Gary Smith Detroit NY, etc. First you need to determine the minimum number of entries for each table to come up with a total of 5 million combinations, and then pair them. EDIT: You're probably going to need to use a loop of some kind that increments through each set. I could probably come up with something basic in SQL, no clue about Oracle.
Primary keys should be unique. First name is bad because you can have multiple people with the same first name. In most cases it's better to just use an auto incrementing identify field as your primary key.
If I get bored at work tomorrow I"ll fuck around with it. Basically you will need to populate a table with entries and a incremental primary key, then select each first+last+city+state, then increment the state by one and cycle until you reach the end, then increment the city by one and increment until the end. 
I solved it. SELECT name FROM Company start with id = (Select * from Company where id = 1) Connect by prior id=parentid So when i search for Company With ID: 1. I get all Connected Companys. Parent, Parent/child, child.
Ok, that makes a lot of sense. Thanks.
This should work. Syntax might need some massaging but: DECLARE @FIRSTNAME TABLE ( ID [int] IDENTITY(1,1), FirstName [varchar](50)) DECLARE @LASTNAME TABLE ( ID [int] IDENTITY(1,1), LastName [varchar](50)) DECLARE @CITY TABLE ( ID [int] IDENTITY(1,1), City [varchar](50)) DECLARE @STATE TABLE ( ID [int] IDENTITY(1,1), State [varchar](50)) CREATE TABLE #RANDOM ( FirstName [varchar](50) LastName [varchar](50) City [varchar](50) State [varchar](50)) DECLARE @FNLOOP int = 1 DECLARE @LNLOOP int = 1 DECLARE @CTLOOP int = 1 DECLARE @STLOOP int = 1 WHILE @FNLoop !&gt; (SELECT MAX(ID) FROM @FIRSTNAME) BEGIN WHILE @LNLoop !&gt; (SELECT MAX(ID) FROM @LASTNAME) BEGIN WHILE @CTLoop !&gt; (SELECT MAX(ID) FROM @CITY) BEGIN WHILE @STLoop !&gt; (SELECT MAX(ID) FROM @STATE) BEGIN INSERT INTO #RANDOM SELECT FirstName , (SELECT LastName FROM @LASTNAME B WHERE @LNLoop = B.ID) , (SELECT City FROM @CITY C WHERE @CTLoop = C.ID) , (SELECT State FROM @STATE D WHERE @STLoop = D.ID) FROM @FIRSTNAME A WHERE @FNLoop = A.ID SET @STLoop = @STLoop + 1 --WAITFOR DELAY '00:00:01' END SET @CTLoop = @CTLoop + 1 SET @STLoop = 1 END SET @LNLoop = @LNLoop + 1 SET @CTLoop = 1 END SET @FNLoop = FNLoop + 1 SET @LNLoop = 1 END As someone else pointed out you'll just need to insert ~48 rows of unique values into each of the @Tables and voila, 5 million random rows that are distinct. If you wanted to have city and states that correspond you'll need to figure out n^3 = 5,000,000 and then populate your tables accordingly. In that case I would recommend combining the @State and @City table into one.
This is a minor item and may not be worth consideration (depending on where you want to ultimately go with this), but... As someone who spends all day, every day working with tables related to students, registrants, and classes, I would recommend a few more tables. * Students (contains student details) * Classes (or Courses if you want to call it that; contains the details about the specific subject offering that change, such as time, location, and professor; FK to Subjects) * Subjects (contains the details about the class that are always true such as title, length, and description) * Registrants (FK to Classes and FK to Students - basically, this ties together the student and the class) Most universities publish a catalog of subjects. They publish this as infrequently as possible. It contains all of the information about a class that is always true. For example, Calculus II always is 3.0 units, has a pre-requisite of Calculus I, and always teaches integration by parts. But then each term, they publish a schedule of classes. There may be multiple offerings of Calculus II on different days, times, or in different locations. And the professors may change. It's a bit more work at the beginning, but it's a whole lot easier than trying to unwind things later.
Just to tack on here so you understand whats going on since its homework. The very first select will look like this: | FirstName | LastName | City | State | | :--- | :--- | :--- | :--- | | 1 | 1 | 1 | 1 | Then the @STLOOP will be incremented to 2 and the next select will look like this: | FirstName | LastName | City | State | | :--- | :--- | :--- | :--- | | 1 | 1 | 1 | 2 | This will continue until the @STLOOP is equal to the maximum ID in the @STATE table, at which point it will increment the @CTLOOP to 2, which will make the next select look like this: | FirstName | LastName | City | State | | :--- | :--- | :--- | :--- | | 1 | 1 | 2 | 1 | At which point it will begin incrementing the @STLOOP again to give you: | FirstName | LastName | City | State | | :--- | :--- | :--- | :--- | | 1 | 1 | 2 | 1 | | 1 | 1 | 2 | 2 | | 1 | 1 | 2 | 3 | | 1 | 1 | 2 | 4 | | 1 | 1 | 2 | 5 | | 1 | 1 | 2 | 6 | This will loop until the @CTLOOP is at its maximum, then it will increment the @LNLOOP, repeat cycles, and finally it will increment the @FNLOOP and when that reaches its maximum the loop will terminate. You'll need to populate data into the @TABLES such as: DECLARE @FIRSTNAME TABLE ( ID [int] IDENTITY(1,1), FirstName [varchar](50)) INSERT INTO @FIRSTNAME SELECT DISTINCT TOP 48 FirstName FROM TABLE Or hard-coding the values yourself but I'd recommend going into Excel and creating a trash table with n-number of values (in this case 48) such as: | FirstName | LastName | City | State | | :--- | :--- | :--- | :--- | | Bob | Jones | Detroit | MI | | Ryan| Smith | Chicago | IL | | Tyrion | Lannister | San Francisco | CA | | Wilma | Bryant | Buloxi | MS | Barbara | Masen | Dallas | TX | Then just import that table and reference it. 
Etl tool. SQL server integration (something that starts with an s maybe suite software solution). You'll need ms visual studio.
SSMS is a client application. What you seem to be looking for is the server. Development edition is free now: https://www.microsoft.com/en-us/sql-server/sql-server-editions-developers 
Also that doesn't seem free? Do I need to subscribe to their community to download it for free? 
So in SQL you can either do the following: INSERT INTO Table2 SELECT * FROM Table Or: SELECT * INTO Table2 FROM Table The latter creates the table and a #Table is temporary but allows you to query it later. An @Table is also temporary but it flushes and disappears once the query finishes executing.
I'm writing queries on datasets that are currently living in Excel workbooks. I think I'll manage until they can finally sort my access out.
Oh, more importantly this is the salient concept. You cannot create a table more than once, so if you tried to do a SELECT INTO instead of an INSERT INTO SELECT then you would be trying to create a new table each time the loop increments (in this case 5 million times) -- SQL won't like that because the table already exists. So you create the table first and then INSERT INTO a single record at a time in this example. You could be inserting hundreds or millions of records with an INSERT INTO but that won't work here. Imagine it as "adding new rows" to something that already exists, whereas SELECT INTO creates a new table that only has whatever you decided to put in there. But, as an added concept... you could use SELECT INTO if you used a dynamic script to increment your table each time the loop passed, so at first pass it would be SELECT INTO TABLE1, then SELECT INTO TABLE2... all the way up to 5million tables. Then you'd need to SELECT * FROM TABLE1 UNION SELECT * FROM TABLE2 ... in order to produce an identical dataset.
I assume you're talking about taking the Querying Microsoft SQL Server 2012/2014 exam aka 70-461. It's a brutal wake up call for many. I've failed it x1 and working on re-taking it. I don't know what your experience is with TSQL but Microsoft has just re-organized their 2017 certification track. It's no longer as follows: DBA Fundamentals --&gt; 70-461 --&gt; 70-462 ---&gt; 70-463 Now theres a defined DBA path. A defined developer path. And a defined BI / Big Data path. I would take the 70-461 soon if you want to take it and see where you land but honestly, it's a beast of a test so I would wait until they roll out the BI certification path. Good luck.
do you have any link that help with that?
Depends on which flavour of SQL you need. In your case I'd go for MS SQL. The express version is free and nearly fully featured: https://www.microsoft.com/en-us/sql-server/sql-server-editions-express
There were a few syntax errors but I just tested it with a sample table filled with 14 distinct entries and it works as expected: DECLARE @FIRSTNAME TABLE ( ID [int] IDENTITY(1,1), FirstName [varchar](50)) INSERT INTO @FIRSTNAME SELECT FirstName FROM dbo.Names DECLARE @LASTNAME TABLE ( ID [int] IDENTITY(1,1), LastName [varchar](50)) INSERT INTO @LASTNAME SELECT LastName FROM dbo.Names DECLARE @CITY TABLE ( ID [int] IDENTITY(1,1), City [varchar](50)) INSERT INTO @CITY SELECT City FROM dbo.Names DECLARE @STATE TABLE ( ID [int] IDENTITY(1,1), State [varchar](50)) INSERT INTO @STATE SELECT State FROM dbo.Names CREATE TABLE #RANDOM ( FirstName [varchar](50) , LastName [varchar](50) , City [varchar](50) , State [varchar](50)) DECLARE @FNLOOP int = 1 DECLARE @LNLOOP int = 1 DECLARE @CTLOOP int = 1 DECLARE @STLOOP int = 1 WHILE @FNLoop !&gt; (SELECT MAX(ID) FROM @FIRSTNAME) BEGIN WHILE @LNLoop !&gt; (SELECT MAX(ID) FROM @LASTNAME) BEGIN WHILE @CTLoop !&gt; (SELECT MAX(ID) FROM @CITY) BEGIN WHILE @STLoop !&gt; (SELECT MAX(ID) FROM @STATE) BEGIN INSERT INTO #RANDOM SELECT FirstName , (SELECT LastName FROM @LASTNAME B WHERE @LNLoop = B.ID) , (SELECT City FROM @CITY C WHERE @CTLoop = C.ID) , (SELECT State FROM @STATE D WHERE @STLoop = D.ID) FROM @FIRSTNAME A WHERE @FNLoop = A.ID SET @STLoop = @STLoop + 1 --WAITFOR DELAY '00:00:01' END SET @CTLoop = @CTLoop + 1 SET @STLoop = 1 END SET @LNLoop = @LNLoop + 1 SET @CTLoop = 1 END SET @FNLoop = @FNLoop + 1 SET @LNLoop = 1 END select count(distinct FirstName+LastName+City+State) from #RANDOM [dbo.Names](http://www.filedropper.com/names_1) source edit: This only works without the DISTINCT property because I know there aren't any duplicate FirstName, LastName, City, or State values in the table. If you have a large sample table then you need to use DISTINCT.
thanks a lot for your help friend, I'll check these solutions out and brief you
You can get a pre-built developer VM from Oracle [here](http://www.oracle.com/technetwork/community/developer-vm/index.html).
thank you so much
check out SQL express and the adventureworks database
IMHO, it only makes sense if your employer requires the certification (and they're paying for it, and they're giving you study time). From what I've heard around the campfire, the 70-461 tests a **lot** on esoteric features of T-SQL, XML, and other things that aren't used commonly in the actual business usage. Getting the certification won't really teach you how to solve problems and do meaningful things with data day-to-day. Pick up Itzik Ben-Gan's [T-SQL Fundamentals](https://www.microsoftpressstore.com/store/t-sql-fundamentals-9781509302000) and/or [T-SQL Querying](https://www.microsoftpressstore.com/store/t-sql-querying-9780735685048). FWIW, he co-wrote the 70-461 exam prep book too.
&gt;SELECT max(time), name, location FROM table GROUP BY name; This won't run at all on some RDMBSes, and on the ones it does run on I think it would mean a name would show twice if it was in two different locations, with the time displayed being the last time it was at that location. SELECT max(time), name, max(location) FROM table GROUP BY name Gets the latest time, and the alphabetic last location, but these may be from two different records. As /u/r3pr0b8 says window functions are exactly what you need, but that's probably the solution that scared you off SO. A solution with is easier to write (but performs worse) is: SELECT name, time, location FROM table t INNER JOIN (SELECT name, max(time) lasttime FROM table GROUP BY name) x ON x.name = t.name and x.lasttime = t.time For last n locations, you definitely want window functions.
Great, thanks, it looks like I should learn about window functions then. Thanks again for the time!
Any role with the word *administrator*, or *operator* in the title is bound to be automated at some point. Go the development route, and always keep moving forward.
Where did the value in RN come from? Is it in your table? If so, it's different than what you'd get from the row_number() function. Also, you say CODE is the id column, but you also say that the ID is the primary key. It cannot be as your sample data shows that the CODE column is not unique.
EDIT: Ha! Here's a fun query. It builds every possible combination of Rights and the sum of their totals in a comma separated list. Also, the numeric value for each of your rights should be a power of two.. declare @table as table ( Rights nvarchar(50), total int ); insert into @table values ('View',0); insert into @table values ('Add',2); insert into @table values ('Edit',4); insert into @table values ('Export',6); insert into @table values ('Delete',8); insert into @table values ('Manage',10); with combinations as ( select cast(Rights as varchar(max)) as Rights, total from @table UNION ALL select cast(concat(combinations.Rights,',',t.Rights) as varchar(max)), t.total + combinations.total from combinations inner join @table t on combinations.Rights NOT LIKE '%' + t.Rights + '%' ) SELECT * from combinations -- WHERE Rights = 'View,Add,Edit,Export,Delete'; 
Not exactly free is it?
I'm not sure, but maybe you could alias the Types table? (left outer join Types t1) or something
I do this regularly on ms sql server. Just give each one a different alias. 
&gt;Any role with the word administrator, or operator in the title is bound to be automated at some point. Oh, I like that phrase. May I borrow it? 
Was about to write that variable query depth calls for a recursive SQL, you beat me to it. Anywho, your query produces 1956 records, which is obviously too many. here's one that produces 63 rows (and ditto on the powers of 2 comment): with t as( select rights, rval from (values ('View',0), ('Add',2), ('Edit',4), ('Export',6), ('Delete',8), ('Manage',10)) t(rights,rval) ), t_rec as ( select rval_sum = rval, rights_agg = cast( rights as varchar(1000)), rval_limit = rval from t union all select rval_sum = t_rec.rval_sum + t.rval, rights_agg = cast( (t_rec.rights_agg + ', ' + t.rights) as varchar(1000)), rval_limit = t.rval from t_rec join t on t.rval &gt; t_rec.rval_limit ) select * from t_rec order by rval_sum
Well, I guess what I intended to imply is that good scripters have a good base to make the jump to development if they want to. The potential is there.
First, you need to create a (free) microsoft live account (fyi linked accounts don't work and I would recommend NOT to create one with your work email to avoid clashes): https://signup.live.com/signup?ru=https%3a%2f%2flogin.live.com%2foauth20_authorize.srf%3flc%3d1033%26response_type%3dcode%26client_id%3d51483342-085c-4d86-bf88-cf50c7252078%26scope%3dopenid%2bprofile%2bemail%2boffline_access%26response_mode%3dform_post%26redirect_uri%3dhttps%253a%252f%252flogin.microsoftonline.com%252fcommon%252ffederation%252foauth2%26state%3drQIIAZVSMW_TQBSW47YoLEGVmFiTDIhz7IvtsyN5SAWVCEpbVQJEluh8PidXfHfGd3HUCnViYK4EQ5FAZWCIYGGC_gQWurAwwsLMxIiDxFQkQHp60vveJ73vfe9dNR3L6TXdMIwDFxPgdKEDXIhSgLGDAPQRwX6IPBQGgzXPdlzPLdYvXgrIyYX-bbjx5OTr01fP3rjHRn2csZJaRPKFcW2qda56nQ7Oc6tUKldWydQMZ0rPEiaXpM5YsYmgCRPvDOPMML4ZxqLW9L0wtgMfAoi9SoYT-iDo2lUZpIh4qe0iN_hca2z3Z3oKl0kW7IB-OYd8r9VTWfBxLpU-MhsSL7vWMhOZ0CPzssypYAmRQlCiLZZoeZ-KY_OTUVCc8YjvnxPcLmie7Y-1jH4t1-r2W3Czij9QK3SDCpoyXdE272qLkzFLWt3r8pCrVB-qBxlQtChpARJagsoEzaRoQZ9gnuPKlr9R21MdddtCCkKjf7FsYTZ_X4RjgSeUU6ErpQW15kwkcq4sQXXnrblWqedSnJpNaAdumBBUjfUIcJMYgxg5CbAdFIcUpSmJ07MV48eK8XK1-ocrHx_uDp-vb714_PpR49579mG1M5ps76lJunWjGIkD5Q_CW6W3A0cZLnd9ssPnszs34ynaGw4mwyjoOaf1__qanw2%26estsfed%3d1%26uaid%3d07a163385541423295a1e396a69bad34%26lw%3d1%26fl%3deasi2%26wsucxt%3d1%26mkt%3dEN-US&amp;mkt=EN-US&amp;uiflavor=web&amp;lw=1&amp;fl=easi2&amp;client_id=51483342-085c-4d86-bf88-cf50c7252078&amp;wsucxt=1&amp;uaid=2146e99cfb3a451da46fc23a5a375486&amp;lic=1 Then, you need to register/sign-up for the Dev essentials (https://www.visualstudio.com/dev-essentials/). You should be able to download after that.
Hi jonr I have the following query; SELECT o.source_type, o.destination_type, r.time FROM orders as o, routes as r INNER JOIN types as t1 ON r.source_area = t1.area INNER JOIN types as t2 ON r.destination_area = t2.area But then how do I link the orders.source_type and orders.destination_type to the t1 and t2 table aliases? Thanks!
Hi jonr I have the following query; SELECT o.source_type, o.destination_type, r.time FROM orders as o, routes as r INNER JOIN types as t1 ON r.source_area = t1.area INNER JOIN types as t2 ON r.destination_area = t2.area But then how do I link the orders.source_type and orders.destination_type to the t1 and t2 table aliases? 
this is perfect! Gave me the exact output needed. Thanks!
Understood, I was trying to keep the example simple. Do us a big big favor, could you give us two or three records in these tables so we can speak in concrete examples.
for a mega bonus, add what the desired output would look like and that will help clarify what you are asking for. Sometimes questions are phrased a certain way that will send is further away from the answer, where as an example output speaks for itself.
Seconded
You can add: USE [database_name] GO To the top of each .sql file and it'll use that database when you run the script or SSMS.exe actually has command line options: --------------------------- Microsoft SQL Server Management Studio --------------------------- Usage: ssms.exe [-S server_name[\instance_name]] [-d database] [-U user] [-P password] [-E] [-nosplash] [file_name[, file_name]*] [-log [file_name]?] [-?] [-S The name of the SQL Server instance to connect to] [-d The name of the SQL Server database to connect to] [-E] Use Windows Authentication to login to SQL Server [-U The name of the SQL Server login to connect with] [-P The password associated with the login] [-nosplash] Suppress splash screen [file_name[, file_name]*] Names of files to load [-log [file_name]?] Logs SQL Server Management Studio activity to the specified file for troubleshooting [-?] Displays this usage information --------------------------- OK --------------------------- So you can just do: "C:\Program Files (x86)\Microsoft SQL Server\100\Tools\Binn\VSShell\Common7\IDE\Ssms.exe" c:\path\to\sql1.sql c:\path\to\sql2.sql
Glad that worked out. As for marking solved... not sure. I'm not very good at reddit, but I don't recall seeing any posts in /r/sql marked as [SOLVED] like they do over in /r/powershell 
You might need to do a bit more thinking this through, I'm not sure you'd want to take the datediff of every row, but maybe only where the rowid is an even number? For example if the ticket starts at 12:00 and the second time stamp is 12:04, thats 4 minutes... if the third timestamp is 12:05, is that 1 minute of work time, or 1 minute of down time? Not sure if you understand what I'm asking or not. 
You probably don't need a LOOP you could use a CTE and break this down into steps and accomplish the datediff just like how a running total is created.
Nice, but one question, if I have thousands of routes and types, how can I make this query without putting in the dual union all with the actual records? Thanks for the great answer.
As a minor addition, you might want to apply an ID to the unique permissions levels first, in case OP has any permissions with the same level. Right? with original as ( select rights, total from (values ('View',0), ('Add',2), ('Edit',4), ('Export',6), ('Delete',8), ('Manage',10)) t(rights,total) ) ,t as( select id = cast(row_number()over(order by total) as int),* from original ), t_rec as ( select rval_sum = total, rights_agg = cast(rights as varchar(1000)), rval_limit = id from t union all select rval_sum = t_rec.rval_sum + t.total, rights_agg = cast((t_rec.rights_agg + ', ' + t.rights) as varchar(1000)), rval_limit = t.total from t_rec join t on t.id &gt; t_rec.rval_limit ) select * from t_rec order by rval_sum
SSRS is included with the SQL license... you have to install it via the SQL Server setup though. If you're legitimately going to be using it for development, you don't need to license anything and you can use the development edition https://www.microsoft.com/en-us/sql-server/sql-server-editions-developers
Man, that was perfect. Many thanks! Byw, do you have any link to a tutorial where I can learn how to do joins? I work with a lot of data but it is really simple, I always do the normal WHERE table.id = table2.id 
So in another thread I've been talking about a similar issue. The developer license is free, correct? So I can just download and install it and be off on my merry way?
I'm familiar with the ASC and DESC options, but I've never seen "LIMIT 12" before. What happens when you leave it off?
The error may be elsewhere in the query. Post the whole thing.
OLTP. We've got indexing and such in place and that's done strategically, the problem is that portions of the queries that get sent are created dynamically by a black box application. So while we can change portions of it, depending on what the users do, it's possible to touch something off that ends up running full scans. We do have the tables partitioned and if we're manually running queries we split over the partitions and piecemeal it out. I think, based on the responses I'm getting, that we might want to have the discussion about more hardware. The issue that happens is that we start having parallel transactions, which is fine, but then someone runs a monster query, and that seems to just tank the system. Too many connections tie the thing up. Although supposedly there's some patches in 12.2 that might help.
You can use a modified tabibito-san method: create a 'gap indicator', calculate a running sum of the indicator to create a basis for grouping, group by the row number minus grouping factor. An article describing the original method: http://it.toolbox.com/blogs/data-ruminations/tabibitosan-the-traveler-technique-in-sql-74049 Here's how this works: with t as ( select * from (values ('UserB',cast('2014-07-14 20:14' as datetime)), ('UserB',cast('2014-07-14 20:35' as datetime)), ('UserA',cast('2014-07-14 20:14' as datetime)), ('UserA',cast('2014-07-14 20:15' as datetime)), ('UserA',cast('2014-07-14 20:16' as datetime)), ('UserA',cast('2014-07-14 20:34' as datetime)), ('UserA',cast('2014-07-14 20:35' as datetime)) )t(userid,tstamp) ), basis_gap as ( select userid, tstamp, no_gap = case when (datediff( mi, lag(tstamp) over( partition by userid order by tstamp), tstamp)) &lt; 15 then 1 else 0 end from t ), group_number as( select userid, tstamp, rn= row_number() over (partition by userid order by tstamp), gap_counter =sum( no_gap) over(partition by userid order by tstamp rows unbounded preceding), group_number = row_number() over (partition by userid order by tstamp) - sum( no_gap) over(partition by userid order by tstamp rows unbounded preceding) from basis_gap ) select userid, first_ts = min(tstamp), last_ts = max(tstamp), minutes_worked = 1+ datediff( mi, min(tstamp), max(tstamp)) from group_number group by userid, group_number order by 1,2 
are you a sql jedi master? Thanks a bunch for the example!
Thanks downloading now. 
I couldn't disagree more. I think that Excel is wholly inappropriate for enterprise solutions. I've created tables like you've described in the past and it just doesn't cut it. 
Upvote for spelling, sound advice also. 
Don't index based on textbook theory, check the logs for the most time consuming queries and index based on the WHERE columns. Do that after the black box has run some heavy stuff. Haven't done Oracle in a decade so don't know the equivalents, but in SQL Derver I would check non-clustered indexes and indexed views (materialized views) for common selects.
I'm not sure? It's not "distinct user_id". Can you give me a better clue?
Just wanted to say, I'm reading over this and printing it out. Your effort has helped someone today!
&gt;ORA-00955: name is already used by an existing object I thought this one would be pretty self explanatory: you're trying to create a table that has the same name as something else already in the database. &gt;ORA-01756: quoted string not properly terminated I don't actually oracle, but it looks like your quotes in that statement are slanty ones (to use the technical term) and in the other statement they're straight - try using different quotes?
If the dob is stored as a date, it doesn't matter what format they get displayed as. If the field is named dob in the table students then: datepart(students.dob, DP_MONTH) Would return, e.g., 2 for students born Feb, 11 for students born Nov, etc. Or at least I think so, I don't actually use Oracle.
Alright maybe one more follow up question, how do I form buckets of dates, lets say I want first third of year, second third how do I get a query to return those months?
Not sure I completely understand but something like, SELECT Case datepart(students.dob, DP_MONTH) when &lt;=4 then '1st third' when &gt;4 and &lt;=8 then '2nd third' else '3rd third' end Will return the string '1st third', '2nd third', '3rd third' for each person in the statement. Or if you wanted a query just to return, e.g., the 1st third then just have that in your where clause, e.g., WHERE datepart(students.dob, DP_MONTH) &lt;5 Would return people just in the first third of the year. (or at least it would if Oracle is close enough to T-SQL that I know what I'm talking about). 
That's exactly what was wrong. Thanks!
Not sure if you saw my edit but a quick google found me this promising script: https://www.mssqltips.com/sqlservertip/1584/auto-generate-sql-server-restore-script-from-backup-files-in-a-directory/ I still think that recovering from backups due to data entry errors is a symptom of a process or design problem. Knowing nothing at all about your setup, the first think I'd be looking at is creating some kind of version control on your document storage. It might even be as simple as slapping a trigger on a table that will stick a timestamped copy of any document in a separate table whenever a change is made. 
I'd also like to know this. I tried MS SQL, but it wont work on the windows version I'm running. Windows 7 Professional, Service Pack 1.
This wont work on the windows version I'm running. Windows 7 Professional, Service Pack 1. Any tips? I just need a basic installation to practice building SQL queries. Thanks.
How are you creating your backups? If you're using [Ola Hallengren's maintenance solution](https://ola.hallengren.com/sql-server-backup.html), the DBATools Powershell module (https://dbatools.io/, which you owe to yourself to get and use anyway) has `Restore-SqlBackupFromDirectory` which will parse through the directory structure created by Ola's backup routine and perform the restore. Otherwise, I'd suggest looking at `restore-sqldatabase` from the `sqlserver` PowerShell module (comes with SSMS 2016) in conjunction with some scripting, or the pure SQL script in the links you've been given. Side note: I recommend that you get comfortable with PowerShell, the two modules I mentioned above, and most importantly **not** being 100% reliant upon the GUI to perform these sorts of tasks.
We use networker for our backups are far as I am aware. I have 0 control over the backup process short of being one of the guys who gets a backup for a user who needs it. 
And here I was thinking there is no meaningful difference between the two. I will keep this in mind when dealing with MVs. great stuff!
You can use the IN Condition and specify any series of Months as a comma-delimited list in the parens: https://docs.oracle.com/cd/B19306_01/server.102/b14200/conditions013.htm 
Yes, I know what you're doing and it just doesn't work as an enterprise solution. You can build out an entire database in Linux for free if cost / budget is a real concern. I'll be honest I have used Excel like you're proposing in the past for very specialized users / requests but its never the way I want to solve a problem and it eventually creates a nightmare for someone to inherit when you move on to a new job.
Here you go: declare @PartNos as TABLE (PartNo NVARCHAR(20),Datedue DATETIME2(0)) INSERT INTO @PartNos VALUES ('G42786','12/31/16') ,('S42786','10/07/25') ,('M42786','10/07/25') ,('G54321','11/20/16') ,('M54321','01/15/22') ,('S54321','01/15/22') SELECT * FROM @PartNos ; WITH CTE1 AS ( SELECT SUBSTRING(PartNo,2,LEN(PartNo)-1) SubPartNo, Datedue FROM @PartNos WHERE LEFT(PartNo,1) = 'G' ) UPDATE @PartNos SET Datedue = b.Datedue FROM @PartNos a, CTE1 b WHERE SUBSTRING(a.PartNo,2,LEN(a.PartNo)-1) = b.SubPartNo SELECT * FROM @PartNos 
32 bit CPU: http://download.microsoft.com/download/E/A/E/EAE6F7FC-767A-4038-A954-49B8B05D04EB/ExpressAndTools%2032BIT/SQLEXPRWT_x86_ENU.exe 64 bit: http://download.microsoft.com/download/E/A/E/EAE6F7FC-767A-4038-A954-49B8B05D04EB/ExpressAndTools%2064BIT/SQLEXPRWT_x64_ENU.exe
I'm glad you found your answer. One of my coworkers has used Idera's Virtual Database in the past to get a quick peek inside of a backup without doing a restore and just pull out a single table. I don't remember how much it cost, but it might be something to consider for the future. 
Added the query. I suck at formatting
Just tested it and you're right - I could have sworn this used to be the case!
You may be thinking of @tableVariables. You *have* to alias them because @tableVariable.ColumnName is not considered legal (for some reason beyond me, likely related to parsing.)
i should have clarified, i ran the statement on a back up of my database. would have been apocalyptic otherwise, especially considering i have no idea what i did. 
Thanks, I got everything done. I didn't want to post the problem because I did want to learn, but I didn't know how to essentially take the month part of the date out, I found extract and used month which wound up working perfectly.
running this in SQL server studio.
Possibly, I also may be thinking of the bastard stepchild implementation that MS Access insists on referring to as SQL.
When you execute the stored procedure, are you using the same exact session &amp; database context in SQL Studio? Also, that is *one hell* of a query. Ripe for re-factoring.
terribly sloppy description, my friend CREATE TABLE table1 ( dudebro VARCHAR(37) , hatsize INTEGER , IQ SMALLINT ); INSERT INTO table1 VALUES ( 'Mark' , 2131312 , 99 ) ; CREATE TABLE table2 ( IQrange SMALLINT , lovalue SMALLINT , hivalue SMALLINT ); INSERT INTO table2 VALUES ( 10 , 90 , 100 ) ; SELECT table1.dudebro , table2.IQrange , table1.IQ FROM table1 INNER JOIN table2 ON table1.IQ BETWEEN table2.lovalue AND table2.hivalue 
 CREATE TABLE students (`Name` varchar(4), `Grade` int); INSERT INTO students (`Name`, `Grade`) VALUES ('Mark', 94), ('John', 73), ('Matt', 85); CREATE TABLE grading_table (`Letter` varchar(1), `Low` int, `High` int); INSERT INTO grading_table (`Letter`, `Low`, `High`) VALUES ('A', 90, 100), ('B', 80, 89), ('C', 70, 79), ('D', 60, 69), ('F', 0, 59); SELECT s.Name, g.Letter FROM students s INNER JOIN grading_table g ON (s.grade &gt;= g.low AND s.grade &lt;= g.high) or SELECT s.Name, g.Letter FROM students s INNER JOIN grading_table g WHERE (s.grade &gt;= g.low AND s.grade &lt;= g.high)
Same everything, I agree it needs to be refactored, this was inherited from a developer who's no longer with the company. Its a mess.
I'll have to ask the other developers to see if we're doing the Date as a varchar for a reason. Thats a valid question, thank you
Update: This seems to be related to the data in the Tables. I had made a snapshot of where it was, then proceeded to purge Accounting records for each accounting period in a relative year. As soon as i purged the remainder of 2010, the report ran without issue. At this point i'll need to backtrack and review the transactions from 2008-2010
Simple. You use something other than the equality operator :) You can put *all sorts* of funny shit into a JOIN predicate, from something like "ON 1 = 1" to "ON Table2.DaysPastDue &gt; 90" - Notice both of those predicates work perfectly fine, despite the fact they don't directly correlate any columns from Table1 to Table2.
You just need to take the case statement query you had and group on user_id. Then take an arbritary aggregate function (min, max, avg, whatever) on your case statements. They will always return the same value and it will flatten the data how you expect. SELECT user_id, MAX((CASE WHEN orders = 1 THEN source END)) as one, MAX((CASE WHEN orders = 2 THEN source END)) as two, MAX((CASE WHEN orders = 3 THEN source END)) as three FROM table GROUP BY user_id
Not sure I understand the question. If the parameter was 5, are you after: 1 2 3 4 5 2 4 6 8 10 3 6 9 12 15 4 8 12 16 20 5 10 15 20 25 ? That's a pretty bizarre use case, is it homework? 
I would just take your source data and cross join to it an integer representing how many times you want to replicate each row. I'll assume it's static and you want to replicate each row 50 times. So you add: SELECT * FROM table JOIN (SELECT 50) ON 1=1 Then you need another table that has each digit replicated for the value of the digit. So the table (lets call it *table_expansion*) would look like: 1 2 2 3 3 3 You could generate this sequence from 1 to 100 with this in Python: for i in range (1,101): print "\n".join([str(i)] * i) Then subselect your initial query and LEFT JOIN the table_expansion. SELECT * FROM ( SELECT * FROM table JOIN (SELECT 50 as expansion) ON 1=1 ) q1 LEFT JOIN table_expansion te ON te.value = q1.expansion 
Thank you. I downloaded the 64 bit version and installed it (I think), but I'm not sure what to do next. There's no desktop icon and no exe in the folder or anywhere I can find. I have a folder called SQLEXPRWT_x64_ENU which contains SETUP.EXE and a couple folders and files. I ran SETUP.EXE which seems to have installed SQL Server. Under the start button I have a new application "SQL Server Import and Export Data (64-Bit)". Now I'm at a loss as to what to do next. Thanks.
Evaluated indexes for missing/bad ones? Recompiled them?
There was a selection process, but I left it as it was under the assumption that everything I needed would be in there and I'd probably be better off not monkeying around with the default since I don't know what I'm doing. Let me try downloading this and installing. Thanks.
It hung up during the install and I cancelled it. I'm going to try again tomorrow. Thanks for helping me.
Expensive in that context is a measure of processing time and memory usage (and maybe rarely some other things like table locking). 
Is this SQL server? If so, FORMAT() doesn't use the "name AS type" syntax. It takes a comma separated list of arguments. Take a look at this and at the section on numeric formatting strings. https://msdn.microsoft.com/en-us/library/hh213505.aspx
Do your own hw
They can range from 10gb to 150gb. And we do get an entire server backup, that's how our system works. Instead of backup up each individual audit file database it backs up the entire server as a single database. I'm assuming that happens s as we have probably 250 servers and each will have 100's of audit files on each. So we get daily backups and then trl files for each hour. As that has been decided by the people responsible for our backups as the best way for us to do it. We might not have a perfect system but for what we have it works and I don't think there's much possility of that changing any time soon unfortunately.
I believe we have just gone up to SQL 2014. I will have a look at this and see if I can get this to work
Just bear in mind that BETWEEN is really slow. I tried to aggregate high volume data by month that way once... would not advise. 
Oh, you can hate every database if you really want to. But why not use time more constructively?
I do! By moving everything off of oracle as soon as possible :) But really, outside having serious issues with how oracle operates (and their monolithic existence and refusal to enhance and expand their DBMS platform), one cannot really argue that the cost of running oracle is anything less than exorbitant. And the cost isn't (in the modern landscape) justified in really any manner of speaking. There are way more options now than there were 10 years ago - especially at the enterprise level. The only reason oracle is still even used in our enterprise is because of legacy systems that we are in the process of building replacements for - on different DBMS's. Certainly knowing how to use oracle is a valuable skill for anyone who works regularly with multiple varying DBMS's - but that doesn't mean one has to enjoy or be happy about it. Nothing ever improved by not complaining :)
 Well you can have the most optimised query but it can still be damm expensive because there is a lot of data. Its really an indication that a query is going to use a lot of server resources which are of course finite 
Pretty much any modern MPP RDBMS wipes the floor with Oracle (Greenplum, Redshift, as examples) most definitely in reads, and depending on the configuration setup, writes too. Modern SQL Server is right up there too, especially when running in clusters. I haven't really encountered any "new work" that "chooses" Oracle, unless it is what they use for literally everything else.
I think you should split this into two queries and UNION them. The first one to get the counts for the top 20 results and the second one to get the count for the NULL category.
ok, i don't know what union is exactly but that's at least a search term i can start playing with. thanks!!
SELECT Country, COUNT(\*) FROM People WHERE Country IS NOT NULL GROUP BY Country LIMIT 20 UNION SELECT NULL, COUNT(\*) FROM People WHERE Country IS NULL
It should be asking you to connect to a server when you start SSMS. It will likely default to the "master" database. If you include a USE statement at the top of your saved script, it won't matter what database is selected in the toolbar - it will change to the specified database when you execute the script. For example: USE mydatabase GO SELECT x, y FROM mytable This will set the current database to "mydatabase" and will remain that way for the remainder of the script (and even after) or until it encounters another USE statement. 
This is the way. There might be a server setting for how "sorting" NULLs works (are they the highest or lowest values), but I'd use a union before I changed that. 
perfect, thank you
I don't fuck with Oracle, but try INSERT INTO TopKCustomer_T VALUES (queryResult, rank). Or the inverse - you need to match the ordinal positions of the columns in the output table. 
Yep, indexes are fine. Nothing fragmented and all stats are up to date 
You can also reference the database or server (if linked) names in the FROM &amp; JOINS, e.g.: select * from [server].[database].table 
 How about: create or replace procedure topkcustomer(k in number) as begin INSERT INTO TopKCustomer_T ( customerid, customername, customerpostalcode orderfrequency, crank ) with full_set as( select c.customerid, c.customername, c.customerpostalcode, count(*) as OrderFrequency, dense_rank() over( order by count(*) desc ) rank_id from customer_t c join order_t o on o.customerid = c.customerid group by c..customerid, c.customername, c.customerpostalcode; ) select customerid, customername, customerpostalcode, OrderFrequency, rank_id from full_set where rank_id &lt;= k; end; One problem is that the count(*) is not guaranteed unique. Thus, use of dense_rank() can result in duplicate rank_id values (i.e., ties on count(*)). If you use row_number() instead, rank_id will be unique but the order on ties will put one ahead of the other (not randomly -- depends on how it got executed). So decide how you want it to act in that case. Straight SQL will perform much better than a PL/SQL cursor loop. What about if there are no customers with orders? 
This is much better than my suggestions, cursors in SQL are to be avoided if at all possible.
I'm pretty much required to use a cursor. I believe that is what is expected.
I can't believe that to be true. Demonstrate the performance difference -- I'll estimate &gt;10x faster (seriously). Of course, I **have** encountered this level of micro-management myself, in which case, I'm sorry. 
Understandable if it's homework or you've been told to, but it's going to perform relatively poorly.
Thanks for the help. It took me a bit longer to set up than I'd care to admit and there weren't any good tutorials or reference materials but in the end I managed to get everything up and running and have a local report server that I can access and work in. Did a demo today for some senior management and it went over very well. So thanks.
Other folks have already given you good advice, I'd like to touch on something a little different. It's possible to forget what server you're connected to, I like to have my connections highlighted just in case. (Connection bar is red in prod, yellow in test, green in dev.) Gives you that extra little visual cue. This will walk you through that: https://msdn.microsoft.com/en-us/library/hh213617.aspx#OpenServerColor 
well, the problem here is that the sequence of result rows from a union query **is undefined** unless the union query has an ORDER BY clause and a union query can have only one ORDER BY clause -- it goes at the end and sorts all the rows in the results from the union query so using a union query is really ~not~ the answer here
https://www.reddit.com/r/SQL/comments/5g2ly0/mysql_order_by_count_but_with_null_category_last/dap9dix/
 SELECT a , b , c , d FROM ... you've forgotten the commas ~between~ items 
In an old sense? No. In a modern sense? Yes, it compares pretty much the same. The consumption is no different. The only difference is the server configuration and layout.
Mostly it means that the query is slow, especially compared to alternative algorithms operating on similar data sets.
Check query plans between running ad-hoc and as proc. Guarantee its doing something different e.g. scan instead of a seek. You can force a proc to re-do its query plan in MSSQL with recompile option. That might help. As noted already, thats one hell of a query but I have seen much worse. Would take me a day or two I'd guess to get that running nicely ;)
Sorry, my mistake. I had thought it'd just append the second result. In that case, you could select the top 20 with a generated ROW_NUMBER OVER() PARTITION BY Country ORDER BY Count (GROUP BY Country), union the NULL result with a row number of 21, and order by that. But I expect there's a far more elegant solution. 
Glad to hear! I've setup SSRS so many times that it's just second nature... the setup "wizard" takes some getting used to... but it makes sense once you do it a few times.
I think that terminology came about because many database engines now use a "Cost-based" optimizer. Each execution step is given a score, the cost, that estimates how much memory/cpu/io/time it will take. The engine will make several plans and take the one with the lowest overall cost. An expensive query would have a high cost for even the lowest plan.
Is this MS - SQL there is quite a bit wrong here. Let us know what DB and we czn help 
I'm almost ashamed to admit that I've always had it installed on my work laptops before I started working. This new job just gave me a blank laptop and admin on it and told me to have fun because no one at the help desk had any idea how to. 
Oh... you want to download and install SSMS Tool Pack. I think it costs a few bucks now but it will save every query you ever run and give you a search feature. So for example in (1) year from now you can pull it up and in the search field type a table name, or a specific function, and it will give you every query you ever ran that matches that criteria. Can't tell you how many times I've accidentally saved a bad copy, or forgotten to save some work thinking I'd never need it again only to go into the search area and save myself a ton of pain. You can even organize it by date, so for example if I know I wrote a loop that did something special I can find that loop and see it was ran on 8/22/15, then I can set the dates at the top to only look at 8/22/15 and search for a generic term like "select" and presto, i get a fully history of everything I ran that day... so I get the loop and anything I did afterwards to validate or add to it. 
Oh jeez! Use a subquery to get counts by country, generate row numbers for that. But now it's getting pretty gross.
So you don't have the InsertedDate in your dataflow right now, you want to generate it at the time the SSIS package is run? If so, it is a derived column you're looking for. https://msdn.microsoft.com/en-us/library/ms139875.aspx Do note that this will return DT_DBTIMESTAMP though. I can't remember if you can implicitly convert that to e.g. DATETIME.
Hi, thanks for your patience. I've tried downloading this twice. The first time, I saw a duplicate of the 64 bit download of the database. The second time, the installer asked me to reboot. following the reboot, I can't see any difference to my environment. Thanks for your help.
FFS, unformatted code is impossible to work with: SELECT DISTINCT LW.LeadId , Case when DateDiff(day, LastActionDate, getDate()) &gt; 10 then 10 else DateDiff(day, LastActionDate, getDate()) end as "Days Since Last Action" , Case when DateDiff(day, LastDistributionDate, getDate()) &gt; 10 then 10 else DateDiff(day, LastDistributionDate, getDate()) end as "Days Since Last Distribution" , Case When EX.Document_DateReceived_CREDIT_REPORT is not null then 1 end as "CreditPulled?" , Datediff(DAY, LW.CreateDate, E.lockdate) as "Days Created to Locked" ,Datediff(DAY, E.LOCKDATE, E.CX_CLOSING_COMPLETION) as "Days Locked to Closed" ,Datediff(DAY, LW.CreateDate, getDate()) as "Days Created to Today" , Case when E.LOCKDATE Is not null then 1 end As "Number of Locks" ,Case when E.CX_CLOSING_COMPLETION Is not null then 1 end As "Number of Closes" , E.loanpurpose , LW.f51 as "Velocify Loan Type" ,--E.loantype , S.Title as "Status" , LW.f11 as "state" , C.Title as "Campaign" , (A.NameFirst + ' ' + A.NameLast) As "Loan Officer" , U.Title as "Team" , (B.BORR_FIRST_NAME_4000 + ' ' + B.BORR_LAST_NAME_4002) as "Borrower Name" , LastActionDate , AT.Title as "Last Action" ,LastActionNote , (A.NameFirst + ' ' + A.NameLast) as "Velocify Agent" , E.LOANFOLDER FROM ActionType AT ,Unit U,Agent A , Campaign C , LeadStatus S , LeadWarehouse LW LEFT OUTER JOIN [ENCOMPASS_DW].[dbo].[Encompass_loanData] E ON cast(LW.LeadId as varchar(50)) = E.cx_leads360id LEFT OUTER JOIN [ENCOMPASS_DW].[dbo].[Encompass_borrowerData] B ON E.GUID = B.GUID LEFT OUTER JOIN [ENCOMPASS_DW].[dbo].[Encompass_loanDataExt] EX ON E.GUID = EX.GUID LEFT OUTER JOIN [ENCOMPASS_DW].[dbo].[Encompass_loanData] E2 ON (cast(LW.LeadId as varchar(50)) = E2.CX_LEADS360ID AND (E.LASTMODIFIED &lt; E2.LASTMODIFIED OR E.LASTMODIFIED = E2.LASTMODIFIED AND E.CX_LEADS360ID &lt; E2.CX_LEADS360ID)) WHERE Createdate &gt;= '1-1-2016' AND LW.StatusId = S.StatusId AND C.campaignId = LW.CampaignId AND A.AgentId = LW.AgentId AND U.UnitId = A.UnitId AND C.title != 'DO NOT USE' AND AT.TypeId = LW.LastActionTypeId --AND (loanfolder IS NULL --OR --Loanfolder = CASE When loanfolder = 'Prospects' AND (loanfolder = 'Adverse' or loanfolder = 'Rescinded' ) Then 'Prospects' --when loanfolder = 'Active' and (loanfolder = 'Adverse' or loanfolder = 'Prospects' or loanfolder = 'Rescinded' ) Then 'active' --Else loanfolder end) AND E2.CX_LEADS360ID IS NULL That said it still looks ugly. This is MS SQL? Just use a CTE to get all the dates in the range you want and join to it for the labels.
Pretty ugly, and he's just dumping the resulting set into Excel so you might as well do the pivot in SQL .
This isn't really a SQL question so much as an excel question. Because you're sql won't return rows that don't exist but you want the excel pivot table to show them with a 0. I would look into some kind of dummy records in your table data, one for each "days since" option. And set some kind of active flag column on the table with those set to zero and real records set to 1. In the pivot table to give a count of the records in that group sum the active flag. 
Use a "numbers table". Google it
&gt; But does it literally cost money to execute a query Yea, you pay 50 cents for executing every line of code. So the more lines, the more pricey is the query. ^^^^^^^LOL
I find myself using dynamic sql for pivots a lot and think you'll only find Excel "more useful" if the columns don't change. I like graphing it in Excel but lately do all of the visualizations in either Tableau or SSRS... users can download them in Excel or save as PDF, or just import them natively into PowerPoint. For example I want to pivot by month but have the month rolling, so every month theres a new column and all the previous columns shift one position to the left. You'd have to do some Excel magic to cope with that in unaggregated data, or go in each month and screw around. Meanwhile I can just copy and paste the aggregated data into Excel where I have a visualization already prepared and it auto-updates. Conversely I can prepare that same visualization in SSRS or Tableau and then just have it delivered or accessible to the end user and never have to do anything. In either case I do the work in SQL. 
yes, the second way is equivalent and shorter your column naming convention is quite poor, by the way look what happens when you ~qualify~ your column names in queries involving more than one table, which is considered best practice, because it self-documents which tables the columns belong to, rather than doing this by "embedding" the table name into the column name -- SELECT customer.customername , customer.customerphone FROM employee INNER JOIN customer ON customer.customerrepnumber = employee.employeenumber WHERE employee.employeedepartment = 2 here's what it should look like -- SELECT customer.name , customer.phone FROM employee INNER JOIN customer ON customer.repnumber = employee.number WHERE employee.department = 2 note you should also be using ANSI joins
Lol sounds like a Kevin Trudeau saying...
The use of ANSI joins is not instinctive to me, but I agree it does look cleaner. Does it matter which convention I use for a small internal database where I'm the only one looking at the code?
No
&gt; IYO, What SQL must a Data Analyst know? 
&gt; IYO, What SQL must a Data Analyst know?
Do you have knowledge of stats? Can you apply that knowledge to business solutions and run statistical tests with python on a query extract? If you do, nothing is really holding you back. Just demonstrate an interest in the industry of the business and how you may have done some of the kind of work that they're looking for, and you're good
I'm sorry, that's insane that they don't know sql. Domain is great, but you have to know what's going on under the hood.
How advanced of stats do I need to know? I took a few econometrics courses and a stats 101 course. Also took calc1-4 in college and linear algebra.
Can you describe what logic (in your words) produces the "sorta like" table? So, you take 1 and a.jpg and lookup what in the second table, exactly?
Oh sure, sorry. I tried to make things as simple as possible to not sound too confusing. I have two tables right now. Everything is sorted in both of them. I basically want to make a new table that has all three columns in the same order. I want to take '1' as a lookup and for my first result to be 'a.jpg' and 'g.jpg', then get the next result as 'b.jpg' and 'h.jpg'. I wasn't in control of how this data was given to me, and I don't want to modify it. a.jpg would be a partial URL to a full sized image and g.jpg would be the thumbnail. PHP will be querying it, getting all the results, and writing it to a page. The image displayed will be the thumbnail and it will link to the full sized image.
&gt; Domain is great, but you have to know what's going on under the hood. No you don't. You have to *trust* what is going on under the hood is done correctly. This is one of the hardest mindsets to break of a DA moving from a world where they can see the "raw" data and the extremely specific details of how a result set is defined to one where ETL has done their calculations and translations for them -- ie, multi-dimensional cubes like Cognos or SSAS.
Because god hates me, thats why. lol, but for real: they are a burst of three images so they called them the same thing. They are a match in their alphanumeric order. 
&gt; How advanced of stats do I need to know? I took ... a stats 101 course. Heh, certainly more than an intro course. That's all I took and I got by but this was a decade ago and things like R or "big data" really weren't common things. But being able to tease out what metrics or numbers they are looking for and correcting any misunderstandings (here's why you can't average a series of averages, no you can't sum the dollar amount of each monthly balance to get one big number, et al.) the client has about the calculation and interpretation of the numbers you provide. A data analyst should also focus on learning the domain/business processes in and out and being able translate that to efficient code. Learning the domain will help you understand what questions they're attempting to answer based on the number they ask for. Knowing how to write performant code that is easily translatable to enable future updates and modifications less painless and quicker. Knowing how to write performant code requires a knowledge of storage structures and execution plans. A more advanced data analyst with things like "senior" in front of their name should start to become aware of processes that flow across objects like functions and stored procedures and will seek to identify a pattern that will help in building out the predictability of the code base or framework they look to build. Presentation of the data. Do yourself a favor and go read Edward Tufte's classic **The Visual Display of Quantitative Information**. Learn when to present which information in which format or style for maximum effectiveness.
Got it thanks to SO. It's been hours searching, but hey, I'm happy. It was like this (replacing id with img_id and val with thumb/text) SELECT A2.id, A2.val, B2.val FROM ( SELECT l.id, l.val, COUNT(*) AS n1 FROM A AS l JOIN A AS r ON l.id = r.id AND l.val &gt;= r.val GROUP BY l.id, l.val ) AS A2 JOIN ( SELECT l.id, l.val, COUNT(*) AS n2 FROM B AS l JOIN B AS r ON l.id = r.id AND l.val &gt;= r.val GROUP BY l.id, l.val ) AS B2 ON A2.id = B2.id AND n1 = n2;
Yeah, that's painful. My solution would be to create a new column for each table. If you're doing this a lot, views are probably best, but for a single run, these subqueries should work: select full.img_id, full.url as full_url, thumb.url as thumb_url from (select img_id, url, row_number() over (partition by img_id order by url) rn from full_table ) full inner join (select img_id, url, row_number() over (partition by img_id order by url) rn from thumb_table) thumb on full.img_id=thumb.img_id and full.rn=thumb.rn I don't use MySQL normally, but I think that should work.
I was going to say you'd use a row number on a more advanced platform. 
Well I ran it on a small subset and it worked, but on the full run I'm still sitting here drinking coffee. I'll give yours a shot. I am far from having any sort of SQL-fu up my sleeve. I just learn what I need to for what comes my way. This is about 2M rows.
I want to be in the top 20%, so what other stats and maths do you suggest I know?
Try a month out at pluralsight. Lots and lots and lots of good videos. 
lol advanced for business majors...
huh
I meant remove the 2 lines after your declare. Your for loops are fine.
use mod(z,b) instead of z%b. %is used for cursor attributes. for example, &lt;cursor name&gt;%found &lt;cursor name&gt;%notfound &lt;cursor name&gt;%isopen 
Thanks, that showed the output, a lot of 2's (lol), but thats the start, thanks my friend, have a good day/night.
If I had to show prime numbers between 7 and 999. set serveroutput on; declare v_cont number; begin for I in 7..999 loop v_cont :=0; for j in 1..I loop If mod(i,j) = 0 then v_cont := v_cont + 1; end if; end loop; if v_cont = 2 then dbms_output.put_line ( I ); end if; end loop; end;
Oh nice, I'll try it tomorrow(using the phone now). I really appretiate that you took your time to try to solve my issue. Thanks man.
Do you work in accounting now?
I dunno, I think of advanced as when you start getting into how the actual engine works. The things you labeled are pretty easy imo once you've done them a couple times.
By "real" problems I mean "real world". So, while I appreciate the interest, for me to explain specific projects or problems that I worked on would be missing the point. My point is that discussing the specifics of the problems you might encounter is less important than diving in!
I am applying for job now, so I'm looking forward to it!
What should I know before I interview?
SQL in general is pretty easy, but i was saying at least you're doing stuff like that. Yes, seeing what the engine, optimizer, and transaction manager do are all advanced sql. I know that data analysts don't know the optimizer or the actual sql engine. And that's not a necessity for their jobs. 
Wholeheartedly agree. This video is more or less fundamental basic MS t-sql. Depending on the type of DBA you are advanced stuff would definitely be what is mentioned above and if you're a MS DBA specifically I'd say query tuning, extended events, pivots, cursors, triggers, full text processing, processing xml and json, and I might even say transactions.
Json.... The whole reason I've moved to postgresql... Holy crap it's powerful
 INSERT INTO TopKCustomer_T VALUES (queryResult.customerID, queryResult.customername,queryResult.customerpostalcode,rank,queryResult.orderFrequency,SYSDATE); I did that with no such luck.. Errors: Error(20,2): PLS-00394: wrong number of values in the INTO list of a FETCH statement Error(20,2): PL/SQL: SQL Statement ignored Error(21,2): PL/SQL: SQL Statement ignored Error(21,122): PLS-00487: Invalid reference to variable 'QUERYRESULT' Error(21,134): PL/SQL: ORA-00984: column not allowed here
To me, advanced SQL is performance tuning and optimization, understanding good database design practices, and maybe more obscure but universally useful things such as windowing functions. If I hired someone for a beginner SQL developer position, I wouldn't expect them to know much or any of the above. If I hired intermediate, I'd expect knowledge of the above. If I hired an "advanced" developer, I'd expect a mastery of the above.
Sorry, that was an incomplete answer meant to get you unstuck. They will be in the correct order if your order by statement is correct. Can you post the entire block of pl/sql you ran that produced that error?
2 choices You can either set the value of the column as a primary key. That way if the value exists it will error and not allow the query to complete. Or you can check the value with code before you insert. Something like IF NOT EXISTS ( SELECT 'NO MANS SKY' FROM GAMES ) BEGIN INSERT INTO GAMES SELECT 'NO MANS SKY' END 
Wow.. That makes sense lol.. Thank you so much!!
Absolutely yes, at my past company we hired people with no SQL experience and would teach them if they had other skills when it came to web sites, etc. When they hired me I was one of the first new hires that had any previous SQL experience, but I had some programming / analytics experience free lance and previous IT experience. 
Just tried it again. Under the START button I now have SQL Server Import and Export Data (32-bit) SQL Server Deployment Wizard SQL Server SSIDB Upgrade Wizard SQL Server SSIDB 2016 Project Conversion Wizard SQL Server SSIDB 2016 Data Feed Publishing Wizar
I don't think you can use IF NOT EXISTS in Access. If he's using a form I think he can use Dlookup, test whether that returns anything, and disable the input if appropriate. 
I'm actually moving there to find work, so this sounds great!
Did you see this when you were installing? http://imgur.com/P4uwFxG
I'm actually writing a short PDF (50 pages) on some of the ways I like to analyze data using Pytho, Excel, SQL.
I meant more like a nice resume and portfolio, but you can leverage that PDF and publish it as a blog for that same purpose.
It worked! Thank you. I still have a long way to go with mysql, but didn't realize that scope_identity() doesn't work mysql (according to stackoverflow). I'm glad to see that last_insert_id does the same thing though. I've been struggling with this for a couple hours, so thanks a bunch
Not really sure what to put in my portfolio. I don't have too many projects made. Webscraper for nba stats that uploads to a sqlalchemy for professional stat lookup. random Wikipedia generator, which asks user if s/he wants to read it, opens in internet explorer, and then records all read articles. Local Price checker. Linear Algebra and Regression on Quant Econ Problems/ Matrix crawler. Job searching script. BlackJack. And a bunch of ETL Scripts. In VBA, I made a project for a previous job. Also, have auto emailing scripts in VBA and SQL queries through ADODB
The difference won't necessarily really show up in an execution plan for creating the indexes; it'll show up in various queries that get data out of the table. In general, add a field to the include statement if it's only being passed through (referenced in a SELECT statement) and add it to the key columns if it is being filtered or joined on.
Also the index takes up less space on the root and intermediate levels since the key is smaller.
I don't actually put 'tbl' in the front because I feel the same way, but it seems to be a common convention, so I just wanted to make the question more readable but thank you! Without getting into details about my database, I have plenty of other situations where I would need this type of action, this just seemed to be the simplest to explain. I've recently found what are called TRIGGERS. It sounds exactly like what I would want, any thoughts? As far as not having the tblResortHotel, do you have a suggestion as to how to go about it? My reasoning is that this table gives me the 'ResortHotelID' which is a unique combination of a hotel with a resort. For example, hotel 1 inside resort 3. With that I can just add the combination id in each relevant table instead of both individual ids. Let me know what you think. Like I said, I'm new to all back end programming, so I'm sure there are better ways
&gt; With that I can just add the combination id in each relevant table you'll need to give more background information on this aspect for instance, my first reaction is "how the heck can a single hotel belong to more than one resort?"
&gt; instead it should be aasser@it.ee for id = 1 and anasser@it.ee for id = 11 how is any computer algorithm supposed to know when to use 2 characters from the first name compared to only one?
Thanks for bearing with me. I finally gave up and uninstalled. I started to get concerned that I was going to cause PC problems like messing up the registry or something like that and decided to quit on it.
My apologies. I understand where the confusion is. The hotel table is really a 'type of hotel' table. For example every resort will have the same number of hotels and each hotel will be of varying luxury, but the hotel names will be the same across all resorts. Let's say there are 3 resorts, and 2 hotel types (the top of the line hotel and the lesser more affordable). All three resorts will have each of those 2 hotels. I'm pretty deep into my project, and it's only for class, but I realize that my table names aren't as accurate as I thought they were haha
You need to create a UNIQUE index (or primary key) on the EMAILS table *before* you run your update otherwise it won't go into the exception block - DUP_VAL_INDEX. update emails set email = null; create unique index email_uq on emails(email); Then run your script. EDIT: By the way, this is a pretty weird/bad way of coming up with email addresses, hoping this is just a homework assignment. 
Thanks! I just got lazy with my logic. Your solution is efficient.
While I agree that there are definitely better ways than SQL to generate email addresses and yes... SQL may be set-based but OP is actually trying to solve it in PL/SQL which is most assuredly *not* set-based. It can however switch back and forth to SQL any time it suits and make the best of both worlds. And it's not even that it can't be solved in SQL (row-by-row even) because it **can** it's just that the solution is too contrived to not look at something else first.
Hey thanks brother, I saw a wee little note in the link above... set numw 20 That fixed the issue for me. 
It depends on where you are right now. First I'd work on basic syntax, then work on actual data sets using different approaches. Then you'll want to learn about more of the actual objects you can create and alter - what is a schema, what are stored procedures, tables, database, indexes, roles, etc. Don't need to know in depth what each are, but that they exist and a gist of what they do. The above is probably what you'll be able to get to without actual work experience. Then you'll want to work on being able to do multi-step operations, so getting comfortable with temporary tables and CTEs probably. Being comfortable with subqueries. Getting to the point where syntax is not an issue. Getting to this point is usually pretty organic and a result of being able to visualize what you're doing and the tools you can do it with via experience. Try creating some stored procedures or functions (or scripts if your work locks down your privileges) to make your life as an analyst easier. Next I'd learn about indexes, what's a b-tree, how does the server actually look for my data, when to use indexes, how to use them. I'd learn this in tandem with reading execution plans. At this point you should be able to tune queries to some extent but the bigger benefit is being able to better understand how everything fits together and learn more about how the engine actually works...SQL is so high level that you can literally go your entire career and never need to know anything in-depth about this stuff since the language is simple and does most of the choosing for you. Next you can get involved in learning about how to actually architect a database - pros and cons of different structures, how to have data properly relate, if/when to create different schemas, locking down different parts based on role...you'll learn about backups, replication, CLR, stuff you probably won't want to do if you're set on being an analyst.
Don't use cursors. http://stackoverflow.com/questions/58141/why-is-it-considered-bad-practice-to-use-cursors-in-sql-server Don't write business logic in sql. Use a proper language. Sql is set based. You're trying to do something row based. 
It is homework assignment indeed, thank you, will try it out.
Yes. That's where my mind confuses. I thought about creating a sp. Which gets the most recent price for each code. Then checks next row. If its' price value is same then ignore it and go the next record. Do this unless it gets 3 different prices. Do you think is this viable?
So once I have completed the process of restoring the trl and the back files we then run a programme to extract the specific audit file we want from the database 
What you're talking about is a cursor loop. While it works, they're slow. Can you provide the full table definition?
No problem. Feel free to drop me a line if you've got any more questions.
looking like a CLR assembly might be the (only) way to go edit: conflicting info... may or may not be a valid approach
[REGEXP_LIKE](https://docs.oracle.com/cd/B12037_01/server.101/b10759/conditions018.htm) is your friend. eg. SELECT [...] FROM table1 WHERE regexp_like(column_name, '(string1|string2)')
REGEXP_LIKE is one part out of a set of native functions that have been implemented since Oracle 10g which allow you to use regular expression matching. The other functions are REGEXP_INSTR, REGEXP_REPLACE and REGEXP_SUBSTR. Here's a little more reading from the official docs: [https://docs.oracle.com/cd/B12037_01/appdev.101/b10795/adfns_re.htm](https://docs.oracle.com/cd/B12037_01/appdev.101/b10795/adfns_re.htm)
You would still use regular expression matching but in your case you also want to define word boundaries. I think this should do the trick: select column_name from table1 where regexp_like(column_name, '(^|\s)toms(\s|$)', 'i'); The 'i' tells you to do a case-insensitive search.
Looking into this... Microsoft really implemented an ugly solution there. To be honest, if I *really* felt the need to use this half-baked feature (and if schemabinding really prevents access to sys.* tables), then I would create my own table to store users &amp; roles and schedule a nightly process to refresh my table from the real deal.
Replace the insert in the trigger with :new.customerid := &lt; your sequence name here&gt;.nextval
I like the REGEXP suggestion from u/LittleRedDot. Alternatively, you can string together a bunch of OR statements, which is essentially what the IN operator is doing for you anyways.
Thanks. My only experience is with MySQL (primary) and MSSQL (secondary). 
i'm testing the CLR approach... code should be good, just haven't gotten it exposed yet... but "deterministic" as determined by MS assumes database state, so since I'm using the DATABASE sys tables, there's no external dependency (which I'd designed it for)... so the lookup can be considered deterministic, which I believe is one of the requirements for schemabinding... so i'm hopeful
Hmm....at first glance I'd guess it is your having clause -- it is simply just returning a number right now but it needs to also have an operator (SUM(sellprice) &gt; 1 etc or SUM(sellprice) &lt;&gt; 0 etc). Looks like you're trying to get the employee with the top commission? Plenty of ways to do it (not considering performance) but if you don't care if there is a tie for 1st, you can either put your calculation in an order by sorted descending then just get the top 1 row or use a subquery/cte and get the dense rank/row number. If there is a tie for 1st and you want to show both, you'll need to subquery or cte and use a rank.
Oh how I wish MS SQL actually had regex built in.
Here's how I solved it in MS SQL: Put all the terms in a temp table @Strings. SELECT ... WHERE EXISTS ( SELECT * FROM @Strings S WHERE COLUMN_NAME LIKE '%' + S.String + '%' )
Of course I have to mention my very own book about SQL performance from developers perspective: http://sql-performance-explained.com/ The best thing? It's also available for free on the web: http://use-the-index-luke.com/
Looks like this one is a piece of crap.
A lot of people I've spoken to recommend [High Performance MySQL](https://www.amazon.com/High-Performance-MySQL-Optimization-Replication-ebook/dp/B007I8S1TY/ref=mt_kindle)
[removed]
Free. One of the best I found http://www.studybyyourself.com/seminar/sql/course/?lang=eng. You have a basic course and an advanced one. You can submit exercises online too.
I recommend [SQL Queries for Mere Mortals ($36.64)](SQL Queries for Mere Mortals: A Hands-On Guide to Data Manipulation in SQL (3rd Edition) https://www.amazon.com/dp/0321992474/ref=cm_sw_r_cp_apa_nCQrybW9TPPNN) and [Database Design for Mere Mortals ($31.57)](Database Design for Mere Mortals: A Hands-On Guide to Relational Database Design (3rd Edition) https://www.amazon.com/dp/0321884493/ref=cm_sw_r_cp_apa_CDQrybA2508FS) Both helped guide me through my masters program, and they are fairly affordable. 
[removed]
Since Version 12c oracle has [Identity Columns](http://docs.oracle.com/database/121/DRDAA/migr_tools_feat.htm#DRDAA109) &gt; Oracle Database 12c Release 1 implements ANSI-compliant IDENTITY columns. Migration from database systems that use identity columns is simplified and can take advantage of this new functionality. &gt; This feature implements auto increment by enhancing DEFAULT or DEFAULT ON NULL semantics for use by SEQUENCE.NEXTVAL and SYS_GUID, supports built-in functions and implicit return of default values.
I wouldn't go that far, but the book has little practical value. Date is very knowledgeable, but he writes like an academic - he can take any piece of prose and suck the life out of it. [The Art of SQL](https://www.amazon.com/Art-SQL-Stephane-Faroult/dp/0596008945/ref=pd_sim_14_4/159-5139956-9497267?_encoding=UTF8&amp;pd_rd_i=0596008945&amp;pd_rd_r=RD7FKXKJBQWMJHSYBWA0&amp;pd_rd_w=7evRt&amp;pd_rd_wg=RwF2o&amp;psc=1&amp;refRID=RD7FKXKJBQWMJHSYBWA0) is a much better book.
perfect, thanks
Hi, This is very likely a coincidence and has nothing to do with disabling strict SQL mode in MySQL 5.7. Additionally, I don't see anything in what you posted that makes it look like your server is compromised. Here's what it looks like without further information: &gt; mysqli_query(): MySQL server has gone away in /var/www/html/wp-includes/wp-db.php on line 1868 MySQL crashing is most commonly caused by running out of memory, but can also be caused by running out of disk space and other conditions. &gt; [Tue Dec 06 07:47:21.097262 2016] [mpm_prefork:notice] [pid 23589] AH00163: Apache/2.4.18 (Ubuntu) configured -- resuming normal operations &gt; [Tue Dec 06 07:47:21.097393 2016] [core:notice] [pid 23589] AH00094: Command line: '/usr/sbin/apache2' &gt; [Tue Dec 06 07:54:16.190667 2016] [mpm_prefork:notice] [pid 23589] AH00169: caught SIGTERM, shutting down That's normal information that is logged when Apache is restarted or its configuration reloaded such as when adding or removing apps, adding or removing domain names from apps, Apache updating, etc. &gt; 130.193.51.38 - - [05/Dec/2016:08:21:55 +0000] "GET /file-archive-1692.xml HTTP/1.1" 404 517 "-" "Mozilla/5.0 (compatible; YandexBot/3.0; +http://yandex.com/bots)" These GET requests don't mean your server is compromised. These are all 404's, so there's nothing actually being served from your site here. The most likely explanation is that someone else has a domain accidentally pointed to your server's IP address. For example, they may have had your DigitalOcean server's IP address before you did and never changed their DNS after they stopped having that IP address. When your server receives requests for a domain that isn't associated with any app, the default app is used to serve the request. The default app is the app whose name is alphabetically first. Some people like to create a separate app named something like "0default" with a blank index.html page so nothing is shown if you visit the server's IP address and requests for unrelated domains pointed to your server's address go there. There's more info on the default app here: [https://serverpilot.io/community/articles/how-to-set-the-default-app.html](https://serverpilot.io/community/articles/how-to-set-the-default-app.html)
where would I include additional stings?
Also could you explain what (^|\s) (\s|$) is searching for? I am new to SQL (lol). Is there a good syntax resource? 
wow that is great. i am wrapping up my cs degree right now - the knowledge gained is less practical, more theory
Seems like you are very new to TSQL, to filter a fixed date it would be: SELECT * FROM TABLE WHERE DATE &gt; '2016-09-30'
You're very welcome.
registry file?
Each column is named the date from when the data is from. The columns contain how much money was spent that month. 
Get *the entirety* of your user folder starting at C:\Users\\{*yourName*}
For `COPY` you need to be database superuser, and "Network Service" needs read access on the file in the file system Consider using `psql` (command line) and `\copy` instead
Right, I think we're gonna need you to post a screenshot of Sql server Management Studio with the table and column names visible on it. 
i get why row level security would require schemabinding... need to make sure that no schema changes interfere with the security model. I also get (mostly) why schemabinding requires deterministic functions... though more so out of its use for persisted computed columns. and I understand why DMVs may not be deterministic... selecting from sys.principals is dependent on the server/instance... which would change between servers, environments, etc... but SOME tables, like DATABASE_PRINCIPALS, should be marked as deterministic, since they ARE persisted within the database. just a matter of applying a broad policy on sys tables / DMVs, and not spending the time/$ to refine the policy where it can be justified... alternatively the use of SCHEMABINDING could actually be split between SCHEMABINDING (locked to the objects from a referential integrity perspective), DATA_PERSISTED_and_ISOLATED (data is stored in the MDF), and DETERMINSTIC_INDEPENDENT_OF_DATABASE_STATE (pure functions/calculations, as opposed to those which are also dependent on DATA_PERSISTED_and_ISOLATED). I'll end up doing a write-up, and eventually a connect feedback... but not today.
I am trying to avoid that specifically. 95% of all the files in there are not useful and I don't want to back up an extra 11GB of data that I don't need only to get the 16MB that I do need. I only have an 8GB thumbdrive and I'm trying to make sure I am only getting the essentials.
I always make a backup of my ODBC entries from the registry. Should be somewhere like HKEY_LOCAL_MACHINE\SOFTWARE\ODBC\ODBC.INI\
That looks like what I'm going to have to do, I just didn't want to hard code it 
Search for dynamic query and depending on your SQL version use the command EOMONTH() to get the last day of each month and use it to create the string for the dynamic query. But in this case hard code will be your best bet.
What is soup to nuts SQLZoo has a great introductory overview tho
Yes, it's a pretty amazing add-in for SSMS; I use the free community version, and am really sad now when I get on a machine that doesn't have it installed. http://www.ssmsboost.com/
Oh right, thanks! I think this worked.
What do you mean by check for null? I assume you mean there's a way to catch nulls and rename it something else? How would you do that?
soup to nuts is just an idiom meaning start to finish
I was trying to read that page but it is written and formatted like a scammy shareware product page from the 90s. It is killing my eyes. I think I'm going to install it to see how I like it. The formatter alone would be helpful, not to mention the ability to locate tables in the object explorer by right clicking.
First, thanks for the helpful response. I accidentally deleted the post when you pointed out my bad reddit formatting I was trying to fix so hopefully you can still answer. I feel like I'm 95% there, just need a bit more help with the SELECT...INTO part. Here is what I have: &gt;CREATE OR REPLACE PROCEDURE CLOSE_SHIPPING &gt;(IN_SHIP_NUM IN NUMBER) AS &gt;BEGIN &gt;SELECT * INTO SHIPPING WHERE SHIP_NUM = IN_SHIP_NUM; &gt;IF SHIP_NUM = 'CLOSED' THEN &gt;Dbms_Output.PUT_LINE('Shipping is closed'); &gt;ELSE &gt;UPDATE SHIPPING SET SHIP_STATUS = 'CLOSED' WHERE SHIP_NUM = &gt;IN_SHIP_NUM; &gt;UPDATE SHIPPING SET SHIP_CLOSEDATE = SYSDATE WHERE SHIP_NUM = IN_SHIP_NUM; &gt;END IF; &gt;END; I get ORA-00923: FROM keyword not found I've tried messing with it for a while but not sure what I'm doing wrong
What you had for the SELECT INTO was correct, except it needed the `WHERE` clause. select SHIP_NUM, SHIP_STATUS, SHIP_CLOSEDATE INTO SNUM, SSTAT, SCLOSE FROM SHIPPING where SHIP_NUM = IN_SHIP_NUM; You also need to add the declarations back in after the word `AS` (see #2 above) and `COMMIT;` after the `UPDATE`s (see #5). 
Hi! @rbardy It Did not work, tried it before, and the datalenght gave a different error because the check, and on other forums People wrote that it would only work with a select..? Thats why I used a subquery. Thanks for the reaction :)
Hi! I think you will get 1 or 2 points Less then the maximum amount of points for that question. Its that way on my study. Don't worry too much and have a Nice day :)
You will likely be violating Acceptable Use by doing that. If you are switching to a new PC, just work with the tech doing the swap.
Quick introduction with examples and online exercises. For free. Well suited for people willing to learn quickly. http://www.studybyyourself.com/seminar/sql/course/?lang=eng.
Can you please explain more about the solution ? Because I have the same problem as yours, still cannot find the correct solution. I would like to try your solution, but I did not know where to changed or put? Thank you :)
you can try to create a computed column as DATALENGTH(cover_image) and then check constraints on it
Sb 
forget SQL, describe in your words how do you get a single record from the userData table given the data from the users table.
I would also recommend you take an English course.
I'm doing a MS online course. I'm getting in the habit of running the query before I submit any answer.
I'm currently doing this one and would recommend it. https://www.edx.org/course/querying-transact-sql-microsoft-dat201x-4 Although it only covers queries, not creating databases, so it's on the beginner side.
Thank you! :)
Sorry i'm not really sure what you're trying to do exactly? Can you be more specific? what field from users or userData table are you trying to join onto the other? What are all the fields in both tables?
I'm honestly still not following, and getting more confused when you say you want to create a new table every time a new user is created. I think you need to articulate in plan english what you're trying to do (ignore SQL) and show us better example of the data and what you want the data to look like.
Cross join, if userData will have at least one row. "Left join on 1=1" if userData may be empty. Since, as I understand, "userData" table name going to be different per user, you'll need to create the SQL dynamically. PS. Anyone mentioned to you yet that this (creating a new permanent table per regular event) is not a good practice?
Cross JOIN will get him what he is asking for, but not what he wants (Or at least not what is best db practices). He's in for a world of hurt if he's going to make a different table for each user. Make 1 table and put everyone's details in that table, add a Foreign Key to this mega detail table that can tie back to email.
Sure it is regedit will let you dump subsets of the hive, but if your modifying your registry manually in that way you probably aren't installing your software properly in the first place, or are doing something pretty dramatic... and I would only really recommend doing it if you were moving from the same device to the same device or if you wanted to copy a specific subkey you should be fine.
This has been said more than once, but _what if there's that one trick that "db professionals" do not want him to know_? 
Either one would work just fine for those requirements. However, PostgreSQL has cstore_fdw which adds a columnar storage format option, which might be quite helpful in your use case. I'm not aware of a MySQL columnstore option, but I'm not a MySQL expert either.
No, but I have experience with columnstore format in MS SQL and find it pretty impressive with very large tables, especially when IO is your bottleneck.
Yep, they make huge improvements if the number of columns is more than a few. However I do not know anyone using cstore_fdw so I wanted to ask if you have experience or not.
The more I think about it the more inclined I am to recommend a output file archival process that saves the output file each day into a zipped/compressed readonly file structure that you simply reach into whenever someone asks for an old file. Doing so would completely remove the need for the sql/server backups + log file nightmare. The drawback would be that it would take time to build up the archive store. It's that or engineering a way of extracting these files (you never gave us the details of how the files are actually generated BTW) for any historical date, bypassing the need to get the backups restored. This would probably require building more tables, more processes to populate them and would most likely be more trouble than it's worth. Best of luck whichever way you decide.. 
In the database itself no, you need to code it in whatever application you are using to show the data, like conditional format in Excel for example.
Which version of SQL are you using? Oracle, SQL Server, MySQL ... All in all, in my opinion Access is a good and easy front-end solution to pretty much any database, if it will be used locally, if you need a web solution you'll need to learn a bit of programming and go for .NET or Java.
AWS should have some sort of console or dashboard, but I'm not familiar with AWS.
Maybe phpMyAdmin with modified user permissions? https://docs.phpmyadmin.net/en/latest/privileges.html 
You should probably be engaging with your tutors for help. You're probably paying good money for the privilege and they'll help you much more than getting spoon fed answers by reddit. Having said that: Nothing you've shown has proved that the constraints actually exist (best to have a look at the design/create table statements to see if they're actually there), but looking at the entity diagram: It looks like your bill table has bill number as the PK, which by definition is unique. That may not be the best approach if the bill number is actually something that is "real" data (i.e., the number which gets printed on the invoice), in which case you should have a separate invoice number, which would be unique (it's bad practice, especially in invoicing, to have an invoice number that can be affected by technical quirks of identity fields as data that users actually use). The foreign key would be something like CustomerID - it's something that corresponds to a key of another table. The constraint would exist to make sure that you don't have a bill that relates to a customer record that didn't exist. btw you're now academically obligated to include this reddit comment in your project's bibliography.
edit: just realized you might not've meant Microsoft SQL server specifically. Itzik Ben-Gan is solid. Particularly T-SQL Fundamentals and T-SQL Querying. I use them as reference when I can't stand to look at a screen or Books Online any longer. https://www.microsoftpressstore.com/authors/bio.aspx?a=0a6ec5c6-5053-4dff-8c45-ebbf279b9c48 Also SQL Server 2012 Internals is more advanced, but opened my eyes to 'how it actually works'. https://www.microsoftpressstore.com/store/microsoft-sql-server-2012-internals-9780735658561
My first instinct would be to try and get rid of the index FFS operation (which seems like it's disproportionately expensive) along with the RIGHT SEMI hash operation by turning the outer-most IN operation into an EXISTS and giving that a shot. It's really difficult to say without having at least some first-hand contextual knowledge of the database. EDIT: I'd dispel the idea that having indexes will necessarilly make everything work faster. It's possible that a good old fashioned full table scan on workorder might work faster (/*+ full(workorder) */). First make sure that your statistics are up to date.
The article mainly answers the arguments of Mixpanel's new query language JQL. You usually don't want to use Python or R to analyze more than a few billions of events, do you? If you take a look at the distributed database solutions, you will see that Elasticsearch uses JSON based query language, Mongo uses Javascript, SaaS products (Mixpanel, Keen.io) also use Javascript and JSON etc. because NO-SQL is a big trend in this industry. My point is that SQL is not just for RDBMSs, you can use it almost anywhere you want to analyze the data.
Thanks for confirming. 
According to [this](http://go.microsoft.com/fwlink/?LinkId=230678) it supports "Standard" AlwaysOn Availability Groups. Do you have the underlying Windows clustering components setup correctly?
Ah, my point was that when you have large amounts of data, Python and R probably would not be effective enough for you. You can solve the distributed computing problem with Spark but I don't know any company that processes more than a few billions of data uses Python as their core component for processing the data. It's perfectly fine when prototyping but in production you usually want to use a low level language for performance reasons but of course it's not required if you have the hardware. In fact, you're right about Mixpanel that most of their users do not need to execute complex analytics queries but the landing page is quite misleading.
I did, and that's the misleading article that lead me to post this question. When I deployed it in my lab, I uninstalled the Standard version and installed Enterprise. It worked flawlessly afterward. Apparently if you upgrade from a SQL 2012 version that supported AAG's to 2014 STD, it retains that functionality. Otherwise you're SOL. SQL 2014 STD only supports Mirroring, and AlwayOn Failover Cluster Instances. 
When you have a hammer.. 
You might want to double check what RDBMS stands for, because it's not what you think. 
HAVING. You need to use the HAVING clause if you want to filter on aggregated values.
Most of the maths packages in python is built on numpy which is implemented in C. It's fast. 
I wouldn't expect that to throw an `ORA-00905: missing keyword` error. I would expect something like this: ORA-06550: line ###, column ###: PLS-00428: an INTO clause is expected in this SELECT statement That's because of this: &gt; SELECT '5' AS num FROM dual; SELECT '6' AS num FROM dual; If you `select` something in PL/SQL, you have to store it somewhere. `AS` doesn't do that. First you'll need to declare variables to store the results of those queries, then you'll have to `select ... into`: DECLARE records NUMBER := 2119875215; num1 varchar2(1); num2 varchar2(1); BEGIN WHILE records &gt; 2119875205 LOOP SELECT '5' INTO num1 FROM dual; SELECT '6' INTO num2 FROM dual; records := records - 1; END LOOP; END; Alternative, if you want to select without storing (the only reason I could think of to do this is if you were trying to burn through values in a sequence), you could use `execute immediate`: DECLARE records NUMBER := 2119875215; BEGIN WHILE records &gt; 2119875205 LOOP EXECUTE IMMEDIATE 'SELECT ''5'' AS num FROM dual'; EXECUTE IMMEDIATE 'SELECT ''6'' AS num FROM dual'; records := records - 1; END LOOP; END;
Sorry about the title, as I was typing the question I realized what I needed was different from the title, but I already pressed submit by then. :( 
If I understand your question right, you want to SUM the payment amount Partitioned By the transaction #, a Window function will work for that. Not to be an ass as it kind of looks like a homework question, but try googling the following. Should have a stack overflow example in the first couple results. sql server window function sum over Window functions are great as you can do other things like running totals
I think if you store it as a variable then you should be able to: declare @qry varchar(max); set @qry = 'CREATE TRIGGER '+quotename('tr'+@TableName+'_IUD')+' ON '+quotename(@SchemaName)+'.'+quotename(@TableName)+' FOR INSERT, UPDATE, DELETE;'; exec (@qry) 
'len' function doesn't count trailing spaces so try trimming it: INSERT INTO table ( [chassis]) SELECT rtrim(quote_chassis) FROM table2 
Yes. I started both applications as Administrator 
Like the idea and excited to give it a shot even if just for personal home use.
Oh oops, it's 4-T in the first table as well. 
Here's a script I used to create history tables and triggers to populate those tables. It's build into the latest versions. Maybe you can glean what you need from it. Declare @history_table_suffix varchar ( 255 ) , @tablename varchar ( 255 ) , @execute bit Select @history_table_suffix = '__h' , @tablename = '' , @execute = 0 Set nocount on Declare @crlf char(2) , @tab char(1) , @table_drop varchar ( MAX ) , @table_create varchar ( MAX ) , @trigger_drop varchar ( MAX ) , @trigger_select varchar ( MAX ) , @trigger_create varchar ( MAX ) Select @crlf = char(13) + char(10) , @tab = char(9) /* cursor variables */ Declare @table_name nvarchar ( 128 ) /* declare cursor */ Declare Table_Cursor Cursor For /* select rows from table */ Select t.TABLE_NAME From INFORMATION_SCHEMA.TABLES t Where t.TABLE_TYPE='BASE TABLE' and t.TABLE_NAME not like '%' + @history_table_suffix and t.TABLE_NAME &lt;&gt; 'dtproperties' and t.TABLE_NAME = case when @tablename = '' Then t.TABLE_NAME Else @tablename end /* open cursor */ Open Table_Cursor /* fetch first row */ Fetch Next From Table_Cursor Into @table_name /* begin transaction */ /* loop through each row */ While (@@FETCH_STATUS = 0) Begin /************************************************************************************/ -- table drop Select @table_drop = 'if exists (select * from dbo.sysobjects where id = object_id(N''[dbo].' + @table_name + @history_table_suffix + ''') and OBJECTPROPERTY(id, N''IsUserTable'') = 1)' + @crlf Select @table_drop = @table_drop + 'drop table [dbo].[' + @table_name + @history_table_suffix + ']' + @crlf Select @table_drop = @table_drop -- trigger drop Select @trigger_drop = 'if exists (select * from dbo.sysobjects where id = object_id(N''[dbo].[iu_' + @table_name + ']'') and OBJECTPROPERTY(id, N''IsTrigger'') = 1)' + @crlf Select @trigger_drop = @trigger_drop + 'drop trigger [dbo].[iu_' + @table_name + ']' + @crlf Select @trigger_drop = @trigger_drop -- table create Select @table_create = 'Create table [' + @table_name + @history_table_suffix + '] (' + @crlf + @tab -- trigger create Select @trigger_create = 'create trigger [iu_' + @table_name + ']' + @crlf Select @trigger_create = @trigger_create + @tab + 'on dbo.[' + @table_name + ']' + @crlf Select @trigger_create = @trigger_create + @tab + 'for insert, update'+ @crlf Select @trigger_create = @trigger_create + @tab + 'as' + @crlf Select @trigger_create = @trigger_create + 'Insert' + @crlf Select @trigger_create = @trigger_create + @tab + '[' + @table_name + @history_table_suffix + ']' + @crlf + @tab + @tab + '(' + @crlf + @tab + @tab -- trigger select Select @trigger_select = 'Select' + @crlf + @tab /************************************************************************************/ /************************************************************************************/ /* cursor variables */ Declare @column_name nvarchar ( 128 ) , @datatype nvarchar ( 128 ) , @character_maximum_length nvarchar ( 128 ) , @numeric_precision nvarchar ( 128 ) , @numeric_scale nvarchar ( 128 ) /* declare cursor */ Declare Column_cursor Cursor For /* select rows from table */ Select t.COLUMN_NAME , t.DATA_TYPE , t.CHARACTER_MAXIMUM_LENGTH , t.NUMERIC_PRECISION , t.NUMERIC_SCALE From INFORMATION_SCHEMA.COLUMNS t Where t.TABLE_NAME = @table_name and t.DATA_TYPE &lt;&gt; 'image' /* open cursor */ Open Column_cursor /* fetch first row */ Fetch Next From Column_cursor Into @column_name , @datatype , @character_maximum_length , @numeric_precision , @numeric_scale /* loop through each row */ While (@@FETCH_STATUS = 0) Begin -- table Select @table_create = @table_create + '[' + @column_name + '] ' + @datatype If ( @datatype = 'varchar' or @datatype = 'nvarchar' ) Begin Select @table_create = @table_create + ' ( ' + @character_maximum_length + ' )' End if ( @datatype = 'decimal' ) Begin Select @table_create = @table_create + ' ( ' + @numeric_precision + ', ' + @numeric_scale + ' )' End Select @table_create = @table_create + ' NULL' + @crlf + @tab + ', ' -- trigger Select @trigger_create = @trigger_create + '[' + @column_name + ']' + @crlf + @tab + @tab + ', ' -- trigger select Select @trigger_select = @trigger_select + '[' + @column_name + ']' + @crlf + @tab + ', ' /* fetch next row */ Fetch Next From Column_cursor Into @column_name , @datatype , @character_maximum_length , @numeric_precision , @numeric_scale End /* close and deallocate cursor */ Close Column_cursor Deallocate Column_cursor /************************************************************************************/ /************************************************************************************/ -- finish table Select @table_create = @table_create + 'date_create_HISTORY datetime' + @crlf Select @table_create = @table_create + @tab + ')' + @crlf Select @trigger_select = @trigger_select + ' getdate()' + @crlf Select @trigger_select = @trigger_select + 'From' + @crlf + @tab + 'inserted' -- finish trigger Select @trigger_create = @trigger_create + 'date_create_HISTORY' + @crlf + @tab + @tab + ')' + @crlf Select @trigger_create = @trigger_create + @trigger_select + @crlf -- check to execute If ( @execute &gt; 0 ) Begin Exec ( @table_drop ) Exec ( @trigger_drop ) Exec ( @table_create ) Exec ( @trigger_create ) End Else Begin Select @table_drop + 'GO' Select @trigger_drop + 'GO' Select @table_create + 'GO' Select @trigger_create + 'GO' End /* Select t.COLUMN_NAME From INFORMATION_SCHEMA.COLUMNS t ( nolock ) Where t.TABLE_NAME = @table_name */ /************************************************************************************/ /* fetch next row */ Fetch Next From Table_Cursor Into @table_name End /* close and deallocate cursor */ Close Table_Cursor Deallocate Table_Cursor 
Holy shit, this is great. Still messing with it to test capabilities but I can confirm it's working at a basic level on osx 10.12.
Option A if you can get away with it KISS. Use GUID's instead of integers for PK
Cool thanks for the feedback. I guess "Option A" makes sense, until there's a specific case/reason for something else? I'm actually not really using sequences/auto_increment, I just said that to keep it simple here to mean a new single-column PK. I'm actually doing something very similar to UUIDs with BIGINTs in order to keep the memory usage on all the indexes down, JOIN performance up, and also for easy integration with stuff like Elasticsearch etc that isn't as well suited to using UUIDs natively. I spent a few weeks researching UUIDs, I think I've read pretty much every page/forum/blog etc on the subject at this point, many multiple times, haha. And I do use them in some places, but kept coming back to the conclusion that they're overkill for PKs + JOINs in these instances. If in the future I ever really needed UUID specifically for PKs+JOINs, my scheme can cast my already-unique BIGINTs to UUIDs. I was more just comparing the options of this table having "it's own new automatic single-column PK" -vs- "using a composite (user_id,thread_id) PK". Any thoughts on that?
Oh well, I guess it's not doable :(
&gt; And we weren't allowed to use IDENTITY Ok. I shall not question the teacher (I presume), but it's really stupid.
Yes, but you have to get into the dangerous game of dynamically building SQL strings and executing those. Something like: declare @db nvarchar(50) = 'DatabaseA'; EXECUTE('SELECT * FROM ' + @db + '.SCHEMA.REPORTINGVIEW');
Dynamic SQL is the only way I'm aware of to deal with this. IOW, rather than just constructing a query, you construct an `nvarchar` string (concatenating with your variables) and then pass that to `sp_executesql` /u/peschkaj gave a good talk on [Dynamic SQL at PASS Summit 2014](http://www.sqlpass.org/summit/2014/Sessions/Details.aspx?sid=6071). I don't think the video is available online but his demos &amp; slides are.
I thought it was online somewhere on PASS's site but, alas, no. You are in luck, though - I've got it available at [https://s3.amazonaws.com/facility9presentations/Dynamic+SQL.mp4](https://s3.amazonaws.com/facility9presentations/Dynamic+SQL.mp4) The content isn't going to be exactly the same, as things change over time, but if you want the commentary to go along with the slides, it's available for you to peruse. Also - if anyone has dynamic SQL questions, feel free to PM me.
You can get around the dangerous aspects of this using `QUOTENAME` to safely escape the parameter as well as `PARSENAME` to verify that it's actually a valid database name. On the whole, this is a bad idea and the application should route reporting queries to the right database. The application can then change the connection string and connect to the right database. TADA, no more random security holes in your application.
This is great! I plan on watching the second I have some downtime. Thank you very much! I was able to figure out the issue I had using dynamic SQL. So I'm very interested to learn where else I can go with it.
Thanks! I was able to get this to work. I also got trunc() to work as needed as well. 
&gt; "Oh yeah SQL Server should automatically cast it to a string." SQL Server do "implicit" conversions, but it has [Data Type Precedence](https://msdn.microsoft.com/en-us/library/ms190309.aspx) meaning that it tries to convert a data from a low precedence to a higher one, and strings is lower than numbers, so when you try to concatenate the INT variable in the dynamic string the SQL will try to convert the string to INT instead of INT to string. EDIT: If you run the following, the result will be 2 print '1' + 1
Do you mind sending me the scripting for the tables so i can try and do it?
the PK in any relationship table, i.e. a table which contains FKs to the tables being related, whether two-way or three-way, should be a composite PK consisting of the FKs would a Primary Key ID field hurt? yes, because of clustering inefficiencies and especially if there isn't also a UNIQUE key on the composite FKs... and if there is, why bother with a separate ID PK?
Upvotes for all. Thank you! 
They are being captured and stored in the DB as a 10 digit phone number without any formatting. 
Then they should be stored as strings, with formatting done at the presentation layer. Since it's 10 digits, I'm guessing this is a North America only application. No consideration at all for foreign numbers? How do you handle people who don't have direct dial numbers, but instead a front end and then you have to dial an extension (`123-456-7890 x2345`, for example)?
This was my very first reaction. Many like to use brackets some locations require a plus for international codes. I would like to make things easy for everyone and remove all special formatting. Only numbers but save them as string. However, I see the formatting is useful for helping end users who are entering info to prevent typos. 
Thanks 
Thanks for the time. 
I am taking your advice. I only did it the other way because that's what I had been told many, many years ago.
Yes, North America only. It is for a private employee and asset tracking application I have been developing over the years. My first task at this employer was to build an employee tracking and equipment inventory app. It has grown to many different itterations over the past 10 years and every year the feature requests get larger and more complicated. In my process to always better things I am always looking for better ways to do things. I am re-doing lots of it with better code now.
This submission has been randomly featured in /r/serendipity, a bot-driven subreddit discovery engine. More here: https://www.reddit.com/r/Serendipity/comments/5hizc3/if_anyone_could_help_me_find_a_specific_query/
thanks for oyur upvote
I have more oracle experience but we have the ability to set the current schema. It looks like mssql has something similar https://msdn.microsoft.com/en-us/library/ms188366.aspx
Thanks!
looks like you are missing a space before "from". p.ProjectDescriptionfrom
Search for sql pivoting. You'd usually do this with a few max(case when...) type statements, and don't forget to group at the end of the sql statement.
&gt; And I can use .php to re-organize it into a table, but ... I feel like I should be able to do this better in SQL without a pivot operation, it's awkward SQL and loses some efficiency php might be more concise and easier to maintain
Fix the table structure and both your query and new stats will be very easy.
You could use a self-join, where you join the table to itself. For one table, you'd get the results of Joe, and the other for Bob. You'd join on the competition key.
Nice of you to track me. I am a developer. I created [PornBOT] (https://pornbot.net), a NSFW automatic GIF-generating robot that tracks all of the porn on the Internet and auto posts to reddit. It has processed over 50,000 scenes to date. Do not tell me that I am unaware of SQL injection methods, as clearly I am. What is funny here are the hacker's literal instructions. What is not funny, is you.
https://i.imgflip.com/1c4uye.jpg
Oww
NoSQL || GTFO
test it until you're happy but the sql standard is very clear on what's supposed to happen
I have this working: SELECT R.RentalID, R.RentalDate, RD.FeeCode FROM Rental_cac R INNER JOIN RentalDetail_cac RD ON R.RentalID = RD.RentalID WHERE R.RentalID = 10 It's not the whole thing because I still need to connect the fee code table but I'm not entirely sure how to attach a 3rd table.
Ok I think I get it now, thanks!
Who hurt you?
Jason Bourne.
The movie was bad. The tech scenarios were stupidly bad. For example... Ignoring the fact that the German hacker guy was stupid enough to access the USB drive while connected to any network, including the Internet, the whole 'deleting the files using a phone in the room' - a phone that was completely unrelated to anything they were doing - made no sense. In particular, it made no sense because we had already established that CIA tech lady had already planted MALWARE.EXE on the thumbdrive. I really wanted to like this movie. I liked the first three and tolerated the fourth (with Jeremy Renner). This one was terrible.
cac? 
no. there aren't any that i could see well done
Isn't it obvious? For science!
i would have used TCPIP
from urban dictionary: on fleek: A word used by uncultured idiots. "I am stupid, so I say on fleek."
My god... 
So Im fairly new to SQL and the fact that you're storing data for a video game is super neat. I wouldn't have paired SQL and video games together so what exactly is the purpose of storing data like this for a video game? Sorry if this comes across as complete naive, Im still learning how to apply SQL. 
Total remaining is an alias you created at the very top level. Since the SELECT is executed after the WHERE clause your query has no idea what the field is. Use the true field name in your where clause - total earned.
Ahh gotcha. That worked perfectly! Upvotes for you! :D 
Table name in quotes?
Just curious, why did the author omit many of the other methods to record data changes? There are more than 2.
Syntactically, I don't see anything. Logically, it's impossible to say since there's no indication of what your homework assignment was.
Professor Scott, I assume? I'm stuck on sever of the Queries haha
UTA INSY 3304? If so feel free to hit me up with a pm with your Skype and we can work together. 
Yes, because corruptions.
Something like this: SELECT T1.fn_Competition, T1.fk_Throw AS fk_Throw1, T1.nm_Thrower AS nm_Thrower1, T1.Distance AS Distance1, T2.fk_Throw AS fk_Throw2, T2.nm_Thrower AS nm_Thrower2, T2.Distance AS Distance2 FROM throw AS T1, throw AS T2 WHERE T1.fk_Competition = T2.fk_Competition AND T1.nm_Thrower1 &lt;&gt; T2.nm_Thrower 
You have to understand that SQL is only one of the skills that data guys have but it is their full set of skills which is in demand today. The last couple of yours there has been a shift towards data driven organizations. Meaning the data has taken a major role on running the organizations and SQL is the defecto language to work with data. For more your way of thinking keeps us SQL developers in high demand and well paid as well :) . Basically a lot of programmers do not have "respect" for data. Thinking that the application is the important part. But it is the data what is important and the application is there just to help users interact with them. With bad database designs comes data quality issues and hard to use/understand data. That makes a high demand for Data guys. 
There's a monumental difference between being able to write SQL and being able to write SQL *well*. Case in point - about a year ago a developer was claiming "the database is slow". I looked at what he was doing as nobody else was having that complaint. Surprise, surprise, his query sucked but he thought it was the fault of the database
Yep! So many little things can make such a significant difference!
Try using case statements or a derived table. Assuming you want it all in one query 
Core i5 should be sufficient. I'd focus on getting more ram and a good SSD. 
&gt; Current position, it's becuase there are no data constraints on the front end application that feeds this db so it's frequently broken and the front end team doens't care enough/understand enough about databases to make a proper fix. Time to start throwing some check constraints on the database (long overdue anyway, you can't trust the client) and give those devs a wakeup call.
I manage a small team, which includes two skilled developers. Your 'people cannot wrap their heads around set based logic' is spot on. I wanted to get out of the SQL-writing, database design, and general database maintenance part of my day-to-day existence, but it just wasn't going to stick despite my best efforts. Someday I'll hire a full-time SQL developer. And he'll look at the stuff I've done and innocently suggest that whoever was doing all the database stuff before should be fired. I'll sigh and tell him that he's probably right.
There's a lot of terrible SQL developers out there. The better ones, like me, act as a lead Sql Developer -- I can't tell you how many times I've had to help other people get their SQL right. It's the difference between a Stored Procedure running 2 hours vs 20 minutes. A month ago a "sql developer" upstairs took the entire productional database down with their 10-hour query that filled the tempDB. 
Case statements allow you to select rows from a table given specific rules that don't apply to the entire table like the where clause does. 
Many shops work at different levels. There are programmers and BAs who sit there writing SQL to perform ad-hoc queries. There are architects who come up with the database design and their are administrator who ensure that everything runs, is reasonably responsive and works. Note that I do not mention the SQL developer. However, there is always a SQL guru in a larger place who is the goto person to check queries and restructure them. The guru may just be a developer, they may be an architect or a DBA but they act as a consultant to others. Business requirements evolve over time. Databases much often change to suit those requirements and data accumulates over time so restructuring becomes necessary.
&gt;you create a database quickly and then proceed to code around it using languages That is part of the reason why we are in demand haha Code in SQL isn't hard, code good SQL is quite hard, besides the knowledge we need to have of the front-end code that will call the SQL queries and the DB structure to tune the performance of the query and also the business to make sure that the result makes sense.
&gt; A month ago a "sql developer" upstairs took the entire productional database down with their 10-hour query that filled the tempDB. Not quite on the same scale, but we used to have an analyst who would run queries, dump the results to Excel, then filter them there. The query took 15-20 seconds to run. She was filtering on an indexed field - with the simplest of `WHERE` clauses, she'd get the same results in less than a second (and far less disk I/O). "Oh, but this way is fast enough for me, it's fine" - sure, let's ignore the impact on the hundreds of other users of the database.
 Yeah needed to use 'to_char' instead of 'to_date'. 
Thanks! I should clarify my needs further though - specifically, I need this query to result in a code that I can export to Excel and be properly formatted for date purposes. Currently, sysdate, to_char() are formatting as general, and to convert to data I have to manually select all values in Excel and use the Text to Column tool (which is a minor task, but it should be avoidable). Do you know of a way to format this properly as a date that will translate appropriately to Excel? 
Yeah, we keep all our stuff in SQL Server databases here. I am shocked at the number of MANAGERS who link Sql Server tables.. into MS ACCESS. We had to replicate our main DB into a secondary one, and force them all to link to the secondary one... because they were bringing down the main db with their horrible access queries.
Hmmm. I was fairly sure it wasn't but after double checking it actually might be. My thoughts on why I didn't think it was: id | user | category | stat | value | date 554 | M443 | Best | Healing | 5370 | 2016-11-29 12:28:44 552 | M443 | Best | Final Blows | 17 | 2016-11-29 12:28:44 . With the category column, I can easily break it off into its own table, and making the stat values, the actual table columns. But I keep going back to the definition of it, and I guess this does meet the criteria of 3NF. Either way, it does feel like it could be done less sloppy, if I was in control of what data there could be. Buut I'm not.
Looks good to me
"To the person holding the hammer, everything looks like a nail."
&gt; select to_date(to_char(sysdate,'dd-mon-yyyy'),'MM-DD-YYYY') from Dual You seem to be overcomplicating things a bit - SYSDATE returns a DATE, so you don't need to convert it. SYSDATE returns the current date and time set for the operating system on which the database resides. The datatype of the returned value is DATE, and the format returned depends on the value of the NLS_DATE_FORMAT initialization parameter.
Can i ask what you mean by getting a schema right?
As a support tech who deals with a lot of less than optimal sql databases this is why they get paid. Too many times companies poorly plan their databases and all sort of problems crop up for me to deal with down the road. Database architecture when done well is an art. 
That's a good way to get fired. The front end team owns the dbs and I just fix things when they break. Office politics is a bitch. The front end team made the collection of tables that they call a database and we database people have to deal with the fallout of the front end team knowing nothing of databases. 
Index management is huge and such a deep and wide subject. Proper query writing and index creation using the subsquent execution plans can be as much of an art than a science.
It just means to treat everything inside the apostrophe as a string. In many languages, a / might be used to identify escape characters and can be misinterpreted. For example, if you wanted to store a URL and it contained /. 
Because it's hard. Lots to learn. Truly great SQL devs are artists with thier work... 
replacing a string column with an id to a separate table where the strings are attached to the ids -- many people think this is necessary for 3NF, but it is definitely not many people also say that 'Best' appearing more than once is "duplication" and should be avoided, but are then oblivious to the fact that the surrogate id number would also be duplicated just as often 3NF is not really well understood by many developers
It's not just the category column that repeats, it's the stat column with it. The stat column is dependent on the category column, and could be made it's own table resulting in less duplication (by one column per row). Then again that would introduce possible null values instead of just not saving the data. 
&gt; could be made it's own table resulting in less duplication (by one column per row) i don't think you want to go down that road you've got Healing under Best and Healing under Assists 
Thanks :)
Whoever downvoted this hasn't had to deal with the paranoia introduced by how shitty SQL*Plus is. Connected to: Oracle Database 12c Enterprise Edition Release 12.1.0.2.0 - 64bit Production With the Automatic Storage Management option SQL&gt; select 'this doesn't work' 2 SQL&gt; from dual; SP2-0042: unknown command "from dual" - rest of line ignored. SQL&gt; SQL&gt; SQL&gt; set sqlblanklines on; SQL&gt; select 'this does work' 2 3 from dual; 'THISDOESWORK' -------------- this does work SQL&gt; not the same error, but definitely a valid place to start debugging.
select * FROM sales as s JOIN customers as c on s.custID = c.ID where c.state like "%CA%" and s.orderDate &gt; (getDate() - 90 ) and s.orderDate &lt; (getDate() -60) ORDER BY c.lastName desc, s.orderDate desc Lets just pull ALL columns (even though we only need 1 column) from ALL sales into memory, then join that to ALL columns (even though we only need 2) from ALL customers, then sort the entire thing in memory, then filter by a non-indexed wild card text comparison, then filter by a math comparison for things newer than 90 days, then filter by a math comparison for things older than 60 days, then sort by a plain text non-indexed field, and then sort by date. what could possibly be slow about this? There are as many ways to optimize this as their are possible replies. I had the fun of optimizing a lot of things like this at a previous job (with out the ability to ask for new index to be added that is).
Wait! You can make it better by working a user-defined function into that predicate.... 
why not link directly from SQL server into an Excel pivot table as a DBO object?
Totally right about the app vs db train of thought some developers have. A poor db architecture will bite you in the ass, in my opinion, *way* harder down the road than a poor app architecture.
&gt; The front end team made the collection of tables that they call a database and we database people have to deal with the fallout of the front end team knowing nothing of databases. So much this... We have a team of people that do nothing but SQL/ETL tasks all day. Our application developers do not leverage their knowledge but instead just tack on columns and tables as needed for their user story requirements to be complete. Boggles my mind that larger corporations are so bad at leveraging available resources.
IME, it's basically a superpower. Everyone needs to get data out of their databases. Someone who can do so skillfully is invaluable.
This will probably perform better as a join, if you have some kind of relationship between the two tables. SELECT fields FROM table.name A JOIN table.other B ON A.field = B.field where B.myCol = 1
&gt; Our application developers do not leverage their knowledge but instead just tack on columns and tables as needed for their user story requirements to be complete. And this is one of the pitfalls of organizing by your skill. It becomes a nightmare to coordinate resources across teams like this as both teams have different priorities. If this is a common occurrence, the application developers should be split into teams that manage specific parts of the app with one or two ETL devs on each team that will engineer the back-end should the front-end need changes.
Well I'm a bit out of practise with sql (as I'm sure you can tell). New job has me tinkering in it once again. Maybe you guys could help further (I'm site you could). Im trying to change the value of one column, for multiple, specific rows in a table (value change from a 0 to a 1). Table has several thousand rows. I need to change "columnA", let's call it, for about a hundred different rows. This is to avoid going through a lengthy multi-step process in the software itself to make some blanket changes. The initial question i asked enabled me to see only the rows i need from the table, to help verify what the column name is need to change. Now I just need the command to change that column for those rows I've filtered. All in a test replica environment of course. Once I figure out the procedure is can hand it over to someone who has access to run it on production. 
Data is king. It runs the business. Business always changes. Always. Even daily. You need to be able to interpret business needs into actionable insights. You do that with data. Accurate data. Data is king.
A few questions: 1) I have never used Hadoop. May I assume all ANSI sql functions are supported? Is there any sort of PIVOT support? 2) are you limited in any way? For example must it all be handled by 1 single query? Or can it be a series of queries inserting results into a temporary (or permanent) table? 3) how many records are we taking? Does efficiency matter? Will this query be run once and once you have the result you are done with it, so who cares if it takes an hour to complete or is this going to be run frequently?
There are 2 groups of character data types in Microsoft SQL Server these days. One is the old char/varchar/text type, and its string literals are denoted with single quotes, like 'text'. The newer, unicode-capable equivalents are called nchar/nvarchar/ntext, and can store more than 1 byte per character to support many more characters, particularly international languages that use non-Latin alphabets. When written as string literals, these are prefixed with an N, as in N'text'.
As someone new to SQL working at a small company, I didn't know this. I just recently linked Access to just one table in our Pervasive database because it's easier to export to Excel from Access. I'm the only person here who uses SQL or queries so I'm not aware of how I might be very inefficient. I thought Access would be useful to learn but, perhaps not? 
 update table_name set column = :new_value where filter_column in ( /* list or select*/ ) ; If you use in with a subselect be sure that it can never return null. If it can return null you will have to use exists. If you have an array of keys for columns you need to update you can google "oracle FORALL".
Bwaaahaha - well, that escalated! "I'll be dead soon, but it is a welcome relief!"
very simplistic. its better to teach what every stuff is doing as opposed to give blunt solutions to very specific problem. like what if I want the 12 best salary (like the last question)? use 12 max operations? why not use rank? question 60 is wrong question 70, better use joins rather than sub queries. same as 73 and so on...
This book helped us a lot. Thanks
Got it. As I understand indexing, correct me if I'm wrong, but it would not make sense to index a column of, say, order numbers which would not necessarily be sequential, but would already be in order numerically. Or do you think that I would see performance increases if I indexed even that column if that's what I'm querying from?
I posted this as a comment recently and now feel like a shill for AWS, but... You should look into [Amazon RDS](https://aws.amazon.com/rds/free/). That link is to their free tier. You can get started with your choice of SQL variants (MySQL, MariaDB, PostgreSQL, Oracle BYOL, or SQL Server) for free.
Do you actually need a `scores` table? You can figure out the actual score by multiplying the % by the maximum score: SELECT a.Client , a.Result , ( (a.Result / 100) * (20) ) as Score FROM Table a
Access is useful for certain things - particularly because it provides a fairly intuitive front-end+back-end solution for people who aren't technical experts in databases. If you were to provide the same users another solution that was a database + some tool to manipulate the databases, you'd have the same performance problems as people complain about with access, or worse, you'd have no one using the tools you provided them. The problem isn't access, it's an architect/developer not understanding the possible performance issues and anticipating a strategy to deal with it i.e. a 'sandbox', stored procedure read-only access, or whatever else. 
That works, but there are pitfalls, particularly in terms of scaling. Make sure you know what the pitfalls are, and what happens when you fall in them. 
Oh I agree that the term is hype but that's a different conversation. It still stands that companies are indeed looking to collect as much info as possible and in doing so need that data analyzed. 
Sure. But thing is that's been our (db guys) value proposition all along. And it's worth money to companies, for the same reasons it was worth money in 1970, or 1870 for that matter. 
A couple of thoughts/// If you are joining querying on the excel workbook - that is generally not the best idea. Instead consider bringing the data in from the workbook (workbooks?) into a temp table - where you can control the data precision and don't have to rely on a round. If you plan on doing this function repeatedly - you may even consider actual tables or InMemory tables - depending on your purposes.. If you control the SQL tables you no longer need to round, it will also let you place an index on the Result columns allowing the join to run faster... If you ultimately DO need to round - there is nothing wrong with performing a math function in a join, however if you are working with MS SQL it will perform better if the you reverse the query so that the function appears on the right side of the Equals.
Lots of errors. Lots of inconsistencies. My favorite: \#11: `SELECT last_name FROM employees WHERE hire_date = '17-FEB-96' ;` \#58: `SELECT last_day('1-11-2016') FROM dual;` If you run both of these in the same session (without altering NLS_DATE_FORMAT), at least one will fail. 
I second this motion. RemindMe! 4 days 
I didn't think about this, but it would probably work. In reality, my goal is to move all of the data in this file over to Access, so I think what I'll do is investigate pulling the data into an Access database instead of Excel to help facilitate that transition. Thanks!
I'm not saying you're not on the "Good" list, but.... you're not on the "Good" list.