Based on that, it sounds like coffeewithalex has the ideal approach
Divide by 100.0 so you get floating point math instead of 100 where you get integer math.
You *should* be able to get that with one query and window functions. I don't have test data to work with, so I winged it with what you provided. It may need modification. SELECT calendar_date, SUM(cIsFS + cIsGB + cIsCH+ cIsGC + cIsWM + cIsTC + cIsAR + cIsPP + cIsC3 + cIsPW) OVER (PARTITION BY calendar_date) AS 'TotalBookedPRU', SUM(cShowedUp * (cIsFS + cIsCH + cIsPW)) OVER (PARTITION BY calendar_date) AS 'Showed', SUM(cIsFS + cIsGB + cIsCH+ cIsGC + cIsWM + cIsTC + cIsAR + cIsPP + cIsC3 + cIsPW) OVER (PARTITION BY calendar_date) as 'Total Booked', sum(cIsNoShow * (cIsC3 + cIsPW)) OVER (PARTITION BY calendar_date) as 'No Shows/Rebooked' avg((cShowedUp * (cIsFS + cIsCH + cIsPW)) OVER (PARTITION BY calendar_date ORDER BY calendar_date ROWS UNBOUNDED PRECEDING) as 'runningaverage' WHERE cAppointmentType = 'Recalls' and calendar_date &gt;= @startDate and calendar_date &lt;=endDate
Thank you! I will check this out!
Awesome! Thank you, I will look into it.
That's good to know!
Checking it out now! That is an amazing project! I will get started on it! Thank you!
I see. For example, say in query B i am returning 600 as my DerivedValue for the month of 2019-01 as in the output above. When I join my query B to query A, the main query is finding any rows that is month of 2019-01 and assigning it with a value of 600. &amp;#x200B; So that it is = 600 \* (how ever many rows there are that is month 2019-01 in table x)?? 
Yep
The WHERE clause is simple but the SELECT clause seems complex to me.
Nearlyfreespeech.net. You only pay for the processing power/storage/sql process you actually use. Can be as cheap as around $1 a month. But, downside is that there's no free customer support. If you've got issues, you've gotta know how to solve them. Not as hard as it sounds though if you're only doing straightforward stuff.
If you enjoyed making reports, have you considered learning some other BI reporting/viz tools? There's plenty of opportunities for Tableau and Power BI developers. Maybe not as plentiful as DBA roles but it would be a useful set of skills to also develop along the way
If you throw the order by in to the outer query then you will get the wrong results in at least some RDBMSs. This is because the rownum filter will be applied before the order by, so instead of getting the first row by some order you’ll just get the first row it finds. Order by in a subquery is completely legitimate.
Hi there! You should definitely use the free version of ScaiPlatform on either [AWS](https://aws.amazon.com/marketplace/pp/B07NPPSPJ1?ref=_pntr_red_ps_r_sql_promo_q119), [Azure](https://azuremarketplace.microsoft.com/en-us/marketplace/apps/scaidata.scai_platform?tab=Overview&amp;ref=_pntr_red_ps_r_sql_promo_q119), or [Google Cloud Platform](https://console.cloud.google.com/marketplace/details/scaidata/scai-platform-2?utm_source=red&amp;utm_campaign=ps_r_sql_promo_q119). You can connect it to multiple SQL databases and you can explore all the tables/views there. You can also add/delete/edit records through the UI, as well as import data to your SQL database without writing a single line of code. At the same time, you can create views/reports/dashboards and even more. 
You can use ScaiPlatform for free on [AWS](https://aws.amazon.com/marketplace/pp/B07NPPSPJ1?ref=_pntr_red_ps_r_sql_promo_q119), [Azure](https://azuremarketplace.microsoft.com/en-us/marketplace/apps/scaidata.scai_platform?tab=Overview&amp;ref=_pntr_red_ps_r_sql_promo_q119), and [Google Cloud](https://console.cloud.google.com/marketplace/details/scaidata/scai-platform-2?utm_source=red&amp;utm_campaign=ps_r_sql_promo_q119).
Hi there! You should definitely use the free version of [ScaiPlatform](https://scaidata.com?utm_source=red&amp;utm_campaign=ps_r_sql_promo_q119) on either [AWS](https://aws.amazon.com/marketplace/pp/B07NPPSPJ1?ref=_pntr_red_ps_r_sql_promo_q119), [Azure](https://azuremarketplace.microsoft.com/en-us/marketplace/apps/scaidata.scai_platform?tab=Overview&amp;ref=_pntr_red_ps_r_sql_promo_q119) or [Google Cloud Platform](https://console.cloud.google.com/marketplace/details/scaidata/scai-platform-2?utm_source=red&amp;utm_campaign=ps_r_sql_promo_q119). It connects to multiple SQL databases and you can instantly visualize all the tables/views in that particular schema or database. You can also add/edit/delete/merge data or import data into the database without writing a single line of code. It also has a built-in SQL console to run code or save views. At the same, you can also create dashboards and share them with other users. 
I haven't worked enough with PostGres to give you a solid answer, but I do know this behavior is caused by SQL Server when it fails to cast one type to another (like your timestamp). Try commenting the casted columns out, see if you run into the same problem again
This is the exact scenario I am dealing with...so is Order by in a subquery just one of those things where there are different schools of thought? 
I use Digital ocean for their $5/ month vms. Googling I found another one, $2.50/ month at Vultr. Never tried it though
It depends on the RDBMS. Eg In SQL Server, there’s been the TOP clause for a while - this gets applied after an order by so is fine to use without the subquery, it’s likely the preferred way of doing it. The equivalent in Oracle came in 12c (2014) with the fetch first clause, up until then you would have had to use the order by in the subquery, it has always been totally legal and the order by will always be executed at the expected level. Your mention of rownum suggests that this is Oracle (but it could be any number of systems that I’ve not used or read the documentation for). There are so many SQL dialects that it would be silly to make a blanket statement like “it doesn’t work” or “it always works”. But definitely in Oracle, it is the recommended way of doing this.
Thanks for the explanation, it sounds like there is more variance between SQL strains than I realized. This particular practice question was in fact of the Oracle variety.
Not a PostgreSQL expert, but I did have access to a DB2 instance running on big iron that limited user queries to 30 seconds of CPU time. 30 seconds of mainframe CPU time was plenty for most queries and prevented accidental run-away queries from impacting the production environments also running on the machine. If PostgreSQL has a similar safety feature, I would be surprised if it was enabled by default. Check with your DBA.
Hi, may I know have you solved this question? I'm facing a similar data structure. Thanks!
DigitalOcean is fantastic. 
I totally understand what you mean. Thanks a lot for the advice ,but the thing is that name cannot be unique in concerns of this database. Think of it as just any real world contacts table. It makes no sense to have restriction on adding the same name. Maybe there is way to point still to id ,but represent the name column of this row? To be more detailed - I am working in MS SQL+ SSMS + Visual Studio (mdi desktop application)
If you know a language like R or Python, Kaggle is a generally accepted place to put data science and visualization projects for portfolios.
I like to keep the `SELECT` as simple as possible if I can. For example you could do ``` SELECT COUNT(a.is_error_one) error_one CAST(COUNT(a.is_error_one) AS decimal(5,2)) / COUNT(*) * 100 error_one_pct FROM client_table t CROSS APPLY ( SELECT CASE WHEN (t.error_one &gt; 0 and t.duration_two = 0) THEN t.session_id END is_error_one ) a ``` t-sql
Thank you! This is a helpful way to think about it. 
What is the difference between this tool and Lucidchart?
You'll have to use a view then, as explained earlier.
You're going to want a group by clause before the having clause, something like: GROUP BY STORE, CLIENT, ITEM CATEGORY Then you're having clause should have the right effect. 
Thanks I am definitely going to give a try tommorow ! I am Always using group by since someone told me "copy paste each item from the select section in the group by" However it indeed seems to be something I can play with. Do you have any ELI5 for this group by clause ? How it works and what it does ?
[This](http://www.zentut.com/wp-content/uploads/2012/10/sqlcheatsheet.jpg) should help. 
Never figured out the 2nd problem ( Problem 19).. asked the "prof" via 2 emails for some type of guidance and NO RESPONSE at all. Gonna keep try at it til 11pm deadline. I'm liking learning the material but definitely not worth the $1800 for the course by any means.
This is my opinion, and my experience. If you have a decent PC then you can easily do what I suggest. Learn other databases. You can create some Linux VMs on your PC and setup both Oracle and PostgreSQL in primary/standby configurations. I have no idea if you can do this with SQL Server. Oracle is downloadable for free for learning and testing. I suggest using Oracle Enterprise Linux for the operating system for Oracle. I suggest Ubuntu as operating system for PostgreSQL. Or if you don't want to learn two different versions of Linux, you can install Oracle on most Red-Hat based Linuxes. DBA tasks : learn to backup and restore. Learn to restore to a certain point in time. Delete an important datafile or directory or log and figure out what you need to do to get the system back up and running. If you can do your high availability solutions, try and setup automatic switchover/failover. Learn the data dictionary. Find out who the 'gurus' are for your database and read their blogs/articles etc. Why do I suggest different databases? Because you may not just have one database being used in a company (we cover Oracle, PostgreSQL, MongoDB and Influx in a two man team), AND (this is important) your pay tends to get bigger knowing several DBs. &amp;#x200B; Good luck. &amp;#x200B;
Thanks mate, I'll keep looking for those roles.
I used CBT Nuggets at uni a lot of networking and they were great, I'll definatley have a look at their DB content. &amp;#x200B; Thanks mate, I had a look at the Microsoft certificates but I was under the impression that certs were really only respected in networking/infrastructure positions? 
Thanks mate I do want to eventually learn Power BI and Tableau as I see that a lot in job roles but I've still got a few other tutorials I'm working through first.
Cheers mate I'll have a look at practising with other DBs
People love certs. It's all based on what your position is. Like project managers need PMP to make good money. Service desk having A+ / net+ / ITIL. Tier 3 or desk side support is good to have MCSA. And developers and admins having MCSE and other vendor certs. 
Yeah you can wrap any code in a stored procedure like so: &amp;#x200B; `CREATE PROC dbo.UpdateStagingSaleEvent` `AS` `BEGIN` `UPDATE #STAGING SET SALE_EVENT = 'SOLD AT AUCTION' WHERE SALE_EVENT LIKE '%Sold At%'` `END` &amp;#x200B; To execute you'd run this: `EXEC dbo.UpdateStagingSaleEvent` &amp;#x200B; &amp;#x200B; You can even add parameters like so: `CREATE PROC dbo.UpdateStagingSaleEvent @SoldAt NVARCHAR(20)` `AS` `BEGIN` `UPDATE #STAGING SET SALE_EVENT = 'SOLD AT AUCTION' WHERE SALE_EVENT LIKE '%' + @SoldAt + '%'` `END` &amp;#x200B; To execute you'd do this: `EXEC dbo.UpdateStagingSaleEvent SoldAt = 'Sold At'` &amp;#x200B; The EXEC isn't necessary to run the procs, let me know if you have any issues.
Fair enough, I'll have a look at some of the certs, it'd also give me a good idea of what I need to know too.
Its not within SQL standard i believe but most engines have it as a plus
&gt;The new server runs much slower than the old one, Generally newer servers perform better than the ones they replace, not slower. What's up with this? &gt; is there any different between the two in terms of which will run faster and cleaner? Yes. Wait...no! Hold on...maybe? It's impossible to answer this question from here. Try it both ways and see how each works. You might even land somewhere in the middle.
This is perfect to start with thank you!!
&gt; Generally newer servers perform better than the ones they replace, not slower. What's up with this? Don't get me started. &gt; Try it both ways and see how each works. Unfortunately I don't have time. I've got a crazy number of queries to get through with a tight deadline so even spending a few hours trying one both ways isn't on the cards. I've already raised the fact with my manager that there isn't any time available to try to make any mass improvements or combine reports where necessary - just gotta burn through them.
Really depends on what you're doing... But in general I'd say no. The real thing is to just avoid poorly performing queries in the hot path.
&gt; ROWS UNBOUNDED PRECEDING I think this isn't available in ssms 2008 :(
If you have to aggregate rows, they're never a bad practice. 
I don't think that it's very unique. I would argue that there are more examples in the internet, StackOverflow, etc for MSSQL than for other RDBMSs. I've used MySQL a bit, and haven't really used the others too much. But as a comparison, I'd say in some ways, MSSQL is easier. Stored procedures: In MSSQL, a resultset is return simply by a select statement in the body of the sproc. In IBM DB2, for example, a cursor needs to be declared. Triggers: Some people find this more confusing, but compared with Oracle, triggers operate on SET operations only rather than providing logic for individual rows that are manipulated. However, the set logic I think makes the code more concise to write. SSMS: It's a best of breed query editor, though the install has become more bloated in recent years. Execution plans are easy to digest. As a general rule, stay away from cursors in T-SQL, stay away from table variables unless you need to use them such as in function creation, and watch out for parameter sniffing (thousands of examples online).
What happens when you add Left join c on c.calendar_date = a.calendar_date and include c.running_avg in the select clause? Alternatively if your SQL server version supports it you could do a window function. You’d remove the c CTE and in the main select include: avg(a.showed) over (order by a.calendar_date asc rows between unbounded preceding and current row) as running_average Avg(
Yes the characterization is overblown. If you have adequate working experience with other syntaxes then you will pick up on T-SQL just fine, it won't take too long at all. In my opinion, you will probably end up liking it better. Also I agree with the other comment that claims there are more resources available for MS SQL Server than any others. 
As others have mentioned, if you're trying to aggregate values then it is not bad practice and in fact the only way to achieve this in SQL. If you're using aggregation to bandaid incorrect join conditions then perhaps your boss is correct. Maybe provide an example.
I've never heard anyone make that claim for all queries. It sounds ridiculous to me, but I'm curious: what's their reasoning?
Don’t believe your boss about anything anymore. If he claims to know SQL then something’s up. It’s not like you’re writing correlated subqueries and he just wants you to accomplish the same in the FROM clause or something 
I find that doing updates is slower so I avoid it. Of course your case may differ. Like if the server has too little RAM.
Like the use of DISTINCT it can be a sign of a poorly written query or poor data. It's not inherently bad. If you have to use a GROUP BY make sure you have a good reason to do it.
Group by isn’t bad lol wtf. 
@currenttime is a date type in your code
I recently made the switch from BigQuery to T-SQL and it really wasn't that bad, with a little dedication you'll have to down in 4 weeks easily, if even since it sound like you are coming in with more experience then I had.
Can you provide the tables DDL and EXPLAIN ANALYZE for your query? 
Jokes on you, my company engineers didn't even define foreign key relationships, and the same key in two tables can have different column names! Seriously. But this is a cool tool, thanks.
r/postgresql
It can be easy if you choose a good platform to learn from! After my bachelors, I tried some good resources like leetcode, datacamp, sqlzoo, datacamp and strata scratch. I found stratascratch most helpful as there are questions from technical interviews from other companies so I found it the best resource for my career to use for interview practice.
This article doesn't mention anything about SQL injections using prepared statements, in fact it states that SQL injections are *not* possible when using prepared statements. This is article simply demonstrates how a SQL injection vulnerability can be exploited. While it does a decent job of doing that, the title of this submission is completely false. Shame on you OP.
Is varchar(8) long enough. Not sure if 108 has separators?
According to the docs: &gt; The database server itself does not require the postgres database to exist, but many external utility programs assume it exists. Did you try to recreate the database using something like: `psql -U postgres -c "create database postgres"` ? 
This is utter nonsense. (If your `GROUP BY` queries appear "to be slow", properly index the involved tables based on the grouping criteria. Markus Winand explains how: https://use-the-index-luke.com/sql/sorting-grouping/indexed-group-by.)
Have you thought of using common table expressions? 
Quick update : I tried the group by clause but It did not change anything. The condition IS applying on the item category and not the total per client. So I started something else: Creating a temporary table which is the exact same as the one I described earlier but without the item category. On this one I'll apply my sales value condition and then, use this temporary table and add the item category. Problem is that I am having trouble building it. Here is my code: http://sqlfiddle.com/#!9/a6c585/90882 (Right table - ignore left table) If you want to understand: Entrepôt stands for Store Code client for the client ID Raison Sociale for his name LibellesecteurVente as the item category CA for sales Marge for margin With that code I am having "1596 rows affected" which seems to be the temporary table. Do you have any idea on that one ? Thanks a lot for your time ! 
How about trying @currenttime = TIME(GetDate()) to just make it a time format for compare. 
. My boss says yes, and always wants me to always write queries without it. there are only two possible conclusions 1. you misunderstood what your boss is saying (maybe he or she means DISTINCT is better when the query has no aggregates) 2. your boss is an idiot
This is your answer.
I would be less worried if he said that about using DISTINCT. 
Make currentime datetime datatype 
I was told in another thread recently that MS SQL executes CTEs each time they're called rather than just calling them once. 
I would compare execution plans between the queries with and without cte tables. 
If the alternative is aggregating in the final reporting tool it will always run quicker by grouping on the server!
CTRs are very, very rarely the answer to a performance problem in SQL Server. 
Yes - do NOT ever delete the postgres database. You can create new copy of it but you should be backing up all databases on the system. Step 1 - reinstall or fix PostgreSQL Step 2 - install &amp; configure pgbackrest for full &amp; differential backups (supports compression &amp; S3)
&gt; SSMS: It's a best of breed query editor, though the install has become more bloated in recent years Unless you're doing DBA tasks, Azure Data Studio is likely sufficient and _much_ less bloated.
That's a bummer, it sounds like the project was scoped incorrectly for dev/improvement time. If you do get a chance to do some perf testing in the future, definitely try it with both a CTE and temp tables. Generally, if the data you want can be reused then a temp table is the way to go, especially if you're a little memory starved. If it's a one-and-done, then CTE's can be cleaner on the in and out, but if it's a query that's run a lot it can get memory and disk intensive. Between each run, flush your cache and set the STATISTICS flags to see how each version performs. On first run with a flushed cache you want low physical reads, and with a known plan and having executed it a few times, low logical reads. I suppose how quickly it runs is important too. :) SET STATISTICS IO ON; SET STATISTICS TIME ON; [query] &amp;#x200B;
&gt; I've already raised the fact with my manager that there isn't any time available to try to make any mass improvements or combine reports where necessary - just gotta burn through them. Did you also tell him that because of the degraded performance of the new server, that these scripts will run slower and there will be increased complaints/support calls/upset customers? Remind him regularly that there are going to be problems due to an unreasonable deadline. Deadlines and scope should never be agreed upon without input from the people responsible for implementation.
Yes, you can copy paste code into it and it lets you see an image of the table and relationships. You can't drag things around or move things like in LucidChart. It uses GraphViz (https://www.graphviz.org) to do the display.
I feel your pain. :(. Hopefully you have an ERD instead of having to talk to a dozen people scattered across the globe to figure out stuff. :)
Yeah then it would lead to no results afterwards and the password wouldn't suffice. Idk why. Thanks.
https://sqlvoice.files.wordpress.com/2014/02/95079-sivakumar_vellingiri_normal_forms_poster.jpeg 
Interesting, my goal right now is to stick with a relational table, but, if that gets to complicated I may look this way.
I vote number-2.
I suggest constraining this language to SQL, only, for the moment. &amp;#x200B; The "boss" may be skilled in other areas. &amp;#x200B; Until he proves otherwise in another specific area, don't let the language "don't believe your boss about anything anymore" cloud your judgement about things not-SQL. &amp;#x200B; Remember that he said something incredibly-stupid about SQL, and juxtapose that whenever you hear new information from him, but don't write-him-off wholesale, so-to-speak. Not yet.
What are you referring to when you say "I've manage to merge the two queries"?
Window function can make a running average an easy task. , AVG(col_name) OVER (PARTITION BY grouping_cols ORDER BY sort_cols ROWS UNBOUNDED PRECEDING) Otherwise, you can use a correlated subquery that will calculate AVG for all rows that are logically ordered before the current row. But subquery solution is usually worse performance-wise. As for your PM question - you can't ORDER BY a table expression (inner subquery) - not even with a trick like TOP 100 PERCENT. It's because in set theory there is no order. You have to order by in the outer query.
&gt; Did you also tell him that because of the degraded performance of the new server, that these scripts will run slower and there will be increased complaints/support calls/upset customers? Oh yeah our whole reporting team has been beating the "this server is significantly slower than the old server" drum since late last year when they made the switch. We've gone from about 40 tables to over 150 for the same data, our main case table that's the basis for all queries not requires 14 joins to recreate. Even the current status of a case - the most basic thing you need for any report - requires joining through three tables to get to. Reports that ran in 1-2 seconds now take over a minute, reports that took 30 seconds now take 10 minutes. The DW team insist it's fine and we're just writing shitty queries, but one of the guys moved over from DW to reporting and realized that the issue isn't queries at all, it's that the server's performance is TERRIBLE, but the DW team still won't listen. &gt; Remind him regularly that there are going to be problems due to an unreasonable deadline. I do but he'd checked out before I even joined the team.
Search Armstrong’s axioms and also normalization on YouTube. That should help it make sense. I’d also talk to your professor, as that’s a confusing concept. Not gonna try and answer since I’m also in class for that right now and don’t wanna be wrong. Just telling you what I did to help learn. 
&gt; That's a bummer, it sounds like the project was scoped incorrectly for dev/improvement time. Long story short they didn't include anyone from reporting in meetings and I was asked when I'd be done and ready for testing a day or so before testing was due to take place. There's approx 3 weeks worth of work just moving the queries. Unfortunately it's for a huge project for our biggest client and my manager is checked out so there isn't much room for pushback, just "get it done as fast as you can". I always say you can have quick or you can have high performing, but you can't have both - they're going for quick. &gt; If you do get a chance to do some perf testing in the future, definitely try it with both a CTE and temp tables. I'm going to take one of the smaller queries and try to find time today to try both. Was hoping there might be some good advice from the sub but it appears it's a "it depends" situation. Our DW team say to always use CTEs - never temp tables - and some of these reports are large an run hourly, so could be temp tables would be the way to go. In summary: I will try the above, thank you.
Ay! You're the person who [set me on this journey](https://www.reddit.com/r/SQL/comments/b9llx2/professional_sql_developers_or_data_analysts_how/ek5w1c8/). Unfortunately I had a chat with our main DW person this morning and he insisted I should always be using temp tables, never CTEs - and turning down his advice is essentially making an enemy, so I'm stuck between a rock and a hard place right now.
Unfortunately we're being told to always use CTEs, never use temp tables. I'm doing temp tables anyway because I'm finding them much faster but I'm aware I'm making enemy's in the process.
While not a direct answer for the question, it may be helpful to sketch out an EER diagram with the various entities and relationships as described in your provided schema, and checking what, if anything violates 3nf. I may be wrong about 3nf, but I guess an example would be an attribute for entity A that appears in other entity or relationship tables, but not as a foreign key (e.g., if course title appeared in the module table)
&gt; Unfortunately we're being told to always use CTEs, never use temp tables. And that's good advice...in other RDBMSs. &gt;I'm doing temp tables anyway because I'm finding them much faster but I'm aware I'm making enemy's in the process. The proof is in the pudding. If you're getting the same data output with better performance by using temp tables, you're doing the right thing. If people are so entrenched in their "CTE good, temp table bad" thinking that they won't consider evidence to the contrary, perhaps that's a sign of the quality of the environment you're working in.
Gotta love dysfunctional offices.
Bingo 
Okay I'm not entirely sure what you're doing in the link you've provided, I'm used to working in T-SQL so it's a little different and the language is throwing me further. That being said, I re-read your original post and would suggest you try something along these lines: SELECT STORE ,CLIENT ,ITEM CATEGORY ,SUM(SALES/TURNOVER) FROM 'TABLES GO HERE' WHERE CLIENT IN ( SELECT CLIENT FROM 'SAME TABLES AND JOINS AS ABOVE' GROUP BY CLIENT HAVING SUM(SALES/TURNOVER) &gt; 5000 ) GROUP BY STORE ,CLIENT ,ITEM CATEGORY Basically adding an 'IN' statement to the WHERE clause to allow your filtering of clients with 5k sales total, whilst not actually displaying that field. I believe this is what you were after...
Okay I'm not entirely sure what you're doing in the link you've provided, I'm used to working in T-SQL so it's a little different and the language is throwing me further. That being said, I re-read your original post and would suggest you try something along these lines: SELECT STORE ,CLIENT ,ITEM CATEGORY ,SUM(SALES/TURNOVER) FROM 'TABLES GO HERE' WHERE CLIENT IN ( SELECT CLIENT FROM 'SAME TABLES AND JOINS AS ABOVE' GROUP BY CLIENT HAVING SUM(SALES/TURNOVER) &gt; 5000 ) GROUP BY STORE ,CLIENT ,ITEM CATEGORY Basically adding an 'IN' statement to the WHERE clause to allow your filtering of clients with 5k sales total, whilst not actually displaying that field. I believe this is what you were after...
I haven't tried but I don't think so. Anyway, in most cases, there should be no reason to have both the versions installed in same ORACLE_HOME. If your purpose is to just share some configurations, you might want to look into TNS_ADMIN? That should help you use the same sqlnet.ora and tnsnames files with both 32 and 64 bit clients.
look to move from helpdesk to application support . lots of applications are sql based.
Since you're already using FILTER statement, use it consistently. Instead of SUM(CASE....) you can do COUNT(*) FILTER (...). plus what's the point in the separate query? Why declare the same client_table structure twice? First create the table, then the function that returns SETOF client_table. I would also advise to not use \copy. Instead just pipe data in and out. That way the control of the files sits in bash and not in psql, and it's clear which user executes what.
Self-joins work pretty much the same way as the regular joins: you have 2 datasets and you're working with pairs of records (1 from the left side 1 from the right) and your join conditions limit what combinations you want to see in the result; it just so happens that both left and right side is the same dataset for the self-join. anywho, a very typical mistake with the self join is to forget to exclude joining to the exact same record (i.e. you're self joining (A,B) it's relatively rare that you would need (AA) and (BB) in the result set) and to exclude recombinations ( do you need both AB and BA in your result set). &amp;nbsp; having said that, if you're pulling ua1.ContractID out of UserAccess ua1 where ua1.UserID is whatever you need an you think that these arent supposed to apply to your user, you might have a different data issue - not related to the self-join, specifically.
Thanks! Good point re: FILTER. I don’t know yet whether my client uses postgresql or another implementation, so I was opting for portability where I could. Though I guess if they don’t, they might up a creek, since FILTER isn’t yet well supported. The query is just in there twice because I don’t know the client’s sophistication level. Ideally I’ll revise down to just the function or query depending on their desired workflow. Re: client_table structure twice, the actual file pulls in many more rows to the client_table than end up in the client_temp table. I just removed them when anonymizing since they weren’t being used for this workflow. Does the SETOF clause still work for this? For data piping, are you just saying use | and &gt;&gt;? Could you provide an example command?
Right now, your query is joining on the same value, so you are just creating a relation that's double everything. Are you looking for Users that have Common ContractID's?
&gt;the actual file pulls in many more rows to the client_table than end up in the client_temp table I don't really understand this part, but the idea is to not declare the same columns twice. Since you insert into the client_table the full result of the function anyway, it means that they both have the same structure. In that case, I would create the empty table, have the function `RETURNS SETOF client_table`. Makes the code simpler, maintenance easier. &gt;For data piping, are you just saying use | and &gt;&gt;? Could you provide an example command? I use this quite often: ``` cat some_file.csv | psql my_db --quiet -f myscript.sql &gt; out_file.csv ``` But this would only work if your script had just 1 input and 1 output. That's why I would just use 1 SQL file per input or output, and orchestrate the rest in bash or Python or something. That's what I would do. You can do whatever you judge to be best :)
Yes, that is what i'm attempting
Ask you boss to explain why this is “bad practice”. It’s a reasonable request and you or him may learn something new.
business intelligence, involves a lot of sql for making views and reports.
`SELECT ContractID, UserID, Count(UserID)` `FROM UserAccess` `WHERE UserId IN (1233, 4637, 4311,1167,1839)` `Group By ContractID, UserID` `HAVING Count(UserID) &gt; 1;`
SELECT ContractID, UserID, Count(UserID) FROM UserAccess WHERE UserId IN (1233, 4637, 4311,1167,1839) Group By ContractID, UserID HAVING Count(UserID) &gt; 1;
I'd say your problem is that you're trying to use HAVING without a GROUP BY Btw OP - how to ask for better help: - Include details on the structure of your tables. We have no idea what the tables you're querying look like - Share details of the error messages Also, get better at reading (and googling) error messages yourself instead of asking for help
It would help if Post your SQL as it stands now
Please provide some details on the structure of the tables you're querying, because I don't see why a student would have to be counted twice
 use coll18_live SELECT STC_PERSON_ID, STC_FINAL_GRADE, STC_TERM, STC_COURSE_NAME from dbo.STUDENT_ACAD_CRED WHERE STC_TERM in ('14/FA', '15/SP', '15/SU', '15/FA', '16/SP') and STC_COURSE_NAME in ('ACCT-125', 'BUSI-103', ('BUSI-105 OR ENGL-120), '(COMS-165 OR CISA-165)', '(COMS-110 OR CISA-110)'
 use coll18_live SELECT STC_PERSON_ID, STC_FINAL_GRADE, STC_TERM, STC_COURSE_NAME from dbo.STUDENT_ACAD_CRED WHERE STC_TERM in ('14/FA', '15/SP', '15/SU', '15/FA', '16/SP') and STC_COURSE_NAME in ('ACCT-125', 'BUSI-103', ('BUSI-105 OR ENGL-120), '(COMS-165 OR CISA-165)', '(COMS-110 OR CISA-110)'
Thanks for the reply. I uploaded it above
Thanks for the bash example! That’s a handy workflow. For the table structure, I meant columns not rows. The actual production script had ~26 columns instead of the 10 I listed, while the output file only has 8 columns (some of which are calculated). Or are you saying that I don’t need to both define columns in the function and in the temp table for export? I do quite a bit of exploratory data analysis in Pandas. I’ll have to think about using it in workflows like this too.
I'm looking into this the structure. The student might get counted twice if he/she successfully completed the cross-listed and its equivalent sometime during his/her college career.. My goal is to see if someone completed all of the requirements for a degree, or is near completion (1, 2, or three courses away).
If you're after row counts, then yes, sys.partitions should be fine. If you want to monitor space usage, look at sys.dm_db_partition_stats: https://docs.microsoft.com/en-us/sql/relational-databases/system-dynamic-management-views/sys-dm-db-partition-stats-transact-sql?view=sql-server-2017
Not a professional, so might be better options. For my personal DBs, I'd make a copy as a new table: SELECT * INTO users_backup FROM users; Then, when you're ready to compare, find a method that works. [This website](https://www.mssqltips.com/sqlservertip/2779/ways-to-compare-and-find-differences-for-sql-server-tables-and-data/) seems to give some good ideas.
Novice here, but you might try looking into database triggers.
Are you looking to start tracking from this point forward, or historically? There are several ways to do it, depending upon the RDBMS you're using and even which version of a given RDBMS. In order of most likely available to you to least likely (as well as easiest to deal with vs. hardest). I'm coming at this from SQL Server. * Implement a trigger on each table which writes a record to a "history" table which tracks what data was changed and when (any RDBMS that supports triggers, which is pretty much any of them) * Implement Temporal Tables (SQL Server 2016+, Oracle, probably others) * Implement Change Data Capture (All currently supported versions of SQL Server, but not all editions, Oracle, maybe others) As for tracking what's been deleted, that's a bit tougher. CDC and Temporal Tables will help you out there with the least trouble. You may want to consider soft-deleting records, using an `instead of` trigger to mark records as deleted (with a timestamp) instead of _actually_ deleting. You can track the deletes with a trigger a well if you want to though.
do you know/realize that in this particular instance count(userID) returns the same value as count(*) (count(1), etc.)? Is that what you intended?
Yeah sorry I tried somehow to communicate the query in a better way, but could not find the right way to do it on this link :/ Anyway, thanks a lok for your answer, it definitely looks a new way to do it for me. I am going to try it tommorow first hour. Really, thanks a lot for your time dedicated in this thread.
No worries mate
i was going to suggest allocation_units but this is better.
Usually I recommend people look at their current position or current company and see where they can help out. Almost everyone has a sql database at their company that is already unmonitored, not backed up, that isn't being utilized. Log in and see if there are backups. Check index fragmentation. Learn a bit of the database administration stuff before you do anything. You'll have data right at your fingertips that you can read and see how it impacts your company. Ask your users what their challenges are and what reporting they are lacking out of the system. Downtime is a great time to do some professional development. Anything you do even in an IT Helpdesk position can help propel you into the job you want, at your company or not.
Yeah. It isn't what i wanted.
This is my ER Diagram that I have at the moment! [https://imgur.com/k2EgQgV](https://imgur.com/k2EgQgV) &amp;#x200B; I have Department figured out, I'm just iffy on checking the validity of the other tables :\\
No you cannot install both 32 bit and 64 bit clients in the same ORACLE\_HOME, and only one release per server. Oracle recommends each client to have its unique ORACLE\_BASE too. Yes order of installation is important. If you are Windows using Microsoft products and .net then you want the 32 bit ODBC and install first. If you are using 64 bit products - you may need 64 bit odbc and 32 bit too. It all depends on what you are going to use the client for and with. You may find that you need to uninstall both and reinstall in the other order to get the desired results.
&gt; Temporal Tables (SQL Server 2016+) These are definitely going to be your best bet, if you viably can implement them. The ability to use something like the following has made debugging process flows a dream for me; especially with time/code sensitive components. DECLARE @StartDateTime DATETIME = '2019-02-29 07:50'; DECLARE @EndDateTime DATETIME = '2019-02-29 07:54'; ....--query, etc etc FOR SYSTEM_TIME FROM @StartDateTime TO @EndDateTime
under **Tools** menu, select **Export Database AS SQL** then in the target database, under **File** menu, select **Load SQL file** and run it
In addition to the previous comment my bet is that sectionid is referenced in instructor rather than section having a record for every instructor.
[Change Data Capture](https://docs.microsoft.com/en-us/sql/relational-databases/track-changes/about-change-data-capture-sql-server?view=sql-server-2017) for MS SQL Server isn't too bad. Obviously, there is a space overhead in storing the extra data.
Check out https://sqldbm.com/Home/ I've never used for what you r describing but I think it might be what your looking for
Thanks will do
You could use MariaDB's [system-versioned tables](https://mariadb.com/kb/en/library/system-versioned-tables) for that.
You have an unnecessary sub query. Rewrite it and ditch that subquery. It's running that subquery for every row in people to see if it should get returned.
Dont inner join on your subquery unless necessary - inner join on the table itself and specify conditions in clauses. Check your indexes / keys on the columns you are joining on.
Put select max(filename), peopleid from pictures group by peopleid Into a temp table, join people to that new temp table.
I think I got a permission denied error?
It sounds like what you might have is a Slowly Changing Dimension. If so, the wiki article is a good starting place on strategies for dealing with them, but it's just a starting place, you can find entire books dedicated to the subject. https://en.wikipedia.org/wiki/Slowly_changing_dimension
Where day = the day And id = (select min(id) from table a Where a.day = the day)
Looks good so far. Is the provided schema supposed to already be in 2nf form? Are you supposed to be making only attribute changes, or can you also create/change tables? I don't remember what participation/cardinality indicators on the diagram represent sadly :/. Are students only enrolled in a single course? And is there a 1-to-1 relationship between departments and managers? A bit of a guess here, but it may be worth considering splitting the department table into two tables - 'department' (with only direct department attributes) and a 'manages' relationship table, storing the relationship between a department and the manager managing it. But I don't know if that is specifically about 3nf.
Do you mind if I DM you the prompt?
Sure.
I don't believe it's already supposed to be in 2NF, but I see what you mean, maybe I think that may have been what was confusing me
SELECT * FROM people m join pictures AS b ON b.ID = b.peopleID WHERE b.filename = (SELECT max(filename) FROM pictures)
Have you studied the CASE statement yet? SELECT .... CASE WHEN STC_COURSE_NAME in ('BUSI-105', 'ENGL-120') THEN 'Something' WHEN STC_COURSE_NAME in ('COMS-165', 'CISA-165') THEN 'SomethingElse' WHEN STC_COURSE_NAME in ('COMS-110', 'CISA-110') THEN 'Blah' ELSE STC_COURSE_NAME END as RequirementsOrSomething
He's tried to get this answer before and didn't like the response https://old.reddit.com/r/SQL/comments/ba7zwd/merge_and_combine_3_cte_columns/
 DECLARE @id INT = SELECT CASE WHEN id = 123 then 123 ELSE (SELECT min(id) FROM table1 WHERE day = '20190101' END FROM table1 SELECT convert(varchar(10), t1.day, 20) + 'x' + t1.name, id, height FROM table1 t1 WHERE id = @id I don't really get what your requirements are for 123, so I just took a guess.
 ORDER BY filename DESC LIMIT 1 Never do this in a subquery... it's just horrible, use Group_concat and substring_index to get Item A sorted by Item B if you must, but never an Order/Limit 1.
 SELECT ua.ContractId, ua.UserID FROM UserAcces ua WHERE ContractID in ( SELECT ua2.ContractID FROM UserAcces ua2 WHERE ua2.UserID in (1233, 4637, 4311,1167, 1839) ) ORDER BY ua.ContractID Is this close to what you're looking for? Get all contracts for a set of users- then pull all users on the contracts?
This will give all counts of 1: ContractA, User1, 1 ContractA, User2, 1 ContractA, User4, 1 etc
You haven't correlated the the subquery - it will only return the very last (alphabetically) filename.
Wow, a lot of really suboptimal answers itt. SELECT * FROM ( SELECT p.*, m.*, row_number() over (partition by b.id order by filename desc) rono FROM people AS m INNER JOIN pictures AS B on B.peopleID = m.id ) x WHERE x.rono = 1
It'll work, but it's hitting the table multiple times unneccesarily and assumes unique filenames.
Get a new boss.
None, make your own field with auto increment on it.
You have a sub-query in the join operand. Without indexes this is undoubtedly going to give you terribly quadratic growth on the size of the results before they are finally trimmed down into what you want. Look at the indexes and take that query out of the join. All you need to do is join the two table directly.
I would personally rate PSQL as more idiosyncratic than T-SQL, but I guess one's view is coloured by which one you learned on.
Thanks!!
ROW_NUMBER() was also my suggestion, from experience this would yield the best performance.
This will work.
What’s cost? Seems to be a variable field in with all the static fields. I wouldn’t put that in the same table.
https://github.com/ripienaar/free-for-dev/blob/master/README.md#dbaas Some PaaS might also let you create a free database (Heroku will give you a small Postgres instance): https://github.com/ripienaar/free-for-dev/blob/master/README.md#paas
Scaleway.com offers potato VPS for 2$ per month. It will run a database engine and will even be good enough for a web application that serves hundreds of customers per hour. You can install Postgresql there and learn to maintain it from the bottom up. Normally people from the US take servers also in the US because of the lag issues when serving US customers. But for learning a DB, a bit of lag won't mean anything.
Your teacher is probly expecting you to answer the product ID for this question.
Hi there! You could use ScaiPlatform for such purposes and it is free in the cloud, like [AWS](https://aws.amazon.com/marketplace/pp/B07NPPSPJ1?ref=_pntr_red_ps_r_sql_promo_q119), [Google Cloud](https://console.cloud.google.com/marketplace/details/scaidata/scai-platform-2?utm_source=red&amp;utm_campaign=ps_r_sql_promo_q119) or [Microsoft Azure](https://azuremarketplace.microsoft.com/en-us/marketplace/apps/scaidata.scai_platform?tab=Overview&amp;ref=_pntr_red_ps_r_sql_promo_q119). It connects directly to your sql database. You can add filters for the time period and you can compare the differences through the UI.
In Oracle you can setup auditing. I advise moving the audit table to another tablespace before starting the audit. Perf overhead will depend on database activity.
Good thinking. If you want to store the price history it would need to be in a separate table with a `current` flag.
Seems like type should be the same as class. Or am I reading it wrong? In any case, yes a SK is the ticket.
Completely untested but does this work? ALTER FUNCTION [dbo].[fn_rptAppointmentsBreakdown2](@startDate datetime, @endDate datetime) RETURNS TABLE AS RETURN ( with a as ( SELECT calendar_date, SUM(cIsFS + cIsGB + cIsCH+ cIsGC + cIsWM + cIsTC + cIsAR + cIsPP + cIsC3 + cIsPW) AS 'TotalBookedPRU', SUM(cShowedUp * cIsFS) + SUM(cShowedUp * cIsCH) + SUM(cShowedUp * cIsPW) AS Showed, Null AS 'Total Booked', SUM(cIsNoShow * cIsC3) + SUM(cIsNoShow * cIsPW) As 'No Shows/ Rebooked', AVG(SUM(cShowedUp * cIsPW)) OVER(ORDER BY calendar_date) RollingAverage FROM reports.dbo.vw_Appointments a WHERE cAppointmentType = 'Recalls' AND CONVERT(datetime, CONVERT(varchar(8),calendar_date,112)) BETWEEN @startDate AND @endDate GROUP BY calendar_date ), b as ( SELECT calendar_date, null AS 'Total Booked pru', null AS Showed, SUM(cIsFS + cIsGB + cIsCH+ cIsGC + cIsWM + cIsTC + cIsAR + cIsPP + cIsC3 + cIsPW) AS 'TotalBooked', null As 'No Shows/ Rebooked' FROM reports.dbo.vw_Appointments a WHERE CONVERT(datetime, CONVERT(varchar(8),calendar_date,112)) BETWEEN @startDate AND @endDate GROUP BY calendar_date ) SELECT a.calendar_date, a.TotalBookedPRU, a.Showed, b.TotalBooked, a.RollingAverage FROM a Inner join b ON a.calendar_date = b.calendar_date )
Why no With Rollback_Immediate? Just curious.
If I had to choose from your list, I think the only thing that would be a meaningful unique identifier is a composite key of (vendor, vendor product id).
HTTPS://livesql.oracle.com Free oracle 18c Database that lives in your browser. No downloads at all.
After the update statement, add a select statement to show the user the current values in the table and then display that in a tablix on the report. That way, they get a sort of confirmation that their update was successful.
Most ERP and sales systems I've used and supported have the concept of a "Price List" entity. Since you can technically have multiple prices simultaneously (i.e. regular and sale price) then using a single row is easily done. Something to keep in mind.
SQL Server developer is free for non "production" usage. So if you're not selling anything out of it you should be fine. Or SQL Server Express is completely free (but has limitations on hardware and space usage).
Even that, by itself is insufficient. Vendor X has "product ID 1234". Vendor Y also has "product ID 1234". If you're looking for a natural key to the table, the combination of Vendor and Vendor Product ID is your best bet. However, as /u/SyntaxErrorLine0 commented, the best answer is none - make a new field with auto increment as the key.
why not SSIS?
I've done this before when i couldnt be arsed waiting to get the devs to do some work. I just created an SP and the report called that. &amp;#x200B; You just need to create the SPm add the 2 fields as params and deal with the table update in the SP. In the SSRS report you just bind your report parameters to the SP params as you would for any report in SSRS and off you go. Just remember the account which runs your SSRS reports will need execute permissions on your SP (a lot of places the reports will run under an account with read permissions only)
MySQL example: create table if not exists product ( vendor longtext, vproduct_id longtext, vphone longtext, productprice longtext, id longtext as ((concat(vendor,vproduct_id))) ) ; The assumption here is that vendor and vendor product id are unique therefore the combination will result in a unique key
your other attempt is the correct one, but you need to take care of your combination of ors and ands. a or b and c is true if b is true, regardless of c. You need (a or b) and c
This is correct. I'd say vendor + vendor product key would be the natural choice. Without an understanding of the actual data, I cannot say for certain. But I would unique constraint those fields and create the incrementing identifier that /u/SyntaxErrorLine0 recommends.
"Never trust data that you didn't put there, and even then, only trust it a little." The only way I would trust 'vendor' + 'productid' is if 'vendor' was changed to a vendor lookup table and we used a autoincrementing primary in there and then did 'vendorid' + 'productid' as the key in 'product'. There exists multiples of the same vendor "names" in the real world, so it's not trustworthy data to assume. I would argue the teacher on the validity of the question. (I've done this, depending on your teacher you either get docked or get bonus points... some are stupid 'by the book' people who don't understand real life and some are cool and really know what they are teaching.)
Using SSRS to do this is a bit.... weird. Can't it be done though a winforms app? or a website?
Completely agree. I live by this quote. &amp;#x200B; I agree with the separate vendor lookup table with its own incrementing identifier. Then one for product which has its own incrementing id and references the incrementing id from the vendor table and has the vendor product id. Thats the way I would be doing this. Then anything that references the product table (say an order) would use the [product.id](https://product.id) that was created. &amp;#x200B; I'm curious what the end goal is, to be normalized? or just to show you can put some thought into a unique key given the basic requirements.
AWS has free-tier versions of EC2 (cloud computer) and RDS (cloud database), and there are lots of tutorials for setting it up. For small stuff it should cover your needs, and you can log in from anywhere.
piece of cake single code block, using AND some_column = COALESCE(v_variable,some_column)
Thanks! So it looks like that would be equivalent to IF v_variable IS NOT NULL THEN some_column = v_variable ELSE some_column Would that also capture the possible nulls in some_column, in which case I could also do AND column IS NOT NULL?
Hmm I'll look into CASE statements. I haven't looked into it yet. I'm relatively new at SQL, so this will be a new command to study up on
Commenting because Im curious what the responses might be
What SQL are you using? Do you know how to check referential integrity? Are you familiar with primary and foreign keys? lots of material online, e.g. [https://www.sqlshack.com/commonly-used-sql-server-constraints-foreign-key-check-default/](https://www.sqlshack.com/commonly-used-sql-server-constraints-foreign-key-check-default/)
Yes. I am. Can you help with the original question?
not on mysql, but i bet you can concoct something based on kill connection_id()
I get where the OP is coming from, SSRS is already a website his users know and ahead has all of the heavy lifting for db connections baked in. I've done a few "reports" that could also perform updates. It works well enough.
Great. &amp;#x200B; So, first of all, have you verified that column1 is the primary key for table1 and the foreign key for table2? &amp;#x200B; Second question: **why do you expect the two queries to return the same number of records?** There could well be multiple records of table b associated to table a. You have not told us anything about the structure of the database, so we have no reason to assume otherwise. &amp;#x200B; For example, if a is the customer table and b is the orders table, when you join them, you can easily end up with more records than in a because each customer may have placed more than one order. &amp;#x200B; Without some more colour on the structure of the data it is really hard to help you.
Using RS in this way could present issues with contention. If two users attempted to update the same record at the same time.
Try doing a left join and add a clause to the WHERE, something like: where a.DateTime between '2019-04-05' and '2019-04-06' and a.column2 like '%xyz%' and b.column1 IS NULL That will bring the data from TableA that has no relation in TableB
Table2 could have 172 pairs of records (344 records in total) with the same value of column1 (pair-wise). Or there could be a 173 records in Table2 with the same value of column1. A better question is - _why_ do you expect the counts to match and did you verify that your 'why' matches the reality?
Thanks for the reply, but for some reason I get an error saying &gt;SELECT is not valid at this position for this server version, expecting : '(', WIDTH which I've never seen before. Some googling tells me it might be a bug in MySQL Workbench, but I have the latest version.
I think you tested it wrong, because `coalesce(NULL, NULL)` outputs NULL and so does `NULL = NULL`, with will exclude rows that have such condition. Here is simple example illustrating this: [https://rextester.com/LFXR10858](https://rextester.com/LFXR10858) I suggest looking at [CASE expressions](https://docs.oracle.com/cd/B19306_01/server.102/b14200/expressions004.htm).
Personally im learning python and finding it pretty easy to use when it comes to connecting the the databases. mysql.connector is very useful.
Your instinct is right with PHP. Other languages can do it too, but using PHP gives you the benefit of also learning the most commonly used scripting language on the web. Moreover, PHP is a web native language and so you won't need to use a framework to do web work like you would with Python or Ruby. With PHP there are a couple approaches you can take. The first is to use mysqli(), and the second is PDO. mysqli is easier but outdated. PDO is going to be more complicated but it's the right way to handle DBs in PHP. It's object oriented, which is what makes it difficult for new programmers who are only used to procedural programming. That being said, there's nothing wrong with learning on mysqli and then transitioning later. Lynda has a great PHP/SQL course where the overall project is to built a simple content management system with CRUD functionality. I think it's a great intro to the topic.
Thanks so much for replying with a through explanation really appreciate it Do you happen to know the name of the course on Lynda pls?
Is this running inside a cursor?
No worries. I'm pretty sure this is it: https://www.lynda.com/MySQL-tutorials/PHP-MySQL-Essential-Training-2-Build-CMS/587675-2.html However, I did not notice that is part 2 of a 2 part course. So you may want to do part 1 as well, as part 2 might rely on it:https://www.lynda.com/PHP-tutorials/PHP-MySQL-Essential-Training-1-Basics/587674-2.html.
That's great I know a bit of html and CSS So these courses you refer too will sit nicely. I'm sure there should be a link of the first course from the second, shouldn't be hard to find, me hopes. I remember a few years ago about mysqli with PhP I thought with all these new languages n frameworks now (which i don't want to use yet) there would have been something newer. Im happy to check out both PhP ways. I have a bit of OO experience too, enough to get by. Thanks a million
could add another coalescing value for null situations. coalesce(NULL,NULL, 'MyValuesAreBothNull')
Yes, that is true, but in this specific case it wouldn't be enough, because `(NULL = 'MyValuesAreBothNull')` IS NULL. Of course you don't really need CASE expression either, you can simply do `(some_column IS NULL OR v_variable IS NOT NULL AND some_column = v_variable).`
Excluding those rows is the desired outcome, so it seems to be working as needed. Yeah, I read up on case for a different statement I was editing, definitely handy. Thanks for the link
No, just in a proc as part of a package
Sorry, I might be misunderstanding the discussion, but here is what I typically do: some_column = coalesce(v_variable,some_column,'OhShitMyColumnAndVariableAreBothNull') I am assuming this is assignment logic, not comparison logic, as thats what it appears the parent poster is doing in the post parent to your comment which I replied.
&gt; some_column = coalesce(v_variable,some_column,'OhShitMyColumnAndVariableAreBothNull') this is essentially what i wrote without the panic button
Select count (1) From orders x Inner join (select z.order_id order_items z group by order_id having count(1) = 1) z on z.order_id = x.order_id
for 1. they practically write the SQL for you in the hint SELECT COUNT(*) FROM ( SELECT order_no FROM order_items GROUP BY order_no HAVING COUNT(*) = 1 ) AS these
So, they non-billable and the non-field are coming from two different tables?
Sorry, I should have clarified. It's all the same table. The field being summed [DSH] has those 5 different categories attached in the same table. I just want to combine a couple of those [TYPE] fields, because they don't need to be reported separately.
Group by Case When Type in (' NonBillable w 1 ', ' NonBillable w 2') then 'NonBillable' when type in (' NonCompliant w 1', ' NonCompliant w 2 ' then ' NonCompliant' when Type = ' Billable' then 'Billable' end
&gt;Yes. I am. Can you help with the original question? -- This really has nothing to do with table2, table2 is actually just a temp table with some records in it. PS Look, maybe you didn't mean it, but you might come across as very arrogant. &amp;#x200B; I asked you a very relevant question. You failed to understand its relevance, brushed it off, and asked me to help with the "original" question, as if this wasn't related. You also said it has nothing to do with table2 (I bet it does!) but failed to explain why. &amp;#x200B; That's not exactly the best way to seek the help, for free, of people who have no obligation to help you and dedicate their free time to do so! &amp;#x200B; My question was relevant because 1) how can I know your level of experience? How can I know what checks you had or hadn't done on the data? 2) the next step was to ask why you expected the two queries to return the same number of records
Bloody hell, that makes life easy.
The lack of effort you've put in here is impressive.
Glad to help.
Are the tables dropped and recreated in the database on each import?
Possibly. I don't control the other end of this process, it's a black box effectively. But I need to consume the data into my process. It needs to scale as fields are added automatically, as they will change on a weekly basis potentially.
select string_agg(c.name, ',') from sys.columns c where c.object_id = object_id(tablename) then combine that into a CSV export command.
I would assume that he's on SQL Server prior to 2017 and therefore doesn't have access to string_agg(). That's the only reason to consider the Stuff/FOR XML method he mentions.
My suggestion is to keep it simple: "DQ is simply the right data for the right purpose at the right time." If you need to dive into the details, then you can start talking about all the dimensions of DQ: structural integrity like cardinality, fit for purpose, conent accuracy, uniqueness, timeliness, etc. I would also relate what this means back to the EDW: give examples of what happens when DQ is poor vs when dq is high.
Just google "data quality" and use that. Give the people at work one or two examples of how that applies to their work.
You could use dynamic sql by declaring a variable to hold your select statement. You can get the column names and ordinal position of the table from information_schema.columns. That may be some funky logic though to achieve your result. Can you use a scripting language like python? You could merely do "select * from table" and iterate through the columns and rows that way to build your delimited string per row. If you need the column names, I'm sure there's a way to get that back in python too or you can use that information_schema.columns system catalog I mentioned earlier. I'm not a python expert. I've been recently playing with it and it is a rather simple language to get up and running with.
I use SQL SERVER as well and agree with Rocco’s Select * from sys.columns etc method with dynamic sql (If you need I can probably send a code snippet tomorrow). Alternatively though, you can: 1.Embed the connection to the sql database in an excel file and write a script to refresh and save in whatever format you need. 2. Similar to 1, if you’re able to get access to Python on your work computer, you can pull the table in as a dataframe, and push it out as a csv or whatever.
There are two similar and related measures and definitions of data quality. Google “DAMA six elements data quality” and it will get you started. Good luck, and please let me know you’re journey along the way. Thanks!
I like the "alternate" definition for high-quality data in the wikipedia article &gt;...data is deemed of high quality if it correctly represents the real-world construct to which it refers. [https://en.wikipedia.org/wiki/Data\_quality](https://en.wikipedia.org/wiki/Data_quality) So, I would say something like: &gt;Data quality is the degree of accuracy and precision in which it represents the real-world. For sub-points, I would put something like: * Low-quality data leads to multiple versions of the truth, conflicting metrics and increased time in which IT spends cleansing data for each individual request from the business. * High-quality data leads to single-source of truth, reliable enterprise metrics, self-service data discovery and quick to market answers to business problems.
**Data quality** Data quality refers to the condition of a set of values of qualitative or quantitative variables. There are many definitions of data quality but data is generally considered high quality if it is "fit for [its] intended uses in operations, decision making and planning". Alternatively, data is deemed of high quality if it correctly represents the real-world construct to which it refers. Furthermore, apart from these definitions, as data volume increases, the question of internal data consistency becomes significant, regardless of fitness for use for any particular external purpose. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/SQL/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
I would counter "useful" isn't the right "spectrum"... you could have some high-quality data that's utterly useless to the analysis you need to do. I'd work backwards... can you come up with say the 3-5 worst "most impactful" examples of low-quality data and how they lead to an inability to provide good analysis or even worse, leading to the wrong decisions. Then work from those to a way to describe/define it... with attributes like accurate, timely, consistent, complete, etc. But ultimately (at least to me) "good/high data quality" means that if the data is available for use, and you use it to do an analysis or build a report, that you can trust that the data is correct and complete.
I'll throw something out there: Apply a popular data manipulation package (pandas, (i'm trying to make Rust a thing in data engineering), Javascript) and do some manipulation to examine some common data issues: null values, inconsistent column types, invalid encoding (the worst) and offer up the time cost of fixing each and the benefits of fixing that higher up the data chain.
 SELECT * FROM table tbl WHERE @@Hostname != 'Server123' You need that hostname check in every where except subselects, it'll run and just return nothing... I don't have a large db to check against right now, so I can't say for certain whether it'll lock rows/tables for any amount of time or not.
powershell/C# - select top (0) * FROM table, then iterate over the datareader.columns to identify the list of fields
What version are you using?
This is the correct answer!
Yes, just create a class library and reference it in your other projects
isnt class library c++ based?
Change the OR in your if statement to an AND. At the moment you're saying if the time is &gt;= to 0700 or &lt; 1900 which means this will nearly always be true (the only case where it is not true would be if @currenttime is NULL).
You could also avoid this confusion by using between: `IF CAST(@currenttime as time) BETWEEN '07:00:00' AND '19:00:00'`
Yes but you can create one that contains methods for interacting with your data.
Can you please give me example of some simple code that opens .csv file, groups it by some column and then save it? I have never used VS and it is literally what I need it for.
Here is a 5 minute video that has what you need: [https://www.youtube.com/watch?v=wRj9PZ2wyZI](https://www.youtube.com/watch?v=wRj9PZ2wyZI)
It's not exactly what you're asking for, but with some fiddling around with predicates and window functions you'd be able to get the data in the order you want. Using the window function will create cumulative sums, but on a huge table it may be an expensive query. SELECT * ,SUM(Cash_Amount) OVER(PARTITION BY Location ORDER BY Sale_Date ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS Location_Cumulative_Sum_Want FROM CumulativeSum ORDER BY Sale_Date
I don't think data necessarily has to represent real world constructs to be of high quality. You can have very high quality estimates or likelihoods for all kinds of things. &amp;#x200B; I also think that very exact data still leads to multiple versions of the truth often - maybe even more often than less exact data. We have vehicles that drive from a-&gt;b. Drivers can push "Start Driving" to generate a timestamp, dispatchers can set it directly, gps data can set it once vehicle moves out of a certain grid or even if the speed goes beyond some limit, door close+ignition on from the driving computer can set it etc. etc. And all of these times might also be recorded differently in the vehicle clock (driving computer/gps) and in the server clock. Not a single of these can be said to be false, yet still they give rise to multiple versions of the truth if you want to define a simple metric such as trip length. Then in the business ofc. you have to define the exact meaning of the term trip length to make sure there is only one definition used across the company. On the other hand if we just said first of the above events sets the timestamp and we don't need to record the others we'd have lower data quality imo, but there wouldn't be as much confusion on say trip lengths.
Sounds like you have one gene with many gene-symbols. This could be represented with two tables and a one-to-many relationship between them.
That sounds like an idea. However, one of the main problems is that I have 3 tables called "disease", "SNP", "genes". The foreign key linking them all together is the gene symbol. My issue is that some tables may refer to the same gene with different names which would be a problem to link them all together.
As others have said; Then you need another table to be a lookup table or you need another primary key. It’s like having a table of fruits with apples, oranges, grapes. But you want to get to the granularity of McIntosh, Cortland, and Fuji.
Ah thanks :)
In your php? Possibly, but this is the wrong place to ask about that. In your sql? Possibly, but mostly because you’re creating your query via string concatenation and leaving yourself wide open to sql injection. Use a real Prepared Statement. If there’s a syntax error, it should be raised through the various layers and shown somewhere. Unless you’re just catching and ignoring/suppressing error messages. Logic errors are tougher to catch and you may have to step through with a debugger.
Your account_id is defined "not null" in your table so your very last query would never return any data and also the "or account_id IS NULL" part will never fire for any records. having said that, you can absolutely use "(account_id = :account_id OR account_id IS NULL)" to "merge" the result sets of your 2 last queries.
It looks like the original this was forked from is still being actively developed. Why the fork?
Use the explain plan to see what part of your queries take the most time and think about ways of rewriting them or adding indexes to help things run smoother.
My fork have changes in gui presentation of data. Gives another views of same functionality. Its one of 1200 forks on same project. Everyone can use modification code including original authors of project if they found that usefull.
&gt; and it's usually just list of not using distinct or taking out sub queries you're looking in the wrong places ;o) try this -- https://use-the-index-luke.com/
Quick follow up for future note and anyone playing along at home: CASE does not work with Excel and Access. Instead, the [SWITCH](https://www.experts-exchange.com/articles/3556/Using-the-Switch-Function-in-Microsoft-Access.html) seems to work rather well. I thank you again!
Commenting because I'm also interested in some resources like this.
If you have a calendar table you can use that... otherwise a CTE would work: DECLARE @MinDate DATE = '2016-09-20', @MaxDate DATE = '2018-09-20'; with calendar as ( SELECT TOP (DATEDIFF(DAY, @MinDate, @MaxDate) + 1) Date = DATEADD(DAY, ROW_NUMBER() OVER(ORDER BY a.object_id) - 1, @MinDate) FROM sys.all_objects a CROSS JOIN sys.all_objects b ) SELECT DATEPART(year,Date), COUNT(Date) FROM calendar group by DATEPART(year,Date)
Honestly practice practice practice examples. Get a dataset offline and do everything in the 70-461 (I think it’s updated) book. Then take each example and put your own flavor on it. Good writing comes with failure writing bad.
Thanks for your time. I modified the table/updated the post to fix the issue with allowing null. In regards to the select query...my query works because I am only trying to get one setting value. The method does not work when I try to get all settings for an account. So lets say the values in the table are account_id | name | value null | email_allowed | 0 1 | email_allowed | 1 null | persist_login | 0 null | homepage_version | small I would want my select to pull down email_allowed = 1, persist_login = 0, homepage_version = 'small'
Some basic rules I try to adhere to: * Try to grab all data from a set of tables in a single pass * Use functions to grab small amounts of data (for example to populate a small lookup table or a scalar variable) versus executing a function for each row of a large data set * join and filter on indexed fields * Use indexed temp tables to breakup problems into smaller manageable chunks * Make good use of window/partitioning functions where it makes sense to use them * Retrieve only data that you're actually using (select * is evil)
If you're using Postgresql, you can create a time series for a date range: &amp;#x200B; select year, sum(day\_counter) days from `(` `SELECT DATE_PART('year', dd)::integer as year, date_trunc('day', dd):: date as cal_date, 1 as day_counter` `FROM generate_series` `( '2016-09-20'::timestamp` `, '2018-09-20'::timestamp` `, '1 day'::interval) dd` `) time_series` `group by time_series.year` `order by time_series.year` &amp;#x200B; Which produces: &amp;#x200B; `year days` `2016 103` `2017 365` `2018 263`
Try treating your single table as 3 different "data sources" (it would have been easier with normalized tables somewhat): a list of "actual users", a list of all relevant/default settings and a list of "customized per-user settings" (subqueries/cte will help you there). Are you familiar with joins?
So I do have a account table, I didn't do it yet but I was planning on having account_id be a foreign key for account_id (this is all getting built right now. I'm fine with setting up a "settings" table, the reason I didn't is I thought I could just get the list by querying all account_id is null values to get the list (every setting will have a default value assigned). I am pretty versed in sql and I use joins all the time. Thanks again for your help.
ps. also, if you only interested in a single account (never more than one), "group by" is another option for you (in your example, the output granularity is just the "name" of the setting)
That's neat!
 [https://sqlzoo.net/](https://sqlzoo.net/)
Sorry...I'm not getting it...how does a join or group by solve the issue
Have you considered GROUP BY GROUPING SETS? I use this function within Oracle, not certain if it or equivalent functions exist within other softwares. select location, item, user_ready, sum(cash_amount) from table group by grouping sets ((location, item, user_ready),(location),(item),(User_ready),()) ;
2016 currently.
This is basically what I think I'm going to do. Iterate over the column list from the sys.tables meta data and dynamically build the parsing string. I was hoping to avoid that, but I don't think there's any other way to handle this natively within SQL, at least not without bumping out to PowerShell or Python.
K so go step-by-step. You have "actual users" and "all relevant/default settings". How do you get "all users default settings" data set?
How did the exam go? Did the Pro SQL Server Internals boom help? Thanks!
Sorry...I don't follow. Just to be clear, I want to get all settings for a specific account/user. I provided the queries I am currently using. I have no idea how to merge them into one query.
Exactly. If every gene-symbol is unique, it can serve as the primary key of the gene-symbol table, which has a one-to-many relationship with the gene table (one gene to many gene symbols). The other tables also reference the gene-symbol table primary key. This allows them to reference any gene using any of that gene's symbols.
I highly recommend the book *SQL Cookbook.* I've made a whole career of what I learned from it.
You should be able to pivot the second table (would beed dynamic sql since there could be any number of cancels) and just join the result on customer.
If i understand your data setup properly, you should be able to use the lateral join to pull the last record from the "accurate log" table. If it is a reconnect, white out the termination date in your output.
One question I was asked twice (the interviewers never checked what the other asked already lol) was if you have 2 tables, write a query to return any rows that do not match. So basically comparing the tables. Threw me for a loop the first time because I’d never had to do anything like that. Second time still kinda rekt me because I researched it after the first time and saw a NOT EXISTS method and he said do it through a join. I got through it but struggled. Never had to use a FULL OUTER JOIN in practice before. RIP.... To be fair, I’d only been using SQL for like 6 months. It was for Google though so there’s that. 🤷🏻‍♂️
\&gt; relational tables &amp;#x200B; Making this statement proves you know jack shit about the relational-theory upon which SQL is loosely predicated. &amp;#x200B; If you really want to learn, do yourself a favor and read a book by Chris Date.
This works on 2016, and gives you a comma delimited string of the values of each row, without knowing the column names beforehand: SELECT STUFF(n.query(' let $vals := for $x in child::node() let $childnodevalue:= concat(",", data($x)) return $childnodevalue return $vals ').value('.','nvarchar(max)'),1,1,'') FROM ( select * from sys.objects /* replace with your table */ FOR XML PATH, TYPE, root('rows') ) X (X) CROSS APPLY x.x.nodes('/rows/row') n (n);
Hi, Yeah! Tricky! This is what I guessed, and the google confirmed it would work. Select Id_pk, col1, col2...,coln from table1 MINUS Select Id_pk, col1, col2...,coln from table2; I also screw these up in interviews. The way I deal with these types of things when I'm at work is: Read it, Google it, Get a cup of coffee, go take a poop, wander around the office and talk about Keanu Reeves, then go solve it. And they don't let you do that in an interview.
__Keanu Reeves is a national treasure.__
Hahahaa!
Why do you want to be a DBA?
I would love to move away from having to deal with a different customer every day and fighting with development. The database stuff is what I love about my job (o was hired as a normal support tech) and would love to focus more on it and move further away from supporting software.
Oooh that looks sexy. I will try this tonight.
I would say that DevOps role is a bit hotter commodity now, unless you're in love with the DBA job. If you've worked with MS technologies before, look into Azure (chances are, you will need to understand eventually other technologies - you can learn on the job tho, I think) You can start here to see the scope of what you'll need to learn/understand: https://www.microsoft.com/en-us/learning/azure-devops.aspx
I elected to take the exam at home and Pearson Vue's program cut out on me 4 questions in. I have a voucher to retake but am choosing to study more before i reschedule. The Pro SQL Internals book was the one to help me finally understand indexes, so there is that.
Yeah I’d also never used MINUS but I read about it when preparing for the interview. Just didn’t remember it on the spot. The second interview is when he said do it with a join and the concept of full outer self join is kinda confusing. Especially when my only experience was reading about it once or twice lol. Oh well.
SQL server has a substring function https://www.w3schools.com/sql/func_sqlserver_substring.asp, you could write a cte and join on the substring column, probably cast the column as the datatype you need
It's not so much then "THEN 1" that's important, as you have noticed it works with anything. It's the "ELSE NULL" that matters most here. COUNT does not count NULL values. So it's just going to count every time the CASE statement returns an actual value, in this case a 1 when the year matches your text string
Convert the int one to string in the join. Inner join blahblahtbl ON CONVERT(ID AS String) = otherId
I read another solution that did a column by column comparison and added up everything that didn't have a match. That seems tedious though. I'm hoping someone smart on here chimes in and lets us know the correct answer.
Often dba's support db's for development teams. Are you sure you want to do that or would you rather be a developer?
a) "re-orient the following query horizontally" is a pivot b) what does "count" do? It, actually, denotes 3 different functions: 1. count(*) - aggregates (adds up) 1 for each row in the group bucket, i.e. returns number of rows in the group (a distinct combination of values from the "group by" clause). 2. count( &lt;expression&gt;) - aggregates (adds up) 1 (_one_) for each row in the group where &lt;expression&gt; is not null 3. count( distinct &lt;expression&gt;) - builds a distinct list of &lt;expression&gt; values per group bucket (a distinct combination of values from the "group by" clause), aggregates (adds up) 1 (one) per each distinct non-null value of &lt;expression&gt;
Well from what I read, using NOT EXISTS was an efficient way of doing it. There was an article that had a few different ways.
Hmm I'm afraid I don't know what BA means in this context. I guess I mostly want to get away from Support by increasing my SQL skills. I still have to work directly with customers a lot. I would like to move away from that and work either on a single database instead of hundreds or be on the more development side of databases instead of supporting. I hope that clarifies at least a little.
Your example query will only return data where the values in your joining columns match. This is the same as an inner join. A similar example would be: SELECT emp_no, job FROM works_on, project WHERE works_on.project_no = project.project_no AND project_name = "Gemini" Though you may need to add which table the columns should come from in the SELECT portion of this one.
BA = Business Analyst.
I want to check that out. It sounds like the smart way to do this. Will it check the entire record?
Idk who down-voted you, but they're wrong as fuck.
 Have you considered using a Big Data platform? Assuming your "billion row" is fairly structured, Something like Impala or Azure SQL DW or Presto on top of some well partitioned data can be a gamechanger.
Best way to learn is using an example table/data set that’s interesting to you. For me it was a database of movies and associated info (directors, genre, release date, actors, genre, etc) If you have a bland data set of inventory it’s hard to be engaged or interested in writing queries
No that is what kinda tripped me up with the join logic. You have to join it on every column for that to work. Same with the not exists. I think the not exists was just procedurally faster from what the article said. But yeah, you would have to set each column equal to the corresponding one which if you have a big table would be a massive pain in the ass. In the case I would think MINUS would be nice because you could probably select *
Which one? There’s one with a lizard on the cover and one with a bird on the cover)
it's an inner join, the word inner is implicit if there's nothing that precedes "join". If you have left/right/full join the "outer" is implicit.
You need a dot(.) between seconds and millisecond not a colon(:) '07:00:00:000' -&gt; '07:00:00.000' '19:00:00:000' -&gt; '19:00:00.000'
Thank you for your explanation! I actually tried to substitute Null with 0 and since COUNT counts 0 I got a whole different number.
I'm a little confused on your last statement : *So it's just going to count every time the CASE statement returns an actual value, in this case a 1 when the year matches your text string* I thought that if I replaced the THEN 1 it doesn't matter so if I do replace the then 1 with 100 I can substitute your statement with the 100 and say: &amp;#x200B; *So it's just going to count every time the CASE statement returns an actual value, in this case a* ***100*** *when the year matches your text string* &amp;#x200B; Now I'm confused again haha. Sorry thank you!
I went into a school district with 0 experience as a break fix t1 and within 3 years I became the DBA they didn't have that position when I started and I knew nothing about SQL but wow did I love it when I found it. Every day I am challenged to try and make my district a better place and I get to work with every department I have learned about accounting, enrollments and transcribing, asset management and Valuation, HR and payroll. It helps that my district is very behind the power curve. I'm working right now to fully automate our staff and student account creation process between all our core applications, building visual reporting service's into our Student information system and trying to get our bussiness office to move to doing asset Valuation in house automatically with SQL so they have to have separate asset system so we can reduce the number of programs we have. If you are looking to really chew on data usability you might think about that, although I do still have to support end users it's not like Helpdesk work I pick my jobs and prioritize my own work.
tried that, it is still giving the same error
Do you have any interest in *designing* databases? From what the model looks like and how it is developed and how the data is accessed. To *administering* the upkeep of the objects you designed and implemented. To optimizing queries by digging deeper into the server systems that are underneath these RDBMS you're running. Or do you want to write queries to drive analytics and reports? I ask because the traditional DBA role has evolved immensely and is not what I thought I wanted to do 8 years ago when I touched SQL. I was in a similar path years ago as a desktop support turned SQL lover. I ended up writing queries and wrapping the data in SSRS, but that got repetitive. I started doing basic administration of SQL servers, setting backup jobs and other nightly health checks. Once that was automated, I was twiddling my thumbs. At that point I wanted to design the database itself. This led into learning how to write complex stored procedures and functions, transforming tables into optimized tables, sketching out schemas with a specific business application, to eventually architecting a solution end to end. There's a whole hell of a lot in between all of that, but it should give an idea for the scale of opportunities that SQL knowledge can bring. Typically you want to compliment with some additional value. Analytics with Python or R. Architecture with .NET. Linux and Windows system and network administration.
I'm guessing the best way to learn is using datasets with questions and answers you can practice with. There are some good online resources you can search e.g. stratascratch. I am giving this examples because I found it helpful as they have datasets with the questions come from technical interviews so all the questions are relevant to working on a job.
Hot damn. This was exactly what I was looking for! I had to wrap the main result in a replace to remove an extra space in between the delimiter and the field value, but other then that, perfect solution!
Hello, I have a question regarding to one of my assignment problems. It's asking to display all the folders that are empty, which in this case are the ones with id = 1, 4, 7, and 11. I have tried these so far but I feel like I'm not getting the concept or the full picture in order to solve it. I would appreciate some guidance. Thanks! [**select**](https://dev.mysql.com/doc/refman/8.0/en/select.html) \* **from** [files](http://cs3.calstatela.edu/adminer-mysql.php?server=localhost&amp;username=cs3220stu55&amp;db=cs3220stu55&amp;table=files); \-- 4. List the names of all the empty folders. \--select name from files where is\_folder = 1 and parent\_id is not null; \-- select name form files where parent\_id is null and \-- select \* from files where is\_folder = 1 ; \-- select distinct f.name from files f, files p where f.is\_folder = 1 and f.parent\_id = p.id; \-- select name from files where is\_folder = 1 and parent\_id &lt;&gt; id and parent\_id is null; \-- needs to be id: 1 4 7 11
There are a few ways to do this. But what the homework is probably looking for is a left join. You can join a table on itself, left joining on id = parent\_id to get records with no matches. There are a couple small steps from there but that's probably the piece you're missing.
Try this - select name from files where is_folder = 1 and id not in (select parent_id from files where is_folder = 0) and parent_id is null;
business analyst
You have quotes around things that should not have quotes Example SET @nextday = 'SELECT DATEADD(DAY, 1, @currentdate)' Trying to set a date field to a select statement will fail. TimeStamp &gt;= concat(@currentdate,' ', '@start_time_shift1') Comparing a Date to the concatenation of a Variable and the name of a variable will fail.
Thanks!! I'll give it a try :)
You need to have department name in the SELECT in the subquery as well as the parent query.
Lizard
In my 2 decades of db career, I’ve only used two types of `join`s. They are inner join and left outer join.
If that’s the code you’re running and you’re getting an error, you have a typo, “form” instead of “from”. Can’t help much with the rest but that I did spot.
I'm not great at SQL, but the version I use requires all subqueries to have an alias, and the parent has to allude to that alias.
Does what it says on the tin but it's a bit sparse in its explanation / detail. For instance, that script will return NULL in the name if the table has no clustered index / key. Similarly if the table only has a Non-clustered index it will return one row containing NULL in the name and then a 2nd row for the same table with the name of the NCI. That's going to look pretty confusing for anyone unaware of what it means.
I'd have a hard look at your database design. That sounds awful!
Assuming this is MSSQL change your datetimes to times and sort the brackets out as mentioned by Daakuryu \`\`\`
&gt; all subqueries to have an alias correct &gt; and the parent has to allude to that alias. not necessarily
Appreciate it. It's seeming like that's the case. Instead of doing a right join couldn't you just put the second table first and the first table second and then do a left join?
That's the answer I was looking for! Thank you!
Thank you! Your reply and everyone else's made it very clear
That’s right!
Awesome then, really appreciate the help!
So if I just remove the department_name column, it’ll run. No need for alias, which has me confused. This is also before adding a group by clause somewhere since I believe that’ll be needed with the aggregate functions but one step at a time on errors haha
ps, i presume you are going to do something with @nextDay at a later point as currently it does nothing
exactly
If you're managing the VM platform SQL Server is running on, you [must read this whitepaper](https://www.vmware.com/content/dam/digitalmarketing/vmware/en/pdf/solutions/sql-server-on-vmware-best-practices-guide.pdf) from VMWare. I don't know if Microsoft has one for Hyper-V, but a lot of the advice in the VMWare document should apply to Hyper-V as well. The number of CPU cores (virtual or otherwise) *will* have an impact on performance. SQL Server will run expensive queries in parallel if it can, but only if you have multiple CPU cores available. This can be the difference between a query running in 9 seconds vs. 90 seconds. If the workload is primarily single-threaded, you may be able to get away with less. But you have to be careful there and may find yourself adding vCPUs anyway. And if you're doing things beyond the SQL Server engine itself (SSRS, SSIS, etc.) you'll have higher CPU requirements as well because everyone is fighting for the same resources. &gt;I have one DBA insisting that SQL runs optimally on 4 cores because, 1 core is exclusively used for this, and 1 core is used exclusively for that, and the 3rd core is used exclusively for this other thing, leaving 1 core for Misc. I don't think that's true, but they're not wrong that a production SQL Server can't skate by on a single CPU. I don't think I've ever run a production or even dev/test instance on fewer than 4 CPUs, but for a very light workload you may be able to get by on two.
How is `template.id` related to `attribute.title` ? Does `attribute` have an `id` column?
&gt; Is this even remotely possible? YES!!! unfortunately, you have to hardcode everything into the query, which makes your table kind of useless (but you knew that)
Sadly I did know that. Looks like i am going to have a fun few hours hard coding
If the template.tableName is pre-defined, you can create a table-valued function where you pull data from individual tables (explicitly) and you'll be able to select from it. if the name of the table can change freely, you'll need to use dynamic SQL and a stored proc, so you'll be able to do exec to get the result.
The table names do not change once they are created.
I don't give any VMs 1 core... they end up running something (virus scan, windows updates, etc...) and making the VM unresponsive. &amp;#x200B; For your situation 2 cores for the template seems fair and if the SQL Server is doing OLTP or SSAS or anything else CPU intensive additional cores can be added. If the SQL databases are designed properly, the CPU usage shouldn't be high... proper indexes and RAM allocation should limit disk reads and CPU usage... but there are some workloads that are going to be CPU intensive regardless.
2 cores per VM is standard for us. I agree with you on your justification.
If it's MS SQL it makes sense in a licencing terms. Else I got nothing
I'm always just used the built in reports that SQL Server provides. &amp;#x200B; SSMS -&gt; Reports -&gt; Standard Reports -&gt; Disk Usage By Top Tables. &amp;#x200B; This only pulls 1000 tables, but that's usually enough.
Ran this on one of our databases and found a table with 5.7 billion rows with just a single ID column and nobody knows what its for. However nobody wants to be the one to drop it either
So if A.end\_date is null, then max(B.reconnect\_date) is the most recent date the service was reinstated.
You could try to run Database Engine Tuning Advisor.
What indexes exist on the table in the first place? What do the two execution plans look like? Are you specifying `order by`?
I have tried order by the date column and then as well, no order by. I will have to check the execution plans as well as indexes. I am a newer developer and have not had to deal with anything like this before. Just mainly reports, updates and stored procedures has been the bulk of my work so far.
Index was definitely the issue. No index on the tables, because they are basically remade from scratch every single day.
Then it's doing a full scan of the table every time, then trying to sort out 150 records.
If you don't specify an `order by` then you are not guaranteed to get the same "top 150" every time you run the query. If that matters to you, specify `order by`.
Just a shot in the dark, but maybe this will help? [https://en.wikipedia.org/wiki/ISO\_week\_date](https://en.wikipedia.org/wiki/ISO_week_date) You would want to modify the ISO formula so that your calendar's leap weeks line up.
&gt; Instead of doing a right join couldn't you just put the second table first and the first table second and then do a left join? Yes. My most frequent use case for a `right join` is when I've already written out the table names and I'm too lazy to rearrange them to make it a `left joint`.
Hahahahahaha! Classic! "what is that table?" "leave it! it's always been there." "ok, but what IS IT?" "Nobody KNOWS what IT IS, but what we do know is that it has always been here. So just leave it alone!"
Huh, these seem surprisingly basic for a job like that. I would have expected questions on window functions, stored procedures, cursors (yeah, yeah), etc. Off the top of my head, I'd look for knowledge of how you count (or otherwise aggregate) only rows meeting a certain predicate (i.e., `sum(case when x then 1 else 0 end)`), and good applications of self-joins and outer joins.
Ask them to explain the difference between clustered and non-clustered indexes, then see if they can explain column store indexes- This is nice for telling how up to date they are Ask if they know how to make a literal string- If they know this, chances are they've been through the weeds of debugging before
exactly. Going to look into a sproc that can create indexes on certain tables daily. Thanks for the help!
Appreciate it. I'll look into it.
You can benchmark SQL query latency, I would suggest you do so on 2 vs 4 core builds and show them that latency is lower on 2 core VM's. The reality is that most application queries are hot garbage, and a lower latency pipeline is often the best way to address them.
"Hey what's this script do?" "I dunno, populates a table with some huge amount of rows, doesn't seem useful" "Let's run it for shits and giggles!" *5 years later* New DBA: "What in the honest fuck is this table for?"
Your DBA is wrong. Queries and other processes will run on any available CPU. SQL decides which scheduler to assign a query to based on load. The actual number of Cores required will depend entirely on workload. There are a lot of configuratio s that can change the behavior of SQL.
I like asking fishing for answers around duplicate results in a result set: it's a good way of distinguishing between the junior levels (who tend to go for SELECT DISTINCT) and intermediate (who will talk about understanding the grain of the joins and maybe the dangers of SELECT DISTINCT).
Tell me more
Ask about how they verify their results and look for problems.
Index only works if it helps to avoid a full table scan. If you are going to read most of the table anyway the index could even hurt because it has to be read too.
SQL questions? for a Data Analyst? okay, here's one -- you are tasked with providing an analysis of year-to-date plus year-over-year as well as actual vs budget... what is your general strategy for the types of queries you would need, and the temp tables you'd create along the way
The calendar table is usually referred to as a date dimension table, and it's pretty easy to create them : [https://www.mssqltips.com/sqlservertip/4054/creating-a-date-dimension-or-calendar-table-in-sql-server/](https://www.mssqltips.com/sqlservertip/4054/creating-a-date-dimension-or-calendar-table-in-sql-server/) &amp;#x200B; There is a concept in data warehousing called the dimensional schema (aka snowflake schema), and that seems to line up with your goals. If you are going to denormalize, I'd suggest looking into it to make sure there is as little reinventing the wheel as possible. Here is a good article that discusses a few approaches: &amp;#x200B; [https://www.jamesserra.com/archive/2012/03/data-warehouse-architecture-kimball-and-inmon-methodologies/](https://www.jamesserra.com/archive/2012/03/data-warehouse-architecture-kimball-and-inmon-methodologies/) My work uses the Kimball methodology, and The 10,000 foot view is you create fact tables, which are snapshots in time and only store cumulative capable data and foreign keys (this keeps them small and fast), and dimension tables which are also snapshots in time joined by those foreign keys that describe the data in the fact table. So for instance you'd have a fact\_WeeklySales table, and then a dim\_department, dim\_calendar, dim\_product, dim\_geogrpahy, dim\_saleperson etc. Each one of those represents a different way to easily slice the data, and each would contain their own hierarchies so you can drill down effortlessly. As nice as that is in sql, there are tools to extend this functionality and bring it into the realm of near magical. For instance you can add a SSAS layer on top of it, which allows end users to use tools like excel or tableau to create their own slices of data in as easy of a manner as using a pivot table. These tools also make any kind of data analysis very fast, need a running total for the past two years with three data slicers, data is back in less than a second. My only additional advice would be to avoid MDX like a plague, it is quite easily the most difficult programing language to learn that I have interacted with.
What’s wrong with your output? What does it look like using your current code? If you `SELECT DISTINCT` will that deal with the duplicate rows? (Also can you please format the code by adding 4 spaces in front of each line?)
adding distinct seems to make no difference to the output, and the output give back nearly 100 rows, I'm wanting one row. Also if I leave he having as '1', then I get 0% on all the rows, if I give it 100 or 1000 I get nothing, if I change the operator to &gt;= 1 then I get oaver 150 rows with most all with 0% and a couple in a few percentages, and the count associated to them is '1'.
Don’t use Group by. Both your statements are aggregates, you only need group by if something is not an aggregate in your select. When calculating the percent try multiply by 100.00 (with the decimals) to convert the integer to a decimal.
With adding Distinct, and removing Group By, now makes the results as one row, as what I was wanting. Except, I leave 'having count(\*) &gt;=1' to get the results, anything else, and its no results. That just doesn't seem right. &amp;#x200B; The main thing I was dealing with, was the percentage. It just doesn't look right. In this example, I now get one row, count of 150, and a percent of 16%. I've trying to get the count of incidents, and those who have a violation of 1 time. I was expecting higher counts, and larger percentages.
Thanks for posting this! Problem is I'm working in BigQuery, so I'm not even sure if it's possible to convert that methodology into something platform friendly. I love working in BigQuery, but my main criticism is that its never been meant for doing things like this.
Ok, did this, too, and not really seeing anything different in the results, with and without the HAVING. In fact on the HAVING, if I get results, its with the operator &gt;=, and I can make it &gt;=1 or 100 or 1000, the result is always 16% and never changes.
Please provide sample data and the output you expect from the sample data. The way you’ve written your query will only ever give a single result because your aggregating the entire table.
Dummied query now as is: SELECT format(count(distinct(icount)), 0) as "Count" ,concat(round(count(\*) \* 100.00 / (SELECT count(icount) FROM database.tablename)), "%") AS percent FROM database.tablename WHERE Thing IN ('this','that','other','stuff') AND day BETWEEN '2019-01-01' and '2019-03-31' having count(\*) &gt;= 1; &amp;#x200B; Result set: Count, percent '150', '15%' )the formatting may be a little off below... What I'm expecting would would be more like: Incidents Percentage Incidents with 1 violation- 200,000 60% (and after copy and mod the date range for new second query) Incidents with 2 - 20 violations- 100,000 10%
Please add 4 spaces in front of each line of code so it will format like this I also need your input data.
SELECT format(count(distinct(icount)), 0) as "Count" ,concat(round(count(\*) \* 100.00 / (SELECT count(icount) FROM database.tablename)), "%") AS percent FROM database.tablename WHERE Thing IN ('this','that','other','stuff') AND day BETWEEN '2019-01-01' and '2019-03-31' having count(\*) &gt;= 1;
Both are integers in the table icount, vcount '7', '81' '115', '116' '2', '192' '68', '7' '178', '188' '6', '206' '1497', '1507' '1768', '1828' '96', '1191' '125', '126'
I need your input DATA. Not your query.
just sent
So what do these integers represent? You mention incidents in your output. Do one of these columns represent the count of incidents?
Yes, as they are counts, normalized data from elsewhere, incident counts, and violation counts
you only have two columns. Is column 1 incident counts and column2 violation counts? What is the percentage you’re trying to calculate? The number of violations out of the total?
icount is the count of incidents? If icount = 1 does that mean there was 1 incident? And you want to count how many times 1 incident occurred? What is the percentage you want to calculate? The percent of violations out of the total?
yes, icount is incident counts. yes an icount can be 1, and would be one incident. Yes want to count the number of incidents. And yes on the percentage, the percent of violations out of the total.
Oh the Google cloud DB solution. It actually already has a bunch of that built in, but I'm guessing your using the managed mysql instance. The way to get that done with mysql is to use the lag function, here is an article on it: http://www.mysqltutorial.org/mysql-window-functions/mysql-lag-function/
Okay. Try this: SELECT icount as incidents_count , count(icount) as count_of_occurrences , sum(vcount)*100.00/(SELECT sum(vcount) from Table) as percent_violations FROM Table WHERE date between ... and ... GROUP BY 1 ORDER BY 1; I will explain this: icount already gives you the count of incidents so you do NOT need to count it again. Then for the violations count, you need the SUM of the violations. Count(vcount) just tells you how many rows there are. Sum(vcount) will add up the number of violations in the vcount column. Here you use a GROUP BY because the first column, icount, is NOT an aggregate. Don’t use group by on an aggregate. You use GROUP BY to add up values associated to a non-aggregated column. So since icount is just retrieved from the table directly, you GROUP BY on it. So lets pretend this is your entire table (I’m on mobile so I can’t copy-paste): icount vcount 1 10 2 230 2 3 1 15 5 52 The SQL I wrote will do this (sorry about formatting): incidents_count, count_of_occurrences, percent_violations 1, 2, (10+15)/310 = 8.06 2, 2, (230+3)/310 = 75.17 5, 1, (52)/310 = 16.77 (The final column will only be the final percent but I’m showing you the math). So what will happen is your first column of your results should be the number of incidents that occurred. The second column: count_of_occurrences, will tell you how many times that 1 Incident occurred. So in my sample table there are 2 rows where icount=1 so it counts it as 2 pccurrences. The last column percent_violations will take the sum of violations associated to the number of incidents in the first column, and divide it out of the total number of violations in the entire table.
Why so low? Are you running Hyper-V or VMware? These virtualization systems are designed with the mindset that vCPU over-provisioning will take place and do incredible jobs at balancing resources. In fact I've been to several VMware workshops where they adamantly advise upwards of 8:1 virtual core to physical core provisioning. If you're running close to 1:1 you're doing things incredibly wrong.
This is fantastic! Only one thing, I add having, having count(icount) = 1 and I get 40 rows back, and not one row. I can mod in a comma, and remove the 1st column the field and do other cosmetic stuff, but that having still isn't giving a single result set.
wait, I did, having incident = 1 and its a single result as expected
and testing it with having incident between 2 and 10 gives of course multiple rows
don’t do count(icount). Look only at icount: WHERE icount = 1 Don’t look at the aggregate value. You don’t want to filter on the aggregate value. You want to filter on the actual. value of icount since that is your number of incidents.
Don’t use HAVING. Use a where clause. icount is not aggregated so you need to look at WHERE icount =1.
ok, did that, and its ok for a single count. I don't know why I'm so hung up on the having. So when I change the: where icount between 2 and 10, I get multiple rows back, and not one row as I was expecting
I like asking about subqueries. Which clauses allow subqueries (select, from, where)? Do they know the difference between a correlated and non-correlated subquery? Which is faster? What are some of the pitfalls of IN vs NOT IN (hint, they better mention NULL handling)? Subqueries aren’t too obscure a concept, so you won’t embarrass a candidate if you slowly increase the complexity of the questions until you hit the limit of their knowledge. I like talking about them because they offer a lot of range to determine depth of understanding.
Did you actually look at the output of the SQL code to understand why there are multiple rows? Each incident count is on its own row. If you did WHERE icount between 2 and 10 then you should have exactly 9 rows corresponding to icount =2, icount=3, icount=4, ..., icount=10. The GROUP BY allows you to see each incident count on its own row without having to manually filter every time you want to see a different icount value. The downside is that you can’t group the percentages easily. If you want to be able to manually filter so that you can get group percentages then remove icount from the select statement and remove the GROUP BY: SELECT count(icount) as count_of_occurrences , sum(vcount)*100.00/(SELECT sum(vcount) from Table) as percent_violations FROM Table WHERE date between ... and ... and icount between 2 and 10; If you do it this way you just have to be aware of the icount filter in your WHERE clause.
I'd have to say that this is working perfectly. And your explains are excellent too. Thank you for taking the time out to help me through all this, it's been invaluable.
Oh damn, that's rad as hell. I've used `LAG()` before, but now I'm really starting to see how I can use it for this purpose.
No problem, glad it worked and makes sense!
Do you have brief sample data?
I wonder if maybe there’s a disconnect between the vocabulary term “correlated subquery” and its actual use. I probably couldn’t tell you with any certainty right offhand what that is, but tell me you want one column in the main query to be a field from another table without using a join and I’d get what you were after. The person who writes SQL all day long still might not know the vocabulary term, but they could know the concept very well.
Yes, virtualization allows you to have a 16 to ratio. 16 virtual cores to 1 physical core. This means , 16 VMs running a single core each. Not a single server provisioned with 16 cores. That's absolutely stupid. &amp;#x200B; You VM's should only get enough virtual cores to run. To over provision a VM with more cores than it needs, will actually cost you performance on the VM itself, and on the host it sits on, and by extension the rest of the VM's that sit on that host. &amp;#x200B; google, vmware cpu ready, and educate yourself
Yes, when you've got something like FROM STUDENT_COURSE e That's assigning e as the alias for that table. Why they chose those letters I don't know, in the real world you'd usually use an abbreviation or something. You'll have to tell us what the error you're getting in the second one is.
 Where b.Month = COALESCE(Select innerA.month From #Changes as InnerA Where innerA.Month = a.month -1, a.month)
ora-00933
Are you running just that statement alone or do you have other statements in the window?
I put those 2 examples above in to see what they did...but this is the first statement. Im going to try something though make sure i didnt forget to upload a script for my assignment
 Select Course_Name,Session_name from course join academic_session ON Course.Session_Id = Academic_session.Session_ID ; i fixed it by deleting natural from the statement
To be honest, I don't get the relationship between vios.reportstaging1 and database.tablename. I am assuming that database.tablename is the incident, vios.reportstaging1 is the violations and these join on a similar col1 table. I am assuming col1 is a key that is in both tables but you are also doing count(*) from the database.tablename table multiplying it by 100 and then dividing it by the violations. That means that your percentage has a numerator coming from database.tablename and a denominator from vios.reportstaging which is super confusing. I think what you need is just a subselect. A query that runs first that your outer query reads from. SET @totalcount = (SELECT COUNT(*) FROM violations_table) SELECT Unique_Violation_Groups, SUM(Incident_Count) AS Incident_Category_Count, @totalcount*100/Unique_Violations_Sum AS `Percent` FROM ( SELECT COUNT(t.incident_id) AS Incident_Count, CASE WHEN Unique_Violations = 1 THEN '1' WHEN Unique_Violations BETWEEN 2 - 10 THEN '2-10' WHEN Unique_Violations BETWEEN 11- 20 THEN '11-20' WHEN Unique_Violations &gt; 20 THEN '20+' ELSE 0 END AS Unique_Violation_Groups, SUM(Unique_Violations) AS Unique_Violations_Sum FROM ( SELECT t.incident_id, COUNT(v.violation_id) as Unique_Violations FROM incident_table t LEFT JOIN violations_table v on t.incident_id = v.incident_id WHERE t.thing IN ('this','that','other','stuff') AND t.day BETWEEN '2019-01-01' and '2019-03-31' GROUP BY t.incident_id ) ii GROUP BY CASE WHEN Unique_Violations = 1 THEN '1' WHEN Unique_Violations BETWEEN 2 - 10 THEN '2-10' WHEN Unique_Violations BETWEEN 11- 20 THEN '11-20' WHEN Unique_Violations &gt; 20 THEN '20+' ELSE 0 END ) i GROUP BY Unique_Violation_Groups, @totalcount*100/Unique_Violations_Sum So if you made the string with the [column names] it should give you the desired result. "Incident with [Unique_Violation_Groups] violation [Incident_Category_Count] [Percent]%" I still feel like your numerator and denominator is off in your percent column.
Object explorer is just the drill down in your servers view isnt it? Just lists all the tables, SPs etc? Or is it something else you are looking for? &amp;#x200B; The main thing that annoys me about ADS is there's no search/filter in that view like you have in SSMS. &amp;#x200B; I still use SSMS for 99% of my work but use ADS for the GitLab integration when making schema changes.
Time for profiler...see what process/machine is inserting rows :).
I can't get into ADS. I keep installing it, messing around for a day, then need some missing feature and going back to SSMS. Clearly it was designed for strictly developers. I don't know if it's ever going to be an administrative or management tool.
Look up String_agg
For some reason my intellisense in SSMS is garbage. It works 30% of the time. I work as much as I can in SSMS but when I’m really getting deep into TSQL... I use Azure Data Studio.
What version of SQL Server? With 2017+, it's really easy. With previous versions, you have to do some wonky `STUFF()`.
Maybe use table variables. I, personally, don’t like to use nested queries, so I generally use CTE or table variables and I find they run much faster.
Have a read through this [link](https://www.mssqltips.com/sqlservertip/2914/rolling-up-multiple-rows-into-a-single-row-and-column-for-sql-server-data/), it breaks the concepts down nicely. There are a couple of steps to it, but totally achievable. Have a go and post your results, I'd be happy to help.
Sure, will do.
I wish that Microsoft would just bite the bullet and form a partnership with Redgate. Their tools turn base SSMS into something great.
Oh shit, yes. I'll have ten open tabs and working on a query to suddenly find that intellisense has stopped working and I have to restart SSMS. I wish they'd fix that.
change to a left join and move those inner join conditions into the where clause
Commenting because I want to know too.
A correlated subquery in the ON clause of a join... is a really bad idea when the tables in question are big. Or ever. Try to stick to straight up comparisons, and move the rest into the main WHERE clause.
I would use a transaction and build 5 temp tables. The nested ifs are slowing your query. However in somewhat new to sql so I could be talking into nonsense.
Try replacing your second with by ','.
can expand on that ? I am new to sql as you can see. but how can I just move the rest into main where cause
Add a ';' (semi-colon) before the 'With'. Example: ;with punchins as( select * from employeeinandout where punchType = 'Punch In' ) ;with punchouts as( select * from employeeinandout where punchType = 'Punch Out' )
Try the below: with punchins as( select * from employeeinandout where punchType = 'Punch In' ) select * from punchins; with punchouts as( select * from employeeinandout where punchType = 'Punch Out' ) select * from punchouts;
Following. Salamat!
I think exploring table variables or common table expressions will help in refactoring this query. Even if it's not a precise solution, I see lots of reuse of the status codes than can be reduced.
I'm not by my computer right now, but you can use subquery in the where clause. The SQL statement will probably looks like this Select * From table Where name in (Select name from table group by name having count(activity) &gt;1)
I forgot to add, the values in the Activity column have to be different
That, I need my computer 😅
[removed]
I want
Lol this is amazing
Where can I find
[removed]
Should be &gt;= 1. Could be a misleading result set.
If you put this up for sale somewhere, would you please share a link?
[removed]
SELECT count(*) as "Fucks given" FROM fuck WHERE fuck_id IS NOT NULL;
Ugh, seriously, this query makes my teeth ache. First - don't put conditions on the left table in the inner join clause, since it's confusing and at times not parse-able by the optimizer. "inner join on &lt;condition&gt;" is functionally equivalent to "cross join ... where &lt;condition&gt;". E.g. "AND n.main_member_id = 79" - move that to your "where" clause. Secondly, imitating "lateral join" (apply) functionality by using correlated subqueries - it's not going to perform well. It seems you're picking up a single record out of all matching ones. If you are on mysql version 8.0 or above then, I believe, you should have access to row_number() function. Use that and filter by row number 1. thirdly, more often than not, "OR" in the where condition is bad news performance-wise. Pre-calc the flag based on business rank and status the same way you did with "to_notify" and use the pre-calculated value in the "where" clause
\`DROP TABLE FUCKS;\`
Please put this on teespring
[removed]
I will post my newly updated code here later.
I work at a school district and I try to alias every assignment table as ASS. I know the kids would get a kick out of it
[removed]
[removed]
[removed]
I would totally buy one if you put it up for sale.
[removed]
I am using SQL Server, but this query should work. PS: Eric has "two hockies". SELECT Name\_, Activity FROM T1 WHERE Name\_ IN ( SELECT Name\_ FROM ( SELECT DISTINCT Name\_, Activity FROM T1 WHERE Activity IS NOT NULL )T2 GROUP BY Name\_ HAVING COUNT(Activity) &gt; 1)
Left Join vs. Inner Join
try this.. https:// bit.ly/2VGwIYO
&gt;https:// &gt; &gt;bit.ly/2VGwIYO This worked.
try this.. https:// bit.ly/2VGwIYO I posted a few links and they were not showing up. Maybe that will work.
try this.. https:// bit.ly/2VGwIYO I posted a few links and they were not showing up. Maybe that will work.
I posted the link and replied to a few people and it was not showing up. Maybe this will work for anyone interested. https:// bit.ly/2VGwIYO
I agree that group concat is da bomb
Bingo, by adding the o table to the where clause you've essentially changed the join from a left to an inner
think it should be SELECT fucks FROM things\_i\_give WHERE fucks IS NOT NULL &amp;#x200B; 0 rows in set (0.00 sec)
What have you attempted thus far in connecting to a SQL Server instance? Have you been given an instance _to_ connect to? Is it requiring Windows authentication, or is SQL authentication also an option? Grab Azure Data Studio, it'll be easier for you than working from the command line.
First time I've ever seen the NSFW tag used in this sub. lol
In the real world, the first place to start is figuring out that the company uses some non-standard fiscal year and you'll need to write all the conversion calculations from calendar date to fiscal date :)
Thanks for your answer. I tried to connect to the local host with mssql and sequel pro. Without success. And no, we weren’t provided with an instance to connect to. I think that the problem lies in the fact that the local host instance hasn’t been configured. And that’s my problem
Thanks. Used 'SELECT INTO' instead to create temporary tables.
&gt; I tried to connect to the local host with mssql and sequel pro If you haven't installed a SQL Server instance onto your computer, that's not going to work. MS SQL Server doesn't ship with macOS and never will. There maybe a copy of mysql or Postgres floating around in your system somewhere though. To run SQL Server on macOS, you need to do one of two things: * Install Windows in a VM, and install SQL Server there * Install Docker and use the containerized version of SQL Server. https://docs.microsoft.com/en-us/sql/linux/quickstart-install-connect-docker?view=sql-server-2017&amp;pivots=cs1-bash Or, your instructor could provide an instance (or collection of them) for the class to use. Depending upon the goals of the class, that might be more appropriate.
Do you have any columns that are Unique to the customer and project already?
Thanks a lot I’ll try that and get back to you if I need it
Exactly what I was thinking, otherwise it's perfect :) ha
Wow terrible. You call that bcsc-UCLA-420 normallized? Makes sense a capitalist pig would sell a denormallized t shirt, finance hacks messin with ma databwases
Wise Owl on YouTube is great for learning SQL Server. Here's a beginner playlist (might cover what you already know): [https://www.youtube.com/watch?v=2-1XQHAgDsM&amp;list=PL6EDEB03D20332309](https://www.youtube.com/watch?v=2-1XQHAgDsM&amp;list=PL6EDEB03D20332309) Here's a playlist with some more advanced concepts: [https://www.youtube.com/watch?v=fjNsRV4zLdc&amp;list=PLNIs-AWhQzcleQWADpUgriRxebMkMmi4H](https://www.youtube.com/watch?v=fjNsRV4zLdc&amp;list=PLNIs-AWhQzcleQWADpUgriRxebMkMmi4H) He also has tutorials on SSIS, SSRS, and C#--all extremely useful skills in the SQL Server world.
Can you describe the rules that dictate the assignment of the project names?
It could be a boolean data type too
/u/slingalot It could be a boolean data type too
Could you get negative fucks? Fucks to take?
Homework or business-related?
 Short on time but I think you are looking for something like this: &amp;#x200B; SELECT desc1.description AS DRIVER\_PRODUCT, desc2.description AS ADDITIONAL\_PRODUCT , COUNT(DISTINCT ADDITIONAL.TRANSACTIONID) AS SUPPORTING\_TRANSACTIONS, COUNT(DISTINCT ADDITIONAL.customerid) AS SUPPORTING\_CUSTOMERS FROM TRANSACTIONS ADDITIONAL JOIN ( SELECT PRODUCT, CUSTOMER\_ID FROM TRANSACTIONS WHERE PRODUCTID = 'PRODUCT YOU WANT TO ANALYZE' AND TRAN\_DATE BETWEEN 'STARTDATE' AND 'END\_DATE' ) GROUP BY PRODUCT, CUSTOMER\_ID) DRIVER ON DRIVER.CUSTOMER = ADDITIONAL.CUSTOMER JOIN ProductDescriptionTbl DESC1 ON driver.productid = desc1.productid JOIN ProductDescriptionTbl DESC2 ON additional.productid = desc2.productid WHERE DRIVER.TRAN\_DATE BETWEEN 'STARTDATE' AND 'END\_DATE' GROUP BY desc1.description , desc2.description HAVING COUNT(DISTINCT ADDITIONAL.customerid)&gt; 10
JOIN is an INNER JOIN. I'd get used to typing INNER JOIN if I were you, simply because at some point in the future for reasons we cannot yet understand it may become something different, and then you'll have some code to rewrite. Might as well be specific now instead of later.
No.
It can be anything. The project name is just a string variable.
If you’re a woman, will you marry me? If you’re a dude, will you adopt me? I love you.
Why two spaces after the equal sign, though?
My professor has all his SQL classes open to anyone on his website. He does videos and has packets you can reference. https://faculty.business.wsu.edu/featherman/t-sql-training/ Another great tool I reference is https://sqlzoo.net/
This is just for a college course. But I'll keep that in mind and make a habit of it
Thank you
Thank you
gay
the difference between the two queries is that the LEFT JOIN returns all companies, whereas the INNER JOIN returns only companies which have investments you can see this by changing DESC to ASC in the ORDER BY clauses
SELECT * FROM fuck WHERE ; drop table fuck;--
Do I need to use a explicit cursor?
I concur.
OK but how do you decide which records get assigned to which project names?
&gt; why for this specific problem they use a left join rather than in inner join? In addition, if the answer did require an inner join what would the problem (bolded below) be reworded as? I dont know the thought process for this example specifically, but in the real world very rarely you'll get a spec that says 'do a left join here'. You'd need to understand the business context and know the data layout to decide what kind of join to use. I guess in this case, the clue was "Limit to only companies in the state of New York" and they could have said "Limit to only companies in the state of New York that have at least one investor" and that would lead you to use an inner join. p.s. also, since you're just learning, I should mention that joins (and other operations) are for datasets and the tables are just a convenient way to refer to a stored dataset; so when deciding what to join with what you need to think what are you trying to get out of it and what datasets do you have (or need to build) for getting that.
I love you guys
That's how they get you.
Can you provide your acceptable answer to this question?
I don't really understand why you're using a cursor at all. Either way you probably need to concatenate a day onto the string then cast as an int and subtract that from today cast as an int and divide that by 365
I like this!!! Honestly we have a multiple choice SQL test that we hand out that covers what a window function is (a function that can look outside of the current record to determine the result) and people bomb it all the time. I’m not just talking that particular question, I’m saying they bomb the entire test. To be honest, “data analyst” where I work is more about data management and building tables / making data available for others to analyze. I suspect many people with true SQL skills want to work in data analytics proper or a data science like department, not data warehousing.
Can you explain what a cursor is?
Yep, you're right. I feel dumb. Lol. I guess the reason I felt this wasn't that was because some extension have a "open in Object Explorer' option and it does nothing.
I'll have to look into Redgate's SSMS plugins. I use their SQL search extension for ADS and it's one reason why I love ADS.
Why do roles have date ranges?
No, it's a `bit` type. Couldn't be &gt;1.
SELECT 1 AS given WHERE EXISTS ( SELECT fucks FROM all )
Still wouldn't hurt to check!
WITH fucks as ( SELECT allFucksGiven ) SELECT none WHERE given = 0
Would it affect the execution plan or will the query optimizer fix it? It the former, I have to object.
This is all coming from salesforce. Roles can and do change over time. We have a datalake that captures the changes over time.
Pretty sure SQL server optimizer handles it. Don't know with any other flavors.
Zero xb fses
So every time you get a new role or an update to a role, a new role ID is created?
No, id stays the same but name of role changes. I’m trying to track those role name changes over time and apply roles to the user changes.
SQL prompt is the single greatest software purchase my company makes for our devs. It is what intellisense should be.
What freak names tables in the singular?
Looks great. I'll have to give it a try and see if it makes SSMS worth it. It would have to be beyond amazing because SSMS is just way to much bulk for what I need. For instance, I have switched all of my development tasks, web and mobile, to Visual Studio Code because I just don't use 90% of the features of IDEs. For passion projects, if I can't do it in vim or code, I don't do it.
You just need to build a case statement that will automatically set the date at runtime.
What are your problems with the logic, other than, you know, writing it down? Your granularity is max 3 records per user history record (prior to role_start, valid role, after role_end), generate these, populate role_start and role_end accordingly (for "pre" the start would be user history start, the end will be the earliest of history end or role start, etc.), then throw out records with invalid ranges.
Could `where be to_give &lt;&gt; 0;`
Doesn't even pass the sniff test. A core exclusively used for anything is a wasted core.
!= you monster
sql DROP TABLE mic ;
It's because in the 3rd expression you're counting the wrong column. You have `companies.permalink` which, according to those conditions, is always going to be null. A habit I've built up for writing filtered aggregation expressions like this is: CASE WHEN condition THEN 1 ELSE NULL END Hope that helps.
Depending on which database engine you're using, the date/time functions are going to be different. You also have to keep in mind what time zone your data is being written in (best practice is to write all your times in UTC/GMT, or use a timezone-aware storage type). I'm just going to assume everything is in your local time zone, and use my engine of preference (MS SQL Server). This uses CTEs to prepare some datetime mangling, and gives you two ranges: one for the current shift, and one for the previous shift. WITH TimeAbstraction AS ( SELECT -- Get a datetime set to when the current shift started: DATEADD( HOUR , CASE -- It's between 12AM and 6AM, shift started yesterday evening WHEN DATEPART(HOUR, GETDATE()) &lt; 6 THEN -6 -- It's between 6AM and 6PM, shift started this morning WHEN DATEPART(HOUR, GETDATE()) &lt; 18 THEN 6 -- It's between 6PM and midnight, shift started this evening ELSE 18 END , -- MSSQL Fast Truncation; use TRUNC(SYSDATE, 'DD') for Oracle CAST(FLOOR(CAST(GETDATE() as float)) as datetime) ) AS Start_TS ) , Shifts AS ( -- Generate two sets: one for the current shift, and one for the previous shift SELECT 'Current' AS Shift , Start_TS , DATEADD(HOUR, 12, Shift_Start_TS) AS End_TS FROM TimeAbstraction UNION SELECT 'Previous' AS Shift , DATEADD(HOUR, -12, Shift_Start_TS) AS Start_TS , Start_TS AS End_TS FROM TimeAbstraction ) SELECT SHIFT.Shift , SHIFT.Start_TS AS Shift_Start_TS , SHIFT.End_TS AS Shift_End_TS , DATA.* FROM Shifts SHIFT JOIN YourData DATA -- Can't use between here, because it's inclusive and would duplicate anything -- appearing on the shift start/end hour in both sets. ON DATA.DateTime_Column &gt;= SHIFT.Start_TS AND DATA.DateTime_Column &lt; SHIFT.End_TS ; Usually answers are meant to be more "leading" rather than directly solving the whole problem for you. I think this is just complex enough that taking the time to break down how it works might actually be the lead instead. :) Hope this helps!
What you have here are two Type2 slowly changing dimensions (SCD). Normally when you're querying this type of data, you're using a single point in time to slice into the data by. But overlapping them? This can seem pretty hard to do, but when you only have two tables to consider and you understand some of the logic, the pattern actually isn't too bad. First, is [testing overlapping ranges](https://stackoverflow.com/a/325964). Seriously, bookmark this answer. It's amazing and I've been going back to it for years! You're going to apply the simplified expression from that answer on the join of the two tables: `A.Start &lt;= B.End AND A.End &gt;= B.Start` This will end up with cartesian result - that's fine. Each row's true effective range is the smallest overlapping window from the two tables: , CASE WHEN A.Start &gt; B.Start THEN A.Start ELSE B.Start END AS New_Start , CASE WHEN A.End &lt; B.End THEN A.End ELSE B.End END AS New_End And this should give you a new, denormalized Type2 SCD result set. If you need to do more than two tables... well... I'd just script progressively joining two tables together, taking the result from the last join forward into the next one, applying the same pattern as above. Hope that's what you were looking for!
I'm just going on what you've provided above. I haven't actually looked at these hacker problems myself. :) One part of the inner query is a poor man's implementation of the window analytics function: `ROW_NUMBER()`. MySQL's feature set is seriously behind the times - and professionally, I tell people to stop using it. Nowadays it's really only good for storing/retrieving data, but absolutely terrible for **any** kind of analytics. Hacky solutions like this got to be pretty common, but for the last decade modern SQL has offered *so much more*. Anyways, if you run only the inner part of the query, you'll see the rownumber incrementing, and a lot of nulls in between names. It probably comes out in a jumble - but if you `ORDER BY Doctor, Professor, Singer, Actor` it might make more sense what's happening, and what the outer `MAX()` will end up doing. `MAX()` does work on strings - it'll just return what would sort "last" - and every analytics function ignores NULL values. Hopefully that leads you in the right direction but let me know if anything still isn't clear. Or if you want to know more about these magical analytical functions! They rocked my world when I first learned about them years ago, and they still tickle me to this day.
The WITH clause only needs to be specified once at the beginning of the query. Every CTE is separated by commas after the first one: WITH CTE1 AS (SELECT ...) , CTE2 AS (SELECT ...) SELECT ... FROM CTE1 JOIN CTE2 ... To the interpreter, it either looks like you're trying to start two different queries, or (in the case of MSSQL) trying to specify query options (which go at the end of the query in a final `WITH()` clause).
Try this Select ( to_date(sysdate,'yyyymmdd') - to_date(to_char(left(pnr,8)),'yyyymmdd') )/365
What the fuck is this?
Obviously, you're not a golfer.
I find this question more on the DBA side of things rather than analytics. The one doing analytics (if they're even aware of these things) really doesn't want to have to care about what it takes to keep a database running smoothly (just that's it's available). They want to focus on the data itself. Meanwhile, the DBA is endlessly frustrated by queries submitted by analytics users. lol If someone's only been a user to a database operated by someone else, they'd never really think about indexes. But, if they've ever wondered why their query times suddenly drastically improve, a good DBA implementing the right indexes or storage strategy is usually the reason. So if you're targeting a data analyst, I'd go through some of these various SQL concepts (in no particular order): * Join vs. Left Join vs. Full Outer Joins * Rather than have them explain how it works, try to create problem statements instead, that consider how one or the other would affect the results. * Aggregating (Group By) - kind of the bare minimum :) * CTEs * Which are really just subqueries, but users that use them often, often show to be more mentally organized. * Windowing (aggregation with the OVER() clause) * This is also great for cleaning out duplicated rows, or cartesian result sets that can't be de-selected by WHERE/JOIN conditions or a DISTINCT clause. * A basic understanding of star/snowflake schema data models * Enough that they can traverse these types of models to get the data they want. * Type1 vs. Type2 dimensions (beyond that you've got a data engineer, not an analyst).
As long as your customer can't have two projects running at the same time (your data seems to meet that criteria), then you can use a running counter that only increments when the Project Event = 'START': , COUNT(CASE WHEN Project_Event = 'START' THEN 1 ELSE NULL END) OVER ( PARTITION BY Customer_ID ORDER BY Event_Date , CASE WHEN Project_Event = 'START' THEN 1 ELSE 2 END ) AS Project_Num The moment the customer has two projects running at the same time this goes out the window - you will need something already in place in your data that takes the place of the `Project_Num` we have here.
Not sure what database you're using, but you can more than likely use a stored procedure. Using a cursor you can dynamically build the query string, and then just execute it. Most engines are capable of this. This is the approach using MSSQL: CREATE PROCEDURE TemplateLoop AS BEGIN DECLARE @Query VARCHAR(4000) , @ID INT , @Table VARCHAR(200) ; DECLARE CUR_Tables CURSOR FOR SELECT templateID, tableName FROM Template ; OPEN CUR_Tables FETCH NEXT FROM CUR_Tables INTO @ID, @Table; WHILE @@FETCH_STATUS = 0 -- Returns non-zero when the fetch fails (no more rows) BEGIN IF (LEN(@Query) &gt; 0) SET @Query = @Query + ' UNION ALL ' SET @Query = @Query + CONCAT( 'SELECT ', @ID, ' AS TemplateID, ''' , @Table, ''' AS TableName, title AS AttributeTitle ' , 'FROM ', @Table ) ; FETCH NEXT FROM CUR_Tables INTO @ID, @Table; END CLOSE CUR_Tables DEALLOCATE CUR_Tables EXEC (@Query) END If you don't have permission to create procedures, you can just remove the procedure block and run the script as-is too.
Am I the only one getting triggered by the double space in front of the 1?
A cursor is like a loop. It reads your data one row at a time instead of in a set. It's not an efficient way to read data. It has it's moments where it's necessary but I recommend not using it if you can avoid it.
how is the database supposed to know what an ID number menas? how do you know that 64123 isn't actually "red T-shirt" ??? where are these item names supposed to come from?
&gt; Best way to maintain User Balance limit them to only one Margarita at lunch
&gt; how do you know that 64123 isn't actually "red T-shirt" ??? this is what i'm having trouble with, i want to assign 64123 as "black T-shirt", while also making it so that both values go in different columns so that every time i type in 64123 it automatically fills in "black T-shirt" in another column &gt; where are these item names supposed to come from? i was thinking that maybe i could assign them at first, but i dont really know how to do this im sorry if i cant answer you clearly, im new to sql in general. i have the idea in my head but i cant translate it into code
best way to store a user balance lets say you have daily transactions records containing eg txdate,amount,txoperation(debit,credut) etc...i dont want to be aggregating the txtable every time i need to get a user balance. 1. Materialize view(index view) but i think down the line it might become the bottleneck when the system becomes highly transactional 2. have a balance table thats updated each time a tx happens [3.save](https://3.save) the txtable with runningtotal column of last balance and select the last record &amp;#x200B; need help guyz
&gt; i want to assign 64123 as "black T-shirt", while also making it so that both values go in different columns so that every time i type in 64123 it automatically fills in "black T-shirt" in another column store item IDs and item names in your `items` table then whenever you need to use an item, like in an `order_items` table where a customer orders several items, you only have to use the item ID google "primary and foreign keys"
oh, this is a good solution, i was thinking of only using one table, so i even though i already knew about primary and foreign keys i didnt think of applying them so considering that i can create a separate table for the items, maybe i can fill it in with columns like ItemID, ItemName, and Price. but what if i try to do something like calculate the total price by multiplying the "price" in the items table with an "amount" from another table? can this also be possible with primary and foreign keys? also, just out of curiosity, is it possible to make a code for my original question without using another table and primary+foreign keys?
Exactly what I was thinking. At the very least &gt;=1. The way it's written, yeah, you can't find 1 fuck but you're not looking for other values.
Hmm it says: ORA-00904: "LEFT": invalid identifier
But it’s unnecessary. If you have an int field you wouldn’t write a condition in your query to ensure it’s not a string.
Yes, when you do a join, you essentially get one big table with access to columns from both tables so you can mix and match them in your calculations as you like to calculate things like total price. I don't think it's possible to achieve your original question. Like r3pr0b8 said, the database needs a lookup to know how ID maps to Name to do it "automatically." If you really want one table, maybe you can just enter the ItemID at first and then use an update UPDATE table SET itemname 'black tshirt' WHERE ItemID = 64123 But that really defeats the point of using SQL and a relational database.
Rec.pnr
Thanks, I figured it out :)
Physical really just means how it would physically exist in the database. As for OLTP vs OLAP, I don't know any tricks but you only really need to remember one letter. Transactional or Analytical. I know that doesn't help much but I'm not sure there would be an easier trick
&gt; a computer database doesn't have physical existence translating logical design into actual tables, then creating those tables in a database -- that's very physical and sometimes the data in the physical database is so big that it cannot efficiently be transmitted electronically, so [the hard drives have to be flown to their destination](https://youtu.be/pAoEHR4aW8I?t=259)
This is what I was looking for. Thank you! I knew it was a bit more complicated and wanted to see if there was a post somewhere on this logic. Thank you for sharing.
This is exactly what I was looking for. Thank you! I knew it was a bit more complicated and wanted to see if there was a post somewhere on this logic. I tried searching for it and wasn’t probably using the right search query. Thank you for sharing.
thanks, i used a join (inner join) like you suggested and i finally got the output that i wanted. this basically answers my question
Time to get a different job. Your boss respect you or your skills. He's closed minded and that won't change. If you want to collect a check you can stay, but if you want to feel valued you need a different situation.
&gt;I understand why the preceding steps are called "conceptual" and "logical" design, but the terminology of "physical" design just makes no ready sense to me, since a computer database doesn't have physical existence. It does, because at that point the database must necessarily exist on disk (or other storage medium). Don't get too hung up on terminology. It could have been called something else, like "technical design" or something. It just denotes the phase in the process where you couldn't work with just pencil and paper anymore.
When he dismissed my knowledge of databases because I didn’t have a certification, I was angry because I know more than him. (He barely understands ms windows) but also I had no rebuttal because technically not having a cert was true despite having gone to classes and invested many hours learning SQL and databases. My instinct was to say “I know more than you about it” but that would escalated it to an unprofessional degree, and in his heart he thinks he knows more than everyone because of his title, and he over inflates any experience he’s had with anything to the level of “master” so he has a high opinion of himself
&gt;since a computer database doesn't have physical existence Uh what? Are you not learning anything about physical storage on disks and how the way tables and indexes are stored on disk impacts performance?
I love you guys...
Never underestimate the bandwidth of a station wagon ... https://en.wikiquote.org/wiki/Andrew_S._Tanenbaum
We've finally reached the point where we are so far removed from hardware that students have never learned a physical "disk" is? Man, I'm fucking old.
I’ll extend a few points that others have made here but the key is simple: the database is your persistence layer. Virtually everything else is loaded from disk to memory and the CPU then runs from there. Databases manage state across reboots and across failures in a practical and consistent way. Sure, they return queries and process SQL but they also are designed to ensure that when a physical failure occurs that recovery to a known point is assured and verifiable. The physical topology comes into play with this part and is driven by your DBMS: SQL Server running Availability Groups require different physical (and logical) architectures than MySQL or Cassandra. But the key is deploying and maintaining a physical architecture (cross server rack or cross data center or cross continent) with the logical in a fashion that supports the business in function and cost. Speed of light and latencies come into play more often than you may imagine as well. The great DBA’s that I’ve met that understand t he physical layout in addition to the logical. Do you need ti know the physical side? Maybe not. But understanding it will help to know more about long running operations that a database provides outside of the “normal”.
I've been doing database stuff full time for 6 years. I've never worked with anyone who had certs. That includes stints doing bdata warehousing at Fortune 500 companies. I was chatting with some old Co workers about it a few weeks ago. The Microsoft certs are only about memorization. They're a joke.
Well, technically sure, but I think most people would agree that an computer database isn't especially tangible. I might have put forward another designation, myself, but judging from other responses, this is where the terminology comes from, so I suppose I just have to grit my teeth and bear it.
That's definitely the key point behind the terminology, yes -- an insight I wouldn't have explicitly had unless I asked, so I'm glad I did.
I would have a hard time imagining any CS program worth accreditation not discussing hardware in some detail, so I don't think you're that old, yet.
Thanks, that’s reassuring. So how do you prove level of competency to your current employer or future employer if you can’t prove it with a paper?
You know that, I know that, but management lives in a different world from us. It's all about CYA, if something goes wrong they don't want to be asked why they let some uncertified college drop out touch a DB, if there is a cert involved they at least have something to deflect the blame. I've also spent time in the fortune 500 grind, and the amount of people I've met with certs is simply staggering. Every contractor I've had the chance to deal with has an alphabet soup of certs, and most of them were unimpressive coders. I remember one of them had me review a proc that literally had a where clause like "where XColumn like '%Value%'", I had to keep looking at him to make sure I wasn't being pranked. My poker face professionalism is probably the only thing that kept me from laughing.
Most places will give you a technical interview. That is where they'll see your skill level. Expect a phone screening first where they'll ask what your work on in a normal day, but it won't be technical. The hiring manager or hr is getting a barrier rough idea of your skills. The technical interview will generally happen as part of the in person interview.
Thanks. I have seen multiple WITH clauses in an online article, though. Probably for an older version or something.
I read your edit, thank you for extending your offer to help me.
As a DBA, that couldn't be farther from the truth. * I run nightly [physical integrity checks](https://docs.microsoft.com/en-us/sql/t-sql/database-console-commands/dbcc-checkdb-transact-sql?view=sql-server-2017) against my databases. * Physical hardware must be considered for backups. What happens if you put the backups on the same disk as the MDF file? And the disk fails? * Read/write speeds, and thus a large part of query performance, are directly related to physical hardware. * The physical filegroups and partitions also have an effect on performance and even some code syntax. * There are so many other things that must be considered which affect or are affected by the physical aspect of the database. Network speeds. Compression. ETL. Indexing. It's too early on a Saturday and I could probably think of a dozen other things. The bottom line here: The physical aspect of the database is extremely relevant to the speed and accuracy at which the data is served up to the front end.
Good bandwidth, but really poor latency.
Relevant written-by-XKCD-but-not-an-XKCD-comic: [https://what-if.xkcd.com/31/](https://what-if.xkcd.com/31/)
Post your query, sounds like you just need to use getdate().
Terry Davis felt your pain. The truth-of-the-matter, so far abstracted, as to be forgotten.
For the OLAP/OLTP thing, just trying out a different mnemonic ... OLAP **A**nswers things. OLTP **T**racks things.
That cursor could be replaced by a select I think
There are times when a CTE is faster and when a temp table is faster, unfortunately, from my experience, it's a case by case situation. As a rule of thumb for me, it seems the more records being returned slowly favors a temp table. It sounds like the CTE's you are running across may be queries wrapped as a CTE versus proper CTE use. Either way, keep up the performance tuning. :)
I always use CTE. Temp tables still wrote to the temp database, so I've ways concluded CTEs in memory would perform better though I've never tested this out. I honestly LOVE CTEs....
The implementation of TABLE variable is not well optimized, and I believe was only added to get feature parity with Oracle.
Recursion
In a nutshell, avoid CTE's when they become too verbose, or too complicated, and break the steps down into #tables to force the query optimizer to behave how you want it to, not how it thinks it should. You do this by testing steps, breaking subqueries into #tables, incrementally adding joins, etc. Note quite a CTE, but an example I was working with this week looked like this: INTO #newTable FROM Table CROSS APPLY function LEFT JOIN table LEFT JOIN table LEFT JOIN table LEFT JOIN subquery INNER JOIN table LEFT JOIN subquery LEFT JOIN subquery Which then broke down into: BEGIN INTO #testTable FROM Table CROSS APPLY function INNER JOIN table END BEGIN INTO #testTable2 FROM table LEFT JOIN table LEFT JOIN table LEFT JOIN subquery INNER JOIN table LEFT JOIN subquery LEFT JOIN subquery END BEGIN FROM #testTable LEFT JOIN #testTable2 END There was no guide to this approach, and I could have played with making #testTable3, 4, etc., to eliminate subqueries, test other combinations of joins, etc. Just tested and the total process went from running in about 45 minutes to running in about 6. Be sure to also test indexing your #tables. Sometimes its faster, sometimes it takes longer to create the index and run the next step than it does to just run the step without indexes.
I choose which of the three to use based on how many rows I expect it to fetch. For table variables anything over say 3k rows is to many and performance will tank. CTE's its about the same, a few more maybe, if the CTE result set gets large its spills into TempDb so it doesn't stay in memory. #temp tables can be indexed so they have that, although you can also do an inline index in Azure with table variables but I have only read about that. Like everything else it depends, but row counts matter for these. I do all SQL Server so I don't know how the other RDBMS would treat this.
I'll be honest, I never actually found a post specifically solving something like this all in one place. It's a cobbling of a lot of learning, knowledge, and grit. And aside from reddit here, I'm not much of a blogger to really put it all together myself. :)
In my experience a #tempTable is much better when the number of rows returned reaches some threshold amount. They're also a good way to refactor complex joins.
Yeah, I almost always take the approach you did at the end, just naturally. And the resulting stored procedure “looks” more verbose but it almost always performs better than “heroic” all-in-one-pass queries, so I’m going to continue doing it I guess.
Thanks for the reply, but it appeares I´m getting a error on the line.By the way when I said ProductDescriptions it was a column with only the Products names, futhermore is there another way of doing this but only using `INTERSECT` function? GROUP BY productid , Customerid) DRIVER ON
I have run into situations where using temp tables blow up my TempDb, so I always use CTE’s.
"Homework", I'm trying to learn all by myself using a book.
CTE guy here - in mostly OLAP environment. I heavily use CTE's to logically seperate subqueries. So instead of INNER JOIN-ing a subquery - I setup that subquery into a named CTE for ease of understanding. It adds nothing for performance IMO but simplifies understanding (when done well). I sometimes use #tempTables to instantiate the intermediate steps in a complex query, either to try and make things easier for the query optimiser or to reduce locking. I still don't really see the benefit in @tableVariables for this purpose - even with very small result sets (mostly due to the lack of stats/cardinality estimation). To me a non-recursive CTE does not add any performance benefit over writing out the query in full - it's just syntactic sugar? Happy to be proven wrong?
I'm interested by your comment "proper CTE use". How does that differ from using queries wrapped in cte's Are you talking about recursion or something different? thx in advance
If you have an id column, you could try [id] % 3, and insert into the tables where it equals 2, 1, and 0.
&gt;but I think most people would agree that a computer database isn't especially tangible. Speaking as someone who's spent nearly half his life managing stacks of hard disks, I can assure you that they most certainly are tangible.
Sysadmin here: You are 100% right on all counts.
I shall try this as soon as I get back home. To better understand, I am doing... Insert into Table1(id) select from MasterRooster (Id) % 3
This is an interesting question because I thought that CTEs essentially resolved to temp tables. However, according to this discussion that is not the case. Thank you!
The % operator returns the remainder of the division. So... 1%3 = 1 2%3 = 2 3%3 = 0 4%3 = 1 5%3 = 2 6%3 = 0 Etc...
Wrapping a query in a CTE just because you can doesn't mean it's the correct choice A proper use of a CTE, IMO, mimics that of a sub select, LEFT, or APPLY clause not slamming data into CTE's just because. It's in the name, common table EXPRESSION. As others have said, if the CTE is pulling back a lot \[relatively\] of rows, perhaps a CTE is not the correct route. As always, viewing the execution plan and understanding which CE you are using is almost the most performant.
Good to know, thanks. I liked those the least anyway, I never used them because they felt harder to debug to my brain, good to know there are other reasons to keep ignoring them haha.
What you are looking for is an agent job that is scheduled to run once a day and update the table. Triggers are only fired when data in the table is changed. I don't know the context this is used in but one of the rules of data normalisation is to not store any data that is computed from base data stored in the same database. Instead of storing the flag, how about building the date logic into the queries and read the expire date instead? You wouldn't need the flag column in the first place. Put an index on expiredate column and when you do queries with the expiredate in the WHERE clause, it will be just as fast as reading the flag column.
You can put indexes on temp tables, so there are many situation when they are faster
Blowing up as in waits or space?
This is heavily dependent on its implementation and shouldn't be taken as a blanket statement.
This is not a 100% rule. If you use a CTE twice, the engine will run the underlying query twice. A CTE is not much more than a named subquery. There are definitely times where it is much more efficient (and potentially better for the overall application or database) to use a temp table. Hell, you can index a temp table.
I'm an OLTP guy and I heavily agree with you. However, CTE's CAN have a massive performance implications and aren't just sugar. Table variables have their place and are fantastic, for, small, administrative data sets. As far as recursion in OLAP is concerned, if it's used properly, has major gains compared to self joins, blah blah. You understand this. Unfortunately, it's not as cut and dry as an EXISTS vs IN.
Thanks for the clarification. I absolutely agree, breaking down queries arbitrary into CTE's only makes it harder for the query optimiser to find a good solution. As you said - the only way you can be sure is to peek at the plan :-)
Thanks for the clarification. I absolutely agree, breaking down queries arbitrary into CTE's only makes it harder for the query optimiser to find a good solution. As you said - the only way you can be sure is to peek at the plan :-)
Space, nearly running out of space.
It’s simple, it’s all about how you are going to use the data inside them. Use a CTE when you want to reuse the results of a subquery multiple times in the same query. Use a temp table when you want to reuse the results of a (sub)query multiple times in different queries. If you’re not going to use the results multiple times then you’re probably just trying to trick the optimizer into giving you an execution plan. I would imagine there are better ways to do this like making sure you have representative statistics. Within a query, the performance between reading rows from a CTE (after you’ve already materialized them) and reading rows from a temp table is going to be negligible. If you feel like indexing your temp tables then ask yourself why you put all that data in the table if you only want to use a bit of it. Of course there are a few suitable uses but they’re not that common.
Curious; How big is your tempdb? How many files? Flash? Separate disk? How many executions/s? :/
Humbly stating, a lot of developers see a shiny function work for an edge case and in out of habit keep running with it without truly understanding under the hood. That's why you are employed in your position. :P
Would this work when I am already using a Where statement? I am on my way home now, I will show you my query once I get home
This sounds like a good example of the old "just because I haven't seen it doesn't mean it doesn't exist". I have not seen major performance hits from CTE's yet, but it's probably just because of the circles I play in. It's good to have the reminder to watch out for it. @TableVariables def have their place - persisting data across rollbacks, small datasets, etc. I just don't use them much myself.
Ahh, thanks for your thoughts, that’s a nice succinct conclusion that hadn’t crossed my mind. In particular: &gt; Use a temp table when you want to reuse the results of a (sub)query multiple times in different queries. The performance-crashing CTE’s that I turned into tempTables fit this bill perfectly.
Agreed. That's the wonderful data world we live in. =\] I meant, CTE's have positive performance implications if used appropriately over the other tool set which could be used. Cheers m8.
I'm curious - when you rewrote that query did you see the same issues of the query spilling over into tempdb? Or did you get a better query plan/memory allocation and avoid the problem?
I think we are all guilty of chasing shiny things (At least I am). Imo it's about maintaining the curiosity/professionalism to ensure the shiny toys work for the general case. :-)
If a CTE spills over into tempdb there's a myriad of reasons which could apply. Is that what you're seeing or what you are cleaning up by not using CTE's?
Guilty of chasing, yes. However, there's a massive difference between chasing and understanding. Case in point, SQL 2019 and how functions are treated. : P Stay hungry bro.
I thought about using because I'm doing this a school project. We did not study about agent job at all. So, I don't think i'm allowed to use. I actually thought about the trigger at first place because i'm required to. Alternatively, I could do it as a query WHERE expire date &lt;= date. But I thought there is some way to do it as a trigger. Thanks for your help and information. I really appreciate that.
\^ solid input
That sounds like a good match for a temporary table. I guess in the bad performance case you were going through the work of executing a slow query to materialize the CTE multiple times so you were doing slow things over and over; in the temptable case you just do it once. Improving performance is all about two things: Do slow things less If you can’t do the thing less, do it faster (which means finding out what’s involved in the process and doing that less (or faster))
Here is my query: &amp;#x200B; \` \--Testing purpose, allows for rerunning of query without duplicate error delete tbNewBerthingOne; delete tbNewBerthingTwo; delete tbNewBerthingThree; &amp;#x200B; \--populating berthing one insert into tbNewBerthingOne (Id,RateAndRank, FirstName,LastName,Rack,Comments) select top 2 \* from MasterRooster where RateAndRank = 'FCA' order by newid(); &amp;#x200B; insert into tbNewBerthingOne (Id,RateAndRank, FirstName,LastName,Rack,Comments) select top 2 \* from MasterRooster where RateAndRank = 'HM' order by newid(); &amp;#x200B; insert into tbNewBerthingOne (Id,RateAndRank, FirstName,LastName,Rack,Comments) select top 2 \* from MasterRooster where RateAndRank = 'CS' order by newid(); &amp;#x200B; insert into tbNewBerthingOne (Id,RateAndRank, FirstName,LastName,Rack,Comments) select top 2 \* from MasterRooster where RateAndRank = 'GSE' order by newid(); &amp;#x200B; insert into tbNewBerthingOne (Id,RateAndRank, FirstName,LastName,Rack,Comments) select top 2 \* from MasterRooster where RateAndRank = 'MM' order by newid(); &amp;#x200B; insert into tbNewBerthingOne (Id,RateAndRank, FirstName,LastName,Rack,Comments) select top 2 \* from MasterRooster where RateAndRank = 'STG' order by newid(); &amp;#x200B; \--populating berthing two insert into tbNewBerthingTwo(Id,RateAndRank, FirstName,LastName,Rack,Comments) select top 2 \* from MasterRooster where RateAndRank = 'FCA' order by newid(); &amp;#x200B; insert into tbNewBerthingTwo (Id,RateAndRank, FirstName,LastName,Rack,Comments) select top 2 \* from MasterRooster where RateAndRank = 'HM' order by newid(); &amp;#x200B; insert into tbNewBerthingTwo (Id,RateAndRank, FirstName,LastName,Rack,Comments) select top 2 \* from MasterRooster where RateAndRank = 'CS' order by newid(); &amp;#x200B; insert into tbNewBerthingTwo (Id,RateAndRank, FirstName,LastName,Rack,Comments) select top 2 \* from MasterRooster where RateAndRank = 'GSE' order by newid(); &amp;#x200B; insert into tbNewBerthingTwo (Id,RateAndRank, FirstName,LastName,Rack,Comments) select top 2 \* from MasterRooster where RateAndRank = 'MM' order by newid(); &amp;#x200B; insert into tbNewBerthingTwo (Id,RateAndRank, FirstName,LastName,Rack,Comments) select top 2 \* from MasterRooster where RateAndRank = 'STG' order by newid(); &amp;#x200B; \--populating berthing 3 insert into tbNewBerthingThree (Id,RateAndRank, FirstName,LastName,Rack,Comments) select top 2 \* from MasterRooster where RateAndRank = 'FCA' order by newid(); &amp;#x200B; insert into tbNewBerthingThree (Id,RateAndRank, FirstName,LastName,Rack,Comments) select top 2 \* from MasterRooster where RateAndRank = 'HM' order by newid(); &amp;#x200B; insert into tbNewBerthingThree (Id,RateAndRank, FirstName,LastName,Rack,Comments) select top 2 \* from MasterRooster where RateAndRank = 'CS' order by newid(); &amp;#x200B; insert into tbNewBerthingThree (Id,RateAndRank, FirstName,LastName,Rack,Comments) select top 2 \* from MasterRooster where RateAndRank = 'GSE' order by newid(); &amp;#x200B; insert into tbNewBerthingThree (Id,RateAndRank, FirstName,LastName,Rack,Comments) select top 2 \* from MasterRooster where RateAndRank = 'MM' order by newid(); &amp;#x200B; insert into tbNewBerthingThree (Id,RateAndRank, FirstName,LastName,Rack,Comments) select top 2 \* from MasterRooster where RateAndRank = 'STG' order by newid(); &amp;#x200B; select \* from tbNewBerthingOne; &amp;#x200B; select \* from tbNewBerthingTwo; \` &amp;#x200B; select \* from tbNewBerthingThree;
Side question, but CTE related, is it possible to save the records you delete from a CTE into another table? For example of my final query command was "delete * from CTE where dupe_count &gt; 1", does anyone know an easy way to also insert the dupes I just deleted into a side table I can keep a record?
For me it is more intuitive to write a complex query using a "heroic" approach, and it might run just fine for awhile, but as the data grows, more dimensions are added to the logic, etc., suddenly you will hit this performance wall where you need to strip it down into segments if you want it to keep performing as expected.
Something like this: insert into tbNewBerthingThree (Id,RateAndRank, FirstName,LastName,Rack,Comments) select top 2 * from MasterRooster where RateAndRank = 'GSE' **AND [id] % 3 = 0** order by newid();
I will test this out! Thank you so much for the help and giving me a new operator to read more into (%)
And also one last question, would I use the % 3 = 0 for each statement?
Pluralsight has a video that covers some of that. It is an ok startet.
&gt; I think the parenthesis is whats wrong with 6 nope, it's the fact that the USING column is qualified the correct syntax is `USING COURSE_ID` because that column exists in both tables that said, using `USING` is not best practice
I personally only really use a CTE when I need to reference the same data a bunch of times and the original query is fairly long and complicated (but it ISNT returning tons of data). I’m just a DBA though, I don’t write code for apps or anything.
What exactly is the assignment? Can you alter the table? If so, you may be able to make EMP_IDRENEEALFLAG a computed column with a CASE statement that checks the date field.
Damn that make sense why i was lost, I think we learn about the using clause or whatnot next week. The week prior there was a join question and we learned about it this week and i was confussed then so i just winged it..ended up being close to the syntax just frustrating going over your chapters and thinking your dumb for not finding why your doing something wrong...then ends up your not it wasnt even covered lol
Is that is how you want to filter it. For every query being inserted into that particular table. So for table 1 would use %3 = 0, for all the inserts for table 2, use %3 = 1, and for table 3 you use the last. If you think about what % 3 is doing, it is essentially seperating your original data into 3 equal size groups without allowing any single record to change groups. X % 3 = 0 is always true for any number that is evenly divisible by 3.
This helps me out a ton. For SQL seeming pretty simple and straight forward it does have a lot of functionality.
I tend to write with CTEs so I don't have to worry about definitions. Once I've got what I want I'll do a few passes to tidy things up which includes working out what I want to turn to temp tables. Generally speaking I like to to put the underlying data I'm going to be working with into temp tables and leave CTEs for logically breaking down the processing. I think the only real reason to use table variables over either is being able to pre define them as database objects. This means you can do things like then pass them to spocs.
CTEs are really just subqueries that you can reference multiple times, can reference each other, or themselves. If you need recursion, you can in principle achieve that with temp tables (or table variables) and loops. CTEs help to keep large complex queries clean, and with good style, a huge query can be plenty readable. The big differences in performance can only be proven by testing (intuition can help, but mine is rarely perfect). _Sometimes_ a CTE or subquery lets the database engine do optimizations it couldn't do if you force it to separate steps with a temp table. _Other times_ building an intermediate result is what allows the engine to optimize each part much better. The differences can be dramatic from one case to the next. Some of the biggest differences are when an index on an intermediate step dramatically speeds up performance. Maybe some day CTEs and subqeuries will allow hints to effectively create temp tables with specific indexes. That would be cool. In other cases, the same temp table result is used in multiple following queries. In that case, it's usually beneficial over CTEs if the cost of generating that intermediate result is inherently expensive and the follow-up queries don't give SQL any way to make it a lot cheaper [of every row is needed and the subquery can't short-circuit which records are in that intermediate result until its results are fully known].
but I don’t think you‘ll be displaying data from tableA though. I think this would only display the first column from the dual table whenever the tableA conditions are met. I haven’t used EXISTS myself but my understanding of it is it just evaluates a boolean that tells the outer table which records to look at.
doesn’t SELECT 1 just display the number 1? I think your outer query will just show the number 1 repeatedly based on how many times the subquery evaluates to TRUE. Where did you get this query?
Happy to help. Just a note, the % (modulo) operator is available in every programming language I have seen. It can be very useful for certain thinks, like detwrmining if a number is even or odd, trimming getting the right most digits from a number, etc.
Honestly, I'd consider doing your scripts in a stored proc in the DB. One per step. Then use SSIS to call each as you go through the flow. Easier to maintain. As far as the C#, I sadly don't have much insight there :(
yes I can alter the table, but what is the point? I looked quickly about case statement, and it seems to be used in queries only. Can you explain more? or give an example how to use in in the table instead of query
In some cases you can create indexes on temp tables and improve performance. Table variables are sometimes faster than temp tables because of red
IIRC table variables can have nonclustered inline indexes starting in MS SQL 2016.
I mean, you just want the one column from tableA right? `Select COL FROM tableA where Col1 = 5000 and col 2 &gt; ? and col3 &gt; ? and col4 &lt;= ?` or are you looking for: `SELECT Col1 FROM tableA WHERE Col1 = 5000` `UNION` `SELECT Col2 FROM tableA where Col2 &lt; ?` `UNION` `SELECT Col3 FROM tableA WHERE Col3 &lt; ?` `UNION` `SELECT Col4 FROM tableA WHERE Col4 &lt;= ?`
This query tests for existence of any record that fits the criteria in tableA where col1 = 5000, col2 &gt; some parameter, col3 &gt; some other parameter, and col4 &lt;= some other other parameter. &amp;#x200B; If there are 1 or more rows in tableA that meet those criteria, you will get a single "1" returned. If there are 0 rows in tableA that match, you will get a NULL/no result. I think this is generally referred to as a LEFT SEMI JOIN.
I’m sorry, but you can do this?
Yea. See: https://docs.microsoft.com/en-us/sql/tutorials/task-14-add-execute-to-control-flow-run-mds-stored-procedure?view=sql-server-2014 Note their demo includes some additional items such as creating variables within the SSIS package and passing them as the variables, making the assumption the procs have variables. If they don't then just use script as "exec schema.procName" without the variables bit. At my last job I finished up half the SQL coding my boss had started about 18mo before my time, and then ended up building nightly automation using SSIS to trigger all the stored procs. Basically it'd refresh the financial data from the Dynamics GP instance we had and it resulted in the new financial reporting platform. Also used stored procs and SSIS ETLs to automate the nightly dataloads from production to the finance environment. About 70ish tables, GBs of data. After the import, we'd run through nearly 70 steps that mostly used stored procs for each portion. For data enrichment purposes and turning it into "final usable data" for multiple sub sections of the finance world (was a wholesale travel co. So hotels vs airfare vs cruise, hotels broken down further by traditional vs prepaid, etc.). The process originally lived in the SQL Agent for nightly automation. One of my last things before leaving was to migrate it all to SSIS, including the automatic scheduling of SSRS reports (instead of using the SSRS built in scheduler). This way everything was within a single place, backed up in TFS to multiple places for redundant data protection (on site, off site tape and cloud).
Main difference is CTE's are not materialised and temp tables are. If you are referencing complex CTE multiple times, you are doing the query multiple times. Temp tables are materialised and have statistics so the optimizer knows the number of rows you are working with. So if you have bad estimates for whatever reason, you ca write part of the query to temp table and have acurate estimayes. Otherwise the bad estimates just snowball
If you process data in C#, it's going to be very slow. I'll explain: A while back I needed to get some data in and out of MSSQL on a Linux server. Had to be done fast, in large volumes. We have been using sqsh for out, but it was old and unmaintained. I thought "C# is fast, mono and .net core work in Linux, let me make a very efficient, ADO.NET only (the lowest level database interface on .NET afaik) data mover. So I did. Minimal instructions, minimal code, everything as fast as possible. It was around 5 times slower than sqsh. In fact, getting data from sqsh and piping it (stdio pipes) to a Python script that process it was still much faster than getting the data using ADO.NET. If you convert your data into objects at any point, you're going to have a severe slow-down of the process. Use SSIS for orchestration only, if speed is what you need. Otherwise use SQL and T-SQL as much as possible.
Can you get fucked.
He is only trying to help Chill!
Google search - computed column case statement
Like the parent comment said, triggers are triggered when the data in the table changes. So in this case I am assuming that a process will update the EMP_IDEXPDATE and this will cause your EMP_IDRENEWALFLAG flag to be set to 1. This way you are not waiting for 05/05/2019 to update the flag, you set it right away when the data in the EMP_IDEXPDATE column changes. That's the definition of a trigger. If there is no EMP_IDEXPDATE then the renewal flag should be 0.
Looking for another job is for sure an option, but if you want to keep your job I would play along. Let your job keep the credit -- part of playing politics is impressing the people that need to be impressed. And a part of unspoken job duties is to make your manager look good. Try separating your emotions from this and wait for another opportunity to shine in front of those who are able to give you a promotion. Aside from this if you have a department that is doing Business Intelligence I would start trying to move closer to them and seeing if they have an open position that you could move into. You could couch it as you needing more technical leadership than your current manager can give.
This is terrific advice, thank you. I don’t want to leave the company since there are a lot of pros that outweigh the negatives I described. I don’t want to leave over just 1 thing that’s annoying instead of parlaying this into something that would benefit me, and your long term strategy is a good one. There is a new operations manager that’s interested in BI and the CFO wants to improve our BI tools as well, so maybe at some point I can ask to work under someone else. (Indeed I had a text exchange with my manager and he said If you don’t like the way I operate you can find someone else to work for. Implying work at another company or another manager I’m not sure but now I need to improve my skill set to be a more useful asset to the operations controller and CFO) There is so much BI tools out there I’m not even sure where to start
I use CTEs for readability and reuse. If the query is slow, I would then think of using temp tables. Also, if you want to create a view, you can use CTEs, but not temp tables. My opinion is that CTEs and temp tables cover entirely different use cases. Temp tables add some complexity, but can dramatically improve performance in some cases. CTEs simply make SQL easier to read, write and debug. They can also allow you to reuse the same code in your query. So, it's not really CTE vs temp tables.
Ok so I might be obtuse here but am not sure that I get your analogy. Are you saying that was the only thing in the sproc?
Indexes. You can create them specifically with Temp Table, for what you need later in the code. CTEs you will depend on existing, underlying indexes on the tables.
Are you the new /u/Vidyakant ?
lol, no it was part of a larger query looking for looking up a credential based on a certain sequence of numbers with an unknown prefix and suffix (thanks FMCSA, your the best). When you put a wildcard at the start of a search string, you guarantee a table scan, it's one of the seven deadly sins of writing queries (number 2 after cursors). I had to explain what a SARG was to the contractor, explain sql string functions, and why his query would have killed the DB. In the end, I rewrote the whole mess during the code review using charindex, and it ended up literally being 10,000 times faster than his query. As for your situation I see their rollback plan was "Pay us more money", I really hate contractors for that, they are selling a service, and they would sure like you to continue using their service.
Oh I know how indexes and search arguments work. Sometimes you do need to do a wildcard search. But yeah, if you're paying a contractor and they are asking you to review their work for basic performance issues like that...
Technically, what you're describing is this: SELECT* FROM Contact WHERE klantid = 9656631 AND typebericht = 'false' and contact_id in ( select contact_id from Contact group by contact_id having count(contact_id)=1) ORDER BY status ASC, datum DESC; But that's kind of ridiculous, unless it's a data quality query? If you literally want a unique list of contact_id values, then this will get you it: SELECT contact_id FROM Contact WHERE klantid = 9656631 AND typebericht = 'false' GROUP BY contact_id; If you want to include other columns other than contact_id, remember that the table has multiple rows for every unique contact.id. So you have to tell SQL which value you want to select out of all those possible values. min(id), max(id), etc. The error message is telling you that you need to "aggregate" the column "id" if you want to return it. https://www.w3schools.com/sql/sql_groupby.asp Good luck!
This will return only unique values from the \[contactid\] column. This, of course, will only return results based on the WHERE filter: `SELECT DISTINCT contactid FROM contact WHERE klantid = 9656631 AND typebericht = 'false'`
You are grouping by home team id, not home team.
You need to put your case logic inside of your group by as well. SELECT CASE WHEN hometeam_id = 10189 THEN 'FC Schalke 04' WHEN hometeam_id = 9823 THEN 'FC Bayern Munich' ELSE 'Other' END AS home_team, COUNT(id) AS total_matches FROM matches_germany GROUP BY CASE WHEN hometeam_id = 10189 THEN 'FC Schalke 04' WHEN hometeam_id = 9823 THEN 'FC Bayern Munich' ELSE 'Other' END AS home_team,
yes, GROUP BY is performed first in your case, it's an alias, so the parser has to look into the SELECT clause for the definition to do the grouping imagine another query -- SELECT SUM(thing) FROM ... GROUP BY company_id here again, grouping happens first... and then the SELECT decides not to show the grouping column -- this is valid (but not useful)
I'm sorry, I edited my post. I currently have 3 rows in it, and it should return 2. After using my own statement I have: row1 : contactid=2 row2: contactid=1 row3: contactid=2 &amp;#x200B; I want it to show both row 1 and 2, filtering out the second \[contactid=2\]. So basically I want the same result as your second query, but with all columns (and sorting them first so I dont filter out the wrong row).
surely this is wrong? This will give you a grouping of every seperate team, he wants to have a grouping of A or B or Everything else. This logic will just give you a count of every unique team
Thanks for your help. Apparently it only work with SQL server. I tried to use it in MySQL but no luck However, I really appreciate your help. I decided to change the table structure so I don't have that attribute.
Some SQL variants e.g MySQL don't follow the traditional rule, and let you group by the column aliases rather than the full expression - the other commenters are correct in that this is not quite the same and will throw an error in some SQL engines.
Or just use Adam Mechanics sp_WhoIsActive like everyone else 😁
&gt; unique team Yes, I agree it's wrong. I was just calling out why it was working fine.
This is a good tutorial for beginners - https://creately.com/blog/diagrams/er-diagrams-tutorial/ Good luck!
This is interesting because I never understood why I could not reference select aliases in the group-by. I used an inline view, and that allowed me to reference the aliases. However, now I know why. Thanks!
That’s how I would do it
Do you always want to default to the largest available label? How do you tell if an item is within a series of containers? My first impression would be to create a table with only the largest label within each series, basically your query WHERE rank = 1. The columns would be Series_id, Series_default_label. Then I would join the item table to the default_label table on Series_id, and Coalesce(LabelCode, Series_default_label) to fill in the nulls with the default. Please let me know if I misunderstood your database structure.
That's the plan. There are bound to be some fringe cases, but in 98% biggest label goes to biggest container. I'll try out coalesce. Thanks! Headed home now will try it out tomorrow.
Thanks! I saw their video tutorial, but hadn't seen that one. Any recommendations on like... Practice examples? I've got the basics down, but implementing it seems to be my downfall
I just found this treasure trove of real-world examples. It may help you out. http://www.databaseanswers.org/data_models/index.htm
If every TableC row references one TableB row, then TableB is the appropriate reference target in my opinion. I don't think it violates any normal form rule.
That is the case, yep. Thanks for some confirmation!
Haha ya I remember the first time I learned this and I felt like the whole world made sense to me for a brief moment.
Came to recommend this
Thanks for sharing [SQL Tutorial](https://www.sql-tutorial.online/)
&gt; (Indeed I had a text exchange with my manager and he said If you don’t like the way I operate you can find someone else to work for. Implying work at another company or another manager I’m not sure but now I need to improve my skill set to be a more useful asset to the operations controller and CFO) Not gonna lie, that sounds really bad. I would make moves sooner than later, or at least go back to him with some words to put some oil on these troubled waters.
Yes it does
I think a good starting point is to identify the kind of role(s) you might want and look at the job descriptions and requirements for any jobs you find posted. Most, if not all, of the major database engines will run on Linux. For your purposes you might want to start with SQLite.
From a development point of view I think most authoring of SQL is within a web stack, though I'm just one guy in a small team. Microsoft seems to be making headway here, even on Linux, so grab MS SQL Server Express, and / or MySQL (AKA Maria DB). SQL is a language unto itself, especially T-SQL. Typically a query would be part of a regularly run report or the back end of a custom web page, and that's developer territory, but Excel is more of an ad hoc reporting and data visualizing tool for one-offs. Honestly I'd look at the job openings I was aiming for and focus based on their buzzwords. The HR person would be looking for the *exact* phrase as handed to them by the department or team lead.
With Linux you could install MySQL, MariaDB, PostgreSQL, or SQL Server (not absolutely sure Microsoft offers the Express free version for Linux) After installing one of those - start working through all the basics of SQL. I would anticipate that if you spent about 100 hours systematically working through tutorials you would probably bring your skills up to a place where you could start talking to companies about an entry level job using SQL.
Coalesce did it. I had to do some weird hanking around with my sequences but after scrolling through about 120 lines of 234, it appears to be working properly. I put the code i used in the OP.
I've been struggling to earn a job as a data analyst. What can I do?
&gt; is there an easy way for me to do this? yes, use MAX() functions SELECT studnum , MAX("semester one") AS semester_one , MAX("semester two") AS semester_two , ... FROM a_table_like_this GROUP BY studnum
wow, holy shit! that was fast. thanks a lot buddy!
Dang this will take some time to figure out, challenge accepted haha thank you so much man, hopefully this works on MySQL
Can you backup the db and restore it in dev where you have write access to learn?
Thank you so much! That was very helpful! Have a great rest of your week
 [https://www.codingblocks.net/programming/database-schema-for-multiple-types-of-products/](https://www.codingblocks.net/programming/database-schema-for-multiple-types-of-products/) &amp;#x200B; This post goes into pretty good details on the basic schema and a few variances and what the pros/cons of each are. &amp;#x200B; You could also do an entity-attribute-value model ( [https://en.wikipedia.org/wiki/Entity%E2%80%93attribute%E2%80%93value\_model](https://en.wikipedia.org/wiki/Entity%E2%80%93attribute%E2%80%93value_model) ). Just keep in mind that this way will require you to do validation / checks on the application / client end instead of relying on the database for this.
Dynamic SQL? Essentially writing queries as strings spliced with variables then executing them.
I just got hired with a bachelor's in computer science, but my SQL skills were **weak**. I had taken two databases courses and learned next to nothing other than how to get by. I didn't know what a join was, functionally, after Advanced Databases. So, in preparation for a job interview, I did the SQL problems on HackerRank. After two days with this, I had my interview and it was the SQL portion that got me the job. I'd recommend simply attacking some SQL problems head on, it's just the fastest way to learn. As for Excel, I'm useless. I sit next to an MIS grad, so I bother him.
If you want to learn Linux and SQL, by far the easiest way is via Docker. It will allow you to run basically all major Relational DBs for free, and with very little resource requirements. (Plus, Docker skills are also in vogue right now, and getting comfortable with Docker will basically open doors to getting your way around Linux) I would suggest you spent some time looking into the basics of Docker, and once you get how docker-compose works, write a compose file that will launch the following: MySQL, Postgres, and SQLServer, and ALSO - SQLpad. SQLpad should be able to connect to all the above DBs and you can practice there. (The SQL variants required for each DB is basically the same, but have some syntactic differences, and some DBs will have additional features compared to the rest) &amp;#x200B; ============================================================================== TBH tho, im not sure how far SQL skills alone will get you, maybe also do some basic programming / scripting courses, since SQL is usually used in conjunction with at least some programming.
I know that too, what I really want to know is how to do it execute in the most efficient way. I don't really understand when I execute an explain statement though.
The best way to learn - use it! 😉 Download firebird and any *.gdb database and try to learn couple of selects
Agreed! I second this.
Agreed! I second this.
Sorry not a suggestion, but I have been reading his (Markus Winand) book: “SQL Performance Explained” and really enjoy it. I like his website too: https://modern-sql.com/. He does a great job explaining pivots.
Sometimes your results WILL be exactly the same if you use an inner vs a left but not always. Inner joins are only used when you only want records from both tables that match. Left join will be used if you want ALL records from one table and only records from the other table that match. For instance, say I have a table for commission employees and a table for items sold. If I do an inner join based on an employee id, i would only get items sold that have a commission employee linked to them. Now, if you only get commissions on certain items, all of the other items sold would not show up, so if you were to report on total sales, you wouldn't be returning the true number of items sold. A left join would return ALL sale items, not just the ones with commission employee tied to it. Hope that makes sense!
&gt; use left join say over right join? you wouldn't -- they are exactly the same, with the tables mentioned in the opposite order an outer join is used when you want **all rows from one table, *with or without* matching rows from the other** left and right are exactly the same
Looks like your SplitToInt is already a table-valued funciton, so it's trivial to make join. But usually it makes no difference if list is short enough. I had a lot more problems with cases where number of parameters passed changes significantly, so it's better to use some other query plan, like merge joins instead of nested loops or vice versa, but SqlServer remembers only the first one without hacks like option recompile.
Thanks so much for this real-world example! That was helpful. It seems that I just need to know my dataset really well in order to decide whch join is needed!
Thanks for the reply! Yeah, the list parameters are typically not used at all and when they are there are only a few values so this seems the cleanest and easiest to maintain.
Edx has some really good t.sql courses on querying and database design. All free with videos, tutorials, and tests. That should give you the base knowledge.
Having a separate table for each item type's custom attributes has the advantage that you don't need to change any existing tables if you later want to add a new item type. If your application uses any kind of object relational mapping this structure is also helpful in that it mirrors inheritance from a base class.
Thank you! I'm going (hopefully) from jobs that had occasional SQL needs to one that is all day everyday SQL. I appreciate gems like this.
Lookup the substring function for item 1. Depending on what database you are using you may have other text manipulation functions available but substring is in pretty much all of them I think. This will allow you do isolate string starting with x. Add that to your where clause and you’re good there.
w3schools has come decent introductory tutorials.
Put any aggregate functions you would like to put into the where clause into a having clause after the group/order clauses
Pardon my idiotic statement but. What? Do you mean I have to write it as SELECT CUS_NAME, INV_NUM, INV_TOTAL FROM CUSTOMER, INVOICE WHERE CUSTOMER.CUS_ID = INVOICE.CUS_ID GROUP BY CUSTOMER.CUS_ID HAVING INV_TOTAL &gt; AVG(INV_TOTAL); ? Sorry if that's not what you meant, still learning this stuff.
No not exactly. Let me run a test in a db though before I answer because you're actually trying to do something I'm not sure will work the way I meant.
Lol thank you so much for this. If you need any data from me let me know I can copy and paste a dummy SQL file into a paste bin.
Treehouse is good IMO. Subscription is usually free with a public library card. HackerRank.com is good for practicing SQL once you understand the basic select, from, where statements.
TrevBales, you got a Happy Number in your comment ID! The Happy Number is 68, and your comment ID was ekzmo68. Here's a link to what Happy Numbers are: https://en.wikipedia.org/wiki/Happy_number. The comment ID is a unique 7 character string which identifies your comment in the sea of Reddit. (I'm a bot by the way, downvote to delete this comment)
So what I meant was &gt; SELECT CUS_NAME, INV_NUM, INV_TOTAL FROM CUSTOMER, INVOICE WHERE CUSTOMER.CUS_ID = INVOICE.CUS_ID GROUP BY CUS_NAME, INV_NUM, INV_TOTAL ORDER BY CUSTOMER.CUS_ID HAVING INV_TOTAL &gt; AVG(INV_TOTAL) Because you need to have anything in the select in the group by. However this doesn't really get what you want because then you're grouping by everything and so nothing will ever be greater than the average of the group. What you really want is: SELECT CUS_NAME, INV_NUM, INV_TOTAL FROM CUSTOMER, INVOICE WHERE CUSTOMER.CUS_ID = INVOICE.CUS_ID AND INV_TOTAL &gt; (SELECT AVG(INV_TOTAL) FROM INVOICE) ORDER BY CUSTOMER.CUS_ID
Only through doing it can you spend the hours of soul sucking bug finding that is necessary to lose your fucking mind. And only then will you learn. Found my fucking bug though so I for that going for me.
That's pretty much it. You just need to understand exactly what data you need. Good luck, man!
Depends on the flavor you want to learn? Microsoft sql? Head over to Brentozar.com and start at the beginning of the blog and read.
Firebird 🤮
&gt; left join say over right join They are the same thing. SELECT A.X, B.Y FROM A LEFT JOIN B ON A.X = B.X is the exact same thing as: SELECT A.X, B.Y FROM B RIGHT JOIN A ON B.X = A.X LEFT JOIN returns the rows where there is at least something in the left side. RIGHT JOIN returns the rows where there is at least something in the right side. Since in the two examples these are both A, it does the same thing. If B.X does not equal A.X, the values from B are returned as a single null row. If B.X does equal A.X, the values returned are as many rows from B that match A on X (can be more than one depending on what exactly you are joining on). INNER JOIN returns the rows where there is a match in both sides, meaning if A.X and B.X are not equal, nothing is returned. Why have both LEFT and RIGHT join at all then? Flexibility, and a lot times you need to combine those two things in a single query to get the right result. &gt; In my query answer, I used inner join and they used left join but the results were exactly the same The results very well may be exactly the same. That's not uncommon at all. Just looking at your query quickly the only way it would be different would be if there was a record in companies that did not have any matching permalink values in investments. You probably don't have any so the result is exactly the same.
You're the best. I got an *odd question* would you be willing to look over this document I have with queries and screenshots and see if I formed these right? If you decline, I understand.
I think it depends on your need. If you want to learn to get a good job like me then I will recommend you stratascratch. They have cool exercises with the questions from technical interviews. I found all the questions relevant to working on a job.
Nothing beats practicing it but to get a general idea of the concepts involved, there are some very great educational videos you can find on LinkedIn if you use your free month trial for Premium.
1) Accpac uses MS SQL as a back end. 2) involving MS Access to act as a middle man to SQL or having it involved in any sort of software dev is NEVER a good idea. Take it from someone who is stuck supporting a piece of 20 year old software that cannot have Access Extricated from it without 10's of thousands of lines of code breaking. There is not a day that does by where I don't wish I could go back in time and abort the parent of whomever built this monstrosity we're using. 3) Before you go in house or farmed out in house software have your boss look at the more mid/high sized company accounting software's already out there. For starters, any current and new staff will likely be familiar with the already established accounting software's out there and secondly despite the cost it will likely be a much much smaller cost than having your own software built. And it won't take years of development followed by years of debugging and user frustration.
This is a terrible idea, sorry. What a great way to set your company up for embezzlement and fraud.
Ah, it won't out of the box. MySQL doesn't support Common Table Expressions (the `WITH ... AS ()` parts). But, you can definitely use variables in place of what this is trying to do instead.
How so? What can an ERP do that we can’t to prevent fraud or error?
Don’t do it. Seriously, don’t. It will create an audit nightmare for your auditors (with an accompanying huge bill), take significantly more time and expense to build, and be a maintenance hassle down the line. /u/dzsquared also makes a good point - your home brew thing is ripe for fuckery, unless you have a super high level of accounting fraud expertise and can control against the various (dozens? hundreds?) of fraud modes that have been used against accounting software. Plus, the feature set sounds like Quickbooks 1987 - how are you going to manage AP and AR? Users with spreadsheets that they upload whenever they get to it? Sounds horrible. Are you going to build a check print module, or is AP breaking out their pens? Contracts? (Contract rev rec is probably your current pain point, right?) My guess is you’re severely underestimating the rule set that needs to be built, the type of challenges you’ll need to overcome, and just how fucked up your input will be if it relies on your clerks to upload original entry into your cobbled together thing. Effective accounting is significantly more than just record keeping. There are a thousand and one discussions on the interwebs and beyond on buy vs build for ERP, and the answer is always the same. Unless building and maintaining ERP software and infrastructure is your core competency, buy the damned thing and adjust your admin processes to fit. Would you write your own spreadsheet app to replace Excel? Signed, the friendly neighborhood CPA
This is just a bad idea. Period
Thank you for this message, for real. I didn’t know how to articulate my hesitations, but you’ve laid it out perfectly
Essentially this boils down to "I mean really what does Toyota know about making cars that I can't do myself? I bet I can whip one up in my garage with some of this steel pipe I got at the plumbing store?"
We have created a sql gui tool. It is called [ScaiPlatform](https://scaidata.com?utm_source=red&amp;utm_campaign=ps_r_sql_promo_q119), which can be used for free on [AWS](https://scaidata.com?utm_source=red&amp;utm_campaign=ps_r_sql_promo_q119), [Azure](https://azuremarketplace.microsoft.com/en-us/marketplace/apps/scaidata.scai_platform?tab=Overview&amp;ref=_ptnr_red_ps_r_sql_promo_q119), and [Google Cloud](https://console.cloud.google.com/marketplace/details/scaidata/scai-platform-2?utm_source=red&amp;utm_campaign=ps_r_sql_promo_q119). It is light-weight and it has built-in forms for tables that map to the table data types, you can add/delete/edit records and even load data without coding. You can create dashboards and graphs, reports through the UI and you can share these across the organization.
Why not SQLite?
I have considered this several times due to needing some pretty specific tools and about 11 different Enterprise systems. It almost never makes sense unless you have disposable cash and IT resources. Feel free to PM me, I'm sure I can recommend a scalable accounting package that is far superior then building your own and will allow you to focus your energy in your core business and helping your team achieve their goals.
technically, those are not duplicate rows, they're duplicate emails also, you're not deleting them, you're deleting all but one of them c'mon, man -- this is computer science we're talking about, you have to be precise
I probably could take a look tonight but if it's for today then I won't have time before the evening
Boeing seems to really has this airplane thing down, but I think I'll just make my own...
Do you... need a hug? Hang in there, buddy.
FYI there is an open source ERP called ERPNext, and it lets you build your own objects using its own internal build tools. It's more suited to manufacturing but its completely customizable.
If the key thing is being able to make custom SQL reports out of it, go with Microsoft Dynamics GP on premise. SQL Server back end. You can write everything in stored procs and then run SSRS reports. Also plenty of companies make plugins to add whatever functionality you need. Above all else, don't go home grown. Not only the items mentioned above but what if the company ever goes public? Or takes outside investors? The former will demand SOX compliance and auditing and the latter may do so as well. It'll be neadly impossible to implement after the fact
Ms access.... management goes why are we paying got an ep, IT MAKE us one.. IT says bad idea management make it any way. Company who builds it goes out of business and you have no one who can support.. sounds like a wild ride but this is exactly how it goes. You should tell your boss saving moneys great. Dealing with a self made piece of crap erp is never worth especially as your manager will be gone in a few years and it wont be their issur
I mean do they? Im certainly not going to be flying on a Max 8 if/when they resume service.
This is an absolutely terrible idea. Unless you intend to go into the business of selling such a product, buy one. Don't build.
Does your boss really think it's worth spending several person-years of effort to build a solution to a problem *someone else has already solved* when he could buy a turn-key products and implement it in less than 2 months?
I hate to say it but I actually full heartily believe that my office is having my team and I build a top level ERP system to work in conjunction with our facility level ERP. It’s a terrible idea and should never be done. Yes it can save the company a good chunk of change but there is way more important and impactful thing we could be doing instead.
&gt; Yes it can save the company a good chunk of change Can it though? Your time is not free! Your testers' time is not free! The time lost to documenting everything for auditor, either up front or during an actual audit because the company decided to write their own thing is not free! Let's say it costs $100K and 3 months to integrate an off-the-shelf system with your environment. And let's also say it'll take 6 months for a team of 4 internal people to release a minimally viable product. Which one is cheaper? The first one is, unless you start playing games with the accounting. &gt; but there is way more important and impactful thing we could be doing instead. And that's exactly it. [Opportunity cost.](https://en.wikipedia.org/wiki/Opportunity_cost) Not only are they spending money the wrong way, they're missing out on the better things you could be doing.
This should really cover the fact that each row has a unique primary key, without which, “duplicate” deletion would not be possible (at least, not without pulling data out and re-inserting it). Also, if you care about referential integrity, this is a super bad idea compared to “soft deletes”.
W3schools taught me everything I know, I hear code academy is the new w3. But yeah you gotta use otherwise you lose it
Don't do it. If you don't have a team of software engineers and dba's at the ready (who wouldn't even consider access) then you cannot do this cleanly. Look at turn key solutions.
Well, from the information given, it sounds like it's impossible. I wish I could be more help, but since you've already tried everything, that means there's nothing left. Or maybe you want to list what the "everything" you tried is, or maybe even give more details on what you mean by "bypass a sql server login".
I've made a whole career out of what I learned from the book *SQL Cookbook*
That sounds shady and probably illegal. If you need the data and justification, talk to the admin team for access. Don't get yourself in trouble, fired at best, for someone being to lazy to play the system.
&gt; I have a project to bybass a SQL Server login step away from the keyboard turn off the computer set your apartment on fire go outside and just keep walking never come near IT again
Like everyone has said, doing is key to learning; however, if you want some supplementary reading on some theory, two highly recommend reads are SQL Antipatterns: Avoiding the Pitfalls of Database Programming (Pragmatic Programmers) by Bill Karwin and Designing Data Intensive Applications by Martin Kleppmann. Both are pretty easy to find as pdfs online if you don't want to buy them I also like use-the-index-luke as an online resource on indexing and optimization. google it
The type of join used can also affect the query plan. Sometimes using a left join gives the optimizer more freedom and results in a more efficient plan and better performance due to the way the optimizer estimates things like the number of rows, I believe.
This is terrible idea unless you have a *very* industry specific need AND you have full team of programmers, dba's, and project managers to support you. It's already been created many times over so don't waste money recreating the wheel. Accounting is accounting.
He is not using VS 2017, the icon is wrong, [here](https://www.google.com/url?sa=i&amp;source=images&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwj55OzW99ThAhXHsFQKHf1mAjcQjRx6BAgBEAU&amp;url=https%3A%2F%2Fvisualstudiomagazine.com%2Farticles%2F2018%2F11%2F13%2Fnew-vs-icon.aspx&amp;psig=AOvVaw3bl115mdScHCUANuCd04Vk&amp;ust=1555515134707110) is the icon for VS 2017. That looks like the icon for VS 2010. With that said you have all of the same functionality. Though why someone would want to use the query designer is the real mystery,
This is a horrible idea. There are plenty of accounting solutions out there that will actually work. This won't.
So, this is literally right up my alley. I do ERP consulting for the construction industry. This is such a bad, bad, bad idea. You have no idea the complexity that goes into the business logic within a fully fledged ERP system. Cost Accounting can be done in Quickbooks or Excel, sure. But making it robust, scalable, and secure is an entirely different story. And if you plan on processing Payroll out of it... well, good luck? I don't mean to sound like rude, but we have a large development staff and this is literally our area of expertise and even we wouldn't attempt this. Toss me a private message if you'd like and we can connect outside of Reddit. We sell construction ERP software as well as implement, customize, and build data integrations for them.
Dynamics isn't stellar for the construction space. We've pulled a few clients off it to construction-centric ERP systems. It just requires too much customization to provide the cost accounting and operations pieces necessary.
Thank you! Here's a link. Please let me know if I did any of them wrong. https://docs.google.com/document/d/15iYkcdLf949MxY7Z0eljcPZJrtn0d_QDzArfiai780s/edit?usp=sharing
Just a few tables to upload data to... Sorry to say but your approach is naive. Building an accounting, erp, crm system is a LOT more complex and costly than you imagine.
This right here.
They have it *down*
As others said: terrible idea. but I am still curious why? 🙂 would you mind to describe the reasoning behind the decision? If you really insist on rolling out your own solution, at least don't start from scratch, there are several opensource ERPs already available, at least consider adapting one of them to your needs.
Which rbms are you working with?
You have a lot of extra, probably unnecessary steps there. Depending on what the the text file looks like, you can likely do this with a single query. SQL Server supports the ability (with an Access driver installed) to read and parse text and csv files. Take a look at the OpenRowset function: SELECT * FROM OpenRowset('MSDASQL', 'Driver={Microsoft Access Text Driver (*.txt, *.csv)};DefaultDir=C:\data;','select * from yourtextfile.txt')
SSMS
There's so many great products out there to handle ERP. Don't reinvent the wheel. Also this sounds like a legal nightmare.
Is this a more standard way of doing it? Is my way like, bad? Its easiest for me to think about I just dont want to get laughed at tbh.
Why not just import the csv directly into your db? Right click on the database &gt; tasks &gt; import data; run through the menu options.
Viewpoint? I just ask cuz of your post + username.
Here's my hot take: https://www.youtube.com/watch?v=31g0YE61PLQ
I used to work there. I'm now a Director at a third party consulting firm. But yes, we specialize in Viewpoint. This reddit account is not 100% anonymous, enough people that I've worked with in the past know it lol.
I wouldn't laugh at this as long as it works. However, if you're going to be working in this environment for a while, might as well learn how to do it with just SSMS.
This is a 1 time thing. Next week basically we have ssis tasks doing it automatically.
Really, If it's just a one-off request outside of (I assume) your development sprint, then it's probably fine to just get it done. Do you have the latitude to whip up a quick package in SSIS to import it, and just run it in Visual Studio? That's probably your cleanest option.
Oh, like you open a CSV in Excel and then add a column and use `=CONCAT()` to build an SQL query? Yeah, that's janky as hell, but for small sets of data (~100 rows max) it works just fine. The big worry is getting the quoting right, so check your data for stray `'` before doing it. I would still recommend loading the data to a staging table and verifying it before you insert it into the live table. If it's slightly larger sets of data, I typically write a one-off PowerShell script to handle it because then you can use parameters and such. Example: $CsvData = Import-Csv $CsvFile $SqlConnectionString = 'Data Source={0};Initial Catalog={1};Integrated Security=SSPI' -f $SqlServer, $SqlDatabase; $SqlQuery = 'INSERT INTO dbo.Employee (FirstName, LastName, Gender, BirthDate) VALUES (@FirstName, @LastName, @Gender, @BirthDate)' foreach ($Employee in $CsvData) { $SqlConnection = New-Object -TypeName System.Data.SqlClient.SqlConnection -ArgumentList $SqlConnectionString $SqlCommand = New-Object -TypeName System.Data.SqlClient.SqlCommand -ArgumentList $SqlQuery, $SqlConnection $SqlCommand.Parameters.Add('@FirstName', [System.Data.SqlDbType]::VarChar, 255).Value = $Employee.FirstName $SqlCommand.Parameters.Add('@LastName' , [System.Data.SqlDbType]::VarChar, 255).Value = $Employee.LastName $SqlCommand.Parameters.Add('@Gender' , [System.Data.SqlDbType]::VarChar, 1 ).Value = $Employee.Gender $SqlCommand.Parameters.Add('@BirthDate', [System.Data.SqlDbType]::DateTime ).Value = Get-Date -Date $Employee.BirthDate try { $SqlCommand.Open() $Result = $SqlCommand.ExecuteNonQuery() } catch { throw $_ } finally { $SqlCommand.Close() } } I'm using SQL Server, of course, so I can get away with using the built-in provider in .Net. Even here I'd recommend using a staging table.
Why not just use the import tool that comes with SQL Server?
If it's 1 time thing, no holds barred as long as it gets the job done :)
I've built insert statements from csv using Excel, and it's okay as long as you're mindful of your delimiters. Watch out for things like smart quotes vs dumb quotes, single quotes vs double quotes, and of course, commas. The best way is to use the native tool of your RDBMS to accomplish it. Chances are it's either MS or Oracle and so you should use SSMS or SQL Developer, respectively.
Fine for the basics, not fine if you want to know advanced features of a particular RDBMS. I have a copy of OReilly Oracle SQL*Plus from 1999, which "Includes Oracle 8i"! Excellent! For for learning newer product-specific features, you'll need something to fill the gap. But remember, SQL is 40+ years old and ANSI standards have not changed significantly in years, so a 2001 book is absolutely fine as anything in there can be applied to a current db, or at least at that level of book. It's not an advanced book, more of a hit-the-ground-running kind of book.
The basic elements of SQL have been the same for a very long time across pretty much every platform. If that book is the only resource you have available and you like it, then have at it; I doubt it will teach you anything that won't apply to current SQL implementations. That said, there are a ton of free resource online that will let you learn SQL hands-on (and lot of free e-books, articles, etc. if you prefer reading.) *edit: it might have old "oracle style" join syntax. When you get to the part about table joins, keep in mind that this style is out of date: select stuff from table1, table2 where .... This is the better way: select stuff from table1 join table2 on .... where ....
It's probably missing window functions which are hugely useful. But the information in there will still be valid. Except for maybe old style joins that has been mentioned in another post.
your foreign key needs to have same columns (at least the number) as your primary key. Visit PK is multi-column: CREATE TABLE VISIT( ..... PRIMARY KEY(VISIT_ID,PATIENT_ID,PROVIDER_ID,VISIT_START),
So to have Visit_ID as a foreign key, I have to bring over all the portions of its Primary Key in the Visit Table?
Is the "SQL In 10 Minutes" book? I have that one. I liked it a lot. It should make a perfectly good place to get started. The changes in SQL over the last decade or so have been in the more advanced features. And those features aren't so much "here's a better way to do process X" as they are "Here's how you can do something that wasn't even possible before unless you were either a genius or a masochist and sometimes not even then"
If you want to have a foreign key, you need to bring all the PK columns from the other table and declare your FK on all of these columns. BTW, why do you need all these columns in the Visit table PK?
Your statement helped me re-think my implementation. Now it's just the Visit_ID as the PK which made the other table really easy to implement. Thank you!
This is by far the simplest solution for a one off task
SQL hasnt changed much since it exists. However, nowadays that you have the internet unlike not so much in 2001, I suggest using the internet instead.
Reviewavg-contactsum Like math.
Two tips for you moving forward: 1) Add four spaces to a line in Reddit and it will make code like this: Example Look like this: Example Which is really helpful for sharing queries. 2) You can share mock table data by doing this: | ColName1 | ColName2 | Colname3 | | :--- | :--- | :--- | | bar | foo | NULL | | foo | bar | NULL | Just remove those four spaces and it will become this: | ColName1 | ColName2 | Colname3 | | :--- | :--- | :--- | | bar | foo | NULL | | foo | bar | NULL |
i always fail the tabular formatting somehow, get frustrated and go with four spaces/manual columns (because of the monospace font)
I made an Excel file that will generate the tabular formatting for me, but either one works.
Depends on what you're looking to learn. If you want the fundamentals of T-SQL as a querying language, then no, use it. Changes have occurred, but the fundamentals will be taught in that book still. If you're looking to learn SQL server administration, then don't bother with that.
Couldn't disagree more. SQL is a language in constant flux. And rightly so. The latest standard is called SQL:2016 for a reason. Drop by https://modern-sql.com to see how the language is alive and evolving. That being said: A book from 2001 can be quite useful to learn about SQL's core (which is stable, indeed). Don't be fooled to believe that all there's to it, though.
data anomalies, and removing them through normalization techniques have not changed that much, so the basics are still the same.
If you are interested in alternatives after reading some of the other comments: Personally, I found Wrox’s Beginning SQL Server 2012 Programming and Microsoft’s T-SQL Fundamentals, 3rd Edition very useful, even after I had been writing queries for 1 year and 3 years, respectively. I taught myself on the job, so the former book introduced me to some of the logic and reasoning behind the queries I was writing. Solidified the fundamentals. The latter book is comparatively more theoretical and technically rigorous but really helped with understanding what I was actually doing with the queries I wrote.
90% will be useful. 10% will be database and version specific.
SQL-as-a-"standard", of which there are several implementations ("products"), are all loosely-based on E.F. Codd's 1969 relational-algebra, which, as mentioned by others, has been slow-to-change, mostly tightened by the likes of Chris Date, Hugh Darwin, etc. Why? Because relation-algebra is based on set-theory, a well-defined-branch of mathematics, which generally stand-the-test-of-time. But, as a "SQL professional", you already knew that, right?
1 you selected from customer table instead of invoice table. 6e is missing sales person id 7 probably should include customer id (technically doesn't say to but there's no pt wo it) I've gotten through 9…not sure when this is due but I'll comment on rest if I have time
&gt;"Teach Yourself SQL in 10 Minutes" [Relevant](http://norvig.com/21-days.html)
You can add indexes to temp tables in mssql and dynamic SQL is not required...
Even if I use local temp tables without using a session ID to link them to a user wouldn't I still have to use dynamic SQL? The queries to populate the temp tables always error since I am referencing an object (temp table) that doesn't exist.
Ah so you cant keep it all in a single session? Hmm
Made the corrections you suggested other than 7, since it doesn't say to. Thank you for this, you're awesome. Good to know I didn't make many mistakes :D
can anyone help please?
Side question: Why the elimination of dynamic SQL? A lot of people seem to have this weird notion that dynamic SQL = bad because someone in the past abused it or broke something with it. Properly used, there's no reason to avoid it.
&gt;Are there other implications of using permanent tables this way? People create permanent staging tables for their ETL processes all the time - without session IDs. They're just tables you move data into and out of on a regular basis. It's fine if you manage them properly. Your data files might be a bit larger than you "need" because they'll reserve that "working" space but you'll have that same issue with the "temporary permanent" tables as well - just without a table persisted to disk when the job finishes. Negative implications of creating and then dropping these tables in the ETL job? If the job breaks/fails and you *don't* have a final cleanup step that everything falls through to, you end up with a bunch of tables kicking around the database.
That's a good point. Right now this thing is a gigantic single stored procedure and every one of the queries are made from concatenated strings with many of the table and column names coming from variables. Almost the entire thing is red. Exactly none of it is optimized either purposefully nor accidentally with execution plans. I suppose I am really looking for some best practices when it comes to staging data for reports. I'd prefer to use queries that can be optimized with an execution plan. So I don't want the "Select into #temptable" part of it to slow the main staging queries. These will involve millions of records.
Got it! Here's what you want: select * from Contact where contact_id in ( SELECT contact_id FROM Contact WHERE klantid = 9656631 AND typebericht = 'false' GROUP BY contact_id HAVING COUNT(contact_id)&gt;1 );
Do you not have an identifier other than the session id, like a user id, that you can add to the tables and then adjust your read queries to filter on that to segregate the data instead?
Using staging tables is normal. Just truncate them before you do the insert. Use that table for one process. Don't reuse tables.
That was an idea. To use permanent tables and key the data to the userid. But I am concerned about the impact to logging.
Thanks! I could use a transaction to ensure that the tables are always dropped. Is it cleaner to Drop the tables after every use or keep permanent tables and just delete everything from them?
Know that I don’t have much context so this may not work for you, but you could do this from powershell if wanted. I don’t know if you’re familiar with powershell but you could essentially make the query a string and replace the database name with a variable and iterate through the list of names for each database and run the query. You can also do this programmatically, you can look at the master table and pull out the database that aren’t the default ones and put those in an array then loop through. Just a thought, we used this technique quite a bit at the last place I worked at.
32000 lines done. Kill me. Boss loved it
I'd just keep the tables permanently and just truncate when done with them/before using them each time around.
Select * from N0
&gt; I have been tasked with eliminating as much dynamic SQL as possible from a legacy reporting tool and am looking for the best way to handle staging data for the reports. Is there a reason you're not building a data warehouse and building your reports from that? Can you use CUBE or ROLLUP? Are you building gigantic PIVOTs or something? &gt; Right now the system uses global temp tables that are named based on the session ID to prevent users from stepping on each other. Why not use actual temp tables? Why do they need to be global temp tables? Aren't you running one stored procedure for one report? You know temp tables survive the entire life of the connection and not just for the first procedure run? &gt; So I am considering using permanent tables keyed with the session ID. You mean like `@@SPID`? You know that gets reused, right, so you'll have to purge your data and manage things if a report dies in the middle? &gt; They can be properly indexed and constrained to improve performance but what will be impact be to logging? Yes, all that data moving around will absolutely be logged. Your transaction logs will reflect the millions of rows being generated. If you're using full recovery, your transaction log backups will be significantly larger and your application will share IO contention with the standard application's transaction log load. Additionally, you may run into waits and potentially deadlocks doing massive inserts on the same table since they all want to write to the same page. Indexes can make that contention worse, though it will probably be offset by the performance gains they provide.
Thanks!
I didn't write the original SP. I assume they used global temps for troubleshooting. There is a parameter that switches between global temps and local temps but a code edit from like 7 years ago set it to use globals all the time. They want me to eliminate as much dynamic SQL as possible. So I could use local temp tables but they require dynamic SQL when I reference them in the stored procedure. So I am leaning toward permanent tables. Just worried about the logging. Would putting the permanent staging tables in a separate DB with minimal logging help?
* dSQL Opens you up to injection if used inappropriately * Adds more complexity to code (hinders supportability/maintainability) * Another layer of error handling * Encourages bad practice (can't figure your query out? Use a cursor and dSQL) * Increases CPU and memory pressure (more contexts) I avoid it, unless it's the only option. I have written some rule engines that required a bit of dSQL. I've also had managerial tasks that needed it, esp. when unit testing. It's not that it's bad, it's just dangerous. It's easy to do things in dSQL, and there are even things that can *only* be done in it. The said, there's almost always a faster and non-dynamic way of doing the same things.
I’m with this guy. Source- my main job is creating ETLs. Lots of permanent “temp” tables. They really speed things up in larger datasets.
Might not help, but you could put these in an ssis package that has a package config pointing to a config file who’s location is based on an environment variable for the database you want. Put that config, and set it appropriately, on each database server. Running the package in dev? It hits the dev server. Running the package in prod? It hits the prod server. This sounds complicated but it is not. Google “package configuration using environment variable”
Is that you Bobby Tables? &amp;#x200B; Can you post the link to the page?
https://findthetoken.com/uNoFVM2Nwu/H36ImhAuc7.php
remove the database name it will be resolved to the current db: [dbo].GlobalProperty
Seems like you're looking for dynamic SQL, such as example B on this page: https://docs.microsoft.com/en-us/sql/relational-databases/system-stored-procedures/sp-executesql-transact-sql?view=sql-server
Look at [sp_executesql](https://docs.microsoft.com/en-us/sql/relational-databases/system-stored-procedures/sp-executesql-transact-sql?view=sql-server-2017) to generate your SQL on the fly. Also, this type of trickery is a possible SQL injection vector if not done carefully.
Yep sure will. Need payment in advance though.
Why from an outdated book when you have great online resources in 2019 e.g. sqlzoo, datacamp, hackerrank, stratascratch, leetcode, etc. Go for a better option like datacamp or stratascratch if you are a beginner.
You may want to consider https://github.com/PostgREST/postgrest
Use temp tables where you can and logical tables where you can't use temp. I'd stay away from global temp tables.
This. Having a three-part name is generally a bad practice (unless your specifically doing cross-database queries).
Did you even do basic search? First Google result for "postgres json" gives reference for built-in JSON functions that among others will turn rowset to json object. [https://www.postgresql.org/docs/current/functions-json.html](https://www.postgresql.org/docs/current/functions-json.html)
It's 10 minutes a lesson, as I explained in the post.
"SQL and Relational Theory", by Chris Date.
&gt; (hinders supportability/maintainability) I can't agree with this more. There's a project lead at work who decided to use dynamic SQL to run all the required stored procedures for certain user actions. They're not necessarily referenced in any config tables or anything because it scans the sys tables for SP names, so the end result is you have SPs which have no concrete indication of actually being used (not called from UI, not directly called from other SPs, etc), but you can't deprecate them because *maybe* they're called by this other script. Makes running a query for used objects when you're doing cleanup at the end of a release kind of sucky.
Your expected JSON structure is invalid, but assuming that you want an array of those elements, this can be done quite easily. If I understand you correctly, the following query returns the basic information you want: select wf.id, st.name, array_agg(p.name) as projects from workflow wf join status st on wf.status_id = st.id join area a on a.workflow_group_id = wf.group_id join project p on p.area_id = a.id and p.status_id = st.id group by wf.id, st.name; You can turn a complete row into a JSON object by using `to_jsonb()` and the aggregate that into a JSON array using jsonb\_agg(). To avoid re-typing the column names, you can wrap the above query into a derived table: select jsonb_agg(to_jsonb(t)) from ( select wf.id, st.name, array_agg(p.name) as projects from workflow wf join status st on wf.status_id = st.id join area a on a.workflow_group_id = wf.group_id join project p on p.area_id = a.id and p.status_id = st.id group by wf.id, st.name ) t; Online example: [https://rextester.com/MKV52182](https://rextester.com/MKV52182)
Why not use one of the OpenSource ERP solutions available at adjust them to your needs? From the top of my head, e.g. * [LedgerSMB](https://ledgersmb.org/) * [OFBiz](https://ofbiz.apache.org/) A very extensive list can be found on [Wikipedia](https://en.wikipedia.org/wiki/List_of_ERP_software_packages)
&gt;*have not changed significantly in years* I'd say the SQL standard *has* changed significantly since it was released in 1989: * window functions * common table expressions * recursive queries (using CTEs) * deferrable constraints * conditional aggregation using FILTER clause * ordered set functions ("WITHIN GROUP") * FETCH FIRST and OFFSET to limit the number of rows returned * lateral joins * XML specification (including XQuery and XMLTABLE) * JSON specification (as well as JSON Path and JSON Query) * Temporal tables and queries Most of the above should be in the toolset of a modern "SQL developer".
T-SQL Fundamentals by Itzik-Ben Gan
Sorry I don't have any book recommendations, I was a trial by fire guy learning on the job/side projects. The only book I read was this one https://pics.me.me/programmer-to-programmer-tm-professional-sol-server-2000-programming-robert-6327483.png back in 2001 &gt; The easy level stuff was, well, easy, but I'm having trouble jumping to that next level. I had to go try this... those first few were not only easy but repetitive... why even bother with so many changing the very basic of things. Are you using any other sites similar to Hackerrank? Their interview proposition stuff seems interesting
I really appreciate everyone's feed back and response to my question. Your suggestion of simply removing the database name did the trick. Can't believe I missed that. Thank you!
I don't know of a gold standard, but these books helped me. Database Design for Mere Mortals - Operational Design https://use-the-index-luke.com/ - Performance by Index Usage Data Warehouse Design - One of the Following The Data Warehouse Toolkit, 3rd Edition by Ralph Kimball Building the Data Warehouse 4th Edition by Inmon Operational Data Store Building the Operational Data Store, 2nd Edition 2nd Edition
Great question. You don't need to combine the IDs manually like that (which would raise the issue you mention). You can just create a composite primary key. Create table unit_equipment (unitid int, equipmentid int, quantity int, primary key (unitid, equipmentid)).
I had a similar use case recently as well utilizing first\_value/last\_value to get the most recent patient status. I had the same issue and while some times it's best not to rely on DISTINCT, it is a tool for a reason. Your only other option is to either utilize a CTE or another tempt table to group it by. Both are valid solutions, look at the execution plans for each and see which is more efficient. I do see a use case for OUTER APPLY here as well. &amp;#x200B; Here is a general application of it with your code and outer apply. This -should- run since its just adapted from some of my existing code &amp;#x200B; SELECT ca.Patient\_ID ,ca1.\[Diag\] INTO #PatientDiagnosis FROM #Cases ca JOIN Diagnosis di on ca.Patient\_ID = di.PatientID OUTER APPLY ( SELECT TOP 1 ca1.Patient\_ID ,FIRST\_VALUE(di1.Diagnosis) OVER(PARTITION BY ca1.Patient\_ID ORDER BY di1.Status, di1.ModifiedDt DESC ROWS UNBOUNDED PRECEDING) AS \[Diag\] FROM #Cases ca1 JOIN Diagnosis di1 on ca1.Patient\_ID = di1.PatientID WHERE ca.Patient\_ID = ca1.Patient\_ID )
&gt; I'm not sure of the underlying mechanisms for how the two IDs are concatenated and I have concerns. For instance, if unitID is 1 and equipmentID is 12, the concatenated PK would be '112' Concatenated PKs aren't combined like that. The two fields remain separate, it's just that each combination of the two values must be unique.
What are you optimizing for? run time? what time does it currently run? are the tables indexed?
See if your work has a copy of SQL Server Developer they can give you an iso for. It's an edition that is full featured and is won't count against their license count. Otherwise, SQL Express is free and downloadable from MS, but limited in features: https://www.microsoft.com/en-us/sql-server/sql-server-editions-express
Thanks! I really appreciate the help!
Thanks for delving into detail, that was exactly the explanation I was looking for; I wasn't sure of the underlying implementation of composite keys and while I figured there was some sort of collision detecting to resolve issues like this. The purpose of the table is to tie the products to their components so I can easily generate submittal packages for engineers via the database; its slightly more intricate than the post makes it out to be, but I was really struggling with the composite key aspect.
I'm optimizing for run time. Current runtime is around 3 minutes, which is OK, but is much longer for longer period (say instead of a week, I want a month). What I see from SSMS is that I have three indexes: 1) For PK (the id column not shown here) 2) For Email (the email column, non-unique, non-clustered) 3) For Transdate (the same Transdate column, non-unique, non-clustered) I think the indexes are pretty much enough, but I'm not sure if I'm using the index. I have no permission to see the plan. I'll put on the STATISTICS IO info, please give me a few minutes to run the code again.
I tried the same thing as OP, downloaded SQL Server developer for free, but found out that I missed one of the permissions. Bright side is that BI gave our team a separate db to play with, with most of the permissions.
Thanks! Do you know of a way to populate data into a temp table without using dynamic SQL? I just need the main SELECT clause to be optimizable so maybe putting it in a table parameter then copying the table parameter data into the temp table?
Query Statistics attached to the end of original post. Thank you!
Options: * Move the subquery into a temp table * You could eliminate the HAVING clause and move it to a WHERE clause after the inner join * instead of doing a self join you could use window functions to find the min and max date, either put the results in a temp table or cte and then filter with a where Just a few ideas
Developer edition is free now, so no need to mess with Express.
yep but payment in advance
Experience is more important than certificates. Maybe the focus should be how to gain more experience and knowledge instead of acquiring more certificates. Not saying certificates are worthless, they just don't hold much of value when you are working with people who know what they are doing.
That completely slipped my mind. Thanks to our was haven't used developer edition in awhile.
Looks better and is easier to read is my reasoning.
You need big letters to handle big data
Right, also if the key you use internally is outside your control, then you should consider a made up (surrogate) key that maps to the external key. Lets say you are told that the unique key an aircraft engine is '999' which is on the name plate, but a year later the company keeps the item exactly the same but changes the name because the number 9 is bad luck in a new market. Now you need to scramble because they changed their ID. Or better yet this external department violates their own claim of uniqueness and creates *another* model 999. Just something to keep in mind. It does add some work up front for you.
One of the simplest ways to set up your own SQL server is to install a SQL Server instance right on your laptop/desktop. Developer or Express editions would work for this purpose. Both are free, but Developer has the same features as Enterprise edition, while Express is much more limited. Then you can connect to it with SSMS, just like you do with the servers at work. Another option, if you’re running Windows 10 Professional, is to create a virtual machine in Hyper-V, install Windows Server on that, then install SQL. This would be a good option if you were more of a DBA than a developer, and wanted to play with both Windows and SQL. Of course, you would need to obtain a copy of Windows Server to go this route, which you might have if you have an MSDN subscription.
Don't be afraid to use more temp tables than you need. It can be a quick way to speed up a query. Always start with the smallest dataset possible and then join out to what you need through the rest of the query. My team and I have hundreds of stored procedures in production and I think we've only used dynamic sql in them one or two times. They pretty much all contain temp tables though. I would highly recommend picking up any of Itzik Ben-Gan's books to learn more about SQL and best practices. Also check out SQL Saturdays in your area. I think you mentioned you are on MS SQL.
OK, I need to make a change to this logic for another application. What would it look like to run the "thread" through ALL contacts, not just the Active? Basically, I need to find each MasterContactID between both Completed and Active, its arrival time, sum the Duration across all of its children (which only exists on Completed), and then the most recent (largest) child ContactID, whether it exists on the Completed or Active table. Does this make sense? New sample data with duration: CREATE TABLE CustServ.WF.ActiveContactsTest ( ContactID [bigint], MasterContactID [bigint], ContactStart [datetime] ); INSERT INTO CustServ.WF.ActiveContactsTest (ContactID, MasterContactID, ContactStart) VALUES (45227565918,45130250258,'3/8/19 17:29'), (45547409273,45545833249,'3/26/19 19:44'), (45547477525,45547474793,'3/26/19 22:45'), (45547491195,45547491195,'3/26/19 23:27'), (45562514264,45562514264,'3/27/19 2:44'); CREATE TABLE CustServ.WF.CompletedContactsTest ( ContactID [bigint], MasterContactID [bigint], ContactStart [datetime], Duration [int] ); INSERT INTO CustServ.WF.CompletedContactsTest (ContactID, MasterContactID, ContactStart, Duration) VALUES (44056741709,45435906188,'3/21/19 18:50', 10), (44986804829,44986804829,'2/25/19 7:48', 20), (44987017334,44986804829,'2/25/19 9:20', 30), (45065111275,44987017334,'2/28/19 13:49', 40), (45065182801,45065111275,'2/28/19 14:11', 50), (45130250258,45065182801,'3/4/19 17:22', 60), (45280077660,45280077660,'3/12/19 13:24', 70), (45318017172,45280077660,'3/14/19 13:16', 80), (45370464489,45318017172,'3/18/19 15:31', 90), (45435906188,45370464489,'3/20/19 17:15', 100), (45510675166,44056741709,'3/25/19 9:35', 110), (45545833249,45510675166,'3/26/19 9:44', 120), (45547474793,45547474793,'3/26/19 22:31', 130), (45873925295, 45873925295, '2019-04-15 13:03:36', 77656), (45900451884, 45873925295, '2019-04-16 10:37:51', 280), (45900467438, 45900451884, '2019-04-16 10:42:31', 18612) ;
&gt; It always hurts my eyes. then you're doing it wrong (or whoever wrote the queries that you're reading did it wrong) it is a common and popular **convention** to write SQL using all caps for the SQL keywords, and lower case (or, at worst, CamelCase) for the user-defined identifiers if you combine this with query **formatting** you will get some very pleasing and easy-to-read results for example, take this original query (posted in another /r/SQL thread), and yes, it hurts my eyes too -- SELECT CUS_NAME, INV_NUM, INV_TOTAL FROM CUSTOMER, INVOICE WHERE CUSTOMER.CUS_ID = INVOICE.CUS_ID AND INV_TOTAL &gt; AVG(INV_TOTAL) ORDER BY CUSTOMER.CUS_ID; first, apply some formatting -- SELECT CUS_NAME , INV_NUM , INV_TOTAL FROM CUSTOMER , INVOICE WHERE CUSTOMER.CUS_ID = INVOICE.CUS_ID AND INV_TOTAL &gt; AVG(INV_TOTAL) ORDER BY CUSTOMER.CUS_ID; now change the identifiers to lower case -- SELECT cus_name , inv_num , inv_total FROM customer , invoice WHERE customer.cus_id = invoice.cus_id AND inv_total &gt; AVG(inv_total) ORDER BY customer.cus_id; now fix all the syntax and logic errors (i know this is not part of "hurts my eyes" but it's **so much easier** now) -- SELECT c.cus_name , i.inv_num , i.inv_total FROM customer c LEFT OUTER JOIN invoice i ON i.cus_id = c.cus_id AND i.inv_total &gt; ( SELECT AVG(inv_total) FROM invoice ) ORDER BY c.cus_name you gotta admit, this is ~way~ better than SELECT C.CUS_NAME, I.INV_NUM, I.INV_TOTAL FROM CUSTOMER C LEFT OUTER JOIN INVOICE I ON I.CUS_ID = C.CUS_ID AND I.INV_TOTAL &gt; ( SELECT AVG(INV_TOTAL) FROM INVOICE ) ORDER BY C.CUS_NAME
Assuming you are on MSSQL... What I usually recommend is don't make UnitID and EquipmentID the primary key. Create a UnitEquipmentID that is it's own incrementing Primary key clustered. Create a Unique Constraint on UnitID and EquipmentID and this will prevent duplicates and acts as an index when you are joining UnitID and EquipmentID on this one to many table. When you make UnitID and EquipmentID as the primary key then you'll have to cluster on something else OR if you clustered on UnitID and EquipmentID, your clustered index will show signs of fragmentation later on and you may have performance issues inserting large amounts of records. You can avoid this by padding your page files but then your row/page will be low and you'll read more pages to get the data you are looking for. With a Unique Constraint, it's basically an index and you can rebuild it on the fly to reduce fragmentation.
Thanks! How do you populate a temp table without dynamic SQL though? When I use "INSERT INTO #temptable" I get errors since the table doesn't already exist.
I feel like I am close: WITH cteAllContacts as ( SELECT ContactID, MasterContactID, ContactStart, 0 as Duration FROM CustServ.WF.ActiveContactsTest UNION ALL SELECT ContactID, MasterContactID, ContactStart, Duration FROM CustServ.WF.CompletedContactsTest ) , cteRecursion AS ( SELECT c.ContactID AS TopContactID , c.ContactID , c.MasterContactID , c.ContactStart , C.Duration FROM cteAllContacts AS c WHERE c.MasterContactID = c.ContactID UNION ALL SELECT r.TopContactID , c.ContactID , c.MasterContactID , c.ContactStart , C.Duration FROM cteRecursion AS r INNER JOIN cteAllContacts c ON c.MasterContactID = r.ContactID WHERE r.ContactID &lt;&gt; c.ContactID ) select * from cteRecursion order by TopContactID
 create table UnitEquipment (unitID numeric(8,0), equipmentID numeric(8,0), quantity numeric(6,0), CONSTRAINT PRIMARY KEY(unitID, equipmentID), CONSTRAINT FOREIGN KEY unitID REFERENCES Units(unitID), CONSTRAINT FOREIGN KEY equipmentID REFERENCES Equipment(equipmentID));
I agreed until you added comma-first.
I don't think your conditions are capturing the time between 23:59:59.000 and 00:00:00.000
Two options. You can use "SELECT * INTO #TempTable FROM Table" OR "Create TABLE #TempTable (ID int, data nvarchar(100))". You can run into truncation issues sometimes by using the first one, so if you know all the possible data types that are going to be used I typically use the second. Great questions though!
I should have worded that better. Is there a way to reference a temp table in another stored procedure without using dynamic SQL?
How do I know if I've won?
HeidiSQL doesn't do that
u/Mattsvaliant has absolutely pointed out the issue, but I do try to futureproof against precision changes (i.e. - missing the events between 23:59:59.999 and 23:59:59.9999). What you really want is for @currrentime / TimeStamp to be greater than 'x o clock', but less than 'tomorrow'. Let's go ahead and declare @tomorrow as a datetime and set it to cast(concat(cast(dateadd(day,1,@currentdate) as varchar),' ','00:00:00.000') as datetime. That way, anything less than First Thing Tomorrow will be included. `DECLARE @tomorrow datetime = cast(concat(cast(dateadd(day,1,@currentdate) as varchar),' ','00:00:00.000') as datetime` and then use... `...` `ELSE IF @currrentime &gt;= concat(@currentdate,' ', '19:00:00.000') and @currrentime &lt; @tomorrow BEGIN SELECT COUNT(TimeStamp) FROM MachineActions where Machine = @machineid and TimeStamp &gt;= concat(@currentdate,' ', '19:00:00.000') and TimeStamp &lt; @tomorrow END` `...` etc.
I meant Mariadb
Thanks u/Mattsvaliant. &gt;Move the subquery into a temp table This seems to improve the speed by 30-40%, down to 2m 48s against 3-4m previously. I'm not sure if it has anything to do with caching so I'll try again later. &gt;You could eliminate the HAVING clause and move it to a WHERE clause after the inner join Sorry but I'm not sure how to do it. I think I need the `HAVING` clause for Min-day, no? I can actually move Max_Day to the primary query though. I'll try it now. **Result**: actually runs slower after moving Max_Day to primary query. &gt;instead of doing a self join you could use window functions to find the min and max date, either put the results in a temp table or cte and then filter with a where My understanding is that I should rewrite the subquery with windows function, store the result in a temp table (as in the first suggestion), and then filter the primary query with `WHERE`. Please correct me if I am wrong. I think the benefit is that I can remove `GROUP BY` right?
Nope you'll need a physical table for that.
Thanks for answering my admittedly silly question in such a constructive manner. I was just reading an all uppercase char tumor when I asked this question.
I'd look for an API from Docusign to use for a scheduled import into a database on your end, not a real-time connection from your SQL Server instance to Docusign.
I think the temp table approach is the way to go, have you tried adding index's on the temp to further improve performance?
Yes I added a non-Clustered Index on Email. I'm kind of really bad at writing these "nested" `SELECT` (basically the outer query has a `WHERE` that changes for every row of the inner query)
I managed to understand to your second point, so the query now becomes: DROP TABLE IF EXISTS #subq; SELECT BIT.Email, MIN(BIT.TransDate) AS Min_Day INTO #subq FROM BITransactions AS BIT GROUP BY BIT.Email --Removed HAVING here and put at end of outer query CREATE NONCLUSTERED INDEX ix_subqEmail ON #subq ([Email]); SELECT BIT_Outer.id, BIT_Outer.Email, #subq.Min_Day AS First_Transaction_Date, BIT_Outer.TransDate AS Transaction_Date, BIT_Outer.TransAmount FROM #subq INNER JOIN NatsDw.dbo.BiTransactionsStage AS BIT_Outer ON BIT_Outer.Email = #subq.Email AND BIT_Outer.TransDate BETWEEN #subq.Min_Day AND DATEADD(day, 7, #subq.Min_Day) AND #subq.Min_Day BETWEEN '2019-04-01' AND '2019-04-07'
"char tumor" ???
Some people prefer comma first, some people prefer comma last. I see merits in both approaches so I mix it up and use both in the same query.
As an interesting side note, if you are using Microsoft SQL Server you want to use 23:59:59.99**7** instead. The accuracy of datetime in SQL Server is to the 1/300th of a second, so anything above that will get rouned up to the next milisecond, meaning that 23:59:59.999 actually moves to 00:00:00.000 of the next day. https://docs.microsoft.com/en-us/sql/t-sql/data-types/datetime-transact-sql?view=sql-server-2017
Sounds like you’d use the HAVING clause (to filter on groups).
comma first makes it simpler to comment out one item from the list for testing purposes.
I assume he means a tumor made of characters.
*Hugh Darwen
I understand why, I just don't like commas first.
then you are a heathen and shall be burned at the comma
Great! Thank you!
Just curious - is this answer rdbms agnostic?
As far as I know yes
I mix it up too, but never in the same query... that's just madness!!
[TRY\_CAST()](https://docs.microsoft.com/en-us/sql/t-sql/functions/try-cast-transact-sql?view=sql-server-2017)
I have used that before, but for some reason it kept failing this time. I want to say it was when I was trying to cast the 4 digit number from INT to TIME but I don't exactly remember.
Interesting. Sounds like you have experienced an overall CAST error versus a TRY\_CAST() error. ***TRY\_CAST*** *takes the value passed to it and tries to convert it to the specified data\_type. If the cast succeeds,* ***TRY\_CAST*** *returns the value as the specified data\_type; if an error occurs, null is returned. However if you request a conversion that is explicitly not permitted, then* ***TRY\_CAST*** *fails with an error.*
Most likely due to legibility, but in MSSQL, watch out for case sensitive collations. XD
 SELECT c.cust_name, i.inv_num, i.inv_total FROM customer c LEFT OUTER JOIN invoice i ON i.cus_id = c.cus_id AND i.inv_total &gt; ( SELECT AVG(inv_total) FROM invoice ) ORDER BY c.cus_name is how I would format it. ;P
I like using sublime text then using find all. In this case you can then select say commas and then move your 28 cursors at once one position to the side and hit return. Now FORMATS. Then you can apply similar logic to just about anything else and get where you need to be.
Alright thanks, I'll look into that. I haven't really done much with API, but will check it out
Thank you Mattsvaliant, I knew something was wrong at the miliseconds, anyhow I have to wait for 12am again today to check on the error. The .net app was throwing an error at 11.59.59 pm - Event message: An unhandled exception has occurred, stating that it was not able to find the data after that. I will monitor it today and reply over here tonight at midnight.
Hi ColWaffles, would there be any problem if I proceed with 999, as there is a timer that runs every 10 miliseconds at the .net app. Would like to confirm on your statement that, anything above 23:59:59.99**7** would be rounded up, if the .net app checks at 23.59.59.998 and 23.59.59.999 would it still work?
Profit
READABILITY
And may Cobb have mercy on his soul.
SQL has nothing to do with math.
Let's take a date: January 10th 2019. If you want everything before that date, but not including any part of that date, then if you were to check '2019-01-09 23:59:59.997' you will get the last time possible in a datetime field in SQL Server for January 9th 2019. If you check 2019-01-09 23:59:59.998 or 999 you will see that the date actually gets rounded up to 2019-01-10 00:00:00.000. You can prove this to yourself in SSMS: declare @Test1 datetime = '2019-01-09 23:59:59.991' declare @Test2 datetime = '2019-01-09 23:59:59.992' declare @Test3 datetime = '2019-01-09 23:59:59.993' declare @Test4 datetime = '2019-01-09 23:59:59.994' declare @Test5 datetime = '2019-01-09 23:59:59.995' declare @Test6 datetime = '2019-01-09 23:59:59.996' declare @Test7 datetime = '2019-01-09 23:59:59.997' declare @Test8 datetime = '2019-01-09 23:59:59.998' declare @Test9 datetime = '2019-01-09 23:59:59.999' select @Test1, @Test2, @Test3, @Test4, @Test5, @Test6, @Test7, @Test8, @Test9 So the long answer to your question is that yes, it can cause problems if you are trying to get a datetime on the 998 or 999 millisecond time. If you read that link I posted in the previous comment you can get some more details. Technically the last value of the millisecond portion of a SQL Server datetime is not 0 - 9, it is 0, 3, or 7. If you round up, you can catch records that happen at midnight of the day after you expect to stop searching, rather than at 11:59 like you are expecting. Does that make sense?
You should be fine just learning SQL.
In this case I'd go with just getting the first row from the table, rather than querying all of the rows and adding the first value to each row. SELECT x.Patient, x.Diagnosis FROM ( SELECT ca.Patient_ID ,di.Diagnosis ,row_number() OVER(PARTITION BY ca.Patient_ID ORDER BY di.Status, di.ModifiedDt DESC) Ordernumber FROM #Cases ca JOIN Diagnosis di on ca.Patient_ID = di.PatientID) x INTO #PatientDiagnosis WHERE x.Ordernumber = 1
Are you going to be doing any statistical big data analysis? if so, brush up on your math for SQL. Otherwise, you should be able to get away with basic algebra and focus more on learning SQL logic.
Hi ColWaffles, I understand, I actually executed the same and I am modifying it to this way select concat('2019-04-18',' ', '23:59:59.999') as datetime declare @Test9 datetime = '2019-04-18 23:59:59.999' select @Test9 declare @Test2 datetime = concat('2019-04-18',' ', '23:59:59.999') select @Test2 Result 1st - 2019-04-18 23:59:59.999 2nd - 2019-04-19 00:00:00.000 3rd - 2019-04-19 00:00:00.000 &amp;#x200B; I noticed earlier I was just using TimeStamp &lt; concat(@currentdate,' ', '23:59:59.999') and not TimeStamp &lt;= concat(@currentdate,' ', '23:59:59.999'), notice the &lt;= you are right, 999 returns the next day. &amp;#x200B; How come the first statement above, on the select concat , it does not return the same . Modifying to such. --- BEGIN SELECT COUNT(TimeStamp) FROM MachineActions where Machine = @machineid and TimeStamp &gt;= concat(@currentdate,' ', '19:00:00.000') and TimeStamp &lt;= concat(@currentdate,' ', '23:59:59.999') END This should work, hopefully, let me update it here at midnight today.
The most upvoted comment and you couldn't be much more wrong... https://en.wikipedia.org/wiki/Relational_algebra I'm not saying you need to understand relational algebra to write SQL, but some basic understanding of set theory is useful.
**Relational algebra** Relational algebra, first created by Edgar F. Codd while at IBM, is a family of algebras with a well-founded semantics used for modelling the data stored in relational databases, and defining queries on it. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/SQL/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
Basic algebra is plenty. If you’re going into data science you’ll need more, but being a dba/etc is not math based.
While that can definitely help you wrap your head about what a database is doing, and is great if you're trying to build or optimise a really complex query, most of the actual work most people will encounter will not need that level of understanding.
Honestly the math i need to know to do SQL i learned in like 3rd grade. I've done some trig, that's about as complicated as its gotten, of course it depends on what kind of job you get. handling numbers with SQL is more about knowing the data types and understanding precision and scale. How one type converts to another. Rounding vs truncation, absolute value. Understanding how to manipulate a number to get the result you need or using functions like [these](https://www.geeksforgeeks.org/sql-numeric-functions/)
You don't need a strong foundation in relational algebra to learn SQL. Most of my coworkers learned the other way around.. learning SQL gave them an understanding of relational algebra. I realize you were just trying to correct him, but the poster seems to be asking about general math.
[https://imgur.com/a/xeVa2Om](https://imgur.com/a/xeVa2Om) Here the schema.. 2. Display the number of students that are eligible for exams. 3. Display the number of students that had a grade of 70 or above. 4. Display highest exam score that Nina received. 5. Display the least number of days off. 6. Display the average exam grade received by each student. 7. Display the average exam grade for the OOAD course. I was wanting to beautify basically my query results. I tried to rename the results using AS but an error popped up or it didint work or idk how or forget how to give the results an alias. also is there a way to join the results the student name can be join with exam results using the student ID? Or search with a join so i can search for nina instead of her student id? or does it not matter
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/eDKBkgb.jpg** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme)^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20el5xww8)
"level of understanding" even kinda overstates the importance of it. Just spending a few minutes to familiarize yourself with some of the terminology and concepts is enough. Then move the hell on, learn SQL, learn your platform-specific functions/features/etc, and start having fun doing it all wrong so you can hurry up and learn how to do it right and have even more fun, and if a legitimate problem you're trying to solve requires some knowledge you don't already posses, take to the internets and learn what you need when you need it.
Someone saids, set theory is required to learning SQL, but I am not agreeing with that. I think relational-on the other words, two dimensional-thinking is must and basic knowledge of boolean operation is plus.
There are only six major areas of maths needed for working with databases. [Set Theory](https://en.wikipedia.org/wiki/Set_(mathematics)), [General Algebra](https://en.wikipedia.org/wiki/Algebra), [Big O notation](https://en.wikipedia.org/wiki/Big_O_notation), [Relational Algebra](https://en.wikipedia.org/wiki/Relational_algebra), [Graph Theory](https://en.wikipedia.org/wiki/Graph_theory), and [Mathematical Logic](https://en.wikipedia.org/wiki/Mathematical_logic). &amp;#x200B; These are usually bundled up into an area called [Discrete Mathematics](https://en.wikipedia.org/wiki/Discrete_mathematics), which is fairly crucial to all aspects of programming, so if you're not familiar with it I'd definitely spend some time getting to know it.
No need for math. Just Venn diagrams are enough to visualize different JOINs when you need to.
 --QUESTION 2 select count (Eligibility_For_Exams) From Student_Attendance Where Eligibility_for_exams IN 'Y'; --QUESTION 3 Select count (grade) From Exam_result Where Grade &gt;70; --QUESTION 4 Select Max (Grade) From Exam_Result Where Student_ID = '710'; --QUESTION 5 Select Min (No_Of_Days_Off) from student_attendance; --QUESTION 6 Select Avg (grade) From Exam_result; --QUESTION 7 select avg (grade) from Exam_result where course_id=188; In Q2, if you only have a single value, I'd just use `= 'Y'` In Q3, is `&gt; 70` going to give you the correct result if someone scored exactly 70? In Q4, if Student\_ID is an int, don't wrap the value in quotes. just use `= 710` Also, in Q4, you might be expected to join some of those tables together so you can say something like `where first_name = 'Nina'` In Q6, you aren't returning a result "for each student" In Q7, you may be expected to join to other tables so you can refer to the course by name rather than id &amp;#x200B; You should be able to name your resulting columns easily similar to this: `select min(column) as Min_Column`, just make sure you aren't using a reserved word as your column name without delimiting it (try double quotes if you need to do that).
So, by definition, what you're doing in SQL is mathematics. That's not at all what most people are discussing when they're asking about what math competency is required, though.
No, you should not. Spend that time learning more SQL. You likely won't use any math at all in SQL, and if you do, it'll be *very* basic match like multiplication &amp; division.
I would use python. Pull the data from SQL via Pyodbc and then parse the relevant field(s) with elementtree’s iterparse. Use the ‘end’ event and look for the tag you’re interested in.
I hold a SQL developer/DBA role where I need to create a lot of financial reporting MI/BI, and I'm unbelievably bad at maths. I have no mental arithmetic skills and no conceptual understanding of algebra/calculus. You will be fine OP, but if brushing up on maths will make you feel more confident it is worth doing.
alright that makes sense and i definitely overlooked some stuff.
Postgres handles xml fields well
Gonna have to go with u/Hajile_S and u/Herdnerfer on this one. I hear about relational algebra every several years or so, look it up, and then promptly forget about it again, as it's just describing joins, but making them way more complex to understand than is needed. I can describe most joins to anyone outside of sales (and a few in sales, but I won't claim all of them) to a point they fundamentally grasp the concept. Add in a bunch of Greek letters and algebraic equations and it only obfuscates the concepts for anyone outside of a hard math and science background, which includes many, if not most SQL DBAs.
Unless it's the first expression in the list
That's why the first expression is always something absolutely necessary
I've been a developer for over 20 years, write SQL all the time, and no real maths knowledge is necessary apart from understanding what sum or average mean. I currently work on analytics projects and any maths calculations we need to do are done by whoever is best at maths on our team, and typically this involves us all googling things and trying to figure it out. The people trying to claim you need to know maths are not being honest. 99.9% of people who write SQL don't go further than averaging things.
SQL is literally applied math. That being said. You don’t need to know the theory of gravity to play baseball.
Nor the "theory" of density-and-buoyancy.
Having a basic idea about set theory is a plus. Other than that - if you're gonna do statistics, you need statistics knowledge. But that's not a prerequisite for SQL itself.
I think you have a bad understanding at what math is. It's not grades on a school subject. SQL is basically set manipulation. Understanding SQL is understanding set theory. So many people keep saying that they're bad at math when they actually aren't. Stop :)
Too many people keep saying that they're bad at math, just because some bad teacher told them they are. People are inherently good at math, with very a minority of people actually not doing good for whatever reason. This is driven by evolution. Our brains are wired to think mathematically from birth. That's why it's so hard for us to not think mathematically. When you see 2 apples, you don't see an apple-like mass of a weird shape, you see 2 distinct apples. That's math. It's obvious, you can't learn it, you can't unlearn it. Advanced math is just expressing stuff through a language that might not sit well with you. Math is both a language and an understanding. You can be a native English speaker while not being able to score high grades in English because of dyslexia or whatever. Same as math - you can be good at it, while scoring bad on math tests. Because math tests are too standardized and require a good level of math-as-a-language manipulation, while not scoring actual math abilities.
I have a degree in maths and computer science, and a masters in computer science which was heavy on database theory and cryptography. I know what I'm talking about.
Well good for you. And I have 15 years of experience in telling people that your master's degree in whatever doesn't mean anything here. Your argument does. SQL is set theory. The best way to understand what's going on is to understand set theory. Relational algebra is basically never used. But to say that no math knowledge is required is completely BS. As it is BS that people are bad at math. We have knowledge. Presenting SQL as completely separate from math is telling people that it's some weird unpredictable engine that spouts BS when they're making a reasonable request.
Why are you using a strawman? You're changing my argument from "you don't need maths knowledge to write SQL day to day" to "SQL theory has no relationship to maths". The fact that you're having to change my argument to "win" is pathetic. I stand by my original point: Developers who write SQL don't need maths. 99%+ of SQL work is insert, select, update, delete. Stop trying to sound clever.
This. Most of us use math quite often. It doesn't have to be advanced math or more abstract logical parts of math to be math. If you can't figure out the degree of one side of a triangle that's not really relevant when you're paying your bills or tracking your calories or figuring out your part of the check at dinner. To answer the OPs question, SQL is much more about logic than math. Once you understand the language and how tables function and interact, the rest of it is just making that happen in ways that are useful to you. You're likely better off spending time on a few good SQL courses than math courses.
The way this code reads implies that, at any given time, you want to be able to count how many Machine Actions there have been since the start of the current 12-hour run. Instead of trying to figure out how to define midnight, you can define the start of the run instead, and then just count timestamps between then and now. CREATE OR ALTER PROCEDURE [dbo].[TimeStampdetails] @machineid int /* Test: [TimeStampdetails] @machineid=1 */ AS DECLARE /* Use a fixed reference point (@now), in case procedure's running time spans midnight. All other times are derived from the reference point. */ @now datetime=getdate(), -- Same as current_timestamp: https://stackoverflow.com/a/24226775 @the_time time, @today datetime, @shift_start_datetime datetime -- 12-hour shift start time. ; SET @the_time = convert(time,@now); -- Strip the date from @now, leaving just time, for comparison to shift start times. SET @today = convert(datetime,convert(date,@now)); -- Convert to date and back to strip time, while letting dateadd() work later. /* Use the current time to figure out which shift we're in now, and set its start time. If the current time equals a shift start, then we're interested in the preceding shift. */ SET @shift_start_datetime = case when @the_time &lt;= '07:00' then DATEADD(hour,-7,@today) -- If it's early morning, we're still in yesterday's night shift. when @the_time &gt; '07:00' and @the_time &lt;= '19:00' then DATEADD(hour,7,@today) -- If it's middle of the day, we're in today's day shift. else DATEADD(hour,19,@today) end; -- Otherwise, it must be after 7:00 PM today, meaning the window starts at 7:00 PM today. /* Since we've determined which shift we're in now, we can just look for time stamps that fall within it. All the conditional logic has moved to the CASE statement. Note that we're only looking up until @now; there shouldn't be any time stamps in the future. If more machine actions have been recorded since the procedure was called, we're going to ignore them, because we're trying to answer the question "How many actions had there been this shift at the point that I called the proc?" */ BEGIN SELECT COUNT(TimeStamp) FROM MachineActions WHERE Machine = @machineid and TimeStamp &gt;= @shift_start_datetime and TimeStamp &lt; @now; END
&gt; "The SQL Coalesce function evaluates the arguments in order and always returns the first non-zero value from the defined argument list." non-zero??? GTFO &gt; "It stops evaluating until it finds the first non-NULL argument." stops until??? GTFO listen, whoever you are writing these "tutorials' -- please hire someone who knows how to write
Not a straw man. You say you've used sql for 20 years, that you have a degree in math, yet math is not necessary. You don't see the bias in that? Yeah look at me I can do so many things, but it's not because of my abilities and training. Understanding this stuff is easy without any prior knowledge. &gt; The fact that you're having to change my argument to "win" is pathetic. Dude wth? You started with "I'm right because i have a degree" just to shut me up! Projection at its finest. &gt; 99%+ of SQL work is insert, select, update, delete. And `select` is about set theory as soon as you use join or group by. So stop trying to be an asshole so much. I started with a friendly tone and ended up like this. You're really talented at this.
&gt;Almost no one writing SQL needs to know what set theory is. I find thinking in sets to be the single most useful thing to do in order to understand how the database works and write efficient queries. &amp;#x200B; So many developers think in the same sequential steps they have in imperative languages and it won't do them any good when it comes to databases.
Lies and more lies.
This is the tru answer. Guy talking set theory and discrete math is confusing SQL with a CS degree.
Everytime I've seen a developer use Coalesce, I've seen incorrect datasets returned. Most likely they read things like this.
To start not much ... To be an expert lots..,
Logic is a part of math and actually a quite advance subject. IMHO if you are good in SQL you should be ok in abstract math.
You join to a table or derived table. A sub query is essentially a derived table. You can join TO a subquery, or you can use a subquery in your where clause. Essentially you’re asking the difference between a join and a table, or between a toothbrush and toothpaste.
I took a computer science math course at community college quite a while back. I remember really enjoying it - lots of ven diagrams, logic gates, things like that. I think that sort of math is what lends itself to databases and SQL.
Not Oracle, but downloading postgres and going through the practice exercises on a real database should help you get the core SQL concepts down: [http://www.postgresqltutorial.com/](http://www.postgresqltutorial.com/)
Subqueries and inner joins can be leveraged to do the same thing. Imagine you worked for amazon, and you had a table of customers who purchased a hat, and a table of customers who purchased a shirt. &amp;#x200B; |Table: AccountsWhoBoughtShirt| |:-| |AccountNumber| |12345| |23456| |34567| |45678| |56789| &amp;#x200B; |Table: AccountsWhoBoughtHat| |:-| |AccountNumber| |45678| |56789| |67890| |78901| |89012| &amp;#x200B; Aside from the INTERSECT function, I can think of two ways to find the customers who bought both the hat and the shirt. Using subqueries SELECT AccountNumber FROM AccountsWhoBoughtShirt WHERE AccountNumber IN (SELECT AccountNumber FROM AccountsWhoBoughtHat) Using INNER JOINs SELECT a.AccountNumber FROM AccountsWhoBoughtShirt a INNER JOIN AccountsWhoBoughtHat b ON a.AccountNumber = b.AccountNumber &amp;#x200B; Someone correct me if I'm wrong, but if we had to join on the account number and something else, it would be much easier to do so with the INNER JOIN vs the subquery. &amp;#x200B; Subqueries are also good for solidifying aliases. For example, the query below calculates a students letter grade on an exam, and then filters to where the grade is an A. &amp;#x200B; Without a subquery, we'd have to put the definition of the column in the WHERE clause. SELECT StudentName, CASE WHEN ExamGrade &gt;= 90 THEN 'A' WHEN ExamGrade &gt;= 80 THEN 'B' WHEN ExamGrade &gt;= 70 THEN 'C' WHEN ExamGrade &gt;= 60 THEN 'D' ELSE 'F' END AS LetterGrade FROM StudentExamGrades WHERE CASE WHEN ExamGrade &gt;= 90 THEN 'A' WHEN ExamGrade &gt;= 80 THEN 'B' WHEN ExamGrade &gt;= 70 THEN 'C' WHEN ExamGrade &gt;= 60 THEN 'D' ELSE 'F' END = 'A' vs *with* a subquery, we've solidified the CASE statement into a new column, LetterGrade, so we can utilize that alias in the WHERE clause. SELECT StudentName, LetterGrade FROM ( SELECT StudentName, CASE WHEN ExamGrade &gt;= 90 THEN 'A' WHEN ExamGrade &gt;= 80 THEN 'B' WHEN ExamGrade &gt;= 70 THEN 'C' WHEN ExamGrade &gt;= 60 THEN 'D' ELSE 'F' END AS LetterGrade FROM StudentExamGrades ) s WHERE LetterGrade = 'A'
Three databases or three tables? Could something like this work? (select [your feilds] from table a) Union (select [your feilds] from table b) Union (select [your feilds] from table c)
I struggle with the use case for subqueries. I understand joins well enough I think.
Three databases, each with their own sales table. Each db represents a company, and each company has a sales table. So if I wanted to implement your solution, would it be: SELECT * INTO #tmpConsolidatedSales FROM (select Customer Number, Customer Name, Sales from SalesTableCompanyA UNION select Customer Number, Customer Name, Sales from SalesTableCompanyB UNION select Customer Number, Customer Name, Sales from SalesTableCompanyC) as tmp And from there I'd write the query I need to arrange the data as I need it, so something like: SELECT CustomerNumber ,CustomerName ,(CASE WHEN COMPANY = 'Company A' THEN sum(sales) else 0 end) as 'Sales at Company A' ,(CASE WHEN COMPANY = 'Company B' THEN sum(sales) else 0 end) as 'Sales at Company B' ,(CASE WHEN COMPANY = 'Company C' THEN sum(sales) else 0 end) as 'Sales at Company C' FROM #tmpConsolidatedSales GROUP BY CustomerNumber, CustomerName Does this look correct?
Couple of things I noticed right off the bat. Since this looks to be homework I don't want to just outright give you the answer, but I will say your case statement won't work like that because you have 3 different 'COMPANY' fields and 3 different 'SALES' fields and you're not telling it which one to use. I would try to get a dataset where you just have this data: CustomerNumber|CustomerName|Company|Sales Amount After you pull it for one company and UNION it with the 2nd and 3rd. Then you'd have a clean dataset you could use for your summary.
Can you not run the query to find out?
I wish this was homework, then I wouldn't feel so stressed! Thanks for the advice. Seems to echo the approach that u/Master_Broshi suggested, which is to union all of the data into a new table and then write the query from there. I guess I needed to break this into smaller portions to solve it.
:) That should be my next step! Just want to make sure I have the overall thought process squared away. Seems like I do.
While I won't say you won't use math in doing SQL, it's no more than anything else in life. You can get away with only knowing the basics. I've honestly not done anything passed basic algebra except for a few rare circumstances. You may need higher math at some point in your SQL career, but you certainly don't need it for learning it.
This is the kind of stuff intigration services was built for and would be very simple to do something like this. I suggest you explore building your very first SSIS package.
Give pgexercises.com a try. It's postgres, but that translates well to oracle, and it's all in the browser.