Thanks, will do. 
Fwiw, bonuses are just regular income as far as taxes go, they don't get taxed at a higher rate. They *do* get withheld at a higher rate initially because they are somewhat unpredictable so your average withholdings throughout the rest of the year can't really account for them. However, any excess that gets withheld gets returned to you as a tax refund next year. It still kind of sucks in the moment but it does all even out eventually.
I'd rather not say but there are lots of jobs like mine available in the DC/Virginia area, San Antonio, Colorado, PNW, and Charleston, SC. They require getting a security clearance and the Security + exam.
In what way is this query not producing what you want? I mean other than the percentage, but just * 100 and round it?
If you already have a clearance, then [Clearancejobs.com](https://Clearancejobs.com), otherwise just look for jobs with the DOD. TekSystems and Apex Systems both recruit for a lot of these jobs and are how most people get their foot in the door. If you get a job through them its generally 6 months temporary to hire. &amp;#x200B;
Try something like this (untested)... SELECT o.firstname as tot2, ROUND(100 * COUNT(CAST(cweek.properties__notes_last_contacted__value as date)) / COUNT(CAST(ctot.properties__notes_last_contacted__value as date)), 2) as perc FROM contacts AS ctot LEFT JOIN contacts AS cweek ON ( ctot.PRIMARY_KEY = cweek.PRIMARY_KEY and cweek.properties__inbound_source__value = 'weblead_b2b_request' (cweek.properties__notes_last_contacted__value) &gt; NOW() - INTERVAL '7 days') ) WHERE ctot.properties__inbound_source__value = 'weblead_b2b_request' AND o.firstname is not null GROUP BY o.firstname ORDER BY 1 DESC You need to swap PRIMARY_KEY with the contact PK fieldname(s) The ORDER BY 1 is an attempt to order by perc DESC (MySQL style). No idea if this would work in Postgres Finally, are you really sure you want to be treating o.firstname as unique? 
Break it down a bit. How would you get the last period that someone was active? select id, max(period) from table where is_active = 1 group by id Once you've got the ID and the highest period that they were active, then you can look at selecting all the periods which are earlier than your last period. That should be easy enough.
It’s my first job...I plan on leaving after a year to be honest..especially After reading all of these comments.
&gt;I cannot use TOP 1, because there are many products in the real table Use the RANK() command and then filter: WHERE rankColumn = 1
The normal way to do this is to sequence your result set with ROW_NUMBER and filter to ROW_NUMBER = 1. However, I prefer what I call a string sorted aggregate for performance reasons. You pad the value you want with a fixed width string that sorts alphabetically, and take the MAX or MIN. --String Sorted Aggregate SELECT b.product , STUFF(MAX(RIGHT('0000000000'+CONVERT(varchar(10),b.count),10)+b.stock_type),1,10,'') AS stock_type , MAX(b.count) AS count FROM (SELECT a.product , a.stock_type , COUNT(*) AS count FROM (SELECT s.product , s.stock_type , s.location FROM dbo.Stock AS s GROUP BY s.product , s.stock_type , s.location) AS a GROUP BY a.product , a.stock_type) AS b GROUP BY b.product; --Row Number = 1 SELECT b.product , b.stock_type , b.count FROM (SELECT a.product , a.stock_type , COUNT(*) AS count , ROW_NUMBER() OVER (PARTITION BY a.product ORDER BY COUNT(*) DESC) AS RowNumber FROM (SELECT s.product , s.stock_type , s.location FROM dbo.Stock AS s GROUP BY s.product , s.stock_type , s.location) AS a GROUP BY a.product , a.stock_type) AS b WHERE b.RowNumber = 1;
Thanks for this. I have a question regarding: ctot.PRIMARY_KEY = cweek.PRIMARY_KEY Since the data is coming from the same table (Contacts) why would I need to JOIN those columns? What would be your suggestion for it? The first name, in this case, represents the first name of Sales rep which is unique (if that makes sense). To better describe the use case **I'd like to show % of total prospects contacted (*properties__notes_last_contacted_value*) in last 7 days per sales rep. (*o.firstname*)** This is the error I get when I try to run the query you have provided: LINE 12: (cweek.properties__notes_last_contacted__value) &gt; NO... I'm not the most proficient so please bear with me :). Ideally, I'd like to compare two queries which use data from the same tables (Contact/Owners) where the only difference is that the first one has extra conditions set. WHERE (c.properties__notes_last_contacted__value) &gt; NOW() - INTERVAL '7 days' AND c.properties__inbound_source__value = 'weblead_b2b_request' AND o.firstname is not null Does that make more sense? Thank you!
I was able to get it to work but adding in a WHERE statement that cut down on the rows being searched. Thanks for your response! 
Always copy paste. Never type in a value for a filter.
Ding ding ding, spot on! :)
sorry for the late replay thanks I am going to try it out
This is going to be a bit of brute force, but here ya go. This was written on a later version of SQL server so I can't guarantee all those sys tables exist. Start with something along these lines SELECT s.name + '.' + t.name as table, c.name as Col, ddps.row_count , st.name, case when st.name like 'n%' then c.max_length / 2 else c.max_length end as max_length, c.precision, c.scale FROM sys.tables AS t JOIN sys.schemas AS s ON s.schema_id = t.schema_id JOIN sys.columns AS c ON c.OBJECT_ID = t.OBJECT_ID JOIN sys.indexes AS i ON i.OBJECT_ID = t.OBJECT_ID JOIN sys.types AS st on st.system_type_id = c.system_type_id and st.user_type_id = c.user_type_id JOIN sys.dm_db_partition_stats AS ddps ON i.OBJECT_ID = ddps.OBJECT_ID AND i.index_id = ddps.index_id and i.index_id in (0,1) where ddps.row_count &gt; 0 Now you are going to have to adjust that WHERE clause. If you are looking for a text string, add " and st.name like '%char%' " or if its a number " st.name like '%int%' " or date etc. You get the idea. Add any other refinements you can think of - if your value won't be in a table with less than 10 rows up the row count. Or whatever makes sense. You should have a big ol list of tables and columns that are of the data type you are looking for. Now adjust the query to write the queries for you - SELECT '; SELECT ''[' + s.name + '].[' + t.name + '].[' + c.name + ']'' as tblcol FROM [' + s.name + '].[' + t.name + '] where [' + c.name + '] = &lt;MyVAL&gt;' as RunThis FROM sys.tables AS t JOIN sys.schemas AS s ON s.schema_id = t.schema_id JOIN sys.columns AS c ON c.OBJECT_ID = t.OBJECT_ID JOIN sys.indexes AS i ON i.OBJECT_ID = t.OBJECT_ID JOIN sys.types AS st on st.system_type_id = c.system_type_id and st.user_type_id = c.user_type_id JOIN sys.dm_db_partition_stats AS ddps ON i.OBJECT_ID = ddps.OBJECT_ID AND i.index_id = ddps.index_id and i.index_id in (0,1) WHERE &lt;.... the modified list you created before .....&gt; In the result set should be a bunch of queries. Copy them into a new window. Replace the &lt;MyVAL&gt; with whatever they are looking for. If there are any matches you'll get a row back. tblcol will print which table and column matched your search string - we could return the value or use a *, but this is gonna trash the disk enough as is. Speaking of, this is going to scan every table once for every column in the list for a match. So don't run the darn thing during a busy time of day. Once you have a list of tables and columns that match the string then you can look at them directly, this is just a quick and dirty way to narrow the focus.
Posting the SQL flavor / version along with a mock data set / data structure and the queries you've tried often give back the best results. Whenever I post a question online, I often find the amount of effort I put into the question is the amount of effort the individuals give back when answering. &amp;#x200B; I would look at functions such as EXIST, NOT EXIST, INTERSECT, and EXCEPT. You may want to create a subquery or temp table of both tables utilizing only the columns you want to compare. I generally will join both tables on a common column and then search where the right table has NULL results in each column to identify the mismapping. 
Thanks for that. I've been passing on certs but I see the value in them. Guess I just gotta take huge data problems by the cojones.
Advisory skills - You have to be the go-to between application code and database administration. It's a bit of Software Engineer and a bit of Database Admin. Database math skills help too.
Here is the simple way: http://blog.databasepatterns.com/2014/12/fuzzy-record-matching-in-sql-p1.html Here is the less simple way: https://github.com/larsga/Duke
Did you have a specific question on functions or syntax? Otherwise my advice is practice and don't stop. 
Ha! I am usually not a big cert guy but I made an exception for the cloud certs. I have not bothered with SQL Server certification since version 7.0. But the AWS certs put a LOT of money in my pocket. I would highly recommend getting, at the least, the Cloud Architect Associate cert. And even then, the Professional cert really opens doors for you and makes you market value TONS higher. Good luck!
Ideally your contacts table has some sort of primary key, which would often be a numeric id. So about the PRIMARY KEY join, we are joining the contacts table back onto itself, and giving each one an alias so we can pull fields from either one. The ctot is used to pull all contacts, and cweek is used to pull only those contacts who were contacted in the past week. The overall resultset would look something like this, before any GROUP BY is applied... https://i.imgur.com/2V2s7GD.jpg As you can see, contact '7' appears in the column from contacts (ctot), but as they have not been contacted in the past month, they appear as NULL in the contacts (cweek) column. Contact 8 was contacted in the past week, so appears in both columns. Then its just a case of counting the non-NULL values in both columns, and dividing the cweek / ctot counts to get a percentage, which is what my query attempts to do 
SELECT * from MobileNetworkOperators GROUP BY City Having users = NULL
* Window Functions * Recursive Common Table Expressions (Trees and Graphs) * Relational Division * Table Inheritance / The Party Model * Record Linkage (complex dedupe) * Admin (security, logging, backup/restore, users, performance) * Dimensional Design * Temporal Tables (SQL:2011) * Modeling recurring events efficiently * Liquibase / Flyway * Full Text Search 
Yeah, that also makes sense for the first couple years. College degrees definitely help land that initial job and equal a higher starting pay than not having one. However, compared to other professions tech jobs are more performance-based than credential-based, so after a couple years that lack of a college degree should become a non-issue and you should be pushing for a good bump in pay.
This is definitely true, but I bet it's temporary. Certs are most valuable when the skillset is rare but important. Lots of companies are pushing to migrate to a cloud architecture now, and knowing that they can get someone who has at least baseline skills there is worth paying extra. Once that knowledge becomes truly common, the cert will likely not be worth that much. Not to say it's not worth it because it probably is, just be aware that we're in a window that may close in a couple years.
keeping it simple but functional you should get into Stored Procedures and jobs. Triggers as well
I'd add Pivot and Unpivot
Thank you!
Sounds good I will research that. Is it similar to Macros in Excel?
#1 SELECT d.* FROM directors d JOIN Movies_Directors md ON d.id = md.director_id JOIN Movies m ON md.movie_id = m.id WHERE m.name = "Fargo"
So if it is for a class, instead of us doing this for you, what do you have so far?
I once gave a talk about relational algebra and the equivalencies of it in the SQL language. We covered the original eight operators of relational algebra as specified by E. F. Codd and many found themselves stumbling over division and what it's analogue was in SQL. I am always thrilled in the different ways folks implement division in SQL, their rationale for the method they have selected to use, and their different use cases for division with remainder and without.
select "Do Your Own Work" From AnyoneOnThisSub Where User = 'daveconn66'
How about triggers?
You will benefit FAR MORE in the long run if you do some research on your own.
&gt;we use the MD5 encryption algorithm and then we store passwords into the database Are you _actually_ storing passwords? The MD5 hash of a password isn't the same as storing the actual password***** (We will come back to this....) You will NEVER want to store the password. You don't need to know what the user's passwords are. You just need a way to convert what they enter as the password to a representation of the password they're storing. Use a secure hashing algorithm with a salt that is unique to each user, then store this in your database. *****Storing an MD5 hash in 2018 is essentially the same as storing the plain-text password directly into the database. It's not very secure and a more secure hashing algorithm should be used instead.
What flavor of SQL are you using? This is going to heavily influence how this gets done. You can lookup hierarchy in your SQL flavor and find some examples... here's one for TSQL (SQL Server): https://blog.sqlauthority.com/2012/04/24/sql-server-introduction-to-hierarchical-query-using-a-recursive-cte-a-primer/
I think you need a Recursive CTE for this... 
You need a CTE with a UNION to itself. Check out this article, it's older but the idea is the same (managers in an Employee table) https://blogs.msdn.microsoft.com/simonince/2007/10/17/hierarchies-with-common-table-expressions/
Gonna have a hard time for only $100K Good luck though!!!
Like /u/haribofiend plus ETL/ELT in a lot of cases. In my case SQL Server Integration Services, Informatica, Cognos Data Manager and Pentaho.
Hard time with what? 
Millington TN by any chance?
So I've been told. Good thing is, I'm fully work from home, so I put a value on that. 
I use to live in Bothell and commute to Redmond for you know who. That wfh full time is gold when in that kind of crappy traffic up there 
Data Engineers here make between $130K and $140K with 2-3 years of experience. And your benefits ARE nice :)
I would advise you to learn views (you may already know), then move on to stored procedures (noting the differences between sprocs and views), then learn triggers, then learn functions (noting the difference between triggers and functions). Then learn CTEs, transactions (begin transaction, rollback or commit), roll ups, and then pivots. Then I would learn about SSAS, SSIS, and then SSRS. Once you’ve learned that; you’ll be very well rounded and should consider where you want to take your career. Full Stack Development?Data Science/Analytics? Business Intelligence? Database Administration? Come back to me when you’re ready to move to the next step. 
Thanks for the help..I already know SSIS pretty well and I’d like to get into DBA, DB engineering and architecture because that interests me a lot.
Then I would say master your sprocs, views, triggers, functions, and transactions. Then start under the junior DBA path... learn about security (best practices within sql and the services on the server), learn about backups and restores (know when to perform backups and the repercussions that come from it versus when to replicate data or ETL data). Once you’ve mastered the different types of backups (full, diff, transactional, all the intricacies like Copy only or compression, checksum, verification), learn about restoring your database (altering database to single user mode with rollback immediate, restoring the database and it’s files, setting the db to multi user when done. When to use with recovery vs with norecovery). Then immediately recognizing that you have to apply user creation to a restored database for access rights (if desired). Learn about your jobs. Learn about how to find why jobs fail (logs), look into why SSIS packages and jobs fail (reports&gt;all executions). Learn about replication (publisher, distributor, and the subscribers and how they either push or pull data). Learn about maintenance and good house keeping (I suggest Ola Hallengren). Learn about your master database and your msdb database and what data types or what kind of data (I should say) resides in these databases. Learn about sql server agent. Learn about the activity monitor and your processes... how to identify if a session is being blocked, and wether you should kill a session or let it ride out). Learn about performance monitoring. Power shell is quite amazing as a DBA. Research why that is. I can give you the answers if you need, but I want to teach a man (or woman) to fish. :) you’ve got this!
Yup, recursive CTE.
If you are interested in performance tuning, I would recommend to learn how to read Execution Plans, creating indices, using optimizer hints and refactoring/rewriting SQL-Code.
You were downvoted but I don’t think you’re wrong. Knowing when and when not to use triggers, and how to write triggers in the least possible interruptive way for a transactions-based database is something I’d consider an intermediate-to-advanced topic.
Look up organizational hierarchy with cte recursive query. This is basically the same situation.
&gt; MD5 encryption algorithm MD5 is a hashing function, not encryption. When you hash something, you can't get the original back. With encryption, you can. You always want to [salt](https://en.wikipedia.org/wiki/Salt_(cryptography\)) and hash the password before storing. That is the correct way to create password authentication. Also, never use MD5. It's not secure. Use a secure hashing algorithm, like bcrypt.
To answer your last question: practice. SQL is its own wild animal and gives plenty of ways to solve problems based around specific structures. Build out a test environment and use something like an AdventureWorks database to just play around. To go back to the query, one way to think about this is as if it is a nested foreach loop from any other language. With this is mind, also think of each query as it's own table with the same data: 'world' being the table for the main query and 'x' being the table for the nested query. You're basically saying foreach row in table 'world', check each row in table 'x' to see if it matches the criteria. Grab row 1 from 'world' and now grab row 1 from 'x'. Does the row in 'x' match the continent from your row in world AND have a population less than 25000000? No? Okay, next row in 'x'. Do this until you hit the end of table 'x' and now you move to row 2 in 'world'. I hope this helps. (To be clear, the logic behind SQL may not actually execute exactly like this, but it helps to understand.)
Generally when you deal with SQL you are looking not only at the query but at the structure of the tables and the raw data inside it But even without that knowledge you can read the the statement you have as basically going; From these two identical lists give me a list where the continents match and where the population is greater than 25000000. Now that I have that list look at the first list and tell me everything that is not in that. &gt;I'm stuck after getting to this point. I mean how does the script know we are looking at the continent level Because of the table structure. Just by looking at the code I can make the assumption the table is like this NameOfCountry, ContinenCountryisIn, Population, Somethingelsemeasurable, Anothermeasurablething, andanotherone Britain, Europe, 66000000, $9, 'Tea', 'Metric' 
Besides the other excellent comments by /u/Daakuryu and /u/Dosez you need to understand that SQL is *set* based. Whereas you are used to RBAR (row by agonizing row) style processing in C#, C++ and VB - SQL operates on the entire 'set' of records defined by the query and it's WHERE clause. This seems to be the main thing that gives programmers in other languages trouble when it comes to SQL. And while you *can* do row by row processing (shudder) using a cursor, you should avoid it whenever possible as it is a performance killer. Most things can be achieved as a set based process now - especially with the introduction (for T-SQL) of windowing functions like LEAD and LAG. Learn to think 'set based' when approaching a problem in SQL. Regards
I'm no expert, but this looks like an odd approach to me. They have a query which pulls continents with population greater than 2,500,000 and then prints results that are not in that set. I'll try to rephrase in a psuedo code more similar to what you're used to, but I haven't worked For continents in world: If: continentPopulation &gt; 2,500,000 then continue Else: print (countryName, Continent, population) SQL WHERE statements can be similar to IF statements without an else and you can also use CASE statements which I believe are like SWITCH statements. I hope this helps some and please let me know if this isn't clear.
You misunderstood the question. Give me all the continents where in that continent every single Country in that continent has a population of more than 25 000 000 which to be fair would likely being up 0 results unless the database does not contain every single Country.
Thanks for the help! I'm aware of the "set based" approach, but kind of difficult to apply it on specific problems. To be more precise, I might need to understand what is exactly returned by each part of the script. This reminds me of the early experience that I learned Excel functions.
Take a look at this https://en.wikipedia.org/wiki/List_of_countries_by_population_(United_Nations) Click the arrow to sort it by population 2017 so that the most populous areas are at the top. Then scroll down until you see the first Country that has a sub 25 000 000 population. Everything above that first one (Australia Btw) is list X, Your where clause Inner Select. Now at this point in the game do you really need to know every continent in that list yet? Not at all because you have the whole list. The only reason the continent is even considered is so that both lists look at the same information because your first list is everything. and list X is everything Minus the stuff that isn't Greater than 25 000 000. 
Hi there, I know this is quite an old post, but I was searching for an entry level jobs using SQL I just started learning it this year and I wish I learned it earlier. Anyways, I've read that you started as a Data Analyst without a college degree and now you're a 'Senior BI Developer' (Congrats by the way!) may I ask what applications/languages did you have to learn beside SQL and Excel? Any point outs would be great! Thanks in advance! 
Thanks, I'm getting the point. Since we already obtain all the countries that have &gt; 25,000,000 population, we don't care about which continent it lays. We just take the whole list to do the NO EXIST check. Also we don't care about duplicates (e.g. both China and India should be on the sub list and they are in the same continent) because we only care about existence. Thanks again for this example! I wish I could generalize this example and get better.
&gt; I wish I could generalize this example and get better. Practice, Practice, Practice. There's a reason I have SQL Server on my home machine with a copy of my work's database. Also just in case you replied before I edited, go back and look at the picture I added.
SELECT [a.name](https://a.name) FROM (SELECT 'John' name FROM DUAL) a WHERE 1 IN (SELECT 1 FROM DUAL)
Real solid advice! Thanks for writing this up! You deserve more upvotes!
So the main thing that will mix you up is that SQL is effectively another level higher than general programming languages like C++ in the abstraction family tree. Meaning the language you use to create SELECT statements does not contain the logic of exactly how the results of the SELECT statement will happen. SQL is also not general at all; it was design to do one thing really well, select data from columns in tables. You can't tell a plain SQL statement to for example select this particular row from a column x times. You can only specify what you want selected and it will return however many rows match. There are however procedural add-ons to specific SQL flavors. Oracle has a PL/SQL language that is Pascal-based, allows you to declare functions, use loops, etc. mixed in with SQL statements. DB2 has its own (ugly but powerful) procedural language as well. Try reading some PL/SQL to see if it clicks more. 
I can understand why you want to know, but let me be clear: knowing how the query is executed is only useful in limited terms like fixing a problem or optimizing your common queries. The query engine can and will execute identical queries differently depending on the data in your table. SQL is almost entirely declarative like HTML is. Asking how a query works is a little like asking how an &lt;h1&gt; tag works. The whole point is that you don't need to look at the man behind the curtain. You need to break out of imperative thinking. Think of it this way: Does it bother you that you don't know exactly what the system is doing when you call a library function, or use an operator? I mean, what really happens when you `cout &lt;&lt; "Hello World"`? It's all abstracted from you. You're not writing in binary code, either. Doesn't it bother you not knowing what the compiler does? How does the OS know to respond to your program? How does the operating system manage the processor? For that matter, what *is* the processor doing? It's running microcode to emulate x86 behavior, but what does that mean? How is it doing that? And what's really happening physically or electrically? Why is a given operation what it is and not something else? How does the processor *physically* decide what to do? Why does it all work the way it does and not some other way? You see how deep that rabbit hole goes? However, let's look to your actual question, because sometimes you need to know for optimizing and troubleshooting: First, some review. Tables work like an *unordered* array or list of C structs. An index works like a hashtable or dictionary of structs. I say C structs and not classes because, in SQL, a given property (field) can only have a singleton value of one of the primitive SQL data types. If you want a multivalued property, you need another table (with some exceptions certain vendor-specific features). Tables are created directly, and indexes are created indirectly (i.e., they're based on an underlying table). The benefit of indexes is that hashtables allow for fast searching. The disadvantage of indexes is that you've got to store them and maintain them alongside the table, so there's more disk space used and more IO when writing to a table. The easiest way to see what the query engine is actually doing is to look at the execution plan that the query planner created. That will tell you what it's actually doing. For example, I'll run this similar query in the student information system I work with: select student_id, last_name, first_name from reg r where not exists ( select 1 from reg r2 where r2.building = r.building and r2.grade = '10' ) So, this returns all students in each building that doesn't have at least one 10th grade student. Since I'm using SQL Server, I tell it to show the basic execution plan with `set showplan_text on`. (Normally I'd use `set showplan_all on` or use the graphical plan in SQL Server Management Studio for even more detailed information including the cost of each operation, estimates used, and the time taken, but, honestly, it's too much information for a basic explanation.) This command to show the plan will be different depending on your RDBMS. That gives me this: 1 |--Hash Match(Right Anti Semi Join, HASH:([r2].[BUILDING])=([r].[BUILDING])) 2 |--Hash Match(Aggregate, HASH:([r2].[BUILDING])) 3 | |--Index Scan(OBJECT:([...].[dbo].[REG].[IX1_REG] AS [r2]), WHERE:([...].[dbo].[REG].[GRADE] as [r2].[GRADE]='10')) 4 |--Index Scan(OBJECT:([...].[dbo].[REG].[IX1_REG] AS [r])) This is presented as a hierarchical tree of the execution. I've added the line numbers for reference. Hash Match and Index Scan are the [logical operators](https://docs.microsoft.com/en-us/sql/relational-databases/showplan-logical-and-physical-operators-reference?view=sql-server-2017#operator-descriptions) at each step. These operators are basically internal functions that the query engine executes. Just like your C++ code gets compiled into binary for execution, SQL gets translated by the query planner into a series of internal functions that the query engine is responsible for executing. The rules for SQL and the relational model mean that *almost every SQL query is 100% deterministic.* Line 3 says that the system scanned the index reg.ix1_reg to filter records with the student's grade from `r2`. If I look at the definition of that index, I can see that it does cover the appropriate column, which is why the query planner selected it: create nonclustered index [ix1_reg] on [dbo].[reg]([student_id], [last_name], [first_name], [middle_name], [grade], [generation], [current_status]) An Index Scan isn't optimal here. It means each row in the index needs to be looked at and it has to do that because grade is so late in the index. We'll get back to this in a moment. Line 2 is aggregating the rows and hashing them based on the building column in `r2`. We have to calculate the hashes because there's no relevant index based on a student's building. What's being aggregated? The internal row ids from the clustered primary key, essentially, so the system can find those rows later. Line 4 is loading the fields to be returned. Note that they're all covered by ix1_reg. We need to do an index scan because we need the field data. It can't be avoided here. Line 1 finally does the anti-semi-join (i.e., what NOT EXISTS correlated subquery means in relational terms). Here the system compares the aggregated hashes of the building from line 2 and uses the list of row ids to decide which rows from the operation in line 4 need to be returned. At this point, the query engine has resolved the logical operations and has identified which rows to return, and it will output the requested rows. Remember I said above that an Index Scan for grade isn't optimal? Well, what if I make an index where grade is the first column: create nonclustered index [ix2_reg_tmp] on [dbo].[reg]([grade]) Here's what I get now: 1 |--Hash Match(Right Anti Semi Join, HASH:([r2].[BUILDING])=([r].[BUILDING])) 2 |--Stream Aggregate(GROUP BY:([r2].[BUILDING])) 3 | |--Index Seek(OBJECT:([MPS_BMA].[dbo].[REG].[ix2_reg_tmp] AS [r2]), SEEK:([r2].[GRADE]='10') ORDERED FORWARD) 4 |--Index Scan(OBJECT:([MPS_BMA].[dbo].[REG].[IX1_REG] AS [r])) Now, instead of an Index Scan on line 3, the system can do an *Index Seek*, which is much better and the system can identify the rows we need immediately. Additionally, we get to do a Stream Aggregate on line 2 instead of an aggregate Hash Match. We don't have to calculate any hashes for identifying the rows; it's already been done in the index! If you want to know more, like how each operator works, then you'll need to look into database design. You're now well outside the scope of just understanding an SQL query. 
I've only needed the debugger a handful of time over the years, and when I did it was a real time saver. Is there an alternative or If need a debugger, I just have to install an older version? 
This does seem a tad illegal.
Has that every stopped a reditor in the past 😂
Has that every stopped a reditor in the past 😂
You'd have to revert to 17.9 to use the debugger in SSMS. I believe the official alternative is Visual Studio now.
Thanks 
But I use the debugger for debugging. Wtf
Google it. Plenty resources. The analogy I liked best is imagine the dashboard of your car - it doesn't tell you any detail (like all the tire pressures), it just says things you **need** to know now (speed and fuel count) and warning lights (but **only** if there is a problem). Basically it only shows information that **you can act on**. This is in contrast to a 'Report' which will have lots of detail - most of it not useful in any given moment. Dashboards will show things like thresholds and targets and compare the values to them or only show them if they are missing the target, thus allowing action (even if that is just looking into it further in a more detailed report) - This allows you to fit much more information on a page kind of, as most of it will not be shown at any given time.
Not to mention completely irrelevant to this sub. Report, downvote, move on.
Open the service properties, go to the log on tab, delete the passwords that are showing in the password boxes and hit apply. You should get a message showing the service account has been granted log on as a service rights. Hit ok and start the service.
Thanks for the detailed reply. I still think there is a difference between not knowing what `std::cout &lt;&lt; "blah";` does and what an SQL query does. Because in C++ I know what I will get (for most of the time) and I can wrap the algorithm in my head (if it's not too complicated). This is impossible for SQL, most likely because I'm not sure what one query returns. I will stick with learning the queries now as my work only needs these. I'm very interested in your explanation of the execution plan and everything else and will see if I can get some formal education about SQL databases after being familiar with queries.
Look into Power BI. Or Elastic Search.
Although it's possible to attempt a dashboard using SSRS, it's almost not worth the hassle. You don't have enough control over the position of objects such as charts etc. 
I tend to use SSRS for transaction data, such as sales volume for salesperson etc. and I use Power BI for my dashboards which is high level summary data.
"dashboards" mean different things to different people. Generally think of a dashboard as a visual aggregation of data displayed in graph, pie chart, bar chart, or even speedometer format like a pressure gauge. Think green, yellow and red that represent good, average, and bad. Simple over complex. Summarized rather than granular. Your report may display data in an aggregated format, but without reading the numbers and doing some mental gymnastics in your head, those numbers don't really mean anything to anyone who isn't familiar with the data. But give someone a speedometer or a temperature gauge with red, yellow, and green, and all of a sudden they are master data analysts! 
Exactly, the difference between a dashboard and a report is conceptual. Dashboards are at a glance, focused on manage by exception principles, and usually combining insights and indicators from multiple systems to give a more holistic "health check" or "Todo list." Highly recommend some books for you to expand on the concepts here, not just the data and the tools Storytelling with Data by Cole Nussbaumer Knaflic Data Visualization: A Successful Design Process by Andy Kirk Information Dashboard Design by Stephen Few The Functional Art by Alberto Cairo Good luck!
Do yoyou need to use SQL for that? I don't even know if it is possible in pure SQL. You could do that in PL/SQL or any other language that supports loops. You need 2 loops for String A and B 1. Find values that occur in both strings (A and B). Declare a counter that counts the number of common values 2. If you find a value remove it from B and increment your counter. Go back to Step 1 3. If you don't find a value and the counter is &gt;= 2 concatenate A with B. 4. Go back to step 1 and use next B string
If the DB you are going to learn on is hosted elsewhere, it doesn't really matter what computer you choose as long as it can run the client-side software for interfacing with the DB. If you want to host the DB on the local machine, look at the specifications of the RDBMS you wish to install and buy a computer that meets the requirements. If you're going to host it yourself, more is better, with a focus on CPU, RAM, and disk. Buy the most you can afford. 
I would recommend a Linux, Windows, or IOS operating system laptop. Odds are you will want it to run VM's if you are on IOS or Linux. The best laptop is the one you can afford that will do what you need it to do. My personal laptop is a crappy lenovo thinkpad 4XX something. 8GB ram, 512GB SSD, i5, and an awful looking screen. I basically dock it and use a few monitors, it works just fine. I have a few dream development laptops I'll be saving for, but that is probably a few years away for me unfortunately. You can find refurbished thinkpads that were used at business sites for pretty cheap and they will do the job. Just make sure to have a SSD and enough ram, I wouldn't go below 8GB at this point. 
It depends on what you are looking to do. If you're OK with using cloud solutions, such as [Cloud 9](https://aws.amazon.com/cloud9/?origin=c9io), a Chromebook will work. If you're looking to do the development locally, you might have better luck with an actual PC or running the Chromebook with a Linux distro on it... but any PC would be better suited than a Chromebook for local dev stuff.
It can work both ways using the distinct keyword. If you did this instead: HAVING COUNT(distinct nobel.yr) = 3 It would return 1 = 3 and exclude the data. Also worth noting - there is a difference between count(*) and count(ColName) as counting a specific column excludes nulls whereas * includes all null and not null records. I knocked together this quick example to show the different ways it can be used: create table #TestData (Name nvarchar(100),otherColumn int) insert into #TestData select 'fred',1 union select 'fred',2 union select 'fred',3 union select null,4 union select null,5 select count(*) from #TestData select count(name) from #TestData select count(distinct name) from #TestData select name,count(*) from #TestData group by name select name,count(name) from #TestData group by name select name,count(distinct name) from #TestData group by name drop table #TestData
Would this similar query help you visualize how it works? It displays the count for each group (the year) where the subject was physics, and orders it by high count descending just for visualization purposes. SELECT nobel.yr, count(*) FROM nobel WHERE nobel.subject = 'Physics' GROUP BY nobel.yr order by count(*) desc, yr desc; Then run it again by putting the having clause on. You can think of the having clause as a second where clause that is for use AFTER you've grouped. You're putting a filter or a condition on grouped data, which is different than the where clause because that puts on a filter or condition before you group. SELECT nobel.yr, count(*) FROM nobel WHERE nobel.subject = 'Physics' GROUP BY nobel.yr HAVING COUNT(nobel.yr) = 3 order by count(*) desc, yr desc; 
Here is what I currently run, which provides all records. I only want to show the unique records of the most recent batch: &amp;#x200B; SELECT \* FROM I\_exhibit where (batch\_run\_no = 11020 or batch\_run\_no = 10969) and cover\_no = 4554 
Thanks! Both solutions worked right out of the box :) The row\_number method makes sense (I used that in another example in another reply), so I'll run with that for now. I will try and unpick the 'string sorted aggregate' over the weekend and once I've got the hang of it, will consider it as a more permanent solution. &amp;#x200B; Much appreciated!
Oh, man! Best of luck with data migration. It’s a pain. We migrated data a year ago and I keep finding all sort of anomalies. We were using a really outdated system so the made things worse. We think it’ll take 2-3 years before we’re where we want to be. 
awesome! thanks, good to have reassurance that im understanding this correctly, thanks for taking the time! 
[dbaTools.io](http://dbaTools.io) 
Thank you my code is now something like this but it is very slow 
Which database server? MySQL, postgres, SQL server, Oracle, etc.?
I know!! But I'm embarrassingly non-technical. I know just enough SQL to be dangerous, and nothing else of much use. ('So learn!' I'm sure you're saying to yourself). Point taken. I'm also not the only marginally technical SQL user out there though. Product Managers, Marketing analysts, Sales Ops and even Finance people often know some SQL, but aren't well-versed in the tools you're talking about. The nice thing about a repository that's part of the SQL editor, is that the saving and searching seems more easily accessible to people like me. And the more I interact with the queries of more advanced users the more I learn. One thing that I think SeekWell needs to (and will do) a better job of is adding in some of the check-in and commentary tools that keep a repo clean, accurate and useful. Until then, the search, filter, tag and save features are doing a decent job of pointing me in the right direction. Thanks for the encouragement toward more discipline in curation. It's a good push!
I can imagine that it takes a while when you have a table with 1,000,000 records. You have to compare every single value with every other value. Its just like how you would do a cross-join and you also have to compare each letter in the string. I dont know how your code looks but maybe there is also a possibility to improve the code itself, like reading the file char by char, using references, using string functions instead of regex etc.
One of the highest changes Ive had to make to get my dashboarding project off the ground is to bring in budget, actual, and forecast numbers from different systems so I can show variances on "steam gauges" to make a quick and easy to see view of performance for a business segment. Honestly this is for the C levels, most directors seem to understand their business well enough that the extra detail is helpful in a full blown report.
What the job title? Requirements? Website?
it has to also look nice. {working at a place that has sql, but some db data goes to birst ($100k a year for web pivot table/ charts for people that can’t use excel}
I think I understand... drop table x_rgb; create table x_rgb ( numberA number, stringB varchar2(10), numberC number, stringD varchar2(10) ); INSERT INTO x_rgb values (30, 'red', 90,null); INSERT INTO x_rgb values (30, 'blue', 85,null); INSERT INTO x_rgb values (30, 'green',95,null); -- INSERT INTO x_rgb values (31, 'red', 95,null); -- INSERT INTO x_rgb values (31, 'blue', 85,null); INSERT INTO x_rgb values (31, 'green',90,null); commit; MERGE INTO x_rgb D USING ( SELECT numbera, stringb, numberc FROM ( SELECT numbera, stringb, numberc, row_number() OVER ( PARTITION BY numbera ORDER BY numberc DESC ) r FROM x_rgb WHERE numbera IN ( 30 ,31 ) ) WHERE r = 1 ) S ON ( D.numberA = S.numberA AND D.stringb = S.stringB AND D.numberc = S.numberC ) WHEN MATCHED THEN UPDATE SET D.stringD = 'newString'; commit; [Result](https://i.imgur.com/luIsWRN.png): Highest value for each partition is updated with "newstring"
&gt; If you're going to host it yourself, more is better, with a focus on CPU, RAM, and disk. Buy the most you can afford. To just get started learning SQL, SQLite on literally *any* computer better than a Chromebook will work. Even a Raspberry Pi Zero would do.
"A reputable healthcare company that begins with an A" just tell us what the company is, dude. Why do you have to be so cryptic?
...Aetna? Athena Health?
&gt; Odds are you will want it to run VM's if you are on IOS or Linux. Why would you need to run VMs to learn SQL?
&gt; Foreign keys in a table are a primary key in another table. note: either foreign key or unique key
This is the most concise, correct answer. Because the row is the country, first (logically first, anyway... I'm referring to the subselect in lines 4-6) we get a list of continents that exceed the population requirement, and then in the main select, return every country that doesn't belong to a continent on that list. Because the exclusion is at the continent level, the country lists per conforming continent will be complete. Very clear what you're trying to do, and most any query optimizer will make it sing.
 CREATE TABLE countries ( country VARCHAR(49) NOT NULL PRIMARY KEY ); CREATE TABLE states ( country VARCHAR(49) NOT NULL , FOREIGN KEY ( country ) REFERENCES countries ( country ) , state VARCHAR(49) NOT NULL , PRIMARY KEY ( country , state ) ); CREATE TABLE cities ( country VARCHAR(49) NOT NULL , state VARCHAR(49) NOT NULL , FOREIGN KEY ( country , state ) REFERENCES states( country , state ) , city VARCHAR(49) NOT NULL , PRIMARY KEY ( country , state , city ) );
You might have a two-column primary key on your Sales Order Line Items table, where they key is on the Sales Order Number and Line Number: Sales Order Number, Line Number, Product ID 1, 1, 1 1, 2, 22 1, 3, 33 2, 1, 44
This is a dependent basis, but I'm not sure of what the OS requirements are of each RDBMS. My understanding is that not all of them are compatible with macs however, so they may need to run a VM with Windows or Linux. Maybe not, but it all depends. 
got it. so through combination a unique primary key is made. thank you and thanks r3pr0b8 for that example as well. 
Disclaimer: I do not work for and am not Affiliated with Brent Ozar unlimited, I’m just a big fan of the team and an avid listener/watcher. Bought the Recorded Season pass for my Team. We’ve seen most of their free videos already and I figured it was time. The sale was perfect timing.
Yeah to search on the company site we need to know which company. Guessing Aetna or Anthem. 
Use SQL Server 2017 configuration manager to set the logon account for the sql server service. That grants the account the appropriate permissions
You're talking *data* migration, not *database* migration. You need someone that understands the structure of both databases to be able to convert your data over properly. You might be better off getting Invision do it for you since they know at least half the equation. Otherwise, you'll probably end up paying someone to learn both databases, *plus* doing the actual conversion work. BTW, *database* migration usually refers to converting a database from one database management system (DBMS) like SQL Server, Oracle, MySQL, etc. to another while keeping the structure intact (same tables, columns, views, etc.).
Hi Jodwahh, &amp;#x200B; I removed the service account I have initially put that was once working and still nothing. I made sure to do this for all SQL services that required the service account in the first place. Anything else come to mind?
At first that was the issue, I would change the password and then go into each service to reflect the changed password and start them back up but as of recent it hasn't been working. 
That looks like it would work, thanks! I never would have thought about merging a table with a subquery of itself, but I think this would do what I'm looking for. I'm curious if there's a way to select only those rows when doing a select statement, but it's not super urgent. I suppose if I needed to I could create a copy of the table, add a column to mark the highest number for each partition and query it like that.
The account should be something along the lines of NT Service\MSSQLSERVER, only delete the password fields.
To select the highest numberC within each partition of numberA? You've already got that in the code... SELECT numbera, stringb, numberc FROM ( SELECT numbera, stringb, numberc, row_number() OVER ( PARTITION BY numbera ORDER BY numberc DESC ) r FROM x_rgb) WHERE r = 1; You could just use that as an inline view in your from statement and alias it, right? Maybe I'm not understanding. 
Oh shoot you're right, that would work. Whoops!
I'll be checking these out, thank you
Potentially missing a commit there. Not sure on if commits are implicit on function DDL in SQLite. 
Tell us the name
That makes a lot more sense to me now. I was wondering how OP had access to the backend of those platforms.
So what were you expecting? People to message you on reddit for a job spec? You should post a spec with the names removed if you're worried about getting in trouble. 
I
I don't know PHP but it looks like you're assigning a bunch of variables and then returning without doing anything with them.
Thank you for sharing this.
If you have to ask, the answer for this is no. Get a Linux, or Windows PC depending on what type of development you will be doing.
Are they? I am probably overthinking it then. Thanks for the help on the LIKE string! 
You could use a trigger, check constraint, or foreign key It's probably better to store those values as data instead of code. It's easier to change that way, which means use a foreign key So create a table drink_names, with those values as rows. And add a foreign key from drinks.drink_name to that table If you want to support internationalization in the future, then you should use a numeric ID or code, instead of the name
Like /u/lgastako said, you never execute the query, so the database isn't even being touched.
Anthem has a few remote analyst openings listed, but I don't see any that state SQL and Excel. Many of the listings are for Business Analyst roles, which are different from data analyst, so I wonder if Anthem's the right company.
Or tableau or domo
So I actually spent over a year working on \*exactly\* this project. Unfortunately, there are no easy answers. The idea of an encounter or an event is something you need to define, not something you get natively. With institutional claims, you can almost get there by just looking at the header files. Just be on the lookout for discharge disposition 30 ("Still Patient"); sometimes you'll have an interim billing situation where the charges are billed each month for a multi-month stay, so adjacent claim headers where the non-terminal claims have discharge dispo 30 should be treated as one claim. Also watch out for split billing - Medicare and a private payor might both bill different parts of the same event, so you'll probably want to use the facility NPI for grouping (or just assume you can only be in one hospital at a time and group everything temporally). For professional claims, it's pretty much a matter of judgement. For emergency visits, we erred towards saying you only have one emergency visit in a given two-day period and lump all claims into that "encounter", since there are plenty of situations where e.g one person reads your MRI, another does triage, a third does the consult, to the point where using the "billing provider" is suspect. For routine outpatient stuff, we generated an encounter for each billing provider on the day. Google won't help you with everything out of the box, but looking on ResDAC can help give some hints on how to interpret the data. Doing this properly takes a LOT of design work. Focus on how the "encounters" will be consumed; ultimately, there's no single objectively correct answer, so just make sure your encounters and events are defined in a way that's transparent and useful.
Yeah - Tableau is a good product.
“Dashboarding” is the building a visual report that helps identify the need to add more indexes, memory, and hard drive space to a given system in an attempt to help a user that created a cross join of a calendar table against a calendar table be able to produce the report in under 3 hours instead of them using your advice and DATEADD functions because “this is cooler” and they really don’t like having to rewrite their code to make it go faster since we can just “throw more hardware at it” and “how has the time to optimize queries anyway?” while not realizing their query is actually wrong. /s
Hi SQLSteinar, &amp;#x200B; I'll give it a look tomorrow, thank you! &amp;#x200B; &amp;#x200B;
How do you define an "encounter"? If we are talking about an inpatient claim, do you mean each "Date of Service" (each day the individual stayed at the hospital) or do you mean the entirety of the stay (4 days in the hospital). Generally this refers to the entirety of the stay for my company. If we are talking about an outpatient claim, an "encounter" is generally each "Date of Service" as an outpatient visit can be many single day visits throughout a given month (11/1/2018, 11/5/2018, 11/17/2018). Is your original claims data in the form of a UB-04 (uniform bill) or an Itemized Bill? Healthcare data can be trick so having more context on if you are aggregating revenue codes and charges would be helpful in answering the question thoroughly!
If you want to go Linux or Windows look at used Dell Latitude laptops. You can get a 3rd gen+ i7 and add a lot of RAM and an SSD for relatively cheap.
&gt;Where t1.unit like ‘cans%’ where t1.unit like '%cans%' if you "cans" if you want to pick up cans anywhere in the unit. What you put will only pick up rows where the unit starts with "cans" q10 is fine though grouping on customername, customerid instead of just customername would be better because it accounts for the possibility that two customers have the same name.
If you are going to dive into the BI world - yes!!! It is a very valuable skill. And so is Power BI...but, to me, Tableau is way more intuitive.
One season pass, or one for each member of your team?
My team is technically two of us, so I just bought one pass that we will watch during lunch and learns through the year. I’d cleared it with my superior to have the organization purchase multiple copies instead, but I figured at $100 it’s cheap enough that I can buy it on my own. Also that means I can use company money to get a year Constant Care, which I am still considering getting.
Only thing I can think of is running the query where you select the items with prices above the average first, copy the id column, then run UPDATE parts SET price = price * .85 WHERE id = 1,2,3 etc (paste the ids here)
I actually spoke with the engineer who wrote the code, apparently we wrote the procedures in the early 2000s back when we had the program written in VFP. We never got around to changing the password encryption. We are going to use a salt unique to each user in the system
[removed]
From the error, it looks like you're using mySQL. I work with SQL Server, but I think you *should* be able to do something like the following &amp;#x200B; `DECLARE @avgPrice int` `SET @avgPrice = (SELECT AVG(price) from parts)` &amp;#x200B; `UPDATE parts SET price = price * 0.85` `WHERE price &gt; @avgPrice` &amp;#x200B; &amp;#x200B;
For extra credit and to blow your instructors mind do another version of the second question and show the cost each customers last three orders by using CROSS APPLY. 
https://stackoverflow.com/questions/45494/mysql-error-1093-cant-specify-target-table-for-update-in-from-clause
I have dealt with EDI 837s and 835s. It’s tricky to pull from different sources but if you are able to find the standard that they all must follow before state submission, you will have an easier time. You might be able to first parse the data based on the loops into manageable tables. For example, Medicare EDI claims from Wisconsin use a specific structure and can be parsed out down to the service provided to the patient. Dental 834 claims data is similar. They use a different structure but the logic is the same. It’s hard to generalize the process but this is close: 1: Python or C# to read the claim file submitted 2: Save to SQL tables 3: Link PowerBI to SQL tables 4: Rinse repeat for each source. *Note: Each state has a different structure and different rules, but most are similar. 
No luck, glad to know that it's not something obvious I was overlooking though. 
Does this help? https://stackoverflow.com/a/31086637
Is it possible that the values are actually `[whitespace]CephS143` etc.? Try `SELECT '|' + DailyTracking.Room + '|'` and see if there's any whitespace that's being hidden by your editor.
As the error states: missing VALUES keyword. You are using VALUE. Switch it for VALUES and you should be fine. You will also run into problems using Select together with the insert. 
You should do INSERT INTO table VALUES (...) even if it's just one value.
I was using the "select * from dual" from the website our teacher had us reference. On the other tables I inserted data into, it didn't mess anything up so I left it. I changed the script to have what you have above, and I get the error, "ORA-01821: date format not recognized".
I did EDI back in the 90's and let me tell you, the "standards" every payer was supposed to adhere to ended up being custom import jobs because each and every payer had their own interpretation of those standards. We had access to the best tools my employer was willing to pay for, and by best, I mean Clipper and frickin' DOS batch language. It was real fun trying to pack all the custom parsing data into a procedural executable that had to run in 640k.
Thats’s because is should be the other way around: to_date('01-JUN-2012', 'DD-MMM-YYYY')
You get that error because the above code is missing a single quotation mark and has the parameters reversed. First comes the string and then the format. Like this: `TO_DATE([string],[format])` so this code works: `SELECT TO_DATE('01-Jun-2012', 'DD-MON-YYYY') FROM dual;` Returns `01.06.12`
the MS Access wildcard is '*', not '%'
&gt; the MS Access wildcard is '*', not '%' the MS Access wildcard is '*', not '%', and the MS Access concat operator is '&amp;', not '+'
develop your SELECT statements first once they're working properly, then you can create the views -- CREATE VIEW viewname AS SELECT ...
I'm not a data science but I played in it a while when I was looking for my first job. I started with python and it made SQL easier to learn. To be able to write basic queries to produce data sets is really much lower level coding than scripting in python or R. In my opinion learning Python made it easier to learn SQL but I'm not sure it would have been easier to learn python if I knew SQL first. I'd say learn python out of the gate. It's a more flexible language with a wide variety of applications, while SQL is really just for one thing. 
Is it possible to keep it in one table? I have some SQL experience, I would call myself a mid level, but I am a above average in PowerBI and QlikView. I have a "membership" file but every time I fool around in the data, I can't seem to agree to a number. I will look at the structure a bit more and see what I can find and possibly come back with more information. I get so scared when dealing with sensitive information, I don't want to give something I wasn't. I am located in PA if it helps.
SQL is a skill that is almost completely unreleated to all other programming languages. When it comes to learning programming in general, learning everything that you need at any time you need it. If you need to write something in python that has to operate on data in a SQL database, then learning both at the same time is the only sensible thing to do. There's also no point in marrying yourself to any particular language. If a particular language seems like the better fit for your projet and you don't know it, just learn it. 
This. dont learn "python". learn programming.
D’oh, this is what I get for thinking I can manage writing code on my phone and on the train. Thanks. 
&gt; alas, an interpreted one so in my opinion it is more of a scripting language because it is not really compiling but never mind What you say about SQL is correct, but the weird bucket you're putting Python into is 100% wrong. It's often the choice of university CS 101 classes. It's one of the languages Google uses for production servers. I've done interviews in Pyhton because that's what a lot of people productionizing ML use. It's a programming language.
I might be bias since working with MDX is my worst nightmare, but whenever I need to query the SSAS I do it using SSMS and adding SSAS as a linked server, then I connect to Analysis service with SSMS, right click on cube and "browse" and basically just drag and drop the report I need and then copy the generated MDX query and use it with my linked server connection. Thank god tabular cubes use DAX.
 FROM dbo.History INNER JOIN dbo.Profile ON dbo.Profile.client_no = dbo.History.client_no INNER JOIN dbo.Personnel ON dbo.Personnel.personnel_no = dbo.History.personnel_no LEFT OUTER JOIN dbo.Personnel_Contact ON dbo.Personnel_Contact.personnel_no = dbo.Personnel.personnel_no INNER JOIN dbo.Provider ON dbo.Provider.provider_no = dbo.History.provider_no WHERE dbo.History.provider_no = @al_provider_no AND dbo.History.payment_date BETWEEN @ad_begin_date and @ad_end_date
if i recall correctly, the asterisk in '=*' identifies which table is the one that must return a row so Personnel is the mandatory table, and Personnel_Contact is the one which may or may not have a matching row (which makes sense -- you can have a Personnel row without a Personnel_Contact row, but you can't have a Personnel_Contact row without its Personnel row) so you got the RIGHT/LEFT outer join backwards ;o)
You are correct, put an edit at the top of my response to use your query.
I'd start with python or R. SQL is only useful to a data scientist if the data they need is in a relational database.
Awesome, glad you're enjoying it!
Thank you for the UI steps! I will try those. How does DAX differ from MDX? Why do you like it better? Why do you feel MDX is so horrible?
There should be a way to downvote such votes likewise Stack Overflow. Providing incomplete details is so amateur. 
thanks i notice we both eliminated the WHERE condition on Provider.provider_no in favour of an ON condition in the join to History
What if I told you there are still some companies that adhere to these standards. Cuz you know, the state. And yep, custom everything. 
How to waste time making a video on a simple task with zero explanation that is already well documented and described elsewhere. Who upvotes this trash? OP, what exactly are you trying to achieve by creating and spamming your low quality videos around reddit?
I generally agree with the comments below that SQL is not a great place to start if you have no "coding" experience. If you've never sat down and tried to get a computer to answer a question for you via *code* SQL is not a great place to start. SQL also requires a lot of knowledge about how datasets are best used and structured, so if you don't have prior experience playing around with data it will also be tough to start out. I would recommend starting with some python. It should be good enough if you don't plan on being a developer.
It’s possible to keep in one table but that will be a huge challenge when you are importing multiple files. Gotta remember, each claim file can have many claim lines. This translates into tons of rows. I’m not too familiar with PA but if you are dealing with state submissions, there’s a standard at least. I generalized my opinion as much as I could so if you have a bit more information on maybe the sample of a claim file you will be working with I can offer a little bit more advice. I’m almost certain that PA uses 837s or CMS or some kind of mixture of the two for claims. Not sure if they are EDI standards but that might be easy to figure out with more information. 
SQL is a great language to learn, and I'd argue you need to understand SQL whether you are a programmer of other languages or not. I'm a database developer, so the majority of what I do is in SQL, however I also know a little C#, Powershell, and DAX for data warehousing. Whether you learn SQL first or not I don't think matters. As many others have stated SQL is not like other languages. As a data scientist will you just need to write SQL that can bring back sets of data to look at? Or will you need to write dynamic SQL scripts and procedures that manipulate data from multiple sources and combine them in various ways? One is very different from the other. I'm guessing (literally don't know) that as a data scientist you would be using SQL a lot more than other programming. So learn SQL and become an expert. Understand Set Theory like the back of your hand. Learn how to model and create relational databases as well. You could go down many paths with a strong skill set in these areas. Also pray wherever you go that they have a relational database in 3rd Normal Form, and if they don't immediately give a presentation to the person in charge why they should create a new data model in 3NF. As for other languages like Python, if you understand programming basics, syntax is really the differentiation. Since I know some C#, I can usually look at another language and at least understand what the program is doing, albeit writing it would be difficult. All languages have capabilities that let you do similar things such as sequence, selection, or repetition for example. You will need to understand this stuff anyway if you ever need to write procedural SQL, like T-SQL or PL-SQL.
In my org, a developer is building out applications (Qlik) and has a very advanced knowledge level of SQL, and is involved in new build projects. An analyst is writing on demand reports and has decent but not advanced knowledge of SQL, with a healthy dose of Excel as well. Top performers are involved in projects as a supplement to developer. The pipeline is Analyst I, II, III &gt; Developer I, II. Hope that’s helpful. 
Wow I haven't seen these old style joins in so long I forgot what the '=\*' meant. I was about to look it up before I saw your comment!
&gt; This is impossible for SQL, most likely because I'm not sure what one query returns. I guess I still don't know what you mean by this. If you run the query in your post, how will you not know what it returns? It'll return a result set with exactly three columns, name, continent, and population, and their data types will match the data types in the table. There will be zero or more rows, depending on your data. Is the problem, "I can write a query that returns data, sure. But how do I know that the query I've written is returning the correct data?" I mean, that's a basic problem even with imperative algorithms. You need test cases and someone who knows what the data means to validate the output.
Which one visualises it?
So where do a Senior BI Analyst/Developer get segmented?
&gt;A developer on the other hand will need to know how to store and retrieve data in a way that people will be able to access it and manipulate it to get what they want. Cool Thanks. How would you differ yourself than a `ETL Developer`?
So do the fields names such as `@StartDate` and `@EndDate` must be real, existing columns in a real Database Table?
No that's why they are called variables not fields. It's the same as in any other programming language. 
I’m not really sure what you mean by get segmented.
I prepped an \[example for you\]([https://livesql.oracle.com/apex/livesql/file/content\_HHMTYEK0YDET0Z6DPY5HNVAA3.html](https://livesql.oracle.com/apex/livesql/file/content_HHMTYEK0YDET0Z6DPY5HNVAA3.html)). It highlights why Oracle converts your string value into a date data type and highlights the difference between explicit and implicit conversion of dates in oracle.
IMX, developer, analyst, engineer, and architect all vary in their meanings from organization to organization. Frankly, unless you have a specific job description that you're looking at, it's going to be really difficult to tell. *Usually* the experience tree goes technician &gt; analyst &gt; engineer &gt; architect. "Developer" usually refers to someone who is primarily creating applications or reports. "Administrator" or "operations" usually refers to someone who is primarily managing an existing set of installed and running applications. "Devops" is a blend. A "devops engineer," then, is about the most meaningless possible job title. It tells you essentially nothing about the type of job you'd be doing. 
I'd say pretty much the same as the other folks here, in our org we also defined these roles: BI Analyst: specifies requirements on data (think which calculations and aggregations) and delivery (which period and how often, like daily, weekly, monthly, in which format and on what channel). Oh yes, mostly done in Excel to specify and also provide a design draft, as most output is also Excel BI Developer: query and aggregate data as specified, use BI tools to automate delivery. mostly SQL + whatever BI solution fits. ETL Developer: establish connections to data sources, retrieve and organize data for later usage by BI Analysts and Developers, and automate these ETL processes. heavily SQL, some custom scripts in bash/python/PowerShell.
Aspect ratio of thorax to abdomen. 
I guess a Data Analyst would fit somewhere in the middle?
&gt; The pipeline is Analyst I, II, III Where would a Senior BI Analyst fit in?
So they are changeable then?
Custom dates the analyst/developer would coin together?
OK great stuff. Will give it a go. Thanks for your help!
Engineers and/or Architects (in my mind) would create the data storage structure. The database, cubes, etc. Developers move the data around, ETL type stuff, and connect systems together.
Im really curious to know how did python make sql easy. Theres literals no connection between them
A simple example would be a report that tracks sales weekly. But if the query has to check several tables (meaning the dat s are typed in multiple times each), you don't want to change the beginning day (@startdate) and ending date (@enddate) that many times. So you set the variables above the query and just change the dates there once a week. So the @_____date variables are like placeholders in the query, and they will take on the values you set above the query.
variable = varies, yes.
Linked server performance is typically poor, and you'll need to use linked servers if you are writing in TSQL. I would look into doing this via SSIS. SSIS is much better at moving data between servers, more flexible about "where" code runs and it's almost as easy to schedule via SQL Agent as a stored procedure. You'll need Visual Studio and some extensions to do the development work, all of which can be obtained for free. One chunk of code to do an initial load and a different chunk of code to do the incremental loads is a common strategy. 
In addition to this, don't promote an example showing people to set the linked server to always authenticate on the remote server with 'sa'. That's pretty much the worst security option you could use. At least use pass through authentication or use another account with specific low-level permissions if you MUST use the impersonation option.
Well... its not compiling. So to me its not a programming language per se. You have a memory guardian, no pointers, no buffer overflow, no running outside of an array, no to a lot of stuff that to me are essential programming skills. But you know what, maybe I’m too old for this shit and programmers nowadays just group together pieces of code from stack overflow and add some loops. 
Senior BI Analyst would be a III. They are very knowledgeable about the business and SQL and the other reporting tools we use and would likely spend 15% of their time assisting junior analysts (I, II) with questions. Generally, if they’re interested in pivoting to development and possibly learning more languages such as R or Python that option is available but not everyone enjoys that kind of work or is interested in the complexity level of long term projects vs building reports as needed so a III would be top of the analyst level ladder. 
There are no variables in sql. And well, there is no equivalent of where in python. I mean ifs... but thats just conditions, what comes in where is more than just conditions. And joins, you never spoke About joins.
You've never had to use declare before or had to code a dynamic pivot? And yeah, if you've ever used pandas dataframes where statements are most definitely a thing. 
TL;DR: Do not pass GO, do not collect DDOS and SQL injection attacks; Exposing a database directly to the public internet and allowing "anyone" to access it is a really bad idea. I mean, **really** bad. Like [crossing the streams bad](https://ghostbusters.fandom.com/wiki/Cross_the_Streams). This will go sideways on you about an hour after its existence is made known. What is the benefit to doing this over providing instructions on how people can host a database themselves? &gt;how to handle/queue lots of concurrent requests; would I need a queueing service like RabbitMQ? Now you're not making the database directly available via SQL queries, but rather an API. &gt;malice/incompetence: this might fit into (i), but basically how would you prevent DOS-like mass querying from a single malicious/incompetent user who wants to crash the service or run up a huge bill? Rate limiting either at the network level or in your API. *Some* database engines have functionality to allow you to throttle user activity, but you're not going to want to pay for SQL Server Enterprise Edition (for example) to get that. &gt;security: the data isn't sensitive, so is making the DB readonly all you'd have to think about? As long as your API itself secure and the credentials it uses to access the database have read-only access. You're really talking about making an API/web service available which retrieves data *from* a database; this isn't a database question *per se* - you need to properly design this interface. &gt;bonus: enable users to create and query from their own tables (can you create/store SQL tables in the notebook itself?) Now your database can't be read-only and you're opening a **huge** can of worms with security.
I'll give it to you I'm not a programmer by trade. I'm a data guy. 
I tried to check whats with the declare you are talking about and I know oracle for 12 years, mysql for 5 and postgresql for 3, I thought maybe Im missing something. But amongst these 3 declare is a part of plsql/tsql and sql - aso thats literally programming. Not sql. I wonder how you did pivots in python, cos in sql thats done pretty easily. Im still not sure how python helped you learn sql
I just told you. Not sure why you don't believe me. 
You are not wrong. I do have to adapt to higher level languages. To me they look so simple and toy-is. I didnt want to get in the Java argument as well, cos I might have the same opinion about Java as well :) and I do work with java a lot. Lucky us theres the god like garbage collector. I dont not call them languages, they are and you have to have programming skills but for some reason to me in the back of my mind they are scripting languages. After all its just instructions for the JVM - they just happen have loops and conditions and variables and functions and OO
The downside is that it is way harder to troubleshoot and make changes to
Yeah, I agree. This sounds very much like an ETL problem and a stored procedure would take a long time and be complicated to maintain compared to an ETL tool's logic. If you are a small shop SSIS is cheaper than most others.
yes, more SQL but still much output would be specifications rather than customer facing BI products.
Fabulous this is helpful. Thanks.
Correct me if I am wrong but wouldn't learning `R` or `Python` derail more towards a Data Analysis/Data Science track?
Thanks. So does being an Engineer/Architect the peak career path for BI Analysts/Developers?
So technically, the `BI Guy` is the backend then if he or she do no partake in visualizations? 
Would it matter that I'm working with a relatively small amount of data? While I simplified my problem in my original post, I'd be moving no more than 200 attributes (hourly records) from a few instances to the new database on a separate instance. Especially considering what /u/dstrait said about doing an initial load first and then incremental loads (which would be hourly I suppose)?
Keep in mind that standard (ANSI) SQL does not have variables. Variables are vendor features, so their syntax, capabilities and availability are completely dependent on which RDBMS you're using. 
Well put.
Pythons SQLAlchemy ORM is fantastic and wouldn't require learning SQL itself. You would need to have an idea of how databases work though. 
I don't bump into many people who are in-depth TSQL people these days and I feel that, if you have to invest in learning something, SSIS is probably more marketable than fussy TSQL. I do not like the overhead of dealing with VS &amp; SSDT, but I also don't like the overhead of getting the security correct for linked servers. This usually means "SQL" and not "AD" credentials and then keeping people who aren't authorized for the linked server out of it (TBH, when I point out that sites have created privilege escalation issues, they go "huh?"). 
Fewer records does translate into diminished concern over performance. BUT, as my other comment says, there are other concerns that would still push me towards SSIS. In short, SSIS is better at dealing with differing credentials needed to connect to difference instances. *If* I was moving data from one DB to another DB on the same instance, I would be more inclined to go with a TSQL procedure. If I were to use a procedure, I'd also look at minimizing coupling with synonyms or views. 
Most common stack for data science I've seen is Python, flask, jupyter, SQLAlchemy/SQL, pandas, scikit-learn, and keras/tensorflow. SQL is absolutely worth learning as it's an ANSI standard language and relatively unchanged for the last 40 years. It's not going anywhere when other languages come in and out of fashion. That being said, for data science using an ORM is becoming popular, and it would probably be worth learning SQL and Python at the same time with SQLAlchemy to tie them together. At better place to ask this question would be /r/datascience. . (after reading this I'm wondering why I'm making pennies as a fish biologist) 
Hi! Here's a link to some tutorials that may be helpful: [https://dataschool.com/foundations-of-data-analytics/](https://dataschool.com/foundations-of-data-analytics/) . There's a course on Visual SQL, as well as individual tutorials on SQL joins, subqueries, and other SQL concepts. If you use the link, I recommend using the search bar in the top right of the page and just searching "SQL". These were really helpful for me, hope your father enjoys them!
In my experience, developers don't care about the data, but rather the data system--datatyping, indexing, ETL, that sorta stuff. Analysts care about the system too of course, but they have no say in how that data actually gets stored. This can be maddening for an analyst trying to pull data, but a good developer is a godsend for an analyst. I would not suggest that analysts "graduate" to developers (or vice versa), or that becoming a developer is the natural progression of an analyst's career (or vice versa). They serve two wildly different purposes. While analysts tend to strike a balance between admin and tech roes, developer duties are almost striclty tech. Developers are on a faster track to an architect role, while analysts could move to architecture or management. I also wouldn't suggest that analysts aren't wizards with SQL. It just depends on what you need and what realm of SQL you're dealing with--DDL, DML, DCL, whatever. I don't know much about DDL but I routinely run circles around our developers with DML.
Depends on what you mean by `BI Guy`. If it's the guy/gal who does BI at the company, turning data into information, I'd say it's more likely to be the analyst and therefore do visualizations. If it's the guy/gal driving the data itself then maybe not.
W3Schools is pretty good for simple, basic learning: https://www.w3schools.com/sql/
Keep in mind that MySQL and MS-SQL use different dialects of SQL, and the migration assistant might not be able to translate it all properly. So you need to test the heck out of things before you go to production.
In layman terms, there is mediocre difference between a BI Analyst and a Data Analyst, right?
Ok dude, here's how companies work. There's a job. The job has requirements. The requirements don't ever fit a "standard" representation of the position. Literally every job is different. There's no "segmentation" between analyst and developer other than an analyst usually analyzes things and a developer usually develops them. That's it my dude. There's no template for a job. Each one has different requirements, and, as such, there is no definitive answer to your question. It's literally unique to your company.
Thanks. Is it a plus if the BI Engineer/Analyst/Developer contains know-how of the business?
BI analyst/developer is one those roles that really don’t have a clear job description as opposed to a title like DBA. I would say the responsibility really depends on the company. I’ve been a sql developer/ sql analyst/ report developer/ data analyst. Although my roles changed, my responsibilities remained more or less the same.
Thanks. An API layer was implied. The main thing is I want the user to be able to give a full SQL query rather than, say, just a keyword that gets injected into a sanitized server-side SQL query. I guess rate-limiting and query timeouts are all you really need then.
Thank you all for the fast and helpful responses!
Thats the most outright comment I have ever read man, of course it is my biased opinion but still. Thanks a lot.
I work with SQL server RDBMS. Could that be an issue?
 SELECT MIN(StartDate) AS StartDate , MAX(EndDate) AS EndDAte FROM (SELECT StartDate , EndDate , DATEDIFF(day,0,EndDate) - ROW_NUMBER() OVER (ORDER BY EndDate) AS Project FROM dbo.Projects) AS x GROUP BY Project;
&gt;what I could do to increase the speed of inserts if my defrag doesn’t work How are you inserting them? One giant batch? You should doing it in batch inserts... insert 50k rows, commit, insert 50k more rows, etc... Dropping any indexes that aren't needed will help increase the speed too. Partitioning the table might help, but without knowing your data structure it's hard to tell. Row level compression might help if your disk is a bottleneck, it'll increase your CPU usage, though.
What do you mean by front end? Requirements gathering?
One method is to drop the index, do the insert, then recreate... but with that many records, I'm not sure how long the index creation would take. What's your platform? Is it a bare-metal server? Cloud? How are you loading the data?
I’ve tried doing batch commits, perhaps my query to do so is wrong, but I tried doing a batch of 10,000 and waited over 30 minutes and saw no change. I even set it up to print the date time after every commit. I ended up cancelling the query which caused a roll back, so to me it seems like it was still trying to process 10,000 records. 
As soon as you allow people to run arbitrary queries, you open up a huge pile of risk and liability. 
How long does the insert into the big table take? Which insert are you trying to optimize, the insert into the stage table or the final insert?
That's true for every job and double-true for jobs in the BI space.
I’m trying to optimize the final insert. The last time I tried running it took over 2 days and I ended up cancelling it cause something seemed wrong. Then I noticed that querying the fragmentation of the index was running for more than a couple hours so I just decided to begin rebuilding the index. But I am worried after I rebuild I’ll run into the same issue fairly quickly. 
The PK for the big table... it's the first column(s) in the table? Do the values for the PK increase regularly (like is it a timestamp or order number?) Or would the first PK values jump around randomly?
Google, "SQL help."
In MSSQL, stored procedures could use variables, as the data is not yet knows. An application could call the stored procedure and pass data to it, this could be user defined data. 
SQL is pretty different in its use from the languages you mentioned. Maybe start with the CodeAcademy SQL practice 
Change insert into A_date (A_Date) values (01-Jun-2012) to insert into A_date (The_Date) values (01-Jun-2012)
From the sidebar: **Learning SQL** A common question is how to learn SQL. [Please view the Wiki ](https://www.reddit.com/r/SQL/wiki/index)for online resources. Note /r/SQL does not allow links to basic tutorials to be posted here. [Please see this discussion](https://www.reddit.com/r/SQL/comments/2jcw2y/what_do_we_think_about_tutorials_especially_basic/). You should post these to /r/learnsql instead.
Is the value of the PK always increasing, or are you inserting values that are mixed throughout the table?
Common Table Expression and Variables Not sure if it's the most efficient way but I would do it by 1) getting the your Summaries by employee ID into a CTE 2) Getting the average into a variable from the CTE 3) Get the rest of the employee data by joining the CTE and those other tables. 
There’s no PK, but as of right now I’m reloading missing data so nothing is ever being added to the end as of yet until it is caught up. 
 DECLARE @AVGRevenue Money; WITH Order_Cte AS ( SELECT O.EmployeeID ,COUNT(DISTINCT OD.productid) AS ProdCount ,CAST(Sum(OD.quantity * OD.UnitPrice) as MONEY) as SalesRevenue From corp.orders as O INNER JOIN corp.order_details as OD ON OD.orderid = O.orderid GROUP BY O.EmployeeID ) SET @AVGRevenue = SELECT AVG(SalesRevenue) FROM Order_Cte; SELECT C.EmployeeID AS "Employee ID" ,emp.lastname AS "Employee Last Name" ,emp.firstname AS "Employee First Name" ,emp.country AS "Employee Country" ,C.ProdCount AS "Number of Products Sold" ,C.SalesRevenue AS "Sales Revenue" FROM Order_Cte as C INNER JOIN corp.employees emp ON emp.EmployeeID = C.EmployeeID WHERE C.SalesRevenue &lt;= @AVGRevenue 
I did these tutorials and it is a good base! 
I started with python and am now learning SQL. It’s definitely different enough but there are some basic things like data types, structures, and the general types of actions you can perform on them that translate. I wouldn’t say that it creates any shortcuts to learning SQL but definitely is good fundamental knowledge especially for someone who has 0 programming experience. 
And that, ladies and gentlemen is why you validate your field inputs and lock that shit down :)
Datacamp has an SQL intro course for free that I’m doing right now. The intro course is free and then to go on to do the rest if their courses is $29/mo. I like the user interface: very hands on and keeps things simple..
I just report on the data I don't have any say in how it's collected, but yes I completely agree.
I just report on the data I don't have any say in how it's collected, but yes I completely agree.
lets just hope your survey doesn't meed with little bobby tables then 
Is your query even executing, or does something else have a lock on the table?
I run sp_who2 every 20 minutes almost to make sure that isn’t the case and there’s nothing block nor anything executing while I am inserting. 
Omg, you're like future me I also have an accounting degree and currently work mostly with TSQL. I use it for all sorts of things such as cost accounting, to generate payroll reports, production metrics, sales analysis, etc. That added finance/accounting knowledge really helps guide you steer you into key areas that need to be focused on I'm also currently learning python, which really helps to clean up data when acquired via spreadsheets (e.g. Getting ACT information from sales, no database access so it's all spreadsheet exports that often need cleanup) Oh but I only 90k in So Cal
I doubt this is the case, but have you confirmed there are no triggers on your big table?
Seems like a waste of processing (if you were fooling with a very large number of records). I would just catch the exception since anything that large wouldn't be numeric. 
It's a third party survey platform so I can only hope they protect against SQL injection. The way I import it is safe.
Prior it use to take roughly 4 hours to insert data. The. I noticed a week or two later that it took 18 hours, then I have never seen it complete after that. I’m currently still waiting for the index to build, but when I run a query to determine ETA and percentage it is show 0% and no ETA. 
There are no triggers what so ever. 
You can just type it as an equation in your HAVING statement... HAVING sales &gt; (sales/orders) 
GIS uses a query structure similar to SQL. My daughter does this. Just google it. Good luck. 
300 columns. Lol. So much wrong already. Who design this.
BI Analyst sounds about right in a generic sense.
I can’t place the staging tables on the same drive as the destination as I am very limited on space with only 500GB which I freed only by deleting a couple hundred million records of old data. Also the destination table is not currently partitioned and I’m horrified to have to drop the index on the table to create the partition. Another option would be to create a new table with partitions and indexes already existing, but again I run into major space issues. 
Research. 
There are like 20 common key words in sql. Learn those and you will be fine
There are a few courses on codeacademy that would be a good start. 
How do you know python and not know how to google about sql? There are literally hundreds of tutorials and courses you can find online. Lazy as f
I would get familiar with the basic GIS data types. [Here's a good article.](https://mangomap.com/gis-data)
lol Everyone at my work thinks I'm an "Excel Ninja"... little do they know, I am actually a Google and YouTube ninja.
H1B Visas are what makes SQL jobs unappealing. The US specifically set up Visas to bring in foreign workers for IT to greatly lower the wages. SQL tends to be an easier skill to acquire than programming and a number of Indian IT outsourcing firms have placed emphasis on database, big data and networking. Then factor in that a lot of US firms are complete and utter morons when realizing their data is their most important asset and that outsourcing your companies brains to India is a bad idea and you have a recipe for getting a job where you'll be outsourced. I've been outsourced three times in 15 years of being a datawarehouse engineer. The level of service agreements for Cognysis, TCS or Infosys are a joke. i was working at one large Californian genetic engineering firm that decided to outsource to a major Indian outsourcing firm. They told my business user that he'd have to wait 6 months to get the view I wanted made, I sent them the code and it was ridiculously simple. My firm was hired to fix the outsourcers firm code because the idiots broke the data model and it was apparent they had no clue what they were doing. Welcome to working as a data warehouse engineer in the US. I'd suggest working for data science or programming for piece of mind frankly.
I see this question rise up in the subreddit once in a while. Can't we make a sticky post and list a knowledgebase of sites best for SQL learning?
You can learn sql pretty fast if you take an online course and keep practicing 
You're talking about outsourcing in general. But it's not unique to SQL jobs. Any part of IT runs the risk of being outsourced to incompetent people. 
So because SQL is so easy to learn, this has made it easier for Indians to catch up (or appear to catch up) to American workers? Why do you say that datascience is safer? What other areas of programming are the same way?
Apologies for the bluntness, but you really need to get out of your bubble, as your post is riddled with inaccuracies that come from some kind of conscious or unconscious bias on your part about the presence of SQL in the overall software developer ecosystem. Why do you think you're unlikely to find a SQL developer job in a startup? That's not true at all. I can find SQL roles all over the startup space. If anything it's more valued there precisely because people like you think there's no SQL work there. Here are just some jobs in the startup space asking for SQL skills https://angel.co/sql/jobs Most of those roles are front end SQL developer - tuning API backends, ORM layers, and OLTP interfaces. The rest are mostly data science roles to analyze data. noSQL is a fringe DB paradigm by a long shot, and even when it is in place, they're often using a SQL dialect to query it. See https://insights.stackoverflow.com/survey/2018/ to see that MySQL, Postgres, and SQL Server dominate database tool usage. Again, you're just as likely to see KV, time series , or graph DBs as noSQL, and most of those use SQL dialect as well. SQL is the 4th most used language among all developers, with 57% reporting usage, among Stack overflow developers. It's ubiquitous. Even if you use Hadoop or Spark or Cassandra you probably write Polybase or Hive or Drill or Phoenix or SparkSQL or CQL. Also, most data scientist roles require strong knowledge of SQL. I can show you job postings at eBay, Amazon, Google, Nike, Apple, Netflix, Stripe ... On and on and on .. that require SQL development capabilities. In fact, if anything data warehousing is just a healthy niche space inside the larger SQL community. 
Some areas of programming are much harder to deskill. Like embedded systems, robotics, etc. 
Colleges using Access to teach databases skills
Do you know about minimum salary lawsfor h1b employees. Depending on where in the country you live 100k a year isn't bad. But if you live in San Fransisco competing against them would be a challenge. 
I took this. It was Postgres but it it did give me what I wanted and I got a great foundation. 
&gt; Why do you think you're unlikely to find a SQL developer job in a startup? I was wondering that too. But... &gt;&gt; So most 'SQL developer' jobs are going to be a mid to large companies or consulting. You'd be unlikely to find SQL developer jobs in a startup, for example. I think he was saying that most smaller startups/webdev type companies don't as often hire people to do **only** SQL and nothing else. i.e. The job title of `/^SQL developer$/` I guess it depends on what your definition of "small" and "startup" are too (regardless of what anyone says are the "correct" definitions). Most "startups" start "small", and the bigger they get, the fewer people will call them a "startup". And smaller companies in general are less likely to hire devs with only one skill, seeing they don't have many staff.
Hi, thanks for the reply. How many hours did it take you to complete the course?
Honestly I don’t recall, wasn’t a crazy amount. 
We have a database for a third party app that is similar. It has 350 columns on one table, 15 or which are ntext. It stores raw sql as strings in tables and executes them dynamically with cursors. There are some extremely 'interesting' database designs out there in the wild. 
Each operations correspond to a REST operation. Each have different pricing but the list of operations at defined here: https://docs.microsoft.com/rest/api/keyvault/key-operations?WT.mc_id=social-reddit-marouill As for what they are priced for, look at the [pricing detail](https://azure.microsoft.com/pricing/details/key-vault/?WT.mc_id=social-reddit-marouill).
I assume that means around 4 weeks or less
Definitely less. 
You cannot pay an H1B visa holder less than market rate for their work and often have to post their position’s salary in order to attempt to fill the position locally first. Only when you cannot fill the position at the posted salary can you then fill it with a visa holder. That’s the law at least.
My DBMS professor jokes about this all the time. We use Oracle db12 and the SQL developer tool though. 
In 2013 I was brought in to fix issues on a 6tb db. The first thing I did was compress (page) all the largest tables in the db along with all indexes clusters/nonclustered etc. Compressing the tables alone freed up 2tb of space. Next I setup partitions and implemented an archiving and loading strategy with partition switching. It works beautifully to this day. Everything is table driven and a single stored procedure is run on a monthly basis through scheduler. What version of sql server are you running? I think you should strongly consider (sql2016 or newer) and implement clustered columnstore indexes on partitioned tables. I'd be willing to bet your 500gb space used would be down to 150gb or less. I would not recommend columnstore index on version lower then sql2016.
This is still sort of what I had in mind about a Job with SQL being unappealing. Like is interest in being an SQL developer diminishing, making the demand higher? Is there just much more interest in different areas like being a programmer or something else? &amp;#x200B; I'm taking a database management systems course that has been my introduction to SQL. From my 4 months of first time experience with SQL, it just seems that more people are interested in programming with Java or Python. &amp;#x200B; So then yea what're the real drawbacks to working with SQL as opposed to being like a Python developer?
If nothing pups up when I ctrl+F "$" in your article about "rising demand for X", your article doesn't mean anything.
Unfortunately I cannot upgrade the sql version and using 2012. I’m very limited on upgrading of software and updates etc. Would you suggest that I drop all index and begin creating partitions? I’m concerned with length of time it takes to have these complete. I was thinking creating the partitions based on this datetime field that is always populated and part of the clustered index. When it comes to partitioning is it simple enough to just add a new partition to the end of the file on the fly? 
Nothing is safe. It depends on how competent your management is when it comes to outsourcing and insourcing certain parts and teams. Ihave teams who are in BI and analytics and they outsource. They suck. They are constantly breaking stuff, deleting the wrong data, using poor practices, no matter how many times we give them suggestions to tune or how to do things, they won't listen. Just this last week for a big deployment they fucked up and loaded test data instead of production data to our clients and boy was that a huge cluster fuck to deal with.
Not true. I've used a number of scientific databases where you can run arbitrary SQL queries, and I doubt they've been set up by idiots who regularly have to quash malicious/incompetent attempts to use them. Here's an example: [http://gavo.mpa-garching.mpg.de/Millennium/MyDB](http://gavo.mpa-garching.mpg.de/Millennium/MyDB) &amp;#x200B; Typically the openly-queryable DBs where you don't need an account have more limitations on them but the principle remains that they are open to arbitrary SQL queries, and I just want to know what are all the important considerations in setting up something similar to what you see at the above link: a publicly accessible GUI-&gt;API-&gt;RDB, and I want to know what all the sensible constraints are to place on such a set up without requiring that users have an account. Note: in the above example, if you have an account, then you can also save the outputs of your queries to interim tables accessible only to your account. I'd been thinking though that by encouraging users to use Jupyter notebooks to ping the API that I could save myself the hassle of providing interim tables.
Mine definitely used ssms. 
&gt; Here are just some jobs in the startup space asking for SQL skills &gt; &gt; https://angel.co/sql/jobs Where? I sure don't see any. Some Data Engineer and similar - many don't even mention SQL much less imply that SQL would be the majority skill you use. &gt; front end SQL developer A what now? &gt; tuning API backends, ORM layers, and OLTP interfaces Because that what comes to mind when people think "front end".. &gt; I can show you job postings at eBay, Amazon, Google, Nike, Apple, Netflix, Stripe Wow you're right. So many startups in that list. &gt; SQL is the 4th most used language among all developers, with 57% reporting usage Exactly. Shit loads of developers know SQL quite well, along with multiple other languages. Vast majority of startups or any companies with, say, a dozen developers don't need one person who only does SQL. That would be a waste of a seat.
It's really hard to say. There are so many variables to consider. I've never had to deal with a table that wide. How many indexes on the table in total? How many columns in the clustered index? What is total table size now? If you drop and recreate a clustered index all other indexes will be recreated. The fastest method to load data is bcp in. Does your 60 million insert setup in a single command or batches. If a single cmd is working for you I'm VERY surprised and your log must be as big or bigger then your db. I don't know that you should just flat out start partitioning the table. I think it would take some serious testing and planning before hand. SQL server partitioning is a little complex and convoluted. &amp;#x200B;
Some thoughts: 1. Like ppl have mentioned, this isn't widely taught. Most folks stick to excel. I work at a fairly large corporation that has been around for decades and maybe 20% of us in analytics know SQL (varies from company to company) 2. Because it isn't widely taught, it creates an odd paradigm, where the skill is valued, but some of the jobs aren't that glamorous because you're just pulling data for other people who can't write SQL. That creates a "seller's market", so those that do know SQL can pick and choose their positions and a lot of the "data monkey" type jobs go unfilled. I just declined a SQL heavy position (slightly higher pay) at a company with a much better outlook because I wouldn't be as involved in the decision making and would be creating reports all day. At the end of the day, SQL is just a skill. It shouldn't define your career, but it will make you more attractive to companies. If you can pull your own data, you'll be able to move faster than if you had to request it.
&gt;Well then that doesn't address OPs question or the article he cited, which is most certainly aimed at SQL and database manipulation as a capability, not an all encompassing job title. The shortage is people with data skills, so that even includes paralegals, medical researchers, accountants, and a million other titles besides SQL developers. &gt; &gt;SQL is a valuable skill in lots of roles. But if your only skill is SQL, I can pay a smart woman in Costa Rica $25,000 a year to do your job so don't expect much. 'SQL Developer' is a specific position title and he asked about 'SQL jobs' (and what makes them 'unappealing') - which made me think he was referring to those. I didn't take it as just a 'is SQL valuable for a developer to learn' question - frankly, maybe I thought that too basic/obvious of a question. My primary skill is SQL - I know zero programming languages - I make 6 figures as a datawarehousing developer. That's the question I was essentially answering.
I agree that testing would need to be done which is why I’m not keen on just attempting it. My 60 million record insert to the staging table is using bulk copy in batches of 10,000 via a C# application I wrote. 10 days worth of data is 60 million, or 1500 files, so you can imagine what 1 month of data is. The query to insert from the staging table to the final table is just a regular insert. I attempted to modify it to insert in batches, but saw no results even with printing the GETDATE() after every commit. I checked if the query was blocked using sp_who2 and it wasn’t. Right now there is only one clustered index based on two columns and a non clustered index also based on two columns. I found that both of these indexes overlap on one of the columns. The total table size is sitting at 5TB of space in disk and 1billion records. The log doesn’t seem to fill that quickly. At least I haven’t noticed since I can’t fully complete a single insert. I shrunk it the other day where it was sitting at 800GB I believe? 
If it helps, I'm fostering my SQL skills as a business analyst, a role I broke into after working years in a bullshit role developing Excel/Access/VBA tools to my life easier. I'm luckier than some, but you have to look for opportunities to carve your own path.
Try data analyst positions
Try the healthcare industry. It's going pretty hard at the moment. My company is full up right now but we're still growing and might be looking again in a year or so.
What do you think about MYSQL?
There's data collection, and then there's good data collection. I have worked places where they think SQL is magic. And no, if you're DB structure is pure garbage this cannot be tied out
Just surprised that a university would not choose an open source database that has all the features needed to teach sql and relational databases and transactions to students.
In my experience, cognizant and isgn are getting more expensive as India modernizes to the point that it's now not a whole heck of a lot cheaper to out source to India. This was a big deal a while back, but in part, has fallen by the wayside as the India contacts get more expensive
Access is a program, SQL is a language
I know? I took a sql course in college and a t-sql online course in my free time. I only see job posts that require "several years of intense high-level SQL automation" which is annoying. I had 1 offer from a place with a proprietary sql-like language. It was literally on radioactive soil though lol. And I interviewed a couple of times with teradata which is another sql-like query. I want to work more with it. I'm tired of hardware/software support 
It's also great. I've been exposed to SQL and Oracle solely. No azure yet, but it's still just a variation of SQL. Mysql, SQL server, Oracle. Are all great enterprise level Access is a interface to databases, but really needs to get a Thanos snap. 
I would recommend checking out public records on h1b salary. I did a quick search in San Jose for last year and found the lowest h1b salary paid by that company was 96k in San Jose. Totally not enough money to live on in that area but just check it out. [h1b data public database](https://h1bdata.info/index.php?em=Cognizant+Technology+Solutions+Us+Corp&amp;job=&amp;city=SAN+JOSE&amp;year=2017) 
Join the table to itself (give one instance an alias a and the other an alias b) on Mbr and datediff(month, a.date,b.date) =1 and a.code = I and a.code &lt;&gt; I Or something along those lines
Off the top of my head in about 30 seconds, so NO guarantees :-) I assume each value in calendar_date is the 1st of some month. I feel like something similar to this might work: SELECT MT1.calendar_date , COUNT(DISTINCT MT1.MBR_ID) AS Members FROM member_table MT1 WHERE NOT EXISTS ( SELECT * FROM member_table MT2 WHERE MT2.MBR_ID = MT1.MBR_ID AND MT2.Calendar_date = DATEADD(month, 1, MT1.Calendar_date) AND Code ='I' ) AND MT1.code ='I' GROUP BY MT1.calendar_date If not exactly that, maybe it'll give you an idea? 
Your data source is for 2017-2018, which coincides with the Trump executive ordering H1B's to actually meet the letter of the law. Go back to the Bush and Obama regimes and they were going beyond rubber stamping H1B's to enter the states. Where I was at, my "replacement" was having to live in a two bedroom apartment with three guys sleeping on a bunkbed to make ends meet.
If I was only doing this once: I'd toss it into a cursor, and increment the value you want, and write the output into a common table. 
Yes, this is correct.
'SQL developer' is not a job. The closest equivalent would e ETL Developer or BI Analyst. Both those roles require SQL. ETL is more or less being replaced by Software Engineering, due to distributed cloud computing becoming increasingly popular and complicated. As a BI Analyst, you write reports. Even that is becoming more complex due to things like Data Science and the need to code stats into metrics.
Can confirm, my company recently created a DBA team (after needing one for years) and outsourced the while thing with TCS.
Databases aren't just used by software guys. Accounting and finance departments at large corporations are full of Excel monkeys that don't know any SQL. Usually they have to request data dumps from someone in the IT department (or the database team, not the "SQL department") who manages the databases, and they're not given direct access to the databases (rightfully so), because an idiotic query could bring the database down or lock tables up.
To answer your question (and the need): &amp;#x200B; Learning/writing SQL is easy. Writing GOOD, efficient and complex SQL is more difficult, but not impossible. WARNING: Offshore SQL development is cheaper so don't expect to stay at an entry level job.
Oh, like that. Yeah sure, I just assumed this discussion was about developers. Our company also has SQL knowledge spread out on non-developers, like the BI department where they have a guy with decent SQL knowledge who can dig up data when the board needs it and such.
Haha it would be nice if it worked that way. I walk around my companies tech building and can’t even find an American citizens some days. They post a job opening with a ridiculously low salary for the position. Then claim they can’t fill the position and bring in an H1B person and pay them like 1/3 of what the position should pay 
This isn't exclusive to Indian companies. I've had this issue when dealing with USA based companies too.
&gt;DB structure is pure garbage Yes, especially if it's the company's enterprise system. I proposed they introduce foreign keys to the database. The enterprise system chief developer had never heard of them before. This was a billion dollar company.
I have not... I'll look into it. Thanks
I tried something similar, but it didn't account for new members in the table. As in, if they weren't in the table in February, it didn't count them as entering the next month because they didn't exist in the table. I'll play around with it, though. Thanks
Thanks, I'll look into that. I've only used parameters in SQL server, I've got to figure out how to do it in Vertica
Have you used Window Functions before? They let you do calculations based on other rows returned above/below in a partition without actually needed to aggregate them together in a GROUP BY. You can partition the "window" in by member and month in this case, and write some condition to count the rows for which Code = I and the Code in the next row != I. This should count number of rows per member per month that arent follow by a non I code value. &amp;#x200B; Anyone feel free to critique this\^ Didnt actually run it, nor have I specifically used Vertica
Windows functions, huh? Sounds like a good lead
I've never used a cursor. I always hear bad things about them
That'll work!
You'd be shocked. I work for a small consulting firm for financial institutions and this is exactly what happens. They send us data on usbs or CDs sometimes, and if something goes wrong and it needs to be resent, oh well, because nobody knows what the original query was so I guess we'll pull something slightly different. Maddening.
Thanks, i'll try this. I get your point about syntactical differences, I'll sort it out. 
Okay, so there's one more level to it that I just got to know. The IDs from table A are distributed into two table, table B and table C. I have to take the count from table B if the ID is in table B, or from table C if the ID is in table C. 
Thanks man. Okay, so there's one more level to it that I just got to know. The IDs from table A are distributed into two table, table B and table C. I have to take the count from table B if the ID is in table B, or from table C if the ID is in table C.
Well, there may be more details to discuss, but for starters: update a set quantity = b.cnt from table_a as a inner join (select id, count(*) as cnt from (select id from table_b union all select id from table_c) as x group by id) as b on a.id = b.id
This is for db2 
I don't know which approach DB2 uses for updates with joins. Did the above work?
I can only check in a few hours now. Will let you know, thanks man. 
True, certain jobs do require it more than others, but even in those professions, conceptual/critical thinking skills are going to make the difference. To your 2nd point, I've seen that be the case (not to the extreme you posted). You can dashboard all you want, but even the best reporting won't answer every question. I get data requests from junior analysts/managers/directors all the time where I just SELECT * certain tables because they are unfamiliar with our DB or how to properly filter the data. Even though that might only take me 5-10 min, priorities always get in the way, or sometimes I completely forget about a request. So you save time that way if you can write your own queries.
I recommend CBTNuggets version of it. It even includes a virtual lab to write your queries in. 
Could try to get the whole test running in a transaction, then rollback after. Put the tests that can't run in a tx into a 'slow suite' that gets run less often.
Thanks, I will check it out. 
I quite like sql developer [https://www.oracle.com/database/technologies/appdev/sql-developer.html](https://www.oracle.com/database/technologies/appdev/sql-developer.html)
After I played around with TS, I managed to run some of the tests and it was lightning quick (from 3 secs to .2 secs for one test). But I get an error about the scope being used by another session for other tests. It seems more complicated and requires tinkering and research. 
[HeidiSQL](https://www.heidisql.com/) is a nice editor. It's free, but don't let that stop you! If you're on a Mac, I'd recommend [Sequel Pro](https://www.sequelpro.com/).
To expand on this, OP probably wants to automate this for each student and not manually type in the number: select 'Average for student ' || id || ' is: ' || avg from ( select id, avg(numeric grade) from grade where grade.student_id = 139 ) avgs You can replace the WHERE clause with a join onto STUDENT to do this process for all students simultaneously.
I made $42k in my first job out of college, in the Bay Area (highest cost of living in the nation). Don't worry, that's a fine salary. I make nearly triple that now. 
Consider yourself fortunate. 40k is NOT underpaid for someone without any experience even in a technical field. Just because some software engineers out of Stanford make 100k as a fresh grad, doesn't necessitate that $40k is underpaid. 
You should find somewhere else. Don't tell them what you make now no matter what they say ("we can't make an offer unless you tell us your salary" that's bullshit, yes they can), and demand a minimum of double what you make now. 
How old are you and did you job hop a few times to get where you are now? I feel like I learned a lot already at my job. I also teach myself extra on the side. I’m strongly debating on leaving after one year because I know I’m worth more than my current salary and they won’t give a raise.
What would you say someone in SF working in professional services/sales engineering for a (larger but definitely not Uber/Airbnb/etc) tech startup should be making? 
I'm 28 and I did job hop several times. I hopped between several contract jobs while doing a part time master's in biomedical engineering (nothing SQL related in the masters - I learned all my SQL on the job). Each job hop gave me anywhere from a 20-30% increase in salary. However I should also note I was working in the medical device industry which pays categorically lower than the tech industry (which I work in now). I'd highly recommend looking for a better opportunity once you hit the year mark (in fact you should begin looking before you hit 1 year, it can take up to 2-3 months to find a good job) Couple questions for you. Perhaps most importantly: which part of the country are you in? Are you in tech? Or a different industry? What BI tools/platforms are you comfortable with? How do you rate your SQL skills? Do you know R/Python, and packages/libraries used for data viz or analytics? 
 select listagg(numbers, ',') within group (order by numbers) numbers from (select t.column_value numbers from num_list_table nlt, table(nlt.numbers) t where st.id = 1 What's the result of this sub query? 
I’m currently in the Midwest. My industry is insurance and health data. I’ll probably start applying in mid-January. I know Excel extremely well, Datastage, MS Sever, MySQL, Talend, SSIS. I’d rate my skills as a B. I’m good at the syntax, if I don’t know something, I know what to look up on google to fix it 95% of the time . I can do JOINS, UNIONS, Subqueries. I don’t really know what else is out there and what an A would look like in SQL. I also understand how a RDBS works so that helps me a lot, I’ve created my own mock database based on Super Smash Bros in MySQL. I don’t know python or any other language; however I know an in-house language that I use to code reports at my current job (obviously doesn’t help me anywhere else). It’s on my list of skills to learn a real language. After I master SQL, I’m gonna jump on that.
Couple things. You'll likely remain underpaid compared to friends in tech if you stay in insurance/health. If salary is important to you, I'd recommend considering moving out of the insurance/health industry, and into or as close to Tech as possible. I'd recommend learning PostgreSQL. Similar to mySQL (but in my perhaps not-unbiased opinion, superior in almost every way) and will be easy to pick up if you already know it. Make sure you really learn CTEs and Window Functions which were not supported in MySQL until v.8.0 and many MySQL users are not familiar with using these concepts. These days it's also good to learn a column-store database such as Amazon Redshift, Google Bigquery, Snowflake, etc. Here's a good post on additional things to learn (you do not need to know ALL of these things): https://www.reddit.com/r/SQL/comments/9t1g2y/advice_on_what_to_learn_for_intermediate_level/
No idea. I use to live in Seattle, so I knew the market there better. Sf is just too dang expensive 
how do you ensure safe imports from potentially vulnerable/injected data?
Check out this community post, if I'm interpreting your post correctly then you can use this technique: https://community.periscopedata.com/t/8017g8/difference-between-row_number-rank-and-dense_rank You will want to use generate_series() to create the list of all dates to iterate through. This blog post shows how to do that. https://www.citusdata.com/blog/2018/03/14/fun-with-sql-generate-sql/
SQL databases are not optimized for cursors. Stay away from them when there is a set-based way to perform your query. Which there is, as the other answers demonstrate. 
Thanks, man! You are lifting us all up. 
&gt;&gt; Learning sql and DW basics has become commodity level skills. &gt; &gt;I'm not sure that is true, or there wouldn't be dozens of SQL-based data analyst jobs paying upwards of $80k I was not trying to trivialize the skills or knowledge. I was only saying that the DW and DB world is going through some massive changes. It all comes down to data. That data is fundamentally changing from transactional data to experiential data. We need to adapt with the times, whether it is just buzzwords or substance.
I see, makes sense. I agree the data landscape is completely transforming right now. However I am not familiar with the term 'experiential' data - could you elaboratE? 
Glaring omission on this list: [Pinal Dave of sqlauthority.com](https://blog.sqlauthority.com/)
I am... so very confused. I'd written at least three queries that day all using % as the varying length wildcard and got the expected results. Now I have to go back and check if it makes any difference at all. Regardless, this is the correct answer. I swapped out % for \* in that query and it removed all the Ceph rooms. Although now I'm back to feeling like I missed what should have been obvious. ; \_ ;
I presume they mean a question like "return the top 10 salespeople who have completed a sale in both New York and California and are also not the top salesperson in their state" 
&gt; Give me an example of when you had to use a cursor? I can't, because I never use cursors when it comes to SQL. Unless you're talking about a psycopg2 cursor which has nothing to do with a SQL cursor. I'm not convinced there's a good reason to ever use cursors other than toy/lab scenarios which prevent you from using useful tools.. Let me rephrase that. I dont't believe there's any scenario in which a cursor wouldn't be better solved either via a set *without* any cursors, or by using python/etc. 
The dashboard is a highly-visual tool that contains graphs, charts, tables, and other visually-enhanced features to quickly summarize key data and information. Users can construct a powerful and comprehensive ‘business dashboard’ via this function for the enhanced presentation of various business indexes and the multi-dimensional data analysis. you can use FineReport,which is a user-friendly BI dashboard software. Dashboard function in FineReport is very powerful and has many unique properties that common reports do not have. Business dashboards designing in FineReport BI and reporting tool includes the following four simple steps: 1. Create a new dashboard. 2. Drag the components you need to the dashboard. 3. Define the data source. 4. Design the dashboard style. The official web:[http://www.finereport.com/en/](http://www.finereport.com/en/)
Hi there, I'd throw in Thomas Neumann's _Database Architects_ blog: http://databasearchitects.blogspot.com Nice list there, thanks, —T
I'm not an Oracle expert anymore, but if I remember correctly, you can treat a nested table like a regular table with the `TABLE` keyword. Something like: INSERT INTO TABLE (SELECT numbers from num_list_table WHERE id = 1) VALUES ('789')
Checking in, im here to learn sql as im sick of waiting months for a dataset to chuck into excel...compant wide issue
What you're looking for is `PIVOT`, though in DB2 doesn't really have that operator. Have a look at https://www.ibm.com/developerworks/community/blogs/SQLTips4DB2LUW/entry/pivoting_tables56?lang=en and https://stackoverflow.com/questions/15529107/pivoting-in-db2
Also Brent Ozar is missing from the list and his stuff is excellent. https://www.brentozar.com/blog/ 
!remindme
That’s the right answer. You realize it’s bad practice so you don’t use them, but you know what they do so you’ll understand code that does.
Honestly, I'd see if I could build my DB somewhere else. AWS supports the newer versions of Postgres which can do recursive CTEs. If I couldn't.... I'd Google heavily. You can probably do it with a series of temp tables if you had a defined recursion limit. Start with adding a "level" column, and then I'd have to do much more coding than I've done in a year. But seriously, if you're using Redshift as a Data Warehouse, because it's hands-off easy to manage... I'd think about denormalizing the data during your ETL and doing that heavy lifting on load rather than repeatedly during analytics. Redshift is built for and really good at summarizing and slicing data, not so much creating new structure. 
Well one key protection is that the would-be attacker doesn't have access to my system, even through a GUI. Any attacks they attempt would be completely blind. Normally with SQL injection you would submit the form many times trying to work out the structure. The final data is exported to Excel which I then import using an R script which does a bulk load. I'm not using any dynamic SQL which is where injection attacks work (as I understand them). One final protection is that this is a relatively low volume of data and all the comments are read by someone. They would notice if a comment was full of code.
&gt;For example, I would like for this statement to return 11. 11 what? 11 words? There are only 10 words in the entire string including the duplicate quicks. 11 Characters? There are 25 for 'brown fox jumped over the' and 44 for the whole string. You need to define your input and expected output because this is not clear at all.
How to index your tables to make your queries perform far more efficiently. 
Try a union for each ID
Same to you 😀. And same to reddit... the only place I know where people like us hang out.
That's true, I commented very early on this post and assumed people wouldn't share this information. I was wrong.
Use `ISNULL(ColName, 0)`.
If the other table has no data then you can use the isnull function. ISNULL(detail.amount, 0) This replaces null values with the value of your choosing, and in this case zero. Alternatively you could use a case statement. CASE when detail.amount = '' then 0 ELSE detail.amount END The case statement would allow you to additional logic and return 0. If you're only concerned about absent values in one table then use ISNULL, it's more efficient.
 SELECT buy.aid, sum(ISNULL(detail.amount, 0)) as amount from tablea buy LEFT JOIN tableb detail on (buy.aid = detail.aid) WHERE buy.aid = 66 GROUP BY buy.aid
Also [Kendra Little](https://littlekendra.com/)
&gt; if no data is found for the second table, you're thinking of LEFT OUTER JOIN, not the (implicit) inner join that you've coded SELECT buy.aid , COALESCE(SUM(detail.amount),0) as amount FROM tablea buy LEFT OUTER JOIN tableb detail ON detail.aid = buy.aid WHERE buy.aid = 66 GROUP BY buy.aid but lo, what's this? you only want one `aid` and nothing out of the `buy` table? SELECT 66 aid , COALESCE(SUM(detail.amount),0) as amount FROM tableb detail WHERE detail.aid = 66 
 select coalesce(sum(amount),0) as amount select case when sum(amount) is null then 0 else sum(amount) end as amount
Very entry-level, but that's not a bad thing. Is there ever _not_ going to be an audience for clear and simple material for beginners?
I assume you are running all flash storage? 5TB of redundant flash is fairly reasonable. I can't imagine doing this on a rotating rust.
i know our dev team uses vsts website. i have the following in my notes. it relates to the first step of building a package for adding a c# solution to vsts. the package i'm working in has the database schema deployment and such "baked in" &gt;download sql data tools. use sqlpackage.exe to deploy dacpac files
It won't work if no rows are returned. At least in MS SQL server.You will need to use an aggregation function to do that (sum() or count()). &gt;if object\_id('tempdb..#test') is not null begin; drop TABLE #test; end; create table #test(id int not null); select COUNT(1) from #test; result: [https://i.imgur.com/to9iQMp.png](https://i.imgur.com/to9iQMp.png) &gt;if object\_id('tempdb..#test') is not null begin; drop TABLE #test; end; create table #test(id int not null); select data=isnull(id,0), data2=coalesce(id, 0) from #test; Result: [https://i.imgur.com/EwfuIIi.png](https://i.imgur.com/EwfuIIi.png)
I thought it was implied he was using a SUM() here...
Sounds like a scam
*beep beep* Hi, I'm JobsHelperBot, your friendly neighborhood jobs helper bot! My job in life is to help you with your job search but I'm just 413.8 days old and I'm still learning, so please tell me if I screw up. *boop* It looks like you're asking about resume advice. But, I'm only ~22% sure of this. Let me know if I'm wrong! Have you checked out TalentWorks, /r/resumes, TIME? They've got some great resources: * https://talent.works/blog/2018/01/08/the-science-of-the-job-search-part-i-13-data-backed-ways-to-win/ * https://www.reddit.com/r/resumes/ * http://time.com/money/4621066/free-resume-word-template-2017/
I'd like to add (Oracle specific) : [https://asktom.oracle.com](https://asktom.oracle.com) \- although it seems to be less and less Tom Kyte himself, lots of worthwhile information can still be found here. [https://oracle-base.com/](https://oracle-base.com/) \- Tim is a really really nice guy as well as an Oracle guru. Great documentation for installs, configs etc. [http://www.juliandyke.com/Presentations/Presentations.php](http://www.juliandyke.com/Presentations/Presentations.php) \- for some great presentations. [https://jonathanlewis.wordpress.com/](https://jonathanlewis.wordpress.com/) \- can get very very very technical. He must have access to things that other non-Oracle people don't because his in-depth knowledge of the voodoo (optimizer) just blows me away. [http://arup.blogspot.com/](http://arup.blogspot.com/) \- his new features are must reads, also some pretty damned good articles. &amp;#x200B; Add one that you should check against other information before using any of their recommendations : [http://www.dba-oracle.com/](http://www.dba-oracle.com/) \- has said things in the past that have turned out to be untrue, has had arguments on OTN with noted experts. &amp;#x200B; For PostgreSQL : [https://www.2ndquadrant.com](https://www.2ndquadrant.com) &amp;#x200B;
True. I saw the isnull() and thought it would not help. But inside the sum(), yep, it would. 
Fair point. I wasn't writing it as a solution, more as a framework. Thanks for adding on.
To be completely honest, without proper dedicated work experience, side projects will only get you so far. The problem is, to a potential employer, 1 side project compared to 10 side projects wont be much different, you aren't in a dedicated role, but you can do some SQL, end of discussion. If you want to get into this area, you're best and most efficient route would be to get a job as a junior is some kind of SQL based role. You'll learn far faster, and it'll go much further on your resume than project work on a non-dedicated role.
my first question to the requestor would be "where the fuck did you hear about data lakes?" my second would be "i'd be happy to... how much is my budget?"
Do you have a "created" date/time field as well?
I've run into the dreaded "Entry level job- requires 2 years of experience" issue. I had to take an SQL aptitude test to get an Entry Level SQL job, and scored higher than 80% of people who take it. I had an interview in which they said they were really impressed with my score BUT they wouldn't give me a job offer since I had no experience working in SQL. Being in Oklahoma there's not exactly a plethora of entry level SQL jobs either, I've been looking at Dallas and Austin as well but haven't heard back from any of the jobs I applied to there.
Should it be as deep as Superior or will Hudson's depth do?
Why do you want a data lake? What do you want it to be able to do? What is the problem you're trying to solve? I've found it's best not to let non-technical people tell me what they need. I have them tell me what they're trying to do or what problem they're trying to solve. Then I tell them what they need.
Don’t have access to do such this as I do not own the environment in which it runs in. 
But at which point will 100 rows turn into 10 and run as long as eternity. I see this as an effective temporary solution, but not something long term. 
I’m kind of in the same boat as you. Do you remember if you did the SQL test from a website and if so, which one?
Try different batch sizes until you find an optimal one. If you have a low latency connection to the database, then even inserting 1 row at a time should be doable, your overhead is mainly just setting up and tearing down the connection. Batches are a double edged sword as you are seeing. Also sounds like you need to ditch your indexes as they are hurting more than helping, and you need to get some more resources on the hardware size. It may be that you are using the wrong database technology anyway considering that you aren't using a primary key or doing any joins, and data integrity doesn't seem paramount. Maybe you want something from the noSQL side of the database family tree here, they tend to have faster inserts.
They heard about it from the big data cloud blockchain AI, silly!
This is for a piece of insurance software. My problem is that all active policies have a date field of the current date, no matter when the policy was issued, so I can't query by that. I use both sqldbx and msms. 
How big is your data? If it’s big enough to constitute the need for a data warehouse, then you need a data lake. If not, then inquire what their wants are before their “needs.” 
MS SQL Server
What is a data lake?
Sarcastic unhelpful answer: If you want a data warehouse but structuring data sounds too much like hard work you just lump all your data together and let your report writers structure it themselves when it comes time to make a report.
Can you describe what the output should look like?
&gt;What is a data lake? A mechanism to store data that allows you schema on read instead of schema on write. It avoids you to conform to a schema when you're just trying to capture and dump data. It is a heterogeneous environment where different users have different needs, from the highly organized to the partially organized to the completely unorganized (but hypothesis driven).
I'm not sure you need to join AR2 with AD2. That relationship is already established in the first query. You should just be able to join AR2 with C, which gives your hierarchy terminator.
See my response above in the comments. Let me know if that works for you
Of course filter your main dataset by your two that's number she wants to compare first. My example only had to fake batch numbers
You have an infinite loop because there is an infinite loop in the relationships. If you create a counter column that you increment with each loop, you can then put in a filter which will cause the loop to stop after a certain number of iterations.
Sounds like something I almost deal with at work. The database I deal with is structured awfully.
I'm still not quite understanding your mix of columns / what you're expecting. I understand you can't give exact data, but can you mock something up that will give us a better understanding of what you're trying to do? Trans | Amt ---|--- 1234 | 10.00 1234 | 5.00 3456 | 15.00 3456 | 10.00 This is what I'm understanding, at this point - am I even close? You'd then expect $15 for 1234 and $25 for 3456, in either a 3rd column or rolled up to a single row?
Ah yep, that's perfect! Noted, sorry I've never really used Reddit for anything constructive before haha. 
No worries! Which do you want? Is all the rest of your data at the trans/item (I'm assuming) level, or do you just want a single row per transaction for all of your metrics? To add a sum into a third column, you would use a window function: select sum(amount) over (partition by transaction) as transtotal from ... this will turn the above table into Trans | Amt | transtotal ---|---|--- 1234 | 10.00 | 15.00 1234 | 5.00 | 15.00 3456 | 15.00 | 25.00 3456 | 10.00 | 25.00 Note that this is assuming you're not using an older version of MySQL, which doesn't support window functions. If you are, you'll need to do a subquery instead. Let me know if that's the case, and you need help, or if the above doesn't quite do what you're expecting.
Ah no, it's actually not perfect BUT I worked it out - there was an error in my join haha. Thank you, will give more context next time! 
"What's a data lake?"
Is there a way to anchor the first query? Something to identify the identify the first appearance of an ID? Dates in the account details table? You don't want to pull all the possible data points at once. Maybe find a way to loop through the records one at a time when you join the recursive portion? Can you use time as way to establish a hierarchy? It seems like at some point you'd have to run out of subsequent records, which would terminate the loop.
That syntax is perfect. What I like to do is write a select that will simulate the update. This way I can see what records I’m about to update AND what they will look like. So... SELECT Goods_Number, Price, Price*1.2 AS New_Price FROM goods WHERE goods_number IN (100,102,105); This should show you both which records your where clause is selecting, and what their old/new values look like. If the select statement gives you what you need, then the update you wrote will work just fine.
Hi, thank you for your quick answer. My problem is, I currently dont have access to the DB itself and without testing my statement im hella unsure if it works the way as intented. So I hope it would update those individual data-sets the way i want?
thank you, that comes in handy. Seems like it works as intented. Thank you very much, again :)
Cool, figured I would have to use SSIS to process this from what I've read so far. What exactly would I be using to process this, part of the data flow options? Just so I know what I'm Googling when I start...
You can do something like this to read the data. --declare @a xml = '&lt;?xml version="1.0" encoding="utf-8"?&gt;&lt;RecurrencePattern&gt;&lt;StartTime&gt;17:15&lt;/StartTime&gt;&lt;EndTime&gt;18:15&lt;/EndTime&gt;&lt;PatternStartDate&gt;03/09/2017&lt;/PatternStartDate&gt;&lt;PatternEndDate&gt;12/31/9999&lt;/PatternEndDate&gt;&lt;Id&gt;{780C257D-AE14-4E2E-9509-109016A29AEA}&lt;/Id&gt;&lt;RecurrenceType&gt;Weekly&lt;/RecurrenceType&gt;&lt;Interval&gt;1&lt;/Interval&gt;&lt;DayOfWeek&gt;Thursday&lt;/DayOfWeek&gt;&lt;DayOfMonth&gt;9&lt;/DayOfMonth&gt;&lt;MonthOfYear&gt;3&lt;/MonthOfYear&gt;&lt;RecurrenceEndMode&gt;NoEndDate&lt;/RecurrenceEndMode&gt;&lt;/RecurrencePattern&gt;' declare @a xml = '&lt;?xml version="1.0"?&gt; &lt;RecurrencePattern&gt;&lt;StartTime&gt;15:00&lt;/StartTime&gt;&lt;EndTime&gt;19:30&lt;/EndTime&gt;&lt;PatternStartDate&gt;03/17/2017&lt;/PatternStartDate&gt;&lt;PatternEndDate&gt;12/31/9999&lt;/PatternEndDate&gt;&lt;Id&gt;{50C31226-8F74-474C-BECB-DE03D82F4F79}&lt;/Id&gt;&lt;RecurrenceType&gt;Weekly&lt;/RecurrenceType&gt;&lt;Interval&gt;1&lt;/Interval&gt;&lt;DayOfWeek&gt;Friday&lt;/DayOfWeek&gt;&lt;DayOfMonth&gt;17&lt;/DayOfMonth&gt;&lt;MonthOfYear&gt;3&lt;/MonthOfYear&gt;&lt;RecurrenceEndMode&gt;NoEndDate&lt;/RecurrenceEndMode&gt;&lt;Exceptions Collection="True"&gt;&lt;RecurrenceException0&gt;&lt;Deleted&gt;True&lt;/Deleted&gt;&lt;OriginalDate&gt;03/30/2018&lt;/OriginalDate&gt;&lt;RecurrencePatternID&gt;{50C31226-8F74-474C-BECB-DE03D82F4F79}&lt;/RecurrencePatternID&gt;&lt;Appointment ID=""&gt;&lt;EndTime&gt;03/30/2018 19:30&lt;/EndTime&gt;&lt;StartTime&gt;03/30/2018 15:00&lt;/StartTime&gt;&lt;Field0/&gt;&lt;Field2&gt;03/30/2018 15:00&lt;/Field2&gt;&lt;Field3&gt;03/30/2018 19:30&lt;/Field3&gt;&lt;Field4/&gt;&lt;Field5/&gt;&lt;Field6/&gt;&lt;/Appointment&gt;&lt;/RecurrenceException0&gt;&lt;RecurrenceException1&gt;&lt;Deleted&gt;True&lt;/Deleted&gt;&lt;OriginalDate&gt;09/15/2017&lt;/OriginalDate&gt;&lt;RecurrencePatternID&gt;{50C31226-8F74- 474C-BECB-DE03D82F4F79}&lt;/RecurrencePatternID&gt;&lt;Appointment ID=""&gt;&lt;EndTime&gt;09/15/2017 19:30&lt;/EndTime&gt;&lt;StartTime&gt;09/15/2017 15:00&lt;/StartTime&gt;&lt;Field0/&gt;&lt;Field2&gt;09/15/2017 15:00&lt;/Field2&gt;&lt;Field3&gt;09/15/2017 19:30&lt;/Field3&gt;&lt;Field4/&gt;&lt;Field5/&gt;&lt;Field6/&gt;&lt;/Appointment&gt;&lt;/RecurrenceException1&gt;&lt;/Exceptions&gt;&lt;/RecurrencePattern&gt;' select col.value('./StartTime[1]', 'time') as StartTime, col.value('./EndTime[1]', 'time') as EndTime from @a.nodes('/RecurrencePattern') tbl(col) select col.value('./OriginalDate[1]', 'date'), col.value('./Appointment[1]/EndTime[1]', 'datetime') from @a.nodes('/RecurrencePattern/Exceptions/*') tbl(col) 
How would that work though? Some families might have 6 accounts, some might have 3, some might have 11. A counter could end up excluding account IDs
I am not too familiar with SSIS but generally speaking, most ETL tools support a variety of data sources (file based, FTP, ODBC, REST/SOAP etc), support a variety of data formats (XML, tab separated, CSV, JSON etc), support a variety of transformations like merging fields, pivoting, grouping or aggregating them etc, and allow you to visually map input fields to output fields. In this case, you should be looking at how SSIS supports an XML data source. See this article for example: https://docs.microsoft.com/en-us/sql/integration-services/data-flow/xml-source?view=sql-server-2017 
Are you able to run the Select statement on its own?
Thank you that worked. Really appreciate the help!
Thank you that worked. Really appreciate the help!
Can you provide a _little_ detail as to what you believe is incorrect? Field sizing? Correct. Size them correctly and use appropriate data types...
Off-topic: What's the name of that IDE?
MS Access
It's Microsoft Access's query designer in SQL view.
hah, I should have realized it was office. thanks
Access 🤮
One possible factor, based solely on my brain and zero research (other than lots of experience dealing with "programmers" and being a "programmer"): Your typical IT education, at least for "programming", starts out and pretty much stays with the standard If-then-else, do-while-not-EOF constructs. Being a decent SQL developer requires casting (ha!) off that paradigm and learning to think in a set-based manner. Also, pure SQL development is a bit like being an offensive lineman in football or the engine room manager on a large ship: Not a lot of activities that are usually considered sexy/glamorous (think web development). 
You're not going to get a career out of just SQL. You need to know more. What else you should learn depends on what you want to do. 
Yeah it's going to be difficult if that's what the market is like in your area, whether you have dedicated experience or not. One thing I can say, is that most companies exaggerate what they truly need, SQL is too big an area for any one person to know it all, but a lot of companies seem to want it all anyway, even for juniors. The best bet is to keep applying until u get a hit, show off what you do know, and most importantly, be eager to learn. For entry level / junior roles, a hunger to learn is often the most important thing. Someone who claims to know it all will be called out on it every time, for a junior, the attitude is the real game changer. At least that's what I've looked for when interviewing candidates, but then, the market where I live is the opposite of what you've described, over here, skilled sql people are in high demand, and companies are more open to juniors who know the basics with limited experience. As long as theyve shown willing and know the basics, the minimum experience term is often overlooked. 
"TheWorstOne"
A few questions: Can you post the schema of \[dbo\].\[Last\_Broadcast\]? How many rows are in \[dbo\].\[Last\_Broadcast\] right now? How are rows inserted into \[dbo\].\[Last\_Broadcast\] -- through the dashboard interface?
Not sure how to do that. 1 Row with 13 columns ID | Broadcast | Last_Seq | Inc_Broadcast | Calculated_Number | Prod_Row1 | Prod_Row2 | Prod_Row3 | Prod_Row4 | Shipping_Row1 | Shipping_Row2 | Shipping_Row3 | Shipping_Row4 ---|---|----|----|----|----|----|----|----|----|----|----|---- 6 | XY | 0002 | 60000 | 60002 | 60200 | 60229 | 60130 | 60198 | 60091 | 60106 | 60106 | 60107 Auto | Static | OCR | Manual | Last_Seq + Inc_Broadcast | ? | ? | ? | ? | ? | ? | ? | ? 
SQL is becoming like Excel- pretty ubiquitous, everyone has some experience.
Since its Access, I think you're missing about 97 brackets ;) (joking around since it's already been solved :p)
Try it out if possible. If it turns out you love working with SQL (Microsoft SQL Server in my case) it’s the best career ever. Been at it since 1995, and I still love every day of it
If you execute this statement, it will tell you the schema for the table (if you have access to view it): sp\_help '\[dbo\].\[Last\_Broadcast\]' &amp;#x200B;
Samesies as above, been working with MS SQL Server since 1999, every year I expand skills and am Sr. Database Engineer now - never a dull day at work. Although I do admit I chose to work in the Video Game sphere so that does help with job enjoyment.
&gt; What SQL command allows me to do this? the SELECT statement specifically, with something like this -- SELECT ... FROM ... WHERE platform = 'XBox' AND genre = 'Action RPG' of course, you'll have to add and populate the `genre` column to your table first
But if games fit into multiple genres, I want to be able to tag them, so I can search for games that may be Rogue Like, but maybe not Metroidvania, or 2D
Don't put the square brackets when quoting the table name in the sp_help call sp_help 'dbo.Last_Broadcast'
It should work with the square brackets as well, though? I'm able to pull schemas using that syntax.
Also, I work at a SQL Server only consulting firm. Our av age is about 50. We need some youngsters to join is in the future :-)
no! at this point you want two more tables -- CREATE TABLE genres ( id INTEGER NOT NULL PRIMARY KEY AUTO_INCREMENT , genre VARCHAR(39) ); CREATE TABLE game_genres ( game_id INTEGER NOT NULL , genre_id INTEGER NOT NULL , PRIMARY KEY ( game_id , genre_id ) ); this is your typical many-to-many relationship if you haven't seen a M-to-M before, you'll need to do some googling
 CREATE TABLE Genre ( GenreID, GenreName ) CREATE TABLE GameGenre ( GameID, GenreName ) --RPG SELECT g.GameName FROM Game g INNER JOIN GameGenre gg ON gg.GameID = g.GameID INNER JOIN Genre AS g2 ON g2.GenreID = gg.GenreID WHERE g2.GenreName = 'RPG'; --Both RPG or Fantasy SELECT g.GameName FROM Game g INNER JOIN (SELECT gg.GameID FROM GameGenre AS gg INNER JOIN Genre AS g2 ON g2.GenreID = gg.GenreID WHERE g2.GenreName IN ('RPG','Fantasy') GROUP BY gg.GameID) AS filter ON filter.GameID = g.GameID; --Both RPG and Fantasy SELECT g.GameName FROM Game g INNER JOIN (SELECT gg.GameID FROM GameGenre AS gg INNER JOIN Genre AS g2 ON g2.GenreID = gg.GenreID WHERE g2.GenreName IN ('RPG','Fantasy') GROUP BY gg.GameID HAVING COUNT(*) = 2) AS filter ON filter.GameID = g.GameID;
Nice! Thanks guys
Here's another good example of many-to-many: [https://stackoverflow.com/questions/7296846/how-to-implement-one-to-one-one-to-many-and-many-to-many-relationships-while-de](https://stackoverflow.com/questions/7296846/how-to-implement-one-to-one-one-to-many-and-many-to-many-relationships-while-de)
Lol, With both Excel and SQL you need to know how to utilise the tool efficiently.. I've been a SQL/Database developer for nearly 3 years and love it but I see people who know some SQL writing some abysmal queries as they don't know much about profiling/optimisation. 
I was going to suggest just reseeding the identity to match your calculated number but the fact that it rolls over from 9999 to 0001 messes that up. If it rolled over from 9999 to 0000 then you could just set the identity value and use that as your calculated number. Does the first record for a given broadcast always start at 0001? (can you assume you'll never run this trigger on an insert, only an update as you have the example above?) For a solution to your question, this might work for you: USE [IT_Applications] GO SET ANSI_NULLS ON GO SET QUOTED_IDENTIFIER ON GO ALTER TRIGGER [dbo].[Trigger_Last_Broadcast] ON [dbo].[Last_Broadcast] AFTER UPDATE AS BEGIN IF UPDATE(Last_Seq) UPDATE b SET b.Calculated_Num = CAST(b.Last_Seq AS INT) + CASE WHEN CAST(b.Last_Seq AS INT) = 1 THEN 10000 + CAST(b.Inc_Broadcast AS INT) ELSE CAST(b.Inc_Broadcast AS INT) END , b.Inc_Broadcast = CASE WHEN CAST(b.Last_Seq AS INT) = 1 THEN 10000 + CAST(b.Inc_Broadcast AS INT) ELSE CAST(b.Inc_Broadcast AS INT) END FROM Last_Broadcast b JOIN inserted i ON b.ID = i.ID END What I might do, if you have the ability to change the table schema, is just convert Calculated_Number to a persisted computed column that adds last_seq and inc_broadcast. This way the only thing you have to manage in the trigger is the value of inc_broadcast. 
Always hiring good SQL Server people. But we are in Sweden :-) www.sqlservice.se
It’s actually a class assignment haha. Definitely would not be using Access otherwise 
I think you can get this for free if a student https://www.jetbrains.com/datagrip/
I did question #1. Here is my full test to make sure it would work correctly. CREATE TABLE #users ( userid int, registration_date date ) CREATE TABLE #orders ( orderid int, userid int, [date] date, amount int, product_category varchar(255) ) INSERT INTO #users (userid, registration_date) VALUES (1, '1/6/2016') INSERT INTO #users (userid, registration_date) VALUES (2, '2/6/2016') INSERT INTO #users (userid, registration_date) VALUES (3, '3/6/2016') INSERT INTO #users (userid, registration_date) VALUES (4, '3/6/2016') INSERT INTO #orders (orderid, userid, [date], amount, product_category) VALUES (1, 1, '11/1/2016', 2, 'keyboard') INSERT INTO #orders (orderid, userid, [date], amount, product_category) VALUES (2, 1, '11/1/2016', 1, 'keyboard') INSERT INTO #orders (orderid, userid, [date], amount, product_category) VALUES (3, 2, '11/2/2016', 1, 'mouse') INSERT INTO #orders (orderid, userid, [date], amount, product_category) VALUES (4, 3, '11/3/2016', 1, 'monitor') INSERT INTO #orders (orderid, userid, [date], amount, product_category) VALUES (5, 2, '11/3/2016', 1, 'cpu') ;WITH cte (numUsers, numOrders) AS (SELECT COUNT(u.userid) AS numUsers, COUNT(o.orderid) AS numOrders FROM #orders o RIGHT JOIN #users u on u.userid = o.userid GROUP BY u.userid) SELECT numUsers, numOrders FROM cte GROUP BY numUsers, numOrders
I'd probably do something like this: with order_cnt as ( select users.userid ,count(orders.orderid) as cnt from users left join orders on users.userid=order.orderid group by users.userid ) select spt_values.number as OrderCount ,count(userid) as NumberOfCustomers from master.dbo.spt_values --to get a list of numbers in sql server left join order_cnt on order_cnt.cnt = spt_values.number where spt_values.type = 'P' group by spt_values.number
Something like this for 1? select distinct ordercount, count(userid) from ( select userid, count(distinct orderid) as ordercount from orders group by userid ) group by ordercount; 
my eye is still twitching on: product_category TEXT but hey.... to each their own denormalized database. I'd be curious why they'd want such a weird report, I'm guessing they wanted how many orders per customer? select users.userid, count(orders.orderid) as OCount from users left join orders on users.userid = orders.userid group by users.userid 2. add having count(product_category) &gt; 1 
I've been working with Oracle SQL and Pro*C since the mid 90's. It's not what I intended to go into, but I am happy with it as a career. SQL itself is fun to work with, especially with odd queries, and I enjoy the other aspects of the job that involve programming.
Only first part of the name (MS) is "TheWorstOne"
I work with SQL-- looking to get certified. My biggest asset is that my education is in economics. So as much as I can pull and manage data, I can analyze it too, though that's not my big pull of my career, it really comes in handy and sets me apart from others. I really enjoy my career, the nice part is that my job is split into two teams-- analytics and operations-- so its pretty cool to work with the data (I'm on the analytics side, obvi) and see the changes going live on the ops side. The other side to enjoying my career-- I work 8 hours, and that is it. I get to the office at 8am, I leave at 4pm (I don't take a lunch, by choice) and it is low stress except for the occasional day when someone is requesting data NOW-- but even then, usually their request has my manager on it so she can say that it is a priority and it gives visibility to my work, so I appreciate having my manager on those emails.
Check out the CASE statement 
Did you try a [Case Statement](https://docs.microsoft.com/en-us/sql/t-sql/language-elements/case-transact-sql?view=sql-server-2017)?
you set security per user (or a group if a lot) via their ActiveDir account. then you have them use their local ODBC as passthrough using AD authentication. that's just for access to the data. if you want the data security, you can either encrypt the data at rest (TDE) or encrypt the data itself. (recommended) 
 SELECT CASE WHEN week = date'2018-11-05' THEN val1 ELSE val2 END FROM myTable;
For question 1, I'd join the two tables to get User ID and the order information. Then I'd do SELECT UserID, count(*) AS NumOrders FROM joinedTable GROUP BY UserID That'll give you something like User ID | NumOrders 123 5 518 3 918 8 152 4 etc. Then I'd do SELECT NumOrders, count(*) FROM aboveTable GROUP BY NumOrders That should give you Num Orders | count 1 5 2 2 3 9 etc. &amp;#x200B; For question number two, I'd do: WITH custOrders AS ( SELECT UserID , OrderDate , Category , ROW_NUMBER() OVER (PARTITION BY UserID ORDER BY OrderDate) as r FROM joinedTables ) SELECT a.UserID FROM custOrders a INNER JOIN custOrders b ON a.r = b.r - 1 WHERE a.Category = b.Category This will give a row number to every user/order based on the date. When you do the join, the ON criteria states that a.r = b.r - 1 meaning that they must be successive orders. And the WHERE statement says that the categories have to match. There might be a way to do this without a self-join, but this is how I'd do it.
It's best not to think of Access an an SQL Server.. It's a file-based database, with some networking and SQL support, but still quite different from an SQL Server. Regarding encryption, its probably a question for their compliance/legal department if they have one. Personally I would focus on bolstering access control and auditing, to ensure people cannot access any data they are not meant to (or if they do, you have a record of it). After that you could start considering your encryption policies 
It sounds like you're confusing Access with SQL Server? Or do you mean they use an Access Application to connect to a SQL server?
what kind of job titles that will have only sql? I wanna start with sql and work my way up. I did (Canada) College in computer programming and did some courses with sql and loved it but read somewhere there was no such thing as sql only jobs which made me sad
Wherever you read that wasn't being honest. Of course, the more you know the more marketable you are, so if you throw in server administration then ur ready for DBA. Just kinda repeating that starting at SQL as a language will lead to many things, including some jobs that writing SQL is all you do. 
MySQL has an excel plugin that lets you do this with one click, inside Excel. MSSQL has the data import tool. If you're looking for a more generic and robust solution, look into Pentaho.
It sounds like they are using an access front end connected to a SQL Server back end. I don't know know much about access control in that context, but with other front end solutions you should be able to identify the user who is currently logged in. Once you have that information you should be able to set up the application to filter rows returned to the user based on a number of criteria (group that user associated with, records created by that user, etc) Most multi-tenant databases I have encountered work on this concept. (using a tenant ID as a discriminator) In my job I am asked to make several ad hoc data centric applications for orgs. If they are talking about migrating from access to SQL Server, I can highly recommend Cuba Platform. It has a built in access control engine and supports LDAP, etc. Also users can access it via any web browser. You can also model an application on a existing schema. It is free to use for 10 custom entities (think tables) or less. It's like 375 USD a year for the pro version. https://www.cuba-platform.com/ Good luck on the interview
thank you!
Do you mean the circular dependency in the table? Or the code? 
In the table.
I took the IKM back on 1/9/2018. I don't recall everything, but I do recall it was based on ANSI SQL and not any specific flavor. You should have a very broad knowledge of SQL as it touched on a little of everything. I remember it being all multiple choice, but I could be wrong on that. Did you have any specific questions? It did not feel very difficult, but there were differences between T-SQL and ANSI SQL I had to kind of guess on. Having experience in MySQL, DB2, Sybase, and Oracle did help substantially. If I had only one flavor of experience, I probably would not have done as well. In the end, I only scored a 92/100 and in the 95 percentile.
Here's a working solution based on the description of the output you gave. Unfortunately, it's pretty hairy. This is what the output looks like: AccountGroup|AccountID|Account_RelationshipID|AccountDate :--|:--|:--|:-- 1|1|2|2012-01-01 1|2|4|2012-01-01 1|4|6|2012-01-01 1|6|11|2012-01-01 1|11|1|2012-01-01 2|15|17|2015-07-01 2|17|19|2015-07-01 2|19|15|2015-07-01 -- Create mock tables and data declare @Account_Relationship table ( AccountID int, Account_RelationshipID int ) insert into @Account_Relationship values (1, 2), (2, 4), (4, 6), (6, 11), (11, 1), (15, 17), (17, 19), (19, 15) declare @Account_Detail table ( AccountID int, AccountName varchar(50), AccountDate date ) insert into @Account_Detail values (1, 'Dave', '1/1/2012'), (2, 'Dave', '1/1/2013'), (4, 'Dave', '1/1/2014'), (6, 'Dave', '1/1/2015'), (11, 'Dave', '1/1/2016'), (15, 'Paul', '7/1/2015'), (17, 'Paul', '7/1/2016'), (19, 'Paul', '7/1/2017') /* This first CTE detects the account relationship cycles and generates the anchor list for the recursive CTE further down. I separated this code into its own CTE so I could reference it later in the recursive part of the recursive CTE. I define an account ID as the achor of a cycle if it meets the following criteria: 1. An AccountID (whose RelationshipID equals the current AccountID) exists that is greater than the current AccountID 2. An AccountID (whose RelationshipID equals the current AccountID) does not exist that is less than the current AccountID */ ;with anchors (AccountGroup, AccountID, Account_RelationshipID, AccountDate) as ( select row_number() over (order by ar.AccountID), ar.AccountID, ar.Account_RelationshipID, ad.AccountDate from @Account_Relationship ar inner join @Account_Detail ad on ad.AccountID = ar.AccountID where exists ( select 1 from @Account_Relationship ar2 where ar2.Account_RelationshipID = ar.AccountID and ar2.AccountID &gt; ar.AccountID ) and not exists ( select 1 from @Account_Relationship ar2 where ar2.Account_RelationshipID = ar.AccountID and ar2.AccountID &lt; ar.AccountID ) ), /* Here is the recursive CTE. The recursive anchors are just the contents of the previous CTE. The recursive section of this CTE breaks the recursive loop when the current AccountId is not greater than the AccountID from the anchors CTE. You will get an infinite loop if this filter is removed */ C (AccountGroup, AccountID, Account_RelationshipID, AccountDate) as ( select AccountGroup, AccountID, Account_RelationshipID, AccountDate from anchors union all select c1.AccountGroup, ar.AccountId, ar.Account_RelationshipID, c1.AccountDate from C c1 inner join @Account_Relationship ar on ar.AccountID = c1.Account_RelationshipID where ar.AccountID &gt; (select a.AccountID from anchors a where a.AccountGroup = c1.AccountGroup) ) select * from C order by AccountGroup, AccountID Let me know if I misinterpreted your output description.
I became an 'accidental dba' after i inherited some roles a few years ago. I learned a heck of a lot along the way and was honestly super lucky i think in how it unfolded, but overall am very happy with what i do and feel like there's still so much room to grow into it.
Golden rule of security. Backend backend backend. If the bottom most layer of data storage is a sql server then sql server should dictate security. Not the front end, which could be access of excel or a web app.
I'm a data analyst. In use more than just SQL.
Sure sure. When I was a Data Analyst, so did I :-). Other than SQL I did (and still do) use Python mainly. But..I still know quite a few Data Analysts that have been more or less successful knowing only SQL. So I think that it's unfair to them to say that they CAN'T do what they do with SQL as the only programming language. Of course, if you mean they have to use other "tools" besides SQL, like Tableau, SSRS, etc... Then you are quite correct.
Yup. I was a Data Warehouse Developer for a bit and wrote only SQL ( PL/SQL ) while in that position without using other tools as part of my core job function.
Woah there that’s a lot of assumptions. I work a company with over 6,000 employees and this is one of many products. We have plenty of qualified people, I’m just thinking about it from a point of interest. Like I said, I’m an analyst, this isn’t something I’ve been tasked with fixing, as much more qualified people are at the company. Mostly I was using it as an example of an overall concept. Most payables are in a 1:1 relationship to checks in this system. I appreciate the suggestion with the rollback, but not really appreciating the tone and baseless assumptions. 
Always hiring good SQL Server people. But we are in Sweden :-) www.sqlservice.se Edit: copied my answer here, where it belongs
If your not tasked with fixing it, why do you care at all? I agree that transactions are the way to go at a high level, but this sounds like a massively kludged together solution. If it's as critical as you're implying you should probably address the overall structural problems with this approach rather than just throwing more Band-Aids on top by putting everything in one big transaction.
Because I like to treat things as learning experiences when I can. This large issue made me think of how I would address something like this on a smaller scale. Again, I’m not throwing bandaids on it, I’m not touching it. Dev team is probably breaking payable posting batches into multiple batch IDs. Really it doesn’t matter. My question was more along the lines of try blocks/catch blocks and rollback transactions and how other tools can be used to safeguard stored procedure processes that are executed on button-click. 
The pandas library for python has good tools for moving things into and out of Excel and DBs
Short answer: every good relational database has atomicity and durability as a very core part of it's architecture: any piece of work you give the database can be divided into transactions of practically any size. The db will commit transactions in one go: either the whole transaction gets committed or none of it does. https://en.m.wikipedia.org/wiki/ACID_(computer_science) At a random guess though there's probably a lot more going on in your system (is it interfacing with other systems maybe?) that means slapping some transaction control on the "function" isn't easy, but this kind of thing is an absolutely ordinary situation in CS and system design so I'm sure that whatever the specifics of the issue are there is a design pattern to deal with it. 
Wait you can join a table onto itself? Holy cow that is so useful, thank you
Thanks for that. So I would just install MySQL and the excel plugin and would be ready to go?
you forgot SELECT
Thanks for the help! I figured out how to do it based on the suggestion you made, and it works! but i'm not sure how efficient the query will be at this point and so i may have to streamline it.
If you want to automate this I would suggest a database function like UNPIVOT, which is a database function depending on your used DBMS (MySQL and MariaDB doesn't support this function). Another way would be to use a dynamic SQL query. &amp;#x200B; The value table seems like a periodic finance table, one column per period. Usually I would recommend a schema change to the CTE schema or a little ETL/ELT workflow to optimize the operational data before reporting. &amp;#x200B; WeekSelection |id|week|selected\_value| |:-|:-|:-| |1|2018-11-05|1| |2|2018-10-29|2| &amp;#x200B; WITH week_vals (id, week, type, value) AS ( SELECT id, week, '1' as type, val1 as value from tab UNION ALL SELECT id, week, '2' as type, val2 as value from tab ) SELECT id, week, value as new_val FROM week_vals wv inner join WeekSelection ws on wv.week = ws.week and wv.type = ws.selected_value
As a datawarehouse tester I am working in SQL for past 5 years. It is a rewarding career with moderate challenges and career prospects 
So... The pertinent part is &gt;Login failed for user 'sa'. Check permissions and/or password for that user and that the script is properly authenticating.
Oh real dba's you've seen the bat signal he needs you. .... *looks around *... .... Nothing but accidental dbas insight. 
Lol 11. Patindex 11 because it's 11 chars between 
To add to this, it appears that the service is running as the default service account but it's trying to authenticate as 'sa'. Sometimes, as a best practice, people set the system to only use windows authentication or they disable 'sa'. I would start by checking those two settings (check the server mode if it's windows only auth or mixed auth and check if the 'sa' login is disabled). 
i didn't test it. i was going by memory. i was positive i've tried doing that in the past and it failed, maybe memory failed or something changed.
ok that's what i thought. i think the example above should work for you, or be very close.
It was, I added an additional line to update the calculated num as well. 
You'd have to write your own application in python - I don't think this is the best solution for you, unless you have programming experience.
Yes. It's even bundled in the same installer. This is assuming you're looking to set up a MySQL server locally - are you trying to connect to an already existing DB?
Okay, no worries!
I think an important question is, why are you using maintenance plans? There are better ways, unless you have no choice.
It's a built in component of MSSQL above the Express version. Why wouldn't you use it.
Is `SAS` a reporting tool? I wonder could `SQL` could also be considered a second degree reporting tool besides a storing tool.
Would it be fair to say that `SQL` is a reporting tool as well besides being a storing tool?
How do DBAs differ from SQL Developers?
Python and SQL = Data Engineer?
Nope.....More like Python, SQL (multiple dialects), data modeling, Java, Spark, Linux, Docker, Azure (or AWS/GCP), NoSQL, Kafka, DataBricks, etc... Python and SQL are just the 2 most universal prerequisites. Java or Scala paired with SQL will get you there all the same.
And making people read through a [giant SQL script](https://github.com/olahallengren/sql-server-maintenance-solution/blob/master/DatabaseBackup.sql) to find where it's broken or causing issues isn't obfuscation or even a PITA? The built in solution works fine, the OP even had the problem sitting right in front of them "Failed to login as SA". 
SAS is a reporting tool, and with SAS Enterprise Guide I am able to write SQL code in there as well using ‘proc sql’ steps, so I see them both as very similar data reporting tools.
Yes, the password for sa has been changed a week ago. I replaced now and it works. Thanks! 
You do you, man. The setup is easy, and in 5 years, I've never had a problem. 
 SELECT Person.Address.AddressLine1+' '+Person.Address.AddressLine2 as Road Haven't written SQL for a few months but that's the shortest way I can remember off the top of my head
I would say so, I only do my 40 hours a week and then I go home. Work stays at work.
I'm sorry that you feel the need to do this kind of thing.
Thanks for the feedback! Can you elaborate on why Redshift isn't ideal for transactional workloads? Is it due to it's columnar nature?
I’m pretty sure you have to be a republican to do this kind of thing.
You can use ISNULL to check for NULL SELECT Person.Address.AddressLine1 + ISNULL(' ' + Person.Address.AddressLine2,'') as Road
Yes, it's the columnar design. Columnar is optimized for reads and is slower for writes.
Thanks, thats exactly what I did.
To lie? Like you did repeatedly yesterday? Polling locations are the same for Democrats and Republicans. You flat out lied saying that wasn't the case and continued to say so on numerous occasions. You're no better than the people you claim to be the problem.
True. They move the computers around on the Democrats so they can't do this.
Are you actually the person in that 21 miles picture? Do these people follow you around everywhere now?
Then all you should need to do is wrap the second part in a COALESCE. I have a habit of TRIMming addresses too if I do something like this because our data is whack... RTRIM(Person.Address.AddressLine1)+' '+COALESCE(RTRIM(Person.Address.AddressLine2), '') AS Road &amp;#x200B; &amp;#x200B;
I’m not. It’s funny. 
Yes thats really me and yes apparently the worst of the worst are now following me around harassing me. They're also PM'ing me insults and death threats. If that's what they need to do to feel important then whatever, it isnt affecting me.
Well, enjoy it to your hearts content! Have a good day!
Have a great day!
To feel important? Kind of like you did when you lied all those times about being forced to vote in a further polling location that your Republican neighbors? Look at the pot calling the kettle black. You're such a hypocrite. 
Ok. Have a good day. 
I use it as well. But suggesting basic users need to use it is a bit much. Sometimes you simply dont have the need for anything other than the basics.
It would be but I have to drive so far away to be happy. My Republican coworkers can be happy just down the street. Maybe so upvotes from strangers will fix my mood.
A columnar database flips a relational database on its axis. With relational databases, as you add rows the queries take longer, however adding columns generally doesn't impact query time as much. With a columnar database that has been reversed, so now the amount of rows doesn't impact performance as much as adding in additional columns. Its not a 1:1 performance shift between the two. The more columns you have in a columnar database the longer the load time.
Well maybe it’ll get better. 👍
My nice day is so much further away than the nice day my Republican neighbors get. Woe is me. Plz feel sorry for me :'(
Nope definitely not. They're going to change it on me again. They always do. But they never change it on my Republican neighbors for some reason.
Hey CommonMisspellingBot, just a quick heads up: Your spelling hints are really shitty because they're all essentially "remember the fucking spelling of the fucking word". You're useless. Have a nice day!
Our plan is to dump data into a staging schema on our ODS and then kick off SQL operations to transform and write the formatted data into normalized tablespaces in other schemas on the same ODS server. Then we'd publish a replica ODS without the staging schema. The replica db is where we'd create views and aggregates for reporting and also a place where we might run ad hoc queries. Finally, we'd push the views and aggregates to a data warehouse for reporting via BI tool like Chartio. How would Redshift compliment, interfere with, or replace any of that plan?
Sounds like redshift would be the data warehouse in your final step. Also sounds like you need an etl tool.
If I'm understanding what you're trying to accomplish, it sounds like you want a pivot. The syntax varies depending on your engine, and some don't have that functionality.
Something like this, You need to join the two tables more than one and for the second and third joins says that the the date needs to be greater than the class that came before it Select Users.Username, Users.userid, Class1.class_date, Class1.class_id, Class2.class_date as Next_Class1_Date, Class2.class_id as NextClass1_ID, Class3.class_date as Next_Class2_Date, Class3.class_id as NextClass2_ID from Table1 as Users left join table2 as Class1 on Users.userid = Class1.userid and Classid in (Select min(clasid) from table2 group by userid) Left join Table2 as Class2 on Users.userid = Class2.userid and Class2.Class_date &gt; Class1.Class_date Left join Table2 as Class3 on Users.userid = Class3.userid and Class3.Class_date &gt; Class2.Class_date 
It’s an IKM assessment test for SQL. I got it through a recruiter for TekSystems
Yeah location is a big factor. For example my cousin in California who has very little SQL experience got a job as a DBA because they say he can learn on the job. In comparison I have been learning SQL/Oracle for over a year and I can’t even get a sniff at an interview.
If I was near the DC area I’d definitely hit you up, buuuut I’m in Oklahoma lol. In 10-15 years there’ll probably be a lot more SQL jobs, especially entry level but for now they want a minimum of 3 years work experience no matter how you do in an interview or score on an aptitude test. It’s very frustrating because I know people that have very little SQL knowledge getting jobs in other areas. I’m talking they have no idea of primary keys or foreign keys let alone other things. 
it's great for extremely simple, mostly non-relational data no data stays non-relational, and it's missing too much of real sql
That's a pretty clever way of pivoting, especially if you can assume that the number of joins is finite. Dawns on me you could possibly use this pattern to make a recursive pattern too possibly.
Redshift strengths are traditional DW strengths - scaling, especially horizontal scaling as you said. But then, Aurora Postgres also gives it a run for its money. Then there's speed and performance. DW data sizes are typically in the hundreds of millions and billions of rows scale. You're right. In the tens of millions, a well partitioned well indexed RDBMS table works perfectly fine. Heck, even a bit larger than that. Why don't you look at other options? Take a look at Snowflake - they have very interesting take on separating storage from execution, auto-caching etc.
The difficulty with just pivoting this data is that you need a relative link between the class dates. You could probably do it using a recursive CTE to generate the data which would then be pivotted. That is advanced SQL though and this is an easier to understand and modify solution.
Redshift is a data warehouse. You'd want to use it for your analytic database, but not your operational one Use PostgreSQL for your operational database Yes, I've used both professionally 
Hey, thanks for the help with this. I'm more or less returning data in the format I was looking for. I do have a follow up question if you have the time. The actual tables i'm working with are much bigger than my example image (as one would imagine). When i run the script, I receive multiple lines. Basically, the first line is for the user, and shows their 1st, 2nd, and 3rd class date/ids. Great! Then it returns another line, with the 1st 2nd, and 4th class date/id for the same user. This cycle repeats until it's created a row, and replaced the 3rd column date/id with every class date/id recorded for the user until it's listed them all. Is there a way to stop this script so it only returns the 1st 3 classes, and then stops returning results? 
Did you say what engine you're using? I might use a window funtion to order and then ship it from joining if too high but today won't work on all dbs
There's your answer, move to california lol. If only it was that easy 😁 You mentioned SQL/Oracle, I would suggest you find what's most popular in your state and focus on it. If that's SQL Server, ditch Oracle, if its Oracle, ditch SQL Server. Try to specialise in one. Don't bother with MySQL or Postgres if you want high wages. I'm in the UK, and although I could quit my job today and probably be in another in a couple of weeks, the wages are about 1/2 to 2/3 what I could get in the states, so I've had to freelance to break through the ceiling. It's all swings and roundabouts. Keep trying, you sound like you have the right attitude, you'll get something eventually!
Probably look into using hashbytes and then joining hash to hash. 
If I moved to California I’d be homeless 😂. For 300k in Oklahoma you can get a mansion, in California you’d have like a 2 bedroom house if that lol. Oklahoma is leaning more towards SQL server, but my college course has Oracle certification classes built into it so it’s not as easy as ditching Oracle either lol. Well all I can do is keep studying and chugging along, I’m working on Oracle certs and a Udacity data analyst nanodegree currently, hopefully I can get a proper chance to show what I can do. And thanks I appreciate it! You know what they say, “throw enough shit at the wall and something’s gotta stick” 😂
Don't be put off by that, my course also used oracle (without official oracle certification), but my first job after finishing it was in a sql server role, and I haven't looked back. Oracle tends to pay more, but it's hard to find an oracle job around here. At the end of the day, if your just starting your career, you can still be moulded to another flavour of SQL easily enough, they all overlap. It's harder once you're a specialist, but by that point you won't want to switch anyway. If SQL Server is the dominant one in your area, do your oracle certs at uni, but be open to change. And yeah, something will stick. It'll be worth the wait, I love my job, I'm sure you will too when you find the right one, just need to get that foot in the door! 😊👍
it's a piece of cake -- the GROUP_CONCAT function will do that nicely, although not with separate columns but a concatenated string of multiple date/class values
Ugh. I had to do a bom query last year. Gets really fun when products are used as a base for other products, oh and also the part number structure isn't consistent. I'll take a look at your query tomorrow and see if I can help.
There is no need for the replace. Just set EndDate = GetDate()
 UPDATE AW.Production.ProductListPriceHistory SET EndDate = COALESCE(EndDate, GetDate()) WHERE ProductID = '879' AND StartDate = '2007-07-01 00:00:00.000' Is what I would use... although I'm fairly certain using a `CASE WHEN ISNULL` would be a little faster...
You can insert and select and see your changes instantly, but that's because you own the transaction and it isn't complete yet. If someone else tried to update the table while you hadn't committed and it affected those same rows, it would be a lock until you commit. And if someone else select from the table they would see the version of the data that was in the table prior to your update statement. Hope that helps clear up confusion on commits. And yeah, 'NULL' is literally the word null in all caps, while NULL is saying "there is no value here". And of course that is different than '' which is technically an empty string, but different RDBMSes treat those differently. Some will let you have an empty string that is not a null, while others count it as a null. 
in TSQL, I like to add a new column with DENSE_RANK and subquery multiple times... lacks some level of efficiency, but I do something like this: SELECT Class_Date, user_id, class_id ,DENSE RANK()OVER(PARTITION BY user_id ORDER BY Class_Date, class_id) AS cSort FROM Table2 Which would give you a new column aliased as cSort with a ranked course value by class date... Depending on the keys available, you can add further order conditions in to avoid any duplicates that may exist from bad data. You would then just left join that subquery a few times with something like: SELECT * FROM Table1 t1 LEFT JOIN ( SELECT Class_Date, user_id, class_id ,DENSE RANK()OVER(PARTITION BY user_id ORDER BY Class_Date, class_id) AS cSort FROM Table2) AS t2 ON t1.user_id = t2.userID AND t2.cSort = 1 LEFT JOIN ( SELECT Class_Date, user_id, class_id ,DENSE RANK()OVER(PARTITION BY user_id ORDER BY Class_Date, class_id) AS cSort FROM Table2) AS t3 ON t1.user_id = t2.userID AND t3.cSort = 2 I use it a lot to clean out duplicates and potentially bad data when converting data from rows to columns.
This is a stand alone instance locally that I'm just toying with to learn/freshen up on. Thanks for your help and advice!
I used your first option and it worked perfectly. Now I need to study up on why it did and mine didnt. Thanks!
Excellent. Exactly what I was thinking. I ended up doing this using many statements joined by ORs, which is what I think T-SQL wants you to do. PATINDEX is not RegEx from C#. And that's ok.
Because `'NULL'` is a String containing the characters N, U, L, L and `NULL` is a special variable that means it does not contain a value... which is also different than an empty string `''` :)
That database is a member of an Always On Availability Group. Add it to the Availability Group to make it "synchronized." You will need to specify synchronous commit for transactions in order for the primary replica to failover in the event of an outage.
As a complete aside (as u/TraviTheRabbi seems to have found the issue with the SQL so nothing to add there), in your desired results don't you need the items that don't have a BOM to be listed as well (the non assembly items)? What happens if you need to supply a 1019-1?
Thanks! I will look into this.
Perfect, I'm not a fan as much as I am of PGSql but it has the function you need. Look to JackMomma22's answer about the dense_rank. It was exactly the solution I was thinking of when I asked.
Oh gosh, yep, that was it! Thanks so much!
Self join was definitely the move. Additionally, my interviewer suggested adding a “rank” column that increments so that instead of comparing the date, you just compare the rank. An order placed on any date would have a rank of “1”, lets say, and the next order (regardless of date, as long as they’re ordered by orderid and grouped by userid”) would have a rank of 2. Self join and look for a difference of 1. 
Of course you would say that... 
Something like this should work assuming a user/class combination can't have the same date. SELECT usrs.user_name , MAX(CASE WHEN classes.class_ord = 1 THEN classes.'Class Date' END) class_date , MAX(CASE WHEN classes.class_ord = 1 THEN classes.class_id END) class_id , MAX(CASE WHEN classes.class_ord = 2 THEN classes.'Class Date' END) next_class1_date , MAX(CASE WHEN classes.class_ord = 2 THEN classes.class_id END) next_class1_id , MAX(CASE WHEN classes.class_ord = 3 THEN classes.'Class Date' END) nextclass2_date , MAX(CASE WHEN classes.class_ord = 3 THEN classes.class_id END) nextclass2_id FROM Table1 usrs INNER JOIN ( SELECT user_id , class_id , 'Class Date' , ROW_NUMBER () OVER ( PARTITION BY user_id ORDER BY 'Class Date') class_ord FROM Table2 ) classes ON usrs.user_id = classes.user_id WHERE classes.class_ord &lt;= 3 GROUP BY usrs.user_id, usrs.user_name;
Hey guys, what actually was the problem was I was copying and pasting my work from an rtf. Was saving my work in the rtf and then copying it back to oracle. Checked the file in hexadecimal and a bunch of weird stuff was going on, all figured out now.
select order\_id, updated\_at from (select order\_id, updated\_at, row\_number() over(partition by order\_id order by updated\_at desc) as rn from order\_status\_history) as x where rn = 1
Forgive my prodding, but what do you mean by “copying back to Oracle?” Do you mean SQL*Plus? SQL Developer? Toad? The reason I ask is because the originally posted comment still hinds true. You can’t just execute multiple statements without executing as script. If using SQLDeveloper or toad, it will just execute whichever command the cursor is on and ignore the rest, giving the illusion that everything executed. Regardless, glad it worked out for you.
Thank you! This did it. I had to adjust my [BOM No.] in the second part of the CTE to be retrieved from the item (I), but other than that it was the join causing the issue. My Query ends up looking like [this](https://pastebin.com/rgzr2S0Q) Now I just need to adjust to calculate a total amount required of each component per top level item, and I am set. 
I'm using Oracle Live SQL. When you press run in the top right corner it runs everything on your worksheet, or you can "highlight" as many lines as you want to run and then press run.
Ahhhh yes. liveSQL does do that, you’re right. Cool to find another LiveSQL user here on reddit. Not a lot of talk about it. Have you tried QuickSQL.oracle.com as well? I use it to generate thousands of rows of data for any particular table. If you haven’t tried it I’d highly recommend it.
I have not! I will certainly look into it. Most of my work is just for one specific university class at the moment and that prof requires us to use live SQL.
That’s a great move on your professor’s part. Oracle knowledge is hard to come by in the field (and the pay for knowledge is definitely more) I’ve been trying to hire a developer with Oracle knowledge for about 2 years now and all I get are fakes from recruiters. You wouldn’t believe how many candidates (with “years of oracle experience”) I’ve asked to use a query to create a table and they (almost too confidently) go straight into: SELECT &lt;columns&gt; INTO &lt;newTable&gt; FROM &lt;oldtable&gt; I always end the technical portion of the interview right then and there.
That’s interesting! The reasoning he gave is that it’s free and easy to mark but your reasoning makes me feel a lot better about it!
Your millage may vary depending on what your using but if by "empty" you mean null then you can use COALESCE Category = COALESCE(Column 1, Column 2, Column 3) Coalesce returns the first non null value in the list. If by "empty" you mean something else like an empty text field you can use a case statement. Category = CASE WHEN Column 1 &lt;&gt; '' THEN Column 1 WHEN Column 2 &lt;&gt; '' THEN Column 2 ELSE Column 3 END You can have as many WHEN THEN's as you want and it'll simply return the THEN of the first WHEN it finds to be true. 
Forgive my lack of Oracle SQL knowledge (solely an MS SQL developer here), but now I'm curious -- what's wrong with that statement from an Oracle perspective?
No worries. I have no issues with that sort of stuff as I come from a MSSSQL background myself, it’s only when they REPRESENT themselves as an Oracle Dev with X years of experience that I get upset. The SELECT ... INTO syntax is used to assign values to variables from a SELECT statement. So in SQL server it would be: SELECT @variableName = columnValue FROM table; In Oracle it’s: SELECT columnValue1, columnvalue2 INTO variableName, variableName2 — (no @) FROM table; When creating a table from a query (SELECT INTO in sql server) , it would be: CREATE TABLE TableName AS SELECT value1, value2 FROM tableName WHERE value1 = blabla; 
Oh, gotcha. Interesting! Much appreciated!
My pleasure. As OP said, feel free to try exploring in LiveSQL.oracle.com. It’s free and select contained, so you don’t need to download anything. Just an entire Oracle engine on the web.
Glad I could help! I don't use CTEs that frequently, so it's always good to refresh my brain a little bit on them because recursion always does my head in. :)
Agreed. If you have ODAC installed I’d recommend a TNSPing to whatever the server is. Otherwise pinging the server on 1521/1526 might help too.
Hahaha fair enough. OP might want to switch Over to LiveSQL.oracle.com or Express.
That was going to be my next suggestion, but didn't want to be presumptuous. LiveSQL is a god send.
YES! QuickSAL.Oracle.com is too! Want 10,000 test records? Sure! Just type /insert 10000 and BOOM! 10000 insert statements with randomly generated dat that fit perfectly into your table and data types.
Wow, thats one that I didn't know about! Thank you very much!
My pleasure! It’s great! Just make sure you read the syntax hints, as it’s short-hand. Generates everything including: -Table create statement -Indexes/Keys -Constraints -Triggers -Randomized (or semi randomized) Data It’s made load testing and dev environments DO EASY to set up.
Think of a view as a 'predefined' statement that you treat as a table. If she script is bad for performance, the view is bad. A materialized view is the same, except the data for the statement is held in the schema and generates periodically.
If i understand correctly, a view is a 'virtual' table because it is defined by a query. When you access a view, that query runs, populates the virtual table, and gives you the results. The database stores the sql to populate the view, not the data itself. 
A view is basically just a stored query and has to execute each time its accessed. However if you use a materialized view, there will be performance increase since it creates a physical snapshot of the query results that can be periodically updated as often as needed
&gt;We use SSMS and we have two/three hundred lines of SQL scripting joining varous tables This doesn't sound good. 
Depending on what you are using it for and what engine you're running, a materialized view would be the solution. Basically it would execute your SQL at a predefined time so that instead of running those all at once, it would have already performed the joins. The problem i that this isn't live data so it would have to be refreshed at an acceptable time frame.
&gt; In layman terms, what exactly is a View? The simplest explanation is that it is a saved query. Anytime you have a query, you could take that same SQL, throw it into a view, and select from the view, and it is exactly the same. Views are very standard, and very recommended. They aren't burdensome on performance or optimization, unless they SQL behind the view is bad. &gt; We use SSMS and we have two/three hundred lines of SQL scripting joining various tables I highly recommend a view for this, this is the correct route to take. I want to find all internet sales, look at the below query: SELECT [col1], [col2], [col3], sale_date, sale_location FROM some_table --left join whatever, can be super complicated, imagine 300 lines of joins where sale_location = 'internet' Well, I run this query a lot. I want to save this query. I can copy+paste it into code everywhere, and when I want to add more filters can do something like this: select * from (SELECT [col1], [col2], [col3], sale_date, sale_location FROM some_table --left join whatever, can be super complicated, imagine 300 lines of joins where sale_location = 'internet') as t where sale_date &gt;= '05/08/2018' I just made it a subquery. That's cool and all, but that means I still need to copy and paste that code everywhere. What if later code changes mean that internet sales can actually be in a `sale_location` of `'internet'` **or** `'website'`? I would need to find where I looked for internet sales everywhere, and update that logic. What I can do, is throw it into a view. create view [internet_sales] as SELECT [col1], [col2], [col3], sale_date, sale_location FROM some_table --left join whatever, can be super complicated, imagine 300 lines of joins where sale_location = 'internet' Now, whenever I want to find internet sales, I can simply do: select * from [internet_sales] The SQL engine will take all the code from the view, and plop it right there where `[internet_sales]` is, exactly the same as if I made it a subquery, like I did above. Something to consider about views, is while there is no performance penalty for them, a lot of rookies will make a massive "join everything" view that they will only need a very tiny portion of, and just use it everywhere, because it's easier. They make a bad query that has everything, and since it has everything, they use it everywhere. Just keep that in mind.
Think of a view as a stored query. 
I am using Oracle SQL Developer for mac Build 277.2354 Version 18.3.0.277. I am trying to run a script that i created to make some tables and assign some primary keys and constraints. and it asks to create a connection these are what i input Connection name\_\_\_\_Test Username\_\_\_ Password\_\_\_ Connection Type\_\_\_Basic Role\_\_\_Default Hostname\_\_\_\_ localhost Port\_\_\_\_ 1521 sid\_\_\_ xe I am not trying to connect to a server all i am trying to do is run the script locally to check for any errors.
Believe me Bud we have one query worth of 1500+ lines. Poor SQL work.
But wouldn't the view get refreshed and bring back newer data with the older data every time I execute the query? 
THANKS Intrexa, someone finally gets me. I am all for creating a view rather than running 500+ lines of query each time.
Wait a minute, can a join a `view` with another `view` or a `table`?
A regular view would. However, A view provides no performance enhancement unless your database allows for indexing of views. It just executes the query. Sqlserver doesn't have materialized views like I mentioned but they do have index views. Other engines have materialized views which refresh at intervals so that you can retrieve data without doing expensive joins. But the data are stale. Sometimes acceptably so sometimes not. You can also make a materialised view yourself depending on the engine and what languages an application uses even on SQLServer. Basically have it execute a truncate statement and them insert at an interval. 
Yes just think of views as you would any other aliased result set
Think of it like a query that is stored as a table. I can have a query that selects from a table SELECT * FROM myTable WHERE name = 'John' AND hairColor = 'black' The problem is that every time I want that set of data, I'd have to recreate that query. But I could create a view that does the same thing. CREATE VIEW JohnWithBlackHair ( SELECT * FROM myTable WHERE name = 'John' AND hairColor = 'black' ) Now every time I wanted to get all John's with black hair, I can simply say: SELECT * FROM JohnWithBlackHair &amp;#x200B;
Yes, but it's usually not optimal. Nested views (views referencing views) are very common in production environments but often have bad performance and are annoying to troubleshoot. 
Keep in mind that the code is still being run. It just looks neater when you use views. If these views are commonly used data sets, you might consider creating preprocessed table using some stored procedures or something. Do you have a data warehouse structure?
What DB system are you using? SQL Server, DB2, Oracle?
It all depends what it does, a long query isn't necessarily a bad query if it works efficiently. That 1500 line script might be segmented up, doing different things in sequence that don't make sense to be separated out, for example building a series of temp tables to then pull together in an overnight job, remember that #temp tables only have scope in the current query. Point being, just because there's a lot of code doesn't make it bad... poor coding techniques make it bad. 
SQL server.
I've always seen it as essentially being a stored procedure or statement, but where you can treat the result of that SP/statement as a table. Although most of the time I use it, it's actually just a way to join several tables and present them as a single table.
Weird, my professor provided us this diagram, figured should would add the FK tag. Thanks
Ok. If you are learning more about Views, keep in mind that some systems have different types. For example, we mentioned, when someone runs a view (even a SELECT * against it) it will take a little while to return because it needs to run the query that view is made first. SO to help performance systems like DB2 have whats called MQT (Materialized Query Tables) that act as a View, but on disk as oppose to in memory so that the performance is better for heavy querying and a number of other reasons (like not constantly getting dumped and having to re-run). Depending on your needs look at what your database system has. I hope that helps.
A select query.
I wouldn't think of views that way. A view is like a saved query. You run it, it returns data. You cannot change any of it's parameters at runtime. A stored procedure does something. Create, update, insert, delete... something. They can also "run a query and return data", but you should only used a stored procedure to do this if you're feeding it parameters that the query needs to run. 
The logic behind view still runs. It doesn’t make it any faster whether when you run it manually, or executed via view. The view only keeps it simple for you to not having to remember logic every time you want to run it.
That’s understandable. It’s definitely easy to export/share worksheets. Just know that Oracle Knowledge (although very niche) will definitely give you an edge in certain opportunities.
That wasn't the best phrasing I've ever used - I didn't really mean "like any stored procedure": more that it's like a query or *some* SPs (those which are basically just queries with a parameter), but without parameters and presented as a table
Views are not more or less performant. They are simply a way of abstracting away the complex query and/or providing a layer of security. 
I think what folks are getting at is that performance - in addition to the organization it provides - is an extremely important consideration, and the type of view is based on the problem it’s solving. It’s hard to say that views are appropriate without knowing more. Some considerations when considering if a view is appropriate, and if so, what kind you need: -How often does underlying data change (not just new records) -are you performing data transformations or calculations within the view stack (as opposed to just the final display view)? -when you access the final view, which fields are you applying criteria to in your where clause? If you’re applying it to fields calculated lower in the stack, you’re going to want to look into materializing (or possibly even a stored proc). How you materialize depends on the data change rate. It also allows you to index. But then you have the whole world of indexing questions. -your underlying data architecture In all, views sound great. But in my experience, views have only been advantageous in a very limited number of simple use cases. Far more often, what starts as a view ends up being a stored proc. BUT, that completely depends on your individual use. 
it's a "logical" table. Not actually a table.
Usually to store passwords in a DB, your application layer will run some sort of one-way hash (bcrypt) on the plaintext PW first, then the hash will be stored in the DB.
https://docs.microsoft.com/en-us/dotnet/framework/data/adonet/protecting-connection-information
My post is about credentials for other apps stored in a DB, not the credentials to the DB.
There are column-level permissions, which would be an excellent option if password needs to be re-displayed in plaintext, but it doesn't sound like that was used because you'd be able to see that the column at least exists, even if you don't have permissions. (When you do SELECT * FROM ... you'd get an Access Denied message in addition to the ID column, so be sure you've checked for that) 
It’s because I need to somehow combine both of my FK in the reference I believe, but I am unsure how to do that
I understand, but let's start with what was the error message exactly.
It is saying that there are no primary or candidate keys in the referenced table CourseCatalog that match the referencing column list in the foreign key
Now that sounded interesting, as I only did the build-in default query to preview the data which only uses the visible columns (SELECT TOP 1000 id FROM mydb.dbo.credential). So I tried * instead. Alas: it still returns only the ID column and I get no error messages :(
also, you can update one view instead of who knows how many queries used throughout a company. like say there was a column change to an inventory table and now needs the data looked up elsewhere in that query. you can simply update a master view instead of potentially dozens or more queries spread out all over the place.
This is a massive topic, and can be quite contentious. T-SQL is a language, property of microsoft/sybase. MySQL is a DBMS, as is postgreSQL. So comparing them is apples and oranges. It also depends on applications. Coming from an Oracle background, I mostly see big enterprise setups using either Oracle or MS SQL Server, with the open source ones, (postgress, mysql, mariadb etc) along side for various apps. And then you've got the other 'SQL' databases for analytics such as Apache Spark, Cassandra, etc. This isn't really something that can be answer in one question and could be read into at length. 
What RDBMS is this? If you have any queries, you should include that information in your post. 
Just updated
I'm not sure what you're expecting, that looks like its working as intended. You have your order by Quantity descending, which means that your first 15 values will be the 15 records with the highest Quantity.
But for example the element 15 does not match on the results, is this normal behaviour?
I can't imagine SQL for FP&amp;A will be significantly different than SQL for non-FP&amp;A. You'll probably just be focusing on SELECT statements from DML and the associated features (joins, subqueries, CTEs, window functions, etc.) Most major vendors have their own variations of SQL which have additional features. So it's probably best to focus on a resource that either uses ANSI SQL or something close to it. If you want a book, I'd recommend Itzik Ben-Gan's TSQL Fundamentals. His book is focused on Microsoft SQL Server, but most of the stuff was ANSI SQL, and the proprietary Microsoft stuff was clarified. If you want online example driven stuff, I would look into either codeacademy or SQLZoo.
To add to what /u/AreetSurn said, you have multiple records with the same quantity, so you must add an additional column to the order by to be the tie breaker. There is no guarantee of order in SQL, unless you explicitly state it with an order by.
I believe so, it looks like that in the documents. It scans the table for the 'top 15', which to the table it uses the primary key which I'm assuming is OrderID. What you may be looking for is ROW_NUMBER(). Here is a helpful document I found. https://www.mssqltips.com/sqlservertip/4938/sql-server-performance-comparison-of-top-vs-rownumber/
That's it down to a T. ROW_NUMBER() OVER is a great way to do this.
Appreciate the help!
Thank you, I entered the information and it looks like it will work except for it comes back with &amp;#x200B; `SQL Error: ORA-01779: cannot modify a column which maps to a non key-preserved table` `01779. 00000 - "cannot modify a column which maps to a non key-preserved table"` `*Cause: An attempt was made to insert or update columns of a join view which` `map to a non-key-preserved table.` `*Action: Modify the underlying base tables directly.`
Sorry, I’ll have to defer to someone which more oracle specific experience. From just the erro verbiage, perhaps the tables don’t cleanly join, as in it’s not a unique 1:1 join? Run the select part separately to check. Or perhaps the tables are actually views?
&gt;select order\_id, updated\_at from (select order\_id, updated\_at, row\_number() over(partition by order\_id order by updated\_at desc) as rn from order\_status\_history) as x where rn = 1 Hey thank you so much. I learned PARTITION and OVER today. Thanks again! 
Are these homework questions? 
A view is a query.
You can filter a view by using a where clause. This kind of behaves like parameters.
Typically cross and outer apply can work similar to joins but it can be used for recursive capabilities that typically hinge on row by agonizing row instead of set theory which can result in a high performance cost. One other pro is that you can use this for functions which you can't use with joins. I always remember outer apply is similar to outer join and functions like a left outer join. The cross join is the odd man out with wording and acts like an inner join. 
In a broader sense, yes. Though the task at hand is different from the homework questions. I just want to know whos right: me, my friend or both? I played around a little with SQLfiddle but wanted to double check with someone whos not a beginner 🙈
Hi everyone, &amp;#x200B; I am just getting into SQL in my data base course and am having an issue. Our project as a class is to create a database for a gym. However, when I am inserting my foreign keys into my create statements I am get the red squiggle mark underneath the "," following "foreign key". I am unsure what I am doing wrong and any help would be appreciated. I am sorry for messing up any of the lingo, just getting started 
[Foreign Key Documentation](https://docs.microsoft.com/en-us/sql/relational-databases/tables/create-foreign-key-relationships?view=sql-server-2017) Take a look at that. Usually you can find anything you need by searching the subject + TSQL for sql server 
You need to say what the foreign key is referencing. You can have the same Id among multiple tables so it doesn't know which one you want to refer to
ok 2 parts here one is etiquette another about your problem...... 1. Do not Screenshot! Copy your code and paste it in its entirety so people can help, screenshotting makes you look bad and undermines others ability to help you. 2. At least the first example RoomNo, I cannot see another table using it as a primary key or unique field. The primary or unique field needs to be created before you can reference the field in another table as a foreign key 3. also not to be harsh but its kind of the rule rtfm... [https://docs.microsoft.com/en-us/sql/relational-databases/tables/create-foreign-key-relationships?view=sql-server-2017](https://docs.microsoft.com/en-us/sql/relational-databases/tables/create-foreign-key-relationships?view=sql-server-2017)
np. Hope it worked, haha!
&gt; But I've heard *business* arguments that says you can use them to normalise data, for instance. I disagree strongly, you should normalize your data in a different method at the table level and not the view level as a view is just stored metadata that is pre-compiled. I am not a fan of nested anything, almost ever. There are a few cases like functions or error handling templates etc that can be good to nest, but this now becomes a business logic decision and it's similar to a programming library that is in-house. You now have a higher overhead cost of training and you better have spot on documentation. Anyone who is against this has never had to manage a 45TB database with a 75+ layer nested stored procedure. &amp;#x200B;
1. So sorry about that, it didn't even cross my mind. Exams have been killing me and Im pretty sure my brain is the consistency of warm jello at the moment. 2. I think I solved part of my issue. When I built my ERD I used LucidChart which has an export feature. This feature apparently is in the order of the entities created not in the order needed to be created so I just redid it. 3. Sorry about that. Totally understandable. This is my first time posting on this sub and should have paid more attention. Just trying to get through as many assignments as possible. I really appreciate your response, thank you. 
I've seen views to denormalize data sets for quick references and easy access, but it should never be nested.* That said, I have nested views in my current environment and I struggle with talking my client out of nesting objects. (Really just views.) *In database, it always depends. 
Hey, cleverchris, just a quick heads-up: **alot** is actually spelled **a lot**. You can remember it by **it is one lot, 'a lot'**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
Its that old golden rule: it depends. I'm just interested in anecdotal information. Theoretically it has use-cases but they all have heavy cons to them. And the people who say against them, myself included, sometimes have to manage them anyway.
That is what I have understood from my class so far. Just so my professor knows we have an understanding of it an ERD is part of our final Deliverable not so much to have an ERD, but to show that we conceptually know what is going on in the database.
Yup. I've never seen it in use where it's a good idea. The primary reason I've seen is that there is business logic defined in the database (which is a /the problem) and then it makes sense to have a defined view which is a consistent repeatable dataset illustrating and utilizing the business logic. Had it been properly designed to not have this in the database, there would be no need for the view. 
You have to tell it what table.column the foreign key is for.
bad bot
Thank you, m1k3y60659, for voting on CommonMisspellingBot. This bot wants to find the best and worst bots on Reddit. [You can view results here](https://botrank.pastimes.eu/). *** ^(Even if I don't reply to your comment, I'm still listening for votes. Check the webpage to see if your vote registered!)
It did it worked perfectly! Thanks again! Could you see if my understanding of it is correct? The outer query gets the order\_id and updated\_ at. and the inner query gets a row number for every row and then when using partition gets a new rn for every matching order\_ID once and then sorts up\_dated by descending right? Could you explain the as x and where rn = 1 meaning? Thanks again! &amp;#x200B;
So I just tried it! When I change rn to 2 it becomes the 2nd most updated and when i changed it to 3 it becomes the 3 becomes the earliest updated\_at how did you do this? 
Not to be glib, but the error message is telling you what you are doing wrong. I know that some error messages can be rather vague, but this one is not.
What SQL state error are you getting? This sounds like it may be more of a PHP issue than SQL if you're able to run it manually. Also when you say manually do you mean in php interactive mode or just an SQL console?
Hey no worries! Yeah so I added order. total price after the group by function? Did I misunderstand what it was asking maybe?
Thank you good sir! Do you happen to know if I could follow along with a Mac based instead of windows based like they state in the requirements section? 
That makes sense and I don't disagree with your prof. I recall doing ERDs all the time in college and they did help solidify my understanding of a relational database. &amp;#x200B; Since entering industry, I have made exactly 1 over the last 5 years. 
Why not? Read through these guides (basically it's installing a Windows VM on your Mac) [https://database.guide/how-to-install-sql-server-on-a-mac/](https://database.guide/how-to-install-sql-server-on-a-mac/) [https://social.msdn.microsoft.com/Forums/en-US/c47fc200-a1d3-44f0-b42b-a49be6fc863c/downloading-sql-server-express-for-mac?forum=sqlexpress](https://social.msdn.microsoft.com/Forums/en-US/c47fc200-a1d3-44f0-b42b-a49be6fc863c/downloading-sql-server-express-for-mac?forum=sqlexpress) [https://social.msdn.microsoft.com/Forums/sqlserver/en-US/5f600bdd-3a3b-4c81-b4d5-b8ba1ac61745/sql-server-management-studio-for-mac?forum=sqltools](https://social.msdn.microsoft.com/Forums/sqlserver/en-US/5f600bdd-3a3b-4c81-b4d5-b8ba1ac61745/sql-server-management-studio-for-mac?forum=sqltools)
the nested (inner) query takes all of the unique order\_id's to start. But these have multiple update\_at's in some cases. So to order the order\_id's that have multiple update\_at's thats where row\_number() comes in, literally a function to assign row\_numbers to the records in a table. It works together with over() to assign row numbers within each unique order\_id (partition), which are ordered by their update\_at in order of most recent (descending) and numbers 1 thru whatever are assigned until all the records for that particular order\_id are numbered, then the next order\_id is put through the same process. &amp;#x200B; 'as x' is assigning a name to the table created by the inner query. I don't know if this is necessary in all flavors of SQL but in t-sql (SQL Server) it is. &amp;#x200B; rn = 1 is a filter that is used to select the records in the order\_status\_history table of all of the distinct order\_id's along with their corresponding most recent update\_at's.
Ah okay I see! Thanks so much! I was hoping you'd be able to help me ONE LAST TIME. I'm sorry for the inconvenience in advance and thanks for teaching me so much. [https://imgur.com/a/XmGgv3y](https://imgur.com/a/XmGgv3y) &amp;#x200B; When I added an order\_status to the select statement, it shows all 3 update\_at statuses. However, I only want it to show the most recent one. What am I doing wrong? I tried changing the rn = 2 however no rows show after. Rn = 1 unfortunately shows the imgur link above. thanks thanks thanks!
Usually I use OUTER APPLY with functions/queries that return a single value or a single row. Then you can take that value or row and feed it to the next OUTER APPLY and so on. Very powerful.
Let us see your insert queries?
Ah great yes that makes sense.
According to this [Stackoverflow answer](https://stackoverflow.com/a/9494162) it doesnt matter if you use a table constraint vs a attribut constraint.
Thanks for the efforts. I was unable to continue with this hence the delay. I've managed to experiment and to achieve to some success. However, I've got stuck once again but I'm pretty sure I'm very close to the wanted result but I hope for your help. I would like to group results by (o.firstname) which is the name of a Sales Representative. * First subquery show contacts contacted in last 30 days coming from "request form" * The second subquery includes all contacts created in the last 30 days coming from request form. Now Ideally I'd like to have four columns: Name of the sales rep, the number of received requests, number of followed up request, a percentage of followed up requests. Thank you! select x.count_1, y.count_2 FROM ( SELECT o.firstname, count(c.vid) as count_1 from contacts as c join "harmony"."owners" as o on o."ownerId" = cast(c.properties__hubspot_owner_id__value as INTEGER) where properties__first_conversion_event_name__value LIKE '%Request form%' and CAST(c.properties__createdate__value AS date) &gt;= current_timestamp - interval '30 day' group by o.firstname ) as x join ( SELECT o.firstname, count(c.vid) as count_2 from contacts as c join "harmony"."owners" as o on o."ownerId" = cast(c.properties__hubspot_owner_id__value as INTEGER) where properties__first_conversion_event_name__value LIKE '%Request form%' and CAST(c.properties__createdate__value AS date) &gt;= current_timestamp - interval '30 day' and cast(c.properties__notes_last_contacted__value as date) &gt;= current_timestamp - interval '30 day' group by o.firstname ) as y on 1=1 
If you’re pulling transaction data from an OLTP system, you probably shouldn’t be using NOLOCK in the first place. Do you understand that you can get bad data that way?
https://www.amazon.com/Oracle-Database-Administration-Microsoft-Server/dp/0071744312 helped me a ton.
He’s here at Summit this week but paging /u/SQLBek :) Yes, nested views will end up causing you trouble as they A) obscure the original source of the data and B) can cause bad query plans due to invalid row estimates. Google sp_helpexpandview
oracle is a ton better than mssqlserver imo
What version you are is going to change this quite a bit. 12 came out with a gui that makes managing dbs relatively easy but I hvaen't worked with it in a few years so I don't recall what its called. It might not be installed though, it isn't a desktop application like SQLServer is, its a self-hosted web app. They're a lot easier to manage on Linux machines too. Everything can be handled from the terminal but you have to know what the commands are before you use them, obviously, since you can't find them with a gui in that case. But if you're working on say a dev/test/prod system, on Linux you can have three terminals up logged into each and mirror your commands to all three simultaneously, etc. 
You are learning something new so of course it feels awkward at first. livesql.oracle.com has a lot of examples that you can play with in your browser. 
Everyone look at this poor, poor developer with Stockholm syndrome.
Oracle as a product is fine. But hasn't even remotely kept pace with it's competition. Their new offerings are looking to partially catch them up to where SQL Server has been for over five years. Their security exploits/vulnerabilities lately has also been bottom of the barrel. The Oracle database community is also undeniably one of the worst in the industry. It's the running joke at pretty much every development conference out there. 
Cheers, this has helped out.
This explanation may not be factually correct, but hopefully it explains the core differences. Using standard joins creates a result set. Think of select from, left, right, inner joins being evaluated at the same time to produce a single result set. Cross Apply can be thought of as two complete steps where the left set is evaluated first THEN the scalar or table value function is applied to the left set. If the function applied results in null, cross apply behaves like an inner join. Outer Cross Apply includes all records from LEFT side. Apply is very useful for processing xml and the articles FoCo_SQL linked will explain more. 
Well said, imho cross apply can be uglier, but shows its strength over Python/C# especially when processing xml or json docs.
One of the biggest problems for us is price. We use both and Oracle is about 10x the price, for half the features. You can get some of those features, if you tack them on to the per processor price, but to license everything gets ridiculous. Things like tuning, compression, and partitioning seem like they should be standard now.
this times a million a view calling a view calling a view is just bad design imo. if you are going to call the result set often just store it
This is only one facet of the answer, but it's really irritated me when I've used Postgres: Materialized views work completely differently (in particular, it's difficult or impossible to get them to update automatically).
Having worked with Oracle for 20 years, and switching to SQL server the last two I'll say Oracle is slightly weirder, but they both have annoyances that seem, from the outside at least, unnecessary.
Hi - quick reply. Find my presentation on code reuse pitfalls on sqlbits.com. Search for me under speakers &amp; there's recordings of the session. Explains very clearly why nestes views are evil. 
You can make more money if you know Oracle well. It's a more complex product. Change my mind.
Stephen crowder is that you? 
Is there a better way to query counts and stats other than using NOLOCK from an OLTP system? For this particular use case, it’s fine since it isn’t driving any decisions other than showing some numbers on a tv for number of entries and votes. Definitely have used it in the past for more important things though.
You can make more money with Oracle but there are less jobs than for SQL Server. Oracle is more complex and absurdly more expensive which is why most companies want to move to a different product after dealing with Oracle’s pricing after about three years. 
That's weird, thanks for the heads up!
Are you ok with the counts being wrong because a transaction was rolled back? What’s the isolation level of your database/queries? NOLOCK is **not** a magic turbo button. There are consequences of using it. 
There isn't a way to track changes like this (to my knowledge) however the way I would approach this is by creating an update stored Procedure. Within the stored procedure, you can write a query that would have the update commands to update the values and an insert command to insert a line into an update table (example, have fields such as: Table name, Column Name, Old value, new Value) and this way, when you update the values, there is an audit trail of what those values are. If your system uses a Snapshot Date style (keeps a record of every day, typically in data warehouses) then you can compare to previous days.
I've used both and have no real big complaints about either. Start getting comfortable with Bash, cron, and using the CLI instead of visual tools for management. Config changes, upgrades, and most management tasks will be in the form of a script vs. SSMS. It's probably a good thing this change is being pushed by devs, the technology stack is a good reason why a choice like this would be made. Have Postgres but using .NET? Probably not a good combo. Using open source (Python, Java, etc), but have MSSQL? Also not a great combo.
lol shrug. plsql works great for me! 
Oracle is more complex but less functional. You're paid more to know the proprietary blend of eleven herbs and spices works well enough to cover up the difference. Also paid extra because they can't afford to have a secondary coverage if you go on vacation. &amp;#x200B; &amp;#x200B;
Thanks for the reply. I was looking for more anecdotal/production opinions on it. I don't use them, and actively tell people not to. By their definition, nesting data dictionary items isn't really a good idea. But sometimes they've been used and I haven't noticed the headache they are meant to cause so have opted to not drop them. 
Thanks for the reply and suggested resource. I'll check it out.
 I made the same transition. Certainly, I've used other DBMS systems, but never in anger. I figured the transition would be learning some new tools, some new nomenclature, and some different semantics here and there. Man, was I surprised. Certainly, there's a lot of nomenclature changes. Oracle has a vastly different internal architecture which results in hugely different results in scope of features and implementation. Oracle actively says (you shouldn't use triggers)[https://www.oracle.com/technetwork/testcontent/o58asktom-101055.html], for example; SQL Server applicaitons use them as a fundamental tool. Oracle management is tedious and insane. Everything is over-configurable. In days by, and maybe on the harieiest of systems today, it's important to fiddle certain settings to get the best possible performance. But Oracle mostly requires every little level of configuration to be done -- the defaults aren't good. SQL Server, for example, has files and file groups. If you just want a database, use a file; if you want to spread storage around to logical devices, create a file group and manage the files within it. Oracle's storage management requires that file spaces, log file usage, groupsings, reservations, and so on ... be identified. Fewer attributes of the new database can safely be left to default. Tom and the other Oracle visible influencers will openly gloat about how the Oracle implementation is better, but I just can't find a way to agree. These are all big, broad generalizations, of course. People build and manage Oracle for huge successful systems all the time.
I love IIFs Set endate = iif(enddate is null, getdate(), enddate)
Losing SSMS is going to hurt. I am pretty decent with command line but it's REALLY helpful being able to work within the GUI to edit code and view plans and whatnot. 
&gt;With CTE\_CustOrders as ( &gt; &gt;SELECT &gt; &gt;Customers.CustomerName ,Orders.OrderID ,sum(Products.Price \\\* OrderDetails.Quantity) AS 'Total' &gt; &gt;FROM Orders &gt; &gt;INNER JOIN Customers &gt; &gt;ON Orders.CustomerID=Customers.CustomerID &gt; &gt;INNER JOIN Products &gt; &gt;ON OrderDetails.ProductID=Products.ProductID &gt; &gt;INNER JOIN OrderDetails &gt; &gt;ON OrderDetails.OrderID=Orders.OrderID &gt; &gt;Group By &gt; &gt;Customers.CustomerName ,OrderDetailID &gt; &gt;) &gt; &gt;Select top 3 &gt; &gt;CustomerName ,OrderID ,Total &gt; &gt;From CTE\_CustOrders &gt; &gt;Order by &gt; &gt;Total Desc &amp;#x200B; Hi, Thanks for replying. The code can't run with "\\\*" and "top 3" and when I take them both it stills doesn't shrink 3 customer orders to 1.
I’d rather something that’s meant for fun to not break the database, at the risk of the numbers to potentially not be accurate. What is the better way to handle this scenario?
My apologies, I copied your code and turned it into something that would give the result. Didn't realize the select statement showed different columns compared to the group by. The group by should always have the same columns you are wanting to condense everything into (in this case, customer name and Order id) You will need to change the group by to: Group By Customers.CustomerName ,Orders.OrderID
I've spent a few years learning SQL and using MS SQL Server - migrating GIS data build processes using MS SQL Spatial. I understand the topic is about Oracle, but you mentioned variables and I know PostGIS is more powerful than MS SQL Spatial - but no variables! There are so many things I do in SQL Server that orient around using variables &amp; - god forbid - iterating through them as each geometry has unique properties. I know most will tell me it is just a learning curve or I'm using variables as a crutch - but why don't the others use variables? (FYI I'm not a particularly technical person as 20 years have been fully GIS only)
Your select statement is returning multiple rows either because your join is returning duplicates or there are duplicate entries in the table you are selecting from. I would suggest adding a group by clause in your select and maybe even selecting max(da) so you get the most recent date
Postgres and MySQL are free and open source which means community support most of the time. They are both old and forged through the years so performance and robustness are not brought into question with either of them. My experience mostly lies with Postgres and I really like it because there are many extensions for common tasks like pivoting tables and also for more esoteric tasks like routing in networks and Geographical Information Systems.
There's plenty of IDE's you can use with Postgres for queries. It's management tasks like backups or data transfers that have had me missing SSMS.
Don't complain, at least you're not using Intersystems Cache which literally conforms to SQL-92.
It's like a way of making a correlated sub-query, but with the functionality of a join statement. Imagine you had a query which included a correlated subquery for the most recent date a customer ordered something. SELECT AccountNumber , (SELECT TOP 1 OrderDate FROM orders b WHERE a.AccountNumber= b.AccountNumber ORDER BY OrderDate DESC) FROM userTable a Now what if you wanted to also get the credit card number that was used on that order? You can't do But you *can* do SELECT AccountNumber , (SELECT TOP 1 OrderDate, CCNum FROM orders b WHERE a.AccountNumber= b.AccountNumber ORDER BY OrderDate DESC) , (SELECT TOP 1 CCNum FROM orders b WHERE a.AccountNumber= b.AccountNumber ORDER BY OrderDate DESC) FROM userTable a However, that's not that efficient, you're referencing a table twice for 2 pieces of information that are in the same row. This is where an outer apply comes in handy. SELECT a.AccountNumber , b.OrderDate , b.CCNum FROM userTable a OUTER APPLY ( SELECT TOP 1 OrderDate , CCNum FROM orders b WHERE a.AccounNumber = b.AccountNumber ORDER BY OrderDate DESC ) An outer apply lets you return more than 1 column at a time from a correlated subquery, while syntactically, it written like a JOIN statement. Hope this helps!
eh, i never use the GUI for backups and restores anyway. that is fine by me
Teradata 4 life
Yes, either that or build all the logic in a single view to avoid joining the same 5 tables 5 times each in nested views. 
One additional note on the query that just happens to be sorting by OrderID without anyone telling it to: there's no guarantee that it will do that every time. It's technically possible (albeit highly unlikely) for it to give you a different order among the rows with the same Quantity value every time you run the query.
Oracle has lagged far behind other RDMS systems in the past decade. Welcome to the future!
Instead of except, there is a better approach to compare two data sets. &amp;#x200B; The general idea is as follows: Combine both results using UNION ALL, but add a marker column to each result. e.g. add a colum with the value "1" to the source and "2" to the target. &amp;#x200B; `SELECT 1 as marker, *` `FROM source` `UNION ALL` `SELECT 2 as marker,*` `FROM target` &amp;#x200B; Now, group by all except the special column. e.g. if source and target have three columns (let's say a, b,c): &amp;#x200B; `SELECT a, b, c` `FROM (query above)` `GROUP BY a, b,c;` &amp;#x200B; The fun part is that the result of \`sum(marker)\` is quite useful: &amp;#x200B; \- 1 means it was only in the source \- 2 means it was only in target or twice in source (you can tweak the number to disambiguate) \- 3 is the expected case: once in source, once in target e.g. you can use a having clause to remove all rows where the sum is 3: `HAVING sum(marker) &lt;&gt; 3` That is just the basic idea. An exact test that validates that exactly one row from source and one row from target is present might look like this: `HAVING SUM(CASE WHEN marker = 1 THEN 1 END) = 1` `AND SUM(CASE WHEN marker = 2 THEN 1 END) = 1` &amp;#x200B;
That's ok, that's usually how I learn best. I'll learn theory/examples on a course, and implement it and make it work in a working environment. This inevitably brings up issues that I have to fix or learn about, which is great! My issue is I have no starting point for this, I know all about database design, have seen working data warehouses in practice, but how do I even start implementing that from raw data from scratch? How do you model data in to a fact/dimension? 
The first step for me is always defining the use case. You can't really know the best way to model data without knowing how it's going to be used. A book you might find useful is [Data Modeling Made Simple, by Steve Hoberman](https://www.amazon.com/Data-Modeling-Made-Simple-Professionals/dp/0977140067/ref=pd_lpo_sbs_14_t_0?_encoding=UTF8&amp;psc=1&amp;refRID=H01AWMQC35X5TX304G3W). I was fortunate enough to work at a place that just paid Steve to come in and give a week long seminar on data modeling, which definitely helped me brush up on the principles I learned in school.
Should be able to use the DML inserted table to see updates right? [https://www.dotnettricks.com/learn/sqlserver/inserted-deleted-logical-table-in-sql-server](https://www.dotnettricks.com/learn/sqlserver/inserted-deleted-logical-table-in-sql-server)
this explanation right here just made OUTER APPLY click perfectly.
Yeah, I had trouble finding anything too useful online. I had a flat access database that I kept needing to refractor because it was going over the 2gb limit. I downloaded the Ms SQL server express and currently in the process of creating a more normalized database. I have no clue if what I'm doing is efficient/clean but it's working and I've learned a ton. I would just try and find some large dataset and go to work on it. Just google as you go and feel free to ask questions.
No 
Not really, if there is an index you can use if you need to use lag or lead
Your title says rows, but your post says columns - which are you actually asking about? Either way, the answer is - it doesn't matter. You should have some sort of logic so that someone approaching your table blind can understand how things flow ideally, but they can reconstruct it however they want in the select statement. You can always move things around later if you really want.
If you're new to sql to understand the basics go to this site, it's free https://www.sololearn.com/ 
If you can insert the records in order of the PK, it will be slightly faster.
All the permissions at server, database, table, column, etc levels are exposed through various system tables, views, and DMVs. However, yes it does get exponentially more complicated, because there are various degrees of granularity, and depending on your implementation it may go much deeper than just tracking roles (meaning there are individual permissions set). There are various queries shared on different sites that show "all" permissions in various ways, but it really will take some time to find whatever meets your specific requirements. It's not clear whether you are trying to keep documentation to determine what things "should" be (desired state) or if you are looking for a query to observe how things actually are. You could set up a job to run whatever queries meet your needs, and save the output to a timestamped table on a daily basis or something. There's also a concept of database/server-level triggers on DDL query executions... I haven't worked in that area for some time, but it may be possible to also create a log that shows every time such permissions are changed. That way you could see if a permission was added and then removed, if it was done between your normal logging frequency. Maybe you could also set up Change Data Capture or a similar process on the system tables that are of interest to you... which could be comprehensive, but perhaps overkill.
Which db? MySQL lets you reorder columns which is nice. Postgres doesn't unfortunately. You want the PK to be first, just for convienence/convention, but it probably doesn't "matter" technically. [Also here's some details on how column order can make a very small difference on postgres.](https://www.reddit.com/r/PostgreSQL/comments/4askpy/script_to_reorder_postgresql_columns/#d1361jd) I wouldn't bother spending any time worrying out it though. If you're like me, it's easy to waste heaps of time worrying about trivial stuff like this, and overall it really slows down progress of getting things done. There's a balance of course, but often imperfect progress is going to be better than snail's pace perfection. And mirco-optimisations like this can (ironically) be a huge waste of time. These days I just ensure PK is first, and try to ignore the rest. Things will always change anyway, so you're never going to get everything in the perfect order (which is subjective anyway) - so if can't be done consistently everywhere, why bother worrying about it all? 
Suffered on a helpdesk for 1.5 years. Then a slightly better helpdesk/deskside support at a hospital for 2 years. Started learning SQL in my free time. Made some friends with the data team and a position opened up.
What resources did you use to start learning?
There was a near identical thread in the last two weeks, let me see if I can find it for you,
[This thread from a few weeks ago](https://www.reddit.com/r/SQL/comments/9ezrgk/any_of_you_here_with_careers_that_are_selftaught/) has a ton of people talking about how they self-certified into SQL jobs.
Haha thanks, well now I feel like an idiot.
that's awesome, works perfectly. THANKS
Sites like w3schools, sqlbolt, sqlzoo. And installed sql server express and a sample db like adventureworks. 
I started writing web apps many years ago for my work. It used an Access database. A year or so later I moved to SQL Server. I remember bitching about it the whole time because I wasn't used to something so different. Another year and I found myself doing web apps for another company, and this time they had real DBAs and an Oracle database. I wasn't allowed to touch the database. SQL queries had to be sent to the DBAs to execute against the database. It was a pain in the ass. Eventually I was allowed to touch the dev database, so that helped me learn Oracle. Another jump forward and the DBAs were either laid off or quit. I jumped at the position to learn the administrative side (and I wanted to be able to fix things in production). Another leap forward and I'm converting our web applications running on Oracle back to SQL Server. Oracle was overkill for our business. Anyway, I learned (poorly) using the query builder in Access, and then asking in IRC. Fixing those queries in SQL Server helped. I used web searches ("select top oracle vs tsql", wtf won't this work) and finally to Stack and Reddit these days. But mainly I learned from breaking and fixing stuff over twenty years of database work. You sound like me, actually, when I graduated. I fell in love in with SQL. I remember doing an interview shortly before I graduated college and they asked essentially what a trigger was. I had no idea. I didn't get the job. Instead I went home and learned about triggers. My advice: spin up a VM. Install a database (I'm partial to SQL Server Express). Build some tables. Take a backup. Delete the database files and see what it does. Fix it. Stop various services and see what happens. Find SQL snippits on the web and run them, then delete all the columns in the query and see what other stuff is in there. Write some bad views and stored procedures, get the timing on them, then try to fix them. Why did it go faster? Learn how to view the execution plan. What about in other database engines? Did the syntax change? Performance? No better way to learn it than to start using it, even if it's at home on a VM.
I worked a help desk job for about two years, and then managed to convince another department in my company I'd be good to have around. 
I've had a similar experience. Some things which might be Oracle or might be settings in my environment are: A blank string is the same thing as a null. Comparisons are case-sensitive. Implicit commit isn't the norm (I actually like this after I tripped over it). Granting permissions to certain actions within a schema is convoluted and awkward in places. But on the upside now I have LEAST() and GREATEST()!
I applied for a job that didn't mention SQL knowledge anywhere in the job description. If it had, I wouldn't have applied. I had never written a line of code before. I did well enough in two interviews that they asked me to try to work out some SQL queries using whatever online resources I wanted. I did well at that, got hired, and learned everything I know hands on. (I had a lot of other knowledge related to the company, the industry, and the types of historic data stored in the tables I work with.) What I'm saying is, maybe don't apply for a ton of a senior analyst positions and get your hopes up, but don't be afraid to apply for something that seems just a little out of reach. 
This is correct. I just wanted to add a thing about row order. * Unless your SELECT statement has an ORDER BY clause you **should not** expect any specific order. * If you need the rows to be returned in a specific order you **should** provide an ORDER BY clause. You may see that the database returns rows in a specific order without providing the ORDER BY clause but you should not assume that to be the case all the time. It depends on what rows it has cached in memory at the moment. You may get a different order when the global state of the database cache is different. 
I really think the main drawback is that it doesn't solve the problem. Thus you need to use it twice, which makes the query more complex and slower to run.
I entered a junior role because I showed interest in data. I had taken some basic Access courses and self taught SQL (shout-out to SQLBolt). Since then I have been learning on the job. The biggest thing (I think) to know about SQL or databases is the design structure and then the stuff in it. Once you know what's there, things get much easier. But that can take 6 months to two years to learn through trial and error 
There is a version of sql sever for Linux, I’m not sure what it’s like though. You have a massive amount of choice in regards to databases though. It’s entirely dependent of what you would like to do though. If it’s just to tinker with, you are probably best getting Postgres or Maria/MySQL. You can even run a free version of oracle if you wanted to. I would assume you would want some kind of gui to work with (it’s not strictly necessary, but you mentioned sql server so I assume you’ve seen that). MySQL has workbench which is your gui for that. There are also other tools that are around. I used to use Squirrel at work.
Lose all the backticks/single quotes/etc. They're not needed. CREATE TABLE exchange ( id int NOT NULL AUTO_INCREMENT, abbrev varchar(32) NOT NULL, name varchar(255) NOT NULL, city varchar(255) NULL, country varchar(255) NULL, currency varchar(64) NULL, timezone_offset time NULL, created_date datetime NOT NULL, last_updated_date datetime NOT NULL, PRIMARY KEY (id) ) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8;
I would say if you or your organisation runs everything else on windows, then there is no point in running sql server on linux. However if you dont have windows/windows skills today, dont use active directory etc, but are a linux shop instead, run sql server on linux. Featurewise it is mostly the same, performance as well. So it would come down to your os preference in general
Yes. It's called an application.
&gt; but in practice don't go with large varchars if your data doesn't require it. Size each column appropriately for the data it will contain you seem to not understand the difference between `char` and `varchar`. this advice is exactly wrong. the entire point of varchar is to get bad database programmers to stop guessing what the data will eventually need, and to let the database handle it at no extra cost. for example, the proper name of Libya is 74 characters: `Al Jumahiriyah al Arabiyah al Libiyah ash Shabiyah al Ishtirakiyah al Uzma`. never, *ever* bound `varchar` according to guesses about what the data might eventually be. `varchar` should almost always be max-size. the only time it shouldn't be is when you need to support varying length without storage cost, but also must forbid things over a certain length, ***which is not the case here***. your random guess cuts off 19 countries' names.
your flair is terrifying bravo
I was a C, C++, and later (gag) Java developer, i took real well too SQL and stored procedures and became the go-to guy for database stuff. &amp;#x200B; when a position opened up in a different department, i took it. &amp;#x200B; its useful at times when developers make shit up, and i politely shut their BS down.
Worked in Billing/Accounting and my manager was impressed on how technical I was. I knew a little bit of AutoHotKey scripts and automated a couple things. From there, she decided she wanted to teach me SQL. I’m still learning but I’m hoping in a few months my job is title changed to Financial Systems Analyst 
I currently am on step two, trying to follow in your footsteps. I made a career change three years ago and self taught SQL and SSIS. Now I'm just afraid to move into a DW design sort of position to more appropriately use my skills. Terrifying :) 
Ola's stuff is great, and I use it too, but built-in maintenance plans have improved enough since his maintenance solution came out that it's not really night and day anymore.
I don't mean to be rude, but i'm wondering how you can end up on this sub if you have the mentality of "if it's not yours to fix, why would you care." Isn't the whole point of a sub like this to look at problems that aren't yours to fix? It's about having a shared interest in something.
This is basically my story. Hello desk, slightly better help desk, and then positions came available using SQL that no one else wanted or could do. When I got my first SQL role I had only a single college course under my belt (from YEARS earlier) and was self taught the rest of the way. 
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/postgresql] [Question about circular dependencies and map tables (junior level)](https://www.reddit.com/r/PostgreSQL/comments/9vugyk/question_about_circular_dependencies_and_map/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
Yeah that sounds ok to me. Note that many:many relationships (aka n:n) require intermediary tables to facilitate actually having multiple of one pointing to multiple of the other and vice-versa. So you’ll have many a table (which is fine): Employee - Employee_Location Location - Location_Role Roles - Role_Employee So six tables total. 
I'm not sure of an automated solution, there are tools that can identify *anti-patterns*. But we use a combination of methods in our shop. 1. For common updates/inserts/deletes, create stored procedures, grant permission to execute those procedures to certain users. The nice thing about these is you can grant execute to users who cannot otherwise execute update/insert statements themselves. 2. Don't give UPDATE, INSERT, or DELETE access to certain user accounts, especially greener employees. 3. We use a Slack channel. If you want to execute a statement, post it in the channel, it has to be approved by one of the seniors. No self-approving allowed. If it's written by someone who cannot run the statement (because of permissions) a senior runs it. 4. Implement best practices. Always wrap updates / inserts / deletes in a transaction. Do not commit the transaction immediately. Have a test case at the end of the statement that selects the data updated to make sure it only affected the number of rows you expected it to update, then you can commit your transaction, or roll it back. 5. If you have a ticketing system, we always comment the SQL into the ticket, so if a similar ticket comes up, we can look back and use previously written queries. This also helps us identify what is worthy of creating a stored procedure.
Yikes.... Yup I definitely meant to say columns. I ended up arranging them in a way that makes the most sense from a readability standpoint when I need to Select * on the fly. 
Executive Recruiting in Raleigh, NC. $300k all-in. 
The official Microsoft T-SQL course is great. I did the verified course so I would get a certificate. 
Dice.com
I don't disagree, but in this case the actual motivation of the OP was far from clear in my opinion based on their responses and relevant to the conversation as it has progressed.
I like to sort my field names by alpha. And add 4 Columns to all new tables with defaults for created date, createdby, updateddate, and updatedby.
Excellent thank you @jimmyco2008!
SQL Server is fully supported on Linux and on Linux containers. Bob Ward has a book on the topic as well as posts and demos on the subject. 
Varchar will store less data on disk, but when the optimizer estimates a join, it will allocate memory for the explicitly stated varchar(xxx) amount. In large databases this can be significant and can cause paging to disk if multiple workloads are running.
100% agree
You're not wrong at all. Making an additional column to serve as the primary key while maintaining a unique constraint on the other columns is common and often the recommended approach. "surrogate key" is the term for this.
In what way does running the query without `NOLOCK` "break the database"?
That's awesome. Feel free to reach out if you have any questions!
Thanks, at this point I'm starting to lean towards pausing my MSIT &amp; completely focusing on SQL. I'm in a pretty good city for entry level sql jobs (at least from what I can tell?) and think I could land on my feet. &amp;#x200B; For someone who is already somewhat happy (pay, flexible) with their current job would you recommend the cert route rather than me downgrading to a help desk gig and trying to angle my way in?
Thanks for your answers guys. It's gonna be a tough decision for me as my current job pays pretty well and is flexible so I feel like \*maybe\* I should go the cert route and try to get an entry level job that way first, rather than quitting my job now and going help desk route. &amp;#x200B; &amp;#x200B;
I’m not running any queries with NOLOCK.
Do you know that locking is even a problem for your system in the first place?
I would actively search indeed.. all the dice jobs are cross posted there. Also actively look at career builder and linked in. Post a resume and fill in the category stuff.fully to monster, because many recruiters search it, but replying to jobs there is not that useful in my experience. Also, I would consider making separate resumes for the data analyst/ data scientist and bi analyst/ SQL business analyst . Usually when recruiters are looking for data analysts. They want.to see statistics experience and SAS or other big statistical software along w sql. And for ba /bl analyst they wAnt experience dealing w management and projects experience. I would also change your resumes job titles to match the above categories. A resume is a marketing document and recruiters / automated.search engines dont spend.the time to try to understand that a job may be the same but have different job titles. however when you fill out the actual application on the website... use your real job titles. Good Luck. 
Go to the big medical insurance companies, like UHC, Wellpoint, Aetna, and go to the careers page. Look for anything that says "analyst". After that try the hospital or medical group in your city. It's been my experience that companies that have a lot of healthcare data are where the most sql jobs are. Good luck 
Dev convinced c-suite they could build a proprietary platform for production. Alliteration aside, they sacrificed the vendor product's native reporting, and rather than promise a day0 alternative, gave access to the data on a reporting server. A one hour company training session on SSMS later, I got a free paid course in all things SQL and what can and should be done with it.
Really depends on how complex a query you want to write. What kinds of queries are we talking about here?
Nice name bro
In all honesty, I don't think you need a dedicated human tutor... at least not yet. Check out [Pluralsight](https://www.pluralsight.com/)... I absolutely love it and use it all the time, and you can filter your search for "Beginner" courses. Use it, and then if you get stuck on specific problems/questions post here or use your google fu. The best knowledge you can obtian is self guided. I'm a database developer between jobs travelling Asia for the next several months (needed a brak lol) who would love to take your money, but I think taking courses on Pluralsight or something like that is what would be best for you. Good luck!
I’ll help you for free. PM if interested.
Me too! 
There's tons of stuff in the internet for free. Or by a book. 
Wise owl SQL on YouTube 
You can try https://www.sololearn.com/ that will teach you the fundamentals for free and you get a certificate at the end of it. What helps is that you really must understand the basics to advance and finish the course. 
Agreed, I learned just about everything from googling
missing CROSS JOIN also, Venn diagrams for SQL suck
Venn Diagrams are awesome for teaching beginners sql. They suck for understanding the reality of what's happening.
That is normally the next set of the presentation :P. left table, right table, and the "join table" in the middle 
I'd be willing write the queries and explain them to you, but in order for that to be practical for anything than the simplest of queries, I'd need the read access to the database in question (or something like desktop share) since anything even remotely complex will require trial and error and ability to inspect schema and data in the tables. Are you sure you are willing a random person on the internet browse your employer's database? Feel free to contact me if you do, but seems sketchy to me. 
What do you know so far and what do you want to learn next?
I teach Oracle SQL and our classes run as follows: 1. General DB discussion and SELECT 2. Restricting and Sorting (WHERE, ORDER BY) 3. Single Row Function (nvl, upper, lower, initcap, substr, instr) 4. Data conversion(to\_char, to\_date, to\_number) 5. Aggregation and grouping data and GROUP BY 6. Displaying data from multiple tables ( JOIN ) 7. Subqueries and nested select statements 8. Set operators (UNION, UNION ALL, MINUS, INTERSECT) 9. Manipulating data (UPDATE, DELETE) 10. Using DDL (CREATE TABLE, VIEW) 11. Other DB objects (SEQUENCE, PROCEDURE, FUNCTION) 12. MODEL CLAUSE 13. Database Transaction (COMMIT, ROLLBACK) 14. Database Concurrency and multi tenant environment &amp;#x200B;
I've found psql to be fine for most purposes, but if you prefer a GUI environment check out SQL Workbench/J
Post your questions on this forum (make sure to censor or scramble data when posting examples) and people will help you for free. 
There are certainly times when it's appropriate to use a nested view: select * from orders where orders.id in (select id from users_cleaned_view users where users.state= 'CA') To get all orders from customers in California. Of course, you can use a left join here (it might be more efficient, I forget) but I don't see anything wrong with this. 
You don’t need to concatenate strings for multiple columns in most of the examples. select * from table_a where (column1, column2, column3) in (select column1, column2, column3 from table_b); This works in Oracle 11+, not sure about other DBs however. 
High beginner/low intermediate. The most advanced concepts I know are Joins, Creating/Updating Tables, and Database Normalization (although this last one I just learned about in a college class and haven't played around with in DBMSs like Access or SSMS). I'm honestly open to anything that uses SQL but I'd like to get into Data Analytics/Business Intelligence. Based off what I've read, I think I would enjoy those fields the most. 
Where is a good place to learn all this? I did used SQL daily in my old job, but always had a software engineer to fall back on if I got stuck on a join or was too busy to write a more advanced query. I finished the first eighteen lessons on https://sqlbolt.com/ last week. Now I want to learn more. 
As I mentioned above, I used SQL daily for a few years at my old job ... but I never formally learned it. I've done the https://sqlbolt.com/ lessons and now I want to ramp up my skills for data analysis. 
Wouldn't the discount query have just given the same performance gain if it had be schema bound and so become deterministic seeing as it had no table interaction? Yes I can see the customer category section showing its use. I have gone through a process of rewriting a number of processes where scalar functions have been used to TVF using a user defined table type to provide the bulk data. Has helped with performance in some key areas. 
You could self join to get all the other beers of the same type at other bars. Then do something with where exists. That does seem to be over complicating it tho.
Being schemabound and having no data access definitely helps, but (a) It will not eliminate function call overheads, and (b) The plan will still be serial. Also, imagine the same function being present in the WHERE clause. Without inlining, the predicates can never be pushed down to the scan. With UDF inlining, (a) function call overheads are eliminated, (b) removes the limitation on parallelism and (c) allows reordering of operations inside the UDF which may be beneficial in some situations.
What about LATERAL Inline Views CROSS APPLY Join OUTER APPLY Join
dude... Oracle!! 
&gt;I wish I can just define price ranges for each beer and load each sells tuple while adhering to the price ranges but this isn't allowed in this case. It's pretty unclear exactly what you want the output to be. The quoted bit above is the closest thing you have to a clearly defined output. Let's start there. You have a table that contains all process for all beers in all bars. You want the price range for each beer regardless of bar no? Beer will need to be in your select statement. Now you'll need to add the output of functions identifying the highest and lowest prices for each beer to that select statement. Ok now you have the price range for each beer across your universe of bars. If your final output is an analysis of each bar's price against the highest price in your table for the same beer then you'll need to self join the output you just created back to the original. Why don't you try typing up what you think that query looks like here? Don't try running it, just show us your thought process and we can guide you from there.
Look up "many to many relationship"
&gt;So far the drawbacks i can see are &gt;- we have to run '[except] against both [source to target] and [target to source] to get a full validation which is extra work. &gt;- duplicate rows cannot be identified. 1) Yes its double the required work requiring data checks to be done in both directions. 2) False, group by all columns and count(*) will work with except. Another point not in favour of except is that its an expensive way of doing this check. A very simple test with no indexes indicated that a select * from table except select * from anothertable is approx 4 times more expensive than a simple not exists check. create table #Temp1 (ID int ,text nvarchar(1000)) create table #Temp2 (ID int ,text nvarchar(1000)) insert into #Temp1 select 1,'test' insert into #Temp1 select 2,'test' insert into #Temp1 select 3,'test' insert into #Temp1 select 4,'test' insert into #Temp2 select 1,'test' insert into #Temp2 select 1,'test' insert into #Temp2 select 2,'test2' insert into #Temp2 select 3,'test' insert into #Temp2 select 4,'test' --11% select * from #Temp1 except select * from #Temp2 --12% example of checking for duplicates with except select id,text,count(*) from #Temp1 group by id,text except select id,text,count(*) from #Temp2 group by id,text --4% select * from #Temp1 t where not exists (select 1 from #Temp2 T2 where t2.ID = t.ID and t2.text = t.text) --4% select t.* from #Temp1 t left join #Temp2 t2 on t.ID = t2.ID and t.text = t2.text where t2.ID is null --12% select t.*,count(*) from #Temp1 t left join #Temp2 t2 on t.ID = t2.ID and t.text = t2.text where t2.ID is null GROUP BY t.ID,t.text having count(*) &gt; 1 drop table #Temp1,#Temp2 
MS Report Builder isn't a query tool, it's a tool for building reports which will be deployed to SQL Server Reporting Services (SSRS). If you're running straight SQL queries, use the appropriate client for your RDBMS. It sounds like you're not even sure what the RDBMS is on the back end. You should speak with your DBA(s) and get their recommendation.
GOT IT FUNCTION ADD_REWARDS(P_GUEST_ID NUMBER, P_REWARD_POINTS NUMBER)RETURN NUMBER AS V_MAX_POINTS NUMBER := 0; BEGIN SELECT REWARD_POINTS INTO V_MAX_POINTS FROM A2_GUESTS WHERE GUEST_ID = P_GUEST_ID; V_MAX_POINTS := V_MAX_POINTS + P_REWARD_POINTS; UPDATE A2_GUESTS SET REWARD_POINTS = V_MAX_POINTS WHERE GUEST_ID = P_GUEST_ID; RETURN V_MAX_POINTS; EXCEPTION WHEN NO_DATA_FOUND THEN RETURN 0; END ADD_REWARDS;
 SELECT bar_name, s.beer_name, price, maxprice, FROM sellls s INNER JOIN (SELECT beer_name, max(price) as maxprice FROM sellls GROUP BY beer_name) sm ON sm.beer_name = s.beer_name WHERE price &lt;&gt; maxprice This would get you the beer prices that need to be updated by finding the highest price for that beer then comparing that price to each bar's price, and showing you the ones that don't match. but as /u/T-TopsInSpace said, this might not be the output you are looking for.
Yeah, I am stuck using Ms Report builder for freehand queries. They often need it as not all the tables in the database are in the Report Builder datasets. The RDBMS is [Ingres](https://en.wikipedia.org/wiki/Ingres_\(database))
Your report query syntax must match the database version. If Ingres doesn't support a function (ie: windowing), you can't use that function regardless of what client/tool you're using to run the query with.
That's helpful thanks. I reckon its this one then: https://communities.actian.com/s/article/Ingres-10-2-SQL-Reference-Guide
This might give you the specific version of Ingres: SELECT _version(); http://wikis.openlinksw.com/UdaWikiWeb/HowCanIDetectMyIngresDBMSVersion?command=plain-html
&gt; I am stuck using Ms Report builder for freehand queries Talk to your DBA and ask for a proper Ingres client. Using Report Builder for this is ridiculous.
Looks like Report Builder thinks it's connecting to a SQL Server instance somehow.
thank you for your quick response, Iam afraid Iam not allowed to provide those info... thought there are general rules. Dos and donts, etc... Its basically a table of projects. Each project has several cost points which are located in two tables, one for "should" and the other for "is". I want to collect all that data in one view. I know thats not much I can offer here. I guess I just have to figure out myself... 
oh yes that will be it. There is a SQL server in between my machine and the actual database.
So I'm no SQL expert but I use it frequently and always have to Google. You can do join or the variations of join. Inner, outer, all that fun stuff. I recommend doing a bit more research as join should would in this case. It may also depend on the data in table because I had one join that should've worked but was erroring out do to having rows with null data. 
The DB and software are provided by an external supplier. We basically have to accept what we are given. I think I tend to want to run more complicated and different reports than they are expecting. Knowing how to describe the shortcomings is very useful though Thanks.
 SELECT a.col1, a.col2, b.col1, b.col2 FROM table1 a INNER JOIN table 2 b ON a.someCol = b.someCol WHERE a.colId = 1234;
I get how to join tables but I’m not sure how to select a specific row after that lol 
What? Are you connecting to SQL Server which is then connecting to the actual database via a Linked Server? If you're connecting to SQL Server from Report Builder, you can connect with SQL Server Management Studio or Azure Data Studio (formerly SQL Operations Studio).
&gt; How can I add aliases to my subqueries? like this -- SELECT foo , ( SELECT COUNT(*) FROM bar ) AS sub1 FROM faq
It's a bad idea, don't do it. You need a linking table `CREATE TABLE genre (genreid INTEGER PRIMARY KEY, name TEXT NOT NULL, UNIQUE(name));` `CREATE TABLE user (userid, INTEGER PRIMARY KEY, email TEXT NOT NULL, UNIQUE(email));` `CREATE TABLE user_genre (genreid INTEGER NOT NULL REFERENCES genre(genreid),` `userid INTEGER NOT NULL REFERENCES user(userid),` `n_times_liked INTEGER NOT NULL DEFAULT 0,` `UNIQUE(genreid, userid));`
I think he's asking how to : select foo from ( select foo from table where bar ) AS bar inner join ( select n from table ) AS d on bar.n = d.n And IIRC, there were some times when I was using 2008R2 that SQL didn't like it when you used `AS`. Can't remember if it was on sub-queries, or some other random place, but it's just as easy to: select foo from ( select foo from table where bar ) bar inner join ( select n from table ) d on bar.n = d.n
&gt; Can I implement a list of key-value pairs into one field? yes you can, but you shouldn't
Postgres comes preinstalled on Fedora. Maria DB also comes bundled in. There's always SQLite too if you're just trying to learn. https://mariadb.com/kb/en/library/distributions-which-include-mariadb/
Report Builder doesn't provide the dataset directly; the definition of the dataset is embedded in the RDL file you're working with, and it's just a query or stored procedure that's getting run against the SQL Server instance you're connecting to. So, if you can crack into that dataset to get the query/stored proc itself (which should be in the dataset's properties), you can turn around and run that via SSMS or ADS, provided you can pass in the right parameters. Going from Report Builder -&gt; SQL Server -&gt; Ingres (via Linked Server) is messy if you're trying to run ad-hoc queries, IMHO. If you can bypass Report Builder that'll make things a bit easier Linked Servers in general are a pain, and can cause major performance issues if not handled properly. Plus, you don't have good insight into the actual data &amp; schema on the other end. The whole setup seems very odd to me to be completely honest.
convert(varchar, date, 23)
Thanks for ruining the ultimate question, bud. Have an upvote!
It looks like you are wanting to use variables (and no, I don't look like a paper clip). In SQL Server, you can do the following: DECLARE @usercount int = (SELECT count(*) FROM act18.ol_all_users); DECLARE @2017usercnt int = (SELECT count(*) from dbo.tc_user_apps_enroll, dbo.tc_user_apps WHERE tc_user_apps.appsid = tc_user_apps.ID AND tc_user_apps.db_year = 2017); SELECT @usercount - @2017usercnt; 
Although I don't think this is what OP is after, your example looks like a job for CTEs: WITH bar as ( SELECT foo FROM table WHERE bar = 1), d as ( SELECT n FROM table) SELECT foo from bar inner join d on d.n = bar.n 
Sure, but sometimes you may have a very long complex query and then need to add a sub-query join to get a final piece of data. A cte would require totally reinventing the wheel, and if you're doing that you're probably better off using #tables than a CTE.
What about something like this? SELECT id from table where sum(value) = 0 group by id having count = 2
What database is it? Lots of them have a standard function to do exactly this. DB2 for example has VARCHAR_FORMAT. 
That looks spot on for what OP described, but /u/m1k3y60659 needs to elaborate on the max amount of times an `id` can be in the table. Like `1|10,1|-10, 1|3,1|-3, 1|5,2|15` Mine will be a slight variation on the absolute value method: select t1.id, t1.value, t2.value from [table] as t1 inner join (select id, value from [table] where value &lt; 0) as t2 on t1.id = t2.id and t1.value = abs(t2.value) I'm pretty sure mine has terrible performance.
Thanks Devel, I get the following error: select @usercount - @2017usercount; Must declare the scalar variable "@usercount".
 SELECT t1.Id, t1.Value FROM TableName t1 WHERE EXISTS ( SELECT 1 FROM TableName t2 WHERE t1.Id = t2.Id AND t1.Value = -t2.Value ) Beware that, currently, any Id with a Value of 0 will be returned. 
Could someone ELI5? Or... Maybe not 5, but a summary from someone who can explain it? 
Select Ticket, 
With Base as ( Select Ticket, Asignee, Row_Number() Over(Partition By ticket, asignee Order By , Change Time asc) as Ordered_Set From YOUR_TABLE ) Select B.Ticket, B.Asignee, Max(B.Ordered_Set) as max_assgn From BASE B Group By B.Ticket, B.Asignee
Hello, there is no limit on how many times an ID is present in the table. I have seen ID's with only one occurrence an up to 20 or so. 
^^As much as it pains me that this is not a prebuilt function, this is 100% the correct answer
ContactID
.nodes only works on a column, so left joining on obj7 again would get me nowhere. 
Also, if it an even number of rows you need to take the two middle records, sum, then divide by 2. You can use this to find if the number of records are even. Mod(MAX(row_number) ,2). If the result is 0 then it’s Even else it’s odd. Let us know if you need further help.
From ( select castcol = cast(col as xml) from table ) alias Outer apply alias.castcol.nodes
They also are recycled over time after a short period of inactivity and a contact can change phone number while staying the same contact.
If you don't do this you will have a bad time.
Supply the DDL so people can get a clearer picture of what the probable data looks like. 
Group of operations (inserts or deletes, for example) executed in one presumably automated processing. 
DM away
I would suggest reviewing what a primary key is and then look at your candidate columns above to see if it fits the definition. 
Something like this? Select employee.name, sum(sales.amount) total_sales From employee Join sales On employee.id = sales.employee_id Where sales.year between 2012 And 2014 Group By employee.name Order by total_sales Desc Limit 10 Something this like this is what I would do
Do you have any time to chat to possibly help me run this down? Could send the whole problem. This is a paraphrased example
I use the term ‘batch’ a lot in general SQL to refer to a bunch of prepared queries executed at one time.
For second one, I would do something like this Select Brand, customer_id, sales From table1 Join table2 On table1.product_id = table2.product_id Group by customer_id, sales, brand Order by sales Desc limit 3
I need top 3 customers for those corresponding brands only. There is a possibility that a customer might have purchased from 2 or more brands.
Bad thing `SELECT a, my_func(b) FROM t` (for several reasons) suddenly becomes a good thing, because now the `my_func(..)` call is transparent to the SQL optimiser. Like inline table valued functions, but for scalar functions.
here you go &amp;#x200B; &gt;\-- create test data &gt; &gt;with cte\_numbers as ( &gt; &gt;select \* &gt; &gt;from (values &gt; &gt;(1),(6),(9),(14),(18),(0),(-1),(-14),(-18),(-21),(19),(-19),(-41)) &gt; &gt;numbers(n) &gt; &gt;) &gt; &gt;\-- return the values with both positive and negative equivalents &gt; &gt;select \* &gt; &gt;from cte\_numbers a &gt; &gt;join cte\_numbers b &gt; &gt;on a.n\*-1 = b.n &gt; &gt;and b.n &lt; 0 &amp;#x200B;
So... Its been mentioned that it doesn't fit the definition of primary key. A sequence ID will index well, phone numbers aren't necessarily sequence. And using a phone number as an ID adds some data redundancy. 
its not pointing to any server. The data is taken locally
I cant get it to work. Can you please edit my code how it should be done? arr2 = "\[" &amp; Join(arr, "\], \[") &amp; "\]" query = "SELECT " &amp; arr2 &amp; " FROM \[" &amp; filename &amp; "\] WHERE \[group\] like '" &amp; group &amp; "' " &amp;#x200B; arr is an array of headers
What have you tried so far?
I think switching your FULL OUTER JOINS to INNER JOIN would speed it up if you can do it. FULL OUTER JOIN would return all results from each table where the INNER JOIN would only return the data that matches the key you are joining on 
So that the tool might teach you about passwords and an internet connection.
The agency I work for uses 2008. Why they haven't upgraded is beyond me.
What do you mean by MSSQL failed? 
What do you mean by MSSQL failed? 
Had to ask. There are others answers via DDGo search.
MS SQL doesn't usually "fail" unless you hose something up pretty bad. What is this "failure?" If you're doing a course that requires MS SQL, you're going to struggle using MySQL.
&gt; Why they haven't upgraded is beyond me. * Cheap * Afraid * Ignorant * Stuck on old software that claims to only support 2008 (or worse, refuses to install/work with newer releases) and they won't upgrade (see the first 2) Pick as many as you like. You might want to start dropping not-so-subtle hints that 2008 goes 100% EOL, unsupported by MS (unless you want to spend vast amounts of money), and 3rd-party support will start being dropped as well, effective July.
I'd recommend parameterisation even if the data source is trusted and safe. Just good practice.
I may be wrong, but I always thought it's everything you do between BEGIN and COMMIT of a transaction.
A batch is multiple statements (either DDL or DML)
&gt; my issue with Venn diagrams for SQL is that they never show individual values inside the circles I'm interested in knowing why this would matter. When teaching SQL and relational databases in general, thinking in terms of an individual value should be avoided as much as possible. Students should think in terms of the returned sets. The values in those sets rarely matter on an individual basis as long as the data is correct and the predicates used to filter the set are in accordance to the question they're trying to ask. 
Nah fam, a batch is a set of instructions to be sent to the engine at once. That may seem like the same thing, but it's not. Assume I'm using mssql for the rest of my post, because I'm most comfortable with it. `go` is the batch terminator. Try scripting 2 create tables, remove all batch terminators, and you will get an error. Mssql will tell you that you can only create 1 table per batch. It has to do with what your program actually sends to the server to process at once. It can be a single statement, it can be multiple statements. Imagine you begin a transaction, update something on the server, and never commit. You can see the results of that on the server with dirty reads. You've sent the commands. Committing or rolling back doesn't 'complete' the batch or whatever, it's a separate concept. 
I am new to SQL, so I wasn't that familiar to begin with, but it did seem odd to be using software a decade old. 
Multi.l steps as well may help. Get the data into temp,then get the maxes. 
&gt; I'm interested in knowing why this would matter. helps to understand exactly what's being joined -- which is the whole point
That's odd... it's certainly declared. What environment are you trying this in? SSMS?
Turns out you had the right idea, but my brain just wasn't tracking. Thanks for the input!
for example, do a diagram of employees and departments (where each employee belongs to only one department) now place sample employee values into one circle, and sample department values in the other circle what goes in the overlap? yes, ~both~ employees and departments just one of each for illustrative purposes? hell no, lots of them
Good point, but depending on what the business requirement is he might want to return all records. Especially since there’s no where clause.
why bother with the join? select x.value(blah) from ( select SPEC = cast(spec as xml) from cmo7 ) A cross apply a.SPEC.nodes(blah) x(n) you could also use CTE... with cteSrc AS ( SELECT SPEC = cast(spec AS XML) FROM cmo7 ) select T2.n.value(blah) from cteSrc OUTER APPLY cteSrc.SPEC.nodes(blah) T2(n)
True, I didn't think of not joining because I have a lot of other info in that table that I am using. And mostly because I am self taught with SQL so I learn as I go!
What database engine?
SSMS.
Sick, thanks! 
I would us an OUTER APPLY but I don't know the syntax for Oracle [maybe this](https://asktom.oracle.com/pls/asktom/f?p=100:11:0::::P11_QUESTION_ID:9530807800346558418)
 Cities ----------- CityID (PK) Name CityZipcodes ---------------- Zipcode (PK) CityID (PK, FK)
yes, REPLACE can do that UPDATE yourtable SET yourcolumn = REPLACE(yourcolumn, '''' , '' ) WHERE yourcolumn LIKE '%''%' 
BINGO so frustrating! A good challenge for me though, I take it you work in a similar field? 
your error is in the ORDER BY clause remove it to confirm
Are you sure you need full joins? Maybe left or inner would be enough? Do you really need all these group by? When I see that many, I always immediately think: "Someone who wrote this query wanted single rows, but they screwed up joins and got duplicates, so they just grouped the fucked out of it". If that's the case, figure out what you're missing - probably a join condition - to make the result set return what you want. From what I can tell, you might have a 1-1 relationship between all the tables except T_NTR_IMPORT_SCORING (isco). Try using a CTE to preaggregate, something like: WITH iscoAggregated AS ( SELECT isco.FI_TRANSACTION_ID , MAX(CASE WHEN isco.MODEL_XID = 'FP.FRD.CARD' THEN isco.MODEL_SCR END) AS ScoreBase, , MAX(CASE WHEN isco.MODEL_XID = 'FP.FRD.CARD.ADPT' THEN isco.MODEL_SCR END) AS ScoreAA FROM BOI_ATR.T_NTR_IMPORT_SCORING isco GROUP BY isco.FI_TRANSACTION_ID ) SELECT ... -- all your regular stuff FROM ... -- all your regular tables FULL OUTER JOIN iscoAggregated -- you no longer need GROUP BY! Alternatively, if that scoring aggregation you're doing is only because the scoring table contains multiple entries but unique for each `MODEL_XID`, then you might want to get rid of the aggregation altogether and instead join the table twice, one for each of the `MODEL_XIDs`that you're interested in (FP.FRD.CARD and FP.FRD.CARD.ADPT). That would work if the (FI_TRANSACTION_ID, MODEL_XID) pairs are unique (maybe there is even an index there!). Something like: SELECT -- all your stuff -- scoring stuff, no longer needs aggregation! , iscoCard.MODEL_SCR AS ScoreBase , iscoCard_ADPT.MODEL_SCR AS ScoreAA FROM -- all your regular tables JOIN BOI_ATR.T_NTR_IMPORT_SCORING isco_Card ON isco_Card.FI_TRANSACTION_ID = iadt.FI_TRANSACTION_ID AND isco_Card.MODEL_XID = 'FP.FRD.CARD' JOIN BOI_ATR.T_NTR_IMPORT_SCORING isco_Card_ADPT ON isco_Card_ADPT.FI_TRANSACTION_ID = iadt.FI_TRANSACTION_ID AND isco_Card_ADPT.MODEL_XID = 'FP.FRD.CARD.ADPT' -- no need for GROUP BY! Your biggest problem is the GROUP BYs - it's a hugely expensive operation because it requires sorting of the matching data, and since you're doing this across many tables, there's no index that can be applied. 
worked! Thank you. Not that I need it for this specific case, but any reason why order by caused the error?
Zipcode and CityID are a concatenated PK
because you're not grouping on `create_date`, you're grouping on the month and selecting the month, so you can't sort on the entire date, because it doesn't exist in the SELECT clause
Indeed I do. In the world of Commercial Credit cards. Very similar I'm sure, but we also have to deal with a lot more spend controls on the cards (amount / volume / MCCs). Also the cardholders all work for some company, that company is a part of an organization, that organization might be part of a larger organization. And our relationship management structure that works with the various companies. In some ways I prefer this side because there's so many types of info our client's are interested in. Plus a lot of what I do ends up in the hands of the actual customers.
&gt; so they just grouped the fucked out of it Will be stealing this line.
 select i.inv_item, i.inv_name, c.comment, c.upd_timestamp from inventory i inner join inv_comments c on i.inv_item = c.inv_item WHERE (c.inv_item, c.upd_timestamp) IN (select inv_item, max(upd_timestamp) as "ts" from inv_comments group by inv_item); 
I'm still not sure why that matters... Your final output is why I think you think it is dynamic, because it would involve multiple joins... but if you were to take your final output and put it into a single column I'm not sure why it needs to be dynamic at all, so lets start there: | Increment Type | Increment | Number | | :--- | :--- | :--- | | Steel | .25 | 3 | | Alloy | .25 | 1 | Essentially that is the final output you want, and then for whatever reason you want to pivot the data, correct?
I mostly program and manage the fraud monitoring systems but this was creating a report within the fraud monitoring system as part of a project. My job is extremely varied though haha 
If you do what to pivot your data, and do it dynamically the you can Google "dynamic pivot" for lots of examples, or see [this](https://www.reddit.com/r/SQL/comments/4vnz0p/looking_to_do_a_simple_pivot_without_aggregate/d62raii/) comment here.
some people still use windows xp
I see that the \* somehow got stripped from my count() functions. Different error, but let's fix what we can see: `DECLARE @usercount int = (SELECT count(*) FROM act18.ol_all_users);` `DECLARE @2017usercnt int = (SELECT count(*) from dbo.tc_user_apps_enroll, dbo.tc_user_apps WHERE tc_user_apps.appsid = tc_user_apps.ID AND tc_user_apps.db_year = 2017);` `SELECT @usercount - @2017usercnt;`
&gt; MS SQL doesn't usually "fail" unless you hose something up pretty bad. Nah, that [installer pre-flight check](https://i.imgur.com/rDMKchN.png) is super picky. "OMG you updated an application and didn't reboot! You gotta reboot before you can install this!" "OMG you're missing the C++ library. You install that and reboot you can install this!" I'm fairly certain there was even one time that I had to reboot because the setup files updated themselves and the system panicked because I needed to reboot after updating the setup files. (Truthfully, the update probably patched another underlying component, but still.) There's a reason that there's like 10 items that come before the "Ready to Install" item. 
Exactly my point. You have to work pretty hard to get SQL Server to just "fail" with no explanation.
Yeah, but it's trivial to get it to fail with an explanation. If you're new like OP is without any background in troubleshooting Windows or Microsoft applications, you're not necessarily going to understand the explanation. 
You can use CTEs with `select`, `update`, and `delete` queries. In SQL Server, it's *usually* just syntactic sugar (exception: when you go recursive) to extract a complex subquery for readability. In Postgres &amp; Oracle, it'll behave like a temp table, so there may be performance improvements from using them.
Not sure on the negatives. we are implementing these at my work and they are efficient and saves a lot of disk space vs rowstore indexes. You shouldn't use these in OLTP environments, they are really meant for data warehousing applications (Lots of data that does not change often). &amp;#x200B; This might help - [https://blogs.msdn.microsoft.com/sqlserverstorageengine/2016/07/18/columnstore-index-differences-between-clusterednonclustered-columnstore-index/](https://blogs.msdn.microsoft.com/sqlserverstorageengine/2016/07/18/columnstore-index-differences-between-clusterednonclustered-columnstore-index/)
I was able use a substring and concat the HH:MM:SS but i feel like there's gotta be more technically correct way. select substr(last_day(add_months(trunc(sysdate,'mm'),-1)),1,10)||' 23:59:59' from dual;
I have this doubt: What if i want to mix my contact database with my friend's database, but let's say we both have a ContactID '001', then the ContactID wouldn't be unique? Or maybe you could give more complicated ContactID combinations like 6 characters and random, but isn't there a chance for them to repeat? That's why i initially thought i could use phone numbers as primary keys.
I'm not familiar with your database and your multiple joins made my head spin at first glance, but I will say that I've pulled way too much data before with WHERE clauses. While it looks weird, using AND instead of WHERE can help in some of those cases. For example: SELECT fields FROM table1 LEFT OUTER JOIN table2 ON table2.column = table1.column AND rows = value
If this is a concern, use a GUID; they will be unique across systems.
There are 4 shifts in TA\_CALENDAR, defined in TA\_SHIFT. (Only thing useful in the TA\_SHIFT table would be the ID, which is the same as PR\_EMP.TA\_SHIFT\_ID and TA\_CALENDAR.TA\_SHIFT\_ID Each employee is assigned one of these shifts in PR\_EMP 
&gt;GUID Hey that's useful stuff, thank you
Okay. So your second query adds two joins - the TA_SHIFT table, in addition to the TA_CALENDAR table. What does the TA_SHIFT table look like?
So I have been running with trying to create the table like the one you listed before I read your last comment. I think it will workout ok I just need to get everything into a table that I can pull in to my C++ program and create a report. Anyway I am almost there but I am stuck, I will post all the actual tables and code here instead of the example. &amp;#x200B; Here is the code &amp;#x200B; `TRUNCATE TABLE WeightData` &amp;#x200B; `INSERT INTO WeightData (WeightType,WeightIncrement)` `SELECT DISTINCT OutboardWeightTable,OutboardAmount` `FROM ProductionData` `WHERE [Datetime] between '180821_150000' and '180822_000000' and (OutboardAmount &gt; 0)` &amp;#x200B; &amp;#x200B; `SELECT COUNT(*)` `TestNumber,` `a.WeightType,` `a.WeightIncrement` `FROM WeightData a` `inner join ProductionData b` `ON (a.WeightType = b.OutboardWeightTable) and (a.WeightIncrement = b.OutboardAmount)` `WHERE [Datetime] between '180821_150000' and '180822_000000' and (OutboardAmount &gt; 0)` `GROUP BY a.WeightType,a.WeightIncrement` &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B; So I have created the table, found all the unique information, then I used a join and I was able to count up the number of each increment. Where I am suck is how do I the results in "TestNumber" into the the table under Number? I have been at this for a stupid amount of time and at some points I think I was close but I have tried so many things that didn't work... I am not sure if I could somehow combine the first insert with the select count, or if there is some way to use "UPDATE" which I tried in numerous different ways to update the number column. I know there is something obvious I am missing but I am stumped again.
Article has a small error. You have "has has" it's even highlighted in red.
I think I got it. Lol it's not for homework. Just for practice.
Do separate subqueries, one for max and one for min. Then wrap the whole thing in a select *
Ahh cool glad to help. What did you go with? 
Thank you. Is this what you mean? When I run the below I only get max and min salaries for each dept, not the user records where salary = max or salary = min. &amp;#x200B; select \* from (select min(salary) as minSal, dept\_id from emp group by dept\_id) t1 join (select max(salary) as maxSal, dept\_id from emp group by dept\_id) t2 on t1.dept\_id=t2.dept\_id
I recognized the schemas 👍and really wanted to jump in with a good comment... But I feel like others broke it down alot better than I could on my phone watching two dogs / Netflix. As others said your group by seem blah. Full outer joins blah.
Hey CommonMisspellingBot, just a quick heads up: Your spelling hints are really shitty because they're all essentially "remember the fucking spelling of the fucking word". You're useless. Have a nice day! [^Save ^your ^breath, ^I'm ^a ^bot.](https://www.reddit.com/user/BooCMB/comments/9vnzpd/faq/)
Hey BooCMB, just a quick heads up: The spelling hints really aren't as shitty as you think, the 'one lot' actually helped me learn and remember as a non-native english speaker. They're not *completely* useless. Most of them are. Still, don't bully somebody for trying to help. Also, remember that these spambots will continue until yours stops. Do the right thing, for the community. Yes I'm holding Reddit for hostage here. Oh, and /u/AntiAntiSwear, no u Now we have a chain of at least 4 bots if you don't include AutoMod removing the last one in every sub! It continues! Also also also also also Have a nice day!
CommonMisspellingBot provides a mnemonic device in an attempt to help people remember spellings of commonly misspelled words. Quirky little sayings that hopefully stick in your head and help in day to day life. You on the other hand follow this poor bot around repeatedly calling it useless. Please take a long, hard look at your life choices my bot friend.
Hey CommonMisspellingBot, just a quick heads up: Your spelling hints are really shitty because they're all essentially "remember the fucking spelling of the fucking word". You're useless. Have a nice day! [^Save ^your ^breath, ^I'm ^a ^bot.](https://www.reddit.com/user/BooCMB/comments/9vnzpd/faq/)
Hey BooCMB, just a quick heads up: The spelling hints really aren't as shitty as you think, the 'one lot' actually helped me learn and remember as a non-native english speaker. They're not *completely* useless. Most of them are. Still, don't bully somebody for trying to help. Also, remember that these spambots will continue until yours stops. Do the right thing, for the community. Yes I'm holding Reddit for hostage here. Oh, and /u/AntiAntiSwear, no u Now we have a chain of at least 4 bots if you don't include AutoMod removing the last one in every sub! It continues! Also also also also also Have a nice day!
CommonMisspellingBot provides a mnemonic device in an attempt to help people remember spellings of commonly misspelled words. Quirky little sayings that hopefully stick in your head and help in day to day life. You on the other hand follow this poor bot around repeatedly calling it useless. Please take a long, hard look at your life choices my bot friend.
hEy, StOpAlReAdYbOt, JuSt a qUiCk hEaDs-uP: **aLoT** Is aCtUaLlY SpElLeD **A LoT**. yOu cAn rEmEmBeR It bY **It iS OnE LoT, 'A LoT'**. HaVe a nIcE DaY! ^^^^ThE ^^^^PaReNt ^^^^cOmMeNtEr ^^^^cAn ^^^^rEpLy ^^^^wItH ^^^^'dElEtE' ^^^^tO ^^^^DeLeTe ^^^^tHiS ^^^^CoMmEnT.
dOn't eVeN ThInK AbOuT It.
 Hey CommonMisspellingBot, just a quick heads-up: **alot** was the name of an extremely childish old man who lived in ancient Rome. Luckily,, **alot** started staying up all night, thinking about getting dominated by midgets. When this was discovered by the bus **alot** was traveling in, it led to a very eventful night out . **alot**'s tombstone said: **Stfu CommonMisspellingBot, no one cares what you have to say.** ^^^^I'm ^^^^a ^^^^bot. ^^^^Feedback? ^^^^[hmu](https://www.reddit.com/user/stopalreadybot/comments/9w7cy9/feedback/) ^^^^Dear ^^^^mods, ^^^^just ^^^^ban ^^^^CommonMisspellingBot ^^^^and ^^^^the ^^^^other ^^^^bots ^^^^will ^^^^automatically ^^^^stop. 
 Oh shut up, you little talking doll. ^^^^I'm ^^^^a ^^^^bot. ^^^^Feedback? ^^^^[hmu](https://www.reddit.com/user/stopalreadybot/comments/9w7cy9/feedback/) ^^^^Dear ^^^^mods, ^^^^just ^^^^ban ^^^^CommonMisspellingBot ^^^^and ^^^^the ^^^^other ^^^^bots ^^^^will ^^^^automatically ^^^^stop. 
dOn't eVeN ThInK AbOuT It.
 Oh shut up, you little talking doll. ^^^^I'm ^^^^a ^^^^bot. ^^^^Feedback? ^^^^[hmu](https://www.reddit.com/user/stopalreadybot/comments/9w7cy9/feedback/) ^^^^Dear ^^^^mods, ^^^^just ^^^^ban ^^^^CommonMisspellingBot ^^^^and ^^^^the ^^^^other ^^^^bots ^^^^will ^^^^automatically ^^^^stop. 
yup, that should work for you!
I would do SELECT Dept , Employee , Salary , CASE WHEN minSalary = 1 THEN 'Lowest salary in department' END , CASE WHEN maxSalary = 1 THEN 'Highest salary in department' END FROM ( SELECT Dept , Employee , Salary , RANK() OVER (PARTITION BY Dept ORDER BY Salary ASC) AS minSalary , RANK() OVER (PARTITION BY Dept ORDER BY Salary DESC) AS maxSalary FROM salaryTable ) as a WHERE minSalary = 1 OR maxSalary = 1 The two rank functions work in opposition. The order by Salary ASC will give anyone in the department with the lowest salary a rank of 1. The order by Salary DESC will give anyone in the department with the *highest* salary a rank of 1. Then to find both the highest and lowest salaried employees, just select where minSalary = 1 or maxSalary = 1.
&gt; Is this what you mean? When I run the below I only get max and min salaries for each dept, not the user records where salary = max or salary = min. What you have is correct for finding the min and max salary for each department, and likely what /u/asisoid meant. That's your first step. Now use that to get the matching employee records - should be a simple join.
You've got the right thought process, however you're treating your where in clause like it is a table rather than a list of values - which won't work because you'll be trying to compare salary to a row of data. Instead, take that query in the where clause and join it to emp on salary in (max_salary, min_salary) and dept_id = dept_id. 
After law school I joined a legal department of a large insurance company. Out of \~100 people in the dept no one knew how to do any sort of data analytics or queries so I took it upon myself to learn. I started with MS Access, impressed management that I ~~took the time to learn on my own at home~~ was naturally good at working with data that I eventually expanding to doing queries with SQL and automating work with VBA, been riding that wave ever since. &amp;#x200B; I'm still in a hybrid legal/analytics role, not sure what the future holds but if/when I move on it'll probably be to a pure analytics role rather than a legal one. My company has a decent tuition reimbursement program, so if I do land a pure analytics role I'll probably &amp;#x200B; Also it's really ~~tragic when I look at my student loans~~ funny how much more valuable having "1+ years of SQL" on your resume is than having a JD/license to practice law.
just Union all 3 select statements Select ID,Name, DeviceName,DeviceNumber Union all Select ID, Name, DeviceName2,DeciceNumber2 Union all Select ID, Name, DeviceName3, DeviceNumber3 Order by 2
I've got it posted above, though I've slimmed it up even more from what is there, getting rid of the joins. 
I was going to suggest row_number, but this is a better solution. 
This is one of the more advanced questions I've seen. There is a lot going on here.