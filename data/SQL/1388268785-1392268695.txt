If you want to have some fun working on your SQL skills, you can give the Schemaverse a go. It is a space-based battle game where you use SQL to play. The game is based on PostgreSQL but, either way, it will still sharpen your skills. http://schemaverse.com http://schemaverse.com/tutorial/tutorial.php 
It's free and forgiving of beginner mistakes. Unfortunately though, a lot of that forgiveness can come back and bite you in the rear when you're doing something complex. IMO, because of PostgreSQL (and SQL Server Express) there's no excuse to use MySQL unless you're joining some legacy project already built on it. 
You learn to read execution plans. 
In MSSQL you can use spatial datatypes which can be indexed to do distance filtering. So yes, some DBMSs can do a lot of optimization on this sort of thing. A nice little blog post going into the performance details of just such a use case : http://aboutsqlserver.com/2013/09/03/optimizing-sql-server-spatial-queries-with-bounding-box/ Note the 3 execution plans in the very beginning of the Blog, specially the plan for the 3rd query (which uses indexing). 
Good suggestion. You should look into using the built in geometric types and functions of your database instead of using plain numeric types. This way you can more easy benefit from indexes.
Thanks, I'll see if I can figure out the EXPLAIN command. Do you happen to know any examples where SQL DMBSs reliably do useful optimizations? Like where it wouldn't make sense for me to manually write extra optimization.
Hi doublehypen - sorry it took me a while to get back to this but everything worked after I played around with the idea that you presented in this solution. Thanks so much!!
Wow, I really appreciate this. The PARTITION BY is actually going to be perfect for the next part of my project, where I need to grab the top N records and sort them by another column (pretty much exactly what you've described above). Many thanks!
Query optimization is no simple thing to do and since DBMS developers have spent years and years just figuring out how to get you the result set as quickly as possible I think it's safe to assume that it is practically as fast as possible. All you can do is to design the tables, indexes and queries so that the DBMS can do it's thing as easy as possible. Here's some reading: http://use-the-index-luke.com/sql/table-of-contents
You said yu're using sqlite, which has EXPLAIN, and also EXPLAIN QUERY PLAN. You specifically want EXPLAIN QUERY PLAN; EXPLAIN shows the bytecode assembly sqlite will run.
Databases like MarkLogic I assume. They are queried with languages like XQuery and Xpath.
This is how you get the previous 7 days. datum BETWEEN DATE_ADD(CURRENT_DATE,INTERVAL -6 DAY) AND CURRENT_DATE If datum is actually stored as a String, then convert it to a Date. 
This link seems to say that it is NOT possible. [ You cannot add or remove features to a SQL Server 2008 or SQL Server 2008 R2 failover cluster](http://support.microsoft.com/kb/2547273)
Well then, I have removed my bad info.
Yes. A multi-value index is sorted on the first value, then the second, then the third, and so on. If you have SQL server show the actual execution plan, it should suggest the index for you to add.
Thanks. Yeah in this case it suggested I added an index ([PRODUCT_ID], [SALES_DATE])
alternativly, ;with ip_list(ip) ( SELECT 'x.x.x.x' union select 'x.x.x.x.x' ) SELECT ip FROM ip_list ips WHERE NOT EXISTS ( SELECT 1 FROM Nodes n WHERE n.ip_adress = ips.ip ) It should be a tiny bit faster, even thou in this case that proberbly is below messuring 
Got it using Case: SELECT IP_Address, CASE WHEN EXISTS(SELECT * FROM Nodes N WHERE L.IP_Address = N.IP_Address) THEN 'Added' ELSE 'Not Added' END AS [Status] FROM (VALUES('1.1.1.1'), ('2.2.2.2'), ('etc'), ('99.99.99.99')) L(IP_address)
When we talk about query execution, we are not talking LEFT / RIGHT / INNER join. We are talking loop, hash and merge. Without writing a wall of text, yes of course the not exists will be resolved by a join. It will not be the same join type that the left join will produce, since the not exists clause will only have to evaluate until one match is found. The Left join has to try to match the entire table, and afterwards filter for the null values in the recordset. For more detail and proof, there are quite a few exellent blog posts out there that really go into detail. In short, no, the not exists will in 99% of the cases be faster. 
I hope you don't write production code with subqueries in the select like that. When I get SQL like that on my desk I usually return it to sender
I dont, this is a one off query to get a single set of data. Not going into any applications.
This is most likely correct unless the optimizer has been improved in sql server to use the same execution plan.
I am happy to see that it solved your problem. :) Here is an explanation for what everything does: * `x` contains the possible peak times (should probably have named it something better than `x`). This includes all start times plus the start of the month. * `LEFT JOIN usage_usage u [...]` finds all concurrent `usage_usage` at every possible peak time during the month. * `GROUP BY x.time` + `count(DISTINCT u.user_id)` counts the number of distinct users at every possible peak time. * `ORDER BY count_overlaps DESC, x.time` + `LIMIT 1` finds the first time with the most distinct users. * `array_agg(DISTINCT u.user_id)` includes who the distinct users were.
I'm not convinced I understand your problem 100% but can you not do : SELECT * FROM your_table WHERE sub_category LIKE 'Multipl%' AND sub_category NOT LIKE '%,%' AND category = 'miscellany'; That's really simplistic but based on your outline should identify all non-multiple records where there's a multiple label - this is assuming that any of your mutliple entires always have a comma in them. (The last line is not necessary btw but just so we're looking only at the miscellany records). If the erroneous records always have the same format (e.g. Multiple Sub Cats: ['Comedy']) then you could do something like the below to fix your problem. UPDATE your_table SET category = CASE WHEN sub_category LIKE '%Comedy%' THEN 'nightlife' WHEN sub_category LIKE '%Visual%' THEN 'enrichment' WHEN sub_category LIKE '%Performance%' THEN 'enrichment' ELSE category END, sub_category = REPLACE(REPLACE(sub_category, 'Multiple Sub Cats: [''', ''), ''']','') WHERE sub_category LIKE 'Multipl%' AND sub_category NOT LIKE '%,%' AND category = 'miscellany'; The above is absolutely dreadful code btw, but it might provide a sort of fix with some work. There are much more elegant ways of doing this (and yes, Regular Expressions would be more elegant in this case - perish the thought) but the above is fairly standard SQL and should work with some tweaking - you'll probably have to change the escaping of the single quote in the REPLACE statements though. Obviously make backup(s) before running something like that and test it on a small subset of your data. I know this isn't a very helpful comment but SQL isn't really good for this sort of problem. I've done the "multiple values in a single column" thing myself - many years ago now and I still find myself shuddering at the mess it created (and the amount of work it generated for myself). I know having another table for subcategories (as /u/doublehyphen says) seems like more work but in the end it ends up being a lot *less* work to do things that way. 
Join the data sets in SQL and put it all into a single tablix. 
well, you can start with naming the DBMS you are administrating. Then people can actually point you to some good material. In the very general, read blog posts, certifications, books. Ps.: please don't take it in a bad way, but to me its a bit frightening to have people with no background being hired into a dba position, at a finiancial institution no less. Good for you to try to learn more, but uhhh i'm getting goosebumps
Thank you! And I appreciate it. Microsoft SQL 2012 is what I am working with. And the individual who hired me knows what my background is and I have support from others who know what they are doing. Most of what I am doing now is report writing and I clear all updates, etc, through those above me. And I have goosebumps too, believe me, it just seemed too good of an opportunity to turn down. 
As long as you have people arround you its all fine. Learn from them, they are your best resource (provided they are good at what they do). One of the first things you should get into, would in my opinion be the execution plans. Once you get into understanding how the db is working, it will lead you into pritty much every aspect of database development and administration. I can recommend sqlservercentral.com as a general source of good material. Also, as mentioned, blogs. As a start, some names worth mentioning would be Adam Machanic, Rob Farley, Grant Fritchey, and many more of course. I'd start off with trying to understand the execution plans. Oh and congratz at landing that job. I like being a dba, so I concur, great opportunity ;)
Look into the SSRS functions called Lookup and MulitLookup. Pretty sure those will do what you need. 
So... I followed your example and went off the deep end. Mind spot checking to see if this query still makes sense to you? My "user_id" field in the usage_usage table is actually a foreign key to my usage_user table. This usage_user table also has a "customer_id" foreign key to my "usage_customer" table. I also updated your query to take into consideration what would happen if a startTime happened before the range in question, yet the expirationTime happened inside the range in question. Further, I have multiple licenses and I only want to find peak concurrent usage on a specific license id at a single time. (Philosophical moment: maybe I should be breaking this database into a database per customer rather than trying to do this all in one giant database? It will make reporting difficult across all my customers, but that is lower priority... sigh.) Here is the result! WITH x AS ( SELECT "usage_usage"."startTime" AS time, "usage_usage"."expirationTime" as endtime, "usage_usage"."license_id" as license, "usage_usage"."user_id" AS user_id FROM usage_usage INNER JOIN "usage_user" ON ( "usage_usage"."user_id" = "usage_user"."id" ) INNER JOIN "usage_customer" ON ( "usage_user"."customer_id" = "usage_customer"."id" ) WHERE "usage_usage"."expirationTime" &gt;= '2013-11-01 00:00:00Z' AND "usage_usage"."startTime" &lt; '2013-11-02 00:00:00Z' AND "usage_usage"."license_id" = '2' AND "usage_customer"."name" = 'Example' UNION SELECT '2013-11-01 00:00:00Z', '2013-11-02 00:00:00Z', '2', '0' ) SELECT x.time, count(DISTINCT u.user_id) AS count_overlaps, array_agg(DISTINCT u.user_id) AS users FROM x LEFT JOIN "usage_usage" u ON x.time BETWEEN u."startTime" AND u."expirationTime" AND u."license_id" = '2' INNER JOIN "usage_user" ON ( "usage_user"."id" = u."user_id" ) INNER JOIN "usage_customer" ON ( "usage_user"."customer_id" = "usage_customer"."id" ) WHERE "usage_customer"."name" = 'Example' GROUP BY x.time ORDER BY count_overlaps DESC, x.time LIMIT 1; Thoughts, recommendations, and criticism greatly appreciated! PS - is there a way to compare X's INNER JOIN to u's? I could do one less INNER JOIN on usage_customer if I could compare usage_user.customer_id against X and u (and just provide an ID in the query) PPS - what does the UNION do and why is it necessary? 
Thank you so much! As I said, definitely scary but people around me are saying I am picking up on it quickly. I will check out the websites/blogs!
Just in case anyone else finds this query and sees value in in it... here is where I ended up (so far). I found that doing everything in one giant query was inefficient because it was doing cross table JOIN's when not necessary. The way I solved this was to create a temporary table first that only contained the data I needed. Then, the problem becomes much simpler. I also didn't see any use in the UNION so I dropped it. CREATE TEMP TABLE my_temp (LIKE "usage_usage") ON COMMIT DROP; INSERT INTO my_temp SELECT "usage_usage".* FROM usage_usage INNER JOIN "usage_user" ON ( "usage_usage"."user_id" = "usage_user"."id" ) INNER JOIN "usage_customer" ON ( "usage_user"."customer_id" = "usage_customer"."id" ) WHERE "usage_usage"."license_id" = '1' AND "usage_customer"."name" = 'Example' AND "expirationTime" &gt;= '2013-11-1 00:00:00Z' AND "startTime" &lt; '2014-1-1 00:00:00Z'; WITH x AS ( SELECT "startTime", "expirationTime" FROM my_temp ) SELECT x."startTime", count(DISTINCT u.user_id) AS count_overlaps, array_agg(DISTINCT u.user_id) FROM x LEFT JOIN my_temp u ON x."startTime" BETWEEN u."startTime" AND u."expirationTime" GROUP BY x."startTime" ORDER BY count_overlaps DESC, x."startTime" LIMIT 1; 
 SELECT [User].userId, [User].firstName, [User].lastName, [User].reportsToUserId, firstName as ManagerFirstName, --Same as [User].firstName lastName as ManagerLastName --Same as [User].lastName FROM [User] WHERE [User].userId=@rootUserId You seem to be selecting firstName and lastName from table User twice. If you wanted userID 3's first and last name, you would have to use a join there.
 SELECT [User].userId, [User].firstName, [User].lastName [User].reportsToUserId, firstName as ManagerFirstName, lastName as ManagerLastName FROM [User] WHERE [User].userId=@rootUserId The first select is selecting the user. I think you want to change the where clause to reportsToUserId instead of userid. So if we just trace the code. the starting point will return the user. then the recursive part will take that and return the people reporting to that set. 
Examine the value of $options before you use it. The most common error seems to be the value for "host". It's almost certainly neither "localhost" nor "127.0.0.1"; for security, databases aren't usually hosted on the web server. You might need to provide more arguments. See [pg_connect() syntax](http://us2.php.net/pg_connect).
Most web hosts run their web servers and their database servers on different machines. But "localhost" will only work if the web server and database server are the *same* machine. If you have some kind of control panel, it will probably tell you what to use. Also, check your email for emails from your hosting provider. Some of them send you an email with connection details when you set up a database. Finally, check your hosting provider's FAQ and help files. 
That's solid thinking but the results that I see after making that change have a problem similar to my original issue. After changing the where clause to use reportsToUserId the initial user that I'm searching on is removed from the results entirely, but any direct reports to that user now appear with their own first name &amp; last name as their manager. Children of those nodes appear properly with the first and last name of the person that they report to as their manager. I'm very new to recursion with cte but the way that I see the progression of this is that the anchor table is created and seeded with data (either with our 1 user or N users who directly report to that user depending on the WHERE clause). After that is seeded the subsequent searches are essentially reaching one step "up" in the recursion/iteration to find first names and last names for the aliased manager first name/last name values. The only data that exists in that seeded table on step 1, is that rows own values - there isn't anything "above" them. I had just thought that in this case that I would get null values in my result set but that's not the case. After a while I tried a number of variants of joins just to see what I would get back (when my brain is tired, I brute force discovery) but I haven't found it yet and still don't understand that first set of records reporting the right manager id, and the wrong manager name. I think I'll sleep on it, put together another scenario for my sandbox in the AM, and return to this after I've read / understood more basics on CTE and what that UNION vs UNION ALL statement is doing in my soup. 
That first select is just to "anchor" the recursion with a single row of data. The results that I actually get back and display are derived from the second select, which does have a join there. This whole recursive cte thing is still partially voodoo sql to me but the results that I get are appearing as I want them to, except for the very first set of data returned - that's the row(s) where my data shows the user as their own manager, all child nodes appear properly like in the screenshot.
The code defining the "anchor" is what is returning the user as their own manager, because that is the anchor row. The code below is not optimal, but it should fix the error. CREATE PROCEDURE [dbo].[GetAllChildrenFromNodeExtended] @rootUserId int = 0 AS WITH RecursiveTree AS( SELECT A.userId, A.firstName, A.lastName, A.reportsToUserId, B.firstName as ManagerFirstName, B.lastName as ManagerLastName FROM [User] AS A INNER JOIN [User] AS B on A.reportsToUserID=B.userID WHERE A.userId=@rootUserId UNION ALL SELECT u.userId, u.firstName, u.lastName, u.reportsToUserId, RecursiveTree.firstName as ManagerFirstName, RecursiveTree.lastName ManagerLastName FROM [User] u INNER JOIN RecursiveTree ON u.reportsToUserId =RecursiveTree.userId ) SELECT * FROM RecursiveTree RETURN 0
Thank you. I wasn't expecting that. Normally you don't even get a thanks. p.s. Don't give up on SQL, the syntax may seem somewhat limited but it can be powerful (and elegant in it's own way) and is definitely worth knowing (career/personal development wise).
Can you post your entire code (either here a pastebin link) instead of the simplified version you included? There might be something elsewhere in the code that you're glossing over.
That would be my first guess, that @variable is being used elsewhere in a calculation that could result in a float. It should be SELECT @variable = [...] so one minor typo in the post could indicate a minor typo somewhere later on that's causing the unexpected output.
Thanks...this was great. This lead me to this article which was exactly what I was looking for http://www.sqlcircuit.com/2012/03/ssrs-2008-r2-lookup-how-to-use-multiple.html
 DECLARE @HowMany INT DECLARE @Results INT SET @HowMany = (SELECT COUNT(*) FROM [My Database] WHERE Machine = 3 AND Code = 5) SET @Results = @HowMany * 10 SELECT @Results as [Results] What does that do?
I did but some of the client's demographic information was presented more than once. It was just easier to make a new sp.
What you're looking for is called a hierarchical query. Oracle has built-in support for this (`START WITH...CONNECT BY...`), but it takes a bit more work in MySQL. Try [this link](http://explainextended.com/2009/03/17/hierarchical-queries-in-mysql/)... doesn't help with the open/close brackets, though.
For what you are doing, loops are a very bad choice. Linked server queries would also not be my goto solution. I would bulk load the entrie thing (in batches, if your transaction log blows up), and then work on the data in a set based aproach on a local staging db. Looping trough quaters is going to be really really slow.
unlimited? do you mean as many categories as necessary, or as many **levels** of subcategory as necessary? in either case you can store them with the adjacency model (this is characterized by the parent_id) but you'll never get them all out if there are truly an unlimited number of levels for up to about a dozen or 15 levels, you can do left joins quite efficiently -- see [Categories and Subcategories](http://sqllessons.com/categories.html) as for the "array", sql doesn't produce arrays, you'll need your application language (php or whatever) to do that
close to what he wants but it should be: SELECT A.userId, A.firstName, A.lastName, A.reportsToUserId, B.firstName as ManagerFirstName, B.lastName as ManagerLastName FROM [User] AS A INNER JOIN [User] AS B on A.reportsToUserID=B.userID WHERE A.reportsToUserID=@rootUserId 
My solution actually worked out pretty well. I'm only pulling something like 20 columns x 5000 rows, 60 times. It takes maybe 4 minutes. It's suboptimal but fine for what I need. Thanks for the input though!
are you sure the sp_XXXX procedures (not files!) are missing? Have you explicitly looked in master? Are you sure you still have permissions to see/execute those procs? Also, see if [this](http://social.msdn.microsoft.com/Forums/sqlserver/en-US/aca7d042-bdd7-4015-bdc9-51b209c92b49/sql-server-2000-sppassword-spaddlogin-spdroplogin-can-not-find?forum=transactsql) helps
Thanks for your response. I looked in master and the procedures are not there. I'm logged as sa but with windows credentials. and that thread looks like exactly what is happening. I was using the sa account remotely to test. (I know this is a no no) and then the password stopped working. any advice?
I think the next thing I'd try would be a repair (one of the options when you run SQL's setup)
Full Code is shown below. There are 7 difference machines (numbered 1-7) so @Machine is set using seven steps in a Scheduled job that runs every minute. Each step in the job sets the Machine number and executes the procedure. I get the same error for both var1 and var2. Instead of being 10 times the count number, it is sometimes 10 times the count number +9 or +8, etc. USE [My Database] GO SET ANSI_NULLS ON GO SET QUOTED_IDENTIFIER ON GO ALTER PROCEDURE [dbo].[usp_MyProcedure] @Machine INT AS BEGIN DECLARE @Start DATETIME DECLARE @var1 INT DECLARE @var2 INT DECLARE @CurrentTimestamp DATETIME SELECT @Start = Start FROM [My Database].dbo.MyTable WHERE Machine = @Machine SET @CurrentTimestamp = (SELECT TOP 1 EventTimestampUTC FROM [My Database].MyData WHERE Machine = @Machine AND Code = 3 ORDER BY RawSeconds DESC) SET @var1 = 10*(SELECT COUNT(*) FROM [My Database].MyData WHERE Machine = @Machine AND Code = 5 AND EventTimestampUTC &gt;= @CurrentTimestamp) SET @var2 = 10*(SELECT COUNT(*) FROM [My Database].MyData WHERE Machine = @Machine AND Code = 5 AND EventTimestampUTC &gt;= @Start) UPDATE [My Database].dbo.CurrentData SET DateTimeStamp = GETDATE(), var1 = @var1, var2 = @var2 WHERE Machine = @Machine END 
[The docs](http://dev.mysql.com/doc/refman/5.7/en/select-optimization.html) have a big chapter on optimizing SELECT statements. Have you read that?
god damn i'm glad i don't have to work with mysql.... sorry, couldn't resist.
&gt; formatting Have you tried using the formatting suggested in the side bar? &gt;reddit's built in code formatting (4 spaces at the start of each line) It should work like &lt;pre&gt;&lt;code&gt;Your formatted text&lt;/code&gt;&lt;/pre&gt; tags and keep your formatting after you edit.
I migrated over to postgres a few months ago and have never looked back.
if i needed free to use or even open source, I would use postgre as well. I'm bound to Mssql in my environment, so I don't really have a lot of options anyway. I don't complain thou, its a really nice DBMS, and its cheaper than the only real contender oracle Mysql ranks a tiny bit higher than access in my book. A really tiny bit
Do these queries and subqueries involve large datasets?
Thank you both very much, this nailed it! 
True, it was a quick hackjob ;) I also went and solved his problem for him with the program in my recent edit and gave some advice if you're interested.
Thankyou!
Thanks, a lot to think about for me since I'm a rookie. I got #2014 - Commands out of sync; you can't run this command now when running EXPLAIN on the most time-consuming query and I'm too embarrassed to show the messy PHP code. I can explain quickly though. I have one simple query that is selecting horse and track information for the current race day from one table and a subquery that is comparing to history and checking live if horses are going to race with new equipment (primarily foreshoes, hindshoes and blinkers), which the trainer has to report before the race, and calculate the results from previous starts for each horse (race result, prize money and so on) with the current equipment. It's basically some simple COUNT and SUM calculations but if you go through 100 horses for a race day and they each have 100 starts in history it seems to get a bit time-consuming to recalculate everything unless the response is cached. Indexing the most used columns that I'm using for WHERE, ORDER BY, and GROUP BY clauses might be what I'm looking for though. I have to read up more on that and that "memory table" you're mentioning for these two most time-consuming queries.
I'm not sure what counts as large datasets but no, they're not very large (at least no so far).
a tourist stopped a cop in new york and asked "can you tell me how to get to carnegie hall?" "sure," said the cop, "practice, practice, practice" :)
It sounds like you're already covering the basics which is a good start. Learn the basic things like how to do a select statement, how to join, where clauses, possibly grouping, etc. Remember that while it's a timesaver and you will eventually, **you do not need to memorize the syntax for everything right now**. It can be overwhelming if you try to do that, but just like everything related to programming, the correct syntax is a 5 second google search away. Hell I've been a DBA for almost 7 years and I'm still not 100% sure on the syntax for a Merge statement. That being said, the best way to learn how to do things is to have a project to do and figure out how to do it step by step. Ask what kind of things you will be doing later on, try to translate it to the AdventureWorks DB, and then work through it. If you have to write a query that brings back a list of employees with their managers and departments with salary averages by department (random example), then tackle that one step at a time and search for answers along the way (i.e. how to group results to get an average). Like most things, textbooks/online classes can give you the basics, but actually doing it yourself will help you 10x as much. Also, I would highly recommend you get involved in a few community sites/daily email newsletters. I like SQLServerCentral and MSSQLTips for that and read them every day. Sometimes there is great stuff like a performance increase by using the new columnstore indexes for data warehouse queries, a quick trick to save you a ton of time, a free tool to make your job easier, or sometimes it's just going to be a review for what you already know. SQLServerCentral also has a great Forum area where I learned a lot. Just browsing the various subforums and topics that looked interesting helped a ton because they're filled with answers from extremely smart people (quite a few SQL Server MVPs post there regularly). Even if you don't need the answer to a question right now, you might in 6 months and think back to that thread and what the correct (or best) solution was. SSC also has a smaller sub-site called Ask SSC (http://ask.sqlservercentral.com/) where you can post a quick, short question and usually get a fast reply. If you're having trouble with something and can't find the answer by searching, try this. You can also just browse through here again to see what kind of questions people typically have and what kind of answers they usually get. Again, you won't use 99% of it right now, but you will in the future. That's what helped me learn. When I started as a SQL DBA, I had zero experience (other than being halfway through a SQL class in college which ended up being nearly useless). I read forums during free time, saved useful queries in a text file (before Evernote was around, which would've made things easier), messed around on my own to try new things, and just tried to work through problems on my own by googling or trial and error. edit: Sorry - just realized this was for MySQL and not MSSQL - the sites I listed are obviously for MS SQL Server. Similar advice though - find a good MySQL community with a newsletter/forum and start reading it.
Most learn by doing. Try taking what you learn in class and applying it in some way to your work. You'll likely run into some real problems that will require troubleshooting or rethinking, and a bit of research - from these you'll learn the most.
:)
Awesome! I'll definitely pick that up after work today! Thank you! 
Thanks for the response. I really appreciate it. I'll definitely look up the stuff you said because I'm pretty sure I'll be working with MS SQL as well. They named quite a few programs and I'm sure that was one of them. Thanks again! Edit: also I'm glad to know I'm not the only person who's gone in knowing essentially nothing. Definitely makes me feel confident I can succeed. :) 
That's actually a great idea. Like after I'm done with my work coming up with pseudo-assignments for myself like querying things in the database for example? 
I don't. I'll check both of those out though. Thank you!
I signed up about 6 or 7 months ago for Lynda.com and I think I'll continue using that site for as long as I work in a technical industry. Trust me when I say it's worth the money. I'm not trying to advertise for the site, but I will tell you how I've used the site. Lynda.com is a tutorial video website. It's higher quality than anything you'll find on YouTube. I have watched hours upon hours upon hours of videos on all kinds of topics on that site. The list of softwares covered is really long. In particular about sql there's a couple of videos by Simon Allard that are a great introduction to database concepts. I'm talking 5+ hours of coherent easy to digest videos about the subject matter you want. Lynda.com is like 25 dollars a month. You could probably pay for a month and fill up on all the sql stuff you can gulp down and cancel your membership after one month. I promise it's a good idea and worth the money. Good luck. 
Sweet I'll look into that as well. Thanks a lot! 
About "select \*", I always wondered if that was an "every case" sort of thing. Say if there are 20 columns and I want 18 of them, should I really write out all 18 column names, or would "*" not make much of a difference? I also wonder what makes MySQL a second-tier engine compared to PostgreSQL. I am by no means an SQL expert, but the limited reading I've done, I see articles saying [MySQL is better](https://www.udemy.com/blog/mysql-vs-postgresql/) just as often as the other way around.
I was once in the same situation as you - had almost no SQL experience and was thrown into a project desigining a pretty complex DB. My biggest mistake was to follow my bosses instructions to "get it working and worry about performance later", which caused huge headaches later on. Bring the scale of your data to production levels ASAP, and just generate it yourself if you have to! That way you'll be thinking about indexes, denormalizing, e.t.c. from the start.
As a broad generalization, MySQL began as a SQL syntax interface to flat files, and the community was much more concerned with keeping it simple and fast than with mathematical correctness. For example, you could declare a column as a Date and then insert February 31 as a date and it would permit it. There's a long list of features it didn't support because the community didn't see the need for them. The join processing performance was so bad that a generation of programmers stopped using them, while the rest of the working database world shook it's head and went back to work. If you are making a CMS with a relatively small group of simple entities, that's the job it was designed for. But don't look at its query plans and think that's what a RDBMS is. You can work around the shortcomings by using different storage engines, but then you need to evaluate compatibility, since many apps assume MySQL=default engine. In any case, you as the developer are responsible for more of the complete solution. The entire reason that RDBMS software was created was that you should be able to query a set of data by stating _what_ you wanted, not _how_ the engine was to retrieve it. XML databases and other NoSQL data stores are very reminiscent of the Hierarchical databases that proceeded Relational databases. MySQL doesn't care about being a good Relational system. This is a pretty good summary of the issues- http://nigel.mcnie.name/blog/how-bad-is-mysql I think the world needs a variety of data storage options - http://martinfowler.com/bliki/PolyglotPersistence.html My data toolbox is MSSQL, PostgreSQL, SQLite and Hadoop. MySQL fits between PostgreSQL and SQLite, but does neither job well. 
regarding 'select *', the biggest reason is to prevent problems if the schema changes. If you really need all 18, then it doesn't make much difference. The point is that fewer columns means you are more likely to have a usable index, less bytes in the data and plan cache or crossing the network. Less is better as a general rule. 
Since you clearly know a lot more than me about such things, and have strong opinions. Do you have any opinion of [OrientDB](http://www.orientechnologies.com/orientdb/)? I was reading a bit about it and liked what I read and feel the way it stores and accesses data works well with how my brain understands data structures. Though it seems to new still for me to use it, and I really like using PHP's PDO, which doesn't have OrientDB libraries yet.
Good list, but I disagree with a couple of your items. (4) Some people like ERD tools some people do not, so i recommend you try one out and see if you like it. They are in no way necessary. (11) I personally prefer parametrized SQL statements for this. Prepared statements are not intended to be used against SQL injection, they just do that as a bonus, instead they are for reusing the query plan so a query does not have to be parsed and planned each time it is run.
Strongly Opinionated is my favorite euphemism. I had not heard of it. The marketing literature sounds promising. If a DocumentDB is the right model for whatever you are working on I would compare it to the other products in that class. I think the risks are: that the product dies or goes commercial-only; that it's hard to find other people able to work in it; that it's hard to use it from non-java languages. There's apparently a java ORM baked into the product. https://github.com/orientechnologies/orientdb/wiki/Object-Database Compare - https://github.com/orientechnologies/orientdb/wiki/Programming-Language-Bindings If how it works feels comfortable to you as a developer, check out the administration aspect too. What kind of tools are there? Can you monitor integrity and performance adequately? What's the disaster recovery scenario? What about backups? Does it prefer to scale up or out? It's got to be better than MongoDB in any case.
It seems you want to do a DIVIDE in relational algebra; check https://www.simple-talk.com/sql/t-sql-programming/divided-we-stand-the-sql-of-relational-division/ or google relational algebra division; they have a 'standard' translation into SQL It is still a hairy query, but 'partition' queries tend to be slow, so this may be faster
You could achieve this with a view, meaning the duplicate data isn't being stored, but simply synthesised on demand. Access the view instead of the table and you have your new column... Been a while since I used MSSQL, but something like create view dbo.vw_Table as select col1, col2, col3, CASE WHEN col1 &gt; col2 and col1 &gt; col3 THEN 'column_one' WHEN col2 &gt; col1 and col2 &gt; col3 THEN 'column_two' WHEN col3 &gt; col1 and col3 &gt; col2 THEN 'column_three' ELSE NULL END as max_col from table But its worth noting, if you need to do this (especially retrieving column names as part of your data), your database design is almost certainly *wrong*. The technique above becomes even uglier when you cater for 20 columns, and even then would have to be modified every time you add a new column You should probably consider splitting the 20 columns into separate rows, with an entity id, field id and actual value. Then to get the maximum column (whether theres 20 or 200 of them), you just use MAX(value) from field_rows where entity_id = xxx
&gt; your database design is almost certainly *wrong* that was my first reaction as well 
Create a @Table variable to hold the max values and column names. Then select from that @Table in your insert statement. I'd suggest a proc that returns the max column instead of inserting into a table, unless the result is heavily queried. Here's some partial sample code typed with my thumbs on a phone.... DECLARE @MaxValues AS TABLE ( ColMax AS DECIMAL(12,4) -- or whatever , ColName AS VARCHAR(255) ) INSERT @MaxValues SELECT MAX(Col1) as ColMax, 'Col1' as ColName , MAX(Col2) as ColMax, 'Col2' as ColName -- etc FROM Table1 -- and perhaps the source table really ought to be organized this way in the first place -- use a select akin to the following in an insert, or better yet, a proc or view SELECT ColName -- will return multiple rows in the event of ties FROM @MaxValues WHERE ColMax = (SELECT MAX(ColMax) FROM @MaxValues) 
Since the date can be null you don't want it in your primary key (edit: rather, it can't be in your pk). You could keep your composite primary key and add a separate unique filtered index (SQL Server 2008+). CREATE UNIQUE NONCLUSTERED INDEX UX_t1 ON t1 ( col1, col2, col3, DepositDate ) WHERE DepositDate IS NOT NULL So the unique constraint will be enforced where DepositDate is not null 
Wouldn't this still throw an error if there is already a row of x,y,z,9/1/2010 and I try to enter a row of x,y,z,9/7/2010 ? I don't believe I spelled that out correctly in my OP in that there can be two rows with the same col1, col2, col3 but with different DepositDates.
so is the query taking very long to run? You are right there there isn't much room for optimization if it is indeed running in 7ms. Your query selects from a subquery. You could indeed remove the subquery and select those same columns off their respective tables, but it doesn't seem that the subquery is hurting you performance-wise. You could try removing it as an exercise, personally I probably wouldn't use the subquery. the formatting looks fine to me. Did you do the formatting by hand or using a tool (e.g. [poor man's sql formatter](http://architectshack.com/PoorMansTSqlFormatter.ashx)? I find these can be helpful if you are cleaning up formatting on existing queries, although you may want the practice. If you find a query that actually needs to have its performance improved (it takes several minutes, etc), do some research on using SQL Server's execution plan to analyze it.
I use [this](http://www.dpriver.com/pp/sqlformat.htm) to format SQL - here's the (black &amp; white) output :- DECLARE @a DATETIME DECLARE @b DATETIME SET @a = '2000-01-01' SET @b = Getutcdate() SELECT lotid, opetypeid, operationdate, lotname, defid, quantity, locid, Datediff (minute, Dateadd(hh, +5, operationdate), @b) AS MinutesBetween FROM (SELECT LotOp.*, L.defid, L.quantity, Loc.locid FROM sitmes.dbo.mmwlots L LEFT JOIN sitmes.dbo.mmwlotoperations LotOp ON ( L.lotname = LotOp.lotname ) LEFT JOIN sitmes.dbo.mmvlocations Loc ON ( L.locationpath = Loc.locpath ) WHERE LotOp.operationdate &gt;= @a AND LotOp.operationdate &lt;= @b AND L.quantity &gt;= 0) AS Data 
I don't think it would since those are unique combinations of the 4 fields covered in the index. That's what I figured based on your explanation.
You would need to look at the execution plan of the query, to see what areas could actually be improved. Everything else is just speculation. I would certainly remove the subquery, although whether that materially affects the query after the optimiser has run is unclear without the execution plan. Forget WITH(NOLOCK), it has its uses but has side effects too, I'm certain your tutor would not be expecting you to add this. Also I wouldn't worry too much about how the query looks readability wise. You can focus on readability, or focus on performance, but in many cases the most performant SQL is not the prettiest to look at.
Yup. That looks more concise to me. Looking at this again, you could also use a BETWEEN operator for the LotOp.OperationDate condition to make it more concise.
If col1, col2 and col3 are your current primary key wouldn't you *want* it to throw an error? How do you intend to identify an individual row in your table? I think you're right to be cautious about what you've done. It may well work but I agree it doesn't feel right. I'm a bit dubious about using dates as keys, but that's mainly a result of using SQL Server too long with only DATETIME to work with (pre 2008). I normally like natural keys (and don't object to composite keys) but this seems like an instance where you want to use something else. Use unique indexes to enforce your business rule (as Nexic suggests) and set the primary key to either some sort of IDENTITY value (or similar artificial/meaningless value) or issue a transaction number. 
An individual row could be identified by col1, col2, col3, and DepositDate. To use real examples, we could have Waiter, 1/7/2014, 999-99-9999, 1/9/2014 And another row of Waiter, 1/7/2014, 999-99-9999, null I think the end result will be a primary key that is separate from the 4 fields, such as an auto increment field, while maintaining the uniqueness of those 4 with an index.
to me everything above 300 is a high time to do some maintainance. 840, you should definitly rebuild that thing.
Sweet I'll definitely read up more on both of those things. Thank you! 
I'm glad you were able to pull it off. This thread has definitely made me feel even more confident that I can learn it quickly and excel in it. Thanks for the tip!
I would check to see if moving the LotOp WHERE clause into the JOIN ON clause speeds things up. Sometimes this can help, depending on the size of the data, existing indexes, etc. DECLARE @a DATETIME DECLARE @b DATETIME SET @a = '2000-01-01' SET @b = getutcdate() SELECT LotOp.LotID ,LotOp.OpeTypeID ,LotOp.OperationDate ,LotOp.LotName ,L.DefID ,L.Quantity ,Loc.LocID ,DateDiff(minute, DateAdd(hh, + 5, LotOp.OperationDate), @b) AS MinutesBetween FROM sitmes.dbo.MMwLots L LEFT JOIN sitmes.dbo.MMwLotOperations LotOp ON (L.LotName = LotOp.LotName) AND LotOp.OperationDate BETWEEN @a AND @b LEFT JOIN sitmes.dbo.MMvLocations Loc ON (L.LocationPath = Loc.LocPath) WHERE L.Quantity &gt;= 0
uh thats an enourmous transaction log, the largest database I've ever worked on was a 7tb beast that was doing 5-10k transactions per second and the log never got above 20 gb. how often are you doing the tranlog backups?
Transaction count has no bearing on the log size what so ever. One bulk load query and puff. 50GB initial size, and autogrow to 200GB is huge thou. I have no idea why your log doesn't get emtied out by a backup, but do you do small time interval log backups at all ? Usually those are taken every 5-30 minutes, and I have trouble thinking of a usecase that generates 50 gigabytes of write operations inside of 15 minutes, if I disregard bulk operations, which should be handled differently than normal DB usage anyway. 
transaction duration has no baring either. I can have a 2TB logical read query running for an hour, and it would not phase the transaction log one bit. The writes do thou. And my point is, that your nightly differential / full backup should NOT be the one that takes care of your transaction log. For that you have the log backups, and those should run on a short time interval. They will release all VLFs that are not activly written to, for overwrite. Thats how you keep the size of the translog under control. Also the translog backups are your, if shit hits the fan, point in time recovery... As to why BruisedGhost mentioned the transaction count. A productive 7TB database is HUGE. Having 5-10 thousand transactions running per secong is HUGE. really fucking HUGE. Thats why that should be an indicator to you, if such a monster runs at a 20gb log size (which is also huge), you should be WELL below that. 
What recovery mode does your DB run in ? I'm venturing to say its not 'simple' ? Please understand i'm not out for breaking your balls, but something is seriously wrong, and you need to know about it, before it hits you in the face
While you are right that your top level issue, is the blocking of the VLF's for rewrite, the backup strategy is what gets you into hot water to begin with. If you have a full recovery mode database, and don't run transactional backups, you can just as well set the DB to simple mode. You do gain nothing from having all write operations logged, if you do not write out that log. So in essence, what you currently have, is a huge log file, where every single (writing) transaction gets written to, but you do not "export" that data. So every night, you run a full / differential backup, that will truncate the log file, since the logged information is no longer valid (the log references the last full / differential backup, at time of the transaction), so this now useless information gets truncated. This is the exact same that you would have if you would run the DB in simple mode. My advice, get hold of whom ever prevents you to change the backup strategy, find a way of telling him he is doing a very very bad job, and fix the backup. If you still have problems, then you invest time in that still existing problem. Until then, if you can't fix the backup, set the DB to simple recovery mode, you do not loose anything, from your current situation. /Edit : Due to the nature of them, snapshot backups, should be very fast. They ain't backups to begin with, but they should be fast. I can't see a file based or DB based Snapshot have any impact on your transaction log problem thou
since its a question of "why do I have a noosebleed" - stop running into a wall - no fix my noosebleed. No. I can not help you with that. btw, your storage provide proberbly would ask you if you were seriouse if you told them you don't do log backups. Thats what the god damn ldf is for, otherwise we could use Mysql or Access ffs
A snapshot is overhead in futur writes. It saves the difference of current data, to what gets changed. Basiclly your snapshot is a "The file looked like this 10 minutes ago". It saves the difference to that point in time, it does not clone the file. If you talk about someting else, you have the wrong definition of a snapshot ! Creating a Snapshot is a thing of milliseconds. If your Frontend can't handle half a second of delay on DB comunication, it sucks. Sucks hard. Still, frontend suckage is NOT a reason to not do proper backups on the backend !
This reeks of homework.
Did I miss, or misread, something, or are you saying you have 200+ databases in full recovery mode with no transaction log backups, and none of them have log file size issues?
I would do it as two queries. Select 'UNION ALL Select username, email From ' + dbcolumn + '..users ' mynewsql From projects Then copy the output back into your query tool and remove the 'UNION ALL' from the very beginning. Now you have the single query to get all the info. I think it's possible to do this as a single query without writing a StoredProc, but if this is a one-off query, mine is the easiest way.
Full (database) backups do not truncate logs.
Is your reporting tools allow you to read data from other database different from the one you are currently connected ?
If you have a non-changing set of databases/tables you could always spell it out explicitly: SELECT name, email FROM bob.dbo.bob_db UNION SELECT name, email FROM maxine.dbo.maxine_db etc Otherwise this may help get you started with Dynamic SQL -&gt; http://www.sqlteam.com/article/introduction-to-dynamic-sql-part-1
What reporting tool? With Actuate (BIRT) most of their products you could still use the two queries. The second DataSet's query would be dynamically constructed using the results from the first. If the Products table doesn't change very often, then you could still generate the UNION ALL query and just manually maintain it. Otherwise, I think you have to write a Stored Procedure.
You very well might have... it was late and I didn't read everything as thoroughly as I could have. I have some resourced bookmarked at work. When I there later today I'll see if they shed any relevant light.
This is more SQL Server centered but you could check out SQL Server Central's Stairways. http://www.sqlservercentral.com/stairway/
you can turn logical reads into megabytes by dividing the number by 128, ie: 127,000 logical reads = 992.1875 megabytes physical = read from disk logical = usually read from cache
also www.brentozar.com great tuesday lunch and learn sessions, lots of them are on youtube. very charismatic bunch
Nothing that fancy, it is just a part of a web based application (HP ALM) that lets you specify an arbitrary SQL statement to build a report out of.
No. It basically just a web form I can give a single query to.
^ This You dont need to resort to dynamic sql. You can join across databases without problem, just make sure your login has read access to them all.
correct, it has more to do with writes. I would increase the frequency of your tran log backups and it should shrink down to a manageable size. 
That sounds horrendously insecure. If you let the user design the SQL, even behind an area that requires a login, I can guarantee you it will get hacked. You're much better off to have a multiple-choice mechanism (and this can be multi-part if you so desire) that lets the user select one of a number of predefined queries, where any input from the user (such as a text field) that is not a multiple-choice option gets sanitized and then passed as a parameter to a parameterized query. Otherwise you're just asking for trouble... and you *will* get hacked.
It is part of a widely used commercial product, and allows just read only queries. The vendor is richer than we are, and I am pretty sure that would change quickly if this was a security issue. Also, I'm the admin of the product. With how I have the role based permissions set up, I'm the only one who can create reports like this. Users only run the SQL that I choose for them. But that is still an important concern, I appreciate you pointing it out.
I took [THLycanthrope's response](http://www.reddit.com/r/SQL/comments/1uod7h/mssql_trying_to_use_a_subquery_to_select_from/cek5lw0) and tried to convert it to a script that will only need to be ran once, with no human interaction. I'm not sure if your report tool would allow this. It builds a select statement from the tables and stores it in @result, then runs @result using sp_executesql. USE [database] DECLARE @result nvarchar(4000) set @result = ( SELECT CASE WHEN ROWNUM &lt;&gt; MAX_CNT THEN SELECT_TABLE+' UNION ALL ' ELSE SELECT_TABLE END FROM ( SELECT 'SELECT 1 FROM '+project_db+'.table_schema.users' AS SELECT_TABLE , ROW_NUMBER() OVER (ORDER BY TABLE_NAME) AS ROWNUM FROM projects ) AS A LEFT JOIN ( SELECT COUNT(*) AS MAX_CNT FROM projects ) AS B ON 1 = 1 FOR XML PATH ('')); exec sp_executesql @result;
There's a number of things you could try. 1. Use dynamic SQL and cursor over the projects.project_db field to create and execute a single SQL statement. 2. Use a SELECT to create a batch of select statements that you copy and paste into the query window: SELECT 'SELECT ''' + project_db + ''', u.name, u.email FROM ' + project_db + '.dbo.users u' FROM projects 3. Use the undocumented sproc sp_msforeachdb to execute 'SELECT u.name, u.email FROM [?].dbo.users u'
Oh yes; little Bobby Tables, we call him.
This is an interesting that avoids the need for a cursor, which wouldn't scale very well. I need to work through the logic in my head some more.
OK. Well, like I said, it's unsupported so it's not guaranteed to be around after patches, upgrades, etc. But it will do what you want.
I have all the db's I want in a table, so I am thinking I can pull them out of that. But first I have to figure out if my reporting module even lets me run stored procedures. I'm pretty sure I wouldn't let it if I designed it.
I have considered a few tricks like that. The problem is similar to the ones with using an undocumented stored procedure. This is a commercial app that could tweak their schema with the next patch. I'm hoping for a solution that works as is. There reporting mechanism isn't even really meant to query across databases, that's already a little abusive.
Very interesting. I did some cursory tests and filtering at the JOIN seems to only help if the filter significantly reduces the joined result set relative to the first table. Since that's a LEFT JOIN, reducing the joined result set ends up increasing the final result set... which I didn't expect either!
I think the developer would have to resort to Dynamic SQL if you want to dynamically join to N number of databases which can vary over time.
Cursors are the worst, loops are mostly bad in general if they can otherwise be avoided. In this situation, if you have a lookup table (which thankfully MS SQL does) that lists the names of the databases, then you can absolutely do this in a set-based fashion.
I hear this as conventional wisdom a lot, that cursors are to be avoided. And the rationale I see given is that people use them to loop through individual rows of a table to update individual rows, for example, which defeats the purpose of using a set-based system in the first place. I don't have any issue with that, and I agree completely. But it seems like I've encountered several situations where cursors have been useful, and it always seemed to be in administrative situations where you had to apply statements to different dbs or tables. For example, for each of a dozen databases in a list, query a certain table (which might have a different name in each db), and then update a certain pattern in a particular field (with the search and replace patterns varying per database). I found it easier to cursor through a lookup table to get the parameters for each database, use a single dynamic SQL statement with the parameters, and perform the operation. I don't think this is a particularly egregious use of a cursor, but the received wisdom makes me feel like a lazy DBA of low moral character. I've always heard that there's nothing you can do with a cursor that you can't do in a set-based manner, but I must admit there have been several situations where I was perplexed as to how I would do so. I'm very interested in doing things the "right" way, though, so this example gives me a framework to build on.
&gt; but the received wisdom makes me feel like a lazy DBA of low moral character. Follow your dreams! &gt; there's nothing you can do with a cursor that you can't do in a set-based manner I see red flags when I read words like always and never. The best answer a DBA ever had was "It depends on...". That said, users often mean 99% when they say "It always happens this way...". You can most often, usually, find a way to do a thing set based. To the point about cycling through a lookup table and doing stuff and things for each entry... again, I feel that it depends. Obviously for some things the tables and changes are small, but the number of DBs is large. For this situation, your number of loops would be large but the size of each loop would be relatively small. This would be the best candidate to go set-based with. On the other hand, if you were in a situation where each loop took on order of 30 minutes, and you're doing 48 databases every day, well now you're running into timey-wimey issues and should probably break stuff up into two discrete jobs. I think the crucial piece to the puzzle is the part where you said you have a lookup table. IF this is the case then, it may be possible to more easily craft some dynamic SQL to run this all at once. Then again, as my boss always says, is the juice worth the squeeze? Is it worth the time it takes to create an atomic bologna slicer? Are we optimizing something that completes in 4ms? Sometimes that's the answer... who cares if it's inefficient when it runs once per day for less than it takes to blink. Sorry for the wall of text. tl;dr; it depends 
For books, the first thing to do is grab a copy of C. J. Date's *An Introduction to Database Systems*. You can pick up a used copy of an older edition for next to nothing.
Is the juice worth the squeeze, I love it. The table update may have been a bad example. I was trying to think of other workflows where I've used this "pattern" and a lot of times it was to do with running multiple DDL / DML statements in a row, on a number of databases, each set having different parameters, and maybe I have to collect some output into a single location. And so a little more complicated than having a SELECT / UNION ALL. The justification I made to myself was that I wasn't using a cursor to control the actual data modification - that is, I wasn't stepping through a table and updating one row at a time - but that I was using it to control the flow of the script itself. In effect, the statements within the cursor BEGIN block become a function that I'm calling with a different set of parameters, one for each row of the driving table. Justification is great, isn't it? I think so. But I've found that, often enough, it's been worth my while to set up a driving table with my parameters, then cursor through it and set up my various dynamic SQL statements and chug through it. It seems cleaner to me to do it this way. This also allows me to monitor the progress of the operation by throwing in various high-tech statements like "PRINT 'You are here: @DBName, GETDATE()' But ultimately, I think you and I agree. It depends. Cursors can be useful in the right time and place. The fact that they're easily and commonly abused doesn't change that. But I definitely see opportunities for me to tighten up my set-based chops here.
I am on my tablet so I can't type out Ana example but you need to use window functions. I believe SQL 2012 has the lead and lag functions. So create a function that takes the difference between the value and the previous day and return 1 for a positive or no difference and a 0 for a negative difference, then create another window function with row number and then filter if the row number is greater than 5.
Thanks for the recommendation, I'll check it out
Thanks, I'll check it out.
Alright so I am in front of a computer now so I can answer you more thoroughly. Essentially, you have 2 different problems. 1. Flag dates whose value is larger than the previous date. 2. Find consecutive dates from above. This is the so-called "island problem". Like I mentioned before, SQL Server 2012 makes problem 1 very easy with the new LAG function. Let's say that the table we're working with is called `t1`. WITH c1 AS ( SELECT t1.date ,t1.value ,LAG(t1.value, 1, NULL) OVER ( ORDER BY t1.date ) AS prev_value FROM t1 ) SELECT * FROM c1 WHERE c1.value &gt;= c1.prev_value The above would return dates where the previous day was smaller than the current date. That solves problem 1. Problem 2 is a bit more difficult. From the above subset of dates, we want to find consecutive dates. The trick here is to know that a set of consecutive numbers MINUS another set of consecutive numbers will always be constant. This is one solution to the so-called "island problem". For instance, let's say I have an ordered set = {1, 2, 3, 5, 6, 9}. I assign each number its "rank" or "row number" starting from 1. Then I create another row and find the difference between the 2. | Set | Row | Diff | |-----+-----+------| | 1 | 1 | 0 | | 2 | 2 | 0 | | 3 | 3 | 0 | | 5 | 4 | 1 | | 6 | 5 | 1 | | 9 | 6 | 3 | Now you can group your set according to the difference. For your data, we are going to use the `ROW_NUMBER()` function and then take the difference between the date and the `ROW_NUMBER()`. That will create a constant for us to group by. I'm going to build upon the above query like this. WITH c1 AS ( SELECT t1.date ,t1.value ,LAG(t1.value, 1, NULL) OVER ( ORDER BY t1.date ) AS prev_value FROM t1 ) SELECT * ,DATEADD(d, - 1 * ROW_NUMBER() OVER ( ORDER BY c1.date ), c1.date) AS islands FROM c1 WHERE c1.value &gt;= c1.prev_value This will return another column called `islands` that groups consecutive dates. The actual value in this field is meaningless, but it gives us a convenient way to group. It achieves this by taking the difference between the row number and the date. Then I'm going to apply another `ROW_NUMBER()` to number each day. That way, we can filter for days that are more or equal to 5. WITH c1 AS ( SELECT t1.date ,t1.value ,LAG(t1.value, 1, NULL) OVER ( ORDER BY t1.date ) AS prev_value FROM t1 ) ,c2 AS ( SELECT * ,DATEADD(d, - 1 * ROW_NUMBER() OVER ( ORDER BY c1.date ), c1.date) AS islands FROM c1 WHERE c1.value &gt;= c1.prev_value ) SELECT * ,ROW_NUMBER() OVER ( PARTITION BY islands ORDER BY c2.date ) AS num FROM c2 The num column here ranks consecutive days in each group. The grouping is done by the `PARTITION BY` clause in the window function (`OVER()`). But remember, the first day here is actually the second day it increased. Because the first day is filtered out because we filtered out days where the previous day's value was less than the current day. So that means, we're looking for values of `num` that is &gt;= 4, not 5. WITH c1 AS ( SELECT t1.date ,t1.value ,LAG(t1.value, 1, NULL) OVER ( ORDER BY t1.date ) AS prev_value FROM t1 ) ,c2 AS ( SELECT * ,DATEADD(d, - 1 * ROW_NUMBER() OVER ( ORDER BY c1.date ), c1.date) AS islands FROM c1 WHERE c1.value &gt;= c1.prev_value ) ,c3 AS ( SELECT * ,ROW_NUMBER() OVER ( PARTITION BY islands ORDER BY c2.date ) AS num FROM c2 ) SELECT c3.date FROM c3 WHERE num &gt;= 4 And that's your query. I know it seems complicated, but that's because I broke it down step by step.
I don't really understand the specs. Can you describe what the users fill out and on what form and why then describe how the data should be stored and why and then what data you want to be able to query/analyze, on what form and why. Please be really detailed about how data should be grouped and why. Would also be great if you could put it on sql fiddle : http://sqlfiddle.com/#!6 and put the table structure with some test data in it and your extraction query on the right (with what ever is the best try you got so far). That way I can help you must faster.
Should have posted that as a reply, oh well it's in here now either way.
Are you trying to generate the query dynamically? If you know the column names in advance then you can just do : SELECT ID, Name, Address, OtherColumn1, OtherColumn2 FROM Details ORDER BY ID, Name (Obviously OtherColumn1, OtherColumn2 should be replaced by whatever the column is actually called) If you're trying to do this dynamically then it's a bit more complicated. 
You need to go read a few books. 
See if there's any rogue data which would cause the constraint to fail.. select * from OrderAddresses oa left join orders o on (o.id = oa.order_id) where o.id IS NULL LIMIT 10 Check both tables are InnoDB too
Check that the type and length of both columns is the same: int is not the same as bigint, for example.
also, if you want to do this dynamically, for no good reason (at least I didn't see one), you should not do it dynamically Your database design should not be vague enough, to need dynamic sql to get names and adresses of what ever person
$ perror 150 MySQL error code 150: Foreign key constraint is incorrectly formed 
As the error message says you cannot have the subquery in your outer join clause so the solution is to rewrite the query. Usually for a request like this it is helpful to provide your: 1. Oracle version (e.g. 11.2.0.3) 2. Sample data in the form of CREATE TABLE / INSERT statements. 3. Explanation of the business logic. 4. Expected output based on the sample data set. I don't have access to an Oracle DB at the moment so I can't take a stab at the fix. 
Ah. This was it. One of the values where int(10) signed, and the other was int(11) unsigned. Thank you
Thank you, no records returned though. My data is clean :)
I'm a little old and jaded now, but IMO the "best" code to solve this is probably the simplest and most explicit code: if object_id('tempdb..#table') is not null drop table #table; create table #table ( [Date] date ,value int ); insert #table values ('1/1/2014', 1) ,('1/2/2014', 2) ,('1/3/2014', 3) ,('1/4/2014', 4) ,('1/5/2014', 3) ,('1/6/2014', 2) ,('1/7/2014', 1) ,('1/8/2014', 2) ,('1/9/2014', 3) ,('1/10/2014', 4) ,('1/11/2014', 5) ,('1/12/2014', 5) ,('1/13/2014', 7) ,('1/14/2014', 6) ,('1/15/2014', 7); select x1.* from #table x1 join #table x2 on dateadd(day, -1, x1.[Date]) = x2.[Date] join #table x3 on dateadd(day, -2, x1.[Date]) = x3.[Date] join #table x4 on dateadd(day, -3, x1.[Date]) = x4.[Date] join #table x5 on dateadd(day, -4, x1.[Date]) = x5.[Date] where x1.Value &gt;= x2.Value and x2.Value &gt;= x3.Value and x3.Value &gt;= x4.Value and x4.Value &gt;= x5.Value;
The answer to this question depends on what platform you're using. Please let us know if you're using MySQL, Oracle, SQL Server, etc...
perl: http://scriptingmysql.wordpress.com/2013/01/10/retrieving-list-of-mysql-users-and-grants-with-perl/
I'm not a heavy MySQL user but I know you can use something like SELECT DISTINCT user FROM mysql.user To get a list of users. And then you can view individual permissions by using SHOW GRANTS. So you could presumably stitch the two together: SELECT CONCAT('SHOW GRANTS FOR ',user,'@\'',host,'\';') FROM ( SELECT DISTINCT user,host FROM mysql.user WHERE user &lt;&gt;'' AND host NOT IN ('','%','::1') ) as F1 And then run the resulting script. Or something like that. But tbh, I'd probably not do in SQL if I was using MySQL as I don't know the schema tables well enough. 
i'm using MS sql 2008.
Yes. It definitely will. For one thing, the query planner will no longer be able to use WHERE from the outer queries to optimize the inner ones. Similarly, it won't be able to use indices on the tables/views from the subqueries in the outer queries. Using CTEs instead of subqueries should improve readability without impacting performance. Oh, and FWIW, the RDBMS definitely matters. In MySQL, for example, it can often be faster to use temp tables, but that's because MySQL's query planner is none too bright.
I just had a project at work to go through a large stored procedure that used MANY temp tables and replace them where it made sense. The main issue was the person who created this procedure never dropped their temp tables so it was actually causing the tempdb to run out of space. It was pretty crazy. Gotta watch out for those temp tables.
1. Oracle version: 11.2.0.2.0 2. I do not use CREATE/INSERT statements 3. I'm trying get people who are listed on the table SGRSPRT for the term 201410. I want their latest standing listed on SHRTTRM. The reason I am doing a JOIN is because some people do not have a standing but I still want to pull them. The reason I am using a MAX is because a people can have more than one term standing so I want to pull the latest one. 4. My expected output would look something like this: http://imgur.com/lPE1nNg 
When I mentioned CREATE / INSERT statements it was because I was hoping you could provide sample data so the other fellow redditors had something to work from. That would make helping you much easier. 
This works really well for that: /* Security Audit Report 1) List all access provisioned to a sql user or windows user/group directly 2) List all access provisioned to a sql user or windows user/group through a database or application role 3) List all access provisioned to the public role Columns Returned: UserName : SQL or Windows/Active Directory user cccount. This could also be an Active Directory group. UserType : Value will be either 'SQL User' or 'Windows User'. This reflects the type of user defined for the SQL Server user account. DatabaseUserName: Name of the associated user as defined in the database user account. The database user may not be the same as the server user. Role : The role name. This will be null if the associated permissions to the object are defined at directly on the user account, otherwise this will be the name of the role that the user is a member of. PermissionType : Type of permissions the user/role has on an object. Examples could include CONNECT, EXECUTE, SELECT DELETE, INSERT, ALTER, CONTROL, TAKE OWNERSHIP, VIEW DEFINITION, etc. This value may not be populated for all roles. Some built in roles have implicit permission definitions. PermissionState : Reflects the state of the permission type, examples could include GRANT, DENY, etc. This value may not be populated for all roles. Some built in roles have implicit permission definitions. ObjectType : Type of object the user/role is assigned permissions on. Examples could include USER_TABLE, SQL_SCALAR_FUNCTION, SQL_INLINE_TABLE_VALUED_FUNCTION, SQL_STORED_PROCEDURE, VIEW, etc. This value may not be populated for all roles. Some built in roles have implicit permission definitions. ObjectName : Name of the object that the user/role is assigned permissions on. This value may not be populated for all roles. Some built in roles have implicit permission definitions. ColumnName : Name of the column of the object that the user/role is assigned permissions on. This value is only populated if the object is a table, view or a table value function. */ --List all access provisioned to a sql user or windows user/group directly SELECT [UserName] = CASE princ.[type] WHEN 'S' THEN princ.[name] WHEN 'U' THEN ulogin.[name] COLLATE Latin1_General_CI_AI END, [UserType] = CASE princ.[type] WHEN 'S' THEN 'SQL User' WHEN 'U' THEN 'Windows User' END, [DatabaseUserName] = princ.[name], [Role] = null, [PermissionType] = perm.[permission_name], [PermissionState] = perm.[state_desc], [ObjectType] = obj.type_desc,--perm.[class_desc], [ObjectName] = OBJECT_NAME(perm.major_id), [ColumnName] = col.[name] FROM --database user sys.database_principals princ LEFT JOIN --Login accounts sys.login_token ulogin on princ.[sid] = ulogin.[sid] LEFT JOIN --Permissions sys.database_permissions perm ON perm.[grantee_principal_id] = princ.[principal_id] LEFT JOIN --Table columns sys.columns col ON col.[object_id] = perm.major_id AND col.[column_id] = perm.[minor_id] LEFT JOIN sys.objects obj ON perm.[major_id] = obj.[object_id] WHERE princ.[type] in ('S','U') UNION --List all access provisioned to a sql user or windows user/group through a database or application role SELECT [UserName] = CASE memberprinc.[type] WHEN 'S' THEN memberprinc.[name] WHEN 'U' THEN ulogin.[name] COLLATE Latin1_General_CI_AI END, [UserType] = CASE memberprinc.[type] WHEN 'S' THEN 'SQL User' WHEN 'U' THEN 'Windows User' END, [DatabaseUserName] = memberprinc.[name], [Role] = roleprinc.[name], [PermissionType] = perm.[permission_name], [PermissionState] = perm.[state_desc], [ObjectType] = obj.type_desc,--perm.[class_desc], [ObjectName] = OBJECT_NAME(perm.major_id), [ColumnName] = col.[name] FROM --Role/member associations sys.database_role_members members JOIN --Roles sys.database_principals roleprinc ON roleprinc.[principal_id] = members.[role_principal_id] JOIN --Role members (database users) sys.database_principals memberprinc ON memberprinc.[principal_id] = members.[member_principal_id] LEFT JOIN --Login accounts sys.login_token ulogin on memberprinc.[sid] = ulogin.[sid] LEFT JOIN --Permissions sys.database_permissions perm ON perm.[grantee_principal_id] = roleprinc.[principal_id] LEFT JOIN --Table columns sys.columns col on col.[object_id] = perm.major_id AND col.[column_id] = perm.[minor_id] LEFT JOIN sys.objects obj ON perm.[major_id] = obj.[object_id] UNION --List all access provisioned to the public role, which everyone gets by default SELECT [UserName] = '{All Users}', [UserType] = '{All Users}', [DatabaseUserName] = '{All Users}', [Role] = roleprinc.[name], [PermissionType] = perm.[permission_name], [PermissionState] = perm.[state_desc], [ObjectType] = obj.type_desc,--perm.[class_desc], [ObjectName] = OBJECT_NAME(perm.major_id), [ColumnName] = col.[name] FROM --Roles sys.database_principals roleprinc LEFT JOIN --Role permissions sys.database_permissions perm ON perm.[grantee_principal_id] = roleprinc.[principal_id] LEFT JOIN --Table columns sys.columns col on col.[object_id] = perm.major_id AND col.[column_id] = perm.[minor_id] JOIN --All objects sys.objects obj ON obj.[object_id] = perm.[major_id] WHERE --Only roles roleprinc.[type] = 'R' AND --Only public role roleprinc.[name] = 'public' AND --Only objects of ours, not the MS objects obj.is_ms_shipped = 0 ORDER BY princ.[Name], OBJECT_NAME(perm.major_id), col.[name], perm.[permission_name], perm.[state_desc], obj.type_desc--perm.[class_desc] 
I'm going to disagree with popular opinion here. In a data warehousing situation where everything we do is a full table scans along with some complicated aggregation, temp tables are about the only sane way we can get the job done.
Should have clarified this, there are cases where temporary tables are the right solution. But using them instead of sub queries as a general rule: that is stupid. You want to use sub queries in data warehousing too, but probably not as commonly as in OLTP. I have mostly done OLTP work.
There are other cases where temp tables are useful. I've optimized some insanely complex queries by creating temp tables, indexing them, and then going from there. And then there's the dreaded Error 8623 ("The query processor ran out of internal resources and could not produce a query plan. This is a rare event and only expected for extremely complex queries or queries that reference a very large number of tables or partitions. Please simplify the query."), which pretty much means "Buy bigger iron or use temp tables."
Yes and no. Sometimes this will increase total query time, and sometimes it will improve it. You have to take each query separately to check the impacts. Within that, some parameters will be affected more than others by the change. However, the impacts can be minimized, and there are alternatives . Maybe instead of sub selects you can use a view . This will give the same level of readability, with adding temp data. Depending on query time, many alternatives may be viable. Tldr: test on each query with different parameters to get true impact.
I still dont see why you would write such a table (or subset of the table) to the temp db. If all you do is range scans, the clusterd index is still quite nice to have. Shouldn't overly complicated aggreations be done in the cube anyway ? I think the popular opinion still stands strong, even thou for your case it might not
 There is times to use subselects and times to use temp tables. If somebody comes up with a rule like this in development they should be fired.
Here's the sad news, SQL doesn't do a variable number of columns well. You could do it with temp tables and/or dynamic SQL, but that's overly complicated. I think you have two options: 1) Have a variable number of rows per date instead of columns 2) Use your front end tool to suppress the date columns that have no values in them. That said, if you insist on the dynamic sql option, this will work: declare @csv nvarchar(max) ,@sql nvarchar(max) --build the csv for the pivot select @csv = isnull(@csv + ',', '') + '[' + cast(Entry_Date as char(10)) + ']' from dbo.[Entry] where employee_id in (1,3) and Entry_projID in (1,3) group by Entry_Date order by Entry_date select @SQL = ' select Employee_ID, Entry_ProjID, ' + @csv + ' from ( select Employee_ID, Entry_ProjID, Entry_Hours, Entry_Date from dbo.[Entry] where employee_id in (1,3) and Entry_projID in (1,3) ) p pivot ( sum(Entry_Hours) for Entry_Date in (' + @csv + ') ) as PivotTable;' exec sp_executesql @SQL 
Generally speaking though, the "pattern" you would use to solve this type of problem would be as follows: SELECT a.field1 , a.field2 , a.field3 , b_max.field2 FROM table_a a LEFT JOIN ( SELECT b.field1 , MAX(b.field2) AS field2 FROM table_b b WHERE b.field3 = 'X' GROUP BY b.field1 ) b_max ON b_max.field1 = a.field1 WHERE a.field4 = 'Y' ; The key is to aggregate the data on the right side of the join first and then join it to the other table.
&gt; They're citing readability as the reason for this change. they are idiots
temp tables as in, temporary tables ###? or, are you talking about pre-caching tables based on precomputed statements and using it more as a hash/lookup array table? 
select user_id from *table_name* where id = 995 and amount &gt; 1000;
select * from table bank where id=995 and amount&gt;50000
 SELECT * FROM bank WHERE id = 995 AND amount &gt; 50000 no "table" in the syntax, IndianAstronaut :) 
As in #. Replacing derived tables, etc with temp tables.
Ah damn. No late night programming advice from me again. :-)
Havent used MSSQL in a good while, but IDENT_CURRENT is not that useful. There's quite a few cases where it will return the identity seed, or often 1, rather than a value which correlates to the primary key. Also if you have a transaction which is inserting into the table, and the transaction is rolled back or aborted (due to error say), then the identity value will still be incremented. This could explain some cases where identity is greater than max primary key
Do the tables in question have triggers on them? 
HA. Indexes? What indexes?
According to documentation, it "Returns the last identity value generated for a specified table or view. The last identity value generated can be for any session and any scope." You'd expect it *usually* to be the max of the primary key, however, keep in mind that you can usually insert on a table with a specific value instead of identity, and MSSQL will use that value and not generate a new identity; this could lead to the identity not be related to the max primary key at all (and this can also happen when you restore/load data from another db, depending on how you do it). Even if your data correlates, your transaction may fail (ident_current increments, but there's no row) , or you can delete the last inserted row; both of this can make ident_current be bigger than the max key.
I'll go out on a limb and say this looks a lot like homework ... in that case, I strongly suggest you make the effort to learn SQL. Modify this question; can you select only certain fields ? how about when the amount in under 5000 ? SQL is *very* useful for most people in CS/IT ; learn it 
Have you actually tried it ? It should be easy, just use WITH replacing your temp table, or define a view instead of your temp table ... I'd be very surprised if you can optimize better than the optimizer :) it CAN happen, but it usually doesn't 
http://www.brentozar.com/
An excellent book on the subject is [ PostgreSQL Replication](http://www.packtpub.com/postgresql-replication/book) which focuses on replication but touches lightly on related subjects like backups, point in time recovery, sharding, and how the WAL is implemented. For backups you have physical (binary copies of the database) and logical backups (SQL dumps done with pg_dump/pg_dumpall). You may want either or both of these for your server setup. There are several tools for simplifying physical PostgreSQL backups but the one that I have heard the most praise of is [barman](http://www.pgbarman.org/). It simplifies backup administration and point in time recovery. You can do the same thing with pg_basebackup and some scripting but barman simplifies the setup. A good book on setting up servers and monitoring and optimizing the performance of PostgreSQL servers is PostgreSQL 9.0 High Performance[](http://www.packtpub.com/postgresql-90-high-performance/book). It is a bit old now, but much of the advice is general and even applicable to other databases. EDIT: Reworded my post to be more coherent and slightly less of an infodump.
you can check my online course Mastering Oracle Database SQL 12c here: http://oracledevacademy.wordpress.com Facebook: https://www.facebook.com/pages/Oracle-Developers-Academy/692659924099670 24/7 online support for any doubt/question you may have 
Let's say tbl1 and tbl2 both have n columns, with 100 records each, and there are no records duplicated between the two tables (to make my life easier). The UNION approach will yield 200 records with n columns and will require that the corresponding cols from tbl1 and tbl2 have the same data type. The "dot" form, otoh, will yield 100 records with 2n columns.
the second query you showed is invalid, as it has no FROM clause now, were you perhaps thinking of an inner join or a cross join?
I assumed he was going to do a old-school style FROM table1, table2.
Forgive me if I do not fully understand your problem but it would seem to me that you are trying to reference a value in a table by using something akin to a pointer. I know that you are just labeling values when you go Dbo.Value1 and such but if you stick Value 1, Value 8 and Value 15 together in the same column1 how do you have any meaningful data at that point? Wouldn't your proposed solution, at least based on the information provided, just result in a jumble of numbers and not data? I tried a couple tricks on my local server to see if I could get anything close to what you wanted....that took me down the path of using [UNPIVOT](http://technet.microsoft.com/en-us/library/ms177410\(v=sql.105\).aspx). That got me far enough that I was able to rank the values based on their positional rank in the original table but then I realized that any further manipulation on my part would make the data meaningless. I can share (my really bad imho) my attempt with you if you would like but I would suggest you either think if what you are trying to do SHOULD be completed in the Data layer, rethink your problem all together or try to provide a little more details.
Here is my attempt the beginning part was just me creating a table to work with that was similar to your situation. Run that and I think you will see what I mean. Once again I apologize if I completely have missed the point it is late on a Sunday. Good luck! With NumberSeq(Number) as (select 1 as Number Union All Select Number + 1 From NumberSeq Where Number &lt; 100), People (Person) as (Select Char(64 + Number) From NumberSeq Where Number &lt;= 5), RanData (PersonId,V1,V2,V3,V4,V5,V6,V7,V8) as (Select Char(64 + Number), Case When Char(64 + Number + 1) &lt;&gt; 'F' Then Char(64 + Number + 1) Else Null End , Case When Char(64 + Number + 8) in ('J','K','M') Then Char(64 + Number + 8) Else Null End , Case When Char(64 + Number + 3) = 'D' Then Char(64 + Number + 3) Else Null End , Char(64 + Number + 5), Case When Char(64 + Number + 2) not in ('D','F','G') Then Char(64 + Number + 2) Else Null End , Case When Char(64 + Number + 4) in ('J','K','H') Then Char(64 + Number + 4) Else Null End , Case When Char(64 + Number + 7) in ('J','I','M') Then Char(64 + Number + 7) Else Null End , Case When Char(64 + Number + 6) not in ('J','K','M') Then Char(64 + Number + 6) Else Null End From NumberSeq Where Number &lt;= 5), DataSet as (Select Person, R.* from People P Inner Join RanData R On P.Person = R.PersonId) -- Actual code relevant to your needs starts here Select PersonId, ValueID, Value From (Select PersonId,V1,V2,V3,V4,V5,V6,V7,V8 From DataSet) p UNPIVOT (Value FOR ValueId In (V1,V2,V3,V4,V5,V6,V7,V8)) as unpvt 
It would have been nice to have some sample data to play around with, on things like that. Well, I'll break down on how I would approach this: &gt; I have 15 data values that I need to be on one line per person in the query. You need to / can use the pivot function for that &gt; I need them to be in the far left column until all values are populated and leave the columns on the right to be null if any person does not have every value. Well, that sounds like dynamic SQL to me. If you write pivoted data to a temp table, it would be quite easy to write a sql query, creating the column order for a dynamic query selecting the data in order to return it. The pivoting will be the expensive operation here, the dynamic SQL on the temp table will be laughable in comparison. I do not see a realistic alternative to be honest, since you need a dynamic column order 
I will investigate this. Thanks. 
&gt;Wouldn't your proposed solution, at least based on the information provided, just result in a jumble of numbers and not data? I do see why it looks like way. But once I find the solution, I was going to apply constants that follow the same logic and stick to the left of the column in question. I am applying these to a vertical file and each person will have a new line (on file) per value of what this returns. You have gone above and beyond, I was mostly looking to figure out if I was following the right path or if others had better ideas. I sincerely appreciate it and will work today to see about applying your solution as what I am seeing in the follow up comment might be what I am looking for. 
Which one are you referring to when you say old-shool? I'm not clear which one you find more readable.
Thanks. I get it and I'm sold. I never cared much before since all my queries have been pretty simple. 
novice here. i wonder if a wm_concat function would work?
I use Oracle SQL.
select style, color, wm_concat(size) from table group by style, color;
I'd do a subquery like: where id in (select max id from other_table group by column).
Eff!
http://stackoverflow.com/questions/12559551/sql-server-equivalent-of-wm-concat-function
:/ our licences are cheaper than yours -.- there is a good work arround with for xml path('') thou
So, I appreciate all of the recommendations here. I did end up using the EDI tool functions to produce the result I needed. So, unfortunately, I need to leave this one unsolved. Again, I appreciate everyone who commented. 
wm_concat, bro.
On top of what everyone has said here I would like to add another point. If you are doing this on SQL procs with extremely high load this can create some major issues with tempdb. Where I work we had to go through recently and remove a lot of tempdb usage as we were reaching certain limitations of tempdb itself which was bottling the server. The following articles are worth a read for tempdb contention issues: http://www.sqlskills.com/blogs/paul/misconceptions-around-tf-1118/ http://www.sqlskills.com/blogs/paul/a-sql-server-dba-myth-a-day-1230-tempdb-should-always-have-one-data-file-per-processor-core/
I made/edit a query like this some time ago: declare @date datetime declare temp_cur cursor for select distinct mydate from [#temp1] order by MyDate open temp_cur fetch next from temp_cur into @date while @@FETCH_STATUS = 0 begin DECLARE @txt VARCHAR(8000) = null SELECT @txt = COALESCE(@txt + ',', '') + [item_no] from [#temp1] where mydate = @date select @txt, MyDate from [#temp1] where Mydate = @date group by mydate fetch next from temp_cur into @date end close temp_cur deallocate temp_cur select * from #temp1
You could get the explain plans through TOAD and compare them to see what differences there are between the two queries. But for a shot in the dark: the implicit type conversion is the inverse of what you anticipated. If y.no is a NUMBER type and x.no is a VARCHAR type, and all of the entries in x.no DO actually convert to NUMBER types successfully through TO_NUMBER, then it will likely be slow. That's because x.no = y.no will force them to the same type -- by default, I believe this tries to do NUMBER types, not VARCHAR types, which is why it's slow -- each value of x.no needs to be TO_NUMBER'd with the overhead of capturing errors to give the "invalid number" error if it fails to convert. If you instead did "x.no = TO_CHAR(y.no)" I believe you would have the same execution plan as "x.no = y.no || ''", both doing the faster "number-to-character" conversion instead of "character-to-number." I think it's probably a matter of which side does the type conversion when an implicit type conversion is necessary. If I'm right, the above line (x.no = TO_CHAR(y.no)) would also have a fast execution time.
Thanks, I'll make sure to read these!
I think you may be mistaken, but without hashing out test examples it is almost not worth discussing. For many cases, SQL Server's views are just replacement select statements. In other words, unless you materialize and schema-bind the view during definition, there are few performance benefits (there are of course execution plans cached and all that). CTEs are similar (again, in many cases but not all). If you write to temp tables (or staging tables) in a warehousing scenario, the developer can *with precision* populate tables of a particular schema very rapidly, build very precise indexing schemes, and execute very intentional queries against it. It isn't outsmarting the optimizer; instead, it's using the strengths of the database engine to work around the inherent inabilities of a system that is being scaled, arguably, beyond its limits. Source: warehousing developer and engineer; big data developer; etc. (probably the same as you, and no offense meant if we disagree). Cheers!
This comment should be upvoted beyond belief. I've worked at corporations that have relied VERY heavily on temp tables, temp variables, and used Service Broker as a non-transactional messaging paradigm... and then five years later their platform was crippled because of the Error 8623. I worked there 2 years and wasn't around long enough to see how they solved this problem. Instead, they kept a senior DBA and senior DB dev around just to keep it running.
(SQL Server user here, so I could be very wrong...) If you ran the first one first, would the query still be in cache when you ran the second? It could be that there is no difference. Does it take 2hrs every time you run the first one?
no converting taking place, they are all varchar's 
You need a program manager that knows the meaning (and the reason for, and how to politely) say "No".
Thanks, just wondering because I will often forget to flush cache when performance tuning queries, and my impressive gains don't really exist.
i hear you, i was trying to run the queries earlier this morning and they kept being cancelled, didn't know the warehouse people were in the midst of their ETL processing for the month... but even after that was over the query wasn't working until i used the optimizer. 
Do they have the same precision?
they should... 
well, generally restarting the sql server service will clear the temp table...
This is one of my favorites... http://sqlserverplanet.com/dmvs/missing-indexes-dmv
Yep, the query never stops executing, I let it run for 30 minutes and then canceled, no error messages showed up. Even when I try to create the same table I don't get the error saying the table already exists, the query just keep executing forever.
Are there a large number of nulls in the y.no column? Maybe explicitly adding the empty string eliminates null processing? I freely admit that I'm guessing here.
You just save my day, by running DBCC OPENTRAN on the tempdb I was able to find the session that was locking the table, then kill it and automatically the table was dropped :) Thank you so much.
thank you i will give this a try
Just spit-balling; Could it be because checking checking if something is zero chars is a simpler function than checking if for example "a" = "a". So if either y.no or x.no is zero chars the check is much simpler, (because you could test if its the same by counting bytes \* rather than the value of a number of bytes.) \* because there aren't 256 values of zero. ----------------- **TL:DR;** Could SQL be checking if something is "" by the length of the string before it checking the char value of the string. **Ex.** "" is zero bytes (not NULL but zero), and there is only one value of zero bytes, but if the value is "a" then SQL would have to check the length and then check the 256 possibilities one 8-bit byte could have. So if x.no = "" the answer is yes so SQL would skip checking the more complicated x.no = y.no (thus skipping a bunch of checking) ------------ Just a theory, I'm not that deep in SQL, but I do a fair bit of programming for fun.
Yeah I don't think that will get me the results I'm looking for. I was thinking more about it (and I'm about to try to code it). I think if I get the basic filtered lists of volumes for fixed disk volumes and mount point volumes where I know they're obviously not need via a simple LIKE clause, I can then create a temporary list of potential matches to database paths and mounted volumes for the fixed drive in question and if there exists any database path that is not like one of those mounted volumes in this temporary list then I keep the fixed drive, otherwise I discard it. This shit is making my brain hurt for something that, on the surface, seems like an extremely easy problem to solve.
You could always build your list by doing LIKE the mounted volume list then UNION the result of a NOT LIKE to get the paths that aren't a mounted volume afterwards. Edit - I'm still not entirely sure why you need the information in this format. If you determined E:\ was a parent of the mounted drives you could just GROUP BY the parent(including the free space of the parent) and sum the free space. 
 SELECT location, ssn FROM ssntable AS s INNER JOIN accounttable AS a ON s.accountnumber = a.accountnumber WHERE a.closedate IS NOT NULL ORDER BY location For specific locations, add AND location = 'Chicago' or something to the WHERE clause. You said 'grouped by' but I think you mean 'ordered by', right?
For clarification, an ssn can have more than one account at one location
is if ssn 555555555 had accountnumber 1 and 2 at location 1, and account 1 was closed this year but 2 was open, I would not want them appearing under location 1. If they had account number 3 and 4 both of which closed this year at location 2 I would want them appearing for location 2 but not for location 1
Okay, my bad. It's more complicated than I thought. Let me mull it over for a minute.
yawn http://dudelol.com/DO-NOT-HOTLINK-IMAGES/A-new-way-of-SQL-Injection.jpg
[Image](http://imgs.xkcd.com/comics/exploits_of_a_mom.png) **Title:** Exploits of a Mom **Transcript:** [[A woman is talking on the phone, holding a cup]] Phone: Hi, this is your son's school. We're having some computer trouble. Mom: Oh dearâdid he break something? Phone: In a wayâ Phone: Did you really name your son "Robert'); DROP TABLE Students;--" ? Mom: Oh, yes. Little Bobby Tables, we call him. Phone: Well, we've lost this year's student records. I hope you're happy. Mom: And I hope you've learned to sanitize your database inputs. **Title-text:** Her daughter is named Help I'm trapped in a driver's license factory. [Comic Explanation](http://www.explainxkcd.com/wiki/index.php?title=327#Explanation) **Stats:** This comic has been referenced 112 time(s), representing 1.27% of referenced xkcds. --- ^[Questions/Problems](http://www.reddit.com/r/xkcd_transcriber/) ^| ^[Website](http://xkcdref.info/statistics/)
There's probably a MUCH more elegant way of doing this, but I'm a little drunk and thinking is hard. Basically you compare the **total** number of accounts per ssn per location with the number of **closed** accounts per ssn per location, and return them if that number is equal. select a.ssn, a.locationnumber from ( select s.ssn, a.locationnumber, count(*) cn1 from ssntable s inner join accounttable a on s.accountnumber = a.accountnumber group by s.ssn, a.locationnumber ) a -- the total number of accounts per ssn per location join ( select s.ssn, a.locationnumber, count(*) cn2 from ssntable s inner join accounttable a on s.accountnumber = a.accountnumber where closedate is not null group by s.ssn, a.locationnumber, a.closedate ) b on a.ssn = b.ssn and a.locationnumber = b.locationnumber and a.cn1 = b.cn2 -- the number of closed accounts per ssn per location 
So, if I understand you right, you want a list of all the ssn/locationnumber pairs where all the associated accounts have a closedate in the current year, yes? I would write it backwards with a self-join, i.e. give me all the ssn/location pairs where there are no null closedates and no closedates before this year. Nulls will fail the "between" condition so you don't need to kick them out explicitly. You can do your date values however you like depending on what your dbms allows. E.g. select distinct ssn, locationnumber from ssntable s, locationtable l1 where s.accountnumber = l1.accountnumber and not exists (select * from accountnumber l2 where l2.accountnumber = l1.accountnumber and l2.locationnumber = l1.locationnumber and not l2.closedate between &lt;date1&gt; and &lt;date2&gt;) order by location; 
Here's some example tables. CREATE TABLE #ssn ( ssn int ,accountnumber int ); CREATE TABLE #acct ( accountnumber int ,locationnumber int ,opendate date ,closedate date ); INSERT INTO #ssn VALUES (555555555, 1) ,(555555555, 2) ,(555555555, 3) ,(555555555, 4) ,(666666666, 5) ,(666666666, 6) ,(666666666, 7) ,(666666666, 8); INSERT INTO #acct VALUES (1, 1, '19990101', '20140110') -- closed this year ,(2, 1, '19990101', NULL) -- open ,(3, 2, '19990101', '20140103') -- closed this year ,(4, 2, '19990101', '20140104') -- closed this year ,(5, 1, '19990101', '20140112') -- closed this year ,(6, 1, '19990101', '20140103') -- closed this year ,(7, 2, '19990101', '20140109') -- closed this year ,(8, 2, '19990101', '20131230'); -- closed LAST year In this example, you only want ssn 555555555 to come up for location 2 and 666666666 to come up for location 1. Right? WITH c AS ( SELECT t1.ssn, t2.* FROM #ssn AS t1 INNER JOIN #acct AS t2 ON t1.accountnumber = t2.accountnumber ) SELECT DISTINCT c1.locationnumber, c1.ssn FROM c AS c1 WHERE EXISTS ( SELECT * FROM c AS c2 WHERE c1.ssn = c2.ssn AND c1.locationnumber = c2.locationnumber AND '20140101' &lt;= ALL ( SELECT closedate FROM c AS c3 WHERE c2.ssn = c3.ssn AND c2.locationnumber = c3.locationnumber ) ) ORDER BY c1.locationnumber; First, the CTE called `c` will join both `#ssn` and `#acct` tables since we need to "group" both `ssn` and `locationnumber` columns. Then we have 2 nested correlated subqueries. The first comes after the `EXISTS` clause. In a nutshell, everything after `EXISTS` says to only return columns from `c` where all the close dates from accounts at a location tied to 1 ssn is this year. How it achieves it is in the second subquery. The second subquery starts with `ALL(` and returns the `closedate` for accounts from a ssn and location. If there is a `NULL` or a date before Jan 1, 2014 (last year), then it will fail the test `'20140101' &lt; ALL(`... and those rows won't be returned to the first subquery. **edit: fixed an inequality.** Also, to clean up: DROP TABLE #acct; DROP TABLE #ssn;
It's been a couple years since I've worked with a RDBMS, so I may be somewhat inaccurate, but do you have any NULLs in y.no? It may be that comparing to NULL is slow, but the OR is very fast so if y.no is NULL, it will compare x.no to '' instead of NULL.
There is an easier way. Take your original query and modify it slightly. SELECT location, ssn FROM ssntable AS s INNER JOIN accounttable AS a ON s.accountnumber= a.accountnumber WHERE NOT EXISTS( SELECT * FROM ssntable s2 INNER JOIN accounttable t on s2.accountnumber = t.accountnumber WHERE s2.ssn= s.ssn AND t.closedate IS NULL ) ORDER BY location So the select in the where clause will return all of the ssn's that have not been closed. And as long as that does not exist it will return all ssn's that have been closed at any location. Or in another words return all of the ssn's where there isn't any open accounts.
I figured out what I need. All I did was create a bit field in the @t_DatabaseFilePath table called IS_ON_MOUNTED_VOLUME and I set that to 1 if the path exists on a mounted volume. Then, if there are any paths that don't exist on a mounted volume and are LIKE the fixed drive volume I keep it in the list. I tested it and it works. I'm going to finish this and then I'll post the code because I think it's useful. **edit** Link to code is here: http://www.reddit.com/r/SQLServer/comments/1vaipz/mssql_stored_procedure_to_check_volume_free_disk/ 
This is pretty funny.
Wrap that in parens. (Paycode = "a" or Paycode = "b") AND.... 
The switch statement requires everything to be true to do the switch. Since you are using OR the parenthesis should fix that so a OR b can be true. 
Well I use BCP command line tool for bulk insert. I never even thought about coding bulk insert myself.
If you have a SQL Server license, you already have an SSIS license. You also get Reporting Services, Analysis Services, and StreamInsight.
It's quick and easy to use the database import/export wizard. If space isn't an issue then just use that. You can even save it as a package to rerun later. You may want to disable then rebuild any non clustered indexes on the destination table to speed up the import. Batching is good if it's a production table actively being used and/or you have limited transaction log space. It's typically a good idea in general though. Any bulk insert operation will be fastest whether it be bcp, the import export wizard, or the .net library
If the rows reside in another MSSQL instance use a distributed query to load them. If they come from a heterogeneous database that your target instance isn't already linked to, then dump your source to delimited text file(s) and use BCP. BOL has a very good set of entries on BCP that include easy to edit example scripts. That's how I did it the first time. EITHER WAY, you should always load to a staging table first, then to your tables. It's what best practices dictates for a reason. And for the love of glob, work this all out in development first. Professionals don't develop in Production. 
Make sure you use the 64-bit version of the Import/Export Wizard. It should be in your Start Menu. 
"and" has precedence over "or". You need to look up the "logical operator precedence" to realize why your code doesn't work. http://msdn.microsoft.com/en-us/library/fw84t893.aspx
well, if you are making 16$/h.... can't get much worse can it ? Getting the exam will definitly help finding a new job.
Well yeah, that's why I'm doing it. But I learned on the job, so I entered with only basic computer skills. They haven't felt the need to pay me, which I understand, but now I'm moving onward and upward. I'm mostly wondering what kind of difference having the certification makes compared to just having experience. 
Experience trumps certs any day of the week. Having the knowledge that the certs will test you on is good, but a certification isn't necessarily required to prove you have it, and a certification alone absolutely will not get you the job.
Is this a viewpoint that most employers have? I've realized that in my 4 yrs experience, I've been doing the same kinds of things over and over again. In doing my certification study I feel like I'm learning more. Have you gotten certified?
I don't have any certifications, and in hiring candidates I never pay attention to that line on a resume. In a technical interview you can expect to be asked a few questions about how to write queries of differing levels from easy to hard. Not knowing how to do the hard one won't get you disqualified unless you're applying to a really top tier place, but at least have a competent answer on how you'd go about it. A slightly longer take-home assignment wouldn't be unheard of either. If going after the certification helps you learn new stuff you wouldn't have come across otherwise, then go for it because that will help you. Just remember, it's the knowledge you gain from the certification you're after, not the piece of paper itself. I've come across way too many people in my career who had page after page of certifications, but had no clue how to do their jobs when it came to a real world test.
I use postgres and we're starting to work with Hive a bit now. It depends on the level you're applying for. If you're just applying for some just above-entry level reporting analyst type roles, then no, you probably don't need much else. Beyond the technical stuff, be good at talking to people. Seriously. Remember that guy in Office Space who takes the specifications from the customers and gives them to the programmers because they don't have any people skills? Nobody wants to hire that guy. They'd rather find a reports analyst who has at least a modicum of people skills to talk to the customer directly to get the specs without any middle man.
The only true way to know how each version behaves would be to check a query plan. Depending on indexes (or lack thereof) both should perform the same. Keep both in your bag of tricks. As well as variations in queries from a DeMorgan's law standpoint.
It's not about how it behaves or performs. It's purely about style and readability.
But is it about performance, at least where I sit. But, if you want to say style trumps performance, be my guest.
OK, I'm confused now. Those 2 queries are semantically identical, and should perform exactly the same (given a sane optimiser). I was asking which style was preferred from a readability POV. What part of this do you think is related to performance?
To be clear, there's no primary key and you should have an exact column-to-column match for the entire row? Your != attempt doesn't work because it matches **every** m row that is not equal to each j row. Though in your example the third row of J matches the second row of m, it doesn't match the first, third, and forth row, so they're being returned. The simplest way to solve this is a `MINUS` statement: SELECT * FROM john MINUS SELECT * FROM mary That is, take all rows from john and remove the ones that are exactly the same in mary. `MINUS` isn't particularly efficient on very large tables, so in that case you'd use a `LEFT OUTER JOIN` where the fields on mary are null.
&gt; It's purely about style and readability if this is the case, then i would make several slight changed to your code -- SELECT a.dummy AS a , b.dummy AS b FROM dual a INNER JOIN dual b ON b.dummy = a.dummy AND b.dummy = 'X' - sql keywords in caps, identifiers in lower case - always write INNER even though it's optional - columns mentioned in a specific order in the ON clause 
I highly prefer the second one. It's more readable and makes semantic senses. Also, this is how I see most people write queries. Also, I prefer table aliases to use the keyword `AS`
I posted this on /r/SQLServer, but you should consider * using the `MERGE` statement; or * just truncate and insert from a staging table Considering that this only a few thousand rows, I'd go with the second option.
I'm not familiar with Oracle syntax, but it should be something like this. SELECT m.* FROM Mary AS m WHERE NOT EXISTS ( SELECT 1 FROM John AS j WHERE m.A = j.A AND m.B = j.B AND m.C = j.C AND m.D = j.D );
I wish more employers were like you :( 
syntax highlighting, where is it? you failed to use it noise? you wrote AS which is not required but at least you used the leading comma convention, so you're fine in my books :)
I think you mean SELECT * FROM mary MINUS SELECT * FROM john 
I'm sorry, but your friend is correct. ANSI 92 joins were introduced to make it clearer in describing the relationships between tables (FK -&gt; PK, FK -&gt; AK, or implied relationships) used in queries. This allowed "where" clauses to be distinct filtering clauses, i.e. predicates which act as a filter on the results of those joins. ANSI 89 join syntax suffered from this lack of segregation, which made comprehension more difficult and had limitations in facilitating complex join scenarios (outer join on multiple tables and full outer join). I pretty much ALWAYS use your colleague's approach. It is clear to other developers what are relationships and what are filters and uses the syntax as it was intended. The only exceptions to this are when you need to outer join on filtering predication. What you are doing is effectively recreating ANSI 89 syntax in ANSI 92 form, which to me defeats the point.
This is correct.
I started at my current job almost 3 years ago. I used to be a pharmacy tech, then a Radio Shack store manager. I started here as data entry/document scanner. Through hard work and training now I'm about to take over the role of DBA upon completion of my degree. My employer could care less about the certs. However, he said that he wants me to take the classes and the exam in case the company gets bought out or one day ceases to exist. He wants me to be marketable, not because it matters at my current job. So in all honesty, I would say take the exam, study your heart out. Not only will you gain more knowledge, but you'll also be in a much better position to attain a job. Experience trumps certs, yes, but experience AND certs ensures you're hired and your know-how is current. Good luck, and seriously, without knowing your current job description, you should talk to your employer about a raise.
How about next time you say ,"Use a cursor!" you just keep that thought to yourself. If you absolutely have to use recursive language, use a while loop.
dont
I like writing out the full schema so I am not looking for feedback on that.
You mean you don't want to alias the DB names? It drives me crazy not to.
But to address your concerns, any reason you're not using a TRY/CATCH on this? Also, isn't dropping a temporary table redundant specifically because it's temporary? I tend away from #Table because of auditing reasons, but you should be able to eliminate those. I also like to place a warning on the top of scripts with hardcoded values, such as: ('EX013', 'EX014', 'EX015', 'EX016', 'EX017', 'EX018', 'EX019', 'EX020', 'EX022', 'EX024', 'EX031', 'EX032', 'G9001', 'G9005', 'G9010', 'G9011', 'G9012', 'TF070', 'TX031') But maybe those are never going to change. I don't know your context.
 ---- I have no idea what this bit does... DECLARE cur CURSOR STATIC LOCAL FOR SELECT DISTINCT '#temp3.' + ltrim(str(dateno)), convert(char(10), tempStoredDate, 121) FROM #temp1 OPEN cur WHILE 1 = 1 BEGIN FETCH cur INTO @dateno, @date IF @@fetch_status &lt;&gt; 0 BREAK EXEC tempdb..sp_rename @dateno, @date, 'COLUMN' END DEALLOCATE cur Select * From #temp3 GO Screw the person that originally wrote this code. This will dynamically rename column names in the database. This isn't changing values in the column, this is renaming the columns themselves. Seems fitting whoever wrote it also put it in a cursor. Stupid is as stupid does. Take all that code and throw it away. DECLARE @DATES VARCHAR(3000) SELECT @DATES = COALESCE(@DATES + ',','') + '[' + Entry_date + ']' from (SELECT DISTINCT CONVERT(nvarchar(10),Entry_Date,101) as Entry_Date FROM dbo.Entry en) T --PRINT (@DATES) DECLARE @sql as varchar(4000) SET @sql = ' SELECT * from (SELECT LastName + '', ''+ Firstname as Employee_Name, CONVERT (nvarchar(10),Entry_Date,101) as Entry_Date, Entry_Hours from Entry En inner join Employee EM on Em.Employee_ID = En.Employee_ID) T PIVOT ( SUM(Entry_Hours) For Entry_Date IN (' + @DATES + ') ) P' exec (@sql) The following code will give you the output you expect. If you want it only for a certain date range, add a date variable and where clause to the @dates code and the query that loads the entry_hours and employee_name table.
upvote for leading comma
(1) Don't use Distinct. Use an actual aggregate. DISTINCT is lazy and there is a certain place in SQL hell for those developers. Distinct is a terrible practice when you are moving or creating operational data. You lose the original granularity of the table, which may befine but you don't know exactly why. Lets say you are moving data from dbo.invoices to dbo.accreceivable INSERT INTO dbo.accreceivable Select DISTINCT i.companyname, i.companypo, i.amount, i.ID as internal_billing, ii.item from dbo.invoices i inner join dbo.invoice_items ii on i.id = ii.inv_id This is a billing system for a company that doesn't sell products. They are completely contract based. They don't deal in units. Using DISTINCT you would get: Bobs Company, 101, $500, 21921, Contract #1 Bobs Company, 101, $1500, 21921, Contract #2 That may be completely ok. However, lets say your accounts receivable program was written a while back by a company that uses a top 1 or only understands 1 internal billing code for the bill processing and mailing. Its not that uncommon, especially if you are already feeding the program using denormalized data. $500 gets paid, $1500 doesn't. In the sample provided the best solution would be to use aggregates to sum the billing amount. INSERT INTO dbo.accreceivable Select i.companyname, i.companypo, SUM(i.amount) as amount, i.ID as internal_billing, CASE WHEN COUNT(id) &gt; 1 THEN 'Multiple Contracts' else ii.item END as item from dbo.invoices i inner join dbo.invoice_items ii on i.id = ii.inv_id GROUP BY i.companyname, i.companypo, i.id Bobs Company, 101, $2000, 21921, Multiple Contracts Keep in mind, are we sending over invoices or are we sending over Accounts Receivable Items for payment? DISTINCT really should be used when you are just trying to get a list of occurances in a field or you are building a list of all your customers in a database on the fly for debugging. It is not intended for the purpose of pulling unique records that it constantly is misused for. DISTINCT also has the performance impact of a sort. It writes to the tempdb first before returning all rows and returns the distinct ones. (2) Temp tables. There is nothing wrong with them. HOWEVER! You are usually DISTINCT in combination with multiple temp tables. In the event you return more than the expected rows because two rows on a given transaction had two seperate values, you could cause a Cartesian product. Thus 1 row may be 2 rows in one of your temp tables and suddently joining it against itself or another table turns it it into 4. I see people over time lose their data integrity because they start down this dangerous path of 0 data validation because they are using DISTINCTS. If you are running MSSQL 2005+, you will actually benefit more from parallelism if you use WITH or if you must Subselect your tables. (3) Its always faster to script out DROP/CREATE #table and use INSERT INTO commands rather than using SELECT ----- INTO #table FROM mytable. (4) Why not join the table and write the correct value the first time? --Adds correct procedure code for internal use UPDATE #Pended SET #Pended.Proc_Code = Client.dbo.CLARITY_EAP.PROC_CODE FROM #Pended INNER JOIN Client.dbo.CLARITY_EAP ON #Pended.Proc_Code = Client.dbo.CLARITY_EAP.PROC_ID (5) This may be faster because you are using multiple joins but the reason this may seem faster is that without the joins you are actually creating a Cartesian Product. The DISTINCT statement removes these rows from your output but those rows must be written to the temp table. So if creates a Cartesian Product with an average multiplier of x600 against 500 megs of data would end up having to write 30gb's of data before scanning and returning the 400 megs of data you need. Including the joins reduces the amount of data being written to the tempdb. Fundamentally, this is why people do what you did. They build temp tables using distinct statements because a Cartesian multiplied by another Cartesian multiplied by another Cartesian causes the SQL query to run forever. However, the truth is, when you are writing a query you should know the relationship of the table you are joining (one to one, one to many, many to one). You should know if there is a possibility that you will get additional rows when joining a table and if you are expecting a row, you should have a method of eliminating the rows you don't want. That may be a MIN, MAX, SUM, COUNT or using an Aggregate in combination with a CASE StATEMENT. LEFT JOIN --Multiple joins for faster processing BUSINESS.dbo.CLIENT_Visit ON BUSINESS_REPORTING.dbo.Client_All_Visits.Encounter_Number = BUSINESS.dbo.CLIENT_Visit.Encounter_Number AND BUSINESS_REPORTING.dbo.Client_All_Visits.Department_ID = BUSINESS.dbo.CLIENT_Visit.Department_ID AND BUSINESS_REPORTING.dbo.Client_All_Visits.Client_ID = BUSINESS.dbo.CLIENT_Visit.Client_ID AND BUSINESS_REPORTING.dbo.Client_All_Visits.Charge_Amt = BUSINESS.dbo.CLIENT_Visit.Charge_Amt AND BUSINESS_REPORTING.dbo.Client_All_Visits.Service_Date = BUSINESS.dbo.CLIENT_Visit.Service_Date (6) You have way to many steps (datasets) especially considering all your tables are local. I have a tendancy to create temp tables or use table variables when I'm dealing with data from linked servers or I'm writing a query for SQL 2000 and I can't use WITH. To grow in SQL, a dataset language, not to be confused with object orientated or high level linear executed operational code; You must write in datasets. The fewest amount of datasets possible while balancing performance, the better. Edit: fixed words. Also how long does this all take to run?
 select a.dummy as a , b.dummy as b from dual a join dual b on a.dummy = b.dummy and b.dummy = 'X' In MSSQL, its preferable you put it in your join statement. MSSQL optimizer looks at the join statement prior to the where. If you have a one to many table relationship in your join statement, adding the b.dummy = 'X' to your join statement may greatly reduce the amount of rows produced before the where statement is applied. Oracle still uses implicit joins select a.dummy as a , b.dummy as b from dual a, dual b where a.dummy = b.dummy and b.dummy = 'X' In which this should be effectively the same query as the MSSQL one. Edit: I seem to be the only one that chooses that method. Might I say I work in tables with 456 million rows and the tables are 40-50gb's. The difference is apparent using my method. EDIT 2: I'll show where this is more effective. Lets say there is additional data. select a.dummy as a , b.dummy as b from dual a join dual b on a.dummy = b.dummy join dept d on a.dept = d.parentdept Where b.dummy = 'X' Lets say dept returns 10-15 rows per row in A because multiple depts share a parentdept id = a.dept. By including b.dummy in your join statement, it can reduce the amount of temp records built before being filtered. In many cases though, a lot of people wont notice much of a difference on simple queries and I usually use the 2nd method unless I'm optimizing my query and "assisting" the optimizer in choosing the smallest datasets to build as possible before filtering the data. Keep in mind, I work with businesses trying to migrate away from SQL 2000 to newer versions but usually I need to simply optimize code so we can stop bailing water from the boat and just before we start plugging the hole.
Kind of new to all this, so please excuse my ignorance, but how do leading commas help?
Yep! Sure are! And self-paced training to correspond. Google-fu it up!
It looks like other folks have covered a lot of the syntax stuff, but I'm really bugged by the fact that you've got your procedure codes hard-coded into your stored procedure, rather than having hooks for the appropriate procedure codes stored in the database somewhere. I'm sure you've got a table for your procedure/service codes. It might make sense to have a column that designates them as viable for this list, and then reference them that way. It will make whatever aspect of those procedures that makes them viable for your list apparent to someone looking at the data, so they hopefully don't have to dig through stored procedures to see which procedures/services fit the criteria, and if there are services/procedures that need to be added or subtracted from the list, it can be handled in one place in your data, rather than in every stored procedure that operates on this set of procedure codes. I've had to fix this exact sort of thing in our data dozens of times, and it's a pain in the ass that could have been avoided. While healthcare codesets don't change terribly often, they aren't static, and it's much easier to address the changes in the data than it is in the stored procedures.
As I said below, this is a style issue, not a performance issue. The queries are semantically equivalent and should have the same plan. If they don't then you need to get a better optimiser.
Ah! This is what I'm looking for! I love the price, $150 is very cheap for a certificate. I will definitely be taking advantage of this. Thank you! 
When everything is mashed into the where clause it is harder to read. Imagine a join over (say) 8 tables, and each table has an additional filter predicate in addition to the predicate on the join columns. In this case there are now 8 predicates in the where clause, each related to a different table in the joins. If I want to see what predicates apply to a given table, I must scan 2 different parts of the query. Sigh. I used the simplest example I thought would illustrate the point, but I think it would have been better to post a real one.
I have not a single query in any of my schemas, that have filtering on the majority of tables joined together in a query. At least I can't think of any... I guess you need to give a real query as an example (if you can, many companies do not like stuff like that). Even thou, having the join conditions seperate from the filtering, is a benefit even in such a case. You at least see the specific filtering at a glance. Sorry to say, but I would fight you to the nails for having the 2nd type syntax in production.
Will this work? SELECT A.ID, A.TERM_CODE_EFF, A.ALvl_Code, A.Admin_Code, A.Major_Code, B.BLvL_Code FROM TABLE_A A INNER JOIN TABLE_B B ON A.ID = B.ID AND A.TERM_CODE_EFF &gt;= B.TERM_CODE AND A.ALvl_Code = B.Lvl_Code;
You make it sound like the 150 bucks is all you need. With as little as 5 months of experiance, trust me, those certs are not to be taken likely. You are gonna have to spend a LOT of your free time preparing for them. And yes I'm talking about the low tier certs.
Where is your while script then? Easy to criticize... And I didn't make this one, I rewrote this one for my needs and if something works, it works...
That table layout ...at least the column naming... it gives me a headache. Anyhow, I think you are looking for something like this : SELECT * FROM tableA a INNER JOIN tableB b ON a.ID = b.ID AND a.A.ALvl_Code = B.Lvl_Code AND convert(date, a.TERM_CODE_EF + '01') &gt;= convert(date, b.admit_code + '01') This is what gets sometimes called a "triangular join", since you will get 0-n rows of table b, for every row in table a. This is generally speaking a bad idea, since it can snowball into a really big performance problem for you. //Edit : you can actually do direct string comparison on that crippeled date string, my initial "its a date, treat it like a date" would be slower, very much slower if there is an index. would still make it a date column thou....
I didn't want to put you down, just be clear on what you are getting into. Last time I saw, the trainings for those certs are a week of workshop, then the exam, and run at 4-5k$. So relativly speaking, 8 months of classes 4k$... thats pritty cheap in comparison :P Just don't underestimate it, self studying for those exams is some serious work.
Or just find brain dumps online like all the smart people do :P
thats like handing out crack laced candy at a playground, you know that dont you..... anyway, the certifications get your job application noticed, you still have to pass the interviews It should also be mentioned, if it should ever see the light of day, cheating on a MS cert exam (braindumps fall under that category) you will get banned for life from the MS certification program. And that will have some not to nice career implications on the long run.
It's a shame, I don't get why certs even matter anymore. with so many people using brain dumps, no one knows shit these days.
this. also ease of commenting out or insert stuff when debugging
Thanks! Elaborate on the "direct string comparison"... not sure I follow...
your are having yyyymm type formats. That is not a valid date format, so I am assuming that the value is stored as a varchar, or char(6) (which would be minutly better) data type. You can do grater than, small than comparisons between strings. bit of pseudo code, but it makes the explaining easier aa% &lt; ab% = true aa &lt; aa = false aa &lt; aaa = true And so on. Now, the reason why yyyymmdd is a default format in many many places is, that this format will work in string comparison. '1999' &lt; '2000' = true Since the number of years has to be the 1st sort value, than months, than days etc, the yyyy mm dd format will work when you do the date comparison based on string representations of that date in string values. I hope that was comprehensible, please tell if not. To be a bit clearer on the last pseudo code example, that comparison works NOT on converting the strings to integers. It works on string comparison. The not converting is very important in this context 
The part of me that knows there /can/ be a performance issue is from near 20 years experience with software vendors like Oracle, Teradata, Tandem, DB2 UDB and others, that /might/ behave differently based upon a style choice like that. Folks can down vote all they want, but I've seen it in live systems. Oracle and UDB are bad about those syntactical differences.
I would wait on the certification. They are nice to have but they won't be as good as ol fashion experience. Teach yourself explain plans, and pinpoint bottlenecks in the database. Learn how to handle slow queries. Learn how to maximize your indexes. Ask a lot of questions of things you see. My experience with db development is that the structures and procedures are only reviewed when needed. There is always wasteful code that doesn't need to be there. Certification is good for resumes but it really isn't the same as experience.
Thank you very much! I would upvote you more if I could! I will try this out and see if it works.
i would like to refer you to [my comment](http://www.reddit.com/r/SQL/comments/1v52yy/mssql_how_to_query_multiple_rows_to_form_one/cesfycv) its not a while, and it should be neither a while nor a cursor.
No. It does not work that way. The where clause will be fully optimized. That means a inner join to table X, while there is a where clause expression referencing table X will be included in the JOIN operation. I know for a fact that since sql 2005, it does not work the way you outlined. So unless you customers are migration below sql 2000, you gotta really update your knowlagebase
Concatenated into a string is what the O.P. wanted I believe.I could be wrong. Just trying to show how that same thing can be done without recursive code.
I'm simultaneously writing/reviewing a 2012 BI project and rewriting dts packages for 2000 servers. I'm working on best practices here.
your code is way worse than recursivenes as far as I see it
Where did you get that from? I know sometimes stuff gets pushed as best practice that just aint true, but this is so fundemental to how the query engine works.... Btw, produce a test case and an execution plan to support your theory, otherwise its just talk. (and i call bullshit on that one)
When I first started out I used preparing for them as my education about SQL server - they test you over all the aspects of the product, which is good because some companies only use a small subset of the features. You don't have to go to classes - there are some good prep books out there. Get a couple of them and work through all the exercises as you can. The get a practice test. Set up a test SQL server at hone or whatever.
I work in marketing for a data and application management company (I won't be more specific so as to not be accused as shillery) and there are plenty of free resources out there. Sign up for webinars, follow #SQLhelp on twitter, read industry bloggers. SQL, in all its incarnations, has an active and incredibly helpful culture built around it. I was a liberal arts major--prior to this job my only experience with this stuff was running a shit LAMP server from my basement when I was in high school--but I've picked up a lot just due to the exposure. 
You were doing fine until 'MINUS isn't particularly efficient on very large tables, so in that case you'd use a LEFT OUTER JOIN where the fields on mary are null.'
Good: Looks like you're doing just fine on your own. Minor: Add trailing semicolons. Lead the commas. Swap for aliases. Use begin and end with your IFs. Grrr: Table missing keys &amp; indexes. (510) nvarchars??
I may be in the minority here, but I am preparing for MCSA cert for the purpose of seeing how I stack up. I am not going the brain dump route because I want to learn to do the job. I have actually honed my SQL skills by studying for the exam. I really think thats what it was intended for.
Thank you for pointing this out.
This is infuriating.
I guess if you can refute it you down vote it.
I don' have a good beginners introduction to SQL at hand. What I can tell is thou, we do not use tools for a good reason. We use tools for ease of work, nothing else. The thing you need to learn is set based thinking / programming, which is somehat contradictional to procedural or object orianted programming. Do not concern yourself with the tools that are forced on you, they are not the important thing. You need to learn how to think in a set based way, once you got that one down, you won't care about the tool or DBMS used.
Do you thoroughly understand what a database actually is and how it works? Do you plan on using this stuff in the future, or are you just trying to pass a class?
Not 100 percent, but a functional knowledge. I'm trying to get a better understanding Because database management/administration seems to have a lot of job openings. I'm in school for IS now, so I'm trying to find a niche that will guarantee me a well paying job as soon as possible. 
This one is good, but you also have to practice a lot.
His query is for Oracle (dual is the give away). Oracle doesn't use "as" on table aliases.
Thanks! Makes sense!
 public static myQuery(string ID) { dbConnect.sendcommand("select * from whateverDataBase where id = " + ID) } That's a really simple example, and it isn't exactly safe or anything, but I think it shows you would mostly write a query once or twice and then generally just send it the relevant data and it will do the SQL on its own once written.
I really enjoyed the "SQL Queries for Mere Mortals: A Hands-on Guide to Data Manipulation in SQL" , it presents the information in a very accessible format. Also you can find a lot of these in pdf formats if you google around.
If you had a table with A, B, C, then a table with 1,2,3,4, wouldn't a full join concatenating the entries do what you want? It worked for me at least, using below. create table #tmp_a(Col1 char(1)) create table #tmp_b(Col2 char(1)) insert into #tmp_a (select 'a' union all select 'b' union all select 'c') insert into #tmp_b (select '1' union all select '2' union all select '3' union all select '4') select #tmp_a.col1 + #tmp_b.col2 from #tmp_a full join #tmp_b order by #tmp_a.col1, #tmp_b.col2 
In PostgreSQL just do a cross join with the generate_series function. SELECT col || i FROM tab, generate_series(1, 4) AS i ORDER BY col, i;
[Navicat](http://www.navicat.com/) is the closest I have come to finding SQL Server Management Studio for PostgreSQL. The Essentials version will probably meet 90% of your needs, if not more. The Premium version has the ER diagram tool. Note that Navicat has one major flaw (to me, at least): it locks template1, preventing you from creating databases outside of Navicat. I work around this by disconnecting and reconnecting when I need to create a new DB programmatically (e.g. with Rails/rake); but, if you don't frequently create databases in this fashion, you should be fine.
What's wrong with pgAdmin?
Here's an example you could use in MS SQL using a recursive CTE to generate your number table. declare @max int = 5 ;with Numbers (Number) as ( select 1 union all select Number + 1 from Numbers where Number &lt; @max ), SomeLettersButProbablyaTable (Letter) as ( select 'A' union all select 'B' union all select 'C' ) select L.Letter + cast(N.Number as varchar(4)) from SomeLettersButProbablyaTable as L cross join Numbers as N order by L.Letter, N.Number 
try this instead, ;with L0 AS(SELECT 1 AS c UNION ALL SELECT 1), L1 AS(SELECT 1 AS c FROM L0 AS A, L0 AS B), L2 AS(SELECT 1 AS c FROM L1 AS A, L1 AS B), L3 AS(SELECT 1 AS c FROM L2 AS A, L2 AS B), L4 AS(SELECT 1 AS c FROM L3 AS A, L3 AS B), L5 AS(SELECT 1 AS c FROM L4 AS A, L4 AS B), Nums AS(SELECT ROW_NUMBER() OVER(ORDER BY c) AS n FROM L5) SELECT * FROM Nums WHERE nums.n &lt;= @max the reason why you should : your recursive query : (10000 row(s) affected) Table 'Worktable'. Scan count 2, logical reads 60001, physical reads 0, read-ahead reads 0, lob logical reads 0, lob physical reads 0, lob read-ahead reads 0. (1 row(s) affected) SQL Server Execution Times: CPU time = 141 ms, elapsed time = 162 ms. non recursive query : (10000 row(s) affected) (1 row(s) affected) SQL Server Execution Times: CPU time = 16 ms, elapsed time = 14 ms. why would you use a recursion running n times trough the query? are you familiar with the term RBAR? 
As you can see from my reply I do not advocate using a recursive CTE. My PostgreSQL example used a set returning function.
Learn http://en.wikipedia.org/wiki/Set_theory and discrete math. They help. Then http://sql.learncodethehardway.org/ edit: or rather: http://sql.learncodethehardway.org/book/
I wholly agree with this, having had to diagnose complicated queries written in both styles I can say that #2 is vastly more comprehensible than #1. Worse yet is when the two methods are interchanged in the same query.
Thank you, sir. I'll begin reading now.
Cross Joins In MSSQL: DECLARE @TempTable1 TABLE (Letter CHAR(1)) DECLARE @TempTable2 TABLE (Number CHAR(1)) INSERT INTO @TempTable1 VALUES('A'),('B'),('C'),('D') INSERT INTO @TempTable2 VALUES('1'),('2'),('3'),('4') SELECT t1.Letter + t2.Number FROM @TempTable1 t1 CROSS JOIN @TempTable2 t2
All good references from /u/badjuice. Having a world of frameworks to learn as well as SQL will make this an exciting challenge. Remember that the easiest way to eat an elephant is in small pieces; do it a bit at a time. If you have a db to play with, play with it. You are unlikely to break anything and you will get the feel for it better by doing than by only reading. Start simple. Create a table and add a few rows. Get comfortable with restricting and projecting data from it (i.e. get comfortable with selecting only certain rows and columns based on rules that you set). Learn how to count rows, sum values in columns etc. There are crateloads of examples out there on the interwebs. Creating your own tables and data means you know, intimately, how it hangs together. This will help you learn much faster than using anyone else's model; you will be be able to define expected results much more simply during learning/playing - and debugging against your own designs is always simpler. Joins will probably come next. Good Luck and Enjoy!
Thanks man, I really appreciate the tips. I have LAMP installed, so I should just got into MySQL and fuck around for a couple of weeks, try new things, and understand the way my database works? In my limited previous experience with SQL, I found my issue being to try to PICTURE the database in front of me, and how everything looks. Are there any resources you know of that can help me understand from that perspective?
I would start with MySQL if I were you because it is closer to MSSQL in a lot of ways and pretty powerful if you want it to be. There is a ton of information on the web on how to get started especially on their own site. Since you already have a LAMP server set up that is perfect. Before you blindly dive in to it though i would read about Database theory and normalization otherwise you will start off making bad design choices. I tend to get eager but if you want to seriously learn about databases the theory is pretty important stuff.
its iterative, not recursive. And I do not use cursors when I can prevent them. Calling a cursor recursive is plain wrong. And by your standards, I'll take it as a compliment 
First of all, thanks, this sounds like an amazing resource and will give it a shot in a few days. Curious, what does JDBC mean. You kind of lost me at the end there. 
Standford has a free [Introduction to Databases](https://class.stanford.edu/courses/Engineering/db/2014_1/about) class right now. I've had a few years of SQL under my belt but I'm still learning tons! Great info on more than just SQL too, such as JSON, XML, Relational Algebra, etc...
[W3Schools](http://www.w3schools.com/sql/default.asp) has excellent references and tutorials on a number of programming languages, including SQL. Also, pardon the self-promotion, but I also published [a beginner's guide to database design](http://www.amazon.com/gp/product/B00FNAMMA8) a few months ago that you might find helpful. It's primarily aimed at a Windows audience but I use a number of examples to demonstrate SQL joins and database normalization. 
this did the trick
I don't know much about SQL but how does discrete math help? I noticed it mentioned in the coursera course too.
Java Database Connectivity. Hope I'm right, looked it myself as I'm also learning SQL. I'm learning SQL via w3schools.com. They have tutorials for SQL and a bunch of other stuff. Cool thing about W3S is that you can try out the code via in-browser DB. 
Actually it didn't work as it yielded more rows instead of the desired 8. The Table A.TERM_CODE_EFF is an "Effective Date/Value". It should restrict what's returned in the result to obtain the desired 8 rows. As long as the TERM_CODE_EFF is less than those of Table B.Term_Code those results should be returned. 
By changing the "&gt;=" to a "&lt;=" I get it down to 13 rows but still duplicating rows for each of the A.TERM_CODE_EFF for both "201301" and "201305"..
http://en.wikipedia.org/wiki/Discrete_mathematics Discrete math is essentially the math of countable objects and sets. Essentially, it's all about how you would quantify, relate, qualify, sort, or iterate any sort of data set. It is the math of patterns, in essence. More to the point, almost all of information theory and computation sciences relates to the type of math inherent in discrete math. Remember, there are no decimals or words in computers, in reality, they are all represented/coded into discrete 1's and 0's; items that are on or off/charged not-charged/signal or no signal/pits and bumps/etc etc. There is no 'real' storage of a 0.2 value- that's just a value assigned to a binary representation; to store a 0.2, you would need an analog medium (which does exist), and to truly process it, you would need an analog computer (which does exist). Also; it's a lot more theory and a lot more 'idea-space' than it is calculations, in my experience; the 'math' of discrete math is very easy; most of it you will learn in set theory (which is a subset of discrete math, in a way) 
#####&amp;#009; ######&amp;#009; ####&amp;#009; *Here's a bit from linked Wikipedia article about* [***Discrete mathematics***](http://en.wikipedia.org/wiki/Discrete%20mathematics) : --- &gt;**Discrete mathematics** is the study of mathematical structures that are fundamentally discrete rather than continuous. In contrast to real numbers that have the property of varying "smoothly", the objects studied in discrete mathematics – such as integers, graphs, and statements in logic – do not vary smoothly in this way, but have distinct, separated values. Discrete mathematics therefore excludes topics in "continuous mathematics" such as calculus and analysis. Discrete objects can often be enumerated by integers. More formally, discrete mathematics has been characterized as the branch of mathematics dealing with countable sets (sets that have the same cardinality as subsets of the natural numbers, including rational numbers but not real numbers). However, there is no exact, universally agreed, definition of the term "discrete mathematics." Indeed, discrete mathematics is described less by what is included than by what is excluded: continuously varying quantities and related notion ... `(Truncated at 1000 characters)` --- [^(**Picture**)](http://i.imgur.com/1L61djb.png) ^- **^Graphs ^like ^this ^are ^among ^the ^objects ^studied ^by ^discrete ^mathematics, ^for ^their ^interesting ^mathematical ^properties, ^their ^usefulness ^as ^models ^of ^real-world ^problems, ^and ^their ^importance ^in ^developing ^computer ^algorithms.** [^(image source)](http://commons.wikimedia.org/wiki/File:6n-graf.svg) ^| [^(about)](http://www.reddit.com/r/autowikibot/wiki/index) ^| *^(/u/badjuice can reply with 'delete'. Will also delete if comment's score is -1 or less.)* ^| ^(**Summon**: wikibot, what is something?) ^| [^(flag for glitch)](http://www.reddit.com/message/compose?to=/r/autowikibot&amp;subject=bot%20glitch&amp;message=%0Acontext:http://www.reddit.com/r/SQL/comments/1voi8u/best_way_to_go_about_learning_sql/cev43tu)
select --distinct top 100 a.company,a.season,a.STYLE, STUFF(( select distinct ', '+ b.ERPCOLORCODE from products as b where b.company = a.company and b.style = a.style and b.season = a.season and b.SEASON &gt; '2014' order by ', '+ b.ERPCOLORCODE FOR XML PATH('')),1,1,'') ERPCOLORCODE, STUFF(( select distinct ', '+ c.SIZENAME from products as c where c.company = a.company and c.style = a.style and c.season = a.season and c.SEASON &gt; '2014' order by ', '+ c.SIZENAME FOR XML PATH('')),1,1,'') SIZENAME from products as a where a.SEASON &gt; '2014' group by a.company,a.season,a.STYLE having COUNT(*) &gt; 1 
It doesn't make a difference on Oracle. I've tried it with several examples, with both the cost based optimiser and the old rule based optimiser and the explain plan is exactly the same.
Indexes matter, and version. Oracle 7.3 &amp; Oracle 8 were horrible with a difference like this.
Sure. Just create the proper joins to the relevant tables and execute.
I've used SQL and SP independtently but never together. Assuming that I've properly built a query that will produce the customer's last name, how do I connect it to the SP form so that it queries my db for the loan number, captures the last name, and populate the "last name" field in SharePoint?
BANNER COLLEGE. WE HAVE A BANNER COLLEGE. Commenting to remember you're here. Snow is about to close us down and I'll look at it later.
Correcting the query above: SELECT b.* FROM SGBSTDN a INNER JOIN SHRTGPA b ON b.SHRTGPA_TERM_CODE = a.SGBSTDN_TERM_CODE_EFF AND b.SHRTGPA_LEVL_CODE = a.SGBSTDN_LEVL_CODE This only returns 4 rows based on Table A. It doesn't return the rows between where "SHRTGPA_TERM_CODE" = 201105, 201201, 201208, 201308
This should do it. SELECT DISTINCT sgbstdn_pidm, MAX(sgbstdn_term_code_eff) term, sgbstdn_levl_code, b.* FROM tablea a JOIN tableb b ON sgbstdn_pidm = shrtgpa_pidm AND a.sgbstdn_levl_code = b.shrtgpa_levl_code AND a.sgbstdn_term_code_eff &lt;= b.SHRTGPA_TERM_CODE GROUP BY shrtgpa_term_code, sgbstdn_pidm, sgbstdn_levl_code, SHRTGPA_PIDM, SHRTGPA_TERM_CODE , SHRTGPA_LEVL_CODE, SHRTGPA_HOURS_EARNED, SHRTGPA_GPA_HOURS, SHRTGPA_QUALITY_POINTS order by SHRTGPA_TERM_CODE
WORKS PERFECTLY!!! THANK YOU, THANK YOU!
Thanks for the reply. It's really more of an "autofill" that I'm trying to accomplish, not a validation kind of thing. 
Let's FusionIO all over this bitch!
&gt; These column names make me want to puke. i feel your pain, but we are often tasked with working on shit we did not create 
From my experience, Sharepoint doesn't really like the scenario you are describing. If you want to click a button and run custom code, you are encouraged to create a sharepoint app with visual studio. Sharepoint can also interact directly with a database table by creating an External List. Unfortunately external lists are scaled down versions of SP lists and don't have as much functionality. For that reason I decided just use SSIS to populate sharepoint lists and call it a day. 
JDBC - Java DataBase Connectivity. It's a means of connecting programs to databases from many different vendors using a common set of techniques. Iguess it's similar to the Microsoft-developed ODBC that has been around for ages now, but developed in, and designed for Java use. 
Everyone knows that SHRTGPA stands for student shortened GPA. SGBSTDN isn't even the table; it's the view. SGASTDN is the table. Woof. I hate this stuff.
Thanks for the reply. I work for a very big company and we have millions of customers. Would an external list be able to house that much data? 
how do you know your sql server environment is ready for ssd.... mhhm how about, as soon as you have an sql server environment ? tempDB on ssd, have it once, never go back.
Really? Recommending moving sequential logs to SSDs? I stopped reading that that point.
If they're a bottleneck, it does makes a lot of sense. When you have more than 1 database's logs writing to a drive, it's no longer sequential. An example here is the Stack Overflow database *could* run on spinnys for the T-Log, but it's a bottleneck to do so. Compare that to our other main SQL instance with 200+ databases, it's already an absolutely terrible fit - that one's effectively random access. Now take even the single-database-per-drive example in Stack Overflow's case, your **writes** may be sequential, but that's not the only thing happening is it? We're reading T-Logs for backups and Availability Group replication at a minimum, that activity competes for the head on a spindle and delays your writes. We use SSDs for data and logs here, and with SSDs it means we don't have to make physical/RAID decisions about volume splits. Instead, we can use one large RAID 10 volume and dynamically decide what needs space accordingly...you just can't do that without SSDs. With spindles you have to make choices about growth/allocation early and be locked into them unless you want to rebuild some arrays in most cases.
Great response nickcraver. Do you mind if I leave these awesome articles here? http://nickcraver.com/blog/2012/02/07/stack-overflow-short-on-space/ http://nickcraver.com/blog/2013/11/22/what-it-takes-to-run-stack-overflow/ 
Oh, I actually haven't done it yet so I don't know. Sorry! I am waiting to have a new dev environment built that has all the software I need. I'm manually updating the list with Quick Edit which is clumsy and time consuming :( But I'm the downloadable SSIS destination works pretty well: http://sqlsrvintegrationsrv.codeplex.com/releases/view/17652
Contact your MS Service Rep before doing anything - direct interventions like this have a nasty habit of voiding your support contract.
Check out [uCertify](http://www.ucertify.com/). I used them to prepare for 98-364 and if anything, it was overkill. You won't need any other books or courses. Good luck.
Thanks.
I just used measureups exam i went through all 150 question twice and did exam mode 5 times and i passed when i took the real exam 
I second this. At first uCertify frustrated me because of some of its questions that were presented intentionally confusingly, but it really does a good job of preparing you. Often, I walked in to the exams nervous and breezed through them due to the uCertify tests being so much harder. 
Working on formatting this for you guys.
This is me building a temp table to try it on... IF OBJECT_ID('tempdb..#Item') IS NOT NULL DROP TABLE #Item CREATE TABLE #Item ( ID int, Label varchar(100) ) INSERT #Item (ID, Label) SELECT 100, '***Food' INSERT #Item (ID, Label) SELECT 101, 'Dairy Queen' INSERT #Item (ID, Label) SELECT 102, 'McDonalds' INSERT #Item (ID, Label) SELECT 200, '***Retail' INSERT #Item (ID, Label) SELECT 201, 'Macys' INSERT #Item (ID, Label) SELECT 202, 'Navy' INSERT #Item (ID, Label) SELECT 203, 'Express' INSERT #Item (ID, Label) SELECT 300, '***Service' INSERT #Item (ID, Label) SELECT 301, 'Great Clips' This is the actual data pull... ;with cteCategory AS ( SELECT ID, Label as Category, ISNULL(LEAD(ID) OVER (ORDER BY ID),99999) as LeadID FROM #Item WHERE LEFT(Label,3) = '***' ) SELECT it.ID, it.Label, ct.Category FROM cteCategory ct JOIN #Item it ON it.ID BETWEEN ct.ID + 1 AND ct.LeadID - 1 ORDER BY it.ID Here are the results... ID|Label|Category 101|Dairy Queen|***Food 102|McDonalds|***Food 201|Macys|***Retail 202|Navy|***Retail 203|Express|***Retail 301|Great Clips|***Service
Already posted how I would do it. Just a side note. This is a HORRIBLY designed table. A denormalized version should be: CategoryID|Category|ItemID|Item A normalized version would have an Item table that has CategoryID in it pointing to a Category table with the CategoryName in it. I realize you may have no control over this, but I just *cringed* when I saw it and thought I may as well throw this out there.
Agreed. That's why I'm normalizing it in a target model.
http://i.imgur.com/rTapVPB.gif
Bad idea for querying the SQL Server. All data for sharepoint is put into 1 table, and it isn't the easiest to pull the specific data needed. This is more of a /r/Sharepoint or /r/Sharepointdev question. But I'll give it a shot, as i wear both hats. Option 1: Have a workflow run after data is inputted that puts in specific information. So either don't include the fields as viewable, or have them viewable but include a message saying that if left blank, then it'll fill with most recent information. Then have the workflow check each field for content, if different, replace, if same or blank, do nothing. You could even create an email to notify the user of what was changed. I'd also recommend having a hidden field/flag that shows the most current information. Option 2: If the customer info is in a separate table, than you can do this without a workflow, you can create a lookup field to do this. Option 3: You can use VBScript to pull the data from relevant places. You can actually grab it all from the XML which is constantly being published. I believe it's the REST stuff. 
Yup - I do DBA and reporting work on a fashion ERP system (that I didn't create). Every single one of the table names are eight characters, with shining examples such as ZZXCUSTR, ZZOORDRH, ZZHCTNSA, etc. Three to four hundred tables, with an incomplete table name taxonomy sheet. It's starting to make sense now, but the last year has been painful.
I understand. Banner has 4300-ish tables and 1200-ish views, and approximately 17 make sense.
SELECT DISTINCT * FROM table WHERE column2 in ('1','3')
What does Periscope do? Can I jump the queue to get into the beta?
cchristy's query will not do what you are asking. I just whipped this up. I think this is what you are looking for: create table tbl_foo(theletter VARCHAR(5), thenumber integer); INSERT INTO tbl_foo(theletter, thenumber) VALUES ('A', 1); INSERT INTO tbl_foo(theletter, thenumber) VALUES ('A', 2); INSERT INTO tbl_foo(theletter, thenumber) VALUES ('A', 3); INSERT INTO tbl_foo(theletter, thenumber) VALUES ('B', 1); INSERT INTO tbl_foo(theletter, thenumber) VALUES ('B', 3); INSERT INTO tbl_foo(theletter, thenumber) VALUES ('C', 1); INSERT INTO tbl_foo(theletter, thenumber) VALUES ('C', 4); /* Earlier responders's query does not work */ SELECT DISTINCT * FROM tbl_foo WHERE thenumber in (1,3); theletter | thenumber -----------+----------- B | 3 A | 1 C | 1 A | 3 B | 1 (5 rows) /* The following works */ SELECT DISTINCT tbl_foo.theletter FROM tbl_foo INNER JOIN tbl_foo tbl_bar ON tbl_foo.theletter = tbl_bar.theletter WHERE tbl_foo.thenumber = 1 AND tbl_bar.thenumber = 3; theletter ----------- A B (2 rows) 
I think your best option is a subqueries...so: SELECT * FROM (SELECT Letter FROM Table1 WHERE Number = 1 ) tbl1, (SELECT Letter FROM Table1 WHERE Number = 3 ) tbl2 WHERE tbl1.Letter = tbl2.Letter 
your code gets a bit cut-off... where would you work the 1 and 3 into it? 
Double checked it just to be safe. In SQL Server 2012 they generate the exact same plan.
 SELECT distinct theLetter FROM tbl_foo f WHERE EXISTS ( SELECT 42 FROM tbl_foo fn1 WHERE f.theLetter = fn1.theLetter AND fn3.theNumber = 1 ) AND EXISTS ( SELECT 42 FROM tbl_foo fn3 WHERE f.theLetter = fn3.theLetter AND fn3.theNumber = 3 ) Distincts are notoriosly expensive, so if you can, aviod them. The Exists clause might look slow, but a decent optimizer will be able to create a much better execution plan (also depending on existing indexes and constraints of course), than by doing a data retrieving subquery. Note that I said decent optimizer, I usually don't have high regards to the MySql one... Worth doing some performance testing thou. You also might get better answers for your problem, if you give us the details of what you are having problems with. The data model matters quite a bit.
From the ASP.NET comment, I'm assuming you're running SQL Server. Do you have sharepoint?
&gt; Double checked it just to be safe. In SQL Server 2012 they generate the exact same plan. also in 2005, 2008 and 2008r2. Guess we know why they used postgree as an example. I doubt it would be reproducable on oracle as well, but there I have no test environment ;)
&gt; zero time That's a recipe for a security nightmare. In my opinion, your first step is to resolve the time issue. Find time. Hire someone. Demand a budget for this or don't do it. Once that's done, then you can worry about the SQL syntax. Any suggestions you find in this thread that are fast and easy are going to lack security.
Good point. We use an Oracle server, but I've just remembered we have Application Express enabled on the server. We have a guy that works with it, so maybe I could go and see him.
you need to include all columns, that you do not aggregate in the group by clause. SELECT MAX([database].[item1]) AS [item1] ,[database].[item2] AS [item2] ,[database].[item3] AS [item3] ,[database].[item4] AS [item4] FROM [mfgdeptsql].[database] [database] GROUP BY [item2], [item3], [item4] (you are also missing a few commas, that would be the next error) To clearify the lingo a bit. Your problem is not the connection to the database, that works just fine (you get an error returned from the DBMS). You problem is in a query to said database. You are also not "accessing the whole database", its just a table in a database. Not a big issue, but it helps to use the correct language, it prevents confusion.
Thank you very much! That worked perfectly.
if its an option, I would rather use MSSQL than MySQL. By a good percentage of database professionals (myself very much included), Mysql is considered as a "well, for your smalltime webforum sure, but...." DBMS, while Mssql is heavyly used in professional and enterprise environments. If you need an open source DBMS, SQLITE and Postgree are good options. 
Oh sorry. I'm doing an IT apprenticeship, and as part of that I need to "Design and create forms to access, enter, edit and organise data in a data base." I've got free reign as to what I use, AFAIK, with the main limit being what I can learn around doing my job and when I'm at home. I was given the task, some sample data and what evidence I need and told to hop to it, more or less. I haven't looked much into SQL or anything much to do with databases, so I wasn't sure what all my options where. Also, thanks for the quick reply.
the version is not important, you are not going to do anything that will be impacted be the express edition. Lets backtrack a little bit here thou. Why do you want to create forms to start with, whats the project? I sadly know of no easy to use generator for data input forms to a database. The database system itself thou, is completly independant of the frontend. You can write java to create a gui to access the database backend, or php, or C#, really anything. If you want a frontend to more easyly work on the database itself, without the need to have a user interface, sql managment studio should work well for you. Its included in the SQL Server, and it offers you some forms of sort, to edit table content without writing everything out in sql.
Based on your syntax and error message, you probably meant MS SQL. In this case, it won't matter, but for future problems, getting that right will help people help you. 
Given these requirenments, well, what follows is highly biased of course. I would write up a small .Net app, since it's a decent programming environment, and visual studio is awesome. Its really easy to create a GUI and some forms with it. For backend I would use SQL Server Express 2008r2 or higher. Since its Express, 2012. When writing the App, I would however self implement the DB layer, the (could be static) class that does the inserts, selects, updates. For learning purposes, I would definitly do that. For personal reasons, I don't trust the data binding stuff, I'm a DBA thou, I like having control over what gets dumped how into my database. As a learning project, I would not use ASP, since thats imo harder to get into, and I would not use PHP (since it sucks (personal opinion)), and in an apprenticeship, you should learn programming. Object orianted programming is the name of the game, and PHP / Javascript, are not the ideal languages for that.
Thank you for clarifying. I know I'll run into something in the future that I'll need help with.
Executing unfiltered sql code to a production hospital database? I might not recommend that course of action. 
This is not some clever strategy, this is a workaround for a known flaw in the PostgreSQL optimizer. PostgreSQL does not consider hash aggregation of DISTINCT aggregates, 
Of course it wouldn't, this is a PostgreSQL specific planner flaw. I have yet to encounter a perfect planner. In some databases they might all generate the same bad plan though.
Just install wordpress and use a simple form plugin. No coding, somewhat secure. Done.
Sandman might be what you're looking for. It's an Python based front on all kinds of databases. Have a look at https://github.com/jeffknupp/sandman/blob/develop/README.md.
Long time wordpress dev here. Drupal definetely has some better solutions to no-code form building.
Word. Yeah whatever gets the job done!
Sure, you can just select whatever text you want to go with it. For instance, if I have a column for server names and server locations I can do the following, assuming they're all strings: SELECT 'The server ' + serverName + ' is in ' + serverLocation FROM servers The result will be, for example, strings that say: The server DC01 is in Tampa, The server DC02 is in New York etc. Note those spaces I put in before and after the text I want to add, those are needed for formatting, they wont be added automatically. If the values you're working with aren't strings, you can cast them as such using the CAST function in most RDBMS. You can also use a CONCAT function instead of +s.
On phone so I can't easily get the links but for open source you can try pentaho. I don't have much experience personally but I have heard good things. 
That's a pure SQL solution (which makes sense in r/SQL) however I prefer to retrieve the data from the database without the extra text and add it on afterwards. For example: select serverName, serverLocation from servers ... print "We have found that " . $serverName . " which is running in " . $serverLocation . " is ready for a reboot"; print "If you feel that " . $serverName . " should be away from " . $serverLocation . " blah blah blah..."; This way, you can use it multiple times in multiple different sentences however you need to.
Pentaho has Kettle which is free. But its kind of just like...janky? Its like apples to oranges in my opinion. Microsoft products have just an army of support/tutorials. Pentaho works on multiple platforms, but is just kind of janky. Aka, not much discussion/support/training
I can't disagree with you there. All the open source platforms have a business model of give away the software then charge through the nose for consulting. However, it can do the same thing as SSRS and SSIS just figuring out how is the hard part!
Nobody?
My employer has gone from best in class to worst in class. They went from Cognos for a reporting solution, this was widely popular, but some rogue SSRS servers started showing up in the corporation. Now they migrated from Cognos to Pentaho, and are using Kettle to build a cube - but only one cube for finance. So they just use the report server a ton for and Pentaho Report Designer for everyone else. And then we just ask for views to be created so we report on views instead of hitting up live tables. No one else in the organization has any cubes or ability to create any self-service BI offerings. The best thing available is some Access reports, or ask us to create some report for them. This is an incredible pitfall of where I work, our clients expect this kind of functionality.
if you got a backup of your database, make a backup of your wordpress files also. 
Theme files, plugins, images and other file uploads are stored in folders, not the database. To backup your files, the wp-content folder is enough. The rest of it you can donwload at the wp site anytime. 25MB is actually a large WP database, you must have hundreds of posts.
You caught me! I spend a lot of my time on T-SQL
Very welcome, happy to help.
If this is MySQL you can set it so that it ZIPS and emails you the DB on set times and dates. Look into that. A SQL file can compress a lot.
Why would you not use the MS SQL suite of applications?
Here are my two queries to find the average height and standard deviation of each gender: SELECT Avg(Actors.Height) AS [Male Avg Height], StDev(Actors.Height) AS [Male StDev] FROM Actors HAVING (((Actors.Sex)="M" And (Actors.Sex)="M")); SELECT Avg(Actors.Height) AS [Female Avg Height], StDev(Actors.Height) AS [Female StDev] FROM Actors HAVING (([Sex]="F") AND ([Sex]="F"));
Let me re-write the question for you to see if it gives you a nudge in the right direction: List the male and female actors WHERE they are more than one standard deviation taller than OR shorter than the average height of their respective sex. You should be able to get your two sets within the WHERE clause of one query. And I would focus less on gender and more on comparing the standard deviation to the average. If you do that, the rest should fall in line accordingly.
What kind of problems are you struggling with? Is there a specific area that you find the hardest? 
Here are some example of the problems I'm given Write a SQLstatement to display all columns of all rows of PET. Do not use the asterisk(*) notation. Write a SQLstatement to display all columns of all rows of PET. Use the asterisk (*)notation. Write anSQL statement to display the PetBreed column of PET. A. (4 points) Write SQL CREATE TABLE statements for ITEM, SALE, and SALE_ITEM. B. (4 points) Write foreign key constraints for the relationships in each of these tables. Make your own assumptions regarding cascading deletions and justify those assumptions. (Hint: You can combine the SQL for your answers to parts A and B) C. (4 points) Write SQL statements to insert a row of data into each of these tables. 
I posted some examples in another comment of questions in my homework. But I'm struggling with it in general I feel clueless when I look at the problems.
Here are some example of the problems I'm given Write a SQLstatement to display all columns of all rows of PET. Do not use the asterisk() notation. Write a SQLstatement to display all columns of all rows of PET. Use the asterisk ()notation. Write anSQL statement to display the PetBreed column of PET. A. (4 points) Write SQL CREATE TABLE statements for ITEM, SALE, and SALE_ITEM. B. (4 points) Write foreign key constraints for the relationships in each of these tables. Make your own assumptions regarding cascading deletions and justify those assumptions. (Hint: You can combine the SQL for your answers to parts A and B) C. (4 points) Write SQL statements to insert a row of data into each of these tables. 
 select * from actors A join (select sex, avg(height) - STDEV(height) 'stdev1', avg(height) + STDEV(height) 'stdev2' from actors group by sex ) b ON a.sex=b.sex WHERE a.height &lt; b.stdev1 OR a.height &gt; b.stdev2 
This is really fundamental stuff that you should learn. If you are having problems having the answers provided won't help you, it will set you back. Here's a hint for the first part *Write a SQLstatement to display all columns of all rows of PET. Do not use the asterisk() notation* That would read like select columnone, columntwo, columnthree from PET If there were three columns. Instead of writing the columns you could use * but that returns everything from that table (and any joined tables).
I fully understand that something new like SQL or Database design can be daunting at first. Thankfully, you should have a required textbook for your class that would provide you really good examples to work from. Also the internet will have more examples than you will ever need. One thing that you will probably not get on this group is the answers to homework, but you will get help hopefully in the right direction. So lets see if I can help. &gt; Write a SQLstatement to display all columns of all rows of PET. Do not use the asterisk() notation I do not know what all the columns are in the PET db, but since you can not use the * notation then you would just do something like this Select Column1 ,Column2 ,Column3 FROM PET; Where you would list out each of the columns. &gt; Write a SQLstatement to display all columns of all rows of PET. Use the asterisk ()notation. This is one question where I will actually give you the answer because, well to be frank, this should be the first thing you learn in SQL. Select * FROM PET; &gt; Write anSQL statement to display the PetBreed column of PET. The solution to this should be very straight forward since you already did this in the first question you posted, but instead of all of the columns just select the ones the question is asking for. ----------------------- For the next couple questions, the syntax can change based on your DB. Lets assume you are using Oracle. &gt; A. (4 points) Write SQL CREATE TABLE statements for ITEM, SALE, and SALE_ITEM. B. (4 points) Write foreign key constraints for the relationships in each of these tables. Make your own assumptions regarding cascading deletions and justify those assumptions. (Hint: You can combine the SQL for your answers to parts A and B) C. (4 points) Write SQL statements to insert a row of data into each of these tables. There seems to be something missing here, I am guessing they provided you what columns should be in those three tables. So to get started, check your text book or at least this site: Part A: [http://www.techonthenet.com/sql/tables/create_table.php](http://www.techonthenet.com/sql/tables/create_table.php) Part B: [http://www.techonthenet.com/oracle/foreign_keys/foreign_keys.php](http://www.techonthenet.com/oracle/foreign_keys/foreign_keys.php) Part C: This is really straight forward, you can use this link for an example: [http://www.techonthenet.com/sql/insert.php](http://www.techonthenet.com/sql/insert.php) I hope this is going to lead you in the right direction.
To elaborate on this. When I hit a query I'm having trouble with I break it into numerous queries and then figure out how to link them. In this case I would - a. list all male/female actors b. a + where they're 1 or more deviation from average. c. a + shorter than average height of their sex. I'd probably even break it down further like C I would do one for male and 1 for female. It's good this way (I find) because I can validate all the subqueries and see how it gets to the final of what I need. Means if you find an issue you'll know there's an issue because you know all the results from the components. You wouldn't believe how many times I've done a query and get results of x and then subquery it and get a different result even though it should be the same result.
Are you looking for something platform specific or ANSI Standard? How is the data being consumed? A lot of presentation tools like SSRS, Crystal, etc let you specify data types which will prepend a currency symbol, either explicitly declared or according to your regional settings. This can be useful if you want to have the ability to modify numeric data further without having to convert it back from character data that would arise from the string concatenation method.
homework?
Yup
I think I'm close based on this. I broke things out better in two queries: 1st Query: SELECT Actors.Sex, Avg(Actors.[Height]) AS [Avg Height], StDev(Actors.[Height]) AS [St Dev Height], [Avg Height]+[St Dev Height] AS [One St Dev Taller], [Avg Height]-[St Dev Height] AS [One St Dev Shorter] FROM Actors GROUP BY Actors.Sex; 2nd Query: SELECT DISTINCT Actors.[First Name], Actors.[Last Name], Actors.Sex, Actors.Height FROM Actors INNER JOIN Query8 ON Actors.Sex = Query8.Sex WHERE (((Actors.Height)&gt;[One St Dev Taller])) OR (((Actors.Height)&lt;[One St Dev Shorter])) ORDER BY Actors.Sex; But when I run it, I get a couple of results outside of the standard deviation range still
I'd like to thank you again. I'm blazing through my problems now.
It's all good. Keep it up. Many jobs in IT love for their employees to have some sql and DB knowledge. It can really make you marketable down the road. 
I would suggest installing SQL Server Express w/tools (it comes with SQL Server Management Studio), then read up on ADO.net for connecting your Visual Studio project to your DB.
I've always been sort of a stranger to sql. But it's not like I am a noob. Lately I found myself in a situation where I have to gain solid knowledge if I want to grow careerwise. I downloaded a couple of books and can tell you without a doubt that O'Reilly's Mysql Cookbook is awesome. The examples the author thinks of are really down to the nature of set theory. Even though they are not real world examples (which can be really abstract), they relate to everyday experience. I would definetely recommend it.
But do they help with getting started, install/linking to projects?
Oh, not really. For this you should google some tutorials...
I have, and I haven't found anything useful. It all seems to be directed towards people who already know how to do this stuff. That's why I made this post.
Have you been able to spend much time around the DBAs in your position? I have found DBAs are pretty open to educating anyone with a genuine interest in what they do, since a lot of people don't understand or care. Look up Brent Ozar's blog post about landing a DBA job. I remember he mentions requesting to shadow DBAs when they work on the weekends for experience, among other things.
i made a similar transition earlier this year. Since I didn't know where to start I started with the SQL 2008 DBA Certification. Just taking the practice tests exposed tons of stuff I didn't know I didn't know. Thats half the battle, figuring out how deep the subject can be. I ended up paying for a CBT Nuggets subscription as their video series is pretty decent, and it has both major testing providers included, which was pretty good at gauging how i was doing on retaining the knowledge. I was lucky to have landed a position at as a production DBA at a Fortune 1000 Financial firm and have great resources around me for continued learning.
An excellent book which taught me the DBA skills to compliment my developer background was [PostgreSQL 9.0 High Performance](http://www.packtpub.com/postgresql-90-high-performance/book). It is aimed at PostgreSQL users but some of the information in the book is universal.
www.brentozar.com
That's what joins are for. Imagine your *table* is called p, and you have another table, say p2, you can then do things like Select * From p join p2 on (p.name=p2.name and p.email=p2.email and p.phone=p2.phone) Now, the semantics may not quite work (say, what happens if two people have the same name, or the same person gives you two different emails) which is why we usually normalize our data, but you can do these things with SQL
I've downloaded and installed SQL Server Express. Thanks for the suggestion by the way, because this was the first database with an installation that I actually understood! Once I've finish the installation, is there a way to create a new database and start editing the way you can in Base of Access? ETA: Looks like I have to do it in the SQL Server Management Studio...I think. I'm gonna fiddle around in here then look at some docs and stuff.
Do you want to transition into a dba role? One natural progression of a sql developer is into statistical analysis. That is essentially the jump I have made. Worth looking into if focuaing on numerics is a strong point for you. 
Yeah you are on the right track. What is also cool is that with SQL Server Management Studio you can also import excel/txt etc files into a database/table! You can also write/test the queries there. I'm not very experienced but you can give me a shout if you need help. 
Could you provide some background on what your project is attempting to accomplish? That would help with a database selection. SQLLite is a great place to start. It's very light weight, is mostly SQL standard compliant, and is easy to include with your project if you are going to distribute it with others. For a more robust, but server/site specific, I'd recommend PostgreSQL. It's very feature rich, relatively easy to install on any platform (Windows, Mac, Linux), and open source so it's free to download and use. Given that you are using VS Express, I'm assuming you'll either be writing C# or VB.NET projects. I found a CodeProject walk through around using PostgreSQL within a C# application. http://www.codeproject.com/Articles/30989/Using-PostgreSQL-in-your-C-NET-application-An-intr
Apparently it's a workaround for a known flaw in PG's planner.
&gt;Given that you are using VS Express, I'm assuming you'll either be writing C# or VB.NET projects. I found a CodeProject walk through around using PostgreSQL within a C# application. The project I'm working on is C++ actually. &gt;Could you provide some background on what your project is attempting to accomplish? That would help with a database selection. Of course, I didn't realize how different databases were that knowing about the project would help to select one. I'm working on a program that simulates a subset of a video game called League of Legends. I wanted to use the database to store the information of the 'heroes' and 'monsters' as well as a few things that modify them.
which database are you using?
I'm assuming he's using MySQL
Well, that's not an "age" column. But you can do this with a CHECK() constraint. There are other ways. alter table public.student add constraint your-constraint-name check( gender in ('M', 'F') ); You also need a NOT NULL constraint on that column, but you've already done that.
AND count(l.borid) = 0 
This is a pathway I'm considering. Right now I'm a business analyst who runs sql queries and writes a few of my own. Then I do stats analysis in Excel with the results. All of this is in a finance department so there is that slant to the things I do. For someone with no SQL certs, and basically only query knowledge (moderate), what are the steps to become a DBA? Do people need to have the knowledge of SQL developer first? What kind of salary can someone expect along the way and including the final destination?
Add HAVING COUNT(l.borid)=0 after the Group By clause.
aggregates don't work like that... If you want to filter on an aggregate value it must be in the HAVING clause or alias the column and filter from a wrapper query. 
Because you have not supplied data/tables I am making some assumptions in this answer. When looking for the absence of a value(aka no records linking borrowers to a publisher) a traditional inner join will not help you. All of the records you are looking for will be thrown out because they do not satisfy the "pubname = 'Mammoth' condition. In order to have records included that have no linking record to mammoth, you will need to use an outer join or a not exists statement. I am going to assume you need all of the tables you have included in your example query and since it is joining to tables that I assume are at a more detailed level than borrower I would use a not exists clause. I have not tested this query at all, but this should point you in the right direction. SELECT b.borname FROM Borrower b WHERE not exists( select 1 from Loan l, BookCopy bc, BookTitle bt, Publisher p where p.pubname = 'Mammoth' AND b.borId = l.borId AND l.bcId = bc.bcId AND bc.isbn = bt.isbn AND bt.pubId = p.pubId ) ; 
Looks like they posted a follow-up: https://periscope.io/blog/count-distinct-in-mysql-postgres-sql-server-and-oracle.html Think they're missing an index?
I'll look into these videos more. Thanks for the link!
I've not actually considered statistical analysis. Are you more or less referring to data mining?
CBT Nuggets seems like a fantastic resource. It seems like the best bang for your buck with the subscription prices. Thanks for the mention!
Might be a copy paste error but surround the query with quotes instead of just typing it out? That way it treats it as a textual value instead of a statement.
Yes, sorry that was just a copy paste issue. The = starts with `SELECT and ends with like '%succ%')` Reddit still removing start and and single quotes, they're definately there.
Judging by the way this was formatted, I think you're using the wrong character to quote the value. Use the guy that's on the double quote key, not the one with the tilde. Any quotes inside the string need to be escaped with a double single quote, so ' would turn into '' which is the same character twice, not the double quote character.
So... update querytable Set QueryExpression = "SELECT... '%succ%')" ?
 UPDATE querytable SET QueryExpression = ' SELECT SMS_R_System.ItemKey, SMS_R_System.DiscArchKey, SMS_R_System.Name0, SMS_R_System.SMS_Unique_Identifier0,SMS_R_System.Resource_Domain_OR_Workgr0, SMS_R_System.Client0 FROM System_DISC AS SMS_R_System JOIN ClientOfferStatus ON SMS_R_System.ItemKey = ClientOfferStatus.ItemKey JOIN OfferStatusInfo ON OfferStatusInfo.MessageID = ClientOfferStatus.LastStatus WHERE ClientOfferStatus.OfferID = ''SMS20284'' AND OfferStatusInfo.MessageStateName LIKE ''%succ%''' WHERE CollectionID = 'SMS88773' Every quote in there is a single quote.
Wow, that worked. I still don't understand why "any quotes inside the string need to be escaped"?
How else is the interpreter supposed to know that a single quote means a literal quote character as part of the string and not the end of the string declaration?
You do not have to give names to your constraints. In this case the below code would be sufficient. alter table student add check ( gender in ('M', 'F') ); 
I guess I assumed it was smart enought to know that 'Select will start the string and '%succ%')' would end it. I mean the select query is fine without the quotes, the udpate query is fine with Set queryexpression = 'g' as a text value, so I didn't think it was any different.
How could it possibly know that? Consider the following bit of a where clause: AND something = 'blah' AND SomethingElse = 'blahblah' How do you know what the correct value for the equality is without explicitly escaped quotes? It could be this: AND something = **'blah' AND SomethingElse = 'blahblah'** Or this: AND something = **'blah'** AND SomethingElse = **'blahblah'** 
really hard to tell without a reproducable query. But 2 table scans, does look like a missing index. So if you guys are listening, post your testing scripts. Thats the new gauntlet being thrown. Also I would like to add, that at least query3, on production code (a bit more complex usually), those nesting nested queries, can bite one in the behind hard. The more complex the sql, the harder a time you give the optimizer. And factor 2 is not that much really, at least when we talk forcing execution plans, factor 2 is quite fine to leave naive. After all, data changes, and if its not a critical query, let the optimizer sort it out in that case I would say. Edit: did you guys run on cached plans? cold cache or preset cache? Heap tables or Clusters? If Cluster, what CI? All those factors to consider .... ;)
Set queryexpression = 'g' seemed the same as Set queryexpression = 'some sql query' if you ask me.
It's all fine and good until a quote character shows up in your string. When that happens, you need to tell the interpreter to treat that character as part of the string instead of the special character that signifies the end of the string.
When you say that quote character shows up, it could be anywhere in the entire string? So even the one way down in 'SMS20284'? Does it then matter how many i have in a string? 
It's a SQL analytics &amp; dashboarding tool that's super duper fast because of some cool statistical sampling and in-memory caching techniques. Shoot me an email to jump the queue: harry at periscope.io
Yeah, data mining. I was able to transition into data mining from being a sql dev/analyst after learned stats. I still do a lot of querying but with statistical analysis as well.
The only thing that turns me away from this is I have no real formal education with finance and statistics. I do have a good grasp for numbers, though, so I think it might be a great route to go. I would just need to pick up the skill set. 
I wasn't at all trying to be an asshole, I was trying to express the answer to the same question you keep asking. I honestly don't understand what part is not clicking for you when I've whittled the answer down the smallest possible nugget of correctness. There's so little room for confusion that I'm baffled by what could possibly be tripping you up so. At least you've clarified that part for me. It's obviously your blinding stupidity.
You are not good at what you do and you start right off by burning bridges with people that are trying to help you. You'll go far, I'm sure.
These guys are clearly doing something wrong. I tried this in SQL Server 2012 (on an Azure Medium instance) and got the same plan for all three. I'd assume they're missing indices on their tables. And these clowns seriously expect to sell an SQL performance tool? Edit: I should also add that not having indices would explain their MySQL performance. It's not that MySQL subqueries are fundamentally slow; it's that it doesn't apply indices to them intelligently. On tables without indices, it obviously makes no difference.
Interestingly, you guys need to learn how to use indices on your tables.
Why try to re-invent the wheel when you can download a [SQL backup script](http://ola.hallengren.com/sql-server-backup.html) for free from a MSSQL MVP?
+1 don't write this code yourself. Use someone else's brilliant script and get on to better things.
I understand where your coming from, but as someone who has been studying in this field since may, out of the many applications I've but in, I have only been to one interview. I'm not saying your wrong but there should be a goal while learning this stuff and I think that if you focus on certification as an end result of all the learning then it will pay off...or hopefully pay off in my case.
I've been working in the field for 10yrs And been to countless interviews. The interview is no different if you have those or not.
Because I have to understand it and test it. I looked at his solution and it was a bit daunting for a newbie. There's a few hundred lines of code.
Understanding how it works is important to me. Is there something wrong with the script I posted?
You are obviously a shit programmer (can't figure out a double tick mark) and an even worse conversationalist, good luck failing at life. 
But wouldn't you say getting a cert helps in getting the initial interview? I mean if are already in the field 2+years then no, but as someone who is getting their foot in the door.
Just shows how ignorant you are for assuming I'm a programmer. Thanks for showing yourself today.
Maintenance plan doesn't provide the flexibility I need, as I want to backup all databases with a prefix x or y, but not with a prefix of z. I could use Ola Hallengren's script, but I don't have the SQL knowledge to know what it is doing exactly. This will cause issues when I want to change how my backup job operates. Would you be able to walk me through Ola Hallengren's script? Also, I didn't see any suggestions as to why my script doesn't work.
That's the neat thing, there are so many reasons the script won't work, specially because of unknowns. That's why people use premade and tested solutions. Your code doesn't deal with a majority of things that could go wrong.
Thanks for the reply. I agree that premade scripts by MS SQL masters would be great, and I have looked at some, which is how I made my script above. I've done some testing on my own, but of course since my SQL knowledge is limited, so is my testing. &gt;Your code doesn't deal with a majority of things that could go wrong. This scares me, but I am not sure what kind of things could go wrong. Again, limited by my lack of SQL knowledge. I guess at this point I need someone to point me in the right direction. I want to understand the SQL scripts I use, since they will eventually go into a production environment after thorough testing, but some of the best solutions out there are just massive in terms of how many lines of code. I did spend a lot of time looking at Ola Hallengren's solution, but to backup 20 databases on a server, it seems like his solution is just massive. It would be a huge undertaking for a novice like me to fully comprehend his solution. Do you have any recommendations on how to get a better understanding about SQL backup solutions and what could go wrong?
&gt; This scares me, but I am not sure what kind of things could go wrong. Most people don't until it happens. What if @path is too long for the windows file system and the create fails? Where are the emails going out? What if an email doesn't go out, how are the alerts triggered? &gt; Do you have any recommendations on how to get a better understanding about SQL backup solutions and what could go wrong? Unfortunately experience is one of the best/worst ways to learn
I hope you are right about that.. Getting initial interviews relies more on luck, opportunity, and experience than education. 
If I understand correctly, you just need load the external data in a table elsewhere in your Excel workbook and then do a VLOOKUP against it. Do you know how to do a VLOOKUP?
you can use the function vlookup to compare a cell to a range of cells. Other than that, well there is vba, but if your query is dependent on the cell value (takes the cell value as a parameter), import the excel file to the db, and do it in sql.
What you probably want to do is turn your query into a subquery. Like: SELECT fraction, fraction * x AS multiple FROM (SELECT round(numerator/denominator) AS fraction FROM table) AS f
You could potentially put your calculations into a subquery and then reference the variables in the outer query: SELECT fraction * 1, fraction * 2, fraction * 3 AS multiple FROM (SELECT round(numerator/denominator) as fraction FROM table);
Could I create a temp table, say #t, then join that table to my external table, and put the output back into Excel? Basically, I just want to do a join, but instead of joining two tables in SQL, I want to join one table, and one Excel Column.
well, first off, I'd import the excel into the db, do the join, and export to excel again. Would be easier and faster. For arguments sake... You could in vba, add a reference to the mscorelib.dll which would give you the functionalities (most of them) of the .net framework in excel vba. Your could then do a select to the database, "load" your excel column as a 2nd recordset (datatable object), and do linq to join the 2 recordsets. I would only do that in a "I was wondering if it could be done" fashion, and it would be everything but performant or scaling, but in theory it could be done (i think). Your much better off doing it in the database using a temp table or a staging table, and export the results back to excel thou
It's pretty easy: =VLOOKUP(A, B, C, D) * A = value to look for * B = table to look in (matching column must be the first column) * C = column to get. e.g., 3 = get the third column * D = range lookup: FALSE = looking for an exact match, TRUE = first match. Most of the time people want FALSE So, say Sheet1 is your local data like so... ||A | B | C | |-|---|---|--| |1|&lt;value to match on sheet 2&gt;|&lt;target cell&gt; | and Sheet2 has your SQL query ||A | B | C | |-|---|---|--| |1|&lt;value to match from sheet 1&gt;|blah|&lt;value to get&gt; | In cell B2 of Sheet 1, you want the formula =VLOOKUP(A1,*ExternalTable*,3,FALSE) With external data, Excel will create a table with the name, otherwise you'd have something like Sheet2!$A$2:$C$500, or whatever. Be sure to use the dollar signs in the reference if you're going to copy that formula down, because otherwise it will shift the reference along with it... 
Are there multiple values of "measurement_time" for the same "measurement_id" that require you to partition by that column? If so, you could try writing it by using the KEEP keyword inside your analytical function, like this: select distinct measurement_id, max(measurement_value) keep (dense_rank first order by measurement_time desc) over (partition by measurement_id) vvalue from your_table EDIT: added DISTINCT keyword
jinx
I'm going to try this out a little later. Since I need to get my data figured out quickly, I just copied out my values, and plugged them into a static query. When It's don't I'll just copy back in to Excel. But I want to try this for sure.
Thanks, that would work, but do you know if it would slow things up? I'm working with pretty large data sets and would rather retype the calculation out if it sped things up significantly.
no, if anything it would be more likely to do it faster- it's performing the calculation once instead of twice. But if that's actually the formula you're using it's prob not going to make much difference either way...
Active directories can be extremely handy for granting access and permissions to groups of people. Simply create a login for the active directory as though it were a normal Windows authenticated user.
Thanks! The calculations are definitely more complex than that so this should be quite useful
That's a great explanation. Personally I prefer index/match as you can get results from any column (you aren't restricted to columns to the right of your lookup value). I am not as good as explaining as you are so will leave it to the excel hero chandoo to do it for me http://chandoo.org/wp/2010/11/02/how-to-lookup-values-to-left/
Each measurement_id in this context could indeed have several measurement_times, and I'd only want to return the latest one. I was looking at KEEP in the Oracle docs but I can't seem to get my head around it. It appears to be used primarily in an aggregation context but that's not really what I'm after; I really just kind of want a row filter. I think that's the ultimate result of what you have there, but if I'm adding aggregation (in particular MAX/DISTINCT keywords seem scary?) I wonder if I increase elegance but maybe decrease efficiency? Or maybe the query optimizer deals with them similarly anyhow. Also worth mentioning, the actual query has *several* "measurement_value" fields being returned (some of which are calculations) so the way that's written I would need to do the dense_rank/keep for every one of them I guess?
I agree that it's not the prettiest, but I usually take the same approach. Another approach is: SELECT measurement_id, measurement_value FROM table a WHERE measurement_time = (SELECT max(measurement_time) FROM table WHERE measurement_id=a.measurement_id); This may be a little prettier, but it requires additional code if the same ID can have multiple measurement_time values that are the same. It could return more than one record for an ID as written. It's also likely to be slower than what you proposed. All things considered, I like your approach better. It's probably faster and isn't too bad as far as aesthetics go, IMHO.
This may or may not cause a drop in performance but it is far from an objective statement, it all depends on the underlying table organization, data volume, DB configuration and hardware. "Improving efficiency" can take you down a hundred different routes but you need to put them through your experience filter and test out at least a few of them that you think will do the job just right (it doesn't to be perfect). If you have a lot of data in your tables then maybe using DISTINCT isn't the best idea because it will have to sort the data first. This query in and of itself is very simplistic and probably not much help. You have to take into account the actual context of its practical use, for example, is it used in a view where you join it with other tables or is it in the sub-query of a standalone report etc etc. You cannot get an efficiency metric by looking at just this little query. In the example above MAX is not an aggregate function but an analytical function (or a windowing function). Aggregate functions work with GROUP BY. Analytical functions also do not reduce the number of rows, that's why I used DISTINCT there. You can also look at FIRST_VALUE if you wish, it is also an analytical function and it works just like KEEP(FIRST): select distinct measurement_id, first_value(measurement_value) ignore nulls over (partition by measurement_id order by measurement_time desc) as vvalue from your_table The answer to your final question is yes, you would have to use an analytical function for every field being returned. This is not necessarily expensive from a resource point of view.
That is quite alright! MAX or MIN is of no importance in the last example, it is just purely a requirement of syntax, you can use either one, it is not a "modifier" and will return exactly 15 if you run it on that data. The important part is the ORDER BY clause. The example by captaincoherent below in the thread would probably work just as well too.
I guess i have a problem with the "It's really not at all complicated and I can't figure out what you're not understanding about this" response. WTF is wrong with you that you have to go out of your way to ask instead of reading for yourself?
I do not think it would slow anything down but to check you can always look at the query plans.
Like he said, he wasn't trying to be an asshole, he just had explained it a couple times already and couldn't figure out what you were having trouble with. Even if it came across as assholish, nothing about it merited the response you gave, particularly considering that he had just helped you solve your problem. What's with all the bile and vitriol?
This design looks correct. As for some recommendations... If you are planning to use this database into future years/seasons, you may want to add an attribute to define which season it is... I would encourage naming the ID fields for player and game similar to the stats table. Consistent naming helps in future designs, adds readability, plus if you want to use some "syntactic sugar" you can then use the USING keyword in lieu of ON in your JOIN and save you some typing (only do this for your own things, USING is actually not a very good thing to get in the practice of using and some DBs don't even support it). Given this is just for yourself, and I imagine you'll want to routinely average up the stats across all played games, you could create a season_average table and then define a trigger on the stats table on insert that would calculate out the stats for the current values of the given season and upsert (merge) into the season table. This means that you don't have to keep the query for averaging around to use in the future, you simply just select from the season_average to determine the stats. SELECT * FROM season_average WHERE season_id = :season_id AND player_id = :playerId; Anyways, that's all I have... :)
Apart from backing up to c: there is nothing technically wrong with it. I just don't see the point of writing such a thing yourself. It's a loop and a backup command.
Yeah, in production it will be a UNC path. The reason I have it is because I will use it to backup databases with name prefixes like x and y, but not z. It will be a for what we call a multi tenant SQL server - we host three different product databases on it. Product x, product y, product z. It sure beats manually updating the maintenance plan every time we add a database. Also, thanks for the response. You were the only one who actually commented on the functionality of the script I made.
Yes I remember the index/match trick, but it's almost always easier to move columns than it is to nest functions.
While `vlookup` will work fine, remember that `vlookup` will only return a value for the first match. It's not a true `JOIN`. If you wanted to do a true `JOIN` operation, you would have to use MSQuery. Then setup a ODBC connection to the Excel File and the SQL database.
Woohoo, internet points for me. Ola Hallengren's maintenance solution script handles using prefixes and other pattern matching, compression, and third party products for backups. It also does index maintenance using different database, tables, and index name patterns and is also edition aware. It makes a DBA look like a magician.
This sounds pretty close to what you're looking for: [Querying an Excel data source by using distributed queries](http://support.microsoft.com/kb/306397#) I usually just put the Excel data into a temp table or junk database then do the join in SSMS and copy it back to Excel though.
This is great, thank you! I'm glad I've got some reading material for this stuff now.
Are you trying to prevent someone from inserting into the table more than 4 lines per each room# (INSERT statement)? Or is your question only about the way you can display the data, regardless of what's already in the table (SELECT statement)?
Yes, you can, but it's pretty convoluted. You can't enforce something like this with a `CHECK CONSTRAINT` alone, and you need to have a materialized view or a trigger that does the counting for you. [This StackOverflow question](http://stackoverflow.com/questions/8770552/can-i-have-a-constraint-on-count-of-distinct-values-in-a-column-in-sql) is exactly your question.
SQL Server comes with three different modes of authentication: Windows, SQL Server, and Mixed. [Here's an article about it](http://msdn.microsoft.com/en-us/library/ms144284.aspx). If you already have Active Directory setup and running, use Windows mode. When you install, *make sure you add yourself or Domain Admins or something to the sysadmin SQL group*! Otherwise, you'll have to uninstall and reinstall. So, Active Directory just takes care of authentication. Then, you don't have to pass user names and passwords via connection string. You just use `Integrated Security=SSPI;` in your connection string, and life is peachy. Now, SQL Server takes care of authorization, which is when you take a user and you grant them privileges on your SQL Server. Take a look at [this article about Principals](http://msdn.microsoft.com/en-us/library/ms181127.aspx), and how you would grant access to certain element inside SQL Server.
did you actually name the column room# ?
without using a trigger it would have to be handled in an API to the table. This means it will only be enforced if you are using the API and not writing code to insert into the table in different places.
Thanks!
There are 2 ways to do what you want to do: 1) at the db level via a trigger or 2) at the interface level by doing a check on the button before the insert operation. Something like "select count(1) from room_usage ru where ru.room#=&amp;var_room_num" If the result is &gt;=4 then don't allow the insert otherwise allow it. You also need to think about update operations. Is it possible to update the room# of a person_id? Correct answer should be yes in a real application (under certain conditions). If you choose to keep the logic within the database then you have to do it with a trigger. Something like create or replace trigger t_room_usage_biu before insert, update on room_usage for each row declare v_num_persons pls_integer; begin select count(1) into v_num_persons from room_usage ru where ru.room# = nvl(:new.room#, 0); if updating and :new.room# &lt;&gt; :old.room# and v_num_persons &gt;= 4 then raise_application_Error(-20101, 'There already are 4 people in this room!'); rollback; elsif inserting and v_num_persons &gt;= 4 then raise_application_Error(-20101, 'There already are 4 people in this room!'); rollback; end if; end t_room_usage_biu; This is untested of course, but it is one possible way to prevent the insert or update of rows which would cause you to have more than 4 person_id's in the same room# and raising an error at the application level. Hope that helps. EDIT: If room_usage is a large table, careful that you should have an index on room#.
I might be missing something, but why dont you just SELECT * FROM spans WHERE spans.end_dt &gt; '20130101' AND spans.end_dt &lt; '20140101' OR spans.end_dt IS NULL 
Because that would be too easy... Holy crap did I just over complicate the crap out of this... Thanks! : )
As a professor that teaches Database Basics, please be careful what you post here. Everything posted here on reddit will be on google. Further, if a professor is concerned with cheating, first thing they do is google their questions. That all being said, if you are having trouble with the fundamentals, talk to your professor, or see your schools learning center. They probably have tutors to assist. 
Just to add to that, we had a case where someone asked for help online last year, eventually the student was getting people to do his quizzes and homework. All posted online. He no longer attends my school. 
your welcome ;)
Why the use of the dates table? Are spans.start_dt datetime or just date? Regardless why not do something like: SELECT spans.* FROM spans WHERE spans.start_dt&lt;'2014' AND (spans.end_dt&gt;='2013' OR spans.end_dt IS NULL)
I was trying to use it as a bridge rather than searching for membership during specific dates... BUT.... I seriously brain farted, probably should of got up walked out of the aisle and walked back before posting. 
This is more of a php question than a SQL question. But you would need to do a POST from the page and have your php script process that POST to send the correct query. Also, look into prepared statements in php (pdo) to sanatize your queries.
I think this is best done at the application level but if you need to make sure the application is not doing anything wrong you could add an extra column Person_number and 2 constraints. A unique constraint on Room#,Person_number A constraint on Person_number 1..4 That way you couldn't have more than 4 rows per room#. But it does mean that your application would need to assign a person_number to each entry.
Don't forget the brackets around the (spans.end_dt &lt; '20140101' OR spans.end_dt IS NULL) Otherwise it will include records that don't start till 2014 in the set :)
You know what's great? That I get to decide who I think is behaving like a condescending prick, and I get to decide how I'll react. That I don't have to consult ya'll, or ask myself if someone had the intention of being an asshole. An asshole is an asshole, and we all behave that way from time to time, lack of intent is not an excuse. This time, i didn't let someone talk down to me when all I did was ask for clarification.
Do yourselves a favor, don't keep reading. OP goes from getting help to calling his helper an "asshole". To think, this is my first thread ever reading on /r/SQL, I'm going to assume this isn't a typical thread.
Correction, OP recieved answer then asked for clarification, instead got a condescending answer to said request for clarification. OP decided to play responders game, instead of feeling stupid as was the intention of the responder. Also, if you happen to find out what typical is, maybe you could post a version of it so we could all look for flaws.
Yeah, that's great- nobody is disputing that. Likewise, everyone else is free to tell you that the way you chose to take the most negative interpretation of what was an innocuous comment from a guy that just fucking *helped you out* (and who was very patiently attempting to help you get the stupidly simple concept of escaping a quote in a string through your thick skull) and reacted to that perceived slight in a manner that was absurdly rude and disproportionate suggests that you're a hypersensitive, insecure, ungrateful asshat. Go get some therapy or something, jackass.
Yeah... you're right. It must be complete fact that I'm the one with the problem. I mean since I came here and asked for technical help, I should be completely willing to get shit on. And if i retort in any way... well then obviously I'm the one that needs help. You're absolutely right. It's nothing to do with you, the responder, or the this forum in general. You guys don't need to change at all.
use a third party product.
There's no free version of Oracle (AFAIK). I'd recommend you download and install either Postgres or SQL Server Express. The latter is capped at 10GB, but that should be more than enough for home practice. Edit: [I was wrong](http://www.oracle.com/technetwork/database/database-technologies/express-edition/overview/index.html?ssSourceSiteId=ocomen)
I'd recommend against bothering with MySQL. It's missing some very important features (e.g. CHECK constraints and window functions), and because it does some bad things like coercing NULLs to zero in NOT NULL columns, can create some very bad habits.
the 'introduction to databases' course on coursera covers set theory before moving into sql
&gt; 'introduction to databases' course on coursera I will give that a look.
column2 and also row1 and row2? please, could you use real names, what you posted is confusing
I don't know if this is what you want, but give it a try and tell me. SELECT B.column2 FROM table1 A INNER JOIN table2 B ON A.column1 = B.column1 WHERE A.column1 = A.column2
http://shop.oreilly.com/product/mobile/0636920030553.do This and the other CJ Date videos and books are amazing. There are many 40% coupons or more to bring down these prices too
So, you privatly start a project, on which you want to work on with a team? I guess the budget is pritty much 2 bucks 50 cents? Be careful about installing a DBMS at home, and make it accesible over the internet. If you open the ports for the sql server to the internet, your database won't be yours for long. Bonus points for having comandshell enabled and running the service under an admin account, than your machine won't be yours for long. You basicially need to write an api / interface to have the application connect to the backend. In that api you can do the much needed security. As for development, just have everyone install a local instance, and supply them with the schema definition and test data. That would be the easiest and safest route to take (also cheapest), also, that webdeveloper in the corner wont screw up your entire database design / data, running whatever went trough his mind.
You need to create a db on that server or have IT create a new instance and give them a list of users to allow access. If you create a db on the existing server, you should have access to only allow SQL logins with passwords so that its easier to manage users. Good luck on the project. 
On what server? I am following this video series (http://www.youtube.com/watch?v=bwBBKwFSEDI) and can't do the piece towards the beginning of Pt 2 since I do not have a server yet.
&gt; I work for an organization that uses SQL Server Management Studio and when I open it, I log into our server to connect to our database. My question is: How do I start something like this for my own project? &gt; I have a personal project that I want to start. Just no. &gt; Can I start it just on my own home computer? wuh? Does your organization have a DBA group? Do you have a manager? You should be raising these questions with them not The Internet. Is this truly a personal project? if so **keep it off of work computers!** you can hit a whole litany of issues by doing personal work at your job.
Thank you, I will look into that!
Thanks for the response, couple of things: &gt;Did you really say that everything now, multiple 10GB databases, is running on servers with 4GB RAM? Yes, but I meant that none of these databases exceed 10GB. Also that performance is not really the issue here. Sure it can be improved, but it's not the reason for this. &gt;The way you're tossing versions out, it sounds like either money is no object, or maybe there's just been some "testing" going on ;) My developers have MSDN accounts, so I'm using their SQL licenses for the DEV boxes, which is allowed. I would not be able to use these licenses for production. We are a non-profit, so while I can get stuff if I need it, I rather not waste anything. &gt;What's the least common denominator in your environment? You didn't mention 2000, that's good, but there are things that 2008 and 2012 can do that 2005 can't. Everything right now is running on 2005. No 2000 stuff in either prod or dev, which is definitely a plus. &gt;two identical physical servers I'd REALLY prefer to virtualize the new SQL boxes as we just dumped a bunch of money on VMware licenses and Veeam for backups. I know people prefer physical servers for SQL but from my understanding this is only storage related. Our databases are currently stored on the SAN (8GB FC) now anyway so wouldn't the overhead be similar on a virtual environment? HA (and FT for that matter) in my VM environment negates any hardware failures, so the only thing I have to worry about is the OS or SQL crashing, right? &gt;honestly I'd go with SQL Server 2008 instead of 2012 What is your reasoning behind this? These databases are really just back ends for some web applications. Not a whole lot of transactions going on, even during peak hours. At most we'll have MAYBE 20 people accessing the db at a time. 
I don't see anything wrong with running everything on a single instance in an active/passive failover cluster. It depends on your budget and what kind of downtime is reasonable for your company. If these databases have low transactions and small workloads then you can get away with running them in a VM. Just do some load testing and capacity planning to make sure you have room for at least 5 years of growth. 
I wouldn't call my self an Excel guru but I'll take a quick stab. First, find all your 'Ultimate' parents (roots). I'm assuming by virtue or being roots they won't have any parents. Filter by the ParentID column to find them. Copy these IDs into another worksheet in the first column. Then do vlookup (http://office.microsoft.com/en-us/excel-help/vlookup-HP005209335.aspx) to find all the roots children. You should be able to copy the formula down so you only have to write it once. Then you can copy across into the next column. Eventually you'll stop getting results in the columns as you reached the end of each tree. 
The following is just my opinion, i have no idea if its best practice or not. But i have been a storage admin, windows admin, vmware admin in the past and am currently an oracle and mssql dba. What I would do is run everything on vmware, especially if you are backed by a good SAN. Ideally you would have a separate vmware cluster for prod and non prod. I would create more than one vm for each environment (prod, dev). The number is arbitrary. You kind of have to feel it out. How many databases can run on one lun without impacting performance. I would definitely keep the mirror and reporting copies on separate vm's. The environment I am in has about 650 mssql databases ranging between a couple MB's and 300GB - between prod and all of nonprod and they are all on vmware. It works great for us. The real kicker with this strategy is getting your prod and nonprod on separate hosts and disk. If you have to have 100% uptime this kind of shoots vmware down and you are much better off going with some sort of failover cluster or availability group. But think long and hard before doing it because you are going to waste a lot of your resources. SQL2008R2 or 2012 - its really up to you but pick one version and make it standard across everything. Never install more than one version on a server if you don't have to and absolutely stay away from more than one instance per server if you can. This is why vmware is so nice for an environment like this, exceptions just become additional vm's. If you need availability groups you have to go 2012. Failover clustering in 2008 and below is kind of a joke, not that it doesn't work it just causes you so much hardware overheard it hurts. Again, this is just my opinion of how I would do it so take it for what its worth. 
ha, sorry, fixed.
Sorry I forgot to put mysql tag in the title.
Where date in (1,3) ? Not sure I'm understanding the problem...
I answered a similar question the other day. I'm pretty sure a similar solution will work: http://www.reddit.com/r/SQL/comments/1w18q8/mysql_select_rows_with_a_specific_condition/cexr47z
Cool, Thanks. I had a cluttered method of doing it... I used two statements, one for 1 and one for 3, and then I inner joined the results of each of those statements... but I like yours better.
Incorrect. Where date in (1,3) returns: rick bob bob nancy nancy Not at all what OP asked for. He specified result should be only bob and nancy. 
Thanks for this
No problem. If you start to go down a road and have more questions feel free to pm me. I will provide as much info from my past experiences as I can. 
You can use joins (or even count and having) but it is easier with IN SELECT Person FROM Table WHERE date=1 AND person in ( select person FROM table WHERE date=3 )
&gt; TLDR; I really want to start my own SQL Database that my project team can work on from their own computers. Where do I start? bro, you're sending mixed massages.
Thank you for for your advice. Some people have been giving really poor answers so I really appreciate this.
How so?
 GROUP BY DATEPART(m,date)
Perfect. Thanks!
You should just be able to use two conditions instead of one, such as (rough copy): select count(*), sum(dollars), (CASE when (dollars &gt;= 2000 and dollars &lt; 2500) then "2k" when (dollars &gt;= 2500 and dollars &lt; 3000) then "3k" when (dollars &gt; 3500) then "3.5k" end) as dollargroup from ord where date(timestamp) between '2013-12-01' and '2014-01-01' group by dollargroup; That's the way to do it that's the easiest to visually understand (in my mind). The other way would be to change the order of the WHEN statements, so that the highest is first, like so: when dollars &gt; 3500 then "3.5k" when dollars &gt; 3000 then "3k" when dollars &gt; 2500 then "2.5k" when dollars &gt; 2000 then "2k" That way, it will pick up values in the correct statement rather than almost everything falling under the "greater than 2k".
Yeah, the tricky part is that any order thats, say, 3501 should count in each category.
So you're saying if you have a single order for $3501 your output should be: 1, 3501, 2k 1, 3501, 2.5k 1, 3501, 3k 1, 3501, 3.5k ?
Try something like this: WITH DollarGroups AS ( SELECT '2k' as DollarGroup, 2000 as DollarValue UNION ALL SELECT '2.5k' as DollarGroup, 2500 as DollarValue UNION ALL SELECT '3k' as DollarGroup, 3000 as DollarValue UNION ALL SELECT '3.5k' as DollarGroup, 3500 as DollarValue ) SELECT dg.DollarGroup, COUNT(*) as GroupCount, SUM(dollars) as GroupTotal FROM ord JOIN DollarGroups dg ON ord.dollars &gt;= dg.DollarValue WHERE date(timestamp) between '2013-12-01' AND '2014-01-01' GROUP BY dg.DollarGroup
That's really helpful. Seems like all I do is put together monthly data!
try this too, please SELECT COUNT(*) AS total_count , SUM(dollars) AS total_dollars , SUM("2k") AS "total_2k" , SUM("2.5k") AS "total_2.5k" , SUM("3k") AS "total_3k" , SUM("3.5k") AS "total_3.5k" FROM ( SELECT dollars , CASE WHEN dollars &gt; 2000 THEN 1 ELSE NULL END AS "2k" , CASE WHEN dollars &gt; 2500 THEN 1 ELSE NULL END AS "2.5k" , CASE WHEN dollars &gt; 3000 THEN 1 ELSE NULL END AS "3k" , CASE WHEN dollars &gt; 3500 THEN 1 ELSE NULL END AS "3.5k" FROM ord ) AS d 
Just enforce a foreign reference from the employee table to the department table. It will prevent a department from being deleted when it's being referenced.
&gt; how would i do that? I do have deptID set up as a foreign key in the employee table. is that enough or do i have to do more? &gt; the table names are Department and it has DeptID, DeptHeadID and the other table is Employee the employee id is EmpID and there's DeptID within that and I have DeptID set up as a foreign key. Thank you for the reply 
I did set up deptID in the Employee table as a foreignkey from the department table
that is enough. If you have the foreign key set up, it will give you an error when you try to run the delete for a department that is referenced by an employee. 
MS SQL Server express is at http://www.microsoft.com/en-us/sqlserver/editions/2012-editions/express.aspx Feel free to ask questions if you get stuck; this is not something I do everyday, but I have installed it a bunch of times, and am running a tiny azure VM with it and IIS (not doing anything useful :) There are other options (Linux and PostgreSQL is another good one), but if you're familiar with MS products, and getting started, this is probably the best option.
Still not seeing the problem here. I have MY project team which is not my Work team. The only thing my work has to do with this is that they have their files structured in a way that I really understand. Therefor, I would like to use a similar structure when MY team creates our project. Nothing to do with work otherwise.
I believe you have just invented prepared statements. :)
Ah oke. Oke so am I right in that with this way of doing things, you are 100% safe against sql injections, as long as you treat every piece of user supplied data this way?
Yea, it's better to use Merge than a "fake delete + insert" to act as an update. However to use merge you need something to merge into your target table. If you want to upsert the table, you can't use the table itself as a self-join source, that will only apply updates. MERGE ControlTable as ct USING (select 'Key' as key, 'ColumnOneValue' as ColumnOne, 'ColumnTwoValue' as ColumnTwo, 'Value' as Value from dual) ct2 ON ct.key = ct2.key WHEN MATCHED THEN SET ct.ColumnOne = ct2.ColumnOne, ct.ColumnTwo = ct2.ColumnTwo, ct.Value = ct2.Value WHEN NOT MATCHED THEN INSERT (Key, ColumnOne, ColumnTwo, Value) VALUES (ct2.key, ct2.ColumnOne, ct2.ColumnTwo, ct2.Value) Replace "select from dual" subquery with an equivalent statement to retrieve your update values as an in-line view you can merge from. 
Well, the downside is that you can only do queries that are in "set of prepared queries" and only in that particular way. That's why it's great for end-users of your system who aren't supposed to do more than that anyway. 
&gt; because not enough coffee yet also made you overlook mentioning which dbms you're running (see sidebar)
Looks like it, my bad.
Thanks so much - your example also helped me to better understand how `MERGE` works in the firs place!
Thanks this is nice. 
&gt; Do you know of any other sample data sets? [sakila](http://dev.mysql.com/doc/sakila/en/index.html)
Which DB is this, and where can I read more about merge, and similar queries. Thanks.
Yes, and in most cases being safe from SQL injections is easy. As long as you rely entirely on prepared statements or parametrized queries (implemented either by your driver or your DB) you are safe. It is when you have to actually build SQL queries you risk injections.
yea, hes not the greatest, It's more or less just learning how to get things to work. No sanitation yet. That's coming next week. This is just the basics. But i'll def check out those sub reddits. thx
It's confusing that you have your own project team, but it's not your work team.
If you Google "upsert" and your database technology of choice you should get some good examples (if it is supported, that is)
Thanks, I will definitely be looking at the Sakila database. I like the BSD licence.
Just make sure your FK is not ON DELETE CASCADE...
Whoever wants to take over that part: [It seems to be fairly easy](http://theiphonewiki.com/wiki/Messages), especially if you don't have to merge message groups or MMS attachments with different IDs. The primary keys are `AUTOINCREMENT`ed by SQLite, so ideally you will only need to `INSERT OR REPLACE` from the `message` table in one database into the other. One should take care to update Apple’s own metadata like the `counter_%` values in `_SqliteDatabaseProperties`, however.
Set up the environment in which you want to learn, get a test database for that environment, then go read some tutorials There are a lot of books and online tutorials that will walk you through all that. You can't learn unless you start actually writing statements.
When i learned SQL i started with a basic introductionary SQL book and did all the exercises as a read through it. When you're done and you feel that you're getting the hang of it, set up a nice environment and start experimenting. Make lots of mistakes and find out why you made that mistake. That is the best way to learn. 
I had a lot of fun (and learned a little) creating a SQL driven website for a friend of mine. I tried to create a website that had a database of customers, services offered, with pricing and time to completion estimates, as well as physical reservations. So far as my environment I started by setting up a mysql server on a local machine, but found it was a lot less headache just to use one of the databases that comes with my webhosting and set up up with "Direct Access" so I can connect to it from either a webpage or a python/php script locally. 
Why not have a third table that contains one row for every recipe and ingredient? In other words, if a particular recipe calls for three ingredients, then there would be three rows - all have the same recipe ID, but each has a different ingredient ID. Would make it very easy to reference in your application, and would serve to enable a bunch of use cases.
I like you.
Thank you! I have not tried that yet, but I think that I can grok the concept. After I get back from errands I am going to map it out and ensure that I can work with it. 
The way I worked with SQL that 'clicked' the quickest was to build a simple [CRUD](http://en.wikipedia.org/wiki/Create,_read,_update_and_delete) site in PHP. Then to create progressively more complex CRUD sites. I worked from scratch EVERY SINGLE TIME until I could write the entire thing without Googling any of it.
you will find that it's really easy to work with also it is the classic and proven way to [implement a many-to-many relationship](https://www.google.ca/#q=implement+a+many-to-many+relationship) -- link goes to google results for that phrase
I'm not 100% sure if you are saying you are working with MS SQL Server in addition to Oracle... but I'm a MS SQL guy, so here's my advice if you want to know about that... Know that SQL Server Express is free and there's plenty to learn without hitting the limitations of that platform. If you find you must learn some of the things not available there, you can get evaluation versions of the other editions. There are also free versions of Visual Studio, BIDS, SQL Data Tools, etc... but for now you can do plenty with just SQL Server Express and SSMS. Microsoft training is expensive... books are cheap. Get some books, there's tons out there. If you are on a budget, I imagine you can find used books on SQL 2000/05/08 for pretty cheap. The core 85% is the same no matter which year of SQL the book is for. I would guess that roughly the same holds true for Oracle databases as well. If you are a fast enough learner and have a bigger budget to work with, CBT Nuggets has video training courses ($100/month) that I feel are excellent. I think they are best for someone who already has some knowledge, because they are pretty fast-paced. They have training in Oracle as well, and lots of other stuff you might be interested in learning.
As with most things in IT, the best way to learn is to put yourself in position where the skill you want to learn has a meaningful purpose. I would suggest finding a project you would like to get done at work, commit to it and by using the Internet and the database documentation you should be able to finish your project and learn SQL in the process.
Overall, the SQL language isn't very difficult to learn. It was created to abstract complexity of accessing data. That doesn't mean that developers don't write complex queries, but most SQL isn't that hard to follow. Learn [DML](http://en.wikipedia.org/wiki/Data_manipulation_language) and then begin working on the [DDL](http://en.wikipedia.org/wiki/Data_definition_language) for your database (in this case Oracle). Most RDBMS use a very similar subset of ANSI SQL, but many deviate for some specific functionality (http://en.wikipedia.org/wiki/Merge_(SQL), [temporal](http://en.wikipedia.org/wiki/SQL:2011#Temporal_support), [recursive queries](http://en.wikipedia.org/wiki/Hierarchical_and_recursive_queries_in_SQL), etc.), so while you are learning the language, ensure that the books and articles you are reading focus on your DBMS (or you'll have to find the comparable syntax). Ad Hoc queries are your friend (under test of course). Like any programming language, the more you write the better you become. When you have free time I'd suggest writing some report-like queries that attempt to pull interesting metrics. Not only is this valuable in learning, but often times you can discover aspects about your data that you -- or anyone for that matter -- knew. Read a good theory book. [C. J. Date](http://www.oreilly.com/pub/au/2136) has many out there that talk about the relational model and gives examples for both theory and practice. Sometimes you'll have to ignore his examples written in his Tutorial D language (personally, I find the syntax of D to be a bit easier to comprehend from time to time for complex queries). Learn to use the DBMS tools for EXPLAINs. Explaining out the queries will help you better understand the optimizer and even other aspects of the DBMS. You'll begin to discover little tricks around how two things could do the same thing but one is considerably more efficient than another. One example that I often encounter is the difference between EXISTS and IN. In some cases one will out perform the other, and other times it won't matter at all. Often it depends on the DBMS you are on, the volume of data, and the way the data is modeled. I've seen significant improvements (orders of magnitude) by simply changing only a small portion of the SQL statement, such as IN to EXISTS. 
You're probably best off doing this in a scripting language. 
http://stackoverflow.com/a/7204867 Edit: [Setting case sensitive collation in the query]( http://m.sqlmag.com/blog/forcing-collation-where-clause-22-jun-2011)
You can also do it with subqueries: SELECT DISTINCT Person FROM People P1 WHERE EXISTS (SELECT * FROM People P2 WHERE P1.Person = P2.Person AND P2.Date = 1) AND EXISTS (SELECT * FROM People P2 WHERE P1.Person = P2.Person AND P2.Date = 3) This is probably worse than doing it through joins.
If I have ten such conditions, would I need ten self-joins? 
From my experience if you put tbl_foo (or the relevant column and row subsets of it) in a temp table the subqueries execute much faster (seconds vs minutes). This might depend on how hammered the server is and how big the tables are.
That will only work if he has a case-sensitive collation.
It's unlikely he'll want to change the collation on his table just to run this query, especially since it sounds like he'd be doing it on a production server, and there's a pretty good chance that might break things.
C. J. Date's *An Introduction to Database systems.* Old editions go for $4-$5 at betterworldbooks.com.
While I'm a big fan of normalization, it's not the first thing I'd look at to fix slow queries. Start by looking at queries which are pulling entire tables or repeating operations over and over again. If you have complex queries that are pulling from each other, try breaking them down into separate queries and running them one at a time to see what takes the longest. MS-Access doesn't have a lot in terms of query analysis but there are still things you can do to debug apps. Having said that, I have published a couple of books you might find helpful. [Microsoft Access for Beginners](http://www.andrewcomeau.com/Pages/ms-access-for-beginners.aspx) is now a free PDF download and covers versions through 2010. It includes a section on normalization. [Your First Guide to Database Design](http://www.amazon.com/gp/product/B00FNAMMA8) covers normalization extensively with multiple examples including the Job Search Plus software that I designed with Access 2007. 
He can set it on the query. http://m.sqlmag.com/blog/forcing-collation-where-clause-22-jun-2011
TIL that is possible
Off the top of my head, I'd say yes. I'd have to see the specifics of what was needed.
[W3 schools](http://www.w3schools.com/sql/sql_intro.asp) And [SQLServerCentral.com](http://www.sqlservercentral.com/) -which is a great, free resource w/ no spam issues (been registered for years) Check out the series [Stairway to T-SQL DML](http://www.sqlservercentral.com/articles/Stairway+Series/75772/) and the series [Stairway to Advanced T-SQL](http://www.sqlservercentral.com/articles/Stairway+Series/104499/)
Even if it wasn't possible for just select statements (as it is), he could dump that data into another table and make it case sensitive.
What DBMS are you using? Apparently, in MSSQL, you create an audit task. http://www.mssqltips.com/sqlservertip/1655/sql-server-2008-tsql-auditing-commands-for-select-statements/
Nevermind, the program I was using was stuck in a loop. For Days.
Since this seems like a one time clean up, you can do the following. It probably won't be the most efficient, but should help you get the job done. Create functions that do a count of lower case and upper case characters. Loop through the text value and use the ascii function in SQL Server to get the ascii code value for each character and increment your lower and upper counts. lower case = between 97 and 122 upper case = between 65 and 90 http://technet.microsoft.com/en-us/library/ms177545.aspx
Is there a reason you are not using SQL Express for this instead of Access? Performance for Access is notoriously atrocious...
Try creating indexes on the columns used to filter out the data. I found two links from Microsoft [About indexing fields and records in an Access database (MDB) - Access - Office.com](http://office.microsoft.com/en-us/access-help/about-indexing-fields-and-records-in-an-access-database-mdb-HP005262351.aspx "About indexing fields and records in an Access database (MDB) - Access - Office.com") [Create and use an index to improve performance - Access - Office.com](http://office.microsoft.com/en-001/access-help/create-and-use-an-index-to-improve-performance-HA010210347.aspx#BM3 "Create and use an index to improve performance - Access - Office.com")
With that size I'd look at moving to a new system. Ever looked at [MySQL](http://www.mysql.com/)? Edit for mixing up my paren's and brackets. Doh!
You should be using the service broker service for this task. Well, if you are on MSSQL.
You need the `PIVOT` keyword here. Google should be able to provide you a bunch of examples and stuff. It's one of the clunkier SQL operators unfortunately.
I have nothing to offer you in the way of answering your question (sorry) but I do want to say that I am amazed there is another VFP guy out there. I have no idea how to program in it and I've found it difficult to find info on it from a totally beginners point of view. I mostly fix small errors that pop up in the dbfs and copy them over to excel if i need to do something the program we use won't do. Good luck.
Note: Pivot only works on 11g or higher.
you can torrent "forta sam's teach yourself sql in 10 minutes"
Agreed. Although Access is a good system to start learning about database infrastructure and SQL coding, it is not a robust system by any means. Very very limited when it comes to "real world" applications.
It sounds like you're a Data Analyst not a SQL Developer. I would go and quickly take the first few classes of a Microsoft Self Paced course and see if it clicks. If it does, you might have a chance, if it doesn't, you should probably disclose this before they find out themselves.
Writing SQL and understanding relational databases is going to be very different from what you are doing. Be sure to be very honest of your skills in the interview and they may be ok with you training and learning on the job. 
How about the RowNumber function? IIf(RowNumber("groupname") = 1, True, False) or something like that.
I've been using SQL for over a quarter century and I still google every statement I write. I'd say writing SQL is easier to fake your way through than database design is. I don't think there is anything I'd called similar though. If you expect to get interviewed about it you should be cramming for a few days beforehand.
Ah, I forgot to mention my experience is with MSSQL. I suspect it is because of frequent reads/writes (locks) and the size of the table that a temp table helps in my case. Btw I was talking about using a temp table for the exists clause, not joins.
Also - The purpose of Normalization is to protect data integrity (i.e. - one fact in once place) specifically to prevent update anomalies. For example - you only want to store your "address" in one place, so when it changes, you only need to update it in one place. Normalization does not, in itself, have a goal of improving 'performance' - in fact, in many cases, 'normalized' schemas tend to perform poorly - in which case the schemas are de-normalized (in certain situations) to improve performance. Generally, as databases grow, their schema are tuned for EITHER Read-Performance or WRITE-Performance - At scale, you must choose one. 
Go for it. If you have an analytical mind you can learn SQL
Apply anyway, even if you don't get the job you will have an extra chance to practice interviewing. Besides, SQL is easy to learn quickly.
I'm going to disagree. I'm in a very specialized niche of the healthcare field and I'd take an analyst with his experience* in my niche over a seasoned SQL developer any day of the week because it'll be far easier for me to teach the OP to code SQL than it would be to teach a SQL developer the quirks of our niche. *Sorry OP, no positions available today.
1. No 2. Who cares? pursue it until you can't go any further then do a post mortum and update your skill set. 
Why would you care about wasting their time? If it sounds like a job you can do for a company you like, then apply.
Do your queries in Dev
Get used to using the Estimated Execution Plan feature before you run your queries to look for bottlenecks and inefficiencies in your queries. Before you run your code, highlight it and then click on the icon (at least on my set up) which is the fourth over from Execute) which looks like two blue blocks connected on top with a green one on the bottom with a white dialog type box also on the bottom. This will give you a snapshot of the estimated resources the query will use ahead of time running the query, it will give you a basic estimation of possible problems and foresight to alter your code to make it more efficient at points if needed. 
 SELECT TOP 100 * FROM BigTable Can be just as bad as: SELECT * from BigTable Y? Boring reasons. http://stackoverflow.com/questions/9616965/why-select-top-clause-could-lead-to-long-time-cost http://stackoverflow.com/questions/6286837/select-top-is-slow-regardless-of-order-by 
* Perform all your query building in Development or your personal testing environment, if you get it running efficiently there it will almost certainly run well on production * Always aim to have the smallest results for sub-queries so large data sets aren't unnecessarily stored * Learn to use temp tables when it is appropriate * Learn to read/view your [estimated execution plan](http://www.mssqltips.com/tipImages2/1856_01.jpg) and your [actual execution plan](http://www.mssqltips.com/tipImages2/1856_03.jpg) try and optimize the query to reduce bottlenecks * If you are unsure about a complicated query, ask your senior DBA to have a look at it or see if you can run it after office hours. * Finally...practice practice practice
Things to avoid if possible: * Large unconstrained outer joins * Multiple aggregate functions in separate sub-queries * Cross linked server joins, try and pull information from one to the other and perform it locally if possible. 
If you got a enterprise edition, run your queries in snapshot isolation. This should minimize blocking. You should still take care you don't flush the server cash with a bunch cross joins or thing like that, sop what the other people wrote still applies
If you've already grown up in an analytical ad-hoc type of role, then graduating to SQL writing isn't that big a jump in my opinion. If you're talking about jumping from ad-hoc analytics to a DBA or database design role, then that might be too much. Also it isn't about being a good or bad SQL developer, its more that my shop is primarily focused on BI outputs where having the business acumen to understand what you're looking at with a metric is every bit as important as having the technical understanding to build a query around it.
An analogy a coworker used to use: &gt; I'd rather take someone who knows construction, and teach them to use an excavator, than take someone who can operated an excavator but knows nothing about construction. Technical skills are easy to teach. Field-specific knowledge requires time in that field. Just the way it is.
"While my SQL skill-set may not be as advanced as you are looking for, I'm excited about the opportunity to learn. Given my background in analytics, this is the next logical step for me."
Suppose this is your form: &lt;form action="yourfile.php" method="POST"&gt; &lt;input type="radio" name="fruit" value="Apple"&gt;Apple&lt;br&gt; &lt;input type="radio" name="fruit" value="Banana"&gt;Banana &lt;input type="submit"&gt; &lt;/form&gt; yourfile.php would be like this: &lt;?php $fruit = $_POST['fruit']; //Gather the value from the radio button (Apple or Banana in this case), then write your query, e.g: $query = 'SELECT * FROM Tbl WHERE fruit="' . $fruit .'"' ...show results... ?&gt; IIRC a radio will let you pick only one option. Regards
Dumb question, but I have Oracle SQL Developer, how do I find out if it's 11g?
 select * from v$version
(NOLOCK) (NOLOCK) everywhere 
Awesome, it is 11g! Thanks!
This makes me feel better. I know there's people out there that can learn faster than me but I like to think I'm quick, I've just never been able to quantify it on a resume.
If you are concerned about your sql skills then get going on them. * Install any of the free versions sql server, oracle, mysql, ... MSAccess may not be what they are looking for. * Download some sample databases * Find some [free sql training](https://www.google.com/search?q=free+sql+training) When you go for the interview, tell them how you prepared for it but taking the initiative to teach yourself. Even if you don't get the job, you will get the experience of the interview and a start on your sql training.
So you already have a solution, you just want it displayed in Excel? If you have a SQL table, view, stored procedure, or just a query you run against a SQL db, you can create a connection in excel and display it as a table or Pivot Table. I don't usually like this site, but here's a good description with screen shots: http://www.wikihow.com/Embed-a-SQL-Query-in-Microsoft-Excel. 
&gt; I still google every statement I write I don't and I've only been doing it for 15 years now. 
SQL Server only right? 
Don't forget to join properly. If not you'll end up with a cartesian join with millions of records. 
I'm also trying to learn SQL and stumbled upon this tutorial app today. I like it so far. http://sol.gfxile.net/galaxql.html 
You have to still be careful with NOLOCK; you still receive a schema lock when you use it; which means some jobs that rely on adding/modifying/removing certain indexes will fail if you are querying during it's runtime. 
So, I found someone on freelancer.com to do this for me. How would you recommend going about that in case there is any sensitive data in there? I'm pretty sure I deleted anything of value, but there might be like a stray last four of my social, or amazon account login, or work door code, etc, etc. I'm thinking either I setup a vm and have the person login through a vnc and work under recorded supervision, or I just trust that a person with a high rating isn't going to look through all the data for anything they could use. Not that it would do them much good, but I'm sure if someone REALLY wanted to do some nasty social engineering they could.
Instead of cursors (where absolutely necessary) what would be a good alternative? MSForEach?
If you are doing any linked servers, ~~use openquery~~ don't use linked servers. Would be my take on it. Not unless you really, REALLY have to And as indexes go, if a self proclaimed beginner came in, and screwed with my index design without talking to me first, holy crap, that person better be able to run very fast. A newley added "bad" index can screw things up real good. Don't create indexes until you understand how that stuff works (instead talk to the dba about your idea). 
The general idea about not using cursors is, to do what ever you are doing in a set based approach. If possible, not to treat each row as its own, one after the other, but to treat the entire dataset at once. Loops in general do not meet that general idea. So anything that gets executed in a loop, is bad. Scalar functions for example, bad (really bad). Cursors, bad. WHILE 1=1 BEGIN END, bad. MsForEach, usually bad. I'd generalize it this way, anything that needs a BEGIN and END, is usually bad. IF would be an exception 
At the end of your transaction, you either commit or rollback your changes. On a mobile right now so no sample but Google should have you covered several million times over. 
Yup, if you're just trying to survey some random rows, the SET ROWCOUNT command is your friend. 
no reporting server?
Type 'Delete *' then press F5 Works every time. SQL cheat codes. I joke. Of course... Just stay out of Production until you are comfortable. You'll be fine if you just have read access. There isn't much you can really do in a select statement to bum out a server. I mean you can. But you won't until you start building crazy reports. Really the problem only occurs when you start to 'join' tables on repeated keys. You'll learn this. And even when you do make the mistake, you'll see that it doesn't bog much down anyway. Everyone else is getting really fine tuned with their efficiency recommendations. Just cowboy that sonuvabitch, get dirty, f-up a query or 10 and learn how to manipulate the tables. Your boss will never know.
I did not know that... In fact, that's extremely good information to know. Thank you for pointing that out!!
I will look at these tomorrow, thank you!
And version. If you are running 2000, merge doesn't work.
All I can say from having worked with sensible sets of data is that you need to have trust in them to some extent. Even if they’re working remotely on your system with tools you’re providing, there is nothing keeping them from simply screen-recording all their actions, like an unsuspicious `SELECT * FROM ...` to verify the merge, for example. On the other hand, I wouldn’t trust myself doing such data manipulations correctly without looking at the data at all. If you already acquired the databases from the phones, you can load them using the [SQLite Command Line Shell](http://www.sqlite.org/sqlite.html) ([download](http://www.sqlite.org/download.html#linux)) and try to look for text messages containing sensible information: SELECT date, address, text FROM message WHERE text LIKE '%keyword%';
Copy and pasted from homework?
haha, thanks!
Well, wouldn't this increase my licensing costs by 4x (2 Dual socket Quad cores = 2 x 8 core licenses)? Also adding in the cost of the additional hardware. This is as opposed to running a single quad core VM.
That is pretty much the only way to do that from within SQL. I'd suggest that if you're doing that across 300 dbs, you should probably be looking at PowerShell or something like it. 
Not needed in Oracle, as Oracle uses MVCC. In DB2, it is WITH UR. Stands for Uncommitted Read, which should provide a hint as to the pitfalls.
A transaction is a way of doing a set of actions in an atomic fashion, like they were a single action. For example, an inter-account bank transfer. You need to update account A, deducting $10 from the balance. Then you need to update account B, adding $10 to the balance. It would be a bad thing if this got halfway done. It's been a while since I read Transaction Processing, but my recollection is that each action you take in your transaction gets written to a log. If at any point in the transaction you issue a rollback, all the log entries for that transaction are undone. Rollbacks allow transactions to be atomic. Either every step is performed (commit), or none of the steps are performed (rollback). You should read Transaction Processing, it is incredible.
If there is an institution teaching stuff this up to date/relevant as part of their Database courses, good on them.
When I was a comp sci major (graduated in 2002), one of my courses was database design and development. They certainly taught us transactions. If an institution has a course on databases and they aren't teaching transactions, then they should be punched in the dick. 
I think with those skills you can get a job as a SQL report writer easily, in my area I see a lot of postings for reporting analysts
yes but don't stop adding other skillz to the old rayzoomay
A new question: Is there any benefit to have the temporary table look for the max in each case clause? I think I am getting the same results if I use the query as follows. with tbl as (select client, case when item='Mac' then "Entry Date" else null end as MAC, case when item='Microsoft' then "Entry Date" else null end as MICROSOFT, case when item='Asus' then "Entry Date" else null end as ASUS from your_table group by client, item) select client, max(MAC), max(MICROSOFT), max(ASUS) from tbl group by client
I started off with yours because it had all I needed right there. I'm planning on reading up on the PIVOT to learn how it works and how to use it, but I will not have time until this weekend. Thanks again!
I was taught them as part of my education, but it consisted of one lecture and a few questions on a test. Hardly sufficient. I have also met several 'professionals' that wouldn't be able to adequately describe how they work. There is a large difference between what **-should-** be getting taught and what **-is-**. 
My company does have a DBA who literally doesn't do anything but the database... but you have to understand, he's really really good at it. Optimizing Postgres is a big part of his job - helping the programmers write efficient queries is a smaller portion of his job. Managing the database is harder than it sounds. The programmers request that columns and tables get added and then, when bugs are being tested on different versions of the code, the DBA has to make sure that the QA guys are accessing a version of the database that existed when that code was written. This is a very tedious "accounting" task that's part of his job. If you imagine that there's a job where someone sends you an email asking, "can you tell me all of our customers who own Fords and have more than 2 kids?" - there's no job like that. Edit: besides, the more related things you know, the more valuable and employable you are. Learn the reporting. Learn Excel (better you than me!). Learn SalesForce, Perl, sysadmin... whatever you can that interests you. Otherwise, you're a dinosaur in this industry pretty quickly.
I agree 100% on the indexes. He said he was getting read only access. But since he said he was new, I would expect him to learn about indexes. I was just giving general guidelines. Of course if you make any DDL changes, talk with the dba first.
&gt; If you imagine that there's a job where someone sends you an email asking, "can you tell me all of our customers who own Fords and have more than 2 kids?" - there's no job like that That isn't true. There are jobs like that. I know because I've done them. It might be called 'Report Developer' or 'Data Analyst', something like that, but I did essentially this for 3, 4 years.
To clarify, you get 2 cores/per-server 'free' when you license. 
Yes, there are a class of jobs you are qualified for. I would look for jobs called 'Report Developer' or 'Data Analyst'. Sometimes you might see it as 'Database Analyst'. Some people call this role a DBA, but it isn't really. What you really want is a role called something like 'Database Developer'. The database developer is good at database-side code and database design. They work with front-side developers - usually 1 competent DB developer can supply 3, 4, 5 front-side developers. 
Work your ass off learning SQL. Be the person that everybody goes to when they have a difficult SQL problem, or when their query takes 40 minutes to complete. Front end reporting tools come and go, but SQL has defied all attempts to kill it, and it will continue to be relevant and employable for the foreseeable future. Also, good SQL gives you the biggest reporting bang for your effort. 
Dude, tell me about it. Fellow beginner here and I, too, find VFP super difficult. What program are you using? Also, please feel free to share any resources you find that you think would help us learn more about VFP, and I'll do the same. Good luck!
Interesting, I learned something today
Cool! The MS Documentation wasn't totally clear on that, and I haven't had a working foxpro installation to use in years. Hope you find it!
I did read the guide.... &gt; **Per Core Licensing Model**: Purchase a core license for each virtual core (or virtual processor/virtual CPU/ virtual thread) allocated to the VM, subject to a four core license minimum per VM. I'm reading that as I need to buy one four core license for SQL on a VM that has 4 cores. Regardless of the hardware of the host the VM is running on. Am I not getting it?
I would say you have a good chance of landing a job involved in databases. I just got hired as a SQL Server SME, and my SQL background is pretty much similar to yours. Thankfully, I will be provided with a training plan to revisit SQL basics and to learn more complex features of SQL that I may not know yet.
No, I am teaching myself. 
It's a giant tome, and probably 15 years old, but if you want to really understand the idea of transactions and fail-proof computing, it is the best.
You can use a binary sort to get the results you expect: eg select * from WebData where LastName like '%[A-Z]%[A-Z]%[A-Z]%[A-Z]%' collate collate Latin1_General_BIN; 
You could try AdventureWorks - a free educational database that Microsoft provides. They reference it with examples in the documentation for SQL Server. FWIW, the "SQL Server Online Books" reference manual is one of my favorite bookmarks of all time: http://msdn.microsoft.com/en-us/library/bb510741.aspx In any case, you should quit using SQLzoo for practice and work towards building your own database instance. At that point, the sky's the limit. 
Microsoft SQL Server Express is free to download and there's an AdventureWorks sample database which is also free to download and install. This database is fairly large and will provide you with plenty of data to experiment on. I wrote [a series of articles](http://www.drewslair.com/2014/04/sql-server-for-beginners-1/) on getting started with SQL Server and Part IV explains how to install the AdventureWorks DB. There are probably similar resources for MySQL but I'm not familiar with them. 
Oh man! Thanks a lot! I love the articles; literally what I was looking for. This is awesome!! I've downloaded MS SQL Server Express, and my next task is to get to AdventureWorks. Very helpful tips. Can't thank you enough!
Glad to be of help. Have fun!
If you go the SQL Server route, be sure to check out the free courses at [Microsoft Virtual Academy](http://www.microsoftvirtualacademy.com). They have everything from the basics to data warehousing.
Data.gov is one place for open datasets to practice on. You can practice db design by loading data into there.
If you use Profiler - don't turn too many events on, for example locks. It can bring a server to it's knees. Batch start and end is normally ok though.
Also, if you'd like real practical examples you could always help me out! I extract information from databases and get stumped at times.
if you are looking for live data, you can download a complete dump from stackexchange, which is a couple of gigabyte [link](http://blog.stackoverflow.com/2009/06/stack-overflow-creative-commons-data-dump/) One small piece of advice thou, be very careful using the word "expert" on SQL. There are MVP's out there that don't call themselves experts
An alternative to the MS route is to get something like Oracle MySQL and one of the many compatible SQL tools (e.g. MySQL Workbench). You really can't go wrong with either route but you'll notice that each vendor has different SQL implementations and features. You should also consider getting an intro database book that covers relational algebra and database modelling besides SQL. Complex queries require some understanding of relational algebra and set theory, at least on a fundamental level (the math is not necessary for basis use). You should also learn about entity-relationship modelling, database schemas, normalization etc. These are methods and basic building blocks for creating your own databases and not just querying existing ones.
Which database platform?
MS SQL
HAHAHA I just noticed that this powerpoint is from the same author as the book recommended in OP's post. sorry!
If you want best practices, Itzik Ben-Gan's TSQL Querying book is fantastic for it. I know you said no syntax but there's no getting around that if you want to go deep into getting better on a specific platform and there are enough differences across the various databases that a platform agnostic tome won't be able to get you all the nice tricks available to that system. Learning how to use CTEs, CROSS APPLY and the WINDOW functions made me a much better query writer in a very short amount of time when I moved from SQL Server 2000 to 2005 and onwards.
It should go around `(RENTAL_FEE * .9825)` which would make it `ROUND((RENTAL_FEE * .9825), 0) AS DISCOUNT`, but you do not need the inner brackets so you can instead write it as `ROUND(RENTAL_FEE * .9825, 0) AS DISCOUNT`. Full query below in case I am unclear. SELECT MARINA_NUM, SLIP_NUM, OWNER.OWNER_NUM, OWNER.LAST_NAME, RENTAL_FEE, ROUND(RENTAL_FEE * .9825, 0) AS DISCOUNT FROM MARINA_SLIP, OWNER WHERE OWNER.OWNER_NUM=MARINA_SLIP.OWNER_NUM
Odd, I swear that was the first thing I tried, must have just mistyped it somehome but it works. Thanks alot!
/u/Doublehyphen is right, but you asking this question is kind of concerning. i mean there are valid reasons to need this particular view, but there are a lot more reasons that stem from bad design. I mean if it is just homework, que sera, but if it isn't, consider reworking the discount fraction to be another table, possibly based on a CustomerClass field in Owner or the like. Also for a lot of applications you'll be better off applying the discount in the logic layer (where you can do all sorts of fancier cross checks, and do things like disallowing discounts with a coupon etc). 
correct me if i am wrong, but applying .9825 to the rental fee gives you the discounted fee, not the discount itself the discount appears to me to be 1.75% also, as an aside, learn JOIN syntax :)
Just simple homework, an intro SQL class. I had it right first but something was mistyped. I think i had an extra ( in there. I realize there are probably way better ways to do this, but this is what the homework asked for. 
You are probably right that it should be how much is saved as opposed to whta the discounted price is. I'll put both in just in the homework just in case. Please explain the join syntax you speak of. I based how I did this on an example in the book, we touched on Join but haven't used to much so I'm not real familiar with it.
Yeah it had the look of homework, but some people take their shiny new skills and run out and try to build infrastructure. Best of luck, SQL is great fun once you get good at it. 
&gt; Please explain the join syntax you speak of. replace this -- SELECT ... FROM marina_slip , owner WHERE owner.owner_num = marina_slip.owner_num with this -- SELECT ... FROM marina_slip INNER JOIN owner ON owner.owner_num = marina_slip.owner_num the more tables being joined, the more clarity and simplicity of the JOIN syntax as compared with the old style comma joins (which are deprecated, by the way) 
I've read the anti-patterns book before. It's not that good in my opinion. A lot of the "antipattern" solutions are poor. 
I appreciate your feedback. I like SQL and think I am picking it up pretty decently. 
makes sense, thanks for the help man!
select count(*) from mytable where mycolumn=0 and mycolumn is null
&gt;select count(*) from mytable where mycolumn=0 and mycolumn is null It can't be 0 and null, should be 0 or null. 
Maybe something like [this](http://stackoverflow.com/questions/63291/sql-select-columns-with-null-values-only)?
WHERE COALESCE(mycolumn,0)=0
WHERE ISNULL(mycolumn,0)=0
except that COALESCE is standard sql and ISNULL isn't :)
If you're only ever going to be joining on account_id and date, there's not much point. If you need to store a reference to these records somewhere else and join back to it a bunch, then it makes senses to use an identity column as the primary key. 
but even then, isnt' this going to count how many zeroes or nulls are in a specific column in the table not how many nulls/zeroes are in the row like OP asked?
Something like :- WHERE date_field between getdate() and getdate()-365 or WHERE date_field between getdate() and dateadd(m,-12,getdate()) or WHERE datediff(m,date_field,getdate()) &lt; 12
You have to do a recursive join to the same table. It should be something like this (you will have to refine the syntax: SELECT a.person_id ,a.date ,sum(b.value) FROM my_table a JOIN my_table b ON a.person_id = b.person_id and a.date&gt;=b.date GROUP BY a.person_id ,a.date Good luck
I think it's worked, I just need to check the data but it looks good... This is what I did: SELECT G1.UniqueID, G1.Date, SUM(G1.Amount) AS SumAmount FROM table G1 INNER JOIN table G2 ON G1.UniqueID=G2.UniqueID AND DATEDIFF(YY,G1.Date,G2.Date)=1 GROUP BY G1.UniqueID, G1.Date Update - it didn't work. I got results but when I take the data and add it together manually for the 12 month period around a date, it's a different amount, either more or less. Can you tell me why you made the A Date larger than the B Date and also why you output the SUM of the B value rather than the A value? Thanks. Update 2 - I get it now... joining table 1 to table 2 with date limits for table 2 means for each date in table 1, there are all the other dates from that date and up from table 2, so summing up the values from table 2 per date in table 1 makes sense. However, I do need to put in the range of 1 year between the dates, but I can't get it work. This is what I've done: SELECT G1.UniqueID, G1.Date, SUM(G2.Amount) AS SumAmount FROM table G1 INNER JOIN table G2 ON G1.UniqueID=G2.UniqueID AND G2.Date&gt;=G1.Date AND DATEDIFF(YY,G1.Date,G2.Date)=1 GROUP BY G1.UniqueID, G1.Date For the date of the first value in G1, I'm getting a higher amount than I should based on the data. Update 3 - Using DATEDIFF with day rather than year works, I think. (I have to go offline now but thanks for your help.)
You have to create a transaction to get atomicity. So before your statement you would issue: BEGIN TRANSACTION Then after the second transaction you issue COMMIT TRANSACTION If the second insert fails, the first one will roll back. 
First you'd probably select in an aggregating function SELECT SUM( COLUMN_TO_SUM ) then add a GROUP BY GROUP BY PERSONID
how is unit price stored? 1938.0293293? 19.99? 191293822?
I posted this blog post a couple of days ago [SQL Sample data sets : SQL](http://www.reddit.com/r/SQL/comments/1wqgam/sql_sample_data_sets/ "SQL Sample data sets : SQL")
Bags Richard! Did you mean to reply to my comment or the OP?
I'm shocked that people not only watched this, but *confess* to it.
It was a book series first. &lt;/thatguy
how is unit price stored? 1938.0293293? 19.99? 191293822?
If you have several billion of the entities, it could indeed hurt. Assuming account_id is an int (4 bytes), we'll say unit_price is a max precision decimal (9 bytes) and the date is a regular datetime (4 bytes). So as it is, the row is 17 bytes. If you have billions of rows you probably need a bigint for the identity, so that's 8 more bytes. A guid is 16 bytes. So you'll be increasing the size by either almost 50% or 100% of that table by adding one of those columns. That really adds up when you multiply times a couple billion. If you're not using it for joins, what's the point?
Using SET ROWCOUNT is just as slow.
I read the series. Watched like 4 episodes.
It's just a treasure trove of bad SQL... really curious if I'm just missing something... how do people even learn to write it this way?? SELECT Member.name AS Singer_Name, Band.name AS Band_Name, Release.title AS Album_Name, Song_Num FROM Band, Release, Memberof, Member, (SELECT COUNT(song.title) AS Song_Num, Release.rid AS Release_Rid FROM song, Release WHERE song.rid = Release.rid AND TYPE = 'album' GROUP BY Release.rid ORDER BY Song_Num DESC) AS sub WHERE Release_Rid = Release.rid AND Band.bid = Release.bid AND Memberof.bid = Band.bid AND Member.mid = Memberof.mid AND song_num = (SELECT MAX(song_num) FROM (SELECT COUNT(song.title) AS Song_Num, Release.rid AS Release_Rid FROM song, Release WHERE song.rid = Release.rid AND TYPE='album' GROUP BY Release.rid ORDER BY Song_Num DESC) AS sub2) ORDER BY Band_Name 
It was so bad. Richard was cast as some pussy twink. Kahlan was at least moderately attractive. 
Why isn't your sql instance patched? That 2050 should be a 5000
.
How is that? My understanding is that SET ROWCOUNT causes the selection to stop after the count has been reached, whereas TOP will select everything, then return only the top x rows.
It looks to me like the author was attempting to show that NULLs are bad because you can't JOIN against them...maybe? My assumption was they didn't know how to use VALUES or their DBMS didn't allow SELECT without a FROM and they had no dummy table to use. So they instead just used CTEs to build two rows in two tables (people, heights). The final statement shows that even though Josh exists under both, with the same value of NULL for age, the JOIN does not include him in the final results...? Anyways, that is the best I could come up with. 
Interesting! At what point does this happen? Immediately or partway through? Are you restoring between different patch levels or editions? Anything interesting output in the error log?
&gt; I'm just left thinking WTF and wondering how someone can successfully use CTEs and yet seem to have such a fundamentally bad understanding of the Relational model CTEs are not complicated or special (aside from recursive CTEs). If you can write a subquery, you can write a CTE. I'm not getting why you think that writing a CTE is some bar or standard for understanding SQL. &gt; Maybe I'm missing/misunderstanding something, but I feel like I'd like to tutor whoever wrote this for a bit... It looks like a demonstration of three-predicate logic (TRUE, FALSE, NULL) in SQL. It's not *bad* though, it's just not how SQL works.
Well I figured it out, I was using the gui to restore the db I switched to running it with tsql and I got a lot more helpful output. I guess since I didnt select perserve replication it removes any replication during the restore process. I brought the db online and followed steps from the following. http://technet.microsoft.com/en-us/library/ms151782.aspx Msg 3165, Level 16, State 1, Line 1 Database 'tlmain' was restored, however an error was encountered while replication was being restored/removed. The database has been left offline. See the topic MSSQL_ENG003165 in SQL Server Books Online. Msg 3167, Level 16, State 1, Line 1
I'm on my tablet so it's hard to type and I might be wrong about the syntax, but I think something like this would work. Would probably perform terribly. I'd have to think about it a little more to figure out how to get it to work faster in 2008 without the `RANGE`. SELECT a.curr_date, a.val, b.runsum FROM tbl AS a CROSS APPLY ( SELECT sum(t.val) AS runsum FROM tbl AS t WHERE t.curr_date &lt;= a.curr_date AND t.curr_date &gt; DATEADD(y, -1, t.curr_date) ) AS b
This post might be helpful to you? http://www.sqlperformance.com/2014/01/t-sql-queries/grouped-running-totals
I think you are on the right track with the self join but I would try comparing using DateAdd rather than DateDiff
I have heard that before a countless times, "I don't think there will ever be a need for ...". If there are no other reasons (performance, size ...) besides not seeing the need for the key right now I would just add it anyway. 
Yep, DATEADD works better. Thanks.
I haven't used CROSS APPLY before but I got it to work without it. Thanks.
Select * From production inner join sales On production.filename = sales.name Is that what you're asking?
I found how to inner join them. However I ended up with three columns that I dont need. * Select Production.Document.Filename, Production.ProductDocument.DocumentNode, Production.Product.Color, Purchasing.PurchaseOrderDetail.DueDate, Purchasing.PurchaseOrderHeader.ShipDate * From Production.Document * Inner Join Production.ProductDocument * ON Production.Document.DocumentNode=Production.ProductDocument.DocumentNode * Inner Join Production.Product * On Production.ProductDocument.ProductID=Production.Product.ProductID * Inner Join Purchasing.PurchaseOrderDetail * ON Production.Product.ProductID=Purchasing.PurchaseOrderDetail.ProductID * Inner Join Purchasing.PurchaseOrderHeader * On PurchaseOrderDetail.PurchaseOrderID=Purchasing.PurchaseOrderHeader.PurchaseOrderID 
&gt; However I ended up with three columns that I dont need so just remove them from the SELECT list also, i think you'll find the use of table aliases will remove some of the "noise" from the query, to let the "signal" show through more clearly... SELECT doc.Filename , pohdr.ShipDate FROM Production.Document AS doc INNER JOIN Production.ProductDocument AS proddoc ON proddoc.DocumentNode = doc.DocumentNode INNER JOIN Production.Product AS prod ON prod.ProductID = proddoc.ProductID INNER JOIN Purchasing.PurchaseOrderDetail AS podet ON podet.ProductID = prod.ProductID INNER JOIN Purchasing.PurchaseOrderHeader AS pohdr ON pohdr.PurchaseOrderID = podet.PurchaseOrderID 
thanks so much! Im a total noob, and appreciate help like yours!
I've done inserts of 1.2 million line CSVs and it works fine using the import feature of SQL Server Management Studio. Works fine as long as you're not trying to insert a string into an integer column.
Why do you need a new table? Why not use the invoice numbers as a key and then have a "InvoiceDetails" table if you don't want to store it all in one table - not sure why you would want a new table for each invoice unless each invoice is going to have none of the same data types from invoice to invoice (which is weird in itself)
Alternatively, you could make a header table that uses generic info, then link that with a detail table that uses the invoice number. You could then turn the detail table to not use the invoice number as the primary key (have a row index be your unique piece for reference) and have the expanding space that I think you're looking for. Don't make a table for every invoice. That's what nightmares are made of. Edit: I meant this to piggyback off /u/qinar. 
How would I save each item from the invoice? A table with columns: Invoice Number - Item Number - Units Purchased?
try something like this: invoice[id, customer, date] item[id, invoice_id, sku, qty, price] select * from invoice, item where item.invoice_id=invoice.id 
Best answer. Read up on normalization and implement it.
Can a customer have more than one invoice? You may want to have a table for all customers, a table for invoices, a table for invoice items, a table for other stuff, like order fulfillment/payments. Do not create a new table for each invoice.
This better not be a homework assignment... INVOICE_TABLE (**Invoice_Number**, InvoiceInformation,.....) INVOICE_ITEMS (**TRANS_NUM**, *Invoice_Number*, *Item_Number*) ITEM(**Item_Number**, Item info.....) BOLDED numbers are Primary Keys, Italic are Foreign Keys 
Thanks for the example, and no it's not a homework assignment. My high school doesn't have anything remotely related to CS.
Further, what you are looking for is a Many to Many Relationship, in order to do that in a DB, you need a Transaction table or Junction Table... which is what INVOICE_ITEMS Table. 
I got the pivot to work, but now I need help with joining 3 additional tables. The source tables are all identical to my source example, the only difference is that they are named sales1, sales2, sales3, and sales4. Each sales table holds older data. So far I have: select * from (select client, Entry_date, item FROM sales1) PIVOT (MAX(Entry_Date) as Entry_Date for item in ('MAC' as MAC, 'MICROSOFT' as MICROSOFT, 'ASUS' as ASUS))
Wait. No. Why would you do a Cartesian product? Either join (which will duplicate the invoice row for each item) or just filter by the invoice id.
How is this a cartesian product? jesse_dev has a join in the where clause.
That would be a fairly silly RDBMS. Column comparisons like this one are always turned into a join when possible. Writing the join out in ANSI syntax is not necessary - the semantics of the query are equal no matter how you write it out. A silly RDBMS could solve your join query using a cartesian product and then filtering as well.
There's a logical join, which is what the FROM...JOIN syntax declares. And then there's a physical join, which is the operation the database uses to solve your logical join. A physical join can be used to solve queries that do not have logical joins explicitly spelled out in them. It's a good idea to spell out the logical join with the JOIN syntax, I absolutely agree with you there. The reason for it is readability for the users though, and not because the DB would use a cartesian join to solve it - that's simply not true for any RDBMS worth it's salt.
Holy crap...I found a script in there containing [wan ip]/phpmyadmin And yes, it is remote accessible. It might be just me, but erm... awesome idea right ?
for what it's worth, here's the code I used in the Stored Procedure... with x(fn, cnt, list, trx_id, l) as ( select File_Number, count(*) over (partition by File_Number), cast(Hold_Type as varchar(100)), Trx_ID, 1 from Hold h union all select x.fn, x.cnt, cast(x.list + ',' + h.hold_type as varchar(100)), h.trx_id, x.l+1 from Hold h, x where h.file_number = x.fn and h.trx_id &gt; x.trx_id ) select * from h
Are you joining, or are you just adding as additional data, in which case you should use UNION
I had never used UNION before, but it looks like that did the trick. (I am not joining.) I just tried it with two table and it looks like it worked. I'm thinking that the other 2 tables will be pretty painless. This is what I have so far: select * from (select client, Entry_date, item FROM sales1 UNION select client, Entry_date, item FROM sales2) PIVOT (MAX(Entry_Date) as Entry_Date for item in ('MAC' as MAC, 'MICROSOFT' as MICROSOFT, 'ASUS' as ASUS))
What you want is FOR XML([full length article link](http://davidduffett.net/post/5334646215/get-a-comma-separated-list-of-values-in-sql-with-for)). Basically uses XML to build a comma delimited list. Sample: *** SELECT P.ProductId, STUFF ( ( SELECT ',' + ModelThisFits FROM ModelProductFits M WHERE M.ProductId = P.ProductId ORDER BY ModelThisFits FOR XML PATH('') ), 1, 1, '' ) AS Models FROM Product P Result *** ----------------------------------- | ProductId | Models | |---------------------------------| | 12345 | A3666,A3667,A8999 | -----------------------------------
bah I wasn't finished typing it out ..... I hate throwing away perfectly good code. Maybe a small side note, you can use the xml subquery together with outer / cross apply to do string concatination on a "joined" table For MSSQL, this is the best option you have. On Oracle there is a function to do basicially the same. On MySql, install another DBMS
asked the same question a few weeks ago, check it: http://www.reddit.com/r/SQL/comments/1v52yy/mssql_how_to_query_multiple_rows_to_form_one/ 
--from user svtr: IF object_id ('tempdb.dbo.#shirts') is NOT NULL BEGIN DROP TABLE #shirts END CREATE TABLE #shirts(articlename VARCHAR(20), color VARCHAR(20), SIZE CHAR(2)) INSERT INTO #shirts SELECT 'foo', 'black', 'S' UNION SELECT 'foo', 'black', 'X' UNION SELECT 'foo', 'White', 'S' UNION SELECT 'foo', 'White', 'M' UNION SELECT 'foo', 'White', 'XL' UNION SELECT 'bar', 'Black', 'S' SELECT s.articlename ,s.color ,fun.stringAggregated FROM #shirts s CROSS APPLY ( SELECT STUFF(sq.colorAsXml,1,1,'') FROM ( SELECT '|' + s2.size FROM #shirts s2 WHERE s2.articlename = s.articlename AND s.color = s2.color FOR XML path('') )sq (colorAsXml) )fun(stringAggregated) --edit: format
You can also install a custom CLR function to make this a group by aggregate.
came here to say this, would add a customer table and a billto/shipto table if you need to consider a customer with multiple ship-tos
Thanks! It looks like someone responded with your code from a few weeks ago below.
Thank you! I'll play around with this one when I have a chance.
Now that it's posted here, I remember seeing this code when you asked it a few weeks ago. The Cross Apply stands out to me because it's not something I use every day.
I think this would be a good idea, except Custom CLR functions are a little above my head and this is only the second time I've had a request for this in 5 years.
I can't accept credit for that code ;) I read it in some blog, or on sqlservercentral, I don't even remember where. Among DB nerds, its proberbly one of the best known hacks, cause everyone had to work arround the issue your having at some point. I just wanted to throw in the option of using it in a "apply", cause that makes it really funky
OP is obviously new to this subject; which is why I started simple. They can figure out how to optimize their queries on their own. Furthermore, I know for a fact that both mysql and postgres would treat the IDs as Indexes, and wouldn't do a scan or a cartesian product.
Yea... pastebin is definitely a possible vector for security leaks
These are not all SQL joins, just the equi-joins. Joins are not conditioned on the '=' operator alone (although it is the most common case). Still the best visualization I know of, and I still use it. Along with this caveat. ;)
I'd disagree with it being more readable. To me it makes no difference, to the developers here who are not specialized in databases it becomes less readable as joins kind of scare them. :) But that's just like... an opinion man. Based on a siutation that I'm in. So you don't need to adopt that ;)
You answered it yourself: for a beginner, WHERE joins seem more readable and simpler. When your databases grow, and your experience does too, you switch to the other syntax. It worked for me, at least :)
good article, quite rightly mentions that cross joins blow up the venn diagram concept also, /u/askur has a great point that the venn diagrams make sense only for equijoins
This is all interesting of course... but what if you don't know the number of columns beforehand? How would you approach that?
You would have to use [dynamic SQL](http://www.sommarskog.se/dynamic_sql.html). Some years back I once used a modified version of this [stored procedure](http://weblogs.sqlteam.com/jeffs/archive/2005/05/02/4842.aspx) (see comments for my version) to dynamically create a pivoted table. This proved to be a headache to maintain and I have stayed away from such solutions ever since. I do plan to do a follow up post on pivoting SQL dynamically.
I stand corrected, a brain fart. I somehow mistaken it with HAVING, which filters the results of GROUP BY clause
I'd love to see this with the older Oracle (+) syntax. I've got to support code with those occasionally and it's always a pain to wrap my head around it.
Spam.
I guess one advantage when updating a row, using WHERE prevents you from accidently calling a full join and updating everything!
UPDATES are quite a different beast. I always triple check my joined updates.
Not to be racist, but each to their own man. There's a C developer sitting next to me currently coding in Python. He's not on his home ground but he's still someone that produced his first award winning game when I was 4 so why the fuck would I think less of him because he's not at home in python, or databases for that matter? There's a web developer below me that I regularly go to when our API acts up because he's much more knowledgable about web service operations than I am. I'm into databases and algorithms, not web services. Not to be offensive, but I don't understand how anybody in a production environment would expect everyone to be equally capable in every area. I sure as hell do not intend to match the ability of everyone here in everything. I have a job to do.
[treasury.io](http://treasury.io/) [populardata.com](http://www.populardata.com/downloads.html) [mysql example databases](http://dev.mysql.com/doc/index-other.html) [imdmpy](http://imdbpy.sourceforge.net/) - python package for imdb [open world db](https://code.google.com/p/worlddb/) [mondial database](http://www.dbis.informatik.uni-goettingen.de/Mondial/) databaseanswers.org has a [data models](http://www.databaseanswers.org/data_models/) section but it's mostly graphics of the database design and in some cases has a Micro$oft access download for the schema. Tons of database examples though. You can actually interact with the stackexchange data dump online, no download necessary. This is probably the greatest learning tool of all. If you want to download *some* of the data, see this stackexchange [blog post](http://blog.stackoverflow.com/2014/01/stack-exchange-cc-data-now-hosted-by-the-internet-archive/) All that should keep you busy for a while ;) edit: forgot to add data.gov [data.gov](http://www.data.gov/) 
Iteratively. They write the most rudimentary part of the query, then add one more piece and futz with it until it works, then add another and then another until they end up with a basically incomprehensible mess. It's not different than writing a small script without any kind of plan and stuffing it all into one line. 
I'm glad someone else understands :) 
I had to do it for a client project. Was time-consuming and hit-and-miss. 
My understanding was that the middle left and middle right joins were simply LEFT OUTER JOIN or RIGHT OUTER JOIN. Is this image more universal? Which form of sql am I using that LEFT OUTER JOIN works?
you can download Oracle Express Edition http://www.oracle.com/technetwork/database/database-technologies/express-edition/downloads/index.html or set up VirtualBox and grab a VM: http://www.oracle.com/technetwork/database/enterprise-edition/databaseappdev-vm-161299.html
and YOU CAN play with Oracle for FREE using the OTN License: "LICENSE RIGHTS We grant you a nonexclusive, nontransferable limited license to use the programs only for the purpose of developing, testing, prototyping and demonstrating your application, and not for any other purpose. " http://www.oracle.com/technetwork/licenses/standard-license-152015.html You'll have to pay for a license if you chose to use the product in production or if you get paid for it...AFAIK...
Or you could do something reasonable like set up an OLAP cube or do this in a proper reporting program. 
sample? what sample?
Thank you Guys! I have much to work with :)
Could anyone give some examples of situations where you'd use the 'a.key is null' or 'b.key is null'? If you're doing a join, why would you not want result from another table?
If I am understanding this correctly... Unmatched queries. Finding things in A that don't exist in B.
You would use this to intentionally exclude results. For example, lets say you have a list of employees in your Employees table and another table for EmployeesOnVacation with a list of everyone currently on vacation. To get a list of everyone who is not currently on vacation, you could do something like this: SELECT Employees.Name FROM Employees LEFT JOIN EmployeesOnVacation ON Employees.EmployeeID = EmployeesOnVacation.EmployeeID WHERE EmployeesOnVacation.EmployeeID IS NULL You could do essentially the same thing with a subselect like this: SELECT Employees.Name FROM Employees WHERE Employees.EmployeeID NOT IN (SELECT EmployeeID FROM EmployeesOnVacation) In both cases, the result is everything in the Employees table that is not in the EmployeesOnVacation table (similar to the A not B diagram that is the lower of the two far left diagrams). You'll find this type of logic anytime you want something that is "not x" (e.g. all vehicles that are not trucks, all shoe styles that are not available in black, etc.) 
I'm sorry but i have not idea what you just said. When i said beginner i meant just that.
So the question is actually, "I want to deceive employers into thinking that I know SQL, when in fact I do not know SQL, so how can I do that?" Is that right?
No no. I want to learn sql by doing a project. A goal to be precise. 
I see--this is preparation for future possible employers. Well, do you know other aspects of programming (like a language, a GUI toolkit, or web development)? I ask because if you do, you could incorporate a database and learn (and later show off) some SQL skills that way. So, do you?
i'm learning html from code academy at the moment. I also was halfway into making a android app but gave up.
Thank you! That makes sense
You should try googling those terms...
dataset - A database or table (kinda like a spreadsheet or list) UI - User Interface - Make a program that lets you look at it in ways other than columns and rows. XML - A markup language, similar to html, but its purpose is to format data. Here is an example: http://www.kirupa.com/net/images/xml_doc2.gif Parser - He wrote an application that could read the xml file created by itunes that could then turn it into a database (or a spreadsheet) so he could easily see information about his itunes usage. *not the dictionary definitions, just what I was able to make up quick to help you understand
Well, to some extent it's a little like you are asking how to do a triple axle ice skating jump...when you're still learning how to just skate itself. Very basic SQL is not hard--it's rather like English sentences in some ways--but employers probably don't want to hire people with the most basic level of SQL. But even if you learned, via a project, some sort of OK or intermediate SQL stuff, it would be a little floating out there on its own and not connected to any other programming technology. HTML doesn't really count, as that is just a markup language, a way to describe where words and images should go and what they should look like. I wonder if you could start with something like Microsoft Access and create a database (a set of information) and the "queries" (things you "ask" of the database) and use SQL that way. Otherwise, I'd guess you'd have to learn a programming language (not HTML).
OP is very new to the subject, trying to connect a .net application I'm writing to a MySql server instead of using flat file... yuck.
It's just a ~ delimited file, if it's a one time import you could use excel to break it up using Text to columns (called something like that I'm not on a computer with Excel right now) and import it into the db as an excel file. If it's something that's got to be repeated with multiple excel files you can certainly achieve this with SSIS.
The first thing I always say when I look at a query is replace that distinct with a group by. 
Every user having their own username and password? Yes. Always. 
Clutch
Is it just for added security? In the case of Reddit, I wouldn't be able to access the database without signing onto Reddit. So they already know what I'm doing. I guess the database username and password seems redundant because it's automatically entered for us anyway.
Assuming you are using MS SQL, you would very much benefit from implementing [active directories]( http://en.m.wikipedia.org/wiki/Active_Directory). Basically, add the active directory as a Windows authenticated login on the server and users in the domain can login with any permissions granted to the active directory. For example, create an active directory called "SQL_ReadOnly", add as a database user with select permissions, and any Windows user added to that group will be able to query tables in the database but make no changes.
The server I use is actually Linux so I use mysql. But it looks like there is a mysql alternative so I will give that a look. Thank you.
I agree, if you already have an OLAP environment set up then its best to use that. The amount of effort and tools your put into pivoting data depends on your needs. Simple once off analysis in SQL and setting up BI platforms to serve reports and OLAP cubes require different magnitudes of effort.
Except the inner join, these are all outer joins. The 'outer' phrase is usually not needed to be specified as it is the default type of join. Edit: - derp
0.03 seconds seems like an excellent time. Lots of joins there. 
Ah I see your confusion. The Password and Username to access the Database is different, you only really need one for your system. What you then want, is the website to take your username and password, check it inside the database on a table of users, and see they have permission to move around the site. I think you've confused MySql Security for Site security. Reddit connects to a server, and has its own username/password to access all the tables. You then log in, reddit logs into the database, and looks up your username and password in a table inside the mysql db, that we shall call for example "users". If your username and password match a row in the table "Users" you then have access to other parts of the site that would normally be hidden if you didnt have access.
Would this allow me to quickly create a webform for modify/entering data into an SQL Server 2008 database? I'm struggling to find what it actually freaking does! edit: i'm very interested in this, i have this exact problem i think you're describing but i dont' want to hand code it all in c# or something like that- I'm not a developer and it would be functional but extremely ugly/time consuming.
Hmmm this does sound exactly what I'm after. I've used Eclipse before for creating some Android applications- it was an awful experience (i'm not a professional js dev, at all) would this require much programming experience? Or is database experience enough to take me through? thanks for your time btw! ninja edit: i'll message servoy for a trial now
True. The other option of course is to just pipe the data into a tool like Tableu and let that take care of binning, etc.
Ouch, what should the first line be imported as? maybe use the ~ as the delimiter, then join what columns you need after that? 
Eclipse does take a little getting used to. However the bits are all there (all the time, which is what makes it look complex). I came at it from being a Filemaker Developer many many years ago (and switched to Servoy in ~2000 - and NEVER looked back.) Servoy themselves went from a proprietary ide to Eclipse in with the advent of Servoy 4.0 which was about 6 years ago, and I had to become familiar with it. It was a learning curve, but I now like it, and can't imagine using something else. You don't have to know how to do **any** coding as you can double click to insert code or form elements by dragging them from the side, or importing sample code, or straight code its-self from the dom tree on the left. Obviously there is some coding required to make more and more complex solutions, but the process of working up to that is given to you by Servoy its-self, or on the Servoy forum, or googling javascript functions. To do what you have specifically identified (one form with some columns) would not require any coding. You can navigate between records with alt-up and alt-down, you can enter find mode by hitting cmd-f (assuming you're on windows), and perform your search once you've specified a search criteria in a find mode form by hitting return. Searches can be extended or reduced, and you can use wildcards (%), or operators such as &gt; or &lt; or ... (so you could find all entries added after a certain date by going cmd-f to enter find, selecting the date added field if you have one, and typing '&gt; 01/01/2014' (without the quotes) for example to find all records where the date added is greater than that date.) Searches can be combined, and you can use multiple search fields simultaneously to create complex reports.. I knew nothing about Eclipse, SQL or Javascript before I started using Servoy. I now feel very comfortable in all three because it teaches you that. I have bounced around between many SQLs in that time too (well, Sybase, Postgres, Firebird, and MySQL). There is no trial version of Servoy. You download the full thing (without a license key it is referred to as the community edition) - for free. If you start to deploy a solution to end users, you would need to buy licenses for the number of potential users you'd have (and it is concurrency based, not per computer). That is where Servoy makes their money, by selling licenses to the clients that you deploy your solution to. I think it is $350.00 per license - but even that I think is community edition if you need less than 5 licenses...
Also, if you're deploying just for yourself (non commercial - just so that you can gain access to an easy way to interact with your data), you could do it all with the community edition.
When you do text to columns, don't choose "Fixed Width" choose "Delimited", then choose other for your delimiter and type in "~"
In terms of the actual login the site uses to connect to the database, you'd typically have a separate login for each 'application'. So the reddit website probably uses a single db login, maybe there is a search engine indexer which uses another, and probably a heap of backend tools which each have their own login too. Its not related to user logins on the website, which are purely application logic (an entry in users table).
I have a few minor issues; I find your script painful to read. I (and many others) prefer aliases to only a couple of letters that help identify the purpose. If I can get away with one letter, I'll use one. This way, it's easier to read and evaluate. Furthermore, I can get the ON clause on the same line as the joins, which makes it easier for commenting them out. I think a standard is to have aliases capitalized, but I personally don't like that because it doesn't stick out as much between AS and ON. The blank lines in between each join was just so I could easily identify which tables aren't directly referenced outside of the joins. e.g., BaseItemCodeApplications isn't used in the where clause. I was just making sure there wasn't an unnecessary join. SELECT DISTINCT minh.Height FROM BaseItemCodes AS ic JOIN ProductSeries AS ps ON ic.Series = ps.Series LEFT JOIN BaseItemCodeApplications AS a ON a.BaseItemCode = ic.BaseItemCode LEFT JOIN ApplicationOptions AS ao ON a.ApplicationId = ao.Id LEFT JOIN BaseItemCodeInstallations AS i ON i.BaseItemCode = ic.BaseItemCode LEFT JOIN InstallationOptions AS iopt ON i.InstallationId = iopt.Id --* io might be reserved, so used iopt LEFT JOIN ProductSeriesCerts AS psc ON psc.Series = ps.Series --* I really don't like this ON clause. --* It's a left join so the assumption that --* the left side of the on clause would take --* priority. Having it backwards and makes it --* harder to interpret at a glance. LEFT JOIN CertificationOptions AS co ON psc.CertificationId = co.Id LEFT JOIN MountingHeights AS maxh ON ic.BaseItemCode = maxh.BaseItemCode LEFT JOIN MountingHeights AS minh ON maxh.BaseItemCode = minh.BaseItemCode --* Why not reference the ic.BaseItemCode? WHERE ao.Id = 1 AND iopt.Id = 1 AND co.Id IS NULL AND ps.HasRecessedMount = 0 AND ( ( maxh.MountingContext = 'insect' AND maxh.Type = 'max' ) AND ( minh.MountingContext = 'exterior' AND minh.Type = 'min' ) ) Now, in terms of performance. Really it's difficult for me to say. If it takes 3/10's of second to return 10 rows from 1,000 records and it takes 4/10's of a second to go through 1,000,000 records, then it's probably not a big deal. Your indexes might make a big difference. 
Play The Schemaverse :D http://schemaverse.com I know of at least one player who has benefitted by name dropping the game in an interview.
I recently wrote something for my media collection. Every show or movie is a record and has meta data stored in the columns, and built an Excel spreadsheet to pull the data based upon what you queried e.g. comedy pulls up all the comedy shows and movies, searching for Jack Black would bring up everything with him in it. It is something built off of data that most people have, or can be easily gathered. Just remember if you are going to use it to impress someone, put some work into it and care about it, nobody will care about something you didn't care enough about to make nice. Keep learning and keep trying, your skill-set will only grow :)
This is exactly what I was looking for. Thank you. So it's not a security risk to have the username and password hard coded into something like PHP? 
If your using a password/username to get access to a DB, then it kind of has to have it hardcoded into the site. The individuals would then need to be checked inside that database inside a user table. So if im not logged into reddit, I cant see any PM's or my fav links. But if I log in, reddit checks my unique username and password I have, with a database table, and if I exist, it shows me my private links. Theres a lot more to it than that, but I suck at explaining things myself. Best thing to do, is create a page, that pulls in data from a database (name of site, or some text from a table.). Then work from there. Create another table called users (or what ever), with just a name and uid, then have a log in page with just a name, type it in, have it check the user table, if that user exists, show a "hello $name". if it doesnt show "sorry you dont exist". Thats a very very basic example of it. Then you start getting into passwords, salts, hash's. auths, etc etc. But I always learn things by building small things, and breaking them ;) 
If it is already in a file I would go to the data tab in Excel and choose "From File" and point it to your data set. Then you will see the import wizard, choose "Delimited". Then you will have to choose ~ as your delimiter. After that you can select the columns you want to import and what type of column it is (Text, Date, Number, etc)
If you learn SQL for one product, it's relatively easy to learn it for the rest; the core concepts are the same. Although I *highly* recommend learning SQL with SQLite instead. Or any other easy-to-set-up database software. Access's raw sql editor is not user-friendly.
Yes. SQL is common amongst all Relational Databases. When you start going into the Big Data realm, it's used alot less. That being said, SQL is pretty common and a great skill to learn. As for using Access to learn SQL. First thing, IT is fine for learning SQL, but has some drawbacks. * Certain functions and keywords are different in Access than they are in most other SQL variants. Microsoft has made Access more like excel and reserved many words which can be confusing. One example is the wildcard function, in most SQL's it's a % sign, but in Access it's a * . * Access's SQL interface sucks, you can't adjust font side, no keyword highlighting, and it's not easy to find. If you do use Access, i'd recommend writing your SQL in a Text editor like Notepad++. This will highlight your keywords, and help you a bit along the way. * Access has wizards for everything, but the SQL it generates is riddled with parentheses and brackets. Just know that many times they aren't needed. * Access handles the Data Manipulation portion of SQL really well, but i've had problems with the Data Definition parts of SQL don't work well. * All Databases have their own variants of Data types, but Access is the most different that i've seen. All this being said, Access is fine to learn with, I use it for the college class i teach on using SQL. We use it because it is what the Textbook i use already uses and it's owned by most students. If you want to be more proficient in SQL, i'd recommend learning with Access, but also practice and learn with mySQL. It's free, and easy to use. Background: I'm a DB Admin, and teach a class on Database Systems at a small college. We use Kroenke's Database book and Access, but i often use MySQL for demonstrations. 
This was awesome. thank you 
I'll take your advice and download SQLite right now. Thank you!
I wouldn't use SQLite, as it has many limitations (not sure which ones had been removed, but foreign key, typing, some outer joins ...). MySQL also had some of those. You will be better served with PostgreSQL, or the express version of many of the commercial ones, like MS SQL Express (I think Oracle and DB2 have similar versions)
For somebody learning the basics, sqlite will suffice. What's more important is allowing a beginner to easily and quickly dig in, and the bigger DBs fall back on that. Installing and configuring something like postgresql is a big turn off to somebody inexperienced with it.
Postgres and Mysql are tough to set up for the beginner.
I printed this sheet off about a month ago to put at my desk. It has helped me tremendously with learning how joins work. 
I'm having trouble making sense of your table definition. Could you please provide a full definition for both tables? Having a quick list of each column's name and datatype would be very helpful. Edit: I graphed it out on my own. I think the key part you're missing is the COALESCE function. It returns its first non-null argument, or NULL if all arguments are null. (IMPORTANCE when CHANGE_NO is null) -- would be COALESCE(IMPORTANCE, CHANGE_NO) On top of that -- for your last line, if you use aliased tables you can join a table to itself. This might help you find what you're after. 
I'm new to a job and we use Microsoft SQL Server 2008 R2. My background with databases goes back about a decade but my familiarity with SQL is fairly limited and sparse in recent years. Do you have a recommended course of action? In reality all I'm ever going to be expected to do is write complex SELECT queries, but the databases are vast and a query may need to join with multiple tables or columns. Sometimes when I'm inside the program and need to start a query I just blink at the blank screen and my mind goes numb... and I know how to program in about 5 languages. For the time being I have some people around me who are assisting me, and a few pre-written queries that I can edit my way through but I'm going to need something more substantial as quick as possible.
&gt;(IMPORTANCE when CHANGE_NO is null) != (IMPORTANCE when CHANGE_NO is null) This part makes no sense. You'd never get a return.
Yea, 'distinct' is shorthand for 'group by all columns selected'. 
'Outer' is not needed for FULL JOIN either.
Well, just looking at it, I question why you even need to join productseries, productseriescerts, certificationoptions. Also, the self join should probably be a join on baseitemcodes. Another thing is you are specifying the 'applicationoptions' id so why do you need to join applicationoptions on id when you know the id is 1. same for installationoptions. 
Noted!
You are missing something. He's not selecting baseitemcodes.baseitemcode he's selecting MinHeights.Height. Grouping by baseitemcode would be pointless in this query since it's '1' anyway. 
The self joins would probably kill paralellism. Also, my guess is he was troubleshooting this query and thus set the applicationid to 1 and the applicationoptions to 1 (and removed the group by) just to test, so that 0.03 seconds per item is probably the "slowness" 
Agree. Access sql is has syntax requirements thats different from sql.
Sql express is free from Microsoft. I would say transitioning to other sql platforms is easier as you understand core concepts that other platforms require an intermediate knowledge of sql servers. MySql now has its own studio which looks pretty nice. Prior to the studio you had to use a third party tool or command line to execute your sql commands. MySql has changed a lot over the last 7 years.
SELECT REPLACE(number,'*','')
 WHERE Table_ItemSN IN ('PART4', 'PART5') or WHERE (Table_ItemSN = 'PART4' OR Table_ItemSN = 'PART5')
Sure you don't want to be using OR instead of AND? A column can only have one value at any time.
I'd still consider MS Sql Express - the AdventureWorks DB is a great learning tool, and there's a ton of tutorials out there on how to interact with that sample DB specifically.
Yup, you're right... Thank you.
&gt;WHERE (Table_ItemSN = 'PART4' OR Table_ItemSN = 'PART5') Solution. Thank you.
I prefer using CTEs to derived tables. It feels much easier to read.
Try this SELECT CHANGES.CHANGE_NO FROM CHANGES JOIN HIST_CHANNEL ON CHANGES.CHANGE_NO = HIST_CHANNEL.CHANGE_NO WHERE CHANGES.CHANGE_DATE &gt; 01-JAN-2008 AND CHANGES.CHANGE_NO IS NULL AND HIST_CHANNEL.CHANGE_NO IS NULL AND CHANGES.IMPORTANCE &lt;&gt; HIST_CHANNEL.CHANGE
Application is fortunately not referring to a software application, of an application of the product, such as where it could be used. It very most likely won't be called 1000 times a second either. More like once every 30 seconds.
I think he means (TableA.IMPORTANCE when TableA.CHANGE_NO is null) != (TableB.IMPORTANCE when TableB.CHANGE_NO is null)
Edited: You're totally right. I took another look at it. I didn't need ProductSeries, or any of the *Options tables. I still needed ProductSeriesCerts because I need to filter away whether there's a null column there. And the self join should have been on BaseItemCodes. You're right, thanks again!
You are correct. I was just setting both of those ids to 1 just to test. The where clauses are actually going to stay, just with variable inputs.
Essentially, for a simple query, yes distinct and group by are the same. Though once you start adding more joins, sub queries, etc. the difference becomes much more obvious. Distinct will be a lot more memory intensive and requires the distinct fields to be stored and sorted in a temporary table, while the group by will use a hash table. It also makes a difference when the grouping/distinct is executed. When joining to a sub query, group by will group the data before the sub query is run. Also, in my experience, the use of distinct is often used to cover up a poorly designed or though out query where joins have not been properly tested. 
Last date of previous month: SELECT DATEADD(s,-1,DATEADD(mm, DATEDIFF(m,0,GETDATE()),0)) In your case (even though I think you didn't copy the entire sql query): (CAST( FLOOR( CAST(mytable.datecolumn AS FLOAT ) ) AS DATETIME) BETWEEN '2013-04-01 00:00:00.000' AND DATEADD(s,-1,DATEADD(mm, DATEDIFF(m,0,GETDATE()),0)))
That's a pretty neat tool there. I did have some tables that needed to be cut out, and so I've updated the query a little. I also cut my LEFT JOINs down a little bit, since you're right for pointing that out - the only join I have in which there would be a `null` value, is the join on ProductSeriesCerts.
thanks. I'll give that a go tomorrow and report back. FWIW That is/was the full query as it was presented to me - I've always suspected the user isn't shown the full query though, which actually makes it slightly more confusing when I try to decipher it using SQL guides etc...
Alrighty! Thanks for your advice! I've cleaned it up a bunch based on your and others' suggestions. I edited the updated query into the original post.
do you want to know about recursive cte's or just what a cte is? I'll start with the easy one... a cte is pretty much just an alias for a subquery. ;with myCTE as ( SELECT something FROM table1 ) SELECT * FROM myCTE cte INNER JOIN table2 t2 on cte.something = t2.something Thats pretty much the same as SELECT * FROM ( SELECT something FROM table1 ) cte INNER JOIN table2 t2 on cte.something = t2.something CTE's are nice for making something a bit more readable, while keeping it fully inlined. Writing it to a temp table would not be inlined, if you are wondering what the difference is 
I heard that it is possible to turn (x) vertical row data into (y) horizontal column headers and group (z) values under (y) the with CTE. I hope this does not confuse you 
&gt; I heard that it is possible to turn (x) vertical row data into (y) horizontal column headers and group (z) values under (y) the with CTE. I hope this does not confuse you I don't quite follow i'm afraid. It sounds like pivoting data, but that is not tied to CTE's. That would be the pivot statement, which 80+% of long time sql people have to look up the syntax on, so forgive me for not responding with a script ;)
PIVOT is an awesome pain in the ass. When it works, it just blows your mind with how amazing it is but getting it to return what you're envisioning is often very difficult. I'm not proud to say this, but once I used a cursor to pivot the data because I couldn't get the PIVOT to work the way I wanted.
You're right about pivot being a pain, but meeeh dont use cursors, cursors are just meeeeeh. 
SQL Server: As long as the html is xhtml compliant, you can handle it using the xml nodes. This example is assuming you are receiving the html as a string: DECLARE @html_string AS NVARCHAR(MAX) DECLARE @xml_string XML SET @html_string = '&lt;div class="checkbox-group"&gt; &lt;label for="A"&gt;&lt;input type="radio" name="prefix_A" value="valueA" id="A" /&gt;Question text for A&lt;/label&gt; &lt;label for="B"&gt;&lt;input type="radio" name="prefix_B" value="valueB" id="B" /&gt;Question text for B&lt;/label&gt; &lt;label for="C"&gt;&lt;input type="radio" name="prefix_C" value="C" id="C" /&gt;Question text for C&lt;/label&gt; &lt;/div&gt;' SET @xml_string = CAST(@html_string as xml); WITH node_val AS ( SELECT Y.ID.value('(input/@name)[1]', 'NVARCHAR(100)') as name ,Y.ID.value('(.)[1]', 'NVARCHAR(100)') as question FROM @xml_string.nodes('/div/label') as Y(ID) ) SELECT name + ', ' + question FROM node_val Will give you the result: &gt; prefix_A, Question text for A &gt; prefix_B, Question text for B &gt; prefix_C, Question text for C edit: Formatting
You're looking for pivot/unpivot. The only issue with these is that you need to know what columns you will have at the end. This means that if you want to have a dynamic number of columns, you will need do something horrible; I can post an example if you want.
Why do SQL Server users prefix some of their queries with a semicolon? I have never seen Oracle, MySQL or PostgreSQL people do that.
concatonating the distinct values of a column into a dynamic sql doing the pivot? Oh yes, thats always fun isn't it :D
I usually just put the semi-colon after the query which needs to be terminated.
to me its personal preference, in T-SQL it doesn't matter if you terminate a query with the semi-colon or not (the cte's being the exception). Its just personal (or company wide) coding conventions
That is mostly due to crappy getting started guides online. MySQL and PostgreSQL are ridiculously easy to get running on Linux and Mac if you just follow the right steps, too bad almost all guides over complicate everything. For example why do so many PostgreSQL guides insist on touching pg_hba.conf? You usually do not need to do that on dev machines. I do not deny it is a real issue for beginners, I am just sad about the reason for it. My steps to install PostgreSQL on Debian/Ubuntu ("id -un" is a command which gets your username): sudo apt-get install postgresql postgresql-contrib sudo -u postgres createuser -s `id -un` Create a database and connect to it: createdb foo psql foo You do not need to do anything more to install, configure and use PostgreSQL until you want to deploy it on a production server.
Yeah, what interested me is that I have only seen it used by SQL Server guys and wanted to know if there was something about SQL Server which made it extra useful.
That helps a lot, actually. I have a somewhat limited background in finite mathematics (matrix theory) and when it comes to thinking about joins the idea of thinking about the columns specifically seems to ring a bell. It's frustrating because (from the perspective of a DBA or programmer), I'm really only using a handful of commands or syntaxes... but they quickly spiral out of control into huge paragraphs of structure. It makes me think it's more similar to Excel, but it's really quite the opposite of that -- but also much similar if you compare it to a formal language. Any good books/online videos/etc., that you'd recommend? I'll probably only be working with SQL for 2 years or so but I like to get good at things.
Mhm, I can't really recommend a book about the concept of set based logic. And that's all that SQL is in a nutshell. I could give you a lot of reading advice on MSSQL performance optimizing and stuff like that, but I can't really give you a recommandation about the basics of the concepts. Sorry about that, it's like with everything, once you got it, its so fundemental to you that you have trouble to put yourself in the shoes of someone that hasn't got it down yet. (please don't take that the wrong way, I don't mean to come over as a feeling superior asshole) If I may say so thou.... 2 years, is more than enough time to "get it down". If you like abstract logic, chances are you are gonna like doing database kind of work. Regular programming is simple logic, database programming is abstract logic, thats how I would describe it. One of the upsides of being a database nerd thou, regular programmers can't do that (sure they could, if they spend the time getting to understand it, but the vast majority doesn't). That leads to some really nice career prospects and salaries, its not like a regular programmer has to apply for foodstamps after all. So, my advice, don't think of it in a "I'll do it 2 years and be done with it" way, if you can handle stress (who in IT doesn't), and like it, database development and administration is a really nice career path. Do a few certificates (I can only speak of the official MS ones), put up a profile on Linkedin, and you would be amazed how many people will want you to consider a new job.
Thank you!! this was a big DUHHH moment for me:)
You are passing a varchar to CONVERT which throws that error: DECLARE @DATEVAR nvarchar SET @DATEVAR = '08/30/2014'; SELECT CONVERT(datetime,@DATEVAR,101) 
I believe you could just change @DATEVAR to be a datetime if it isn't. Otherwise you might have an implicit conversion to a varchar that is throwing the error and some of the function that work are just overloaded nicely. That is what it seems like from my perspective. 
if you're going to do it for just one salesrep, you can simplify it SELECT SUM(sales) [total_sales] FROM table WHERE salesrep = 'JDOE' doesn't give you a column with the salesrep, but you already know what it is !
It's because WITH is also used for query hints and will cause a syntax error if whatever came just before it isn't terminated properly. Also FYI, Microsoft has indicated that in future versions of SQL server, all queries will need to be terminated with semicolons. 
Correct. I use % in a few like statements when I'm trying to find a descriptive name of something. 
A CTE is just a named subquery that comes before your main query instead of after it. The unique thing you can do with a CTE would be a Recursive CTE - i.e. iterating through a parent/child relationship table with an unknown number of levels. You can look up specific examples of that, but they're not super common. It keeps things much easier to read than nested subqueries. Some others have already posted examples, but for the most part there's nothing you can do in a CTE that is different from a subquery after your select.
I've been doing database design and development as a significant portion of my workload for ~7 years now. I've used forward read-only cursors twice and pseudo-cursors with a while loop about the same number of times. I think I can live with that :)
If you're finding PIVOT is painful then you may want to look at letting the presentation layer handle that, it's often much simpler - a PIVOT table in Excel, or Tablix in SSRS, for instance. It's not always an option, but sometimes it can save you some grief. I've used PIVOT for some ugly queries - census data was the worst, over 1000 rows and columns!
First off, how about if we [read about SQL](https://en.wikipedia.org/wiki/SQL)? Depending on the [level of adherence to a standard, ](https://en.wikipedia.org/wiki/SQL#Standardization) SQL, you can learn how it works and any code you write will work with any product that implements that standard (MS-SQL, SQLLite, POSTGRES, Oracle, etc). The key important is to [understand](http://www.codeproject.com/Articles/34142/Understanding-Set-based-and-Procedural-approaches) how [set based logic works.](https://www.simple-talk.com/sql/database-administration/the-road-to-professional-database-development-set-based-thinking/) Once you understand the difference between set based and imperative programming, everything else is just syntax. 
I found it easier to write dynamic SQL that generates the pivot statement so I don't have to worry about the values in the result set. PIVOT is incredibly cumbersome.
In addition to recursiveness, CTE's can also be nice if you have a complex data set that you have to refer to multiple times, but with different parameters. Something like: with cte as ( select value, another_value, row_number() over (partition by value order by sort_value RN from table ) select * from cte c1 inner join cte c2 on c1.value=c2.value where c1.RN=1 and c2.RN&gt;2 CTE's are handy if the CTE logic can change, so you only need to change it in two places. If you wanted to do the above using subqueries, you would have to do it in two (or more) places. select * from (select value, another_value, row_number() over (partition by value order by sort_value RN from table) c1 inner join (select value, another_value, row_number() over (partition by value order by sort_value RN from table) c2 on c1.value=c2.value where c1.rn=1 and c2.rn&gt;2 Not as powerful as recursiveness, but is still handy.
Ironically that's the sole reason for every recursive cte I've ever used. Also why that was the only example I could think of. 
I do primitively understand SQL but when I've used it in the past it's been in limited application and something I created. Now I'm in an environment where there are multiple databases, lots of joins, and a ton of columns that don't necessarily have documentation or which are intuitively named.
I replied to someone one with this remark: &gt;I do primitively understand SQL but when I've used it in the past it's been in limited application and something I created. Now I'm in an environment where there are multiple databases, lots of joins, and a ton of columns that don't necessarily have documentation or which are intuitively named. Like because my experience formally goes back so long, and all my practical experience is limited... I feel almost like I need to completely relearn what the fuck I'm doing. Every day gets better and I'm able to write progressively larger queries but just learning what everything is and how things are built is daunting.
:)
I think I might be sending you more questions in the future. I've decided to switch from one side to the other to fill a gap before moving forward in my career. I pursued and took this job very specifically and did some related during the transition. In 2 years I may still be writing queries, but it isn't the path I've taken. Working in the capacity that I hope to be in then it will be a benefit to be able to do it myself and understand as much as I can but more in a business orientated sort of way?