A nice intro to SQL, many entry points.
SQL server products for performance monitoring, backup and recovery, compliance and security, tools for SQL server admins and developers. www.idera.com 
SQL defrag manager is the only SQL Server defragmentation solution in the industry that automates the time-consuming process of pinpointing fragmentation "hot spots" and takes action to defragment automatically, or on demand. www.idera.com
SQL compliance manager is a comprehensive auditing solution that tells you who did what, when and how on your SQL Servers. SQL compliance manager helps you ensure compliance with regulatory and data security requirements such as Sarbanes-Oxley, GLBA, HIPAA, PCI DSS, and Basel II by providing customizable, low-impact auditing, alerting and reporting on virtually all activity across multiple SQL servers. SQL compliance manager goes beyond traditional auditing approaches by providing real-time monitoring and auditing of all data access, updates, data structure modifications and changes to security permissions. SQL compliance manager also includes powerful self-auditing features to ensure that you are alerted to any changes to data collection settings or attempts to tamper with the audit data repository. www.idera.com
Idera SQL secure helps to identify holes in SQL Server security and ensure compliance with increasing audit requirements. SQLsecure collects and analyzes permissions data from SQL Server and Active Directory as well as the file system and registry to show who has access to what database objects and how that access is granted. SQL secure also monitors changes made to access rights so that unapproved changes can be easily identified and fixed. SQLsecure also collects and evaluates key security settings within SQL Server and provides proactive recommendations to improve server security. www.idera.com
From implementations with tens of SQL servers to enterprises with thousands of servers spread around the globe, SQL safe backup is the only SQL Server backup and recovery solution that scales to meet the challenge. www.idera.com
SQL diagnostic manager is a powerful performance monitoring and diagnostics solution that proactively alerts administrators to health, performance or availability problems within their SQL Server environment, all from a central console. SQL diagnostic manager minimizes costly server downtime by providing agent-less, real-time monitoring and customizable alerts for fast diagnosis and remediation of SQL Server performance and availability problems. SQL diagnostic manager also pinpoints common performance issues such as worst-performing code, and provides extensive historical metrics for in-depth trend analysis. www.idera.com
SQL mobile manager is a mobile management console for Microsoft SQL Server. This product enables you to securely manage your SQL Server environments from a remote location using a BlackBerry or Windows Mobile device. Easy to install and use, SQL mobile manager provides powerful diagnostics and remediation capabilities enabling you to assure the performance and availability of your SQL Server environment. www.idera.com
Idera SharePoint backup is a powerful backup and recovery product for Microsoft SharePoint sites and site collections. It enables administrators to ensure the safety and integrity of their SharePoint content by automating the backup and recovery process. Idera SharePoint backup provides automated scheduled backups, monitoring of backup and recovery operations from a central console and reduces administrator workload by enabling site administrators and users to recover their own documents. SharePoint backup also saves on storage costs by employing powerful compression technology to reduce backup sizes by up to 95%. www.idera.com
PowerShell Plus is the most advanced Interactive Development Environment for PowerShell available today. Designed to help administrators and developers quickly learn and master Windows PowerShell, it also dramatically increases the productivity of expert users. PowerShell Plus features a powerful interactive console, an advanced script editor and debugger and a comprehensive interactive learning center all integrated into a single product. Try PowerShell Plus today and see how much more productive you can be! www.idera.com
PowerShell Plus is the most advanced Interactive Development Environment for PowerShell available today. Designed to help administrators and developers quickly learn and master Windows PowerShell, it also dramatically increases the productivity of expert users. www.idera.com
That is **awesome**.
Can you give an example of what you would like to see?
I guess the OP is never coming back.
Hi, I wrote this command line tool which uses a mostly-SQL syntax and plays nice with UNIX pipes (actually it works on Windows too but I don't think many people bother with the command line on Windows). It also has some pretty-formatters and converters. I hope it's of use to some people...
Ever asked your self how things work inside the SQL Select statement? We won’t be talking about how to write SQL Select statement but rather about the algorithms and the methodology behind the Select statement and how SQL decides which algorithm it will use to filter out the results and return our expected results.
like this? http://www.dpriver.com/pp/sqlformat.htm
It's an easy front end to any SQL database, gotta give it that. Hell, it can even be an easy GUI to some old legacy UNIX system a company is stuck with. You'd rather do it every single time with a web page or .NET app? Really? &gt;"Real Access developers don't use macros … we write VBA code." &gt;I don’t even know where to begin with that one. Heh. Ok, that was good.
Trouble is you don't often find people using Access just as the GUI. I've lost count of the number of shitty, horrible Access databases that someone has slapped together which has then become, over time, a core business app. They're always horrible. Access must die. Even if I grant it can make a useful GUI, it definitely has no place in business as a database or application environment.
I've used it a lot as a front end to other databases. This was the only really solid reason I could think of why Access rocks, so I was interested in the original article just to see how they stretched it out to 10. I do think it's a great prototyping tool, but I can only give it 1/2 point for that, since we all know what happens with prototypes.
He might of even been bordering on trolling in some areas, but it is was fun to read. Good post! Very worthy of /r/SQL.
&gt; we all know what happens with prototypes There's nothing as permanent as a temporary solution...
Your best bet is to use the key index field from the SQL table. Add a new field to your import table with the same data type. Use what ever means you have (Item Name and Item Unit of Measure) to update your import table with that key. You may only get some matches that way. Use other queries to get all the matches. Use that key to update the prices from the import table to the master Table. It may take a little longer but that's better than updating the wrong item. edit: Spelling 
If you're looking for a good way to do this programmatically, I suggest exploring building a SSIS package with BIDS (Microsoft Business Intelligence Development Studio). If you have SSMS installed, you likely have BIDS as well.
an SSIS package would be the best tool to automate the process but you still have to figure out the process first.
Start by running a couple of queries: SELECT COUNT(DISTINCT(ItemName + CAST(ItemUnitOfMeasure AS VARCHAR))) FROM widgets SELECT COUNT(1) FROM widgets If these two queries give you the same result, then the ItemName and ItemUnitOfMeasure are unique enough to do the update in the manner you describe. If not, you're going to have to figure out something else - preferably, obtaining a spreadsheet that includes the ItemID in it. SSIS takes some overhead to learn. If you don't have time for that, my suggestion would be to take the following steps: * Ensure you have a viable backup of your current widgets table * Right-click your database; Tasks; Import Data * Use the wizard to get the data from your .xls into a new table, say, newWidgets Then, if and only if the two queries above gave the same count: UPDATE w SET w.ItemPrice = nw.ItemPrice FROM widgets w, newWidgets nw WHERE w.ItemName = nw.ItemName AND w.ItemUnitOfMeasure = nw.ItemUnitOfMeasure (This syntax is deprecated in 2008, but works in 2005 and is what I'm used to...)
Agreed. I figured you started on the, "how" so I would add to the "where". 
Nice detail. Do you write for a text book? You could.
Great- just by reading the update syntax, I can tell that's something to what I was looking for. I didn't realize I could set it like that. I'll play around, thank you!
Yeah, I've been thinking I ought to dive into it. This project has a close deadline, but I'll take your advice and start reading up. Thanks.
I only have 1 point to give. Fantastic reply.
Start by making a backup of your database. Then, run this query, substituting whatever table/column names apply: UPDATE foo SET threePercent = budget*0.03 Alternatively, you could change threePercent to be a derived column and have its value calculated automatically. The easiest way to do this would be to right-click the table in Server Management Studio and click Design, and choose the threePercent column in the list. Down in the properties pane, scroll to "Computed Column Specification" and expand that so you see the "Formula" input. Enter budget*0.03 and save the table.
You could use a derived column as shaunc described, or you could use a view, such as: CREATE VIEW budget\_percent AS SELECT budget * 0.03 as 3percent, [any other fields you need to identify which budget you're looking at here] FROM budgets\_table END Then you can simply query the budget_percent view the same as you would any other table. You can also easily add additional columns with other percentages. Why do you need this information stored in the database, though. Why not just calculate 3 percent when you need 3 percent?
hey, thanks for help. I believe i need 3% stored in database because our crm application can use DB columns to display data. It does not do any data manipulation (just display) e.g. wages budget is £1000. Materials budgets is always 3% of wages budgets. Currently, im doing it manually cause i know nothing about sql server. So i thought, if i can calculate 3% on SQL server then all i need to do in CRM application is to map calculated field to the correct column. I hope it makes sense. Thanks again!
thank you
I'd add that the *best practice* (at least the way I would do it) would be to add a Updateddate column to the table, and not do an update at all -- just load new prices as they come in with a date value, and write a view to show your most current dates. This will help you when inevitably someone on the business side wants a feature that needs to know what the price was for an item two years ago. Accountants don't use erasers. Neither should the database. 
Red Gate has a good formatter. 
In my 12 years of experience, I've found that experience is what speaks loudest. That said, I still wouldn't mind having a piece of paper or two just so I can feel good about myself, so I'm interested in the answer as well.
Do you have any experience? That goes at the top. If you only know 'the core language', did you learn it on MySQL, MSSQL, Oracle, etc? I work with MSSQL, and actually many of the employers/managers I've worked for have had bad experiences with those who have the MS cert - MCDBA - in SQL. If you have no other experience, I'd say highlight it, but personally I have a slight bias *against* candidates with a SQL cert. Really though, the point of the resume is to get you the interview, not the job. If you're going to get a cert I would see it as more of a way to refresh and expand your knowledge of the langugage itself, not so much as a way to bolster your resume. If you're after bolstering your resume, doing a volunteer job doing sql on some project would be time better spent than memorizing the study material to get a cert. 
Well, I may confuse the issue, but why not store the value you want to offset for the budget as a separate field, and do the calculation on the fly when needed. cause as soon as you save 3%, somone is going to ask for 4 or 5 or 2. so just save the 3 as a parameter and use that to do the calc. table contains wages float, budget float, etc) select wages * Budget as BudgetWages, from table.
I am actually studying to take a MySQL developers exam tomorrow here is a link to the FAQ about it. I dunno how much weight this will have in a few years with Oracle taking over Sun and Sun owning MySQL. It's sort of in Oracles best interests to let SQL die I think http://www.mysql.com/certification/certfaq.html
I have my complaints about Oracle... but I think I'd rather work with Oracle than with whoever wrote this.
I don't know oracle, but I googled *oracle current date* and found that SYSDATE is the function you want. GETDATE() is an SQL Server function. As far as I can tell, you're right about the -1 giving you 24 hours ago though, so I think this may work, assuming Timestamp is a date field on Table: SELECT * FROM Table WHERE Table.Timestamp &gt;= SYSDATE - 1
Depending upon your SQL variant, try SELECT foo FROM table WHERE MessageTime BETWEEN '2010-05-21 09:11:00' AND '2010-05-22 09:11:00'
In SQL Server (and probably other variants), you can operate on dates the same way as numbers, using &gt; &lt; &gt;= and &lt;= So this same query could be done as such: SELECT foo FROM table WHERE MessageTime &gt;= '2010-05-21 09:11:00' AND MessageTime &lt;= '2010-05-22 09:11:00' Or if you wanted to get all records where the time is before 2010, you could do SELECT foo FROM table WHERE MessageTime &lt; '1/1/2010' You'll note that SQL Server (and probably others) can interpret various different date format strings.
thank you, i will try on monday
MSSQL server has a 'soundex' function that tells you degrees from true. It looks like [mysql has the same function](http://dev.mysql.com/doc/refman/5.0/en/string-functions.html#function_soundex)
Agreed, Soundex is about the best thing out there for this kind of join.
I hate sites that make you register before reading them. Not a bad article though.
Agreed.... It's a great site, it just sucks that they make you register. 
FYI if you use the [User-Agent Switcher](https://addons.mozilla.org/en-US/firefox/addon/59/) Firefox extension, and set your User-Agent to Googlebot, you get right in.
Very nice, I just updated and am now using that.
I totally disagree. People that de-value certifications are the people without certifications. I think that there is a HUGE difference between MCDBA and MCITP: DBA, so I wouldn't bother listening to these peoples ;)
Which time stamps are you referring to, the ones on the actual .mdf files?
Correct, and also the .LDF files.
My .ldf files all share a timestamp equal to the last time the active cluster node was bounced; the .mdf files all reflect the most recent time they were backed up. I just did a DBCC CHECKDB (it came back clean), and sure enough, it set the mtime of that database's .mdf to the current time. Never noticed that before, but it sounds like your network admins are right. Though I'm curious why network admins are patching the SQL servers ;)
You really shouldn't be using phpmyadmin for big imports- it's really just not the most efficient way to do it. Instead use the MySQL client directly, which makes this task shockingly easy (and much faster)- mysql -h localhost -u USERNAME -p DATABASENAME &lt; /path/to/file.sql Change the username and database name to the appropriate names and you'll get prompted for your password. You can also change 'localhost' to a non-local machine (an ip address or hostname) and, assuming permissions are setup properly, you'll be able to transfer your dump over a network. This is useful if you're storing backups on a different machine. To your original question (how long should it take)- what's the size of the database, engine, hardware, etc etc? There needs to be a lot more information here to tackle that one.
Thanks for your reply, I just tried this but received the error message below; ERROR 1044 (42000) at line 21: Access denied for user 'db#####'@'%' to database 'database#####' I am sure this password is the correct one for my current server, does it need to be the password for the database I am importing? This is all new to me so I greatly appreciate any help!
Are you doing this from the same computer as the phpmyadmin installation or from a different one? If it's the same server then you should be using the same username/password combination. If it isn't the same server then you need to make sure the user has permission to access the server from that machine (mysql's authentication can be host based).
I'm doing it remotely via SSH using the username/password combination of the new mysql database that I created that I am trying to import it into. Should I be using the original databases password/username?
You should use the one you are using with PMA. If there's no login for PMA, then it's hardcoded in configuration and you should check out the config file.
Are you doing it over SSH or are you doing it remotely? SSH means you're doing it locally on the machine itself. Is this your server or is it a shared hosting account? You may be able to get their support guys of the hosting company to restore the backup for you.
"I'm doing it remotely via SSH". This statement holds true. "SSH means you're doing it locally on the machine itself". This statement doesn't quite hold true. The MySQL server is not local to the user, the computer running the ssh client is. Therefore, the user is inputting the commands locally, which are being executed remotely. 
Really? You waited 9 days to argue about semantics? The difference is where the connection to MySQL is established. If he's doing it remotely then MySQL will see an IP address, where as doing it on the machine itself means MySQL will see the connection as "localhost". Since OP was having issues with credentials this is very important- if his user is only setup on MySQL for localhost then he needs to establish the MySQL connection on it's machine or fix the user information.
I got some limited experience with Netezza in a previous job.
I didn't wait 9 days to argue semantics. I replied near immediately after when I read your comment. Your question was "are you doing it over ssh or are you doing it remotely?". You likely wouldn't SSH to localhost would you so in essence you are asking "are you doing it remotely or are you doing it remotely?". You then state that "SSH means you're doing it locally on the machine itself", again, isn't the purpose of SSH to cause execution on a remote server, thereby meaning you are doing something remotely. I'm sorry if I misunderstood you, but I still feel as though your statement didn't make sense. In addition, the statement "MySQL will see the connection as localhost" is also a little off-track. MySQL will see the connection as an IP address as localhost will be resolved to 127.0.0.1 before the connection is established. If you don't think that is right, then try editing the C:\Windows\system32\drivers\etc\hosts file on Windows or /etc/hosts file on Linux to remove the entry for localhost, restart networking services and then try and establish a connection to MySQL using the hostname of localhost. Edit: spelling
"Remote" is a relative term, and in this case I'm speaking relative to MySQL. If someone logs into MySQL from their computer, over any network, then it's a remote connection to MySQL. If someone logs into a server through SSH, and then established a connection to that machines MySQL daemon, it's a local connection to MySQL. Additionally, although this is slightly less relevant, you can connect to MySQL over sockets as well as tcp/ip. There is a difference, or rather than can be a difference, between '127.0.0.1' and 'localhost.
I don't know enough about MS SQL to answer this authoritatively, but you should try, in general, to think relationally and logically rather than procedurally. You know the inner query is going to look something like this: SELECT ParentTable.BUbbleCount, COUNT(BabyTable.ParentTableId) AS babyCount FROM ParentTable, BabyTable WHERE ParentTable.ParentTableID = BabyTable.ParentTableId GROUP BY ParentTableId HAVING babyCount &lt; ParentTable.BUbbleCount So now that you have the inner query, you just want to insert a part of that table into your BabyTable: INSERT INTO BabyTable (BUbbleCount) SELECT BUbbleCount FROM (&lt;insert select from above here&gt;) Does that help?
I saw this on my phone on my way home from work and just had to take a crack at it. If I understand you correctly, your table structure looks something like this create table ParentTable ( Id integer not null primary key, BubbleCount integer not null default(0) ) create table BabyTable ( Parent integer not null, Field1 varchar(255), Field2 integer ) and when you're done the number of rows in BabyTable with a given Parent id should equal BubbleCount for that Id in ParentTable. Here's what I came up with: declare @table as table ( N integer not null identity(0,1), Field1 varchar(10), Field2 integer ) declare @c as integer declare @stop as integer set @c = 0 select @stop = MAX(BubbleCount) from ParentTable while @c &lt; @stop begin insert into @table values('default', null) set @c = @c + 1 end insert into BabyTable select tmp.Id, tbl.Field1, tbl.Field2 from ( select pt.Id, pt.BubbleCount, COUNT(bt.Parent) Existing from ParentTable pt left outer join BabyTable bt on pt.Id = bt.Parent group by pt.Id, pt.BubbleCount having COUNT(bt.Parent) &lt; pt.BubbleCount ) tmp, @table as tbl where tbl.N &lt; (BubbleCount - Existing) We start by declaring a table variable with an auto incrementing identity column starting at 0 plus columns matching any columns you want to default when you insert rows into BabyTable. Then we determine the maximum possible number of rows we could be inserting into BabyTable for any Id by doing a MAX(BubbleCount) on ParentTable and use a while loop to populate our table variable with that number of rows. Now the magic happens! The inner select is basically FusionGyro's query MSSQL-ified, and we use it to get the parent Id, BubbleCount, and the number of records already in BabyTable with that parent Id. The outer select then cross joins this with our table variable, using the auto incrementing identity column to restrict the number of rows returned. You know you've done it right when this query returns no results (same as the inner query from above): select pt.Id, pt.BubbleCount from ParentTable pt left outer join BabyTable bt on pt.Id = bt.Parent group by pt.Id, pt.BubbleCount having COUNT(bt.Parent) &lt; pt.BubbleCount Apologies if my explanations are less than clear; it's a Friday night and I'm about a [3], but I promise you the code is good! 
have you tried using a ten character date? 01/03/2011?
Well, first off - you need some % signs around your like clauses - if you are trying to match those dates exactly, why not just use =? Also, what records are you trying to update? From what I can gather the purpose is to that if you have any prices whose dateto is unset (i.e. either null or an empty string), whose datefrom is not 27/7/2010 and which have the same EOF as another prices record which has an unset dateto and 27/7/2010 as datefrom you want to set their dateto to 1/3/2011? Explain what you are trying to accomplish and I should be able to help you out more - is EOF the primary key of the prices table? EDIT: Fun trick - if you're using SQL Server you can use the COALESCE() function to simplify your query. COALESCE takes a list of parameters and returns the first non-null value in the list. So instead of: dateto like '' OR dateto IS NULL You could do: COALESCE(dateto, '') = '' If dateto is null - Coalesce will return '' and the comparison to '' will be true. If dateto already equals '', coalesce will return that and the comparison will be true. If dateto has any other value, that will be returned and the comparison will be false. 
that's exactly what I'm trying to accomplish here. EOF is just a column, there can be many rows with the same EOF. The reason I cannot use '=' is that SQL rejects the query because the columns are of type text (another long story that involves excel, c# and many many hours, seeing as that I know almost nothing about c#), so the next available option is LIKE. If I put %LIKE% will it match the string exactly? Anyway, any ideas as to how I can pull this thing off are highly appreciated :) (I have managed to do the same thing with a while loop in c#, by looping through the results of the select argument, but with 50000 records inserted and 200000 to search and update, it kinda freezes the whole thing...)
Yes I think I solved this one with careful formatting. (hopefully)
I am a little confused by what you mean by "SQL will reject the query because hte columns are of type text"? Can you clarify that more? I've tried cleaning up your query (while still keeping the like clauses) - switching in the IN clause to an EXISTS and adding my coalesce suggestion - does this one work? UPDATE prices SET dateto = '01/03/2011' WHERE EXISTS (SELECT 1 FROM prices p1 WHERE p1.datefrom LIKE '27/07/2010' AND COALESCE(p1.dateto, '') = '' AND p1.eof = prices.eof) AND datefrom NOT LIKE '27/07/2010' AND COALESCE(dateto, '') = ''
&gt;I cannot use '=' is that SQL rejects the query because the columns are of type text I don't think that's the case... = works fine with text values, unless they don't match exactly. what hypo11 tried to tell you is is to put '%' not in the LIKE themselves but in the string they are matching [the like clause works like this](http://www.techonthenet.com/sql/like.php) another thing that I see is that you use a subquery with the same table as the update, you rename the table but don't use the alias in the query... i think you might get an error there 
I'll check that first thing tomorrow. I'm probably wrong, but what does "select 1" mean? Does it retrieve one record only? In any case, thanks a ton for the help, hopefully it will work :)
The SELECT 1 is just there so that a value is returned to compare with the WHERE EXISTS clause. 
As Elus mentioned, I changed your "in" clause to an exists clause. Now, rather than retrieving the whole list of Eofs that match the datefrom/Dateto criteria for every eof of the price records we are updating, we merely check that a single one exists. This is far more efficient and should run signifcantly faster on a large data set. Since it is now only checking that at least one record is returned by the nested select statement, we don't have to return any specific fields to match on. So we just have the nested query return a "1" if any records are found - for simplicity. 
yes, but I need this resultset to determine which rows I must update. I could use all the eof data from an array in c# (retrieved from excel file before inserting in the db), but I was told that a select inside sql would be quicker. In any case I have to check every single one of the last inserted eof codes for other rows with the same eof but with the date restrictions.
"The data types text and varchar are incompatible whit the equal to operator". I get this when I try to run this query :(
&gt; In any case I have to check every single one of the last inserted eof codes for other rows with the same eof but with the date restrictions. Why? Here is your original code (leaving off the last where clauses, they're irrelevent to this discussion), let's analyze it: UPDATE prices SET dateto = '1/3/2011' WHERE (eof IN (SELECT eof FROM prices AS prices_1 WHERE (datefrom LIKE '27/07/2010') AND (dateto LIKE '' OR dateto IS NULL))) So let's see what this will do: it's going to go through the entire prices table and for each record it's going to take it's EOF. Then it's going to retrieve the list of EOFs which are already in the prices table which match the date criteria check and see if the EOF of the record we're considering updating is in that list. Net Result: If there is even a single record already in the prices table with the same EOF as the one we're considering, but which matches the date criteria we've specified, we're going to update the one we're considering. Now consider my query (again, excluding the last where clauses for clarity): UPDATE prices SET dateto = '01/03/2011' WHERE EXISTS (SELECT 1 FROM prices p1 WHERE p1.datefrom LIKE '27/07/2010' AND COALESCE(p1.dateto, '') = '' AND p1.eof = prices.eof) Now what will mine do? It's going to go through the prices table and for each one it's going to look for any records already in the prices table whcih have the same EOF as the one we're considering ("AND p1.eof = prices.eof") but which also satisfy our date criteria. Once it finds a single record that works, it knows that we have to perform the update and we can move on to the next record in the prices table. Net Result: If there is even a single record already in the prices table with the same EOF as the one we're considering, but which matches the date criteria we've specified, we're going to update the one we're considering. Same net result - but I believe the EXISTS clause is going to be a lot more efficient. Or am I missing something about the problem that would require the IN clause? EDIT: An exists clause won't be universally faster than an in. If your nested query returns a very small set of records, the IN might be faster - but based on what you've been saying I'm thinking that's not the case.
It sounds like a trigger isn't really what you need. It sounds like you want is either a calculated field (a field defined on contact2 whose entire definition is to be the sum of the 10 other fields), which unfortunately won't let you join to contact1 to check for client type. Or you could use view which could return the 100 columns of contact2, plus a 101st column - "total budget" which is only populated when the type for that contact (as defined in contact1) is "client" - then the calculation of total budget would be performed upon selecting the row out of the view. 
Well, I have no knowledge whatsoever to dispute your hypothesis, so I'm just going to accept it, as you seem to know the subject at hand. I just got confused with the "select 1", and thought that it would only match one eof row, and I needed it to loop through all the codes. So I'll give that a try tomorrow (although I did run it and got an error (see my previous [comment](http://www.reddit.com/r/SQL/comments/cuaik/its_driving_me_nuts_how_the_hell_is_it_not_working/c0vdno9) ))
Googling for the error you listed "The data types text and varchar are incompatible whit the equal to operator" seems to indicate that you can't perform operations using the "=" sign on fields of type "text." I guess your database uses "text" for datefrom, dateto and/or EOF? If so, you may have some trouble using my code. Is the type of the columns something you are free to change in your example (in which case I'd recommend switching to varchar's, or even better, varchar for eof and datetime for datefrom and dateto)? If not, you may be stuck doing it your way or at the very least casting the data to type varchar in my code. EDIT: I modified my suggested code to work with the "text' type - I've never really dealt with that type before so I'm not positive this is a 100% correct or optimal way to do what you're asking with those data types but it should be a good start and may even work for you as is: UPDATE prices SET dateto = '01/03/2011' WHERE EXISTS (SELECT 1 FROM prices p1 WHERE p1.datefrom LIKE '27/07/2010' AND COALESCE(p1.dateto, '') LIKE '' AND CONVERT(VARCHAR, p1.eof) = CONVERT(VARCHAR, prices.eof)) AND datefrom NOT LIKE '27/07/2010' AND COALESCE(dateto, '') LIKE '' 
i'd go with the view if it's not a lot of rows
the dateto and datefrom columns are text, because c# refused to parse the excel data (with bulkcopy) if they were varchar (or anything else for that matter). Couldn't I use your query with EXISTS and all, but not use coalesce?
It's not just the COALESCE that it's choking on, it's the "p1.eof = prices.eof" in the EXISTS statement. You can the version I included with casting in my edit of my above comment, or this way which does it using a join: UPDATE p1 SET dateto = '01/03/2011' FROM prices p1 INNER JOIN prices p2 ON CONVERT(VARCHAR, p1.eof) = CONVERT(VARCHAR, p2.eof) WHERE p2.datefrom LIKE '27/07/2010' AND COALESCE(p2.dateto, '') LIKE '' AND p1.datefrom NOT LIKE '27/07/2010' AND COALESCE(p1.dateto, '') LIKE '' I assume you're running these on something non-critical so that if there's a mistake in one of these queries and it's updating the wrong rows that's not a big deal and can be easily undone?
for now I can experiment, in any way with the data, but when it goes live, I'm guessing there is no room for mistakes since, if it messes an update, we wouldn't know how to repair the table. I don't think though that p1.eof=p2.eof is a problem. I'm pretty sure it worked fine with the = operator as-is.
Yeah, that's what I meant. That you're not dealing with production data while testing these queries. In the tests I just ran creating columns using the "text" data type it was choking on the p1.eof = p2.eof, but if it's working for you without casting, then that's fine, you can scrap the COALESCEs and go back to what you had before. Those are just for simplifying the code to make it more readable and less dense.
I have about 12k rows and i cannot really use view because the CRM application can display table fields only. 2 questions. is it worth to enable trigger for 400 rows ( 11k rows are not clients and calculations are not required). shall i update table when required and disable trigger? Will trigger significantly slow down the server? If i populate "client type" field in contact 2 will i be able to filter then? many thanks
Well it worked with EXISTS but not with coalesce, so I just switched to LIKE / IS NULL, but kept the exists, and it is now working. I got 10 seconds for inserting 12000 rows in a db with 12000 rows already in. Then 11 secs for the second 12000 rows pack (same ones different date). Then 15 secs for a subset of the 12000 rows (about 2000). Then 26 secs for the big set again and finally 17 secs for the big set once more. So all in all, I'd say it's OK, performance-wise, although I got some "Not Responding" on my form near the end of the procedure, but I guess that's just crappy programming from my part :)
You could try the join method that I posted and your original method for performance comparisons and use the best one. Good luck. 
thanks again for your help! there was no way I could have done it without it :)
i think grandaddy did a song about this Sit on the toaster like a rock No need to worry about a shock All of the microwaves are dead Just like the salamander said The refrigerators house the frogs The conduit is the hollow log Data Warehouse appliance national forest Air conditioners in the woods Data Warehouse appliance national forest Mud and metal mixing good Meadows resemble showroom floors Owls fly out of oven doors Stream banks are lined with vacuum bags Flowers reside with filthy rags A family of deer were happy that The clearing looked like a laundry mat Data Warehouse appliance national forest Air conditioners in the woods Data Warehouse appliance national forest Mud and metal mixing good 
If you are able to put client type on contact2 then you should be able to make Total Budget a calculated field which only populates for records where type is client.
First, what db are you using? Second, first I think you should be looking at indexing strategy if it's doing a full table scan on each column for each value. Create a seperate index on each column. Best thing from a perf pov probably would be do do the searches on each column seperately. Third, it sounds like you're you're looking for multiple values on each pass. Best thing to do from a perf pov here is to insert those values in a scratch table and join those values to the table you're looking for: eg. Create Table scratch (Searchterm varchar(30)) Create Clustered index CISearchTerm on Scratch(Searchterm) ... Truncate table scratch Insert into scratch (Searchterm ) ... (your values) Select * from table t inner join scratch s on t.ColumnA = s.Searchterm To actually do the query you want all in one query is pretty easy, but it won't be any faster than doing the queries seperately -- because you'll still need to search through each index to find the right values: Select * from table t inner join scratch s on s.Searchterm = t.columnA or s.searchterm = t.columnB or s.searchterm = t.columnC
I would try using UNION or UNION ALL: SELECT [ColA] as [Match] FROM [db] WHERE ColA = 'whatever' UNION SELECT [ColB] as [Match] FROM [db] WHERE ColB = 'whatever' UNION SELECT [ColC] as [Match] FROM [db] WHERE ColC = 'whatever' ----- **edit**: actually it looks like you would rather have this instead: SELECT [db].* FROM [db] WHERE ColA = 'whatever' UNION SELECT [db].* FROM [db] WHERE ColB = 'whatever' UNION SELECT [db].* FROM [db] WHERE ColC = 'whatever'
Sorry guess I forgot the basic information that I'm using SQL 2005. Amaxen, you are correct, I am searching multiple values over multiple columns Right now I have something which resembles SELECT * FROM [master_DB] JOIN [columnA_criteria] [ca] ON [ca].[columnA] LIKE '%' + [master_DB] + '%' SELECT * FROM [master_DB] JOIN [columnB_criteria] [cB] ON [cb].[columnB] LIKE '%' + [master_DB].[columnC] + '%' SELECT * FROM [master_DB] JOIN [columnC_criteria] [cc] ON [cc].[columnC] LIKE '%' + [master_DB].[columnC] + '%'
That makes it easier then. bottom line is, use the query analyzer (Query | include actual execution plan), run the query. A tab will come up after you run the query with a lot of diagrams. Look for the ones at the base. Mouse hover over them. If they indicate they're 'table scans'|'PK scans'|'Index scans' then you're searching the entire table. What you want is for them to all say 'index seek'. If they don't, create an index on each column and/or change the sort order. You can try several variations like the join method, and etc, and run them all at the same time, the query analyzer will give you info on the relative resources consumed by each query. As I said, basically most of the time that is going to be taken is doing the searches -- if you index properly it doesn't really matter if you can make all three searches in the same query or not. One other thing: If your search term is '%whatever%', then it's always going to have to go to a table scan. If the search term is 'whatever%', (note the missing %) then it can do an index seek. The reason is that the second value is orderable -- the b-tree algorithm can figure out only a small range of values to look for. If you're searching through large blocks of text, generating full-text indexes may be your next option to take. Speaking of which, another method you might consider if you really want to do the search against all three queries at the same time would be to generate a calculated column that adds the three columns together, and search that, eg: Create table scratch ( ID int , ColA varchar(10), ColB varchar(10) , ColC varchar(10), ColAll AS ColA + ColB + ColC) Insert into scratch values (1, 'aaa', 'bbb', 'ccc') Select * from scratch ID ColA ColB ColC ColAll ----------- ---------- ---------- ---------- ------------------------------ 1 aaa bbb ccc aaabbbccc 
I have created caclulated field. I does calculated the values; however, i cannot see it in columns list?? i can select it but i do not i see it a a column? how can I see it and how can I drop it? if i try to drop i get * Msg 3728, Level 16, State 1, Line 1 'ucthours' is not a constraint. Msg 3727, Level 16, State 0, Line 1 Could not drop constraint. See previous errors. *
** Msg 3728, Level 16, State 1, Line 1 'ucthours' is not a constraint. Msg 3727, Level 16, State 0, Line 1 Could not drop constraint. See previous errors. **
Are you using SQL Server 2005 or better? If so, you probably want a [Common Table Expression](http://msdn.microsoft.com/en-us/library/ms190766.aspx) (CTE). For example, WITH temp (name VARCHAR(255)) AS (SELECT TOP 3 DISTINCT name FROM observation) SELECT name AS field1 FROM temp WHERE name='rock' SELECT name AS field2 FROM temp WHERE name='paper' SELECT name AS field3 FROM temp WHERE name='scissors' ..though if you already know in advance that the names are going to be rock, paper, and scissors, I guess I'm missing the point of the intermediate temp table. 
I don't think it requires one seeing your idea. I am now on 2005 although I was recently on 2000 so I didn't know about this one yet. Thank you very much for your assistance.
Just realized the title of the post sounded rather odd, sorry about that...
Use a temp column that holds the order field info. It's ok if you know those are the four IDs you want, and that is the specific order you want. SELECT *, CASE ID WHERE 6 THEN 1 WHERE 12 THEN 2 WHERE 8 THEN 3 WHERE 13 THEN 4 END ORD FROM MYTABLE WHERE ID IN (6,12,8,13) ORDER BY ORD The last field, "ORD" takes the value 1 or 2 or 3 or 4, depending on the value of ID in that record.
Yeah I found a similar approach right after I posted on reddit. select * from Webpages where ID in (6,12,8,13) order by case ID when 6 then 1 when 12 then 2 when 8 then 3 when 13 then 4 end; But thanks anyway :)
OK, I just noticed I wrote "where" instead of "when". 
Conventionally, I would write this query with implicit inner joins like so: SELECT tinvent.lId, tinvent.sPartCode, tinvent.sName, tinvbyln.dInStock, tinvprc.dPrice, tinvbyln.dLastCost FROM tinvent, tinvbyln, tinvprc WHERE tinvent.lId = tinvbyln.lInventId AND tinvent.lId = tinvprc.lInventId AND tinvprc.lPrcListId = 1; However, I don't think there's anything wrong with the way you've done it, it's just harder for me to read. I know other SQL users who prefer to be explicit about their inner joins though, and their code would look more like yours: SELECT tinvent.lId, tinvent.sPartCode, tinvent.sName, tinvbyln.dInStock, tinvprc.dPrice, tinvbyln.dLastCost FROM tinvent INNER JOIN tinvbyln ON tinvent.lId = tinvbyln.lInventId INNER JOIN tinvprc ON tinvent.lId = tinvprc.lInventId WHERE tinvprc.lPrcListId = 1; There's no difference between these ways of doing it. I can't guarantee that Access or MySQL are going to optimize these queries the same way, but they should, because they all mean the same thing. The only way to be sure would be to use EXPLAIN.
For the record I've spent years doing it both ways, and explicitly stating the inner joins is far better, especially as you start to build more tables on. It might not make much difference on a smallish query you write and throw away, but if you will need to maintain it later, it's far more readable, easier to add/remove joins, easier to debug. Also, when you don't explicitly write the joins, you limit what you can do with outer joins. It's been a few years since I did this but I believe you can only do a single outer join if you're not explicitly stating the joins. 
I used Access' query view to generate the code then edited it. It worked until i added an ORDER BY at the end. Now I can't get it to work again. I think your way makes more sense in my limited experience. Both are definitely more readable. Thanks.
Your second one generates a syntax error on the inner join statements, but that might be an access thing. But the first one works.
First, I would say get rid of those parentheses. There's nothing wrong with them but they can be confusing when reading queries if they're not specifically useful for anything. The lPrcLstId=1 will produce identical results whether you include it as part of the inner join or as a where clause. In this case it's more of a conceptual thing. If you put it as part of the join, you're telling the query to only join records where that value is 1. If you put it as a where clause, you're telling the query to do a more complete join and then limit the final results. The reason it's important to understand this is that even though it results in the same thing in this case, it will matter if you do an outer join. (I apologize if this is over your head but it's a lesson I didn't explicitly learn for years so it can help in the future.) When you do a left outer join, you're saying that you want all results from the first named table, and then join up a second table anywhere you can, but not to eliminate records from the first table if there is no corresponding record in the second (as you would with an inner join). This is where it can matter if you do the limiting clause on the join or in a where clause. If you were to include the limiting clause on the join, it says that you only want to see values from the second table where that particular value is what you ask for, and the rest return nulls where they didn't have that value. If you include the limiting clause in the where, you're automatically going to exclude all rows where that value is null, which is counterproductive to the idea of the left outer join. You'll end up with fewer rows in the total output, all of which will have the value you asked for. When doing outer joins, you will consequently use the where clause for these limiting statements most of the time. Let me know if that makes any sense at all. It would be easier to show with example tables. =)
I had just read up on outer joins about an hour earlier. heh Wouldn't including the lPrcLstId=1 as part of the inner join be faster? 
You know, I briefly wondered the same thing. I won't say definitively but the database optimizers are wonderful things and you rarely have to worry about writing your query in such a way as to make it faster. This is usually done by good table design and indexes, rather than with the query itself. I'd say writing it either way would come back in the same amount of time because the optimizer would interpret it in the best possible way and run it like that.
For the size of database I'm working with, the time difference is negligible. There's only about 800 records in the first two tables, 3x that in the third. Guess I'm harking back to my dbase days in the 90's when hardware speed was a factor. 
&gt; It might not make much difference on a smallish query you write and throw away, but if you will need to maintain it later, it's far more readable, easier to add/remove joins, easier to debug. Like I said, I know people on both sides of the issue, but I haven't personally found it to make a big difference either way. &gt; Also, when you don't explicitly write the joins, you limit what you can do with outer joins. It's been a few years since I did this but I believe you can only do a single outer join if you're not explicitly stating the joins. That must depend on the database. I mostly use PostgreSQL and haven't run into that limitation.
Yeah, but what I'm saying is that regardless of the conceptual way you write the query, a good db will figure out which way is the fastest way to give you the results you asked for and use that method.
Cool, thanks.
are the columns that you're using in the where clause indexed?
Some are but most are not and, unfortunately, there isn't anything I can do about that. I can change the structure of the query generated by the application--which is external and written in VB.NET (don't laugh)--but I am not allowed to change anything within the database itself. *EDIT: Just to add: the query runs very quickly if I remove the main where clause, like so: SELECT [fields] FROM qryMailMerge WHERE [criteria] ORDER BY [field] But then I end up with many duplicate customers as I get a record for every booking rather than just the latest.
Use a Common Table Expression where the CTE encompasses what is currently the subquery. Also, do you have an index on the booking ref column?
Unless I'm mistaken, I don't think CTE is supported in MS Access. No, the booking_ref column isn't indexed unfortunately--in fact only ID and date columns are indexed. But I'm not so sure that is the problem here, because I get near instantaneous results in other queries referencing that column. 
IN statements are often not performant. I think sql2008's query optimizer sometimes gets around this, but try this alternate version, where you you create and use a virtual table: SELECT [fields] FROM qryMailMerge q Inner join ( SELECT MAX(Booking_Ref) FROM qryMailMerge WHERE [criteria] GROUP BY ID ) as vrt On vrt.Booking_ref = q.Booking_ref Order by [field] The reason why INs and especially NOT IN are not performant is that optimizers frequently don't see INs as set-based, and tend to be forced to try every individual record one at a time to do the match. Edit: formatting.
Subqueries are something to avoid if you're concerned with performance. 
That looks promising but Jet is throwing a "Too few parameters." error. That usually happens when there's a reference it can't find--such as when a table or column name is misspelled. EDIT: To make the query as simple as possible, I've run it selecting all the fields and with no criteria or order by clause: SELECT * FROM qryMailMerge q Inner join (SELECT MAX(Booking_Ref) FROM qryMailMerge GROUP BY ID ) as vrt On vrt.Booking_Ref = q.Booking_Ref But it still throws the parameters error.
Yay I've got it working and it *is* much quicker--thank you so much! I just had to give the aggregate MAX column an alias and use that in the join reference: SELECT [fields] FROM qryMailMerge q INNER JOIN ( SELECT MAX(Booking_Ref) AS Ref FROM qryMailMerge WHERE [criteria] GROUP BY ID ) AS vrt ON vrt.Ref = q.Booking_Ref ORDER BY [field] 
Right. Forgot about how aggregations need an alias. tl;dr : avoid using in and not in if possible. 
i dont get it. im running informix, so what?
There's some sort of delicious irony in the fact that I'm seeing mysql errors all over that site...
What database server? I would do some googling and make sure its time datatype can support a 12 hour format. If not, or if you can't figure out how to do it, perhaps you could create a view or a calculated field which returns the time as a varchar and you can put some parsing logic in there to subtract 12 from the hours if it is over 12. Though really that logic should be up to whatever is querying the data to put it through some sort of "DateFormat" routine. 
In oracle, from a DATE datatype you can use to_char(timestamp, 'HH24:MI') for 24 hour time, or to_char(timestamp, 'HH:MI') for 12 hour time.
Assuming it's a DATETIME field in your database, there is no format per se. It's simply a value of the time stored as a numerical value (such as the number of days since Jan 1, 1900) and shown to you as a date. You can't change the format inside the database itself. If you want to see it in 12 hour format, you do that by formatting the value in your query. Of course, the exact syntax to do so depends on which database server you're using. I googled "sql server format date" and got good results back on the first try. Do a similar google search and you should have your answer. PS. If your time values are actually stored in a text/varchar field rather than a datetime field, then your database doesn't know they're time, it just sees them like any other string value. What you would need to do in this case is convert/parse the time field into a datetime value, then convert it back to a string in 12 hour format. Though personally, I would recommend NOT storing the 12 hour time like this (you can see what a pain it is), but rather just converting your time into a real datetime value and storing it that way. If you don't care about the date portion you can just make them all 1/1/1900 or some other default value. Of course, the function/syntax for converting a string to a datetime will also vary depending on your database server.
There is probably a better way to structure your data, but I'll give you some suggestions on the assumption that you cannot do so. First let me make sure I understand you correctly, 2 pieces of data indicate that an "items" record is a rebate - the "category" field and the itemno field - a rebate will be in the "rebate" category and its itemno field will be the item which it is a rebate for, plus the word "rebate"? And you want to build a list of all of the distinct items with their costs being represented by their actual cost minus a rebate - if any? To do this, you could LEFT join the items table to itself - the left instance would be restricted to rows which represent actual items, the right instance would be restricted to rows which represent rebates, so for example: SELECT actual_items.itemno, actual_items.cost - COALESCE(item_rebates.cost, 0) as cost_after_rebate, FROM items actual_items LEFT JOIN items item_rebates ON actual_items.itemno = LEFT(item_rebates.itemno, LEN(actual_items.itemno)) AND item_rebates.category = 'rebates' WHERE actual_items.category &lt;&gt; 'rebates' Does this make sense to you? Am I understanding the problem correctly? If you have the ability to restructure how the data is stored, there are a lot of ways to make this more straightforward, so let me know.
Yes, you seem to have been able to comprehend my post! I don't understand everything and I've never even seen COALESCE before, but I'll plug in my data and see what happens. I really just needed somewhere to start and I think you have provided me that. Unfortunately my database is controlled by our enterprise software so I don't have any control of structure. Thank you for the quick response, I'll orange you again if I need any further explanation!
I wouldn't expect my query to work right off the bat, of course but it should be a good starting point for you. COALESCE() is very similar to ISNULL() - it takes 2 (or more parameters) and returns the first non-null value. So in this case if item\_rebates.cost is null (which it will be if there is no rebate for the itemno we are looking at in actual_items), then it will return 0 - so that no rebate is taken off of the cost of that item. If you are having troubles getting altering the query to work with your data or need me to explain in any more detail how my query accomplishes what you're trying to do, write back and I'll do my best to help you out.
It may have to do with the cross join. Cross joins are normally a bad thing. They give you all possible combinations of the 2 tables which can often lead to a massive result set. In my opinion you should try changing it to an outer join. I'm not sure what datatype @ServiceDuration is, but if the system has to convert it or calculate if for every single result in your cross join, your performance will be awful. That could explain why explicitly defining it as an INT would increase performance. Alternatively you could try telling it how to convert it by changing @ServiceDuration to CONVERT(INT, @ServiceDuration). Either way, I'd recommend reevaluating that cross join.
One thing you could try is to sanitize your parameters. Sometimes there's an issue with parameter-sniffing by the query analyzer: Declare @Serviceduration2 int Set @Serviceduration2 = @Serviceduration SELECT a.CalendarDate StartDate FROM @AvailableTime a CROSS JOIN @AvailableTime b WHERE a.CalendarDate = DATEADD(Minute, -( @ServiceDuration2 ), b.CalendarDate) 
It's not a parameter... yet... 
@ServiceDuration has already been declared as an INT. I tried the outer join, but it didn't have an effect on the performance. Again, once I change @ServiceDuration in the DateAdd to a hard-coded 60 (or any other int), it speeds up dramatically. I'm at a complete loss here.
You are a P.I.M.P.! SELECT actual_items.itemno, actual_items.descript, actual_items.cost AS 'Item Cost', item_rebates.itemno AS 'Rebate Item Number', ((actual_items.cost * .01) + actual_items.cost) - COALESCE(item_rebates.cost, 0) as cost_after_rebate, item_rebates.cost AS 'Rebate Cost' FROM items actual_items LEFT JOIN items item_rebates ON actual_items.itemno = LEFT(item_rebates.itemno, LEN(actual_items.itemno)) AND item_rebates.category = '121100-SMB' WHERE actual_items.category &lt;&gt; '121100-SMB' AND (actual_items.category IN (SELECT category.category FROM xxx.dbo.category WHERE category.parentcat = '121000')) OR actual_items.category = '121799' One quick question though. How can I round my cost_after_rebate to 2 decimal places? I've used ROUND before, but I'm not real sure where to put it. I'll trial and error it till I figure it out anyway, but I figured I'd go ahead and ask too. Thanks again, I wish I could upvote you more than once!!!!
Hmm... maybe try replacing DATEADD with DATEDIFF? I'm not sure what range you'd need to restrict to... but something like: WHERE DATEDIFF(Minute, a.CalendarDate, b.CalendarDate) = @ServiceDuration
Happy that I could help! Out of curiosity, why do you add 1% on to the actual items cost before subtracting the rebate to calculate cost\_after\_rebate? Also, I thought of one potential flaw in my logic: If there is an item whose itemno is 12345 and another item whose itemno is 123456, you could wind up with 2 records in your resultset for itemno 12345 - one having removed the rebate cost for 12345rebate and one which removed the rebate cost for 123456rebate. If this is a realistic scenario, you'll need to rework the join condition - I might recommend: FROM items actual_items LEFT JOIN items item_rebates ON actual_items.itemno + 'rebate' = item_rebates.itemno
I'd try it, just to be sure. The the other thing I immediately thought of was that your where clause isn't SARGable, but looking closer it looks like it is. 
Tried it already. ;) Same performance issue. 
That's our 1% margin. So its actually Cost * 1% = Price - Refund. So we buy a product, sell it for cheaper than we buy it, then file a back-end rebate to actually make money. Its tough to compete in the hardware field. I'll make your change to the join condition. I did research joins a bit more after going through your solution, and I think your solution is actually going to help me with some other reports I've been asked to run. So thanks yet again!
To answer your rounding question, you would just wrap the entire statement that defines cost\_after\_rebate in ROUND, such as: ROUND(((actual_items.cost * .01) + actual_items.cost) - COALESCE(item_rebates.cost, 0), 2) as cost_after_rebate
&gt; I did research joins a bit more after going through your solution, and I think your solution is actually going to help me with some other reports I've been asked to run. Good to hear. It sounds like you're being thrown into SQL when maybe you don't have a lot of background in it, so I definitely recommend researching different topics to help improve your skills - joins are a great place to start, especially the difference between an INNER JOIN, OUTER JOIN (aka LEFT or RIGHT join) and a CROSS JOIN. I'm happy to try to help answer your questions or point you in the right direction whenever I can. Good luck!
Perfection! The join change worked flawlessly, as did the rounding.
I'm thinking the problem might be with calculating @ServiceDuration. Is this static across the result set or will it change for each row?
Static across the result set.
Tried it with no success. Thanks, though.
Is this any better: SELECT StartDate FROM (SELECT a.CalendarDate StartDate, DATEADD(Minute, -(@ServiceDuration ), b.CalendarDate) Result FROM @AvailableTime a CROSS JOIN @AvailableTime b) SQ WHERE StartDate = Result
No difference.
I really don't like cross joins... how about this: SELECT a.CalendarDate StartDate FROM @AvailableTime a LEFT JOIN @AvailableTime b ON CONVERT(varchar,a.CalendarDate, 1)=CONVERT(varchar,b.CalendarDate,1) WHERE a.CalendarDate = DATEADD(Minute, -@ServiceDuration, b.CalendarDate) **EDIT: I got so hung up on the performance aspect that I didn't realize you were cross joining a table to itself. I can't think of any reason to do that. Why are you trying to link a table to itself?**
&gt;I can't think of any reason to do that What would you do instead? I'm doing other comparisons in the WHERE clause. Here is what it looks like. SELECT a.CalendarDate StartDate, b.CalendarDate EndDate FROM @AvailableTime a CROSS JOIN @AvailableTime b WHERE a.CalendarDate = DATEADD(Minute, -( @ServiceDuration ), b.CalendarDate) AND ( b.RowNumber - ( a.RowNumber + @ServiceDuration ) ) = 0 AND ( DATEPART(Minute, a.CalendarDate) = 0 OR DATEPART(Minute, a.CalendarDate) = 30 ) ORDER BY StartDate ASC Honestly, I've been working on this procedure for such a long time, that I think I may have been taking the wrong approach in the first place. I'm trying to figure out open time slots (by the minute!). Yeah.. I'm going insane. **EDIT: YES, the @AvailableTime table contains *every minute* that is available**
I'm assuming that @AvailableTime has lots of rows then. Is that right? If so you definitely want to avoid using a cross join. If it has 10,000 rows a cross join will produce a result set of 100 million rows. If we use a left join it drops it down to 10,000 rows which is ten thousand times smaller and easier to work with! Here's the code I would use: SELECT a.CalendarDate StartDate, b.CalendarDate EndDate FROM @AvailableTime a LEFT JOIN @AvailableTime b ON CONVERT(VARCHAR,a.CalendarDate, 1)=CONVERT(VARCHAR,b.CalendarDate, 1) AND DATEDIFF(Minute, a.CalendarDate, b.CalendarDate) &gt;= @ServiceDuration WHERE b.RowNumber -(a.RowNumber + @ServiceDuration ) = 0 AND DATEPART(Minute, a.CalendarDate) IN (0, 30) ORDER BY StartDate
Thanks, that definitely sped up the result a bit. **EDIT: It didn't speed things up at all. The cross join with the hard-coded service duration is still faster. **
I hope it helps. Feel free to send me a PM if you need anything
another redditor (Verifex) gave me a hand fixing the user table by starting the server with --skip-grant-tables and resetting the password... that allowed me to run mysql_upgrade to fix the issue...
http://bugs.mysql.com/bug.php?id=48862 the thread with the most information I found
They're fake errors. Did you read any of them?
http://developers.facebook.com/opensource/ may be they use scribe for that? May be even Cassandra
To answer your first question: yes, these databases are *huge*. I would guess that the upvote data is kept in two places: 1) A table called 'user\_likes\_comments' with just a user id and a comment id. 2) In the 'comments' table, an integer count called 'user\_likes' that is updated every-so-often from the first table. This is what's displayed to users.
I guess this is how I assumed it worked... but I guess I just figured it would be so.. *big*. Like, just storying all the comments of facebook is obviously huge, but then devoting like 100x as much to people liking every little thing... wow.
*Big, like you think of gods as huge..*
Turtles all the way down.
As the PHP of databases, MySQL comes with a slew of technical debt thanks to its non-acid legacy, sloppy standard support, largely ignorant community and history of buggy releases. It's also GPL'ed, if distribution is or should ever be a factor. Postgres is solid and I'd rank it similar to SQL Server, even if cost and control over my infrastructure didn't matter. Just don't presume that retraining (possibly hostile) users and dba staff is free. The [documentation](http://www.postgresql.org/docs/current/static/index.html) is excellent, its community is knowledgeable and helpful, and there are plenty of [options for paid support](http://www.postgresql.org/support/professional_support). IMO pg runs best on *nix, which is another factor for your cost/retraining considerations. 
Who cares about the cost, the performance differences alone will save you money in terms of needed hardware. Sorry that I can't provide you cold hard statistics of how much you'd save, but I'd imagine the only real cost would be...well however much your servers costs? I own a few online businesses and I don't remember having to pay any other additional fee except renting the hardware (servers) and domain name (I don't use any crap like cPanel etc). tl;dr I don't really think pgsql (mysql *could* after all they are owned by oracle now) charges per server at all..I could be wrong though
Outside of retraining, if you don't buy a license you may actually save money. I've been able to run MySQL and PostgreSQL (not at the same time) on a laptop with reasonable performance without sucking up a ton of memory or CPU time. Granted, I wasn't doing a lot, but at least Oracle was a bloated mess. As for which to choose, that depends on what you're doing. MySQL is much better supported in terms of what languages have drivers, but PostgreSQL has very feature-rich.
Um, guys, not to be ungrateful here, but I'm actually just asking about the *cost* of licenses. I can't get a straight answer from the MySQL guys or the Postgresql guys, so I'm turning to Reddit for help. I'm beginning to suspect that only argumentative university students use these technologies. I hate Oracle, btw. I'm a SQL Server guy. I'd rather use MySQL than Oracle, despite the obvious technical flaws of MySQL. 
The PostgreSQL license covers all you need to know about licensing costs, terms of use and so on. You can find it here: http://www.postgresql.org/about/licence Don't worry, it's just a few paragraphs long. 
ok, thanks, but I don't see a dollar sign anywhere in there. Can you give me an idea what the actual cost is? Or rather, the only dollar signs I see are to denote variables -- not the cost of the license. Any idea what the actual cost is? 
&gt; *Permission to use, copy, modify, and distribute this software and its &gt; documentation for any purpose, **without fee**, and without a &gt; written agreement is hereby granted*, provided that the above &gt; copyright notice and this paragraph and the following two &gt; paragraphs appear in all copies. I quoted part of the license for you, and added some formatting. "Without fee" means just that: gratis, free of charge, at zero cost, for free. 
I agree that I prefer Postgres as far as I've been able to test it. And I know that retraining cost is going to be huge, Other major headaches include the *ix factor you mention, plus, cross-compatibility issues will also likely be very costly. I feel like I have a handle on these issues and roughly what they'd cost. But I don't know anything at all on how the licensing process works for a profit-making outfit like ours. We're not just a shop running a website with a couple of databases to support it -- we have over 100 dedicated database servers, and I'm sure Postgres will be after us to pay up if we use their tech. I just have no idea how the process works or what ultimately we'd need to be paying them. On Edit: Really, I'm biased towards continuing as we have been, and not switching to a system. But even so, to do an honest analysis, I need to know the costs associated with going to Postgress. I can't quantify what the actual cost of the licenses are now or what they would be in the future once we get 'locked in' to Postgres as we are 'locked in' to MSSQL now. I was hoping that someone who actually deals with Postgress in the real world would be able to fill me in on this aspect. 
That isn't what I hear from other guys who run MySQL in their shops, though. 
Also, I need to ask -- how much experience do you have in a true for-profit, production enviornment? 
try datediff or [google](http://www.google.com/search?sourceid=chrome&amp;ie=UTF-8&amp;q=microsoft+access+date+difference)
That was exactly what I was looking for, thank you.
Postgres is free open source software, that's it. As for performance in a real for profit company, we have had great success with it. In general, and this goes for any database, the more memory you are will to allocate to it, the better. I notice down below that you mention *nix being an issue, it runs on windows if you want, I don't know if there is a major performance difference there. Then again a simple ubuntu server is pretty easy to get running with postgres.
I work in an environment at a Fortune 500 company that runs postgres and SQL Server Environments. Here's the ups and downs. SQL Server: Comes with all the features by default. (SSIS, SSRS, SSAS) Supports Cluster / Replication / Log shipping Knowledge base is extremely good. Finding employee's to support the system is extremely easy (Certifications / Common Technology) General Ease of Use (SSMS, Automatic Query Profiling, Etc) Postgres: Free, I.e. license doesn't cost you anything. Works on most operating systems (I've never had a problem with the instances hosted on a windows environment, and the performance differences are negligable at best) Great Knowledge base. Really, if I were to make a decision today in a production environment I would go with SQL Server for a couple of reasons. #1. Easy to find employee's, standard system (No entrenching of knowledge) #2. SQL Server Express on &lt; 10GB databases #3. Simple, and integration with other microsoft products is very easy. (.NET C#, CLR, etc.) Really, with Postgres, you don't pay any more with the employee's, it's just that you've got less choice when it comes to who you hire. That being said, Postgres is an amazing product from a technical perspective. It supports almost any datatypes you can find (Pick values in our case) and it really is imho the best open source system out there. Compared to SQL Server and Oracle, postgres is lacking very few features. 
I suck at formatting :( Edit: Fixed
Well I'm going to start with the obvious question (since I can't see the data), try excluding the "having duration &gt; 5" part of the query and see if it shows up with a duration &lt;=- 5.
Thank-you for replying, GunnerMcGrath :) I tried your advice, but that did not turn up my missing person. I dumped the table data [here](http://www.hhminecraft.com/mc_visitor.txt) if that helps at all. I appreciate any advice you can spare!
Would it be possible to provide the data as a SQL dump with INSERT statements please?
[Yep!](http://www.hhminecraft.com/mc_visitor.sql)
Well, it works in sql server, (87 rows returned, for 87 distinct users in the data set) though there were a few syntactic differences. The one I'm curious about is whether MySQL allows you to reference a calculated column alias in the HAVING clause. It's not valid in some engines, and MySQL has a nasty habit of silently accepting incorrect syntax and returning random results where aggregate functions are concerned. I'd suggest rebuilding that query one column at a time until you see which function call begins preventing records from returning, then work from there.
Okay fair enough, I was wondering maybe if it was a MySQL issue. I'll keep plugging away at it. In the meantime, if anyone has any other theories, I'd love to hear them :)
I tried running the full SQL query through phpmyadmin and it returns 87 users as well. I'm beginning to think it's my php output. Egads. I really don't know what's happening here.
Okay, this is weird. I looked at the tables and it seems that the sum of duration for the missing user "limitforce" was the greatest of all users. So, I tried making another user called "null" with a greater amount of duration and "Limitforce" appears in the table. It seems the output skips the user with the most sum of duration. I guess it's fixed now? Thanks to everyone who heeded my call.
Plenty! http://www.sql-tutorial.net/ http://www.sqlcourse.com/index.html http://www.functionx.com/sql/Lesson01.htm http://www.sqlservervideos.com/ https://www.microsoftelearning.com/ http://www.intelligentedu.com/newly_researched_free_training/Database.html --&gt; http://ocw.mit.edu/courses/ 
Thanks for the link dump. I looked through a few of these before and they weren't really complex enough. However, you added a few new links I had not explored so I will do such. Thanks again.
[Transact SQL Cookbook](http://oreilly.com/catalog/9781565927568) goes through many interesting examples on how to query data. Once you get past that then you can go to a tech specific book like Inside SQL Server 2005: T-SQL Querying by Itzik Ben-Gan or move ahead to something really fun like Data Warehousing using the Ralph Kimball methodology. Good hunting!
Play my game? http://www.schemaverse.com It's a game played entirely in SQL...Might be a little too confusing though for a beginner :/ Edit: I am working to fix the learning curve issue. There is a lot of new documentation that will be added to the games wiki. Until then, this guide is pretty up to date: http://schemaverse.com/Abstrct-TheSchemaverseGuide.pdf New Edit: I have a new interactive tutorial up to help teach basic sql concepts and the game itself: http://schemaverse.com/tutorial/tutorial.php Any feedback regarding the new tutorial would be great :)
Here are a couple general goto lists for freely available technical material on the web. http://stackoverflow.com/questions/194812/list-of-freely-available-programming-books http://www.reddit.com/r/csbooks/ That said, I've never found a *great* comprehensive introduction to SQL online, though section two of the [PostgreSQL manual](http://www.postgresql.org/docs/9.0/static/sql.html) is an adequate overview. When you really need to ramp up quickly, it's a better use of time to find a good book. If you need to do this on the cheap and live in or near a major metro, don't overlook the public library for reference material. The SQL language has been stable for a long time, and you don't need an absolutely current book to pick up the basics. [Safari Bookshelf](http://www.safaribooksonline.com/) is another fantastic option. This gives you access to nearly everything from O'Reilly. Microsoft Press, Apress, and other major tech publishers. There's a ten day free trial, and it might be worth subscribing for a month or two if ten days isn't sufficient. So, to the recommendations. As you're probably aware, trial versions of SQL Server are available freely from Microsoft. If you're already on Windows, this is probably the most straightforward way to begin with minimum installation hassles. Most tutorials covering SQL Server reference either the old Northwind (no longer included with SQL Server 2005 and up, but available [here](http://www.microsoft.com/downloads/en/details.aspx?FamilyID=06616212-0356-46a0-8da2-eebc53a68034&amp;displaylang=en)) or the newer [AdventureWorks](http://blogs.msdn.com/b/bonniefe/archive/2008/02/28/the-adventureworks-2008-family-of-sample-databases.aspx) sample databases. If she's fairly technical, she might jump into the deep end with something like [ T-SQL Fundamentals](http://www.microsoft.com/learning/en/us/book.aspx?ID=12806&amp;locale=en-us) (see library, safari books, or read it at a bookstore for the cost of a cup of coffee) For a slower start, look into an introductory book on Microsoft Reporting Services. (MS's answer to Crystal Reports, included with SQL Server) Avoid theMS Press Step by Step series, as it is almost universally crap. Larson's books are a better starting point. See [Microsoft SQL Server 2008 Reporting Services](http://www.amazon.com/Microsoft-Server-2008-Reporting-Services/dp/0071548084/) or similar. These will cover SQL in the context of generating reports with graphical tools, which may be a more palatable start for the non-programmer.
Thank you!
Very impressive, now I'm going to have to play. 
Thanks, I will take a look!
Anything by [Ken Henderson](http://www.amazon.com/Ken-Henderson/e/B001IOFJL2). Don't be put off by the "Guru's Guide" title (I always found it cheesy), Ken was a master at SQL Server and his books are some of the best I've ever read on SQL.
Thanks for the recommendation. I'll take a look to see if any of his stuff is useful for us.
My god, I 100% agree. Ken is amazing. For basic ANSI sql I've always liked My favorite book is Introduction to SQL: Mastering the Relational Database Language by Rick F. van der Lans. 
*Nothing* is new in SQL Server 2005. It's all at least five years old. It also might help to know what sort of interaction they will be having with SQL server, i.e. will they be writing queries, managing the server/instance, etc. (of course, I say that, and I don't have any books to recommend, but if I did have one to recommend, I would need to know what sort of interaction was desired before pointing at a particular book).
My boss bought me the Microsoft Press MCITP SQL server 2005.... it was not to my liking one bit. So I really dont have an answer but I can tell you what not to get.
He probably means new relative to SQL Server 2000. The jump from 2000 to 2005 was very significant especially with the Business Intelligence suite.
Is that book designed to act as a guide for studying for the certification exams? Those are typically written so that people can pass those exams. I don't find them to be actually useful for referencing things on the job.
I should have inserted a smiley in there or something, as I was trying to be a bit of a smart aleck. That being said, I'm not sure there's much reason to go to SQL 2005 right now instead of 2008 R2. Not that it's much better than 2005 in practical terms (it might be, I just haven't encountered a great need for the newer features quite yet - then again I'm a bit of a novice ), but it will have a longer shelf life.
Trust me, if it were up to me I wouldn't be upgrading to 5 year old software... hell, I wouldn't be running 10 year old software in the first place but my place of work is a like that and I'm afraid there isn't anything I can do about it really. I'll make an edit to the main question including more information. Thanks.
That is exactly what they are written for because they serve me no real purpose when administering my SQL db's at all. They are basically VERY generalized guides to learning what you truly need to know.
Microsoft will usually find SQL Server MVP's to write books like Inside SQL Server: T-SQL Querying in order to address specific portions of admin/development with SQL Server. These are the books you want to buy. 
Dammit....I wish I would have ran into you 3 months ago instead of wasting my time with these MS Press Books. Thanks for the heads up elus, I am going to check out a selection of these books now on Amazon to see what I can get my hands on. 
If you don't mind cheap, I found that O'Reilly's SQL Pocket reference to be quite good. It was less than $10 when I bought it. Not really a tutorial, but it does show you just about everything you can do in standard SQL plus some of the variations you come across in different databases.
[For IA-64 the answer is no](http://support.microsoft.com/kb/312090/) , Otherwise the answer is [maybe.](http://support.microsoft.com/kb/875423/) It looks like SP1 for Windows Server 2003 may resolve the problem your technician was referring to: *Windows Server 2003 with SP1 and x64-based versions of Windows Server 2003 include changes that let 32-bit programs run on a 64-bit server cluster. When a 32-bit program runs, the 32-bit resource monitor runs in parallel with the 64-bit resource monitor.*
I really appreciate the comment, thanks! I'm still looking for something definitive while I build my test cluster.
Yes, what is the server type? Depending on the server you will need to use a different function and syntax. 
SELECT (DateDiff ("d", date1, date2) * costfield) as total FROM stayhistory WHERE stayid = X
Thanks! Most would say eccentric rather than impressive but I will take the compliment :P
Are the books still fully applicable to SQL Server 2008 r2? Or, if I pick up one or more of his books, should I also try to find another that's been updated to 2008?
Depends on what you are looking for. His book on architecture and internals is an incredibly in depth look at what happens under the hood. IMO it's a must read for SQL Server production DBAs regardless of version. Most of his books were written for 2000 and 2005, so they may be a bit dated if you're looking for a reference on 2008 or 2008-R2 you may want to look elsewhere. 
 select somefieldfromA, somefieldfromRIn, somefieldfromROut from activity A left join referral RIn on A.referral_id_in = RIn.ID left join referral ROut on A.referral_id_out = ROut.ID
God bless Aliases.
Remember to tell us which DB you're working with, that will help us give the right answer. Oracle DECODE(item1rebate.cost,(item1.cost - item1rebate.cost), item1.cost) as total_cost Or MySql IF(item1rebate.cost,(item1.cost - item1rebate.cost), item1.cost) as total_cost Or countless others depending on the db type.
My favorite is using an alias to join a table to itself. I'm sure it's not recommended, but it's still cool.
It's perfectly fine when called for, it's called a self-join. 
What are $year_select and $region_select? I assume they are variables that are coming from your php page? Is Year_select selecting a year? What might this query look like after the variables are plugged in? Something like: SELECT SUM(2008) as totals from agglomeration_growth_rate WHERE Country = 'USA'? What does the agglomeration_growth_rate table look like? What happens when you run the query with the same parameters without the SUM clause?
SELECT SUM(1950-1955) AS Totals FROM agglomeration_growth_rate WHERE Country = USA 1950-1955 is the name of a column in the agglomeration_growth_rate table in the DB, btw. edit- fixed a mess up
Your query is attempting to return the result of 1950 MINUS 1955 (hence -5). Wrap $year_select in double quotes: SELECT SUM("$year_select") from....
1950-1955 = -5. Isn't that a coincidence?
Well I had to use single quotes (which I would assume should yield the same result?) because I tend to use double quotes over single in my PHP. Using the same example of 1950-1955 and USA, it's outputting just 1950. 
Already figured that out thanks to hypo11. Led to another issue, unfortunately.
Single quotes wouldn't work because single quotes are used as text string identifiers in SQL. Double quotes are used to identify column names. At least in SQL Server. What SHOULD the sum of the 1950-1955 coumn where Country is USA be? EDIT: PS - if you have any control over your database change those column names now - your'e asking for nothing but trouble with a column name like 1950-1955.
okay I switched it to double quotes and now it outputs 0. I'd have to go through the database and manually add up all the values, but it should be around 200 or so including a decimal.
I'm not in a convenient spot to look it up but one of my favorites is "SQL Server Recipes" or something similar. Lots of useful scenarios with queries to get you unstuck. The only command-line utility I'm aware of is sqlcmd.exe ;)
If you have access to the database run the query with double quotes directly on the server, instead of through PHP. 
MySQL uses backticks (\`) for quoting identifiers. so SELECT SUM(\`$year_select\`) might work better if you are indeed using MySQL. Just be sure that $year_select is not coming from any user input if you are adding it to the query like this. I believe double quotes in this case makes the SUM function try to sum a bunch of text strings, which results in a zero. More info here: http://dev.mysql.com/doc/refman/5.0/en/identifiers.html
Check out Common Table Expressions and FILESTREAM. Both are really powerful and useful. 
Okay I've been playing around with working directly from phpmyadmin before I drop it into PHP. Here's my SQL- SELECT SUM(1950) AS total_growth FROM agglomeration_growth_rate WHERE Country = 'Canada' as it stands that is outputting a huge number (11,700) when it should be a much smaller number (20.8). My assumption is that it's just adding up the totals in all of the entries in the column "1950", completely disregarding my WHERE statement. 
No, you're still misunderstanding. SELECT SUM(1950) AS total_growth FROM agglomeration_growth_rate WHERE Country = 'Canada' Will take every record in agglomoration_growth_rate where country is canada and add 1,950 to a total for it. Based on your value of 11,700 I am guessing there are exactly 6 records where Country = 'Canada' in that table (1950 * 6 = 11,700). To identify a column that has a numerical name you need to wrap it in backticks (EDITED thanks to uto). Change that query to: SELECT (`1950`) AS total_growth FROM agglomeration_growth_rate WHERE Country = 'Canada' And you should get the right result. EDIT: uto is more familiar with MySQL than I am, it seems, and he recommends using backticks for quoting identifiers. So try: SELECT SUM(`1950`) AS total_growth FROM agglomeration_growth_rate WHERE Country = 'Canada' instead of the double quotes. EDIT2: I reiterate, if you have any control over this database, changing the column name from 1950 to something like year1950 will greatly alleviate all of your problems of this nature.
Thank you, I was not aware of that. I thought that double quotes were ANSI SQL. I have updated my most recent response to refer to backticks in lieu of double quotes.
There we go, that did it. I'm new to all this, so apologies for misunderstanding. Unfortunately I don't really have enough control over the db to change the names like that, or I would. Thank you so much. You've decreased my stress level twenty-fold. edit- and thanks to uto
While you can use SQLCMD to manage a SQL server from the command line, I don't think that would really give much advantage over using SQL Server Management Studio. Management Studio would also give people new to SQL a graphical clue to what they're looking at/for. 
Where is the date supposed to come from?
The answer column where question = date. edit: And based on HeaderID. All rows with the same HeaderID should end up with the same date.
You might want to rethink your table design. Maybe add a date column to tblAnswer. Or do this: SELECT h.ID, h.Username, q.QuestionText, a.Answer, foo.MyDate FROM tblHeader h INNER JOIN tblAnswer a on h.ID = a.HeaderID INNER JOIN tblQuestion q on q.ID = a.QuestionID JOIN ( SELECT HeaderID, QuestionID, Answer as MyDate FROM tblAnswer WHERE QuestionID=1 ) foo ON foo.HeaderID=h.ID Note the hardcoded QuestionID.
Unfortunately it's an inherited project. The answer table does in fact have a date column, but it's generated when the user submits their answer. Their answer of a date can be any date they want. Ideally I'd put this into a different column, AnswerDate, but that would take a fair bit of rewriting the front end in order to not break old stuff. Ultimately I will be doing a major overhaul of the app (comprised of a mobile device and web page) but that's a long way off. Anyway, that's an interesting looking join, never seen one like it. Will play around with it now and see what happens. Thanks! edit: That worked a treat. Thank you very much. Where do I send the cheque? I can work around the hardcoded QuestionID by doing something like WHERE QuestionID = (SELECT ID FROM tblQuestion WHERE QuestionText = 'date?') for readability.
http://www.juice.to/
brb, getting credit card.
Can you give us the parameters you're using? I've run a few different scenarios and the worst I got was a NULL. Bottom line when you're getting divide by zero errors though is, stop dividing by zero. That's advice I've always been given. One way to work around divide by zeros is something like this NULLIF(ColumnName, 0). That'll return NULL if you divide by a 0 rather than crapping out. That doesn't fix anything though
Aww no comments still. Somebody at least has to have something mean to offer...
this is pretty interesting. I'm going to give it a look over the next few days. 
I don't know much about any of this except trying to google sql 2005 on server 2008 info is really not fun at all. I voted against the combo at work simply on ability to google the combination. 
Ya, I will admit it is a lot to take in which makes for a bit of a learning curve. The wiki has some (what I hope are) good starting points so hopefully that will help. Thanks for taking a look though! I look forward to hearing your thoughts. 
http://www.reddit.com/r/programming/comments/ea5u0/database_fundamentals_free_ebook/ You got beat by 3 hours.
But it wasn't posted to SQL.
Ooh, good catch. Yeah, my bad.
Just out of curiosity, have you tried using a java stored procedure? That may make things easier. This page may also help: [http://psoug.org/reference/regexp.html](http://psoug.org/reference/regexp.html)
Thanks sybrandy, Unfortunately the permissions around stored procedures are locked up pretty tight. A java stored procedure would be an elegant solution though. I work at a large private university with a lot of schools and things are locked down tighter than Fort Knox. I may wind up coding something in python or java to get the job done to sift through the information anyways. However we try to stick to only PL/SQL and Hyperion for extracting information so that we can share reports and all speak the same language. I'm going to present an alternative solution to the problem in a day or two, and if that isn't a workable solution I'll need to convince the boss and boss^2 to let me develop something in java or python. 
Only problem is that the full outer join looks like a Cartesian join. I've seen this page help out many a people on stack overflow though.
I'll get something rigged to load submissions automatically to the twitter feed. Until then, each tweet will be lovingly hand crafted, like fine furniture. We'll web 2.0 the shit out this bitch!
 Ick. Guy with 10+ years experience here. 10 table queries are going to be ugly and hard to understand (especially with subqueries!) , period. So, first thing: do you know already what the query is *supposed* to do, or is that what you're trying to figure out? One approach I use is to start off with a simple 'Select *', and then I highlight the first table, e.g. Select * from TABLE1 Then, Select * from Table1 inner join Table2 on... Then Select * from table1 inner join table 2 on.. inner join table3 on.... If they are all inner joins, basically you can think of each one as just a set of additional filters. (and the where clause filters the filters). Typically most bugs in an already established query revolve around duplicates in a table where the programmer didn't expect one. Feel free to post the query here -- can't guarantee that I'll be able to solve your problem without looking at the data but it couldn't hurt. 
&gt; Does anyone have any advice on how to picture/comprehend the result-set (tablespace) for a larger query? How do people who write queries with ~10 joins and large numbers of aliases comprehend what they're doing? We don't write it all on one line AND /\* */ comments are usually OK (standard?)
Ah, SQL. Where the left joins are right and right joins are there only to mess with you. ;) Draw the joins. Table1 - Table2 - Table3 \ Table4 - Table5 is much easier to follow than from Table1 inner join Table2 on Table1.FK = Table2.PK inner join Table3 on Table2.FK = Table3.PK inner join Table4 on Table1.FK2 = Table4.PK inner join Table5 on Table4.FK = Table5.PK Or even better, let the tools draw them for you. Use a graphical query editor to visualize the data flow if one is available for your database. Aliases are there to help you: Orders - Customers as Buyer \ OrderItem - Products - Customers as Manufacturer not to confound you: Orders - Customers \ OrderItem - Products - Customers as Customers1 
I agree with the comments, for me the most important thing is layout, I can manage stupid aliases if the layout is decent. If you can, have every thing from one table on one line of the select, makes it easier to tell where it came from quickly. Select Cu.FirstName, Cu.LastName, Ad.Address1, Ad.Address2 From Customer CU inner join Address Ad on Cu.iCustomer=Ad.iCustomer ---We might get dupes here due to Home / Business where FirstName='John' and LastName='Smith' Order By LastName 
As has been mentioned, proper use of whitespace is essential for complex queries. There are a number of SQL formatting tools available. Give this a try. http://www.dpriver.com/pp/sqlformat.htm Alternatively, there's a decent graphical query designer available in SQL Server Management Studio. Highlight your query, then select Design Query in Editor from the Query menu. As to getting your feet wet with SQL, go through the back posts in this forum. "Where do I start" topics come up now and again. [Here's one.](http://www.reddit.com/r/SQL/comments/e08wt/ma_lost_her_job_as_an_as400_and_other_mainframe/)
I was recently given a similar task. I was given 14 tables that all related to a single incident. I needed to join them, but I didn't know how the joins were supposed to be made. I created a single incident in a test DB and pasted all the rows and columns from each table that had to do with the incident. I then put them in a spreadsheet. I color coded the cells by relationships and made a [map.](http://imgur.com/CNUM0.png) I had to *see* the relationships to write the joins. I would suggest you do the same. This will really help you get to know the DB you are working with.
If you know what the query is supposed to do, you can work backward from that concept -- i.e. start by writing the query as you would do it from scratch, and then having that done, go and look at the ugly query to see how they did it. The exercise will at least get you into the mindset of the developer who wrote the original query. 
The SQL Formater is awesome! I was trying to look up best practices for formatting SQL queries just yesterday. I had this really long query and I wanted to send it to my boss cleanly formatted so he might be able to understand what was going on. This was perfect!
Something about seeing somebody else post about your own project is pretty neat :D Thanks breetai If anybody has any questions feel free to ask! The best place to go to learn about how to play is: https://github.com/Abstrct/Schemaverse/wiki/How-to-play Also, if you don't have your own postgres client I do have pgmyadmin installed and available at http://www.schemaverse.com/pg/ The game may seem odd but it gets kinda neat once you learn the schema and start to write scripts (pl/pgsql) for your ships so that they play on your behalf. 
add echo "Connected to MySQL&lt;br /&gt;"; after: or die('fail'); If you see that, you are connected and you just need to run a query.
Looks like the version of PHP that yum downloads doesn't come with mysql support by default 
Your php.ini probably has display_errors off, which is a good thing in production. If you are developing on this machine though, turn it on. You can also check this setting using phpinfo();
Thank-you for the responses guys, they've helped a lot, both on an emotional level, and technical. Last Thursday my boss and me went through the query I was struggling with, and explained that it's actually the output of both legacy code, and partially re-written by the SQL Reporting Services query "optimizer", making the joins much harder to understand than they needed to be. grudolf, I've used your join simplification method two or three times now, it breaks things down well. Thanks for helping me feel validated amaxen, it was good to hear that it's expected that one have trouble comprehending a 10 table query. Also, writing my own copy of code I don't understand is something I've done once or twice in my career, but hadn't thought of applying it to this field, will try this. giveitawaynow, It's good to hear it's still normal comment code in SQL (C#/Delphi programmer prev.), I've added some to a couple of the more complex queries, although SQL Reporting Services appears to be stripping them &lt;_&lt; CrossWired: Thank-you for the advice on formatting, I hadn't thought of keeping fields from each table on a separate row when dealing with masses of fields, I'll try doing this - it could help reduce the time it takes to understand what a query is doing when revisiting it thethax: The visual editor is neat, nice to see its used in the industry, bit annoying that it leaves subqueries as text, but it's great for quickly selecting fields in a table, and getting an overview of table contents IMetBubbRubb: Your spreadsheet/map looks like exactly what I'm looking for, in terms of understanding how components in this db relate, I'll give it a go after work (I don't know if my manager would appreciate me doing this within work time, and it could be a fun side project :) 
select year(date) as Year, Month(date) as month, Count(*) as ct, sum(Dollars) as totalDollars From table group by year(date), Month(date)
Testing that out right now! Thanks for the feedback already :)
Thanks for the assistance, but that isn't quite what I've been tasked to accomplish. I need to write the query so that it checks a record and if any other records are with in 30days of that one, it doesn't show them. It then goes to the next record past the 30day mark and does the check again, showing only the records that aren't within 30days of it, so on and so forth. Thanks again though!
you should just have a date lookup table. trying to do stuff like that in a calculation would drive me crazy. 
&gt; I'm sure Postgres will be after us to pay up if we use their tech Postgres is free. Read the BSD license they ship under. You can take their source code, change all references of "PostgreSQL" to "Monkey Poop DB" and sell it for a billion dollars if you want to. They aren't like Oracle...they won't/can't hunt you down and charge you. &gt;get 'locked in' to Postgres as we are 'locked in' to MSSQL Of the databases I've administered, (Oracle, MySQL &amp; PostgreSQL), Postgres is the one database that is easiest to move away from. Their export utility (pgdump) has the option to export your entire setup to ANSI SQL so you can easily import (with minimal editing) to another database flavor.
If you choose to get support, that'll cost you some money. To download and install the database and use in a production environment is completely free.
explain the "30 day intervals" better, please -- 30 days based on what? Your example says 12/10 and 12/26 would both be returned...how do those two dates end up in separate 30-day intervals?
12/10- would be under the first date, 11/20. So the first date outside of the initial 30 day window would be 12/26
You'll have to use a correlated NOT EXISTS subquery on the where clause. In the subquery, you'll have to check out for that patient if there are any visits in the preceding 30 days
I think it's something like this SELECT T1.FIRST_NAME , T1.LAST_NAME , T1.PATIENT_TXT , T1.DATE_OF_SERVICE FROM #MOLE_SURG T1 WHERE NOT EXISTS ( -- NO SERVICE IN THE PRECEDING 30 DAYS SELECT NULL FROM #MOLE_SURG T2 WHERE T2.DATE_OF_SERVICE BETWEEN (T1.DATE_OF_SERVICE - 30 DAYS) AND T1.DATE_OF_SERVICE AND T2.PATIENT_TXT = T1.PATIENT_TXT ) AND T1.PATIENT_TXT = 'jhfggyhitui' 
I imagine it like this. For every table that'll use, I'll imagine I'm writing a FOR statement in a programming language. This FOR statement will be running through every record of the table. Every time I join with another table, I'll be placing another FOR inside the previous FOR and there will be an IF statement with the JOIN criteria. You only keep going with the joins if the IF's all checkout - if they don't, the closest FOR so far will move on to the next record (only talking about INNER JOINs here). The other thing you have to know is what is the purpose of the join. You'll be either getting more information for the current record or you'll be joining to get child records. You ALWAYS have to know if you're going from FK to PK or PK to FK(If you see a FK joining with an FK, watch out for possible problems). Many-to-Many associations screw that up a little bit, so it's good to pay attention about them. Also, always mentally separate JOIN criterias from filtering expressions. Finally, draw the join tree. One node for each table. Draw an arrow pointing from the FK to the PK(or vice versa if you prefer, just keep consistent). I try to keep this "join tree" very neat, because it is my mental model of how the query suppose to work.
Hmm, maybe you set up these domain accounts to have access to each SQL database at time of install. But this is not my expertise, yet. 
The user report databases get created on the fly, and sometimes get refreshed. I really need to be able to add these rights as soon as they're created, attached, or restored. 
It sounds like you're referring to the AD groups created upon install (such as SQLServerMSSQLUser$ComputerName$InstanceName), used to run SQL Server, and not designed to administer SQL Server. I haven't heard of these not being created on a domain controller, but it would make sense. (SQL Server displays a warning about installing onto Domain Controllers for good reasons, especially if SQL Server is running under a privileged account. I won't preach about this security point here though.) When installing 2008, you get to choose which AD group should be added to the Sysadmin server role, and these people will have full control over the server. After installation, you can create a login for the AD group that your desired admins will be part of, and then grant that login the sysadmin role. This will then give them full control over any database, and over the entire instance. They will be able to read all data, and drop databases, so it's a lot of trust to give. Alternatively, create a login for each individual account, and grant them sysadmin rights in SQL Server, but an AD group is a lot cleaner and easier to manage. If this is too much power (but it sounds like you're used to this from previous versions), you should consider manually having to grant rights on each database after attaching. Consider it a standard post-restore database step that you do. Another option is to create a SQL Server Agent job that runs every few minutes, which simply applies db_owner rights to a set of users for a specific set of databases. This will limit the amount of power you are granting.
As a test, I added one of the domain users as a login and granted them sysadmin rights. No go so far. Part of the problem (and I probably should have stated this right off) is that the application requires the user to have vision to the SQL database (I'm a tech, not the programmer). If I look at the SQL server, I can see the .mdf and .ldf for the attached DB. From the client's workstation, I can't see the file. If I modify rights on the DB files directly, the user has vision to them via the mapped drive. What I'm looking for is for that to happen automatically upon the creation of the files. Still testing/troubleshooting... 
Ah, is this SQL Server Express? The only reason your users would need file system access to the mdf and ldf files is if they are running SQL Server on their local machines and attaching the database files locally. If this were the case, you wouldn't need SQL Server installed on the DC. Anyway, you should be able to grant your users full control of the directory where the files are located, and those permissions should propagate to all files in the directory. It still sounds like a very odd architecture though. 
The prgoram I'm working with requires the users to have file system access to the mdf and ldf. I'd agree that the program is unique in that way and for fear of physical harm from our programming department, I'll leave it at that. I'm going to set this scenario up virtually and work it out here. My customer has a viable workaround for now (adding permissions by hand). I've also found some inconsistancies on their end with different domain users.... I'm going to hold off for the weekend and will report back later. Thanks for all the comments, much appreciated!
My colleague used brain dumps like this one from scribd: http://www.scribd.com/doc/45849681/70-432 He passed easily and even said that many of the questions where the EXACT same. He might have been lucky, but I would definitely check that out. If you search for others on scribd then be sure to exclude the ones containing lss than 10 pages, they are just put up by someone to try to sell the rest (which seems like scams too).
I had to do something similar, but I wanted to warn the user instead of fixing their string. I used a pl/sql function that: took the string as input for each character: got its ascii value if outside of the ascii range: get the 10 characters before and after the bad character, save as error message raise exception with error message It's definitely doable in PL/SQL. edit: I was doing it on a varchar2, doing it on a CLOB might be trickier. Link to my pl/sql function: http://pastebin.com/pmAxnWuA
You can create a subreport where one of its parameters is the person_id field
I think you are very close. Try changing your group by clause to: GROUP BY anumber, IP Also, you don't want the distinct if you are using a group by.
This is a big help. I think if I combine this with a subquery, it should do it - although I've never done subqueries before, so I'mm going to learn that now. thanks! 
In MS SQL Server, I would write that query like this. SELECT COUNT(DISTINCT IP) AS Count, anumber FROM articles GROUP BY anumber ORDER BY Count DESC 
Two books that you may read at night while working with databases in the day: http://www.fehily.com/books/SQL-Visual-QuickStart-Guide-3rd.html http://pragprog.com/titles/bksqla/sql-antipatterns The combination of reading and hands-on is good. And there is an SQL-relatede channel on IRC (Freenode): #sql which you will probably find useful.
SELECT * FROM Google.com WHERE query = 'SQL Tutorials for Beginners';
W3 Schools is the primer our company uses: http://www.w3schools.com/sql/default.asp You can actually practice what they teach in the browser. 
I gave one of my guys this book for Christmas since the focus of his job will be converting from FoxPro over to SQL towards the end of this month, highly recommended.
Sequel, don't know why.
didnt ask why, almost did but thanks for thinking about it.
My dad used to say MySequel and it pissed me off so much...
http://en.wikipedia.org/wiki/SQL "SQL (**officially pronounced** /ˌɛskjuːˈɛl/ like "S-Q-L"...." Wow, i never knew. I'll stop pronouncing it as sequel.
In my circles, sequel commonly refers to MSSQL. And SQL refers to syntax.
S-Q-L
S-Q-L While we're on the subject, I also say (although not so much anymore) S-C-S-I instead of Skuzzy.
Yep, same here. As I mostly talk about SQL Server, I'll say Sequel Server, often shortened to just Sequel. I call the language T-Sequel or Transact-Sequel, but say that you write S-Q-L queries, and try to be compliant with the S-Q-L standard. My-S-Q-L. P-L-S-Q-L. I'm happy for other people to use either pronunciation - I can still understand them. As I say "SQL Server" so often, that extra syllable would quickly add up over the course of a few hours. I'm sure if I was talking about Oracle more often, that would be shortened to "Or-cle".
This is exactly how I pronounce it too, Sequel for MS stuff and saying things like "ANSI Sequel" or "Transact Sequel," butt say "S-Q-L query" or "Write some S-Q-L slave!"
sequel. It's tiresome to say 'S-Q-L Server'
The only problem there is that if you say "ANSI Sequel", Joe Celko will eat your children, as the ANSI standard is the one that declares it to be specifically pronounced S-Q-L.
its only one more syllable. 
i've always said em-ef-em instead of modified frequency modulation. 
im giggling about calling oracle orcle, and i think i will from now on.
what if i dont have kids? i had no idea ANSI had a standard for it.
NoSequel.
Thanks for the answer, it worked well. I had tried copying the stored procedure to a query to make this work but this required both the stored procedure and query to be mantained, using a subreport kept the code that requires maintenance small.
 I can't help with your question, but would recommend heading over to [/r/database](/r/Database/). It tends to be a bit more active than this sub.
I use [Ignite](http://www.confio.com/) with MS SQL Server on a daily basis. I'm not crazy about the fact that it creates itself a SQL login with the sysadmin server role, but overall I'm impressed with the product and it's usefulness. They've got a free trial and it looks like it works with Oracle too.
&gt; I want to be well prepared and know my stuff not just what is contained in the exam. Then don't use the braindumps. You are much better off using the training materials that you mentioned. They will more than prepare you to pass the exam. However, another important part of preparation is to use/play with the product, especially the parts you are not as familiar with, as much as possible. -- MCITP Database Administration and Development
Recommendation: if you try any of Idera's tools, give them a completely bogus company name. They've tried to call me at work (to "see how I liked the product" aka try to sell me a license that my company isn't going to pay for) repeatedly.
Same in MySql.
I'll take a look, thanks. Actually, I'm ok with them calling me. If the product works, I want to buy it! I am providing a recommendation for a company that might end up buying it.
Use group by and aggregate functions if the notes are truly unique to each alarm. select agentGuid, note, min(eventDateTime) as StartTime, max(eventDateTime) as EndTime, count(1) as NumEvents from ksubscribers.dbo.monitorAlarm where alertEmail &lt;&gt; 'this@email.com' and eventDateTime between '2011-01-01' and '2011-01-12' group by agentGuid, note order by StartTime desc
It sounds like you understand how to write the query if you could make DISTINCT work on the note column. If the timestamps in the note column are always formatted as in your sample, then you can remove them with either of the following column definitions: right(note,len(note)-patindex('%-[0-9][0-9]%',note)-3) AS note or substring(note,patindex('%-[0-9][0-9]%',note)+4,8000) AS note You should be able to use DISTINCT on either of these columns to get the results you desire. 
I think you missed the "GROUP BY note" in your example query.
Oh, the irony! Thank you :)
You are right to do this in SQL and not PHP.. SQL is made for this! Always do as little data manipulation as possible in your PHP and Javascript. Try this: UPDATE F SET Component_ID = C.ID FROM Furniture_Sets F INNER JOIN Components C ON C.SKU_Name = F.SKU_Name AND C.Component_Name = F.Component_Name
Thanks! I actually ended up doing it in php to get things going. I didn't realize you could use AND in a Join etc, will certainly save this code for another rainy day! Thanks heaps!
You certainly can use AND in a join. Keep that in mind because there are a ton of cool problems you can solve with JOIN logic. They are every bit as flexible as the "WHERE" clause and can solve many problems where you might be tempted to use two or more queries.
If I'm struggling with a query, I'll write down the names of each table and column name I see near JOIN. Once I connect those dots to see how the data is related, the other details (SELECT, WHERE, etc) just seem to click.
SQL Management Studio is a very awesome tool if you have Windows to run it. For \*nix, the "freetds" library is vital. It includes ODBC drivers and a few native CLI apps as well. PHP "mssql\_\*" functions work with FreeTDS. For a decent GUI tool, check out the Java-based SQuirreL. It's a "generic" query tool that connects to any JDBC drivers you have for your servers. That means SQL Server, MySQL, AS/400 DB2, or almost anything else is supported by one tool, and it runs almost anywhere as well.
clean, fast, old school = sqlcmd MSSQL-Management Studio is the one and the beast one for SQL Svr.. R2 is pain in the neck if you are having multiple server which you need to migrate R2 database to normal 2008 server.
Use the foreign keys to navigate through the tables. Start from the outside, using the tables that don´t have any foreign keys. Start building a graph using the foreign keys as edges. Also, take a look at your view definitions to see important tables and common filtering techniques used in your schema.
If you have Visio you could use the database reverse engineer feature and end up with a somewhat usable diagram. 
The first and most important step (for your career in general) is to appreciate that this type of situation is going to be far more common than inheriting a database that was well designed, had consistent naming conventions, and had documentation of any sort. Generally the only time you will get this kind of database is if you create it yourself. So as frustrating as this can be, look at it as your chance to develop some very valuable skills that you will use for the rest of your working life. I agree that if you have foreign keys this will be a lot easier, but I'm going to assume that there are no foreign keys, or at least not enough. For me, the best way to learn a database schema is to look at the front-end code and the queries that are already written against the database. Go to a commonly used part of the system and look at all the queries and start diagraming relationships between tables. Also remember that if the database design is bad, the previous developers may have had as much trouble with it as you, and so you must take everything with a grain of salt and realize that if you find contradictions, it could be that something in the system is actually wrong. Finally, RESIST the urge to rewrite anything (other than the work you're explicitly given) for at least 6 months. Even if you think you have things worked out, there are probably tables, relationships, or procedures that you know nothing about and rewriting something prematurely is a great way to introduce unintended consequences elsewhere in the code that may not show up for a while. Good luck! It's never an easy task to take over a crappy database but it's a valuable skill to master.
WTH?? I didn't know you could COUNT(DISTINCT ...) That's nice. Thank you.
just be glad you don't have to deal with the monstrosity at my last job the "database" was spread across 5 different databases totaling around 600 different tables containing roughly 300 million records no foreign keys, not a drop of documentation all access was via inline SQL from the web apps with only a handful of stored procedures that are mostly unused had fairly heavy usage, running about 10 million transactions a week took weeks to get even minor changes put in and working because everything was so fragile
So are you, or are you not, the SQL ninja you claim to be?
He's the Beverly Hills SQL Ninja
One method you can try is using a trace: Note, while this is simpler if it's all sprocs, this can also be made to work if you have dynamic sql being sent in from web pages, etc as well. Anyway, run a trace against your database, using just the sproc trapping function. Let it run for about an hour into a table. You can then go through and analyse which sprocs got fired off, when, and in what numbers. This will give you a pretty good idea of just what the database acutally does and where most of the use is coming from. 
How about [split -l](http://manpages.ubuntu.com/manpages/intrepid/man1/split.1.html)?
seems somewhat promising. The only thing about this one is that it seems to split things only based on what size you specify the pieces to be. In this case, I'd be afraid of accidentally doing something like splitting a query in half because only half the query fits inside a chunk. I'll keep looking, if not I may just have to bite the bullet and either a) split it manually 2) write my own tool to split SQL. thanks though
Do you have access to connect to the MySQL server from your machine? If so, use the mysql command line utility to connect to the server and feed it the SQL directly. mysql --host=serverName &lt; your12MbTextFile.sql
What is this sproc you speak of?
stored procedure. also called a routine in some systems. If your database only uses sprocs, congratulations, you have an easy job. If, however, you have websites, apps, binaries, etc, all calling up and executing random sql, your job is going to be a lot harder. 
oh, Stored Procedures = SPROCS, ok i know what those are. thanks.
Well at the Moment I am Using Quest's Spotlight for SQL Works well just went and bought the Enterprise Version.
What about "PostgreSQL"? I usually say "postgress-Q-L"
Sequel. It's easier to say. "Sequel Server", "MySequel", "sequel query", "SequelLite" - only one of these are correct but it's how I pronounce each of them. I drop the acronym completely for "Postgres."
Well, if you're referring to the Microsoft product, 'sequel server' *is* the right way to say it.
Whoops, looks like I left out the 'where' in this version.. but it is in my current working one. :) 
The ProcedureID to multiplier relationship should be stored in a table rather than hard-coded into the query, then you would not need a case statement.
It actually is, but I needed values I can change on the fly for different reports. I am certainly not married to this code, though.. I'm just looking for a good way to display it concisely. 
Perhaps using the MIN function to return the lowest value of usedtoaccessdevice, then filter off all the 0s. Something like this: SELECT deviceid, min(usedtoaccessdevice) FROM rn_ip WHERE min(usedtoaccessdevice) &gt; 0 GROUP BY deviceid;
group by table_name
 select distinct ri.deviceid from rn_ip ri where not exists (select '1' from rn_ip ri2 where ri2.deviceid = ri.deviceid and ri2.usedtoaccessdevice = 0)
Ok. If you want to keep the case statement but display it more concisely you could write it like this: case when dspd.ProcedureID in (1,7,23,26,29,37) then 1 when dspd.ProcedureID in (2,25,27,36) then 2 else 0 end
If I remember correctly, you can't use an aggregate function like min() in a WHERE clause. Try HAVING instead. 
Hey.. thank you both for your help. That did the trick. :) .
I know, I know. I just always leave it out when using a command line... cheers. 
If you don't have a 64bit machine you won't be able to use 2008R2. I use 2008R2 Developer.
I vote for 2008 R2 Developer. 
Few things: * Easily eliminate all of the '05 editions. * Just learning? 2008R2 Express - http://www.microsoft.com/express/Database/ which I BELIEVE is licensed to be used however you feel (except its limited in its hardware usage abilities so not ideal for heavy database usage). Developer is the same as Enterprise EXCEPT its limited to just development and testing (no production environment abilities). * "Future work opportunities" - might want to read the license agreement to using MSDN-AA software. Its only for use while you're at the school for academic reasons. Seriously though, Express is more than enough for what you really need to start out on. From Wikipedia: &gt; [SQL Server Express Edition](http://en.wikipedia.org/wiki/Microsoft_SQL_Server#Editions) - SQL Server Express Edition is a scaled down, free edition of SQL Server, which includes the core database engine. While there are no limitations on the number of databases or users supported, it is limited to using one processor, 1 GB memory and 4 GB database files (10 GB database files from SQL Server Express 2008 R2[28]). The entire database is stored in a single .mdf file, and thus making it suitable for XCOPY deployment. It is intended as a replacement for MSDE. Two additional editions provide a superset of features not in the original Express Edition. The first is SQL Server Express with Tools, which includes SQL Server Management Studio Basic. SQL Server Express with Advanced Services adds full-text search capability and reporting services.[29] If for some reason you feel you need, then go for Developer - unless you plan on using it in production... in that case, Enterprise but realize the license requires that you get rid of the software when you leave. It may have changed, or it may be different by institution, so I apologize if someone proves me wrong on the licensing. 
Developer has all features others do. Go with that
&gt;2008R2 Standard That would be my vote. Especially if you're just learning. Go the Express route if you think you will ever want to use anything you build for "production" purposes.
who downvoted this? that's the point of developer. it's whole point of existence is so that you can use features that exist in enterprise and data center without having to pay for a license of enterprise or data center.
SQL 2008 R2 works just fine on 32-bit systems. Windows 2008 R2 is a different story. This has led to much confusion/consternation in my workplace.
Thanks for clarifying that.
Because it's one less syllable.
It looks like the '+' character is being escaped on the way into the database and isn't being unescaped on the way out. Some programs will automatically unescape characters, which would explain why it appears correctly in some programs and not others. FYI, the html escape code for '+' is '&amp;amp;#43;'
they definitely use key/value-based DBs (like cassandra) for these types of things which are much, much, much faster for their given purpose than RDBs like mysql. 
Thanks! I think I figured it out. I hadn't created the report but was editing it. It had in textbox ' ="89 days+" and all I did was take out the =" " and it appears to work correctly now =-) I should have tried doing something simple like that instead of searching Google forever!
What exactly is being formatted? Formatting a query should have zero effect on its speed. 
you can make a stored procedure on your database instead (which will retain all your formatting &amp; comments), and have your report in BIDS point to that procedure.
That depends. If the driving query is small and the subquery is large, then it might not be so bad. Especially in something like SQL Server, which will use a spool operator to deal with this sort of thing. In fact, with that query it will probably cache the subquery as it's not a correlated subquery. Could well be fast. *Actually*, if there is an index ID and booking ref, that query could actually be pretty fast. Of course, if that subquery is large enough and it gets cached, it could also spool to disk or it could give you memory issues. 
It's removing comments, renaming table aliases used in sub-queries, and removing top-factored code. E.g.: -- Get the first names of all employees who are male, alive, have a -- last name of "Bloggs" and live in New York or Paris SELECT FName FROM EMPLOYEES WHERE (Table1.Alive = 1 AND Table1.Gender = 'M' AND Table1.LName = 'Bloggs' ) AND (Table1.City = 'New York' OR Table1.City = 'Paris') Becomes: SELECT FName FROM EMPLOYEES WHERE (Table1.Alive = 1 AND Table1.Gender = 'M' AND Table1.LName = 'Bloggs' AND Table1.City = 'New York') OR (Table1.Alive = 1 AND Table1.Gender = 'M' AND Table1.LName = 'Bloggs' Table1.City = 'Paris') This makes things a lot harder to read.
select r.remittance_id, r.remittance_type_cd , r.remittance_sequence_no,r.COLLECTED_AMT, r.paid_dt , rh.responsibility_centre_cd, rh.EFFECTIVE_DTTM from remittance r, remittance_history rh where rh.REMITTANCE_STATUS_CD = 11 and rh.REMITTANCE_TYPE_CD = r.REMITTANCE_TYPE_CD and rh.REMITTANCE_ID = r.REMITTANCE_ID and rh.REMITTANCE_SEQUENCE_NO = r.REMITTANCE_SEQUENCE_NO and r.REMITTANCE_STATUS_CD = 11 and rh.EFFECTIVE_DTTM = (select max(EFFECTIVE_DTTM) from remittance_history sub where sub.REMITTANCE_TYPE_CD = r.REMITTANCE_TYPE_CD and sub.REMITTANCE_ID = r.REMITTANCE_ID and sub.REMITTANCE_SEQUENCE_NO = r.REMITTANCE_SEQUENCE_NO) --------- This is a pattern I have used when I only want the most recent record. You can also use min() for the oldest record.
I've changed up your query a bit for performance, and make the assumption that you have some type of RH.UNIQUEID select r.remittance_id, r.remittance_type_cd , r.remittance_sequence_no,r.COLLECTED_AMT, r.paid_dt ,rh.responsibility_centre_cd, rh.EFFECTIVE_DTTM, max(RH.UNIQUEID) as History from remittance r inner join remittance_history rh on rh.REMITTANCE_TYPE_CD = r.REMITTANCE_TYPE_CD and rh.REMITTANCE_ID = r.REMITTANCE_ID and rh.REMITTANCE_SEQUENCE_NO = r.REMITTANCE_SEQUENCE_NO and rh.REMITTANCE_STATUS_CD = r.REMITTANCE_STATUS_CD where r.REMITTANCE_STATUS_CD = 11 group by r.remittance_id, r.remittance_type_cd , r.remittance_sequence_no,r.COLLECTED_AMT, r.paid_dt ,rh.responsibility_centre_cd, rh.EFFECTIVE_DTTM 
That's a pain in the ass. I ran into a similar problem and I think it was because I upgraded projects from SSRS 2005 to 2008. One way around it would be to create a schema in the database specifically for reporting, create a user to access that schema with read/exec permissions and just create stored procs to handle report data. This way you'll be using Management Studio to handle changes to report data. This also makes it easy for some types of code changes if a dev in the team doesn't have BIDS installed. The SQL editor provided my SSRS is awful and I rarely ever use it to develop with.
From a software standpoint there isn't much of a difference. Enterprise limits you to something like 4 virtual machines where as Datacenter has no restrictions. From a hardware standpoint there are some big differences. Datacenter will let you add/remove/swap processors and memory while the server is running. Enterprise would blow up. Enterprise supports up to 8 x64 sockets, Datacenter supports 64 x64! TLDR- Enterprise should be fine.
well, not if we're considering putting this in our data center. :p
this says enterprise has "unlimited Virtualization" http://www.microsoft.com/sqlserver/2008/en/us/enterprise.aspx what's the double talk here? 
I have executed both queries and am receiving 1% more results CrossWired's query. For the receipts that were reserved, more than once, I am seeing duplicates. br0kenface's query seems to be the most accurate, although I am still trying to figure out why CrossWired's query is not exclusively returning records with only the max history_sequence_no (unique ID for the History table) I am still getting multiple history_sequence_no's for receipts that have more than one reserved history entry I will be using br0kenface's query, but I am still curious to learn why CrossWired's query is returning duplicates. 
That's for 2008. In 2008R2 they removed that and limit you to 4 VMs + the host. http://www.infoworld.com/d/virtualization/microsoft-changes-sql-server-2008-r2-virtualization-licensing-156
so is there much difference between 2008R2 Enterprise and 2008 Data Center? 
Is there a 2008 Data Center? I thought that was introduced in R2.
Did you mean 2008R2 Data Center and 2008 Enterprise? The only major difference there would be the hardware support. DataCenter will let you hotswap and go beyond 8 physical processors (multicores only count as 1). Both offer unlimited virtualization. There aren't many new features in R2. If it was me I'd be tempted to buy whatever is cheaper to license (I suspect 2008 Enterprise).
Yeah, you're right, it looks like data center is just an extension of enterprise http://www.microsoft.com/sqlserver/2008/en/us/datacenter.aspx
yeah, Enterprise is cheaper 
upvotes for you, thanks for the input
Happy to help! Good luck!
He has EFFECTIVE_DTTM in his group by clause. Since this field is unique for each history row, it is not really grouping anything, and you will still get multiple rows per remittance record. If you remove rh.EFFECTIVE_DTTM from both the select and group by clauses in his query, it should have the same number of results as mine. I am guessing you need the last transaction date, though, in which case the sub-query method is what I would do.
Full-text searching is slow especially when you've got a lot of rows to check against and it will just keep getting worse as you add more. One way to tackle this problem would be to pre-filter to a subset of rows. You could add some computed columns to your table with metadata that will hint the db engine. For example you might store the product soundex value in a column and then restrict your matching to the subset of rows with product names having the same soundex as $name. Another approach might be to build a search term lookup table that stores common search terms and products matching those terms. However realize that if it's performance you're after than you might have better luck going with a search server of some kind that's built specifically to index and search your data. Google for 'Lucene', 'Xapian' for examples of what I'm talking about. And here's a few articles on the subject if you want to know the why as well as the what: http://infolab.stanford.edu/~backrub/google.html http://philip.greenspun.com/seia/search
You need to get the text searches out of it. Your best bet is to create a table of product IDs that map to names. Search on this smaller table first, then use the resulting ids to pull from the products table, with an index on the ids. You can keep the name table indexed by name and that might help too, but probably not by much.
I think you can leave off the "rs:Command=Render" part. You might try: http://reportserver/Reports/Pages/Report.aspx?ItemPath=%2fTest+Reports%2fTestReport1&amp;Year=2010
this has to do with the fact that an average DBA today isn't able to install any of Oracle's products on a unix system unless it also has a full gnome desktop. disgusting, really..
Try this: select sum((points/credits)*count)/case when sum(count) = 0 then NULL else sum(count) end from (select points, case when credits = 0 then NULL else credits end credits, count from tbl ) Some cases are for checking of division on 0. If you need some grouping, then add it: select CrseNo, sum((points/credits)*count)/case when sum(count) = 0 then NULL else sum(count) end from (select CrseNo, points, case when credits = 0 then NULL else credits end credits, count from tbl ) group by CrseNo
pro: * easier management * simple backup con: * possibly slower table (depending on how locks are handled) * have to stream from sql server rather than allowing the web server to optimize streaming from disk * have to manage file system and live with constraints of file system (duplicate named files, etc)
I actually got it to work! An alias was missing which caused the error. Thanks a lot!
Best of both worlds - use a FILESTREAM column.
With 100 rows, you aren't going to feel much of a difference either way, and the simplicity of storing it in the database will probably make that the easier choice. If you were dealing with a lot of data though, I'd definitely store your files on something like [S3](http://aws.amazon.com/s3/) or a similar service, which is dead cheap and takes the load, storage, and backup requirements off your plate, in exchange for the added complexity of having to transfer the files up via an API.
Do you have to use SQL Server? If not, there's a MySQL extension called blobstreaming that looks to make handling blobs in a database easy by allowing you to access the blobs via a RESTful interface. This eliminates any need to write code to extract blobs from a database and send it to a browser.
you don't need a temp table, you can use a "CASE" (like a switch) SELECT CASE ColumnNameForTheNumberYouAreReading WHEN 2 THEN 'STARTED' WHEN 3 THEN 'STOPPED' WHEN 4 THEN 'COMPLETED' WHEN 5 THEN 'ERRED' WHEN 6 THEN 'DELETED' END FROM TheNameOfTheTableYouAreSELECTingFrom
There are lots of ways depending on where you are writing the query and how much access you have to the database itself, as well as which DB you are using (SQL Server, MySQL, Oracle). I assume the report draws its data from a stored procedure? In that case it is as simple as: CREATE TABLE #mailbox_statuses_tmp(mailbox_status_id INT, name VARCHAR(50) INSERT #mailbox_statuses_tmp VALUES(2, 'Started') INSERT #mailbox_statuses_tmp VALUES(3, 'Stopped') INSERT #mailbox_statuses_tmp VALUES(4, 'Completed') INSERT #mailbox_statuses_tmp VALUES(5, 'Erred') INSERT #mailbox_statuses_tmp VALUES(6, 'Deleted') Be sure to drop your temp table at the end of the stored procedure: DROP TABLE #mailbox_statuses_tmp You could also use table variables which don't need to be dropped. If this won't work for you and there are additional constraints, respond and we will work through a solution that works for you. EDIT: Skyguard's case statement works just as well.
Awesome thanks! This looks simple enough that I can probably get it to work :-)
you are welcome... a simple solution is always preferrable
Have something in the ELSE clause to cover any cases that haven't come up yet or in case the field is NULL. 
Definitely use table variables like: DECLARE @MailBoxStatusTable TABLE ( StatusID INT, StatusName VARCHAR(255) ) If your procedure errors 1/2 way through; you can get into all kinds of mess as the #Table hasn't been dropped. This doesn't happen with the above table
 pro: Easier replication / mirroring / load balancing con: Higher database load Reduces cache from db server significantly. Larger delay until you can start sending data to the client 
If these are permanent statuses, why don't you have a reference table and do a regular join? This can't be the only query you use that needs English descriptions of these statuses.
Do you mean add a reference table to the database, or is there a way to do that in the report? They're planning on adding a table eventually, but it's making every report I do kind of a pain in the ass until they do =-\
Yes, I would add it now if possible, i mean its reference data, it should be in a static table.
I don't think so, but you shouldn't need to. Everything that the wizard does can be done after the fact as well. Is there something specific you are trying to change?
very cool. This would save a lot on search costs. 
 I have always gone along the likes of this but try to draw them as a venn diagram ;) 
Looking at your query it seems like you're searching for places where the first name is in the username at the very start, or where the last name is in the username in any position. &gt; WHERE username LIKE CONCAT(FirstNames.firstName,'%') So if the first name is "geoff" then that where statement will match any username that begins with "geoff", like "geoff432" or "geofftheman". Cases where the username and first name are different- like with a user named Geoff and the username "applejacks", or even "TheGeoff", would miss in your query. The last name query is similarly flawed- you're comparing the last name to the username, so if the user doesn't have it in his username then it won't show up in the list. The grouping and having parts of the query just through things off even more. Why would you group by a key when the key is guaranteed to have only one result? If you were looking to do a count of unique first or last names than a group by there might make sense, but I really can't figure out what you're trying to do with that there. 
&gt;Why would you group by a key when the key is guaranteed to have only one result? Because there can be more than 1 firstName match per Key. E.g. Sal or Sally for username 'sally'. I want the longest match. It doesn't matter if their first or last name isn't in the username; I don't care about those cases. I also am not concerned with cases where their firstName doesn't begin the username.
Wouldn't the HAVING clause of your first name statement preclude you from finding any usernames of users whose first names are not the longest first name in the system? I'm not sure off the top of my head if the MAX(CHAR_LENGTH(FirstNames.firstName) is going to restrict you only to first names that match the longest name in the table (so if there is a first name "Christopher" you wouldn't find any users who have a first name of less than 11 characters) or if it is only going to find the maximum within usernames that match on the WHERE clause - such that the start of the user name matches the start of the First name - in which case may be Geoff isn't showing up because "Geoffrey" is in the table? Either way, I'd try removing that having clause and running it and see what different results you get. 
The problem is that you're referencing columns in your select list that don't exist in your group clause. This is invalid. Most database engines will correctly throw an error and refuse to run such queries. MySQL just produces random results instead.
PM me for specifics but if its a default instance of sql you'll need access on port 1433
Very basically, you need to enable network access to even allow remote connections. You also need to make sure the firewall (if enabled) is open on that port (probably 1433). * First, make sure you understand the security implications of doing this. If you don't then STOP. * Otherwise, go to Start &gt; MS SQL Server &gt; Server Network Utility * Enable the TCP/IP protocol It has been a while, but I also vaguely remember a bug related to 2000 and network access. You needed a patch to get TCP/IP connections working. If you have any of the service packs, it should be fixed.
Two things come to mind. I realise I'm not telling you how to set this up, but they're definitely worth considering. 1. As SQL Server is installed on the web server itself, then I wouldn't recommend opening it up for TCP communications. You'd be better off using shared memory or named pipes to connect to the server. If you do decide to go the TCP route, I'd triple check to make sure it won't accept connections from the Internet. 2. SQL Server 2000, unused since 2004? This will probably be Service Pack 3a at the best case. SQL Server 2000 is now out of support, and so it may be worth looking at upgrading to 2005 or 2008 Express edition. These are still supported and more secure.
[huh?](http://www.reddit.com/submit?url=http%3A%2F%2Fwww.codinghorror.com%2Fblog%2F2007%2F10%2Fa-visual-explanation-of-sql-joins.html)
I do select count (*) as "something". not sure if it works that way in everywhere, but as is the sql server
If you are using SQL Server, this page is relevant (from Books Online 2008 R2, but the 2000 version has the same text): http://msdn.microsoft.com/en-us/library/ms179300.aspx Essentially, "Column AS ColumnAlias" is the preferred method, while "ColumnAlias = Column" is an older syntax for compatibility with older versions of SQL Server. Personally, I use AS unless I'm assigning the column value to a variable (eg @variable = SomeColumnName), but I can see how having the aliases all lined up on the left may be appealing, if that's what you're used to.
 Yes this is doing more. Your setting a variable to count(*) Try this COUNT(*) AS [Total Rows] 
Double-quoting your column names is only valid with QUOTED_IDENTIFIER set to ON. While that is the default for newer clients, some older clients connect with that setting turned off and the double quotes won't work.
Yeah I usually use brackets anyway
 SELECT COUNT(*) 'Total Rows' FROM Table; or SELECT COUNT(*) AS 'Total Rows' FROM Table; Multiple columns: SELECT ColumnA AS 'ColumnAAlias', ColumnB AS 'ColumnBAlias' FROM Table; Just put the alias after the column and before a comma. Depending on your DBMS, 'AS' is optional. Edit: BTW, its not a bad habit at all to alias, but just make sure you're not putting all your ability to identify a column on the alias. Make sure the actual column name is clear enough (or defined somewhere) that you know what it is without an alias. Doesn't change any results, just makes it easier to understand as the DB gets more complex.
Sorry, that's incorrect, assuming we're talking about SQL Server. There is no variable there. What he posted and what you posted return identical results.
All of these produce identical results in SQL Server: SELECT Alias = COUNT(*) FROM Table SELECT 'Alias' = COUNT(*) FROM Table SELECT [Alias] = COUNT(*) FROM Table SELECT COUNT(*) AS Alias FROM Table SELECT COUNT(*) AS 'Alias' FROM Table SELECT COUNT(*) AS [Alias] FROM Table '' or [] are required if your alias has spaces in it. If no spaces, you can exclude these if you wish. So this works: SELECT [Some Alias] = COUNT(*) FROM Table But this does not: SELECT Some Alias = COUNT(*) FROM Table Which you use is ultimately your preference but I have tried each of these at various times and ultimately settled on: SELECT Alias = COUNT(*) FROM Table And using the brackets when necessary. The reason for this is that when you have a complicated query that has multiple calculations or subqueries in the formulas, it's MUCH easier to tell what you're looking at when the column names are listed at the beginning of each line, rather than having to scroll around to the end of the line (or find the last line if you've used line breaks in your subqueries). So I end up with something like: SELECT Column1 = (SELECT TOP 1 Value FROM SomeTable WHERE Variable = @Variable) , Column2 = (SELECT MAX(T2.Value) FROM Table1 INNER JOIN Table2 ON Table1.ID = Table2.ID) , Column3 = Num1 * Num2 + Num3 - Num4 FROM Table as opposed to this: SELECT (SELECT TOP 1 Value FROM SomeTable WHERE Variable = @Variable) AS Column1 , (SELECT MAX(T2.Value) FROM Table1 INNER JOIN Table2 ON Table1.ID = Table2.ID) AS Column2 , Num1 * Num2 + Num3 - Num4 AS Column3 FROM Table 
 Fair enough :) 
 SELECT COUNT(1) FROM ... COUNT(*) will often if not always invoke a table scan. If I was at work I'd check some execution plans to verify it, but in general, unless you really need everything, avoid the asterisk.
That's kind of where I was headed with this. I, too, use the 'Alias' = Column syntax because I find it makes complex queries much easier to read for a human. I posed the question because I wasn't sure if this was "correct" since I couldn't seem to find any documentation on this. I know it works, been doing it for years, just wasn't sure why.
Great point, didn't think about that when posting my example but you're right, COUNT(1) or even COUNT(ColumnA) would be preferred rather than using *.
I have seen this shown in textbooks and web examples everywhere, but I find that in the "real world" it quickly becomes cumbersome when calculations, case statements, etc are involved. Trying to look past the calculations to find the column name isn't easy. GunnerMcGrath provides a pretty good example below so I won't repeat it here. In his example you can very quickly locate "Column2" and figure out what it does. Putting the alias after the calculations "buries" the names in code, forcing you to read through all the code to spot the aliases. If you don't alias much in a straight-from-the-table query then it's a moot point. However, throw in a bunch of logic and you'll thank yourself for making the column alias names easy to locate.
That's what I think as well, and you seem to be in agreement with me, however I'd still love to find some better documentation. http://msdn.microsoft.com/en-us/library/ms179300.aspx That does tell me the syntax is valid, it's just in there for backwards compatibility. Perhaps I'll keep digging for older documentation where they show this syntax.
(Insert SQL Server disclaimer) Actually, COUNT(*) is pretty benign, but the poor * has a gotten a bit of a reputation over the years when it's in a SELECT clause. From http://msdn.microsoft.com/en-us/library/ms175997.aspx &gt; COUNT(*) does not require an expression parameter because, by definition, it does not use information about any particular column. As a test, perform a SELECT COUNT(*) FROM SomeTable, where there is a non-clustered index that is quite skinny compared to the clustered index. If you look at the execution plan, you're likely to see an index scan of the non-clustered index. The reason for this is that the skinny non-clustered index will fit more rows per database page, and so fewer pages need to be scanned to fulfill the query. Edit: Formatting
By the way, I prefer using no brackets or quotes when possible and using brackets when necessarily instead of single quotes, simply because single quotes get turned red in the management studio and I don't necessarily want all my column names to be highlighted like strings. Again a matter of taste, since some people may really like having their column names highlighted, but since you rarely give column names to EVERYTHING (especially when the default column name is what you want to use), you end up with some aliases highlighted and others not. One more little bonus to using the [] brackets.. in sql management studio, if you double click on a string enclosed by brackets, it highlights the whole thing just like it would do for a single word. =)
What I'd really like to know is what was wrong with the old way that they felt they needed to change it. Maybe the AS format is standard SQL and the = format isn't, not really sure. AS is easier to use when you're moving really fast because you tend to type out the value you want before you type the column name (unless you really force yourself into the habit of thinking of the name first) but that doesn't really lend itself to good maintainability in the long run.
I have never used SQL Zoo before but I am happy to advise you in any way that I can. However, I am reluctant to write the queries for you if you say this is an 'assignment." What sort of assignment is this? Having never used SQL Zoo before, I was playing with it and it seems kind of neat - but is there a way to check your answers are correct? For example, the first question on the page you sent mentions "events that will clash" - I ASSUME this means any events where there is any overlap in times between the ones currently assigned to co.CHt and the new ones he will be assigned when he gets all of co.ACg's events assigned to him. But is there any way to confirm when we write the query to find those that that is the correct answer? Anyhow, do your best to come up with the starts of some queries - figure out the actual challenges in each problem that you are having trouble overcoming and I will happily do my best to help you through each one.
SQL Zoo is awesome. It taught me how to SELECT, INSERT, UPDATE, DELETE and JOIN in the first two days at my new job. You answer each question with a SQL query. They return the result to your query and the result of the expected answer and let you know if they match or not. If I remember correctly, they only verify that your result set matches the expected outcome. So you could cheat by producing a completely bogus query that manages to return the proper result set.
It seems like only some of their test questions allow you to see if the query you are submitting results in the correct answer. The questions that the OP has linked to only return a resultset (as best as I can tell) - not a "That is correct"/"That is incorrect" result.
You could do something like this: create table roles ( id int, name varchar ); insert into roles values (1, 'designer'), (2, 'qa'), (3, 'manager'); create table people ( id int, role_id int, name varchar ); create table products ( id int, name varchar, ..whatever else you need.. ) create table signoffs ( id int, product_id int, person_id int, signoff_date datetime ) Only when a product has three corresponding rows in signoffs, each with a person with different role_id, would you be done. Yours has the benefit of being able to write application code more easily for the problem as currently defined. Mine will be easier if the business rules change. For example, if an additional role is added as part of the workflow, you'd only need to insert a new row into "roles", and change your application code, not make a schema change. In your schema, you'd need to add an additional set of columns for that new role.
Thanks. Do you see any other problems with the approach I originally represented?
Assuming you have MS SQL Server: First, you need the 'front end' tool that MS provides. It's called [Sql server management studio](http://en.wikipedia.org/wiki/Microsoft_SQL_Server#SQL_Server_Management_Studio) Probably this is installed on the server that has the database on it. To 'view' the database, you use the SSMS to point towards the server that holds your database. From there, you have a visual interface that should open up to see individual databases on the server. From there, you can open up the individual objects on that database. I can't tell from your description what 'having to make the same changes twice' is, but it sounds like you want to have multiple sites use the same DB, which shouldn't be hard. Code is usually found in the 'stored procedures' part. Probably what you'll ultimately have to do is add one column to one or more table, and modify one or all stored procedures to look for this column when doing stuff with the database. 
Basically, yeah. I have two store-fronts and every time a new product is added / modified I have to do so twice. This and many other reasons are why I would like to have both sites work from the one DB. The only problem is: everything. I found iSQL viewer on source forge, however I have no idea how to use it exactly. I only have FTP access to the host....
Bare minimum, you will need to: * get a user/pass for the SQL server * figure out what kind of server it is (probably MySQL or MS SQL). If no one knows, you will need to be able to log into the host machine to figure it out. Do you have physical access to the server?
First you need to figure out what database product you have. Do you have Microsoft SQL Server, MySQL, Oracle, PostgresSQl, what? 
What Operating system? If you have full access to the file system through FTP I could tell you where to look..... 
I'm going to use a fictional table/view and no joins or aliasing for the sake of clarity. SELECT Customer, YEAR(InvoiceDate), MONTH(InvoiceDate), SUM(InvoiceAmount) FROM InvoiceTable WHERE InvoiceStatus = 'Unpaid' AND InvoiceDate BETWEEN '3/1/2011' AND '6/30/2011 23:59:59' GROUP BY YEAR(InvoiceDate), MONTH(InvoiceDate), Customer ORDER BY YEAR(InvoiceDate), MONTH(InvoiceDate), Customer And you might get a result like this: Customer | Year | Month | Sum ---------+------+-------+--------- Bob | 2011 | 3 | 10215.67 Steve | 2011 | 3 | 9811.25 Bob | 2011 | 4 | 14302.78 Steve | 2011 | 4 | 7221.95 Teresa | 2011 | 4 | 320.93 Bob | 2011 | 5 | 12755.22 Steve | 2011 | 5 | 8991.00 Teresa | 2011 | 5 | 117.25 Bob | 2011 | 6 | 12201.51 Steve | 2011 | 6 | 5100.17 So in summary, group by year and month. If you need help using this in your query let me know but I think you should be able to figure it out from this simple example.
What connection string does the web page use to connect to the DB? 
 You may want to look into replication. Or replication for certain table's Though it may cause problems in your application if things are changed on both sites at the same time.
Try adding something like this to your SELECT list: *Sum(Case When i.due_date &lt;= DateAdd(d, 30, GetDate()) Then i.currency_amount Else 0 End) AS DueNext30Days* That'll get you a total for anything due in the next 30 days. You can also use BETWEEN and adjust the DateAdd function to get a sum for any date range you want. 
Remember if you can do this you can now filter post-aggregated values (eg. Year(invoicedate), Month(invoicedate)) by using the HAVING clause. SELECT Customer, YEAR(InvoiceDate), MONTH(InvoiceDate), SUM(InvoiceAmount) FROM InvoiceTable WHERE InvoiceStatus = 'Unpaid' AND InvoiceDate BETWEEN '3/1/2011' AND '6/30/2011 23:59:59' GROUP BY YEAR(InvoiceDate), MONTH(InvoiceDate), Customer Having Year(invoicedate) &lt; 2012 and month(invoicedate) &lt; 12 ORDER BY YEAR(InvoiceDate), MONTH(InvoiceDate), Customer 
HAVING is typically used with the aggregate function(s) themselves, since its purpose is to filter data after the actual grouping takes place. For example if you wanted to see all invoice sums over 10k you'd write "HAVING SUM(InvoiceAmount) &gt; 10000.00". You can't do this in the WHERE clause. Your example doesn't need the HAVING clause as Month() and Year() aren't aggregate functions and I might argue that doing it in the WHERE clause is more optimized anyway since the restrictions are applied immediately during the initial search rather than after the grouping. Though unless your dataset is very large the benefit would be negligible. Still it's just good practice to use HAVING how it was intended.
You should not be allowed to work in the food industry.
Thanks heaps for this! This got me sorted, this is the code I ended up with for the record: SELECT c.name, c.customer, c.address5, c.analysis_codes2, i.currency, sum(i.currency_amount) AS currency_amount, Sum(Case When i.due_date &gt; DateAdd(m, 0, '2011-03-01') Then i.currency_amount Else 0 End) AS CurrentDue, Sum(Case When (i.due_date &gt;= DateAdd(m, -1, '2011-03-01')) AND (i.due_date &lt; DateAdd(m, 0, '2011-03-01')) Then i.currency_amount Else 0 End) AS OneMonth, Sum(Case When (i.due_date &gt;= DateAdd(m, -2, '2011-03-01')) AND (i.due_date &lt; DateAdd(m, -1, '2011-03-01')) Then i.currency_amount Else 0 End) AS TwoMonth, Sum(Case When (i.due_date &gt;= DateAdd(m, -3, '2011-03-01')) AND (i.due_date &lt; DateAdd(m, -2, '2011-03-01')) Then i.currency_amount Else 0 End) AS ThreeMonth, Sum(Case When i.due_date &lt; DateAdd(m, -3, '2011-03-01') Then i.currency_amount Else 0 End) AS FourMonthPlus FROM scheme.slcustm AS c RIGHT OUTER JOIN scheme.slitemm AS i ON c.customer = i.customer WHERE (i.open_indicator = 'O') GROUP BY c.name, c.customer, c.address5, c.analysis_codes2, i.currency ORDER BY c.name ASC I'll insert the 2011-03-01 with PHP unless someone has an idea for that? just needs to be the first day of the current month
I use count(9) since it's involves minimal finger movement. It's all personal preference. 
I do select count(9) [Count Of Something]. No need for AS or =
If you add LIMIT 25, 1 at the end of your query, it will return only the 25th row. If you want 25, 50, 100, and 250, just issue four separate queries with the appropriate LIMIT clauses.
Normally you use WHERE clause to filter your data down, and for this specific query I don't see why you couldn't use a where clause to filter the clients table down, e.g. AND clients.id IN (1,2,3) Since you're using a GROUP BY, another option is to use a HAVING clause, which filters the data **after** the GROUP BY. Another side tip: I find my queries much more readable when I use JOIN ON syntax, e.g. SELECT * FROM clients INNER JOIN orders ON orders.client_id = clients.id It helps keep the WHERE clauses clean and easier to read. Edit: I completely mis-read the question. As mentioned by shaunc, you want a LIMIT statement on four separate queries
The concept you're after is result ranking. Googling for "mysql rank" will be illuminating. Specifically, you want to calculate a rank, aka: a row number, for each row of your result set. You'll then be able to qualify on that rank as needed. Here are a [couple](http://stackoverflow.com/questions/3576304/mysql-row-number) [examples](http://www.fromdual.com/ranking-mysql-results) of the technique. If you do wind up issuing multiple LIMIT queries as others have suggested, look into the UNION operator. This will allow you to concatenate the results into a single set without making multiple round trips to the server.
I'm not familiar with what's available in mysql, but in MS SQL SERVER I would definitely use ranking. You just need to figure out what to rank them by.
Well, that seems [rather arbitrary](http://www.wolframalpha.com/input/?i=831105021+seconds+before+25+May+2010+09%3A56%3A21). After is even [more arbitrary](http://www.wolframalpha.com/input/?i=831105021+seconds+after+25+May+2010+09%3A56%3A21). Do you have another timestamp to corroborate this one?
Look for a function somewhere in the database itself. I have seen a random-seeming timestamp like this before and the application was using a database function to convert it for its own reporting. I just used it as well.
By the way, it turned out to be number of seconds since 1985 IIRC. It was the date the company was founded or something...
Your SQL Server is sitting on a Windows 2003 or 2008 OS somewhere. You access it via Remote Desktop. This is true whether you have VMWare or a physical server. You don't really care about the hardware layer, unless this is a huge application that has potential performance issues (probably not). You don't even need to know that VMWare exists.
Also, you could even manage it completely remotely. You would just need the sa user/pass and SQL Server Manager.
MS Access should have some built in functionality to link to tables residing in a networked SQL Server instance. Given your web dev experience I would have just opted to do the front end on their intranet. ASP.NET and C# plays nicely with SQL Server. 
This is a variant of unixtime. Unixtime is the number of seconds since jan 1 1970, but any arbitary date works. The big advantage to storing dates in this way is that you store a datetime value - 8 bytes, in an int value - 4 bytes. 
A virtual server works very much like a real one, except there's no physical place for the keyboard, mouse and monitor plugs like you might expect from a real PC. Most of the time, you would use RDP or SQL Management Studio to connect to the box exactly as if it were the real deal. You would only ever use the "Virtual" tools if you needed direct access as if you were plugging a keyboard, mouse and monitor into the server (such as an OS failure). Other than rare circumstances, you will never know the server exists within a VM structure, unless you are trying to negotiate for support with crappy and dated software providers. (Don't worry, Microsoft isn't one of them... on this point, anyways)
admiralwaffles is on to something. I have no clue what DB RQM uses, but I have access to Oracle and MySQL. I imagine it would be different depending on your DB. The key is the research that admiralwaffles did. The beginning of time according to your value is 01/23/1984 03:26:00 and the end of time is 09/24/2036 16:26:42. In Oracle I was able to convert your integer to a date using either the end of time or the beginning of time. -- Convert spriteburn's date using beginning of time = 01/23/1984 03:26:00 SELECT to_char( to_date('01/23/1984 03:26:00','MM/DD/YYYY hh24:mi:ss' ) + (ABS(-831105021))/60/60/24, 'MM/DD/yyyy hh24:mi:ss') FROM dual; -- Convert spriteburn's date using end of time = 09/24/2036 16:26:42 SELECT to_char( to_date('09/24/2036 16:26:42','MM/DD/YYYY hh24:mi:ss' ) + -831105021/60/60/24, 'MM/DD/yyyy hh24:mi:ss') FROM dual; In MySQL: SELECT '1984-01-23 03:26:00' + INTERVAL ABS(-831105021) SECOND FROM dual; SELECT '2036-09-24 16:26:42' + INTERVAL -831105021 SECOND FROM dual; EDIT: In MySQL 
yeah, i was reading up on this while browsing the net... but how would you convert it?
 any vmware solution would probably still beat access with multiple users over a network drive.
 any vmware solution would probably still beat access with multiple users over a network drive.
 any vmware solution would probably still beat access with multiple users over a network drive.
Generally I use a function, but the function basically has this sort of thing in it: psuedocode: Create function dbo.foo (@int int) Returns @datetime datetime Select @Datetime = dateadd(ss, @int, '1/1/1985') End For datetime to unixtime Create function dbo.bar (@datetime datetime) Returns @int int Select @int = datedifff(ss, @datetime, '1/1/1985') As you can see, you could just as easily use the conversion within your code: the function is just there to reduce the maintenance cost of your code. End 
 select c.*, p1.* from purchases p1 inner join (select custid, max(time) as time from purchases group by custid) p2 on p1.custid = p2.custid and p1.time = p2.time inner join customers c on p1.custid = c.custid order by c.custid Hope that helps!
admiralwaffles' solution will work on all platforms. If your flavor of SQL supports ranking window functions, that would be another way of returning your dataset. Basically you would rank each custid's purchase by descending order of time and then return the one with rank = 1.
Wow, I don't feel bad now for not knowing ... this is way more complicated than I expected. Thanks for the help!
Never feel bad for not knowing! To explain what I did a bit (and I did this quickly because people were pouring beer in the other room), I used a subquery to build a result set of the most recent purchases by a customer, and then joined the purchases table to that to get the purchase detail. Subqueries are powerful things, but they can be expensive given the right data set and the wrong query. Anyway, when you ask, make sure to give the platform that you're on. What I wrote was ANSI SQL, but different databases have some specialized functions to make this a bit easier and/or comprehensible.
Since the OP has his answer, may I hijack this thread? I have a similar problem, and I clearly have a mental block about "inner join". The solution is related to ranking window functionality, I suspect. I'm using mySQL. I have multiple groups of numbers and I want to find the median of a subset that is clustered together within each group. So, I sort each group and find the usual median. Using that median, I throw out any numbers outside of median +/- 30%, and get a new "de-outliered median". (I may iterate this a few times.) Now I put a window on the sorted numbers, in each group, near the final median (window == the 8 below-median and the 8 above-median + median = 17 numbers). I want these groups of 17 numbers to do something else with. So, what is the inner join doing above, and will it help me? Is it the same thing as selecting rows "where a.custid = b.custid" (but more powerful)? If so, why use inner join? There is probably some simple concept here I'm just not getting yet.
much appreciated, thanks!
&gt;I used a **derived table** to build a result set 
Just a warning that if there are two or more purchases for a customer with the same timestamp then this will return multiple rows for the customer. That may or may not be something you can live with. If not, then it's better to do this kind of thing with a unique index. If you can guarantee that the most recently inserted record is also the latest purchase then you can use MAX(purchid) instead of time. Assuming purchid is an incrementing identity. If you cannot guarantee that, then you'll have to use ranking functions (if they even exist for your db engine).
If I am reading your question right, it is exactly the same as including the join condition in the where clause. It is just a syntax preference. I find that using an 'on' clause helps when you are joining 3+ tables. It is just easier to read and keep the logic straight.
Is JOIN or WHERE faster? (ie. why are there two methods?) Or do they both create a huge table, and then delete non-selected rows? Anyway, thanks for your help.
Neither should do a full table scan if the tables are indexed properly and there is no performance difference I am aware of. I have only ever read that it is a syntax preference. If you are using MySQL, you could try an EXPLAIN statement on each method and see if there is any execution difference.
I'm thinking this is where joins come in?
I may have misunderstood the question, but it sounds like you're describing a basic join. In which case: SELECT m1.id, m1.memberid, m1.interestid, i1.interest FROM memberinterest m1 INNER JOIN interest i1 ON m1.interestid=i1.id Should return : +- +---------- +----------- +---------+ | id | memberid | interestid| interest| +- +---------- +----------- +---------+ | 1 | 1 | 1 | reading | | 2 | 1 | 2 | movies | 
I feel dirty on doing someone else's homework, so I'm going to downvote this... but I'll give you a hand to not be complete dick. Let me clarify-- you basically just want to show the "memberinterest" table, but instead of "interestid", you want "interest" to show? Fair 'nough: SELECT mb.id, mb.memberid, i.interest FROM memberinterest mb LEFT OUTER JOIN interest i ON mb.interestid = i.interest;
The issue with the INNER JOIN will be that it will rely on the foreign key constraint being set up properly. If it wasn't, then its possible to have one where id=3, memberid=1, and interestid=3 but it will never show in that join because there isn't an entry in the interest table with an id=3. It really depends on the situation, OP-- if you want only ones that give a value, go with this one. If you want one that will show the memberinterest regardless if theres an interest match, go with the one [I suggested](http://www.reddit.com/r/SQL/comments/gkp4a/hit_a_bit_of_a_wall_on_this_assignment/c1o8uup).
Have a derived table with all possible schedtime values as well as a grouping value then left outer join that with your original result set and sum by grouping value.
 Compute how you would do normally then join with a tally table to fill the gaps where a tally table would be 0 - N number of points. you will probably want to use collecse to get rid of the nulls as well
This is difficult mainly because you want 30 minute intervals. 1 hour would be much easier, which I will show you below. I am assuming the schedtime is actually some sort of datetime field. I am also assuming your SQL server has some sort of Hour() and Date() function (which may have different names that you will need to look up). Try: SELECT Date(schedtime), Hour(schedtime), SUM(Value) FROM table_name GROUP BY Date(schedtime), Hour(schedtime) EDIT: forgot the SUM()
If you need 30 minute intervals, you would create a lookup table that contains intervals for a 24-hour period. I.E.: interval_name | start_time | end_time 1 | 00:01 | 00:30 2 | 00:31 | 01:00 You would then join to this table using an on clause like: on MINUTES(schedtime) between start_time and end_time
SQL Ranking or Windowing functions are probably your best bet. Unfortunately, I'm not familiar enough with them to give you any more help.
Thanks for your help guys, I assumed it was where a join would be needed and your replies and my own research have helped me immensely! Upvotes all round.
LEFT and RIGHT OUTER JOIN calls require you to be strict about Null reference handling though.
30 minutes = 1800 seconds. divide schedtime by 1800 to chop it up into 30 min intervals SELECT FLOOR(schedtime/1800)*1800 AS StartTime, SUM(datapoint) AS DataSum FROM mytable GROUP BY FLOOR(schedtime/1800)*1800 If you need a specific start time then just add an adjustment value.
How large are the increments? Are they always $10,000? If so, you could just use a simple formula to do it. I don't have Access in front of me to figure the exact formula, but I would do this: 1. Divide by 10000 1. Lop off the fractional part with INT() 1. Multiply by 10000 1. Use the answer from 3 as an input into a text field that says "$x to $(x+10000)" If the increments aren't always $10,000 then it becomes harder, but you should still be able to use a formula to produce what you want. The formula will just get a lot more complex if there are varying ranges. 
The increments I would want to create would probably be something like 1) $0 2) $1 to $250,000 3) $250,000 + However, the value in the field can be literally any dollar amount. So i want to define each row as falling into one of the segments above.
Try looking into the Case statement. Again, I don't have Access in front of me, but try something like Select Dollar_Amount, Case When Dollar_Amount = 0 then "$0" When (Dollar_Amount &gt; 0 and Dollar_Amount &lt; 250000) then "$0 to $250,000" Else "$250,000+" END as New_Field From Table 
This is exactly the type of stuff I'm looking for, many thanks. I'll try this bit out, and report back on how it works.
Wouldn't it just be easier to run a query such as: UPDATE users SET password = (select password from users where user_id = 1) WHERE user_id = 2 (assuming "1" is the person whose password you know and "2" is the one you dont')?
I have no idea, maybe...I really know nothing about SQL queries. there is a "User_name" field and "Password" field. So I should be able to run something like, UPDATE User_Name SET Password = (select Password from User_Name where User_Name= 1) WHERE User_Name = 2
Yes, that should work. Give it a try and let us know how it goes.
Close, but not exactly. What is the name of the table that has the two fields named user_name and password? That is what is supposed to go after the word "UPDATE" - assuming it is called "users" here. Also, if the user_name field is a text field (varchar) you will need to wrap the user names in single quotes, so assuming the user whose password you know is Jim and the user whose password needs to be changed is Bob: UPDATE users SET password = (SELECT password FROM users WHERE user_name = 'Jim') WHERE user_name = 'Bob' Let me know if you still have trouble.
after trying ~100 passwords, I finally figured it out. I was concerned about leaving out the table name, so thank you for posting that info. Any recommendations for beginners guide to SQL?
I've heard good things about sqlzoo.net
 Suggestion use 2011-01-01 00:00:00 but normally 2011-01-01 is enough
Wouldn't a "select * from table where year(timestamp) &gt;= '2011'" work? Greater than symbol included in case you wanted to run for 2010 to current, for an example. I read using LIKE is bad on performance, so using a "where timestamp like '2011%'" probably isn't a good idea. Sent from my phone, sorry about formatting.
Agreed on the use of year(timestamp). I doubt there's a significant performance difference when using SQL server but I've run afoul of silly localisation issues with dates - e.g. doing partial comparisons with DD/MM/YYYY and then running the same code on a MM/DD/YYYY server. Using year() / month() / day() functions eliminates any ambiguity. One minor point though, if you do use year() (or the day or month functions) the comparison is numeric - so you'll just want to use year(timestamp) &gt;= 2011 (the 2011 isn't a string). Also to be picky, "timestamp" is probably a poor choice for a column name given that TIMESTAMP is both a reserved SQL keyword and a MS SQL datatype (although the latter usage is entirely different in meaning/function).
You typically don't want to do it that way since year(timestamp) probably isn't indexed.
This is correct - SQL Server isn't able to index based on a function, unless this is persisted as a computed column. Seeking on YEAR(timestamp) will result in the YEAR() operation being performed on every row in the table.
While your use of WHERE timestamp &gt; '2011' resolves successfully to 2011-01-01, it is a little ambiguous, and not immediately obvious to the next developer who looks at your code. If you forget to include the single quotes (WHERE timestamp &gt; 2011), you'll be comparing to 1905-07-05. The best format is to use YYYYMMDD. See [this page](http://sqlblogcasts.com/blogs/tonyrogerson/archive/2008/01/05/date-format-inconsistency-yyyy-mm-dd-or-yyyymmdd-when-2007-04-10-is-4th-october.aspx) for an example of where yyyy-mm-dd and yyyymmdd can be ambiguous. Also remember that you should use &gt;= instead of &gt;. Otherwise you'll miss out on datetime values that occurred without a time component (i.e., 2011-01-01 00:00:00.000) The best format is: SELECT * FROM Table WHERE [Timestamp] &gt;= '20110101' 
yeah. i meant &gt;= went ahead and fixed it up there
Since it's Access and not specifically SQL, besides the Case statement, you could also look into using Switch. 
How'd it work out?
poifect! had to fix the syntax up a little bit, but it worked great. thanks so much!
When working with datetime fields, strings are implicitly cast to datetime for the comparison. &gt;SELECT CAST('2011' AS datetime) &gt;2011-01-01 00:00:00.000 Looks legit to me.
&gt;See this page for an example of where yyyy-mm-dd and yyyymmdd can be ambiguous. Aaaaah! Why would anyone ever want or use Y-D-M format??
that looks pretty elegant to me too
Well the point I was trying to make is that you don't even have to do the CAST. Because your "timestamp" field is a datetime, writing this: &gt;select * from table where timestamp &gt;= '2011' Is implicitly going to run this: &gt;select * from table where timestamp &gt;= CAST('2011' as datetime) So I don't see any problem with how you are doing it today.
ah. that works for me :)
You need to pay attention to data types.You're using SQL Server, so a value of 0 in a datetime data type is actually '1900-01-01'. Ergo, you need to either return a varchar or an integer to do this. Varchar way: case when c.ClosedDate = '1900-01-01' then '0' else convert(varchar, c.ClosedDate, 120) end as ClosedDate Integer way: case when c.ClosedDate = '1900-01-01' then 0 else cast(convert(varchar, c.ClosedDate, 112) as int) end as ClosedDate
Outputting c.closedDate just once in a computed column causes SQL Server infer the column type to be a datetime. Outputting the integer 0 into a datetime field will cause the implicit cast: &gt;SELECT CAST(0 AS datetime) &gt;1900-01-01 00:00:00.000 So you're looking to replace the 1900-01-01 with a 0, but SQL Server is implicitly converting that back into the same value you started with. Try returning NULL instead of a 0.
Not bad. I had never heard of CTEs. If I am reading it right, is it kind of like joining to a subquery? There must be a law that ERPs need to have shitty obfuscated field names. :P
Prefixing all lines with 4 spaces will make it all look like code, rather than a mix of indented monospace text and non-indented variable-width text. Prefixing all lines with 4 spaces will make it all look like code, rather than a mix of indented monospace text and non-indented variable-width text.
CTEs are one of my most useful tools - they can really help with optimization and killing your need for temp tables.
CTEs are very much like sub-queries, but have the following useful properties: 1. They're always named. 2. They are defined at the beginning of the query, and *order matters*. Any CTE definition can reference any CTE that has been defined before it. 3. Many recent DBs allow special self-referential syntax to build recursive queries (!!) 4. The database builds all of the CTE result sets in memory, then you can join them together by name as if they were tables or views. They're very useful in long queries like this one, because you can build a bunch of smaller result sets with self-contained queries. That helps a ton with readability as well as debugging, since every CTE produces its own result set that you can inspect. Then once they are all ready, you blow through a final SELECT statement and join the CTEs together as appropriate.
 Prefixing each line with four spaces is not a brilliant way to enable monospaced text. Especially not using a textarea field with my web browser. I've never preferred Reddit's formatting rules.
It's still missing planner hints spread everywhere, senseless merge joins between things-that-should-not-be-joined, more magic numbers, context sensitive fields, one or two "logical tables" modelled as EAV's, I18N ready fields and some apparently random filtering on XML columns to really approach the inner circles of SQL hell. ... yes Oracle applications, I'm looking at you
They're closer to a derived table. 
I almost never do joins of more than a couple of tables. Do some basic joins put the results in temporary tables. Join temp tables back together. Multi table joins will recompile badly in future, as data sets grow queries compile in different ways. So minimizing the number of joins in any individual query, by joining few tables at a time and using temp tables tends to be reliably efficient.
Thanks for the tips guys. I haven't had a chance to try out the suggestions yet, but everything makes sense at this point. I'll probably get back to the project on Monday. Thanks again!
Usually CTE's are used for recursion - when needing to dig down through hierarchy and get all the children or all the parents of an item. I actually find that Oracle's STARTS WITH/CONNECT BY by is easier to use when recursion is needed, but CTE's can be quite helpful when dealing with hierarchies on MSSQL.
SELECT TheField FROM (SELECT TheField FROM TheTable GROUP BY TheField ORDER BY Count(*) DESC) WHERE rownum &lt;= 10 ORDER BY rownum 
Worked like a charm! Thanks!
Hi! Im going to be a bit of a spammer here and link my game I'm working on. http://www.schemaverse.com I have made it with the hopes that it will help to teach a lot of different aspects of databases including the basics (Tables, Views, SELECT, INSERT, UPDATE, etc) and more complex areas like writing functions, security and optimization. Unfortunately at this stage it seems like it might be too complex to actually teach but I haven't had many people try it out yet. It isn't built on mysql so it may not be what you are looking for but it should still give a lot of transferable knowledge regardless of your database of choice. I certainly welcome you to try it out. If you need any help at all while getting to know the game, or sql itself feel free to send me a message or post to the games discussion group (linked on the site).
Try http://www.sqlzoo.net
That is a great resource! Thanks for the link
No problem. Also, use r/sql as a resource, too. And you are always welcome to private message me if you have any questions or are confused about something and I will do my best to explain it to you or walk you through it.
First of all, lets get down to the nitty gritty. There is a HUGE difference between being a DBA and being a SQL programmer. DBA's "manage" databases including normalization, backups, optimization, reliability, etc... A SQL programmer "mines" the data and uses tools such as SSRS to create solutions to business problems. The later is much, much more marketable for a career. I suggest considering a some basic business classes to understand basic accounting. I am currently looking to hire several contract SQL programmers to fulfill some business needs. My two cents.....
First of all, lets get down to the nitty gritty. There is a HUGE difference between being a DBA and being a SQL programmer. DBA's "manage" databases including normalization, backups, optimization, reliability, etc... A SQL programmer "mines" the data and uses tools such as SSRS to create solutions to business problems. The later is much, much more marketable for a career. I suggest considering a some basic business classes to understand basic accounting. I am currently looking to hire several contract SQL programmers to fulfill some business needs. My two cents.....
I'm pretty sure these are basically equivalent, and its one of the things that your database is capable of optimizing. I just tried it both ways on one of my databases and the [EXPLAIN SELECT](http://dev.mysql.com/doc/refman/5.0/en/explain.html) was identical for both variations. If you're concerned about performance here, make sure to go over the [indexing documentation](http://dev.mysql.com/doc/refman/5.0/en/mysql-indexes.html) for your platform, and verify the indexes by running your important queries through EXPLAIN. If you're curious about pure syntax, I find literal JOIN statements more readable and semantic: SELECT ... FROM table_1 AS t1 INNER JOIN table_2 as t2 ON t1.id = t2.t1_id INNER JOIN table_3 as t3 ON t1.id = t3.t1_id 
Most query optimisers will process both statements identically. (I have no evidence for this assertion but I'd be surprised if it made any difference on any decent RDBMS). I normally write queries using your first method - it's easier to keep track of what I'm doing. Also, you should use ANSI join syntax, i.e. : SELECT t1.PKey, t1.Col1, t1.Col2... FROM Table1 t1 INNER JOIN Table2 t2 ON T1.Key=T2.Key INNER JOIN Table3 t3 ON T1.Key=T3.Key Old style joins (i.e. "where t1, t2, t3") are much harder to maintain and much easier to accidentally miss join conditions, producing unintended cartesian joins. 
It's *possible* that, in the first example, it will union results between t1 and t2, and then union that set to t3; the latter may union t1 and t2, and then t2 and t3, and then union those two sets. Is the value-sharing expressed via constraints or some other construct within the database? The optimizer may use that information to distill both queries down to the same execution. Like synt4x said, you're best off running it through the optimizer (such as Oracle's EXPLAIN PLAN generator) to see how the optimizer actually executes the query. Each database has its own quirks.
Sounds like my concerns are moot. Once I get some time to look up Oracle's EXPLAIN PLAN I will run the different versions of the query to see what actually is going on. But that will have to be another day. Thanks all for the input.
In light of the fact that there's (possibly) no important difference here, then I'd go with a more semantic approach. So, for example if PKey is the identity column for Table1, then you should be joining on that key (your first example). What you're essentially doing in the second example is referencing a reference and that could possibly get confusing down the line. 
It's different syntax to do the same work, but I prefer the way you do it. I have enough work going on in those join predicates, so I like to define them separately from my where statements.
That's very cool and well written. It's hard to say if I'll remember to use it when the opportunity arises.
A well written article which explains the concepts well, but I have a couple of comments regarding the use of bitwise operations stored in integer-type columns, which the article doesn't mention. Bitwise status values used to be used a lot in SQL Server 2000 and earlier, (see SELECT name, status FROM master.dbo.sysdatabases), but this practice has fallen by the wayside for a number of reasons, and later versions now prefer a separate column for each value. First, the BIT column type is sufficient for bitwise operations. Each BIT column will take up a single bit, stored in groups of 8 (e.g., 1 bit column will take up 1 byte, 8 bit columns will take up 1 byte, 9 bit columns will take up 2 bytes). This has the storage efficiency advantage of a single bitwise number, and you also get a separate column name for each bit, making querying easier. Additionally, if you perform a bitwise operation in your WHERE clause, you'll be applying this operation to every row as the query is run. For small tables/views (such as sysdatabases) this is fine, but if you used a bitwise column in a table with a million rows, you'd have to do the bitwise operation a million times. Contrast this to a couple of BIT columns, which can be indexed (possibly with a filtered index). (Admittedly, the index will bloat up the database, and preventing bloat is why you were using bits in the first place, but I'd rather spend 100 MB on an index than have to process bitwise operations for 50 million rows each time a query is run). My general rule would be: use bitwise columns if the database engine won't be looking up the values (use the application), or if there will never be more than ~1000 rows processed (e.g., in the SELECT clause). Otherwise, use multiple BIT columns.
Most important to least, long strings at the end. 
In general, order the columns in whichever way helps you best understand the table in question. If I had to describe a method to my own madness it might be this: Primary key/identity always first, then I categorically group columns and order the elements in the group semantically or intuitively. Each group itself would be ordered from most to least informational or meaningful. Foreign keys follow the same rules which means they might not always be near the start. For example: [ID], [Last Name], [First Name], [Address], [City], [FK_State], [Zip], [Phone], [Extension], [Date Created] State (meaning state or province) is a foreign key reference to illustrate that FKs need not always go near the start. This table has 10 columns, ordered by five cognitive groupings: id, name (last, first), address (addr,city,state,zip), phone (number, extension), metadata (date created). I feel this kind of ordering is easier to read and understand for others. Compare this to an alphabetical ordering: [Address], [City], [Date Created], [Extension], [First Name], [FK_State], [ID], [Last Name], [Phone], [Zip] which most people wouldn't actually use, but it illustrates how strange or confusing things can get if you don't order columns in a more intuitive way.
Awesome feedback thank-you! I'll add it to the blog entry
PKs first, foreign keys second, essential information next ("Status", creation date, etc) and the details towards the end. I generally order them by how I think I would display them on a report.
dude just store your increments in a TABLE and learn how to write non-equijoins.
haha telling me to learn something without explaining how or what i'm supposed to learn is one of the less helpful tips you couldve offered.
dude build a table in Access, do the math in excel, and copy and paste the results in. 
That would be useful if this was something that I had to do one time. But this is something that needs to be repeatable daily, and can be run in access with an autoexec macro that can be scheduled for any time of day. anyways, i already solved it, thanks.
You only reference the reviews table in the subquery, but you have rating in your where clause *outside* the subquery, so the error is refering to that one. If there's no rating column in your games table you'll get the error. At first glance it looks like you also need a 'rating' column in your 'games' table. edit: can we see the structure of your tables? (what columns each table has) and what is the rating system? 1 to 10? 1 or 0? why would 'rating' be 1 if a person has reviewed it? Does that mean they only have a choice of either liking it or not liking it (like = 1 not like = 0) 
This is the games table. Field Type Collation Attributes Null Default Extra Action title varchar(45) latin1_swedish_ci No platformsAvailable varchar(45) latin1_swedish_ci Yes NULL genre varchar(15) latin1_swedish_ci Yes NULL players varchar(25) latin1_swedish_ci Yes NULL info varchar(300) latin1_swedish_ci Yes NULL picture varchar(100) latin1_swedish_ci Yes NULL This is the reviews table. Field Type Collation Attributes Null Default Extra Action title varchar(45) latin1_swedish_ci Yes NULL platform varchar(20) latin1_swedish_ci Yes NULL author varchar(15) latin1_swedish_ci Yes NULL text varchar(500) latin1_swedish_ci Yes NULL rating tinyint(1) Yes NULL datePosted date Yes NULL genre varchar(45) latin1_swedish_ci No None My rating system is that users either select whether they liked the game or not when they review it, stored as binary in the reviews table (1/0). Then from that, a percentage is made on how many liked it (this gives the game on platform review, stored in the gameonplat table (can also show that if needed) and then an overall game rating is made from an average of the ratings of a game on diff platforms (calculated and shown on the website, not stored in the db, although I might have to have it stored in db, so that the end where rating &gt;= '60' can be done? not sure) 
IANA SQL expert (more of an experienced noob) and there's probably a MUCH better way of doing this, but here's what I thought of, until something better comes along... SELECT title FROM games WHERE genre = (SELECT genre FROM reviews WHERE author='Jeklah' AND rating = '1' LIMIT 0,1) AND ((select sum(rating) from reviews where title = games.title) * 100) / (select count(*) from reviews where title = games.title) &gt;= 60 LIMIT 0,3 
sorry, but what are these for? ((select sum(rating) from reviews where title = games.title) * 100) / (select count(*) from reviews where title = games.title) just not sure what they do ohhh, do they work out the % of ratings? thanks, but i got the rating system all working, i just need the sql for suggesting games. unless im wrong in thinking thats what it does haha
I might have misunderstood what you are trying to do, but it looked like you wanted to only show titles that got more than 60 percent of their reviews rated 1, that bit of the query does that. (It works out the percentage of reviews of said title that were rated 1) to work out what one thing is as a percentage of another you times it by 100 then divide it by the other thing, so the above sql works out what percentage of reviews of a particular game were rated 1. edit: In response to your edit: You might have your rating system working, but if you want to use a game's rating as part of the suggesting behaviour *without storing the rating percentage* you'll need to include it in the sql. From what I can tell, the sql query will suggest games that this person has reviewed and liked AND are liked by more than 60% of other reviewers.
Alright maybe I described it badly. At the moment, I have a system that goes as follows. People review games, saying whether they like it or not. This is stored as rating (1/0). A rating for gamesOnPlatforms is worked out from this, and stored in the database in gamesOnPlatforms table. A 2nd rating is calculated (average of the same games on diff platforms (eg black ops on pc gets 100, on ps3 it gets 70, overall rating is avg of 100 and 70). This isn't stored, but showed on a webpage, but could be included by either using global vars in php, or storing it in the db, whichever is easiest. What I want to do, is write a query which suggests 3 top games (top being games with an overall rating above 60) within the genre the user likes the most (worked out by looking at what games the user has reviewed and liked, and i guess which genre he has reviewed/liked the most compared to the other genres (i assume the user will be reviewing/liking more than 1 genre)). If that clarifies things a bit more.
SQL Server's INCLUDE clause allows you to add the values of those columns to the leaf level of the index, and not have them bulk up the non-leaf pages of the index. For example, consider if col1 and col2 are integers, and col3 and col4 are CHAR(100), and you have 1 million rows in your table. ~~By using INCLUDE (col3, col4), you save 200 bytes per row in your non-leaf index pages, or 200 million bytes (~190 MB).~~ (Edit: Thanks Markus!) By using INCLUDE (col3, col4), you save 200 bytes per row in your non-leaf index pages. As each row would be about 250 bytes long, you can fit around 30 rows per page, and so you'd save 6 MB. Not quite the 190MB I originally specced, but if you include more than 200 extra bytes of included data, or have more than a million rows, you can see this adding up. This is useful if you never actually use col3 and col4 to look up rows - you only use them as covering columns. Additionally, as your non-leaf pages are smaller, your total number of index pages is smaller, and you have fewer pages to navigate down the B-Tree when doing searches against the index. The drawback to this is the reverse of what I just stated - you won't be able to use that value to look up. For example, consider the case where you have a query with a WHERE clause of "WHERE col1 = 'a' and col2 = 'b', and that you have 1 million rows where col1 = 'a'. If you have an index on (col1, col2), then this is easy to look up - go straight to the rows where col1 = 'a' and col2 = 'b'. If you have an index on (col1) INCLUDE (col2), then you can easily get to the rows where col1 = 'a', but you'll then have to scan each of those rows to find the col2 value, as it's hidden in the INCLUDE part, and is not part of the index key. You'll also get statistical benefits from having extra columns in the index key, but by this stage (more than 3 columns in), this wouldn't be terribly helpful for the optimizer. My general rule is that if the extra columns are small, it's better to include them in the index key - you may soon have a different query that could benefit from having the extra columns in the index key. If the columns are large, it's best to INCLUDE them. tldr; Not using INCLUDE wastes space for covering indexes, and increases the number of pages in the index.
Thanks for the reply. But suppose that in the end some analysis shows that index(col1, col2) include (col3) is the right way to go. Now what if I am not using SQL Server and I am using Oracle? Is there an alternative to index(col1, col2, col3) that has the space saving benefits of the include clause but also indexes on (col1, col2)?
I'm certainly not the best option for suggesting performance optimisations for Oracle, but the main benefit of the covering index in SQL Server is to avoid costly bookmark lookups, which can add an additional 2-4 page reads to each row. These are common in SQL Server, due to clustered indexes being the standard, but as Oracle doesn't use clustered indexes as often, the need for INCLUDED columns may be lessened. For example, if a non-clustered index in SQL Server is used, the cost of the lookup can be an additional page read, which may be more acceptable. So, maybe Oracle simply doesn't need INCLUDEd columns, and just adding all columns in the key is fine. I'd love to hear the opinion of some Oracle guys on this too!
Thanks for this post. I think you cleared up a lot of confusion I've had on whether to include columns with the index, or simply index on them.
It's a good question, but a difficult one to answer, due to the continuum of SQL skills. As usual, I'll speak for the SQL Server world, but I think the general picture is relevant for both Oracle and SQL Server. [Here is an interesting point of view](http://blogs.lessthandot.com/index.php/DataMgmt/DBProgramming/MSSQLServer/what-does-a-sql-server) on the T-SQL required for each level of your career. The problem is that each job is different. In the Oracle world, it's a bit easier to find SQL-only positions, but they're not as common in the SQL Server world - you're usually expected to be either a DBA, or a developer that also works in the database. I've worked in places where "the SQL guy" consisted of just granting permissions to the databases. I've worked in places where it involved tuning OLTP queries on multi-terabyte databases that handle hundreds of thousands of transactions per second. You might end up writing queries for reports for weeks on end. I'm currently doing work on analysing DTS packages (SQL 2000) to get them upgraded to SQL 2008. So, it's a hard question to answer (which explains the upvotes but no comments). To get started, I would define yourself clearly. Are you a SQL developer, or are you a DBA? How much cross-over do you have between the two? Which direction would you like to move into? There are subclasses here too - Dev DBA vs Prod DBA vs SQL Report Writer vs Database Designer/Architect... Next, I would recommend highlighting your tuning experience. Be able to explain the techniques (code changes, or adding indexes, etc), and explain the metrics of the previous query, and how much quicker it's running now. For example, you might explain that query X was running 60 times per minute, 20 seconds per execution, and performing 2 GB of data page reads, which won't all fit in memory, leading to disk thrashing. By adding a covering index to match the query filter, you brought the query down to 2 seconds, and 10 MB of data page reads (and no disk thrashing). tl;dr Database professionals don't skimp on details. 
They're identical, and this is not an issue of m2m or m2o, which are conceptual structural issues.
It's actually quite easy. select @rownum := @rownum+1, foo, ... where ... and @rownum in (1, 50, 100, 150, 200, 420);
Ranking means sorting a list on values, and handling collisions in a well defined form. This is actually just about row numbering. select @rownum := @rownum+1, foo, ... where ... and @rownum in (1, 50, 100, 150, 200, 420);
Ranking is a terribly incorrect approach. I apologize I didn't see this until a month too late.
No, he doesn't. He just wants to select where rownumber in.
This is a terrible approach - it will get him the window he doesn't want, and it will force MySQL to fragmentarily handle the query with cursors, destroying most of its optimization strategy. select @rownum := @rownum+1, foo, ... where ... and @rownum in (1, 50, 100, 150, 200, 420); 
Statspacks or AWR reports ( if you have the diagnostic module) , also you could create a trigger to put a trace on any connections from that particular application. 
Here ya go. http://www.simple-talk.com/sql/t-sql-programming/creating-cross-tab-queries-and-pivot-tables-in-sql/
Still having problems? if the data that will become the columns is in fact dynamic, you will need to either build dynamic SQL, or use the cross tab, my method of attack would be through the use of a temp table and use dynamic SQL
I don't think this counts as a single tsql statement, but assuming you don't have a particularly large number of features (say less than a couple of hundred) you don't necessarily need a loop, cursor or temp table (since you mentioned trying PIVOT I'm assuming MSSQL 2005/8). As supern0va said, though, you do need dynamic SQL if the features can change. Here's (admittedly I'm guessing here) a simplified version of your schema with some sample data, let me know if I'm too far off... create table apps ( [app_id] int not null primary key , [app_name] varchar(100) not null ) create table features ( [feature_id] int not null primary key , [feature_name] varchar(100) not null ) create table app_features ( [app_id] int not null , [feature_id] int not null ) alter table app_features add constraint pk_app_features primary key ( [app_id], [feature_id] ) insert apps select 1, 'app 1' insert apps select 2, 'app 2' insert apps select 3, 'app 3' insert apps select 4, 'app 4' insert features select 1, 'feature 1' insert features select 2, 'feature 2' insert features select 3, 'feature 3' insert features select 4, 'feature 4' insert app_features ([app_id], [feature_id]) select 1, 1 insert app_features ([app_id], [feature_id]) select 2, 1 insert app_features ([app_id], [feature_id]) select 2, 2 insert app_features ([app_id], [feature_id]) select 3, 1 insert app_features ([app_id], [feature_id]) select 3, 3 insert app_features ([app_id], [feature_id]) select 4, 1 insert app_features ([app_id], [feature_id]) select 4, 2 insert app_features ([app_id], [feature_id]) select 4, 3 insert app_features ([app_id], [feature_id]) select 4, 4 I'm using PIVOT with dynamic SQL, but I have to use two differnt variables to display the actual feature names instead of the id's for column names. declare @feature_ids varchar(8000) = '' , @feature_names varchar(8000) = '' , @sql varchar(8000) select @feature_ids = @feature_ids + quotename([feature_id]) + ', ' , @feature_names = @feature_names + ' case when ' + quotename([feature_id]) + ' &gt; 0 then ''yes'' else ''no'' end as ' + quotename([feature_name], '"') + ', ' from features if @@rowcount = 0--no features, this query will fail raiserror('No features', 16, 1) else select @feature_ids = left(@feature_ids, len(@feature_ids) - 1), @feature_names = left(@feature_names, len(@feature_names) - 1) select @sql = 'select [app_name] as "app name", ' + @feature_names + ' from ( select [app_name], features.feature_id from apps inner join app_features on apps.app_id = app_features.app_id inner join features on features.feature_id = app_features.feature_id ) as sourcetable pivot ( count(feature_id) for feature_id in (' + @feature_ids + ') ) as pivottable' if len(@sql) = 8000 --if length is 8000 string is truncated raiserror ('Too many features', 16, 1) else exec (@sql) Obviously the error handling is pretty weak (as is the formatting) but it should produce the following output, handle any number of apps, and a reasonable number of features. The case statement to do yes/no instead of 1/0 is probably better handled in the UI, though. app name feature 1 feature 2 feature 3 feature 4 app 1 yes no no no app 2 yes yes no no app 3 yes no yes no app 4 yes yes yes yes Hope it helps. 
hi! I'd also like to say thank you for that explanation--although there is a little glitch in your math. The space-saving is not 190MB because the non-leaf nodes don't include all records. E.g., the first level above the leaf nodes includes only one entry per leaf node. If, for example, each leaf node holds 30 rows (~250 bytes each for a 8k page), the first level leaf nodes have 1/30th of the rows. You space estimation is off by that factor. However, the higher branch nodes benefit from the saving so that many more rows fit into a single node and the B-Tree can, potentially, save a full level. I'm not exactly sure about the benefit of INCLUDING for updates. As per my understanding, updating INCLUDING columns only will never change anything else as the affected leaf node. Is that true, can you confirm that? Regarding Oracle: As you suggested, the Oracle way to covering indexes/index-only-scans is to include the columns in the index--without any special syntax. Just add them after they key columns. In Oracle, I suggest Index Orgainzied Tables (IOT, the "clustered index" equivalent) only if there is no other index needed on that table. IOT's have an INCLUDING clause--although that works quite different. It is, however, possible to have an Oracle IOT which doesn't include all columns into the b-tree. The other columns are stored somewhere completely separate (in a different segment), not in the leaf nodes. Accessing them triggers additional IO.
Hi Markus, Thanks for that pickup - you're absolutely right, as not every row will be represented in the leaf pages. &gt; I'm not exactly sure about the benefit of INCLUDING for updates. As per my understanding, updating INCLUDING columns only will never change anything else as the affected leaf node. Is that true, can you confirm that? I can think of a couple of exceptions. If you are updating the included column, then there is usually no need to touch the non-leaf nodes, as the order of the rows is unaffected, and the included columns aren't represented on the page. The ONLY time when an update to an included column would affect the non-leaf pages is when there is insufficient space on the leaf page, causing a page split. This would result in half the rows moving off onto a new page, causing changes to cascade up the index tree. You can also use a covering index for updates. Imagine your table was (ID int, Col1 varchar(20), Col2 varchar(500), &lt;more columns&gt;), and a non-clustered index on (ID, Col1) INCLUDE (Col2). Consider the following query: UPDATE Table SET Col2 = 'Something else' WHERE ID = @id AND Col1 = @col1 AND Col2 = @col3 This can then use the included index to determine which rows to update, remembering that the first phase of an UPDATE or DELETE is essentially a SELECT. Wow, every time I ask about Oracle, I'm struck by how similar, and how completely alien it is. How do non-clustered indexes (NCIX) work on an IOT? In SQL, the NCIX will include the clustering key to enable a lookup back into the clustered index. An IOT with only a subset of columns just sounds like a non-clustered index. Or, it could be similar to a clustered index which has columns stored off-row, such as row-overflow or large-objects. Thanks for the discussion!
[you are looking for the Transpose command](http://www.brighthub.com/internet/web-development/articles/91895.aspx)
Similar to ---sniff---'s recommendation, you are looking for a PIVOT operation. They are rather complex and also you will need to know the complete list of product names in advance. If you wanted to do it in a completely inefficient, brute force way, you could also write a stored procedure that would take the result set productName | buyPrice | sellPrice - cursor through the rows once creating a temporary table - each iteration of the cursor adding a new column to the temp table. Then once you have a temp table of the format: priceType | product1 | product2 | product3 | .... | productX You could then insert 2 records into that table, buyPrice and SellPrice with all NULLS for the product columns, then cursor through the original data set again, and use some dynamic SQL each iteration time executing something like this: EXEC('UPDATE tmp_table SET ' + @productName + ' = @buyPrice + 'WHERE priceType = 'buyPrice'') EXEC('UPDATE tmp_table SET ' + @productName + ' = @sellPrice + 'WHERE priceType = 'sellPrice'') But boy is that ugly and inefficient. OK as a one time kind of thing, but not something I'd want running frequently in a production environment - especially on a dataset of any significant size. EDIT: You could probably also use some dynamic SQL to join the original result set to itself once per product and returning the buyPrice and SellPrice of each instance of the table in the join. 
Here are some explanations on how to do this for Oracle: http://forums.oracle.com/forums/thread.jspa?threadID=2174552#9360005
[You might check this out.](http://www.bigresource.com/Tracker/Track-ms_sql-TVn1W91U/)
Here's a way to use a function to do it in the query: CREATE FUNCTION sfunc_GetIDs (@Parameter varchar(8)) RETURNS varchar(255) AS BEGIN DECLARE @IDs varchar(255) SET @IDs = '' SELECT @IDs = @IDs + CAST(id AS varchar) + ', ' FROM Params WHERE Parameter = @Parameter RETURN LEFT(@IDs,LEN(@IDs)-1) END GO SELECT DISTINCT Parameter ,dbo.sfunc_GetIDs(Parameter) FROM Params 
If you're using SSRS why don't you take a look at the Matrix/Tablix control. 
thanks guys, i'll look into these and see if i can't get this report to format properly :)
 SELECT application.application_name as [Application], CASE WHEN SUM( CASE WHEN feature.feature_name = 'Spell Check' THEN 1 ELSE 0 END ) = 1 THEN 'Yes' ELSE 'No' END as [Spell Check] FROM application LEFT JOIN feature ON application.application_id = feature.application_id GROUP BY application.application_name Repeat the "Spell Check" column for every feature that you want to report on. It's tedious, but it works. Use a script to generate the SQL for you.
With MS SQL 2005/2008 perhaps the [pivot](http://msdn.microsoft.com/en-us/library/ms177410.aspx) command might serve?
Something like this is what you want. Typed off the top of my head; untested. $inner = implode(' as c union all select ', $case); delete from info outer join (select {$inner} as c) as inner on inner.col = info.neweggid where info.neweggid is null; 
Why are you running a sub-query here? $SQL = "DELETE FROM `info` WHERE `neweggID` NOT IN ($deleteID)"; That should work just fine. By the way, you need to escape those $item values.. 
MySQL doesn't permit SELECT in a subquery if it is the table you are trying to delete from &gt;This would work flawlessly, aside from the fact that I found out in MySQL you can't use the same table in the subquery as you do in the main delete query. If only NOT IN didn't have to be a subquery, this would be simple.
using(Key) :P
 $cases = array('N82E16811119225', 'N82E16811119210'); $placeholders = implode(',', array_fill(0, count($cases), '?')); $sql = "DELETE FROM `info` WHERE `neweggID` NOT IN ($placeholders)"; $stmt = $pdo-&gt;prepare($sql); $affected_rows = $stmt-&gt;execute($cases); Works as long as the number of elements in the array isn't insanely large. If it is then you should be storing them in a table anyway. If you don't use prepared statements (you should) then just replace $placeholders with a quoted, escaped, comma-delimited list of ids.
Right, but ninjaroach is showing you that you don't need a self-referencing subquery. There is no select.
No, that I understand. When I was trying to do this yesterday maybe I royally messed up some part of my code, but MySQL didn't like my NOT IN statement using ('___', '___') as the parameters. I read somewhere on the MySQL site that MySQL didn't support NOT IN from anything but a subquery, hence why I went that direction. Re-worked it this morning with just ('___', '___') and it's working fine now, don't know what was up. Thanks!
I think that you should upsize from Jet / Linked Tables to TRUE client server architecture called 'Access Data Projects'. they are 10 times more efficient than Jet. Call me if you need help with it 253 882 9024
 SELECT m.mid, m.far_sid, f.name, m.near_sid, n.name, m.stamp FROM monitorings m LEFT JOIN switches f ON m.far_sid = f.sid LEFT JOIN switches n on m.near_sid = n.sid The trick here is that you need to do two separate joins against switches, one for the far_sid and another for the near_sid.
What about SELECT m.mid ,m.far_sid ,sfar.name ,m.near_sid ,snear.name FROM monitorings m left outer join switches sfar on sfar.sid = m.far_sid left outer join switches snear on snear.sid = m.near_sid EDIT: I did outer just in case name is not defined in switches you will still get some results back EDIT2: This is for TSQL, not great with MySQL syntax.
TL, DR: select InsJ.BName, InsJ.FiTitle, Fo.Title from ( select B.id as Bid, B.Name as BName, Fi.Title as FiTitle from Buddy as B left join FavoriteFilm as Fi on B.id = Fi.buddyId ) as InsJ left join FavoriteFood as Fo on InsJ.Bid = Fo.buddyId order by InsJ.BName; ---- I'm not sure if I'm solving the right question, so I wrote an example with some obvious data to make it clear what I'm doing. create table Buddy(id integer auto_increment primary key, name varchar(100)); create table FavoriteFilm(buddyId integer, Title varchar(100), foreign key(buddyId) references Buddy(id)); create table FavoriteFood(buddyId integer, Title varchar(100), foreign key(buddyId) references Buddy(id)); insert into Buddy(name) values('Joe'); set @Joe = last_insert_id(); insert into Buddy(name) values('Steve'); set @Steve = last_insert_id(); insert into FavoriteFilm(buddyId,Title) values(@Joe, 'Apocalypse Now'), (@Steve, 'Brother'); insert into FavoriteFood(buddyId,Title) values(@Joe, 'Pizza'), (@Steve, 'Coffee'); -- ---------------------- -- This here is the query -- ---------------------- select InsJ.BName, InsJ.FiTitle, Fo.Title from ( select B.id as Bid, B.Name as BName, Fi.Title as FiTitle from Buddy as B left join FavoriteFilm as Fi on B.id = Fi.buddyId ) as InsJ left join FavoriteFood as Fo on InsJ.Bid = Fo.buddyId order by InsJ.BName; which yields mysql&gt; select InsJ.BName, InsJ.FiTitle, Fo.Title from (select B.id as Bid, B.Name as BName, Fi.Title as FiTitle from Buddy as B left join FavoriteFilm as Fi on B.id = Fi.buddyId) as InsJ left join FavoriteFood as Fo on InsJ.Bid = Fo.buddyId order by InsJ.BName; +-------+----------------+--------+ | BName | FiTitle | Title | +-------+----------------+--------+ | Joe | Apocalypse Now | Pizza | | Steve | Brother | Coffee | +-------+----------------+--------+ 2 rows in set (0.02 sec) Make sure to test the answers you're getting. The query I gave gives very different results from the other two queries that other people gave. I'm not honestly sure what you want, so I'm not saying I'm right or they're right. But we are all three importantly different, and each arguably correct, and you need to sort out which one of us is doing the thing you actually want.
Thank you. As I said, this is merely a curiosity (it has no real-word application) but I will pass along the replies to him and update here. will also post results from all three replies here. Thanks again!
Thank you, I will check this out and update with results.
Sure.
Thank you, ninjaroach, this actually worked out perfectly! Never thought of doing it with two left joins! Don't think I will ever get good at MySQL (or any kind of SQL).
Keep in mind with these joins you may run the risk of not returning any data if the switch is not specifically defined in table "switches". For example if you have a sid not set up in "switches" and a record gets inserted into "monitorings" with that undefined sid, you will not get a record back at all. So, be rigid about always putting definitions in "switches", or do outer joins.
Um, that query uses outer joins...
Oh geez, you're right. I don't know why I thought that was default inner. Carry on!
I know I'm late, but at least I have a quick-and-dirty solution, namely Excel! If you're just looking to present the data (and not have a permanent solution), you can easily do it with Excel (literally in 5 seconds): PIVOT is great, but it's complicated - especially with a great number of columns (I love Ben-Gan's T-SQL Fundamentals, but even he sticks to known columns). So, here's how to do it in Excel in 5 seconds: First, paste your data set (productname | buyPrice | sellPrice) in Excel. You'll have 3 columns, and x rows (x dependent on the number of products you have). Next, select all data (including your headers if you have them) and Copy. Then, wherever you want the new dataset to appear, right-click in the first cell, then select Paste Special. In the bottom left of the dialog, there's a Transpose option. Select it, and hit OK - tadaaaam! http://i.imgur.com/ZdLnM.jpg 
Also relevant: http://msdn.microsoft.com/en-us/library/ms143432.aspx &gt;The maximum number of bytes in any index key cannot exceed 900 in SQL Server. You can define a key using variable-length columns whose maximum sizes add up to more than 900, provided no row is ever inserted with more than 900 bytes of data in those columns. In SQL Server, you can include nonkey columns in a nonclustered index to avoid the maximum index key size of 900 bytes. So, that's why include columns exist in Microsoft's implementation of nonclustered indexes.
Thats a crosstab report, you can achieve that by using the t-sql pivot statement Select your results into a temporary table Pivot a select * from that table, this is the same as the transpose function in excel, i.e. row values become column header SELECT * FROM #temptable PIVOT ( MAX(ID) FOR [Parameter] IN ([1],[2],[3],[4]) ) AS p You should get a row a for Each Parameter PIVOT ( MAX(ID) --Return the maximum value in the id column for each FOR [Parameter] IN ([1],[2],[3],[4]) 1,2,3,4 now become column headers the table looks like parameter,1,2,3,4 the max(id) is going to be the row value where the value of max(id)matches the same value in the column header Paameter,1,2,3,4 a,1,2,null,null b,null,null,3,4 ) AS p for an unkown set of IDs you can generate a dynamic list of column headers as outlined here http://www.tsqltutorials.com/pivot.php declare a3 
The log_reuse_wait_desc isn't a setting - it's a status of the database (from sys.databases) showing the reason why the active portion of the transaction log for that database cannot be reused, and can change quite rapidly. Books Online lists the possible reasons as: * 0 = Nothing * 1 = Checkpoint * 2 = Log backup * 3 = Active backup or restore * 4 = Active transaction * 5 = Database mirroring * 6 = Replication * 7 = Database snapshot creation * 8 = Log Scan * 9 = Other (transient) The most common statuses you'll see here are "Nothing", "Checkpoint", "Log Backup" or "Active transaction". Checkpoint will be seen in databases in SIMPLE recovery, as the checkpoint task flushes dirty pages to disk, and so the transaction log records are no longer necessary. "Log Backup" is seen in the FULL recovery model, when there's data in the log, and the only thing stopping it being reused is a log backup. You cannot clear a log past the start of the oldest active transaction, so if you see "active transaction", there is still a transaction running, and no amount of backing up will allow you to clear the log until that transaction is committed or rolled back. If you see "Nothing", this indicates that there is no reason the log cannot be reused. This is likely to occur when the active log has been backed up, and the current Virtual Log File (VLF) is the only one that is active - there's nothing preventing a log being reused, because there's nothing that can be overwritten at this time. In your case, if you are in the Full recovery model, and you see the log_reuse_wait_desc as "Nothing", then there is no reason the log cannot be cleared - most likely your database hasn't had enough activity since the last transaction log backup to have more than a single VLF currently active. You would see nothing if you just forcibly truncated your log (bad!) and then looked at the setting. What sort of alerts are you getting? Out of log space in the ERRORLOG? If the log is getting full, then the most likely reason is that your transaction log backups are not occurring frequently enough. If you're in the FULL recovery, you MUST take transaction log backups, or you'll run out of room, and the log_reuse_wait_desc will stay reporting "Log Backup". Edit: Rewrote, once I got to a computer (as opposed to the phone).
Many many thanks, i was half way there. My only problem with the forums i found online, were the age old problems with forums. "What is a log_reuse_wait in SQL its not working..." "Use Oracle" *slaps forehead*. That doesn't answer my question. Your response however was amazing and i thank you very much :) 
I don't believe this is doable with triggers. You'll most likely need to dump your data to a temp table, then sanitize it with a stored proc and then bulk import into it's final table.
Default is inner, but LEFT is not the default (it's short for LEFT OUTER).
GO as a batch delimiter doesn't make sense inside of a stored procedure and as I recall you can't use them inside of SP blocks. Your best bet would be to put them all in their own SPs and then run them consecutively in a TSQL script. As for deadlocking: locking hints on your statements can help curb some of that. But analysis of why it's deadlocking and refactoring your design a bit is the best way to solve that problem. Usually what's happening is you've got two statements that lock the same tables but in opposite order. If you can find them and reorder them then that will typically solve the deadlock.
GO itself won't have any effect on the locking. As long as they're separate statements, any locks held by individual statements will release when the statement in question completes (assuming you haven't opened an explicit transaction and aren't using an elevated isolation level). Consider using NOLOCK/ReadUncommitted hints for joins if dirty reads are acceptable. Traceflag 1222 is very helpful with pinpointing exactly which statements are involved in the deadlock. 
Laziness and firepool are spot on, but I like to ramble on about SQL Server (I'm also on holiday, so I have nothing better to do), so please bear with me! As Laziness mentioned, GO is a batch delimiter, and is not actually a command in the T-SQL language. It is simply used in SQL Server Management Studio as a command to separate batches of commands - each batch is sent through from SSMS to SQL Server to be run individually. Consequently, the batch model does not make sense inside stored procedures - you're not sending separate batches. (It is possible to change the batch delimiter - it's amusing to change it from GO to something like SELECT on your colleague's machine...) firepool's idea of traceflag 1222 is excellent advice for tracking down deadlocks. Another alternative which will show the same information is to run Profiler (or better yet, a server-side trace) and capture the following events: * Locks, Deadlock Graph * Locks, Lock: Deadlock * Locks, Lock: Deadlock Chain Once you have more information about what resources are involved in the deads, you'll be in a position to fix the problem. There's always at least two processes involved in a deadlock, and the solution may be to make sure only one is running at a time overnight (e.g., if both are scheduled tasks, you could separate their run times). A common cause of deadlocks are processes that hold locks too long, or lock too much of an object - for example, if you don't have an appropriate index on a table, then you'll need to scan that table. Scanning can take a while, and you're then locking the entire table, when you may only have needed a single row. To answer your question - "can you use dynamic SQL to force separate batches?" You can, but this would be ugly, and not actually necessary. Each statement in a stored procedure is by default a separate entity, unless you've explicitly started a transaction, so there is not much point in executing each step as dynamic SQL as locks won't be holding between statements in your stored proc. The reason that breaking it up into smaller steps is working is probably due to the massively-complicated single statement that ends up not being compiled consistently, and due to a lack of appropriate indexes that make it difficult for SQL Server to choose an appropriate access path to the data, resulting in table scans. Again, use trace information to determine the two processes involved, the exact SQL statement that's involved, and the exact resource. Once you know this, you'll be able to focus on the problem. 
I occasionally find that a particularly complex query will run much more slowly than I think it should. After all, I'm only joining 5 or 10 views, a couple of large table-valued functions, to a couple of our largest standard tables in the database. What could go wrong? =) Most of the time, though, the thing that slows it down is relatively innocuous. Two queries that run quickly on their own go dead slow when you put them together, especially if some of the underlying calculations or functions are particularly complex. I generally take these and optimize them by breaking them down into multiple steps, creating temp tables out of the logical pieces and then joining the temp tables together. The side benefit of this is that although I like one big query that does everything as much as the next guy, it's actually a bit easier to read and maintain when it's split up. Just today I had one of these scenarios (I'd say they happen once every 3-6 months). Two large but not insane tables joined together which run very quickly. Two other fields would need to be calculated by summing up some other values in a tempt table and left outer joined to my main dataset. Unfortunately, this not only went very slowly but pulled the entire database to a standstill for the rest of the company for 55 seconds. =) The solution was simply to insert the main dataset into a temp table (adding the two columns i needed as null values), and then doing two separate update statements, one for each subquery/value. All that together went in 5 seconds. Admittedly some of the time your solution may simply be a missing index (and you can look at the execution plans to see if it suggests any new indexes), but when you're talking about views that starts to get very finicky. By the way, you said you broke them into smaller batches, I'm assuming you are saying you're running the same query multiple times, just for small sets of data that you're merging together. If so, I'd say that's almost definitely the wrong way to go. Try the temp table route. If you have any questions about my suggestions, ask away.
 close mysql -u&lt;username&gt; -ppassword datebasename &lt; file.sql
thanks 
&gt;Before importing, I have to disable the trigger because the procedure wants to run after every row is inserted From what I understand, that's just the way Oracle does triggers. There are ups and downs to it, sounds like you're hitting one of the downsides. Is there any way you can pre-sanitize the import data so the trigger quits messing you up?
I could manually... but that would defeat the purpose, unfortunately. No biggie. Work just put me on a project now, so this thing is out of my hair. If anyone finds out, though, feel free to post it for others. Thanks!
Have your manager get with the IT manager. There's no reason for them not to create a table for this.
Historical data belongs in a database, no some local CSV that you maintain. This should be a schema design issue that is handled by your DBAs; reporting should be handled by any interface you agree on i.e. Crystal, etc.
Can you get away with having Access on your machine? If your needs don't extend to large databases, Access might at least be a good place to start, and it might be a good way to get around the "no SQL servers" rule. It's also a good intro to relational databases, which you may or may not need - I'm not sure. You mention the prospect of adding a table, but you haven't said anything about the database you would be creating the table in, so I'm kind of thinking that there might be more of a need to gauge where you are in terms of understanding databases before we try to figure out what your actual needs are.
The reason it's a breach of protocol is because it writes data about specific data that is under contract and under regulations (sorry for the vagueness of this). Access however is a go, due to it's just temporary and all data is automatically stored in other tables. Access however is not my favorite solution to this issue. Due to the query limits of Access SQL and the size limits of results, Access is very limiting for what I need. Thank you though for your suggestion, I really appreciate it. I can not upvote you enough. The reason why I'm creating this table is so I can call up this table, which has historical information, to relate to data that took place on the history date. For example, on Tuesday 2010 the sky was grey,the tempature was 90 degrees, and the month was September when the specific replication of data took place. Currently, there is no historical information about the day, year, sky, and tempature. There is only one table that isn't backed up every day. I'm currently making a back up of it and need it to be a table that I can query in some sort of thing LIKE Access. I'm supposing Crystals might be a bit better, but anything that isn't in SQL Server Management Studio I have found to be subpar. Which is why I installed a server to add such .csvs as tables to connect my server to the adhoc server. I suppose I should look for better adhoc reporting software that isn't as limiting, slow, manual or clunky as Access or Crystals. Don't get me wrong, Access is great for simple solutions for light data mining - however I wish I was mining very little data.
Totally agree with you friend. However, I wish the DBA and VP agreed with us.
I agree. However, some opinions and thoughts are not budging and a solution needs to be in place for my sanity. 
Ok so you want the advantage of SQL but aren't allowed to use a local instance of SQL Server? If you are allowed direct read access to the server may I recommend [Database .Net](http://fishcodelib.com/Database.htm) It's basically SSMS but you can connect to many more datasources with ease (Such as Excel or Access) whilst still using SQL. It also requires no instance, it's an exe with no install. So you could create an ETL query to pull the data from your production databases into a local Access database, or straight to an Excel report, then just run that daily through Database .NET?
I think I know the reason why staff movement is quite fluid in your company... I don't think this is a technical problem - it's a data management issue, and as such, falls to the DBA/IT department to make an architecture call, and it sounds like they're dropping the ball heavily. If the issue is that you're not allowed to make copies of the data due to some auditing requirement, then a CSV extract file is going to immediately break this requirement. Although you're using Access/temporary tables as a temporary store, the CSV extract needs to be forever. Of course, if you have that kind of auditing requirement, there's probably also a requirement that an audit trail is kept. Your boss has an interest in getting accurate historical reports, and the IT department are the data stewards. Given your IT requirements, it should be up to IT to either develop a working solution, or to say that it cannot be done, due to X, Y, Z (and these had better be good reasons!). If your boss is the owner of the data, and IT doesn't want to offer a workable solution, then higher mediation may be required. It sounds like the IT department/DBA wants to be obtuse for whatever reason (which is highly out of character for DBAs!) CSV extracts sound fine, as long as they're being extracted to a data warehouse, but this greatly complicates the design, when a simple trigger on insert/update/delete operations would be so easy. The historical data can be supported/protected a lot easier if it's in a similar form to the source data - and that means a database. 
I think you solidified what I've been meaning to explain for awhile now. Thank you for your input. I really appreciate it.
Wow, this is great! That's the ticket. It looks like that is what I'll be using. Thank you very much. 
Running Windows? if so, You can create an odbc connection that points to a folder of CSV or XLS files, the folder is your "database" and each file is a "table". You can use QueryExpress.exe (no install required) and you can use an OLEdb connect string to connect to your folder/database. http://www.connectionstrings.com/ http://www.connectionstrings.com/textfile http://www.connectionstrings.com/excel Might sound complicated, but I assure you it is not. 
Your management cannot hold you responsible for what they will not allow you to control.
Check http server error log in your XAMPP installation, usually at x:\xampp_install_path\apache\logs
What's you're overall feeling about lynda? Some friends started working they're recently &amp; I had no idea what they did or that they were useful.
I've used lynda for things years ago to give me a basic knowledge to build off of. For things such as dreamweaver, javascript and c++. I was assuming the SQL would be just as useful. We'll see if I can get rid of this error.
I submitted a new query to refresh my error log to see where it started. Here is the entire log: Http://rapidshare.com/files/740225109/error.log
Nothing much there. However, you seem to be running a quite old version - those SQL tutorials should work without problems on a newer [XAMPP](http://www.apachefriends.org/en/xampp-windows.html), with a functional phpMyAdmin installation (user root, no password). 
I installed windows 7 instead of windows xp, and a newer version of XAMPP and getting an error when loading XAMPP control panel now: "XAMPP Component Status Check Failure [3]/ Current Directory: c:\xampp Install (er) Directory: c:\xampp Run this program only from your XAMPP root directory." Also receiving : "$cfg['Servers'][$i]['tracking'] ... not OK [ Documentation ] Tracking: Disabled" When pHpmyAdmin is run. Would it be any simpler using Microsoft SQL server 2005 or 2008? EDIT: 00:32AM Reinstalled again and receiving Apache HTTP Server has stopped responding. This is after using both root and web users.
I am working in my 3rd position as a sql dev or dba (MS SQL Server) in a career that has spanned only 9 years so far. I have no college degree, and just recently got my first cert due to it being a requirement by the company I am working for, as they like to show off certification rates to their clients. I am currently staffed with Microsoft (v- for anyone that is relevant to), and have been working on the same project for over a year. The Seattle area is a GREAT place for people who want to do something MS SQL related. However, whenever I have started a job search, I have found that it is more important to talk through the process you use to architect, develop, test, tune, etc, than to get the textbook definition of everything right. When I had my interview at MS, I reversed the definition of inner and outer joins (yeah, i know!), but when they asked me to design a normalized database that would hold geography info from countries to states, I talked through how since each element was different (Country, region, state, county, city, neighborhood) they should have their own tables, with identity keys joining each, etc. I'm simplifying, but they loved how I explained WHY I designed it, and I was brought onto a project that a bunch of great people from my firm had already been turned down for. As far as the day to day stuff, it's not always learning new things every day. It is a lot of "I've written this query before, it should be in my notes somewhere", and a lot of unit testing and working with the business guys (analysis, sales, management, whomever) to provide the CORRECT results. Typos and copy/paste bugs will kill you! Lastly, I would like to stress that it's important to have some humility in the database arena. A lot of other developers do not see database guys as "Real" developers, and when you give them results they don't always appreciate all of the hard work that goes into giving them the data they want. So be humble, and work for your own approval and appreciation, because honestly, when you make a breakthrough on a task, it can be fun and rewarding!! Good luck!
I can't upvote this enough. Learning how to do join clauses instead of using the where clause to join took my skill level to a whole new level (honestly for me it was from 'passable' to 'competent', but that was like 7 years ago), and shows that you know how to do it right if you are working as a database professional. As far as to join t3 to t2 or t1, I always try to join foreign keys (im guessing t2 and t3 are FKs) to the table with the Primary key (im guessing t1), I work in MS SQL, and I think the query optimizer would handle each situation in the same way if you have proper indexing, but it's easier for troubleshooting and testing to join directly to the PK.
I agree with this completely, and would say if you want to eventually become a DBA, learn what it is like to be a dev first. DBAs with dev skills can do some REALLY cool things!
Strange, I tried to install it and everything went smooth: * downloaded archive (not installer) and unpacked it on the root of the drive. Everything is placed in c:\xampp directory. * navigate to c:\xampp in explorer * execute setup_xampp.bat once * start xampp-control.exe In xampp-control: * start apache and mysql from it (not as a service, so there's no need to select users), both are shown as _"running"_ * apache admin opens browser at [http://localhost/xampp/](http://localhost/xampp/) and there's a link to working [phpMyAdmin](http://localhost/phpmyadmin/) in the tools section. The "Component Status Check Failure [3]" message [supposedly](http://www.makeuseof.com/tag/install-windows-web-server-pc-xampp/) appears only on Windows 7 64bit, but it shouldn't effect the functioning of webserver. But sure, if you're in it only for SQL, Microsoft SQL Server has it and it's more user friendly than apache+php+mysql+phpmyadmin. You can download a free edition with management tools from [here](http://www.microsoft.com/downloads/en/details.aspx?FamilyID=967225eb-207b-4950-91df-eeb5f35a80ee&amp;displaylang=en). You'll probably have to find a MS SQL oriented tutorial though.
Ahh you see I downloaded the archived version as well and read on the XAMPP webpage that when installing to root directory c:\XAMPP not to run setup_XAMPP.bat and just to proceed with control panel. I also use linux as well so if I cannot get it functioning properly on win7 x64 i'll test linux next. Youre saying to not install the user 'web'? Is there a way to actually select what user you want to use or is it done by just editing the .pHp files?
What kind of user is 'web' - is this a system user account, apache authenticated user or mysql user? As long you're the only one with access to XAMPP on Windows you don't need a system or apache user. As for mysql user, you could run all your applications/databases with 'root' account or follow the recommendations and use a different user for each database. In both cases, you have to enter mysql username &amp; password in the .php file.
Web is the user that the tutorial has you make, granting all privlidges and is linked to localhost. Basically an all access offline only account. Its named web because the excersize files were made with web being the mysql user. I am now switching to win 7 x86 after attempting an install in ubuntu but I could not get the mysql to start when launching xampp from terminal. Error log said network not set as localhost or something.Not a big fan of the way ubuntu works since you have to jump through hoops to access root. I just dropped it. Will test on win7 x86 if that doesnt work I will try to find a more bare version of linux than ubuntu. I cant stand the ubuntu gui. Its too constrained.
Here are the pHp and mySQL exercise files that came with Lynda. I've been trying to get [SELECT * FROM City] Query to work in the SID.php as the tutorial says this is how you test if everything is working properly. [Lynda_Exercise_Files](https://rapidshare.com/files/4273942248/Lynda_SQL_Exercise_Files.rar) It is still not working after format to win 7 x86, still receiving a whole ton of Notices when I browsed to locahost/SID/SID.php using XAMPP 1.7.4. When using XAMPP 1.7.0, Apache HTTP Server crashes when browsing to localhost/SID/SID.php.
Yes, mysql database users are fine, I thought you were having troubles with system users. Try running that query in phpMyAdmin first.
I ended up using version 1.7.1 of XAMPP after getting so frustrated and deciding tp test every version. Everything works smoothly on 1.7.1 which is odd but i'll take it lol. Thanks for your advice anyway man. Do you have any suggestion on a less restrictive linux build than ubuntu? Not sure if you use linux.
I still have a pair of old RedHat EL installs in production, but everything else is running Ubuntu. :) They're mostly remote servers without GUI so I don't find them that restrictive - 'sudo su' does everything I need.
 SELECT * FROM tableName WHERE columnToSearch LIKE '%Substring I Want%' I wouldn't suggest that you go the whole ID, alias, blob field route (known as E.A.V.) -- you will break a lot of the efficiency gains and advantages of a relational database and you will constantly be working to pull the data that you want from it.
 free text search 
 SELECT f.f_feature_number, f1.name, f.to_feature_number, f2.name FROM feature_link f INNER JOIN feature f1 ON f.f_feature_number = f1.fnumber INNER JOIN feature f2 ON f.to_feature_number = f2.fnumber 
I wish I had seen this comment before I typed out essentially the same thing. :)
Thank you, I will try tomorrow once I'm back at work :)
A second opinion is a good thing
What you're looking for is sixth form normalcy. Reply if you still need help.
What SQL flavour are you working with? SQL Server, Oracle and Postgres all have NTILE as a windowed function. Other platforms may also implement this, it's part of the later SQL standard. Here's a [link](http://www.java2s.com/Code/Oracle/Analytical-Functions/TogetaclearerpictureoftheNTILEfunction.htm) explaining the Oracle implementation. Transact SQL (i.e. SQL Server) page [here](http://msdn.microsoft.com/en-us/library/ms175126.aspx). 
Thanks for the help, this a a good start for me. I'll work with the SQL people here and see what we can make of it. I'm only a mildly capable Db user. Well, maybe extremely capable if you consider the abilities of 98% of my co-workers. It's an Oracle Db. I don't know more than that. I'm not using a regular terminal-type interface. I'm required to use a REALLY terrible "web" interface build with PowerBuilder. It looks like internet 1993. 
I don't actually understand what you're asking for. If you'd give a tiny piece of sample data and a tiny piece of expected output, you'd probably get a lot more answers.
Stored procedure that selects from/modifies a view, perhaps, and expects some kind of key? Restrict access to everything else? Only thing I can think of off the top of my head.
 .dbf is foxpro. Try exporting it to csv then importing it
I'll give that a shot. Thanks :)
updated submission, hopefully it's better.
Quite a bit. :) Will read after work, if someone doesn't beat me to it.
Take it in pieces. 1) specific groups in AD can be addressed by creating a master Group in AD, then adding that group to a custom SQL server login, then modifying the perms on the tables/databases for that sql server login. 2) the app thing is going to be more difficult, but you can use an assembly function to write some code to test if there's an entry in regedit in the local machine. Do the test to find the value, if value isn't present trap the error, return a message to the user. 
I can see two major paths you could take: application roles, or logon triggers. An application role is a database-scoped solution that allows you to transform into a user that has a specific set of privledges, on the condition that you know the password to activate the role. For example, your users log on (via their Active Directory login) to your application. They only have the barest minimum of rights on the database. The application then sends through a sp_setapprole 'AppRoleName', 'secretpassword' command, which grants the user the rights granted to that application role. If you don't know the password, you can't assume the app role, so the only way to assume the app role is via the application. More details on [Application Roles](http://msdn.microsoft.com/en-us/library/ms190998.aspx) from Books Online. A login trigger is a server-scoped solution which will analyse all logon events. If you detect your users logging on with an incorrect application, then you can reject the login. See [Logon Triggers](http://technet.microsoft.com/en-us/library/bb326598.aspx) in Books Online for an example. The program_name column in sys.dm_exec_sessions will give you the program name, and this can be assigned in your connection string. In terms of sysadmin access, with application roles, this doesn't matter - your sysadmins have full control over the database from the backend. In the logon trigger solution, you'd want to code in some exceptions for your sysadmins just in case - you don't want to lock out your sysadmins, or SQL Server Agent, etc!
&gt;Can anyone give me a few tips/hints as to how to create multiple fields/views for when creating a search field for a specified username? I think you just want to create one view that brings all of your data together. Then you would filter that view WHERE username = '?loggedInUser'
I have never heard it suggested that using surrogate keys (if I understand how you are using the term correctly - an artificially added column that exists just for uniqueness because the data doesn't lend itself to it) is not good design. Many tables will absolutely need one. Imagine a "person" table. Short of a social security number, which is bad form to keep in a table in many circumstances plus nit everyone has one, anyway (non-citizens for example). What could be your primary key there? A name is not good enough, obviously. Name plus address? Still no (plenty of people share a name with their dad and share a home). Plus this data is all subject to change, and you do not want to be changing your primary key for a record if it is at all avoidable. Imagine an even simpler example: a "comments" table where the only field is the text of the comment. There you absolutely NEED an identity column. The only instance I can think of where a surrogate would be a bad idea is in a table whose purpose is to relate two entities together. Such as a table which links professors with the courses they teach. There you want your key to be (professor_id, course_id). To add a surrogate key to that table kind of defeats the purpose. The same professor should not be able to teach the same course multiple times (unless you are adding, say, a semester_id to the table, in which case that should be part of the key, too). I am not sure why your coworker thought this was bad design. Maybe he was being more specific to a certain case than you realized (such as the example I gave above). Or perhaps he is just misinformed? 
Another consideration that your college might have been considering is the difference between SQL Server's cluster key vs the primary key. By default, the table is clustered on the primary key, and the clustered key is then used in all non-clustered indexes. In this situation, a narrow clustered key (such as an INT IDENTITY surrogate) will be more efficient (4 bytes per row in the index), whereas a wide clustered key (such as a composite, e.g., a char(20 and a bigint) will be less efficient (28 bytes per row in the index). Personally, I'll use a natural key if possible, if it doesn't overly complicate joining the table - a composite natural key that needs 4 columns is annoying to use compared to using a surrogate. Otherwise, I'm in agreeance with hypo. Quick quiz - what makes a primary key constraint functionally different from any other unique constraint?
Does anyone mind showing me a simple demonstration of when to use natural key and when not to? I can't find any videos or samples. THANKS!!
http://www.sqlteam.com/forums/topic.asp?TOPIC_ID=51268 The third post down said that adding a PK to an identity column doesn't make it unique. So how can we prevent duplicate data if we only use identity as the PK? Is that what composite keys for? And does that mean junction tables are all composite keys since their PKs are from the PKs of its related tables?
From that thread: &gt; A primary key is used to uniquely identify a row in a relational table. If you put a PK on your identity column you have not uniquely identified anything, because you could continually insert 'IMPALA' into your table without violating the PK constraint. We have had countless discussions about this here. This is referring to the fact that if the original poster in that thread were to put a PK on the IDENTITY column, he would be able to insert 'IMPALA' (as a car model) multiple times, and get a new IDENTITY value each time. This could be solved by placing an additional unique constraint on the CarModelName. Doing it this way, you'd end up with at least two structures (more if your table is a heap) - a clustered index on the IDENTITY column, and a non-clustered index on the CarModelName (to support the unique constraint). You now have a little bit more overhead, but at least you now have a surrogate key.
I'll see if I can dig something up when I get home, but this is very much a divided topic in the database world - much the same as the PC vs Mac, Oracle vs SQL Server, uppercase vs lowercase SQL style arguments. Some people use surrogates for everything (often citing easy of development and joining, and performance), while some people avoid surrogates like the plague. The rest of the people try to keep a balanced view, and don't tend to join the inevitable flame wars that tend to develop on this topic.
I think taejim said it well. This is a divided topic. There is no simple answer of when to use a natural key and when not to. A lot of it is a judgement call, taking a lot of factors into consideration: the nature of the data, the likelihood of it changing, the necessity to support duplicate data (like 2 people with the same name and address example I gave above), the likely size of the table (for performance consideration), the real world objects you are modeling on (when appropriate), the ease with which you want developers to be able to join to this table etc.... In many, many cases, the decision to use a surrogate vs a natural key will not be a game changer. Make a decision, and if it isn't working, change it. The ability to make the right decision at the right time will come with experience.
I take the pragmatic route: If the data itself does not naturally carry a key, then create your own identity column and make it part of (or all of) the primary key. Username? No problem, you got your key right there. Warranty claim number? Identity key it is. Warranty claim line items? Composite key of claim number + "auto" incrementing line numbers.
what DB are you using?
You could do a union, something like this: SELECT part_number, rev, step, "table1" AS tablename FROM table1 UNION SELECT part_number, rev, step, "table2" AS tablename FROM table2
oh jeez i think that's probably the best option! hadn't even thought of that. 
I genuinely don't know. It's the company's database. ninja: any way i could find out?
This would work to some extent, but will not give you distinct part numbers and revisions. You'll need to wrap that union in a larger select with a where clause that can decide which table to pick if there are duplicates, or you could use a group by part_number,rev on the whole thing to just count how many tables have each part.
what are you using to access it?
I agree, this is by far a simple solution to the problem. You could use WHERE NOT EXISTS (query) to only union the non duplicates.
Microsoft SQL Server Management Studio
The DB is MSSQL server then.
 use the full names when you referr to the tables/columns select * from tablea a join tableb b on (a.id = b.tablea.id) etc.. you can also alise the output column select column as mynewname 
FYI, when you connect to the database it should have text along the lines of servername (SQL Server 9.0.xxxx - DOMAIN\username) The 9.0.xxxx part is the version number. 9.0 would mean SQL Server 2005. 8.0.xxxx would be SQL Server 2000 and 10.0.xxxx would mean SQL Server 2008
Keep in mind that in mssql, union performs a distinct operation. You would most likely want union all. 
Thanks, I figured that was it when I was doing some research. Eveyrthing autogenerates like a charm. So beautiful :D
Do the tables in your select statements have the WITH(NOLOCK)?
TL;DR: an unreferenced quote from Michael Stonebraker, as of now not verified by Facebook. And then an entire article debating the merits of sql and nosql. [w00t](http://www.postgresql.org/about/news.1323) [w00t](http://www.piware.de/2011/06/debianubuntu-packages-for-postgresql-9-1-beta-2/)
Classy. Upboat.
Oh, I thought it was an add for "NewSQL". Hadn't heard that term coined before. &lt;sarcasm&gt;I can't wait to hear more about that.&lt;/sarcasm&gt;
SQL doesn't scale. Create db to run completely in memory. What. The. Fuck. That **will not scale**. 
He lost me after "webscale" :/
Data types. Your column internal_id is a varchar, but your first statement you are comparing it to an integer value. This causes MySQL to convert internal_id in every row to an integer for the comparison to take place. That's a waste of CPU. If all of your internal_id values really are integer, then you should change the column data type to integer to save space and further speed up your queries.
Thanks for explanation, I was thinking about that but still, I thought it would have been more clever to convert my value to a string instead of all the values in my table....
&gt;I thought it would have been more clever to convert my value to a string instead of all the values in my table.... It would have been more clever for your situation, but that's not how SQL works. When you are comparing a column against a value you provide, you are implicitly telling SQL to convert the column's data type to match the same as you've provided.
As put by a Facebook DBA: http://dom.as/2011/07/08/stonebraker-trapped/ (no that is not me/my blog, just someone's blog which I happen to read occasionally)
(SQL Server syntax disclaimer) You're pretty close: SELECT * FROM BOOKS WHERE bookTitle NOT IN ('Ulysses','Magician','Harry Potter') However, your example smacks of using a book title as a primary key, indicating that the book title must be unique. You'd be better off using a different unique property, such as an ISBN. That said, if you have a unique constraint on your book title, you can do a very quick lookup of the existing title when inserting: IF NOT EXISTS (SELECT 1 FROM Books WHERE bookTitle = 'New Book') INSERT INTO Books (bookTitle) VALUES ('New Book') While your original design choice was to search the list of books you already have and see if the only you're trying to insert doesn't exist, this looks for the one you're trying to insert, and only inserts if it is not found.
Thanks for your reply, I'll test this shortly. Looking at it though (and having not yet tested) I would expect that statement would return "A Hobbits Guide to Footgrooming" as the only result. Where as I'm trying to return Magician as the result as it's the only book not already existing in the db. (fyi the table name and rows are just an example, I would definitely choose ISBN if it was a real life example.... good advice though! )
You can use a sort of subquery which has a similar effect to a temporary table without actually constructing one. For instance, using the EXCEPT functionality available in most SQL variants: SELECT 'Harry Potter' as bookTitle UNION ALL SELECT 'Ulysses' UNION ALL SELECT 'Magician' EXCEPT SELECT bookTitle FROM BOOKS; Will give: bookTitle ======= Magician Similarly: SELECT B1.bookTitle FROM ( SELECT 'Harry Potter' as bookTitle UNION ALL SELECT 'Ulysses' UNION ALL SELECT 'Magician' ) B1 LEFT JOIN BOOKS B2 ON B1.bookTitle = B2.bookTitle WHERE B2.bookTitle IS NULL; Gives the same result. Depending on the SQL variant you're using there will be other ways of constructing the set without recourse to the UNION ALL stuff above. 
&gt;SELECT * FROM BOOKS WHERE bookTitle NOT IN ('Ulysses','Magician','Harry Potter') I don't think that's what he's after. He wants (as a result) the value in the list, not in the table.
Ah, yes, you are right. Let's try again. The table contains: Harry Potter - Ulysses - A Hobbits Guide to Foot Grooming Your list of new books to add (which may or may not be already in the table) are: 'Ulysses','Magician','Harry Potter' Your query isn't to say "SELECT FROM Books", because you're actually wanting to return from your list of books to add. So you want to instead "SELECT FROM NewBooks". You'll need to convert that into a table (instead of a comma separated list), and then you can: SELECT bookTitle FROM NewBooks WHERE bookTitle NOT IN (SELECT bookTitle FROM Books). Or, a left join: SELECT bookTitle FROM NewBooks LEFT JOIN Books ON NewBooks.bookTitle = Books.bookTitle WHERE Books.bookTitle IS NULL 
Ah excellent, EXCEPT and INTERSECT are new terms I had not yet discovered. This will be very useful, thank you! (DB2 by the way)
Yeah, I gravitated to his example SQL statement and got caught up with that. My mistake!
Cheers, I was fairly dubious about being able to achieve the above with a comma separated list. I might have to bite the bullet and create a temp table or use the method suggested by Dharma Police. Thanks guys.
I can't replicate what you're describing, but I've only got a SQL Server 2008 install available. However, views are only defined at time of creation and unless schema binding is used, won't necessarily reflect any changes to underlying tables. So if you do this: CREATE TABLE base_table (a int, b int, c int, d int); CREATE VIEW view1 AS SELECT a,b,c,d FROM base_table; ALTER TABLE base_table ALTER COLUMN D CHAR(1); And then check INFORMATION_SCHEMA.COLUMNS for view1, it'll list column D as an integer. Similarly, if you drop the column entirely then it'll still appear in the system catalogue, but if you then try and query the view you'll either get an error about the specific column being missing or (if you've used select * in your view query) then : View or function 'view1' has more column names specified than columns defined. Or at least, that's what I get when trying now. The only part of SQL Server that I recall being "tolerant" of columns being missing is Reporting Services - from what I recall it handles missing columns/fields without any error when running the report (obviously if you rebuild/redeploy then it complains). So basically I've no idea. :) As a general point you might want to consider using WITH SCHEMABINDING on the key views in your system. That way, if in the above example I'd attempted to drop column "D" then I'd get: The object 'view1' is dependent on column 'd'. ALTER TABLE DROP COLUMN d failed because one or more objects access this column. It makes schema changes *really* annoying but given the possible ramifications of dropping / renaming columns perhaps it should be. It's required for indexed views anyway, should that ever be required.
First, let me say that this is by no means a best practice. However, in certain cases, this function has helped us solve this exact problem with relative ease. Do not expect this to scale, and please, use this function with caution, and only in the correct circumstances. Many times, you can find a much better solution using raw SQL, however, this will help you with one-off queries of the type you are working on when you just need to do a one-off. This is a modification of code found from other sources, and I apologize for not being able to provide proper attribution. For MSSQL: CREATE FUNCTION [dbo].[TextListToTable] (@text varchar(8000), @delimiter varchar(20) = ',') RETURNS @Strings TABLE (val VARCHAR(255)) AS BEGIN DECLARE @index int SET @index = -1 WHILE (LEN(@text) &gt; 0) BEGIN SET @index = CHARINDEX(@delimiter , @text) IF (@index = 0) AND (LEN(@text) &gt; 0) BEGIN INSERT INTO @Strings VALUES (@text) BREAK END IF (@index &gt; 1) BEGIN INSERT INTO @Strings VALUES (LEFT(@text, @index - 1)) SET @text = RIGHT(@text, (LEN(@text) - @index)) END ELSE BEGIN SET @text = RIGHT(@text, (LEN(@text) - @index)) END END RETURN END Using this, you can do this (MSSQL): SELECT * FROM dbo.TextListToTable('Ulysses','Magician','Harry Potter') myBooks WHERE myBooks.val NOT IN (SELECT bookTitle FROM books) Again - this funciton should be used with caution. Don't just go storing lists all over the place. I provide this answer because sometimes you just need to "get stuff done" and architecting a true relational solution will not work.
DharmaPolice and alfet are 100% correct. Views will not automatically update without Schema Binding. Most good SQL diff tools will have an option to script the re-compiling of dependent views when you make a table change. We use AdeptSQLDiff and Red Gate, and both support this option spcifically to deal with this case. If you do not have these tools, I highly recommend investing in one of them. They're cheap in the grand scheme of things and will save you immeasurable headaches going forward.
What is the data stored like? (maybe the column schema would help, if you can post that information on the internet).
http://i.imgur.com/sCXLC.jpg Here's a wizard generated report and it's code to give you an idea. Hope that gives you an idea of how it's set up. Sorry about the blurring it's confidential stuff for the most part. 
To answer the question posed in your title is relatively straight-forward but I'm not clear exactly what sort of output you want. Here's one approach, for listing each node and then a count of how many times packet loss is above 0 and another count of how many times packet loss is above 50. SELECT N1.Caption as NodeName, SUM(CASE WHEN R1.PercentLoss &gt; 0 THEN 1 ELSE 0 END) as Greater_Than_0, SUM(CASE WHEN R1.PercentLoss &gt; 50 THEN 1 ELSE 0 END) as Greater_Than_50 FROM Nodes N1 INNER JOIN ResponseTime R1 ON N1.NodeID = R1.NodeID GROUP BY N1.Caption ORDER BY N1.Caption Would give three columns along the lines of: NodeName Greater_Than_0 Greater_Than_50 Node1 10 5 Node2 20 0 etc... This would cover the entire date range in your data which is probably not very useful in day to day use so you would probably want to add a date range in (as a where statement) or (more likely) add in another column to show the month. If the count / total is misleading (e.g. packet losses happen in clusters) then you might want something like: SELECT t.NodeName, COUNT(*) as Occs FROM ( SELECT YEAR([DateTime]) as Y, MONTH([DateTime]) as M, N1.Caption as NodeName, MAX(R1.PercentLoss) AS Max_Packet_Loss FROM Nodes N1 INNER JOIN ResponseTime R1 ON N1.NodeID = R1.NodeID GROUP BY YEAR([DateTime]) , MONTH([DateTime]), N1.Caption HAVING MAX(R1.PercentLoss)&gt;50 ) t GROUP BY t.NodeName This would give a count of how many particular months each Node had a percentage loss of above 50. If this was purely a report to show the worst 10 nodes then you could just ask for: SELECT TOP 10 /* same shit as in first case */ GROUP BY N1.Caption ORDER BY Greater_Than_0 DESC There are at least 10 different ways of approaching this problem however, it just depends on what exactly you want as an output / what's being measured etc. Usually people want something they can put into Excel and play with / analyse on their own time. Rather than spending a lot of time second guessing this, I'd slap something quickly together and get feedback. End-user requirements invariably change when they actually see a report, even when you spend hours formally agreeing a specification in advance. 
Thanks a ton dude! this is an excellent start to what I need. I'll be able to take it from here I'm sure. Much appreciated!
You can try sqlite or MySQL. 
Also, I'm not sure if it makes a difference, but both VMs are running on the same physical SBS 2008 server.
SQL Developer Edition + SQL Studio -- if you want to stick with Microsoft it will suit you very well.
The suggestions I was going to make have already been mentioned so I'll just add that it depends on the sort of environment you're working in / with and what you're interested in learning about exactly. If you're currently running Access on your home PC then you'll probably find it quite straight-forward to install SQL Server Express and then use the Access upsize wizard (or whatever it's called) to migrate an existing database to SQL server just to get the hang of things. SQL Server Express isn't particularly complex to install but the shift towards connecting to a server / service might take a while to get your head round if you're unfamiliar. SQL Server Express has limitations but they're not too annoying for getting started. As ninjaroach has already said, you can also use the development or trial versions too which are time limited, but allow much larger databases, etc. If going down that route isn't appealing (or you can't install services) then you might want to experiment with SQL databases in a web hosting environment. Most standard web hosting deals offer at least MySQL access and others will offer PostgreSQL too (or instead). You can of course install these things on your home computer as well, but that might be a bit tedious if you're just interested in playing with SQL and aren't looking at web development generally. Finally, if you don't want to install anything and just want to get to grips with "pure" SQL code without any DBA stuff then SQLite is awesome. No bloated installers, no services running, no tedious permission issues - just an executable and a database file. If command line access is too minimalist then there's a plugin which turns Firefox into a half-decent SQLite front-end too.
Did you rename the server's hostname and sql server name? http://msdn.microsoft.com/en-us/library/ms143799.aspx Is there any replication setup on the sql server?
Keep using Access for your front end development but install the dev edition of SQL Server for your database. This allows you access to Microsoft's Business Intelligence suite and even familiarizes you with some Visual Studio like interfaces. The logical step after you've done some projects utilizing the above would be to learn to harness the .NET framework for your application development.
we changed the server's hostname, but not the sql server name. we seem to be running into an issue running sp_dropserver. the old server name had a - in it (Server-Name), and doing sp_dropserver Server-Name returns invalid syntax near '-'. anyone know if there's a way to fix this?
As part of the release process we do on customer databases, we run a procedure to refresh ALL views on a production system. The procedure gets all views from sys.objects, then runs this query: exec sp_refreshview 'ViewName' wrap an exec() around the procedure call and it won't stop if it fails to refresh a view. 
I did something like this once in SQL Server - a huge single statement with tons of inter-referencing CTEs. It was so clever... and it was fucking slow as shit. I went back to using temp tables.
Maybe it needs quotes around the host name so it doesn't get confused by the dash. I'm just shooting in the dark so I don't know if this has anything to do with your problem. 
Microsoft SQL Server Management Studio
In postgresql you could do: SELECT concat( (SELECT '1'), (SELECT '2'), (SELECT '3') ); or if some of union queries return more than one row : SELECT string_agg(num,'') AS number FROM ( SELECT * FROM ( SELECT '1' AS num UNION SELECT '2' UNION SELECT '3' ) AS q1 ORDER BY num ) AS q2; 
This view is surprisingly not slow. However, it's been changed since then to include a bunch of joins using LIKE on CHAR(30) fields and that slowed the query down from 15 seconds to over 60.
Have you only been using Access' visual query designers, or have you been looking at the SQL view of your queries? Once you get a SQL Server install, switching to the SQL view and looking at the SQL Access creates from the visual designer can help you understand SQL. That's how I learned anyway. Once you get to where the basic SQL syntax is second nature, you can start learning about things like CTE's and windowed functions, or try some of Joe Celko's [SQL Puzzles](http://www.computerworld.com/s/article/9012350/Can_you_solve_Joe_Celko_s_SQL_puzzles_)...
That's good to know. Thanks a bunch!
String aggregation in TSQL can be performed by using the [for xml method](http://codecorner.galanter.net/2009/06/25/t-sql-string-aggregate-in-sql-server/). There are ways to do it using the CLR but it's overly complicated. If the number of rows you're aggregating never changes then there's a much more efficient yet less elegant way to code it. I think you've overly simplified your example here which may prevent you from getting an optimal solution from this subreddit but the above should get you going in the right direction.
I don't use oracle a lot, so I'm not sure if it supports referring to aliased columns in the where clause. What happens if you change: and prepdate &gt;= '21-JUL-11' to and TRUNC(GetPeriodDate(a.ContractIID,9,0)) &gt;= '21-JUL-11' 
Does it tell you what line the error is on? What is your schema for these 4 tables? 
You've got to repeat the trunc in the where clause. You can only use aliases in the order and group by clauses. Also, dear god don't use a user defined function in the select. Create a temp table and join. 
 You cannot reference prepdate. Use trunc again (fastest) or use the having clause. Or use a sub select (can generate temp table in background)
You guys are great! I swear I tried using trunc again, but I swear a lot! Let me get some coffee, give it a go, and get back to all my new friends!
What happens? It works! Thank you!
I'm very interested in what you have to say, since.. apparently I'm doing something 'almost dangerous' in your mind. I can only assume GetPeriodDate is my user defined function. I tried to take a peek at it in Developer but its encrypted? &lt;stop laughing&gt;
&gt; Add a block comment at the beginning of the text inside the query window so we don’t execute erroneous or even harmful statements Eh, you shouldn't be developing like this on a production database instance for one thing. But if you have to for whatever reason, you should at least enclose your statements in begin transaction/rollback transaction. Then you can execute and observe the results of as many potentially harmful statements as you want.
Even in a development database there's still sometimes code you don't want to execute at a given moment in time though. Sometimes it's not even a data integrity issue as such - you just don't want to accidentally run some enormously expensive query on tables you know in advance are locked for some reason. You're right about transactions of course, it's interesting that (in my limited experience/observations) a lot of SQL Server users don't seem to use explicit transactional control as often as with other platforms. It might just be a default settings thing - the GUIs for other platforms have automatic committing "off" as standard. It could also be a simple laziness thing. I find I write enormous amounts of adhoc query code which accumulates in various degrees of mess. It's either have it all in a few files (and then you're dealing with a situation that I think the OP is referring) or in lots of different query windows. I'd prefer the latter of these two situations but as a result when I come to shut down I I'm confronted with a "Would you like to save changes to SQLQuery1-100.sql" message requiring 10 minutes working out which file I put that really useful query 4 hours earlier.
When developing, I often have two franken-queries mixed together with lots of line breaks and dash comments. Then I adjust the comments to execute which part of the query I want. Example: -- DELETE SELECT * FROM MyDevDB.MyDevtable -- WHERE SomeCriteriaIMayWant
I got two problems with using username as primary key. 1. The index will be clustered on something that you can't predict how it will be filled up, that is your index will produce lots of page splits and index rebuild will take lots of time. 2. Usernames are varchar or nvarchars, no way do I want all related tables hold a username column and join on them. Instead create surrogate key with UserId and create a non clustered index for username if you need it.
SELECT PERSON_ID FROM ATTENDANCE A WHERE A.EVENT_ID IS IN (95, 96, 97) AND A.ATTENDED = 1
in is being used like an or statement; it's returning people with attended=1 for event_id=95 OR 96 OR 97, not and.
select distinct a.person_id from attendance a join attendance b on a.person_id = b.person_id join attendance c on b.person_id = c.person_id where a.attended = 1 and b.attended = 1 and c.attended = 1 and a.event_id =95 and b.event_id = 96 and c.event_id = 97 That should just about do it.
I misread, sorry. This isn't a very clean way of doing it, but here's another way from a non-pro: SELECT A.PERSON_ID FROM ATTENDANCE A WHERE A.EVENT_ID IS IN (95, 96, 97) AND A.ATTENDED = 1 and SUM(ATTENDED) = 3 GROUP BY A.PERSON_ID
Another way using a group by instead of self joins: select person_id from attendance where event_id in (95,96,97) and attended=1 group by person_id having count(*)=3
Don't write this in PHP, this is a job for your database! Because you want to inspect more than one row per person_id, you'll need to write an aggregate query to summarize the results. Try starting with this: SELECT person_id, SUM(attended) FROM events WHERE event_id IN (95, 96, 97) GROUP BY person_id HAVING SUM(attended) = 3 They key here is to make sure your HAVING statement is looking for the same number of entries contained in the WHERE event_id IN (......) clause. You could use PHP to solve that part while building the query. EDIT: [br0kenface](http://www.reddit.com/r/SQL/comments/j0c2u/need_help_with_sql_command/c283ctx) took the same approach but came up with a slightly cleaner query and result set. I like his better.
You can do this without an aggregate, too: SELECT person_id FROM people p WHERE NOT EXISTS (SELECT 1 FROM attendance a WHERE a.person_id = p.person_id AND event_id in (95, 96, 97) AND attended = 0) I'm assuming "people" is the master table that holds your people ids. You could also do SELECT DISTINCT person_id FROM attendance with the same where clause. And, for educational purposes only, while I'm coming up with other ways to do this which will work (but which may not be the best way): SELECT person_id FROM attendance WHERE event_id = 95 AND attended = 1 INTERSECT SELECT person_id FROM attendance WHERE event_id = 96 AND attended = 1 INTERSECT SELECT person_id FROM attendance WHERE event_id = 97 AND attended = 1 EDIT: My first approach requires that there is a record in attendance for every event for every person. If person 3 has no record whatsoever for event 95 in attendance, but has attended = 1 for 96 and 97, my query will include them whereas yours will not. 
 select distinct person_id from attendance join (select 95 as id union select 96 union select 97) as eventInner on event_id = eventInner.id where attended=1 and event_id=? having sum(attended)=3; Basically a slight rephrasing of NinjaRoach's approach for performance. 
What is the SQL you are currently using?
You want to do a left join on * table a to b * table a to c Then you can select [id].id, [apples].id, [orange].id That should give you what you're looking for.
Left outer join specifically, in case there is no orange or apple. :)
Not sure why you're getting a doubling of rows then. SELECT Table1.ID, Oranges.Oranges, Apples.Apples FROM (Table1 LEFT JOIN Oranges ON Table1.ID = Oranges.Table1ID) LEFT JOIN Apples ON Table1.ID = Apples.Table1ID; 
Not sure if I'm reading the problem correctly, it's too early in the morning, and typing this on a tablet sucks ass, but: --(I'm a SQL Server guy, so you may need to tweak syntax) SELECT name, isnull(cnt, 0) FROM ( --this subquery will get tags --First grab parents SELECT tag_id, name FROM tag WHERE name = @string AND synonym is null UNION --then grab kids, making sure that the parents wouldn't match SELECT tag_id, name FROM tag t INNER JOIN tag p on p.tag_id = t.synonym WHERE t.synonym is not null AND t.name = @string AND p.name != @string ) tags left join ( --this subquery will get tag frequencies SELECT tag_id, count(*) as 'cnt' FROM tag_mapping GROUP BY tag_id ) tagcounts ON tags.tag_id = tagcounts.tag_id ORDER BY cnt desc, name If I'm understanding this right, that might work... maybe... EDIT: Stupid formatting...
You're quite welcome. In your code, you can probably ditch the having clause, as the count couldn't possibly be 0. I used the isnull() function up top to replace any null values with a 0, as nulls would just make the ordering goofy. You might also want to avoid using "name" or "count" in your code, as they are reserved words. At the very least, enclose them in [square brackets] to indicate you're referencing an objects of some sort in the database.
&gt; you can probably ditch the having clause, as the count couldn't possibly be 0 ~~ Actually, I have a few stray tags that don't have any mappings, so I'm using the `having` clause to ignore those.~~ ignore this, i'm stupid &gt; I used the isnull() function up top to replace any null values with a 0 Is `isnull()` specific to a SQL implementation? I couldn't find it in Postgresql's documentation, that's why I removed it &gt; You might also want to avoid using "name" or "count" in your code Will do, thanks again!
Ah, yes, that's a MSSQL specific function. You might find a suitable replacement here: http://stackoverflow.com/questions/2214525/what-is-the-postgresql-equivalent-to-isnull
Just a heads up. [Coalesce()](http://msdn.microsoft.com/en-us/library/ms190349.aspx) is standard ansi-92 and works on all major sql engines. It's the same thing as isnull() except coalesce accepts more arguments.
I have spent far too much time in Microsoft's playground :-/
Hey now don't feel bad. SqlServer is a sweet enterprise DBMS and knowing your stuff on that platform is job security my friend!
Understand what "relational database" means; how ER-diagrams work; keys (foreign key, primary key, candidate keys); SQL joins (inner, outer, left); and possibly normalization (+ its purpose). Also read up on indexes if you haven't dealt much with them.
One thing that I'd suggest you read up on is data normalization, which is essentially the art of formatting data so that it's database-friendly. It's a very common task for database-centric jobs. Other then that, a few SQL specific things: * Make sure you understand what NULL means in database terms and the associated three-valued logic. The expression "1 != NULL" does not return as true in a SQL query. Make sure you fully comprehend why this is so. * Be aware of the differences between IN and EXISTS (and NOT IN, and NOT EXISTS) and the kinds of situations where one is preferable to another. Be aware of how IN statements interact with NULL values. * Know how to use the HAVING clause. * Understand what an index is and the kinds of situations where they will and will not help a query run faster. * Be aware of old style join syntax (table names simply separated by commas in the FROM clause with all join conditions in the WHERE clause with \*= or =\* used to indicate outer joins) Some older professionals grew up with that shit and refuse to abandon it. YOU on the other hand will use ANSI SQL joins or I will personally come over there and beat you with a hose. * Know how to get the execution plan for a query. 
&gt; They are using MS-SQL but I don't think that really matters for the interview. ... wow. You need to start reading *right* *now*.
&gt; Be aware of old style join syntax (table names simply separated by commas in the FROM clause with all join conditions in the WHERE clause with *= or =* used to indicate outer joins) Some older professionals grew up with that shit and refuse to abandon it. YOU on the other hand will use ANSI SQL joins or I will personally come over there and beat you with a hose. Do you work for my company? (And if not, do you want to? It sounds like you'd fit in well)
Many of the specific SQL concepts that I would ask have already been covered here, but I'm going to throw out a more general piece of advice: **Think out loud. ** If I ask a developer to go up to the white board to write some code and solve a problem, I want to hear him talking his way through it. I don't want him to be silent for 5 minutes then go up and put some code on the board. I want to hear him asking questions to himself and answering them to demonstrate the thought process that he is going through as he approaches a problem. Things like "Well this table won't necessarily have a record for every person, so I'd want a left join here" or "I can use a group by and having clause to limit the results to people who own at least 3 cats" - stuff like that which give me as the interviewer an idea of how you approach a problem, that you understand the concepts you are working with, and that you can communicate those thoughts to someone else. Even if you have a few minor syntax problems in your actual code, I'd call it a correct answer if you've already demonstrated to me verbally that you've solved the problem mentally.
when using/practicing indexes. use the execution plan as reference to see that you acomplish what you want.
I remember reading quite the long blog post about this. The author ran his operation over a really large set. The conclusion escapes me but I'm pretty sure that coaelsce is good now and that isnull shouldn't be used. I use isnull still though cause I can't spell coalesce without a cheat sheet. :D
Hi, I gave this a spin in mssql. The first query returns what I think you want. The second is the first thing I could think of that returned what you actually got. Hope it helps! declare @IdTable table (ID int);insert into @IdTable values (1);declare @apples table (ID int, bInclude bit);insert into @apples values (1,1);declare @oranges table (ID int, bInclude bit);insert into @oranges values (1,1) SELECT t.ID ,a.bInclude ,o.bInclude FROM @IdTable t LEFT OUTER JOIN @apples a ON t.ID = a.ID LEFT OUTER JOIN @oranges o ON t.ID = o.ID; SELECT t.ID ,a.bInclude ,null FROM @IdTable t LEFT OUTER JOIN @apples a ON t.ID = a.ID UNION SELECT t.ID ,null ,o.bInclude FROM @IdTable t LEFT OUTER JOIN @oranges o ON t.ID = o.ID;
This can't be said enough. I'm even more impressed by candidates who ask if they can draw a problem out on the board. To me, explaining your solution - or even your attempt at one - is actually better than silently staring at the problem for a minute, then blurting out a correct answer. You aren't impressing anyone by having fast, valid answers to everything; in most businesses, team-work and the ability to convey technical information to non-technical people are more important.
I'm the database architect for the company I work for; over the last few years I've helped hire a junior DBA, a few contractors, several software developers, and a few business intelligence/reporting analysts. We try to avoid multiple rounds of interviews, so our first round interviews are relatively long - usually around 90 minutes, depending on how well the conversation goes, and whether or not the person can answer any of our questions. The general flow of our interviews goes: * Introductions - This isn't unique to us, most interviews will start by having you introduce yourself. Use this as an opportunity to highlight parts of your experience that you didn't/couldn't put on your resume or cover letter. Great time to mention outside projects. This is also your first make or break moment in the interview; if this goes poorly (like if all you say is "I'm Bob and I need a job"), we're probably already thinking of what questions we can trim to get you out the door sooner. * HR/canned questions. Boring, overused, non-technical stuff, but be prepared with solid, textbook answers. HR loves this shit. What's your biggest weakness. What expectations would you have of your manager. How well do you work on a team. Where do you see yourself in 5 years. * Technical stuff. This is where we're going to grill you with questions ranging from simple theory - normalization, backup routines, basic database objects, etc - to hands on stuff. I'll get to some questions in a bit. * Semi-technical/environment/team questions. We do this mostly to figure out whether you'd be a good fit on the team. We might ask about various development methodologies - SCRUM, Agile, TDD, etc. Questions about how you'd get resources to solve a problem you're stuck on. A favorite of mine is asking you to prioritize a handful of tasks if they were all assigned to you at once. Etc. * Wrap-up. If we like you at this point, we're going to ask more personal questions that start to make the HR rep uncomfortable. What do you do outside of work. What do you do for fun. How many computers do you have in your house. Etc. At this point we're trying to see if we'd like working with you. If you get questions like this, you probably did at least OK on the rest of the interview. As for questions themselves, our interviews are pretty fluid. We have a bank of several dozens questions that we take in with us. We'll start out asking the same questions of all candidates, and move on to other ones depending on how they answer. We try to tailor the questions to the persons experience. For instance, if you come in with a lot of MySQL experience, we're going to avoid many of our MSSQL-centric questions. A few of the questions or topics that we try to cover: * Normalization. This is so basic that we hate asking it, but it filters out a lot of people who might otherwise waste our time. Understand it. Consider memorizing the textbook definitions through the 3rd normal form. Be able to create a normalized database structure. * Indexes. We don't expect interviewees to be an expert on this topic, especially entry level ones, but we do expect that they at least understand vague what an index is and why you'd use it. Know the difference between clustered and unclustered indexes. * Joins. Left, right, inner, outer, full, cross. For an entry level we'd ask about them all, but really, we'd want to know that you know the difference between an inner and a left join. * Design. We'll create a set of application requirements, and ask you to create a table structure that would support all the specified requirements. This will almost always involve the implementation of a many-to-many relationship necessitating a bride/mapping table. * Having. Panda mentioned this as well. Surprisingly, a lot of people don't know how to use the Having clause properly. * Data types - though this will be specific to the database system used by the company you're applying at. We ask about the differences between varchar and nvarchar, char and varchar, varchar and text, floats and ints, etc. We may also ask about how much space is consumed by specific types. * Dimension vs fact. For an entry level position, we expect the definition of a fact table to be a little fuzzy, but a dimension is almost self-defining, so we hope for a good answer on that. If you're able to describe what a dimension is, we're also going to ask the difference between a type 1 and type 2 dimension. * Troubleshooting. We provide a small table schema, usually 6-8 tables. First we'll have you define the relationships between the tables using educated guesses based off the table and field names. Then, we'll hand you a few queries, and ask you to describe why they won't run, what data they'd return, or what logical errors they have. * Prioritization. We'll give you a few tasks, and ask you to prioritize the order you'd handle them in if they were all given to you at once. An example might be: A client has asked for a report showing sales for the last quarter; A database for a production system used by 40% of our employees has become corrupted and needs to be restored from a backup; A developer has asked you to make changes to a stored procedure for a product due to ship in 2 weeks; A business analyst needs help fixing his query that is returning 2+ million rows when he expects only 10. For any question like this, think about each situation in terms of money. The corrupted database is likely costing the company money in real-time, and should be addressed first. Clients need to be kept happy if they're going to be kept at all, so he's next. The developer's product is also nearing launch, so he's third, as he has a tiny bit of wiggle room and thus can wait a bit. * Backup strategy. We'd ask you to describe how you would implement backups in our environment. We already have this in place obviously, but we want to make sure you understand how to protect the data in a sensible manner. At the end of most interviews, after you've left, we'll ask the HR person how well they understood your technical explanations. Being able to connect with non-technical people is hugely important, and if you can explain a complicated concept to a person who can't tell a calculator from a keyboard, that's golden. As was stated elsewhere, be sure to vocalize your thought process. Even if you don't get to the answer we're looking for, if we can follow your train of thought, we can better determine whether or not you had any idea what you were doing in the first place. And as a last note, which is so basic it probably shouldn't need to be said, but in my last round of interviews *nobody* did this: Bring paper and a writing utensil. Yeah we have shit there for you to write on, but this shows such an utter lack of preparedness that it makes me not want to trust you with production environments that are a critical part of millions of dollars of revenue.
Hey thanks for the advice. I read up on alot of this before the interview and I did ok, they didn't go over this much but then again it is for a position straight out of college. They asked lots of behavioral questions and lightly touched almost everything you mentioned. Apparently if I get hired i will get alot of formal training and small tasks before I'm assigned my own projects so they told me in the end they were just making sure I was someone who could learn and think properly. I'm pretty sure I will get the offer. Thanks!
They had no MS-SQL related questions, they said it would be completely irrelevant. You learn the tools and the MS-SQL specific stuff on the job anyways. They just tested my foundation in db design and query formulation in general. They said they also provide lots of formal training on MS-SQL when you are hired so they don't fret about it.
Yea, I used the board like crazy. I did a few designs and some queries.
Good luck!
I can see it just fine.
I've been using Toad for a couple years now and it's a pretty comprehensive suite of tools, however I'm on Oracle. Just a thought. 
Just to follow up, I got the job! Tomorrow I have my meeting with HR to go over the hire process and stuff. Thanks a bunch!
Congrats man!
I liked the tutorials on SQL zoo. I'd post a link but I'm on my phone. 
What environment do you have access to? There are enough small differences between SQL variants for this to influence / restrict your decision regarding tutorials. Unless there's a reason to switch, you're probably best of sticking with whatever platform they use at your job. - SqlZoo has already been mentioned - [Link](http://sqlzoo.net/) - I've not looked at W3Schools SQL tutorials., but I like their web stuff since they assume you know absolutely bugger all. [Link](http://www.w3schools.com/sql/default.asp) Otherwise go for one specialising in your platform. The fundamentals of SQL are very simple - deceptively so in fact. If you're looking to fully *grok* SQL then it might be worth starting a little database project of your own - e.g. to catalogue your DVD collection or something. Most SQL books will have a sample database to work through and of course you should work through that first, but you may find it only clicks when you're working with information you properly understand/own. The sample databases which come with some SQL platforms (e.g. Microsoft's AdventureWorks) are a bit too complex / unwieldy for a beginner to grasp. Everyone learns in their own way of course, but personally, the most I learnt was from the monumental fuck ups I managed to achieve on my own. Final advice: Stay clear of MS Access if you want to understand SQL.
This may show my level of ignorance on this, but is the environment the software that I use? If so, it's a custom system for our client. The manual we have for the program doesn't go into any depth on actually using SQL. It only has a "Understanding a database concepts" section, which is quite basic and a chapter on how to use the ad hoc query tool. I like the suggestion about making my own database. I definitely mess around with that. &gt;There are enough small differences between SQL variants for this to &gt;influence / restrict your decision regarding tutorials. I don't fully understand the implications of this statement. I will be using SQL to write queries. Will the language/syntax (or the use of SQL) differ with each environment, if I'm fundamentally asking for the same operation? I do have a list of queries that are commonly used in my role that I copy and paste as I need them. So far what I've learned as been from screwing around with them and seeing what does what. 
Well basically there are a number of implementations of the SQL standard, by various companies. The major platforms are provided by : * Oracle * IBM (Informix or DB2) * Microsoft (SQL Server) * Sybase * MySQL (Free) * Postgres (Free) All the major platforms implement the same basic functionality but there's enough little differences to make switching between them slightly annoying. Core querying, e.g. SELECT * FROM table_name Will work on all systems. But for example, to only get 10 records in MS SQL Server it's: SELECT TOP 10 * FROM table_name While on MySQL it's: SELECT * FROM table_name LIMIT 10 On Informix it's: SELECT FIRST 10 * FROM table_name Most *basic* functionality is more similar than different but when it comes to things technically outside the SQL Standard (like loading data from files) then every platform has entirely different approaches which aren't even vaguely similar (syntactically). So, anyway....If it turns out that your system is running on Microsoft SQL Server then the easiest option for you would be to download the (free) SQL Server Express version from MS. There are demo/development versions available of the other platforms too so it doesn't really matter which it ends up being.
Ok, so I need to ask someone in the know here what environment or SQL standard our system uses. 
Or just find out yourself. On MSSQL for example you can run select @@version to see what's running. As for learning SQL, think of projects you could do, and try to complete them. Reporting/business intelligence is a great way to start, and may even produce useful results for you or your company.
I don't have permission for that function, but I did find out our system is based on SQL Anywhere. 
Then you're likely using Sybase :)
Make sure whatever drive you're installing on is set up as a shared cluster resource. EDIT: And isn't the Quorum drive. The C drive should also work I believe.
There are ISO standards for SQL that cross vendor boundaries. You will want to be familiar with: - ANSI transactional isolation levels - the concept of semaphores and locking - keep your transactions small in duration - Account for deadlock (not block, know the difference) errors. They happen. Nothing will stop them. Make it part of your application's state machine. - binary trees for indexes - the difference between clustered, nonclustered, and 'covering' indexes - use indexes whenever possible - JOIN syntax in queries - JOIN implementation algorithms such as nested loop, merge, and hash - how the sql implementation caches execution plans - SQL servers tend to suck at numerical computation do that elsewhere - stored procedures are your friend - connection pools are your friend If you can grasp those concepts, you will know what questions to ask and information to search for inside the vendor documentation. Figure out how to measure the performance of your statements. If you can't measure it, you can't determine whether your actions help or not. Logical reads good. Physical reads bad. Wait times for resources more than a few milliseconds are bad. 
http://www.amazon.com/SQL-Antipatterns-Programming-Pragmatic-Programmers/dp/1934356557/ref=pd_sim_b_3 http://www.amazon.com/Joe-Celkos-SQL-Smarties-Programming/dp/1558605762 http://www.amazon.com/Art-SQL-Stephane-Faroult/dp/0596008945/ref=pd_sim_b_4 http://www.amazon.com/SQL-Relational-Theory-Write-Accurate/dp/0596523068/ref=pd_sim_b_7 And then read a book on whichever dialect of SQL you're using, because they're pretty different.
I just don't get the warnings against Access - yes it has its problems, and you wouldn't want to build an enterprise system on it, but it does provide some tools that can help you grok how relational databases work. Just being able to build tables and relationships in a visual environment can be quite helpful. Are there any particular bad habits that one would pick up from Access that I'm overlooking?
Well I admit I am a little biased against GUIs in general and Access in particular but I don't think Access provides very good examples if you're learning SQL. The SQL that Access generates is quite poor and (IMHO) difficult to understand in some cases. It has also enough of it's own quirks to be annoying (e.g. the use of a different wildcard character, the fact that the distinction between views and procedures is somewhat obfuscated, etc). I agree that it does have a nice query editor which can help visualise relationships between tables, but I've seen people become very skilled with Access who in the end had gained very few transferable skills - they knew quite a lot about the Access event model, and how Access forms worked, but they had no idea about SQL or E/R modelling or whatnot.
&gt; Final advice: Stay clear of MS Access if you want to understand SQL. Same thing goes for MySQL. I would definitely suggest PostgreSQL if one wanted to learn *real* SQL, as it is really close to ANSI SQL. By the way, MySQL is not exactly free, you can check [this answer](http://stackoverflow.com/questions/225987/can-someone-explain-mysqls-license-and-what-it-means-to-closed-source-developmen).
Makes sense. I don't even think about Access in terms of the SQL it might generate, or really in terms of its Forms/Reports functionality. I just recall setting up tables and relationships with it, and it helping me to "get" relational database structures. 
sqlzoo.net has some good tutorials with awesome interactive quizzes. It got me up to speed with basic SQL queries in 2 or 3 days. On the other hand, actually *mastering* SQL took me many, many rewarding years.
Yes, start with sp_helptext [sproc name], then you can modify by using ALTER SPROC instead of CREATE SPROC. Pro Tip: I usually tie &lt;ctrl&gt;-1 to sp_helptext in management studio, then it is very quick to see the code behind a sproc or view.
Perfect! Exactly what I was looking for! Thanks a bunch!
As someone that did years of Access development I think it's much better if a programmer skips the training wheels and goes directly to writing SQL statements by hand instead of being reliant on the gui. It ends up becoming a crutch and I find that it is orders of magnitude slower if you do all your manipulation with a visual tool. There may also be certain SQL statements that can't be expressed using the GUI.
Holy crap, I've been using Query Analyzer and then Management Studio since 2005 (been using MSSQL since 6.5) and did not know about hot key script assignments. I know what I'm setting up tomorrow! Thanks.
Hey, COALESCE still kicks butt on Microsoft's playground, too. It's cleaner than nesting a bunch of calls to ISNULL() if you want to check for more than one field for NULL before defaulting in a value.
&gt;I'm trying to optimize this query, and I recently read that COUNT( * ) has to do a full table scan to get the number of rows of a table. This is highly dependent on what database you're using and only holds true for certain systems (like Postgres, for example) To answer your question, COUNT(*) is only going to be calculated once no matter how many times you use it in a single query. That's only one of hundreds of optimizations that occur in most RDBMS systems.
when the query is run, the optimizer realizes that it doesn't have to calculate count(*) on each set more than once.
A quick SHOWPLAN shows that SQL Server, at least, is smart enough only to do one scan on a query with multiple COUNT(*)s. |--Compute Scalar(DEFINE:([Expr1004]=[Expr1003]/Convert([Expr1002]), [Expr1005]=Convert([Expr1002])/[Expr1003])) |--Compute Scalar(DEFINE:([Expr1002]=Convert([Expr1015]), [Expr1003]=If ([Expr1016]=0) then NULL else [Expr1017])) |--Stream Aggregate(GROUP BY:([table1].[columnA]) DEFINE:([Expr1015]=Count(*), [Expr1016]=COUNT_BIG([table1].[columnC]), [Expr1017]=SUM([table1].[columnC]), [table1 |--Sort(ORDER BY:([table1].[columnA] ASC)) |--Clustered Index Scan(OBJECT:([databaseA].[dbo].[table1].[PK_table1]), WHERE:([table1].[columnB]='Aug 15 2011 12:00AM' AND [table1].[col 
 If you do count(*), count(*) it will index scan the primary key or a random field. This however can be an ambiguous operation. It is better to have a field name. The reason for this is that count(id), count(somethingelse) can return different results depending on the database engine + options. This happens because count(field) will return the total rows - null values.
But be warned that, at least in SQL Server, using the same user defined function more than once in a query WILL force it to run twice even if it shouldn't have to. At least in my experience.
Select count(1)
What kind of user-defined function are you using and what version of SQL Server? I'm a bit skeptical of your statement because of [this page](http://msdn.microsoft.com/en-us/library/aa175085(v=SQL.80\).aspx) in the MSDN manual that goes on in fair detail about how SQL Server decides whether or not your user defined functions are deterministic or not. If your user defined function is deterministic (that is given the same set of inputs, the function will *always* output the same result), then SQL Server knows it and will use it to optimize your query. It only has to calculate the value of MyUserDefinedFunction("MyArgument") once for the query and will re-use the returned value just like COUNT(*). EDIT: As the downvotes show, I'm completely wrong here. SQL Server does not cache the output of user defined functions. To do so would be reckless: maintaining the cache across tens of thousands of rows could easily consume more resources and CPU time than simply executing the function each time. SET SHOWPLAN_ALL ON SELECT ABS(1), SomeUserDefinedFunction(2) Check out the Computer Scalar step in row 2. The value of ABS(1) has already been evaluated at this point and the output shows up simply as "1" (int) but the second expression -- SomeUserDefinedFunction(2) -- still needs to be computed. The UDF will be evaluated each and every time, even when you re-use "SomeUserDefinedFunction" with the same arguments in the same query. If you *really* want to cache the output of your User Defined Function, consider using a table / temporary table / CTE for storing pre-calculated output for your UDF then join to it.
why not just declare a variable and assign it the value of count and use that throughout the rest of the query?
Generally DBs will get that right, but it's not actually guaranteed. Better to use a grouping function like avg(), which will have fewer problems with accumulated floating point error and which will be calculated rather faster.
That's what I was going to do, if the COUNT( * ) function did more than one full table scan.
Why would a user-defined function in the SELECT clause be a bad idea if it's well-written?
You don't really need the "ORDER BY rownum" clause in the outer query.
Nice addition, but I never use the GUI for backups and restores - I've always used the raw SQL commands. /elitistsqlnerd 
It's about the function call overhead. The UDF will be called for every row so you can imagine a result set of just 1000 rows will call that function 1000 times. The overhead of 1000 function calls is extremely high. SQL is optimized for set operations and UDFs don't operate on sets (until MSSQL 2008, at least). Forcing a set operation (temp table or inline code) makes for a much faster execution. (caveat: Oracle is much better at row by row operations and as such doesn't see as much degradation from UDF calls. That said, the function call overhead is still pretty high and should be avoided if possible)
It's about the function call overhead. The UDF will be called for every row so you can imagine a result set of just 1000 rows will call that function 1000 times. The overhead of 1000 function calls is extremely high. SQL is optimized for set operations and UDFs don't operate on sets (until MSSQL 2008, at least). Forcing a set operation (temp table or inline code) makes for a much faster execution. (caveat: Oracle is much better at row by row operations and as such doesn't see as much degradation from UDF calls. That said, the function call overhead is still pretty high and should be avoided if possible)
Thanks. I was aware of the overhead, but was wondering if there were other limitations.
 SELECT c.car , m.model_id , m.model FROM models AS m INNER JOIN cars AS c ON c.car_id = m.car_id WHERE c.car_id = (SELECT car_id FROM models WHERE model = 'Mustang')
Thanks, this is similar to an answer I finally got on the mysql subreddit [here](http://www.reddit.com/r/mysql/comments/johiz/query_help_returning_all_values_in_another_table/c2dxmct). Thank you for your help :) **edit:** I see your saw it too, haha!
Look into the [union operator](http://www.w3schools.com/sql/sql_union.asp). You will need to make sure both tables are returning like data (same number of columns, etc), but the query would essentially be: SELECT * FROM cats WHERE cat = "black" UNION SELECT * FROM dogs WHERE dog = "brown" 
Why cant you say select * from table where cat = black and dog = brown? Do you need two seperate reports and such? Whats the objective of the data fetch? 
It's two separate tables.
UNION will work, but if the two tables have the same columns (as is required for UNION), for data that similar I would recommend combining the two tables.
Ah ok. So it this a part of an automated process? And is this mssql?
Plus it would be OR ... I daresay cat = black and dog = brown will have zero results.
well, if it's two fields (cat, and dog) in the same table, then there might at least be 1 record with cat = black and dog = brown.
 ^-- This
 If you are using mssql and c# you can do that and it will return 2 tables in the dataset. 
So many times ive had to look up how to kill sessions when i detach or bring down databases. The only thing that makes me feel any better is "if this was easier, maybe I'd be out of a job" :)
As it is, the OP's query will generate a syntax error; you can't use AND that way.
Integration with SharePoint 2010 products. Didn't we have this before? What did RS do with sharepoint before? I'm confused :)
select * from models m, cars c where m.car_id = c.car_id where model like '%modelName%'
Yeah, I actually thought the same thing when I saw that bullet. We don't use SharePoint very heavily here though, so I didn't know for sure.
This article by the customer advisory team for SQL disagrees. The code is in the article so you can test it for yourself. http://sqlcat.com/sqlcat/b/msdnmirror/archive/2011/06/24/unintended-consequences-of-scalar-valued-user-defined-functions.aspx Also, the MSDN page you linked is from SQL 2000. This article is dated June 2011
GetDate() is considered non-deterministic, so SQL Server treats the UDF wrapped around GetDate() as non-deterministic, therefore the UDF gets called for each and every row. The MSDN page I linked to (for SQL Server 2000) is not very different from [this one](http://msdn.microsoft.com/en-us/library/ms178091.aspx) for 2008. EDIT: I'm wrong. Don't listen to strangers on Reddit.
SQL Server 2008 SP2 has actually been available since September 2010 - Microsoft are, for some reason, displaying that it has been published since 22 August 2011. Perhaps more interesting is that the CTP for SQL Server 2008 SP3 was released on 22 August 2011 (available [here](http://www.microsoft.com/download/en/details.aspx?id=27150), but I wouldn't recommend applying it to any servers yet). A quick glance over the [release notes](http://support.microsoft.com/kb/2546945) doesn't result in anything too exciting.
Diff between UNION and UNION ALL: UNION will return only unique rows, so requires more work UNION ALL won't take the unique-ifying step, so may return faster (depends on data volume)
If 'select * from v$version' returns anything, you're on ORACLE. 
Holy crap! I might not be reading these any time soon but I'm definitely going to stash these away. Thanks for the link!
Are any of them good? And FWIW, 8 of 10 are about SQL Server.
glancing at those they all look like theyre from sqlservercentral.com free 365 days a year. ive only read the one on sql server execution plans.....it was good.
Why would you group by the aggregate function SUM() ?
I created some test tables that covered the columns in the query and the index structure. It executes on a SQL Express 2008 R2 instance just fine, though I only created like...1 row of each. There is some usage of reserved words in the query. [Datetime] for example. The biggest change I made was to actually give each table a declared name rather than trying to use its two-point name. And I agree with DrHankPym, I don't think that will get what you're looking for. Good luck. USE [Sandbox] GO DECLARE @StartDate DateTime DECLARE @EndDate DateTime SET @StartDate = DATEADD(d, DATEDIFF(d, 0, DATEADD(m, -3, DATEADD(d, 1 - day(getdate()), getdate()))), 0) SET @EndDate = DATEADD(ms, -2,DATEADD(d, DATEDIFF(d, 0, DATEADD(d, 1 - day(getdate()), getdate())), 0)) set nocount on IF object_id('tempdb..#tmpJoin') IS NOT NULL BEGIN drop table #tmpJoin END create table #tmpJoin (fromDate datetime, toDate datetime) insert into #tmpJoin values(@StartDate,@EndDate) set nocount off SELECT n.NodeID, i.Comments AS Comments, n.Caption AS NodeName, i.InBandwidth AS Recv_Bandwidth, Maxbps_In95, Maxbps_Out95, Maxbps_95, SUM(t.In_TotalBytes) AS Total_Bytes FROM dbo.[Nodes] n INNER JOIN dbo.[Interfaces] i ON (n.NodeID = i.NodeID) INNER JOIN InterfaceTraffic t ON (i.InterfaceID = t.InterfaceID) INNER JOIN ( SELECT AA.InterfaceID, dbo.GetInBps95th(AA.InterfaceID, @StartDate, @EndDate) AS Maxbps_In95 FROM ( SELECT DISTINCT A.InterfaceID FROM dbo.InterfaceTraffic A INNER JOIN #tmpJoin ON A.[DateTime] &gt;= #tmpJoin.fromDate AND A.[DateTime] &lt;= #tmpJoin.toDate ) AS AA ) as RESULT_IN ON (i.InterfaceID = RESULT_IN.InterfaceID) INNER JOIN ( SELECT InterfaceID, dbo.GetOutBps95th(AA.InterfaceID, @StartDate, @EndDate) AS Maxbps_Out95 FROM ( SELECT DISTINCT A.InterfaceID FROM dbo.InterfaceTraffic A INNER JOIN #tmpJoin ON A.[DateTime] &gt;= #tmpJoin.fromDate AND A.[DateTime] &lt;= #tmpJoin.toDate ) AS AA ) as RESULT_OUT ON (i.InterfaceID = RESULT_OUT.InterfaceID) INNER JOIN ( SELECT InterfaceID, dbo.GetMaxBps95th(AA.InterfaceID, @StartDate, @EndDate) AS Maxbps_95 FROM ( SELECT DISTINCT A.InterfaceID FROM dbo.InterfaceTraffic A INNER JOIN #tmpJoin ON A.[DateTime] &gt;= #tmpJoin.fromDate AND A.[DateTime] &lt;= #tmpJoin.toDate ) AS AA ) as RESULT_MAX ON (i.InterfaceID = RESULT_MAX.InterfaceID) WHERE ( (n.IP_Address = 'censored') AND (i.InterfaceAlias LIKE '%Telus%')) OR ( (n.IP_Address = 'censored') AND (i.WANFeed = 'BI')) OR ( (n.IP_Address = 'censored') AND (i.WANFeed = 'BI')) OR ( (n.IP_Address = 'censored') AND (i.WANFeed = 'BI')) GROUP BY n.NodeID, i.Comments, n.Caption, i.InBandwidth, Maxbps_In95, Maxbps_Out95, Maxbps_95 ORDER BY MAXbps_95 DESC Drop table #tmpJoin
This is great! Thanks! I ran it as is with the IP_address field = 'censored' because I forgot to change it and the server didn't stall out / no errors when queried. oddly, when I put the proper IP addresses in, the server stalls out and I'm going to have to check SQL manager for any errors when I try to run it through there. I'll update this message shortly.
What values are you using for GameID the first and second time you run the inserts?
So from this I can't really tell what's causing the issue, but I do have a few suggestions: 1) Is it possible that there is some data already in the table when you are doing the insertion? 2) I'm not familiar with errors of this format - what database are you using? It's interesting that the error message seems to be referencing a unique index on the "GameOver" field. Can you successfully insert the second record if you change GameOver from false to true in the second insertion? Depending on what database you are using, perhaps there is also a command to find out the exact definition of the index named "CONSTRAINT_INDEX_3EF" 3) Assuming the fields in your table are NULLABLE by default (since you're not specifying NOT NULL), what happens when you try to run the 2 inserts with JUST GameID included. Then truncate the table and try with GameID and NextTurn. Then truncate and do GameID, NextTurn and Team1 - perhaps that can help us narrow down the source of the problem. Try these things and get back to me and we can figure this out.
It looks to me like you're winding up with a UNIQUE constraint on the GameOver field somehow. Since it has a default value (false), it makes sense that you weren't able to add a second record with just GameID - as it was attempting to insert a second "false" value for that field - and that violates the constraint. What happens if you drop and recreate the table and DON'T create the first entry using only the gameID? Do you then run into the problem again? Or now that you've dropped and recreated the table even once, you don't hit the problem at all any more? Did you create the table differently the first time than the second time? If you do run into the problem again, I'd try running a query such as this one: ALTER TABLE tblgames DROP CONSTRAINT CONSTRAINT_INDEX_3EF or possibly it would just be DROP INDEX CONSTRAINT_INDEX_3EF I'm getting the "CONSTRAINT_INDEX_3EF" part from your error message - it's possible that if you drop and recreate the table, the constraint could be named differently.
Why don't you move you CASE statement down somewhere in the WHERE clause? That way you can eliminate those records from being "seen" by the UPDATE statement at all.
MyColumn = CASE WHEN MyColumn = 2 THEN 4 ELSE MyColumn END Though as suggested above a Where clause would be better and more efficient
Something else to know about CASE statements: The engine will exit evaluations when it finds the first true condition. You want to be doing as few evaluations as possible for large sets. So, if you estimate that there are 60% 3s, 20% 1s, 10% 2s, and the remaining 10% are other numbers the order of your CASE should look like Case when mycolumn = 3 then blah when mycolumn = 1 then blah when mycolumn = 2 then blah else blah. 
Either filter and omit the else, or say ELSE mycolumn
if one omits ELSE, won't the value be set to null?
I think what he's saying is to filter in the WHERE so that there isn't a need for an ELSE, then drop the else. Otherwise, you are correct, leaving an ELSE off would set the column to NULL if one of the cases didn't match.
There's nothing at all wrong with using two queries when you need two separate data sets. You don't need to explicitly create temporary tables where subselects (also known as derived tables) will do, however. select bp_subtotals.customer_name, bp_subtotals.invoice_type, bp_subtotals.sub_total, bp_totals.company_total from ( select customer_name, invoice_type, SUM(quantity * unit_price) as sub_total from billing_processed group by customer_name, invoice_type ) as bp_subtotals left join ( select customer_name, SUM(quantity * unit_price) as company_total from billing_processed group by customer_name ) as bp_totals on bp_subtotals.customer_name = bp_totals.customer_name 
I would add a TIMESTAMP, and use that. In not sure what you mean: 'without having to do a separate select query?' can you elaborate?
I use Top 1 in the select line and then use Order By 'Primary Key' Desc
Use 0.1 for both instead of zero and then you can round the results down do that the final value is 0.
Use Where 'Value' != 'Whatever you put into the Else'
I changed to .1 from 0. I'm still getting "missing keyword." I changed from IF to CASE like below, but still have the "missing keyword" errors. CASE WHEN SUM (CASE WHEN (wo_age &lt;= '90' AND ipd BETWEEN '09' AND '15' AND prj = '9GQ') THEN 1 ELSE 0) = '0' THEN .1 ELSE SUM (CASE WHEN (wo_age &lt;= '90' AND ipd BETWEEN '09' AND '15' AND prj = '9GQ') THEN 1 ELSE 0 END) / SUM (CASE WHEN (ipd BETWEEN '09' AND '15' AND prj = '9GQ') THEN 1 ELSE 0 END) END reset_mpd3
I don't see Select or From in your query you posted. 
Trimmed out for brevity. SELECT dol_uic, instl, CASE WHEN SUM (CASE WHEN (wo_age &lt;= '90' AND ipd BETWEEN '09' AND '15' AND prj = '9GQ') THEN 1 ELSE 0) = '0' THEN .1 ELSE SUM (CASE WHEN (wo_age &lt;= '90' AND ipd BETWEEN '09' AND '15' AND prj = '9GQ') THEN 1 ELSE 0 END) / SUM (CASE WHEN (ipd BETWEEN '09' AND '15' AND prj = '9GQ') THEN 1 ELSE 0 END) END reset_mpd3 FROM mgt0.vw_Dol_Clsd_Wo GROUP BY dol_uic, instl ORDER BY dol_uic, instl If I comment everything out and just leave the first two fields it works perfectly.
Yes, when you do an insert, look at the RETURNING clause. http://www.postgresql.org/docs/9.0/interactive/sql-insert.html
Seems like you shouldn't need SUM's or IF's... reset_mpd1 =(SELECT CASE WHEN wo_age &lt;= '30' AND ipd BETWEEN '01' AND '03' AND prj = '9GQ' THEN 1 WHEN ipd NOT BETWEEN '01' AND '03' OR prj &lt;&gt; 'PGQ' THEN 0 ELSE NULL END)
You're right that without the ELSE, it would be null, but it wouldn't matter it you're excluding those records with a filter. 
Is this MSSQL, Orace, MySQL? All of them?
what I usually do for those, is wrap the divisor with **nullIf(..., 0)**, then the whole thing with **isNull(..., 0)** anything divided by NULL is NULL much easier than big ugly IF or CASE statements
MSSQL specifically. I don't know if the rest of them work this way or not. 
In vanilla SQL, no. Almost all SQL dialects have an extension for this - PVH gave you the Postgres extension (operator RETURNING) and rushoffailure gave you the MsSQL extension (operator OUTPUT.) Oracle has RETURNING. MySQL and SqLITE don't implement an extension like this. If you want portable SQL, the answer is "no." If you want single-server SQL, the answer is "it's down to which dialect you're writing; probably yes." The vanilla approach generally is to do something in a function or a trigger. For the record, I've been burnt by extensions too many times, and *****I recommend sticking to ANSI SQL*****. The conveneinces don't stay convenient, and making changes in the original queries in your SQL is often exceptionally difficult 
He means he wants this in one query: insert into foo(bar) values(4); select magic_fetch_last_row();
It took me a while to figure out what you were trying to do. Next time, it'll help if you mention what your goal is, instead of just showing a broken query. (Indeed, what you're actually generating is bizarre, and I'm worried I "figured out" a bug, rather than your real goal.) It appears you're trying to divide the count of people satisfying some age criterion and two very bad project criteria by the total count of people satisfying those project criteria, to get the ratio of young people in situation X. Something like this (not tested because there's no data model) should be what you want. No real SQL engine will scan twice; they're aggregates. select count(agebounded.*) as count_with_age, count(unbounded.*) as total_count, (case when total_count = 0 then 'Empty Set' else (count_with_age / total_count) ) as youth_ratio from (select prj from real_table where wo_age &lt;= 30 and ipd between '01' and '03' and prj='9GQ') as agebounded, (select prj from real_table where ipd between '01' and '03' and prj='9GQ') as unbounded; 
this. or do another query which runs select * from table where PK = @@IDENTITY if the PK is used as identity.
Please write 'Race Condition' one million times on the blackboard;)
Thanks! Unfortunately (on both counts) I'm using mysql and didn't state that in my question.
Thanks! Unfortunately (on both counts) I'm using mysql and didn't state that in my question.
Thanks! Unfortunately (on both counts) I'm using mysql and didn't state that in my question.
If it helps, last_insert_id() will always be yours within a transaction. That way you're safe if: 1) You insert a row about violins 2) I insert a row about violence 3) You ask for the last row. In circumstances where you're using last IDs, unless you want potentially someone else's insert, *always* *work* *inside* *a* *transaction*. Then again ... that's true of most things in SQL.
 You filter by a where to remove that problem from your results before doing the calculation eg where x - y != 0 You then union The filtered results to your output with an ajusted calculation not causing a divide by zero
Easy enough to test: Do something like Select Case when 1 = 1 then 'one When 3=3 then 'three' Else 'none' See what that returns for you 
Forehead slap...of course :)
Of course, I'd test it out against a decently largish data set. But I can't think of any reason why they wouldn't pare the branches. 
I know that this is an old post, but you might be able to give me some input so here goes: Right now I work as an accountant. I've realized that I don't actually like accounting and that the only parts of my job that I thoroughly enjoy are building dashboards and reports in Excel. (I know some VBA, but I'm learning more all the time) I realize this isn't exactly database work, but are there similarities? Do people without Comp Sci degrees ever get hired on in these roles? Specifically business intelligence? I think I could learn the technical stuff on my own, but I don't know if it's even worth it if I wouldn't get looked at for an interview. Any suggestions for breaking in from a different background?
Well, if it boosts your confidence any, I not only don't have a Comp Sci background - I don't have a degree at all. I went to school for 2 years for Elementary Education, realized I actually really dislike kids, and decided to take some time off to decide what to do. A part-time summer job quickly turned into a full blown IT career, and here I am today. Anyways, what you're doing in Excel has little (something, but not much) to do with databases, and can actually enforce some really bad habits. At the very least, start playing with Access. In my opinion, it's something of a bridge between Excel and real databases. Your Excel knowledge, however, IS highly relevant to the BI field. Though Excel is total rubbish for storing data, it's still one of the best tools available for analyzing data - it's just a matter of hooking Excel into your database either directly, via scheduled exports, or tools like PowerPivot. If you want my advice on how to start that career move, here's what I'd suggest, and would really make an applicant stand out to me: * Like I said, start using Access. If for nothing else, than to start thinking relationally. Spend time designing your Access database(s) *right* - look up normalization, data warehousing, etc. * Start reading BI blogs - since it doesn't sound like you want to be a DBA, focus on BI delivery, not data warehousing. I know I just told you to look into it, and it's still important to know, as you're going to need to understand *what* you're querying, but you needn't get caught up in the arguments of ROLAP/MOLAP/HOLAP. * Pick a mainstream BI tool, get your hands on a copy, and use it. If you can get your employer to let you do this, awesome. If your company is using Cognos, Crystal Reports, Reporting Services, or anything else along those lines, go chat up your DBA, see if you can start doing some reporting in your down time. If it's not an option at work, get your hands on a copy somehow. SQL Server 2008 R2 Developer edition is $40 @ Amazon. And since you're here on Reddit, I'm sure you're also smart enough to find copies for free if you're feeling overly cheap. * Once you're using a BI tool at home, try to make something useful. Are you dieting? Create a report to track your progress. Make a basic budgeting tool. Shit, what I did when I first started with databases was to make a tool to better track my achievement progress on Xbox 360. Anything in your life that involves quantifiable, organizable information you could conceivably shove into a database and report off of. Once you're comfortable, and have a disc loaded with a database file, report definition files, start looking for jobs. Use Dice, Monster, etc. Be sure to put up a phone #. At first, take any interview offered to you directly from a company (*not a recruiter*), just for the experience. I suggest avoiding recruiters because you'll end up burning bridges when they realize that you're not qualified - so stick with individual companies. I'm ~~a little~~ intoxicated at the moment, so sorry if this is a little rambly. Feel free to PM me if you have any more specific questions!
Thanks so much for this. It's incredibly helpful. 
Wait. What?
What is the difference between a subselect and a temporary table?
 SQL&gt; desc simple_table; Name Type Nullable Default Comments ---- ------- -------- ------- -------- A INTEGER B INTEGER C INTEGER SQL&gt; select * from simple_table where c = (select c from simple_table where a = 1) A B C 1 1 3 3 2 3 With a UNIQUE INDEX on A and an INDEX on C, this should be fast and cheap. I added a ton of junk rows to the table and EXPLAIN PLAN didn't show a rising cost, but perhaps I'm misusing EXPLAIN PLAN.
 SELECT A,B,C FROM ABC where C = (select C from ABC where a = 1); There's no optimization required, there is only 1 value in column a = 1, if its indexed and the table statistics are up to date the subquery will not cause a full tablescan, neither will the main query if C is indexed. It doesn't matter how large your table is. Assuming all values in A are unique, there's no optimization tricks needed. 
&gt;the table involved is very large Define "very large," please. We have tables in our warehouse that are &gt; 2 billion rows and we use sub-selects daily. &gt;as few table scans as possible If you're indexed correctly, you won't do a table scan. Also, if you are using Oracle, look into partitioning the table so you can prune at a date or range level. 
A subselect (subquery?) is like: SELECT rec FROM table1 WHERE ColumnX in (SELECT ColumnX FROM Table2 WHERE ColumnY = 'value') &lt;-- subselect A temporary table is an actual table that is more persistent (?) than the results of a subquery.
I'm really uncertain as to how the "A = 1" relates to the results you want, given that A = 3 on the second record. Maybe I'm just reading it wrong?
[?](http://sqlblog.com/blogs/aaron_bertrand/archive/2011/01/14/sql-server-v-next-denali-additional-states-for-error-18456.aspx)
Could be all sorts of issues with the domain account for that user or computer. Try resetting the password or removing &amp; recreating the domain account / membership.
&gt;Is there any way to make this query do "manual_index.ordinal = (just the lowest one)"? Or do I need to break this into two queries - one on the manual_index that JUST returns the first row ORDERed BY ordinal ASC, then run the second query off the id I get from that (which would probably work, but is using two queries for something I bet can be done with one)? Sorta. I'd use a derived table. Check to make sure you have an index with ClientID, ordinal on manual_index. SELECT CONVERT(TEXT,content),title FROM manual_content INNER JOIN manual_index ON manual_index.id=manual_content.field_id Inner join (Select Client_Id, min(ordinal) as MinOrdinal from Manual_Index Group By Client_ID) as MI On MI.Client_ID = Manual_Index.Client_ID AND MI.MinOrdinal = manual_Index.Ordinal WHERE manual_index.client_id = [the client ID number] ORDER BY manual_content.page_index ASC
Maybe something like this: SELECT CONVERT(TEXT,content), title FROM manual_content INNER JOIN manual_index ON manual_index.id=manual_content.field_id WHERE manual_index.client_id = the client ID number AND manual_index.ordinal = (SELECT MIN(ordinal) FROM manual_index WHERE client_id = the client ID) ORDER BY manual_content.page_index ASC 
See, these are the kind of things you don't learn when you're a total hack. I didn't even know nested queries were a thing. [I'll just leave this here](http://static.rateyourmusic.com/lk/f/a/f0b8777daa18d8341b30c5ccd7317bcb/3282407.jpg)
Somebody else posted something similar (TIL about nested queries!), but this is what I went with, since putting it in the WHERE seems more logical to me. Upvotes for everybody!
We've all been there. I used to crush my head trying to figure out how to do aggregates in the same query as regular queries as well. 
Its all about the status value look on repltalk or sql protocols They both have the list
&gt;Now, given A = 1, write a query that returns the following result set: Should really have been: &gt; Write a query that returns a,b,c for all rows where C is the value of C where A is 1 (occurs only once in the dataset) Ok that was confusing too.
Q1 : Select * from ABC Where C in ( Select x.C from ABC x where x.A=1) Q2 : Select * from ABC Where C = ( Select x.C from ABC x where x.A=1) Q3 : Select x.* from ABC x inner join ABC y on y.C = x.C WHERE y.A=1 **Note** : I would stick to the query (on server memory/cpu cost) rather than touching more IO processes like temporary tables .. unless you run out of memory to compile the huge result. I mean why would you use temp tables than subqueries ??? I don't get this idea actually. Comparison : http://imgur.com/9U4vJ If I were you I might start thinking about horizontal partitioning.
The subselect should be as fast as anything, but here is an alternate method: SELECT * FROM Table T1 INNER JOIN Table T2 ON T1.C = T2.C WHERE T1.A = 1 No matter how you do this, optimizing the query will likely be a matter of indexing properly, probably adding an index to columns A and C. Hope you get an A on your homework. ;)
temp table will cost you more IO which in many cases not worth to have it. Instead OP should have manual partitioned views on the table.
 SELECT s.*,prev.rank AS prevrank,prev.time_retrieved AS prev_time,prev.rank-s.rank AS moved FROM `serps` s INNER JOIN serps prev ON s.keyword_id=prev.keyword_id AND s.search_engine_id=prev.search_engine_id AND s.time_retrieved &gt; prev.time_retrieved WHERE s.search_engine_id IN(1,2) GROUP BY s.keyword_id,s.search_engine_id ORDER BY s.time_retrieved,s1.time_retrieved DESC If there were 5 rows for the same search_engine_id and keyword_id, all with different dates, wouldn't this pull in something like 10 rows by a cartesian product on the time_retrieved values? by way of example... if the time_retrieved values were 1 2 3 4 5 for a single engine/keyword pair, then I think you'd get... serps.time_retrieved | prev.time_retrieved | 2 1 3 2 3 1 4 1 4 2 4 3 5 1 5 2 5 3 5 4 right? So only in the case where there are just 2 instances of a keyword/engine pair would the given query do what you want. I'm no DBA, but my guess would be that you'd want an index on the engine_id/keyword_id combo, and then have a function that takes an engine_id and keyword_id and a time and finds the prior time_retrieved for that combo, effectively. function getPriorTime(p_search_engine_id IN NUMBER, p_keyword_id IN NUMBER, p_time_retrieved IN &lt;timetype&gt;) return &lt;timetype&gt; IS priorTime &lt;timetype&gt; begin select max(time_retrieved) into priorTime from serps where search_engine_id = p_search_engine_id and keyword_id = p_keyword_id and time_retrieved &lt; p_time_retrieved; return priorTime; end; Now you run your query like so: SELECT s.*,prev.rank AS prevrank, prev.time_retrieved AS prev_time, prev.rank-s.rank AS moved FROM `serps` s INNER JOIN serps prev ON prev.keyword_id = s.keyword_id AND prev.search_engine_id = s.search_engine_id AND prev.time_retrieved = getPriorTime(s.search_engine_id, s.keyword_id, s.time_retrieved) WHERE s.search_engine_id IN(1,2) AND s.time_retrieved = (SELECT MAX(time_retrieved) from serps s2 where s2.search_engine_id = s.search_engine_id and s2.keyword_id = s.keyword_id) GROUP BY s.keyword_id,s.search_engine_id ORDER BY s.time_retrieved,s1.time_retrieved DESC; Without good indexes, this will run like crap thanks to the embedded subquery and the function which is effectively another subquery. With indexes, those should both be fast lookups, although they would get slower as you gained more time data for a given engine/keyword pair. I'd rather have a single table with just the 2 most recent values for always-fast queries unless you really want history on something like this.
To answer the first part of your question: no, that's what the GROUP BY s.keyword_id is for - it limits it to one pair (the most recent pair) per keyword. I'll have to look into the subqueries though, that is a different angle I hadn't though about.
I'd use a common table expression to get a set containing the top 2 rankings for each search_engine_id. You should write the query in all ways you can figure out that produces the correct results and then look at the execution plan for each of them.
Essentially, I'm looking for duplicate records for the same item.
 select serial_number , count(distinct item_id) from item_list group by serial_number having count(distinct item_id) &gt; 1 order by 2 desc you will normally have the most represented serial number and only the one with at least 2 elements. and if you want the list of the item simply include the subquery in a clause where without the count ... and serial_number in (/*previous query*/) 
You could do a self-join... (note: I'm a T-SQL guy, so the syntax may not be exactly the same; you should be able to get the gist, though.) Select a.[Serial Number], a.[Item ID], a.[Item Name], b.[Item ID], b.[Item Name] From [Item List] a Join [Item List] b On a.[Serial Number] = b.[Serial Number] And a.[Item ID] &lt;&gt; b.[Item ID] Order by 1, 2, 4
I can't figure out how to make the group by do that... I went ahead and created a table to try some queries out and I can't get the GROUP BY to cooperate with the selected fields as shown. FWIW, I'm running on an Oracle 10g database.
I think you mean &gt; group by serial_number instead of &gt; group by item_list but otherwise, looks right to me.
yep exactly, i mistaken with the from .. edited ...
T-sql again.. erm.. probably wont work with Sybase or SQL anywhere. Hrm. Anyway - here is our way to de-dupe. Very flexible. WITH dupes AS ( SELECT row_number() OVER ( PARTITION BY serial_number ORDER BY item_name, item_id ) AS rn ,* FROM item_list ) SELECT * FROM dupes WHERE rn &gt; 1 
That's a fun and very textbook question. You'll want to use multiple CASE statements. The easier of the two will be the CASE statements against HomeTeamScore vs AwayTeamScore for deciding whether to award 3, 1 or 0 points for wins, ties and losses. The more difficult CASE statement will happen around the Schedule &amp; Results tables, where you will need to sort out who is HomeTeam and who is AwayTeam.
Yes, I think you are on a good track by considering a case statement. It sounds like this is schoolwork, so I can't write the query for you, but I would imagine you could do this with a case statement to determine whether the team won or tied the game and then a SUM() statement to total the team's score. Try to write something as best as you can and if it is not working write back and I can try to help you with issues you encounter.
A case statement is not necessary. Use a subquery to get wins at home, then one for wins away, and another for ties. Then add the wins together in the outer query, multiply by 3, and add in the ties for your final points. 
Oh, say that, then. I don't speak sybase, so you may have to jigger this a bit, but: select l.id as 'ID 1', r.id as 'ID 2', l.serial from table as a join table as b on a.serial = b.serial where a.id &lt; b.id; The idea is to show every pairing that isn't a self-collision, and only once. Self collisions are `id`=`id`, and you can get only-once by asserting {3,8} instead of {8,3}, so insisting that one's ID is always larger than the other's neatly ties it in a bow. mysql&gt; create table WheresDuplicateWaldo(id integer, serial integer); Query OK, 0 rows affected (0.10 sec) mysql&gt; insert into WheresDuplicateWaldo(id, serial) values(1,1), (2,2), (3,1), (4,4), (5,2), (6,6), (7,2); Query OK, 7 rows affected (0.03 sec) Records: 7 Duplicates: 0 Warnings: 0 mysql&gt; select l.id as 'ID 1', r.id as 'ID 2', l.serial from WheresDuplicateWaldo as l join WheresDuplicateWaldo as r on l.serial = r.serial where l.id &lt; r.id; +------+------+--------+ | ID 1 | ID 2 | serial | +------+------+--------+ | 1 | 3 | 1 | | 2 | 5 | 2 | | 2 | 7 | 2 | | 5 | 7 | 2 | +------+------+--------+ 4 rows in set (0.00 sec) Notice that not only does it find every affected serial, but it also finds every affected pairing. This may or may not be desirable: if you have four colliding IDs for a serial, it'll report six rows (ab, ac, ad, bc, bd, cd). You could reduce that with a roll-up function on the serial reporting the first instance and a concatenated series with the rest, but some SQL engines have length limit problems and so on. YMMV.
Ok, thanks for the input. I need to be able to pull other columns as well, so I can compare the equipment at the same time. The self join mrdelayer suggested seems to be more of what I'm looking for.
I don't think you can set the path to the configuration itself to an expression within the package so that this expression is resolved at runtime but before the configurations are loaded (configs are loaded first).
Thanks, this helps a lot. I'm able to add more columns of the item information so that allows me to compare the possible duplicate records. I'm dealing with a very large amount and variety of equipment (100,000s of separate pieces). Thanks, again.
break it down: you already have a list of salary grades and you can narrow it down to Fisher's grade with another statement in the WHERE clause. Therefore: SELECT last_name, grade_id FROM demo.employee e, demo.salary_grade s WHERE e.salary between s.lower_bound and s.upper_bound AND grade_id IN ( SELECT grade_id FROM demo.employee e WHERE e.last_name = 'FISHER' ) ORDER by grade_id desc;
Apart from the *, this works in DB2
For the love of sanity, can anyone explain to me what the benefits are to dynamically configured ports?
I've tried something like this before, and when I just tried what you offered, I got the same results as I listed above. It pulled all of the names, instead of those in the same salary grade as Fisher. I appreciate the help, though! Class is in a few hours, so I may just end up consulting the professor about it. :)
yup, pretty straight forward. Use SQL for set-handling and clr for anything else that you need to run fast in your environment. clr can be used to speed up your most used scalar funcions and regular expressions is where they really shine. 
It's not often we get graphs in r/SQL. And those are some nice graphs.
Thanks! There'd be more data points, but I couldn't be bothered waiting around to collect more data.
Thanks Mate can never have to much info.
Select * From booking Where startdate between ('10-1-2011' and '11-2-2011') And enddate between ('10-1-2011' and '11-2-2011') You have to add a day to the end date since it's exclusive. Next time at least try. 
Surely that will just return bookings that fall between the criteria dates? I probably haven't explained myself well enough: what I need is the list shown in the second table above. That corresponds to the blocks labelled "Available" in the picture. They don't actually exist anywhere, they're just the gaps between the bookings and the criteria dates – I put them there simply to illustrate the data I need returning. &gt;Next time at least try. Believe me, I've researched hard to try and work out how to do this. I just didn't come up with anything that helped me work out where to even begin. I know some basic SQL, like the query you've posted, but I'm a bit out of my depth when it gets more complicated.
So you want to find ranges that are not already booked?
Yes. The "Available" blocks in yellow don't exist, they just represent the date ranges I need returning.
I am trying to learn too due to job duties. I got this book on Kindle and it's been great so far. There are 5 in all. [Beginning SQL Joes 2 Pros](http://www.amazon.com/Beginning-SQL-Joes-Pros-Hands-/dp/143925317X/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1317171220&amp;sr=1-1)
Great question. This is known as a [gaps and islands](http://www.google.com.au/search?q=gaps+and+islands+sql) query. Here's an example I knocked up for SQL Server. This should all work in Access (except for the temporary tables - you might have to use real ones). It only considers a single slot for simplicity's sake. You can either run the code in a function for each slot, complicate the SQL to make it work with multiple slots, or search for sample gaps and islands code that handles multiple groupings. Compared to your sample values, there's a couple of off-by-one issues with the dates - mostly because your "end date" in your Bookings table is classified as the first "available date" in your requested output. -- Create a numbers table SET NOCOUNT ON CREATE TABLE #nums (n INT NOT NULL PRIMARY KEY) GO DECLARE @rows INT SET @rows = 10000 -- prime the table INSERT INTO #nums VALUES (1) -- loop around while rows are being inserted WHILE @@rowcount &gt; 0 BEGIN INSERT #nums SELECT t.n + x.MaxRowNum FROM #nums t CROSS JOIN (SELECT MAX(n) MaxRowNum FROM #nums) x WHERE t.n &lt;= @rows - x.MaxRowNum END GO CREATE TABLE #Booking (ID int, Slot int, BookingStart datetime, BookingEnd datetime) INSERT INTO #Booking (ID, Slot, BookingStart, BookingEnd) VALUES (1, 1, '2011-10-16', '2011-10-25') INSERT INTO #Booking (ID, Slot, BookingStart, BookingEnd) VALUES (2, 1, '2011-10-31', '2011-11-17') -- Let's look at our tables SELECT * FROM #nums -- numbers, 1-10000 SELECT CONVERT(datetime, '2010-12-31') +n AS d FROM #nums -- Dates 2011-01-01 through to 2038-05-18 (10,000 days) SELECT * FROM #Booking -- Two bookings, both for slot 1 SELECT ID, BookingStart, BookingEnd, d FROM #Booking b JOIN (SELECT CONVERT(datetime, '2010-12-31') +n AS d FROM #nums) dates ON dates.d BETWEEN b.BookingStart AND b.BookingEnd -- This returns a row for each date in your range -- We'll put this into a temporary table - Access might be a bit limited here, but you'll find a way SELECT ID, BookingStart, BookingEnd, d INTO #AvailableRanges FROM #Booking b JOIN (SELECT CONVERT(datetime, '2010-12-31') +n AS d FROM #nums) dates ON dates.d BETWEEN b.BookingStart AND b.BookingEnd -- Insert some boundary dates (these days are considered NOT AVAILABLE) -- There are the values you'd use in your query INSERT INTO #AvailableRanges (d) VALUES ('2011-09-30') INSERT INTO #AvailableRanges (d) VALUES ('2011-11-01') -- Determine the gaps SELECT DATEADD(DAY, 1, prev) AS start_gap, DATEADD(DAY, -1, next) AS end_gap FROM ( SELECT d AS prev, (SELECT MIN(d) FROM #AvailableRanges AS B WHERE B.d &gt; A.d) AS next FROM #AvailableRanges AS A) AS T WHERE DATEDIFF(DAY, prev, next) &gt; 1; DROP TABLE #nums DROP TABLE #Booking DROP TABLE #AvailableRanges 
Is there any reason this needs to all be done in 1 query? It's been a long time since I've used access - I don't know if access allows stored procedures, but is there any way to store a series of queries and use temporary tables? Based on my research, I'm not sure if there is. But if there is, here is how I would do it (as a stored procedure in SQL Server): The first thing you're going to need in your procedure is a list of dates that is actually stored in a table. There may be some sort of fancy way to do this with date functions, but unless you need these "bookings" to extend decades into the future, you might as well just create a table that holds every date, in order, between now and, say, 5 years from now. We'll call that table "dates" and it has 1 column 'dt" You can populate it one-time in a variety of ways. if you need help with that, let me know. This post is already going to be a long one. For the sake of optimization (and it will also help us in a later step) we'll store the small subset of dates that you are actually querying for in a temporary table. In this case @dtStart and @dtEnd are the parameters to our stored procedure that indicate the dates that are being searched: Create Table #tmp_dates(dt) INSERT #tmp_dates(dt) SELECT dt FROM dates WHERE dt BETWEEN @dtStart and @dtEnd Next step is to create a temporary table called "available_dates" and translate the booked ranges that you've stored in the booking table into an individual record for each date that is NOT booked for each slot (for the sake of this example I am calling the table you listed "bookings", and I am assuming the existance of another table called "slots" which lists all of the potential slots which exist? Otherwise how would we know that there even WAS a slot 7?). The logic here is that we pair up every slot with every date in the search criteria (using a cross join) and eliminate those which fall in the range specified by a booking (the not exists clause): create table #tmp_available_dates(slot_id INT, dt DATETIME) insert #tmp_available_dates SELECT s.slot_id, d.dt FROM slots s CROSS JOIN #tmp_dates d WHERE NOT EXISTS (SELECT 1 FROM bookings b WHERE b.slot_id = s.slot_id AND d.dt BETWEEN b.start_date and b.end_date) Now we need to translate your temporary table back into ranges. We'll create another temporary table to hold our final ranges: CREATE TABLE #tmp_available_ranges(slot_id INT, dt_available_start DATETIME, dt_available_end DATETIME) We can populate this table first by determining the start date of each range of availability. In this case it would be defined as a date in the availability table where the date immediately before that date is not in the availability table. We'll leave the end date blank, for now: INSERT #tmp_available_ranges(slot_id, dt_available_start, dt_available_end) SELECT slot_id, dt, NULL FROM #tmp_available_dates tad WHERE NOT EXiSTS (SELECT 1 from #tmp_available_dates tad2 WHERE tad.slot_id = tad2.slot_id AND tad2.dt = DATEADD(d, -1, tad.dt)) Our last task is to set the end dates on the ranges whose start dates we've just set. This will be the last day before the start of the next range, no? UPDATE tad SET dt_available_end = DATEADD(d, -1, tar2.dt_available_start) FROM #tmp_available_ranges tar INNER JOIN #tmp_available_ranges tar2 ON tar.slot_id = tar2.slot_id AND tar2.dt_available_start &gt; tar.dt_available_start WHERE NOT EXISTS (SELECT 1 FROM #tmp_available_ranges tar3 WHERE tar3.slot_id = tar.slot_id AND tar3.dt_available_start &gt; tar.dt_available_start AND tar3.dt_available_start &lt; tar2.dt_available_start) That last one may be a little confusing. Basically what we're doing is joining the ranges table to itself, with the requirement being that the right hand side of the join is going to include all of the ranges for the same slot id as the left hand side of the join, but which have a later starting date. The where clause then eliminates any ranges except the very next range. And that's it, your final result set is just one simple select: SELECT slot_id, dt_available_start, dt_available_end FROM #tmp_available_ranges And that's it. Sorry if I have some typos or something in there, I haven't set up a database or anything to test out this code. And it's possible this is not the optimally efficient way to solve this problem, but I believe it will work, and pretty quickly at that. Write back if you need me to further clarify how any of this works. Even if you can't use this solution because of the limitations of access, hopefully it will help you understand SQL a little better. EDIT: For simplicity, The SQL is all together here - it doesn't look so daunting without my explanations splicing it all up: Create Table #tmp_dates(dt) create table #tmp_available_dates(slot_id INT, dt DATETIME) CREATE TABLE #tmp_available_ranges(slot_id INT, dt_available_start DATETIME, dt_available_end DATETIME) INSERT #tmp_dates(dt) SELECT dt FROM dates WHERE dt BETWEEN @dtStart and @dtEnd insert #tmp_available_dates(slot_id, dt) SELECT s.slot_id, d.dt FROM slots s CROSS JOIN #tmp_dates d WHERE NOT EXISTS (SELECT 1 FROM bookings b WHERE b.slot_id = s.slot_id AND d.dt BETWEEN b.start_date and b.end_date) INSERT #tmp_available_ranges(slot_id, dt_available_start, dt_available_end) SELECT slot_id, dt, NULL FROM #tmp_available_dates tad WHERE NOT EXiSTS (SELECT 1 from #tmp_available_dates tad2 WHERE tad.slot_id = tad2.slot_id AND tad2.dt = DATEADD(d, -1, tad.dt)) UPDATE tad SET dt_available_end = DATEADD(d, -1, tar2.dt_available_start) FROM #tmp_available_ranges tar INNER JOIN #tmp_available_ranges tar2 ON tar.slot_id = tar2.slot_id AND tar2.dt_available_start &gt; tar.dt_available_start WHERE NOT EXISTS (SELECT 1 FROM #tmp_available_ranges tar3 WHERE tar3.slot_id = tar.slot_id AND tar3.dt_available_start &gt; tar.dt_available_start AND tar3.dt_available_start &lt; tar2.dt_available_start) SELECT slot_id, dt_available_start, dt_available_end FROM #tmp_available_ranges EDIT 2: It occured to me that this solution probably will not populate an end date for the last range since there is no next start date to subtract 1 from. To solve this, you'd create one more temp table: CREATE TABLE #tmp_max_available_dates(slot_id, dt) INSERT #tmp_max_available_dates(slot_id, dt) SELECT slot_id, max(dt) FROM #tmp_available_dates Since the latest date we'll even put in #tmp_available_dates is @dtEnd, this will either be @dtEnd or (in the case where there is a booking that starts before @dtEnd but ends after it), the last available day before that booking. Then you just need one more update before your final select. UPDATE tar SET dt_available_end = tamd.dt FROM #tmp_available_ranges tar INNER JOIN #tmp_max_available_dates tmad on tar.slot_id = tmad.slot_id WHERE tar.dt_available_end IS NULL
I never knew this issue (which I have come across before professionally) was called "gaps and islands" - makes sense. I am still reading your code so that I can understand this approach, but it looks to me like your solution is not taking the fact that we are returning ranges from multiple in the final result set into consideration. It's probably as simple as adding slot_id to the select and adding a few group by's, but you may want to edit your example for the less SQL-skilled original poster.
You're absolutely right - I'm only taking into account a single slot (I did mention that limitation in my post). I spent a few minutes trying to change it into a multiple-slot query along the lines you suggested, but I'm home sick today, and the solution eluded me. It's a start, at least.
Sorry, didn't catch that mention in your post. I had only skimmed it when I noticed this limitation. Please peruse my solution (posted in its own comment) to see what you think of it as it compares to yours (it may not be as efficient, but I think it may be a little more readable/accessible to a new developer). Like you, I may definitely have overlooked some stuff (see Edit2 for an example of that), but it's past bedtime and I'm losing focus.
You'll first want to build a view ('query' in Access) which you can run date range queries on later. Hopefully this will give you a good start: SELECT slot_ID, startDate, endDate FROM ( SELECT B1.slot_ID, DATEADD(DAY, 1, B1.endDate) AS startDate, DATEADD(DAY, -1, B2.startDate) AS endDate FROM Booking B1 LEFT JOIN Booking B2 ON B1.slot_ID = B2.slot_ID AND B2.booking_ID = ( SELECT TOP 1 booking_ID FROM Booking WHERE slot_ID = B1.slot_ID AND startDate &gt; B1.startDate ORDER BY startDate ) UNION SELECT slot_ID, null AS startDate, DATEADD(DAY, -1, MIN(startDate)) AS endDate FROM Booking GROUP BY slot_ID ) T WHERE startDate IS NULL OR endDate IS NULL OR startDate &lt; endDate Produces output: slot_ID|startDate|endDate :---|:---|:--- 1|*NULL*|2011-10-15 1|2011-10-26|2011-10-30 1|2011-11-18|*NULL* 2|*NULL*|2011-10-20 2|2011-10-27|*NULL* 3|*NULL*|2011-10-02 3|2011-10-20|2011-10-22 3|2011-10-29|*NULL* 5|*NULL*|2011-09-21 5|2011-11-07|*NULL* Save it as a view/query. From there you can write another query to run against it for the date range and any other criteria. [Edit] In case you're wondering about the nulls. Those represent an infinity of "free" dates into the past/future. I'll leave it up to you how you want to handle them. [Edit2] If you want it to more closely match your table, remove the DATEADD stuff. But then you'll have overlapping booked + free dates. Edit2.5 Nevermind, you actually do want overlaps. In that case I'll leave it up to you to strip out the dateadd()