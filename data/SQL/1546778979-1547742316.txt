let me fix that. I haven't been typing it that way into sql
An RDBMS won't invent a field on its own like this. It's the program you're using being dumb.
Thanks for the info! I’m going to check out my local CC on courses along with pursuing the MTA.
Appreciate the input! I think the dev server is a great place to get more experience, and especially perfect to practice for the MTA.
Is "columns_fk" a column name or the constraint name? You have to specify the constraint name that was given when the constraint was created! If none was given the constraint was automatically named and you have to find out the constraint name from the table metadata.
Substitute EXTRACT(YEAR FROM timestamp) -- (MySQL/MariaDB, PostgreSQL and Oracle Database) for DATEPART(year, TIMESTAMP) -- (T-SQL) &amp;#x200B;
Total incompetence is the reasoning. They chose it because our version of SQL server is not compatible with the version of SSIS that we use... so we're going to Informatica.
If you dont have a SQL Server to connect to then you need to install one first. Azure Data Studio is the SQL server browser and SQL IDE. 
This was my initial argument, so yes it's what I argue...
Thank you so much.
I think you just need a table like below: Images Table: Image_ID INT IDENTITY (1,1), Image_Location VARCHAR (Whatever guys your needs) Entity Table: Entity_ID INT IDENTITY (1,1), Entity_Type VARCHAR (Whatever fits your needs) Image_Entity Table: Images_Entity_ID INT IDENTITY (1,1), Image_ID INT, Entity_ID INT Image table holds your images. Entity table contains the different entity types, event, athlete, etc. Image_Entity table holds the references between images and entities. 
Am I correct in understanding you have 2 databases with the same data but in different structures? If so my recommendation would be to not do the difficult task of maintaining two sources but make the users (applications) only use 1. Which depreciates the second. In reality this will probably be a process of months if not years but it will probably solve more then just keeping databases in sync.
Oh and otherwise Python is my favorite language to keep systems in sync so yeah, go Python. But it will have to be a custom script most likely. P.s. I have no experience actually keeping databases in sync like you describe.
They are two different programs. One is something I created myself, the other is an open-source program. Each has features I want and until I can fully merge them I was hoping to run both concurrently while syncing the databases
If I understand correctly, sometimes table A syncs to table B, and sometimes table B syncs to table A? 
Yes and No There are 4 tables in each database. Two of the tables are straight one to the other. The other two tables have shared information that needs to be rearranged for the two tables. Those are the two that are making the queries extremely complex
The tables that have a specific source and target will be easy, under the circumstance that both databases are on the same server, (please confirm). I'll write a script. I'm not sure what you mean by the other two tables have shared information that needs to be rearranged for the two tables. Could you give an example?
I think the comments are the problem. I'm not sure about phpmyadmin but in standard SQL comments are started with "--" and not with "//" or "/* ... */".
ive swapped all the comments to single line '--' ones put im still getting the same error
Given that both databases are on the same server, or you can create a linked server connection between them, you could easily set up a merge to pick up differential changes from source to target. Here's an example merge that will keep inserts/updates/deletes on source table in sync on the target table side. https://www.mediafire.com/file/7ewlv4sgeqfdia2/MODERATORISNOTHAPPY\_1.sql/file I've got an idea on how to solve the other issue you're having as well with the other two tables having transformations performed on them before keeping the target side in sync, but I'll wait for an answer on the previous question.
The second statement looks fine to me. However, in the third statement you are missing the column name. The syntax for setting the default value differs between DBMS, see for example here: https://www.w3schools.com/sql/sql_default.asp Also, you should try running the statements individually to find out which one really produces the error.
Comments can be done with /* and */. This is to block comment instead of comment row by row. 
ahh yeah cheers I see where I have gone wrong
Thanks for the resources and help. 
It might be making those fields automatically because you might be joining on a many-to-many which is to be avoided in relational logic. Make sure one of the fields you are joining on is a unique identifier.
 &gt; So i made a relationship from the table pacjenci to wizyty and only to wizyty but for some reason the row Id_pacjenta shows up in Entity9 and 11? Like why? You made Pacjenci -&gt; Wizyta an _identifying_ relationship, meaning the key of Pacjenci is a part of Wizyta's primary key (you can see it being marked PFK). Any foreign key to Wizyta (from Entity9 and from entity11) will include all columns from the primary key of Wizyta. This is exactly how it is supposed to work. 
I realized that! And i changed the ones from pacjenci and lekarze to wizyty to *non-identifiying,* because even if they ID's are required in wizyty i don't want, for example. for the row of wizyty which has id\_pacjenta 1 to be deleted if i delete that row in pacjenci. Am i understanding this correctly?
They are on the same server. I realized I forgot to mention that it was MySQL so I'm not sure if merge would work. I created some with insert on duplicate key update but that doesn't help with deletes. &amp;#x200B;
Oh, yeah, it won't work in that case. Here's what I would do though for that case: &gt;Parameterize your LAST\_UPDATED\_DATE in a table of some sort so you can see the date range that was previously scanned. You don't want to have to do a full table scan every time. &gt; &gt;\--CLEAN UP DELETED RECORDS &gt; &gt;DELETE FROM targetTable &gt; &gt;WHERE PK NOT IN (SELECT PK FROM sourceTable); &gt; &gt;\--UPDATE EXISTING RECORDS &gt; &gt;UPDATE targetTable &gt; &gt;SET targetField = sourceField &gt; &gt;WHERE [sourceTable.PK](https://sourceTable.PK) = [targetTable.PK](https://targetTable.PK) &gt; &gt;AND (sourceTable.sourceField &lt;&gt; targetTable.targetField) --ADD MORE FIELDS TO YOUR LIKING &gt; &gt;AND sourceTable.LAST\_UPDATED\_DATE &gt;= @parameterForLastUpdate &gt; &gt;\--INSERT NEW RECORDS &gt; &gt;INSERT INTO targetTable... &gt; &gt;SELECT sourceValues &gt; &gt;FROM sourceTable &gt; &gt;WHERE sourceTable.LAST\_UPDATED\_DTS &gt;= @parameterForLastUpdate &gt; &gt;AND [sourceTable.PK](https://sourceTable.PK) NOT IN (SELECT PK FROM targetTable); That still does not clear up what to do with your other problem that involves transformations, but are you able to give a brief example of that?
&gt;How do relationships exactly work in SQL Well, you see Bobby, when one table like another table...
the # in that case is a date string delimiter that syntax is valid only in MS Access if i recall by the way, it's weird that your WHERE conditions allow for the return of engagements which started after they ended 
It should be part of the syntax for the the database the book is using. I think access uses it, not sure what else. Look around the date or data types section of the book and you should be able to find it. 
that's kind of what I thought, just wasn't sure. Thank you.
Yeah, Access uses the # sign for dates, and I don't know of any other SQL flavor that does, except for Tableau's fakey sorta-SQL syntax.
Data integrity is for sissies
Are you just trying to return the record of the user and product that is still active? I think I'm missing something because that would be too easy. 
The referential rule behavior is a separate thing, it does not depend on whether your FK is a part of PK of the referencing table.
Do you think this is a good course for an absolute beginner?
Did you end up doing the Udemy SQL course? How was it?
So you want the top 5 most recent per customer? If the customer has &gt;= 5 active subscriptions, get the five most recently added subscriptions? If the customer has &lt;= 5 active subscriptions, get all active subscriptions and the most recently ended subscriptions to make up five? Something like SELECT * FROM (SELECT *, row_number() over (partition by cust_id order by CASE WHEN END_DT IS NULL THEN 99999999 ELSE END_DT END desc, start_dt desc) OrderSeq FROM orders) x WHERE OrderSeq &lt;= 5 
How are you loading this data? I'd guess that you can handle this with a relatively simple ETL.
Hi - it would be the column name, underscore FK (this is what a lot of resources seem to suggest) so in my case it would be customers\_fk (customer table underscore fk. &amp;#x200B; the constraint was just 'ADD FOREIGN KEY CONSTRAINT' and then I named the column and what it references (the parent table, and column). &amp;#x200B; I did notice when I went into the table schema and clicked on constraints, that the primary key/foreign key columns were (of course) listed in there with longer names (with underscores/appended letters to mark them as FKs)..should I use that name when I say drop constraint? So it would be ALTER TABLE x, DROP CONSTRAINT - and then use the name of the constraint in the schema? 
this is neither useful nor answers the question. If you don't know, don't respond - or maybe you should pretend to know, but on Git hub. 
Is yr table set to accept null values?
My comment was both snarky and funny. If you don’t have a sense of humor, get one - or maybe you should learn why you need data integrity, but in a database. 
Also yr mobile n home #s are set as varchat but in sql its not quoted
You have one fewer columns in your value list than you do in your column list. I suspect you're expecting TO_DATE('2016-04-21', '2018-01-12') to return two columns. Instead, it's converting it to a single column and taking the second parameter as the format specifier.
I got about halfway through it, then starting working on learning R instead. The course is okay, but it could be more concise. I've been working on R through Datacamp, and I really like their format and pace. There's some datacamp SQL courses as well, I'd check that out for sure
https://www.kaggle.com/datasets
Ohhh thanks very much! I'm new to this so I appreciate the help.
Just at a glance, your TO\_DATE() looks incorrect. Looks like it should be TO\_DATE('2016-04-21','YYYY-MM-DD').
I concur with this. TO\_DATE() should have the date you want to format and the date format so the database knows how to handle it.
Adventure works is the Microsoft produced free sample data set for SQL Server. https://docs.microsoft.com/en-us/sql/samples/adventureworks-install-configure?view=sql-server-2017
Does entity "new" exist?
Much obliged. 
T.Y. checking it out right now.
I’ve liked modeanalytics its interactive &amp; quite informational. You won’t need to download the data ; just write &amp; run the queries in their online editor.
https://www.brentozar.com/archive/2015/10/how-to-download-the-stack-overflow-database-via-bittorrent/amp/ /r/DataSets
I use this from Github [https://github.com/Microsoft/sql-server-samples](https://github.com/Microsoft/sql-server-samples)
Yes, very much, it assumes you know nothing (as I did)
&gt; I did notice when I went into the table schema and clicked on constraints, that the primary key/foreign key columns were (of course) listed in there with longer names (with underscores/appended letters to mark them as FKs)..should I use that name when I say drop constraint? So it would be ALTER TABLE x, DROP CONSTRAINT - and then use the name of the constraint in the schema? Yes, because you did not specify a name when you created the constraint, you have do identify it by the longer name that was automatically assigned.
For starters, some things you should know are: Set operations (union, union all, intersect and except) you should know how these work Indexes - be prepared to talk about clustered and non clustered indexes as well as columstore indexes Data modelling and DW design There will likely be SSIS questions (like DW design this may depend on the job spec but most employers look for this) If the first stage is competency then you can expect questions like 'Can you tell me about a time when...) I hate these types of questions but I always try to have different examples for different scenarios, such as: problem solving, working under pressure, helping others, making a mistake etc. Good luck
Yes this is the correct behaviour as by default a week starts on Sunday which is 30th December, you can change the default first day by using the SET DATEFIRST function. See [here](https://docs.microsoft.com/en-us/sql/t-sql/statements/set-datefirst-transact-sql?view=sql-server-2017)
Datefirst is set to 1, UK company so Monday is the start day.
Thank you so much for the response and thank you for the good luck. I will probably need it! I am all good with union and union all, not familiar with intersect and except so I will do some studying on that. Quick google seems easy enough. intersect = results that match in both queries except = results that appear in the left statement and removes the results from the right statement. Also I am not very strong on indexes. I will be doing some reading on that. 
Other RDBMS have a corresponding ISO_WEEK/ISO_YEAR pair. It seems MS SQL does not. I found this stack overflow answer which might help https://stackoverflow.com/a/42819846/1228394
Not a fan of using maintenance plans but... Connect to your instance management &gt; Maintenance plans You can configure a maintenance plan to use a SQL agent job as a sub plan. Click around in there and you may find some more answers. 
Try [Chinook](https://github.com/lerocha/chinook-database) which has sample databases for SQL Server and many others. 
ISO_WEEK exists in MSSQL as isowk, spot on regarding a lack of ISO_YEAR however. 
Don’t have much to add but want to wish you good luck. You sound like you know a fair amount and are willing to learn even more. You’re probably in a good position!
If you're using SQL server, you can use an UPDATE statement &amp;#x200B; UPDATE a SET a.Discount = b.Discount FROM OrderDetails a LEFT JOIN Customers b ON a.AccountNumber = b.AccountNumber (or whatever your join criteria is) WHERE b.AreaCode = ... &amp;#x200B;
*beep beep* Hi, I'm JobsHelperBot, your friendly neighborhood jobs helper bot! My job in life is to help you with your job search but I'm just 476.6 days old and I'm still learning, so please tell me if I screw up. *boop* It looks like you're asking about interview advice. I'm ~40% sure of this; let me know if I'm wrong! Have you checked out CollegeGrad, HuffPo, LiveCareer, etc.? They've got some great resources: * https://collegegrad.com/jobsearch/mastering-the-interview/the-eight-types-of-interview-questions * http://www.huffingtonpost.com/rana-campbell/10-ways-to-rock-your-next_b_5698793.html * https://www.livecareer.com/quintessential/job-interview-tips * http://people.com/celebrity/worst-job-interview-answers-askreddit-answers/ * https://www.thebalance.com/questions-to-ask-in-a-job-interview-2061205
Thanks for all your help. I was finally able to get it working thanks to you and a few other replies that pointed me in the right direction. In the end I kept it simple and added a datetime field so that I could use the MAX(datetime) function to filter out my results instead of using log\_id. My final SQL looks something like this: SELECT exercises.workout_id,logs.exercise_id, log_id, exercise, MAX(datetime), set1, set2, set3, set4, set5, weight FROM EXERCISES LEFT OUTER JOIN LOGS ON exercises.exercise_id = logs.exercise_id WHERE exercises.workout_id = 2 GROUP BY logs.exercise_id ORDER BY log_id; I still have to test it completely end to end, but so far it seems be to working as intended. &amp;#x200B; You're right, there are so many more things that I could track/features I could add in. Which I may do at a later date. At the moment I'm focusing on getting the core functionality working as intended without bugs, and keeping the UI clean and simplistic. And I'll go from there. &amp;#x200B; Feels good to finally have this part working after weeks of frustrations and errors. Thanks again :) &amp;#x200B; &amp;#x200B; &amp;#x200B;
Cheers mate, Im pretty new to sql, what are a and b?
If I run the following: &amp;#x200B; set datefirst 1 declare @date date = '2018-12-31' select datepart(wk,@date), DATEPART(year,@date) &amp;#x200B; I get Week = 53 and Year = 2018 which I would expect, it's strange that you are getting Week = 1
Don't bullshit and be clear what you understand - even if it is patchy. I have interviewed many people on SQL positions in my team and they "Sound good" and say all the right things. But when asked to demonstrate or use the things that they know, they soon come clean about what they actually know, it is like they do not expect the interviewer to have a technical proficiency. Often the level of knowledge doesn't put me off for most of the positions, but the bullshit puts me right off regardless of how good they are.
Yes I was aware isowk existed but it seems pretty useless without isoyear. 
He aliased the table so you don't have to write everything out.
I am always honest in interviews. If I don't know something or never used it I will say that. I feel like the interviewer would rather hire someone honest who is willing to learn over a liar 
Apologies I should have clarified, I'm using isowk not wk.
Good answer and good luck
Thank you for posting this. I have an interview day after tomorrow for a DBA role, and I was looking for something like this They sent me this job profile : " As discussed, please find the below job description for the role of database administrator Involved in database management and query optimization Creating database objects like table, views and triggers Work on batch processes, import export backup and app support Generate reports User and database security management Backup and restore database daily Install and configuration mysql 5.1 Programming in sql for daily updation Disaster recovery " Hopefully I do well. I'm a fresher 
Good luck! 
oh right yeah I see thanks
I would recommend [https://www.safaribooksonline.com/](https://www.safaribooksonline.com/) which is about \~$25 a month. With this subscription, you can begin by checking out the book T-SQL Fundamentals. There is also SQL in 10 minutes and SQL for Mere Mortals. &amp;#x200B; I think you aren't giving yourself enough credit, with 10 years of administration experience and 3 years of SQL experience, you know a lot more than you probably realize you know. You also know through experience, there is a TON you don't know. (I feel this every day. I only have 8 years of IT experience, although it is mostly specialized in SQL. I still feel junior to intermediate personally.) You also passed the 461, which is one of the easier but still DIFFICULT exams. &amp;#x200B; I can't give any advice for PostgreSQL / MongoDB, but I would recommend finding some good books or online videos. Brent Ozar has some online videos that are great, but honestly, they are kind of costly. I got in on a black friday deal and got a year subscription for \~$100, otherwise they do offer month to month. (This is SQL Server specific though.) &amp;#x200B; See if there's a [https://sqlsaturday.com/](https://sqlsaturday.com/) near you! This is free training and it's a great way to meet like minded folks. I'd also recommend to check out [meetup.com](https://meetup.com) and see if there are other database professionals nearby. There are two monthly meetups I try to attend. 
Have you tried looking up the job [glassdoor.co.uk](https://glassdoor.co.uk)? I've had some luck in past interviews with people posting their experience/questions. Good luck 
A WHERE clause only shows results where the result is true. Outside of IS NULL, or other functions designed to work with NULL, any operation performed on NULL returns NULL. NULL + 5 is NULL. LEFT(NULL, 4) is NULL. NULL LIKE '*R*' is NULL. So your operation (checking LIKE '*R*' returns NULL), but a WHERE clause only shows where the result is TRUE. 
 WHERE yourdate BETWEEN CURRENT_DATE - INTERVAL 6 MONTH AND CURRENT_DATE 
A NULL value is best thought of as 'unknown'. This gives rise to 3-way logic - true, false, and unknown. A NULL value (unknown) can never match a "like 'X'" statement, nor will it ever match an X = Y style statement. The only way to explicitly include NULL values in your output if you're doing any filtering, is with "X is null" (I'm keeping it simple here and ignoring NVL and it's likes). If you want to include NULLs and filter other values, you have to explicitly handle the NULLs. The best way, if possible, is not to have NULL values at all, because handling 3-way logic can get complex. But obviously this isn't always an option.
Thanks for the reply! I think I got it: so basically the "Not Like '*R'" returns false for Null values, because you can't really use "Like" statement on NULL values?
I hope to collect information of various vans and SUVs on craigslist or other sites, that I may be interested in purchasing. This will likely last at least 2 years, when I hopefully have enough money to purchase one. To reduce excess row duplications, i.e. where a row of data only has one unique attribute (excluding primary key column) compared to other rows, I’ve tried to implement many associated entities. Cars of the same model will have identical repair costs in regards to parts and labor. Hence I have associative entities with 3 attributes. Cars of different years are regarded as their own model, since a particular year can have its own design flaws/premature failures. I would like to collect information on cars I haven’t found for sale, hence the 0-to-many (&gt;o) cardinality from model → car_ad. DIY (do-it-yourself) time, refers to how long it would take a ‘shade tree mechanic’ fix the problem on their own given standard hand tools and jack stand. Interior dimension is a ‘weak entity’ and uses the partial key (pK), tied to its parent’s primary key (PK), model_id. I am willing to take the risk of spending less money on a car that in need of maintenance, as long as I can afford said tools needed to fix it &amp; doesn’t take ‘too long’ to repair. Hence the emphasis of tracking ‘special tools’. Off topic: ERD was design with draw.io, the layout displayed for ‘design_flaws’ entity, is coincidental, and do wish the other entities look/was arranged like it. This is xml file that may be loaded onto draw.io and possibly Lucidchart as well. https://drive.google.com/open?id=1rWNhFfWwGbKLFKTHBT7yoTTFTfvYq8a8 
I’m not familiar with how Snowflake works per se, but in other databases you convert your VARCHAR (string) data to VARBINARY in a SQL statement and search for out of range values. The standard ASCII table goes from 0x00 to 0x7F (0-127 in decimal). You can limit even more by pulling out control characters at the front of the range so that you just find non-printables and a non-whitespace chars (0x0A, 0x0D, etc). This all assumes a strictly English view of the world with no accented chars.
This is potentially international data, and due to branding nearly every product contains a 'é' in the description.
Wait. You can directly query craiglist?
Glad I could help. Unfortunately, your new version may or may not work, depending on what you're looking for. It will give you the most recent DateTime for each unique combination of all the other fields. So if you change weights or reps in even one set, that will produce a new row in your results. If that's what you want, you're set, but if not, you'll still need to do something to pick out the single most recent event. 
Thank you, the way you explained it really nailed it down for me.
If you honestly think your problem of not being able to insert data is because of foreign key constraints, please remove yourself from whatever project is requiring you to attempt this, and take a relational database theory class. While Zzyzxx is trolling you, he is absolutely correct in poking fun at your disregard for data integrity. If you simply wanted to know how to drop a constraint, maybe you should have framed your original post as, "Hey everyone, what is the syntax for dropping a constraint? My googler-button broke...". Instead, you posed a question that makes most database administrators cringe within the first sentence...
The exact syntax for updating one table to values based on another table actually varies a little by platform - what kind of DB are you using (e.g., MySQL, MSSQL, Postgres, etc.)? Also, you probably don't want a straight swap. If the existing column in your main table is varchar (takes text strings) and you're replacing it with an integer, you will want a new column of integer type.
I use MSSQL. 
The answer is always python :) Several years ago I let it loose onto my audio library to recode improperly encoded Russian names. There are plenty of modules that recode text by detecting the encoding automatically and converting it. Doesn't usually work if the text has more than 1 encoding (someone stuck together 2 halves in different encodings for example). Just Google it. It's do it myself, but some a**hole yanked the optical fiber somewhere and I'm out of Internet until the crappiest internet providers in the industrial world (German) will decide to come fix it. Until then I'm trying to spread the 100MB left on my mobile plan and a Google search would consume at least 5 :) So, you would need to make a temporary database just for this maybe, write all the broken records there, fix them, then do a bulk update on Snowflake. By the way, how is this combo working out for you? If I could inquire about the size of the queried data, usage intensity, and monthly cost for Snowflake, it would really help me figure out if it's worth considering it for our tableau backend.
I'm not an SQL expert, but scrubbing data would seem to be best done outside SQL? Maybe a small script looping over a result set? You're real problem isn't in SQL, it's in the Unicode data (presumably your data is in a Varchar column which is Unicode in Snowflake). Scrubbing that data can be complicated and kind of depends on how it was broken in the first place (e.g., utf-8 =&gt; iso-8859-1 =&gt; cp1252?). I don't think it's as easy as running through the column data and saying "I see you have an é, let me change that for you." You'll more than likely need to look for common byte patterns that reflect common chars (e.g., I see a lot of â€™ in my data, so just replace that string with a right quote or single quote). It's not much of an answer, but I think you would be better served with a non-SQL approach here.
Aside from procedural concerns (how tf are you going to parse all this from CL ads???), I think this may be needlessly complex. Snowflake schema is generally reserved to OLTP databases. I'm going to go ahead and assume you intend this to function as such. This is likely true, since it seems you'll be scraping, but DB-ingress is unlikely to be a bottleneck in that progress. An OLAP or Warehouse design is going to be much easier to use, and will facilitate reporting in a much more efficient way. There are multiple circular references, which in my experience, leads to potential disintegrous data. If any of my criticisms would stop me from using a model, it would be this one. The issue is that changing any attribute in a circle can cause the FKs pointing at it to no longer be 'valid', where the data no longer reflects the true state of things. Some of the FKs (maintainance_id on damages_tools) don't seem to have a target (e.g.: no maintainance_id in damages table). I could just be misinterpreting the ERD, tho. You could probably replace the [year] table with... nothing. Just remove, and store the year as an attribute. This would also allow you to remove two junction table, I think. You could also use a dateDimension table, but that's probs overkill for this. On the [interior_dimension] table, you have model_id listed as the PK, but the ERD indicates it has an FK to [model_data]? They should likely be merged, and the dimensions of a model are likely unique to that model, a fair portion of the time. Additionally, since the individual dimensions are numbers, they're probably very small, on disk. Don't take this personally, or anything. I just figured I'd weigh in. It's totally possible I missed important details, as I could only give a cursory glance, and not a deep dive. Hope it helps! 
Could you try just sticking an `INSERT` into the while loop and see what happens?
Insert into what?
Create temp table Inside the while loop - Insert into temp table as new record After the loop is concluded, select from temp table
I don't think this will work. I mean, it might, but are you referring to myTable as a temp table? Because I can't use temp tables in the SSIS. If it's something else and I'm just not getting it, feel free to explain. I'm still a relative beginner.
I don't believe a temp table will work when this is inserted into the SSIS package, though. Is it possible to use a CTE or something? Sorry if this is a dumb question. I'm still relatively new.
In that case you're in luck, MSSQL supports joins in an update statement which makes this much easier. Say you wanted to populate the column CountryID and you already had Country in your main table and lookup table: UPDATE m SET m.CountryID = R.CountryId FROM MainTable M INNER JOIN ReferenceTable R ON R.Country = M.Country.
Thanks for the help! 
No, create a permanent table and use it. If you only want the table to include the values from a single execution, add a PURGEDATA statement right before the WHILE.
I've been working on redrafting this a little bit but I've come up to a point where I'm not sure how to resolve this many-to-many relationship. Would you have any recommendations? My problem is that there can be more than one Ombudsman in an Ombudsman Team and there could be more than one Ombudsman Teams an Ombudsman can be part of. By introducing Ombudsman into my ERD, I can resolve the many to many between Contacts/Ombudsman Team; however, it creates an issue between Ombudsman and Ombudsman Team. I haven't been able to come up with an associative/link table to place between them. [ERD of Contact/Ombudsman/Ombudsman Team](https://drive.google.com/open?id=1tO5bD5aaqAFPTrlTAT-N32A942xamYTS) 
T.Y.
What? Why?
The template for an associative entity will be usually something like this: Table A (a_id, other_columns) &lt;many-to-many&gt; Table B (b_id, other_columns) Will become: Table A( a_id, others) -&gt; TableAB (a_id, b_id) &lt;- Table B (b_id, others) and both columns (a_id, b_id) will be a key for TableAB
Thanks! I may or may not have the ability to create a separate table for this. I'll run it by my coworker tomorrow.
I'm sure I didn't explain right but what I want is a little bit more complicated. Basically, I want to select in a way that multiple records are selected as one record for the same customer. It's very easy of course to just retrieve multiple records for the same customer. The reason I want to have just one record for multiple products is that this makes it easy for me to do further analysis on different product combinations for customers. The answer by fauxmosexual works actually. &amp;#x200B;
Okay, I understand that part but I wasn't sure about relationships with the associative entities. Are the relationships with associative entities only one to many? Do they just not apply in this scenario? Thanks for your input, I really appreciate it. I could just create a table in between Ombudsman and Ombudsman Team that wouldn't necessarily be used for anything to resolve the many to many. 
Thanks, that's an effective solution. I have found the following which is perfect for me: &amp;#x200B; [https://dba.stackexchange.com/questions/63698/need-help-with-sql-server-pivot](https://dba.stackexchange.com/questions/63698/need-help-with-sql-server-pivot) &amp;#x200B; It suggests a combination of using ROW\_NUMBER and PIVOT. The row\_number part is as described by fuaxmosexual and the pivot table can then help to select the different records as different columns. 
Sadly, You will have to make your own function. Here is one from Stack Overflow: [https://stackoverflow.com/questions/22829604/what-is-iso-year-in-sql-server/22830524](https://stackoverflow.com/questions/22829604/what-is-iso-year-in-sql-server/22830524) CREATE FUNCTION [dbo].[isoyear](@date DATETIME) returns SMALLINT AS BEGIN DECLARE @isoyear SMALLINT = CASE WHEN Datepart(isowk, @date) = 1 AND Month(@date) = 12 THEN Year(@date) + 1 WHEN Datepart(isowk, @date) = 53 AND Month(@date) = 1 THEN Year(@date) - 1 WHEN Datepart(isowk, @date) = 52 AND Month(@date) = 1 THEN Year(@date) - 1 ELSE Year(@date) END; RETURN @isoyear; END; 
I would encourage you not to use numeric values for countries, but two letter ISO codes. They are a world standard and will make your data inhetently more readable. 
does your database support materialized views?
It's apparently Apache Solr. Who knew? lol
new to sql, I have no idea what that is, but im guessing I dont aha
which brand of db do you have
im using phpmyadmin
Very much this. By using arbitrary identity ID numbers to represent your data, you are breaking first normal form. I know this is counter to everything they teach you in school about databases, but the only time to use IDs like that is when you are designing a star or snowflake schema.
 CREATE TABLE IF NOT EXISTS copy_of_services( service_name VARCHAR(50) UNIQUE, service_price DECIMAL(19,4), service_qty int DEFAULT '0', service_price_total DECIMAL(19,4), CONSTRAINT services_pk PRIMARY KEY(service_name) ); insert into copy_of_services (select service_name, service_price, service_qty, (service_price*service_qty) as service_price_total from services; 
Thank you for updating me on what it actually was :) 
Your latest drawing does _not_ have a many-to-many relationship between Ombudsman and ObmudsmanTeam. You latter idea (that you need an extra table to resolve many-to-many) is correct - you'll need to have ObmudsmanObmudsmanTeam table.
Just apply to anything with sql, entry level or not. 
Depending on what specifically your aims are (there are a few different routes you could take where SQL would be your primary skill), I would recommend looking into Business Intelligence Development or Data Analysis roles. 
Go on Indeed. Enter "sql" in the search bar. Apply to whatever you want.
https://kyup.com/tutorials/create-new-user-grant-permissions-mysql/
cheers mate
1st - Interviewing is fine but be realistic with in the scope of the job description. If you lack any kind of solid experience, I highly recommend you create some projects in Github. This is a place to showcase specific things to show your resourcefulness and creativity. Lots of entry level positions for SQL Server require or prefer 70-461 or equivalent to show you understand basic querying fundamentals and everything in between. Learn how to deploy Availability Groups and other common HA solutions you will see in a MS Enterprise environment. Understanding AWS / RDS / EC2 basics is critical IMO. You should pick up a Cloud Academy course and familiarize yourself with common cloud based solutions to set you apart from other people who claim they can spell 'Sql". Best of luck to you!
Seriously ? Can you read the error? It tells you exactly what is wrong.
You’ll probably need more than just sql. No one is probably going to hire someone who only knows sql but in combination with others. Like knowing Tableau or SAS, Python, Microsoft BI, etc
Training a lot, since your not the youngest 😂
Take an application like Cherwell with hunderts of tables.. Import Export insert move 
Usecases are useful. Cursor understanding. Look for data mining too
To be an expert you need to know not only SQL itself, but 1) lots of DBA things 2) performance tuning 3) any SQL extensions such as PL-SQL for Oracle, T-SQL for Microsoft, etc. 4) Database designing and tools Each of these topics takes at least 1 year to be good at. So overall it's at least 4 years. In reality you may need even more. I have 15+ years of experience and I still don't consider myself an expert. 
Thanks for the insight. My mother says that she is a database expert but she has worked with databases her entire career and is retired now. Hoping I can one day reach her level lol. 
*beep beep* Hi, I'm JobsHelperBot, your friendly neighborhood jobs helper bot! My job in life is to help you with your job search but I'm just 477.4 days old and I'm still learning, so please tell me if I screw up. *boop* It looks like you're asking about job search advice. But, I'm only ~24% sure of this. Let me know if I'm wrong! Have you checked out Forbes, LiveCareer, TalentWorks? They've got some great resources: * https://www.forbes.com/sites/karstenstrauss/2017/03/07/job-hunting-tips-for-2017/#794febea5c12 * https://www.livecareer.com/quintessential/15-job-hunting-tips * https://talent.works/automate-your-job-search
That's pretty nice, Ty.
I did try but the only interview posts they have are for customer service.
There's a lot of jobs that feature SQL as an element of the job. If you search for SQL as a required skills you'll find a wide variety of jobs asking for it. Some examples: QA tester Reporting analyst / BI analyst Pretty much any kind of data analyst type role. Then there's more blatantly SQL roles like DBA or SQL developer. 
I understand your `transaction_date` column is a timestamp, right? If that is true, for the partition, what you want is the truncated date (which is the date fields only, discarding time fields).
Can you show what your output looks like now and what you want your output to look like?
No, it's just the date. I can get the timestamp in another column but not using it for this.
What it looks like now: &amp;#x200B; &amp;#x200B; What I want it to look like: &amp;#x200B; &amp;#x200B;
Sorry, my tables didn't copy in... will sort that out now
Maybe I'm lacking understanding of what you need. Can you write a sample? What I guessed: user_id | transaction_date | Row_Num_Overall --------+------------------+---------------- 100 | 2019-01-06 | 1 100 | 2019-01-06 | 2 100 | 2019-01-07 | 1 Is that what you need?
This means the \`transaction\_date\` column has data on it's time fields. Try to format the date output...
Change ROW_NUMBER() to RANK() and give it a shot. 
No, it's the other way around... I want the same dates to have the same row number and then increase the number for following dates. &amp;#x200B; What I want it to look like: &amp;#x200B; |row\_number|transaction\_date|user\_id| |:-|:-|:-| |1|08/09/2017|99999| |1|08/09/2017|99999| |2|17/09/2017|99999| &amp;#x200B;
RANK() is not for that. RANK is for building ranks in a way the partition is equals, the same rank is applied.
Can you explain that a bit more please? Why does this mean there must be time on the date fields and how does this affect the partition, etc? Thanks.
I'd have to disagree. I use RANK() for this type of situation quite frequently. 
I wondered about RANK but unfortunately that just made them all Row Number 1. &amp;#x200B; However, when I remove transaction\_date from the partition, it made the same date rows 1 and the next date 3. Which isn't what I want but is getting closer..
Yeah, that's what I was going for, the second outcome you had. What additionally needs to be changed to get back only what you need?
I finally understood what you want. By definition, \`RANK\` jumps places when there is a "draw". If you need to ensure the sequence, use \`DENSE\_RANK\`.
Yep, I agree, DENSE_RANK may work surely better for you if you don't need gaps in the numbers. 
I need the number to increase by 1 for every new date. With RANK the number increased based on the order number so went to 3 but I wanted it to go to 2 because it's the 2nd date.
You need to use DENSE_RANK then. 
You need to use DENSE_RANK then. 
Trying that now, thanks.
Giving it a go now, thanks.
It worked! I removed transaction\_date from the partition and it worked :D Thanks!
Yep, got it. Removed transaction\_date from the partition and it's worked. Thanks.
Cool. 
Learn sql and a learning language like python
&gt;when I am writing the queries, I sort of visualize the table in my head to help me figure it out. There's an aspect of logical thinking involved that I really enjoy! This is *super* useful and where a lot of beginners fail. I'm sure your Excel experience from accounting helps tremendously here.
The difficult part is the vast array of directions you can take with "SQL" or "Data". You have all of the legacy and theory that you need to learn to catch up, but the industry is moving at light speed with different ideas and technologies. Realistically to become an expert, you'll need to pick a niche at some point. For now, just become a sponge and absorb as much as you can. While you'll make more money specialized in data, you won't want to go down that niche for many years and it's a very good idea to round out your skills and knowledge.
Can you please link us to the bootcamp you're learning from? Thanks!
Hmm ok I see. I'll keep this in mind. Thank you for the advice
Discount link to my course on Business and Data Analysis with SQL. I'm working with SQL for over 15 years, and recently put this course together last summer to help analysts and programmers better understand the power of SQL when attempting to answer business questions. https://www.udemy.com/business-and-data-analysis-with-sql/?couponCode=WHAT_DATA24
If you're looking to go the admin route, vs sql developer, I'd say learn powershell before digging into another non-sql language.
Well this hits home...... I got an MBA in accounting years back. I went that route because accounting is a solid, defined 'trade'; as opposed to those, you know, squishy marketing, finance, or management disciplines :). After a few years of ~~power bookkeeping~~ accounting with a F500 company, I moved into financial reporting, which led to a better-paying opportunity with the database management team. And, just as you described, I was hooked! Accounting is actually a great thing to major in before jumping into sql.....both kind of exercise the same brain muscles, if you will. You learn to visualize data and derive trends on the fly. And there isn't a day that goes by where you don't know just a little more at the end of the day that you didn't know when you plopped into your chair. Enjoy the ride! 
I second Randy. Having an undergrad in Accounting, I realized this field is very redundant and will hate my life sooner and later. Now I am a Senior BI Analyst simultaneously working towards MBA in Business Analytics. Having a Business degree and working with SQL/Data Analytics is a win win situation in today's market. Good Luck Mate. 
Were I in your shoes, I’d probably look at finding a Business Intelligence Developer or Data Analysis job where I could use SQL (and related tools) and leverage my accounting domain experience. You can find an entry level position for this without too much difficulty, I think, especially if you demonstrate proficiency through your own personal studying and projects. Learning enough SQL to be productive is relatively straightforward and doesn’t take too long. You’re probably closer to that level of proficiency now. Developing domain-specific knowledge, however, is (at least, in my opinion) the slower or more difficult aspect. You can certainly get by on SQL skills alone, but you’ll be more valuable if you can tie that with industry or field knowledge. In addition to the syntax, basic querying skills, and domain knowledge, your other objective should be to develop what I call “data sense”— a manner of intuitively thinking of how tabular data structures work in a few paradigms (normalized and relational, dimensional, etc.). Most enterprise structured data sources will be in this sort of format, so having thinking about data in that way be natural and passive will help you solve the types of data formatting and so forth you’ll encounter in your role. Once you get to a more intermediate stage, begin conceptualizing how the database management system(s) you use work. Start thinking about some of the set theory principles relational data modeling makes use of. Start researching indexing, database statistics, the system tables, query execution plans, etc. Then starting think of the role you want to develop into next— ETL Development to move data in and out of databases or integrating different systems, Database Administration where you focus more on the health and performance of servers and databases, etc.
I stumbled into SQL in my career when my employer decided to go with an ERP system back in the early 00's. Of all the things I do at work now as a Business Analyst, writing SQL scripts is what I enjoy the most, and I think i'm pretty good at it. It has been an incredibly useful skill to have in my arsenal. I owe so much of my ability to the book *SQL Cookbook* \- it's been out for many years now, but it's still a superb book for developing a good SQL foundation - i can't recommend it enough.
That is interesting bit of information. i will take it one step at a time.. learns sql and then jump to other venues.
Thank you for the information.. that is very helpful indeed.. i will look into it. cheers.
thank you for the link..
i have trieed that.. but i think my skills are not adequate.
start by making a study guide, take the skills measured points on the microsoft exam page and rate yourself in how proficient you are on each topic (and be honest). keep it simple, this is the first topic and subtopic you need to know. [Configure data access and auditing (20–25%)](https://www.microsoft.com/en-us/learning/exam-70-764.aspx#syllabus-1) * Configure encryption * Implement cell-level encryption, implement Always Encrypted, implement backup encryption, configure transparent data encryption, configure encryption for connections, troubleshoot encryption errors do this for all the topics and rate yourself and then My recommendation, do not rely on the book, use it as a last ditch effort to look up stuff. if you still want to read the book front to back then review it properly. read a chapter or two and then review it by putting the book away and writing about what you just read. if you can concisely sum up what the chapter was about then you have a good "theoretical" knowledge on the subject. Be aware that theoretical knowledge is not enough to pass the exam, many features and options of sql server are hidden in stored procedures, menus and other controls. you need to get yourself some "hands-on" experience in Microsoft Virtual Labs. [https://www.microsoft.com/handsonlabs/SelfPacedLabs](https://www.microsoft.com/handsonlabs/SelfPacedLabs) these labs are somewhere between 90-240 minutes and guide you through certain tasks. hands-on experience and repetition of tasks is the best way to learn (at least for me). hands-on experience is 17x better then just reading and watching videos on a particular subject. &amp;#x200B; do the EDX courses listed on the Microsoft exam page. it is free and are official exam guides. [https://aka.ms/edx-dat243x-about](https://aka.ms/edx-dat243x-about) [https://aka.ms/edx-dat244x-about](https://aka.ms/edx-dat244x-about) [https://aka.ms/edx-dat247x-about](https://aka.ms/edx-dat247x-about) &amp;#x200B; These are just guides and cover things globally and not in-depth, you will pick up a thing or two but the videos and courses do not cover everything. Check out the resources Preparation options on the exam page for more in-depth reading. &amp;#x200B; &amp;#x200B;
Sure.. any other keywords i can look into.. do let me know. thanks for your help.
Interesting.. i will look into that./
For an entry level role, knowing the basics should be enough. You can develop your skills as you go. It's a matter of selling yourself to the employer and being confident in doing the job.
Accounts should know sql IMO... it's the logical next step after Excel
⣸⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⣀⣶⣶⡆⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄ ⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⢸⣿⣿⣿⡇⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄ ⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡆⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⡿⣿⠉⣭⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄ ⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⢹⣟⣠⣿⣦⡄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄ ⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣧⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⡿⠷⣹⣿⣿⠇⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄ ⢹⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⢠⣤⡉⠙⠋⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄ ⠘⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠈⠉⠉⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄ ⠄⠹⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣧⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⡀⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠤⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄ ⠄⠄⠻⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡄⠄⠄⠄⠄⠄⠄⢠⣤⡀⢠⣠⢦⡄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⢀⠄⠄⠄⠠⠄⠄⠄⠄⠄ ⠄⠄⠄⢿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠄⠄⠄⠄⠄⢸⣿⣿⣿⣿⣴⣌⢿⠄⠄⠄⠄⠄⡀⢀⡰⠂⠄⠄⢠⡄⠄⠄⠄⠄⠄⠄⠄⠄⠄ ⠄⠄⠄⠈⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⠄⠄⠄⠄⠄⢸⣿⣿⣿⣿⣿⣷⡁⣧⠄⠄⠄⣰⣷⡾⠃⣠⠄⣀⠊⢅⠄⠄⠄⠄⠄⠄⠄⠄⠄ ⠄⠄⠄⠄⢸⣿⣿⣿⣿⣿⣿⣿⠟⢻⣿⡇⠄⠄⠄⠄⢸⣿⣿⣿⣿⣿⣿⣿⠘⡆⠄⢀⣿⣿⣧⣫⡤⣫⣾⡇⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄ ⠄⠄⠄⠄⠄⠹⣿⣿⠿⢿⠏⠁⠄⠄⣿⡇⠄⠄⠄⠄⢸⣿⣿⣿⣿⣿⣿⣿⣦⡀⠄⢸⣿⣿⣿⣿⣶⣛⣉⣡⣀⡀⣀⣀⡀⡀⢀⠄⠄⠄ ⠄⠄⠄⠄⠄⠄⠋⠁⠄⠄⡀⠄⠄⠄⢹⣧⣤⣤⠄⠄⠸⣿⣿⣿⣿⣿⣿⣿⣯⡁⠄⠘⣿⣿⣿⣿⣿⣿⣶⣾⣿⣿⣿⡟⠇⠊⠁⠄⠄⠄ ⠄⠄⠄⠄⠄⠄⠈⣔⢶⡣⠆⠄⣀⣾⣿⣿⣿⡏⠄⠄⠄⠈⠻⣿⣿⣿⣿⣿⡽⠻⠄⣴⣿⣿⣿⣿⠇⣿⣿⣿⣿⠧⠄⠄⠄⠄⠄⠄⠄⠄ ⠄⠄⠄⠄⠄⠄⠄⡵⠋⠄⢀⣴⣿⣿⣿⣿⣿⠁⠄⠄⠄⠄⠄⣿⣿⠫⣻⣷⣢⣴⣾⣿⣿⣿⣟⠁⠄⠙⠛⠋⠉⠄⠄⠄⠄⠄⠄⠄⠄⠄ ⠄⠄⠄⠄⠄⠄⠄⠄⠄⢠⣾⣿⣿⣿⣿⡟⠁⠄⠄⢠⣿⣿⣿⣿⣿⢧⡞⣿⣷⣿⣿⣿⣿⣿⡯⠢⠳⠄⡠⠄⠄⠄⠄⠉⠄⠄⠄⠄⠄⠄ ⠄⠄⠄⠄⠄⠄⠄⠄⣴⣿⣿⣿⣿⣿⡿⠁⠄⠄⣠⣾⣿⣿⣿⢿⣿⣎⣾⣷⣿⣿⠿⠿⠛⡁⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄ ⠄⠄⠄⠄⠄⠄⠄⠄⣿⣿⣿⣿⣿⡿⠃⠄⠄⠄⣿⣿⣿⣿⣿⠈⢻⣿⣿⠟⠛⠁⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄ ⠄⠄⠄⠄⠄⠄⠄⠄⠸⣿⣿⣿⡿⠄⠄⠄⠄⣾⣿⣿⣿⣿⣿⠄⠈⠋⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄ ⠄⠄⠄⠄⠄⠄⠄⠄⠄⢹⣿⡿⠄⠄⠄⠄⢰⣿⣿⣿⣿⣿⡏⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄ ⠄⠄⠄⠄⠄⠄⠄⠄⠄⣾⡟⠄⠄⠄⠄⠄⣿⣿⣿⣿⣿⣿⠁⠂⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⢀⣠⠄⠄⢀⡀ ⠄⠄⠄⠄⠄⠄⠄⠄⠄⠘⠁⠄⠄⠄⠄⠄⠘⠿⣿⣿⡿⠿⠄⣀⠄⠄⠄⠄⠄⠄⠄⢠⣾⣶⣶⣶⣦⣤⣄⣠⣦⣤⣤⣴⣾⣟⣶⣷⣿⣿ ⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⡟⠄⠄⠄⠄⠄⠄⢠⣾⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣾⣿⣿⣿⣿ ⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⣀⣀⣀⣠⣤⣾⡇⠄⠄⠄⠄⠄⣀⣼⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣶⣾⣿⣿⣿⣿⣿⣿ contact my author, u/solodas to rant about how you hate this bot and how it should be banned.
To be honest, getting a decent understanding of sql (D.M.L and D.DL) does not take that long, but it’s part of a bigger toolset you will need for a career change. Understanding that the select statement always produces rows of data made up of columns, that joins and / or sub queries just add more columns to the end of the row, that the select &lt;xxx&gt; clause either reduces the no of columns you output or adds derived columns, and the where &lt;xxx&gt; clause normally reduces the number of rows. Add to that an understanding of aggregate date and group by / having clauses and finally a good understanding of sub queries and different join types and you are mostly there. A lot of filters are on dates so work hard on understanding time stamps and intervals. So what else do you need. Once you have a query that mostly works you may want it to work fast. This involves understanding indexes and looking at performance / explain data to see how the sql engine is accessing the data. Sometimes as coders, a query needs to be broken down into several steps and we use temp tables for this. So get an understanding of how to do that. It can make things much more readable. Understand self joins, but be careful not to get yourself in a never ending loop. Understand stored procedures and when to use them. Also user functions. Also views. These are typically used more in programming situations than one off reports. Which brings up the bigger toolset. If you want to be a database administrator (d.b.a.) then you need to learn the sql engine and how to install it, D.D.L (how to create tables, indexes fragments etc.), storage management (how to add, delete, manipulate disk space), database design (building models, E.R.Ds, normalization etc) and management tools. A data architect, tech lead or project lead will often manage the schemes and physical design working closely with the d.b.a If you want to develop reports and business intelligence (dashboards) you will need presentational tools such as excel, SRRS, report builder experience. If you want to be a programmer who loads data into a database and or creates web apps or desktop programs to process user input, run billing or end of month routines to manipulate or summarize data or present data as part of an integrated system, you will need programming language experience like php, python, java, .net, etc. Re timeline, you can be decent in a year and as good as anyone needs in 2 years. You will always feel like someone else knows more than you. That’s ok. You just need to be good enough to write working queries. Also understand the second time you write a query it will always be better than the first. Try to be aware of the standards and stick to them when you can, using vendor specific syntax as little as possible. I hope this helps. 
I'm currently a Jr DBA and most of my experience is in sybase, but we are going to be using MS SQL a lot later this year. Thank you for the great ideas and links, after reading most of the book I began to worry about how difficult the exam will be. Thank you so much!
I'm currently a Jr DBA and most of my experience is in sybase, but we are going to be using MS SQL a lot later this year. Thank you for the great ideas and links, after reading most of the book I began to worry about how difficult the exam will be. Thank you so much!
Concatenate all the columns together and do the same thing. 
PRAISE
learn PLSQL/T-SQL, i write PLSQL all day and frickin love it. 
That's not LE TOUCAN!
Great suggestion! I’ve given that a go but it seems just as hard, any hints on how I do that? 
I've been doing databases as a main and secondary focus for over 10 years now, and never heard of usecases outside of UML which, in my mind, has very little to do with databases. I used cursors very rarely and for the most part they were either a class in a database API with zero flexibility, it something that should be avoided if it's directly in databases. As for data mining, that too had little to do with databases themselves and is more of a machine learning thing. So what are important topics when it comes to databases? Scaling, distributed systems, relational and non relational data structures, mastering query languages, query plan and optimization, database performance tuning, server tuning, ETL, replication, other administration tasks. Are we living in different worlds? Care to bring more context into this?
PowerShell is only good for a single database engine, running on a small minority of servers, that is very rarely used for anything large scale. Bash however is present on all operating systems, and can automate management of all the major database engines out there, with the exception of MSSQL that's running on windows. With Microsoft's move to more open source, and MSSQL being available for Linux for more than a year now, one could safely say that the biggest bang for buck is bash. Or indeed Python, as it runs on all operating systems and is quite good as a shell language too, and it's very fast once it starts up, and most data processing is on python today anyway. I've met people managing hundreds of gigabytes of data, making models and prediction, without even touching a database server, all in Python and Pandas. I would even go ahead and say that for data centered jobs, knowing Python is far more valuable than knowing anything more than basic SQL.
 exec = &lt;&lt;SQL {{ template "_boot" }} INSERT INTO users(name, email, password, time) VALUES( '{{ .Input.user_name | .SQLEscape }}', '{{ .Input.user_email | .SQLEscape }}', '{{ .Input.user_password | .Hash "bcrypt" }}', {{ .UnixTime }} ); SELECT * FROM users WHERE id = LAST_INSERT_ID(); SQL The syntax of this configuration has me worried that it's using string concatenation instead of properly using the SQL provider's parameterized queries. Sorry, I barely trust the vendor's provider to properly handle the inputs. I'm *never* going to consider another `mysql_real_escape_string()`. 
It's pretty much impossible to make meaningful suggestions without really knowing much of the details of whatt he sproc is actually doing and how the data is being used. One pattern you could look at is creating a scheduled job to prepopulate a table with as much of the calculation as you can, and altering the sproc to just apply filters to that table. Without knowing a single thing about what you're doing, about what is causing the delay, whether you've got any tolerance for stale data, and the nature of the calculations the query is doing it's pretty impossible to say whether this will work for you. You say the queries have been optimised: has the DB been optimised? Do you have the right indexes, and is your data/queries candidates for performance improvements from partitioning?
If the component insists on getting all the data, then that is the big problem. Fix the component because that sounds incredibly stupid. Transferring the whole data seems to be the only issue, since filters help.
Only for a MS environment though. Bash, python, and perl are the Unix hat trick.
I used the materials listed in this [post](https://www.mssqltips.com/sqlservertip/4696/exam-material-for-the-microsoft-70764-administering-a-sql-database-infrastructure/) to study. Good luck! :)
Thank you for your reply. &amp;#x200B; I can't post the query, but the DB has been optimized and all of the indexes that can be put in place are in place. The sproc uses several #temp tables, which I added indexes to, but didn't have much of an impact, if any. The physical tables have indexes in place. The idea for the additional table / scheduled job is a good one.
Completely agree.
One way to do it: SELECT SUM(LEN(Col1) + LEN(Col2) + LEN(Col3) + ...) as TotalCharacters FROM Table 
No parameterized values here: https://github.com/alash3al/sqler/blob/0a741b62a4db50251520a3af069c34483f526853/context.go#L32
[I see](https://i.imgur.com/yDXg1X7.jpg).
Is partitioning an option? If the issue is that there are large volumes of data, some of which can be always outright excluded (too old, out of scope, etc), a partitioning strategy can help things along.
Read up on "SQL Table Inheritance" / subtype-supertype relationships Using one table is called Single Table Inheritance. It's fast and convenient, but requires the use of `null`s, which some people don't like Using one parent table, with separate child tables for each subtype is called Class Table Inheritance, and is slower but doesn't use `null`s
definitely not C, you are to young to undergo those types of agohy there is no D A is a valid option, and it depends basically on the number of rows and also the number of columns -- neither can be too high but i can't nail down what "too high" means for you which leaves B, and good for you for thinking of it, except the foreign keys go in the other direction do some googling on "supertype/subtype tables" and it should become quite clear
Seems like we r living In different worlds. 
This works well for one table, but I have over 500 tables to do this for 
I would say, it depends. Since database is a Middleware it comes to your application. What is your purpose? If you never use cursors, well, nice for you, I need them every day, they are the only way to trigger some weird functional processes. Data mining helps to get faster in understanding your data. Why have objects change in a period of time, is that right or wrong... Usecases, haha.. You have done different work it seems. Sorry for bad English. 
I wouldn't worry about how, but why?
Is this for MS SQL?
Perfect! Thanks!
Haha yeah I thought C sounded painful... Supertype/subtype tables look perfect. Thanks for the response!
How large is the table(s) this procedure is populating? Each time the procedure runs, how much of the table(s) is changed?
Ok, I see what you meant. It's just that you're emphasizing another side of the same work and using different names. Use cases, or data models to use in different scenarios. Data mining is better done using external software (like R or Python). 10 lines of R code for Prophet model for example, to answer the questions about the causes of changes.
I've forced the usage of prepared statements internally with the latest update, thank you for your advice. &amp;#x200B;
Thank you, I will try this (but in Postgres, not MySQL). 
You don't have to reply if you don't like what you see. It'a as simple as that. 
&gt;One pattern you could look at is creating a scheduled job to prepopulate a table with as much of the calculation as you can, and altering the sproc to just apply filters to that table. This is what I would probably do. It's another thing to manage (the scheduled job which repopulates the table needs to be managed and monitored) but usually it's worth it. Unless your data is all really time sensitive, at least part of the query can be just refreshed daily or hourly or whatever. We have a web app which does something similar and adopting this approach took query runtime down from 15-20 seconds to under a second which was the goal. Having a semi-static table also allowed to apply some updates/cleansing to it which wasn't viable when we were querying originally. 
What database software are you using?
What were your responsibilities as a banker. Did you have to do any analysis that you could position well on your resume to land a data analyst job now (with SQL skills)?
Once I read: &gt; without any programming language I close that browser tab. Utter nonsense.
We do this as the last step of our data warehouse for our dashboards, materialized tables is how our shop refers to it. Huge speed increase!
Over at https://dbatools.io/, the banner on the front page says "Powershell &lt;3 SQL" :) OK, so...dbatools is more geared toward SQL Server DBAs but there are "user" functions in there as well. PowerShell is (more or less) like any other programming language that has SQL Server client libraries. For your use case, I'd actually recommend two PowerShell modules: `dbatools` and `ImportExcel` (by Doug Finke). Both are available in the PSGallery, so you can install them with `Install-Module dbatools,ImportExcel'. Get your report into a stored procedure (if it isn't already), then you can execute the report procedure and export the results right to Excel. Invoke-DbaQuery -query "exec YourReportProc" -database YourReportDB -sqlinstance YourSQLInstance | Convertto-DbaDataTable | Export-Excel -path c:\reports\YourReport.xlsx You _could_ use Microsoft's `sqlserver` module and `invoke-sqlcmd`, but `Invoke-DbaQuery` is more robust IMHO (disclosure: I've contributed to the dbatools project) and gives you more flexibility/safety in that you can use parameterized queries with it. Spend some time over on the dbatools site to see the other ways that you can leverage PowerShell with SQL Server; it's saved me untold amounts of time as a DBA.
Connecting to MSSQL and executing queries is dead simple in PoSH. Executing SQL on multiple SQL Servers is dead simple as well. https://www.red-gate.com/simple-talk/sql/database-administration/why-this-sql-server-dba-is-learning-powershell/ 
At my last employer I did a ton of these My best advice use ssrs. Powershell can do this, but it's klunkey. Ssrs exists to do this. Ssrs is a better resume builder if you're a sql guy like me. Ssrs is free if you're already paying for SQL server. I'll dig around and see what code I've got stored and edit this when I find stuff.
I dont’t see you preventing SQL injection anywhere. It looks like a SQL developer would define the template string, where or how exactly is that executed against a database? I see in your routes file, you iterate through HTTP request query parameters and the body of the request. I imagine an API consumer would frequently want to specify a column name to filter on result set returned by a SELECT statement. How are you allowing that column name to be specified dynamically, when some (maybe most?) databases don’t allow column names, table names, etc. to be parameterized? I have serious reservations about your approach. I’m not trying to be mean, but I see this project hs over 30 Github stars already. I’m trying to prevent you or anyone who uses your library from disaster. All it takes is one simple little OR 1 = 1 and it’s game over.
I think you mean SSIS? 
Maybe I'm thinking of "bad cardinality estimate"?
Non-sargable?
Does poweeshell let me format Excel as a table?!
I believe you would be able to create a COM object in powershell to manipulate Excel data into whatever format you want.
True that it's non-sargable because of this, but the term I'm looking for is what specifically describes the issue itself that occurs as a result of it being non-sargable due to using a function in the WHERE clause.
True, but for the trainings cause.
[here](https://www.reddit.com/r/PowerShell/comments/9yzjvj/comment/ea70mdg?st=JQOWO7VF&amp;sh=b21d710b) is a relevant thread i posted in over at r/powershell
Seems you didn't read the code well, anyway, I'm using prepared statement under the hood, if you look at sqlx. NamedExec and Sqlx.NamedQuery you will find that. Please, read carefully before commenting
Yes, you should. There is compliance, the DBMS can ensure everything for you. And there is the fact you can't be absolutely sure of how your framework interacts with your database, what kinds of queries, how they write data, etc.
Not a MySQL person but atleast in SQL Server relationships such as FKs are used to help with joins etc. A database with proper PKs and FKs will perform better than the same database without.
Ok first off - you need to say what database engine you are using (SQL Server, MySQL, Oracle etc). Secondly this is a very easy problem to google and you should start with google before asking a question on a forum. This is basic IT skills 101 that any fresh faced first line service desk monkey could tell you. If you came here with code examples of what you have tried and any errors you are getting etc I would give you the exact code you need but you have not and I generally only help people who put in some effort. Apologies if this comes across a bit strong but I am a firm believer that one of the most basic and essential IT skills is to be able to use google and to actually do that at as the first step in trying to solve a problem.
A agree with you on cursors. Except for some edge cases and DBA type stuff they are 99% of the time used to avoid having to refactor some big procedure or similar that only processes a single row at a time and you need to perform that logic on an entire results set. Cursors, triggers and dynamic SQL are all the same in that they are heavily misused and the majority of examples you find in the wild are due to bad coding/design. One of the databases I manage is a 3rd party application that stores SQL strings in records and iterates over them using cursors building up the SQL dynamically and executing it. The same database can have up to 10-15 triggers per table - some of which fire off these dynamic SQL cursors. Its also funny enough one of the worst performing databases I manage. 
Sorry. And yes I got that from google :)
Can confirm - have a BS and MS in Information Systems - Google taught me 95% of what I know. 
Absolutely. For a beginner, they might seem like overkill and slightly painful to get just right, but they'll save you hours of pain in the future when "someone doesn't understand the ERD" and tries to do something they shouldn't. 
&gt; One of the databases I manage is a 3rd party application that stores SQL strings in records and iterates over them using cursors building up the SQL dynamically and executing it. Funny you should mention that. I'm doing exactly that right now, and I've lost sleep for several nights trying to figure out a better way and hating myself for doing this. The issue is that there are some values that depend on quite a lot of stuff in the data. So basically something like this: ``` If field1 starts with a big letter and the 4th digit of fieldB between 4 and 9 and not 3rd digit of fieldB between 3 and 5 then some_other_field=2 ``` and I have tons of such stuff, and they get modified every once in a while, and I hate myself for having done it in the source code, with GIT commits, so I'm taking it out, letting other people have fun with it as much as they want, I'll just write an importer and will use it to build dynamic SQL. :( Writing it entirely in a procedural language is also bad, as it would be impossible to debug. With this at least I can also output the raw SQL query on request, which can be debugged easily, unlike DB-side procedural languages. Writing it outside of the DB is also an option, but I'd have to import 100 different fields in a procedural language just to output 20. The code would be horrible. If I write this in my CSV I probably won't see another job proposal. But at least I don't have any triggers so I got that going for me. I've been religious about avoiding triggers as much as possible, which is easy given that our database does not actually hold the data, but is just for ETL and OLAP.
Excel *is* tabular. I don't understand the question.
&gt; Powershell is really powerfull when interacting with excel, but it's slow. Like slow at the pace at which a human can do it That's because you're interacting with the COM object. If all you're doing is *basic* import/export/manipulation/formatting, `ImportExcel` is **way** faster.
`Oracle -ne SQL Server` You *can* use the base .NET Framework classes to connect to SQL Server very similarly to how that post does it for Oracle, but we've got functions/modules that make it soooooo much simpler.
Today I had a problem, permission was changed and nobody knew why... I found this Www.sqlservercentral.com/articles/permissons/130850 Such a nice thing to learn... 
&gt; There is compliance, the DBMS can ensure everything for you. This is known as **referential integrity** and it's a cornerstone of good RDBMS design.
Yep
What is it that you got from google? 
May be a silly question, but... is DATE_TRUNC a PostgreSQL-only function? I can't remember seeing it before, but I only work in MSSQL.
Selectivity *mis*estimation.
Thank you so much!
I had to find and identify some weird characters that did not show up even copying the value straight out of SSMS into Notepad++ Please note that I already knew the ID of the record that contained the bad data. The below (modified to remove my companies table schema) targets a single record and spits out the ASCII code of the characters it finds which you can then google to work out what you are dealing with. It's not going to solve your particular problem but it is solving a similar situation and it should be possible to modify it to fit your specific needs or give you an idea of an approach you can use. Note that the insert statements into #TempCharacters were generated using SQL. I've had to remove the majority of the inserts to get under the 10k char limit in reddit comments but the only bit that changes is the incrementing ID. create table #TempCharacters(myChar char,ind int ) INSERT INTO #TempCharacters select CHAR(1),1 INSERT INTO #TempCharacters select CHAR(2),2 INSERT INTO #TempCharacters select CHAR(3),3 ......... INSERT INTO #TempCharacters select CHAR(254),254 INSERT INTO #TempCharacters select CHAR(255),255 --select * from #TempCharacters declare @len int declare @currentPos int = 0 select @len = len(SomeField) from DATABASE..TABLE (NOLOCK) where .... while(@currentPos &lt; @len) BEGIN if( select replace(substring(subtitle,@currentPos,1) ,' ','') from DATABASE..TABLE (NOLOCK) where .... ) LIKE '%[^a-zA-Z0-9]%' BEGIN select cast(replace(substring(subtitle,@currentPos,1) ,' ','') as char) as [Character],@currentPos as Pos,TC.ind from DATABASE..TABLE E (NOLOCK) JOIN #TempCharacters TC on cast(replace(substring(subtitle,@currentPos,1) ,' ','') as char) = TC.myChar where e.SomePK = @PK END select @currentPos = @currentPos+1 END drop table #TempCharacters 
i believe so... Oracle has a similar function, TRUNC however, to extract a portion of a date, datetime, or timestamp, the way to go is the EXTRACT function, which is standard SQL that said, applying a function in a WHERE condition usually means the query is not **sargable**
&gt; The answer is always python :) Only if you are a python developer with minimal skills in other languages. You do not need Python to do this. I have dealt with similar situations using pure SQL many times.
You first want to find the specific issue that is maxing out resources. Are you maxing out connections? Are you maxing out disk reads or writes? Do you have slow queries that are maxing out cpu? It very important to know that question so you use the appropriate scaling solution instead of just throwing money at a problem. Just because you have a lot of content doesn’t mean you need to horizontally scale. Better indexing or partitioning could save you. Even if your db if perfectly optimized, you still need to know if your problem is reads vs writes. The most likely solution is that you will need to scale reads. In which case you create a second database in streaming replication mode. You would place both this server and your master server behind a load balancer. You would then update your web app code to use the load balancer host for read queries and the master database host for write queries. 
Okay, thanks! And yeah, that's a great point about sargability. When I'm reformatting datetimes to dates it's Usually in the SELECT, and 95% of the time it's CAST(datefield as DATE) so that Excel doesn't mangle date data.
When you wanna scale your database across machines or across availability zones , there are 2 ways to do it. The first way is to do database sharding where each database server in a cluster holds only a subset of all the records . What sharding enables us to do is to scale the writes or reads . So a multi master database like Cassandra helps us to scale the writes whereas a database like mongo Db would enables us to scale our writes as only the master takes the writes and slaves take the reads . Even caching technologies like redis support clustering Where each redis master in the cluster is in-charge of a subset of key value pairs . So sharding is an architectural design pattern . Every shard can have a replica too . So sharding is about splitting the data between multiple database servers whereas replication is more about redundancy or the duplication of data across multiple servers or database clusters or availability zones etc.
Assume you have a MySQL cluster in Florida region and one in London region . What you can do is load balance requests between the 2 regions or database clusters is to have a global load Balancer (GLB) based on HAProxy. DNS anycast is used in the networking layer for routing database requests . What this enables us to do is forward the request to the nearest database cluster or the nearest region where our Db cluster is running 
I wonder if the default database for your account has changed. [Did you try this article by Pinal Dave?](https://blog.sqlauthority.com/2008/11/04/sql-server-fix-error-4064-cannot-open-user-default-database-login-failed-login-failed-for-user/)
I did try that article with no luck. SA is disabled and I do not have any other type of login. I think my default database has changed, but cannot confirm anything as I cannot get in.
What error does it give you when you change the default db to connect to in ssms? Still 4064?
I don't even get that far. I cannot get past the login dialog. It throws a 4064 error.
See, that's the part that confuses me because what I am talking about is prior to the login phase. &amp;#x200B; [https://imgur.com/a/MyBxbcE](https://imgur.com/a/MyBxbcE) &amp;#x200B; You are clicking options on the login screen after selecting your database and inputting your login. Then you can change the default DB, I would be curious to know what happens when you try to browse for databases to set to the default. To resolve it though, I would recommend to manually type master in that line, then try to connect. Does this still give the 4064? You should be able to do these steps before the login actually occurs.
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/TSg1syF.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20edn97zx) 
Logged in as another user. My regular domain account that I use to access SQL is not listed under users. SA is still disabled. The target database I'm trying to access is "unavailable." 
Your login may not appear as a user if permissions are done correctly. Your login should typically inherit the permissions through sys if it is a sys admin account and sys should be a user on that database. When you change the default db in the login console prior to logging in, it should allow you to default to another db resolving the error. I would look to the sql instance under security and logins to verify what kind of permissions your account has and what it is tied to. Once you know that, you can begin to hunt down that access at the db level. 
Recoding text to another encoding with automatic detection in under 10 lines of code? You don't need to know python to do it in Python
That worked! Thanks! Now, to figure out why the database is not online to users.
A quick red flag is if the DB looks in the SSMS portal as not online, recovery pending, not synchronizing, etc. 2nd thing to check is why you didn't have permissions to the DB. (You could, it could just be down, but I'd look at verifying that.) Next thought would be to move to the SQL Error logs and possibly the Extended Events on the server. Check the file groups and locations, verify there is space and the drives are working where the files and filegroups live. Past that, you should hopefully have monitoring processes and software in place that alerted you to the issue and have logged the problems as they came up.
* Snowflake is not Microsoft SQL Server. * The solution you offered for mssql would be abysmally slow compared to a Python implementation * The solution you offered does not solve the problem * You don't need to know Python to solve it in Python * I know Java, C#, PHP, C, JavaScript, Go, Nim, and even if I barely knew Python I'd still say it's best done in Python, because I did so in the past, with zero knowledge of Python, and it worked great. Do you know Python?
I was able to figure this out after typing all of this out using LEAD() function. I was just thinking about how to solve this problem completely wrong.
&gt; a big website and we have a lot of content are you *sure* that you're hitting a point of needing to scale the database, not just tuning? I ask because a LOT of developers seem to ignore the impact that simple tuning can address... even stack overflow handles all their traffic on a SINGLE SQL server (with failover replicas in NY/CO), granted they have other stuff like caching layers - ref : https://nickcraver.com/blog/2016/02/17/stack-overflow-the-architecture-2016-edition/ I somehow doubt that you're dealing with their volume of traffic.
I have local admin privileges on the server. Running the SSMS as administrator allows me to access what I need. Just double-clicking the SSMS shortcut gets me in with a non-privileged account. 
The user account in question just needs read access to the db, and it appears to have that in the SSMS. The UDL file I use to test access keeps failing at initializing provider. 
 A NoSQL database is simply a database that does not use SQL. SQL was designed to be a query language for relational databases, and relational databases are usually table- based. **Difference between relational (SQL) and NoSQL database** In a relational database, records are stored in rows and then the columns represent fields in each row. Then SQL allows you to query within and between tables in that relational database. For more details : [https://www.tutespace.com/2019/01/nosql-introduction.html](https://www.tutespace.com/2019/01/nosql-introduction.html) &amp;#x200B;
you would have to provide more info like where do you feel now you are slowed? how much content is "so much content that we need to scale" I personally sit on an Oracle DB with more than 100 tables and a quarter of them have more than 1.5 billion rows. and we handle this just fine with one main DB with many many hits per minute. so how big exactly are we talking about here?
Whenever I read or listen to stories like that, I instantly imagine poor ORM produced queries, written by a "senior fullstack developer" who can't tell a difference between `UNION` and `UNION ALL`, performing joins between thousands upon thousands of records in their code instead of on the database. I also imagine piles of cash I'll get for consulting in optimizing that hot garbage. Another one that triggers a cha-ching sound in my head is "We have 3 million records across 6 columns and must move to big data architecture because performance on our single 512 GB RAM server with 64 cores is poor and we need scale".
&gt;3 million records across 6 columns So...small enough to use Access comfortably. 
I see. Thanks. Any speed losses for not doing this in the SQL? or its only for the error prevention.. 
sorry! just thought it might be helpful to show you can use sql commands inside powershell.. i will try to learn more about your mentioned functions and modules though
So, if I understand correctly, you: 1. Received a ton of raw data from a vendor. 2. You took this raw data and massaged it into a schema that works for your application. 3. Now, you still want the raw data to exist alongside the data that you will be using in the application, not necessarily for the application to access, just for historical purposes. 4. \*But\* you want to be able to trace back the records/values that you are using in your system to the raw data.
I suggest that you look at dfinke's ImportExcel module, which supports an ever-increasing list of features for manipulating XLSX files. 
Iirc, there is an advanced tab on the ssms login dialog that lets you specify a database. Try master and tempdb databases in the database field in that tab. 
SQL server
I think it must be SQL lite version I don’t have the agent on mine
\- We have so much content that we need to duplicate the database to speed things up As others have written, tuning is where you want to start instead of throwing hardware at it. Of course nobody really likes tuning, hard to measure cost/benefit whereas doubling the ram or adding CPU is easier for Excel types to grasp. Pity cos in the few decent tuning exercises that I've been involved with the results have been impressive. SQL gurus aren't cheap. Partitioning, index usage (having too many that aren't used), even data purges (or moving it to another db) might be looked into. For a certain DB, we found that 98% of usage was for data that was 6 months or younger, so we moved the older data to another DB with cheaper storage. Far easier to handle. Good luck. &amp;#x200B;
Did you give the sql assessment yet? I have to give one over the weekend so I just needed few tips. 
I LOLed so hard on "3 million records" its so true! 3 million to me is like a single partition on a hourly partitioned table :)
Oh hi Mark! Just wanted to show appreciation for your blog and it's enormous help in our process of improving our data warehouse. Keep doing the good work!
I'm glad it's been helpful. Thanks for the kind words.
what did your EXPLAINs reveal?
I'd try using the windows "Run As" feature to launch the udl interface as that other user.
I have not tried that - I will now. I'm guessing performance varies depending on other factors based on your suggestion?
There are a great deal of factors which can cause a plan's performance to degrade or become enhanced, that is partially why you will hear a lot in the database realm "it depends". Just a few things: &amp;#x200B; * Database optimization engine * Numbers of rows * Current indexes * Statistics * Memory / disk / cpu * Load on server * Server and database settings
no, i'm saying that your EXPLAINs will tell you which of those two statements is faster my guess is, neither
How can one do an EXPLAIN in a query? Sorry, I am a SQL noob.
SQLPlus the executable command line tool? I usually call it as part of a heredoc within a bash script. Other times I'll use it for DDL one-liners if I have to log in as sysdba to do something. Maybe do things like set a parameter or create spfile from pfile. I'll also use it for scripts that produce quoted CSV for b2b integration projects or other ETL. I mean if you're going to automate anything from a script, sqlplus is naturally what is used because I'm not aware of any other natively supported method. 
I don't. I use DBVisualizer to run queries and pull extracts.
Excel
This is how we do it at Microsoft
Everything from your SELECT needs to be in the GROUP BY if its not within an aggregate function
If you are grouping by a CASE you will do so such as: GROUP BY CASE WHEN X = Y THEN A ELSE B END You can't include the `AS 'Alias'`. This used to throw me.
Hey thanks for your response! Does it mean I need to group by clknumber, location, etc..?
Yes or take CLKNUMBER out of the GROUP BY all together since you don't seem to be aggregating anything? If you're trying to remove duplicates by grouping by everything, just use SELECT DISTINCT
I use it for long running SQLs or PLSQL blocks. It can be easily executed on server and I can leave it overnight. For example some geographic tasks
thank you very much for your time! Select Distinct removed the duplicate CLK values without using group by! 
You have to have the same number of expressions in every SELECT statement. So if you have 4 columns in the first one, you need 4 in each of the other ones as well. P.S. They also need to be the same data type so make sure of that as well.
Do you think I could just add 2 null columns to third table? 
Ya you can. I foresaw you asking this question so I tried to get an edit in fast enough but you must have replied before I got it in.
Depends on the platform. Postgres, MySQL and SQL Lite all allow you to just put the alias in the group by clause.
You could, but it's not a great idea. Just change your query to synthesise the 2 extra fields
What RDBMS?
Are you trying to find distinct values only?
You'll want to look into restfulAPIs and stored procedures.
google "EXPLAIN *yourplatform*" where *yourplatform* is MySQL, MS SQL, Oracle, whatever
Generally the bottom one. No one can truly answer that without knowing your db structure. I'd your have large tables with well indexed columns, the bottom one would be massively better.
Oracle's SQL Data Modeller is a popular one. Can work with most DBMS, all you need is the jdbc jar file. 
Thank you much! 
How would I do that?
I will reply once I put the kiddos to bed. I have some thoughts on Unions and Union Alls. 
Just like /u/Data_Is_King described above... SELECT Col1, Col2, Col3, Col4 from Table1 UNION ALL SELECT Col1, Col2, Null, Null from Table3
I appreciate it. Not rush at all
Ah ok that's what I thought you meant but wanted to be sure . Thank you
Using redshift but hoping for something that isn't specific to any one RDBMS
Oracle usually scares me off but will check into this , thanks
TOAD is pretty good with dispalying FKs. 
Homework?
Lol, my question was theoretical. Very interesting stack overflow post, though. Thanks!
No, I was trying retrieve some info from my database and am still fairly new to SQL. The 'fruits' and 'color' examples I used are just simplified versions of the actual query I'm running. Suffice to say I figured out the answer after trial and error: SELECT (SELECT sum(quantity) FROM fruits WHERE date &lt;= '2018-01-04' AND color = 'Red') - (SELECT sum(quantity) FROM fruits WHERE date &lt;= '2018-01-04' AND color = 'Blue') as difference;
Ah I never thought of separating reads and writes. That's pretty smart, thanks!
It's best not to have queries inside of your SELECT statement if you can help it. Could slow down performance, especially if correlated. Let me look. 
Will second Oracle SQL Data Modeler as a decent program.
Oh interesting! I didn't realize it worked like that. Thanks for telling me! I'll definitely google for a few of those terms and learn more.
So as the other stated above you can just use an empty field in following statements. All SQL Statements being combined by Union All statements must have the same number of columns. Select Column1, Column2, Column3, Column4 from Table1 UNION ALL Select Column1, Column2, Column3, Column4 from Table2 UNION ALL Select Column1, Column2, NULL, NULL from Table3 I am glad to see you use an Union all instead of an Union. You should completely avoid using an Union unless you have a very specific reason to use one. Using only an Union will potentially make you lose rows where they match between two or more queries, It is just best practice to start with an Union All and then you can go back to use an Union if it is needed or makes more sense to use. &amp;#x200B; I would agree that you should let the heavy lifting be done in SQL when you can. There will be cases where you can't but in my experience I have always tried to do the most work in SQL. &amp;#x200B; If you have anymore questions, feel free to reach out and I'll attempt to help.
The question is purely theoretical, I'm not sure how many rows are too many rows and requires special scaling techniques. But that is a lot of data that you're talking about in your case, lol.
Hmmm I have heard of backend servers being localized to certain centers to be closer to the end user, but I wasn't aware that databases could also do that. I see.
Ah actually I have heard of techniques like that, moving less used data into cheaper (slower) storage, and having faster storage for more used things. And tuning indeed does sound better than trying to scale the database itself. Thanks for the help!
I've worked with a few DBAs and it seems like many of them start out as SQL analysts and developers. I don't think those positions are highly coveted, so you may be in pretty good shape shooting for entry level roles. 
Agreed. Most (although probably not all) DBA's cut their teeth with some sort of development experience. Depends on the role though, if a company is looking for a pure hardware/admin DBA with no query development or tuning, it could fly.
SQL Zoo and W3 Schools will teach you the coding side. I suggest skimming database design for mere mortals for the rest. It's hard to learn SQL unless you understand set based thinking and the relational model. Good luck! 
Agreed. Stick with free web resources until you land a job and figure out what flavor of SQL you'll be working with. Then you can get a book to step up your game.
more from the SO developers - https://samsaffron.com/archive/2011/05/02/A+day+in+the+life+of+a+slow+page+at+Stack+Overflow
Thanks for taking the time. Giving the whole thing a shot now. I'll see how it goes. 
You could do some case statements to get the different sums in the same select statement. Select totals.RedTotal - totals.BlueTotal from (Select sum(case when f.color = 'Blue' then f.quantity else 0 end) BlueTotal ,sum(case when f.color = 'Red' then f.quantity else 0 end) RedTotal from fruits f where date &lt;= '2018-01-04' --If you have lots of colors, will help performance wise to include this In operator and color in ('Red', 'Blue') ) totals &amp;#x200B;
You don't need to add new columns to accomplish this. Depending on the data type of the tables which **do** have those additional columns, you could just do this without changing the structure of the tables: &amp;#x200B; SELECT Col1, Col2, Col3, Col4 FROM TableName UNION ALL SELECT Col1, Col2, **'' as Col3, ''' as Col4** FROM TableName2 &amp;#x200B; &amp;#x200B; you may also just be able to do NULL as Col3, NULL as Col
As others have said, it would be depending on the tables and how everything is structured. If table A and B are small with a small amount of fields being selected, I might chose the first one but normally I would choose the second one. &amp;#x200B; My main issue is that you have an UNION instead of an UNION ALL. You should never use an UNION instead of an UNION ALL unless you have a specific reason to do so. If you use just an UNION, you could potentially lose records when everything matches between the two queries. Just be careful when using only UNION. 
DbSchema isn’t free, but there’s a free 2 week trial and then it’s only $100 bucks. I use it for work and love it. It’s got about 80% of the features of the really high end tools for &lt;10% of the price. The developer is pretty responsive to issues and it’s Java based with a Groovy plugin system if you’re into that sort of thing.
I've been a former DBA. When i was a DBA i knew how to build complex queries but my dba job didn't quite need me to do that. Such dba are called as core DBA's. I quit that job very soon - didn't actually enjoy managing multiple database without challenges. While my short span of 2years over it I left my skills were unused since a long time and when you dont use your skills often you started loosing the hold of it. Later after practicing the skill set I changed my job and made sure in next one I did what I liked to do. In this job I got to use my coding skills + most of time I was even able to advice the dbas when they have task of optimizing queries or backup strategy but will never be other around. It is important to find out what one truely loves to do and then pursue it with passion. 
DBA here. Started out in corporate retail and quickly became the report guy. Became the reporting system owner. After that took a job as a DB developer/devops guy. Finally landed a DBA role about a year ago. Gotta say though, I do a lot less SQL than I did at any of my previous roles.
 SELECT sum(case when color = 'Red' then quantity else 0 end) - sum(case when color = 'Blue' then quantity else 0 end) as difference FROM fruits WHERE date &lt;= '2018-01-04' /* one tablescan */
How much programing did you do as a dba I really like working more with software than programing 
Tbh, I hardly did any. As I've said in my DBA job we had multiple databases to handle with a small team and it was round the clock shifts. I didn't even get a chance to learn the data models of the database I was admin of. 
What was you're main role then? 
How do you like it? 
Administering databases. Monitoring the jobs and multiple ad-hoc requests like vacuuming, sharding etc... 
Do you enjoy it? 
SSRS is free and was built for exactly this
These articles helped me quite a bit. DBA types (career paths) https://www.brentozar.com/sql/picking-a-dba-career-path/ DBA Training Plan; https://www.brentozar.com/archive/2013/11/free-ebook-sql-server-dba-training-plan/amp/ Developer/DBA Roles/Duties https://www.brentozar.com/archive/2018/06/job-duties-for-database-developers-development-dbas-and-production-dbas/amp/
"I am glad to see you use an Union all instead of an Union. You should completely avoid using an Union unless you have a very specific reason to use one. Using only an Union will potentially make you lose rows where they match between two or more queries, It is just best practice to start with an Union All and then you can go back to use an Union if it is needed or makes more sense to use. " This is really misleading. Without considering performance at all, you shouldn't avoid using a union or a union all, they are different predicates with different purposes. There is no "potentially" lose rows, that is the point of a union, to get distinct rows. If you had two tables with some duplicate rows and some non duplicated rows, (or if you're selecting 3 columns) it will only return distinct values. If you want to retrieve all rows from all tables, use a union all. Whatever you do, understand what you are doing ahead of time. If you have say... a list of customers and you want unique names, don't start with a union all and work backwards. If you do and your UAT doesnt have the same name twice, it will pass... but when you get to prod and you encounter duplicates, troubleshooting (especially if SQL isn't your forte) will be much more difficult. 
Data Engineer here. As a DBA you won't really be coding much, they don't have time to do that. I consult with a DBA when tuning something because they know the configuration inside out, and most DBAs are great at writing SQL, but they don't do it. They do things like routine maintenance, maintain security, manage resources and ensure availability. If you want to code, become a developer. If you enjoy configuring and optimizing, go the DBA route. 
Consider simplifying your query by using a table to represent the logic. It will simplify the query and improve performance. Most db's wont be able to optimize the query with the multiple nesting of case statements.
For some time yes... 
The company ethically are incredibly sketchy. But their products tend to be very good. And free. 
SELECT sum(case when color = 'Red' then quantity when color = 'Blue' then quantity * -1 else 0 end) as difference FROM fruits WHERE date &lt;= '2018-01-04' /* one case statement */
Thank you so much for your insight! I am a complete beginner in SQL! :)
Best answer, least effort, topped off with a little bm. Very clean!
Glad I could help! Usually everyone in this sub seems so much smarter than I, haha
I really like it a lot. I may not be doing as much SQL, but I'm still learning a ton each day. As an admin I spend much more time on system setup, security, permissions, chasing down obscure DB bugs, dealing with space issues, etc. I occasionally do some performance tuning which let's me dig into some SQL. I guess what I'm driving at is if you really enjoy writing queries and developing structures then a developer role might fit you better.
I disagree with you that it is misleading but I agree that you should know which one you want to use first when writing the query. If someone is not proficient at SQL, I still believe that a UNION ALL would be safer. This is because you are not going to know the difference between UNION and UNION ALL and you WILL lose rows when you don't expect it. Also, UNION ALL is better performance wise as well since it can skip the sort and distinct calculations that the UNION has to do. &amp;#x200B; This example was in a Production Environment for several years before I found it. There are two types of Invoices where all fields are almost identical but are used for two different things. INCORRECT Example select i1.ActivityID ,Month(i1.Invoice1PaidDate) MonthPaidDate ,Year(i1.Invoice1PaidDate) YearPaidDate ,SUM(i1,BaseAmount) Invoice1Total from invoices1 i1 Group by i1.ActivityID, Month(Invoice1PaidDate), Year(Invoice1PaidDate) UNION Select i2.ActivityID ,Month(i2.Invoice2PaidDate) MonthPaidDate ,Year(i2.Invoice2PaidDate) YearPaidDate ,SUM(i2.BaseAmount) Invoice2Total from invoices2 i2 Group by i2.ActivityID, Month(i2.Invoice2PaidDate), Year(i2.Invoice2PaidDate) In this example there are four fields being returned and here are the descriptions: * ActivityID - This is a parent object where all the invoices roll up into. So an Activity can have multiple invoices with some being in each table. * Invoice1PaidDate / Invoice2PaidDate - This is the datetime field that is set to the current time once the status of the invoice is set to 'PAID' * Month(PaidDate) - Gets the month of the paid date. * Year(PaidDate) - Gets the year of the paid date. * BaseAmount - This is the total that is on the Invoice * Sum(BaseAmount) - In the above example it gets the SUM of the base amount field at the InvoiceID / Month / Year granularity. **Example Data** &amp;#x200B; Invoice 1 Data: * Activity ID = 432156 * Invoice1PaidDate = '2018-03-25' * BaseAmount = 5000.00 Invoice 2 Data: * Activity ID = 432156 * Invoice2PaidDate = '2018-03-20' * BaseAmount = 5000.00 &amp;#x200B; The issue here is that all four fields are identical between the two queries but combining duplicate rows in this example would mean the total amount would be incorrect. So in the example above it would remove $5,000 from the total. So you are planning for the total to be one total amount but then it shows up $5,000 short for some reason. This is a simplified example of what actually happened but it gets to the point of where an UNION ALL would have solved the issue. &amp;#x200B; Like i stated in the beginning, I agree that you should know which one you should use when building the query but that isn't always obvious. I am not saying to NEVER use a UNION but you need to have a definite reason to use one. Otherwise it could have complications like I stated above. UNION ALL will be faster and safer in the end. &amp;#x200B;
What if there are no `'Red'` or `'Blue'` fruits in table `fruits`, though? One (or both) original queries will yield `NULL` in that case, yielding a `NULL` difference. Your query will yield `0`. (That may be sensible, but it is different behavior.) Cheers, —Nit Picker
&gt;I'd try using the windows "Run As" feature to launch the udl interface as that other user. I can't see the run as [http://prntscr.com/m5vccs](http://prntscr.com/m5vccs)
select col1 || '\_\_\_\_\_\_\_\_\_\_' || col2 from owner.table;
I have taken quite a few learningtree classes, all on-location never online. I enjoyed them and I think I learned a lot.
I only get results explaining what MS SQL Server is.
Yea! I Dont "love" coding but I can learn it rather quickly. I prefer configuring and that route a lot more. 
I used the Oracle exam guide only and was able to get an 88% on the test. But I spent a lot of time with that guide. After my first time through it I took the practice test on the attached CD-ROM and get a 53%! Yikes. But that motivated me to re-read the guide, really focusing on my weak points, and I was able to do well on the test after my second pass through. I probably spent 40 real hours of total study time on this. 
I defenitly Dont enjoy the programing side of things more, I like software based work more
could not locate it with right click
does this help? https://docs.microsoft.com/en-us/sql/relational-databases/performance/display-an-actual-execution-plan?view=sql-server-2017
could you please show the CASE and where you want the suffix appended
If I had money to spend on training I would consider doing a conference. In the UK the best one is SQL Bits and it's awesome, in the US there are alternatives such as PASS etc. [https://www.pass.org/AttendAnEvent/FindAnEvent.aspx](https://www.pass.org/AttendAnEvent/FindAnEvent.aspx). Most of them are free and you may only have to pay for travel and accommodation. I've sat in classroom based courses several times and honestly can't say that any of them were ever worth the money. You can do plenty of online training for free such as EDX, I would start with this course [https://www.edx.org/course/querying-data-with-transact-sql-0](https://www.edx.org/course/querying-data-with-transact-sql-0). You also don't have to pay the $99 for a certificate. My other top tips for learning are: &amp;#x200B; 1. Subscribe to different blogs and newsletters, they usually have really interesting stuff in them. try and take time to read them when they come in, I particularly like brent ozars daily email and the weekly newsletter from sqlservercentral 2. Go to [sqlservercentral.com](https://sqlservercentral.com) and head to the forums, I really enjoy taking peoples problems, replicating them in tempdb (where you can do no harm) and trying to solve them. The cool thing is that if you aren't able to solve it someone will come along with an elegant solution and you can see how it works. Happy Learning :-)
Take an on-site class. If you try to do a class in the office you will constantly be interrupted.
SQL for Smarties by Joe Celko Joe Celko helps write the SQL Standard, so there's no better source
Yes, thank you. Do you know of any resources that teach what all those icons mean? How did you learn to read execution plans. Thank you for your guidance.
I would say that if you do an online class that you be allowed to do it from home or from anywhere other than your desk. When we do training, that's what we tend to do just to not be interrupted by phone calls or visitors.
Just tested... you can launch cmd.exe via Run As, and then in the command window launch your UDL file and it will pass through the credentials you specified when you launched cmd.exe
It depends on the person and vendor. I've used Learning Tree International and I go to the local office even for online classes. When it's in person you can easily talk to the professor and classmates, plus they have free snacks/coffee. For online classes you get a cube with dual monitors, they are responsible for your internet connection, and still free snacks/coffee. Also, if you live with someone or are a parent you can get the same distractions as work.
The reason I say this is not misleading is that I don't think there is anything safe about a union all unless you fully understand what is happening downstream. If it seems like I'm being pedantic, I suppose I am... but that's because that's how computers handle your queries. Even in the scenario you provided, while you would have been able to sum month/activity with the output and get the correct some... the data is pretty useless beyond that... I would go as far as to say that unless you sum the month/activity or add a source column (which would have resolved your original issue An output of ActivityID,YearMonth,InvoiceTotal 432156,201803,5000 432156,201803,5000 &amp;#x200B; is just as dangerous as an output with only one line in my opinion. Unless you're identifying the origin table grain duplication (my own term) is very dangerous. Either you select ungrouped values or you sum all grouped values in one step. 432156,201803,10000 &amp;#x200B; To demonstrate this, lets just use the following results from the union all query: ActivityID-----------YearMonth-----------InvoiceTotal 432158-----------201803-----------5000 432158-----------201803-----------5000 432157-----------201803-----------9000 432157-----------201803-----------5000 432156-----------201803-----------1000 432156-----------201803-----------5000 432155-----------201803-----------9000 432155-----------201803-----------8000 &amp;#x200B; No idea what you're doing with that data, but suppose you wanted to count the number of months each activity id has sales over 7000. You can't work with the data the way it is and it can be damn near impossible to identify where by profiling data because the incorrect values can seem random. &amp;#x200B; you would find yourself with 432158 : 0 432157: 1 432156: 0 432155: 2 &amp;#x200B; when you should yield: 432158 : 1 432157: 1 432156: 0 432155: 1 In reality, any other data you have joined the result of the query (especially when it is being used in a visualization tool) will generate duplicates on output. &amp;#x200B; &amp;#x200B; The correct solution in the case you presented is not a union at all, rather it is a join and should generate a lot of questions. &amp;#x200B; I would propose something like this: Ideally you have a date dimension (if you're new to sql, GOOGLE IT, it will save you months over the course of a career) but assuming you don't. (Yes this is not ideal performance, but imho you seek logical correctness then tune.. if you're just starting out, don't try to tune before you have the logic validated... you are wasting your time) /\* You can replace 19000101 with an appropriate start date or with a parameter You can replace 23000101 with an appropriate end date or with a parameter \*/ WITH DateList AS ( SELECT DATEADD(DD, 1, CONVERT(date,19000101)) Date ,FORMAT(DATEADD(DD, 1, CONVERT(date,19000101)),'yyyyMM') YearMonth WHERE DATEADD(DD, 1, CONVERT(date,19000101)) &lt; CONVERT(date,2300101) UNION ALL SELECT DATEADD(DD, 1, DT) FROM DATERANGE WHERE DATEADD(DD, 1, DT) &lt; CONVERT(date,2300101) ) SELECT IsNull(I1.ACTIVITYID,I2.ACTIVITYID) ActivityId ,dl.YearMonth YearMonth ,SUM(I1.BASEAMOUNT,I2.BASEAMOUNT) InvoiceTotal FROM DateList DL LEFT JOIN INVOICES1 I1 ON I1.InvoicePaidDate = DL.Date LEFT JOIN INVOICES2 I2 ON I2.Invoice2PaidDate = Dl.Date Group by IsNull(I1.ACTIVITYID,I2.ACTIVITYID) ,dl.YearMonth &amp;#x200B;
[SQL Cruise](https://www.techoutbound.com) I mean, if your employer is gonna pay for it...
presumably you will want to know the IDs which do *not* always have the same description SELECT ID FROM yourtable GROUP BY ID HAVING COUNT(DISTINCT description) &gt; 1 
no, sorry... i don't use that database, nor that front end google is your friend! 
Check out window functions (Lag, lead, first, last).
I agree that an UNION was not the best answer but this was in an example that was used in a Production Application. It caused errors in how they used an UNION instead of an UNION ALL. The data was put into Activity/Month/Year granularity so it can sum by Months and then sum by the Year. So with an UNION, it would sum March to be $5,000 and with a UNION ALL it would sum $10,000. Each month field would sum by the Month and the Total would sum by the Year. So they had a table like this: &amp;#x200B; |Activity|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec|Total| |:-|:-|:-|:-|:-|:-|:-|:-|:-|:-|:-|:-|:-|:-| |432156|0|0|5000|0|0|0|0|0|0|0|0|0|5000| This would rarely happen because there were other type fields included that could also match between the two different tables. &amp;#x200B; Saying that an UNION ALL is not safe seems strange to me because you know EXACTLY how it is going to join the two queries together. It will never change dependent on the data that is in each query. This is not true with only an UNION and you have to know if it is possible for duplicate records to exist. Then need to know if you want to keep the duplicates (My example) or remove duplicates. Still think there is a place for both UNION and UNION ALL but knowing that a UNION ALL will always act the same independent of the data is safer. &amp;#x200B; Plus I do love the Date Dimension, it is wonderful way to deal with dates. 
You should know that the criterias you write for the WHERE clause are evaluated for each row of your table, in a way you answer the following question: "which rows in my table fulfills my criterias?". That said, and assuming you want to discover all rows that have equal values in both ID and Description columns, you should write a criteria on your WHERE clause to compare ID and Description for equality. You could compare if they are different too, if that's what you need.
Thank you so much! You got me on the right track and I was able to solve my problem! 
Complementing what u/Klashedd said: [SQLite Window Functions](http://www.sqlitetutorial.net/sqlite-window-functions/)
Learned about those last year from one of our developers (I am a "power" user). That opened up a realm of possibilities I wasnt aware of with SQL. I used to export the data to CSV to the process it with Python..
 SELECT CASE ... END + '_____' AS [Column] FROM table AFAIK you don't even need the CONCAT function, just use +
I just dont know what you mean by act the same. I'm not saying you're wrong, I just view it from the other side. I think they both behave consistently. I mean if you never had duplicates before and somehow you now do, a union all statement would have appeared to "change". Am I misunderstanding?
 DECLARE @bgl_reading TABLE ( reading_id INTEGER, user_id INTEGER NOT NULL, reading_date DATE NOT NULL, reading INTEGER NULL ); INSERt INTO @bgl_reading (reading_id,user_id,reading_date,reading) VALUES (1,1,'2019-01-10',162),(2,1,'2019-01-08',170),(3,1,'2019-01-05',160) SELECT *,lag (reading,1)OVER (Order by reading_date ASC)-reading as [SincePrior], (SELECT TOP 1 reading from @bgl_reading order by reading_date asc)-reading as [SinceFirst] &amp;#x200B; &amp;#x200B;
I'm thinking that with an UNION ALL you will always get all the data and with an UNION you will lose the duplicate rows (which you may not know/expect to happen). I guess you're correct that they are consistent but I have worked with developers that just assumed duplicate rows could not happen and used an UNION. You just have to know your result set and how you want to handle duplicates. I am doing BI work in SSRS with mostly financial reports and duplicates happen rarely but need to be included for totaling (like my example). 
I had a warehousing class one time that was heavily into data cubes but they didn't look like this! Also wondering if we can get a Skyrim port. 
I don't mind working. This is a personal project but pretty much right after posting it got back-burnered for 72 hours or so.
I didn't mean to imply you didn't like working :). Love personal projects. Hope it goes well. Let me know, will this work for you? I'm always kind of interested to see if actually helped anyone.
Cannot recommend highly enough [Brent Ozar's training classes](https://www.brentozar.com/training/). I took his 5-day performance tuning live online class 4 years ago. Paid out of my own pocket because my employer at the time didn't want to invest. I have since left that place and found a much better paying and more interesting job. I couldn't have done this without the knowledge gained from Brent's class. That investment has long ago paid for itself, and then some.
Change your query to: `where yourdatefield &gt;= STARTDATE and yourdatefield &lt; dateadd(day,1,ENDDATE)` Or if you like, do that math when passing the parameter to the query/stored proc. SQL Server, when comparing `date`s (from your input) to `datetime`s (in your table), will always go to midnight of the date specified.
Well I, too, find working with Time Zones very complicated to understand. The concept is pretty easy to understand, but the conversion is eventually hard to comprehend. First of all, I assume your table's column data type is defined as `timestamp` (which is short for `timestamp without time zone`). Since I don't have your table, I defined a CTE to work with your specific date. You should also know that, since I'm in São Paulo/Brazil, the same date is presented differently since we are in different time zones. ``` WITH my_table AS( SELECT '2018-12-01 12:00:00'::timestamp source_timestamp ) SELECT source_timestamp, pg_typeof(source_timestamp) FROM my_table; -[ RECORD 1 ]----+---------------------------- source_timestamp | 2018-12-01 12:00:00 pg_typeof | timestamp without time zone ``` The function `pg_typeof` returns you the data type of its argument. For `source_timestamp` is obvious, but it gets better: ``` WITH my_table AS( SELECT '2018-12-01 12:00:00'::timestamp source_timestamp ) SELECT source_timestamp, pg_typeof(source_timestamp), source_timestamp at time zone 'UTC', pg_typeof(source_timestamp at time zone 'UTC') FROM my_table; -[ RECORD 1 ]----+---------------------------- source_timestamp | 2018-12-01 12:00:00 pg_typeof | timestamp without time zone timezone | 2018-12-01 10:00:00-02 pg_typeof | timestamp with time zone ``` Here we can look what happens when you use `at time zone` expression: it actually converts the date to work with the time zone approach. In this case, the value is literally the same: ``` WITH my_table AS( SELECT '2018-12-01 12:00:00'::timestamp source_timestamp ) SELECT source_timestamp, pg_typeof(source_timestamp), source_timestamp at time zone 'UTC', pg_typeof(source_timestamp at time zone 'UTC'), (source_timestamp) = (source_timestamp at time zone 'UTC') is_equal, EXTRACT(EPOCH FROM source_timestamp), EXTRACT(EPOCH FROM source_timestamp at time zone 'UTC') FROM my_table; -[ RECORD 1 ]----+---------------------------- source_timestamp | 2018-12-01 12:00:00 pg_typeof | timestamp without time zone timezone | 2018-12-01 10:00:00-02 pg_typeof | timestamp with time zone is_equal | f date_part | 1543665600 date_part | 1543665600 ``` As you can see, comparing both values for equality evaluates for `false`. This happens because the data types are different. But if you understand a date is nothing but a number, extracting that number (which is the EPOCH part of the date in PostgreSQL), you can see we are talking about the very same value. It starts to get wierder when you apply that second `at time zone` conversion: ``` WITH my_table AS( SELECT '2018-12-01 12:00:00'::timestamp source_timestamp ) SELECT source_timestamp, pg_typeof(source_timestamp), source_timestamp at time zone 'UTC', pg_typeof(source_timestamp at time zone 'UTC'), (source_timestamp) = (source_timestamp at time zone 'UTC') is_equal, EXTRACT(EPOCH FROM source_timestamp), EXTRACT(EPOCH FROM source_timestamp at time zone 'UTC'), source_timestamp at time zone 'UTC' at time zone 'America/New_York', pg_typeof(source_timestamp at time zone 'UTC' at time zone 'America/New_York'), (source_timestamp) = (source_timestamp at time zone 'UTC' at time zone 'America/New_York') is_equal, EXTRACT(EPOCH FROM source_timestamp at time zone 'UTC' at time zone 'America/New_York') FROM my_table; -[ RECORD 1 ]----+---------------------------- source_timestamp | 2018-12-01 12:00:00 pg_typeof | timestamp without time zone timezone | 2018-12-01 10:00:00-02 pg_typeof | timestamp with time zone is_equal | f date_part | 1543665600 date_part | 1543665600 timezone | 2018-12-01 07:00:00 pg_typeof | timestamp without time zone is_equal | f date_part | 1543647600 ``` It's presentation is not showing us the time zone, confirmed by the data type. The equality test evaluated to false because, even the data type being equal, the actual data is not. According to (PostgreSQL documentation)[https://www.postgresql.org/docs/current/datatype-datetime.html] when you apply the `AT TIME ZONE` constructor over: (1) a `timestamp without time zome` value: &gt; Treat given time stamp without time zone as located in the specified time zone (2) a `timestamp with time zone` value: &gt; Convert given time stamp with time zone to the new time zone, with no time zone designation That said, in the first case we are just seeing the same data from different point of views. On the other hand, the second case actually creates a brand new data in a way time zones are not needed. In fact, I just noticed they talk exactly about that. Ow, that when a long road, but I think it becomes easier to understand. 
Maybe start with Doom?
Thanks for the info. I am still studying from the book but I also found a 10 "course" on Udemy that has five practice tests on them. https://www.udemy.com/1z0-071-oca-step1/ 1Z0-071: Prepare for the exam using this Practice Test I have only taken the first test, I got a 45%. I love it! That may seem weird, but it showed me where I was lacking. I have created a list of items to review and learn. I will take the next test when I have finished studying and then do the same. I have scheduled my real test for Feb 1, so I am hoping this will get me there. I know the passing score is 63% (https://education.oracle.com/oracle-database-sql/pexam_1Z0-071) but I want to get as close to 100% as I can. Just passing isn't good enough. I want to know my stuff and then move on to PL/SQL. 
it would help a lot if you'd give a trivial example set of numbers with the expected output. it's not entirely clear what you're asking for
Okay, so you need to write some data back to the sql? If yes No coding skills: you start with some tools which allows you to connect and modify data. Coding skills req: You start which whatever coding language you choose (python or node.js these are people choosing nowadays) and get the connector/adapter of the sql and write code which’ll take in a csv or whatever and write those columns into the database.
Welcome to the wide world of analytics. https://stackoverflow.com/questions/1342898/function-to-calculate-median-in-sql-server Let me know if you have further questions.
this is in mysql. I do use python a bit pandas, numpy, etc to fix up larger sets of data for tableau. But first time i am getting write access to a db...already messed up the other day with bad update statement. I just dont know what to do in sql.
Yep, STO has carried me for a long time. 
It might look something like Campus date Target Value Store1 1/11/2019 22 Store2 1/14/2019 28 Store3 1/7/2019 24 Store4 1/10/2019 19 Store5 1/12/2019 18 Store2 1/10/2019 16 Store3 1/10/2019 29 Store5 1/13/2019 16 Store1 1/16/2019 24 Store3 1/14/2019 29 and i am thinking i would need to do something like Campus date Target Value Store1 1/17/2019 21 Store2 1/9/2019 22 Store3 1/13/2019 21 Store4 1/15/2019 19 Store5 1/13/2019 20 Store2 1/13/2019 30 Store3 1/16/2019 25 Store5 1/15/2019 22 Store1 1/6/2019 20 Store3 1/10/2019 22 Store6 1/9/2019 21.0 Store6 1/13/2019 22.0 Store6 1/15/2019 21.0 Store6 1/13/2019 19.0 Store6 1/13/2019 20.0 Store6 1/16/2019 30.0 Store6 1/15/2019 25.0 Store6 1/10/2019 20.0 where store 6 is a new value. The median aggregation is actually already written and getting somethin like this in the raw data will aggregate once the entire workflow(siss package) is deployed. (another script aggregates for median. 
could you put four spaces in front of each line on your example, please, to get reddit to format it? 
Brilliant! 
Agree on using Learning Tree. You can get up to speed on the whole Ms stack quickly, but on site is a must in my opinion. Oracle too I believe. 
its a stored procedure doing the aggregation. fixed the formatting 
oh ok, stored procs are fine i'm glad you fixed the formatting. when i tried to do it i made wrong choices `:)` i'm a little confused about the nature of the aggregation. do the dates factor into it? is it a one week window?
Yes, so there are about 30 other columns, the dates get rolled up to a monthly and a YTD Value with aggregate later. so it changes to like campus timeperiod value store1 201801 20 store2 201801 20 store3 201801 20 etc. Here i want to also store 6 with the median for the time period.
Ok but I still don't know what the median actually is, in context. Like. Suppose I have store1 at 10, store2 at 20, and so on up to store5 at 50. You said you want four out of the five stores. Which four? That median could be 30, or 25, or 35 It's going to end up something like this: SELECT x.val from data x, data y GROUP BY x.val HAVING SUM(SIGN(1-SIGN(y.val-x.val))) = (COUNT(*)+1)/2
Just right of the bat, the numbers are very disparate. 
In this case all the stores except store 1
&gt; disparate. That's for sure. But this is what the boss wants. I have them in separate charts so I don't know what the issue is with the heights. They should all just be full since they aren't related. 
Can you just change the background color to that light green so it _looks_ like they're the same height? 
That's an interesting fix! I think the numbers would look a bit weird if I did that given that they wouldn't line up even if they looked to be the same height.
" You just have to know your result set and how you want to handle duplicates." Tldr version of all of my comments! The reason I kept insisting on replying is in the hope that if someone sees this, they will see the discussion and figure that out. It will make both of our jobs easier. 
Hi, I think you need to change your data bar type to 100% stacked - the graph is intended to show prportione split rather than comparing to other bars in this case? 
If that's the case, I would say go the DBA route or the sysadmin route. If you dont love coding, it's not likely to appeal to you later.
That was my original plan but they do want the numbers themselves which is why this is a pretty dumb graph because the end goal is basically 100% stacked data bars but with the actual numbers... What I don't get is why the bars I am using are coming out different sizes when they are three entirely different graphs with only one bar on each of them. I would think they would all come out as full sized.
It's needs to play Crysis.
If you're talking about Microsoft SQL Server, Itzik Ben-Gan travels all over the world teaching classes, and he literally wrote the book on T-SQL. You can check his schedules at http://tsql.solidq.com.
Ohhh very interesting reads. Those devs are really transparent and do a really good job at showing actual stats/numbers.
Does someone want to explain the use-case here?
Don't worry, I didn't take offense by any means. And I love to expand my knowledge. But I just got another project thrown at me, so that makes 2 this weekend, so I wrapped this up at the application level instead. If it's of any interest here is my javascript code. Actually, the first half of the 50 lines or so are purely for readability: [https://repl.it/@dexygen/BglAddTrending](https://repl.it/@dexygen/BglAddTrending)
Look into dense rank. 
Is this homework?
Not homework for a class or anything. It's through a self learning program
Oracle?
Appreciate the advice. I took a look into it, but I'm having trouble figuring out how to utilize it to achieve the results I'm looking for. If I understand correctly, DENSE_RANK will give me a numerical ranking of values in an ordered column, and it will repeat the same rank number for any duplicates it finds. So if I used it on the **Persons** table I listed above, ordering by the Date column, I would get something like this: [Row#] | ID | Name | Date | State | (dense_rank_value) ---|---|---|---|---|---| 1| 8 | Dean | 2/2/2017 | WI | 1 2 | 2 | Triana | 3/5/2017 | CA | 2 3 | 6 | Jonny | 3/5/2017 | AR | 2 4 | 10 | Gary | 4/6/2017 | NV | 3 5 | 4 | Dermot | 7/3/2017 | NY | 4 6 | 1 | Jonas | 1/1/2018 | TX | 5 7 | 3 | Byron | 1/1/2018 | NJ | 5 8 | 5 | Thaddeus | 1/1/2018 | AL | 5 9 | 9 | Brock | 5/1/2018 | OR | 6 10 | 7 | Hank | 12/1/2018 | MD | 7 &amp;nbsp; But what can I now do with this information? I can see that rows 2 and 3 are duplicates (with rank 2), and I can see that rows 6, 7, and 8 are duplicates (with rank 5), but how do I now exclude anything without a duplicate rank?
I took a couple SQL classes in college and have spent the last 3 years using SQL heavily at my job. Recently started this course to boost my resume and I can definitely say it covers everything you would need to know and more. 
I took one very basic class in college but for the most part have been self taught. I use SQL every day in my job at one of the largest real estate companies in the US. If you have the opportunity, learning while working is ideal in my opinion. I also recently started the Microsoft certified sql dev course which covers everything you would need to know and more. 
Thank you !
Something like this will work I believe. With dups as (select count(*) as count, date from persons group by date having count(*) &gt; 1) Select * from persons where date in (select date from dups) 
It’s not difficult. Just use the lead/lag functions.
I actually just posted an edit with a solution I figured out on my own, but I just gave your code a shot and it worked flawlessly. I'll have to study it a bit later when my brain isn't so tired and see if I can make sense of it. Your solution looks quite a bit more elegant than mine, but how would you compare what I came up with to yours? Do you see any potential issues with the way I wrote my code? &amp;nbsp; Also, if I can pay you back for the favor you've provided, I'll help you out with reddit formatting. When you wrap a string of text between two asterisks on reddit, it makes the string italic. That's why the third asterisk you typed came through with no issue. You can use an escape character to keep those first two asterisks without converting the text to italics. &amp;nbsp; To type this: &gt;With dups as (select count(\*) as count, date from persons group by date having count(\*) &gt; 1) Select * from persons where date in (select date from dups) &amp;nbsp; Use a \\ character, like this: &gt;With dups as (select count(\\\*) as count, date from persons group by date having count(\\\*) &gt; 1) Select * from persons where date in (select date from dups) &amp;nbsp; You can also use two line breaks to put a single line break in your comment/post. And if you want to make even more space, use &amp;nbsp; surrounded by line breaks.
I don't think there's a practical application here. It's a beautiful combination SQL and 3d rendering. I guess it's got the same use-case as a good poem.
OP: care to expound on what inspired this and what challenges you encountered developing it?
Oh I'm pleased it worked and thanks for the escape character example. Testing that \* here and \* here. As for your code, it looks fine to me, except I don't think you need the WHERE clause. The INNER JOIN should perform the filtering sufficiently. Essentially, we're both using a subquery to list the duplicated dates. And then we're linking the results to the main table - you're using a JOIN, and I'm using an IN. 
That’s the most beautiful thing I ever seent!
Thanks again for checking it out and for helping me out.
(without actually looking at it) Is it modular enough that you can specify a particular RDBMs' dialect of SQL to run checks against, or postgres only?
It's easy with window functions. But basically you are onlu using those two tables from the model. CTE would also help Prepare different sets (best in CTE or subqueries). First set: Join customers with payments made in 2007, group by customer, sum by amount, order the top 10. So you have name, year, sum and too 10. Now to tackle the months, you either have to do pivot or running total. No idea what your solution should look like. 
Nice bro thx !
Thank you !
Thats why you take database dump/backup before you do anything. Take a backup and modify the data, revert back if some fuckup happens. Thats how you’ll learn.
Thank you bro
Inline query to get the dates with having and then use this inline query in a where statement.
Good job! But for your example: &gt;uses `timestamp with time zone` instead of `timestamp`, or that a column named `foo_id` has a FOREIGN KEY reference. Why? There are plenty of legitimate cases where this would be necessary rather than a mistake. For example when dealing with Tableau as front end, having time zone information in the column causes problems. Or sometimes information from external data sources don't contain time zone information and it is unknown until later, when metadata is downloaded and attributed, rows can be assigned geographical positions and time zone information can be attached. There is a reason Postgresql offers timestamp without time zone data type. Same for foreign keys. What if it's a micro service database, that doesn't contain the primary table, so the foreign key can't be created? What I'm saying that such specific rules should be optional.
Dense rank is on the right track in the sense that a window function would simplify this, but as you stated it doesn't get you all the way there as it only count the number of dates with duplicates. You could of course then run a query on the table with dense rank added to grab only the ones with duplicate dense ranks, but we haven't really simplified anything by doing that. What you want is a window function using count and partition. WITH CTE AS ( SELECT [Row#] , ID , Name , [Date] , [State] , DENSE_RANK() OVER ( ORDER BY [Date]) AS [DenseRank] --Added just for fun , COUNT(*) OVER (Partition by [Date]) As NumOfDupes FROM #Dense ) SELECT * FROM CTE WHERE NumOfDupes &gt; 1 &amp;#x200B;
Column A | Column B | Column C ---------|----------|----------
``` INSERT INTO movies (person_id, name) SELECT f1.id,f2.name FROM famous_people f1 inner join famous_people f2 on f1.id=f2.id WHERE f1.id=1 and f2.name=1; ``` Although the SQL above is not semantically identical to yours, specifically the `on f1.id=f2.id`, this might be something based on which you could further work on.
Isnull( field_name, ‘’ )
Thanks for the reply, but I’m not really following. Where am I placing this?
I would like to better understand why you would like to do this and why you are referring to rows 2 and 3 as duplicates. 2 and 3 aren't duplicates. Seems as if you are getting a random name back in a group of dates. Get max id; group by date. So you get 1 record per unique date. /shrug will this w
What exactly are you looking for? Just date of the first and last contribution for each customer_no, or also the value of their first and last contribution? 
You have to throw that around every field in your final select statement. It will replace nulls with blank strings. It is essentially a coalesce(). You are going to have to explicitly give the field its name because ssms won’t know what to call it. Ex) Isnull( id, ‘’) as id
Your final select should be something like Id, Sum(gift) over (partion by I'd), Min(first gift), Max(second gift)
OP why do you have first and last dates for ID 1 in one row, but only included the dollar amount for the first? Should this be summed? Also, do you just want the min and max dates per ID? If so, seems to me a very simple grouping. ID, SUM(amount), MIN(firstgift), MAX(lastgift) GROUP BY ID
Ohhh! And playing around with this should give me my desired result in the second table, not just created the same result but now with empty fields?
Whoops, fixing it! Had the wrong info in the tables. 
I have a working solution, but first I want you to try and I'll give you some guidelines. First, do a self-join on this table. Departure city of one table is the arrival city of another. Of course, the departure date must be smaller or equal (depends on you) than the return date. Then you just add up prices from both ways, select just the columns you need and sort by your criteria (not group by, I think that was a typo). A hint to get you started if you still don't know: &gt;!select * from #Departures as dep join #Departures as ret on dep.Departure_city = ret.retal_city!&lt;
a nice share &amp;#x200B;
thanks useful!
Add into where statement: where first gift is not null and last gift is not null
SQL is so fun, isn't it?
That seems plausible also, I’ll give it a run through. 
Most of the time I actually really enjoy using it. This particular problem was pretty frustrating though. 
The reason is that it's what was assigned to me, and there's not really any leeway to have the results printed another way. You are correct that the second and third row of the table are not duplicates to each other, but both of them contain a Date value that is duplicated one or more times in the Date column. Unfortunately I don't think the solution you provided will work since I can't have the final results printed as a grouped sets. I did figure this out last night and edited my post, though, so I think I'm good. I definitely appreciate your suggestion in any case. 
You're absolutely right, which is why none of the rules are actually active by default, other than simply checking for syntax errors. The idea was to make this as opt-in and customizable as possible. And then of course there will be exceptions to any rule, so you can disable an active rule [on a per file basis](https://squabble.readthedocs.io/en/stable/#per-file-configuration).
Thanks for the additional info! I managed to figure out a way to get the results late last night, but I could always learn something new. I'll give this a shot later today. 
Wonderful, thank you so much. I was trying all kind of things with UNION but I had no idea I could self join a table. &amp;#x200B; My querry is below. Still under work but feel free to leave me any suggestion. &amp;#x200B; Thanks again ! &amp;#x200B; &gt;SELECT &gt; &gt; t1.departure\_city, &gt; &gt; t1.arrival\_city, &gt; &gt; t1.price + t2.price as price, &gt; &gt; t1.departure\_date as 'Depart Date', &gt; &gt; t2.departure\_date as 'Return Date', &gt; &gt; Strftime('%Y%m%d', t2.departure\_date) - Strftime('%Y%m%d', t1.departure\_date) as diff &gt; &gt;FROM &gt; &gt; trains as t1 &gt; &gt; JOIN &gt; &gt; trains as t2 &gt; &gt; ON t1.arrival\_city = t2.departure\_city &gt; &gt; AND diff &gt; 0 &gt; &gt; AND diff &lt; 4 &gt; &gt;WHERE &gt; &gt; t1.departure\_city = "Paris Gare de Lyon" &gt; &gt;ORDER BY &gt; &gt; "Depart Date", price &amp;#x200B;
Using TSQL this was my test scenario drop table if exists #Departures create table #Departures ( Departure_city varchar(100) , retal_city varchar(100) , Price int , DepartureDate datetime ) insert into #Departures (Departure_city, retal_city, Price, DepartureDate) VALUES ('Paris', 'Dijon', 50, '20190115') ,('Paris', 'Dijon', 140, '20190115') ,('Paris', 'Dijon', 40, '20190124') ,('Dijon', 'Paris', 42, '20190117') ,('Dijon', 'Paris', 60, '20190117') ,('Dijon', 'Paris', 40, '20190126') select dep.Departure_city , dep.retal_city ,(dep.Price + ret.Price) as price , dep.DepartureDate , ret.DepartureDate as ReturnDate from #Departures as dep join #Departures as ret on dep.Departure_city = ret.retal_city where dep.DepartureDate &lt;= ret.DepartureDate -- And other conditions order by Departure_city, DepartureDate, price 
Yeah I see you already have solutions that work, but nonetheless this is an almost textbook use case for a window function so it doesn’t hurt to learn this one too!
make them pay for a credit course from Harvard University's Extension school https://www.extension.harvard.edu/course-catalog/courses?keyword=sql&amp;term_description=Spring%20Term%202019 all those courses are online and all give university credit 
What you describe would work. Here are some other things to consider... Are the job's (or whatever) header information stored in another table? If you added a processed flag and a processed completed datetime field to the existing table that would eliminate the need for "to be analyzed" table. The processed timestamp would give you some history information for troubleshooting/auditing purposes. &gt;&gt;&gt;&gt; SELECT * FROM JobHeader WHERE Processed = 'N' After the analyze job finishes then it'll update the processed flag from N to Y, and insert a timedate stamp.
You need a sum?
Look into ROW_NUMBER and MAX window functions. The exact syntax and availability of these window functions might differ based on your DBMS.
Appreciate the feedback! I know how to use row number/rank/dense rank to give me the first and largest. The issue I'm having is doing it tied to date, particularly the first date. I'm on MSSQL 2012.
The reason I was thinking of deleting processed data was to improve efficiency. I'm not clued up with the inner workings of SQL so I assumed less entries in a table the quicker it would run and the lower the load on the server. If having them with a processed flag isn't any less efficient than deleting them then ignore the next question. After processing I could store it in a "processed" table to ensure there's a log of data that's been executed. Would this be a good solution?
If you know how to use those window functions then I'm not sure where you're getting stuck. WITH CTE AS ( SELECT ID, Amt, FirstGift, ROW_NUMBER() OVER(PARTITION BY ID ORDER BY FirstGift) AS RowNum, MAX(Amt) OVER(PARTITION BY ID) MaxAmt ) SELECT ID, Amt, FirstGift, Amt-MaxAmt AS Diff FROM CTE WHERE RowNum = 1;
 WITH CTE AS ( SELECT ID, Amt, FirstGift, ROW_NUMBER() OVER(PARTITION BY ID ORDER BY FirstGift) AS RowNum, MAX(Amt) OVER(PARTITION BY ID) MaxAmt FROM Gifts ) SELECT ID, Amt, FirstGift, Amt-MaxAmt AS Diff FROM CTE WHERE RowNum = 1; Is this what you're looking for?
What RDBMS?
&gt; I assumed less entries in a table the quicker it would run and the lower the load on the server. Sure, but it's relative. Even on modest hardware, SQL Server can select and order by hundreds of thousands of records with sub-second response times. Unless you think the table record count will quickly grow into the millions, I'd stick with adding a flag to the current table. Another consideration is how many parallel queries/jobs will be utilizing the data. When data is being read from a table there is a momentary lock put on that table. If there's going to be one main process/procedure then performance shouldn't be an issue whichever way you choose. If tasks are going to be run parallel (many at once) then you may start to notice performance drop somewhat since one process may have to wait for a table lock to clear from another process.
Not quite sure what you mean, I'm using Python 3.6, flask, and sqlalchemy.
Gotcha. 
The data is only being moved to the server and executed there due to the client program being unable to run certain commands. After its ran once it'll never be ran again. I'm using Twilio for automated texting but once a python script is compiled it doesn't work so the best solution I could think of is to move the data to the server and execute the code there instead. Unless that changes anything I think I'll go for your idea of having a flag. Thanks for all the help with this, much appreciated.
I don't think you'd want to set it up that way. Maybe I'm misunderstanding what you're aiming for, I'm not familiar with Flask or SQLAlchemy, but it looks like you'd be storing multiple Relationship foreign keys per user? If that's the case, each new request would require a new record in User. Same for Friends.
As someone who works with this type of thing daily, there are many ways to handle it. Using the example of marking something in the Work Queue with a timestamp of when it was completed (instead of deleting) helps with execution visibility, but you are also correct that there are limits to how much you want in the queue. For this, you could have a secondary process that archives "old" entries on a regular basis (ex. nightly). This keeps your history intact while maintaining optimal performance. Additionally, as was mentioned, locking can be an issue. Easy options for this include separating processes. Example, your customers insert their Jobs to say a Job Entries table, which is then ingested to the Work Queue by a single process managed on your end. This also helps with things like load balancing and prioritization if you have more than one machine responsible for consuming the work or some work is more important than others. 
You could also consider setting the flag to 'P' (Pending) as soon as you grab it, but prior to processing it. This way if the job takes much longer than expected, your next job run wont grab it and try to process it a 2nd time (since it will just be looking for processed='N') Really you should just use a queue manager like gearman, but I get that it probably doesn't warrant it right now. 
Just a heads up that using Strftime in your join will likely increase the run time of the query. It may be better to bring it down into your WHERE clause. It looks like you want a return trip in 3 or less days, in TSQL I'd write it as WHERE t2.departure_date between t1.departure_date and DATEADD(dd, 3, t1.departure_date)
After the job is completed an entry is added to another table so I think that should suffice. Although load balancing isn't really an issue with this project it's been helpful to hear factors I should consider in the future with larger and more complex systems. Cheers for the input.
Use ‘union all’ 
Can you provide how the tables are related/joined? I think I understand but am a little confused.
Sure! In the order table there is a foreign key to the users table through user_id (which user made this order). In the ordered items table there is a foreign key to the order table, as in which order do these purchased items belong to. There is also a foreign key to the items table to correspond with the actual item. 
Do you need to sum any order totals among the users, or just provide the most recent?
Just the most recent order of every user. My main problem is trying to figure out how to get the most recent order of every user
Try this: SELECT [a.NAME](https://a.NAME), a.COST FROM ( SELECT [u.NAME](https://u.NAME), i.COST, ROW\_NUMBER() OVER(PARTITION BY [u.ID](https://u.ID) ORDER BY o.\[DATE\_PURCHASED\] DESC) AS "RN" FROM \[USERS\] u INNER JOIN \[ORDERS\] o ON [u.ID](https://u.ID) = o.USER\_ID INNER JOIN \[ORDERED\_ITEMS\] oi ON [o.ID](https://o.ID) = oi.ORDER\_ID INNER JOIN \[ITEMS\] i ON oi.ITEM\_ID = [i.ID](https://i.ID) ) a WHERE a.RN = 1;
Thank you very much! I actually don't have my own table since this was a hypothetical question I thought of to practice SQL and I had no idea how hard it would be. If i may, what does a.RN = 1 mean relative to the tables? and what does the row_number() call do relative to this query? I'm trying to parse together how you constructed this so I can understand how to make queries like this in the future. Thank you very much. 
Yeah, it's not hard. RN is just an alias I used for the ROW\_NUMBER() that I generated in the subquery. The ROW\_NUMBER() essentially says partition the results by the user id, then order the ascending number by the date purchased descending. So, if you had a couple users (1 and 2) that made multiple purchases, it would look something like: &gt;USER\_ID,PURCHASE\_DATE,RN 1,01/01/2019,1 &gt; &gt;1,12/01/2018,2 &gt; &gt;1,11/01/2018,3 &gt; &gt;2,12/31/2018,1 &gt; &gt;2,12/30/2018,2 &gt; &gt;2,12/29/2018,3 So on, so forth.
Woah that's smart. Thank you very much! Do you recommend any online resources for practicing SQL? (online interactive shell, readings, practice problems, etc)
Check if your version of SQL supports GROUP BY WITH ROLLUP Otherwise, yeah, union with another query that just groups by one column and has empty string or null for ID
It's a valid option, and one that a lot of systems implement today. But you know what works even better as a message queue? An *actual message queue* like RabbitMQ.
Another commenter (deleted his comment) suggested a similar solution. I'm looking into that but due to time constraints I'll be going with my solution for now. After I'm finished with that I'll start poking around with RabbitMQ and another couple that were suggested so for a future project I'll potentially use that. Thanks for the advice.
Hi there - your solution worked! I went into th schema &gt; table column constraints and grabbed the new names that Postgres had assigned after I had made the foreign keys. Using those names I was able to drop them. Thank you very much for your help, I really appreciate it, and can finally move on! :) 
Here is an initial starting point for your sql environment: https://www.w3schools.com/sql/trysql.asp?filename=trysql_asc
Not at my computer, but could you try importing them as lesser used characters like the pipe or something, then running an update afterwards?
Look into group by cube or as the other person said, Rollup. That does all levels whereas group by cube you can decide. [Microsoft Reference](https://docs.microsoft.com/en-us/sql/t-sql/queries/select-group-by-transact-sql?view=sql-server-2017) 
If you want to play around you can go on SQLFiddle. Or just install Microsoft SQL Server Express on your local machine and play with it there. I prefer SQL Server Express because it's got most/all of the functionality I think you want to play around with anyways. 
You need to explain your problem more clearly. I'm not sure what you actually want here? &gt; But when we have same number with both ACTIVE and ELIGIBLE status then you have to show only data with ACTIVE status. does this mean that if the total of ACTIVE and ELIGIBLE is the same, then only show active? Or do you mean if you results like this: 1, active 1, eligible you would want show the active result? 
This is what I would do: https://www.mediafire.com/file/u1me5v2hbo2hhma/SUSHANT7276.sql/file 
I mean If ID has both status values then select record which has status "active" otherwise select all
Check this out. [https://www.reddit.com/r/SQL/comments/afladq/select\_one\_record\_out\_of\_multiple/edzixnz](https://www.reddit.com/r/SQL/comments/afladq/select_one_record_out_of_multiple/edzixnz)
You might check whether Postgres has a built-in string sanitizing function(s). Something like Oracle's `quote_chars`.
Rollup worked perfect thank you very much!
Rollup worked perfect thank you very much!
 SELECT MSDSID, STATUS = ‘ACTIVE’ WHERE STATUS IN(‘ACTIVE’,’ELIGIBLE’) GROUP BY MSDSID HAVING COUNT(DISTINCT Status ) &gt;1 You don’t need to return the actual record, if you are only limiting to those two statuses, then this is pretty simple. 
What about other ids which has either status? Will above query return those too?
No. The having clause will limit to id’s which have both statuses. 
Then how to combine other records with this resultset?
&gt;MSISDN STATUS &gt; &gt;501041622 ACTIVE &gt; &gt;501041621 ELIGIBLE &gt; &gt;501041620. ELIGIBLE &gt; &gt;501041620. ACTIVE This is how i'd do it to get to your required resultset based on the post. &amp;#x200B; CREATE TABLE #Table ( MSISDN INT NOT NULL ,\[STATUS\] VARCHAR(10) NOT NULL ) &amp;#x200B; INSERT INTO #Table(MSISDN,\[STATUS\]) VALUES (501041622,'ACTIVE'), (501041621,'ELIGIBLE'), (501041620,'ELIGIBLE'), (501041620,'ACTIVE') &amp;#x200B; \-- DEBUGGING TABLE INSERT FOR REFERENCE SELECT \* FROM \#Table ; SELECT D.MSISDN ,D.\[STATUS\] FROM ( SELECT MSISDN ,\[STATUS\] ,\[RNK\] = DENSE\_RANK() OVER (PARTITION BY MSISDN ORDER BY CASE WHEN \[Status\] = 'ACTIVE' THEN 1 ELSE 2 END ASC, \[STATUS\] ASC) FROM \#Table ) D WHERE D.\[RNK\] = 1 &amp;#x200B; \-- CLEANUP &amp;#x200B; DROP TABLE #Table
This is how i'd get to it based on your original post expected results: CREATE TABLE #Table ( MSISDN INT NOT NULL ,\[STATUS\] VARCHAR(10) NOT NULL ) &amp;#x200B; INSERT INTO #Table(MSISDN,\[STATUS\]) VALUES (501041622,'ACTIVE'), (501041621,'ELIGIBLE'), (501041620,'ELIGIBLE'), (501041620,'ACTIVE') &amp;#x200B; \-- To check the above insert &amp;#x200B; SELECT \* FROM \#Table ; &amp;#x200B; \-- Actual required query SELECT D.MSISDN ,D.\[STATUS\] FROM ( SELECT MSISDN ,\[STATUS\] ,\[RNK\] = DENSE\_RANK() OVER (PARTITION BY MSISDN ORDER BY CASE WHEN \[Status\] = 'ACTIVE' THEN 1 ELSE 2 END ASC, \[STATUS\] ASC) FROM \#Table ) D WHERE D.\[RNK\] = 1 ; DROP TABLE #Table GO
Here you have some books and also where you can download them: [http://www.java67.com/2017/08/5-free-sql-books-for-beginners-and-experienced-pdf-download.html](http://www.java67.com/2017/08/5-free-sql-books-for-beginners-and-experienced-pdf-download.html) And here are some tutorials: [https://hackernoon.com/top-5-sql-and-database-courses-to-learn-online-48424533ac61](https://hackernoon.com/top-5-sql-and-database-courses-to-learn-online-48424533ac61) &amp;#x200B; Good luck!
Sorry I am new to reddit. Not sure why it says MySQL under my question. This is related to MS SQL Server. Though I am happy with MySQL tests as well.
Sorry, I am new to reddit. Not sure why it show MySQL under my question? This is related to MS SQL Server. Though I would be happy with MySQL tests as well. 
I'm new to reddit so struggling pasting the code so it's readable. How do people normally put code in so it's well formatted? Link to code example: [Example (Data in Pastebin)](https://pastebin.com/jemPnjHK)
[http://sqlzoo.net](sqlzoo)
Here you go: [https://www.microsoft.com/en-us/learning/exam-70-761.aspx](https://www.microsoft.com/en-us/learning/exam-70-761.aspx) I just got my 70-764 and this is the next one I'll be taking. Note that I never actually needed certs to get a job as a DBA, not one employer ever asked me for them. But I figured its about time I got them anyway.
Can't we use CASE or IF EXIST to make it more simple?
Depends how performant you’d want the code. I’m just trying to answer your question with an example of how to do it. Window functions perform well. 
Change WHERE Id = 1 to WHERE Id IN(1,2,3) Etc. 1,2 &amp;3 in the in would be the Id that you wanted. 
How about SELECT MSDSID, STATUS = ‘ACTIVE’ WHERE STATUS IN(‘ACTIVE’,’ELIGIBLE’) GROUP BY MSDSID HAVING COUNT(DISTINCT Status ) &gt;1 UNION SELECT MSDSID, STATUS FROM TABLE WHERE STATUS IN ('ACTIVE','ELIGIBLE') GROUP BY MSDSID HAVING COUNT(DISTINCT Status)= 1 Will this yield same result?
You’d have to try and compare. 
Yes.. thank you for your efforts and time 😊 ❤️
Look up analytical functions. You'll be using those a lot. 
The only ones worth getting are the ones offered by Microsoft/Oracle themselves. That said, most folks I know do not have a cert or if they did, they've let it expire.
Those are the ones I was talking about. Can't decide if it's even worth it to get any. 
It says “my SQL” in the title :)
Check out: https://www.eversql.com/sql-syntax-check-validator/ 
You gotta use an aggregate function. You need to introduce COUNT(1) into your SELECT clause. Everything that's in the SELECT clause must be either an aggregate function or must be mentioned in the GROUP BY clause. Then you ORDER BY descending by COUNT(1). Pro tip: if you use an alias in your select clause for the aggregate, you can use the same alias in your order by clause. 
How was the 70-764? I’m studying for it now. I’m not an expert but I’ve been working in the environment for a couple years, how long did you study before taking the test? I’m also interested in getting the 70-76 after as well
It’s hard to justify the cost, it isn’t hard to justify obtaining the knowledge. My personal view is learn as much as possible to succeed, learn then break stuff then fix it and repeat. My thought has been having the certification puts you ahead of the competition when considering employment. My experience is some people get certs but don’t know anything and when something breaks their piece of paper doesn’t come in and fix it for them. So if you wanna be good just practice if you test well and wanna try to misrepresent your knowledge with a certificate do that. But if you have put in the work and get the certification you’ll only benefit and grow and open more doors. Write off the exam cost at tax time and call it a day
I'm mostly an on-premise DBA, so the Azure questions where a little more difficult for me. Expect allot of questions focussing on security/encryption and you need to have a very good understanding of how backups and transaction logs work. I had several questions with disaster scenarios and how I would recover from backup.
Makes sense. I feel pretty capable of handling the day to day activities and break fixes that you're taking about. Literally all the cert would be is "justifying in writing" my knowledge. Don't take that as I can't learn more. I always believe someone can learn more, regardless of what they're doing. 
100% go for it. I’ll never forget about the time I got beat out for a project because the other guy had a certification and i didn’t (otherwise we were both very well qualified), the project was on ground breaking medical research. A rare opportunity that I would’ve been apart of. Happened years ago and stuck with me ever since. 
Damn, that blows. 
Try: /r/learnSQL 
Under your post, there should be an "edit flair" or "edit tags" or something like that.
I think HackerRank has some SQL tests.
First off, college knowledge of SQL is usually just the tip of the iceberg. If you like it now, you'll probably love it when you get into the more advanced stuff. No joke, read Microsoft TechNet (or RDBMS equivalent that relates to you) and learn about all the cool functions that exist that you didn't know about. The more analytical knowledge you have, the more valued you look for an entry level job. Lastly, no company expects you to know everything fresh out of university. Your first job will be one of the largest learning experiences that you ever have. 
Rollup is working perfect for having a grand total row and subtotals. I'm still not sure how to order my grand totals so I have the highest grand total first followed by it's subtotals, then the next highest grand total followed by its subtotals and so on?
It was a learning opportunity for me for sure. Also, the same decision I would’ve made if I was the one making the call. When it comes down to the big stuff like people’s medical attention, the certification shows you care with passion (after you prove you have the skills deserving of said certification) Other projects, guys with certifications just punch the clock, make excuses, and utilize billable hours watching YouTube videos trying to learn something they’re expected to know, given the certifications they proudly boasted about in the interview. Kinda venting here but I spend a lot of time cleaning up (uncommented) code after them. If you code and find yourself on YouTube during work time for hours each day, then you need a babysitter not a certification. I see this in places it’s easy to get away with like law firms (easier to give the perception of working since nobody else has these skills) yet uncommon in techier industries like media/finance (since more people know enough to tell when someone’s in over their head vs tackling complex objectives)
Thank you so much. I have a bundle for 70-765 with my 70-764, I’m still leaning towards the 70-761 over the 70-765 do you have any thoughts on this?
Thank you so much. I have a bundle for 70-765 with my 70-764, I’m still leaning towards the 70-761 over the 70-765 do you have any thoughts on this?
Thanks for the reply! So 1 in this instance is saying I want the item what appears the most ? Then DESC will take care of the rest?
My intention is to do both 761 and 765. Having 761, 764 and 765 will get me MSCA certified. Luckily my company is paying for it all.
No. Look for business-side IT positions. Can confirm, they do exist. 
What do you mean business-side IT?
There are generally positions in most companies that are actual "IT" positions, then there are positions for business units outside of "IT" that handle the department's internal questions and coding regarding only their department. 
&gt;I don't know if the queries in my current and past jobs were simple. Does happen. I was quite confortable with my SQL skills until I had to do a different job, so I've read and learned new tricks. Yet, sometimes I see queries, triggers and functions that still require a lot of attention before one can actually understand what they do, and most importantly, how they're doing it.
Out of the ton of coding competition websites out there how will you rank them purely for SQL practice ( for someone who's giving interviews ) from best to worst? Hackerrank Hackerearth Codeforces Leetcode Codeground Codechef Topcoder ( I'm not sure if SQL is there in all of the above, I just wrote this from a screenshot I had taken ) Thank you 
Apologies for the poor formatting in advance, I'm writing from mobile!
To be a data engineer or architect you generally need IT experience. You can be a data analyst or business analyst which will is a mix of IT and business as you will use tools like SQL but also be well versed in the business.
Consulting companies are always looking for fresh meat. I used to be an electrician and did 3 years of evening classes in IT. I signed up with a consulting company and first assignment they dropped me in was a L3 DBA position supporting over 200 SQL instances. And I'm not unique by far. Friend of mine was a graphics artist. She joined another consulting company and is now a .NET developer. I could keep going on. Some say you need to start at the bottom on a L1 helpdesk job and work your way up. If I had to do that I would probably be in jail for trying to strangle my users after the 99th call that the cup holder was broken... :P
You're probably looking for data analyst. Not business analyst, they do something different (more Project management) 
I have no idea, I've only dabbled a bit with hackerrank just to see what it's like. Never did any serious work on it or the otehrs.
Look into data analyst and business intelligence roles. The latter may or may not be considered IT depending on the company, but are generally open to non-IT experience. Data architecture and engineering are very deeply IT, you're unlikely to be successful starting there.
In addition to the MS Cert stuff VTOL mentioned, check out [sql server central](http://www.sqlservercentral.com). When I was still a dev and early in my SQL career, I thought myself mostly through a few good blogs and looking up whatever I didn’t know from their Question of the Day. Archive is massive and multiple choice. Even a few years in as a DBA, I run through a couple dozen when things are slow at work or if there’s a new feature I don’t know much about.
I failed the 761 (by one question) in for 2012 because about half the damn test was about querying XML...I had (at that point) worked in SQL for over 6 years and never had to query XML once... I have since used it a large handful of times..
Thanks for the heads-up on this.
Give http://pgexercises.com a try. It's not a scored test per se, but does follow a question and answer format.
Normally you put a star or a column in an aggregate function instead of 1. For example you could have column 'quantity' so you could do SUM(quantity). But because you don't need any particular column, just a count of rows, you can use constant like 1.
I can recommend downloading and restoring the example databases of adventure works from Microsoft on sql server express and doing some exercises like querying unit sales per year and region, typical customer profiles (#kids, #cars, bought bikes before etc). If you want to practice more intricate problems in databases like performance considerations or query tuning, you can get a copy of the entire stack overflow database but that requires sql server developer edition due to its size. Bbut I think developer is free now as well just you can't put it into production as you could sql server express.
You can seek data analyst role on business units. In the company I'm working, marketing department has a group of analysts working on queries to promote insights for decision making. That is a nice role, IMO.
I need to ask: how are you importing those data? Command line? If so, with batch inserts or COPY command? If you are using another tool, which tool. I think you first need to provide how you're doing it before we can help you to think in a way how to solve your problem. As a matter of fact, I often need to perform imports like that, I use command line with `\i` batch inserts. I usually only need to escape the `'` caracter.
[https://www.tekstream.com/oracle-error-messages/ora-00979-not-a-group-by-expression/](https://www.tekstream.com/oracle-error-messages/ora-00979-not-a-group-by-expression/) &amp;#x200B; &amp;#x200B;
Another pro tip: ORDER BY clause can mention the criterias by the column indexes (instead of mentioning the alias): ``` SELECT region, COUNT(1) customersAmount FROM customer GROUP BY region ORDER BY customersAmount DESC ``` Can be expressed like this: ``` SELECT region, COUNT(1) customersAmount FROM customer GROUP BY region ORDER BY 2 DESC ```
Well done! I just stared your repo. I intent to look closer to it this week. Maybe I can have some ideas and propose a PR? Thanks again and congratulations!
Try with "group by d1.dname"
Sounds to me like you want to return a default value. My SQL is rusty but I think you need a conditional statement or 3 with the EXISTS() function. Of course the real question is, why are you doing this in SQL and not whatever language you are programming the rest of your app in. 
Yeah that’s a pickle. If your personnel database would be potentially broken by deleting a record from the other database then I suggest you never delete or you bring over a copy of jobs, etc to the personnel DB that you only ever add to. That way you could reliably report on what a persons job was long after that job didn’t exist anymore in your company org. But you must ask yourself why are you deleting these records? Is that truly necessary? Or could you soft delete them or flag them in some way so they stay around but don’t impact other data?
Do you have a schema example? Typically we would never delete a job, just add fields 'Job Start Date' &amp; 'Job End Date' for example
Your best bet would be looking at data analyst / BI analyst positions. I started out working with excel and a little bit of SQL. As my knowledge developed I was able to move into a engineer role which revolves mostly around designing data warehouse solutions and more recently big data. If you are looking for the DBA side, I would recommend an IT support position and moving from there as you will have to understand more than just the database but the infrastructure around it as well.
If you are developing it then keep it in one database and split the company roles by schemas. Secure the accessibility of the sections at a schematic level. I’m his would make data constraints an automated step if you co figure foreign key constraints correctly. I guess that’s how I’d approach it if I was doing it greenfield. 
 SELECT nvl(bank_id, -1) as "BANK_ID", nvl(bank_name, "BANK NOT FOUND") as "BANK_NAME", nvl(branch_number, -1) as "BRANCH_NUMBER" FROM ce_bank_branches_v WHERE branch_number = '12345'; 
If this is a static report or query you're needing that's going to be permanent, and isn't some ad-hoc query then you can use PL/SQL with an EXCEPTION block. PL/SQL has really good exceptions handling. EXCEPTION WHEN NO_DATA_FOUND THEN // Do Something END; 
Another way to do this is to write your delete functions so that you remove the data from both places. We take this one step further also write an audit log to a transaction table and make use of audit tables that are never deleted and archived. 
Right... That was what is meant by group by exp... Worked perfectly man, tnx!
It's good for ad hoc queries, but I wouldn't recommend it, because SELECT clause can change all the time.
Thanks, but this needs to be run as a stand along query. I cannot use PL SQL for this. The only option I could think of is to make it a view, with which I am struggling as well.
Maybe your argument is based on working with dynamic SELECT generation. But without that bias, I disagree. Anyway, I'm not here to write on stone, you can write the ORDER BY clause you like!
It is Oracle, and I can only use SQL in it. I will explore the EXISTS() option. Thanks for your inputs.
Can you say: WHERE branch_number = coalesce('12345','-1') 
Select ColumnA, ColumnB, ColumnC From SomeTable Where ColumnA = {dynamic value} Union all Select -1, 'branch not available', -1 Where not exists ( Select ColumnA, ColumnB, ColumnC From SomeTable Where ColumnA = {dynamic value} ) 
try this trick SELECT nvl(bank_id, -1), nvl(bank_name, 'BANK NOT FOUND'), nvl(branch_number, -1) FROM ce_bank_branches_v c, dual d WHERE c.branch_number (+) = '12345' and d.dummy &lt;&gt; c.branch_number (+)
W3Schools
SELECT TOP 1 * FROM ( SELECT * FROM source WHERE branch_num = 123 UNION ALL SELECT * FROM VALUES (-1,'bank not found',-1) ) a ORDER BY Branch_ID DESC basically union in the dummy value, then sort by ID to prefer a real value
why do they need to be separate databases? what's wrong with separate schemas (as mentioned by /u/pacman_and_dot, which is the correct answer)
there is no TOP in Oracle 11, afaik
So your entire app is within the database? 
When using group by, you can only select columns that are: - Part of the group by expression - Aggregates (sum, count, etc) Think about it like this: What could the database do with values that aren't one of these? If you're asking the database to select department name but that's not part of the grouping function, what should the output show?
Nope. My group belongs to marketing, used to belong to operations. We interface with IT, have many IT related privileges, but we are not IT.
Yes. That makes sense. I'd been trying so many combinations wondering why none worked. Won't forget now though :)
This works!!!!! Thanks a lot!
No TOP 1 in Oracle
&gt;SELECT nvl(bank\_id, -1), nvl(bank\_name, 'BANK NOT FOUND'), nvl(branch\_number, -1) FROM ce\_bank\_branches\_v c, dual d WHERE c.branch\_number (+) = '12345' and d.dummy &lt;&gt; c.branch\_number (+) You could rewrite this to become a bit more universal (it would still need a bit of tweaking for other systems, but the nvl and the (+) make it extremely Oracle-specific). &amp;#x200B; SELECT COALESCE(bank_id, -1), COALESCE(bank_name, 'BANK NOT FOUND'), COALESCE(branch_number, -1) FROM ce_bank_branches_v c RIGHT OUTER JOIN dual d ON d.dummy &lt;&gt; c.branch_number AND c.branch_number = '12345'
You will need a table "dual" in a non-Oracle DB then
True, that's what i mean with some tweaking. Not all of them have a dual table like that, but Db2 has sysibm.sysdummy1 for example. For other systems, you could probably create a temporary view using a WITH statement. This one should work in SQL server for example `WITH dual AS (SELECT 'X' )` `SELECT COALESCE(bank_id, -1), COALESCE(bank_name, 'BANK NOT FOUND'), COALESCE(branch_number, -1)` `FROM ce_bank_branches_v c RIGHT OUTER JOIN dual d ON d.dummy &lt;&gt; c.branch_number AND c.branch_number = '12345'`
index your tables on columns used for joins and searches
I don't think that I've seen this mentioned: but if you're attempting to have two separate databases influence one another, you might consider a model that never deletes anything, but instead creates a record for what to do. So if a slowly changing table deletes a record, it will not actually delete the record, but APPEND a record that said, "ID so and so is no longer in this table." That way, you can have them talk to one another and feed updates and then from that generate what the actual relational schema is. It's a lot of engineering though so if you can ti them together it probably would be better to do that but sometimes the simple solution isn't an option one reason or another. (Particularly for enormous data sets that may be sharded etc.)
It depends on why it's running slow in the first place. Are you using table variables instead of temp tables? That's usually good for a quick win.
I'm not really taking about a specific issue, more the nuances that make queries run faster. Temp tables are a good one - I've never used a table variable, I just go straight to temp tables.
look into case expression
Figured out what went wrong and want to answer my own questions here for anyone to use for future reference. &amp;#x200B; If for whatever reason you are using MySQL x86 (32-bit), this is the issue due to 32 bit limitations. My logging program was compiled with mingw x86, so to interface to MySQL I also had to build the drivers in 32 bit. &amp;#x200B; If you still have the database, it is likely possible to recover your data by installing the corresponding 64 bit version of MySQL and pointing your data directory to the old 32 bit one. &amp;#x200B; TLDR: If you are running MySQL 32 bit, upgrade to 64 bit.
are you're using MySQL/MariaDB? "databases" in MySQL/Maria aren't actual separate databases, they are all in one database with separate schemas You can have foreign keys between schemas, but not databases
Thank you. I am trying that now. Went with: &amp;#x200B; CASE WHEN Work_Center.Type = 'Indirect' THEN SUM(Job_Operation_Time.Act_Run_Hrs + Job_Operation_Time.Act_Setup_Hrs) ELSE SUM(Job_Operation_Time.Act_Run_Hrs + Job_Operation_Time.Act_Setup_Hrs) END AS "Hours" But now I get two rows instead of two columns. I tried doing two CASE expressions and got two rows and two columns. I feel like I am getting closer, though.
I went with this then used expressions to change the data labels to the data I wanted!
Please do! I'd be happy to incorporate any feedback.
Yup, you get 2 rows since you had to add Work_Center.Type to your group by list, most likely. Now a bit of "theory": "Sum" (and other aggregate functions) work on an expression (what you put inside the sum function) across grouped records. "Case" expression selects a different expression based on a condition. It seems you want your "Hours" value to be an aggregation of different values depending on the record, so instead of choosing an aggregate, you'd want 1 aggregation of a conditional (ie. not a case -&gt; sum but sum -&gt; case).
[This is an overly simplified PDF I link a lot.](http://cdn.swcdn.net/creative/infographics/1403_Confio_SQL_Server_Tuning_Infographics_8_5x11.pdf) I think it does a good job of first glance and off the top of the head generic troubleshooting with performance however. Most techniques and tips should be understood before implementing. Sure adding NOLOCK can increase the speed of a query, but do you understand why it can be faster? The results could prove hazardous pending the circumstances. When I deal with performance, there is a basic philosophy I take when working. To conduct performance tuning, my approach is: 1. Observe and measure. a. (SP\_Who) – Measure and collect SQL from SP\_WhoIsActive. b. (PaulPerf) – Paul Randal Wait Stats. i. Custom Scripts 1. perf - Datafile space.sql 2. perf - find my sql plan.sql 3. perf - GoogleCpuLoadInfoV1.0.sql 4. perf - quickpoll stats.sql 5. perf - Table statistics.sql 6. perf - whatisrunning.sql c. (Ozar) – Scripts used to diagnose and troubleshoot. d. (Index) – Index sanity check list. 2. Question and research. a. (DPA) – Basic tuning check list. 3. Form hypothesis. a. Years of experience and education using the information collected and analyzed above. 4. Experiment and record data. a. Environment should be a replica of production minus the data should be faked and the hardware should be equivalent along with equal software. Exception can be made for lower level testing and deployments through varying layered architectures. 5. Analyze and conclude research. a. Results should be analyzed. i. Hot and cold results. (Storage / memory) ii. Query plans / Wait statistics / Business impacts / Data integrity 6. Replicate and test. a. If you understand the problem and solution, you should be able to replicate and test and finalize a project overview with start to finish detailed documentation to allow another user to re-create and follow your process to replicate the same result. There are a few scenarios in performance tuning: 1. Everything is working. You measure and gauge a baseline. From the baseline, you pick your top worst performers that would have the largest impact to the business to increase performance and work from there. 2. Something is broken, you find the specifics of the problem and resolve it. 3. You are developing something new and you want it to be performant, scalable, and easily administrated or automated. You measure along the way, gauge baselines, and try to break or improve the baselines. My philosophy on baselines and gauging the typical day to day is that you set your tool that you will use to measure to capture information based on X. Where X is the amount of time you care about. If I run a query to capture all queries currently running every 5 minutes and it logs that information into a table, I am saying that I only care about queries that run frequently enough to where you would catch it in between 5 minute run times or queries that run longer than 5 minutes. If I set this threshold to 5 seconds, that indicates I care about all queries that I can catch in 5 seconds or runs longer than 5 seconds. If I place an agent monitoring tool / profiler trace / extended event trace on my instance, I care about everything that ever runs for the duration of the monitoring. 
Look into "not exists" condition, for example
Could you let us know which database you're using, and provide more detail about the table you're querying - most importantly, how the dates and times are stored? It looks like you've got a field for year and one for month, which would be very unusual - is that correct? Also, is 'startmonth' and 'endmonth' parameters? Your example would return any record with the word targetyear in the year column and anything with a string in the month column that are between the words 'startmonth' and 'endmonth' alphabetically. Imagine for a second a less crazy world, where you're using say MS SQL and have a single datetime activity_date. Then it might go something like: Declare @StartDate datetime = '2018-01-01' ,@EndDate datetime = '2018-12-31' ,@location = 'Your location name' SELECT ua.user_id FROM user_activity ua WHERE ua.activity_date between @StartDate and @EndDate AND ua.location = @location AND user_id NOT IN ( SELECT ua2.user_id FROM user_activity ua2 WHERE ua2.activity_date &gt; @EndDate AND ua2.location = @location) Simpler, and more universal, would be to just identify whether each record was the last record for that person - that's not quite what you asked but might solve your problem: With User_Activity_final_status as ( SELECT * CASE row_number() over (partition by user_id order by activity_date desc) WHEN 1 THEN 'Final activity' ELSE 'Not final activity' END as Final_Activity_Status FROM user_activity), SELECT * FROM User_Activity_final_status WHERE --your filters go here AND Final_Activity_Status = 'Final activity'
Just wanted to mention the test you took was the 70-461 vs the 70-761 which are both similar. You would want to look at the materials they test on though as they do sometimes change the material necessary to pass. The 70-761 may or may not have questions regarding XML, but also JSON. The 70-461 is limited to XML material and does not include the new JSON features as that was introduced in SQL Server 2016.
Thanks for the clarification I forgot the exact number just remembered that it was the "querying SQL Server" test), it makes sense that they would update it over the years, and nice that JSON is now supported (my company is still on 2012 and we usually run 4 versions behind on all tech)
Read this : Database Design for Mere Mortals: A Hands-On Guide to Relational Database Design (3rd Edition) by Michael J. Hernandez Only you and your company can answer these questions, we on this sub have no idea what information you want for jobs, people, departments etc. Does a job have a 'level', 'grade', salary constraint etc. Does a person need their latest appraisal stored, next of kin info, training received etc. Do you have to keep certain information due to legal requirements? Since you are storing employee information do you have other liabilities (privacy stuff)? This is database design and is a very highly skilled occupation. I've got (too many) years as a DBA and I know how to spot a bad ER model, but I'm nowhere near the level required to create a good one. This is one of the reasons NoSQL got so popular - people who can create database models are EXPENSIVE. Good luck.
Well u kinda have the issue where a value is null, but the row exists. It depends if u want this behavior or not for those cases 
1. Which database- Sql server 2. You've got the table i'm querying in full there. Yes it has a int value for month and an int value for year as two separate columns (not my work, just how it was given to me). 3. I don't actually have parameters in the query, this is a one and done, so i'm just hardcoding start and end ranges. So for example where year &gt;= 2000 OR where year = 1999 and month = 12 sort of thing. That said I was able to hash it out from your example. Not sure why I couldn't get the other responses "exists" solution to work, but after seeing how you structured you example I was able to warp my NOT IN query to work (which i had tried before but I think I blew the syntax).
EXISTS or NOT EXISTS instead of IN or NOT in. If you don’t need a column from the second table, using exists vs a Join/IN will speed it up a ton. Also, CTE has helped me a ton as well.
&gt; EXISTS or NOT EXISTS instead of IN or NOT in. &gt; If you don’t need a column from the second table, using exists vs a Join/IN will speed it up a ton. Does this also extend to something like this? CASE WHEN ID IN (1, 2, 3, 4, 5) THEN 'Yes' ELSE 'No' END As Column I use these a lot in case statements, so if there's a better/faster way I'm all ears.
I got it, thank you so much for your help!
IN is great to use when you have hard-codes values. But EXISTS is better when using values from a Subquery.
Could you do me a favour? Find the person who designed the table and tell him that I think they're dumb. Thanks.
Not OP, but I'm currently supporting an application that was built entirely in Oracle Forms. Pretty much all the logic in this app is a stored procedure, package, trigger, view or some form of Oracle policy or framework within the database (eg user access is enforced with Oracle Virtual Private Database; web APIs are essentially stubs calling PL/SQL procedures; etc). When I started, I was floored; this app struck me as everything I know you *shouldn't* do (business logic in the data layer, etc). But this application is absolutely a thing, and you'll find there are plenty of things just like this out there, *especially* in Oracle-land. I should mention that this is a commercial product my employer has purchased, not an in-house solution (although it's been heavily customised and extended). It's also impressed upon me how good oracle's database engine is at optimisation. Some of the crap in this system involves poorly written dynamic SQL being executed in near-real time as part of web transactions (please don't ask, the less I think about it the happier I'll be... Needless to say that API is **not** directly exposed to the front end!) and yet somehow, no matter how badly the devs seem to try to write terrible procedures and queries, it seems to know how to compensate for lack of developer competence and not suck performance-wise. Admittedly that's not a good thing for encouraging code quality. 
IIRC, Equivalent is something like this: `SELECT c1, c2 FROM t1 ORDER BY c3 FETCH FIRST 1 ROWS ONLY` Has pretty negative performance implications though (for complex queries, the entire one still executes. In our case we found it was exponentially better to dense_rank() or row_number() in a CTE or subquery and select only #1 from that). 
Gotcha. Super helpful, thank you.
sqlzoo.net above all. You can start learning some SQL right now, interactively, with no up-front time invested. Secondary to that, livesql.oracle.com. Get a free account and have a database at your fingertips, with limitations of course. Thirdly, download an "Oracle virtual appliance" which is a virtual machine pre-loaded with linux and Oracle 12c. You have full control here. Next, spin up your own VM with Microsoft SQL server if you want MS stuff. But again as far as learning some useful stuff quickly, sqlzoo might be all you need. 
Thank you for the suggestion, just started it, still will be looking for more resources Incase I finish with time to spare 
thanks for the link!
avoid using cursors. if you need a small set of data from a large table, then create a temporary table to populate with just the subset that you use. then reference the temp table instead of the original table.
[https://aws.amazon.com/certification/certified-developer-associate/](https://aws.amazon.com/certification/certified-developer-associate/)
MCSA if you're using MSSQL. 
MCSA if you're using MSSQL. 
&gt; CTE has helped me a ton as well. FYI, CTEs are only a performance enhancer in Oracle because Oracle can materialize them to temp tables. In SQL Server, they will not improve performance as it does not have that ability.
You could aim for an Oracle certification in SQL or a Microsoft Certification in T-SQL. 
This is very very true, however CTEs in SQL server allow you to isolate a smaller subset of data while retaining the source table’s index. So there is a performance advantage if you’re running aggregates or recursive CTE. Not to mention the organizational advantages it has.
That's really interesting, thank you.
Why do you need the MAX() function here?
Going to back this. At the very least the newest book for SQL, https://www.amazon.com/Exam-70-761-Querying-Data-Transact-SQL/dp/1509304339, was rather good. 
Thanks! Is that free or paid?
What about MySQL?
Is this okay with MySQL?
Oracle? MySQL?
Read sql for smarties by celko
Well, kind of. MySQL, SQLServer, and all the others all have syntax unique to that platform. But the base for most of those is T-SQL (Transact SQL). The book that I linked is MSSQLServer's implementation of it, so there is some things that are unique to SQL Server. However, the book focuses heavily on the shared T-SQL fundamentals. So you will still get something out of it I think. I have never actually looked into MySQL specific training as all my jobs have been at microsoft shops, so I wouldnt know quite so much about MySQL Specific (although I use it regularly) training. 
to pull a single value out of multiple rows NULLs are ignored by aggregate functions like MAX()
It tells you when you click the link. 
Learn TSQL. It's very very lucrative to get an Enterprise gig. Highly recommend learning Postgresql.
Congrats!
Thanks! This community has been very encouraging. 
I’m a noob to all of this. What are their differences?
So T-SQL is the base for MySQL? I’m still learning the ins and outs of SQL. It’s still somewhat blurry since there are so many DB’s. 
There is a registration fee on some. But this is a good find. 
Will do when I get a chance to go to Barnes N Noble this weekend. I might have to since you guys are feeding me such great information. 
I want to make sure that I learn what I can in SQL to be somewhat in the average range before I go back to Python. SQL &gt; Python &gt; ? Not so sure about the 3rd one. Others recommended either JavaScript or PHP. 
Awesome do they have something like that for python developers?
Congrats!
it's on safaribooksonline.com which has a free trial (and is easy to extend with multiple email addresses ;)
Yes, but in Oracle 12 and later
If you dive in CE_BANK_BRANCHES_V you can see, it's not null https://docs.oracle.com/en/cloud/saas/financials/18b/oedmf/CE_BANK_BRANCHES_V-view.html https://docs.oracle.com/en/cloud/saas/financials/18c/oedmf/CE_INDEX_BANK_BRANCHES-tbl.html 
I'm pretty sure we've both worked on the same application before, haha. The application I was maintaining was an Oracle Forms enterprise app with all but the simplest of functionality held in the database. There were around a half a million lines of PL/SQL that ran this thing. Was insane to try and maintain, but you learn a lot at least.
Hehe. Thank you. 
Thanks man! There will be more to come and I’ll share the experience to this community. Hopefully it inspires and encourages other people to join. 
I can't speak to how long it will take you to get the certification (I'd say a month or two of dedicated study, less if you take the easy route and just rote-learn exam braindumps). But generally speaking, there aren't really a lot of entry-level DBA positions. The DBAs I know all ended up there after gaining a lot of experience, generally in roles like sysadmins, data developers, or cloud/infrastructure engineers. A DBA cert isn't going to get you a DBA job straight out of the gate without a lot of relevant experience, DBAing is just too important to leave to people without a proven track record and the job itself doesn't really lend itself well to having junior/assistant DBA roles.
I know virtually nothing about python, but JavaScript is the hottest thing since sliced bread so definitely jump on that train.
No, T-SQL is proprietary and specific to SQL Server. It is mostly compliant with standard SQL but also has a lot of more add-ons for more procedural type programming. Plus some built in functions like IIF(). In the professional world you tend to see SQL Server or Oracle at bigger companies IMO. Some PostgreSQL as well. 
congratz! im currently working on mine at my job :)
If I were you, I would think about the environment you want to work in and the job classes that go along with it. SQL itself isn't really a "field." It's really a split between database administrators and developers. DBAs tend to be specialized in specific environments and often are not the best pure "programmers", but they have to know how the systems work really well from architecture to memory management. If you wanted to do administration, there are specific certifications both in the db you would administer and the OS they generally reside on. So a SQL server dba would want to have a foundation in SQL Server administration and Windows Server administration + batch or Perl. Oracle DBAs tend to work more in Linux, so a background in Red Hat adminstration and Bash would be solid for that job. Everything is changing now though, so knowing distributed systems (Hadoop, mongo, etc) too as well as deployment tools like Jenkins and Azure would be big plus. On the dev side, the hours are better and there is less firefighting, but keeping up with the changing landscape means constantly learning new tools and languages. I'm an ETL(ELT) developer. My job means being really fluent with my source and target systems so I can performance tune on both, and knowing all the ways I might interact with those systems beyond SQL and my ETL tool (ODI). That means OS (Batch/Bash), application (Java, groovy), and web (js, python, jython, API development, JSON dictionaries). That said, you will learn a lot of these on the job, but understanding the workflow is really beneficial.
I will do a job search to see but in this community opinion, which is easier to get into, Oracle or MS, and which is an "easier" job, developer or administrator?
&gt;which is an "easier" job, developer or administrator? Depends on the person, I personally prefer the administration side, while my coworker prefers the development side, but we're both considered 'DBAs' inside our company. *shrug* For me MS was easier to get into than Oracle, but that's just because my first (and subsequent) jobs working w/ SQL were all MSSQL.
The common differences between PL/pgSQL and TSQL are some data types such as BOOL vs BIT, or timestamp vs date time; some functions such as now() vs getdate(); and DDL shortcuts such as SERIAL vs INT IDENTITY(1,1). However, they are pretty alike.
Not something that would fit in a comment. Google is your friend.
From what website you take the course?
Ok first off don't use varchar(max). You don't need that much space for this data. Secondly your data did not have more than 1 level of children for MyRoot. I added a test record to ensure it works with more than one level. Thirdly - this will error if you have more than 100 levels or a circular loop in your data (e.g. a child of a child that points back at the parent). declare @Name nvarchar(100) = 'MyRoot' create table #t (ID int, P_ID int, Name varchar(max), TypeOf char) insert into #t values (1, -1, 'MyRoot', '') , (2, 1, 'A', '') , (3, 1, 'B', 'B') , (4, 1, 'C', 'D') , (5, -1, 'B', '') , (6, 5, 'B1', '') , (7, 5, 'B2', '') , (8, 7, 'B21', '') , (9, 7, 'B22', '') , (10, -1, 'D', 'E') , (11, -1, 'E', 'F') , (12, -1, 'F', '') , (13, 12, 'F1', '') , (14, 12, 'F2', 'G') , (15, 12, 'F3', 'B') , (16, -1, 'G', '') , (17, 16, 'G1', '') , (18, 16, 'G2', ''), (99, 2, 'ZZZTest', '') ;with CTE AS ( select * from #t where name = @Name union all select t.* from CTE C JOIN #T T ON t.P_ID = c.ID ) SELECT * FROM cte SELECT * FROM #t DROP TABLE #t declare @Name nvarchar(100) = 'MyRoot' create table #t (ID int, P_ID int, Name varchar(max), TypeOf char) insert into #t values (1, -1, 'MyRoot', '') , (2, 1, 'A', '') , (3, 1, 'B', 'B') , (4, 1, 'C', 'D') , (5, -1, 'B', '') , (6, 5, 'B1', '') , (7, 5, 'B2', '') , (8, 7, 'B21', '') , (9, 7, 'B22', '') , (10, -1, 'D', 'E') , (11, -1, 'E', 'F') , (12, -1, 'F', '') , (13, 12, 'F1', '') , (14, 12, 'F2', 'G') , (15, 12, 'F3', 'B') , (16, -1, 'G', '') , (17, 16, 'G1', '') , (18, 16, 'G2', '') ;with CTE AS ( select * from #t where name = @Name union all select t.* from CTE C JOIN #T T ON t.P_ID = c.ID ) SELECT * FROM cte DROP TABLE #t
&gt; A DBA cert isn't going to get you a DBA job straight out of the gate without a lot of relevant experience Unless it's a company that sells DBA services (or questionable quality) on the cheap and gets by on saying "we have X,000 Microsoft-certified DBAs ready and waiting to support your systems, call today!"
Excellent question! Totally on you. What do you want to do? This is the part of the game where you've mastered the basic controls, and now you're forced to go down a path. Do you want to be a good developer, a moderate analyst, or an evil DBA? :)
 cast(ps.PS\_Date as date) &gt;= cast('2018-07-30' as date)
Also googling tsql or mssql cast would have gotten you there...
Thanks but this doesn't work in Access. The issue here is the Access side of the code. I ran the query without CDate in SSMS and it runs just fine, but putting that into VBA to execute on SQLServer and it doesn't work again. 
Hey, an "evil DBA" here. And i would nobody with an genric SQL certifiacte interact with any of my 200 Database Clusters! &amp;#x200B; Either learn something usefull ( like PostgreSQL ) or go and install your own SQLite! 
Evil DBA here, as well. I wouldn’t let anyone touch my clusters unless they work here and are also a DBA. And even then, some of them are iffy. Lol. We all start somewhere. Choose your path, and then go learn about it. He doesn’t need your clusters to download an adventureworks database, and learn about backups and restores, security, stored procedures, views, triggers and functions, indexing, Ola Hallengren’s script, system databases, learning about temp tables, learning about IO and how there’s a cost analysis to doing things. You need a direction. Start there. 
It would be so great if this would work, wouldn't it? &amp;#x200B; But the reality looks different. &amp;#x200B; I'm talking about "nobody touches my database" of course \^\^ &amp;#x200B; You are right about the point to start and the direction you go
Why wouldn't it work? You start off learning about these things, and then you go out in the world and you find yourself a Jr. DBA position. That's exactly what I did, and it paid off. It's not hard. It took me a month to find it. I had to move cities, but it was totally worth it. These days, there are DBA consulting groups (Dallas DBAs is currently looking for a Jr. DBA, and I know Kevin Smith personally to recommend).
&gt;I'm talking about "nobody touches my database" of course \^\^ Because there are users, and developers, and data analysts. &gt;You are right about the point to start and the direction you go ￼&gt; You start off learning about these things, and then you go out in the world and you find yourself a Jr. DBA position. That's exactly what I did, and it paid off. And i hope it will for others to! I'm always happy if there are new, young people, coming to us to work as DBA. Because there are not many they want that. 
Happy cake day!
I'm not sure why you need interoperability between sql server and msaccess, but write a function 'cdate' in SQL server, maybe?
There are many differences between ANSI SQL, MySQL, PLSQL, T-SQL and many other databases. Most Relational Database Management Systems (RDMS) have their own variation of the language SQL. They mostly adhere to ANSI standards, but all of them deviate in some capacity. Because of this, the syntax, actual commands to use, and the way the same command operates can be different between systems. To summarize at a high level, T-SQL is used in SQL Server by Microsoft, PLSQL is used by Oracle. PostgreSQL and MySQL have their own variations but I don't believe they have an "acronym" for their variations of the SQL language. There are many more differences, but it is much more nuanced and deep.
MySQL is in the top three databases used in the world and I do believe there are certifications you can pay to take (not too expensive, but it will cost you). What I would personally recommend is to look at the job market where you live or where you want to live, what kind of SQL jobs do you see? Is Microsoft SQL prevalent? Or maybe it's Oracle? While MySQL is commonly used, it's not AS common to see used in Enterprise environments. It's one of the most popular back ends for websites, which is why it's so high in the rankings. Most web sites won't need a DBA, so MySQL is not usually the easiest to find a job in. That said, I have seen an influx in Remote 10+ year experienced MySQL folk and I have been trying to hire for a local MySQL DBA too. It's not frequent to find jobs with this skillset and there's not vast competition with this skillset either. (Anecdotally and locally anyway.) If you start learning one version of database, your skills can transfer over to another. So don't feel like if you begin using MySQL or Oracle or SQL Server that you're locked into that version. I've worked on: MySQL, SQL Server, Sybase, DB2, and Oracle.
For anyone attempting this, I highly suggest [acloud.guru](https://acloud.guru) courses. You can get them for $10 pretty often on Udemy.
Basically a subform in MSAccess RowSource needs to be populated with SQL Server data. As it's a RowSource you can't use a pass-through query which would fix this issue. So it has to use the Access/JET SQL syntax and I just cannot make this work. 
Just as [distraughthoughts](https://www.reddit.com/user/distraughthoughts) and [paulinobruno](https://www.reddit.com/user/paulinobruno) said, there are many analyst positions outside of the IT department at many places. Where I work we have several data analyst positions in the actuary department and marketing. Their knowledge of SQL is more limited to basic queries, but they use it. Obviously they also have skills in their respective departments as well so take that into account.
Is it just fun or maybe you have practical purpose to apply this engine for real projects?
I"m basically at near zero in MSAccess, but can you put your query into a view on SQL server, link a table in msaccess to the view and then link your subform to the table? 
This is a pretty straightforward question... what are you having issues with? Count() where OrderStatus is 'Completed' order by DateOrdered
pretty new to SQL honestly. theres so much online about it...i get pretty overwhelmed
The script would look something like this: SELECT dateordered, count(*) as “NumberOfOrders” FROM company_orders WHERE orderstatus = “completed” GROUP BY dateordered 
 TIME_ADD(timestamp2, INTERVAL 30 MINUTE)
what you are looking for is called "SQL Injection", and is generally a terrible idea. Have fun.
The question was how do I make this work in SQL server... Why not pull the data from access using ACE oledb or a linked server?
SQL and a BI tool like tableau is usually good enough to land a data analyst gig. The problem you might be running into is that employers want to see you with experience doing those things. Which lands you into the ol' how do I get experience in the first place issue. If you have any sort of experience in the past that you could say you used SQL at is one strategy, but just make sure you can walk the walk by learning it on your own.
Thank you 
SQL is a programming language /pedant
&gt; If you have any sort of experience in the past that you could say you used SQL at is one strategy This is very difficult for Equity Research. I did have to create lots of charts in Excel but no one used Tableau for this. Someone with Wall Street experience can easily call BS on me.
I guess, one thing I could do is go back to my charts and re-create them using Tableau and link them on my CV as a portfolio. What do you think? 
you probably need to use the LIKE statement. Google it. &amp;#x200B;
I'd say udemy if you need something to put on your CV as a qualification. Seems more official than YouTube even though the content is probably the exact same.
Udemy always has courses on sale and there pretty good from my experience. I recently bought "The Complete SQL Bootcamp" course from Udemy and I'm really enjoying it. 
JavaScript definitely is. Most of the people I’ve talked to either an actually programmer or enthusiast have mentioned JavaScript at least 3 times in our conversation. The thing is, I’m not really into the front end part. Leaning more towards data science, analysis, machine learning, and AI. 
I see. I’ll definitely check out T-SQL. It seems like the next step to take. 
Thanks. How’s it going for you?
Might be a bit overkill but if I understand correctly things like Elasticsearch are designed exactly for this type of thing. 
why not both ™
I started in the industry without a technical background as well. when I started all I knew was some sql and familiarity with a popular data integration tool at the time. today I'm a data architect. definately possible. there's some other stuff that you can pickup to put on your resume as well that will make you more attractive - etl, data warehousing, aws/azure data services etc. I think coming from an equity research background is a good fit for transitioning to a more technical role. pm me if you need a resume review or if you have any specific questions.
For what it's worth I use T-SQL every single day in my job and I love it. Definitely a very hard language, especially if you only know procedural languages. If T-SQL is your first language I'd argue that's a benefit lol 
When I mentioned “field”, I was talking about programming languages. I am not sure if SQL is even considered as a programming language. But field was the best definition I can associate it that time. I jumped fields before. Started with Python, then switched to Swift, then SQL, then back to Python. Caught myself going in circles so I decided to stick to just one and narrowed it down to SQL. Your breakdown on this one was great! The light at the end of the tunnel isn’t that small and blurry anymore. I’m learning a lot from you and the guys that commented on my post. Would you say that T-SQL is a good next step?
i cannot install third party app Unfortunately 
With Udemy, you're paying for a curated product, which I think is a benefit if you're just learning a topic. With a comprehensive course like from Udemy, you're less likely to end up with a hole in your knowledge. If you want to brush up on a specific topic, then watch a YouTube video &amp;#x200B;
where lower(name) LIKE lower('%input_string%')
I have a problem with any company that has SOL in their name...
Can you link us to this self learning program? I wanna improve my SQL skills as well.
Something like this. Select top 10 name ,[Total_Comm] = sum(commission) From table_name Group by name Order by 2 DESC
If commision is total and not per share, then wouldn’t it just be: Select name, sum(commission) as total_commision from table Group by name Order by sum(commission) desc Limit 10; ?
So it would be : where lower(@pCustomer) LIKE lower('%input_string%') ? 
Honestly I would say hone your ANSI SQL. T and PL have their uses, but the foundation is solid ANSI SQL. Most problems can be solved without them and people who over rely on procedural languages in SQL tend to write really inefficient code. I only use PL for dynamic string building for reusable functions/stored procedures or for error handling and even those use cases are pretty limited. Learn your tuning utilities and really understand explain plans. From a developer stand point, understanding the differences in the platform optimizers is huge.
My follow up question would have been: "can you rewrite this using row_number and not top/limit?"
The variations I understand. But it seems like there are only 2 main server players... Microsoft and Oracle. 
SoloLearn. 
Glad to help! :)
I want to be all of it! But a moderate or somewhat close to average analyst would be nice. I am leaning more towards the data science/analyst/ machine learning/AI field of study. Python is next on my list. 
Great information! I gotta start somewhere quick. Since MySQL is already within my reach, I’ll jump on it and get it out of the way. Since MySQL is handled by Oracle now, it is just fitting to jump on Oracle next. I’ll figure out what’s hot or not in the market later. Right now, just really looking to learn. Focus on one thing and finish it. Get it out of the way and move on to the next one. 
https://en.wikipedia.org/wiki/Transact-SQL Its a bit more than just Microsoft, but its mostly related to sql server. But there is a vast overlap of syntax because t-sql and mysql both extend ANSI SQL standards for their own proprietary uses. Think of it as a way to say, different platforms want more tools than just basic ANSI SQL. But ANSI SQL is the standard they are based off. My earlier comment meant to say ANSI SQL is the base, not T SQL. My apologies. https://www.w3schools.com/sql/default.asp has a list of syntax unique to certain platforms listed out for you. 
Mobile reports deployed to SSRS and consumed via SSRS or the power BI App?
&gt;&gt; Since MySQL is already within my reach Just a FYI you can get a free version of Oracle, SQL Server, PostgreSQL, MySQL, MariaDB, DB2, Sybase, and many more. Most DB's have a free or developer version you can learn on. &gt;&gt; Since MySQL is handled by Oracle now, it is just fitting to jump on Oracle next. If you are just "writing" SQL, it is not a massive transition between all of the languages but there are a lot of differences. In addition, I have spent the majority of my IT time on SQL Server and I would consider myself novice to intermediate after eight years of experience and multiple certifications. It truly depends how far you want to go down the rabbit hole on each database. And while you can learn and transition between them relatively easily, I think it's best or easiest to pick a single platform to try and stick primarily to. (Not to say you can't change direction or become flavor agnostic later in your career.) &gt;&gt; Focus on one thing and finish it. Get it out of the way and move on to the next one. This once again truly depends on what you want to do and where you want to be. You can honestly devote your entire career to databases and SQL, there is really no dead end to it.
just add a boolean flag which tells you if the record has already been used?
Thanks! I understand that it's not compatible with SharePoint though... or am I wrong? That would be great...
You're not wrong, Walter, you're just an asshole.
Lol get out of here bot....shoo!
I'm checking to see if the record already has a worker\_id before assigning it - similar functionality. The problem is that if two requests happen at the same time, the database will not have updated that record with a worker\_id yet. So two users will get the same assignment and then the worker\_id will get overwritten. I need to lock the resource for writing when the first user requests the assignment. Not sure how to do that though.
What SQL db are you using? In MS SQL, this would be pretty easily accomplished just by adding transaction isolation serializable. https://docs.microsoft.com/en-us/sql/t-sql/statements/set-transaction-isolation-level-transact-sql?view=sql-server-2017
I'm using postgresql
I think most of the other users just don't understand what you're talking about to be honest. I have not heard *great* things about the MS SQL version, but I believe this is what you're looking for: https://docs.microsoft.com/en-us/sql/relational-databases/search/query-with-full-text-search?view=sql-server-2017
You should google transaction serializable for postgressql. It probably supports it, it's a pretty great sql db.
Could you make stored procedure on SQL server and exec that in VBA?
.sql files don't contain data that you then view in a table. They contain the scripts to create, modify, update insert etc. You should be able to open the file in any robust text editor. My personal favourite is Sublime but notepad++ will work fine 
Ok thanks, I'll look it up.
You can definitely get a job as a junior data analyst with just SQL and Tableau experience. If you have been applying to those jobs and haven't gotten a response I would take a look at your resume and how you're presenting your skills and history. 
There is a tool called https://sqlitebrowser.org/ that you can install that doesn't require configuration like most RDBMS engines do. Just install it, make a database file, and then do File -&gt; Import -&gt; From .SQL. Point it at your SQL file and it should import. Depending on the dump of the SQL file, you'll possibly have errors as each RDBMS has slightly different syntax, but you should be able to get just about anything in with minor changes to the file.
As someone recently hiring junior BI developers I was impressed by those that had made sample BI apps and published them. They sent links as a demo with job applications. Mainly Power BI as you can publish free on cloud. They tailored their apps to the sector I work in (pharmaceuticals) using publicly available datasets from the UK government. They showed how they had integrated data into SQL from the various sources and how they had modelled it in SQL before the BI layer as proof of there competence.
Hi. Thanks for the reply. I did “File, Import, Database From SQL file” and chose the .sql file. I clicked “Open” and it asked me “Choose a filename to save under”. I didn’t understand this part. What should I do?
Well, it’s technically in the same suite. So you can jump from sharepoint to PowerBI in one click, and it uses the same Workspaces as sharepoint. The user is already logged in (assuming you have O365) and it’s a really smooth transition, I would 100% do PowerBI in your situation.
So this is going to depend on how your SQL file is constructed. If it's just a table of data, you'll actually have to create a database file *first*. If its actually a database (as in, it describes the tables) you'll do what you did previously and indicate the name of the file in which you want to name your database. Normally, a relational database is an application handles where tables and other data are stored itself. With SQLite, you specify the location of the database as a file (same as, for example, an Excel spreadsheet).
&gt; I was impressed by those that had made sample BI apps and published them. They sent links as a demo with job applications. Upon thinking about how to do this, I realized that there are lots of gaps in my knowledge. In the classes that I've taken, all the data has been made available for me and all I've had to do is return queries. I haven't really transformed/cleaned/etc. the data. What exactly do you mean by modeling the data in SQL before adding it to the BI layer? 
Thanks! That sounds perfect, but are you sure that doesn't require SharePoint online? I'm using SharePoint 2016 with EUM to manage external access. I know I can't deploy Power BI to this but I can to SharePoint online. I do have office 365 though.
PBI plugs directly into the O365 model/suite. You use your same O365 user. For $9.95/mo per user you can deploy a PBI report to any workspace. All viewers need a pro license as well.
ETL. Extract data from source, Transform data to clean and normalise, Load data into SQL. You can then model the data in SQL or the BI tool of your choice which is essentially joining and aggregating the data in the most useable way for your desired application. The model defines the joins, relationship and aggregation of that datasets. There are lots of consideration but reading about Kimball architecture data warehouse will probably give insight into considerations when modelling , such as slowly and fast changing dimensions, keys and performance Vs ease of use. Good luck.
To open code as a table you need to run the code against a database
Yes, thanks. I'm a big fan of Power BI. But in this use case, I want to deploy reports in an existing SharePoint 2016 site and not require those users to have a different login to see the reports. The Pro option requires an additional login unfortunately.
Do you think you could tell me what I need to learn in order to achieve a project that I just thought up? Suppose I want to analyze economic data (inflation, GDP, population) from various sources: Fed, BIS, IMF, etc. If I were doing this in Excel, I would simply download the data as a CSV file and import it into Excel. And then I would create the various charts in Excel. If I were to use SQL and Tableau, what role exactly would SQL play here and what would I actually do with it? Would I simply be replacing an Excel spreadsheet with the SQL database? And thus, is Tableau doing all of the work? 
Good idea.
I would focus on automating the solution eND to end. so you source data from some website. pick one with an api that you can write a script using python or language of your choice to source a csv from. then create stage tables in a sql dB like posgres. or sql server to load the data. again this should be done programatically. you can clean/ confirm the data in this stage as well in a programmatic way. then create reporting tables that the data can get loaded into. the key here is that the design should retain history and you should be able to query the interesting economic measures in various ways. the design should be performance so read up on star schema design. the data loading again should be done via a script. then build a tableau sahboard on top of this data. you'll need to sue something to automate all of this. maybe use cron if if you are working on a nix ssystem. there are more sophisticated schedulers out there bit it's overkill imo. the process should run daily/weekly etc. think about how to version data. how to deal with data changing, performance etc. this bleeds into data engineering a bit but this type of project will get you to appreciate the entire analytical data workflow. production systems will be more complex and have dedicated teams for each part. but working through this will give you a good understanding and make you a more valuable analyst as you'll be working with dbas , data engineers etc. 
SQL and Tableau is definitely enough. Especially for Data Analyst positions. Anybody who has "Data" in their job title knows SQL. It is the one essential. Learn Python too though. They complement each other well. SQL for the heavy lifting data manipulation and transformation, and Python as a go between for just about anything else. Think of them together as your "hammer" and "Swiss Army knife". Having Python and SQL on your resume will be able to land you a ton of jobs. Python especially is quickly becoming the language to learn to be in demand across a wide range of technical positions even outside of Data practitioner roles. It gives you a ton options and a lot of potential for career growth.
Definitely! Any free books for it?
Why is that?
Notepad++ has a size limit and I think 7gb file falls outside of that range. 
I’ve just read about it. RDBMS are rarely ANSI SQL reliant. But I do get your point... learning the base of the language is a great foundation to build and work on. The other sub sets of SQL is just there to fill the need if ever... using the right sub sets to get the job done as effectively and efficiently as possible. I hope this is actually what you meant. 
without the syntax errors we'll never know however, no, `mysqldump` is extremely reliable
No worries. I almost forgot about that site. It’s a very informative site nonetheless. I got what ANSI SQL is now from reading and the comments I got from you and other responders. Very informative I may say. I think I’ll play around MySQL first using MySQL Workbench and read more on just SQL. I think there might be an option in MySQL Workbench where I can only strictly ANSI SQL. Not sure if these all make sense. 
Great insights! I’m just picking the ones that’s a bit more user friendly and accessible in terms of learning it... learning environments, books, etc. My company uses MySQL. Not too long ago, a coworker recommended XAMPP back when MySQL was still part of it. I guess it was now replaced by MariaDB. I also looked at some stuff online and almost everybody is going something on the terminal. I have “0” knowledge of it. So this is definitely in the pipeline even before I go back to Python. 
SOL = Shit Out of Luck
I don't know of any good free ones. I learned everything from on the job and this book I thought was incredible: T-SQL Fundamentals by Itzik Ben-Gan It's not free but you will definitely know the foundations of T-SQL if you get through it. 
Lol. 
Gotcha! Thanks. 
Hey good on you for posting your own answer. Some poor soul will find this in two years after googling for hours and you'll make their day.
Oracle joins is probably one of the core areas for testing someone in SQL interviews. Here is a great article on preparing for SQL joins: [https://support.dbagenesis.com/knowledge-base/oracle-joins/](https://support.dbagenesis.com/knowledge-base/oracle-joins/)
I recently came across [this page showing survey results about which databases are popular among developers](https://www.eversql.com/most-popular-databases-in-2018-according-to-stackoverflow-survey/). 
Again it matters what you are doing. An analyst will rarely use T and PL at all. If you were designing a transactional system, then that's a different story. When I design loads into our Oracle DW, PL is just some wrapper code around ANSI inserts. It's the ANSI I have to tune. Point is this, I wouldn't try to learn all of T or PL. I would get really good at ANSI and then find a platform to problem solve in. Then develop your T or PL knowledge around specific use cases.
You should type something like "my_database.db" when it asks for a filename to save under. The process of importing is turning the `.sql` file into a `.db` file that you can browse, search, and "view as a table". The `.sql` file is just instructions for how to (re)create the database. The `.db` file is the usable one you're really after. As others have said there are slightly different flavours of SQL so when importing you might run into an error or two. Hopefully you'll be able to fix it work around any errors. It'll help if you can read the first 10 lines or so of the SQL file you have in a text editor and tell us if there are any lines that look like comments with a database system and version. 
Transaction logs contain all the changes to enable the database to be rolled forwards from a point (ie the last backup) to a point in time. Temp space is used for things which are transitory in nature - temp tables, space for joins/sorts which won’t fit in memory etc. 
Um I would imagine it being more along the lines of. Search_cursor(varchar2 IN input_string) IS Select customer_name from customer Where lower(customer_name) LIKE lower('%input_string%'); That would be your cursor... You then would create a loop, looping through the returned names, printing them to the screen. And then setting the selected name into a variable @pcustomer.and then I'd open th cursor again, inputting the variable u set into the cursor. And do some further error checking to ensure it's the right pin pointed record. Then print to screen. Hope that helps.
Thank you for clarify :) .
You're basically asking if you could get one of, if not the highest risk and highest responsibility database jobs with (just) a cert you managed to cram into your brain and pass after just a few months, to which the short answer is "no." As others have mentioned, the DBA role is far too important to leave (read: trust) to the new and/or inexperienced. To be blunt, apart from your apparent honesty, you don't have any of the traits or skills seen or expected from a typical DBA. You've made a series of bad decisions in the last 4 years, you're unemployed, barely employable, in debt, have no transferable or relevant skills, and you seem to be only interested in becoming a DBA because it's a high demand and high paying position. I get it; everyone makes mistakes, but if any of the aforementioned are evident on a resume it's pretty much an immediate no-go. If you do somehow manage to get an interview, at some point you're going to meet with someone (senior DBA, manager, director, etc.) who will ask probing questions to learn who you really are and what you're all about and that's where your past sins and current lack of qualifications will disqualify you. DBA is a great role, but it requires years of hard work and dedication (likely as a developer IMO) before anyone in their right mind would let you within arm's reach of a dev server, let alone production. This is precisely why the job is in high demand and pays (very) well. And just FYI, you're gonna have to eat shit for a while as the new(est) DBA. Now that I've taken a steaming dump on your dreams of landing a job as a DBA right off the bat, I would still wholly recommend it as a career. IMO your best bet to break into the field is to sign up for a free account on AWS or Azure and start with the developer certs 70-761 and 70-762. Show a lot of proficiency and potential in the interview and you might be able to land yourself a gig as a junior developer with a decent salary. Work your ass off for a few years, learn from your mistakes (and fuck me will there be a lot of them at the beginning, speaking from experience), cut your teeth, show me you've taken the initiative to learn the internals, and then.... maybe.... we can talk about giving you the keys to the kingdom. Good luck!
 As u/Cal1gula said I am not sure what are you talking about, From what I could gather I would do something like this. CREATE PROCEDURE MYSPNAME @Customer VARCHAR(50) AS SELECT * FROM customer c WHERE ((@Customer IS NULL) OR (c.Customer LIKE '%' + @Customer+ '%'))
Thanks!
I had this exact same issue a long time ago and I don't remember how I solved it, other than to say I Googled it and after reading some documentation learned that what you think the ACE.OLDDB driver is, is not really it, and I had to download it from some other source and install it as a work around, which then fixed everything.
Is it 64bit/32bit issue? 
I was going to ask the same thing.
I would run the MySQLDump to a local directory and then copy the files down through the SFTP. How does the SFTP (You are using SFTP and not FTP right?) work? Is it automatically grabbing the files you post to it or is it a manual process? I've seen automation where the SFTP tries to write an underlying copy of the file and then writes the full file afterwards as an integrity check, but the automated piece starts to grab the underlying copy before it's completed. 
I would recommend Udemy. I'm learning SQL on there and they give a lot of examples. They also give advice on best coding practices to (e.g. how to structure your code so it's more readable).
It's trying to use the 64 bit version which isn't installed and you are seeing the 32 bit installer installed. I bet if you open SSMS and use the data import/export wizard this is where you will probably see this error as well. https://www.microsoft.com/en-us/download/details.aspx?id=54920 Download the 64 bit version and run the install.
SELECT 'FAILED/STOPPED JOB' AS ISSUE, j.name as job_name, jst.step_name, max(jh.[message]) as err_message, s.name as schedule_name, js.next_run_date, js.next_run_time, jst.last_run_date, jst.last_run_time, jst.last_run_outcome FROM msdb.dbo.sysjobs j join msdb.dbo.sysjobschedules js on j.job_id = js.job_id join msdb.dbo.sysschedules s on js.schedule_id = s.schedule_id join msdb.dbo.sysjobsteps jst on j.job_id = jst.job_id LEFT JOIN msdb.dbo.sysjobhistory jh on j.job_id = jh.job_id and jst.step_id = jh.step_id WHERE last_run_outcome IN(0,3) and last_run_date = cast(year(getdate()) as varchar) + cast(right('0' + cast(month(getdate()) as varchar),2) as varchar) + right('0' + cast(day(getdate()) as varchar),2) AND j.name not like 'collection%' and j.name not like 'sys%' and j.name not like '%admin%' group by j.name, jst.step_name, s.name, js.next_run_date, js.next_run_time, jst.last_run_date, jst.last_run_time, jst.last_run_outcome 
I suggest not installing Office on the machines where you need to use the ACE OLEDB 12.0 driver. 
What I do is setup these types of jobs to retry upon failure after X minutes, Y number of times... and email / alert on all failed jobs. The retry takes precedence over the next job run, so if it fails 3x in a row it'll report a failure. Not sure if this fits your needs, but it works for us with jobs that might get killed once or twice in a row due to a deadlock.
No performance differences, just readability and preference. I think most people use a combination of UPPER CASE, lower case, and Mixed Case when writing SQL. I tend to always make "functions" upper case, such as SELECT, FROM, LEFT JOIN, WHERE, IN, GROUP BY, etc. I also tend to use upper case aliases so A., B., C., etc. For field names I used mixed case and will take whatever is on the table. If the column name is DateRange then it will look like this: `A.DateRange`, but if its all lower case, or all upper case, then I'll use that. For abbreviations and such I tend to use lower case, e.g. `DATEPART(wk, FieldName)`. That is my standard style guide for *finished* queries, however I tend to write exclusively in lower case when I'm working on the fly, or developing something new. Once I'm happy with it I'll go back and reformat it. This is very useful because often I don't have time to really validate something 100%. Someone needs a quick fix made and I'll pop into some stored procedure or view and add it on the fly. Then a few weeks later we have a problem with something and I crack it open again. I've learned to "trust myself" when the code is formatted to my style guide, and when there is a problem with formatted code then I tend to suspect the problem is on the ETL side, or that something changed in the data. On the other hand if I crack open a sproc and see lots of lower case unformatted code then I doubt myself and assume the error is something I didn't anticipate, like a case statement where all the cases aren't represented, etc, or performing a ROW_NUMBER() on a varchar field that contains only numbers and 11 becomes 2.
In my org we are all lazy and write everything in lower case except string matches. Of course we could do something like where lower(something) = 'yourstringhere' but that would be going above and beyond. For us, it's more about breaks on key words like select, where, from, order by, etc, and indenting where appropriate for readability and nested statements. Sometimes if companies are large and have to collaborate often, they will have a "house style" that all writers of SQL must adhere to. 
You're my sql formatting twin. 
You're my SQL formatting twin.
i prefer lowercase. but a cowoker that just retired -- wrote everything in uppercase. it annoys me more than anything lol. i like the way lowercase looks. 
Are you me?
No performance increase, in fact I found that there was a performance decrease with uppercase. I couldn't type so fast while holding down the shift key.
I do everything you do aside from abbreviations. I still uppercase those. So DATEPART(YYYY,DateField)
&gt; Are there any performance increase if I write SQL queries in upper case? In ASCII, uppercase characters are associated with lower numerical values for decimal/octal/hex, so when the compiler is parsing the statement, it doesn't have to scan as deep into the ASCII index to find each value. This equates to faster queries than with lowercase characters.
imagine the filter that you have for columns in excel, if you type partially the name of the customer it returns each customer that contain that letters in the order typed in the customer field. EX: i type "olv" i get volvo , rolv, olv. If i type "vo" i get vogo, vovo, vostra
 imagine the filter that you have for columns in excel, if you type partially the name of the customer it returns each customer that contain that letters in the order typed in the customer field. EX: i type "olv" i get volvo , rolv, olv. If i type "vo" i get vogo, vovo, vostra
Ideally, you aren't re-parsing the same query enough times for this to be a significant improvement.
When `A` is 65 and `a` is 97, that's almost a 50% reduction in the amount of data which needs to be scanned per letter. That's a tremendous optimization.
I understand the concept of using what you need to get the job done. My issue is finding the next step after SQL Fundamentals. Judging from all the response I got... it seems best to focus more on the SQL fundamentals, then get familiar with terminal. Eventually go back to Python. 
Looks like I made the right choice sticking to MySQL. 
dude you need a comma in front of CASE 
Why are you re-parsing the same queries over and over in the first place? Any decent RDBMS should be caching the compiled plan and using that.
It really depends on the SQL flavour thiugh isnt it?
Our shop adheres to the devops methodology, so we have our cache TTL set very low in order to iterate more quickly rather than staying with the same stale, cached data for longer durations.
SQL dialects are pretty consistent on having commas between elements in lists. What case do you have in mind?
*Please* tell me that you've forgotten a `;-)` here.
Yepp , i didnt see he missed the come before the case... My bad
This is better be irony. I'm saving my upvote until this is confirmed.
[It's possible](https://i.imgur.com/ZfQcr1T.jpg)
You'll never know.
then it just has to become our egg moment. Have an upvote.
I wouldn't call them "functions"; more of keywords reserved in SQL.
I like to write my code in all variations so it makes it hard to understand when I go back in later and try to see what the hell I was doing 
If you happen to work in SSMS... Ctrl+A, Ctrl+Shift+l Its not my preference, but that will make everything lowercase
This is pretty easy to workaround, though. Just do: SELECT * FROM SYSTABLES WHERE NAME LIKE 'Z%'; Z is near the end of the alphabet, and will warm the processor's cache with the later characters. Of course, if you're running AMD servers, you'll need to do this: SELECT * FROM SYSTABLES WHERE NAME LIKE 'Q%'; SELECT * FROM SYSTABLES WHERE NAME LIKE 'a%'; because the processors have shorter cache lines and therefore a shorter prefetch policy.
I prefer to capitalize random letters so it looks like my sequel was written by a psychopath
That's what capslock is for. 
The face-lift for the SSRS Web Portal came about with SQL Server 2016 and SSRS 2016. So this is the earliest version you'd need to upgrade to. SQL 2019/SSRS 2019 is still in Pre-Release, so if you're needing to upgrade there's no reason to wait for the latest and greatest. If you're running SSRS on-prem, then you won't need a Power BI account. That is only optional - if you want SSRS to be able to pull and display Power BI Dashboards and Paginated Reports. It does look like you'd need a Premium Power BI account to do this however. Regarding the licensing though, you'd probably be better off talking to your licensing rep or a Microsoft licensing support person. Based on the few MSDN articles I've poured through, I can't find an explicit answer on this. 
Similar here, although alias's will match the case of whichever DBMS I am working in (ex, oracle is uppercase, greenplum/postgres lowercase, mssql upper or mixed case depending on table, etc). Also, I will crucify anyone who uses single characters as table alias's. I get it if you are just quickly running something that you will not keep, but if its code to be reused... for the love of pete make it readable by using helpful aliases. My general rule is that anything that is base keywords like you mentioned get uppercase treatment, functions and aggregates that operate on column values are lowercase, built-in functions (like ROW_NUMBER()) are uppercase... etc. Commas leading while in development, moved to commas trailing after finished for readability for anyone supporting later. Join criteria are always encapsulated in parenthetical to assure that one can track which items should be logically part of the clause (we have complex joins often in our environment). ON clauses are indented twice past the joining table, and the joining table is indented once past the previous table like so FROM table INNER JOIN tableb ON (tablea.col1 = tableb.col1) INNER JOIN tablec ON (tablec.col1 = tableb.col1)
I just assumed this was the default everywhere.
Thanks for the feedback!
What RDBMS are you using that allows A) you to change that and B) caches so poorly that this is advantageous?
\#agile
And they say DBAs have no sense of humour.
I honestly have no idea whether you're serious and I love it.
My SQL is artisinal so I only use pure 100% hand-cased characters.
I use single characters to alias, and only single characters. A-B-C, X-Y-Z, X1-Y2-Y3, etc.
And this is why people should format their commas on a new line :) SELECT SalesOrderDetailID ,SpecialOfferID ,OfferDescribed ,CASE
[MAKING SQL QUERIES IN CAPITAL LETTERS MAKES DATABASE TO SENSE URGENCY AND RUN FASTER](https://twitter.com/shipilev/status/703176579191410689)
Either you only deal in simple SQL or you are a massochist. Using single character abbreviations in a SQL script that has 20 table references is enough to make anyone insane.
I am me
I use single character aliases all the time unless I'm dealing with 2 tables that similar names. For example SELECT top 10 * FROM accounts a JOIN budgets b on a.id= b.accountid JOIN chart c on c.accountid 
I deal in very complex SQL and that is why I only use single letter abbreviations. &gt;Using single character abbreviations in a SQL script that has 20 table references is enough to make anyone insane. First thing I do with something like this is break it down into a series of chained sprocs that all build together, which tends to improve its efficiency. Working with single characters make it easier for me to break down and flowchart to look at join, join orders, etc. But I don't tend to work on stuff like that *unless* I'm breaking them down to improve them. The rest of what I do is more statistical programming and writing algorithms, and then using them makes more sense to me, X, Y, Z, etc before sequential, and X1, Y1, Z1 are functions of them, etc. Makes it a lot easier when I have a list of final columns such as: A. B. C. D3. E4. F2. G6. ... Z2. That's an extreme example, but you can see what I'm saying. If you write something that is thousands of lines of code... I need to know where Z2 is, not where "orders" are.
You definitely aren't working with complex SQL if you don't need to know which table Z3 even is.
Z3 isn't a table, it is a set of tables, or a calculation of tables.
Is there an easy way to do this, like the way IDE's do this automatically ? Or do you really need to manually hit the shift key?
And what is it calculating? And how would anyone besides yourself know what it's calculating and be able to remember it as they try and decipher the entire query? 
Just an update on this, I finally found something on licensing. [It looks like you will need a Power BI Premium account](https://msdnshared.blob.core.windows.net/media/2017/05/image327.png) or SQL Server Enterprise with Software Assurance. [Full Blog Article](https://blogs.msdn.microsoft.com/sqlrsteamblog/2017/05/17/a-closer-look-at-power-bi-report-server/)
Anyone with familiar with statistics would know. A simple example might be an average, or some other metric that is going into a larger predictive model. A more realistic example would be something like this: `B.Revenue * F3.Average / H2.NumYears` with accompanying comments on what the equation is doing at that step in the process. Most of what I write in SQL you can write as an equation on a white board.
The problem with using mixed case is that you need to use double quotes
What double quotes? That might be DB specific. I use MS and don't need double quotes with MixedCase, only for dynamic SQL.
If you can write it as an equation on a white board, that's not complex SQL on a complex database. Also, it seems pretty clear you probably don't work in a shared responsibility environment. It doesn't matter how easy to understand the logic might be - it's still a waste of time to have to read and understand every single equation in a script just to know that "Z3" is "subtotals for widgets". The sane person who cares about others reading their code would just use an alias such as "WIDGET_SUBTOTALS". Using single letter alias' would be like naming API endpoints with single letters. Oh, right, go read the entire source code if you want to know what enpoint Z3 is, rather than just looking at it's name and selected elements. It's only logical if the code is only for you. 
Your formatting sure is something...
Postgresql
No offense, but based on what you said in your first comment I can tell you've never written a predictive model, etc. &gt;Also, it seems pretty clear you probably don't work in a shared responsibility environment. No, one of our *requirements* for existing is that we get an environment of our own choosing. We do not use robust servers in the same way that you do, we use them to run calculations that can take over 12 hours. We aren't assembling anything, we aren't importing anything. That's 12 hrs for a model score. One time I wrote a calculation that came up with a hypothetical set of data that was about 15 trillion rows starting with an initial set that was no greater than 500k. 
I did in fact receive the error initially doing an import (screenshot is taken from the import exercise). Downloaded and a company admin installed the 64 bit driver, still got the error (came to reddit after trying the 64 bit driver). Can't use the 64 bit importer because excel isn't listed on available files. Can't use the 32 bit driver because MS Office is 64 bit. Interestingly, I was able to import using the 32 bit importer after converting the file to .xls
Downloaded and a company admin installed the 64 bit driver, still got the error (came to reddit after trying the 64 bit driver). Can't use the 64 bit importer because excel isn't listed on available file types. Can't use the 32 bit driver because MS Office (and pc) is 64 bit. Interestingly, I was able to import using the 32 bit importer after converting the file to .xls
So I had a user whose process was to download an xlsx file and save it as xls file and I remembered fixing it and removing that step. I thought it was an ACE OLEDB issue. That was like 4 years ago. Is it throwing an ACE OLEDB error specifically? I wonder if this is a missing JET driver. I thought the wizard used ACE. I remember Google fixed it fairly fast.
Yes. and the line WHEN SpecialOfferId = 3 THEN "Big Discount" should be WHEN SpecialOfferID = 3 THEN "Big Discount" &amp;#x200B;
You can also buy a very expensive tool that will capitalize built-in keywords as you type them automagically.
"Who wore this shit?! Oh wait, that was me."
I don't need to imagine anything... I linked you the exact thing you've described twice now.
Performance is inversely related to font size. Bigger the font, faster the query. We all know this to be true because the bigger fonts make the query more important. 
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/datascience] [\[MySQL\] How to get the first item of each "clustered" group?](https://www.reddit.com/r/datascience/comments/agquhn/mysql_how_to_get_the_first_item_of_each_clustered/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
The internet is serious business.
It sounds like your kind of work would be suited with a more data science based approach, like R or Python modules. 
I've used capitalization to identify keywords, as well as formatting. In general, I find that formatting a query improves readability significantly more than using capitalization. That said, I do love me some Pascal case in table, variable and column names.
Yeah, I'm sorry. I couldn't figure out the Reddit formatting... 
Yes, most of my counterparts work heavily in those environments and I use a bit of both, but have focused more primarily on Python for web scraing and ETL. Why? Because most companies don't have R servers, they have SQL databases. Really there is nothing you can do in R that I can't do in SQL, just that R is different and "better" in a lot of ways, for a lot of things. Still with databases, data privacy, etc., there is a lot of work in SQL in my field, and not a lot of people who are any good at SQL (not that I am, either, but I can get the job done.) I fight all the time with DBA's though who really have a poor grasp of what I do, and why I need certain things that they don't need, to do their job, which they obviously feel is superior in terms of technical ability. Can be frustrating but we almost always win out, generally because we aren't in IT.
Yep that worked. I also had to use parentheses around the CASE statement and I had to change the double quotes to single quotes. (When I had it as double quotes, SSMS seemed to think I was talking about a column) So I did the comma recommendation by you, open parentheses, case statement, then END, close parentheses, then used "AS XXXX" so I could call it a name. That fixed it. It was a seemingly simple issue, but still learning all of this. Thank you for your help!
I got so sick of all the style guides out there advocating uppercase that I published a style guide of my own for use lower case advocates. https://gist.github.com/mattmc3/38a85e6a4ca1093816c08d4815fbebfb
Here's how I would do it in SQL Server.. I'm not sure how (or if) it would translate to MySQL: &amp;#x200B; WITH Temps AS ( &amp;#x200B; SELECT id, temp, \[timestamp\], ROW\_NUMBER() OVER (ORDER BY \[timestamp\] ASC) AS RowNum FROM Test &amp;#x200B; ) &amp;#x200B; SELECT T.id, T.temp, T.\[timestamp\] FROM Temps AS T INNER JOIN Temps AS PreviousT ON T.RowNum = PreviousT.RowNum+1 WHERE 0 BETWEEN T.temp AND PreviousT.temp OR 0 BETWEEN PreviousT.temp AND T.temp
The hostname will be the name of the machine you install it on (or a named instance on that machine, if you pick that option when you install). The database name is whatever database name you use when you create a database to host your data. If you need to load millions of rows quickly, have a look at SQL servers bulk insert, BCP or the import/export wizard.
I haven't used this before, but from what I can tell in [this article](https://docs.microsoft.com/en-us/sql/t-sql/functions/last-value-transact-sql?view=sql-server-2017), it looks like the ORDER BY portion of the OVER clause contributes to the unique side of the data set. For example, the article has three different Lastvalue values for the department of 'Document Control', even though the PARTITION BY is only for Department. This appears to be because the Rate column is in the ORDER BY and there are three different Rate values for that department. If you change/remove your ORDER BY, does it change the result?
Could you mock up an example of what you want the output to look like? Just glancing at it, I see your confusion, just not sure what direction to point you in. It may or may not have something to do with the order by. 
at least a,b,c makes sense here. Imagine always having first table as a, second table as b, etc....something like this: SELECT top 10 * FROM chart a JOIN accounts b on b.id= a.accountid JOIN budget c on c.accountid = b.id I see stuff like this all the time and want to hurt people
Did you refer to this: &gt; FROM sharedprocesses AS sp As a nonsensical abbreviation? That couldn't be more sensical and will have much better readability in SELECT and WHERE statements when tables have similar columns. If I have to constantly look back to the FROM statement to keep track of which table stuff comes from, it's poorly written. 
[S]hared[P]rocesses = sp, I work with a lot of people who will abbreviate stuff like that. &gt;That couldn't be more sensical and will have much better readability in SELECT and WHERE statements when tables have similar columns. If I have to constantly look back to the FROM statement to keep track of which table stuff comes from, it's poorly written. My from's and joins are typically derivatives of themselves. 
I have, and I do. I am a full stack software developer who works in big data in the health insurance industry. I do everything from application and API development to ETL and reporting, and even predictive data modeling, which is a huge thing in the health insurance cream industry. And also tell you writing models in SQL is an excercise in futility, and most data scientists would use something like R instead. And what they wouldn't do is name their variables A1, Z3, etc. SQL is a very robust and purposeful procedural language. It lends itself very well to the intended workloads, and can do some very interesting stuff when stretched beyond it's original intent. That doesn't mean it is good practice to write shitty code, even if it's just for you - it's a bad habit in any form of programming, but especially in a procedural language like SQL. I'm sure you enjoy working how you do, but I would hate being the person picking up your pieces when you ultimately move on to another company. 
You fight with DBA's because they know what you are doing is categorically poor practice and bad for their environment.
I think you're just an oddball case for what most people are referring to here after reading through your other comments. Most people are talking about large-scale relational database queries and that's just not what you are doing. So whatever works for you. My example from above is the stuff everyone hates with single character aliases SELECT {columns list} FROM chart a JOIN accounts b on b.id= a.accountid JOIN budget c on c.accountid = b.id You'd have to look at the WHERE clause every single time you looked at a column in the query because there's no way anyone would keep that straight.
I use notepad++ and it has auto complete. 
&gt;, and even predictive data modeling, This is where I'm doubting you. You threw this in at the end. We aren't doing "reporting," we're doing serious number crunching and spending millions of company dollars to run advanced tests that directly contribute to the bottom line of the company. Essentially, we are literally responsible for making the company money. We work with some of the top tools for the industry, including SAS. We do nothing but work on these things, and only occasionally dabble in things like ETL for some boutique reasons. &gt; And also tell you writing models in SQL is an excercise in futility This again tells me you don't know what you're talking about. We work with the largest vendor in our space in the country, and they are using SQL databases for a variety of things, just like we are. We also work with another leading vendor in our space who recently wrote about environment as being "best in class" for the industry. What you are saying is not congruent with reality. &gt;SQL is a very robust and purposeful procedural language. It lends itself very well to the intended workloads, and can do some very interesting stuff when stretched beyond it's original intent. Including being the backbone of an analytics shop. &gt;That doesn't mean it is good practice to write shitty code This style was taught to me by the modelers who trained me, it was not something I invented. You're talking about people who make hundreds of thousands of dollars a year and have worked for some of the largest companies in the country, and who have advanced degrees from top schools in the country. No offense... but you sound like you don't know what you're talking about. &gt;I'm sure you enjoy working how you do, but I would hate being the person picking up your pieces when you ultimately move on to another company. Based on what you've said so far I think my management would find your skill set lacking. You may make more money than me, but we aren't interested in hiring full stack developers. We might be interested in hiring someone who has experience working with them, but not in the capacity of asking for environment recommendations. Just being real with you, not trying to be offensive. 
Your response has me heavily doubting your own claims. You seem super intent on tooting your own horn. You are free to continue to be a pretentious cock, and I'm free to stop responding. 
I'm totally fine with being the oddball, but as I've said... this style was taught to me, I did not invent it. I regularly will have sessions with colleagues where we diagram our equations in the context of x = (), y = (), if we take x / y = z, then z*x^2, etc. We then take pictures of those diagrams and write code which we can share back and forth. I don't understand how that is difficult to understand, or difficult to see how it works. You might not like it, and that's fine, but I don't see how this is anything other than preference... and my own *preference* is to write my aliases with single characters that show a progression of like things, and then structuring my SELECT accordingly. I find it makes it a lot easier to edit/add fields, break queries down into chains of sprocs to improve execution time, etc. &gt;You'd have to look at the FROM clause every single time you looked at a column in the query because there's no way anyone would keep that straight. But we really aren't looking at things like that. This is irrelevant to our worldview. I know the column names by heart and the tables themselves have been designed by me to serve a specific purpose.
&gt; But we really aren't looking at things like that. This is irrelevant to our worldview. I know the column names by heart and the tables themselves have been designed by me to serve a specific purpose. That's why I said what people are complaining about doesn't apply to you. You doing something very different than what everyone else is talking about in this thread.
&gt; I also had to use parentheses around the CASE actually, no, those parens are superfluous
You could tell me whatever you want but I'm not going to believe you. Why? Because you're a full stack developer. I spent over 10 years working my career in that direction before switching to the path I'm on. My mentor is the head of analytics for a multi-billion dollar company. My former boss is now a director at another billion dollar company. I have become a senior manager at a billion dollar company. Unless you are specifically working in /r/analytics, then I'm not interested in your opinion when you make such stupid remarks such as, "I doubt you've written anything complex" --&gt; when you know I'm talking about writing a model. Ever written a full predictive model in SQL? Aaaand we're done here. That answer is no. You already said SQL wasn't a good tool for it. By the way, head over to that sub and see how many people can even write a basic SELECT. Then you'll understand why our teams focus on SQL and heavily use it to prepare data before being consumed by a tool like SAS, or SPSS. Let me know how far you get without using SQL. By the way, I just turned down a role at one of the largest healthcare companies in the country to be their lead data scientist. I also founded and ran an online healthcare company for two years. What you're saying makes no sense to me.
WOw sO cONvinCIng! I aM ExpOSeD aS a FraUd!!! YOu gOT Me nOW!
It really fucking frustrates me, I'm sorry, but I deal with this all day long when I'm talking to DBA's. Not dumb shit like an alias, but when I need something and all I get are ignorant opinions. I use the word ignorant purposefully because it isn't that they're stupid people, it's that their condescending and don't know what the fuck they are actually talking about. So what do I have to do? I have to go over their head. Like I always do. Then I get what I want after fighting for days or weeks. It literally takes me 10-100x more effort just to do my job than it needs to because of this shit. Yes, I'll grant you 99% of the SQL user base doesn't use SQL this way. Yes, I'll grant you that it might be the designed intent of using SQL. But that doesn't change the way a lot of companies are set up, and it doesn't change the fact that I need to get shit done. Nor does it change the fact that there are jobs doing this. Sorry /vent
I think your own words make their own case.
You also have to look at from their perspective. The DBAs are ultimately responsible for the health and maintenance of the system. Of course they are going to be uneasy about people doing things that are probably better done in other tools (in this case R/Python/etc) or that they don't understand because they get the 3am call when something is broken. Would you want to wade through thousands of lines of other people's code that only makes sense to them to figure out why the database locked up? I'm just like you in that I'd much rather be allowed to do what needs to be done to get the job done right now, but in a lot of case there's resistance for good reason even if we don't understand what it is or in some cases even care because it's not our problem. I wait for weeks for things that feel like they should take 40 seconds when it comes to database thing I don't have the rights to do myself, but it is what it is. They need to be comfortable with what's going on and be able to debug problems later
Hacker Rank is very useful, you can test skills ranging from basic to advanced. [https://www.hackerrank.com/domains/sql](https://www.hackerrank.com/domains/sql)
&gt;My company has suggested to stand up a server and have the application(s) running so that if users need to access the data in the future, they can do so in their "regular" way. How often does this requirement *really* come up? And do they *really* have to have the full application for this? What happens if they accidentally change data in this "archived" database? I'm going to admit that I haven't read through all your pros &amp; cons in detail; this is just stream of consciousness here. The downside of archiving the database and shoving it off to cold storage is that if you need to resurrect it *with* the application, you have to rebuild the whole stack *at the application version where the database was archived*. If you're running on VMs, that means taking an image of the VM and stashing it away alongside the database at that point in time. But what happens in 6 years if you don't have a virtualization platform that can read those images? Let's say you get a new CIO and they're all hot and bothered about The Cloud, and push *everything* up to EC2, eliminating all on-premises physical machines (including VM hosts). 4 years down the line, you have to bring up those VMs - but where and how can you do that? If these are physical servers, will they wear out? Boxes running Windows 2003 are probably already pushing 10 years old, can they last another seven? The more stuff you keep around "just in case", the more security exposure you have, the more storage you have to pay for, the more electricity you have to pay for, and the more things you have that can break. All for a "just in case" scenario? Keeping the databases live on an instance of SQL Server means all the same things, plus the overhead of backups running against them, integrity checks, etc. - even though they're not changing. Lay out the costs, in dollars and cents, of keeping the full stack available or in "freeze-dried" form, for 7 years. The space. The backup requirements. Maintenance. Security management/exposure. Do they really want to spend 5 figures a year _just in case_ someone needs 5 records off a personnel table 6 years from now that could just as easily be retrieved by restoring a database and running a query? My personal feeling is that for data that's been deemed "archived", you don't need to have the ability to retrieve immediately, and you shouldn't be expected to produce more than the raw data. Reports can be reconstructed, business rules re-coded if they're documented properly. It'll be read-only anyway. As [The Architect](https://en.wikipedia.org/wiki/Architect_\(The_Matrix\)#The_Matrix_Reloaded) said, "there are levels of survival we are prepared to accept." I'm actually dealing with something similar, but smaller in scope, right now. I've got a few hundred DBs that aren't needed anymore, but we're required to keep them for a year per customer agreements. My plan is to take a final archival backup of each database, encrypted, and stash them in Amazon Glacier. Dirt cheap long-term storage. If it's a little slow, who cares? We probably won't even have to pull the backups anyway. Then keep the certificates, passwords, keys, etc. required to restore the databases in an enterprise password vault managed by the infrastructure team. With the length of your timeline, you will want to keep tabs on any breaking changes in SQL Server with regard to restoring backups from older versions to newer versions. Right now, you can restore a 2008R2 backup onto 2016 (and I believe 2017 and 2019, but I haven't tested). But what if SQL Server 2023 *doesn't* support restoring the backups that you have archived? You'll need to keep an older instance kicking around. Or, anytime you upgrade SQL Server, you could restore your archives to the new instance, then repeat the backup/encrypt/cold storage cycle to keep them current.
Thanks for your answer. I'll look into it.
Self Join
I try my hardest to look at it from their perspective, but we are responsible for the company making financial goals, making money, and growing for the future. Simply put: There is no company without us. Our input in terms of generating revenue, forecasting revenue, and creating budgets impact all global operations. This might sound like I'm taking it out of proportion but I'm not, and companies have become addicted to analytics and using them to help grow revenue. This is a critical role in many companies today, and many of these companies work with antiquated systems. It isn't good enough to say you can't do something in SQL, or that SQL isn't an appropriate tool if the total cost is 0$ because you already own the license. &gt;Would you want to wade through thousands of lines of other people's code that only makes sense to them to figure out why the database locked up? If my database locks up it should not concern the DBA, and it is my responsibility to make sure it doesn't lock. Just turn it off and turn it on. The only thing I ask is that no one else has the ability to lock it up like that except for me, so keep shared services out of your mouth. I don't have time for amateur nonsense. I document my code and make it accessible for someone *like me* to inherit, not someone else... not commenting on you specifically as an individual, but what the fuck do I care if a DBA can read or understand my code? A DBA is never going to inherit or take my job. It's a totally separate skill set. If you're worried about having to support code like that, then why would you put me in an environment where I could take other people down, and not just let me do my own thing and be responsible for myself? You wouldn't... and now my proposal from 6 months ago asking for an independent environment suddenly makes sense. You know one time I was asking for something like this and our DBA's/IT were being very difficult until we got into a meeting with the CTO (their boss) and in front of them all I asked how much extra money it would cost the company if they just did exactly what I said. I think the number was like $2000. In front of the CTO, the CFO, the COO, the CMO (my boss), and the CEO I started to do the math of how much money was being lost in the meeting, and then just bluntly asked them if they had a solution that met all of my needs which was different than the requirements that I had originally outlined. On a scale of 1-10 how quickly do you think I got what I wanted? I can tell you how quickly because I wasn't even able to finish estimating how much money was spent arguing with me before they gave me what I asked for. &gt;I'm just like you in that I'd much rather be allowed to do what needs to be done to get the job done right now, but in a lot of cases there's resistance for good reason even if we don't understand what it is or in some cases even care because it's not our problem. In my experience there are not good reasons, there are people who lack insight into what our job is, and why we need the things we need. I have tried explaining myself to them over and over, and in the end I just get insulted. They don't even know that they're saying something insulting to me when they say something like, "I used to be a database developer, and I didn't need that." Cool story. You never used to do what I did. The DBA's that I love working with get it, and don't argue. They try to guide us and give us options, but when we explain that we need something different than that they don't make us go too far into the weeds as to why. They understand that 99% of the time they can solve our problems with a 2 minute conversation, and that it's better to work with us than against us. I really miss our last DBA, he was a really old man but both of us were certified COBOL and RPG programmers from back in the day so we'd trade Y2K war stories. He loved our group, which in my experience was really rare, but we did 99% of his job for him. When audits came around for stuff like GDPR he knew he could make a 5 minute phone call to us and we'd tell him everything he needed to know about our server &amp; databases. We were probably his only clients that he never had to do anything for other than simple routine things. &gt;I wait for weeks for things that feel like they should take 40 seconds I'm involving C-level people if you make me wait more than 30 days for something. If I need something done on the database and you don't respond to one of my instant messages within 30 minutes of it being sent, and I can see that you're online and not in a meeting? I'm putting an emergency change request in asking for something to be done and then I'll let you track me down after your boss's boss gets an email. Last week we had a sproc run early so the final dataset wasn't correct and it needed to be reran. I messaged our DBA's about it and after 35 minutes that's exactly what I did. It was shocking how quickly they got back to me. The way I look at DBA's is like this: You know more than I do, but my backup plan for retirement is to become a DBA if I fail at rising to the top of my field in analytics. Ya'll just normalize data, make shit simple, and use complex aliases so you know what table something comes from. Take backsups. Work with permissions. Pretty easy way to make 100k if this analytics game doesn't work out.
There’s no difference from a technical point of view I personally just like it because it makes it clear what’s a KEYWORD and what isn’t.
Whenever I utter the words “which cross eyed, inbred fucking moron wrote this abomination?” I already know the answer...
I really appreciate your insight. My thoughts are pretty aligned with yours. To answer a few of your questions: &gt; How often does this requirement really come up? And do they really have to have the full application for this? At this point, the department asking the questions is convinced they will need full access to the application at a moment's notice. I'm fairly new to the organization, so a win for me would just be getting the application out of the picture to begin with. A backup-only solution--which I agree is the best--may not even be considered. &gt; But what happens in 6 years if you don't have a virtualization platform that can read those images? Let's say you get a new CIO and they're all hot and bothered about The Cloud, and push everything up to EC2, eliminating all on-premises physical machines (including VM hosts). 4 years down the line, you have to bring up those VMs - but where and how can you do that? Yes! This is the stuff I haven't really considered. I haven't been 100% focusing on the downsides of the Sys Admin stuff. To be honest, I don't have a lot of experience on that side. I'll definitely keep this in mind. &gt; Lay out the costs, in dollars and cents, of keeping the full stack available or in "freeze-dried" form, for 7 years. The space. The backup requirements. Maintenance. Security management/exposure. Do they really want to spend 5 figures a year just in case someone needs 5 records off a personnel table 6 years from now that could just as easily be retrieved by restoring a database and running a query? This was basically going to be my approach from the start. It will be easy from the money side, even giving the option of a live SQL server vs. application servers AND SQL instances behind them. The security aspect too. I'll also look into Amazon Glacier, we're looking into cheap storage for backups anyway so that might be a good approach. Thanks again for your post. 
I use an AutoHotKey script passed down to me by a former coworker. Works very well. Here's the code: https://pastebin.com/2CGECZfX
Have you tried just NVL()'ing the records? Set them to something that will never exist in the tables?
If you have to constantly go over the heads of experts in their fields to people who are not as knowledgeable (managers of people, not of tech) then perhaps it is you who is the problem. There's always the odd BOFH type of dba, but if every single one shoots you down, it's because they know something you don't. 
"there is no company without us". Ahhh, the grandiose arrogance. There is no company without them, either.
Inherited knowledge is often incorrect. Example: see racism.
200 bucks ain't bad, especially with a better snippet manager. Jeez.. the hours I have saved.. probably should start saying the months I have saved.
No, really, our company would fold without us. We control the spend and are responsible for the responses. You do realize what direct response is, and how many millions of dollars are spent on the strength of a model, yes? Great. I can tell you exactly what our costs are, what our profits are, and how we (our group) are directly responsible for those profits. That literally is our job. That literally is why we report to the senior management that we report to, and literally why we get what we want. That is precisely how management views our jobs. Whether or not they get their bonus or the year is a result of what we do, and what we do is what goes to the board of directors. You would have no idea how to spend your money without us, or how to make the most off it. You'd be lost. The company would get destroyed by competition.
I'm an expert in my field, just not in SQL. &gt;There's always the odd BOFH type of dba, but if every single one shoots you down, it's because they know something you don't. No, I've worked with great DBA's in the past, it's just more a general rule of thumb. Probably about 80/20. 
But there's no you without them. Take away the dbas and you have no data to do your analysis and no place to run your sql. You'd be manually transcribing everything into excel or using pencil and paper. The competition would destroy you.
How to use an alias is not a valid example here. We don't really work with "tables" per se, we work with calculations. X isn't a table, it is a calculation across tables that represents something. It joins to Y because Y is the letter in the alphabet, and then they join to Z. If I have 4 tables I start with W. In my select everything that is getting pulled is sequential, so all the X's, all the Y's, and all the Z's. XYZ then becomes something... maybe it comes A, and then B is another thing that involves an X1, a Y1, and a Z1. X and X1 will look the same, but have a slight variation, maybe a different time frame, or some other difference. X and X1 are going to be compared... there might be an X8.... X is the important thing to keep track of because X represents a real thing from a set of tables. 
Arrogance always leads to ones downfall.
I'm sorry, but I'll just be real with you: I'm a DBA's customer. If you don't keep me happy, we'll find anther provider. My patience for any other type of relationship is zero. This isn't because I'm an asshole, but because of how I've been treated by the profession over the course of several years, and through finding that it is the best way to get results. &gt;You'd be manually transcribing everything into excel or using pencil and paper. The competition would destroy you. Who do you think is easier to outsource to India? A DBA, or your statistical modeler?
No, under valuing yourself does.
I'm sure your outsourced to India dba will be super responsive to your needs. Just accept they are valuable professionals the same way you are, and perhaps the way some of them treat you could be a reflection on you.
You mean as responsive to my needs as you (collectively) in this thread have been? Gee... I'll think about it... are you guys saying you want to get off your high fucking horse? Because the nice thing about Indian resources is that they don't argue. 
You have needs from this thread? Ego building? Misplacing hostility? Not sure what else I see. You dropped a three paragraph profanity-filled rant on my comment saying I understand why you want to alias your things your way. From that point forward I've just wanted you to see your coworkers as professionals 
No, this thread started with me commenting on a practice I was taught, and then me having to explain why I find it convenient. This was met with a lot of comments about how I must not do anything complex, how it was shitty code, etc. My patience for that shit is zero. I'd rather work with someone in India.
Then it must have been the quotations.
Not that I know of. I try to keep everything in camel case, so everything is usually lower case unless I hit tab to autocomplete or I’m making a function/procedure. 
You are missing the frame specification of the window. You have it partitioned correctly and sorted correctly, but now you need to define which frame of those records are in play. With your first query (FIRST\_VALUE), the default frame is from the beginning of the partition through the current row. Thus, by not specifying the frame you still get the desired result. With LAST\_VALUE, however, you need to specify you want to look at the entire partition using ROWS BETWEEN \[Start\] AND \[End\]. In this case, start is UNBOUNDED PRECEDING and end is UNBOUNDED FOLLOWING. Proper code is below. SELECT LAST_VALUE(PatientName) OVER (PARTITION BY PatientID ORDER BY UpdateDate Asc ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS WindowTest , * FROM Presentation.Patient &amp;#x200B;
Lot of people use uppercase. Unfortunately, it is not ergonomic. Readibility of code is better when you do everything lowercase.
I tend to uppercase functions and mix cased on everything else. &amp;#x200B; That is, until my d\*\*k Lead Engineer sees it and makes me go back and mix case everything. I dread those weekly micro-managing meetings.
Yo me It seems like you need another table which stores the primary key. The ID in this table can be a FK relation to the PK column in the other table. Ex: TAble1 has Id - int as PK Any and all fields that are 1 to 1 relationship. Table 2 has Id - int as FK referencing TAble1 id column This is a table which holds a Many to Many relationship with any additional meta data Let me know if this is what you are trying to figure out? 
Maybe what you want to do is something like this: Users (Id PK, First Name, LastName, Email, other 1 to 1 data fields) UserEmploymentStatus(Id FK referencing Users(ID), Start Date, EndDate, Manger ID, etc. ) 
Aasci characters of uppercase supercede lower case characters and thus are factually less memory intensive. With that said, in modernity the lift is virtually no existent, in the eighties and nineties however it could matter significantly
If you have software assurance with enterprise you will get the power bi reporting server. Remember this is not real time automatically updated reports. This is on demand reports. So you will not get live updates. You will get live updates if you get the power bi service which I think is a seperate monthly cost. Not included with enterprise licensing. 
Where lower takes more memory than where upper.. aasci uppers supercede lowers and take less memory. Doesn't matter really if you're working with a modern computer but it did indeed once matter
Lol. Got into an argument with somebody today that the query they showed me was mine on the same auspices
And aasci dictates it to be true
Just follow normalization rules
In my original line of thinking, I was curious if it is valid or acceptable practice to modify my data within my database in order to normalize it? And is this more of a business systems question than a normalization question? If I were to use your suggestion, would the process be something like this and set id as my primary key? `CREATE TABLE distinct_id_table AS (SELECT DISTINCT id FROM original_table)` `ALTER TABLE distinct_id_table ADD PRIMARY KEY (id)` This might be a dumb question... but in general what does it mean if your intended primary key has a duplicate value? My hesitation in using this above method is once I JOIN with other tables, I have to specify the same types of WHERE conditions to get the bolded values for 001 every single time. |id|name|start\_date| |:-|:-|:-| |001|Barry Allen -Temp-|**2018-01-01**| |**001**|**Barry Allen**|2019-01-01| Would the optimal solution be to use your suggestion, have this original table and a distinct\_id\_table, and create a view to get the parameters I need? 
Glue his CAPSLOCK key down.
Nice one. 
That's not standard SQL
If your task is to normalize the source data then you will have to design and develop transformations. Yes, you will change source date to weed out rubbish - in accordance with agreed upon rules to determine correct values. Can you define rules to determine the correct name? And start date? Don't allow faulty data to pass if you have ways to eliminate it. 
I don't know if this works in oracle, but (if I understand you correctly), I'd do this in SQL server: On exists ( select a.f0,a.f1,a.f2 intersect select b.f0,b.f1,b.f2 ) 
That is pretty much standard SQL except for the format of the interval constant. The following would be pure standard SQL: where column1 &gt; column2 - interval '30' minute ^((btw: it's Postgres, not Postgres))
Yup. This is us.
Organic, free range SQL. I like it.
SELF JOIN I think you mean
 SELECT EXTRACT(MONTH FROM theday) AS themonth , AVG(clients) AS avgclients FROM ( SELECT DATE(start_date) AS theday , COUNT(DISTINCT id) AS clients FROM thetable GROUP BY DATE(start_date) ) AS data GROUP BY EXTRACT(MONTH FROM theday) 
I'm receiving the error using the import wizard - which is why I don't understand the error if the engine is clearly installed. Already tried the suggestions from Google...but...my PC...the googles...they do nothing...
I actually would like to see that presentation seeing as how you have no idea what I do everyday. Just make sure you name the file only one letter, otherwise I'll rewrite the whole deck because that means you are clearly an amateur who doesn't know what you are doing.
Seems like the question is: what would cost more? 1. Maintaining these legacy applications and keeping them running on VMs. This includes the cost to pay whoever maintains it - keeping in mind things like you said where the dev leaves and you have to pay somebody an arm and a leg to do the job. 2. Paying however many people are required to maintain the databases, continuously update them to whatever SQL version the company is running, and run all of the queries for users that want to access the data (at least the users that don't have access and/or don't know SQL) I'm inclined to believe #2 is would be cheaper due to potential labor costs. Hell, sometimes re-writing the application just to access the data is cheaper. &amp;#x200B; If this were me, no question I would go with option #2. I would gather information about what legacy data people need to access and how (what search parameters). Then I'd use SQL reporting services to write reports based on that and give people access to run those reports using the SSRS web portal. As time goes on, you'll likely get to a point where all of the legacy data that people need are accessible via reports. If they really aren't lying about needing to access this data frequently, a lot of your DB team's time is going to be spent on running queries for people until this is done. If you can eliminate that, its a no-brainer that this is the cheapest route.
I don't live in burgerland. I don't even live in the first world. Two of my friends just got data science positions with no prior work experience in IT at all. There are hundreds of entry level SQL (yes, literally listed as "Entry Level") jobs advertised online in my city. 4 paragraphs and you didn't even manage to answer the question on the title of the OP.
This is typically done using unique constraints (or indexes). But it's hard to tell without seeing the table definitions
In Postgres you probably would want to use SERIALIZABLE as the isolation level. 
/u/uvray explained it very well, but if you need a further explanation check out Doug's talk here: [https://www.brentozar.com/training/t-sql-level/6-windowing-functions-design-21m/](https://www.brentozar.com/training/t-sql-level/6-windowing-functions-design-21m/) The solution is around 17:10, but the whole video is good.
Yeah coming from SQL Server and PascalCase column names that I love, it irritates me so much in pgsql that you have to quote and snake_case looks terrible to me.
 select distinct JOB_ID, Status, message from job_definition_exec where status not in ('0','-1403') and END_DTE &gt; sysdate -1; Assuming 0 and -1403 indicate success? Is JOB_ID unique to each execution, or is it the same value for the same job?.... ie: there will be multiple records in the table with the same JOB_ID, one for each time the job runs (~every 5 mins)?
What you do doesn't matter every day. No one person's job matters when you talk those kind of dollars relative to cheap licenses. That's the entire point of the presentation: This is a total waste of everyone's time, why are we here --&gt; What is your counter proposal if any?
I would buy what you're saying.
Teradata SQL assistant is a free tool and can automatically convert lowercase to upper for reserved words. This setting has to be enabled from option. 
Replace the statistical modeler with AI
By function I think you mean reserved words.
Yes, that is the grand plan one day after all the doctors have been replaced, and just about everyone else. It will be one of the last jobs that is automated, and with machine learning that is one of the primary things we aim to do (automate jobs out of existence). Fun time.
&gt; At this point, the department asking the questions is convinced they will need full access to the application at a moment's notice. I'm fairly new to the organization, so a win for me would just be getting the application out of the picture to begin with. This is where I'd bring out a variation on Brent Ozar's "ok, you say you can't ever lose data and can't ever go down? Well, here's what that'll cost" from his RTO/RPO worksheet. When they start seeing that it'll cost high 5/low 6 figures a year, they'll change their tune real quick. If you're new to the organization, maybe you have experience from past jobs you can draw from that will give more weight to your arguments? Basically a "yeah, we tried to do this at OldCompany and this is what it cost and how painful it was" sort of thing.
It's all just preference, either for the individual or the team. I've moved away from upper case and rely on syntax highlighting and formatting. The key is to choose a style and stick with it. 
Nice! 
Our host has 128gb of RAM. Wondering if we should budget for 256gb so we can use lower(). 
why not use triggers again?
Perfect, thank you very much!
I don't know oracle that well, but taking too long during a trigger in sql server may mean your code is cut short and not completed.
Use a trigger to create a record in a table that represents a queue of work for your python script. Run the python asynchronously against that queue. If triggers are not viable for a reason I don't understand, then just track the latest record ID your python program has processed and next processing will get everything after that, assuming a sequential ID. Otherwise you'll need to keep all the record IDs every time you process to get the ones you haven't processed next time.
Hey! I resemble that comment!
*A relevant comment in this thread was deleted. You can read it below.* ---- If you can write it as an equation on a white board, that's not complex SQL on a complex database. Also, it seems pretty clear you probably don't work in a shared responsibility environment. It doesn't matter how easy to understand the logic might be - it's still a waste of time to have to read and understand every single equation in a script just to know that "Z3" is "subtotals for widgets". [[Continued...]](https://www.resavr.com/comment/uppercase-preferred-writing-sql-11865950) ---- *^The ^username ^of ^the ^original ^author ^has ^been ^hidden ^for ^their ^own ^privacy. ^If ^you ^are ^the ^original ^author ^of ^this ^comment ^and ^want ^it ^removed, ^please [^[Send ^this ^PM]](http://np.reddit.com/message/compose?to=resavr_bot&amp;subject=remove&amp;message=11865950)*
Your primary key CANNOT have a duplicate value. That is why it is a primary key. You can just add a new field that is a "No Shit ID" field, and give a unique number to each line. Then, you can go back through and select on the table where "No Shit ID"s have entries with duplicate "id"s, and then cull as necessary. 
the above is perfect only if you consider '31' to be a valid result for a case when, for example, on Jan 1st you had 31 clients and no (zero) clients for every other day of January. Average per day should normally be calculated as &lt;total by period&gt;/&lt;number of days in the period&gt;
Despite the upvotes, the is wrong if a usual definition of average is used.
Can't you do the calculations in PL/SQL?
yep that's it, thanks so much! for some reason I thought unbounded preceding and following WAS the default - had no idea it was unbounded preceding and current row.