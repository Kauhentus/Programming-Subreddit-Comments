Thank you so much for that! I finished off my subselect query today for the names that I wanted to eventually be added with the comma's and and... but I can't figure out how to insert it into your query. Would I make it a string then replace your mystring etc stuff? Here's my subselect: select isnull(replace(_givenname + ' ' + middlename + ' ' + lastname, ' ', ' '), '') + isnull(companyname, '') as Name FROM table1 WITH (nolock) LEFT OUTER JOIN table2 ON t1key = t2key LEFT OUTER JOIN table3 ON t1key = t3key 
Autocorrect misspelled it?
That's great, just remove the first open bracket to the left of the ISNULL for middlename and lastname, as there's no matching closing bracket for them, then you're golden. Alright so that scenario is a bit more complicated as you have to use FOR XML PATH to concatenate rows. Even more so with concatenating multiple columns in the subquery... I'd probably want to use CTEs to simplify it but are you familiar with those? I would suggest doing some research into video tutorials on the above functionality first, STUFF with FOR XML PATH and also CTEs. Use the first CTE to create the name columns concatenated; then in the next CTE, select from the first CTE with your STUFF FOR XML PATH function; then in the final CTE, select from the second CTE and use the REVERSE(STUFF(REVERSE())) code to replace that last comma with an and. If anyone else can think of a simpler way to do this please weigh in, I'd like to know too! Unfortunately SQL isn't really geared to easily write this kind of stuff as it makes the data very hard to work with afterwards. The resulting data is only really useful in a report. On the plus side you will learn a lot writing it :-)
&gt;(select isnull(replace(ISNULL(givenname, '') + ' ' + ISNULL(middlename, '') + ' ' + ISNULL(lastname, ''), ' ', ' '), '') + isnull(companyname, '') as Name &gt;FROM table1 WITH (nolock) LEFT OUTER JOIN &gt;table2 ON t1key = t2key LEFT OUTER JOIN &gt;table3 ON t1key = t3key &gt;FOR xml PATH ('')) So this concatenated all the names into one column for me, that was nice :D I'll take a looksee at the CTE stuff you suggested, do you have any suggestions for resources? 
Awesome :-) I personally like Wiseowl tutorials, they have a website and YouTube channel. Very clear and well laid-out guides.
EOMONTH is 2012 so if your stuck in the past like me your solution is the simplest approach.
The default terminal on the Mac is *Terminal.app*. Open the terminal and type mysql, it should say something like ERROR 1045 (28000): Access denied for user 'USER'@'localhost' (using password: NO) If you are not seeing that then it's either not installed or not running. If it's not running it will complain about being unable to connect: ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/tmp/mysql.sock' (2) Try running *mysql.server start* I'm pretty sure most versions of MacOS ship with a version of mysql. If that's not the case then you will probably want to install Homebrew as your package manager and install mysql that way. So once you know the mysql client is installed, you can install the database. Make sure you're in the unzipped file directory: mysql -u root -p &lt; sakila-schema.sql mysql -u root -p &lt; sakila-data.sql Then log in to mysql with *mysql -u root -p* and your database should be there. To select the database, just do: show databases use sakila show tables 
&gt;created An index on t.created would certainly help, as right now you are looking at ALL tickets in the table and there are lots
Yes it can't be Unique if you can create multiple rows with the same time. The order won't matter for filtering, only if you order the query by that field which you aren't doing here. This index should help a fair amount as you will have many less tickets to look through
Awesome, I'll try this out now - thanks.
Using an inner join also speeds it up 50%. Inner join should be fine as I don't need to retain the secondary tables.
I tried doing all those things but it says "command not found" How would I go about fixing this? When I open the sakila.mwb file, it opens in MySQL Workbench with the tables and everything.
 I would look at having another table than has an event\_id (if you have this) and a row for each individual tag for that event\_id. Something like: |EVENT\_TAG\_ID (PK for new table)|EVENT\_ID (FK to events table)|TAG| |:-|:-|:-| |1|567| agent| |2|567|cs:open| |3|567|ct:question| |4|567|nosql\_db| Then as long as TAG and EVENT\_ID are indexed, you should find a query like the following does an index seek instead of a full table scan. AND t.tag = 'DT_GA'
Interesting - thanks for that. I've got something similar in other places so it should work.
`CASE` does the same thing as `DECODE`. I'm not sure what else I can tell you.
No problem. I’m talking from Microsoft sql server experience, so I hope I haven’t put you on a wrong turn. 
Nah, it's essentially a lookup table. Can't see why it wouldn't work / speed things up.
If you're filtering on fields from Events, that implies that every Ticket will have corresponding Audit and Event records. So there is no reason why you should be using outer joins. Change both joins to inner joins.
Yeah, I noted that in an earlier comment - thanks
Change to inner joins and create indexes on tickets.created, tickets.ticket_id, audits.ticket_id, audits.audit_id, events.audit_id. Depending on how many unique values there are for field_name and type on Events, you might want to create indexes on them also. CREATE INDEX tickets_created_idx ON tickets(created); CREATE INDEX tickets_ticket_id_idx ON tickets(ticket_id); --&lt;&lt;Already exists if it's a PK CREATE INDEX audits_ticket_id_idx ON audits(ticket_id); CREATE INDEX audits_audit_id_idx ON audits(audit_id); --&lt;&lt;Already exists if it's a PK CREATE INDEX events_audit_id_idx ON events(audit_id); 
Option one, much more logical and easier to maintain.
 Questions|Answers :--|:-- What is your favorite color?|Orange. Cats or dogs?|Foxes. 
I see, that's an expense I can spare. Thanks :)
I see that you got what you were looking for, and I'm glad. I just wanna make a small comment that it's generally against best practices to try to format strings inside SQL. Sometimes it may be necessary - but it's "not the right tool for the job" and is best to leave it formatting to whatever tool you're passing the data to. Not trying to be a pain, just hoping that in the future to save you some headache - because SQL isn't great at it and generally makes it much harder than formatting the data on the other side. 
[SQL Indexing and Tuning](http://use-the-index-luke.com/) is another must read to learn SQL (Performance). Another one is [Modern SQL](http://modern-sql.com/) to understand latest SQL standard which will help you to write simpler SQLs.
thank you!
This is a snippet from my resume. If SQL isn't your primary skill set but you want to list it, I'd add it similar to the technology added on mine under "Server Technology". It's just software I've used in the past I want employers to know that I've had experience with it even though it's not my primary job function. **Software Skills, Education, and Social Media** **SQL Technology** • 7 years of: Microsoft SQL 2000 – 2016, 3 years of: Oracle 10i - 12.1, 2 years of: MySQL 5.6 - 5.7, Sybase 15.5 – 16, DB2 9.5 - 10.5, Amazon RDS hosted SQL Server and Oracle. • SSIS, SSRS, SSAS, DTS, and Visual Studio. • Database Performance Analyzer, Redgate Toolbelt, SentryOne, ApexSQL, MySQL Enterprise Monitor, Toad Data modeler, and DBA Tools Powershell library. **Server Technology** • Microsoft Server 2000 - 2012 R2, Ubuntu, and CentOS. • VMWare, VSphere, and Citrix. • Git, Team Foundation, Solarwinds Network Performance Monitor, Solarwinds Server and Application Monitor, WS\_FTP, Filezilla, WinSCP, Putty, Powershell, and Symantec Backup. **Education** • MCSA – Developing Databases with SQL Server 2016 o MS SQL 70-762 Developing Databases with T-SQL (July 2018) o MS SQL 70-761 Querying Data with T-SQL (December 2017) • ONLC – Oracle Database 12c: SQL Fundamentals (August 2014) • Advanced Excel classes hosted at FRCC. (June 2009)
Did you have any projects as part of the Udemy course?
thank you so much!
yup building a web app
You can add the project to your resume and include the amount of data, any functions and complexity you used as bullets. Query optimization, performance throttling, etc.
I feel you on this one. I added 'MS SQL Server 2005/2008R2' on my resume after completing a web based application with MS SQL back end. What makes this difficult to add or prove in a resume is the experience. I can't speak for everyone else but for my situation, I knew ZERO about MS SQL before that project so adding just MS SQL on the resume immediately assumes you know all about it such as administration, IDE/Management Studio, Stored Procedures, Functions, views, etc. The general rule I have to adding ANYTHING to a resume, if you are questioned about it, just have the confidence to back it up at any given time. Example: 'Experience in SQL #' Then have a projects section where you describe how you used SQL in a specific project.
thanks!
I think this is boiling down to file server permissions. I'm stumped, because I've begun running at myself (domain admin, should have full access) and getting nowhere. I created a second job, and a parallel set of folders, and it runs fine there, so I think I just need to dig through the permissions and get things arranged there. know of any good tool that watch process between servers and can pin point where the "deny" happens?
*beep beep* Hi, I'm JobsHelperBot, your friendly neighborhood jobs helper bot! My job in life is to help you with your job search but I'm just 325.7 days old and I'm still learning, so please tell me if I screw up. *boop* It looks like you're asking about resume advice. But, I'm only ~24% sure of this. Let me know if I'm wrong! Have you checked out TalentWorks, /r/resumes, TIME? They've got some great resources: * https://talent.works/blog/2018/01/08/the-science-of-the-job-search-part-i-13-data-backed-ways-to-win/ * https://www.reddit.com/r/resumes/ * http://time.com/money/4621066/free-resume-word-template-2017/
Depending on the what the OS is of the system hosting the file share, you may have some options, Win 2008+ has Audit Event 5145 which will give you who tried to access a file and more, just need to enable on the hosting system for troubleshooting.
Check the following web site [http://www.studybyyourself.com/seminar/sql/course/?lang=eng](http://www.studybyyourself.com/seminar/sql/course/?lang=eng). The course, along with examples, is quite easy to follow. You can submit exercises as well and some of them are quite difficult. Everything for free. 
Thanks bruh
Go for a practice oriented tutorial. The following resource, for free, may help you [http://www.studybyyourself.com/seminar/sql/course/?lang=eng](http://www.studybyyourself.com/seminar/sql/course/?lang=eng). 
Go for a practice oriented tutorial. The following resource, for free, may help you [http://www.studybyyourself.com/seminar/sql/course/?lang=eng](http://www.studybyyourself.com/seminar/sql/course/?lang=eng). 
good bot
good bot
Starting from column that has most distinct values (has higher cardinality) is important, but more important is to consider if you will use the index to search for some subsets of index fields. So imagine that you have index consisting of ('surname', 'profession') - this index would also allow you to efficiently search by 'surname' and by ('surname', 'profession'), but not by 'profession' only. So if you are planning to make a lot of 'profession' -only queries, it would be a good idea to change the order of fields in the index to ('profession', 'surname'). 
Use GitHub and post queries you have created along with explanations of what the query does. Then put a link to your GitHub repository on your resume.
Generally it depends on how you are going to use the index. Let's take this list: \- 1,Z \- 2,Y \- 3,W \- 4,X If we ordered it by the first column, it would appear exactly as it is displayed. What if we ordered by the 2nd column then the first? \- W,3 \- X,4 \- Y,2 \- Z,1 Now imagine if instead of letters and numbers, it was dates and names. How are you querying the table? Are you primarily searching by peoples names or are you searching by their dates? The clustered index is how the table is physically ordered and you are configuring that setup based on the first piece of the index, then you would organize it from there. So perhaps your clustered index is date created and then person id. The index itself is unique because of the person id, but if you are querying frequently based on the person id it is going to have a hard time finding those because it's ordered by Date then ID. If we reversed it, it would be hard to do a scan of the date range because it's first ordered by person id. [A clustered index follows the rule of four:](https://www.red-gate.com/simple-talk/sql/learn-sql-server/effective-clustered-indexes/) 1. Unique. 2. Short. 3. Increasing. 4. Static. [So typically the clustered index is one column ](https://stackoverflow.com/a/389545/5149122)in the table with the rest of the columns being included in the pages and the column indexed follows the four rules above. It doesn't have to follow those rules, but you typically want a really good reason to avoid the norm. Likewise, it can be advisable to use more than one column in a clustered index. So the reason you see such ambiguity is because the answer isn't concrete. There is no yes this is perfect or no this is the worst. Each solution has pros and cons and it is up to you and your business to decide which of those possibilities best fits the organizations goals. This is going to come from how the table will be used and because of that, it's hard for us to give you a solid answer. What we can do is give you guidelines and information to help you make an educated decision on the route to take. I would HIGHLY recommend to [watch this video,](https://www.youtube.com/watch?v=MpAKdy54Eqg) it is AMAZING and very helpful. I have watched it no less than five times because I felt I got something out of it each time. (Across the span of a year.) Another great resource to learn more on Indexes is [https://use-the-index-luke.com/](https://use-the-index-luke.com/) and [T-SQL Querying by Itzik](https://www.amazon.com/T-SQL-Querying-Developer-Reference-Ben-Gan/dp/0735685045). Please let me know if you have any questions or if I stated anything that did not make sense. 
Thank you very much. I will review these videos and the whole answer when I have more time to spare!
Oh I didn't end up finishing it. The last query I pasted was as far as I got... The strings were too difficult to work around so I've kind of hit a stump 😅
I think it could be beneficial, and in the worst case you can just delete that index and create a new one - SSMS allows you to do this.
&gt; are the results from that scan stored in TempDB as well? You're likely looking at this the wrong way. The engine knows what it needs to do before it's actually executed; this is why, outside a few informational items, estimated and actual execution plans will always be the same given the same input. The work that needs to be done with the temp table would be optimized around the operations it needs to do (hence why the article shows that it uses the same one). The standard question for query plan presentation is: Do you read them left to right, or right to left? The correct answer is both. AKA the scan will likely filter/calculate/populate the tempDB with only the information it needs to proceed. &gt; Halloween Problem This is basically a non-issue unless your running a heap or an extremely poorly optimized high-load system, or extremely poor queries/client code. As a side note: I've seen this occur precisely once, where the above mentioned weren't the cause, in an appropriately spec'd production environment. It was a bug related to SQL Servers handling of recursive multi-threaded plans on large data-sets. &gt; If the results of the scan are not stored in TempDB then theoretically each row would need to be pulled via a brand new look up on the table. That definitely will not be happening, this would decimate performance.
Also watch tutorials about MS SQL Services (SSIS/SSRS/SSAS). You’ll learn about ETL with SSIS, web deployment/reporting apps with SSRS, and multidimensional databases/OLAP (online analytical processing). SSAS will also teach you about concepts like fact vs. dimension tables and star schema vs. snowflake schema data mart styles. I know it seems like an overkill, but the trick is to convince the hiring manager in an interview that you’re knowledgeable. These will really help you understand what data warehousing is capable of all under Microsoft, and combined with your T-SQL syntax knowledge you’ll become a subject expert and ace an interview.
Command not found for the first and not found for the second.
I actually went to file &gt; Open SQL Script and put the two .sql files into Workbench and it seems to work. I can execute queries and everything. Is there something else I should do or am I good?
Thanks, I've never seen that before, but I think I should be able to make it work with that. I'll play around with it and see what I can come up with.
Pretty much. Except an accdb file is an Access database, not a SQL server database file.
If workbench gets you what you need, just go with that. I recommend you install something like MAMP if it doesn't work out. Best of luck!
Like /u/mikeblas said, that's an MS Access file, not a SQL Server database. You can connect it to your SQL Server instance using a linked server. See [this Stack Overflow post](https://stackoverflow.com/q/28438859/1324345). You'll need to make sure you have the appropriate driver installed for it to work. But what are you going to do with this thing once it's linked? If you need to make changes to the Access database and send it back, then SQL Server isn't where you need to be - you need a version of MS Access that matches what your client is running.
What DBMS? If SQL Server, you’ll need to grant the calling user permission to execute the proc on the other Database. If Oracle, you’ll need to establish a DBLink from one database to the other and call via an anonymous block or stored procedure. If other, none of the above is useful. End If;
Alright. I gave it a little look and think I came up with your solution. **Assuming that you can get all the names in 1 comma separated list** (I can help with that if you need help). I created a temp table to test with. I think I got all the cases covered (1 name, 2 name, more than 2). I'll go ahead and dump everything here so that it makes more sense than cherry picking just the answer create table #tempString( Names nvarchar(255) ) insert into #tempString values ('Mark, Danny, Sam'), ('Steph, Vinny'), ('John'), ('Terry, Jerry, Lenny, Danny') select case tS.commaCount when 0 then Names else reverse(stuff(reverse(Names),lastComma, 1, '&amp; ')) end as Names from (select Names ,charindex(',', reverse(Names)) as lastComma ,len(Names) - len(replace(Names,',','')) as commaCount from #tempString ) as tS So what we do: * We get the full comma separated list, the char index of the last comma (from right to left), and the number of commas * If there's no commas, we leave it alone * Otherwise, we reverse the string, replace the last comma with your ampersand and a space (we need a leading space for the ampersand - but we're reversed still), then reverse it back. Let me know if you have any questions, or if you need help getting your comma separated list.
IIRC you can't actually call a procedure over a DB link - what you can do is insert into a polling table and when the scheduled job to poll the table finds a row, it can execute the procedure. 
I must be remembering it wrong, or mis-remembering as we call it
Don’t get me wrong. The way you’re saying is also a good approach since it’s modularized/compartmentalized. 
Listen buddy, I said I didn't think it worked for procs. I was wrong. Don't let me off the hook. Finish me. 😂
Well said. If OP decides to go the SQL Server route, I'm going to shill for Itzik Ben-gan as a resource. The guy is phenomenal and while the books he writes ain't exactly cheap ($40-50 usually), they're well worth it as training tools for certification exams and as quick reference guides for everything beyond.
Yep. This is one disclaimer that will bite hard in an interview. 
He's dead Jim.
OP could get the 70-461 for pretty cheap used and it has a lot of great material. I wouldn't say it is as structured or up to date as fundamentals / querying / 761, but it is cheap and chalked full of great information. His books are a little pricey, but if you use a CITI card you can use the rewind feature. I bought his books on Amazon and found places where it was cheap and submitted the price match. I believe I got Fundamentals for \~ $20 and Querying for \~$35 all said and done. I got the 70-76X series guides really cheap too, I got all of the SQL Server ones along with the e-books for around \~$120 on black friday last year. (Plus Brent Ozar's videos for $100.)
First, thank you for your response! Once it’s linked I’m trying to create a user interface for my clients database. But through that interface I also need to make sure my client f can add new datasets to the DB. Does that make sense? What should I be doing differently?
I would not recommend keeping a linked server to an Access database kicking around to be used as your standard mode of operation for the system (application -&gt; SQL Server -&gt; Linked Server to MS Access). Either go all-in with SQL Server (migrate the data out of Access and into SQL Server), or (ugh) keep that data separately in Access and have your application connect to SQL Server and Access independently.
generally speaking this is a bright red flag of extremely bad design
* Access as a front-end to Access - fine * Access as a front-end to SQL Server - fine * Access as a front-end to SQL Server pointing back to Access - nope right out of there I recommend *not* relying upon Access to store mission-critical, performance-critical, large-volume data in the first place. I also suggest separating your Access UI and your Access data into separate files (from the UI file, link tables from the data file) if you're going to keep your data in Access.
Lag works great, thanks for pointing it out to me. 
So the data is already in a SQL server. The client had someone working on an access file before they left and gave the project to me. I’m also working remotely so that complicates things for me a little bit. They gave me team viewer access to their SQL server. Through there I have to establish a connection to my local machine and work on an access front end. I’m so confused... I do appreciate the help 
So...what's in the Access file? Data, or code/UI?
bruh
Then why is SQL Server in the mix at this point?
Tough crowd 😅
Why do they want you guys certified? Is it to cooperate with partners more smoothly?
Cool. 
An MCSA is a very affordable method to gain experience for SQL Server; especially if you are a proficient self-learner. * The books are \~$50 * \--They come with sample exams that give you a better idea on the nature of the exam questions * Exams are \~$150 (you can often get additional discounts) * \--These exams are actually more difficult that most online training would lead you to believe * \--There are often questions where you have to thoroughly know your stuff to get above 95% * There are three levels: * \--70-461: Querying SQL Server * \--70-462: Administering SQL Server * \--70-463: Building a Data Warehouse in SQL Server All three exams plus textbooks would be less than $600 total; which is a bargain. 
Just keep in mind that passing the exams means that you've passed a somewhat arbitrary test - it doesn't mean you _really_ know how to do the job day-in/day-out. These exams often have a bias toward new features and features that people _don't_ use in the daily grind.
Thank you, yes I realize this. I do well with directed self-learning. Working towards a somewhat tangible goal helps me. The certificate itself is more for management. The learning is what is important to me. I guess how I should have better phrased this question is: What certification course will be best in helping me learn database administration. I can query the database just fine. I can modify and work with the data. I understand the layout of the database. But I need to know more about the admin side of things. Reading and REALLY understanding explain plans. Making sure our database is running optimally. Auditing the system. Monitoring the system. Knowing what needs to be done in a disaster recovery situation. Making sure backups are good. Plenty of other things I can't think of at the moment. As I said, with all these sorts of things, I'm flying by the seat of my pants. I only have time to take them on as they come, and learning on my own after work is just a no-go. This is an opportunity for me to have work time set aside to learn and improve myself. I hope that clears things up a bit.
I wanted to go ahead and help you get a comma separated list, too. I glanced at what you were building and it looked overly complicated. I want to insert another comment in here that it's best practice to always include the table you're using when you reference a column. I don't always do it on quick-and-dirty examples, but I'm trying to fix that bad habit myself (I had to edit my previous example!). I tried to rebuild your pasted sub-select with tables 1, 2 and 3 - I'm guessing on what table is which, but I hope this makes sense (I made a scenario where you're a consultant and you are tracking who worked on what jobs): Create table #Employees( EmployeeID int ,GivenName nvarchar(255) ,MiddleName nvarchar(255) ,LastName nvarchar(255) ) create table #Contracts( ContractID int ,CompanyName nvarchar(255) ) create table #EmployeeContracts( EmployeeContractID int ,ContractID int ,EmployeeID int ) insert into #Employees values (1, 'EmpA1' , 'EmpA2', 'EmpA3'), (2, 'EmpB1' , 'EmpB2', 'EmpB3'), (3, 'EmpC1' , 'EmpC2', 'EmpC3'), (4, 'EmpD1' , 'EmpD2', 'EmpD3'), (5, 'EmpE1' , 'EmpE2', 'EmpE3') insert into #Contracts values (1, 'CompA'), (2, 'CompB'), (3, 'Compc') insert into #EmployeeContracts (EmployeeContractID, ContractID, EmployeeID) values (1, 1, 1), (2, 2, 2), (3, 2, 3), (4, 3, 4), (5, 3, 5), (6, 3, 1) So you'll want a list with a ContractID then a comma separated list of the Employee's full name and the name of the company. I prefer using SUBSTRING over STUFF - it's a matter of personal preference (I probably learned it first, but it's easier to wrap my mind around what's happening). select distinct ec1.ContractID ,substring( (Select ', ' + ISNULL(ISNULL(e.GivenName,'') + ' ' + ISNULL(e.MiddleName,'') + ' ' + ISNULL(e.LastName,''), '') + ' ' + ISNULL(c.CompanyName,'') AS \[text()\] From #EmployeeContracts ec2 WITH (nolock) LEFT JOIN #Contracts c ON ec2.ContractID = c.ContractID LEFT JOIN #Employees e ON ec2.EmployeeID = e.EmployeeID where ec2.ContractID = ec1.ContractID ORDER BY e.EmployeeID For XML PATH ('') ), 2, 1000) FROM #EmployeeContracts ec1 WITH (nolock) 
The SQL 70-7XX(61/62/64/65/67/68) series is also good, although there is a noticeable lack of training resources when compared against the older series. The underlying knowledge requirements are very similar. It also has twice the number of required exams / books, doubling the cost of the previous one; if they're willing to cover that, have at it OP. Learning about temporal data / encryption / cloud hybrid solutions might not be needed at a smaller company, but definitely won't hurt to know. 
/u/Coldchaos gave you excellent information on the certs. Based on what you have shared, I personally think you going for the DBA cert from MS is a great way to go. Just make sure to use more than one resource when studying. [MSSQLTips ](https://www.mssqltips.com/sqlservertip/4696/exam-material-for-the-microsoft-70764-administering-a-sql-database-infrastructure/)has some resources you can use and honestly you could probably skip the 765 since that is based on Azure. Focusing on the material in the 764 and then going back and evaluating where you need practice and what technologies your company uses that weren't covered would be a good final step to cementing your education on administration for now. If your company has the extra money, I absolutely would recommend the Brent Ozar video training series. I think you could use that as a precursor then roll with the cert. 
I did 70-761 a couple of month ago, certainly was harder than I thought it would be, but I passed... Studying for 70-762 now, loads of stuff in there I doubt I'll need to use but it's given me awareness and if I ever came to use some of it I'll know what to Google. I've only bought the books and exams, 5+ years experience. If you have working knowledge of sql I'd practice the examples in there. Pass mark is 700/1000 I believe, no idea how the scoring is worked out
Holy crap thank you so much! I feel bad for saying that I figured out the comma seperated list already though... I'm out atm but I'm super excited to apply this to my dataset!
Is the slug column a foreign key of another table? (If "someSlug" exists in your tag table does it also exist as the primary key of another table?)
Hmm. This requires icky dynamic SQL but you can build a query string like: DELETE FROM tags WHERE name = ? AND tag NOT IN ('tag-1', 'tag-3') (I'd actually use placeholders for the tag names and bind them all to the prepared statement to prevent an injection attack vector but I'm on my phone and that's a lot more effort to type).
Thanks a lot. I can take care of making it a pdo statement to prevent injections and this bit sounds better than my drastic erase and insert approach. Thanks again
Yes. Manually concatenate the fields with Sql native formulae. Or do it in the presentation later. Scalar valued udfs are pretty bad for performance. 
INSERT INTO table-name (column-names) SELECT column-names FROM table-name WHERE LIKE '%\\\\%' 
You could also do a regexp to search, but that's not necessary here.
I work at a public tech company in the DC area and have a few data analyst positions on my team I'm trying to fill. Send me a PM with your resume or LinkedIn if you're interested.
I was using SQL and BOE/Crystal Reports intensely for work, then switched departments to a SAS shop. The only reason that department used SAS was momentum. They had like 20 years of datasets in SAS format, and many of the employees were lifers. Plus IT was dysfunctional and couldn't offer a better analytics platform internally. I hated SAS with a passion, but it had a macro language that was pretty useful for DIY ETL and managing lots of SQL queries and doing heavier analytics (p charts and all that stuff). Otherwise it was a complete basket case of cruft, weird alchemy and inconsistent syntax. Everything more than a simple query felt like I was duct-taping Frankenstein together. 
I think this would be what you are looking for: with xyz as ( select lag (State,1) over (order by HistoryID) as prev_status, status, lead (State,1) over (order by HistoryID) as next_status from your_table where your_condition='something' ) select * from xyz where status ='error' ; with XYZ will give you all records in DB, presenting next columns: previous state, current state, next state. Later you select from there all records that have errors. Could be that there are some errors in syntax, did this on the fly, but in general this is what you need if I understood you well. cheers
It's the OR part.
It's the OR condition at the end, which has equal precedence to the other conditions. Parentheses will fix this. AND ( events.value LIKE '%distributed tracing%' OR events.value LIKE '% DT %' ) 
Interesting! Never ran into this before, thanks :)
Wrap everything after the final AND in brackets and it'll work: SELECT audits.ticket_id FROM events INNER JOIN audits ON events.audit_id = audits.audit_id INNER JOIN tickets ON tickets.ticket_id = audits.ticket_id WHERE audits.created &gt; "2018-07-31 00:00:00" AND events.type = 'Comment' AND (events.value LIKE '%distributed tracing%' OR events.value LIKE '% DT %') 
Your `WHERE` clause is formatted incorrectly. Wrap your final two conditions (for `events.value`) in parens. Otherwise, you're getting *any* event `like % DT %`, ignoring the dates.
wheres your showplan?
Here is the correct answer `select Shift_date,Shift_type, count(*) as cw from (` `select Shift_date,Shift_type,Manager,Operator,Engineer1,Engineer2,staff_code from Shift inner join Staff on staff_code = Manager` `UNION` `select Shift_date,Shift_type,Manager,Operator,Engineer1,Engineer2,staff_code from Shift inner join Staff on staff_code = Operator` `UNION` `select Shift_date,Shift_type,Manager,Operator,Engineer1,Engineer2,staff_code from Shift inner join Staff on staff_code = Engineer1` `UNION` `select Shift_date,Shift_type,Manager,Operator,Engineer1,Engineer2,staff_code from Shift inner join Staff on staff_code = Engineer2) s` `group by Shift_date,Shift_type`
So if your DB supports windowed functions you might try something like this: UPDATE st SET NetRevToDate = n FROM ( SELECT NetRevToDate, SUM(ISNULL(NetRevenue,0)) OVER(PARTITION BY SourceID, TransactionID, LoadNumberID [etc...] ORDER BY DateId ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as n FROM StagingTable ) st This definitely works on recent MS SQL (2012+ I think?) It actually should perform reasonably well, since the variable you want to update isn't part of an index itself, so the system just has to do a massive sort then scan through the table once
 SELECT a.[ID], 100*(b.Value-a.Value)/a.Value AS PercentDiff INTO percentDiff FROM oldStats a INNER JOIN newStats b ON a.[ID] = b.[ID]
You need to do a distinct first, then the counter. like DISTINCT(x), count(*) .
You mean: Select Distinct Count(Color.Widget\_Color)? If so, I also tried that and didn't get the correct result. I've tried all kinds of joins an selections and nothing has worked.
On second thought, maybe the point of my ACTUAL query will help: I work for a company that has vendors that service specific zip codes. I'm trying to count how many vendors service a specific zip code. That said, if two vendors service five zip codes, but I only want to know about one of those zip codes, the result is "10," indicating that ten vendors service that zip code (instead of two). The zip/vendor info is in the subquery, the main query is trying count the vendor id#.
It's a little hard to answer this 100% without an example or two of what your data looks like, but I will answer it to my understanding. You would join the two tables, with the criteria that DateTime is between the outage start and stop time. And then use a case statement to determine whether to choose Value 1 or Value 2 based on the reason. This may generate duplicates, but like I said I don't know exactly what the data looks like, so this is just supposed to give you an idea. SELECT a.DateTime ,a.Value1 ,a.Value2 ,CASE WHEN b.ReasonID = 'blah blah' THEN a.Value1 WHEN b.ReasonID = 'yadda yadda' THEN a.Value2 END AS AdjValue FROM TimeData a LEFT JOIN OutageData b ON a.DateTime BETWEEN b.OutageStartTime AND b.OutageEndTime
OK, I think this will give me a good start. An example of the data can be provided, but I won't have access until the week starts. I'll try to provide a better explanation of the data and update my original post. Table 1 will have time series data, constantly logged into the database (the term in my industry is "Historian") it simply logs values from controllers in the field and timestamps them. Table 1 will have Value1 = "Predicted Efficiency" and Value2 = "Actual Efficiency" An "Outage" is when "Actual Efficiency" falls below 95%. When this is detected, I write an entry into a database that logs the "Start time" of the outage. When we reach over 95% efficiency I update the last record with an outage "StopTime." The contract states that &gt;95% efficiency must be maintained or funds can be withheld. However, if the outage is caused by the client (they shut off the equipment etc) we can substitute use "Predicted efficiency" during these times. This is why I have a "ReasonID" which primarily assigns blame so we can make this substitution. 
I guess my question would boil down to how the query optimizer decides to execute a query. From my other reply here, I'm doing some table EXCEPT statements for database normalization. Some of the tables we have are very large, (some have over 100 columns, but generally 20-30 columns). What we do is that we'll take the columns that are often the same value yesterday as they were today, and insert those into a table, which is given an identity column. So if you have: Jan 1 - John - Smith - Carpenter - 45 Jan 2 - John - Smith - Carpenter - 45 Jan 3 - John - Smith - Carpenter - 45 we can see that John - Smith - Carpenter - 45 is repeated very often, so we'll take those 4 columns and assign them a number (123 for example), so instead of storing those 4 columns, every day, we can store just Jan 1 - 123 Jan 2 - 123 Jan 3 - 123 Basic stuff. So each day when the table is normalized, we have to check if the combination John - Smith - Carpenter - 45 already exists, which means we must scour our dictionary for it. That means that we'd do SELECT FName, LName, Profession, Age FROM DailyTable EXCEPT SELECT FName, LName, Profession, Age FROM Dictionary That will give us only the new combinations of those 4 columns. It would seem like the simple case that when it runs into our friend John Smith, SQL's execution plan would be * Scan through dictionary for everyone named John * Scan through the resultant records for everyone with the last name Smith * Scan through the resultant records for everyone who is a carpenter * Scan through the resultant records for anyone who is 45 So you're essentially running the query SELECT FName, LName, Profession, Age FROM Dictionary WHERE FName = 'John' AND LName = 'Smith' AND Profession = 'Carpenter' AND Age = 45 It would seem that the order of the clustered index should match the order of the columns in the dictionary, but like I said, this boils down to how SQL decides to execute the query.
I bet you it's the join/subquery, not count distinct that is giving you unexpected results. From your example, it would be something like this: Select distinct Count( Vendor) as cnt, ZipCode From Vendor V Left Join ServiceArea S On V.ID = S.VendorID Group By ZipCode; I may be confusing what you are wanting. Also I'm on mobile so excuse any formatting errors.
Don't know which type of DB you use (MS, mySQL, Oracle...) but you could do it with "with as" function instead of subquery. with xyz as ( your subqery here) select distinct (your_col) from xyz;
&gt; It also has twice the number of required exams isn't it just 3 exams ? https://www.microsoft.com/en-us/learning/mcsa-sql2016-database-administration-certification.aspx STEP 2 - EXAMS Pass 2 required exams. Be sure to explore the exam prep resources. STEP 3 - CERTIFICATION Earn your MCSA: SQL Database Administration server certification. STEP 4 - CONTINUING EDUCATION Earn an MCSE by passing 1 related exam. 
I am working with sqlite as a database for data analysis, so i guess precomputation makes sense in this scenario! Thank you :)!
&gt; Adding the column ... forces you to nail down all the places the underlying data may be updated to ensure the code accounts for keeping the computation in sync. A trigger that automatically updates the column to the new value whenever a row is updated helps with this issue, btw.
All good. Exported the database from local server to AWS using pgadmin. 
Would this do it for all columns at once without naming them?
Yes
47
Question doesn't even make sense. Put more effort into it please 
Easiest way would be via a « before insert trigger » to validate the data prior to insertion and reject invalid data. This however is only recommended for OLTP workload as it’s much too heavy for bulk insertions.
In all seriousness all ya gotta do is write a query 
It's probably that subquery that's slowing things down. I'd try a more direct update with join right in the update statement. UPDATE M SET M.Age1 = M.Date - P1.Birthday, M.Age2 = M.date - P2.Birthday FROM Matches AS M INNER JOIN persons AS p1 ON p1.id = m.p1_id INNER JOIN persons AS p2 ON p2.id = m.p2_id WHERE m.m_id = matches.m_id
I'd even be ok with an effortless question as long as it made sense.
It's going to completely depend on the software and how it is storing those products. If it stores product information in a database, and if you can get access to the database, then you can query it to find the information.
I'm on the same boat as you, and based on the Table of Contents and the reputation of No Starch Press I bought Practical SQL by Anthony DeBarros. 
Can you clarify what you mean? Like each table has Value1, Value2, and Value3, you want % diff b/w a.Value1 and b.Value1, and then % diff b/w a.Value2 and b.Value2, etc?
Just copy and past it into notepad
You should be able to just look at things that are in the same week of the month, the same month of the year and the same year.
My problem is that I don't understand how to write the query so that I can construct the events that are repeating. For instance, today is Aug 12th. If I scheduled a weekly event starting today at 8:00am, the database only stores *todays* event, and then it stores the repeating pattern. So the event for next week doesn't exist in the database in `event_calendar`. So, when I go to view next week's events, how do I query the database to find events that are happening that week? What I have so far is to pick the "end" of the week, search for all events that have an end date before that date, and search for all events that have a start date after the start of the week that don't repeat. That will give me all of the static events from `event_calendar`, but it won't give me repeating events from previously.
https://use-the-index-luke.com/ is a good resource.
Yep thats it
I did not know about MySQL's invisible Columns. Is that a way to assist with incremental refactoring? So if code somehow breaks, the DBA can simply toggle visibility back on?
Invisible columns are available on MariaDB. These are columns which can be given an INVISIBLE attribute in a CREATE TABLE or ALTER TABLE statement. These columns will then not be listed in the results of a SELECT \* statement, nor do they need to be assigned a value in an INSERT statement, unless INSERT explicitly mentions them by name. It should help you control which columns are returned from SELECT\* and which must be entered in an implicit INSERT statements.
I see. Nobody in my shop uses splat column expansion anyhow. It's frowned open in my shop.
Take out the where clause. I copy and pasted it from yours but it's actually not needed. Aside from that it should work as long as SQLite uses basic join syntax. I know that statement will work on MS SQL but there's no TSQL referenced, so any instance that confirms to the basic SQL standard should interpret it properly.
&gt; I guess my question would boil down to how the query optimizer decides to execute a query. I'm going to tackle this in three pieces. 1. How does SQL Server execute a query? 2. How is optimization affected by the Clustered Index? 3. What should you do? The life of a query can be broken down as: (Resources for this bullet found at \[1\]) 1. Syntax / the query itself. T-SQL has been written, you hit F5. 2. SQL Parses the syntax and checks the query. This is a logical query process. 3. SQL binds the objects and loads metadata properties, this is the binding process. 4. SQL generates an execution plan, this is where the optimization happens. 5. The query executes. SQL is using information from the logical step and binding phase to help generate the metadata. The more complex the plan, the more difficult it is to optimize effectively. Sometimes aggregating your query into steps and using staging tables / temp tables in the process can assist with row estimation. Something important to note is that SQL is not going to always pick the best plan. It is deciding on the BEST plan for the given situation. It could spend a few minutes to pick the perfect plan, but if the only difference between plans is milliseconds, there is no gain or benefit from it. So it tries to meet in the middle and pick the best bang for your buck essentially. The optimization step becomes broken down as: (Resources for this bullet found at \[1\]) 1. Simpliciation stage - SQL removes contradictions, joins tables, and works with the data set based on statistics and cardinality data. 2. Trivial plan - It checks if there is an obvious plan to use, if not, it moves onward. 3. Statistics update. 4. Stage 0 optimization - This is considered our "transaction processing". If the query is simple and uses a minimum of three joins to access few rows, this is typically where the query gets optimized. You do not receive a large amount of optimization choices in this stage, it usually decides it would be trivial to spend additional time to optimize. 5. Stage 1 optimization - This is considered our "quick plan" piece. Most queries fall into this category where it weighs performance vs time to estimate. 6. Stage 2 optimization - This is considered our "full optimization" piece. If the query is complex enough, it may enter this phase where it will take more time to weigh the decisions of processing the query. Ok, so that kind of helps us understand what SQL is doing under the hood and how it is picking things. What about in the case of clustered indexes though? Those steps above are kind of high level and doesn't exactly give us a great pinpoint method to solving this. Well, in SQL Server, if you perform a clustered index seek, the plan becomes a "trivial plan" because there is one best method to execute the query. (With a clustered index seek.) - 93 You can actually enhance the speed of the above if you use a nonclustered index seek and a range scan because there will be more rows per leaf level page, which thusly creates less IO. Now, here's where the debate you heard comes into play. &gt; I read on Stack Overflow I think, someone said that the best way is to start with the column that has the most distinct values, and then decrease from there, but then a guy below him said he's wrong. I haven't been able to find a good source of into on this. You ready for the answer? It's going to be a shocker. "It depends". Here's what Itzik says about it: &gt;You will hear some recommendations that claim you will get better cardinality estimates if you place the column from the more selective predicate first in the key list because the optimizer creates a histogram on the first key column. This is not really true, because as I explained earlier, if SQL Server needs a histogram on a column that is not a leading key in the index, it creates one on the fly. What could be a factor in determining key order when you use only equality predicates is if one column is always specified in the query filter and another only sometimes. You want to make sure that the column that is always specified appears first in the index key list because to get an index seek, the query filter has to fer to the leading columns. (93) \[2\] Alright, well then. this brings us around to what do we do? 1. Identify the version of SQL Server you are on. If you are using SQL Server 2016 and up, I'd recommend to look at the Query Store feature. [Janice Griffin has a great slideshow on configuring it to best practices.](https://www.slideshare.net/SolarWinds/welcome-to-the-2016-query-store) 2. Use the features given to you. If you are out of luck with Query Store, you can still use DMV's, Extended Events, or [SET STATISTICS XML](https://docs.microsoft.com/en-us/sql/relational-databases/performance/display-an-actual-execution-plan?view=sql-server-2017) locally to obtain the query plan. 3. Build your stage / test environment as similar as you can to production. 4. Try several variations of the Clustered Index + Non-Clustered indexes. There's no way for us to tell you exactly how it is going to look. We can give well thought out educated guesses and drive you towards best practices, but you're going to have to put in leg work to test, measure, and tune. That's just the way SQL is some days since it's a declarative language. I hope this answers your questions. I know this was kind of a round about way of saying, go figure it out. The answer as I said is that I can guess and tell you to do a clustered index on lastname + first name (think about phone books and how they operate), but at the end of the day, SQL is going to do its own thing. Now while that seems scary for a moment, take a breath, and realize that it's ok. We want SQL to figure things out and 99.99% of the time it does it pretty well. When it doesn't, we can tune the query typically and if it has issues still, it could be based on underlying architecture. Which is exactly why it's important for you to run those tests. If you want a more specific answer at this point, please provide table create statements, SQL Server version, and a sample faked data set. I can use tools online to generate more fake data. I can take time (it won't be immediately) to re-create as similar of an environment as I can and run some tests for you to bring you a closer answer. You will still need to perform those "to do" steps I posted, there will be no way around that. But I am willing to put in more assistance if you guys are having issues still. \[1\] Korotkevitch, Dmitri. *Pro SQL Server Internals*. Apress, 2016. Information regarding how SQL Executes and optimizes a query, pages 463-466. \[2\] Itzik Ben-Gan, et al. *T-SQL Querying*. Microsoft Press, 2015. Snippet regarding indexes on page 93.
That did the trick! Thanks!
You need a WHERE clause. In between FROM and GROUP add a line that says WHERE Inventory = 'live'. I'm not familiar with MySQL, I'm guessing it would be: WHERE "public"."inventory" = 'live'.
Not that I know of. Or if there is, it's not simple, and implementing it would probably take longer to just write it out.
much thanks :)!
This is much cleaner than my original attempt too, thanks!!
You need an OUTER JOIN
OK, well that's not the most helpful table format to do what you need, but it's possible. SELECT DISTINCT CategoryID FROM public.inventory WHERE CategoryID IN (SELECT CategoryID FROM public.inventory WHERE Inventory='stock') AND CategoryID NOT IN (SELECT CategoryID FROM public.inventory WHERE Inventory='live')
Jesus. Why can't people edit stuff like this before publishing it?
When you say one row took 3 seconds, how did you force it do just 1 row? If you had just 1 match, but 3 million persons, the query engine still had to go through all 3 million persons (twice actually) in order to update that one match. So it may not be that much longer to do them all. Otherwise I don't see anything horrible about the query. If you haven't indexed anything, then clearly putting an a covering index on persons.id and persons.birthday will likely help. EXPLAIN QUERY PLAN would help troubleshoot.
Mine, too. That's one of those few things that universal to about every SQL dialect: No wildcards.
This should get you what you need. Written it as a SELECT for testing, but obviously modify as needed to make it an UPDATE. The IFNULL is important because if the subquery doesn't find any records, it will return a NULL instead of a zero so you need to handle that. SELECT *, IFNULL((SELECT COUNT(*) FROM matches m1 WHERE m1.date &lt; m.date and m1.person_id = m.person_id),0) as matches_to_date FROM matches m Note that performance may not be great here, because for each row its got to go back and do another query essentially, and I'm not sure how well SQlite can optimize that.
I take it, it worked? If so, you're very welcome. Updates with joins are tricky but they can do powerful things.
Having the same ad between what felt like every paragraph made me stop reading.
This works..! And exactly as you said, it is slow. Given 3 million entries, this would take forever to run :(!
Excellent thanks you so much!!!!
3s for one row by putting a `WHERE` clause specifying a specific entry in `matches` This actually finished running in less than 30s!
Making an index on date will help. I'd also store the date as an integer (the seconds since the epoch; standard Unix time) because that's a lot faster to compare than strings are. You might be able to use a [computed index](https://sqlite.org/expridx.html) if you don't want to convert your data... Something like `CREATE INDEX matches_idx_time ON matches(strftime('%s', date))` and then use `strftime('%s', date)` instead of the plain column in your where clauses.
The whole thing is an ad; the closing paragraph is "buy our product". 
I tried this, unfortunately it didn't help.
What about one on (id, date)?
 I don't work for free, so I'll spend a minimum of time on your advertising campaign for you: &gt; Who is using MySQL and MariaDB? Who cares? &gt; The main conclusion from this document is that you can’t rely on the fact that you can easily migrate from one to another. I can't rely on the fact? If I can't rely on it, how is it a fact? &gt; Therefore, if you are using GCP, you might have to consider using MySQL, Certified MariaDB images are available on GCP. &gt; MariaDB's developments are open for a public vote and mailing lists discussions "Developments"? Maybe you mean "development process"? 
`person_id`, `match_id`, and `date` all have indexes hehe!
A multi column index, not multiple single column index.
just tried this `CREATE INDEX date_match ON matches(date, match_id)` and ran the same query resulting in same run time :( yikes..
Ops! I edited too late!
I'm not that familiar with SQLlite, but is the IFNULL really necessary? AFAIK, in most other SQL implementations, count will always return an integer, even if it is 0. 
Definitely care about the actual count of previous entries. Excuse my ignorance, is this what you mean? [pastebin link](https://pastebin.com/h8h6HRVq)
 If you're looking for some very hands-on "learn-by-doing" practice problems, that teach basic to advanced SQL with well-designed, real-world practice problems, check out [SQLPracticeProblems.com](https://sqlpracticeproblems.com/#utm_source=r1&amp;utm_medium=r1&amp;utm_campaign=r1). I developed it after teaching a SQL course where the material I had to to teach from was poorly structured and academic. Afterwards, the students emailed me, saying they needed practice problems, so I developed the course! Contact me (email in the FAQ) if you have questions. 
How often do you have to do this? DECLARE @prefix VARCHAR (10) SET @prefix = 'hp' IF NOT EXISTS ( SELECT 1 FROM sys.tables WHERE Name = 'LimitBreak1' ) CREATE TABLE LimitBreak1 ( @prefix datatype, @prefix + '_growth' datatype, .. etc ) Then just change the prefix and the table name I haven't tested this, I'm on my phone. You might need to adjust the code a bit but that's basically it
By how often as in how many columns or tables? It'd be 148 columns across 4 tables with every table having the same name on their columns. How much data? Probably ~300 entries. This table is to fetch/get data, not add to it regularly, maybe once every few weeks at most.
Why 4 tables? Just add one more column called break_level or something similar and populate it with 1 to 4. Easier to add WHERE break_level=1 than to figure out which table name you need.
Each limit break table has 36 different values
Yea automated. And no I don't drop and rebuild them daily.
Mind sharing an example? I was thinking on doing that but wouldn't that be one table with over 100 columns?
Thanks, this was a good update on the subject as I don't do a lot of work with either DB.
Hey there, thanks for the recommendation. My solution is as written in the post. What is happening in the query you recommended? Those keywords are completely foreign to me
Why would it have over 100 columns if all of the columns across the four tables you want to make are the same in each table? If each table has columns named a, b, c, and d (for example), then the combined table only needs column a, b, c, and d.
So if you only have to ever create these 4 tables one time than just do it with the script, it takes 30 seconds. Copy and paste and change the table name and foreign key where needed, since they have the same columns. I don't see a need for automation if you're not dropping and recreate them. Plus automating it is going to cause you to write more code, which it seems you're opposed to. 
I'd split it before you get to the point of performing the `INSERT` in your ETL job. IOW, don't do it in SQL but whatever language your ETL job is written in.
OH! OH! OOOOOHHHH!!! (Thunking head on keyboard) I had it grouped by something I removed in “SELECT,” but didn’t remove in “GROUP.” Thanks everyone for your advice, but it turned out to be an id10t issue!
I think what you've described is terribly non-normalized. You really shouldn't have 4 tables that are identical in everything but the table name. Combine the 4 tables into 1 table and make it 38 columns....with the 38th column named "LimitBreakLevel" or something similar. Moving on to your actual question - I'm not sure what you're asking. You're asking how to have 4 tables? Are you asking how to create the tables? Are you asking which flavor of SQL you should be using? I think we need either some more info or some more clarity - because you're not really asking any question other than "Is there a simple way to achieve this?"
A beautiful and thorough answer. Thank you very much.
What version are you on? If it's &lt;2016, RIP. `DECLARE @js nvarchar(max) =N'{` `"address": {` `"state": "Example State",` `"street": "Example St",` `"number": "4/33",` `"suburb": "Example Suburb",` `"postcode": "0000",` `"country": "Example Country"` `}` `}'` `SELECT` `[State]` `,[Number]` `,CASE` `WHEN CHARINDEX('/', [Number]) IS NOT NULL` `THEN SUBSTRING([Number], 0, CHARINDEX('/', [Number]))` `ELSE NULL` `END AS unit_number` `,CASE` `WHEN CHARINDEX('/', [Number]) IS NOT NULL` `THEN SUBSTRING([Number], CHARINDEX('/', [Number])+1, LEN([Number]))` `ELSE [Number]` `END AS street_number` `FROM` `OPENJSON(@js)` `WITH (` `[State] NVARCHAR(200) '$.address.state'` `,[Number] NVARCHAR(200) '$.address.number'` `)` There are likely a lot of efficiency gains to be made on this. 
ROFHowling
Take your upvote and get out. 
Does your book involve using real .sql files , that we can load up on mySQL Workbench and play around with? 
Ok, thanks Randy
How couldn't I come up with this... Sometimes my brain tries to overly complicate these things...
I was worried about normalization. i'm pretty bad at explaining but [someone came with an answer](https://www.reddit.com/r/SQL/comments/96tl7y/four_tables_with_the_same_columns_that_all_have/e43yjza/?context=3) which is what you said too. I don't know how I didn't think of this.
It happens. It's super easy to be focused on some bigger picture and do silly things. Forest for the trees, or whatever. Just try to remember any time you're having to type the same things a bunch of times, you're probably not normalized. In schema creation and beyond. Hope that you got the answer you were looking for.
Does the Pluralsight course provide real .sql files you can play around with? Thanks!
Yes, there's a practice database that you set up, and then work with that. 
Glad you got it working nicely. For EXPLAIN, i was referring to this... https://www.sqlite.org/eqp.html Whenever you have a query performance issue, prefix your query with EXPLAIN and rerun it. It will give you detailed info about the execution plan, what indices it has considered and what indices it ended up using. It takes most of the guesswork out of why it is not performing like you want it to
Of those choices, method 1. From what I understand of your explanation (are you proposing writing a full on frontend?), I'd feel quite uncomfortable using method 2 because of the scope for things going wrong. Method 1 will take a little longer to administrate (though you can define the required roles and map logins to those, instead of defining perms per-table, unless each user has very particular requirements), but you don't have many users and method 2 sounds like it comes with quite a dev burden anyway. The ideal solution (IMHO) is AD or similar groups mapped to roles. Of course, that comes with prerequisites.
Elastigirl would have difficulty with a stretch this long.
It's coming from an outside source unfortunately, and is fixed JSON that won't be modified.
2017....so no RIP required :) Thanks for this, will give it a whirl!
Both. One typical setup is for users to authenticate to the application (PHP web app in your example) and the application connects to the database via predefined service accounts that have limited privileges. This way users do not directly log into the database. It tends to be better for security to isolate the users from the database, and it allows for connection pooling for the application (better performance, among other reasons). The application should not use accounts with admin privileges.
That's odd, that's what I did, adding EXPLAIN prefix, and the pastebin content showed haha.. maybe I did something wrong 
DROP DATABASE DumbJoke
 DECLARE @pun varchar(15) SET @pun = 'where clause' IF @pun = 'bad' PRINT N'Boo.'; ELSE PRINT N'Sensible chuckle.'; GO Boo. 
Ah, you mean application layer security, but also have one read-only database user, one update user, and one user with delete priviledges. Then all of the lowest tier users would use the SQL read-only user to connect to the database, while having individual logins at the application level. Clever (and obvious now that you've explained it)! The only draw back to application layer security is proper password handling - making sure that they're hashed and encrypted. There are standard secure ways of doing this now though, so it wouldn't be that hard. I guess with either method I (or whoever writes the software) could intercept their passwords and store them and see if the users used the same password for their banking etc. 
A modern web application framework will have all that built in, too.
yes, my next plan is to learn Laravel.
I'm not an expert on databases, just browsing through looking for an answer. But I think he means, Extract the JSON for example with python (I normally use python, that's the only reason why I brought it up) then load the result, use regex to separate it into 2 columns and then insert it into your DB. Basically fix it before it hits the DB instead of fixing it in the DB 
The number of full time students plus the number of part time students will always be the total number of students. So either I'm missing something or you don't care about the part time / full time field. You can get your results with: SELECT COUNT(*) / 3
Do you have some sample data or a diagram maybe?
Something like ALTER TABLE myTable ADD FTE AS ((FullTimeStudents + PartTimeStudents) /3) Then figure out whether you want the values to be stored, and add the PERSISTED keyword if so. If I've misunderstood and you don't have separate columns for full time and part time, you'd have to refractor or implement this as a view or something I think.
 SELECT (SELECT COUNT(*) / 3 FROM &lt;table&gt; WHERE TYPE = 'part time') + (SELECT COUNT(*) FROM &lt;table&gt; WHERE TYPE = 'full time') It can be prettier with some group by but I'm lazy right now and I feel you only care about the results.
What does this even mean? If you have all the data, in the form of full and diff/log backups, then yes. Restore the full, then any diff backup, then the log backups in sequence using WITH NORECOVERY for all but the final one (else the incomplete transactions will be rolled back).
Include a table: TeamAssignments ------------------------ EmployeeID TeamID EffFromTimestamp EffToTimestamp --null if current assignment. That way when you can find team assignments for a specific date.
That's what I have with the 'managers' table, but I'd like to know if it's quicker for the back-end to pull all the performance reports and then look up who the relevant manager was during that period, or if it'd be quicker to have the manager hard-written along with each entry and then to be able to just select all records where the 'manager' field equals the selected manager. Does that make sense?
It depends on the volume of data, which columns you're indexing on, and what your specific query looks like.
You just need to order it by a date or an ID or something. It's hard to say exactly without knowing your table but: SELECT TOP 100 \* FROM table ORDER BY column DESC
Does your table have an ID column (or unique primary key of some sort)? If so, just do: SELECT TOP(100) * FROM table ORDER BY yourIdField DESC If it only has a date field, you would do the same but replace yourIdField with yourDateField. On mobile, so formatting might suck.
You rock! I got it to work. 
Glad I could help!
While obtaining a MCSA cert can help guide your study path, just know that those certs have a list of topics they pull from. The idea is that you will learn and investigate all topics so when quizzed on anything, you can receive a passing grade. This is a nice tool to creating a road map and testing your knowledge and giving you something tangible to show other employers. My primary recommendation is that you use more than one source during your study period and to focus on technologies directly involved in the now with your current employer. Once you have a good grasp on the basics and are able to accomplish your job to deliver value, you can branch outward to other topics or things that interest you. So yes, it's a good idea. It's also not the only idea and there are many ways to learn. You should create while learning so you can build a portfolio to showcase as you grow. 
Great answer, thank you very much.
Whoops haha, I did know and you are right. Damn autocorrect. And yes, they use SQL server here. As for the Overkill thing...that's just me. I'm the future proofing, go big type (or at least I'd like to think so) Ok cool, I'll take a look. Thank you!
Trying that but it won't bind the tmp.storeinventory I'll keep at it. Thanks for the suggestion.
Sorry, I should have been more clear! The 'managers' table could contain several entries for a single employee. A new entry is created each time the employee moves manager, and the previous entry is updated with the last date the employee was under that manager. So for example, the table below shows 5 employees. Employees 00002 and 00003 changed manager on August 13th. |emp\_id|manager\_id|date\_start|date\_end| |:-|:-|:-|:-| |00001||8/7/2018|12/31/9999| |00002|00003|8/7/2018|8/13/2018| |00003|00008|8/7/2018|8/13/2018| |00004|00001|8/7/2018|12/31/9999| |00005|00003|8/7/2018|12/31/9999| |00002|00008|8/14/2018|12/31/9999| |00003|00001|8/14/2018|12/31/9999| I plan on setting it up so that whenever I change a user's manager, it updates the existing entry with the relevant end\_date and creates a new entry to show they've moved. I hope that answers your question!
I had a feeling it would be an "it depends" situation! I'm thinking that I might set up both - one table that only logs the employee ID and requires reference to other tables, and one table that hard-writes all of the information as it's logged. I'll test them both and see which one returns the fastest results.
I think so. From what you describe it seems like you could do this with outer joins to get your desired view. Then based on the amount of data look into indexing and tuning your performance for the render. You're scratching the surface of dimensional modeling or DW design pieces that'd make it more structured and performant
is there a reason why you can't just specify the dates you want? the current issue with your join is that you're requiring t1 and t2 selldate to be equal
&gt; So in the import table (t2) the date may be '2018-08-04' but in the sales table (t1) I want it to update the row that has a selldate of '2018-08-13' Then you would have to get rid of the t1.selldate = t2.selldate restriction?
Yeah i have to get rid of the selldate. That was the issue. The problem is I don't know the exact date of the import from the ETL. It's just the first date reported. So what I ended up doing is updating the etl so that every date on the import has the store inventory. Then I just do the update. It's a little slower, but it was much easier.
I don't know what the dates will be. They put the inventory in the first row of the flat file that I import, by store. So if they report 7 days it's different date than if they reported 10 days. I did a row (over) but that had limitations on the join. In the end I just filled the dates with the inventory number. That way I avoid the join on date. There is only one anyway so it was kind of an ugly hack, but it works.
You probably want your logs to be set up as some kind of transaction log that captures the relevant information outside of the relational structure of your "live" data. This would be how a database would keep track of invoices, etc. Imagine if you only had a relational structure that held the price of each item in a table with items. Every time you changed the price, every invoice that related to that table would change. When you make an entry in the log, you record the current state of the things you need... employee-id, manager-id, date of entry, etc. Then you just query your log as needed and you don't have to worry about changes in the structures.
Actually it's DATEPART(dw, GETDATE())
Also, since you are working with SQL Server and are interested in learning it / becoming certified, the book T-SQL Fundamentals may be an excellent starting point for you. 
Access is probably the simplest front-end if everyone is using Windows. You'd want to follow the [basic steps as outlined by Microsoft](https://support.office.com/en-us/article/import-or-link-to-data-in-an-sql-server-database-a5a3b4eb-57b9-45a0-b732-77bc6089b84e) to connect an Access to SQL server. In this case, you'll be linking to the server, rather than importing the data. Depending on your security requirements, you can use per-user security (so everyone needs both the Access file and access granted to their account on the database), or a shared connection (everyone connects to the SQL server as the same SQL user). Once you've successfully made the connection, you'll have a linked table or tables. That can then be used to build queries, reports, forms, etc. 
Access can definitely be used as a quick solution with minimal programming experience, but I personally wouldn't rely on it for the long-term. You can set up a data source to connect to your SQL table and drag/drop a GUI interface. Depending on the complexity of your needs, you can hook it directly up to the table and just have an excel-like interface, or you can use forms to make more of a 'friendly' interface. With that said, you're still going to have to understand about database structures and deal with things like user concurrency...
If you don’t want to the Access route, WinForms is probably the easiest way to quickly roll out an interface. It’s been around forever, lots of resources on the Google machine, and free. Just download Visual Studio Express and you’re off. 
If you're in charge of that, and you don't do any programming....there's this thing called the paradigm of failure. Make something halfway acceptable so they realize you suck at that and promote you to the next thing. If you're serious, Python tkinter has GUI components that are easy to use. Depending on the exact SQL server flavor you've got, there's options for querying it through Python as well, passing it all through tkinter....but obviously, that would involve coding...as designing a user interface generally does.... But I have a feeling you were put in charge of that with full expectations that it would be, at best, halfway passable...unless you've done a lot of lying.
If op means MS SQL Server then Python is probably not the way to go. I dont believe the ODBC drivers don't support stored procs. 
I do mean MS SQL Server so Python is probably out 
Why would that be a deal breaker? If it's all going to be locked up in an executable then you could copy and paste the query into a cur.execute stored in a function, it would act the same as running a stored procedure when it's called with just a minor difference in how it's called...no?
Why not use ssms as the interface?
Yea, it would. However, with ops lack of experience that just adds an additional layer of complexity. Additionally, you have to think about hosting said interface. If this is your run of the mill small/mid size business I'd assume their probably going to have this running on a Windows server and I don't think the two will play nicely. As much as I dislike Microsoft, there's gonna be a lot less hoops to jump through of you just stay within the MS stack. 
The more I think about it, and the situation as OP stated, you're probably right. 
This sounds like a good use case for PowerApps, if licensing costs aren't prohibitive and and a cloud front-end is acceptable..
As a person that built way, way, way too much stuff in MS Access with VBA back in the day... tread lightly. What makes for a pretty decent one person solution becomes a nightmare at larger user volumes. You didn’t mention how many people would need to use and if those are simultaneous connections or not. Or if they are writing/editing data or just viewing. Access behaviors when multiple users are sharing a SQL Server table leaves much to be desired. Updating ODBC connections is a pain (we wrote VBA to do it automagically on the Access splash form). It gets worse when people are trying to use the same Access file from a network share instead of everyone having their own copy locally. While this might be the fastest path to a GUI, it will leave you with a mountain of support debt if someone tries to it scale it up.
There's a free version of visual studio. Lot's of ways to write an app with that, and youtube videos to walk you through it. I'm going to suggest something off the wall: Lazarus. It's an open source Delphi clone and there's no faster way to make a natively compiled database app except to get Delphi (which isn't free). You can target Windows, Mac, Linux. It's free.
Would you recommend Access over SSMS?
Relatively new to sql so not sure what more advance people think but, I thought the tutorial was pretty decent. Signed up with my work email, work for a Fortune 500, so their sales people would not stop bugging me. Otherwise I liked it. 
Yeah, I closely relate to everything said here and agree completely. 
Mmm.. That a good point. Just thought it be a heavy load to check at every request. 
And you need a credit table, then check each transaction against the credit amount remaining. 
if I'm dynamically calculating, what purpose would that credit table serve? I would prefer not to do any heavy accounting ledger type tables that are constantly written .. i have a feeling it would cause some kinda nightmare
Desktop or web-based UI? If it's the former, then get VisualStudio and learn C#. If you've done any programming at all, then it's pretty nice. If it's the latter then Laravel is nice and simple: &gt; https://laracasts.com/discuss/channels/general-discussion/connect-laravel-to-microsoft-sql ...no idea what it's like with MSSQL Server though. Either way, you're probably going to have to get a lot of help from your DBA. 
[removed]
My company spent a month or two evaluating it. Some of the features were convenient, but overall I found their syntax to be really limiting, their graphing tools lacked many obvious options that we needed, and overall the entire system would make your browser grind to a halt if you made a report with more than a handful of queries/charts (i.e. anything useful). I tried both chrome and firefox and at some points they'd be using 50% cpu (on an i7 processor) and 8 GB of RAM. Which is frankly absurd. We were evaluating them because their pricing model was attractive. A flat fee would let us use their system for any number of "White label embeds" to make a client facing report integration on our site. We ultimately moved on for all of the reasons above. 
If you're using MSSQL Server 16+, you get to use temporal tables for them too!
The tricky thing about database design is that, barring outright poor practice, "better" can be a nebulous concept. It depends on your workloads, volumes, available server CPU and IO, any statutory requirements etc. I'd lean towards option 1. You already have an SCD ("managers" table), which makes things easier. Queries should be acceptably performant with good index support (e.g. nonclustered on EmployeeID, ManagerStartDate, ManangerEndDate). You're talking about two joins (analyses, employee, manager) largely on integer PKs/FKs (so presumably index seeks), nothing too wild. Not that the extra storage required should be much of an issue for option 2, unless you were to add a \*lot\* of columns. If "analyses" is large (1m+ rows) then you could consider a columnstore index.
Option 2 is going to query faster, if that's your main consideration.
Kind of dangerous to assume IDs have any meaning (i.e. date sequentiality) IMHO.
There might be but your performance will be absolutely horrendous. It's probably better to move the data.
Why not just generate new 7 character ID and have a table that links between new and old?
commence a regular data export from one to other and join as you like
Have a look at http://sql-format.com/ I'm always using this tool to format my code
Good approach. Keep track of the old ID number and the source system. Create a new unique ID to use post merge. It would be ok to create a new table, or simple add columns to the existing table.
Yeah that’s true. I guess I didn’t think about it because pretty much every table I work with would fall into highest ID = newest record unless it’s an extremely small table mapping something like a user type.
I am. However, I don't have access to the new database. Also, as I understand, this is a limitation of the API and not the database explicitly.
While the name is indeed "SQL" and not "Sequel", in a sense [the former is indeed short for the latter](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6359709)... (starts to appear on the second page)
There is a reason most accounting tables look similar. If you're going to track billing you'll need something similar imho. 
Niiice. I like the ability to alias a window, as well. It's amazing, they went from zero support to what appears to be fully featured windowing.
Identify the data you need first either in a cte or temp table. Join that to the target update table on the unique Id then update. Also check your indexes. 
You can generate GUIDs in just about every SQL Studio though right? Personally I would see this as an opportunity to clean up an unideal system
This sounds like a pretty basic accounting ledger. * Charge the CC and record a credit in the user's ledger * As the user uses the system, record debits in their ledger * Every X minutes, query the table to find users who have an outstanding balance below your threshold (can do this with a single query using the right aggregate functions) * Charge the CC for the users who are below the threshold Adjust X based on how quickly users typically burn through their credit. For example, E-ZPass (highway tolls) has a threshold of $10 remaining and charges $25 to my credit card when I dip below that. But I don't get charged the instant I drop below $10 - there's a slight lag. I think I've even see it let me go negative for a day because they have my credit card on file, so they know it'll catch up.
[First link of SO](https://stackoverflow.com/questions/3662766/sql-server-how-to-lock-a-table-until-a-stored-procedure-finishes)
can you get the company to pay $299 ? https://www.xojo.com/store/ it has a drag and drop interface which will make things much easier http://developer.xojo.com/desktop-quickstart it uses the basic language so its simple to use 
If the relational database management system is SQL Server, I would recommend to use SQL Server Management Studios which is a separate download and install now from the engine. [https://docs.microsoft.com/en-us/sql/ssms/download-sql-server-management-studio-ssms?view=sql-server-2017](https://docs.microsoft.com/en-us/sql/ssms/download-sql-server-management-studio-ssms?view=sql-server-2017) SQL Operations Studios may also be a good alternative now if you are only viewing tables. [https://docs.microsoft.com/en-us/sql/sql-operations-studio/download?view=sql-server-2017](https://docs.microsoft.com/en-us/sql/sql-operations-studio/download?view=sql-server-2017)
SQLite in Android 
The model database is copied each time a new database is created on the system--it's not used for anything else. So if this is a custom object, you can backup the model database, delete the object from the model database, and try again... doing so won't impact anything in production unless you're creating new databases all the time that rely on this object being there...
It is difficult to really recommend anything without knowing more. "User interface" is pretty vague...or is vague. What is this user interface trying to do? Who is the primary demographic for the interface? Analysts? Executives? Are the users centrally located or remotely located? What type of actions are the users supposed to perform on the interface? Input data? Update Data? Only retrieve reports? If reports, are you talking about data dumps like a CSV of some table? Or pretty Tableau like dashboards. Is there a budget? If so, what's the budget for implementation? What is the budget for maintaining? Who is going to be in charge of maintaining this? 
PowerApps looks like an amazing tool to use and I think I'm going to use it going forward. Right now I'm looking at tutorials and examples. Thank you so much!
All of the paid products have managements tools that are included with the server license. You just need to download them. That's a good place to start. In the FOSS area, a lot of folks at my company really like TOAD.
DBVisualizer is my go-to tool. There's a free version, but some features like searching through results and uploading CSVs are only included on the Pro version.
UPDATE t1 SET t1.Col1 =t2.Col1 ,t1Col2 = t2.Col2 FROM table1 t1 JOIN table2 t2
Thanks very much!
I used TOAD for years but came to really like PL/SQL Developer (not to be confused with Oracle's free version called SQL Developer). I just really like the interface better. Took some getting used to at first, though. 
None of the data except the password is encrypted, its all present and correct as it should be. The password is encrypting but not as sha1. I don't know how to find out what its encrypting it as? If its not inserting the password and instead inserting a blank space, would that show in the database as an encrypted string or would it just be a bank space? Sorry I hate asking silly questions, Im still learning
Can you implement redis for W3 cache? Should significantly improve performance.
Thank you, between this and another comment on my other post I've worked it out
What was wrong?
I was encrypting it in the INSERT...VALUES rather than before. I've changed it so $pwdnew=SHA1($password), and then put $pwdnew as the value
Access doesn’t do much for you that’s intuitive. It’s either a table view or you build a bound form. You can make it intuitive via good GUI design, but if you go the form route, it’s only as good as you make it. Some of the worst forms I’ve ever seen were built in Access. And a few of the best too. If you are needing a step above Excel for row by row editing, it’s OK. But if you do lots of mass updates, custom logic or operations, etc, you will have to do it all in code behind. Be aware that large record sets and wide tables may perform badly, and record locking will be problematic. The most successful things we built there were interacting with 3270 emulation software and gathering data locally, using a few SQL tables for reference and lookups. The “work” tables were loaded with data all unique to the user by VBA routines and were local inside Access. I don’t know the workflow, but if it could be some type of hybrid like this, it can be effectively used and avoid most of the pitfalls.
I’m thinking since it’s an internal app that doesn’t need to be scaled I’ll just use PowerApps. It accomplishes everything I need to do and can take multiple data inputs be it SQL or Excel 
`coalesce()` is your friend https://www.db-fiddle.com/f/dkrT7RLBmzTbWLx3xUB5bd/0 
1. please provide what DBMS you are using 2. what do you mean "skip blanks"? Like, blank fields or entirely blank rows? If the fields are actually blank ("") it should just work, if they're null then you can add case statements or replace may work. 3. you need a new "data guy"
For instance, sometimes a person might not have a middle initial. Instead of looking like John Doe, it would come out John Doe. 
`CASE WHEN MI IS NOT NULL THEN MI||'.' ELSE '' END "MidName",` Should work. 
I believe this is just adding a period to the middle initial. What I’m looking for is a way to concatenate the name when, for instance, someone has no middle initial. Right now I’m being told that blank values just add a bunch of spaces. 
Okay... so you want to turn prefix, firstname, middlename, lastname, suffix into a single string? But if some components of the string are null, you want the query to be able to handle that? select trim(CASE WHEN PREF IS NOT NULL THEN pref||' ' END || CASE WHEN FIRSTNAME IS NOT NULL THEN firstname||' ' END || CASE WHEN MI IS NOT NULL THEN MI||'. ' END || CASE WHEN LASTNAME IS NOT NULL THEN lastname||' ' END || CASE WHEN SUFF IS NOT NULL THEN suff END) FROM mytable; 
Well, for starters, wrap the deletes and inserts in a transaction based try/catch block. That way, you can have a roll back if any errors occur.
So, no salt?
If you have to repeat this task, put it in a stored procedure and schedule it
I believe so.
Here's what I'd do: SELECT ID, PREF, FIRSTNAME, MI, LASTNAME, SUFF, LTRIM(COALESCE(PREF, '') || ' ') || LTRIM(COALESCE(FIRSTNAME, '') || ' ') || LTRIM(CASE WHEN MI IS NOT NULL THEN MI || '.' ELSE '' END || ' ') || LTRIM(COALESCE(LASTNAME, '') || ' ') || LTRIM(COALESCE(SUFF, '') || ' ') As FullName FROM Constituents http://sqlfiddle.com/#!17/8ce31/1 I can't see if it's what /u/boy_named_su did as the db-fiddle link doesn't seem to be working any more.
What do you consider "worth while", can you elaborate a little bit as to why you'd want to be certified? 
Jesus why? Make a web service around what you need. That way sql &lt;-&gt; service is running properly, and the latency is well handled by the http protocol. WCF would do neatly if you're in the .net world. 
COALESCE is just a CASE statement.
Hi, Although there is some manual approach to fix certain errors and issues in MDF, NDF, LDF files as well as any level of an issue in the transaction log. But, sometimes due to the certain higher level of issues, things are not done as we hope so at that phase any user can use a [SQL Database Recovery](http://www.databasefilerecovery.com/ms-sql-database-recovery.html) Tool to work without any interruption.
Well since he wants to transition into a different career path, I assume the certification has something to do with strengthening himself as a candidate for a data science job
It all depends where you want to work so if the area you live in has companies that utilize Microsoft products then it may be beneficial to get certified. I heard the MCSE is decent.
Hi, I think firstly any user should first try [manual ways](https://poonamrblog.wordpress.com/2015/11/26/how-to-repair-corrupted-sql-server-database-manually/) to fix any error in SQL Server. In case, things go out of hand then at that time user can go for an tool to repair any error in SQL Server. Thanks
Sure, but if it's a transition into a new career than I think certs can help. If it's a move into data science, having a degree in a proper field and PhD would be important. Certs wouldn't be the only option for them either, creating a portfolio to showcase and gaining experience in an internship or volunteer work would probably help more though. 
Sounds like you've been very successful with structured paths. Given your past, I'm sure you will succeed at MCSE or MCSD. It sounds like you have very desirable experiences already and many would be interested in learning from you! I'd suggest exploring a non-structured path. If you are interested in all of the topics covered by an MCSE or MCSD, can you adapt the material and examples to create a curriculum suitable for a CFO / CIO, then blog about it? I would be interested :)
You should be able to use the import data wizard to create and import the table so you can then script it out to get the correct definition. However as others have said... avoid using select *
What is this crap? A random download of some 'source' files with no context or explanation? As blogspam goes this is a poor attempt.
 SELECT a.id, b.id, sum(amount1) as amount1_sum, sum(amount2) as amount2_sum FROM a join b using(id) GROUP BY a.id, b.id HAVING sum(amount1) = sum(amount2)
Ha, I like it! Wish I could say that this is something that I could do soon...I suppose there is no harm building out what such a knowledge base would look like. I already like to run my mouth online so may as well combine that trait with my current and future knowledge. Thank you /u/emican!
Certificates are more for getting you in the door. I see that you are using MS SQL Server. Luckily they make a one user version available for free for non-production purposes. I would suggest grabbing that plus some test databases and playing. Start with getting thoroughly familiar with queries and tables and then work your way through joins. Now you have some basics, you can decide what else you need to know and choose courses accordingly. The two main directions are administration where you learn care and maintenance of databases as well as optimal design for performance or developer where you would look at how the JS links with the database (generally via stored procedures). In a small place you may need to know both admin and development but they are distinct directions.
Quick glance, but wouldn't you want the operator in the HAVING clause to be &lt;&gt; instead of = ?
Good question. Worthwhile with regard to perceived usefulness on the "market" for like-minded professionals and potential clients / employers. I don't hold any certifications at present aside from my MSA and like the implied ability of making my capabilities more transparent to others.
&gt; Worthwhile with regard to perceived usefulness on the "market" for like-minded professionals and potential clients / employers. Most of the time my cert is a checkbox for the recruiter. If being certified is ever brought up, it's usually just "Are you certified in any technologies?" in which I reply "Yes I'm a Microsoft Certified Solutions Associate for SQL Server 2016 development." They check the box and we move onto the next question, this can sometimes help you get past HR and get an interview. When it comes down to picking someone, they are going to make sure the person can a) achieve the job and b) fit in with the culture. If they are having a hard time deciding between two people and it's a tie breaker, the cert could be considered in the tie break. They would probably also weigh volunteer work, homework assignments if any during the interview, current contributions online, speaking events, etc. So I think there is value in the cert, but if your goal is perceived usefulness on the market, having code to showcase or speaking events under you belt are better in my opinion because it's not just a way to say you're competent in that technology, there is proof. Certs can be faked. (I still think there is value in certs and there is value in your use case, but I think there could be more value in other methods. I've written a little about [why and who should become certified](https://jonshaulis.com/index.php/2018/07/25/why-and-who-should-become-certified/) if you are interested, I'm specifically interested myself in the Data certifications from Microsoft.)
yep. 
this doesn't work for me - it provides the total sums of the Amount columns instead of the sums per ID
 SELECT a.ID, A_Amount, B_Amount FROM (SELECT ID, SUM(Amount) as A_Amount FROM A GROUP BY ID) as a JOIN (SELECT ID, SUM(Amount) as B_Amount FROM B GROUP BY ID) as b ON a.ID = b.ID WHERE A_Amount != B_Amount;
This is what I came here to post. Should work perfectly.
 If you're looking for some very hands-on "learn-by-doing" practice problems, that teach basic to advanced SQL with well-designed, real-world practice problems, similar to what you're trying to solve, check out [SQLPracticeProblems.com](https://sqlpracticeproblems.com/#utm_source=r1&amp;utm_medium=r1&amp;utm_campaign=r1). I developed it after teaching a SQL course where the material I had to to teach from was poorly structured and academic. Afterwards, the students emailed me, saying they needed practice problems, so I developed the course! Contact me (email in the FAQ) if you have questions. Also, redditors get 30% off the Professional package (discount code reddit30). 
can a product have more than one price for the same date? if so, your DISTINCT is redundant if not, your DISTINCT is redundant SELECT product , price , date FROM table INNER JOIN ( SELECT MAX(date) AS latest FROM table UNION ALL SELECT DATEADD(DAY,-1,MAX(date)) FROM table ) AS two ON two.latest = table.date 
Do you understand (it's not a trick question) what keys, grain and functional dependencies are? If so, the normalization required in real-time life is pretty simplistic (there are always exceptions, for sure): 1. 1NF: Pretty much always you'd want your data structure to be in 1NF (you have a good, stable key to data and your columns contain a single data point from your data domain/data type) 2. BCNF: Most of the time you'd want to eliminate partial/non-key functional dependencies (i.e. all your functional dependencies are in the form of &lt;A KEY&gt; -&gt; column. "A key" because there could be many keys in your table) 3. Quite often you'd want to break BCNF for performance reasons. Please remember to measure performance before doing any optimizations - premature optimization is a persistent anti-pattern. Not using ANSI syntax makes complex queries very hard to read and makes certain outer join combinations outright impossible without using subqueries. Depending on whether you'd consider 'WITH' clause a part of ANSI syntax, the old style will not allow you to write recursive queries either (with the exception of 'connect by' proprietary syntax).
I'm not sure what grain is. I just had the hardest time breaking the tables down into different pieces. Like finding the primary key went alright. But then finding and eliminating partial dependencies and then transitive dependencies is where I got a little lost 
Use a CTE? https://docs.microsoft.com/en-us/sql/t-sql/queries/with-common-table-expression-transact-sql?view=sql-server-2017 Alternatively you could create a table variable in your SQL for the report. e.g. --Temp Table: CREATE TABLE dbo.#Cars ( Car_id int NOT NULL, ColorCode varchar(10), ModelName varchar(20), Code int, DateEntered datetime ) INSERT INTO dbo.#Cars (Car_id, ColorCode, ModelName, Code, DateEntered) VALUES (1,'BlueGreen', 'Austen', 200801, GETDATE()) SELECT Car_id, ColorCode, ModelName, Code, DateEntered FROM dbo.#Cars DROP TABLE dbo.[#Cars] 
&gt;Is there a simple way to extract/copy the data structure from a table in a linked server? SELECT * INTO linked_server_name.database_name.table_name FROM source_server_name.source_database_name.source_table_name You probably want to check if the table exists first before you create it else you'll get an error if it already exists. e.g. IF EXISTS (SELECT * FROM linked_server_name.database_name.information_schema.tables where tablename = 'table_name') DROP TABLE linked_server_name.database_name.table_name SELECT * INTO linked_server_name.database_name.table_name FROM source_server_name.source_database_name.source_table_name 
I disagree with this. If you're doing a SELECT * INTO then IMO it doesn't matter if you specify the fieldnames or not. Waste of time as the destination table will be created as a copy of the source table irregardless of if you specify the fieldnames. Unless you only need to copy some of the fields and not all of them then specifying the field names is required. When I deliver SQL training I often advise not to use SELECT * in most circumstances. There are the odd circumstance (like you just want to quickly check a table) where it's fine to use and IMO the SELECT * INTO is one of them. 
SQL Tools for Oracle and Management studio for SQL server
Hi. Thanks for the reply. Good call, I’ll have a look when I’m back at work. Cheers !
Thank you!
Thank you! 
You're breaking the rules.
How do you need to handle cases where an ID is in only one of the tables? You might want to make that a full outer join just to be sure. 
I had to look up what ANSI Syntax even meant, but do people even use the "non-ANSI" approach on joins? I can't say I've ever come across anyone using the WHERE clause to make a join in either my career or in reading any reference material online.
Sadly yes, I still come across queries written using non-ANSI joins from time to time. I have a colleague who is a bit old school and he still does it occasionally when he's in a rush. Some (quite old) reporting systems generate SQL using non-ANSI syntax which adds to to the fun when decoding their mess.
I’ve never heard grain; I was taught to refer to the concept as “cardinalities” and have been referring to them as such for a while. Are the two words interchangeable?
Right, that makes sense. Essentially grain is just the concept of data granularity. I guess I got confused by the depiction a few comments up, since it was expressed like crows foot notation, which is a visual representation of cardinalities. Still though, I learned something. 
Yeah I know all bout them bridge/bin tables. 
For someone like myself who is only an advanced excel user, with zero knowledge of SQL, how many focused and dedicated hours of learning/practice would it take me to become proficient in SQL, enough where I would be "work ready", meaning able to pass a typical SQL test an employer may give me. I know everyone learns at a different pace, so you can give me a range of hours, if that helps you give me a more accurate answer.
Doing work for 20 years a certain way doesn't meant that modern (common) techniques and terminology are bad. This is how you become complacent and fall behind.
toad is awesome. 
You can store anything in a table. 
DB2 supports federated databases which should provide support for exactly this requirement. Its been a few years since I set one up so excuse my lack of detail. IIRC, database addressing will be a variant of instance.db.table@server or similar. Once remote tables are defined in your DB2 catalog, then define views for table navigation to removes complexity. It becomes just like working locally. Performance was only affected as much as network latency and handshake protocols. Surprisingly quick actually. As I said, its been a while (retired now) but the concept does work and may be an option I expect. Keywords you can start with are "federated" and "DB2". I was working on z/OS and AIX at the time. My exercise was complete once I successfully joined DB2 with Oracle, SQL Server etc. Hope this helps.
It's just a string.
You replied to yourself.
🙄 drat.
Every &gt;10 value signifies a new event for a device. The event includes all the following rows until &gt;10 again. These events needs to be ID'd. Not sure how moving those rows into a temp table would help.
I really appreciate the help here. Yesterday, the old query ran 79 minutes. Today it ran in 17. I had no idea windowed/partitioned functions were so useful and fast! Thank you so much. Here's the final query in case any are interested. I did not use the BETWEEN UNBOUNDED PRECEDING AND CURRENT. UPDATE StagingTable SET NetRevToDate = ISNULL(n,0.0) FROM StagingTable AS spf JOIN ( SELECT SourceID ,TransactionNumberID ,LoadNumberID ,BusinessFinancialID ,BranchID ,BranchRoleID ,DateID ,SUM(ISNULL(NetRevenue,0)) OVER (PARTITION BY SourceID ,TransactionNumberID ,LoadNumberID ,BusinessFinancialID ,BranchID ,BranchRoleID ORDER BY DateID ASC) as n FROM StagingTable (NOLOCK) ) st ON st.SourceID = spf.SourceID AND st.TransactionNumberID = spf.TransactionNumberID AND st.LoadNumberID = spf.LoadNumberID AND st.BusinessFinancialID = spf.BusinessFinancialID AND st.BranchID = spf.BranchID AND st.BranchRoleID = spf.BranchRoleID AND st.DateID = spf.DateID 
I finally cracked it with some assistance. As I though, the solution was actually really simple. My thinking simply got really stuck and I couldn't see the solution. select a.\*, sum(Value\_greater\_than\_10) over (partition by ID order by row\_number) as Event\_ID from Table1 a
"The goal of r/sql is to make a place for providing interesting and informative SQL content and discussions". Answered in r/learnprogramming
Please stop trolling me. There are no rules posted for this subreddit. There is a query help post about 10 posts down on this sub and has 5 upvotes. I posted here to maximum input to help me learn SQL.
Can you post an example of the source table? It almost sounds like your source data is denormalized, which may make this more cumbersome.
What dbms? SQL Server? MySQL? Details will differ. Your first job is to get read/write access to all (or some) of the tables in the database. (Administrative access is better.) Given the trouble you've had in the past, I'd recommend you pay a knowledgeable consultant to work out the details with the hosting company.
Maybe something like this? If I understand correctly what you are trying to, this WHERE clause is functionally equivalent: UPDATE a SET available_quantity = completed_quantity FROM quantumschema.activity a INNER JOIN quantumschema.activity_lot al ON al.activity_id = a.activity_id LEFT JOIN quantumschema.activity_alternates aalt ON aalt.activity_id = al.activity_id WHERE a.activity_id = ? AND EXISTS (SELECT 1 FROM quantumschema.activity_lot al2 WHERE al2.lot_id = al.lot_id AND al2.activity_id = al.Activity_id AND al2.sequence &gt; al.activity_id) AND (a.operation_type = 'standard' OR aalt.is_primary = 1) 
Ah, well the table we have at work is actually OK. There's no redundancy between columns like I have in my example. I can't post any of its data though, so I had to come up with an example which I guess is rather convoluted, lol. Sorry about that. I'm really just trying to see if there's a similar methodology in SQL to what I can easily do in Python, for example. I feel like we're way over-complicating what's an easily solvable issue when using another language. It's like we're using a hammer to drill in a screw. I know you can do something like this which is along the right train of thought, but it can't handle outputting new rows for each NULL column it comes across: SELECT ID ,name ,CASE WHEN is\_rented = 1 THEN 'Currently Rented' ELSE 'None' END AS Rental\_Holdup FROM rental;
I've just asked the guy at the hosting company to send me the details of the file names and sizes we're dealing with. I might be able to look up some old records and determine which SQL it is. 
The rudimentary solution is to write a select for every column and then union all the results: `select name, "not in stock" as rental_holdup from t where not_in_stock is not null` `union all` `select name, "currently rented" as rental_holdup from t where currently_rented is not null` etc and then one extra union all for the "None" use case `union all` `select name, "none" as rental_holdup from t where not_in_stock is null and currently_rented is null ...`
I knew the most basic SQL for my previous job (SELECT,FROM,WHERE) a few months ago, then quit to travel. In my free time I learned a little more because of a job I was applying for that wanted SQL knowledge. I still know very basic SQL, got the job, and am learning a ton more on the job. So I guess what I'm trying to get at is I had barely any applicable experience, but landed a job that I need to know it at. You can always learn on the job, just depends on whether you get the job or not. 
It depends on the proficiency you're looking for. I'd say that with 40 or so hours of study with a very practical, focused set of problems like mine, you could do quite well if the SQL test is more basic.
No, we need to see the schema. What are the columns, what's where? We don't need actual data. &gt; I feel like we're way over-complicating what's an easily solvable issue when using another language It's probably easily solvable in SQL as well, and without looping through results.
Sounds good thanks
Screenshots?
If you can understand (and explain) SELECT, FROM, WHERE, GROUP BY, ORDER BY, types of joins and primary key you'll likely know enough to land a junior position.
I'm not familiar with HeidiSQL, and have no clue if you're using SQL Server or something else. You could write a loop like this: DECLARE @i INT SET @i = 51 WHILE (@i &lt;= 322) BEGIN PRINT 'F' + REPLICATE('0',3-LEN(RTRIM(@i))) + RTRIM(@i) + ' VARCHAR(100),' SET @i = @i +1 END You could also use something like Excel to create the names, or a macro in something like Notepad++.
Yes, thank you! Union is what I needed! OK, thank you so much! Now I understand how to tackle these types of problems! I really appreciate your time and effort!!
My first data job did not use SQL. I didn't know anything about databases. I ended up unwittingly writing a primitive database engine in Excel VBA, including concepts like nested loops, merge joins, aggregation, and drill throughs. Tbe next job hired me into a 70% SQL role for 30k, still not knowing any SQL. Everything was so much easier than Excel, it was liberating. I made a full fledged data mart for the company in like my 6th or 7th week. I don't expect my experience is typical, but I think data is the the kind of thing that people either get or they don't. If you get it picking up SQL is easy, although you'll never stop learning new things. If you struggle with data it can take years of experience to write passable queries.
This is very encouraging, i want to move from an operational type job into a more data analysis role where SQL would be around 40% of the job duties.
That's pretty much the conclusion I've come to. I know it's definitely *a* way to do it, I just wanted to make sure it was the *best* way. Just wanted to make sure
I don't know SQL enough to write a query or script from memory without testing/playing with it, but here are the pieces I think you will need to make it work --primary_pmi and member_nmbr SELECT member_nbr, alt_key FROM table1 GROUP BY member_nbr, alt_key HAVING max(EndDate) SELECT member_nmbr, alt_key, ROW_NUMBER () OVER (PARTITION by member_nmbr ORDER BY EndDate DESC) as pmi_order FROM table1 
I knew zero sql, although I knew what it was because I had mucked about with mysql when doing some web stuff (wordpress, etc). I picked it up pretty quickly -- I'm certainly not an expert but I learn new stuff all the time. I work at a data analyst at a healthcare center, on the informatics team. It's great fun though sometimes I do get a bit worried that they will discover that I don't really know what the hell I'm doing. Been doing it 2 years though and so far they are still happy with me so I think I'll be ok. :D
&gt;Everything was so much easier than Excel, it was liberating. So true! I really thought it would be harder --- now I avoid Excel as much as possible. 
Ah, great! Glad it was helpful. Good luck in your SQL adventures! :D
21 years full-time xp and people always complain I'm not strong enough with clustering
I've honestly never done this before. I've been working on the SQL side, been told to bring the queries created in SQL over to Excel to allow clients to automatically generate reports from a certain date (parameter). So far I have created the connected to the database pointing Excel to the correct stored procedure, just unsure where to go from here. I believe the connection is created correctly as when I select it from existing connects I get the error message stating that no parameter has been supplied, which is expected as I have not yet passed the parameter into SQL. Its working out how to bridge the gap in-between that I am having difficulty with. 
My first role was an application administrator. I needed to be the sysadmin, developer, network, telecom, dba, and business analyst. I only had three servers to manage though and it was all relatively small / light weight. I was able to understand the basics of SQL syntax without any kind of complexity and I had an idea for how performance worked in SQL Server. Some questions I had: * Would this insert or update perform faster? * Identify what this query is doing. * Tell me how you would give me X from this table diagram. Ultimately, they weren't looking for me to be perfect or fluent in SQL, they just wanted me to have basic competency and the drive to learn. I illustrated this by studying for the interview and with my past work experience with the company. It was an internal promotion for me, they did have a few individuals outside the company interview too though. This was also the first interview / experience where I learned you don't wear white socks with a black suit. 
This one worked for me, just pulling in a table based on a date field and not running a sproc though. http://snippetsandhelp.blogspot.com/2013/05/excel-2013-getting-data-from.html
Thanks for the link. When I am selecting the tables, do I just select all the ones that I have used in my Stored Procedure? Or do I just use a similar select statement to the one shown in the link
It's been 7+ years since I did this but I think you load the date in the cell into a variable with VBA and then pass it with your query. 
Make sure you understand the differences between UNION and UNION ALL
Bringing back nightmares. I use SSRS or QlikView for this now. So much nicer. 
I use SSMS to build queries, are you talking about a truly dynamic query where a user could chose a table? When I used to build Excel VBA connections that at a previous company I had problems with version control and updates. At my current client I created a category of reports /dashboards we call Data Dumps. Typically just tables than can be easily exported to Excel. SSRS is good for realtime data. QV for bigger data sets with basic slice/dice for dimensions, etc. Pros: Centralized version control. Centralized User access control (depending how you're doing the connection in your VBA connection string. Most of my new report requests start out as a data dump now the incrementally turn into specialized reports or Dashboards. 
I think in the last function, you need to have the Correct column as the first input. Pretty sure, from when I've used it in the past, but I'm not during at a SSMS console right now to verify
I work for a large company (Fortune 10) and so most of my real hard data work is in warehouses/lakes, not SQL Server. I use SQL Server for ad-hoc stuff and as an archive of our reporting (it's too large for Excel, around 1.5m records a week, and I work in Finance so it's really critical from a legal standpoint those numbers don't change). The dashboards I build do change tables based on user request, as I have different tables for say different arms of the business. In most cases practically this just means it changes the SQL in the VBA to a different SP entirely, it's not a variable passed in SQL but stored in VBA to determine what SP to kick off. I use SP rather than building SQL into the VBA entirely as if I need to edit the query it's easier to open SSMS and change the SP rather than edit the SQL in the VBA IDE and re-distribute the front-end Excel file. I also have some where some major tables are viewable and users can select column(s) to summarize by, this allows the data to be shrunk down to an Excel level (it has a check if records exceed a million rows) and in those cases the query is just dynamically written in VBA and then passed in whole, with the downside of course that if it needs edited you have to re-distribute the workbook. I have quite a bit of experience in VBA, it's where I started before SQL, and I wrote very complex (10k-20k lines of script) automation there prior to learning SQL, so I'm a bit more comfortable in that world. That said, the VBA scripting to do those types of things is *really* easy, at least compared to a lot of VBA things, like heavy automation where you have to start calling windows APIs and nonsense.
I just kind of fell into my SQL job. Within a month I was doing correlated subqueries in temp tables and CTEs, and querying across multiple databases. If you're using it day-in, day-out, you learn very quickly. If not, the learning curve will be flatter
If you realize you don't know fully what you're doing, you're just like the rest of us. No one knows everything. And if they say they do, fire them
I'm a software engineer, and SQL knowledge is about 30% of the focus of my role on my team. It took me about 2 semesters as an intern (maybe 8 hours a week on SQL specifically) on this team to be considered productive (meaning that I could spend my time that the company is paying me for to yield results without the aid of someone else). At this point I'm much better, but with teaching (applicable workplace training, not academic training), you'll be good enough to start contributing after sometime less than 6 months on the job. You can easily show the company you're worth the hire with the understanding the above comment suggested.
It's the only in-database way I can think of outside using custom stored procedures to perform the insert instead. Otherwise would have to be application layer.
The issue is that you are not filtering for successful charges. Your window function looks correctly formatted, but it is returning the previous record from `x.created` regardless of the value in `x.status`. 
2-3 years of self taught system administration and VB/SQL development did it for me. I have no college degree, started in IT at Circuit City. Got a gig doing general IT for a small manufacturing company and the app dev bailed. I was already managing the SQL servers, but this got me into code. I integrated the existing app into an ERP using DTS, built some reports with SSRS. Applied for a sysad gig at a large healthcare org focusing on SQL and got it. Then after a couple years I convinced them they needed a DBA. A year after that I left and got Sr. DBA gig, doubling my pay.
What are you supposed to wear then? 
You’re looking for the JDBC, the sql c library or some other way of connecting a scripting language to your server.
I really hope this is test data, because that sure looks like PII to me. Google "recursive CTE" if you haven't already; it's probably the tool you'll want to do this with. From the snippets you shared, it looks like you can define your set of primary PMIs by finding all alt_keys that don't join to any alt_key_2. If that is a true statement, knowing that bit of logic might help you out.
So I should include a having statement that filters on successful charges by saying: " having x.status = 'succeeded' "? Should I use the max function anywhere? Then what do I do? Can you provide some code examples? I am still having trouble with the logic. I will try to simplify the end goal here: for each row find the previous date the customer successfully paid last. Each payment interval (in this case every 1 month) a customer gets a brand new invoice or bill. If that bill fails, the credit card is tried again for that same invoice until the invoice ID is successfully paid, closed, or forgiven. A customer can have multiple subscriptions or only 1 subscription. I only want to find for that same subscription ID group, and for each invoice ID group, find the previous invoice ID group's most recent successful charge date. I am using [https://prestodb.io/docs/current/](https://prestodb.io/docs/current/) I am subtracting 4 hours to convert into EST time. Originally the data is in UTC time. The data is 2 days behind. Maybe 8/08/2018 was the customer's last payment and they canceled the service afterwards. Or since this customer pays monthly, their next expected payment will be 9/8/2018 which has not happened yet. 
Okay so I am having this problem I had before and it is also in your example and I do not know what to do. Column 'x.subscription\_id' cannot be resolved x.subscription\_id actually should be invoices.subscription\_id but it returns an error. How do you join a value from the parent table that was originally joined in the parent table from a child table? Changing to invoices.subscription\_id also did not work.
0. Figured it out as I went. 
I have never met a single data professional who didn't just fall into SQL. If you have a chance to get exposed to it and it "clicks", my experience is that those people take on projects that use progressively more SQL. I started my career as a Data Analyst using a relational UI to extract raw datasets into SPSS. I started Pasting Syntax, which would dump the raw SQL strings. Once I realized I could use the SQL server to run joins I previously had to slowly sort &amp; merge in SPSS, I caught the SQL bug and haven't looked back since.
I fell into it. Started by making changes to some SSRS reports the company had paid a contractor to build and never used. 2 years later and the whole BI environment is a mix of SSRS (for operational reports) and PowerBI for ad-hoc analysis / exec dashboards. Everything is modelled in SQL; previously there was a lot being done manually through Excel by accountants.
Can't you just group by columns? Unless I'm not reading this properly your example seems to only need that. Maybe the use of CASE would also help
No problem, remember that dates have a specific format to be interpreted as dates not strings.
What kind of jobs can I get with SQL? I always hear knowing just SQL is never enough
what kinda job was it? Was it just pure sql??
I knew the basics, joins, selects, group by, order by, etc. When I started interning as a DBA I was given scripts to write and it used to take me a week or so to develop the scripts they needed. Now it takes me an hour to 2 days or so tops, depending on what is needed. And my team is willing to place me alone to handle majority of scripting. As a DBA, it's not too often you'll be creating many scripts as opposed to say a SQL developer or BI especially since many scripts can often be reused. That said, at least in my position, understanding is most important since I review scripts often.
After I wrote my first relational database I felt like I understood SQL fairly well. You kids today get to just "use" stuff... we had to invent it.
If this is confusing, then most likely you studied or were interested in anything database-theory related long time ago. From your 'explanation' below I can see that you don't know exactly what cardinality is, but you've come up with a 'practical' definition that, I've got to admit, is pretty practical. Splitting denormalized data sets to more normalized storage is probably a second nature to you by now, so why would you even care to understand the theory behind it?
I struggled with SQL, I knew C++ and JS before learning SQL and it got me all fucked up. 
Trolling game's pretty weak, but I'll bite: my definition of cardinality comes straight from Kimball's Data Warehouse Toolkit, and you're clearly incompetent at actually connecting with people so why answer OP at all? Stick to trolling, at least that comes natural.
I didn't get what you want to achieve, sorry. Why are you grouping by? Maybe "with as" clause can help you to first get all needed data, including LAG column. Similar case [here](https://www.reddit.com/r/SQL/comments/962w9t/oracle_find_only_previous_rows_in_a_window/e3zz3di/)
What you're doing exactly?
;with v as( SELECT TOP (5000) s1.\[object\_id\] FROM sys.all\_objects AS s1 CROSS JOIN sys.all\_objects AS s2 ORDER BY s1.\[object\_id\] ) insert (#values) select r = ROW\_NUMBER() OVER (ORDER BY \[object\_id\])
But how? Are you following some tutorial, are you using some tool or add-in? 
I am using MsSql and learning from youtube. I do not know about any tools till now.
I don't still know how you're importing data from Excel :) There are multiple ways, and you're not telling which way you're doing it. 
[Bulk insert](https://docs.microsoft.com/en-us/sql/t-sql/statements/bulk-insert-transact-sql?view=sql-server-2017)
I am importing the values manually using Insert into table values and then its value. I do not know any other way as i am a beginner. Please enlighten me in this
You're literally copy/pasting the values from Excel into your SQL Insert query? There are definitely easier methods. What version of SQL Server are you using? 
It's in the link, with more detail than we could possibly relate by guessing what you need.
I would double check that you older values have the correct net revenue and are not including net revenue after their dateID. That’s where the CURRENT part of the logic comes in. If you only looked at the most recent date you wouldn’t have seen it.
How can I handle a situation where one tenant i.e event owner can have more than one event? 
An event can have more than one ticket class..let's say early bird,VIP etc..and the ticket class can have more than one ticket and thanks I'll definitely heed your advice..waiting to hear your response..
Data Analyst / Process Analyst is a good entry-level to get your feet wet. If you're good at Excel and have peeled the skin off a Crystal Report or SSRS Report to muck around with its business logic, then that puts you ahead of 95% of the people at Generic Business(R).
Ah OK, I understand now. Anyway, sorry can't go into details now as I'm just going to the lake for some beers :) But, I'll try to explain maybe you can do the code by yourself. You can use "with as" instead of a temp table, to filter out things. So I would do something like: with xyz as ( select all failed invoice_ids, get the earliest (partition by charge_date ) ) select lag (1, invoice_id) from your_table where invoice_id in (select invoice_id from xyz); Hope you understand the concept, this way you should get desired results. As I said, I'm on the run now, so I just wrote idea/concept. Let me know if it helps, if not, I can have a look later to provide you the SQL. Cheers
It's good that you are including the tenant_id in every table. One thing i've started doing in my multi-tenant databases that is working out well is making my primary key include both the identity column and the tenant_id column. This way you don't have to create a fk back to the tenant table for every table and it prevents bad actors from inserting a foreign key value that points to a record from another tenant (this is prevented at the database level instead of having to prevent it at the code level).
So what can you recommend on this table schema of mine 
Thank you Import Aand export helped me. Thanks a lot
Thank you
Import export helped thank you
Thank you
Of course it would be best to clone the environment, create some user connections, and test the script of kicking the users in a non-prd environment first. Even set some win conditions around reconnects or whatever the need is for getting back to multi user. Running a couple Excel reports or Extract jobs to simulate traffic? Maybe do some sort of DMView look if you're concerned about running processes before you kick? 
So ad hominem is what you seem to excel in - keep disparaging opponents and they'll go away, right? So, assuming (and it's probably too ambitious of an assumption) that you actually are willing to listen and learn anything new or going against your pre-formed opinions, even if you have read that verbatim, Kimball himself is a practitioner more than theoretic. "Cardinality" is simply the size of the set. So, in practical terms, for a given multi-set number of records, cardinality of a certain column (or sub-set) could lead you to a ratio - which is, in practical terms, more important to indexing, for example. Also, I intend to provide the information at the level I would consider relevant to the other party's context, not to educate or eli5 to a random internets ignoranti. When I connect, it provides benefit to another party. If not, oh well - it's not my freaking job.
Thanks for the advice. Can you please check my version of your query below? Mine runs but it currently does not return anything for the last\_successful\_charge column. How is that possible?
You're setting @ReturnVariable to zero even if the rollback doesn't happen.
It returns the cartesian product. It combines all the rows from table A with all the rows from table B. 
very cool thank you
Is there any reason you can't ask the support team of Syteline that question? 
uninstalling doesn't seem to fix it, only restoring/rolling back snapshot of whole system and blocking that update.
Data or Business Analyst.
&gt;Your code works correctly for me (sp1) - but it needs some major work. 2016 has a built in string_split function - so why not use it? If not, why reinvent the wheel when it is easy to find well-written and tested splitter functions that work with earlier version?
I am so glad that I work for a medium follower for upgrades to newer versions. 
I've worked on Syteline and pulled that sort of information, what exactly are you looking for?
Asking people other than those who actually know the data model is just very, very odd, so I wanted to make sure you have asked. 
Thanks for your input!
In college, one of my class taught me about cubes and making them in SQL management studio and also making reports out of them. Is that what you mean?
 StartSale and EndSale would seem more appropriate for each ticket than the ticket class..BUT can't the Ticket class act as a reference to all those tickets so if i make a change on the TIcket class it still references all the tickets within it..I'll remove the EventID on the ticketClass, and what do you think of the one to many r/ship between TicketCLass and ticket.Thanks
Here's a sneak peek of /r/ship using the [top posts](https://np.reddit.com/r/ship/top/?sort=top&amp;t=year) of the year! \#1: [Laurie Carter inaugurated as 17th university president](http://www.theslateonline.com/article/2018/04/carter-inaugurated-as-17th-university-president) | [0 comments](https://np.reddit.com/r/ship/comments/8ecyoj/laurie_carter_inaugurated_as_17th_university/) \#2: [March for Humanity launches Black History Month events](http://www.theslateonline.com/article/2018/02/march-for-humanity-launches-black-history-month-events) | [0 comments](https://np.reddit.com/r/ship/comments/7xe2pe/march_for_humanity_launches_black_history_month/) \#3: [Weekly Discussion Thread - Ask questions, discuss teachers &amp; majors, find events, and more! Anything goes - week of January 24](https://np.reddit.com/r/ship/comments/7smuqk/weekly_discussion_thread_ask_questions_discuss/) ---- ^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| [^^Contact ^^me](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| [^^Info](https://np.reddit.com/r/sneakpeekbot/) ^^| [^^Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/8wfgsm/blacklist/)
The only way I'm aware of to do this requires a primary key. Could you consider adding in a primary key or creating a compound key of the two values instead? So have a + b = c and make c your pk. [Stack overflow explains better than i ever could](https://stackoverflow.com/questions/2717590/sqlite-insert-on-duplicate-key-update/2718352)
I thought about it a bit and thats what I ended up doing. I guess i didnt think i will be really needing it, but giving it some thought i was able to implement one. Thank you so much for your reply! 
You're wrong, cardinality is not "simply" the size of the set. That's just the grade schcool mathematical definition. In database management, it also refers to the ratio of unique values. It literally has an additional meaning in this field, used by both theorists and practitioners for decades. So not sure who is "learning something new" in this scenario. Everything else you said just reinforces your earlier posts. You're not helping OP precisely because his context was that of an ELI5 which you completely ignored. So again, why'd you bother?
I will keep that in mind, thank you so much 
OK, so I had a look and did it. I couldn't do it on the fly, so I created following [table](https://imgur.com/SRS0Du2). (This table should mimic rows 25-34) from your sheet. Of course you need to do the join to get this data. I did it in simple way. After this created this table and inserted data (I used postgres DB). You can see how it looks in DB [here](https://imgur.com/hmHSSpo) So to get previous successful charge_date I used following query: with xyz as ( select * from ( select *, dense_rank() over (partition by inv_id order by charge_date) as RKG -- rkg=1 will provide earliest charge_date for every invoice ,LAG (charge_date, 1) OVER ( PARTITION BY sub_id ORDER by charge_date) AS prev_charge -- provide previous charge_date from tbl_1 --tbl_1 is table which I created ) q where rkg=1 -- to select only earliest charge_date ) select sub_id, inv_id, status, charge_date, prev_charge from xyz where status='failed' -- now we want to filter data to show only failed ones ; I think this is it. If you still have trouble, I'm willing to help but in that case you need to provide me create scripts and insert statements so I have the exact data. Cheers 
What version are you running?
For this reason I like to use ansi join syntax. Every new table added to the query needs an explicit join keyword. Select * From t1 Join t2 On t1.column =t2.column Your statement above would look like the following in ansi: Select * From t1 Cross join t2 It tends to be more explicit and clear. I used Oracle syntax when i first started because it was less to type. 10 years later i appreciate a few more characters for clarity. 
Sqlite also calls it [upsert](https://www.sqlite.org/lang_UPSERT.html)
Put a ? in the query, and when it runs in Excel it’ll ask you to pick a range. There’s a checkbox in that pop up to make it permanent and from there it should refresh data each time A14 changes.
Try formatting the date variable into text.
Here are the results to that, I give that a go before. It seems that the stored procedure is executing before being passed the parameter. Picture 1: [https://imgur.com/dQ8Olps](https://imgur.com/dQ8Olps) \- Replacing the date with the ? Picture 2: [https://imgur.com/fOMVpCg](https://imgur.com/fOMVpCg) \- The result. Is it possible to reorder the sequence of executions perhaps? To request the parameter before running the stored procedure, because it seems that the stored procedure is executing first before asking for a parameter. 
I can offer no assistance to your posed question. I would like to offer some input on a different level though. Why are you doing this in excel? Excel is not a great reporting tool. IMO this should be in SSRS or another reporting tool. I'd have the conversation with leadership about using the right tool for the job and addressing tech debt rather than potentially making more. PS if they don't want to address the tech debt, it may be time to look for a company that does want to address it.
Within the VBA script. It appears you may be concatenating a text string and date value.
You should say "you're welcome" to yourself since you thanked yourself!
Thank you for all the help so far. Your query is SO CLOSE to what I am looking for. I will try my best to explain. This is my version of your query that works on my database: WITH xyz AS (SELECT * FROM (SELECT *, Dense_rank() OVER (partition BY invoices.id ORDER BY charges.created) AS RKG, -- rkg=1 will provide earliest charge_date for every invoice Lag(charges.created, 1) OVER (partition BY invoices.subscription_id ORDER BY charges.created) AS prev_charge -- provide previous charge_date FROM charges left join invoices on charges.invoice_id = invoices.id ) q WHERE rkg = 1 -- to select only earliest charge_date ) SELECT xyz.subscription_id, xyz.invoice_id, xyz.status, xyz.created, xyz.prev_charge FROM xyz WHERE xyz.status = 'failed' -- now we want to filter data to show only failed ones The result your query achieved for this customer subscription id is: sub_6sH2AoAgw91Rpw in_1Bdl15AXHoKknsExiWpFFNnj failed 2017-12-27 created 2017-11-27 prev_charge sub_6sH2AoAgw91Rpw in_1BozmtAXHoKknsExoBSRhFTo failed 2018-01-27 created 2018-01-26 prev_charge Even though this is true based on the current logic being used, the prev\_charge dates should be the same dates 11/27/2017 because the last time the customer actually had a successful payment in their billing history was on 11/27/2017. I want to be able to something like a lag function that loops through that customer's entire billing history until it finds the last known successful charge date. See what I mean? In the screenshots there are 2 failed invoices: in\_1Bdl15AXHoKknsExiWpFFNnj and in\_1BozmtAXHoKknsExoBSRhFTo? in\_1Bdl15AXHoKknsExiWpFFNnj failed on 12/27/2017 and never succeeded. Then after 30 days of failing, in\_1BozmtAXHoKknsExoBSRhFTo was created (since the other in\_1Bdl15AXHoKknsExiWpFFNnj did not get paid so another invoice on 1/27/2018 got created since the customer always has a new invoice on the 27th of every month) and then that in\_1BozmtAXHoKknsExoBSRhFTo finally succeeds on 2/8/2018. Our example shows 2 invoice IDs failing in a row but some customers have more than 2 invoices failing in a row so I want to know when the last time that customer successfully paid based on all previous history. 
I can definitely help when I get on my work PC on Tuesday, if you haven't got an answer by then I'll post up some code and examples. 
Is this for info151 at vic? 
You will need to install a database somewhere, could be locally on your machine. Then you will probably want something like SSMS to work with it. We would need more info about data size, version of SQL it is targeting, etc.
Tut tut! What do you have so far? as per the **sidebar**: "If you are a student or just looking for help on your code please do not just post your questions and expect the community to do all the work for you. We will gladly help where we can as long as you post the work you have already done or show that you have attempted to figure it out on your own"
do you have a picture of the entire table?
You can use rtrim(column1), rtrim(column2) etc to truncate spaces. As long as your report isn't white space delimited, I don't foresee an issue.
Not a visual cheat sheet per se, but a fantastic resource. https://www.w3schools.com/sql/
Try these https://zeroturnaround.com/rebellabs/sql-cheat-sheet/ https://www.codeproject.com/Articles/33052/Visual-Representation-of-SQL-Joins https://www.slideshare.net/mobile/videotuition/oracle-sql-for-beginners-ddl-dml-dcl-tcl-quick-learning Also remember ANSI is the standard, but each vendor has their own flavor built on top of ANSI. You can even create your own if you like the pain of doing so. 
Depends on how they exported it. If you have a file per DB table you should be able to open the file in a text editor. That said, those will likely be big files. Normally you'll need to mount the DB on an install of whatever software the DB was created on.
I haven't looked at the particular function carefully, but in general, unspecialized databases tend not to be fast at handling very large strings (or very large anythings). I don't know if this would be faster, but would it be "close enough" to count the number of spaces and add 1? (To count spaces, take the difference between the length of the string and the length of the string-with-all-spaces-replaced-with-nothing.)
This is a great one. Im trying to build a comprehensive one sheet with a minimal database example and an example for various statements 
&gt; take the difference between the length of the string and the length of the string-with-all-spaces-replaced-with-nothing. select length(replace(str, ' ', ' ')) - length(replace(str, ' ', '')) + 1 from ... where ... Generalization: Any idea how to replace consecutive spaces and/or control characters for one single space?
The company I work at has at least 2 entry-level positions that use SQL and many many other positions that use SQL. Many people started the jobs without knowing any SQL. I say go for it. 
Offhand, not without regular expressions (or something along those lines).
Pick a dialect, i.e., Oracle and stick with it. Once you have it for one dialect then it usually becomes easier to change it to another like SQL Server, MySQL or Postgres.
[Say no to venn diagrams for sql.](https://blog.jooq.org/2016/07/05/say-no-to-venn-diagrams-when-explaining-joins/)
Please tame regular backups so you dont have to use stuff like this
I have personally printed out and used the first link when I can never remember syntax for data modification. I would also recommend making you own. I have one that has the methods of doing window functions and PIVOT syntax. 
Here, have fun: with xyz as ( select * from ( select a.*, dense_rank() over (partition by sub_id order by charge_date desc) as RKG ,LAG (charge_date, 1) OVER ( PARTITION BY sub_id ORDER by charge_date) AS prev_charge from tbl_1 a where status='succeeded' ) q where rkg=1 ), abc as ( select * from ( select a.*, dense_rank() over (partition by sub_id order by charge_date desc) as RKG ,LAG (charge_date, 1) OVER ( PARTITION BY sub_id ORDER by charge_date) AS prev_charge from tbl_1 a where status='failed' ) q where rkg=1 ) select xyz.sub_id, case when xyz.charge_date &lt; abc.charge_date then xyz.charge_date when xyz.charge_date &gt; abc.charge_date then xyz.prev_charge end as the_date from xyz join abc on abc.sub_id=xyz.sub_id ; 
People with domain knowledge and SQL (and Excel) skills can become Data Analysts and there are many opportunities for this combination. I dont know where you are based but a quick search in [Monster.com](https://Monster.com) gives 27,000 data analyst openings in the US. I would recommend adding Excel skills and also look at a visualization tool like Tableau and you will find plenty of opportunities.
So it's not that the data itself has extra white spaces (it doesn't), it's that the column itself *allows* for up to 50 spaces, so all those spaces appear in the report. I can change the options for Query results to only display a maximum of 30 characters (as low as it will go), but even that is too much white space. I would like the column width to "auto-format" so to speak to fit the length of the returned results. Hope that makes sense.
While it's true that knowing just SQL isn't enough for a career, it's still a very valuable skill to have in your toolbox. If you can combine SQL with first rate Excel and a decent ability with Tableau, along with what you've done in Finance, you'll have a foundation to work with. 
Will you have cases where an ID will only have a SignIn entry? Should that be included also?
Maybe something like this? Select everything from the table, then remove any SignIn records where there are more than 1 record for an ID. select a.id, a.type from table a minus select id, 'SignIn' as "type" from ( select b.id, count(*) from table b group by b.id having count(*) &gt; 1 ); 
I didn't see to_char on those 
just do CASE select. Example: Select case when your_col=1 then 1 WHEN your_col=0 then 0 when your_cos i null then 2 end from your_table; 
I'm with you...his comment is way over-complicated. Not only for the sake of a beginner trying to learn, but for anyone. It takes something that's supposed to be simple, easy-to-follow and easy-to-implement and makes it overly verbose and uses lots of (meaningless) jargon.
Personally, I much prefer join sub-selects over sub-selects in the actual select. I find it a lot easier to follow, manage, and show intent for future readers of my code. CTEs are a good answer as well - but personally I find them less clean. Practically - the optimizer will decide the best way to get you the data anyway (whether or not it's realistic to rely on that is arguable) so in theory CTEs or sub-queries should perform the same leaving it up to taste. You also have xx.Audit_id = ff.audit_id and xx.audit_id &gt; ff.audit_id in the same where...I'm guessing you won't get any results from that. It also looks like you're looking for the "find_next_5" where you'll only get 1 result if you use "MIN(Audit_id)." I've made a couple changes to your select to hopefully point you in the right direction. Select Ff.Audit_id ,ff.audit_time ,ff.audit_event ,nextAudit.Audit_id as find_next_5 From audit ff outer apply( select top(5) audit_id from audit xx where xx.Audit_event = 5 and xx.Audit_id &gt; ff.Audit_id order by xx.Audit_id asc ) nextAudit Where ff.audit_event = 10 Order by ff.audit_id desc I went ahead and also used an outer apply instead of a join...it's effectively a left join that allows you to reference tables from outside it's own scope (an inner join would be a cross apply) and use a WHERE clause instead of an ON clause to join. I find them much more readable for future-code-readers. But just a personal preference, again. This should give you (up to) 5 rows per ff.Audit_id...That may not be what you want. You could get a comma-separated list of the next 5 rows using a [SUBSTRING](https://docs.microsoft.com/en-us/sql/t-sql/functions/substring-transact-sql?view=sql-server-2017) (I can help you with that if you need, too, I just already worry this is too much in one post).
Ah interesting , thank you very much. I will test this later today. Yes basically there could be N number of event which have happened after the current event has triggered which I’m trying to catch. Likewise there are audit events in the past which could also affect the order’s info. So it’s it’s running up and down an orders history to catch specific things. 
Maybe it'll help, maybe not, but check out [Lead](https://docs.microsoft.com/en-us/sql/t-sql/functions/lead-transact-sql?view=sql-server-2017) and [Lag] (https://docs.microsoft.com/en-us/sql/t-sql/functions/lag-transact-sql?view=sql-server-2017) functions. Without knowing much more, I'm guessing that the temp table suggestion provided by /u/KING5TON is your best bet - temp tables have a whole lot of benefits, especially with efficiency. But you'll still need to clean up some of your WHEREs and function uses (using = and &gt; in the same where, using min when you want more than 1 result). Good luck! Feel free to keep asking questions.
It seems that your comment contains 1 or more links that are hard to tap for mobile users. I will extend those so they're easier for our sausage fingers to click! [Here is link number 1](https://docs.microsoft.com/en-us/sql/t-sql/functions/lag-transact-sql?view=sql-server-2017) - Previous text "Lag" ---- ^Please ^PM ^/u/eganwall ^with ^issues ^or ^feedback! ^| ^[Delete](https://reddit.com/message/compose/?to=FatFingerHelperBot&amp;subject=delete&amp;message=delete%20ID_HERE) 
Will do!
Great stuff! Thanks
Check the following web site [http://www.studybyyourself.com/seminar/sql/course/?lang=eng](http://www.studybyyourself.com/seminar/sql/course/?lang=eng). The course, along with examples, is quite easy to follow. You can submit exercises as well. Everything for free.
Sorry, I forgot to mention in my initial post that I'm writing this in Oracle so DATEDIFF does not work. I've updated my post to include the fact that it's being written in Oracle.
Google is your friend: https://stackoverflow.com/questions/28406397/datediff-function-in-oracle
This is correct. Subtracting the dates will do. Just use an NVL or COALESCE on both of them to account for NULL dates.
I saw that page when I searched, but they're looking at the difference between two specific dates (as is every other post I've found on this subject). In my post I'm asking how to do it when I don't have specific dates. The page you linked has this posted as the answer: SELECT TO_DATE('2000-01-02', 'YYYY-MM-DD') - TO_DATE('2000-01-01', 'YYYY-MM-DD') AS DateDiff FROM dual That doesn't match up with what I'm asking... I'm not working with specific dates (2000-01-02, 2000-01-01, etc etc) I'm working with data points I'm pulling in that have a date value. Now I might be able to use this method of just subtracting the dates, but there has to be at least something else I'd need to do because I've tried writing that same code, but replacing the '2000-01-02' and the '2000-01-01' with EFFECTIVE\_DATE and TERMINATION\_DATE and it doesn't work. I guess it's possible my syntax was incorrect though. Any idea how that should look?
If EFFECTIVE_DATE and TERMINATION_DATE are already DATE columns, you don't need the TO_DATE function. That's for converting strings of characters to dates. Most likely all you need is TERMINATION_DATE - EFFECTIVE_DATE
Thanks for you response. Certainly on the right track. I'll provide more details about the data. So this is the current code that I have written, which provides all the data: SELECT DISTINCT STU.[StudentID] ,STU.[StudentNameExternal] ,STU.[StudentYearLevel] ,ABS.[AbsenceEventTypeCode] FROM [dbo].[vStudents] STU JOIN [dbo].[AbsenceEvents] ABS on STU.StudentID = ABS.ID Which then provides data like this: 
 SELECT DISTINCT STU.[StudentID] ,STU.[StudentNameExternal] ,STU.[StudentYearLevel] ,ABS.[AbsenceEventTypeCode] FROM [dbo].[vStudents] STU JOIN [dbo].[AbsenceEvents] ABS on STU.StudentID = ABS.ID and ABS.AbsenceEventTypeCode = 'AllDayAbsence' WHERE STU.StudentId NOT IN (select StudentId from [dbo].AbsenceEvents where AbsenceEventTypeCode &lt;&gt; 'AllDayAbsence');
That query doesn't give me any data! :(
Use `COALESCE()` https://www.w3schools.com/sql/func_mysql_coalesce.asp $q = "UPDATE users SET first_name = COALESCE(first_name,'".$fn."'), last_name = COALESCE(last_name,'".$ln."') , email = COALESCE(email,'".$e."') , WHERE user_id = '".$uid."' "; not sure if that's right due to the quoting...
Thank you 
It worked with a little bit of tweaking, just trying to figure out how to do refresh the session variables now
Which RDBMS? PIVOT is what you’re looking for.
Sweet! It works! Eliminating the TO\_DATE function is what did it. That's a good learning for something to look for in the future. Thanks for all your help!
That did it! Thanks for your help, I appreciate it! 
&gt; Also remember ANSI is the standard, but each vendor has their own flavor built on top of ANSI. You can even create your own if you like the pain of doing so. [Surely nothing could go wrong](https://xkcd.com/927/)
Hahahaha!
No problem. To be fair I just clarified a bit. It went my answer. Just please do your due diligence and wrap them in NVL or COALESCE functions. Otherwise the NULL values will bite you.
Fantastic. Glad it worked.
There is nothing wrong with just doing the department like '1%' you don't have to do the not like or not exists below. As 3 is already not like 1% and will not be included. So, Mike in 1A is returned and Mike in 3c and 3d isn't returned but Andres will be returned twice.
Microsoft Virtual Academy. It’s free, they give you free software like SQL server and an Azure subscription and tons of courses for free. 
 SELECT Name FROM data WHERE department like ‘1%’ EXCEPT SELECT Name FROM data WHERE department not like ‘1%’ 
&gt; Mike in 1A is returned and Mike in 3c and 3d isn't returned this is not what was asked for mike should not be returned at all because he is in both 1 and 3 
Can you give an example of what the desired result would look like? Also, what kind of db are you using?
using Microsoft SQL. Desired result would just be adding a new column with the desired averages. 
But what do you mean by the desired averages? Like, would it have 7 rows and average of .425 for 1 (because it falls between minid and maxid for rows 1 &amp; 2), .55 for 2, 3, and 4 (because they fall between minid and maxid for all rows), etc?
While your first query will have the correct results, IMO your method is not sound. For instance, what if there is bad data where its not just L or R? When answering data questions in general, it is often a good idea to have your query more closely match the request. As an interviewer, I would think you understood what the expected result was, but your design choice is odd. &amp;#x200B; It is good practice to use aliases as well, even for practice problems. When you start working in bigger databases with multiple table joins, this becomes obvious and second nature. When I see SQL written without table aliases, I question how much SQL a person has written and how complex it might have had to be. I wouldn't discount someone entirely of course, but it would raise questions. &amp;#x200B; Even if for an intro position, it is still pertinent, as it would be a differentiator. The second question annoys me, as there is 1 swipe per day in the dataset. I don't work with SQLite, but I would assume there is date math that can be done without any additional division if you want the number of days between 2 dates.
I'll try this and report back thanks! (:
The timestamp not being a format that works with sqlite time and date functions is annoying.
Hey there thanks for your comments! I suppose I'll just have to spend more time on this subreddit and communities like it to identify best practices for queries such as the first question. For the second question, that's my mistake. My invented database doesn't convey the nuances of the stroke data. Basically, there are millions of users and millions of swipes per day. I'd even venture to say that you couldn't use the timestamp as a primary key since its possible to stroke more than once per second. I suppose I should edit the database to reflect these constraints. Again thanks for your remarks -- I will think on them!
This is a great response and exactly the kind I was hoping for. Thank you!
 If you're looking for some very hands-on "learn-by-doing" practice problems, that teach basic to advanced SQL with well-designed, real-world practice problems, check out [SQLPracticeProblems.com](https://sqlpracticeproblems.com/#utm_source=r1&amp;utm_medium=r1&amp;utm_campaign=r1). I developed it after teaching a SQL course where the material I had to to teach from was poorly structured and academic. Afterwards, the students emailed me, saying they needed practice problems, so I developed the course! Contact me (email in the FAQ) if you have questions. Also, redditors get 30% off the Professional package (discount code reddit30). 
1) I would expect there to be a \`FOREIGN KEY\` from strokes table to a user table. I would then do it the following way. By doing this, there is no need to \`DISTINCT\` (which can possibly be expensive), and it's also quite clear what you are filtering by. \`\`\` SELECT u.user\_id FROM users u WHERE \-- to check that the user has R stroke EXISTS ( SELECT NULL FROM strokes s WHERE s.user\_id = u.user\_id AND s.type ='R' ) AND \-- also check that the user has L stroke EXISTS ( SELECT NULL FROM strokes s WHERE s.user\_id = u.user\_id AND s.type ='L' ) \`\`\` &amp;#x200B; 2) Try to reduce the number of \`SUBQUERY\` when you are \`SELECT\`ing, it's not very efficient, key thought with SQL is to think \`SET BASE\` a) I started off with the following which would give me \`\`\` SELECT date(s.date\_time), COUNT(\*) FROM strokes s WHERE s.type = 'R' GROUP BY date(s.date\_time) \`\`\` b) I then needed, for that row (s), another stroke (s1) such that 1) Has type 'R' 2) (s) stroke\_id matches (s1) user\_id 3) (s1) stroke\_id matches (s) user\_id 4) s and s1 happens both on the same day \`\`\` Note that I use \`LEFT JOIN\`, because I didn't want to remove the rows that didn't match (I needed them for the total) &amp;#x200B; SELECT date(s.date\_time), COUNT(\*), COUNT(s1.user\_id), COUNT(s1.user\_id) / CAST(COUNT(\*) AS REAL) FROM strokes s LEFT JOIN strokes s1 ON s1.user\_id = s.stroke\_id AND s1.type = 'R' AND s1.stroke\_id = s.user\_id AND date(s.date\_time) = date(s1.date\_time) WHERE s.type = 'R' GROUP BY date(s.date\_time) \`\`\` &amp;#x200B; I altered your dataset a little bit, and the second was based on my interpretation of your query &amp;#x200B; [http://sqlfiddle.com/#!5/9696d/1](http://sqlfiddle.com/#!5/9696d/1)
it's been long day so hopefully i'm not missing something, but on your "count(*) as matches" subquery: &gt;AND A.stroke_id = B.stroke_id AND B.stroke_id = A.stroke_id ? Does that even return any results? I tried running just that subselect in the SQL Fiddle, returned nothing: SELECT * FROM strokes AS A INNER JOIN strokes AS B ON A.user_id = B.stroke_id WHERE A.type = 'R' AND B.type = 'R' AND A.stroke_id = B.stroke_id AND B.stroke_id = A.stroke_id Is it something more like this? SELECT * FROM strokes AS A INNER JOIN strokes AS B ON A.user_id = B.stroke_id AND A.stroke_id = B.user_id WHERE A.type = 'R' AND B.type = 'R'
My best guess is that you are being inconsistent with the parameters you pass to DATEPART. You're passing "year" on one side and "YY" on the other. 2001 does not equal 01. 
Why does the first datepart say year and the second is yy? 
Good call. Fixed that but still getting years that are not 2018.
Mostly because I have no idea what I am doing. Fixed that but still getting years that are not 2018.
It looks like your WHERE is condition1 OR (condition2) AND condition3. Try WHERE (condition1 OR condition2) AND condition3.
I'm replying from mobile, so it's a little hard to tell.... but it looks like your "or" is allowing the other values. Check your parentheses. 
My thought process was that the second r_stroke more directly causes the match, so I wanted to only count the second r_stroke. But then I realized the math worked out so that it was the same if I only counted one of the two r_strokes. But I see your point. Both r_strokes are required for the match, so both r_strokes should contribute to the match.
Great, glad to been of assistance. Watch how you join your conditions, if you are not sure of the order they will be applied put them in (), kind of like Algebra.
Thanks for your critiques! It will take me a while to wrap my mind around them but they seem like good things to know for query optimization. Cheers!
Thanks a bunch! I'll see how we go~
I'm a fan of subqueries, so here is how I would approach. Is there no surrogate key for a person? How are you able to tell if this is one Mike or three? SELECT t1.* FROM table1 t1 WHERE t1.Department like '1%' and t1.name not in ( SELECT name FROM table1 t1 WHERE t1.Department like '3%' )
Hey thanks for your comment! I have heard great things about Insight. Amidst growing skepticism towards data science boot camps, Insight retains a good reputation for job placement. Yet.. I've heard that Insight is fairly selective -- to the point where if you're admitted, you probably don't need the bootcamp in the first place. So, were I to apply and be admitted, I'd be concerned about losing money going to a camp like that. Its a bit of a catch-22, no?
Sez you're anon, so nothing 
I've not heard the thought that if you can get in, you don't need it. Having been there (for data engineering), while there were certainly a few of the 17 in my cohort that I guess didn't *need* it, the vast majority of us benefited greatly. And yes, in addition to technical stuff and a main project, there is interview practice, resume review, and actual presentations by and to hiring companies. It is true that it's selective, but even if you don't get it, the entrance coding exercise would be a good test of skill. 
either log in as root or go through [root password recovery](https://tecadmin.net/steps-to-reset-mariadb-root-password-in-linux/)
Table1 Guard ID = table 2 Guard_No, but table2 guard_no has 3 different ID's associated with it and no one to differentiate between any of them. SELECT t2.id, guard_id, name, debt_status, team, skill, min(s_date) FROM table1 left join table2 t2 on guard_id = t2.guard_no left join table3 t3 on t2.id = t3.id group by guard_id, name ;
This is admittedly better english than the usual blogspam we get here but this article is an awful lot of words for something rather simple and already very well defined and explained elsewhere. Also from another article on the same site: &gt;TRUNCATE command is faster than DELETE operation as it does not use transaction logs False. You can prove this easily by truncating inside a transaction and rolling back. From the article on improving query performance: &gt;Remove any redundant mathematics What were you smoking when you wrote that? Also how can you have an article on query performance without even mentioning query plans? Please take your blogspam elsewhere. 
&gt; False. You can prove this easily by truncating inside a transaction and rolling back. It depends on the database, in Oracle a TRUNCATE cannot be rolled back.
Fair enough, I am T-SQL server myself. The article does not make a distinction and has adverts all over for more T-SQL orientated stuff. In addition: https://www.janbasktraining.com/online-sql-server-training "World's leading open-source database" Just an inch or so above a Microsoft SQL server logo. That website is full of false information and lack of clarity. As I said... blogspam, I guess not everyone can write detailed and informative and accurate stuff like Brent Ozar.
From SQL no from PL/SQL yes several ways depending on which version you're running.
I would also like to recommend another effective and secure SQL database repair software named as Kernel for SQL Recovery. It can help to repair or inaccessible database files and recover all stored items. For more information, visit: [https://www.nucleustechnologies.com/sql-recovery.html](https://www.nucleustechnologies.com/sql-recovery.html)
Am I right in saying they also need to be named identically and be of the same data type? 
Oh cool! Thanks for the tip.
That is a GREAT way of thinking about it! Yeah, I have a long way to go here...I am still learning basic arithmetic in that analogy. Thanks again! 
So I am guessing they are not the same? Where abouts would i go to learn about SSRS report?
Same data types yes. They don't have to be named the same but the output will display names from the top query
 SELECT * FROM table1 Join table1 b on a.columnA = b.columnA WHERE a.columnA = 'your stuff'
What do you want to do? Visualization experience is becoming more requested. Tableau/PowerBI
SELECT * FROM table1 a Join table1 b on a.columnA = b.columnA That should do it if you just want the rows where the columns are equal, and not equal to some specific value. It’s called a “self join.”
Would aliasing these queries and selecting from the resulting temp table be a better choice maybe?
Here's a link to Quora discussing many aspects of SSRS and SSAS (which would use cubes more extensively): https://www.quora.com/Which-is-the-best-book-to-learn-SSRS
I'd have to see what you mean.
Your criteria is a little unusual - you realize that this will return completely different results depending on which month of the year you're in (if you're in December, you'll get almost 12 months of results, in January you'll get just this month and last)? &amp;#x200B; If you're looking for some very hands-on "learn-by-doing" practice problems, that teach basic to advanced SQL with well-designed, real-world practice problems, similar to what you're trying to solve, check out [SQLPracticeProblems.com](https://sqlpracticeproblems.com/#utm_source=r1&amp;utm_medium=r1&amp;utm_campaign=r1). I developed it after teaching a SQL course where the material I had to to teach from was poorly structured and academic. Afterwards, the students emailed me, saying they needed practice problems, so I developed the course! Contact me (email in the FAQ) if you have questions. Also, redditors get 30% off the Professional package (discount code reddit30). 
You could maybe do it with a CLR function.
I've started to dabble in PowerBI a bit. I find it very interesting and I think I could definitely benefit from learning more about it. What I’ve enjoyed the most is building queries or procedures that are used for special projects. A person or group is starting a new project, or there is a new element of the business that we need to monitor more closely. Such assignments often require spending hours navigating through various screens in our ERP system, locating bits of data and then copying/pasting that data into a spreadsheet. I can work with that person or group and find out what data they are specifically looking for and build a tool they can use to do their job. That being said, simply mining the data and putting it in a spreadsheet is relatively simple. I like to take it further, that is if the group I’m working with doesn’t prefer the status quo. If I understand the goal, and I have the data, and I know the process in which that data will be analyzed, then why wouldn’t I just include logic to perform the analysis also? Instead of providing data that still needs to be reviewed by a person, filtered, modified, etc, why wouldn’t we build something that does all of the analysis for you and simply generates something indicating what actions to take based on the parameters they provided. Furthermore, we could integrate a stored procedure that takes those proposed actions and performs them automatically. Evaluating processes and methods people are using and finding ways to make them faster, more efficient, or coming up with a solution that just eliminates them altogether is where I’ve been the most successful. At the end of the day I’m a guy who enjoys solving problems.
A few points... 1. if you've been working for 10 years in SQL, you don't need a degree. Work experience speaks volumes more. 2. I'm a software developer trying hard to teach myself SQL and I suck. My company in NYC needs more SQL people who can bridge between the database team (because you understand SQL) and the business team, using tools like Tableau (but have little to no understanding of relational databases but want to create pretty charts.) To point #2 above, there is so much you can do with SQL experience beyond working only with databases. Visualization is HUGE today, mainly because we have loads of data and no way to make that data appear meaningful to the business.
Thank you for the reply. I totally agree about visualization tools. I see so many people open a spreadsheet and their eyes glaze over. Displaying data using a visual medium makes them more willing to engage. It's definitely something I'm looking into. 
Rig up a couple of rows of data from both queries in a spreadsheet, keeping data types the same in any one column. This should let you map out your columns. The dates from both could go in the same column maybe. If you have more columns than you do for one or both queries then select NULL for that column in that query. The column headings come from alias on the first query. What I have done before is put a text column on the end of the queries, 'annotation' or similar. 'cash query' on one and 'bank query' on other for example, that would label the row with what that query was trying to achieve. 
Thanks for your guidance, you two. You got me where I needed to get.
It is down for me too
I was wondering the same.
Thanks for your comment. Looks like you included users that both stroked left on each other. Why's that? Also, I agree that filtering s1.date&gt;s2.date doesn't make sense.
Wow that's a great write up! Good level of detail. 
I think you're misunderstanding. There are two separate questions you're meant to address using the "strokes" table. The first question asks for a query that returns all the users who have stroked both right and left. The second question asks for a query that returns the percentage of right strokes that lead to a match. /u/theseyeahthese was referring to the second question. More specifically a subquery within the second query that counts the number of right strokes producing a match. In contrast, it looks like you are trying to write a query for the first question. 
I think at that point it might be preference, but if you're struggling and more comfortable with the temp table, I'd go that route 
Sounds like you might want to do a GROUP BY. SELECT *, COUNT(*) FROM Table GROUP BY ColumnA HAVING COUNT(*) &gt; 1
Working fine for me. 
you want SELECT COUNT(DISTINCT POSITION) FROM jobhistory
Bit irrelevant, but it is normal procedure to delete from your database, sounds sketchy?
1. No, this is simply a lock. 2. Yes, if a deadlock is detected, sql engines usually will kill one ("the victim") process. 3. It's just that - Fortune 100 companies employ a lot of people and a lot of them not of Tier A.
1. This was blocking, not a deadlock. The open transaction was blocking other sessions from accessing the rows or table(s) they needed. 2. SQL Server does detect deadlocks. This wasn't a deadlock. Just garden-variety blocking 3. Most monitoring tools will send out an alert when a query is being blocked beyond a set threshold; SentryOne (my drug of choice) by default will send out alerts when a query has been blocked for at least one minute. I'm sure all of the other monitoring suites can be configured to do the same. You can even do it [with Agent alone](https://dba.stackexchange.com/a/205590/35474). FWIW, it shouldn't have taken 50 employees 30 minutes to have a DBA run basic diagnostics including `DBCC OPENTRAN` to find the offending transaction **if basic monitoring and diagnostic tools are in place**. Running [`sp_whoisactive`](http://whoisactive.com) would have shown the blocking session in about 3 seconds.
I’ve seen company situations where there is a need to delete records like this due to a certain dependency issue in legacy versions of DBs, but they also had a delete-trans audit table + SSRS report. That being said, still not best practice and kind of sketchy. 
Agreed with all of this. And SP Blitzlock is fun to use too. https://www.brentozar.com/archive/2017/12/introducing-sp_blitzlock-troubleshooting-sql-server-deadlocks/amp/
I'm a big fan of LINQpad. But it's mainly meant to work with Microsoft SQL Server. You can always use Access as well and connect to the database using linked tables.
Unless it's a foreign key referencing another table, I just go with the first case, and specify the exact table.column in queries (Usually with a short alias of the table name).
My opinion: * Object names should be meaningful. "ID" is too generic. If you use a meaningful name, you can use that same name on the other end of a foreign key constraint. Queries become a lot easier to follow and relationships between tables are more readily apparent. * Object names should not use [reserved words](https://docs.microsoft.com/en-us/sql/t-sql/language-elements/reserved-keywords-transact-sql?view=sql-server-2017) or non-alphanumeric characters (no spaces, no punctuation, no special characters, etc.). * Object names should not use [Hungarian Notation](https://en.wikipedia.org/wiki/Hungarian_notation). Tables are strongly typed already and that information is readily available from the system catalog. If developers want to alias things, that's their prerogative. But aliases should be meaningful as well, else they just make debugging more difficult.
DBeaver is pretty good, particularly if you are just doing queries and don't need admin functions. It's also nice because you can use the same IDE across a multitude of database flavours. 
I like SQL Workbench/J
Bit of both. Our practice is that the primary keys will be more specific than "id". They end in "id". So customer_id, supplier_id, but the other columns should be succinct and easy to follow. If your table names are descriptive enough then doing something like customer.name works great. Another convention we follow: the primary keys for a table is always the table name then "_id". So you can find what the id belongs to when you find it as a foreign key. So you see account_id in the customer table and know that you can find the table "account" to see what information it has.
Data Engineer here, My organization has a team of dedicated data engineers who have rotating shifts to handle production DB issues, but this issue is a bit ridiculous. For stuff like this we don't even bother putting in a ticket. It's like.."Hey, Bob. Somebody has a bad query that our automated SQL alerts picked up on. Wanna manually kill it?" Bob - "Naw, looks like our 'sp_long_running_queries' SQL job already killed it and send an email to the user account associated with that DB role". The idea that it was so costly to fix may indicate a lack of sufficient automation, poor quality assurance, and an organizational structure where DBAs are isolated and inaccessible. Also, sounds like your company needs to visit the idea of test driven database development. tSQLt is a great unit testing framework for SQL Server. Postgres and Oracle have some until testing stuff too floating around out there. You test your application code, right? So test your database code too!! 
It really depends on how it is built and I've seen it both ways. Consistency is key. id when it's the PK, tablename_id when a fk
Yes, this does work. I worded this very poorly, I realize that. Like say, I'm new. And it is difficult to draw tables well with the editor here. Thank you for your help and patience.
Hmm maybe check the execution plan? It's likely different with five threads than with one.
You could use the convert function. [http://www-db.deis.unibo.it/courses/TW/DOCS/w3schools/sql/func\_convert.asp.html](http://www-db.deis.unibo.it/courses/TW/DOCS/w3schools/sql/func_convert.asp.html)
I see where you were going with this. The issue is due to the interval getting added to the tbl1.date directly. This query adds the interval days first (without first checking if it's a business day or not), and then looks for the next greatest non-business day. E.g. the query: 1. looks at row where [tbl1.date](https://tbl1.date) = 2018-08-17 2. adds the interval to that date: 2018-08-17 + 3 = 2018-08-20 3. Then gets the next date greater than 2018-08-20 where dt.isbusinessday = 1 4. returns 2018-08-21 &amp;#x200B; That's why it works on the dates where the ([tbl1.date](https://tbl1.date) \+ tbl1.interval) falls on a weekend. You'll probably want to go with row numbers. Trying to keep it as close to your original as possible, here's one to try: SELECT tbl1.date ,tbl1.id ,tbl1.interval ,( select top (1) [date] from ( select dt.[date], ROW_NUMBER() OVER(ORDER BY dt.[date]) row_num from dimension_table as dt where dt.[date] &gt; tbl1.[date] and dt.isBusinessDay=1 ) as derived WHERE row_num = tbl1.interval ) as computedbusinessday from mytable as tbl1 &amp;#x200B;
There's nothing to explain. It's just syntax. See for yourself [here](https://www.w3schools.com/sql/sql_distinct.asp)
I would use the second set you posted. For the specific case you posted. If they had a more generic firstName lastName style naming convention then I would have disagreed. C.firstName and S.firstName are different things after all and that tableAlias.columnName convention is how sql works when you have more then one table anyway, at least that is how I roll.
Thanks for taking the time to comment. Ideally, I want to just use the calendar dimension table to determine if business day instead of writing out a bunch of logic to determine if it's a holiday or not. If that makes sense.
SQLOps, free, made by Microsoft and runs on a Mac.
Thanks for this. I think you're likely right. Looking back on your reasoning it makes complete sense. I'll give this a go. Thanks again!
Absolutely am, thank you 
I'm pretty much a novice myself but this article may help you understand more clearly. https://blog.jooq.org/2016/12/09/a-beginners-guide-to-the-true-order-of-sql-operations/ 
Here's a sample query joining a customer table with a supplier table: select customer.id, customer.name, bridge.customer_id, bridge.supplier_id, supplier.id. supplier.name from customer left join bridge on customer.id = bridge.customer_id left join supplier on bridge.supplier_id = supplier.id You can see that the use of dot notation clarifies which 'id' (or address, city, state, zip, other commonly used field name) we are talking about. IMHO the time you need to add clarification to commonly used field names is in situation like a bridge table. YMMV.
&gt;for some reason I can not get my dates to load as type DATE Fix that before anything else so you can store your dates properly. Everything else is just a hack that will eventually come back to bite you.
You can download SQL Server Management Studio and connect it to a SQL Server. It's the standard MSSQL tool
Thanks alinroc. I am using Navicat 12 and it really doesn't like DATE. I've tried insert statements in numerous formats and it wont insert them properly. I tried messing with the DEFAULT value (and not) and it doesn't work. Now I need to do a DATEDIFF() and I'm stuck...
SSMS is the standard tool for SQL Server. And only SQL Server. And I know people that prefer to use TOAD for SQL Server. 
For the course, I would just use whatever your instructor is using. When you're done with that course, I would encourage you to change over to whatever DB you're using at work. Not Access, I mean the DB Access is pulling it's data from. There are free versions of MS SQL Server and Oracle. Most of the others you might be using are free anyway. If you're going to learn SQL, you may as well learn the dialect you'll be using at work.
&gt; I am using Navicat 12 and it really doesn't like DATE Then get a client that works properly. &gt; I've tried insert statements in numerous formats and it wont insert them properly Are you certain that you're using the correct format for your RDBMS? What error are you getting? "really doesn't like" isn't helpful.
Thanks for your comment.
Disclaimer: I'm not as familiar with Oracle as I am with other RDBMS. That said, I'd look at the [EXPLAIN PLAN](https://docs.oracle.com/cd/B19306_01/server.102/b14211/ex_plan.htm#g42231). If I had to summon a guess, the single threaded execution is buffering all its results for an operation. Where the parallel one is re-using the same table space for all five threads. 
I tried to revert to an old version of Firefox in order to be able to use the same platform as the instructor but figured I’d check to see if there was something else I should be using as a long term solution. Thanks. 
Thanks for the clarification :) 
I think it may help a bit to elaborate that DISTINCT is acting as an operator here on it's own... I initially only ever saw/used it directly after SELECT, but it can be placed in an aggregate function like this. The alternate option would be a subquery like: SELECT COUNT(t.p1) FROM (SELECT DISTINCT position AS p1 FROM jobhistory) AS t
Just started a job recently as a Business Analyst, and need to up my SQL skills. My boss got me "Sams Teach Yourself SQL in 10 Minutes" and it has really helped me gain a better understanding. Each chapter is sorted out in 10 minute increments, yet goes in depth enough to provide you with more than basic knowledge. Highly recommended. Aside from that, I've used SQL Zoo and W3 to help me out, then just play around with test data.
Thank you!
Can you not just use what you put down? AND (e.previous_value LIKE '%@xyz.com%' AND e.value NOT LIKE '%xyz.com%')
COUNT is not a subquery. It's just an aggregation. Subqueries are in the FROM clause. Read this: [SQL Order Of Execution](https://www.periscopedata.com/blog/sql-query-order-of-operations)
I am trying: SELECT DATEDIFF(YEAR, decision\_d, last\_pymnt\_d ) as datedd FROM loan\_data WHERE id = 54734 And getting this error: Incorrect parameter count in the call to native function 'DATEDIFF'
I just wanted to get back to you, this solved my problem. Thank you very much!
Sweet! Thanks for letting me know. Glad you could interpret my insomniac-1am-explainations... I barely understood what I wrote when I re-read it this morning haha
Try group by column 'order'
&gt; I am using Navicat 12 and it really doesn't like DATE. I've tried insert statements in numerous formats and it wont insert them properly. show us your CREATE TABLE statement then show us the inserts that didn't work
Look up the function in your sql implementation's manual to see how it's supposed to be called.
Specifically for SqLite I'll recommend SQLites own browser https://sqlitebrowser.org/ But the main thing is get an IDE that lets you both run queries and visually inspect the tables and schemas of your database. That is the best way to "connect the dots" of your entities and relationships, their attributes, keys, etc.
Can you update your OP to add what the expected results are with the given dataset? The problem you present isn't difficult, but I'm having a hard time understanding your business logic.
Your explanation is hard to follow. Clarifying a few things would help you, me, and others understand what you want for your results. What is the `wonum` column and why are you grouping by it? Can you give an example of what the earliest instances of the original and final status column would be? Can you make a prototype table that shows us the answer in the format you want? 
Id add order to your group by and put a distinct after select. Technically you shouldnt have to but id try it.. 
For one you have where fromstatus = 'done' and that column is a date. So your or condition is only getting the data from the second argument.
Sorry guys for the confusion. I'm new to SQL so bear with me. I've updated the table above. What I want to achieve is a table where it groups it by the location and shows a count of "done" and "missed" in the "Original" and "Finalstatus" columns. Since the location may have the same order number show up multiple times, I only need it to be counted it once. This is where I"m stuck as my curent query will show this, Instead of &amp;#x200B; Hopefully this clarifies my issue? 
I see you made some updates - I'm still not really sure what you're trying to do, but I've made some changes which should get you in the right direction. 1) As//u/MrDarcy87 pointed out, your where clause was using the wrong column for 'status' vs 'fromstatus' 2) Your group by is using a column which makes no sense given the information you've provided - updated to use the correct column 3) If you're looking to only get a maximum value of 1 in either result column, SUM isn't the right way as that will add up each instance as you're currently seeing. MAX will give you 0 or 1 grouped by the order id. Try this and see if your results are more in line. If not, you're going to need to provide more sample data, and your EXPECTED results. SELECT order ,MAX(CASE WHEN status='done' THEN 1 ELSE 0 END ) as original ,MAX(CASE WHEN secondstatus='Missed' then 1 else 0 END) as finalstatus FROM ordertable WHERE department='Commercial' and customer='ABS 123' and ( status='done' or secondstatus='missed' ) GROUP BY order
Just FYI, you can't select order without having it in the group by if it's not aggregated(SUM, AVG, etc..), which you don't want. You need to change the select to location and not order. 
This is quite funny to be honest. &gt;DBA forgot to include COMMIT in a transaction This is why writing ad-hoc sql and deploying your own code is not a good idea on a prod server. Infact its a terrible idea for this exact reason. I work in a team of 2 dbas and we specifically get each other to check our code and deploy it to avoid this exact scenario. I usually run any data change in a transaction that is rolled back at the end, check the affected row counts then re-run as a commit. This is just to ensure that I've not been sent a update missing a where clause to deploy. You can use code to detect if there are any open transactions, you can also automatically commit or rollback if there are. &gt;it took over 50 salaried employees and 30 minutes to resolve. This is the funny bit. It would take 1 competent employee a couple of minutes to find and fix this. Oh instance xyz is going slow... ok run sp_who2... see a few blocks... follow the chains, spid 55 is blocked by 122, ok 122 is blocked by 87. Ok 87 is not blocked by anything so its spid 87 thats causing it all... dbcc inputbuffer(87) - ok so they ran that code... idiot. Run kill 87. Boom issue resolved. Literally about 2 minutes including logging onto the live system. It really is that simple, I did it a couple of weeks back on a prod system and this week on a dev system. This is all down to poor IT processes and lack of expertise. If I were you I would downgrade that 'dbas' access to read only and get some template scripts created - IE a try catch commit rollback pattern or something. &gt;They were tying to delete a record per normal procedure If you need to delete records 'per normal procedure' and are doing a manual delete statement then you've got a piss poor process. Either add a delete button to a user interface somewhere or wrap the delete in a stored procudure and give whichever monkey does the deletes execute on that specific stored proc. I'd seriously reconsider your current DBA and think about finding someone who actually knows how to deal with these situations cause it is ridiculous that it took 50 headless chickens a half hour to sort a blocking issue. Its like 50 mechanics taking half an hour to work out the handbrake is still on which is why the car wont drive. Hilarious image of them all running around but its seriously highlighted that your business needs to sort out its IT team and get some new heads in that actually know what they are doing. This page lists 6 techniques for finding out which process is blocking stuff https://www.mssqltips.com/sqlservertip/2429/how-to-identify-blocking-in-sql-server/ This is hilarious but in a sad way, like the time I saw a hobo try to put stash his bottle of booze in a tree. He was so drunk that he did not notice the twigs he was reaching for (which would have never supported the weight of the bottle anyway) were a couple of metres above his reach. I don't think anyone would have wanted to steal his half drunk bottle of cheapo cider anyway.
If an order's secondstatus has been updated at a later date from missed to ok, should that still count as a missed? C457897 went from missed on 1-1 to ok on 1-2. How should this be counted?
Given the current logic, here's the most recent query we're at. I believe your expected results are incorrect, as there are not 2 orders from Chicago which have a secondstatus of missed. http://sqlfiddle.com/#!9/28cee9/1 SELECT Location ,sum(done) donetot ,sum(missed) missedtot from ( SELECT Location ,`Order` ,MAX(CASE WHEN status='done' THEN 1 ELSE 0 END ) as done ,MAX(CASE WHEN secondstatus='Missed' then 1 else 0 END) as missed FROM ordertable WHERE department='Commercial' and customer='ABS123' GROUP BY `Order` ) a1 group by Location
Ignore this one, go to my post below :)
Seconded. If you aren't using a MS implementation (in which case use SQL server management studio), SQL Workbench is the way to go. 
Very well researched!
If you're looking for some very hands-on "learn-by-doing" practice problems, that teach basic to advanced SQL with well-designed, real-world practice problems, check out [SQLPracticeProblems.com](https://sqlpracticeproblems.com/#utm_source=r1&amp;utm_medium=r1&amp;utm_campaign=r1). There's both a MySQL edition and a SQL Server edition (if you purchase the Professional Package). I developed it after teaching a SQL course where the material I had to to teach from was poorly structured and academic. Afterwards, the students emailed me, saying they needed practice problems, so I developed the course! Contact me (email in the FAQ) if you have questions. Also, redditors get 30% off the Professional package (discount code reddit30). 
Thank you again for the help. I had a fun time trying to get this code to run and figure out what it was doing. This is close yet again but the WHEN statement logic is off. This is a nice learning experience for both of us. In my version of your query the results were: XYZ query only = success rank #1 = 8/8/18 created, previous = 7/8/2018 ABC query only = failed rank #1 = 2/8/18 created, previous = 2/7/2018 Entire query output (filter only on this customer ID): cus\_6sH2VDQpyQnN35 in\_1CwqKzAXHoKknsExyMUZSnPG 07/08/2018 previous [https://imgur.com/a/PMIsz2n](https://imgur.com/a/PMIsz2n) See how in this query in\_1BozmtAXHoKknsExoBSRhFTo and in\_1C3NajAXHoKknsExbdyfSll8 goes from 11/27/2017 to 2/8/2018? Is there a way I could have the query say do this: When the failed charges for that customer are in between the previous successful charge and next successful charge for that customer, then return the previous successful charge. My version of your query: WITH xyz AS (SELECT * FROM (SELECT a.*, DENSE_RANK() OVER (PARTITION BY invoices.subscription_id ORDER BY a.created DESC) AS RKG, LAG(a.created, 1) OVER (PARTITION BY invoices.subscription_id ORDER BY a.created) AS prev_charge FROM charges a left join invoices on a.invoice_id = invoices.id WHERE a.status = 'succeeded') q WHERE rkg = 1), abc AS (SELECT * FROM (SELECT a.*, DENSE_RANK() OVER (PARTITION BY invoices.subscription_id ORDER BY a.created DESC) AS RKG, LAG(a.created, 1) OVER (PARTITION BY invoices.subscription_id ORDER BY a.created) AS prev_charge FROM charges a left join invoices on a.invoice_id = invoices.id WHERE a.status = 'failed') q WHERE rkg = 1) SELECT xyz.customer_id, xyz.invoice_id, CASE WHEN xyz.created &lt; abc.created THEN xyz.created WHEN xyz.created &gt; abc.created THEN xyz.prev_charge END AS the_date FROM xyz left JOIN abc ON abc.customer_id = xyz.customer_id I was unable to join on the last line the subscription\_id because the subscription\_id did not exist in either ABC or XYZ tables. The customer ID and invoice IDs exist though so I could join on those. 
Never heard of it before. What don't you like about toad or sqlDev?
Two things: the query may execute differently when using parallel threads, so you're not comparing apples to apples. The other thing is TEMP is used when the temporary results do not fit in memory. If you are using 5 threads, you will have 5x process memory in use, which means less need to spill over to TEMP on disk.
Right, creating the index after the data is inserted is substantially more efficient (vs indexing while data is inserting). Depending on how the job is set up, you could likely add the index drop/create to the scripts? Side note: Oracle is my forte. I'm just getting into this stuff with MSSQL Agent and powershell... still a noob there.
I'm no professional, but my answer would be: select CITY, ZIP, COUNT(*) as COUNT from tblPatient group by CITY, ZIP having COUNT(*)&gt;3 order by 1, 2; 
Quick silly question: How many keysources do you have? If less than 50, you could parallelize it by partitioning your table and running parallel insert queries with tablock.
It just means to order by the 1st attribute listed and then the 2nd attribute. It could have said "order by CITY, ZIP"....saves keystrokes I guess.
As far as I know this isn't possible. I believe Microsoft considered it as a feature request many moons ago but nothing ever came of it. I am sure if it is possible there are other, much smarter, people on here who will help out :) 
Ah okay, got it. Thank you for your help! 
Your network team would likely be able to track what traffic is coming from what server with timestamps. If you have multiple instances it could get tricky depending on their port configuration.
I don't believe that information is stored historically, but you can get that information for current activity, so you can capture it over a period of time to determine this, using a variety of methods - SQL trace, extended events, capturing snapshots of DMVs.
Once you get rid of the indices as per /u/var_missmal 's recommendation, you can actually use minimal logging if the db is in simple or bulk-logged recovery mode. (If the DB is in full recovery mode, consider toggling it just for this proc and toggling back, or better yet, set up a separate staging database that IS simple recovery.) Then, do an INSERT with the TABLOCKX hint to get an exclusive lock. Exclusive lock + empty table + heap + simple/bulk logged recovery = minimal logging nirvana. Then you can put on all your fancy indices after.
Thank you very much for your response. I was thinking about using the min statement. This looks way more sophisticated. As I cannot test the code above, what do you think is wrong? 
Also, I would suggest to create stored procedure and ideally job, that would run it in for you. :)
SQL Executes the query in the following order: 1. FROM 1. WHERE 1. GROUP BY 1. HAVING 1. SELECT 1. ORDER BY 1. LIMIT (TOP) So the ROW_NUMBER that happens in the SELECT is executed AFTER the where clause preventing it from being used there. Usually work around that by using sub-queries to do the row numbering then in the outer query filter to where I = 1 SELECT * FROM ( SELECT client_id, trans_id, trans_time, ROW_NUMBER() OVER (ORDER BY trans_time) AS I FROM data ) AS X WHERE X.I = 1
It's because ROW_NUMBER is evaluated after the filters in the WHERE clause have been applied. You can't filter by ROW_NUMBER if it hasn't been calculated yet.
Thank you very much. I have not thought about the order of queries. That has given me deeper insights in SQL. Thanks!
Makes sense, thank you for your insights. This is a good day for my SQL knowledge!
And now I have an answer of *why* it works this way. Thanks, amigo.
I'd self join for each code. 
What database are you using?
MySQL
 SELECT a.ValueString [Customer Name] , b.ValueString [Date of Birth] , c.ValueString [Address 1] , d.ValueString [Address 2] , e.ValueString [Postal Code] FROM @YourTable a JOIN @YourTable b ON a.CustomerNo = b.CustomerNo AND b.ValueCode = 2 JOIN @YourTable c ON a.CustomerNo = c.CustomerNo AND c.ValueCode = 3 JOIN @YourTable d ON a.CustomerNo = d.CustomerNo AND d.ValueCode = 4 JOIN @YourTable e ON a.CustomerNo = e.CustomerNo AND e.ValueCode = 5 WHERE a.ValueCode = 1 &amp;#x200B;
Well, crap. I never use MySQL. I'll say this much though: If at all possible, copy the .csv file to the actual machine the MySQL db is running on, then remote into that machine and run the import process on that physical machine. Doing everything over a network connection can be a huge bottleneck.
How about loading the data in to a test environment to alleviate strain on the main db. Then use fully qualified names in a bulk import that commits every 1000 or 10000 rows via an over night job. I don't personally use MySQL but when I need to do this via Oracle I load in to a test system then use a dblink to bulk import overnight. Good luck
split the file up into multiple parts that will run successfully.
To follow on, **sp\_who2** is an undocumented superior version of sp\_who. It includes additional, exceedingly useful, information like connecting program name, last batch run time, and CPU/Disk usage statistics.. On top of generally better legibility in the output. 
This is probably one of the worst ways to go about doing this. You'd be significantly better off using Extended Events, or Profiler if they aren't available. Even if you wanted to go the networking route, you'd use a tool like Wireshark and perform a capture on the server itself. Trying to pull active connections from internal networking equipment with this goal in mind is just silly. 
I was able to get this to work SELECT CustomerName [Customer Name] , DOB [Date of Birth] , Add1 [Address 1] , Add2 [Address 2] , PostCode [Post Code] FROM ( SELECT t.CustomerNo Customer , MAX( t.ValueString ) ValueString , MAX( CASE t.ValueCode WHEN 1 THEN t.ValueString END ) CustomerName , MAX( CASE t.ValueCode WHEN 2 THEN t.ValueString END ) DOB , MAX( CASE t.ValueCode WHEN 3 THEN t.ValueString END ) Add1 , MAX( CASE t.ValueCode WHEN 4 THEN t.ValueString END ) Add2 , MAX( CASE t.ValueCode WHEN 5 THEN t.ValueString END ) PostCode FROM #test t GROUP BY t.CustomerNo ) AS SourceTable PIVOT ( AVG(Customer) FOR ValueString IN ( [Customer Name], [Date of Birth], [Address 1], [Address 2], [Post Code] )) AS pvt ORDER BY pvt.[Customer Name]; &amp;#x200B;
So a CTE is only good for a single transaction, i'm wondering if your transaction is being used by your update, so it's not working when you select.
I'm probably misunderstanding the question, but here are my 5 cents: * if you want to set a value for all but a first row in Account, then you should PARTITION BY Acc, then you order by a column that specifies which row is first. * then you select all rows, where RowNumber &gt; 1
Actually this does work, the reason it didn't work before is because i was using '2/2017' when it was supposed to be '2 /2017' This helps. But, is there a way i could call that without having to use that entire thing in my where clause? 
I think that has to be it. If I'm partitioned by chg rather rather than by acct than if two accounts carry the same chg value, which is limited but possible, then the value would be set to 0 even though it's a new acct. I'll try when I get back from lunch, but it would make sense that it would cause some issues but not affect the whole table. Thanks for pointing that out. One of those dumb errors you read over the query like 10 times and don't spot.
I forgot to mention there is also the 3rd party '[sp_whoisactive](http://whoisactive.com/)' version which has even more bells and whistles.
Streaming from Profiler is not awesome. It will slow SQL server down quite noticeably. You should avoid this if you can and use it sparingly if you must. 
Because that value you want to search by does not exist, you can't. You could however select into a temp table with your expression as a field and select out of there if you want to be able to just filter by your derived value. 
I don't know how technical you are but I would use Python + Pandas to read the file and push it to the data base in chunks.
Ya, I haven't found a viable need to use it since EE came out; especially with the performance hit. Only mentioned it since OP said the server was older.
If you can get this authorized through standard change management processes; this is definitely the way to go. Although some companies are confusingly adverse to using these amazing community toolboxes. Usually something to the tune of being scared of losing their proprietary blend of 11 herbs and spices trade secrets.
Depending on your database, there's probably a more efficient way of doing this, especially with the number of rows you have. But if those are your only columns and they're small varchars and int, I think this should help out. It does have a little bit of a staging area in a CTE to get each combination points, then the main select gets things nicely put together. WITH Lev1 AS ( SELECT DISTINCT t1.Team1 ,(SELECT SUM(t2.Team1_Points) FROM teams t2 WHERE t2.Team1 = t1.Team2) AS Points FROM teams t1 ) SELECT Team1 ,SUM(Points) AS Points FROM Lev1 GROUP BY Team1; My output on your test data looks like this. Team1 | Points -----|------ A | 6 B | 3 
Set something going overnight if your workplace isn't 24hours 
Thanks this helped. I was also able to verify the null issue using ' WHERE id NOT IN (SELECT b.id FROM b\_customers b WHERE b.id IS NOT NULL) 
Maybe take a SQL course on Coursera, but it may be too simple for u. I guess u can do a 7 day trial to see if it fits u. IBM has na intro to database and SQL there, that I recently took and learned alot -- but I am a student learning these stuff not a professional. 
Hey, ninabooboo, just a quick heads-up: **alot** is actually spelled **a lot**. You can remember it by **it is one lot, 'a lot'**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
[This book](SQL in 10 Minutes, Sams Teach Yourself (4th Edition) https://www.amazon.com/dp/0672336073/ref=cm_sw_r_cp_apa_g5fGBbTDM71YQ) was passed around my first jobs office to us interns as we scrambled to learn all we could to get promoted to a junior position... it kick started my career. 
That’s fantastic. Congratulations! And it sounds like fun data to work with. You are in a great position to learn analytics quickly; already knowing the business side gives you a huge leg up; having contextual data to dive into and learn all the tools/coding is the best way in my opinion. And your existing knowledge should help with the imposter feeling as well. Totally normal and common feeling by the way… I still feel it and I’m a senior data analyst. Just give yourself some grace and remember that there’s likely no one else that could step into that role right now that could do better than you with the business and data knowledge you already have. Everything can be learned. StackOverflow will be your best friend. And even years from now you’ll get requests that are out of your wheelhouse, so don’t feel afraid to think up and offer and alternative solution instead. Reports can always be expanded later. One thing I wish I’d learned more formally earlier on is normalization. The concepts are somewhat intuitive, but knowing what a great foundation for a database really looks like is critical. Above all else, have fun with it. It’s amazing when you get to do what you love.
don't remove them, then you have to start over when you re-add them just turn them off temporarily instead. much less total work
you need to turn the indexes and triggers off first. otherwise it's going to sort every single row.
ok I just came off of a 25 hour long deployment, so my brain is a little wonkey, forgive me if this is rambly. The first thing I would recommend is do not use a CTE. You can use the same logic but select into a table variable. Then you can select * from that table variable to see how the data populated so that you can evaluate your query better. Also in most cases a table variable will be more performant then a CTE. DECLARE @YourTempTableName TABLE ( rno INT, Chg Money, Acct VARCHAR(50) ) INSERT INTO @YourTempTableName (rno, Chg, Acct) Select ROW_NUMBER() Over(PARTITION By Chg Order By Acct) As rno, Chg, Acct From #Temp201805101557 
I import to a staging table first. The staging table has no indexes, keys, nothing. Once its in, it's easier and quicker to manage and move. 
Awesome, thank you. I never turn down book recommendations (sorta to my detriment).
Thanks a bunch for your thoughtful response. I appreciate the vote of confidence and heed your insight for sure.
Look at OLAP or windowed analytical functions. Depending on your platform, they could be called different things. Anything you can do with these functions can also be done with self joins and/or recursion, so find what you’re comfortable with. 
Depends. Are you looking for administration skills or the nuances of the SQL engine?
Break down the problems into manageable results and then start joining them to get a final result. For me I often think: To get this I need this other thing. But to get that I need some other thing. But once I have those two things, then I can do this thing and join it with those things, then get my results from that. For me that involves a lot of CTEs or in-line views. But you definitely have to break it down into manageable blocks so you can validate and build from it. 
Select into will out perform inserts. If you have to do inserts drop the indexes and create them after loading the data. Also use the hint with(tablock). https://drillchina.wordpress.com/2012/12/18/use-tablock-to-boost-your-insert-into-select-performance/ 
Good bot. 
A bit ruthless, but I agree with it all
Unless im really dumb, which could easily be the case, is that the only difference you have is thag you've given your sum an ALIAS. Alias's are just temporary names for tables or columns in tables, they only exist for the duration of the query and are only for readability purposes. There shouldn't ever be a case where this makes a difference 
I'm going to venture a guess that Query 1 is a typo and the SUM() function is not supposed to be there for your example. Otherwise this query would not run. Assuming that, it completely depends on your dataset. Is `c.name` the primary key/unique? If not and there are multiple 'David' records, Query 1 will return price \* quantity for each 'David' row individually, where Query 2 will aggregate them all into one row. 
thats just the number of days since that high or low (compared to today)
also, isn't this just returning the max date and min date? I want the date that a USD high occurred and usd low occurred
Oh my bad! What's your primary key column on this table? Just an Id field? 
yep
Thanks for this. Have you stumbled across any good texts on normalization, or did wiki suffice?
Yeah that's the goal. It was because I was partitioned on chg rather than acct. It was really throwing me because chg was a static value for each transaction for each account so there was always a clear winner.
Edited the original response. Not the prettiest, but should work
Very much appreciated, I shall try it when I get home 
YouTube, WiseOwl SQL
Sorry, it's been years so I can't remember the resource that made it click for me. Depending on how you leard just one resource might not suffice, but I skimmed the wiki page and it's pretty solid (although dry :) ). After you have a general understanding of the concepts, watching something like [this YouTube video](https://www.youtube.com/watch?v=UrYLYV7WSHM) for an applicable example might be helpful if you're visual like me.
Only in 10 mins??
You’re filtering a single aggregate with 1 condition. The use of HAVING to filter grouped rows will work the same as using WHERE to filter individual the rows in every instance I can think of, given the query as a whole. 
I have a non SQL suggestion. If you have a lot of repeated Excel tasks you might want to look at automating them with Python. This will obviously be value add for your company (save time and money) as well as an introduction to object oriented programming and a new skill set for you 
Hmm. It works here with your sample data set: [https://www.db-fiddle.com/f/bDw9uRKiFnJP23nmqbQ48x/1](https://www.db-fiddle.com/f/bDw9uRKiFnJP23nmqbQ48x/1) Might be something with the table structure or data? 
Thank you, yes. Ever since starting with this company I knew early on that a lot of my day-to-day tasks can (will) be automated at some point; I’ve no coding background though. But of course I keep seeing Python as the go-to language for both beginning coding and anything to do with general data. Would you agree that a need-based approach to learning Python (or any language) would make sense / do better than a sort of linearly trekked ground-up learning? Like, get some basic concepts and fundamentals down then tackle a real world scenario I encounter daily or weekly by identifying a problem or opportunity for automation / streamlining then learn the relevant stuff for that purpose? Thanks for your input. 
strange indeed. Only thought is to put an index on the `slug` column. Might speed things up since there are so many joins on that column in this query. Other than that, impossible to tell without seeing the database. Good luck! 
One of many spelling/punctuation issues in the comment. Go figure, "ninabooboo" identifies as "transfeminine" (whatever the fuck that means). I was curious if they were ESL or just illiterate, turns out I have one more reason to hate the trans movement of 2018.
If you're a book learner, I read Database Design For Mere Mortals and got a good foundation. 
I'll just have you know that you are a male. You were born with a penis, XY chromosomes, you aren't a female. We don't live in fantasy land where you decide what you are. We accept reality is it is, and reality is - you have a dick, you shag women, women give birth, the cycle goes on. BUt gENDer iS DIFFereNT Than sEx!?!?!? No, it isn't. At some point we draw a line of demarcation between what constitutes as your choice and what is a fundamental, unequivocal fact of existence. Gender and sex are synonymous because up until the last century, your gender has been based on what you were born as, as an animal, as created by evolution. It's great that you identify as a female, but beyond it being fucking revolting, it defies many basic truths of reality. You cannot just "identify" yourself as Asian when you are Caucasian, it's just an undeniable biological fact. I'm no bigot, I'm open to new ideas. I'm about as liberal as she goes when it comes to economics/change in general, but I can't deal with this perversion of sex that becomes increasingly prevalent in a disturbingly large audience. Societal conformity is perfectly normal, and we don't just deny it by making up genders. I suffer from Aspergers Syndrome (AQ score 45, parents score me 43-45 as well), so I can readily attest to the endeavor to be something you fundamentally aren't. I have been scolded time and time again for behaviours I see as normal. But reality has it, we **aren't what we are, rather we are what society wants us to be**. I would find minimal success in the world if I repudiated every standard the world laid out for me. Its imperative that one complies, and should they not, they become destitute of attributes necessary to converse with others, relate to others, negotiate with others, and function in society. It's just a little rant about my gripes on the fairy tale trans communities that retrogress much of the world's progress into the stone ages. 
 The first query needsa group by as well in most databases, but other than that it should be identical
It would be great to see execution plan of your query. How much time does select statement take?
I hope you get better. 
both right now i am at end of learning RHCSA and have plan to learn SQL, administration databases
These will have different performance plans and scale will be an issue if the db gets large, the number of queries is high, or you are dealing with other performance issues. Order of operations within the core SQL engines would say the best performance would come from the where vs the having. 
Can I assume that the combination of KeySource, TransactionNumber, OrderNumber and KeyDateFinancial makes a unique record? You might want to put either a constraint on the table or some kind of calculation at load time that builds a concatenation of those fields. Put a clustered index on each table if they don't already have one. OfficeID, FinDateID, etc - just make them IDENTITY INT fields. Then put a nonclustered index on the concatenated field. Now you're just joining on one field. Inserts go faster because you don't have an index on each of the 4 fields, reads will probably be faster as well. Give that a shot. Test first of course. And check the before and after query plan don't just assume faster run time means it's better.
Try looking into the RANK command
So the second me will work in all SQL database, but not the first one?
What is helping me with the transition is Excel ODBC functionality and studying the query output in Excel. The ability to tweak the query in Data &gt; connections and hit Refresh makes it easy to filter and analyze what SQL just did exactly. 
In most cases I find it hard to learn anything as theory before I actually need to use it. But one thing that helped me with motivation to learn more advanced SQL was just thinking about the fact that many times in the past, I've told myself that I'm "too busy" to learn that stuff right now, so I'll just solve this problem in application code rather than SQL. My tip is that if you find yourself having a similar thought... realise that this is actually the perfect opportunity to learn something new, even if it turns out that feature wasn't the right tool for this job. And yes maybe it will take a little longer to complete this task in what feels like "the hard way" right now... but you're going to save time overall in the long term for a few reasons: * you'll have better SQL skills to solve other problems * you're not going to have to maintain messy application code for something that would have been much better to do with some SQL views etc - I have many regrets related to this kind of thing * your more advanced database usage is more reusable for other features without writing more code Not really what you were asking I know, more a personal motivational theory that I think helped me. :) Also, while I'm generally not into buying books (or using ebooks etc) to learn programming, I think learning more advanced SQL can actually be something where can make sense. Or even just reading through the official manual from start to finish. By reading something from start to finish without actually have a specific goal in mind... Along the way you'll hear about some features you didn't even know existed, and realise that some of them would have been good to know about for certain tasks you've done in the past. The "how" isn't the important part here (you'll probably need to look it up again when it comes time to use them)... the more important part is finding out that they exist to begin with, and knowing "***when*** to use it"... I feel that this is the main factor in many of us never getting around to learning more advanced SQL... we don't know when to use a certain tool if we don't know that it exists. 
Mode Analytics has a great tutorial, I find it made me more self-reliant than Code Academy's tutorial. Also, follow this guy's [guide ](https://www.essentialsql.com/how-to-write-queries-write-the-query-step-3/)when planning out queries, it will help organize your thoughts and take a look at all the pieces needed for the result. Write out the columns needed, what tables they're in, and if anything needs to be done to that (like SUM, COUNT, etc). Then find out the relationship if you need to join tables.
I second that book. It helped me when I moved from the business side to the data analysis side.
 If you're looking for some very hands-on "learn-by-doing" practice problems, that teach basic to advanced SQL with well-designed, real-world practice problems, similar to what you're trying to solve, check out [SQLPracticeProblems.com](https://sqlpracticeproblems.com/#utm_source=r1&amp;utm_medium=r1&amp;utm_campaign=r1). I developed it after teaching a SQL course where the material I had to to teach from was poorly structured and academic. Afterwards, the students emailed me, saying they needed practice problems, so I developed the course! Contact me (email in the FAQ) if you have questions. Also, redditors get 30% off the Professional package (discount code reddit30). 
No, add a group by to the first one. The second one will execute slower. For your homework it won't matter, but no self respecting DBA would allow #2 into production. &amp;#x200B; HAVING is the last thing the db processes. That means it's processing all of the other records for all steps, but excluding them right before returning the data set. When you use the WHERE it is only processing records that meet that requirement and never sums up the total for recods you don't care about. For your example performance doesn't matter, but if you get in the habit of writing non performant code, you will have issues on the job.
Thanks. So basically if I add a group by on the first one, it will work on all SQL db? Why when I tested it it isn’t necessary? Is it just the sum operation works differently on different dB? Thanks for the tip on Having as well. 
Fantastic! First the non-technical stuff. Some of what you do will be hard and frustrating. That is good. Most people learn better while struggling to solve a problem. Learn some basic SQL. Then, immediately apply what you know to a problem. Get some help. It all comes down to the following: learn, do, learn then learn, do, learn, teach Learn something, apply it in the real world, then learn some more. Once you get to advanced-beginner status, teach it to someone else. This way you will continually increase your accumulated knowledge. The teaching step helps you cement the concepts in your own mind. On the technical side, you probably want to focus on a single stack of tools. Don't learn every new tool and language. Focus on a small tool set and get really good with them. You can always add new tools later on. You can do almost anything with SQL (required), R/Python, Excel and PowerBI (or any visualization platform). These tools are easy to learn, have tons of resources and are fairly ubiquitous. 
How well does this work? https://www.db-fiddle.com/f/bDw9uRKiFnJP23nmqbQ48x/1 WITH cte AS ( SELECT slug , usd , mydate , row_number() over (PARTITION BY slug order by USD) low_rank , row_number() over (PARTITION BY slug order by usd DESC) high_rank FROM historical ) SELECT h.slug , h.usd as high_usd , h.mydate as high_date , l.usd as low_usd , l.mydate as low_date FROM cte h LEFT JOIN cte l ON l.slug = h.slug AND l.low_rank = 1 WHERE h.high_rank = 1 ;
If you add the GROUP BY to the first it'll compile on SQL Server. &amp;#x200B; If you change "JOIN" to "INNER JOIN", you'd get to the ANSI standard (will work on any modern DB), but I could be wrong. I only work in SQL Server.
Having might not even get picked up by any index either right? Because first an index looks for joins, and, where, select ect. If an index is found best to work and not consider name = David it might do a scan of the index vs grabbing and index that knows name and only maybe 200 rows bs unknown 9billion. Is that kind of what your saying...? I'm so shit when thinking about all this. 
Anytime you have an aggregate function in your select, you should have a group by clause. HAVING is generally used for including a filter in your query for another aggregate.
Sounds like others answer your question but since your probably newish to sql I thought I'd tell you about the format command. It's so much easier to convert dates than they way you did in the example. It could just be something like format(mydate, Yy/Mm/Dd). Now I'm on my phone so autocorrect wants to make life hell.. But check it out on Google. It's awesome... 
I tried this. You didn't read my description, did you? https://stackoverflow.com/questions/49194719/authentication-plugin-caching-sha2-password-cannot-be-loaded 
Sweet. Thank you for the resource. 
Thank you very much for this. 
I’m learning right now as well, SQLbolt is a good place to start
Start with a SELECT statement
The side-bar has a bunch of linked resources: https://www.reddit.com/r/SQL/wiki/index Did you need something more than those? There's also /r/learnSQL/, which has lots of great resources.
 If you are going to use Microsoft’s SQL Server (it integrates nicely with Excel and Power BI) I highly recommend the book ‘T-SQL Fundamentals’ by Itzik Ben-Gan. The dialect of SQL used by SQL Server is Transact-SQL, T-SQL for short. Like you I come from a finance background and at the beginning of last year I needed to learn SQL. We use both Oracle SQL Developer and MS SQL Server at work. I bought the above book from Amazon, the author is a Microsoft certified Most Valuable Professional (MVP) and a trainer, so he knows how to teach well. Since you are extracting data from the database you’ll need to focus on learning SQL developer skills. (A database administrator on the other hand looks after the setting up and putting data into the database). Initially, focus on learning about joins. Once you start putting real code in to production, you’ll probably find you’ll end making regular tweaks to improve the code, I recommend using Git to manage this process. Also a good tip my SQL mentor taught me is to keep the complex SQL code out of Excel and instead create a table (or view/materialised view) with the results of your query and in Excel use the simple query of Select * From the above said table That way you can can use Git with your SQL code to track changes over time. Good luck, it took me about 6 months of daily working with SQL before I started to get my head around the SQL way of solving a problem. (SQL works with sets, kinda like working with Excel tables, CTRL+T). 
Sounds silly, but this is how I learned. I've never had any proper and or online training. 4 years later and it's all development, CTEs, window functions and the like. Creating ETL plans via script and SSIS. Next is SSRS. You'll reach a point of understanding the fundamentals, the rest is good decision making and Google. Also, an understanding of the data you're working with. 
Some good resources are given here already. However, if you're looking for some very hands-on "learn-by-doing" practice problems, that teach basic to advanced SQL with well-designed, real-world practice problems, check out [SQLPracticeProblems.com](https://sqlpracticeproblems.com/#utm_source=r1&amp;utm_medium=r1&amp;utm_campaign=r1). I developed it after teaching a SQL course where the material I had to to teach from was poorly structured and academic. Afterwards, the students emailed me, saying they needed practice problems, so I developed the course! Contact me (email in the FAQ) if you have questions. Also, redditors get 30% off the Professional package (discount code reddit30). &amp;#x200B;
 hmm I'm getting an error: #1064 - You have an error in your SQL syntax; check the manual that corresponds to your MariaDB server version for the right syntax to use near 'cte AS ( 
In the Select statement add (MinID + MaxID)/2 AS Avg
You’ll need an analytical function (also called a window) Average(value) OVER (portion bi minID, maxID) as Avg
I did. 
At some point you will need to try some examples and to write your own queries so you will have to choose SQL realisation. T-sql, MySQL, Qracle, Psql...
Simply SQL if you are looking for offline resources. After that, reference manuals.
If this works, this is way better than doing a massive join function...
This sound very logical and oftentimes I try to do the same. But my DBA usually complains about me creating queries that are not optimized or are made of too many joins. I seem to struggle with aggregation and queries where the same table is to be joined. 
What DBMS are you using?
Thank you for posting the YouTube resource. 
I appreciate your post, but honestly it sounds a bit beyond my understanding at the moment...
This is amazing. Love the explanations and the hints. Thanks so much!
I really appreciate the help, bookmarked. 
You are welcome. Also if you want to see even more complex queries I have read this book: https://masteringpostgresql.com. It describes some more complex queries with good explanations. Unfortunately it is paid, does not have any exercises and it uses Postgres specific stuff where available.
Saving to check later 
&gt; This is what overhwlems me. I don't have adequate tools to determine which one to use in a specific sceanrio. Yeah sounds about right. A book or ebooks or something should help with that hopefully. Let us know if you find anything interactive tools that are good for this. Also maybe trying some really specific Google searches for what you're trying to do, sometimes you'll get results where people are trying to do exactly the same thing, and there'll be a few different answers on the different relevant features that could be used. &gt; I see myself writing a lot of nested queries and then joning them. I break stuff up into lots of small views, even if they're only used for one query. It makes everything so much easier to build and debug in isolation. And quite often after making them, it turns out that they can be used for other queries too. In most cases you should be able to replace nested queries with views. Right now I'm working on some code that looks at my maildir email storage to determine which folders I've dragged emails into, and auto-generates Thunderbird filter rules based on the locations. Even for something as simple as that, I'm using about 5 SQL views.
Did you create the second table? The streak table is a bit weird as it looks like it relies on order of the rows. A better design would be something like |Streak| |-| |streak_id| |player_id| |streak_match| |streak_id| |match_id| Then your query could be like ```sql SELECT s.streak_id, AVG(m.opponent_rank) AS avg_rank FROM streak s INNER JOIN streak_match sm ON sm.streak_id = s.streak_id INNER JOIN match m ON m.match_id = sm.match_id AND m.player_id = s.player_id GROUP BY s.streak_id
The second table isn't really a permanent table but just the result of the query below - and it's not always the same. For example, the query I posted can be changed to ask for certain parameters in that streak (e.g., nationality of player, show only top 100 ranked players, online/offline, "flawless" wins, ...). That's why I can't make a "fixed" table or view that has a list of all streaks. You just made me aware I should clarify this in the question, thanks, will do so in a second.
If you plan out your DB schema in advance, there should be no need to regularly add new fields to the tables. Adding new fields would be a rare event (once every few years), and would be done by rebuilding the table. My advice it to talk to the client, find out the BUSINESS requirements for the project, then YOU determine the best IT solution to deliver on those business requirements. Don't let the client dictate the technical details on how to implement the solution.
I don’t have a side gig, but I would love to get one just for the experience alone.
Found the solution - it's basically done by using Postgres's filter function, which also makes this entire thing soooooo much more readable: select player_id, count(*) as streak_length, avg(opponent_rank) as avg_opponent_rank from (select m.*, count(*) filter (where result = 'l') over (partition by player_id order by date) as streak_grp from matches_m m ) m where result = 'w' group by player_id, streak_grp; Works perfectly, not the fastest, but very easy to understand and expand. Hope this helps someone out.
replace ISNUMERIC(column) = 0 with ISNUMERIC(COALESCE(column,'nope')) = 0 
Very cool. Thanks so much!
Good to know! In sales/marketing ops. We just use sf as our db, and it was set up messy. I often have to export data and transform it, so I can make reports off it. 
 Like this: SELECT * FROM stage WHERE ISNUMERIC(coalesce(startup,1)) = 0 OR ISNUMERIC(coalesce(active,1) )= 0 OR ISNUMERIC(coalesce([recovery],1) )= 0 OR ISNUMERIC(coalesce(onhit,1)) = 0 OR ISNUMERIC(coalesce(onblock,1)) = 0 OR ISNUMERIC(coalesce(damage,1)) = 0 OR ISNUMERIC(coalesce(stun,1) )= 0 So a null value is converted to 1 before the isnumeric check, resulting in a "Pass" for that value. Your pass criteria is "Row has field with non-numeric, non null field"
thank you :^)
That YouTube link is super helpful, thank you :)
how does that make work easier?
DB migrations keep track of database changes so as you grow your database, you have a history of those changes. You can version them in case you need to roll back the DB if something bad happens. For example, Let's say you move your application to production and hand it off to your client. He makes a ton of changes, breaks the app and hands it to you to fix. You have the original working app to start from in your versioning and migrations. Then you can bill him to make the changes he screwed up. 
This is why I don't do side work. Married with three young kids.
&gt; So a null value is converted to 1 before the isnumeric check, resulting in a "Pass" for that value. no, by your method, COALESCE turns the null into a 1 for that column, which ~is~ numeric, hence ISNUMERIC returns 1 (i.e. yes, a 1 is numeric), and thus those rows are selected? that's ~not~ what OP wanted
&gt; everything up to the first null well, yes... COALESCE is only looking at one column, though so if the column value is null, COALESCE changes it to 'nope', which is not numeric, so that row is returned if the column value is not null, then COALESCE leaves it alone, and the ISNUMERIC test is applied to the value, which may or may not be numeric
I had a 2 hour class at uni and managed to land myself a 3rd line tech support role dealing with SQL mainly.
90% of everyone has no idea what they're doing. If you know what you're doing you're not pushing yourself hard enough. You've got this!
so sum it up for me... if the column contains a NULL, your solution ~will not~ return that row, right? this is ~not~ what the OP wanted
You could try wrapping it in a try catch. In the catch, log the one that failed and fetch the next row. 
Could you give me a primitive example? Here is an example of one of my loops, I have about a dozen of them chained together, so this one will run, then the next one, and so forth. In total there are probably about.... 1200 loops being executed in a full load across 12 sprocs, so it would be nice to have a specific record of what failed and have the rest of the calcs continue running if they don't produce an error. USE [Database] GO /****** Object: StoredProcedure [SCHEMA].[090b_CALCNAME_Calc_1] Script Date: 8/26/2018 8:56:26 PM ******/ SET ANSI_NULLS ON GO SET QUOTED_IDENTIFIER ON GO ALTER PROCEDURE [SCHEMA].[090b_CALCNAME_Calc_1](@CalcID int) AS BEGIN TRY BEGIN TRANSACTION DECLARE @SCHEMAType nvarchar(255) = (SELECT SCHEMAType FROM [Database].[SCHEMA].[mpCalcs] WHERE CalculationID = @CalcID) DECLARE @ClientList TABLE ( ID int identity(1,1) , Client nvarchar(255)) IF ( SELECT SUM(CAST(isTrigger AS int)) FROM [Database].[SCHEMA].[mpClients] A INNER JOIN [Database].[SCHEMA].mpCalcs B ON B.CalculationID = A.CalculationID ) = 0 INSERT INTO @ClientList SELECT DISTINCT Client FROM [Database].[SCHEMA].[mpClients] WHERE CalculationID = @CalcID ELSE INSERT INTO @ClientList SELECT DISTINCT Client FROM [Database].[SCHEMA].[mpClients] WHERE isTrigger = 1 AND CalculationID = @CalcID DECLARE @Loop int = 1 WHILE @Loop !&gt; (SELECT MAX(ID) FROM @ClientList) BEGIN DECLARE @Client nvarchar(255) = (SELECT Client FROM @ClientList WHERE ID = @Loop) DECLARE @TempSource nvarchar(255) = (SELECT TempSource FROM SCHEMA.mpClients WHERE Client = @Client AND CalculationID = @CalcID) DECLARE @SegmentID nvarchar(255) = (SELECT SegmentID FROM SCHEMA.mpClients WHERE Client = @Client AND CalculationID = @CalcID) DECLARE @ClientProfile int = (SELECT ClientProfile FROM [Database].[SCHEMA].[mpClients] WHERE CalculationID = @CalcID AND Client = @Client) DECLARE @Clause1 nvarchar(MAX) = (SELECT Clause FROM [Database].[SCHEMA].[mpVariables] WHERE ClauseID = 1 AND CalculationID = @CalcID AND ClientProfile = @ClientProfile) DECLARE @Clause2 nvarchar(MAX) = (SELECT Clause FROM [Database].[SCHEMA].[mpVariables] WHERE ClauseID = 2 AND CalculationID = @CalcID AND ClientProfile = @ClientProfile) DECLARE @Clause3 nvarchar(MAX) = ISNULL((SELECT Clause FROM [Database].[SCHEMA].[mpVariables] WHERE ClauseID = 3 AND CalculationID = @CalcID AND ClientProfile = @ClientProfile), '') DECLARE @Clause4 nvarchar(MAX) = ISNULL((SELECT Clause FROM [Database].[SCHEMA].[mpVariables] WHERE ClauseID = 4 AND CalculationID = @CalcID AND ClientProfile = @ClientProfile), '') DECLARE @SQL nvarchar(MAX) = ' SELECT CASE WHEN B.SCHEMALabel IS NOT NULL THEN B.SCHEMALabel ELSE ' + '''' + @SCHEMAType + '''' + ' END AS ''SCHEMAType'' , A.DateRange , ''OrderID'' AS ''IDType'' , B.ORD_ID AS ''ID'' , A.Client , A.Country , A.Region , A.LaborTypeCase AS ''LaborType'' , B.Numerator , B.Denominator , B.MeasureQuantity , B.MeasureDescription , NULL AS ''Goal'' , NULL AS ''isFinancial'' , NULL AS ''Red'' , NULL AS ''Orange'' , NULL AS ''Yellow'' , NULL AS ''Chartreuse'' , NULL AS ''Green'' , NULL AS ''Threshold'' , NULL AS ''ComputedMinutes'' , 0 AS ''isContractual'' , ' + '''' + CAST(GETDATE() AS varchar) + '''' + ' AS ''Mod_Stamp'' FROM SCHEMA.veDate A LEFT JOIN ( SELECT * , CASE WHEN ComputedMinutes &lt;= MeasureQuantity THEN 1 ELSE 0 END AS ''Numerator'' FROM ( SELECT * , TotalMinutes - MinutesBefore9am - OrdDistAfter6pm - Minutes6pmTo9am - WeekendMinutes AS ''ComputedMinutes'' FROM ( SELECT * , [NumberofDays] - [NumOfWeekDays] AS ''NumberOfWeekendDays'' , (([NumberofDays] - [NumOfWeekDays]) * 10 * 60) - Ord1stDistDateisWeekend - OrdApprovedisWeekend AS ''WeekendMinutes'' , (NumberofDays - 1) * 14 * 60 AS ''Minutes6pmTo9am'' , CASE WHEN DATEPART(hh, ' + @Clause2 + ') &gt; 18 THEN DATEDIFF(mi, DATEADD(dd, 0, DATEDIFF(dd, 0, ' + @Clause2 + ')) + DATEADD(dd, 0 - DATEDIFF(dd, 0, ''18:00:00''), ''18:00:00''), ' + @Clause2 + ') ELSE 0 END AS ''OrdDistAfter6pm'' , CASE WHEN DATEPART(hh, ' + @Clause3 + ') &lt; 9 THEN DATEDIFF(mi, ' + @Clause3 + ', DATEADD(dd, 0, DATEDIFF(dd, 0, ' + @Clause3 + ')) + DATEADD(dd, 0 - DATEDIFF(dd, 0, ''09:00:00''), ''09:00:00'')) ELSE 0 END AS ''MinutesBefore9am'' , DATEDIFF(mi, ' + @Clause2 + ', ' + @Clause3 + ') AS ''TotalMinutes'' FROM ( SELECT ' + @Clause1 + ' , ORD_ID , A.Client , Country , Region , CASE WHEN A.LABOR_TYPE2 IS NOT NULL AND A.LABOR_TYPE2 &lt;&gt; '''' THEN A.LABOR_TYPE2 WHEN A.LABOR_TYPE2 = '''' AND A.[LABOR TYPE] &lt;&gt; '''' AND A.[LABOR TYPE] IS NOT NULL THEN A.[LABOR TYPE] WHEN A.[LABOR TYPE] = '''' THEN ''None'' WHEN A.[LABOR TYPE] IS NULL THEN ''None'' ELSE A.[LABOR TYPE] END AS ''LaborTypeCase'' , ' + @Clause2 + ' , ' + @Clause3 + ' , CASE WHEN DATEPART(dw, ' + @Clause3 + ') = 7 THEN (10 * 60) - DATEDIFF(mi, ' + @Clause3 + ', DATEADD(dd, 0, DATEDIFF(dd, 0, ' + @Clause3 + ')) + DATEADD(dd, 0 - DATEDIFF(dd, 0, ''18:00:00''), ''18:00:00'')) ELSE 0 END AS ''OrdApprovedIsWeekend'' , CASE WHEN DATEPART(dw, ' + @Clause2 + ') = 7 THEN (10 * 60) - DATEDIFF(mi, DATEADD(dd, 0, DATEDIFF(day, 0, ' + @Clause2 + ')) + DATEADD(day, 0 - DATEDIFF(dd, 0, ''08:00:00''), ''08:00:00''), ' + @Clause2 + ') ELSE 0 END AS ''Ord1stDistDateIsWeekend'' , DATEDIFF(dd, ' + @Clause3 + ', ' + @Clause2 + ') + 1 AS ''NumberOfDays'' , (DATEDIFF(dd, ' + @Clause3 + ', ' + @Clause2 + ') + 1) - (2 * DATEDIFF(wk, ' + @Clause3 + ', ' + @Clause2 + ') + CASE WHEN DATEPART(dw, ' + @Clause3 + ') = 1 THEN -1 ELSE 0 END + CASE WHEN DATEPART(dw, ' + @Clause2 + ') = 7 THEN -1 ELSE 0 END ) AS ''NumOfWeekDays'' , 1 AS ''Denominator'' , MeasureQuantity , MeasureDescription , B.SCHEMALabel FROM ' + @TempSource + ' A LEFT JOIN [Database].SCHEMA.mpLaborMappings B ON B.Client = A.Client AND B.SCHEMAType = ' + '''' + @SCHEMAType + '''' + ' AND (B.[Labor Type] = A.[Labor Type] OR (B.[Labor Type] IS NULL AND A.[Labor Type] IS NULL)) AND (B.LABOR_TYPE2 = A.LABOR_TYPE2 OR (B.LABOR_TYPE2 IS NULL AND A.LABOR_TYPE2 IS NULL)) AND (B.LABOR_CATG = A.LABOR_CATG OR (B.LABOR_CATG IS NULL AND A.LABOR_CATG IS NULL)) LEFT JOIN [Database].SCHEMA.[mpGoals] C ON C.SCHEMAType = ' + '''' + @SCHEMAType + '''' + ' AND ((C.Client = A.Client AND C.SCHEMALabel = B.SCHEMALabel) OR (C.Client = ''DEFAULT'' AND A.Client NOT IN ( SELECT DISTINCT Client FROM SCHEMA.[mpGoals] WHERE Client &lt;&gt; ''DEFAULT'' AND SCHEMAType = ' + '''' + @SCHEMAType + '''' + ' AND SCHEMALabel IS NULL ) ) ) WHERE A.Client = ' + '''' + @Client + '''' + ' ' + @Clause4 + ' AND A.SegmentID = ' + @SegmentID + ' ) Z ) Z1 ) Z2 ) B ON B.DateRange = A.DateRange AND B.Client = A.Client AND B.Country = A.Country AND B.Region = A.Region AND B.LaborTypeCase = A.LaborTypeCase' INSERT INTO [Database].[SCHEMA].[tabDashboard] EXEC sp_executesql @SQL INSERT INTO [Database].[SCHEMA].[tabCalculations] SELECT @Client , @SCHEMAType , @SQL SET @Loop = @Loop + 1 END COMMIT TRANSACTION END TRY BEGIN CATCH -- Determine if an error occurred. IF @@TRANCOUNT &gt; 0 PRINT 'Error found!...' + OBJECT_SCHEMA_NAME(@@PROCID) + '.' + OBJECT_NAME(@@PROCID); -- Return the error information. DECLARE @ErrorMessage nvarchar(4000), @ErrorSeverity int; SELECT @ErrorMessage = ERROR_MESSAGE(),@ErrorSeverity = ERROR_SEVERITY(); RAISERROR(@ErrorMessage, @ErrorSeverity, 1); END CATCH; https://pastebin.com/2f8HPJWT
yikes. brain fart. thanks
I think CONTINUE keyword is what you're looking for. Catch the error in TRY..CATCH. Then do some logging if you want and increment your @Loop and CONTINUE.
I think you'll need the TRY..CATCH inside the loop. Then it will catch the error and continue execution. Make sure to increment your @Loop variable in the CATCH block.
I don't have much experience with them either. I believe you'll need to wrap the inside of the loop in a transaction and TRY..CATCH, roll it back if it fails. The parent transactions should work as you would expect.
Just be careful with ISNUMERIC. There are inputs which evaluate to true that you wouldn't expect: ISNUMERIC('-.') evaluates to 1.
&gt; so sum it up for me... if the column contains a NULL, your solution ~will not~ return that row, right? &gt; this is ~not~ what the OP wanted It's exactly what OP wanted: &gt; This shows me everything I want plus all entries that have NULL in them, which I don't want. 
Why is that? Thanks for the heads up. I mainly have stuff like '_' or '~' in there instead of a null being used. I don't know why anyone would do this but its my problem now I guess.
Initial thought is to use DENSE RANK and partition by item and order by restaurant. This is how you could tell which items actually do occur in multiple restaurants. From there you can use the PIVOT function if you need your items or restaurants in columns instead of rows
You can cast '-.' as MONEY. I recommend not using ISNUMERIC as its output is unintuitive and difficult to predict.
i just want to see eveything that isn't numeric so that I can correct it manually. 
Sorry for the late reply, This worked perfectly. &amp;#x200B; I didn't realise we could do this on SQL. This is really good, Thanks.
Looks like you're missing the ID value.
Are you running this select statement manually? Or are you seeing the error in a web page?
For these sorts of requests that create a pseudo pivot, I create a subquery join for each 'food item'. From there you will be easily able to find common items by 'Y/N' flags in the where clause. SELECT F1.RESTAURANT, CASE WHEN F2.RESTAURANT IS NULL THEN 'N' ELSE 'Y' END AS HAMBURGER FROM FAST_FOOD F1 LEFT JOIN ( SELECT DISTINCT RESTAURANT FROM FAST_FOOD WHERE FOOD_ITEM = 'HAMBURGER' ) F2 ON F1.RESTAURANT = F2.RESTAURANT Repeat the left join for every food item that you have or want to compare. I've been in Teradata world for a while, so excuse any SQL syntax I made have flubbed.
Typically, what type of ID value would go in here? Sorry for the incredibly terrible question.. The part of the website having this issue is from a birthday calendar that ends in September. Left hand column, near the bottom [https://www.cinfed.com/max2/?l=1234&amp;p=cd80f4a842d1d448fde403ac6356a6ce](https://www.cinfed.com/max2/?l=1234&amp;p=cd80f4a842d1d448fde403ac6356a6ce)
Take a look at the data inside the Salesreps table (select top 10 * from salesreps) and see what data is returned in the ID column, I'm going to guess something like 1, 2, 3. 
Webpage &amp;#x200B;
Yeah, could also be a GUID. We would have to run sp_help to find out what data is supposed to go in there. Stepping back a bit, though, it seems to me that this might be caused by the application malforming the SQL string due to an empty string where the ID should've been provided. Unless he's running it straight from a SQL management console.
Looks to me like the web page doesn't have an ID when parsing out the SQL query. You looking at an actual record, or is it some kind of test page?
Do you have a web developer that maintains/builds the web site? It looks like the web backend is not supplying a required piece of data (the sales rep ID) to your database when it's making the query.
Your website is probably supposed to send the parameter values for "status" and "id" to the SQL query, but the ID parameter was empty. Check the website code first... see how the ID parameter gets populated. But ideally, your code should contain logic which would handle cases like this (where the parameter is empty, it should populate a dummy value or remove the "and ID =" clause entirely. `SELECT * FROM salesreps WHERE Status=1 AND ID=`
Looks like I skipped the first rule of troubleshooting, which is "Do I have access to this space?". I do not and have reached out to the person who does. 
What RDBMS requires rebuilding the entire table when adding a field?
Ha, probably the best thing you could do to get it fixed ASAP
You don't HAVE to, but it would be better for performance to have all fields of a record stored sequentially on disk.
The nice things about table variables is, that they don't roll back when you rollback the transaction. So you could have your query update the table variable with some flag and then inspect it after the failure.
100%, I could not agree more.
quality of this article is unacceptably poor smells a lot like spam reported
If the query was parameterized you would probably get another error so I am assuming it is probably vulnerable to SQL injection. Also, it's performing "SELECT *" so between those two things it's probably safe to assume your engineer is a lousy programmer, doesn't know sql, or they didn't write this code and inherited a pile from another dev. This is why people get hacked. I am assuming this isn't public facing which is why it's still operating today.
Not equal to...
who downvotes a correct response? nasty petulance in this sub 
because it was a wrong statement. He said "**either** less than **or** greater than" which is not true. It means "**both** less than **AND** greater than" which means not equal.
Yes, 'greater than' means after. `where hiredate &gt;='DD-MM-YYYY'` Where hiredate is after YYYY-MM-DD (use the universal date standard).
&lt;&gt; is "not equal to". For dates it's okay to use, but for timestamps, be aware that your timestamp may be stored with fractional seconds, so if you only list the timestamp to the seconds value, it may not behave correctly if the timestamp was inserted using `current` and contains fractional seconds.
How do you do that?
Does it have to be in all restaurant categories or is two or more enough?
This is tricky because the version number both isn't a number and can't be converted to a number and then safely compared. You need to parse out the constituent parts of the version numbers and compare each individually. Have a look at the answers to [this Stack Overflow question](https://stackoverflow.com/q/11364242/1324345), [this one](https://stackoverflow.com/q/13315756/1324345), and [this one](https://stackoverflow.com/q/23809712/1324345)
I can't recommend https://use-the-index-luke.com/ enough :)
I made a joke, said I was kidding, and gave my legit reasons below that. No reason for you to suggest I have a "deeply flawed view of reality" for that. Wanting to gather opinions on a product from people who might have more experience in the area isn't some veiled attempt to "push a product". Nothing about my account history even suggests that. People are interesting. It's a joke. Chill out. That reply was such a bot "does not compute" type thing. 
select e.ename, e.hiredate, j.hiredate jones from emp e, emp j where j.ename = 'JONES' and j.hiredate &gt; e.hiredate ( so &gt; makes j.hiredate greater then e.hiredate this means that it shows who is hired before jones?) and we could do it like this then? e.hiredate &lt; j.hiredate?
thankyou!
In that case, I believe there would be a better way of getting and displaying your data. In your example, the duplicate items are mentioned in each row while only Restaurant differs. I think it would be better to have this: CommonItem| RestaurantAggregate ---|--- Hamburger| Wendy's, McDonald's, Burger King Milkshake| Wendy's, McDonald's, Burger King Chicken Nuggets| Burger King Baked Potato | Wendy's Then you could specify how many restaurants need to be present for the record to appear. 
https://smile.amazon.com/SQL-Tuning-Generating-Optimal-Execution/dp/0596005733/
I found that CAST(PARSENAME(softwareVersion, 4) AS INT) &lt;= '68' AND CAST(PARSENAME(softwareVersion, 1) AS INT) &lt; '106' worked in every case except for when the major version is lower and the latest release has a higher number (ex. 66.0.3440.139 vs 68.0.3440.106). The solution that ended up working is Cast('/' + Replace(softwareVersion, '.', '/') + '/' AS hierarchyid) &lt;= Cast('/68/0/3440/106/' As hierarchyid)
Can’t you just replace the “.” With “” and compare the numbers that way? 10000010 &gt; 10000009 Just like 1.0000.01.0 &gt; 1.0000.00.9 Right?
THANK YOU!!
On SQL Server 2012+, you can use the `EOMONTH()` function and just add one day to the end of the date's current month. `select dateadd(day,1,EOMONTH(getdate()))` 
where are the purchases? also, what's the grand total comprised of? here's a query without those two result columns SELECT c.id , c.name , COALESCE(d.qt_debts,0) AS qt_debts , COALESCE(d.value_debts,0) AS value_debts , COALESCE(p.qt_payments,0) AS qt_payments , COALESCE(p.value_payments,0) AS value_payments FROM client AS c LEFT OUTER JOIN ( SELECT client_id , COUNT(*) AS qt_debts , SUM(value) AS value_debts FROM debts GROUP BY client_id ) AS d ON d.client_id = c.id LEFT OUTER JOIN ( SELECT client_id , COUNT(*) AS qt_payments , SUM(payment) AS value_payments FROM payments GROUP BY client_id ) AS p ON p.client_id = c.id 
Can you elaborate on this please? I was under the impression that because I'm using nested commits from one sproc to the next that if a child sproc which is triggered by the parent fails, that all of the changes from all of the previous children will also be rolled back. Such as: Sproc 1 - Global: BEGIN EXEC SProc2 EXEC Sproc3 EXEC Sproc4 COMMIT Sproc 2 - BEGIN, ... stuff ... COMMIT Sproc 3 - BEGIN ... stuff ... COMMIT Sproc 4 - BEGIN ... ERROR ... COMMIT Sproc 5 - Nothing happens In that kind of daisy chain model I was under the impression that since 3 failed and 1 was executed 3, that nothing would be committed at all for any children and everything would roll back.
I tried loading the 3.5GB CSVaa file again and it took 23 minutes instead of 13
Wow that was sick! Thank you, brother! I will spend some time studying this! The other values of the table are basically subtract the both values for the total, and data of the receipt so can used as filter. This "Debt" table (i couldnt find a better word in english) its the money the employer expects to receive, so there is a date for the payment timeline, and i have to do a if clause to avoid debts that are already paid. But again, thank you, i was kinda losing it here hahaha
Yes, that query will only work if you have MySQL v8.0. Just realized re-reading this thread... do you mean you literally have a column named "date" in your database? If you do, that's probably where your issues are coming from. Thought you were just using pseudo code with that so didn't point it out. You never want to use reserved words as column names. That's why I used mydate in my example.
Watch out, this guy is having meetings with VPs and Execs about this shit while making six figures! He's a big boss, don't rustle his Jimmie's! He knows how to market himself. Check him out. He's so cool!
Watch out, this guy is having meetings with VPs and Execs about this shit while making six figures! He's a big boss, don't rustle his Jimmie's! He knows how to market himself. Check him out. He's so cool!
Watch out, this guy is having meetings with VPs and Execs about this shit while making six figures! He's a big boss, don't rustle his Jimmie's! He knows how to market himself. Check him out. He's so cool!
Watch out, this guy is having meetings with VPs and Execs about this shit while making six figures! He's a big boss, don't rustle his Jimmie's! He knows how to market himself. Check him out. He's so cool!
Watch out, this guy is having meetings with VPs and Execs about this shit while making six figures! He's a big boss, don't rustle his Jimmie's! He knows how to market himself. Check him out. He's so cool!
Watch out, this guy is having meetings with VPs and Execs about this shit while making six figures! He's a big boss, don't rustle his Jimmie's! He knows how to market himself. Check him out. He's so cool!
Watch out, this guy is having meetings with VPs and Execs about this shit while making six figures! He's a big boss, don't rustle his Jimmie's! He knows how to market himself. Check him out. He's so cool!
Wow, three messages in a row. You must be angry. Like I said. If you want to advance your career. Work for free. Learn how to market yourself. 
Watch out, this guy is having meetings with VPs and Execs about this shit while making six figures! He's a big boss, don't rustle his Jimmie's! He knows how to market himself. Check him out. He's so cool!
Watch out, this guy is having meetings with VPs and Execs about this shit while making six figures! He's a big boss, don't rustle his Jimmie's! He knows how to market himself. Check him out. He's so cool!
Watch out, this guy is having meetings with VPs and Execs about this shit while making six figures! He's a big boss, don't rustle his Jimmie's! He knows how to market himself. Check him out. He's so cool!
Watch out, this guy is having meetings with VPs and Execs about this shit while making six figures! He's a big boss, don't rustle his Jimmie's! He knows how to market himself. Check him out. He's so cool!
Watch out, this guy is having meetings with VPs and Execs about this shit while making six figures! He's a big boss, don't rustle his Jimmie's! He knows how to market himself. Check him out. He's so cool!
What?
Are there more than 2 data points?
Sorry, responding to /u/notasqlstar who seems to feel the need to have a dick-measuring contest in another sub. 
according to documentation it seems like there's no else statement as the else is implicit in the default frog\_level\_condition. [https://docs.aws.amazon.com/redshift/latest/dg/r\_DECODE\_expression.html](https://docs.aws.amazon.com/redshift/latest/dg/r_DECODE_expression.html) &amp;#x200B; I'm no expert through so maybe someone else has a better perspective.
Using EOMONTH() is best practice for sure
usually either manually, or by unix command line hacks.
Check out BigQuery on GCP. Signing up nets you $300 in credit for a year, with lots of public data sets to practice on. You'll get SQL practice and cloud exposure at the same time.
The short and simple answer is no one here can answer this question without knowing your environment/website. Regardless of how bad the query is or the code that supports it, an ID is data specific and requires data/process specific insights. I would not recommend continuing looking for help here for this type of problem as your may make your company more vulnerable than it already is.
The values are simply not being pulled from the tables, so your table isn't being able to select all values from salesreps where their status would be 1 or 0; likely active or not active and their ID isn't within the stated range.
I don't do a lot of creating of dbs or tables... May need some 'go's or semicolons to finish those spots, but I'm not sure. Down by the insert though, you have a few issues. You mlneed to put you varchar values in quotes. Double check your column names declared for the insert as well. A few of these dont match the columns you created. E.g. there is no cinemaname column. There is a Name column. 
Agreed, I would also add missing the declaration of if a column is allowed nulls or not null.
Throw some apostrophes on the text values in your values statement... VALUES ('EyeCandyWellington',' EyeCandyCinema', 'Te Aro', 'Te Aro Street', 42, 0800 930 850, 'half price') Maybe the phone too sine there are spaces but not 100% on that one. 
After CREATE DATABASE statement, add: `USE ExampleCinema GO` You need do this so that your next statements create the table under the new ExampleCinema database. &amp;#x200B; CREATE TABLE statement: * Change `Name` column to `CinemaName` * Change Phone datatype from INT to VARCHAR (in order to retain preceeding ze * Fix primary key syntax (this is not valid for SQL Server). If your intent was to have CinemaID and CinemaCity make up the composite PK, then it'd be `CONSTRAINT PK_CinemaID PRIMARY KEY (CinemaID, CinemaCity)` INSERT statement: * Change `CinemaPhoneWellington` to `Phone` * wrap all varchar values in single quotes So it will look like this in the end: CREATE DATABASE ExampleCinema GO USE ExampleCinema GO CREATE TABLE Cinema ( CinemaID VARCHAR(20), CinemaName VARCHAR(20), TheatreID INT, CinemaCity VARCHAR(20), Suburb VARCHAR(20), Street VARCHAR(20), StreetNumber INT, Phone VARCHAR(20), StudentDiscount VARCHAR(20), CONSTRAINT pk_cinemaid PRIMARY KEY (CinemaID, CinemaName) ); INSERT INTO Cinema (CinemaID, CinemaName, Suburb, Street, StreetNumber, Phone, StudentDiscount) VALUES ('EyeCandyWellington', 'EyeCandyCinema', 'Te Aro', 'Te Aro Street', 42, '0800 930 850', 'half price') &amp;#x200B;
 SELECT code, max(value) FROM table GROUP BY date(string), code ?
about max(value), doesnt work for code 633 because its latest value is lower than its earlier value
ah sorry i didn't notice that. i believe this is a greatest-n-per-group issue but you will need to convert the date string todate to make it work/identify the greatest date. [but i believe this stackoverflow thread should resolve your issue.](https://stackoverflow.com/questions/7745609/sql-select-only-rows-with-max-value-on-a-column)
 Nope - sql cert still exists
For joins you should get an understanding of relational databases. Also read [this](https://blog.codinghorror.com/a-visual-explanation-of-sql-joins/) article, followed by [this](https://blog.jooq.org/2016/07/05/say-no-to-venn-diagrams-when-explaining-joins/) one. What helped me understand sub queries (at a most basic level) it is a full SQL statement that can be used to replace a join when used with an IN (or NOT IN). Its how I choose to use sub queries 
That's another issue. The datatype of phone is declared as an INT, but the value (with spaces) is a char.
It doesn't matter too much with the spaces, it was just easier to read, but thanks for pointing it out.
It does not, not for 2016. If you go to Microsoft certification page you will see 3 options for 2016: BI development, database administration and database development. For 2016 you will need to choose one of these certification paths.
/u/JanBaskTraining please read our sidebar. This tutorial is far too basic for the users of /r/SQL. Try /r/LearnSQL.
Question 1 - there's every chance your fighters can have a rematch so I'd go with #1 - ResultID as PK, and two FKs. You may want a date column also. Question 2 - AwardID/SeasonID as the PK looks fine.
What kind of *monster* would put GUIDs into the Id field...
Does ms sql support regular expressions?
Reverse the string, find the first non-numeric character. Use the length of the string minus the location you just found as the starting point for the substring and substring out the number. 
Aside from food. You’re my favorite.
One of the best online course. For beginners and for free. With examples and online exercises. [http://www.studybyyourself.com/seminar/sql/course/?lang=eng](http://www.studybyyourself.com/seminar/sql/course/?lang=eng). 
If you google for how to extraxt the filename from a full path you'll find MS SQL code for the thing you're looking for.
1. use row_number() to order the rows by date and group by value, and mark the rows by number 2. select the rows where row_number is 1 select code, value from ( select *, row_number() over(partition by code order by date desc) from foo) foo_numbered where row_number = 1 
hi le848dave, I did try reversing the first non-numeric char, but it's giving me reverse value too lol 
sorry please excuse me for being rude, with post like this, please don't bother posting thanks. There's a learning curve for everyone, and yes I did attempt to do homework before posting here.
Use an Apply
You only reverse it to find the first non numeric character which is your starting point for the substring. Your substring starts then from LEN(&lt;&lt;your string&gt;&gt;) - (the location you found - 1). (You're counting backwards from the normal string to get your starting point in the string value in it's normal direction) DECLARE @myString NVARCHAR(10) = 'ABC1234'; So SUBSTRING(@myString, (LEN(@myString) - (PATINDEX('%\[\^1-9\]%', REVERSE(@myString)) - 1), (PATINDEX('%\[\^1-9\]%', REVERSE(@myString)) - 1)) Mind you this is just before I go to sleep and untested but should be close to working. Effectively it is: SUBSTRING(@myString, (7 - (5 - 1)), (5 - 1)) You can clean all this up and not keep doing REVERSE and PATINDEX and LEN by doing some CROSS APPLY but that's just to clean up the code
Except for CHARINDEX you'll need to use PATINDEX with the correct pattern. You're looking for a non-number, not a backslash.
It looks like the hgroup in your 3rd table doesn’t have unique values. For instance 01 takes on main and finance as names. So when you do the left join (last join you did) its going to your table and joining for every 01 you have BOTH a row for finance and a row for main. Then you are counting rows and since the data is duplicated you are getting more than what you expect. 
Didn’t see your reply. I wrote my 2nd comment trying to better explain what the situation might be in case I didn’t do a good job in my first comment 
Is there any way around that given the data provided? It’s very likely that I just missed a table, or column in the mock-up from memory, as my company DB has a massive amount of tables.
Thanks for helping me out, I attempted it with the suggested code, it's giving me Non-Numeric stuff too. ,SUBSTRING(@string.icd10, (LEN(@string.icd10) - (PATINDEX('%[^1-9]%', REVERSE(@string.icd10)) - 1)), (PATINDEX('%[^1-9]%', REVERSE(@string.icd10)) - 1)) as 'tasdasd'
Thank you MunaLoona, I haven't ignore you. I am still testing the syntax as it is giving me error. As I change it to PATINDEX, it requires 2 arguments. As soon as I fix it by adding the 2nd argument, it says "The left function requires 2 arguments" Will let you know the progress, wanted to give full shoot first before asking for another help. 
Maybe simple but workaround solution is like below: *caution: some DBMS doesn't support distinct keyword in aggregate function.* SELECT g.code Security_Group, Count(distinct u.code) Users, Count(distinct m.id) menu_num FROM USER u LEFT JOIN group g ON g.id = u.hgroup LEFT JOIN groupmenu m ON m.hgroup = g.id GROUP BY g.code
Isn't having cinema name as a primary key a bad idea? You're going to end up having Odeon in there 100 times under different street addresses.
Yes, but was just fixing syntax so it'd run. We go down that route and I'll be breaking the sample data out into at least 3 tables to normalize it and we'll scare OP away from SQL for good ;)
Just as a stab, we want Cinemas, Address and Discount table? I newish to SQL myself. 
Update customer set cardid = (select top 1 cardid from cardid where cardid not in (select cardid from customer)) where cardid ='' -- set that as null or whatever your empty cardid is. 
What’s wrong with getting certified for 2012/2014? 
whats the difference between theater and cinema? 
Is it possible that constraint name is already in use? Run SHOW ENGINE INNODB STATUS After you get the error and have a look at the latest foreign key error.
This is the exact type of system I was thinking about for GUIDs in the "ID" column.
select code, value, date from table t1 inner join (select code, max(date) from table group by code) t2 on t1.code = t2.code and t1.date = t2.date The subquery gives you the max date for each code, then you join that to the table to pull in the value.
 Update customer set cardid = (select top 1 cardid from cardid where cardid not in (select cardid from customer where cardid is not null)) where cardid is null;
Ah, id have called the theatre the 'screen'
Why not just combine them into a single case statement? CASE WHEN frog\_level\_condition is NULL then 'No frog status info' WHEN frog\_level\_condition = 3 then 'Pending jump' WHEN frog\_level\_condition = 5 then 'Authed jump' .... ELSE frog\_level\_condition END as frog\_status
I’m sorry, I’m pretty new to SQL and couldn’t really word my question. It’s asking me to find invoices from “invoiceitem” where “b” is the third letter from the left. Thank you for your response! 
Realistically, all "prestigious" certs have the same pros and cons. I've never seen a job in the last 10 years that REQUIRED a cert, but I have seen jobs REQUIRE a degree. The degree in itself is also kind of rare just because a lot of places will substitute experience, not all, but a lot of places. Think of the certs as something to do to make yourself better, be a "tie breaker", or help you get past HR to the interview. They are not the only activity that can accomplish those three goals however, so if it doesn't fit your preferences, you can look to something else. I recently obtained my 2016 MCSA in SQL Development, I am working on my 2016 MCSE in Data Management and Analysis. I would recommend to always work towards the newest certs, but there is nothing wrong with the old certs. It all depends on needs and goals, but 95% of the time, it's best to work towards the newest certs. 
This doesn't even contain anything about deleting duplicate rows, it just tells you to SELECT DISTINCT. There are other ways to delete duplicate rows, but my favorite is ;WITH d AS ( SELECT UID, ROW\_NUMBER () OVER (PARTITION BY every field that should create a unique field if Concatednated together ORDER BY UID) AS RowN FROM Table ) DELETE FROM d WHERE RowN &gt; 1 
For this that'd make a lot more sense, have Chain (eg Vue, AMC, Cineworld), Cinema (Oxford Central, Leicester Square), Screen (1, 2, 3 ,4 etc) 
I had the same complaint about the article. I like your idea, but be careful concatenating values ('a' + 'bc' will end up being the same as 'ab' + 'c'). Using a delimiter that doesn't appear in your data within your concat should help.
I probably didn't use the best term there. I don't actually concatenate the values, it is actually partitioning on each field. When any of the fields change, the row number resets to 1. so if a field called first name and last name were the same for some rows, it would increment the row number by one until one of the fields changed. Hope that makes more sense!
Check out the merge statement
What do you mean?
you don't need the inner select he posted, just select * from table where where left(fieldname, 3) = 'b'
Awesome thank you very much for the response, I’ll try it out when I’m home from work. So it should look something like (the table is tblinvoiceitem and the subset is invoiceitem): Select * From tblinvoiceitem Where left (invoiceitem, 3) = b If I were to need to adjust it to say right or 2 instead of 3, I would just switch out the wording for the needed adjustment? This is actually the final problem of this homework set, but I have 6 more to do throughout the course so I assume he will have something similar. Thank you. 
&gt; We go down that route and I'll be breaking the sample data out into at least 3 tables to normalize it and we'll scare OP away from SQL for good ;) Nothing wrong with terrifying the younger generations
Well I just cancelled everything, then ran delete and only took 13min. Now doing the update
this is an atrociously bad article... reported as spam example -- &gt; The basic syntax of the delete command in SQL is given as below – &gt; &gt; DELETE FROM ‘table_name’ [WHERE condition]; anybody that attempts to use that exact syntax (with the microsoft curly quotes) is in for a world of hurt and they will not really understand why
GROUP_CONCAT is da bomb
Are you sure their wasn't a transaction holding the table up or something before?
What do you mean transaction? I said I had 2 queries plus the autovaccuum. You're saying there's a hidden 4th one?
mine was similar to yours. window functions in MSSQL have significantly reduced the complexity of my queries 
[I had to look up a use case because I wasn't sure when you would need such a unique string produced.](http://www.mysqltutorial.org/mysql-group_concat/)
it sure beats the pants off of FOR XML PATH, don't it
For me it's just the general design pattern of serializing complex queries in successive CTEs. When I start writing some logic and I know it's going to be a beastly query, I write it as a CTE from the get-go. Then, the first time I need to use a window function as a join condition, predicate, etc. - or just have some complex expression I don't want to re-write - it's already encapsulated for me. And my code tends to be really easy to follow because it mirrors your thought process of "Okay. First, look at the first/biggest X for each Y. Then, consider all those Y such that Z. Then..." Or maybe phrased more succinctly, I'm just raving about window functions like everyone else, but with the slight twist that they get their biggest value as encapsulated predicates, not just in the terminal SELECT.
I wouldn't exactly call a 20gb table small. This particular table is string aggregating values from another table based on location. Both tables have indexes
I'll try to get back to you when I'm no longer on mobile but gist of it is this. In transaction rollback only db objects get rolled back. That's tables, temp tables, procs, views but not table variables (@table) they persist between transactions and live while the session is active
has an error on t1.date = t2.date
Sounds like you're all set then. If you do need others to take a look a the code, this is a pretty helpful community. 
Thanks man. It just looks like I was misunderstood somehow lol
Group By and PASS TV are two great YT channels if you are looking for SQL Server.
Batch insert?
Jumping on the SQL Server train: * [Bert Wagner](https://www.youtube.com/channel/UC6uYLOg8N_NzjkikbJ_vrtg) * [The PASS Virtual Chapters](https://www.youtube.com/results?search_query=pass+virtual+chapter)
and it'll be many more years before shops get around to upgrading
Good catch, was working late last night :(
[https://www.youtube.com/watch?v=7Vtl2WggqOg&amp;t=2457s](https://www.youtube.com/watch?v=7Vtl2WggqOg&amp;t=2457s)
I went to bed at 5:30am so I somewhat feel your pain. ;_; Though in my case I blame Skyrim... and fucking Bethesda :/ Tried to fix an issue I was having with and the steam workshop and in the process realized the Steam version's loader is beyond garbage (steam already has an auto update feature for workshop item why the fuck do you need to check each and every mod one by one every time you want to start the game...) So instead of **playing** Skyrim I wound up spending 9 hours cleaning up, re-downloading, replacing and uninstalling/reinstalling hundreds of mods though Nexus instead. 
Thanks, I have a development session tomorrow and have been trying to mentally digest what I'm going to do. My total code base is about 6000 lines right now.
regxp &amp;#x200B; ROWNUMBER &amp;#x200B; Replacing DISTINCT calls with GROUP BY for easier reading (and I think it's slightly faster)
I have 2 servers, identical code bases between them, identical staging tables, and identical mapping tables. I run the process and end up with 20,000 extra rows on one server than on the other, out of a dataset of 14million. FML.
Are you using a single INSERT...SELECT statement or multiple INSERT statements? How many indexes is it updating in the target tables? Do they get disabled or dropped before loading?
2 Separate INSERTS for each of the two tables. There are no index in the initial table, though there are indexes in the two tables. How much improvements are there in disabling or dropping the indexes?
&gt; window functions in MSSQL Do you have any resources that you can share for this?
Not actually SQL, per se, but I just discovered that you can ctl-r search in the postgres cli :mind-blown:
Alt-Shift &lt;3
Recursive CTEs.
This is an explanation I found helpful. r/https://www.sqlshack.com/use-window-functions-sql-server/
No you weren't. You told me you were drinking a beer and smoking pot by 6pm. 
My experience is more with MSSQL, but I can’t pass up an opportunity to try to help Zeewulfeh. Firstly, you appear to have an extra comma at the end of the columns you are selecting, I’m not sure if DB2 allows this. The main issue is that you have two from statements, so it doesn’t know which to use. Rather than having two from statements, you want to join the joined table to the third table, meaning you left join T1 and T2 and left join the result to T3. This would be accomplished by removing the second from … t1.
I've used this a couple times for a reference: https://www.red-gate.com/simple-talk/sql/t-sql-programming/sql-server-2012-window-function-basics/
this was a pretty helpful resource i used to get me started. you are basically partitioning a dataset then performing some function over that partition. check out the link for some examples. https://www.red-gate.com/simple-talk/sql/t-sql-programming/sql-server-2012-window-function-basics/ 
Everyone remembers where they were when they learned: * JFK was shot * 9/11 * Alt-Shift
Heavy sigh...
Yup, you and Bandit nailed it, it appears. Thank you!
Okay, I had to get rid of the create table command for some reason, but everything else seems to work! It's actually running now, thank you!
You work in a hospital?
Cross Apply. When you really get to use it in the right circumstance, it's a game changer.
Could you expand on Rollup? Thanks sounds like something i would need
Holy. Fucking. Balls.
It doesn't look like you are joining the SAL table. It's just a range, so I'm not sure if your result is correct unless I'm missing something.
Big pharma.
1. Table Inheritance / The Party Model 2. Order of Execution: FROM, WHERE, GROUP BY, HAVING, SELECT, ORDER, LIMIT
OPTION (OPTIMIZE FOR UNKNOWN) to battle parameter sniffing and bad query plans.
For people who do a lot of ad-hoc query writing, starting your WHERE clause with: WHERE 1=1 Sure, it's basically a no-op, but the beauty is that now, if you always start a new line for your boolean operations, you can comment out any individual thing and not have to go do "special" things to comment out your first criteria. WHERE 1=1 AND OrderDate &gt; DATEADD(dd, -5, GETDATE()) AND OrderStatus in ('New', 'Held') is a lot easier to turn on or off individual pieces of as you investigate the data. This has saved me a lot of time as I come into a new customer organization and I'm trying to determine what the root cause is when the report they used forever is no longer returning any data. It's not as game-changing as learning how window functions work, or getting comfortable with recursive CTEs, but it's such a simple pattern to get used to doing, for how often that I'm happy I can just comment something out to see how the new result looks, that I'm glad someone showed it to me a while ago. 
Likewise, holding ALT while dragging the mouse. UNION queries to Insert data have never been easier.
SQL Server or Oracle? Because in SQL Server every time I've tested it, it has had performance issues. Not to mention the well documented bugs https://www.mssqltips.com/sqlservertip/3074/use-caution-with-sql-servers-merge-statement/
To address your original question, yes, running a delete and update statements at the same time can block each other. 
The problem is that sql doesn't do what I want it to. It only does what I tell it to. 
upvote for upsert! 
Yep, you want to do something like this: select a.datefield, isnull(sum(entries), 0) as entry_counts from TallyDateRange a left join TallyEntries b on a.datefield = b.datefield group by a.datefield
You can also just the leave and on the line above the statement where you need it, so if you comment out that line it runs: WHERE \[2\_week\_Avg\_Run\_Duration\] &gt;0 AND Last\_Run\_Duration&gt;0 AND Run\_Duration\_Variance\_Percent&gt;25 WHERE --\[2\_week\_Avg\_Run\_Duration\] &gt;0 AND Last\_Run\_Duration&gt;0 AND Run\_Duration\_Variance\_Percent&gt;25 WHERE --\[2\_week\_Avg\_Run\_Duration\] &gt;0 AND \--Last\_Run\_Duration&gt;0 AND Run\_Duration\_Variance\_Percent&gt;25
If you don't want to use 1=1, you can use power(sqrt(2-2+2),2)+2-2/2*2/2-2 + power(sqrt(2-2+2),2)+2-2/2*2/2-2 = 2
Man, this makes my problem seem like child’s play lol. Good luck, hope you get things figured out! 
Thanks!! Will try. 
Thanks for linking that! Didnt run into any issues yet...knock on wood.
thank you
SUM(a) OVER (PARTITION BY x,y,z) Has seriously saved me so much time rather than having to group the current table, aggregate, then rejoin on the original table. I just never really looked into this feature. My boss was using it everywhere so I was somewhat forced to learn it. Also using similar syntax for running totals helped me a ton on a few projects.
I get that we are dealing with another language besides English from the screenshot you provided, but it makes it difficult to follow when some things are renamed and others are not, and some things we have to guess at. Just use the original names if you want to. It makes no difference to me what it's called because the SQL is the same. But for example, there is no DNAME column in any of your tables, and you don't specify which table it is supposed to come from. Obviously you probably mean NAAM in the DEPT table. But the way it is written now would produce an error. There is also no LOSAL or HISAL, so we first have to figure out which columns LOSAL and HISAL refer to in SALGRADE before we can help with the query (ONDERGRENS is LOSAL and BOVENGRENS is HISAL). 
There is no SAL table. Did you mean SALGRADE? OP is clearly joining to SALGRADE. The word "SAL" you see is OP translating/renaming the MAANDSAL column in EMP to an English column name.
Thankyou! yes it is in dutch, the tables we got from school are in dutch but in the program we use english tables, sorry for that.
Plenty of free courses online - take a look at Microsoft Virtual Academy or edX. Also, MS SQL Server is free in developer version (I think it's called) - basically for non-production environments - meaning you can have a full SQL install on your local box for free. Choose your course, install SQL, download the sample databases and get your hands dirty.
I've had many issues with MERGE. Love it, but won't use it in production code any more.
common table expressions and outer applys help a lot
That on MSSQL, CTE's aren't materialized, and are executed on every single pass.
Pivot tables took about 8 tries to really wrap my head around, but once I did it changed how I wrote aggregations for ad hoc reporting.
Or F5
&gt; how can i use the WHERE BETWEEN clause on Outer Joins? The dates will be provided by the application you need to put the WHERE condition into both subqueries LEFT OUTER JOIN ( SELECT client_id , COUNT(*) AS qt_debts , SUM(value) AS value_debts FROM debts WHERE datecolum &gt;= '2018-01-01' AND datecolum &lt; '2018-03-01' GROUP BY client_id ) AS d ON d.client_id = c.id 
Sum(case when bla =something then 1 else 0 end) as Instead_of_pivot Yeah.. This was a huge game changer because pivot is so annoying.. And doing weird temp tables to get info otherwise annoying 
I'm not at my pc, what does it do!
&gt;Move to the current tool window toolbar What exactly does that mean? I can't try in SSMS atm
I've been on projects where someone else has either been stronger in optimizations and concurrency or responsible for them all together. Learning about MVCC, **column**\-oriented DBMS, and optimizations has changed how I think about SQL. [https://en.wikipedia.org/wiki/Multiversion\_concurrency\_control](https://en.wikipedia.org/wiki/Multiversion_concurrency_control)
sweet baby Jesus, please be more specific. Include your table definition(s) and sample data and expected output
This will help: select chapter from yourtextbook where lesson = @getdate()
so i figured someone could help me on here
You need the sum of one column with one condition in a where clause. Come on man, you can do this. 
i've tried that lol maybe im inputting something wrong
I use the returning clause with updates and inserts and deletes.
Which 2016 SQL cert is the equivalent of querying data? Is it the BI analytics?
Hey, xzekke, just a quick heads-up: **seperate** is actually spelled **separate**. You can remember it by **-par- in the middle**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
Depends on the data. If the data is Male and Female than it would be WHERE Gender = 'Male';
This is it. Just filter on gender 
That worked, i swear i tried that maybe i put male instead of just M but thanks for your help man
Table should look like this: client_id client_name date qt_debts value_debts qt_payment value_payment total(debt-payment) user should choose an interval of dates to display the reports. Lets say client bought in 3 separate dates, within the date interval, only one entry should appear since table is the total amount of debt/payment per client. 
Can you elaborate on why this is useful? I just copied and pasted this into a text editor to try to understand what is great about it and cant see what you are saying haha
Well you're not wrong. You could easily just memorize the keyboard command for line commenting your selection. &amp;#x200B; The trick is if you delete the initial --, the whole block comments out without the need to select it. Add it back and your block is commented. I guess another advantage is that it's clear where the end of what you should be commenting out is, since your end point is already there. One thing I use it for is unit testing my data at individual points while I'm writing or editing a large stored proc. I can uncomment and check if it's doing what I think, then comment the block back, but the query I used to test it is still in the proc if I need it later.
So, I've used compound indexes in the past for similar issues. Here is the document for it. https://dev.mysql.com/doc/refman/8.0/en/multiple-column-indexes.html
&gt; I don't think I can create an index that will help with this MySQL supports [composite indexes (aka multi-column indexes)](https://dev.mysql.com/doc/refman/8.0/en/multiple-column-indexes.html). You do not need to create a new column for the join. You can join on more than one column. The convoluted hash idea really is overkill.