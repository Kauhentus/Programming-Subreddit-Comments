Could possibly be solved with a cte ;WITH CTE AS ( SELECT taking_id FROM development_table GROUP BY taking_id HAVING COUNT(taking_id) &gt; 1 ) SELECT t1.taking_id, exam_id FROM development_table t1 JOIN CTE ON CTE.taking_id = t1.taking_id Hope this works. I'm not to familiar with MySQL but if it supports CTE and HAVING clause. Edit: MySQL does not support cte so exchange that part for a temporary table
Which DB?! (Asked count #767).
&gt;The reason I use OPENQUERY is so I don't have to worry whether I'm pulling or pushing. It's executed remotely. No. That's not how open query works. &gt;I just do INTO #TempTable. I believe it creates indexes automatically. Inserting would actually be faster without indexes, so this isn't it. No. It doesn't create indexes. The temptable is a Heap but that is GOOD. &gt;I don't think so. I don't think it would make a difference since the query is executed remotely. It's not that big of a table to sort. If there is a sort in the source remote query, the query may be executing and writing the query to tempdb before returning any recordset. &gt;I've done this before but for smaller data sets. The transfer speed tops out at 2mbps every time. I want to know how the other method gets around this limit. What other method? 
Does TableA and TableB have indexes on sys_id? If your dbms supports table clustering you could cluster your tables by the sys_id indexes. Table_B may not need clustering if that is the only column you are selecting and your dbms supports index-only queries
There are 85 indexes for TableA and 24 for TableB. What is clustering and how do I do that? 
I'm using Sybase IQ. There are no indices that start with SYS_id, but there are quite a few that end with SYS_id. These are other pks that I don't need to join.
So, merging from large to small as opposed to small to large can save time?
Thanks, this got me started on the path to the solution, have updated OP.
inorite?
I just never trust SQL server to make a smart decision when it comes to making execution plans. If table B has 600 million rows and table A has 130 million rows, I'll put table A as the joined table if I'm forcing a hint on the join. Especially a merge join. With the merge join hint, your telling SQL to sort the joined table to match the primary table on the column your joining on. You'd rather sort 130 million rows rather than 600 million rows. Sorting though is very costly. Using join hints is usually what I do in desperation when the SQL engine isn't choosing the best execution plan. My best recommendation would be to first check that both tables have primary keys and clustered indexes (these usually are the same). If sys_id isn't the clustered index in the table, ensure you have an index built on sys_id. If there is at least primary keys on the tables, some sort of clustered index and sys_id is indexed either clustered or non-clustered. The SQL engine will match up on indexes, then per those indexes will start pulling the data per the index match. If they are both non-clustered indexes its not optimal but you should see results quickly and steadily. If you can provide index information, that would help us help you.
When you say "no protection", do you mean no protection server side? Or could this page be vulnerable client side still (assuming this is the only user input on the site)? 
An attacker can submit any post they want to your server, no matter what you have on the page. Making it a number field in the html doesn't restrict them.
try `row_number()over(partition by name order by row rows unbounded preceding and current row)` edit: you would have to actually have a row number field to work on ofc .... oh only when the name changes I'm dumb.
For what it is worth, OPENQUERY does indeed execute the query on the remote server. To answer the original question, the last step of the import wizard allows you to save it as an integration services package so that it can be run again in the future. You can run these packages from a command line using the DTEXEC utility. This is likely the route you will want to take to transfer a large dataset from one server to another. In the future, you can also add change tracking and other integration services features to speed up the transfer. If SSIS is too much overhead, you may want to look at a utility like BCP. 
Simplest way would be to set your variable to `None` and insert that.
 with qry as ( select 1 AROW, 'Frank' NAME union all select 2, 'Frank' union all select 3, 'Mary' union all select 4, 'Victor' union all select 5, 'Frank' union all select 6, 'Frank' ) select a.AROW, a.NAME, sum(GRP) over (order by AROW) RESULT from ( select g.*, case when NAME = lag(NAME) over (order by AROW) then 0 else 1 end GRP from qry g) a order by a.AROW | AROW | NAME | RESULT | |------|--------|--------| | 1 | Frank | 1 | | 2 | Frank | 1 | | 3 | Mary | 2 | | 4 | Victor | 3 | | 5 | Frank | 4 | | 6 | Frank | 4 |
Ok that's kind of what I was thinking but I wanted to double check here :) 
Yes, yours is much more concise. I rarely use window functions other than `row_number()`.
Yeah, first time I ever encountered something like this. I simplified the data/query of course, but my basic issue was a what I am calling a "donut" issue. Where the "Name" repeats, stops and then repeats again later. I was doing a simple min/max row to see where the name starts and ends, but I was getting some overlap with the beginning and ending rows. Thanks again.
Usually your website would already have the LAMP stack installed, so you could always just go with MySQL. No idea regarding cpanel.
 select a.name, a.other_name, a.the_rank from ( select f1.name, f2.name as other_name, percent_rank()over(partition by name order by f2.value) as the_rank from foo f1 inner join foo f2 on f2.name = f1.name and f2.startDate &lt;= f1.startDate and f2.endDate &gt;= f1.endDate ) a where 1 = 1 and name = other_name edit: the more I think about this the more I don't think my solution quite makes sense, but you would need to do some sort of join against the table itself to create these ranges that you are looking for. This may point you in the right direction.
What is the purpose of this server? If you are just experimenting you can install SQL Server Express/Developer Edition on a workstation. You haven't described any particular use of the server, which is what will really determine a lot of the hardware requirements.
Oops, typo! Thanks. 
what are you counting?
I would like to count the no. of rows. I thought Count (*) would give me that. Am I mistaken?
If I'm not mistaken you can select info in a query, or a count, not both. If you really only want a count of the rows that are selected the statement should be: SELECT Count(*) AS Degrees FROM C_Record WHERE (((C_Record.[C#])=[Enter Value])) //Parameter Input from User GROUP BY C_Record.BunchofColumns; Again, correct me if I'm wrong, I only took a beginners course on SQL but the fact you're trying to select columns as well as a count of all columns doesn't work I think.
Nah, thts not it. See the [example #3] (http://www.techonthenet.com/sql/group_by.php) 
Yeah, but in this case your GROUP BY statement doesn't seems to be grouping anything. I'll try to explain: the reason why the count works like that in a GROUP BY statement is because you can count the rows that you have grouped together. But you're grouping the thing you are trying to select, thus it will count each group. Now because you have grouped this way each group will have 1 row in it, the count function then counts that, and give you "1" as output.
You're just going to get duplicates using your approach, is that what you want, if so, why?! Also, your current approach as a worst case scenario of 130 million x 600 million = 78 quadrillion (78,000,000,000,000,000) index scan / hash merge / row comparisons and rows in your temp table. Even if each took one millisecond, it could take around 22 hours and 1ms is being very generous, it'll probably be slower than that. Plus you'd run out of memory or disk space first. You have to be realistic here! I would stick to unique records, and as you are joining on Sys_ID, TableA.Sys_ID will be the same value as TableB.Sys_ID. As long as Sys_ID is indexed on both tables, this following query will be considerably faster, probably minutes rather than hours or days : select TableA.Sys_ID, TableA.State_Code, TableA.Zip_Code, TableA.Birth_Date, TableA.Gender, TableA.Source_code, TableA.Sys_ID into #testtable from DM.TableA where exists (select null from DM.TableB where TableA.Sys_ID = TableB.Sys_ID)
just because an AD group can't own a schema, doesn't mean that the users in that AD group can't query with schema references. A valid query source (table/view in the FROM/JOIN part of the statement) uses the following format: [[[linkedserver.]database.]schema.]table|view there is no *need* for database to be specified either... and in most cases I discourage it... schema is good for several reasons, but not necessary... but ANY level can be used by ANY user, as long as they have permission to the OBJECT (or, via chained permissions when that comes into play) edit: exception to above: schemabound objects (sprocs/views) can and MUST use the format schema.table|view... cannot reference linkedserver / database, cannot exclude schema... not that you're likely to run into them.
apparently it can: http://sqlblog.com/blogs/aaron_bertrand/archive/2009/10/11/bad-habits-to-kick-avoiding-the-schema-prefix.aspx 
That is your own fault then for letting it happen. 
No protection means I can change "number" to "text" and submit it and the server will be none the wiser if you just concatenate the pieces together. This shouldn't be a question except for the theoretical though, there is no good reason not to use parametric queries which remove the chance of SQL injection all together
making sure all references are schema-based is supposed to be one way that helps prevent that. I'm not really sure what you're getting at - re "my fault". Defensive coding by fully qualifying the object name prevents the wrong item from being accessed inadvertently. You're now suggesting that I forego the protection afforded by schemas AND accept additional responsibility for a mistake somewhere else in the code? 
You might be better off using something like Talend Open Source (or some other data tool) to just download the data and join it offline. Your limiting factors are server resources and connection speed. The only other way around it would be clustering (not sure if SybaseIQ even supports that. I do regularly perform operations like this on millions of rows in Pivotal Greenplum without much issue... but we have a massively clustered system. One last option might be to create a recursive function or recursive with clause (not sure if supported in IQ) that handles one row at a time. It is going to take longer, but it wont lock up the server.
Thanks for clarifying how this scenario works. I like how discussions like this can turn into an ethical debate about whether it's ever right to code a certain way! How about if I'm only using php with xampp locally and have no intention of letting others use or see my code? If there's no one around, does the pope shit in the woods?
Yeah, super edge cases that very rarely happen.
You're going to invest multiple thousands of dollars in building a physical SQL Server box and you know literally nothing about the product right now. "A market research team" doesn't even *begin* to describe what you're going to be tasking SQL Server with (the workload), so no one can even attempt recommendations here. You're going to make huge mistakes (we all do sometimes, but we take steps to mitigate the damage). Many of them. In your database, with your configuration and with your hardware selection. You're going to pick the wrong hardware because you don't know what your database really needs, and then have to buy it all over again and restart. You're going to set up your server and it'll be a security nightmare. You'll set up the database and it'll be all wrong. If you really want to make yourself crazy with hardware selection read [Glenn Berry's blog](http://www.sqlskills.com/blogs/glenn/). **But**, since you don't know what your server's requirements are going to be yet, you're just going to be guessing anyway. You need to do a lot of learning before you get to the point where you're buying or implementing *anything*. You need an expert to do this for you the first time, and you need to get yourself major training if you're doing it alone once it's up and running.
Sorry I don't know access but in raw SQL assuming a single column named "foo" in table named bar: SELECT foo, count(*) FROM bar GROUP BY foo; Edit: if it were me I'd actually write it like this: SELECT UPPER(TRIM(foo)), count(*) FROM bar GROUP BY 1; In order to trim white space and coerce to upper case so as to make the grouping case insensitive. 
Awesome shut_up_birds! That first SQL line worked perfectly. The second is a no go because MS Access (T-SQL?) requires UCASE() in place up UPPER(), and another reason I can't figure out. But hey, your first line did the trick. Thanks a lot man :D
 SELECT wordColumn, COUNT(*) FROM tableName GROUP BY wordColumn ORDER BY wordColumn asc If you want to restrict your results to words with a certain count, use a having clause to perform a check on aggregate data. SELECT wordColumn, COUNT(*) FROM tableName GROUP BY wordColumn HAVING COUNT(*) &gt; 20 ORDER BY wordColumn asc If you want to restrict your results to certain words or words... SELECT wordColumn, COUNT(*) FROM tableName WHERE wordColumn in ('gerp','whop','foo','woof') GROUP BY wordColumn ORDER BY wordColumn asc
Whenever it's a problem finding duplicate values. Usually you're looking for a HAVING clause in order to perform a COUNT on grouped data. Looks like a lot of responses on here already covered this. If you haven't solved it yet, i'll look more into it. 
Thanks for this. I need some quiet thinking time (which I don't have right now) to ponder your code. I agree that a join on itself feels like the way to go.
One of the speakers at a conference I was at last year said they saw a client system with well over 100 indexes on one table.
the data is from sharepoint and I'm not sure if report builder supports functionality like that. 
Is this SQL Server Reporting Services (SSRS)? I think the syntax for the expression would be something like: =Fields!FirstDate.Value &amp; "-" &amp; Fields!SecondDate.Value
yes this is SSRS. Give me a second to see if that works. 
A pivot like this should turn your data into something you can use in the report: SELECT ID, [Green Light], [Yellow Light], [Red Light] FROM YourTable PIVOT (MAX([DateOfStatusChange]) FOR StatusChange IN ([Green Light], [Yellow Light], [Red Light])) pvt But if they can have more than one date per status, you'll only be getting the most recent with that...
none taken and yeah it is true i don't know anything about it but im on the learning curve and im investing full time in this. this question was given to me by a company that i was applying for and i was confused and i told them i know nothing about MS SQL. and they said that is okay
I just came here to see what the problem was but I just want to say you have an excellent manner and way of explaining things in very straightforward, concise, and engaging fashion.
Thanks, that means a lot! :D
You also stated you aren't where you are really going to delve into SQL. r/SQL probably inst the best place to post.
I can't believe no one else mentioned this. If you use SELECT INTO #temp FROM blah blah blah Rather than CREATE TABLE #temp (blah datatype, blah datatype, blah datatype) INSERT INTO #temp SELECT blah blah blah If you do this, you're gonna have a bad time. *** *When* you do a SELECT INTO, SQL has to first scan your *entire* query and store *all* of the field names you want into some kind of memory/holding area. *Then* it has to look at *all* of the data sources. And *then* it has to look at eeeeeevery possible value, for every field you're calling, in every table, and make a determination - is this an INT? Are we sure about that? Is it a BIGINT maybe? Could it be a FLOAT? What if it's a VARCHAR and the first few values happen to be numbers. Not to mention, it's going to make shitty decisions based on its assessment. Zip codes. Meh. That's just an INT, right? Don't worry about those wacky east coast ones with 0's in front we can just trim the zero off and it'll be fine. Phone numbers? Social Security numbers? BIGINT, for sure. Dude. I'm sure that there are indexes you can drop and create and other performance enhancing tricks you can do but first things first. Get rid of that SELECT INTO I've seen a lot of DBAs spank a lot of developers for that exact type of code. And I'd wager you'll get at least a *moderate* performance increase once you do it.
Also, if you have 85 indexes (seriously?) I have to ask, how often are you resetting statistics. And with 85 indexes .. I mean, you hear about how hash collisions are theoretically plausible. I wonder if you've inadvertently manifested a case study. But it's possible your reads are slower because of this. Do you have any conflicting indexes that might sort the same field both ascending and descending for example? If so, does your query plan attempt to use both? Because that's just a loooong walk around the block to recreate what is simply a non indexed heap.
 select * from other_industries where company_name != 'Apple';
That's nice when you don't want some complicated composite PK and would rather just filter the data the query returns (speaking of null files specificially). I'll be using this. Thank you.
Its nice when you're working with transactional web logs that don't have PK's and query fields that are unparsed. 
Oracle has several ways to do this with its regexp_like, translate and regexp_replace functions. 
Did you really mean to put OR in there? It seems to work better with AND. 
Can you explain more about this magical where exists? What sort of kungfu shit you're doing here?
This was such a pain in the ass for me to find, I started off trying things like, "SQL NOT LIKE %", etc. I ended up finding the NOT LIKE integer piece and then just played with it to get it to NOT LIKE %. 
You might also be able to use isnumeric(id) = 1 (all #s) or isnumeric(id) = 0 (some value ain't #). I use it all the time to filter out bad data from our client/matter tables - 12345-0011, looks good. 12A12-00C4, how...how did that get in there?
I'll test this out in place and see if the results are the same. Thanks.
Thank you so much. I learned something this morning. 
Doesn't make sense to me. If one of these is enough, no need for the second one.
No, either an AND or an OR would function to achieve the same results. If it were a single line you could write: ID NOT LIKE '%[%]%[^0-9]%[%]%' It was just sample code. 
I can't help you with the product searching, but I can however answer your last question. You are completely right, you need a 3rd party to achieve this, as no biometric authentications are natively aupported by MSSQL. 
Yes it's correct, you added an attribute (quantity) to the relation that link product and inventory. A couple of months ago I worked with an ecommerce platform and the relation product warehouse was modeled in the same way.
You could maybe prevent a theft of the fingerprint database from ruining everything, but you can't prevent the problem /u/sqlburn described - if somebody gets the fingerprint off the admin, it can't be revoked. Fingerprints are like usernames, not passwords.
Is this for improved security, or simplicity? As others have pointed out, if that database were compromised, you'd be in a world of hurt because fingerprints can't be changed. IMHO, fingerprints should only be used as a portion of a multi-factor authentication scheme - print plus password or time-based token.
This is.... wrong. Authenticate against the domain. set up policy requiring whatever password or device you want for your admin group... and authenticate against the sql server with active directory/windows authentication... 
I searched report builder, the only hits were in SQL and SQLserver
If you mean by that, replace all ' (single quote) for '' (two single quotes following each other) then yes, if you mean anything could you please elaborate ?
This could be an option, but do you know if its possible to make a view by just pasting the select query that outputs XML by using FOR XML ? in SSMS when I click views &gt; new view I have to start specify what tables I want to use and what not, making this alot of handwork again.
Fingerprints should be used as usernames, not passwords. 
What are you trying to do? Recursive SELECT statements against a single table? I recently had to do some tricky joins using subqueries. If you can give a little for info I may be able to help.
Quite often I'll use a subquery in an UPDATE statement. UPDATE SomeTable SET Field1 = 77 WHERE UID in (SELECT UID FROM SomeTable WHERE SomeValue = 88) More example UPDATE SomeTable SET Field1 = (SELECT sum(Amount) FROM Table1 WHERE Year = 2015) WHERE SomeValue = 99 UPDATE SomeTable SET Field1 = ((SELECT sum(Amount) FROM Table1 WHERE Year = 2015) - (SELECT sum(Amount) FROM Table1 WHERE Year = 2014)) WHERE SomeValue = 66 Here's a small piece of production code to give you a better idea: UPDATE SalesRollingForcast SET Total = (SELECT sum(GrossMargin) as GrossMargin FROM Budget WHERE Code = @Code and BudgetYear = @CurrentYear) WHERE Code = @Code AND Year = @CurrentYear AND Label = 'GrossMargin'
Thank you, I think I have read your comments elsewhere in this sub. You seem to be very active here, unless I am confusing the username with another one. Would you be able to simplify the query located here https://www.reddit.com/r/SQL/comments/3ice80/i_challenge_you_to_simplify_this_query/ using a derived table? I would like to get rid of the intersect basically. Thanks!
Challenge accepted :- select table1.column1 from table1 join table2 on table1.column2 = table2.column2 join table3 on table3.column2 = table2.column2 join table4 on table4.column3 = table3.column3 where table1.column5 = 2 and table4.column6 = 10 intersect select table1.column1 from table1 join table2 on table1.column2 = table2.column2 join table3 on table3.column2 = table2.column2 join table4 on table4.column3 = table3.column3 where table1.column5 = 2 and table4.column6 = 3 ; What's my prize?
 ;WITH [Table] AS ( select table1.column1, table1.column5, table4.column6 from table1 join table2 on table1.column2 = table2.column2 join table3 on table3.column2 = table2.column2 join table4 on table4.column3 = table3.column3 join table5 on table5.column4 = table4.column4 ) select column1 from [Table] where column5=2 AND column6=10 INSERSECT select column1 from [Table] where column5=2 AND column6=3
Awesome, thank you, I will test it. Can you do it without the intersect?
 select table1.column1 from table1 join table2 on table1.column2 = table2.column2 join table3 on table3.column2 = table2.column2 join table4 on table4.column3 = table3.column3 join table5 on table5.column4 = table4.column4 where table1.column5 = 2 and table4.column6 in (10,3) group by table1.column1 having count( distinct table4.column6) &gt; 1 
I'm going to test this but I think this one might be a winner, thanks!
yup, this was what i had in mind when i asked you about uniqueness because without the uniqueness, using GROUP BY would collapse dupes, which then of course would not be an equivalent solution
the reason you're getting an error is because mysql doesn't support TOP grab your mysql manual and look up LIMIT
I substituted for the real column names and ran it. It's works perfectly, yields same exact results and does it 90 seconds faster! Thanks!
You have to write your own scripts to do a log backup.
Okay, thanks. Not sure why there were Google results for "sql TOP" I must not have been paying attention to what DB the code was actually for. 
in MS SQL intersect will also remove duplicates, so in MSSQL case 'group by' should be equivalent.
After substituting for real table and column names I ran the script. It gave an error on line 3 that stated table1.column5 was an invalid identifier, I removed it and ran it again then it stated that on line 3, table4.column6 was an invalid identifier. I removed that identifier and then it stated that line 12 column6 was an invalid identifier. I'm not sure what the issue is, but I think your on to something here.
If those are the actual names of your columns, try putting brackets around it to see if that fixes the issue.
 select table1.column1 from table1 join table3 on table3.column2 = table1.column2 join table4 as a on a.column3 = table3.column3 join table4 as b on b.column3 = table3.column3 where table1.column5 = 2 and a.column6 = 10 and b.column6 = 3
This will outperform all of the other solutions on here, what's my prize? select t1.column1 from table1 t1 where t1.column5 = 2 and exists (select null from table2 t2 join table3 t3 on t2.column2 = t3.column2 join table4 t4 on t4.column3 = t3.column3 and t4.column6 = 10 join table4 s4 on s4.column3 = t3.column3 and s4.column6 = 3 join table5 t5 on t5.column4 = t4.column4 and t5.column4 = s4.column4 where t1.column2 = t2.column2) EDIT : Fix for table4.column6 predicate 
Yea, I'm still learning. Luckily our DBs are huge, but spread out. This customer's lots table is by far the biggest we have that I've seen so far.
Well spotted!
It does. Based on the lack of information about the tables, I made some assumptions. I assumed that joins were on not null foreign keys making extraneous joins unnecessary. If I had access to the tables, I would check of course.
Yeah I made the same assumption then realised it probably wouldn't work.
Thank you, I will test it out tomorrow morning and get back to you.
I tidied it up with the aliases. I'm kidding about prizes, it's a noble gesture but most of us just like the challenge. I might try another approach so watch this thread.
Another couple of solutions : select t1.column1 from table1 t1 where t1.column5 = 2 and exists (select null from table2 t2 join table3 t3 on t2.column2 = t3.column2 where t2.column2 = t1.column2 and 2 = (select count(distinct t4.column6) from table4 t4 join table5 t5 on t5.column4 = t4.column4 where t4.column3 = t3.column3 and t4.column6 in (10, 3))) And.... select t1.column1 from table1 t1 where t1.column5 = 2 and exists (select null from table2 t2 join table3 t3 on t2.column2 = t3.column2 where t2.column2 = t1.column2 and exists (select null from table4 t4 join table5 t5 on t5.column4 = t4.column4 where t4.column3 = t3.column3 and t4.column6 = 10) and exists (select null from table4 t4 join table5 t5 on t5.column4 = t4.column4 where t4.column3 = t3.column3 and t4.column6 = 3))
What I said about using `TOP` w/o `ORDER BY` is true for `LIMIT`, BTW.
I am not into scripting, would able to learn it. I wanted to know if someone had a ready script that I could modify as per my needs. 
assuming you want 1 line when you only have QVENDORADDRESS1 (you never actually say what your problem is / what you want) then here's a solution using CASE (there's probably also a quick way to do it just using ISNULL but I'm tired and this should work: &amp;nbsp; SELECT CASE when QVENDORATTNCO is not null then QVENDORATTNCO+CHAR(10)+ISNULL(QVENDORADDRESS1,'') else ISNULL(QVENDORADDRESS1,'') end as Address FROM CONTACTS WHERE CONTACTCLASS = 'Vendor - ACCTG USE ONLY' AND Status='Active' &amp;nbsp; You might want to make it more complicated if there's a chance qvendoraddress1 could be null too. 
The CONCAT function replaces nulls with empty strings. Try this: SELECT CONCAT(QVENDORATTNCO, CHAR(10), QVENDORADDRESS1) AS Address FROM CONTACTS WHERE CONTACTCLASS = 'Vendor - ACCTG USE ONLY' AND Status = 'Active' I'm not entirely sure what your problem is. Let me know if this is what you are looking for. EDIT: I just saw that this was for SQL 2008 which doesn't have the CONCAT function. You can use COALESCE instead: SELECT COALESCE(QVENDORATTNCO + CHAR(10), '') + COALESCE(QVENDORADDRESS1, '') AS Address FROM CONTACTS WHERE CONTACTCLASS = 'Vendor - ACCTG USE ONLY' AND Status = 'Active'
You get an upvote from me because I have been "the guy that knows a bit about databases" in my last two roles after they have already been up and running at a production level for years. I have spent 5 years digging myself out of an ever deepening hole because they didn't hire a DBA to begin with.
That's really cool of you. The original query was a challenge for me lol. Thanks for all your help!
Actually, any of the fields could be NULL. We are still trying to clean up all the addresses.
The table names and column names used here on reddit are generic.
&gt; where t1.column2 = t2.column2 It is!
Yes it is! LOL that shows how weak my SQL skills are, I really wish I could get this one to work, I'm going to move onto testing your other ones for now. Thanks! 
Which line is the error referring to?
OMG, I'm sorry. I'm not in the right schema. Give me a few, I'm going to retest everything.
Create a sample table and dump some junk data in. Are you saying you want to get a count for each quarter over a two year period? It should be as simple as something like: SELECT DATEPART(qq, datefield) AS 'Quarter' , DATEPART(yy,datefield) AS 'Year' , COUNT(DISTINCT id) AS 'Count' FROM X WHERE datefield BETWEEN '2013-07-01' AND '2015-06-30' GROUP BY DATEPART(qq, datefield) , DATEPART(yy,datefield) ORDER BY DATEPART(yy,datefield) , DATEPART(qq, datefield) 
Unindexed columns
Which DB?!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
Unless you specifically set it otherwise, concatenating 2 strings where one of them is NULL will yield NULL. You can take advantage of this by moving the "+CHAR(10)" inside the ISNULL check. If QVENDORATTNCO is NULL then concatenating it will CHAR(10) will still return NULL and the entire string will be replaced with ''. If you want to be explicit and make sure that no joker set NULL concatenation to OFF, set it before the SELECT statement. SET CONCAT_NULL_YIELDS_NULL ON; SELECT ISNULL(QVENDORATTNCO+CHAR(10),'')+ISNULL(QVENDORADDRESS1,'') FROM CONTACTS WHERE CONTACTCLASS = 'Vendor - ACCTG USE ONLY' AND Status='Active';
I have used something similar to this but it is counting distinct just id's in that particular quarter. I need each quarter to represent a rolling two years of distinct id's
I see, bummer. I was having fun with this. Thanks for participating, it was fun! :)
Also fixed the first one, try that again, too
That's an evil request for a relational database to do. A cube would handle it better. Sorry... WITH cteRanges AS ( SELECT CONVERT(date,DATEADD(QUARTER,DATEDIFF(QUARTER,0,GETDATE()),0)) AS StartDate, CONVERT(date,DATEADD(QUARTER,DATEDIFF(QUARTER,0,GETDATE()) + 1,0)) AS EndDate, 1 AS Itteration UNION ALL SELECT DATEADD(QUARTER,-1,StartDate) AS StartDate, DATEADD(QUARTER,-1,StartDate) AS EndDate, Itteration + 1 AS Itteration FROM cteRanges WHERE Itteration &lt; 8) SELECT StartDate, EndDate, (SELECT COUNT (DISTINCT ID) FROM Table WHERE Date &gt;= r.StartDate AND Date &lt; r.EndDate) AS DistinctCount FROM cteRanges AS r; edit: I can imagine another way to do it, but it's quite ugly and I'm not sure it would be any better.
The only other way I can think of to do it would be to union multiple segments together. I dig your method though.
You always have to be careful to include your DBMS when Googling. Otherwise you usually just get the MSDN page from SQL Server.
cancelled at 4 minutes, I'm going to look into how the columns and tables are indexed.
edw = enterprise data warehouse i'm guessing the other one is for oltp
This is called "record linkage". That should help in Googling. I think sql server has a fuzzy join functionality. You may be able to use that if you're clever. 
These may help : [Finding Similar Rows](http://www.codeproject.com/Articles/610103/FindingplusSimilarplusRowsplusInplusSQL) [Fuzzy Lookup and Grouping](https://msdn.microsoft.com/nb-no/library/ms345128.aspx) [String Differences Amount](https://msdn.microsoft.com/en-us/library/ms188753.aspx) 
If you use OPEN QUERY in your CTE, you are PULLING data as you are executing the query on destination server and pulling data from source server. While executing queries directly and receiving data directly in your way using OPEN QUERY is not bad, there are implications when trying to join the results of OPEN QUERY with a local table or another source's table - mainly that your local server cannot do proper query cost analysis to get the most efficient execution. So, typically joining remote tables with other remote tables / local tables across the network is not the most efficient method of querying. In some cases (and yours may be one of them) it may be better to subscribe to a remote table by importing it locally, then joining it locally. Alternatively, you may benefit from using **distributed queries**, as your data is on multiple servers, but you'll have to run some metrics to test this as it will be data dependent. [Here's an article](http://blogs.msdn.com/b/sqlsakthi/archive/2011/05/09/best-performer-distributed-query-four-part-or-openquery-when-executing-linked-server-queries-in-sql-server.aspx) on the two methods 
SQL Challenge = *"Can you help me rewrite this to make it faster?"* select table1.column1 from table1 join table2 on table1.column2 = table2.column2 join table3 on table2.column2 = table3.column2 join table4 on table3.column3 = table4.column3 where table1.column5 = 2 and table4.column7 = 2 and exists (select null from table5 where table4.column4 = table5.column4 and table5.column6 = '000') and exists (select null from table5 where table4.column4 = table5.column4 and table5.column6 not in ('000','005'))
lol, your rewording of my challenge into a question is pretty accurate :) sssshhhhhh don't tell.
Would this work if I need a table1.column1 number that has a row with a table5.column6 equal to 000 and the same table1.column1 with a row that is not equal to 000 or 005?
That's what both solutions are doing.
Ok, awesome. Thank you, I will test both of these soon.
Good point, I'll bear that in mind
Removed the second and on line 9, assuming it was a typo. After substituting table and column names an error was returned stating "invalid number". ORA-01722 I'm not exactly sure what the counts are doing. I just want one table1.column1 number that has one table5.column6 that is equal to '000' and one table5.column6 value that is not equal to '000' or '005' for the same table1.column1 number. Is that what the counts are accomplishing?
lol yeah, typo on my part again but I fixed it. Running /u/ichp right now but it's already at about 8 minutes and I think the original took around 4. Going to cancel and run yours now.
I'm not even sure if I would have the authority to index something and I'm working on a similar DB in regards to the amount of data. I would think that they would have things figured out given how large this company is, I'll have to ask some questions around here.
This one returned in 1 minute 45 seconds!!!!!!!!!!!!!!!!!!!!!!!!!!!! way faster than the original, testing the data now.
I can't connect to the site right now, do you have to upload a backup of your database, or just the DDL? Are table schemas covered by HIPAA? There should be no identifiable information in DDL statements - just a description of the table structure.
You could also import into the database as a temporary table. 
For us we apply some encryption to our regulated data, then pass it off to AWS where we have a lot more flexibility of what we can do with it. Due to the way we do the encryption we can still get lots of really useful analysis out of the encrypted data without ever decrypting it in the non-regulated environment. Turning over the schema wouldn't be enough in itself to get into that data, but it'd put someone one step closer.
What would I need to download in order to do that? Actually, to simplify things, what's the simplest version of SQL I should download? Thanks
I wasn't really thinking of that but it makes perfect sense. However it did speed up over all run time. The query standalone was 2 minutes faster, but as a sub query to a larger query, the whole operation was only 1.5 minutes faster. My whole goal is just to build up my tools. I'm pretty reliant on intersect and I would like to get out of doing that. Decreasing run time just makes this whole thing a little more fun. 
I assumed you had a SQL server instance already. My bad for making that assumption. If you decide to go this route I think you will want to get SQL server Developer Edition. 
I would probably try this (Windows): http://dev.mysql.com/downloads/windows/
Blowing the spool space suggests you have one or more of the following : * An inefficient query * Tables where statistics have not been properly collected * Tables where the primary index was poorly chosen (high skew). The only way to know for certain is to look at the explain plan for your query. Post it up here. NOT IN is a right anti semi join, this has set equivalence to NOT EXISTS (with a correlation) if the query column is not nullable. NOT EXISTS will outperform NOT IN in most situations if the relevant columns are indexed correctly and will require less spool space accordingly.
If you have MS office installed you could always import it in Access and write some "SQL" in that. Although it might shit itself when confronted with 450k records.
Doesn't make sense, sorry.
 table1.column1 ----&gt; unique_alias1-----&gt;unique_table5.column6 unique_alias1-----&gt;unique_table5.column6 
I would never use a tool that STEALS my code like this one does.
YES, read the other posts.
thats just syntax, and the usual "50 interview questions about SQL" crap. If you have someone that knows about database programming on the other side of the table, you are in deep shit with that aproach. I'd honestly go with the "I know to not know much but I started to get into it, and I like it this far" approach. That I'd hire over the memorized questions guy any day of the week.
I didn't say memorize answers. Those absolutely are the things people ask, especially when the bar is low (SQL "preferred")
Oh I didnt want to insinuate that you did. My point is more along the lines that one can't really "learn SQL" inside of a couple of weeks, even months. So I'd rather advice to keep reading up on stuff and being upfront about not knowing much, *yet*. The only way of learning the SQL / the mindset needed, that I know of is the practical application of it. I really do not know a single person that learned database programming from a book or a tutorial. You get the syntax of those, but not the mindset.
Yeah it might be better to present yourself as data savvy than "I learned a tiny bit of SQL and I'm a quick learner." After all, it's not like anyone has ever told an employer they were a slow learner. 
PostgreSQL is open source and supports window functions, check the source code out [here](https://github.com/postgres/postgres).
The solution is so easy. Add a column called IS_REDDITOR varchar2 (1) as a YN flag and limit your results to "Y" in the where clause. 
Haha. If this comment was in a higher traffic subreddit it would have a 1000 upvotes. Lol
It's all about mastering INNER vs LEFT JOIN, know the difference and what they do. The rest isn't that tough.
The correct way of doing what you're describing would be to use the partitioning features of enterprise edition and then it gets a whole lot easier. So you're left with the choice of spending development hours coding your own solution that will work well enough, or pay for enterprise that just works.
*Bit column
Yes, you can do this with synonyms, it's what they are for. Synonym = something known as something else. We do exactly this all the time, pointing synonyms to different schemas depending on context. 
But can I use synonyms without declaring the table? I use 30ish tables. Is there a way to do select from [syn].tablename?
Check out books by Itzik Ben-Gan. Read anything he wrote online. But honestly, nothing is going to really truly prepare you until you do it. Be honest in the application and interview. Tell them you are learning and are a fast learner. 
This worked out well, thanks to both of you. Quick followup question. Is it possible to do something like this? declare @db_date varchar(10) set @db_date = '201501131' :SETVAR dbName "ues_" + @db_date
Doing Code Academy now... I think it's better than sqlbolt which is what I was doing before.
I'd try to help you, but there's still no context or enough data to see associations or results you want. It's getting less clear what you're trying to do. Two rows, clarifies nothing.
I'll give it one more shot :) Again, I apologize for a poor description. I wan't one table1.column1 value that has a value of 2 for the table1.column5 value and is related to two table1.column1 values that have a value of 17 for the table1.column5 value and both of those values have different values for table5.column6. select table1.column1 from table1 join table2 on table1.column2 = table2.column2 join table3 on table3.column2 = table2.column2 join table4 on table4.column3 = table3.column3 join table5 on table5.column4 = table4.column4 where table1.column5 = 2 and table4.column7 = 2 and group by table1.column1 having count(case table5.column6 when '000' then 1 end) &gt;=1 and count( case when table5.column6 not in ('000','005') then 1 end) &gt;=1
would you like me to give you some questions to be answered on the adventure db to test that assumtion? I can ensure you, I can come up with things that are everything but easy to solve.
No, that's the purpose of Bit, its a true/false field.
I'm generally aware of what's efficient. Our DBA's don't deserve more, believe me.
I know, I know, but varchar(1) is wrong on many many levels. Variable length with max length of 1 and the overhead of variable length, no matter what the purpose, varchar(1) is wrong. This being a boolean makes it more wrong no question about it, but anything you save as a varchar(1) is wrong.
I tell you what, you write it as an intersect query and I'll work from that, because if I'm honest you're not very good at describing your problem.
No you'd do select * from synonym_for_table You can have phantom synonyms on some DB platforms, for example on Oracle you can have a synonym to a table and then drop the table. The synonym can still exist, but points to a "phantom" object. But I've never tried it on SQL Server. In any case, I would create a dummy table or view as a place holder with the correct column names and types and point the synonym to that until you have a proper object.
What do you have so far?
SELECT ONO, SUM(PRICE * QTY) FROM ODETAILS, ORDERS, PARTS WHERE ODETAILS.ONO = ORDERS.ONO AND ORDERS.ONO = PARTS.ONO AND SHIPPED IS NULL I know that its somewhere after "WHERE" it isn't correct, but i dont know how to fix it exactly 
What is the key for odetails?
ONO and PNO
Does that answer your question?
S'ok. Your code doesn't join odetails to parts, unless that's just a typo.
You wrote `orders.ono = parts.ono`, but parts does not have a column `ono`. Change `orders.ono` to `odetails.pno` (edit: and parts.ono to parts.pno obviously, since it exists). Better yet try running this, and use the join syntax instead of putting the join conditions in your where clause. SELECT ONO, SUM(PRICE * QTY) FROM ODETAILS inner join ORDERS on orders.ono = ODETAILS.ono inner join PARTS on PARTS.pno = ODETAILS.pno WHERE SHIPPED IS NULL
It spits a error out with: "Column 'ONO' in field list is ambiguous"
Replace with orders.ono
Not sure about "automated report gen", but performing async exports over a query and popping out some formatted files is common and simple. Here's an example workflow in RoR: 1. Push "Export" job onto a queue (use [active_job](http://edgeguides.rubyonrails.org/active_job_basics.html) now, with your preferred async framework e.g. [sidekiq](https://github.com/mperham/sidekiq)). Have a model that encapsulates the export properties (what to export, what format, who to notify etc). 2. When it runs, kick off the export in your model and execute the query (high-level [activerecord](http://guides.rubyonrails.org/active_record_basics.html)/[arel](https://github.com/rails/arel) or a database function/sql), iterate across results and spit out a file ([CSV](http://ruby-doc.org/stdlib-2.1.6/libdoc/csv/rdoc/index.html) is part of stdlib, [nokogiri](https://github.com/sparklemotion/nokogiri) for XML, [JSON](http://ruby-doc.org/stdlib-2.1.6/libdoc/json/rdoc/index.html) in stdlib plus some [rails helpers](http://apidock.com/rails/v4.2.1/ActiveModel/Serializers/JSON), XLS or other "office" formats I would seriously recommend jruby and talk to [apache-poi](https://poi.apache.org/) jars). 3. Update whatever model you encapsulate the export info with the filenames, status, times, other meta. 4. Maybe push another job onto queue to email interested parties that job is done (or use something like [actioncable](https://github.com/rails/actioncable) or some other pub-sub messaging framework to update a UI) I am sure there are "query-&gt;report file" higher level helpers, but if you're an intermediate programmer queries/results/aggregation/file formats/queueing should be in your arsenal at an interview rather than "I would just use super-format-report-gem".
ok, sorry about the code not displaying correctly, I thought that i formatted it right, anyways this is the updateCommand sql statement UpdateCommand="Update [Course Faculty] SET MEID=@MEID, [First Name]=@[First Name], [LastName] = @[Last Name], [Major ID] =@[Major ID], [Email Address] = @[Email Address], [Phone Number]=@[Phone Number]"&gt;/asp:SqlDataSource
Like this? UPDATE my_table SET new_column = 'DUPLICATE' WHERE my_column IN (SELECT my_column FROM my_table GROUP BY my_column HAVING COUNT(*) &gt; 1) EDIT: Fixing small issue, didn't actually try the code just wrote it off the top of my head.
And being a Stack Exchange user, I'll give another method here (upvotes for the answers you like best!). I find it easier to follow the logic with CTEs and analytic functions, but if you're on MySQL you may need to do it this way: UPDATE m SET duplicate_flag = 'omg dupe' FROM my_table m INNER JOIN my_table d ON m.duplicate_column = d.duplicate_column AND m.my_row_id &gt; d.my_row_id;
When this comes up, I do updates in waves. Start with the strongest matches (join on first/last/ssn/address/zip) and merge those first. Then slowly loosen the restrictions with each successive update (first/last/ssn/zip, then first/last/ssn, first/last/address/zip, first/last/address) until you start getting false positives.
I've written a ton of these scripts. Ruby has a CSV library. You want to use roo for xlsx files. roo-xls for xls files. Nokogiri is extremely powerful for XML and other markup languages. As for dumping into a database you're going to want to use activerecord. it's an ORM(object relational mapping). Study up on these gems and see what you can do with them.
I don't see the word 'nvarchar' in that statement. Are you sure that statement is causing the error? Is your program sending something else to the server? If you are running the usual Visual Studio plus local SQL Express instance, SQL Profiler is a good way to watch what your program sends to the server. Something else to check: Is there an UPDATE trigger on your table? A problem in there might masquerade as a problem with the update statement. Lastly, it bugs me that there is no WHERE clause in that UPDATE statement. Does the grid automatically build a WHERE clause for you? If so, something that the grid builds might be causing the syntax error. Maybe you don't have the grid configured 100% correctly?
&gt; from July to November, August to December, etc. i'm going to guess you're using MySQL (note -- please see sidebar) UPDATE table_name SET column_name = column_name + INTERVAL 4 MONTH 
Please stop posting basic tutorials to this subreddit. Please read the sidebar.
Comparing NoSQL and SQL databases is like comparing tractor unit to sport car. You can say "sport cars are useless, Porsche 911 can't take 20 tons of carry" or "tractor units are crap, my sport car have better acceleration", but it's pointless. **NoSQL databases have other use cases than SQL databases**.
The statement you have written is very close to what you want to do but Instead if SELECT REPLACE(COLUMN_NAME,'2015-07', '2015-11') DATES FROM TABLE_NAME you want to do UPDATE TABLE_NAME SET COLUMN_NAME=REPLACE(COLUMN_NAME, '2015-07', '2015-11') FROM TABLE_NAME
using string functions to update a date column is a miserably ugly hack if it even works just sayin'
Yes you are correct but he asked for string replacement even though it is a date I still did it for strings if it was a date I would have used dateadd() function instead
If Apache Drill is wrong, I don't want to be right.
 SELECT Licence_ID , Licence_Issued , Licence_holder , (SELECT MAX(Fee_Date) FROM Table2 WHERE Table2.Licence_ID = Table1.Licence_ID ) AS Fee_Date FROM Table1 ORDER BY Fee_Date DESC; For each record in Table1 you look up the maximum Fee_date joining on Licence_ID in the inline sub-query.
 select t.Licence_id, t.Licence_issued, t.Licence_holder, m.Max_Fee_Date from Table_1 t left join (select Licence_id, max(Fee_Date) as Max_Fee_Date from Table_2 group by Licence_id) m on m.Licence_id = t.Licence_id 
Brilliant! Such an elegant solution. I think I was confusing myself by trying to bury the MAX() function several layers down in a WHERE clause. Thank you so much.
This also works! It's nice to see multiple solutions to the same problem. Really makes me examine them more closely to understand the inner workings. Thank-you.
Thanks, I have to just do specific dates to fix someone's input errors.
`LATERAL` (or, in MSSQL, `CROSS/OUTER APPLY`) is one of those newish features that just makes me go "How did I ever live without this!?" What it's really doing is joining a table-valued function to the result set, so it has a lot of versatile uses outside of this. One very convenient thing is when you have long annoying formulas that get used several times, you can do a LATERAL ( SELECT &lt;long formula 1&gt; AS f1 , &lt;long formula 2&gt; AS f2 , &lt;long formula 3&gt; AS f3 , &lt;long formula 4&gt; AS f4 ) AS l Then, in your `SELECT`, you can just refer to `l.f1`, making your code much easier to read.
This actually worked best for me. Thank you very much!
I know you didn't ask this question, but in case you (or anyone else) ever ends up in a situation where you need the whole row (and not just one column) this is my favorite solution: http://stackoverflow.com/questions/121387/fetch-the-row-which-has-the-max-value-for-a-column/123481#123481
Time to set up a presentation tool, I guess.
Will the existing DB be receiving updates in the meantime? What sort of replication are you planning? Regardless, after setting up your internal DB you'd then need to set up the subscribers using a replication snapshot I'm no expert since I haven't used SQL Server in a good few years now, but I don't think your idea can be done without some downtime on the subscriber 
I'll look into PIVOT, but I think the stored procedure is probably the lowest-maintenance solution for me. Thanks!
I work at a midsized collection firm. We have locations in 5 states and have a central It department. Seems like they are always behind an really busy. I don't work in the IT department but told them I'm interested in learning. This was 6 months ago and I've been learning Sql since then and have become a huge resource for random reports from managers when IT is busy. ITS BEEN GREAT! 
Work for a mortgage servicer that is heavily reliant on us. I work on a dedicated team of analysts that all technically work for the same team, but we all support different departments. Allows us to have centralized standards on some things.
F100, we use SAS, TERADATA SQL assistant, Tableau, and yea the almighty Excel
System centralization and having a unified data management strategy, a la source systems =&gt; Datamarts =&gt; Data Warehouse, plus institutional(tribal) and documented knowledge of schemas and layouts is really the only way to not have conflicting numbers. If people are trying to get the same numbers in different ways, from different sources, they're never going to mesh.
&gt; - Aurora is compatible with MySQL 5.6. That means the applications, tools, and drivers you’re already using with MySQL can be used. - You can launch a new Aurora database from an RDS MySQL DB snapshot. - 5x increase in performance when compared to MySQL. If you work with a lot of MySQL I could see this being really useful for the increased performance.
I am getting 96k, from 14k per 2 core license times 3 servers with 6 cores each. I did speak with the developer and he thinks they are talking about SQL users, so he said we would be fine with 2014 Standard with 1 CAL.
Row index = ID? I'm not sure why you would need to use LIMIT instead of just specifying your row ID in the WHERE. Nothing springs to mind as being more efficient than issuing separate queries. Are you using the same connection?
If your servers have 6 cores, you have to license for 6 cores - it's licensed by the [# of cores presented to the OS](http://blogs.technet.com/b/uspartner_ts2team/archive/2011/11/30/a-concise-sql-server-2012-licensing-overview.aspx). Standard Edition is about $3700/core per [this page](https://www.microsoft.com/en-us/server-cloud/products/sql-server/purchasing.aspx). Depending upon how your licensing is set up, you may only have to pay large sums for your production instances. SQL Server licensing calculations can require an advanced degree; talk to your MS rep to get definitive numbers. If you need someone and you're in the US NorthEast, I know someone at MS who can find you the right person to talk to about pricing if it's outside his wheelhouse.
I work in a hospital and use SQL almost everyday. The typical use will be very basic, lots of select statements to extract data on patients, some joins and calculations using population data, pasting to excel to produce pivot tables and graphics. If you have a CS degree, you are more than qualified (I am also a hiring manager). Just say you have a CS degree and are learning SQL, and can perform many of the functions needed to extract and work with data to support decision making etc....
Im looking around online and dont see anything that doesnt use LIMIT, do you have an example of what you would use. Yes I am using the same connection to access different DB's 
the benchmark setup seems a bit tailored to ideal conditions (i.e. one can tailor a benchmark to make the mysql optimizer faster than MSSQL, Oracle, DB2 and Terradata), but nice feature. Will sorting operations be offloaded as well? Merge sorts should be perfect candidates for GPU offloading, and that would change things on large table to large table joins quite a bit I imagine (merge joining vs hash joining). Also, anything being done on the query optimizer to factor in this feature? Just having it as a table scan would make it a pretty academical feature.
If there's no difference in putting `AND` or `OR` between A and B, it simply means that A = B. You can't beat logic. Or I don't understand what you're trying to say.
I used this to divvy up calls for a call center a long time ago. It sounds like you may have a similar task :)
That's the one thing I immediately thought of, haha! Also, I found a woarkound for the -1 thing you're doing in the join. It will keep the original #names sequencing correct for the first record. on ((r.rec_num-1) % @max_name_num)+1 = n.name_num Based on [this guy's page](http://www.bennadel.com/blog/2240-creating-repeated-sequences-with-the-modulus-mod-operator.htm).
&gt; It was just sample code I'm not trying to say anything. It was just sample code. 
&gt; excel good lord, I feel for you. My boss is a seasoned IT vet and he despises excel and access applications. He blocks excel and access from our servers because he doesnt want ppl creating applications with them. Shit rolls down hill when that person leaves the company or the app breaks because they come to IT for support and we just throw our hands up in the air. 
I *just* learned that someone in marketing created a macro enabled spreadsheet, that uses our official macro "report" spreadsheets to gather data because he didn't have rights to query the database directly. His spreadsheet actually calls the official spreadsheet, manipulates the dropdowns, and scrapes it to do whatever else his spreadsheet does. It's both ingenious and horrifying at the same time. I need a new job.
Looks like homework. Don't you have a visits table, with customer and date/time in? Your query should be based on that.
I'm new to sql but would love to learn if this would work! 
1. There is an ANSI standard for the SQL language. Various implementors (Oracle, MS, Postgres, etc.) have their own flavors, for various reasons (often it's "we implemented this feature before ANSI officially defined it, so our syntax is different from ANSI but the result is the same"). Once you understand the basics, moving between them isn't *too* difficult. 2. MS Access is kind of a database, but compared to a true RDBMS platform, it's pretty weak. It uses its own flavor of SQL, doesn't support everything one would expect, has some crazy syntax, and deviates from the norm in a a few places. MS SQL Server is far, far better in every respect. Access is best used as a front-end to a SQL Server database, if you need a rapid-development GUI environment for forms and such. If you use Windows on the desktop, get SQL Server Express Edition (it's free!) and learn SQL, not Access's bastardized version of it. 3. PostgreSQL, MySQL, SQLite and the aforementioned SQL Server Express Edition come to mind immediately (Oracle and DB2 must have free editions too). If you're on Windows, SQL Server Express Edition will probably be the quickest &amp; easiest to get up and running with. Management Studio is pretty nice to work in. 4. I wouldn't say necessary. But I also wouldn't recommend a specific language without knowing what you want to do with the data. Different tasks, different languages. 5. "I also see the application in business" - well, any corporation is "business" so this is a bit vague. The world runs on data and right now most of that data resides in relational databases, or passes through one at some point or another. I work in residential real estate and we use SQL Server to run **everything** * Our property management system (tracking people, leases, units, move-ins, move-outs, phone calls, service tickets and everything else in the day-to-day operation of apartment complexes) * Our accounting (general ledger, invoices, payments, etc.) * Many of our internal business processes, both in IT and the rest of the company, are driven through a ticketing system w/ a database back-end. * HR (managing payroll, employee data, bonus payouts, benefits, etc.) * Metrics for our VMWare environment * Managing our virtual desktop and SCCM setups
Yes if you wouldn't mind and you had the time? That would be great. 
Very helpful...thanks for taking the time to respond. I'll look more into SQL server.
&gt; I wouldn't say necessary. But I also wouldn't recommend a specific language without knowing what you want to do with the data. Different tasks, different languages. Powershell on the SQL Server side. 
Pivoting seems overkill here. Won't this achieve what you want? SELECT TIMESTAMP, SERVER_NAME, PROCESS, ABS(DATEDIFF(SECOND, [LAST_CHANGED_NP], [LAST_CHANGED_P])) [DIFF] FROM ( SELECT TIMESTAMP, SERVER_NAME, PROCESS, PRIMARY, MAX(case when PRIMARY = 0 then LAST_CHANGED end) as [LAST_CHANGED_NP], MAX(case when PRIMARY = 1 then LAST_CHANGED end) as [LAST_CHANGED_P] FROM Table GROUP BY TIMESTAMP, SERVER_NAME, PROCESS, PRIMARY ) ORDER BY TIMESTAMP DESC
Great answers! A few extra comments... SQLFiddle.com is a website where you can play around with the all of the main players in the RDBMS game, without installing anything. Fantastic for practicing. SQL Server (free version) is indeed a great starting DB, but I'd also recommend PostgreSQL as it is the most comprehensive, feature rich, and ANSI compliant unrestricted free RDBMS available and it's less resource hungry than SQL Server.
Virtualize, you get 4 cores from the $7k standard license as I understand it. Give the machine the fastest processor you can find, and load it with RAM and SSD's. You should have no trouble scaling. You also get nice features like always-on, SSIS, and the SQL agent when you buy SQL standard. Don't underestimate how gimped express is. Your developer is high on drugs if he thinks that 1 CAL is ok. Microsoft wants a CAL per user even if you are running a public facing web-app with 1 million users. That's why core licensing exists.
That looks like a really, REALLY useful function that I could have used a bunch of times over the last year or two. Now, if only we weren't still stuck on SQL Server 2008...
Thank you! I will definitely go grab these books. 
Might wish to consider migrating; or more accurately, convince management that you need to migrate. Mainstream Support for 2008 / 2008 R2 ended 2014-07-08 (you still get security patches from Microsoft but it has no warranty or feature patches). 
Easiest way I could see to determine the NULL percentage for a column would be: SELECT (SUM(CASE WHEN yourColumn IS NULL THEN 1.0 ELSE 0.0 END) / COUNT(*)) * 100 AS PercentNulls FROM yourTable 
&gt; BIGINT apparently needs to be 52% null to acheive 40% space savings, I just don't understand how they got that 52% Software Engineers (*edit - and CS/Mathematicians*) that get paid large sums of money to optimize the engine usually figure those out using average cases and heuristics. &gt; if you specify the column name, count will not include NULL data Yes and no, hence the CASE; NULLs should always be explicitly handled if they are what you are considering. If you don't handle it correctly it will not; doing it the way you listed will also always round down, due COUNT returning an INT.
The Oracle 2-Day DBA documentation is great start https://docs.oracle.com/database/121/ADMQS/toc.htm
Have a look at [SQLite](https://www.sqlite.org/). Free, single exe file command line program that provides an amazingly complete DBMS.
Ok, so this still makes me wonder if I didn't word things correctly, but it almost sounds to me like you're saying there's no way for me to calculate say...decimal/numeric(12,3)...I just have to pick the listed decimal/numeric(1,s) or decimal/numeric(38,s) %'s they have listed on the aforementioned page?
That's the thing I was thinking about - how could I get the row that represents the start time for each airing of an episode? Maybe first get a list of quarter hour times and row number them, and then join the table to itself on program-episode number to the previous quarter hour. The start time would have a null value for the prior quarter hour. That query is kind of intensive on a pretty large data set.
I think I figured it out - I used your idea of breaking the problem up, so I first find the rows for only the start times of each program code. So you see: 7:00 Episode 1 7:30 Episode 2 8:00 Episode 3 Now, when I run a row_number() on that, partitioning by episode name, you get it numbered correctly. I'll have to go back and then apply that numbering to all the quarter hours associated with each episode, but this solved the problem. Thanks for the help!
Generally speaking, yes. The values listed are calculated with the engine overhead and additional factors. If you were to put it into a formula it would look something like: =((1-(NonSparseBytes / SparseBytes)) * 100) + OverheadValue For example with the REAL datatype (4 bytes non-sparse / 8 bytes sparse): =((1-(4/8)) * 100) =(1-0.5) * 100) =50 So the *minimum* that the REAL datatype given no additional overhead is 50%. Obviously that is never the case so the additional values for handling the storage of the data/page management into account. How this overhead factor/value is determined is up to people far more intelligent than myself. For an example on how this can be calculated/applied in a mathematical sense you could read up on [Sparse Matrices](https://en.wikipedia.org/wiki/Sparse_matrix), specifically the Yale method. 
I'd recommend not using Microsoft SQL. Extract, Transform, and Load is like pulling teeth with it vs any other product. Maybe Oracle is worse but I wouldn't know. Bulk insert and BCP both really, really, suck. Merge sucks. SSIS is a mess, and isn't cheap. Ostensibly it's for our own good, but sometimes you just want to get the job done right? I'd recommend postgres as being the leading platform at the moment, but if you have mysql/mariadb resources available take that route.
I'm not familiar with HP Vertica or its date arithmetic. But in an example involving a date, assuming Oracle like date arithmetic and using the technique I outlined, something like `b.Date between a.Date - 30 and a.Date` 
Using select * is bad practice in a production environment for many reasons. You should be selecting the individual columns anyway, making a default order obsolete. What happens if the table structure changes? Is it more convenient to have your APP break because you used select *? instead of just selecting what you need.
Thanks, this is a great quuckstart doc. Loads of studying to do. 
Pretty close! This slightly modified version produces the same results as the PIVOT query. SELECT TIMESTAMP, SERVER_NAME, PROCESS, DATEDIFF(SECOND, [LAST_CHANGED_P], [LAST_CHANGED_NP]) [DIFF] FROM ( SELECT TIMESTAMP, SERVER_NAME, PROCESS, MAX(case when [PRIMARY] = 0 then LAST_CHANGED end) as [LAST_CHANGED_NP], MAX(case when [PRIMARY] = 1 then LAST_CHANGED end) as [LAST_CHANGED_P] FROM Table GROUP BY TIMESTAMP, SERVER_NAME, PROCESS ) T ORDER BY TIMESTAMP DESC As does this one, which brings it down to a single SELECT statement. SELECT TIMESTAMP, SERVER_NAME, PROCESS, DATEDIFF(SECOND, MAX(case when [IS_PRIMARY] = 1 then LAST_CHANGED end), MAX(case when [IS_PRIMARY] = 0 then LAST_CHANGED end) ) [DIFF] FROM Table GROUP BY TIMESTAMP, SERVER_NAME, PROCESS ORDER BY TIMESTAMP DESC The actual execution plan of running all three in a single query shows a 34/33/33% split, so it would appear that (at this time) they are all essentially equivalent performance-wise (using SET SHOWPLAN_ALL ON shows that the TotalSubtreeCost for the three statements gives 10.89837 for the PIVOT and 10.77569 for the other two, so *slightly* edging out the PIVOT in terms of performance). I think I got stuck in "MUST USE PIVOT" mode. While the PIVOT method worked, and is at least roughly equivalent performance-wise, it is also more difficult for the next person to understand/work on. So, in the interest of reducing future headaches, I'm going to use the simplest of the three - the last one. EDIT: That said, wrapping it in a subselect makes filtering easier - it can all be done in the WHERE clause, without having to use HAVING.
Oracle has the concept of external tables. You simply define a table with the columns and datatype that matches your CSV file and Oracle mounts the CSV file as a table and is immediately queryable using SQL or transformed and manipulated using PL/SQL, join to tables, everything. They are amazing, so easy and intuitive to work with. The external table validates the data too. You can define multiple external tables on the same file, by applying a LOAD WHEN cause, so can handle variant record CSVs or special handling of header and footer lines. I've worked on many RDBMS, and I've found this to be the easiest way of processing, transforming, and working with CSV data by a long way.
I used the inline view approach `select * from (select... ` for a couple of reasons. In my job I often need to debug sections, and that allows me to extract just that section and run to interrogate things - plus it doesn't have any real performance hit to the overall query, as your metrics prove. It also allows me to turn the inline view into an actual view easily. See my point? The final approach, although cleanest isn't as flexible.
That's exactly how I've done transitions in the past; lease company gives me access to the new server with requested OS/SQL version installed, I restore a recent DB backup, install the required applications, add the new subscriber to the replication publication, and check that updates roll through as expected. Come game day, DNS is altered to point all traffic at the new box. Everything goes well, it stays as is; everything goes to pieces, it goes back the way it was. After a few weeks without requiring rollback, the old box is decommissioned. I actually found it disturbingly easy, as soon as I'd actually done it at least once. I even completely replaced the publisher without any significant incident/outage. I think the biggest problem is that the lease company wants to shift their customers to virtual machines rather than physical boxes; however, transitioning to VMs will cause the company, er, "difficulties" come audit time. So the leaser wants bigger and bigger financial commitments to continue to provide physical equipment, rather than privately hosted VMs; naturally, manglement doesn't want to commit to a contract for reasonably spec'd physical equipment when they've been told that the vaporware App v2.0 will have vastly reduced minimum hardware requirements. I'm currently putting all of this in a document (including pricing for the new hardware from the lease company), with my recommendation that the best time to transition is six months ago, but immediately would suffice. My boss can then reject or ignore my recommendations as he likes, but at least I've covered my backside.
Below the form fields, when recieved back will be an array as weeknum[0], weeknum[1]. Javascript will see it as the same. The checkbox value will be $con = mysqli_connect('localhost','root','root','test'); if (!$con) { die('Could not connect: ' . mysqli_error($con)); } $week = '1'; mysqli_select_db($con,"TEST"); $sql="SELECT * FROM table WHERE week =$week"; $result = mysqli_query($con,$sql); echo "&lt;form name="form"&gt; &lt;table&gt; &lt;tr&gt; &lt;th&gt;Week Num&lt;/th&gt; &lt;th&gt;Memo Date&lt;/th&gt; &lt;b&gt;&lt;th&gt;Confirm&lt;/th&gt;&lt;/b&gt; &lt;/tr&gt;"; while($row = mysqli_fetch_array($result)) { echo "&lt;tr&gt;"; &lt;input type="hidden" name="weeknum" value=$row['weeknum']"&gt; &lt;input type="hidden" name="memodate" value=$row['memodate']"&gt; echo "&lt;td&gt;" . $row['weeknum'] . "&lt;/td echo "&lt;td&gt;" . $row['memodate'] . "&lt;/td&gt;"; echo "&lt;td&gt;" . "&lt;Input type = 'checkbox' Name ='confirm' value= 'confirmed'&gt;" . "&lt;/td&gt;"; echo "&lt;/tr&gt;"; } echo "&lt;/table&gt;"; echo ",&lt;br&gt;&lt;input type="submit" name="formSubmit" value="Submit" /&gt;&lt;/form&gt; mysqli_close($con);
What the table currently looks like: http://i.imgur.com/ZlbzzIv.png 
Honestly I'm indifferent about VM's, they've both saved and caused me headaches. Dynamic resources and redundancy have made my patch/performance outages almost non-existent, but they have the same drawbacks if the host/network/storage decide to defecate the mattress. 
What error are you getting? What are you trying to do? Also post specifics of your environment if you want help.
You need to download and install SQL Express, however this only runs on Windows. Do you have a windows environment you can run it on?
Thanks took me a while there to figure out what you meant but i got the idea and now it works great :) 
English as a second language =)
Something like this? SELECT * FROM ( SELECT Customer, Part, Version, Period, (Quantity * Amount) As Rev FROM yourtable JOIN yourothertables ON whatever WHERE 1=1 ) AS x PIVOT (SUM(Rev) FOR Period IN ([Period 1], [Period 2])) AS pvt
Would I basically just throw my entire regular query into the nested query there? In order to show all of the periods do I need to call every one out like that?
Yes and yes. You can use dynamic SQL to populate the column list if you don't know the values ahead of time. It's not worth the trouble if you already know them though. Also, I just noticed an error and fixed it...
Thanks guys. I think we'll have to go with free stuff for budget reasons, so it's between Postgres, MySQL, SQLite, and Oracle. I'm gonna work on a comparison, write a memo to that effect to people I need to run this decision by, and go from there. I really appreciate all the answers.
You should be able to use ORDER BY on any of the columns, including the ones created by the pivot. Is that not working?
This is inherently not a relational database if this is possible. What you need is multiple tables. One solution is an Orders Table , Merchandise Table and maybe a junction table between the two (order id , merchandise id) 
Sample code that doesn't make sense. Okay.
Why are they wanting to do this? Does it have anything to do with flexibility and or turnaround time for requests sent to your team? 
Had this exact issue 12 months ago. 1) you will end up spending more time fixing and having database issues if you give them free access 2) they will complain about the data being wrong (see #1) 3) setting up a cube will result in 1000% less headaches. &amp;nbsp; If someone really wants to get involved and learn SQL - let them, but how many of their reports are truly adhoc vs having done proper BA work to make sure that you are giving them the proper tools?
I think they just want to learn it honestly. It doesn't have anything to do with flexibility or turnaround time.
There may be opportunity for compromise here. IF you're able to create a model of the data within SSDT and publish that model out to an SSRS instance, you could have your finance folks create Report Builder reports against that model without having to know any SQL themselves.
Well as someone who came from finance I may be a bit biased. As the other poster noted, finance people are generally fairly smart and could do this if they have the interest. Now if you were talking about sales people, I would run from that one. Give them bits of access as you feel their skill deserves. IT hoarding data access drives me crazy. I'm assuming this is a DW type situation as well. 
They want to use SQL and we actually already have Tableau licenses. It's not about them creating their own reports, it the manner in which they do it.
You can fuck a lot of shit up with letting them do that. It might be better to create a data warehouse / cube and then give them access to that via Excel Power BI or some sort of drag and drop "create your own reports" type of thing.
We're currently creating a cube and having them connect to power pivot right now.
Just a thought......may require a bit of work, but worth the effort. Web interface, choosy form to pick the report, then the parameters (usually dates??) and click submit and the page tells them to expect their report in their email shortly.
I've worked in IT for years and transitioned to working in finance in two different companies (as an ETL and ERP developer and ERP system admin and dba) and here is my take on it. In my experience, you have to take a look at your finance employees to determine if that would be a good idea. One company had nothing more than what I would call "finance techs", meaning $10/hr folks who are just users of a system who get roadblocked when anything doesn't go according to procedure and go to their manager for help. In my current company, the finance employees are MBAs, comptrollers, actuaries, and CPAs. On this level, I can talk about "cardinality" and they know what I'm saying. If they run into a problem, they will solve it on their own or go a different route. I'm better at SQL than they are (I think), but they don't come to me for support even though my door is open. SQL-based reporting was opened up to Finance in both of the companies I worked at. For my previous company with the $10/hr finance peons, releasing SQL was a train wreck. There is at least one person that will write something dumb like (SELECT * FROM * FULL OUTER JOIN * WITH * WHERE MY SUBQUERY IS SELECTING * FROM *) aka give me the entire database in one drop and make sure performance is worst-case scenario. If they worked at a saw mill, these people would chop down a tree to make a single toothpick, in other words. All of those queries ended up with the IT dept involved. So in summary, I think it is OKAY as long as you have intelligent people working in finance, and someone steps up to be the go-to support guy for data-related questions. Someone has to vouch for what the finance people put out. Who is taking responsibility and overseeing it? Now for a word of caution. You have to know what they are using the data for who is using the data and for what purpose, and I'm not talking about security (although that is another concern). There was a problem that my old company ran into one time involving auditors. The auditors asked for some basic info, and the finance department whipped up a query to get the data for them. As you know, RDBMS is based on set theory, predicate logic, relational models, etc. Therefore, improper joins and null values can get you into a lot of trouble. The auditor asked for something like "I'd like to see person demographics, and if they have a loan or a CD on record, let's see that info too". So the employee did an inner join between the "person" table, the "loan" table, and the "certificate" table, and of course it only returned records were a person had both of those products, which effectively "hid" the data from the auditors for all customers that did not meet that condition. Not disclosing all data to auditors is bad, even if accidental. Other times data is used for decision making. But if the data isn't right because your finance folks don't understand the data in every tables, much less the joins, then the decision based on that data could be made under false assumptions. Finally, when I worked in IT before my finance days, I ended up being the report writer for about half my hours because everyone else fizzled out or decided it wasn't part of their job. Everyone is excited to have it at first, but then they don't want to solve any problems when something doesn't go their way. Then they will do it less often and they will lose any skill they had learned. People will start washing their hands of it and someone will pick it up and be the expert and support. One more thing I will mention that has helped us is to set up a data warehouse. If you have a formal RDBMS as part of some system, chances are it is highly normalized. For the average person who doesn't have a good understanding of the schema, it is best to denormalize your data and warehouse it. It is huge and redundant, but it allows people to do a big data pull and then stop raping your database because they can dump it into excel and use a pivot table for that. A data warehouse also helps reduce the issue of dropping data due to incorrect joins, simply because the data is very denormalized so joins are not always necessary. People like that because it's a one-stop-shop to get what they need, even though it isn't as pretty as a 3NF setup. 
there wouldn't seem to be too many problems of giving them access to non-production archives (24 backups, cubes on separate hardware, etc). As long as they have read only access of course. there is still the possibility that someone will nest 5 different selects with incredibly inefficient queries and bog down the system. so either be prepared to coach them to avoid that, or just have a daemon that kills their user name's connections on queries running &gt; x time limit.
I would not give them WRITE access to a database outside of the standard application front end. As for reporting, I'm a big fan of "self service" but I set them up with data source views in SSRS and use report builder to create the reports. That way you control the data relationships and integrity, they are just dragging and dropping the fields onto the report.
Thank you, this helped me get on track and fix the issue I was having. 
Be prepared to be constantly answering questions and fixing little things like having a user try to log in too many times and locking down a server and have none of your scheduled reports run... I think a good compromise might be setting up some mentorships and maybe have some of the individuals complete easy tasks while being supervised by said mentors. Damage control might be easier that way.
If you think the individual is capable take a chance and give them read only access. I just recently transitioned from a sr financial analyst to a sr business analyst with little growing pains. I was mentored for a few years from a guy in IT on SQL During that time I reduced reliance in IT through said mentorship and internet training. I feel like an understanding of advanced excel formulas helped jump start my SQL transition. 
FYI - SQL was originally intended to be used by business managers, and not IT professionals.
If they are fine with read-only access and they do at least 50% of their learning in their free time with the stipulation that they cannot bug you about queries, then I would say sure. I just wouldn't want to see productivity plummet for a while because people are trying to learn when then should be working. Not that its a bad thing to learn but it cant be all on company time.
*I'm inexperienced with SQL myself, so I'm afraid I can't be of much help. For any regulars who come by though, here's the original post cleaned up a bit:* &amp;nbsp; I have some experience with coding, and my job that I am at now, at a multidisciplinary practice, involves using a piece of software with a SQL database. The software that we use is not exactly user-friendly, so I found that I can write rough code that will query the database, which we then put into an Excel spreadsheet, and mail-merge onto a "travel sheet" for patients that has their appointment time, name, insurance information, etc. on the page (not the most ideal but I don't have time to figure out a better way - I would love to hear ideas though). We want to be able to put the diagnoses on there as well, but I'm not sure how to go about it. I apologize if it's fairly simple I've tried to pivot the data and do several other things, if you guys could help, that would be great. &amp;nbsp; For each table, `TableName.PatientID = Patients.ID` &amp;nbsp; **Patients** &gt; ID | FirstName | LastName | CurInjuryDate --: | ------------ | ------------ | ------------ 3 | Mickey | Mouse | 09/01/2015 00:00:00 4 | Minnie | Mouse | 07/25/2015 00:00:00 &amp;nbsp; **Appointments** &gt; PatientID | ScheduleDateTime | Status ----------:|------------------------------|-------- 3 | 2015-09-06 08:30:00 10 | 1004 4 | 2015-09-07 09:30:00 10 | 1004 4 | 2015-09-10 10:30:00 10 | 1004 3 | 2015-09-10 11:30:00 10 | 1004 3 | 2015-10-17 12:30:00 10 | 1004 &amp;nbsp; **Diagnoses** &gt; PatientID|Code |Description |Seq|CreatedOn ---------:|--------|---------------------------------|----|--------------------------- 3|722.4 |Degeneration of IVD (Cervical) | 1|2015-09-01 4:30:00.577 3|728.85 |Muscle Spasm | 3|2015-09-01 4:30:00.577 3|720.2 |Sacroiliitis | 2|2015-09-01 4:30:00.577 4|737.0 |Idiopathic Kyphosis | 1|2015-07-25 13:30:00.683 4|728.85 |Muscle Spasm | 2|2015-07-25 13:30:00.683 4|784.0 |Headache | 3|2015-7-25 13:30:00.683 &amp;nbsp; **Treatments** &gt; PatientID|Description ---------:|------------ 3 |Ask to use a Mouska-Tool. &amp;nbsp; What I need is a query (for tomorrow's appointments, which are a future date and have `Appointments.Status = 1004`) that produces the following table: (As a note, `Diagnoses.CreatedOn = Patients.CurInjuryDate` must be true) **My Table** Patients.FirstName | Patients.LastName | Appointments.AppointmentTime | Patients.CurInjuryDate | Diagnoses.Code (1) | Diagnoses.Code (2) | Diagnoses.Code (3) | Diagnoses.Code (etc.) &amp;nbsp; I have found a way to get the appointments, and a way to get the diagnoses, but I have not found a way to integrate the two together. What I have found is the following works to get all of the appointments together: (There are a few extra categories that we need on the sheet that I omitted from above, apologies) SELECT Patients.FirstName, Patients.LastName, Patients.AccountNo, Patients.ChartNo, Patients.BirthDate, Patients.CaseType, Appointments.ScheduleDateTime, ApptRooms.Room, Appointments.AppointmentNote, Patients.Box15Qual, Treatments.Description FROM Patients INNER JOIN Appointments ON Patients.ID = Appointments.PatientID INNER JOIN ApptRooms ON Appointments.ID = ApptRooms.AppointmentID LEFT JOIN Treatments ON Treatments.PatientID=Patients.ID WHERE (Appointments.ScheduleDateTime BETWEEN '9/01/2015 8:00:00 AM' AND '9/02/2015 5:00:00 AM') AND (Appointments.Status = 1004) ORDER by Appointments.ScheduleDateTime &amp;nbsp; The diagnoses are entered in the Diagnoses table *every* time a patient is checked out. So if Minnie Mouse has 3 diagnoses, that Diagnoses table will have 3 different entries per appointment, so what I figured out is the following code to get the diagnoses: SELECT * FROM Diagnoses as D LEFT JOIN Patients AS P ON P.ID=D.PatientID WHERE D.PatientID='3' AND Convert(DATETIME,Floor(Convert (Float,D.CreatedOn)))=convert(DATETIME,Floor(Convert(Float,P.CurInjuryDate))) ORDER by D.Seq &amp;nbsp; If this is too much, I apologize. I don't have anyone here who can help me with SQL, and I am banging my head against a wall. Any help is much appreciated. Thanks!
So this pretty much is what I do. I manage a data warehouse for a 120 million dollar a month expense line at my company. We have the data warehouse tables all under full change control with an open sandbox for the finance people to use. The quality of finance folks ranges from really awful to people that have masters degrees in MIS and probably know more about a properly setup warehouse than I do. So, we build a warehouse and give them the ability to read off the warehouse but they can only change their own stuff. Also I don't answer SQL questions without a really good reason (like you are doing something really bad). Any report that becomes big enough it would run into different folks getting different results gets taken over by my group and pushed into production after we go through it. This reporting has been used over the past 6 years to save 10's of millions of dollars.
I would also like to add that I teach a quarterly Intro to SQL class for my company and I always open up my class with the following quote: "A good financial analyst has the ability to use SQL to retrieve and analyze data"
Seems like everyone has sounded off on this one already but I want to clarify one major thing. The issue is not really the users ability to learn SQL. They could use a GUI like access and odbc to the data. The true issue at hand is understanding the data. In my environment our transactional databases have several relationships that if not careful can create duplicate records rather easy. Without an identifying field a user may never know about the dupes. This will be your biggest gap and biggest win if you can properly communicate it. I think this is why so many have suggested cubes, because if designed properly it is almost dupe proof.
So the general syntax is actually the same no matter how many tables you join and you don't even need to select any results from them (only do this to better understand *how* to join tables) Update to my first select statement, adding the space table, and the rack column from the space table. &gt; SELECT bag.id, bag.cassette, cassette.space, space.rack &gt; FROM BAG &gt; INNER JOIN cassette &gt; ON bag.id = cassette.id &gt; INNER JOIN space &gt; ON space.id = bag.id So what we have done is add the space table, when the space ID matches the bag ID. You can add tables as you need this way. If you want to introduce a table that will only match some of the time, you would use a LEFT OUTER join. Does that make sense to you now?
Thank you so much! I feel like we are friends now. We both don't know SQL, I can't format, you can, it's a match made in reddit heaven. Seriously, thanks.
Do you have a server login and the database security user? It's a common problem when people restore databases that they don't create the server login which leaves the database user orphaned. Also ensure that the database user has the login attached as well. 
Additionally, does your service account have permissions to [Log on as a service](https://technet.microsoft.com/en-us/library/Cc794944%28v=WS.10%29.aspx)?
The recursive CTE may be better (not sure), but you can also use an XML trick. I explained the technique [elsewhere](https://www.reddit.com/r/SQL/comments/3hcczb/simple_substring_help/cu67rpb), or you can check [this](https://www.mssqltips.com/sqlservertip/1771/splitting-delimited-strings-using-xml-in-sql-server/) or other guides for doing it specifically with comma separated values.
Title is misleading. 
Wtf did I just read... I'm to new for this shit
I am a perfect example of a lot of the issues and concerns as well as the perks of allowing others to do sql. I work at a law firm that specializes in collections we had an entire IT staff quit over the course of 6 months. I knew the manager... The guy that became manager that is and he knows I like playing with tech of all sorts. I told him I knew java and home over 15 years ago and made websites for businesses in high school. He sat me down and explained he expects nothing from me and it's all at my own pace. I started to learn in January and got some basics down but didn't grasp it a a whole lot. Then I took it more seriously over the last few months... As things got more complex I had to take his time away and he would have to show me things. I really had not done anything to useful yet. Now it's been 9 months and I automated almost every report we had back logged for clients for my department. We were behind over two years... And I've written things to help with our production and calls. I've been told I've been one of the best gambles they had. I've had our director ask me for reports because he knows IT is just to behind and he needs it done today. As someone who is 30 and made a late life decision to get into IT this has been huge. 
When running a query output it to excel file instead of grid
Here's a simple tutorial that may help -&gt; http://www.itwire.com/it-people-news/cio-trends/58044-roll-your-own-business-intelligence
Huh, never saw that as an option. I'll try/look for it on Tuesday. Thanks.
You can look at [this free course](https://learn.saylor.org/course/view.php?id=91). I haven't finished it yet, but it offers a lot of more specific readings than just basic SQL courses, tutorials. Before you start, you may want to take the exam to check if the level of the course is appropriate for you. This way, I was able to find out, there was indeed some 'terra incognita' to explore yet.
Some benefits of using UUIDs: * others can't guess entity IDs based on one existing entity (the chance to actually guess right is minuscule) * others can't estimate the number of records * you can merge instances without primary key conflicts/renumbering.
How so? I read the post and it seems spot on to me. 
Thanks! This looks promising. 
Have you seen plsqlchallenge.com? Not a course as such but they release a new challenge every week, have a back catalogue and explain the solutions. It's my go-to for practicing sql skills
This is cool. I'll definitely use it as a supplement.
The thing is I don't want to use the wizard; we want to try and do this all from the script itself.
ooooh
I will make around 63k. 
How difficult was SSRS for you? Was there some kind of technical interview for your SQL skills?
I did have a technical interview so they have an idea of what I know. I have little experience with SSRS but they know that. I assume they will have a standard on how reports will be?
* If you are doing reporting of any kind, do as much inside the database engine as possible. Use SSRS as your presentation only, with as little aggregating, grouping, sorting, calculating as possible. * Learn the business you are dealing with. A lot of people say "Data is Data" and it doesn't really matter, but it sure as hell makes life easier to understand the CONTEXT of the data, especially in your interaction with end-users * Don't get frustrated when you can't figure something out. SQL, as a language, is easy. Break problems down into small, discrete chunks. Concentrate on WHAT you want to do, Google will tell you HOW. 
It's an absolute time saver. It also lends itself to a leading comma syntax instead of trailing. Ctrl+ shift + arrow or click/drag comes in handy, too. I wish I knew that one earlier. 
What is your career goal? Healthcare IT, finance, government work? If so build something towards that end. A healthcare billing program could get 20 tables no sweat as could a project cost estimating system.
"I decided to name my company 'AdventureWorks'. Here we have their ERP/MRP database"
developing information systems.
Performance trending and monitoring. Yo dawg, I heard you like information systems, so I built an information system for your information systems 
How about finding some data you find interesting first and then trying to answer questions based up that data? Although 20 tables might seem big, that's not big when the data is normalized, particularly when combining several datasets. I often find some interesting things can be found when combining datasets based upon location, like tying some data to census data for that location to get some demographic data. Here are some places where you might find some interesting data ... [https://www.reddit.com/r/opendata](https://www.reddit.com/r/opendata) [https://data.sunlightlabs.com/](https://data.sunlightlabs.com/) [http://www.data.gov/](http://www.data.gov/) [https://aws.amazon.com/datasets/](https://aws.amazon.com/datasets/) [https://github.com/caesar0301/awesome-public-datasets](https://github.com/caesar0301/awesome-public-datasets) [https://delicious.com/pskomoroch/dataset](https://delicious.com/pskomoroch/dataset) [http://opendata.stackexchange.com/](http://opendata.stackexchange.com/)
Pretty much anything with temporal data will get you to 20 tables in no time. 
Ifnull sounds perfect, going to check that out. They don't come back at all, because they don't exist. There are no year 1 or 2 sales for this particular company, but I want my output to say there are 0 sales in y1 and 2.
you need a LEFT OUTER JOIN from a table containing the years you want to show, to your data table you could use year numbers ... CREATE TABLE years ( yr SMALLINT ); INSERT INTO years VALUES ( 2011 ) ,( 2012 ) ,( 2013 ) ,( 2014 ) ; SELECT years.yr , SUM(sales.amount) AS year_sales FROM years LEFT OUTER JOIN sales ON YEAR(sales.transdate) = years.yr GROUP BY years.yr but that awkward join condition might get better performance if you use start and end dates -- CREATE TABLE years ( yr_start DATE , yr_end DATE ); INSERT INTO years VALUES ( '2011-01-01' , '2011-12-31' ) ,( '2012-01-01' , '2012-12-31' ) ,( '2013-01-01' , '2013-12-31' ) ,( '2014-01-01' , '2014-12-31' ) ; SELECT YEAR(years.yr_end) AS year , SUM(sales.amount) AS year_sales FROM years LEFT OUTER JOIN sales ON sales.transdate BETWEEN years.yr_start AND years.yr_end GROUP BY YEAR(years.yr_end) 
Try a set returning function. https://www.periscope.io/blog/reasons-not-to-use-mysql.html Point #2 in the article
Assuming that you've got any sales in the years in question, you can do this with a CROSS JOIN without a separate table: WITH CustYrSales AS ( SELECT CustNo,YEAR(SaleDate) SaleYear,SUM(SaleAmt) SaleTtl FROM Sales WHERE YEAR(SaleDate) BETWEEN YEAR(GETDATE())-3 AND YEAR(GETDATE()) ) SELECT c.CustNo,c.CustName,y.SaleYear,cs.SaleTtl FROM Customers c CROSS JOIN (SELECT DISTINCT SaleYear FROM CustYrSales) y LEFT JOIN CustYrSales cs ON cs.CustNo=c.CustNo AND cs.SaleYear=y.SaleYear
 SELECT CASE WHEN UserID1=4 THEN UserID2 ELSE UserID1 END UserID FROM TableInQuestion WHERE UserID1=4 OR UserID2=4
Thanks mate, this is what i was looking for
I second the cross join solution. In my queries I add a column of "1"s called CONS (constant). This makes counting records easy. To add nulls I make sure that every variable combination needed has a dummy record with "0" in the CONS column. I do this using a crossjoin. Then I can aggregate using Sum(CONS) which creates zeroes where there is no data. Makes excel charts nice because it doesn't skip days when there are no events.
some that i have encountered -- - Head First SQL by Lynn Beighley - Beginning SQL by Paul Wilton and John Colby - Simply SQL by Rudy Limeback - Learning SQL by Alan Beaulieu - Sams Teach Yourself SQL in 10 Minutes by Ben Forta - Sams Teach Yourself SQL in 24 Hours by Ryan Stephens - SQL For Dummies by Allen G. Taylor - SQL in Easy Steps by Mike McGrath - SQL: A Beginner's Guide by Andy Oppel if you're brand new to SQL, you should go visit both a library and a bookstore, see what's on the shelves, before you make a purchase
thanks but idk I'd rather read a book or something to go at my pace.
Can you even run a bat file on OS X? You should be able to connect to a SQL database from any OS as long as you have the correct ODBC drivers setup. (this is my understanding, but I may be wrong and have never done this from a mac)
you have to be good in 2 key areas -- data modelling and the business unit nothing turns business unit executives off faster than some IT punk who doesn't understand what the business unit does and what it needs you gotta be able to converse convincingly in business language (not just echo the words and phrases) that's often a tough challenge for data folks, tougher than creating cubes and dashboards
To add onto this, data analytics can be very simple to very, very, very complex (forecasting work in the energy industry for example). The BEST analysts I've seen are those who have the perfect blend of both mathematics and scripting knowledge. Don't be surprised if you are expected to know a programming language (C#/Python/Java/etc) in order to accomplish your goals. 
 CREATE TABLE SQL_TEXT (SQL VARCHAR(MAX)) INSERT SQL_TEXT SELECT 'DELETE STUDENTS' GO DECLARE @SQL varchar(max) SET @SQL = (SELECT TOP 1 SQL FROM SQL_TEXT) EXEC(@SQL)
to start out, i highly recommend this book: http://www.amazon.com/gp/aw/d/0321553578/ref=pd_aw_sim_14_of_17?ie=UTF8&amp;refRID=1GTZQDSJWAVF7ECZMNQS and as a follow-up, this is a great book to immerse yourself in the potential uses of SQL and provides great examples and 'recipes' of queries you can do: http://www.amazon.com/Cookbook-Cookbooks-OReilly-Anthony-Molinaro/dp/0596009763 i taught myself SQL about a year ago for work and it is extremely easy to pick up, these two books were great resources 
Little piece of advice... any company that lists a position as "data rockstar" is not a company worth working for.
You can't pull the data correctly from the 20 different databases all supporting software from different segments of the business unless you understand the business from the ground up. Otherwise, you will constantly be pulling reports that make no sense and alienate your users. If you create a report where you pull financial data and you give it to the operations people that don't understand accounting either, the accounting people will walk in and DECIMATE your professional reputation. So, the best way to provide reports that mean something is to start speaking with the consumers of your information and asking them what they need and why. Eventually, you will begin to know the data you are pulling and then start to suggest other fields to pull in to enhance that report, based on what other users that are viewing that same data set. What sucks is you have to listen to the users. Once you open that valve, it is hard to make them realize that you don't DIRECTLY work for them. You are just there to help. 
Why? 
Thank you! I blame mobile :/
i used the title informally in the thread, not as an official title, i figured that was obvious, but otherwise i agree with your post
I appreciate your post, but it's not really a trajectory... in other words, what's the roadmap starting from point 0 to hopefully someone who is a competent professional about this. I'm not clear on what order I should learn any of this.. what are the building blocks, and what are the following steps
I mean more in the context of where does one start at from point 0 (not knowing anything).. what sort of things should i teach myself in the beginning and in what ways do I build on those skills?
There really is no single trajectory and a lot depends on where and if you are currently employed. For some people mastering excel is the first thing to do. For others, it's SQL.
I ask because i came into my job with a pretty good competency at Excel, and now SQL database stuff is being thrown on my plate which is good, but I'd like to build on those skills in a way that would be useful to both my company and my career/skill set
Plsqchallenge.com. It's Oracle oriented but exactly what you're looking for
wow, that's ridiculous
I mean it is a thing - OPENROWSET will let you do it so long as the excel template is already set up for you. However it isn't really recommended for big data dumps.
Oh yeah. I've never been in an environment where it would work unfortunately. 
1. stop using the term "data rockstar"
So you want pdf_output to be dow&amp;pdf_output? ID | dow | pdf_output ---------|----------|--------- 1 | abc | 1abc 2 | cdf | 2cdf would that be correct of what you're looking for? Not sure why you doing a insert and then selecting the other 2 if they are all in 1 table...
Yes, that would be good. Or having a space between them. I am not sure how to go about this.
as you're using php i'm gonna assume mysql SELECT id, dow, CONCAT(id, dow) AS 'pdf_output' Should do it for you...
no sorry, pdf_output remains null after running.
 SELECT S.studentid, S.firstname, Stuff((SELECT ',' + SP.phonenumber FROM studentphonenumbers SP WHERE S.studentid = SP.studentid ORDER BY phonenumber FOR xml path(''), type).value('.', 'varchar(max)'), 1, 1, '') as phonelist FROM students S GROUP BY S.studentid, S.firstname | studentid | firstname | phonelist| |-----------|-----------|-----------------------------------------------------| | 1 | Adam | 111-111-1111,222-222-2222,333-333-3333,444-444-4444 | | 2 | Billy | 111-111-1111,222-222-2222 | | 3 | Cindy | 111-111-1111,222-222-2222,333-333-3333 |
You probably need `update` instead of `insert`. Try with this: update memo_list join ( select id, CONCAT(id, dow) output from memo_list ) temp on (temp.id = memo_list.id) SET pdf_output = temp.output 
Very impressive... but I need each phone number in it's own column - not concatenated in to a single column.
I know in MS SQL we can simply use the addition operator such as this: update memo_list set pdf_output = ( SELECT id+' '+dow AS 'pdf_output') where memo_list.id = temp.id
I understand that. What we're saying is that the insert portion is superfluous. You can simply do the select ... concat portion in your fpdf class and ignore storing this data in a separate column within your db. An insert/update is not required.
that's because stupid sql server uses an arithmetic operator to do concatenation sql standards require double-pipes operator || this is an option under mysql, which originally had the CONCAT function
It's OK to use crutches when you're still working on getting your legs under you. It doesn't all come together at once.
Any data in your database that you don't use is junk taking up space. Any data you don't use that can be used against you in the event of a breach is a liability. Social security numbers are not metadata...
Turns out the fields had time values in them. I changed my SQL to this: AND trunc(WorkOrder.ActFinish) &gt; trunc(WorkOrder.TargCompDate) Adding the TRUNC fixed my results. Thanks for your help /u/ziptime. It was my poor assumption that the value in the field only showing a date means it doesn't include time. 
Looks like you could be using Oracle SQL Developer, if so, [**change your settings to display time aspect**](http://www.zen-workshop.com/blog/v2/learning/default-date-field-display-format-in-oracle-sql-developer/).
That's a good call. I may look into that if I can't figure this out in an hour or so. Thanks!
Awesome, good to know! Thanks again.
It's entirely believable for a media agency. In which case you probably just need to know how to use tableau and possibly some sql scripting and sas modelling. And how to give convincing presentations. 
I'm not saying that what you're asking for can't be done, it's that what you're asking for won't be done very easily nor very quickly--as it doesn't scale well--based on your current table structure. And you current structure is almost certainly 3NF normalized transactional data. For analyzing data, it's best to have *completely denormalized* data in a [star schema](https://en.wikipedia.org/wiki/Star_schema) make up of dimension tables &amp; fact table(s). Once your data is in that format, querying it to show what you want is fairly trivial &amp; will have dramatically better query performance. I know that this is somewhat of a "non-answer" answer, but it's the right answer. Also, I don't want to imply that your existing table structure needs to be replaced or altered--consider the star schema to be just that: another schema. Except the source is your transactional tables.
What database technology are you using? For SQL server: SELECT TOP 50000 &lt;columns&gt; FROM &lt;table&gt; ORDER BY CHECKSUM(NEWID()) But it is highly technology dependent. IIRC, some SQL flavors have a SAMPLE keyword to gather a random sample of rows. 
Utilizing [BINARY_CHECKSUM](https://msdn.microsoft.com/en-us/library/Cc441928.aspx) is generally much faster for a sample size. Return TOP 10 Percent of a table pseudo-randomly: *** SELECT * FROM YourTable WHERE (ABS(CAST((BINARY_CHECKSUM(keycol1, NEWID())) AS INT)) % 100) &lt; 10 *** 
Hi, I am using MS SQL Server (T-SQL). Thanks 
Do you mean CTRL &gt; SHIFT &gt; ARROW? I just tried that for the first time and it's awesome. Free-form object alignment instead of it snapping to other report item alignment. So many frustrating times could have been saved if I knew this earlier.
OP, do they really have to be randomly picked? These checksums will have to fire for every record in the result set. I have tables with billions of records, query will never come back. 
What does the checksum() or binary_checksum() part do? I remember I did this once with just newid(). Did I screw up? :P
Whoops, that's the one. You can also click/drag with the mouse to save a fraction of a second at times. 
Thanks. I checked out the link to Wikipedia you left. I don't have much knowledge on database design other than a few college courses - I thought 4NF was the holy grail of database design? How would I implement a star schema with such a simple data structure? I see that I could use tables for Funds, Dates, and Prices and I feel like Prices would be... superfluous and difficult to manage at best. My "fact" table would be a collection of foreign keys, as I understand the star schema to work. I don't really understand how this schema helps with these complex queries. Perhaps if I describe my end state you could help me understand. **End State Goal** Table containing the fund name, summary about it, price, percent daily change, rolling 1/5/10/YTD returns, potentially various financial ratios (Sharpe, etc). So that's a large number of inline views. Does a star schema change things for that?
Sounds like you would need a derived table so you only get the max values from tblOrders. This should give you the max OrderDateTime based on the OrderID and HEOID_Fkey. SELECT heo.HEOID ,(heo.LName + ', ' + heo.FName) AS HEOFullName ,ord.OrderDateTime ,ord.ProductExpiryDate FROM tblHEOs AS heo LEFT OUTER JOIN ( SELECT OrderID ,HEOID_Fkey ,MAX(ord.OrderDateTime) AS OrderDateTime FROM tblOrders GROUP BY OrderID ,HEOID_Fkey ) AS ord ON heo.HEOID = ord.HEOID_Fkey LEFT OUTER JOIN tblOrderDetails AS orddet ON ord.OrderID = orddet.OrderIDFKey;
FYI, I have updated the original post.
FYI, I have updated the original post.
i'm just the analyst but my boss definitely indexes a few things. he doesn't have a formal education so i'm at a loss for how efficient he is being as i've never seen a computer science DBA in action. we have common queries running all day long. super simple scripts. occasionally some massive queries hit it though and just suspend everything. when we kill them everything else processes immediately. it's just probably killing some process one of our staff is running but nobody is speaking up. are there any books / articles / tips about indexing that you prefer? i would love any recommendations in that area.
from our query of the current queries running (during slow times) we're able to grab the exact queries that are taking forever. there are these @LNQ queries that slow us down and my boss has no idea what they are. then there are other queries that just query through all of our schemas because unfortunately we have over 300 schemas... those massive cursor queries seem to choke it up as well. thanks for the advice. if you get any ideas from any of these details I'd love to hear more.
Ah ok, I see the mistake. You need to get the max date + HEO ID combinations first, then retrieve the Order ID that's associated with it. This requires two steps (derived tables) because by the OrderIDs are unique so grouping on the OrderID and HEOID_Fkey at the same time will retrieve a row for each OrderID. This gets the result set you are looking for. WITH MaxOrderDate /* Get the max order date for each HEO */ AS ( SELECT HEOID_Fkey ,MAX(OrderDateTime) AS OrderDateTime FROM tblOrders GROUP BY HEOID_Fkey ) ,MaxOrderDateOrderID /* Get the order ID for each max order date and HEO combination */ AS ( SELECT MAX(tblOrders.OrderID) AS OrderID /* Max is used here again in case there are multiple orderes on the same date */ ,MaxOrderDate.HEOID_Fkey ,MaxOrderDate.OrderDateTime FROM MaxOrderDate LEFT OUTER JOIN tblOrders AS tblOrders ON MaxOrderDate.HEOID_Fkey = tblOrders.HEOID_Fkey AND MaxOrderDate.OrderDateTime = tblOrders.OrderDateTime GROUP BY MaxOrderDate.HEOID_Fkey ,MaxOrderDate.OrderDateTime ) SELECT heo.HEOID ,(heo.LName + ', ' + heo.FName) AS HEOFullName ,ord.OrderDateTime ,orddet.ProductExpiryDate FROM tblHEOs AS heo LEFT OUTER JOIN MaxOrderDateOrderID AS ord ON heo.HEOID = ord.HEOID_Fkey LEFT OUTER JOIN tblOrderDetails AS orddet ON ord.OrderID = orddet.OrderIDFKey;
I don't have links/opinions for ya, although I'm sure a quick google on indexing would prove useful. I do know that you need to take into account overhead - if you just go around indexing everything than the overhead for storing those indexes &lt;&gt; worth the efficiency. As for the queries - **CTEs, Temp tables** are your friend - split up the subqueries of the bigger queries into this smaller, manageable parts rather than one big 'superquery' with subselects in subselects in select not ins, etc. and check the run time of each. Once you find the quick parts, you can set those aside and work on how you can best optimize the parts that takes minutes+ individually. Without seeing your queries, that's the approach I would take.
Also keep in mind the width of what you're returning. Your query already needs to return a certain number of rows based on requirements - but it doesn't have to return all of the columns if they're not required in the final output. Wide Table = Larger strain, assuming we're talking a table 100k+
MySQL's `CONCAT()` and `REPLACE()` [string functions](https://dev.mysql.com/doc/refman/5.0/en/string-functions.html) will do the trick. You can nest several calls to replace multiple things at once, e.g. `SELECT CONCAT('just', REPLACE(REPLACE(REPLACE('testingstuff', 't', ''), 's', ''), 'g', ''));` `REPLACE()` is case sensitive, so you'll probably want to have a look at the `LOWER()` function too.
since were talking about lets say 2-3million rows, would you think ill see a considerable amount of slowdown in the performance? 
If I understood your problem correctly, the easiest way is with the ROW_NUMBER() window function: WITH A AS ( SELECT HEO.HEOID, HEO.LName + ', ' + HEO.FName AS HEOFullName, O.OrderDateTime, OD.ProductExpiryDate, ROW_NUMBER() OVER(PARTITION BY HEO.HEOID ORDER BY O.OrderDateTime DESC) AS Row FROM [tblHEOs] HEO INNER JOIN tblOrders O ON O.HEOID_Fkey = HEO.HEOID INNER JOIN tblOrderDetails OD ON OD.OrderIDFKey = O.OrderId ) SELECT HEOFullName, OrderDateTime, ProductExpiryDate FROM A WHERE Row = 1 I believe it will work even if the only date is a NULL, but please test it. I assume you're using SQL Server. If not, the same can be achieved with joins.
I'm not a web guy but I believe removing www subdomain from the URL could lead to a completely different web page. The only thing you want to do is replace http:// with https://? CASE WHEN URL LIKE 'http://%' THEN CONCAT('http://',SUBSTRING(URL,8)) ELSE URL END Note that URLs can have http:// in the middle of them, so a straight up string replace is not a good idea unless you know for certain that doesn't happen for your 2-3 million URLs.
Before you blindly kill processes, you should have a look at WHAT you are about to kill. SELECT s.session_id ,s.host_name ,s.transaction_isolation_level ,s.original_login_name ,s.original_security_id ,r.command ,execPlan.query_plan ,r.reads ,r.writes ,r.logical_reads FROM sys.dm_exec_sessions s INNER JOIN sys.dm_exec_requests r ON s.session_id = r.session_id CROSS APPLY sys.dm_exec_query_plan(r.plan_handle) execPlan And after you killed it, you can have a look at the query and tune it, or take the stick of learning to the knees of the developer running his queries on production. Just killing stuff is not really a way of handleing slowness, its just a quick relieve, but it is not a fix. Diagnose what runs, why it runs, and what you can do to tune it. Have a look at the sys.dm_exec_query_stats view. Here you find statistics on the query plans cached, how long they run, how often they run etc. 
Then /ignore.
&gt; there are these @LNQ queries that slow us down and my boss has no idea what they are. Are you sure that isn't L**I**NQ? If that's what it is, they're coming from a .NET application probably written with Entity Framework. EF is notorious for producing queries that do terrible things to your database. There *are* ways of wrangling it, but you'll need to work with your developers.
I second the motion to use temp tables liberally. Massive multi join queries are inefficient and difficult to understand.
The star schema is the jazz of database design--pretty every classical rule of database design that you were told to follow are going to be broken in the design of a star schema. (It's all about *knowing how &amp; when to break the rules properly* to carry the music analogy one step further...) Ralph Kimball's [The Data Warehouse Toolkit](http://www.amazon.com/Data-Warehouse-Toolkit-Definitive-Dimensional/dp/1118530802/) is widely considered the bible of data warehouse/star schema design. While there are other schools of thought on star schemas, he is considered to be *the* starting point for jumping into star schema design. Yes, the fact table is made up of FKs to the dimension tables &amp; measures--typically counts &amp; financial elements. And the measures will contain previously calculated and/or derived values so that when it comes to generating results in your queries, there will be very, very few calculations performed--it's more about retrieving rows &amp; performing few/any aggregate functions to display your results. You'll realize the potential power of dimension tables when it comes to dates alone. Think of what it would take for you to create a report with your transactional tables that compare data from only the first Monday of every month. Finding the date &amp; determining whether or not it's a Monday...ugh. But as a dimension, dates can be accessed like this (let's say I want the first Monday for every month in 2015): SELECT sum(total_sales) , sum(product_cost) , sum(gross_margin) , avg(gross_margin) , sum(widgets_sold) --or could be count(widgets_sold) depending on how you store the value FROM FACT_TRANSACTIONS INNER JOIN DATE_DIM on FACT_TRANSACTIONS.date_id = DATE_DIM.id WHERE DATE_DIM.year = 2015 and DATE_DIM.day_of_week = 'Monday' and DATE_DIM.week_of_month = 'First' GROUP BY DATE_DIM.month; And based off that query, it should be easy to see how you could easily filter those results on particular fund(s) by just adding a join to the FUNDS_DIM dimension table &amp; adding the specific fund(s) in the WHERE clause.
You're probably right I imagine it is LINQ. They're infrequently popping up so I can't grab one right now. I think our dev is aware and hunting this code but we have him stretched so thin it's just not a priority unfortunately. Thanks for the info, definitely interesting to get a sense of where they are coming from.
I would suggest picking up a copy of [`sp_askBrent`](http://www.brentozar.com/askbrent/) and running that while you're experiencing the slowdowns. It'll help you pinpoint the hotspots. While you're at it, get Brent's `sp_Blitz` (linked at the bottom of the above page) to do other diagnostics on your database and server. You'll be amazed (and probably horrified) at what you learn.
Just continue with the CASE statement: CASE WHEN ... THEN ... WHEN ... THEN ... ... ELSE ... END
Holy crap. **It works.** I’m not yet entirely sure how (still trying to decipher it), but it really works. Upvote. Upvote. Upvote. Oh, drat. *That* only worked *once.* Now trying all the variations of the data itself (null dates, expired dates, expired dates with more current dates, etc.), to see if this SQL can be “tripped up” in any way, but the first three variations (the most important ones) passed with flying colours. THANK YOU!! I don't think I would have come up with this on my own. …Although I am still not entirely sure how you managed to cover null values without including a MAX(IFNULL(column, 'dummy value')). From what I can see (so far) you just worked around that entirely, such that it wasn't even needed. Edit: What’s with the commas on the newline? Is that so you don’t accidentally leave a comma at the end of the last column name, thereby screwing up the statement? That’s a new one for me. My borderline Aspergers really doesn’t like that convention from an æsthetic perspective, but from a practical one I think I'll be using it from now on if I ever break the column names up into newlines like that. Edit 2: TIL about WITH statements. After 13 years dealing with SQL. Wow. Mind=blown. This may take some time to fully grok. Edit 3: TIL that WITH statements are Oracle/MSSQL only. No wonder I’ve never come across it before. Just got back into MSSQL/ASP.NET after a long (8+yr) hiatus, and never worked with Oracle before. Edit 4: Enjoy the gold. You deserve it.
Thanks guys. I actually managed to figure it out shortly after asking the question. (Generally how it works, right?) WITH CTE AS ( SELECT parent as TopParent , parent , item , 0 as Level FROM myTable UNION ALL SELECT c.topparent as TopParent , t.parent , t.item , Level + 1 FROM myTable t JOIN CTE c ON t.parent = c.item ) This is a bit different than what I ended up using due to how I needed to deliver the data, but I think it accomplishes what I had set out to do. (Not sure about you, but the word "parent" is starting to look weird to me now. )
The article I linked shows why, there is a drastic performance improvement doing it that way. 
What sort? It's using modulus to get a percentage sample instead of a TOP (*edit - Which also has the added benefit of only requiring a single scan to compute*).
Seriously though download the trial and try it out it is the best single source SQL ide that I have ever used. 
Just include an order by column set to zero in your inital CTE query and add one to it in the recursive step and order by that. I think that is what you are after WITH counting_numbers (num) AS ( SELECT 0 mynum UNION ALL SELECT mynum + 1 mynum FROM counting_numbers .... ) SELECT * FROM counting_numbers ORDER BY num I didn't test this code, but hopefully it makes some sense.
Good point. Depending on the firmness of the random requirement, this TABLESAMPLE expression suggested by /u/ericdemott may be a faster alternative. 
try TOAD
Well you are going to need a SUM(...) and you are going to need a GROUP BY ... statement, it might look something like this: SELECT city, SUM(sales_tax) FROM your_table GROUP BY city If all you have it the product price you could calculate sales tax (it is 6% here in PA) SELECT city, SUM(product_cost * 0.06) FROM your_table GROUP BY city Without knowing your schema a little better (what tables you have and what data is in them) that is the best example I can give 
Can you provide a little more information? Does your company currently use a database engine? If So, what is it? MS SQL Server, MySQL, PostgreSQL, etc. What kind of report do you need? Does it need to be some fancy looking thing? Can you put it in excel? etc. Your current project should be relatively easy. An example would be something like select city, sum(tax) 'SalesTax' from table where state = 'CA' group by city or select city, tax from table where state = 'CA' But this is very simplified as you may have to join multiple tables to get this data. If you are running SQL Server, it includes a reporting tool called SQL Server Reporting Services (SSRS). You can use either Business Intelligence Development Studio (BIDS) or SQL Server Data Tools (SSDT) to create the SSRS report and schedule it. 
I'm actually not positive which we are using. I'm in the process of contacting support to find out. We don't run our own server or anything. We have a storefront that we manage through Volusion. The report does not need to be fancy. The example I was sent was just 4 columns that had City, State, subtotal (for the city) and sales tax paid (for the city) This is what I have now: SELECT * FROM CUSTOMERS WHERE State='CA' And if I try and run that I just get a server error(which I expected, I was just testing for syntax errors) I've tried to run the CREATE TABLE command at the top of the code, but that returns a syntax error. 
Wow. See, this is why building stuff for just the learning experience is so worthwhile. Thanks a ton.
Hi, here at my place of work I regularly need to have a front end to oracle and microsoft DBs. For oracle, I like SQL developer. It's FREE! It uses TNS so you'd have to have the oracle instant client driver installed, which isn't a big deal to set up, then it works via odbc. For microsoft, we use Benthic and connect through odbc. It's lightweight and low cost. A license is $40 if I recall. Other SQL-capable software that we use is MS Access (via pass-through query) and Crystal Reports. But if you just want to write some straight up SQL though ODBC, SQL Developer and Benthic all day. 
&gt; MS Access (via pass-through query) I'd only use this as an absolute last resort. Mostly because I find Access insufferable.
I use dbVisualizer because we use Vertica and it's just easier with the drivers being built in already. Not the best interface, but it does have some solid features. I also use 0xDBE from JetBrains. It's an EAP right now but it works pretty well. The auto complete works really well, but it like to reindex and sync with all of your objects often so if you have a large database or many schemas, it can be painful. 
The issue is that I need the "-- anchor member definition" to be ordered, so the result is ordered. Can you understand?
At my previous two companies I've used [SQL Examiner/Data Examiner](http://www.sqlaccessories.com) and [SQL Delta](http://www.sqldelta.com/index?page=deltass). Both seem to be around $400.
That's how I would have done it! Nice job!
Don't tell me what to do mom
http://www.apexsql.com/ offer a wide range of tools, but their pricing is comparable to RedGate. 
Almost the same thing for getting filename from a path: SUBSTRING(FilePath, LEN(FilePath) - CHARINDEX('\', REVERSE(FilePath)) + 2, 10000) Note that windows file names can be longer than 255 characters.
You put your string down flipped it and reversed it.
The freelancer didn't leave any of his code behind?
I use Navicat to access our Oracle, MSSQL and MySQL Database. Makes light work of data ingestion from tons of file types and is really user friendly. 
Welcome to the real world, kid. 
I wrote a post with instructions on building the new fts5 extension. Also a detailed post about using SQLite FTS with python. And lastly, a restful search server powered by SQLite. * http://charlesleifer.com/blog/building-the-sqlite-fts5-search-extension/ * http://charlesleifer.com/blog/using-sqlite-full-text-search-with-python/ * http://charlesleifer.com/blog/meet-scout-a-search-server-powered-by-sqlite/
Wow, thanks for the gold!! On my cake day also! Hopefully it works for all cases and doesn't fail anywhere for you. Are you wondering how the null values were skipped over with the max function? There's no need to replace it with dummy values because if there is any value at all within the grouping that will automatically be larger than null. Feel free to PM me if you need more clarification of the query. Yea, that's why I use commas at the beginning of the line when I break up column names into multiple lines. I started to do it that way because Microsoft does it by default when you right click a table in the object explorer and choose "Select top 1000 rows". Common Table Expressions (CTEs) are awesome for cleaning up your code for readability and maintainability. Just remember that their scope is limited to only the select statement that is immediately following it.
LOL, and easy deletion of the UID field, and a creation of another with an A_I was enough to fix it.. xD
I'd suggest NOT going ahead and running a resource hungry trace on a production server without at least a bit of background knowledge first! You might even have certain views that can return the most painful queries based on high reads, duration, I/O etc. For me optimising a query then starts with the code, are there any gotcha - non sargable functions in where clauses, querying unnecessary data, poorly formed queries. Then you can get stuck into indexing, poor statistics etc and as a previous person suggested, get learning how to read query/execution plans. None of it is a real quick win, a lot of learning is involved. 
For query tuning and optimization I like Peter Larsson, Itzik Ben-Gan, Brent Ozar, Tomas Kejser and Adam Machanic in no particular order. With any problems I run into I check what they've written to get a reality check. With experience you learn to recognize common pitfalls and anti-patterns. If you are looking for a book, the first I'd recommend without knowing your background would be Microsoft SQL Server 2012 T-SQL Fundamentals by Itzik Ben-Gan. After that possible SQL Server MVP Deep Dives (both volumes).
Thank you for your response. I will do as you suggest and use SQL Profiler. I've never used it before but I found this post that suggests some good resources for it: http://dba.stackexchange.com/questions/519/sql-profiler-tutorials-for-newbie. Thanks again!
I'm afraid I don't quite understand what you mean
http://use-the-index-luke.com/
Probably SQLite ;-) 
Well he did say only in CA, so I guess maybe a filter would be in order. I did miss the monthly point though.
Ok, I think I see, you can carry the anchor through the CTE, just dont' modify it... with tree (...) as ( SELECT 1 AS Level, CAST('GE' AS VARCHAR(1000)) AS path, CAST('GE' AS VARCHAR(1000)) AS anchor FROM ge WHERE ... UNION ALL SELECT Level + 1, CAST(Path + '-' + ....), anchor FROM ge JOIN tree ON ..... ) Then you can order by the original anchor... is that what you mean?
Redshift does not have indexes. SQL optimization and Redshift optimization are too different things. I suggest you to read Amazon documentation.
Thanks for the advice, I'll use an SQL instance instead. As for the string, I'm creating a simple text string that allows my users to save certain profile options alongside their chain of command. It's a bit of a messy way of doing it, but it's the only real option I have since many of our systems here don't exactly play nice with each other.
Ah okay...yeah I've been in that kind of situation before. Working with poorly designed (and/or legacy) apps is the bane of my existence. Good luck!
In my opinion, there are two main things you can do: Compartmentalize with TEMP tables and join on indexes (and create indexes in TEMP tables when it makes sense). I've gone back and refactored a lot of our existing code and taken queries from 5 minutes to one second using these two techniques. The bit about joining on indexes is self-explanatory. For compartmentalizing, an example follows. You want to create a query that gets the latest "temperature" value documented for a a specific body of patients. If you do a join that grabs the patient, the documents, and the document values (including temperature) at the same time, your query is going to take forever. Instead, you write one query defining the patient population and dump those patient identifiers into a temporary table. You write one query defining the documents that contain temperature into another temporary table. You then join all the tables together, including inner joining the temporary tables to the original tables. This makes your query lightning fast. 
https://support.microsoft.com/en-us/kb/822400 
That looks like a really simple join. In the scenario I'm talking about, you'd be looking at: SELECT * FROM Patients INNER JOIN Locations INNER JOIN PatientDocuments INNER JOIN PatientDocumentObservations WHERE PatLocation = 'Dallas' AND PatDoc = 'Vitals Document' AND PatObserv = 'Temp' Instead of doing that, you might do: SELECT PatID INTO #Patients1 FROM Patients WHERE PatLocation = 'Dallas' Then grab the PK ID of each of the document names and observation names. You always want to join on indexed columns. You perform the text "where" search on the smaller catalog table and grab the PK for joining, like this. I'm only adding one observation and one document here, but we usually deal with dozens at a time. SELECT DocumentID INTO #DocumentInclusions FROM PatientDocumentCatalog WHERE PatDoc = 'Vitals Document' SELECT ObservationID INTO #ObservationInclusion FROM ObservationCatalog WHERE PatObserv = 'Temperature' Then SELECT * INTO #PatientDocuments FROM #Patients1 INNER JOIN PatientDocuments INNER JOIN #DocumentInclusions; So now you have a list of all patients that have a document in which that observation lives. SELECT * FROM #PatientDocuments INNER JOIN PatientDocumentObservations INNER JOIN #ObservationInclusions; So, yeah, it's a lot more code, but it runs much, much faster. I'm not sure if this is needed in every case, but we have hundreds of millions of rows and it's a critical production database we're querying off of, so queries have to be extremely fast. 
The problem I'm not cross joining, haha. I'm not sure what the optimizer is doing, but it's taking 5 minutes without temp tables and 1 second with them. Again, I'm talking millions upon millions of rows (and we're looking for just a few dozen in each table). The optimizer is definitely not smart enough in our case.
&gt; I'm assuming this functionality is offered by LocalDb. IIRC it depends on which version of VS you're using - as Microsoft typically does, they've started using similar terms for technically different items. Previous documentation will refer to 'LocalDB' but what it's actually referring to is an SQL Express Instance. The new 'LocalDB' which was introduced with SQL Server 2012 (I think...) is as you described - a file, somewhat like SQLLite, but without the SQL Server Express instance attached to it. Only reason I bring it up is because one of my co-workers was recently bit, big-time, by the distinction.
You can do log shipping in SQL 2000. It's basically setting up a constant restore of transaction log backups on a separate system. There's always going to be a lag time involved though, for instance if you do log backups (and ship them) every 15 minutes, then you could stand to lose up to 15 minutes of data. You'll also have larger windows as well, as in SQL 2000 I believe full backups will block log backups, so if your full backup takes 2 hours then during that time you'd be at risk of losing 2 hours worth of transactions. SQL 2005/2008/2008 R2 support database mirroring, which is like a more robust and continuous version of log shipping - but you can only have one principle and one mirror. SQL 2012 and up support always on availability groups, which is like a mix/match of mirroring and other replication technologies, you can have multiple target systems that you're shipping data to with different use cases (i.e., warm standby, or read-only replica, etc). Since you're on SQL 2000 I'd suggest that the path of least resistance is to get to 2008 R2 - you can still run your db in SQL 2000 compatability mode on that version of SQL server, AND you can also run database mirroring. If you don't have any worries with compatability issues between 2000 and 2014, then by all means upgrade to the newest version - I only suggest 2008 R2 because the jump that from 2000 can be done with a VERY minimal amount of work.
That would explain the difficulty I was having getting it to play nice with ssms 2010 then. I think VS will be using LocalDb, since I'm using 2014/5.
I would just do employee start / employee end as datetime. You can write queries that can then pull for the 15 minute intervals, and keeps all historical information. Also should a year from now someone say "You need to track every 5 minutes", you won't be rewriting.
I must be in the minority, but Toad gives us all kind of problems ....
Riiight, that eases my worries a lot. I feel that BI is the way to go but when I've provided dashboards and reports people look at them and don't really seem to do much more with them (Well unless it's something they do on a management level without involving me). I am currently pushing to start using data more resourcefully but I find it really hard being a junior to convince people who've done the same job for 30 years that this is the way to go. For example, we have a quoting system that was just thousands of excel files with data in each one. I've transformed that into a database driven system because I feel we can use that to target customers more effectively, capture data more intelligently and create targets for the future but these guys are arguing as to why we need it when their current system works fine. Madness to me.
Hey, look guys, ANOTHER generic description of simple joins!! If you were looking for anything specific or helpful, this is one more page you'll have to scroll through and then leave behind. 
I think it's all in the presentation. Some random guy who wasn't specifically hired for data analysis may not be listened to unless he presents the data in a concise and useful way. Since this guy has experience working at his company for a bit he should hopefully have some idea how things work and can use his inside knowledge to do some rudimentary analysis that he can explain to people with minimal math knowledge. This is all theoretical of course, but it'll be a good chance to try to do some BI before deciding to switch jobs. 
Yep, absolutely. Put it together in a nice report. Dark blue background, off-white text. Bold, simple statements with a few snazzy graphs and tables. The difference a piece of paper makes to some people is amazing.
Yeah if you've ever seen those cool info graphs online that you'll see from news sites or something designed to appeal to the masses, that's what you want. Extra time spent on analysis may be for naught if they're not interested. Extra time spent on the presentation will most likely get them interested. Just know what you measured and what your results mean. If they do start paying attention, you'll need to master your argument and explain it to them. Once you can get them to look at it and maybe use it, then you could go more in depth. 
My thought exactly. This isn't a post on data analysis, but a post on basic relational algebra in SQL. 
30% of BI deployments fail. From my experience BI projects need to be driven by department / division heads or higher and not by MIS / IT. They will befit the most from it and must be the driving force. Most BI implementations fail because of lack of use. When your heard of nerds is telling you about this cool new tool it does not get the same attention as when the CEO or your Boss's boss is laying down the law on what tools to use. Even high tech businesses fail in BI. I heard that Facebook's first attempt at BI failed badly.
&gt;Note /r/SQL does not allow links to basic tutorials to be posted here.
You only want results for patients who died. Right there that implies that the only way you're going to have death data is if you have a patient. Therefore, you should be able to do an inner join between the patient &amp; mortality tables. And to get only the death (read: final) instance, you'll want to use max(discharge_date). So the query will look something like this: SELECT patient.name , max(mortality.discharge_date) FROM patient INNER JOIN mortality ON patient.id = mortality.patid WHERE ... GROUP BY patient.name;
Change your SELECT statement from: SELECT b.STATUS_DATE, a.B1_ALT_ID, b.STATUS to: SELECT COUNT(DISTINCT a.B1_ALT_ID) or just SELECT DISTINCT a.B1_ALT_ID
None of these Venn diagrams accurately describes the fact that a join is really a filtered cartesian product. They tend to pretend that JOIN = INTERSECT, UNION, EXCEPT, etc.
Embrace the `APPLY` (in PG/Oracle it would be a `LATERAL`, but the point is the same): SELECT * FROM patients AS p CROSS APPLY ( SELECT TOP 1 * FROM discharges WHERE patid = p.patid ORDER BY discharge_date DESC) AS d 
Never seen that before, I am a self-taught hack. Will try that for fun, has lots of application in my work, and looks really efficient.
`CROSS APPLY` and `OUTER APPLY` (its left-join style brother) are super useful. It's one of those fairly recent additions that leaves me wondering what I did until now (answer: mucking with window functions). They're not always the most efficient though, so take a look at the plan. Edit: Realized I had an extra "d." in my thing.
Interesting. This doesn't seem like it would have very good performance, but I would be interested to see.
I'll have to see if Teradata implements it. Then again, it would be hard to leave `qualify` behind.
That sounds pretty awesome. Does TD also the one that lets you specify a window name like in the SQL standard like SELECT ROW_NUMBER() OVER (wnd) AS rn , MAX(col1) OVER (wnd) AS mx , MIN(col1) OVER (wnd) AS mn , SUM(col2) OVER (wnd) AS sm FROM tbl WINDOW wnd (PARTITION BY col3, col4 ORDER BY col7 ROWS 6 PRECEDING) or is that just wishful thinking?
I'm actually not sure. I'll look into it. It does seem to implement most standard SQL, but it's got it's own funky stuff as well.
I would love that. When I saw it in the SQL standard and realized it won't work in SQL Server I was very disappointed. It's one of the many reasons I'm trying to move us over to PG. That said, this sort of shorthand is one of the great uses for LATERAL/APPLY. You know how sometimes you have an obnoxiously long expression (usually involving dates) that gets used like six times in a query? SELECT &lt;long_expression&gt; , MAX(&lt;ridiculously_long_expression_that_eats_up_four_lines&gt;) OVER (&lt;window&gt;) , MIN(&lt;ridiculously_long_expression_that_eats_up_four_lines&gt;) OVER (&lt;window&gt;) , AVG(&lt;ridiculously_long_expression_that_eats_up_four_lines&gt;) OVER (&lt;window&gt;) FROM tbl becomes SELECT l.x , MAX(l.x) OVER (&lt;window&gt;) , MIN(l.x) OVER (&lt;window&gt;) , AVG(l.x) OVER (&lt;window&gt;) FROM tbl CROSS APPLY ( SELECT &lt;ridiculously_long_expression_that_eats_up_four_lines&gt; AS x) AS l And while that has the exact same plan, it's sooooo much easier to read and maintain.
What are you running the query in? There may be different output options depending on what you're using. 
we have always had consultants do our Great Plains and all we did was make sure to take backups on a regular basis and right before they started
T-SQL - Thanks 
Do you need the columns STATUS_DATE and STATUS? If not, do a count distinct (as josh_bsb suggested) on your current result (with multiple B1_ALT_ID). easy way: - put your current result in a CTE or temp table. - do a count distinct on your CTE/temp table. 
You either need to add a primary key to Table2 and select the MAX(Fee_Due) and then INNER JOIN or you need to just create a key with the RANK() function and filter it equal to 1. Something like below: ;WITH RankingCTE AS ( SELECT RANK() OVER(PARTITION BY Table2.Permit_ID ORDER BY Fee_Due DESC) AS RowRank , Table2.* FROM Table2 ) SELECT Table1.Permit_ID , Table1.Permit_Ref , Table1.Permit_Status , Table1.Permit_Holder , Table2.Fee_Due , Table2.Fee_Paid , Table2.Notice_Sent FROM Table1 INNER JOIN Table2 ON Table1.Permit_ID = Table2.Permit_ID INNER JOIN RankingCTE ON Table2.Permit_ID = RankingCTE.Permit_ID WHERE RankingCTE.RowRank = 1* Adjust your joins as necessary. The top portion uses a CTE (Common Table Expression) to order the Table 2 entries by the Fee Due date, more recent being lower in rank. It then filters out all rows other than the most recent. You will run into problems the way I have this written if you have two entries in Table2 with the same Fee_Due value. In order to remedy this, add additional parameters to the ORDER BY clause of the RANK() function (something like ORDER BY Fee_Due DESC, Notice_Sent DESC). These additional parameters will not come into play until a tiebreaker is needed. EDIT: Clarified my thoughts, which seemed messy to me.
Thank-you, this appears to work correctly! I need to go away and learn more about CTEs, I think. I can understand the code when I see it written there in front of me, but have trouble writing it from scratch without a reference point. Thanks again!
In 16 years of working with databases professionally, I have never written a formal query tree. I've looked at lots of query *plans* generated by the DBMS, but I don't think that's the same thing. From what I've gathered from a 30-second Google search, it sounds like she just wants a visual representation of the logic in your `JOIN` and `WHERE` clauses. I usually write those out in longhand as bulleted lists.
&gt; (or your own, several months down the line) so, so true... 
query tree? i've worked with SQL databases since 1987 and have never come across this
Thanks everyone. Even though I couldn't get the original query to work right, I solved it with a combination of the output and excel (remove duplicate). Your insights did help. Thank you
Haha seconded. I didn't realize how quickly I progressed in the beginning until I had to debug something I wrote early on. "Who the hell wrote this crap? Oh, it was me... Why didn't I just.... ugh... Starting from scratch."
What RDBMS are you using? I've never seen `==` used for an equality test in SQL. Short on time but check out `PRECEDING` and `FOLLOWING` for your `OVER()` clause.
He is asking if it's DB2/oracle etc.. Looks DB2 to me.
SQL is a platform-agnostic term for the language used to query databases - Structured Query Language. What **product** are you using? There is no RDBMS product simply named "SQL"
you're welcome remember, it won't work if column 2 and column 3 are different for same column 1 like: c1 | c2 | c3 ---------|----------|---------- aaa | red | yes aaa | red | no if this is the case, you have to add some extra code to avoid duplication
Have you tried using a cross apply? You can cross apply back to your table and select top two where the date in your cross applied table is before your original, and order it by date desc. Not at computer currently but I can add more detail if you would like.
Like the other responses, I've never heard of a query tree. She could possibly be talking about [B-Trees](http://use-the-index-luke.com/sql/anatomy/the-tree), but I *highly* doubt that since this is a beginner class. She could be talking about user the GUI query builder, like from [MS Access](https://www.mssqltips.com/tipImages/1482_4.jpg)
You could right click "Save Results As" into csv. That might eliminate any pasting issues. Also, you could wrap the columns in a REPLACE(REPLACE(REPLACE(column,CHAR(10),''),CHAR(13),''),CHAR(9),'') to get rid of any line feeds, carriage returns, or tabs.
that is pretty awesome! thank you so much. its a great example of what I am after. 
I feel like the other solution is too complicated for your requirements (you don't seem to care what values are returned for columns 2 and 3). select c1, count(*), max(c2), max(c3) from my_table group by c1 Unless I'm missing something.
Thank you both for your replies!.. I think this method might meet my needs a little better thank you also! Both methods work, and I definitely learned something today from both of you thanks again!
Is there a reason why you want to show c2 and c3 - if it doesn't matter which will be chosen when you have multiple c2-c3 records? I mean, why not just count and show c1 only? 
The real query has C2, c3, c4, c5 etc, and each of those will correspond to the value of C1, what i'm doing is searching for unique windows event error logs, and trying to keep track of how many times that error has occurred in the past month, it appears the Message itself is all i need to test on, the rest of the fields are typically in sync with the unique message thanks
If you ever need something deterministic (i.e., you want c2 and c3 plus the count, and c2 and c3 need to be the most recent updates) look into window functions, and in particular `row_number`.
You deserve, at least, Reddit Copper for this.
Hahaha that song came up on spotify radio just as I read this post. I made myself laugh.
True, but he didn't specify which version he's using, so I figured I'd mention it.
You can use an many criteria as you'd like for your `ON` clause; it just has to be something that will evaluate as TRUE or FALSE in the end. Also, you can specify a subquery for the the source table like MERGE dbo.target AS T USING ( SELECT s1.*, s2.colA, s3.colC FROM dbo.source1 AS s1 JOIN dbo.source2 AS s2 ON s1.id = s2.id CROSS JOIN dbo.source3) AS S ON T.id = S.id AND ( T.FirstName &lt;= T.LastName OR S.FirstName &gt;= S.LastName) WHEN ... Or, in the interests of making it easier to read, you can ship that off into a CTE: WITH source AS ( SELECT s1.*, s2.colA, s3.colC FROM dbo.source1 AS s1 JOIN dbo.source2 AS s2 ON s1.id = s2.id CROSS JOIN dbo.source3 ) MERGE dbo.target AS T USING source AS S ON T.id = S.id AND ( T.FirstName &lt;= T.LastName OR S.FirstName &gt;= S.LastName) WHEN ... As far as your three destination tables goes, that's a bit more complicated and may well involve three queries (one per table). Could you explain more about what you're doing with those?
What version? If it's 2012 you can use [lag](https://msdn.microsoft.com/en-us/library/hh231256.aspx) and [lead](https://msdn.microsoft.com/en-us/library/hh213125.aspx).
Thank you. Yes its 2012. I'll give this a try.
I think you need to find out what your instructor means when she says "query tree." 
Could you post the table schemas and an example of inputs and outputs?
I have left work for the day, my apologies. I will try to post some sanitized content tomorrow, especially if I cannot resolve this issue on my own. Thank you for your help so far, it has been effective.
After your CTE, SELECT q1.DateId , q1.Vector , q1.ErrorPage , q1.rownum FROM LogLines AS q1 INNER JOIN LogLines AS q2 ON q1.rownum-q2.rownum BETWEEN 0 AND 2 WHERE q1.ErrorPage="50199"
Don't be that guy that asks a question then disappears...
I'm not exactly sure but probably not. I added an example that looks close to what she wants, except that hers has the actual natural join bowtie symbol in them. Apparently I'm supposed to make these first then write my SQL statement from the tree.
Ah sorry, I had no idea this wasn't common database lingo. I've edited in an example of what she is talking about. 
Sorry, I'm here! I had a funeral to go to today and completely forgot to check Reddit.
Do you want - per B1_ALT_ID - the most recent status date and the alphabetically highest status? In your current code, your status and status date do not have to belong together. &amp;nbsp; Example: ALT_ID | Status | Date ---------|----------|---------- 13-10577| Approved | 2015-09-18 13-10577| Not Approved | 2015-09-17 13-10577| Plans Approved | 2015-09-16 Now, if you do a max on Status and Date, you will get the following: ALT_ID | Status | Date ---------|----------|---------- 13-10577| Plans Approved | 2015-09-18 So, the date you see is not for the status you see (and vice versa). &amp;nbsp; **Is this what you want, or... do you want to see the most recent ALT_ID (based on the date)?** That would be: ALT_ID | Status | Date ---------|----------|---------- 13-10577| Approved | 2015-09-18 
I don't quite understand what you want... &gt; And I want to first of all, get all Y for each X, and then for all of these values of Y every value of X and then set them to be one of the contained values (e.g. the first): You lost me at "*and then for all of these values of Y every value of X*". Also, I can't follow your example. :P. &amp;nbsp; Can you reformulate it? 
It's hard to explain, we want to use both X and Y as keys. Here's an example in Python: http://nbviewer.ipython.org/urls/gist.githubusercontent.com/jamesmcm/2554d5d4498b5d46d42d/raw/587ff552c34027e85ee199d5d8e5cb192e5550d1/gistfile1.txt
Took a look at your example. That doesn't make intuitive sense when I look at it and I have my own methods of organization. I hope she's not pushing you guys through a lot of work around this concept -- it's not something I've ever used or *would* ever use, at that.... Similar to what others have said, I will write out a few notes if my query is complex and I want to chunk it up, but basic things like * "I need to first rank and select top 20 ____ for ____ time period" * "Take that and average the ____ value" * "Compare that to the same calculation for ______ time period, highlighting the largest variances" something like that
Lol I am sorry that you have to deal with this issue... Seems like a DBA's nightmare. I don't have a resolution for you if neither ID is constant. Is there any way you can audit this incoming data so that you have some sort of historic log and can match up incoming transaction data to your historic dataset, where you would presumably have applied some form of unique ID?
Is the other key NULL in casecone of them has the value? You can use select coalesce(uid,tid) if that is the case. 
I guess what he is is kindly saying is you are working with a rather broke system.
Why is ('D', '5', 'E'), when 'E' isn't linked to TransID 5? Why is ('E', '6', 'E'), when 'D' is the first reference to TransID 6? Your logic is inconsistent!
If I'm understanding you correctly, you have a table like this: CREATE TABLE user_transactions ( user_id int NOT NULL, tran_id int NOT NULL, &lt;some other columns&gt;); Now, sometimes the `user_id` will change, so you want to get three columns, the user_id, the tran_id, and then a possible second user_id, which would just be: SELECT t1.user_id , t1.tran_id , t2.user_id AS user_id2 FROM user_transactions AS t1 LEFT JOIN user_transactions AS t2 ON t2.tran_id = t1.tran_id Now, it sounds like the transaction id could change as well, so you want to get all of the transactions that correspond to either user_id. That's pretty easy to add as well: SELECT t1.user_id , t1.tran_id , t2.user_id AS user_id2 , t2.tran_id AS tran_id2 FROM user_transactions AS t1 LEFT JOIN user_transactions AS t2 ON ( t2.tran_id = t1.tran_id OR t2.user_id = t1.user_id) AND &lt;some other conditions to keep things sane&gt; However, that only allows one change. If you're going allow for multiple changes, you're going to need a recursive CTE, the exact syntax of which depends on your RDBMS, but should go something like this WITH cte( user_id, tran_id, user_id2, tran_id2, rdepth, &lt;other columns&gt;) AS ( SELECT user_id, tran_id, user_id, tran_id, 1, &lt;other columns&gt; FROM user_transactions UNION ALL SELECT c.user_id, c.tran_id, t.user_id, t.tran_id, c.rdepth+1, &lt;other columns&gt; FROM cte AS c JOIN user_transactions AS t ON ( t.user_id = c.user_id2 OR t.tran_id = c.tran_id2) AND NOT EXISTS ( SELECT 1 FROM cte WHERE user_id = c.user_id AND tran_id = c.tran_id AND user_id1 = t.user_id AND tran_id1 = t.tran_id) AND &lt;some other conditions to keep things sane&gt; AND c.rdepth &lt;= 10 --Or however deep you want to go ) SELECT * FROM cte That said, I feel like something's not right here that should be advisable or necessary. Edit: Typos.
I take it CTE's work fine with an Oracle db? edit: Unfortunately its a legacy system so there's no chance we can change that now. It's tough enough having to learn ADF overnight but doing so with this schema has been a nightmare.
I believe CTE's should be fine. You could always use temp tables or do it in your joins though. Yeah it's sad that when stuff like that is wrong you so rarely get a fix.
Which DB?
If your DB supports analytic / window functions, and I'm understanding your poor explanation : select t.*, min(TEMPID) over (partition by USERID) ACTUALID from ( select q.*, min(USERID) over (partition by TRANSACTIONID) TEMPID from THE_TABLE q) t | USERID | TRANSACTIONID | TEMPID | ACTUALID | |--------|---------------|--------|----------| | A | 3 | A | A | | A | 1 | A | A | | A | 2 | A | A | | B | 2 | A | A | | B | 2 | A | A | | C | 4 | C | C | | D | 5 | D | D | | D | 5 | D | D | | D | 6 | D | D | | E | 6 | D | D | | E | 7 | E | D | | F | 4 | C | C |
Do the 6 other fields in the GROUP BY contain any unique data? it will only group (and then sum) the rows if ALL the fields are the same from row to row. Otherwise it sees the row as unique and can't group it, and as a result it appears to not sum correctly. 
DateTime is unique. I just took that out and it works like a charm! Thanks!
Sorry. I tried cleaning it up now so that the code shows up with the code formatting. As for when cursors should be used, unfortunately this assignment requires me to use a cursor to loop, so I have to.
 select top 10 Self-Description, cnt from (select Self-Description, count(*) cnt from table group by Self-Description) top10 order by cnt desc
i will try this. QUALIFY row_number() OVER ( PARTITION BY occupation ORDER BY num_members DESC) &lt;= 5; That worked, but i want to test yours
This is far too basic for this subreddit.
- what did you install exactly? the official package from the mysql/oracl website? - how do you know that there is a password? what do you mean with 'interacting trough terminal'? 
Maybe the point of the exercise is a hands on demonstration of why cursors suck?
Installed official package from Oracle Website. Sorry for my incorrect nomenclature. I am using Terminal ( command prompt utility) to try and reset the password. 
Hi - reset your root password by typing sudo su - in your terminal (console). This will ask for your password and switch you to root user. Then, as root, you can set a new password by typing passwd in the console. This is all for your root user password, though. Not the mysql root password. These can be (and usually are) different. When you are connecting to your localhost server, either to shut it down or to run commands in it, try mysql -h 127.0.0.1 -u root with no password first. Then add -proot to see if that is the password. If you can't get in to your MySQL server at all, kill the process (not sure whether 'service mysql stop' will work in OSX, but it might) by typing ps ax | grep mysql and then sudo kill &lt;# of process from ps listing&gt; there will be several processes and you should kill them all. Then you can restart MySQL server with authentication disabled, so you can get in there as root &amp; change your password, by typing screen -d -m sudo mysqld_safe --skip-grant-tables Now you'll be able to get back in to the MySQL server with mysql -h 127.0.0.1 -u root and once in there, use mysql; update user set password=PASSWORD("NEW_PASSWORD_HERE") where User='root'; flush privileges; quit Now your MySQL root password is reset, and you can shut down your MySQL server with mysqladmin -h 127.0.0.1 -u root -pNEW_PASSWORD shutdown and then restart it the normal way with sudo /usr/local/mysql/support-files/mysql.server 
I like a couple questions... "if your database queries are slow, how do you identify the problem and fix it? " usually this continues on to discussion of indices, and how they work and what tradeoffs they have. Junior guys never know how indices work. "How much faster does an index make a lookup?"; when someone says "Uh, I don't know... ten times?" that's how you know they're not experienced. My other favorite is "given a table with three columns : player, game, score. How do I get a list of the top score in each game?" that ones easy, so anyone who can't get that doesn't get the job. To separate the men from the boys, ask for the top three scores for each game. Then if they get it using window functions, I ask for a different solution since windows are too easy [correlated subqueries are awesome] Of note is that this is "can you write a SQL query" type questions. "can you administer a SQL database" type questions tend to be highly database-specific. "What are some backup strategies you might use for a 1MB database, vs some strategies for a 1TB database" is a good question
oh, i like your first question :-). as a complete newbie outside of the most basic sql queries, what do i look for when he starts talking about indices? 
Well, the real and correct answer about indices is that it's a tradeoff, and will use big-O notation [this is also super naive but it's easily an adequate answer]: * Without indices, insert is O(1) and lookup is O(n) * With indices, insert is O(log(n)) and lookup is O(log(n)) * Indices also cost an extra O(n) space EDIT: To track what specifically causes the slowdown, you'll want to see some variant of discussion about query planners, or EXPLAIN, or something like that
&gt; My other favorite is "given a table with three columns : player, game, score. How do I get a list of the top score in each game?" that ones easy, so anyone who can't get that doesn't get the job. To separate the men from the boys, ask for the top three scores for each game. Then if they get it using window functions, I ask for a different solution since windows are too easy [correlated subqueries are awesome] Could you answer that one?
Top score in each game is easy: SELECT MAX(score),game FROM tbl GROUP BY game For the second part, there's lots of ways to do it [windows are the most obvious and most simple, but not all databases support them, and it's a fun exercise to solve this without them]. Personally I like a simple correlated subquery: SELECT score, game FROM tbl outertbl WHERE score&gt;(SELECT score FROM tbl innertbl WHERE innertbl.game=outertbl.game ORDER BY score DESC OFFSET 3 LIMIT 1) There's another solution involving a cross join but that's not as fun: SELECT A.score, A.game, COUNT(A.score) AS rank FROM tbl A CROSS JOIN tbl B WHERE A.score&gt;B.score AND A.game=B.game GROUP BY A.game, A.score HAVING COUNT(A.score)&lt;=3
Check if MS Access has a datediff function (I don't remember if it does right now) then use something like datediff(year,birth_date,Date())&gt;21 
identifying when it becomes O(n^n) 
or a more efficient way -- WHERE birthdate &gt; #1994-09-19#
sure hope those aren't real people you've exposed
Frankly, I found the [Technet Article on Availability Groups](https://technet.microsoft.com/en-us/library/gg509118%28v=sql.120%29.aspx) to be fairly comprehensive. That said [this article](http://www.techrepublic.com/blog/the-enterprise-cloud/sql-server-2012-alwayson-high-availability-database-for-cloud-data-centers/) looks to have the TL'DR version of what you are looking for: * Install Windows Server 2012 in two computers or virtual machines (VMs), each with a single network interface card (NIC). * Create a two-node failover cluster without shared storage. You will need a cluster name and an IP address for the cluster network name. * Install SQL Server 2012 Enterprise on both computers as if they were going to be stand-alone SQL servers. -When you install, use a domain account for the SQL server services. -Open the Windows Firewall on ports TCP 1433 and TCP 5022. * Create a temporary "seed" database on the first SQL server using SQL Server 2012 Management Studio. This database will be used to establish the AlwaysOn cluster, and then can be deleted after the first production database is deployed. * Make sure the database is of the "Full" type model, and perform a SQL Backup job. * In the Management Studio, create an AlwaysOn Availability Group and an Availability Group Listener. (The Availability Group Listener is essentially the virtual (or clustered) SQL Server instance. There is a one-to-one relationship between availability groups and listeners.) -Assign a DNS name and TCP IP address for the AlwaysOn Availability Group and an Availability Group Listener. -Assign a shared network folder that is accessible to all SQL servers that will have AlwaysOn database replicas. * At the AlwaysOn High Availability node, right-click and select Add A Database To An Availability Group. If your database is of the Full type and has been backed up, the status will be "Meets Requirements". Click Next. * Select that you will perform a full synchronization, using the shared network folder you specified in step 6(b). Click Next. * Enter security information to access the primary database replica. Click Next, observe the validation and click Next, and then Finish. * Observe after a moment that new database replica on the secondary node in the SQL AlwaysOn availability group has been created, as seen in Figure B.
I see someone answered your question on SOF, but there is still a question remaining...why are you using a cursor? This is as an RBAR (linear) solution which is absolutely atrocious, SQL is a set based language, not a programming one. The whole thing ~~could~~ should be replaced with something like: *** DROP PROCEDURE IF EXISTS populateStudentData; CREATE PROCEDURE populateStudentData() BEGIN -- Create table -- NOTE: VARCHAR(15) is much to small for a lastname, should be larger CREATE TABLE IF NOT EXISTS studentData(gpa DECIMAL(3,2), lastname VarChar(15), classyear INT(1)); -- If the table already exists, remove all previous information to prevent redundant data TRUNCATE TABLE studentData; -- Insert the data from the student INSERT INTO studentData(gpa, lastname, classyear) SELECT student_gpa(studentNum), LName, class FROM student; END; *** Currently, the method you have requires at least three table seeks (assuming Student_number is indexed, otherwise **[three table scans](http://replygif.net/i/651.gif)**) from the 'student' table for *every student in there*, plus an additional table scan for the cursor and whatever aggregate overhead is needed by the student_gpa function. This work will also be done completely linearly as well, basicall The above solution will only take a single table scan and whatever aggregate overhead is required and can be fully optimized by the engine. **TL'DR - Cursors are garbage, and outside a few very specific maintenance/debugging cases should never, ever, EVER be used.** *Edit - I really hope student_gpa isn't written using a cursor either.*
There is no fit-all solution with this kind of thing. I've seen some recently where people have put a low resource DC and tertiary listener / AG node in Azure. Then if your primary server room goes down (given a DR senario): * Step up the Azure server specs * Promote the DC * Change the listeners over if required That is definitely the ridiculously TL'DR version of how it would work, but the benefit is that you have a redundant domain and database environment. *[Edit - Pictures!](http://blogs.technet.com/cfs-file.ashx/__key/communityserver-blogs-components-weblogfiles/00-00-00-96-45/7851.Topology.jpg)*
Awesome yeah this is basically what I've followed. I guess my confusion is with the Listener. Is that going to create a DNS entry itself or am I to that?
Here's the issue I have: http://i.imgur.com/Ey2HSUH.png Edit: ah ha, Please work with your domain administrator to ensure that: - The cluster identity 'SQL2014$' has Create Computer Objects permissions. By default all computer objects are created in the same container as the cluster identity 'SQL2014$'. - The quota for computer objects has not been reached. - If there is an existing computer object, verify the Cluster Identity 'SQL2014$' has 'Full Control' permission to that computer object using the Active Directory Users and Computers tool.
Does it specifically say use a cursor, and if so, is this a database specific class? 
What SQL syntax is that? I don't recognize it.
offset. I don't see it used much.
The sum filed contains space,it seems no a valid field expression 
Step 3 gets a count of a result set. The variable has a default value of -1, so it will always have a value. Also, as I mentioned, the post-execute breakpoint at the sequence container shows the variable has a value &gt;= 0. This is definitely not the problem.
I built a simpler version of what I'm trying to do [here](http://imgur.com/6gta10s). Those script tasks are just MessageBox.Show calls... I think this pretty much concludes up it's definitely not something I'm doing and is either a bug or an environment issue.... Any guesses?
Just curious, but what does an entry level SQL position like that pay? 
Good SQL trick is to learn how to do a join against the top N rows for each match. Kind of a pain in the ass, usually involves some cross apply or rank magic. Window functions and common table expressions are additional weird capabilities of the tools. Also make sure you have your data types down, difference between floating points and integers and numbers and currency and decimal, that sorta stuff.
&gt;Without indices, insert is O(1) and lookup is O(n) Well, if its not a hashtable lookup which one would find on a hash join. That could be a nice followup question too, whats the big O of a hashtable lookup, when would one encounter that, and what are the downsides of a hash join, something like that (follow up by merge join / loop join). 
You don't need a group by at all for that code. Window functions don't require them. 
yup, first page of the text book - however I have to admit, cannot get out of the habit of calling it sequel.
Difficult to answer as personally, I have never had the same interview content wherever I have been. It might be a strict, Q &amp; A, ie what is the difference between a clustered, non clustered index (common!) or maybe what is a CTE used for? Perhaps its more of a discussion based one, what makes a good clustered index, or how you do approach a problematic query? 
&gt; Also, what is a good way to test my skills? [sqlzoo](http://sqlzoo.net)
&gt; My "technical interview" consisted of a couple basic questions like 'given two tables Interns (id, fName, lName, managerID) and Managers (id, fName, lName), select a list of the full name of each intern and the full name of their manager. Pretty trivial query writing, they're mostly looking to see you think your way through it. No, they're trying to find out if you know SQL at all. I've had several interviews with people who said they were at an "intermediate" level of SQL knowledge but couldn't explain the difference between an `INNER` and `OUTER` `JOIN`, or what a primary key or clustered index did.
Below is my solution for TSQL. I expect that it doesn't differ much from PLSQL... drop table #Emps select EmpNo , First , Last , Report_to , Supervisor_name = cast(Supervisor_name as nvarchar) into #Emps from ( select EmpNo = 111, First = 'John', Last = 'Smith', Report_to = 111, Supervisor_name = NULL UNION ALL select EmpNo = 222, First = 'Jack', Last = 'Johnson', Report_to = 111, Supervisor_name = NULL UNION ALL select EmpNo = 666, First = 'Bob', Last = 'Roberts', Report_to = 111, Supervisor_name = NULL UNION ALL select EmpNo = 888, First = 'Rob', Last = 'Loblaw', Report_to = 222, Supervisor_name = NULL UNION ALL select EmpNo = 999, First = 'Don', Last = 'King', Report_to = 222, Supervisor_name = NULL ) e select * from #Emps update #Emps set Supervisor_name = concat(boss.Last, ', ', boss.First) from #Emps left join #Emps boss on #Emps.Report_to = boss.EmpNo select * from #Emps Result: http://i.imgur.com/000sxWp.png Had to cast Supervisor_name to nvarchar, I was getting errors ("Conversion failed when converting the varchar value 'Smith, John' to data type int.").
I had written this query earlier (with exception to the is null clause), and I received "ORA-01427: single-row subquery returns more than one row". It makes sense in my head but it didn't work for me. The inner query does return multiple records because each record is the name of the employee's supervisor. 
Ah OK, you need a correlated update like this then : update (select t1.SUPERVISOR_NAME, t2.LAST||', '||t2.FIRST as NEW_SUPERVISOR_NAME from EMP t1 join EMP t2 on t2.EMPNO = t1.REPORTS_TO where t1.SUPERVISOR_NAME is null) t set SUPERVISOR_NAME = NEW_SUPERVISOR_NAME 
I think I get where you're going with that, but I receive the error: "SQL Error: ORA-01779: cannot modify a column which maps to a non key-preserved table 01779. 00000 - "cannot modify a column which maps to a non key-preserved table" *Cause: An attempt was made to insert or update columns of a join view which map to a non-key-preserved table. *Action: Modify the underlying base tables directly." When searching for what that means, I came across this: You can update a join in Oracle if the following conditions are met: -Only one base table is updated -All other tables are key-preserved: each of them must have at most one row for each row of the base table. I'm sort of at a loss for understanding about what "key-preserved" is. 
Can you expand on those statements?
The inner query that ziptime wrote should never return more than one row unless EMPNO is not unique in your table. In that case use SELECT DISTINCT.
This is maybe not the kind of answer you were looking for, but, I've always found that brand new, box fresh underwear and socks give me a sense of comfort and relaxation that really helps my confidence.
My correlated update is effectively an update on an inline view (the joined table is effectively a view). Oracle allows updating of views if they are key preserved, which means the row from the base table will appear AT MOST ONCE in the output view on that table. I'm replying from my phone, I'll send another approach when I'm at my computer.
Sure. First, just enough about big-O notation to let you understand what I mean. If I tell you I'm thinking of a number between 1 and 100, and you have to guess, but I can't say "higher" or "lower"; you start at 1 and keep going up until you get to 100. You expect to make about 50 guesses on average. No matter how big the range is, you expect to take about {range/2} guesses, right? This is **O(n)**; given an input size of n [100 in this case], your algorithm will take, usually, something of similar order of magnitude to the size of the input. Doubling the input size will double the number of guesses you have to take. Now I tell you I'm thinking of a number between 1 and 100, but I *can* say "higher" or "lower". You start at 50, I say "higher", you go to 75... each time, you split the remainder in half. This will take, usually, log-base-2(n) guesses. Note that doubling the size of the dataset will only increase the number of guesses you have to take by 1, whereas the previous example takes double the number of guesses. So this is an **O(log(n))** algorithm. The other big-O I present is O(1). This is "no matter how many things you have, this operation can be done in constant time"; for example, "show me the first number in a list of numbers" is **O(1)**; no matter how long the list is, it takes the same amount of time to look at the first one. So, to get to the point: by default, with no index, insert into a table is O(1); doesn't matter how many items you currently have, "slap another row on the end" is a constant time proposition. To find an item in the database, you have to work through the whole table; you don't know what order they're in [remember, the insert was "slap one on the end", and the rows could have been inserted in any order]. So it's O(n) lookup. An index works by keeping an ordered copy of columns that are declared as part of the index. So if you insert the numbers 6,5,1,2 into a table, the index is a copy of that that says "1,2,5,6". Note that since it's ordered, I can do lookups on that in O(log(n)); I look at the middle item in the index, and say "higher" or "lower", then look at the middle of the next half, etc. Of course, I just said it's a copy; so you have to eat O(n) space on disk to store it; the size of the index is equal to the number of rows in the table. And, finally, insert; note that to insert into an indexed table, I have to do the table insert [slap-it-on-the-end, O(1)], but I also have to do the insert into the index. In order to do that, I have to first *find where to put it* [which is the same O(log(n)) search I did for a lookup], then put it in. This is pretty much ELY5, there's obviously a lot of other things that go into this. But anyone who could explain that to me would get the job as a SQL developer. Finally... you asked for clarification on all my statements, so the last bit: * Your database compiles SQL down to code it runs internally [since SQL isn't exactly machine language]. Part of this compiler is a "query planner", which figured out, given your SQL, what the best way to run it is. Query planners are just like optimising compilers for other languages. The SQLite one is spectacularly well described, [here](https://www.sqlite.org/queryplanner.html). * To get insight onto what a query planner did, and to help identify what your database is doing and thus why queries are slow, you use something like "EXPLAIN"; for example, your database might support you doing "EXPLAIN SELECT * FROM tbl", which instead of doing the SELECT will show you the output of the query planner. You dig through that, looking for telltale signs that are known to be slow, to help figure out what's going wrong, and, for example, which columns to put indices on. * Tell tale signs of slow stuff commonly include "full table scan" [which is the O(n) operation], or "using temp B-tree" [which is a bunch of stuff that includes making a temporary copy on-disk of all your data, typically going to be O(n*log(n)) or worse, in addition to the slow disk usage]
Can you share more about the department as to what it does? What metrics do you need to collect? What these are helps answer the next question. What factors would control how data can be entered (e.g. by each user, a lead or designated, entirely without direct user input)? How much granularity is needed in your metrics?
150+ people is getting into the realm of [Microsoft Project](https://products.office.com/en-us/project/project-and-portfolio-management-software). It's not cheap, but it has been honed to a pretty flawless product. Alternatively, or in conjunction with, you could use SharePoint or products like [Jostle](http://www.jostle.me/) and create pages where people update and communicate through. Project also integrates very well into SharePoint. That said, I'd highly recommend against developing your own, especially in given the size of the company; there are just too many use-cases / dynamics / workflows / etc to account for. You'll end up spending much more on man-hours for development and maintenance than you will just licensing a solid product. 
If you are using SQL Server for something like this for month by month: *** SELECT t.Year, t.Month, t.TransactionType, t.tSum, e.eSum FROM (SELECT YEAR(transactionDate) AS Year, MONTH(transactionDate) AS Month, TransactionType, SUM(transactionValue) AS tSum FROM transactions GROUP BY YEAR(transactionDate), MONTH(transactionDate), TransactionType) AS t CROSS APPLY (SELECT YEAR(estimateDate) Year, MONTH(estimateDate) AS Month, estimateType, SUM(estimateValue) AS eSum FROM salesEstimates GROUP BY YEAR(estimateDate) Year, MONTH(estimateDate) AS Month, estimateType) AS e WHERE t.Year = e.Year AND t.Month = e.Month AND t.TransactionType = e.EstimateType AND t.Year &gt; 2014 AND t.month &gt;= 1 *** Obviously without knowing what the shape of the data is, it's hard to guess. 
I'm storing the calculated name data because it is part of a data flow from one system to another, and the application depends on it being there. The real table has about 120 fields and 160,000 records. Anyway, a few min ago after consulting with my co-worker who is smarter than me, he corrected the mistake. I used your first code example which did work, but when applying it to my real data, I reversed the join logic accidentally. It wasn't easy to see it because the production data is of course way more involved and has stupid naming conventions vs the simplistic example I provided, but the theory is the same. Also I was missing something with aliasing that definitely mattered, but not sure what that was. SQL is about 20% of my job but sometimes I get hit with things like this that I get hung up on. I think you've provided another solution for me a while back. You must be one of the smart ones. Thanks for your help! 
Thank you so much, that's exactly what I was looking for, and it solves my problem! Sorry for not telling the initial structure of the tables. And in fact I am using the exact table set-up you provided. My initial thought was saving the result (user-info joined with league) in an array and use #n for-loops to fill in the table: foreach($users as $user) { if($user-&gt;league_id == 1) { ... } } But your solution is much more efficient, thanks again!
Thanks for this reply. Glad I stumbled across because I was about to post a question about that exact thing -- grouping in an SSRS grid or aggregating in a stored procedure. I inherited a project where he grouped in SSRS and I badly want to rewrite it all. Hate to rewrite things but good to know it's the right thing to do.
Hmm, first off, I second /u/Coldchaos's thoughts on just buying a license for the right tool. But, if your boss is dead set on keeping everything in house, this sounds like a project better suited for your dev. team. You could be the one to spec everything out, but they would do the heavy lifting. If you don't have a dev. team, you may be able to look into contracting a team to create this as a web app. Bluntly, I do *not* believe that this should be your function, but you would probably need to be involved in its creation. That sounds like a tool for project managers. Anyway, we probably need more specifics as /u/PandemicVirus pointed out. With that, we could point you in the direction of what tools might fit the bill (however, Coldchaos already got you started there) :P I suggest looking into those options and pricing out others. See what you can present to management. In the end, you may have a hybrid where you get a cheaper product to process most of the data, but the project manager can handle the nitty-gritty details.
Appreciate the input. I concur.
Yea, I really hate the idea of silo this data in access data bases here and there. Apparently, we have PeopleSoft somewhere, but no one seems to know where or who has access to it. Regardless we still need to capture much more information then is currently being gathered.
I am looping through each element of a table variable and then performing inserts using that element. 
Have you tried using a merge statement? Or you could probably get away with just one insert statement, no need for a loop.
Never mind. Turns out break and continue do not work the way I thought they do.
select case when x then a when y then b **else 'notblank'** end
WHERE. To get rid of rows, you need a WHERE clause. SELECT CASE WHEN x THEN a WHEN y THEN b ELSE NULL -- (implied by omission of an ELSE clause) END text FROM t WHERE x OR y
I would argue for #3 in this case (if I read the OP right as in the favorite selection is optional and 'no favorite' is a valid choice as well).
swear that wasn't there before, je suis le blinde
I'd do #3.
I would put the CASE in a subselect, and filter on 'text'. &gt; WHERE text is not null Otherwise you have to change the WHERE everytime the CASE is extended.
Interesting, I'll check that out. Thanks. Oh and it's worth noting we use Lotus Notes :( 
IMO, it depends on the application. Namely, is the favorite color an optional field (option 2), or is the user forced to pick a color. If it's the latter, the user choice is what's being recorded, and that choice could be "no color." There's also the issue of his not specifying which version of PG he's using. In older versions (pre-9.0b2, I think), `WHERE col IS [NOT] NULL` wouldn't use an index. This latter was my reason for suggesting 3 (purity is nice and all, but this is isn't a religion), though I probably should have asked.
I'm using version 9.4. With that said I agree it would depend on the use-case but in my case I think I'm probably best off with #2. Thank you both for the input. 
&gt; coming out of a someone with a technical background how the fuck do you know anything about my background “Databases, Types, and The Relational Model: The Third Manifesto”, by C.J. Date and Hugh Darwen (3rd edition, Addison-Wesley, 2005) 
You had me intrigued. So, the complicating factor was your specifics about... ... &gt;Item Price x Quantity of each Item ID on the Latest Date registered in the table. You needed a sub-select of dates grouped by Item ID and only grab the most recent. Ignore the table creation, just look at the last select statement and see if you can hijack it to fit your data. declare @temp table(Trans_ID numeric(15,0),Trans_Item_ID numeric(15,0),Date_Trans date,Item_Quant numeric(9,0)) declare @item table(Item_ID numeric(15,0),Item_Price numeric(9,2)) insert into @item select 1,7.80 UNION select 2,3.00 UNION select 3,1.75 UNION select 4,0.50 UNION select 5,3.50 UNION select 6,4.23 UNION select 7,6.01 UNION select 8,6.36 UNION select 9,3.34 insert into @temp select 1,1,GETDATE(),3 insert into @temp select 1+ROW_NUMBER()over(order by N)--Trans_ID+(1+(ABS(CHECKSUM(NEWID())) % 4)) ,Trans_Item_ID+(1+(ABS(CHECKSUM(NEWID())) % 9)) ,DATEADD(DAY,(ABS(CHECKSUM(NEWID())) % 100),Date_Trans) ,Item_Quant*(1+(ABS(CHECKSUM(NEWID())) % 9)) from @temp cross join (select top 100 N = OBJECT_ID from sys.objects) A /* select Trans_ID,Trans_Item_ID,Date_Trans,Item_Price,Item_Price*SUM(Item_Quant) from(select Trans_ID ,Date_Trans ,Trans_Item_ID ,Item_Quant ,Date_row = ROW_NUMBER()over(PARTITION by Trans_Item_ID order by Date_Trans) from @temp) A */ select Trans_ID ,Trans_Item_ID ,Date_Trans ,Item_Price ,Cost = Item_Price*SUM(Item_Quant) from(select Trans_ID ,Date_Trans ,Trans_Item_ID ,Item_Quant ,Date_row = ROW_NUMBER()over(PARTITION by Trans_Item_ID order by Date_Trans) from @temp) A join @item on Trans_Item_ID = Item_ID where Date_row = 1 group by Trans_ID ,Trans_Item_ID ,Date_Trans ,Item_Price -- ,Item_Quant -- Sorry, this is indeed needed. edited. ,Item_Price order by Trans_Item_ID Edited for formatting. 
1. why swear? 2. I don't see us discussing impedance mismatch (and I won't participate in those anyways) - this is strictly classic, no need for any extensions ("D" instead of SQL and whatnot) And I'm actually going to quote the above book: &gt; A scalar data type (scalar type for short) is a named set of scalar values (scalars for short). ... D shall provide facilities for users to define their own scalar types (user defined scalar types); A classic workaround for those is a reference table with identity keys and some descriptive attributes for the elements. http://www.dcs.warwick.ac.uk/~hugh/TTM/TTM-2013-02-07.pdf 
You could check out [Toggl](https://toggl.com/) if you need time tracking.
no, only when I try to do the cross tab query. The ProductionReport runs fine. It does put out #Num! where it divides by zero, which is ok because at least it runs.
Ok thanks, i'm looking at google and take a read to see how i can adapt it to my needs.
Edited my question to be more clear, sorry.
Yes i want: each unique ItemID based on the Latest Transaction Date along with Quantity, UnitPrice and TotalPrice registered on that latest transaction.
In the ProductionReport, I added this: WHERE [Production Minutes]&lt;&gt;0 AND [Volume Processed]&lt;&gt;0 This seems to have fixed the problem. I am still debating on whether or not I need anything that contains a 0 value for future. It would still be nice to solve my original problem, but for now I think I have a work around...
LOL "typo" i call 'em brain farts ;o)
That is correct. Case statements are incredibly useful for handling goofy scenarios you may come across or just generally allowing you to pass in new logic for various scenarios. The structure is CASE WHEN &lt;qualifying logic&gt; THEN &lt;if qualifying logic evaluates to true, select this value&gt; ELSE &lt;if qualifying logic evaluates to false, select this value&gt; END &lt;AS ColumnName&gt; You can have as many WHEN/THEN statements as you'd like. Sample statement below. SELECT CASE WHEN State IN('TX','MS','AL','GA','FL') THEN 'Gulf Coast' WHEN State IN('CA','OR','WA') THEN 'West Coast' WHEN State IN('SC','NC','VA','MD','DE','NJ','NY','CT','RI','MA','NH','ME') THEN 'East Coast' ELSE 'Not a Coastal State' END AS CoastalRegion FROM Location
Thank you!
must be the American English pronunciation versus the British English pronunciation.
You did an old school join by cross joining all the tables and putting the join conditions in the where clause. Instead you should do from books inner join table on table.col = books.col for each join. This way the join predicate is adjacent to each join and you no longer need a where clause. 
It's more of a convention. But eventually you'll end up doing left joins, which requires the join syntax, so mixing the two is just confusing. Also if you forget to add a join condition for one table in the where clause the results can be disastrous and potentially even unnoticed! Using the join syntax avoids those issues, and once you start doing complex queries you'll appreciate having the conditions adjacent as well. 
Use a merge
Teachers, those that couldn't hack it in the real world.
[SQL Joins](http://www.w3schools.com/sql/sql_join.asp) [SQL SUM() Function](http://www.w3schools.com/sql/sql_func_sum.asp) [SQL GROUP BY Statement](http://www.w3schools.com/sql/sql_groupby.asp) You will get a lot more positive response if you show that you have at least tried to do your own homework before posting here. Show us some code, even if it's terrible.
As omegatheory pointed out, be consistent in your joins. And I would recommend using aliases for your tables, it makes everything shorter. :P. And... use the table name in your select, it makes reading easier. I would also change the order of the joins. select [Author last name] = c.Last_name , [Title] = t.Title from title t join author a on t.AuthorID = a.AuthorID join contacts c on a.account_no = c.account_no 
Thanks for the reply. You're right, this would work. I'll have to test out the performance hit, as this is being done for millions of rows. 
A filtered index might help with that.
Can't believe this didn't cross my mind. Makes sense, I'll give this a shot. Thank you! *edit* I did use the first section as part of my solution. I also created a scalar function I called CharIndexN, which is CharIndex with a parameter to specify Nth occurrence, to replicate the second section. Thanks again for your help.
Okay, so this is getting into territory that I'm not very familiar with. From my understanding of indexes, they should not be applied where values will change. In this case, the values for each of these fields could be changed a few times in a year, for each row. On the surface that seems to make this a moot point, but am I incorrect in that assumption? Thanks again!
This really is the right answer. If you want good comprehensive overviews, you get what you pay for. Plural sight does a fantastic job and I'm pretty sure if you looked hard enough you can find a one month free deal. ;-)
This would work: SELECT date, sum(case when log_text like '%normal%' then 1 else 0 end) as normal, sum(case when log_text like '%abnormal%' then 1 else 0 end) as abnormal FROM logs GROUP BY date
Well that looks fancy. Let me fool around with that. I've only ever used CTEs for recursive queries.
Yep, this. It will make your life so much easier. 
Even if possible, you need to not do that, it's stupid and of questionable ethics. 
Make sure that the line above `;with normal as` is empty; put two enters (instead of one) after `CTEs:`.
Wait... you have an AD (which you use to manage users I presume), and you also have an MS Access file with passwords... Why? And why would you track changes and even store passwords? Am I missing something?
Who is the "WE" you are referring to? If your employer is pushing out this policy it stands to reason they want to improve security to avoid the types of things the group you are in are currently doing. Does your employer know what you want to do? I highly doubt it. If I found out a team was trying to do what you are currently doing, I would be terminating a bunch of you. WTF would you be storing user passwords? That would make it easy for anyone who has access to that database to do questionable things.
It's not questionable. It's flat out wrong. 
There is no legitimate reason for doing such a thing. Users set their own passwords in active directory. AD should be configured to require password changes every 90 days. (Edit: AD stores credentials securely) Done. There's no valid reason to store the passwords in plain text in a database, and no way to do it without essentially what is a man in the middle attack, using a custom application to set AD passwords instead of built in Windows functionality (basically, the app would accept the credentials and then write them to the database and AD). If your company is subject to any kind of regulatory oversight, doing this is probably a massive violation. You've essentially negated all security by keeping plain text passwords available. 
Do you want a lawsuit? OP, the fact that you even posed this question is beyond reckoning. 
This is just terrible. May as well just store the password on the website under /SQL
In oracle we say Procedural language as PL/SQL. But, in DB2 we need to say like SQL Procedure. How to declare data types DECLARE v_salary DEC(9,2) DEFAULT 0.0; DECLARE v_status char(3) DEFAULT ‘YES’; DECLARE v_descrition VARCHAR(80); DECLARE v1, v2 INT DEFAULT 0;
I agree with just about all other posters here on their stance wrt password storage. However, syncing just about anything else between AD and SQL Server is certainly possible via [a linked server construction](https://www.mssqltips.com/sqlservertip/2580/querying-active-directory-data-from-sql-server/). This may come in handy to check users and groups, create phone numbers lists, or (especially) perform quality control. I've seen some AD stored datasets and the quality was always extremely messy.
You want to sync your SQL (Access) DB *to* AD? That is, update AD from your database? Or have database tables that mirror AD? Having database tables that mirror AD is useful sometimes. In SQL Server, I have a linked server using the "OLE DB Provider for Microsoft Directory Services" and do just that - periodically download data from AD into database tables for querying. I used to query the linked server directly as data was needed, but this proved to be slow, and sometimes required cursors for certain types of queries. Performance-wise, it was much more performant to query the data from SQL tables at the expense of it not being 100% "live". If you want to update AD from your database tables, I would use a Powershell script to do that, though I suppose since you are using Access, you could do it in VB code also. Now we come to the password issue... You have already been judged (rightly so) about storing passwords in an Access database, so I won't deal with that. But... you will not be able to query AD and get passwords from there to compare to your database. You cannot unencrypt the password stored in AD. However, you could query AD to get the age of the password to determine when it has been changed last. You don't need to know the password from what it sounds like, you just need to know the age. 
You could try configuring multiple agent jobs to run simultaneously although whether DBCC will throw a face at concurrent operations, I don't know. Plus, you will dramatically increase server load.
I agree, this is what my department head wants. I'm just the grunt that has to research it 
I believe the reason he wanted to do this is so that we can get into a users profile to do updates and install software. I know there are ways to do that without having to establish something like this, "and preferably i would rather not do this due to the lack of security. 
I expect you could make ADMINISTRATION.PRES_ID nullable. The you could create the Administration then update with president later. Alternatively you could have a link table between ADMINISTRATION and PRESIDENT. 
Ah, thanks! ;with normal as (select date, count() as [count] from logs where log_text like '% normal%' group by date), abnormal as (select date, count() as [count] from logs where log_text like '%abnormal%' group by date) select a.date, a.[count] as normal, b.[count] as abnormal from normal a join abnormal b on a.date = b.date 
You can't add a domain / infrastructure admin account to all your assets for this purpose? You can't use GPOs? You can't use WSUS? This is all kinds of fucked up and your IT director should be fired.
&gt; I believe the reason he wanted to do this is so that we can get into a users profile to do updates and install software That's bull. Those updates &amp; installs should be pushed out with GPO, WSUS, or if it has to be a deskside thing, an alternate user account used by admins. Those methods are **more** secure than saving every user's password in a database. Whoever is making these decisions needs to have the whistle blown on him.
Then you tell him it's nearly impossible and unethical to boot.
Sounds like he is stills trying to manage the network like he is running LAN Manager and WfWG 3.11 in the late 80's....
teach a man to fish. trying to learn new techniques.
The two queries appear to do two different things. Your version answers the question as posed - show the years in which he was in more than 2 movies, and how many movies he was in during those years. The "more complex" version tells you which years he appeared in the *most* movies. IOW, if he was in 10 movies in 2015, 9 in 2014, and 10 in 2011, it would return 2015 &amp; 2011.
So its just by coincidence then that the two queries yield the same result? That clears it up. Thanks!
Take a look at SQL BOLT. Great interactive tutorials and I find them better than SQL Zoo.
I came from 0 sql experience to landing a job that is all SQL all day. I did this by going to Coursera and doing the intro to sql course offered by stanford university. It starts very high level and then gets rather deep. More than just learning via watching videos I downloaded the adventure works database offered by microsoft and gave myself tasks scenarios and built tasks from them that I had to do research on and figure out the best way to go about it. It helps you learn practical application to the things you learn in the videos. 
I'd do it a little different I'd use a CTE to determine the max(timestamp) for table A then Join that to a join on table a and B on userID so something like WITH CTE AS ( SELECT userid, MAX(timestamp) as timestamp Group by UserID ) SELECT a.Userid, a.cardnumber, a.position, a.status, b.status FROM a JOIN b on a.cardnumber = b.cardnumber join cte on a.userid = cte.userid and a.timestamp = cte.timestamp where a.status &lt;&gt; b.status EDIT: forgot group by and naming the cte column.
Imo, learn using whichever dialect the potential employer uses.
It's my username all the time. I'm not sure anymore if I'm always working or never working. It is work related, but more of a personal project than an assignment.
A variation on this I have used in the past would be the use the LEN() function. For example: SELECT CAST(dateField as DATE) ,COUNT(CASE WHEN LEN(log_text) &gt; LEN(REPLACE(log_text,'abnormal','')) THEN id END) AS normal ,COUNT(CASE WHEN LEN(log_text) &gt; LEN(REPLACE(log_text,' normal','')) THEN id END) AS abnormal FROM logs GROUP BY id, CAST(dateField as DATE) With large amounts of rows I've found this can perform better. 
Break it down to smaller groups including the statement it seems to be missing. Perhaps setup an easy if / then statement. If it still misses it then break that down. You need to figure out why it's missing it with YOUR data. 
Is your select handling null values?
I am an expert with Muscle Databases. As well as PostRescue-L, and Minecraft Treacle Server. Sign me up.
I'm more of an Oral Cow guy, myself.
I've spent the past two years on Squeel Server, but we're trying to move to Poster Squirrel.
Thanks for the responses. I'm not familiar with writing expressions so I thought it was an obvious mistake like missing a comma or parentheses. Someone on another site asked why I didn't use a function instead. I never wrote one before so after a couple hours of looking up example functions and experimenting I came up with this and called it in the report. Works perfectly. Public Shared Function CheckValue(ByVal CTP As Decimal,BH as Decimal,CR as Decimal) As Decimal If CTP = 0.00 AND BH &gt; 0.00 AND CR &lt;=0.00 Then Return -1 If CTP = 0.00 AND BH &lt;= 0.00 AND CR &lt;=0.00 Then Return 1 If CTP = 0.00 AND BH &gt; 0.00 AND CR &gt; 0.00 Then Return 1 If CTP &lt;&gt; 0.00 AND BH &lt;= 0.00 AND CR &gt; 0.00 Then Return 1 If CTP &lt;&gt; 0.00 AND BH &gt; 0.00 AND CR &lt;= 0.00 Then Return -1 Return 1-(CTP/CR) End Function
Psh, get with the Nose Equal generation, man. Bigged Eighta is the future. 
Because I am then retrieving the data for MPDF to create a PDF, however, do to formatting issues and various constraints I need to put two variables with an &amp; between them into one piece of information. Retrieving the data seperately will cause line breaks in the PDF 
Then you'll just want to use something like: INSERT INTO test.temp_holder (Weeks) VALUES ($week_number_one + ' &amp; ' + $week_number_two) 
What RDBMS? Either way, you'll want to force them to be string types and use concatenation (the '+' is often overloaded to handle strings, but not always. Something along the lines of: CONCAT ( string_value1, string_value2, string_etc) 
&gt; My client a world leader in their chosen sector I do not think this is the only problem here.
http://imgur.com/0Zz3g10 heres screenshot. Its noticable which are PK and FK gues 987 = 9878987 didnt put the whole thing
It worked thanks! could you explain to me how it actually works? i thought join just adds the tables together at a common colomn? what exactly happens and what when you add 2?
A booking needs to know the hotel and the room, as hotels have different rooms at different rates. You weren't specifying that.
The problem is the joins. The hotel to room relationship is one to many, and the hotel to booking is one to many. When you join like this, the database doesn't know which room is for which booking. In fact, based on the joins, I would assume one books the entire hotel and gets all the rooms.
 I noticed that they use JOIN. Is that bad style? I always write that as LEFT JOIN because it's more explicit. 
`JOIN` is, by default, `INNER JOIN` in every SQL implementation I've used. `LEFT JOIN` is an `OUTER`, at least w/ SQL Server. I imagine that's the case with others as well, since there's [no such thing as a `LEFT INNER JOIN`](http://stackoverflow.com/questions/2389204/left-inner-join-vs-left-outer-join-why-does-the-outer-take-longer).
Is it a big database? Joining on strings can be slow. See if those columns are indexed, try filtering by "top 5" or whatever the equivalent syntax is for mysql, and try doing the first couple joins in a separate query just to test them.
I spoke too soon. It is the last join which is giving me the trouble. I edited the code again. The problem is that I need to query WASTE_CHAR_2007 and WASTE_CODE_2007 with separate values from the MAN_GEN_2007 table. But from what I understand, an INNER JOIN only creates a virtual table using data from the previous inner join. Or something like that. So the 3rd inner join is querying data from a table which doesn't contain that data. I think.
I don't really understand the question or what you're trying to achieve here, but on the face of it it seems like a really bad idea. Whats the problem with having the identity as the clustered index, and then set a composite unique key which covers each column you are trying to protect? 
I'm trying to speed up my query return time. Also it's much easier to foreign key to each table. I should preface by saying I'm not programming in SQL, all my code is in C# and I access the database using entityframework only My main table has a candidate key that's basically 6 of the columns. So rather than doing a query on all 6. I have a simple way of identifying each row by a unique key that is a combination of the 6columns but represented as numbers. clustering or indexing on them seemed complicated and took too much size-wise. clustering on the primary identity key seemed useless since I realized I can get much more efficient querying by clustering on my key as an index. Reading up on clustered index I understood that my cluster should be so that my query can access rows close to each other. clustering on my primary key identity will disperse the data without much sense since it's just auto increment. So if I want a certain days data I can query the clustered key by speciying a range on my key and then the return values are very closely grouped. One part of the key was just to group the whole data into two different types. So rather than specifying a number, say 0 and 1 somewhere in the key format, I assigned one type to have negative key and other to have positive key. Now this introduced new problem as my cluster breaks down something like (gfedcba-abcdefg) the first part is a mirror to the other. so for rightHandSide group (the positive group) I'm trying to get c-TO-f on the other I will try to get f-TO-c So thinking of something like that left me uncertain of any benefits and I just ended up using 0 and 1 to identity each entry and this increased my number of digits in the key to 20. decimal(20) and decimal(19) have a huge size difference. I'm not really sure what my question is exactly. But I guess I'm looking for some guidance in how to think in terms of SQL. 
Thanks for the reply! What about any decent SQL hosting services? Can one of these be recommended?
Use something like Azure instead
This sounds like a lot of premature micro-optimization which may or may not work. Do you have performance issues presently? You almost certainly won't outsmart the query optimizer, but you *can* do things that will lead it into making poor decisions.
The website is pretty good, albeit with some dumbed down information for the not so technical. However, it does seem to be a marketing platform for the website owner's book. Can't blame him though I suppose.
INSERT INTO test.temp_holder (Weeks) VALUES (CONVERT(varchar,$week_number_one, 101) + ' &amp; ' + CONVERT(varchar,$week_number_two, 101)) Using Coldchaos's query and adding ",101" after the week columns will format the dates into the standard mm/dd/yyyy
I'm not up on mysql syntax, but try putting the table name inside square brackets instead of single quotes: $query = mysqli_query($connect, "SELECT * FROM [319567]");
it's hard to know whether to recommend ETL or insert / conversion guides without more details. how complex are we talking here? going from one database to another or is everything taking place in the same database?
change SELECT * FROM '319567' to SELECT * FROM `319567` those are **backticks**, mysql's (non-standard) method for delimiting problematic identifiers
Producing two rows with `VALUES`: VALUES (1), (2) Produces 1 2 [SQLFiddle](http://sqlfiddle.com/#!15/9eecb7db59d16c80417c72d1e1f4fbf1/3453)
I think you're heading down the wrong route with this design personally. I've never encountered a query where the correct way to optimise it was by encoding all the candidate fields into a single (unsuitable) datatype and then using that as a PK. It also makes your database much less self-documenting.. if a new dev comes along he's going to look at this encoded weirdness and just think wtf. If I were you, I would just put an integer identity field on the table, and use that as the PK. Its compact (bearing in mind that *each* non-clustered index already has to contain a copy of the PK for the row). It can still be very fast assuming its indexed properly It also ensures your table grows contiguously, minimising page fragmentation (which in turn means more pages can be crammed into the cache, meaning your query performance is better). Then put a unique index covering the 6 fields you want included. If you're still not happy with the query performance with that design, post the query aswell as the execution plan (using EXPLAIN &lt;query&gt;for MySQL, or SET SHOWPLAN_TEXT ON; &lt;query&gt;; in MSSQL etc) &gt; Also it's much easier to foreign key to each table. In the short term maybe. But just do it properly now and you'll save a lot more headaches in future. If you *really* want to be able to query rows using this compound decimal idea, you could then create a VIEW which compounds the fields allowing you to select them that way. Like I say though, I'd avoid doing that because it will end up causing more problems than it attempts to solve
This would be my first time doing it, so it's going from our current accounting software's sql database over to a new accounting software sql database. When you say complex, what do you mean? Number of tables? Over 1000 if that's what you mean
Buy a domain and host your mysql on godaddy or other service for a year for like $12. Those services will be far more reliable than using dynamic dns and running the mysql service off your machine. 
To be honest I've never seen a guide for migrating data. You're pretty much going to have to have someone from the new system that understands where everything goes. And if you can someone from the old system. They'll sit down and go over each column 1 by 1 and figuring out where it goes. Then writing the sql script to make that happen. You're pretty much screwed without any knowledge of either system. If you miss even 1 FK you'll have issues. With a 1000 tables the likely hood of missing a couple is pretty high. Unless the columns in both databases are named well you'll have no clue where to put what. Not to mention if the old company didn't have good denormalized data and the new company does or vice versa then it'll be a nightmare moving it over. Especially if in 1 system they used a bunch of M2M mappings and in the other they didn't.
99% of this work is reviewing the current tables, eliminating redundancies and building the new tables to be able to handle the same data. Migration is the easier part. Look into SSIS on youtube or even bulk insert of csv files if you're using SQL server Express edition. Someone has to verify what these 1000 tables do and how they're all linked up (for those tables that actually do point at another table). That is a huge undertaking. I hope it's sort of easy in the sense that there are like 900 tables doing roughly the same things copied over and over instead of 1000 unique tables with unique relationships.
&gt; I've ended up with exactly what you are saying Except that I made my primary key nonclustered. Which database server are you using? I wasn't aware you could have a Primary Key which isn't also clustered (at least in MySQL/InnoDB and non-recent versions of SQL Server). &gt; So you are saying that with this I have way more page fragmentation as my key won't grow the table with consistency. Is that about right? It would depend on the nature of inserts. When you have an identity PK, every insert just gets appended onto the end of the table pages (either filling them completely before creating a new page, or at least mostly filling them (depending on the index 'fill factor'). By default MSSQL leaves about 20% free on each page (I *think*), with MySQL/InnoDB trying to leave about 1/16th free To achieve a similar fragmentation pattern under your design, you would need to guarantee that records are inserted in the exact same order they will be stored, otherwise you risk only partially filling pages before new ones are created (meaning the table takes up more room on disk, aswell as in RAM). &gt; If I move the clustering back to my auto-incremented index. I will be returning much more data per-page in my queries? Kinda, it will probably be able to store more rows per page. It depends on a few other things though, such as how wide each row actually is and hence how many rows can theoretically be placed on a single page. It doesn't affect how much data your query actually returns to the client (ie your software), but may mean it can access it with less I/O (storage is nearly always the slowest part of a database). &gt; Secondly, should I just do away with the key completely and just put a unique index on the 6 fields that uniquely identify each row? You should always have a primary key on the table. If you don't, then most database systems will create a hidden PK field behind-the-scenes, so they can store it as an identifier in the non-clustered indices &gt; EDIT: I noticed that while my largest table's key value comes to 13bytes (decimal 24) &gt; But if I were to use 5 different columns as a unique index for the table, they add up to only 11 bytes. So that makes even more sense. Again it depends what database you're using, but usually when you see decimal(10), its not saying it will consume 10 bytes to store it. Its shortcut for decimal(P,S) with P being the precision/the integer part, and S being the scale/mantissa See https://dev.mysql.com/doc/refman/5.1/en/precision-math-decimal-characteristics.html
can i see the entire query you ran, as well as the error message
Ah ok. I think it's sound marketing, then! 
It can use Excel tables, work with the data in an in-memory database (SQLite), connect to various external databases. move data in both directions, invoke VBA commands from SQL, has a VBA API so it can be automated from VBA. I started it as a hobby, and it took about two years of work (on the side). It was quite a bit of work and I think its really useful to anyone that spends time in Excel but knows SQL. I've harnessed my keen video making skills to put together some tutorials at various stages. Here they are: https://www.youtube.com/watch?v=Ld-mbyAGsow https://www.youtube.com/watch?v=1vjlEd2-bJQ https://www.youtube.com/watch?v=cCXPPCByoK8 https://www.youtube.com/watch?v=QRlyLcoJMA4 So what do you think? Also I could use any suggestions on how to market it? :) 
Sounds amazing, I wish my employer would allowed us to download addins for excel. I am stuck importing into TOAD then exporting it back out once I'm done
&gt; Will there is any difference as opposed to using a very long decimal in a single column index that also conveys the same information as those 6 columns when I break it down in my code. Ultimately I just can't say. As with everything db related, its a case of experience, benchmarking and trial and error. All I can say is I'm a bit obsessive about my query performance, and in my experience your design is not one I would ever revert to (although naturally I've experimented with similar tactics in the past to see if it helped). There's also other things to consider not just select performance. If you have to jump through hoops to run ad-hoc reports, or your database is insanely cryptic even to a dba, then you've got maintenance problem. But whats missing here is really a reason why you're attempting this in the first place? Are you seeing a specific performance problem with your query, or just trying random things because you have a hunch its slow? You should go by what the database insight is *actually* telling you, not by hunch... which means inspecting the execution plan and performance metrics. So far you haven't provided anything which actually suggests there is a real problem which warrants all of this (ie, premature optimisation). &gt; So basically, is there is difference in performance if a unique index is single column vs multiple columns. &gt; In particular I realized that my own key index was using 13bytes in each volume for that 1 column. Whereas, if I assign the 6column-set as a compound(or is it composite?) unique index, they uses 11bytes between them. Again, It's hard for me to say without sitting infront of the database and poking around. But look at it this way... If you have a composite decimal field which encodes all 6 fields, then its only useful when you are querying on all 6 fields at once. The index will not help if you want to create an aggregate report which breaks things down just by the top two levels (since it will need to decode every field just to see if it qualifies). By contrast, if you have an index covering fields (a, b, c, d, e), you can query on (a, b) and it will allow the database to still use that exact same index just as effectively as if you were querying on (a,b,c,d,e). To give a comparison, there are several places in my database which store a date as DATETIME. In order to create group reports on these, I occasionally denormalise the datetime out into extra year, month, day fields, which allows me to report on year, year/month, or year/month/day (especially when using group by). If I was to just store them as a single encoded datetime, then MySQL will still return identical results, but it has to do a lot more work to decode each rows timestamp and generate the resultset. Forget the difference between an index and a unique index. It really has zero effect on query performance, other than a tiny hit when it comes to inserting. 
It should work, what is the error? 
OH.MY.GOD
Thank you so much! This has been super helpful. My interest in this custom key came about because I didn't realize you can actually index any column you want I thought you were stuck with the primary key and I just didn't understand how anyone can make sense from an identity pk. It was only yesterday after many months of grinding at my program that it finally hit me. My struggle started when my first version of Db got too confusing with just Ids. So the next version I made my main unique columns as primary keys. This made life super easy and for 2 months I was really getting some of my c# program classes working. Then everything slowed down and SQL informed me that my DB size had maxed out because SQL server I was using was the free version or evaluation version. After that I tried splitting my tables into 2 different databases and as my Db went, so did my program. All classes started acting up and I was doing a lot of patch up. A month back I had almost given up and then 2 weeks ago someone gave me an SQL enterprise edition to install. Since than I've been really thinking hard about this but snapping out of my older version of key took a while and in last 3 days today I actually worked on it non-stop like every waking minute and fixed up all my classes. I think within the week I should have my project in working order. My SQL knowledge is very very limited. I use c# so I had to make a decision between pursuing sql when I got it it a few months back and sticking with c# which I only started earlier this year. So I stuck with c#. This is a very personal project so I don't think anyone other than me will ever really get into it. My data isn't very large. I'd say it's close to 15GB. And once I get it working everyday it will add 50-100MB everyday. If you don't mind, I may get in touch with you again after a few weeks to get an understanding of what you mean by &gt;You should go by what the database insight is actually telling you, not by hunch... which means inspecting the execution plan and performance metrics. That looks interesting. I only need my data to run some strategies and back tests. IT's not very intense. In my head I know exactly what I need and it's just a matter of getting all of it into code. 
Looks sweet, will try in the morning 
Thanks, let me know how you like it!
Cool, if you have time please post back thoughts, ideas, suggestions and all that good stuff
This is wonderful. I am definitely going to try this out once I get in the office this morning.
I've been waiting for this my whole life.
So, I work with a bunch of people who manage data but are relative SQL novices. Usually when they need to do operations like this, I tell them to load their data up in Microsoft Access and do their querying there. Is there anything that your plugin provides that would make Excel a better avenue for this work than Access? (Gimme your pitch :P)
Do not forget to run *sp_refreshsqlmodule* after altering an existing view. This to make sure that the metadata is up-to-date. See https://msdn.microsoft.com/en-us/library/bb326754.aspx for more info. &amp;nbsp; If the name of your view is 'view', then run: EXEC sys.sp_refreshsqlmodule 'dbo.view'; &amp;nbsp; Example 4 is a 'weird' one, btw. 
Thanks- i'll bring up the metadata refresh with our DBA's. I forgot to mention.. I think I'm going to ask end users to put "option (recompile)" at the bottom of their scripts that hit the View because of how dynamically they are using the View. Would love to hear thoughts on that. Fixed examples, thanks
Our new software vendor have only migrated someone out of our current software's database before. they claim to have a loose roadmap based on the one time they did it , but they didn't seem really confident in their wording. so they might be figuring it out as they go along again. that said, this is why i want to be as prudent and watchful as possible to make sure they're doing it right and making sure i'm asking the right questions and making sure they don't over look anything. I want to make sure i'm asking the right questions, and be quality control as much as possible. 
Yes, MSSQL basically runs the entire content of the view as if it was straight SQL. The **very important caveat** is that it will join all the tables whether the SELECT from the view utilizes them or not; which can cause them to be slower than you would expect. Additionally if you are joining from large result set views to a table already in the view, there is a good chance there is more work being done than is needed. A case would be with your example #1, where you really only need **orderheader, orderdetail** (*since orderheader already contains 'salesperson'*); when you use the view though, you are querying all the tables. So it's really doing four more joins then is needed to get the results. Views are great when created to solve specifically defined problems or granulating permissions, but you have to be careful that what you are doing isn't causing the engine to do drastically more work than is needed; especially when your results sets are large. 
Yes. Anything stored within the database is backed up.
I can when I get back. On mobile right now. 
A caveat that is worth noting, while security permissions are backed up, logins aren't at the database level, and thus aren't backed up. What that distinction means is that if you have back up a database, and restore it to a different server, you can end up with orphaned security credentials.
Indeed, which is why I said "everything in the database" The users are in the database, the logins however, are not. Unless it is an encapsulated database I believe.
I figured you knew, but it is a 'gotcha' for people who haven't dealt with it before.
It sounds promising. One use case I imagine is for business users to maintain (Slowly Changing) Dimensions in BI implementations. We've considered creating 3-tier web front-ends, but this may do it for us. 
I think your correct.
Schemabinding just means that you cannot make changes to the underlying table that would change the *definition* of the view. This means that if you're selecting column salesperson, you cannot make any modifications to or remove the column salesperson from the table. Replication shouldn't have an effect on schemabound views, but if they do you could just drop the views first and then let the replication recreate it, no?
Of replication could be setup on sys.views and sys.sql_modules: SELECT * FROM sys.views AS sysviews INNER JOIN sys.sql_modules AS sysmod ON sysviews.object_id = sysmod.object_id
A self join is just joining a table to itself. The classic example is when you have an Employee table with an EmployeeId primary key and a ManagerId column. To find each employee and their manager you can join the Employee table to itself. SELECT E.Name AS EmployeeName, M.Name AS ManagerName FROM Employee AS E INNER JOIN Employee AS M ON E.ManagerId = M.EmployeeId Of course, because you are using the same table twice you have to give it an alias - E and M in my example or A/B/X/Y in your example. Also I think you have an extra semicolon in there?
Are you sure you want "group by" and not just plain ol' ordering? Also you said you had 3 tables but in this statement there is only two. However if you wish to just order things, you can replace the word "group" with "order" and you'd have a list of department, first, and last name ordered by department in ascending order. You can also do "order by department, lname" and it would do a 2nd-level sort by last name within each dept. Also with use of the inner join (=) you assume that every employee has a department and every department has at least one employee. That's probably the case but joins can get you into trouble if either one of the aforementioned cases are false.
The name of the table should be "Orders", not "Order".
Oh my god I would kiss you if I could. 
And that's a nice easy straightforward example...I guess the nesting is where I just can't follow things. Using the results of one join in another join...or including multiple expressions in the 'ON' portion of the JOIN. Part of the confusion is that SQLZOO doesn't really offer much discussion of the theory behind the syntax...which is why I'm hoping someone can steer me to a better resource.
[ಠ‿ಠ](http://img.pandawhale.com/post-30824-Jack-Nicholson-Creepy-Nodding-t5bI.gif)
Seriously though, THANK YOU. It was such an obvious fix. THANK YOU THANK YOU THANK YOU!!!!
thanks - when I initially looked into them, I thought they meant the data itself couldn't change. After your post and reading the article below, it seems it might be possible. The article did warn quite a bit about the overhead of maintaining the index on the view with replication constantly inserting records into the table. https://www.simple-talk.com/sql/learn-sql-server/sql-server-indexed-views-the-basics/ I know you can't say for sure but based on the information i've given, would you recommend a schemabinding indexed view ? 
Looks like the consensus is to just change it from GROUP BY to ORDER BY, let us know if that doesn't work out, though.
Protip: Double click the error text in the SSMS output window and it will take you right to the syntax error!
Would I see the changes after I execute a SQL statement if it were successful? Such as selecting * from a table after the changes take place.
&gt; Additionally if you are joining from large result set views to a table already in the view, there is a good chance there is more work being done than is needed. A case would be with your example #1, where you really only need orderheader, orderdetail (since orderheader already contains 'salesperson'); when you use the view though, you are querying all the tables. So it's really doing four more joins then is needed to get the results. Most RDBMS support some form of table elimination, and SQL Server certainly does. Simply converting any joins that do not affect the cardinality of the result to outer joins will avoid the problem you describe.* If the statistics are up to date and PK-FK relationships are properly defined, there will be no performance impact. So to take example 1 and transform it into this: SELECT l.location ,s.salesperson ,c.customer ,c.customername ,h.invoicenumber ,d.item ,i.itemcategory ,d.qty ,d.sales FROM orderheader h LEFT JOIN orderdetail d ON h.invoicenumber = d.invoicenumber LEFT JOIN salesperson s ON h.salespersonid = s.salespersonid LEFT JOIN customer c ON h.customerid = c.customerid LEFT JOIN location l ON s.locationid = l.locationid LEFT JOIN item i ON d.itemid = i.itemid If I then execute the following query: SELECT v.InvoiceNumber ,v.SalesPerson FROM view v WHERE v.InvoiceNumber = 123456 The query engine will only perform lookups on OrderHeader and SalesPerson. *The eagled-eyed will notice that OrderHeader to OrderDetail certainly affects the cardinality of the final result set. I'd generally recommend using OrderDetail as the primary table and having an additional view for OrderHeader. This will prevent the number of rows returned being dependent on the columns selected.
Thank you!!! I think this is exactly what i was looking for! 
For those that find themselves in this scenario, here is the cheatsheet: *** -- List orphaned users without logins EXEC sp_change_users_login 'Report'; -- Re-attach user to an existing login EXEC sp_change_users_login 'Auto_Fix', 'username' -- Create a new login and attach it for the user EXEC sp_change_users_login 'Auto_Fix', 'username', 'loginname', 'password' ***
One way to see if the data was being changed in a transaction that hasn't been committed, run your query two ways: * `select * from table where CRITERIA` * `select * from table with (nolock) where CRITERIA` If you get different results, you probably have an uncommitted transactions. If you get the same results, it's still possible.
There are much better people than me, but I vaguely remember having trouble with the execution plan being stored through ssrs. Try adding the clause to re-optimize the plan. I don't remember it, but a little googling should point you in the right direction. Edit: http://stackoverflow.com/questions/20864934/option-recompile-is-always-faster-why
To answer your question, absolutely. You can use set-based operations to do anything a cursor can. 
If you're on SQL 2008 or above you can just use CAST(OrderDate AS DATE) and it will truncate the time.
I agree, however in this context he's looking for anything ending before 10/1
I'm not sure why table elimination isn't more widely known. It's insanely handy. The outer/inner join performance thing is a bit overstated as it depends on the relationship between the entities. In zero/one-to-one and many-to-one relationships the execution plan often ends up being identical. It's important to keep OrderHeader separate so the one-to-many relationship between it and OrderDetail is preserved. In your case, I'd build one view for OrderHeader that incorporates SalesPerson, Customer, and Location and another for OrderDetail that incorporates Item.
You might be having an issue with parameter sniffing by SSRS. Try explicitly casting the user value to the same data-type as the column you're searching.
Actually not sure, it may actually have been the report builder that has the visual query builder. I have seen sales folks use it successfully.
Declare your parameters. This should solve the issue. 
This is why all of my instances have a SQLCMD.exe job that executes sp_help_revlogin to a *.sql script (stored on DataDomain) nightly. For log shipped instances that script is output to and run against the secondary each day as well. 
Agreed. But if you still want some sort of visual spacing in the name, I would recommend a _ instead of a -.
I use it sometimes to throw together some tables and get the joins all written out for you. This comes in handy when you're not 100% sure of the key columns. Then I modify the SQL to suit my requirements.
Does T-SQL support natural joins? As far as I know, it does not...
`natural join` is a terrible idea (in general). Use a proper `inner join`. You already know the fields that need to be joined on.
Brent Ozar gave a talk yesterday on choosing if AwaysOn is right for you. He went over the manpower, time, knowledge, and expense needed. It is too expensive in all of these areas right now for my company.
&gt; PS : Why did I never see queries indented like this ? For a couple reasons. 1) People's indentation is heavily influenced by how and where they learned to write SQL statements, and is mostly preference (my next points are subjective) 2) Having your FROM and GROUP BY indented further than your select is ugly 3) Putting a new line before a semicolon is kinda ridiculous 4) It's more readable to have opening and closing parentheses at the same indentation level 5) Putting a new line after SELECT but not after FROM or GROUP BY makes it uglier. 6) Using spaces to indent is annoying (fite me) Here are two examples of how I might format the same query (not that I'm *objectively right*, it just looks better in my opinion.) example 1 SELECT * FROM records NATURAL JOIN ( SELECT MAX(last_comm) AS last_comm, company FROM records GROUP BY company ); example 2 SELECT * FROM records NATURAL JOIN ( SELECT MAX(last_comm) AS last_comm, company FROM records GROUP BY company );
Here is some sample code that accomplishes what you want. What I'm doing here is creating a CTE that numbers each row by modified date descending (which means the latest date gets a '1'), starting over for each company (meaning the latest date of each company gets a '1'). After that all you need to do is update the table and set last_comm to 'Y' wherever its row number is '1'. DECLARE @comms TABLE(id INT IDENTITY(1,1), company_id INT, last_comm VARCHAR(1) NULL, modified_date DATETIME); INSERT INTO @comms VALUES(1, NULL, GETDATE()); INSERT INTO @comms VALUES(2, NULL, GETDATE()); INSERT INTO @comms VALUES(3, NULL, GETDATE()); INSERT INTO @comms VALUES(1, NULL, GETDATE()-1); INSERT INTO @comms VALUES(2, NULL, GETDATE()-1); INSERT INTO @comms VALUES(3, NULL, GETDATE()-1); INSERT INTO @comms VALUES(1, NULL, GETDATE()-2); INSERT INTO @comms VALUES(2, NULL, GETDATE()-2); INSERT INTO @comms VALUES(3, NULL, GETDATE()-2); WITH rn AS ( SELECT id, ROW_NUMBER() OVER (PARTITION BY company_id ORDER BY modified_date DESC) AS row_num FROM @comms ) UPDATE c SET last_comm = 'Y' FROM @comms c INNER JOIN rn ON c.id = rn.id WHERE row_num = 1; SELECT * FROM @comms
Excellent application of windowing functions. One could do it w/ a correlated subquery as well; would be interesting to see the performance characteristics of each.
As /u/ublasto_blastocyst said, this is likely parameter sniffing. If you are doing a text query try adding Options(Recompile). If it is a SPROC add "With Recompile" after the as statement in your create. http://stackoverflow.com/questions/20864934/option-recompile-is-always-faster-why Edit - just read the full chain and saw that someone else found this article too. 
Actually trying that right now. Learning how to pass variables to a SP.
There's a huge problem when you have two tables with fields sharing a name but holding different data. Why not be explicit and use a proper `inner join` instead of the crutch that is "`natural`"?
And then you add 1 "union" and all is fucked up. I see SQL as hierarchical Query + WITH | +Queries + (union) SELECT | +ALL|DISTINCT (not needed) | | +fields | +FROM | | +tables | | +JOIN | +WHERE | +GROUP +ORDER ; `Having your FROM and GROUP BY indented further than your select is ugly` this is not art. horizontal alignment !? You have some time to lose... `It's more readable to have opening and closing parentheses at the same indentation level` But to have the semicolon at same level as query start, no ? `Putting a new line before a semicolon is kinda ridiculous` But before two special character it's ok !? OK, you talk subjective Maybe this is subjective for you, maybe their is a best way to indent. 
this is the database view http://imgur.com/WC55y7Y
Because I join twice the same table, so they obvioulsy will have the same name. Two different tables with same columns names is a bad idea. I join on the same columns so they hold exactly the same datas. I think natural joins are bad in a lot of case, but not this one.
When you access the same table twice, you need to rename it so that the database knows to which you refer. That's why in the code you found online it says `customers c1, customers c2` instead of `customers, customers`. Then for the rest of the query the names of the two tables are in effect c1 and c2. You need to prefix all column names with those table names so that it knows which one you mean.
&gt; Two different tables with same columns names is a bad idea. But it happens, and we have to deal with it. &gt;I think natural joins are bad in a lot of case, but not this one. They also aren't *necessary* in this case, so don't form the habit of using them in the first place when there's no situation where they can't be replaced by a more explicit join.
B or C, leaning towards C
From a SQL Server standpoint this will be antiquated with the release of Polybase in 2016. Personally I think each product should focus to what they were designed for, and are good at; then build interfaces that allow native-like access to their product regardless of RDBMS used. To me, this seems like adding a third wheel to a motorbike to be able to call it a tricycle.
I was just wondering how a sql past was nsfw...
I have it broken into an SP now but it isn't any faster. The problem is that I need to pass a variable to the SP, and then multiple variables to the final select. There is no option to fix the SQL.
Availability groups are enterprise only. The classic failover cluster setup is available in standard. AlwaysOn is a marketing term for the whole array of options. The two types should be referred to as failover cluster and availability groups.
Oh, I see. Thank you. I didn't realize that.
I have them declared in the main query already, but I don't have them declared in the smaller data sets.
&gt;And then you add 1 "union" and all is fucked up. Not quite SELECT * FROM records NATURAL JOIN ( SELECT MAX(last_comm) AS last_comm, company FROM records GROUP BY company ); UNION SELECT * FROM records NATURAL JOIN ( SELECT MAX(last_comm) AS last_comm, company FROM records GROUP BY company ); &gt; This is not art. Sure it is. Art is "the expression or application of human creative skill and imagination". Creativity and imagination are important tools for those who write code. Besides, have you ever had to debug ugly code? &gt;horizontal alignment !? You have some time to lose... I'm not sure what you're saying here. However, proper indentation does have the benefit of increasing your code's readability and therefore its maintainability. &gt; But to have the semicolon at same level as query start, no ? Fair point, but I've never seen this anywhere else in any language &gt; But before two special character it's ok !? OK, you talk subjective I'm not sure what you're saying here. Before two special characters? Are you talking about the parentheses? &gt; Maybe this is subjective for you, maybe their is a best way to indent. As I said earlier, there's no real "best" way to indent. The best way is to format your scripts according to your company's standard guidelines.
done. thanks for looking.
What is the purpose of the sequence column in your output?
the sequence of events, to know which event came before another afterwards. so in the final example, I will still know that in session 1 a purchase happened after the user was visiting a purchase page, but in session 2 a purchase happened after a promo was viewed. 
good point, I don't. this data is from a huge table/log that has no primary key. 
As a Microsoft SQL Server consultant, I'm guessing the [Oracle] tag is what makes it NSFW.
&gt; Tab is used in Microsoft Words to switch rulers, or in all other soft to switch focus. In code, it's misused to align things. Every time I used it, it was pain. From [Wikipedia: ](https://en.wikipedia.org/wiki/Tab_key) &gt; The word tab derives from the word tabulate, which means "to arrange data in a tabular, or table, form." When a person wanted to type a table (of numbers or text) on a typewriter, there was a lot of time-consuming and repetitive use of the space bar and backspace key. To simplify this, a horizontal bar was placed in the mechanism with a moveable lever stop for every position across the page, called a tab stop. So the tab key was originally used as a formatting key and was given an additional use as a context switch, so it's not misused to align things. The benefit to Tab is that you are able to customize the length of your tabs in the majority of text editors. For example, you prefer two spaces, but I prefer four spaces. If you have your IDE set up to have 2-space tabs and I have mine set up to have 4-space tabs, I can send my code to you and it will be indented how you like it, and you can send your code to me and it will be indented how I like it. &gt;thanks explained me. I should be nicer No worries, you're friendly enough
I see what you want, fortunately for you I just dealt with a nearly identical issue at work a couple weeks ago. ;WITH unfiltered AS ( SELECT [Date], [Session], [Page], [Type], [Sequence], ROW_NUMBER() OVER (PARTITION BY [Date], [Session], [Page], [Type] ORDER BY [session], [sequence]) AS rn FROM Purchases ) SELECT [Date], [Session], [Page], [Type], [Sequence] FROM unfiltered WHERE rn = 1 ORDER BY [Session], [Sequence] This should get you what you want. Note, if the Date column is actually DATETIME, this won't work, you'll need to do a CAST([Date] AS DATE) in the partition by.
 select date, session, page, type max(sequence) from T group by date, session, page, type ?
Good catch.
Its the logins and the roles they have as well as the passwords.
So I'm assuming that the actual value of the Sequence column doesn't have to be preserved, just the values have to remain in the same order (so your output could have had sequence values of 7 thru 1, for example). I'm wondering about select date, session, page, type, sequence = ROW_NUMBER() OVER (ORDER BY date, session, sequence) FROM Purchases GROUP BY date, session, page, type, I'm doing this from memory so it might syntactically be bad, and may not even work... but it might give you something to start with 
Whats the pay look like?
thanks, just checked and it does look pretty good, but unsure if they offer on site web based, they talk about the cloud.
Sorry marked it as problem was solved 
Pretty sure he's looking for a rate estimate
That's a great idea. I'll give this a try. It might be what i missed. Thank you!
Sql
Gosh, I haven't practiced my pitch:) Well here goes: for one thing, you don't need to import into Access. You also don't need to export it back when you're done processing. It's all in place, convenient, it's just there waiting to be used at a moments notice. And the easier it is to use, the more you're going to use it, it becomes worth it even for little tasks. Now for the fancy stuff: there's also embedding queries (thus making tables refreshable), query automation (changes to one table automatically can update another table), ability to automate queries from VBA, ability to connect to several different types of databases and easily move data in either direction, ability to invoke VBA functions as if they were SQL functions, ability to invoke many .NET functions as if they were SQL functions (regex for example). Too many words for a sales pitch, but I'm not really a sales person and am too proud of the many features to leave the best of them out:)
I am not sure that would work. Max will just throw in the max sequence number in a set no? 
Very cool approach. I'll give this a try. Thank you!
Thanks! I hope you try it, buy it love it and tell everyone how good it is:)
MS SQL Server?
Thank you very much
Yes
haha :)
Oh, I see what you're saying. So if your test data had two rows with same everything except sequence number, but they were separated by another row with a different type or something, then it should return all three. So you want to exclude duplicates, only if they're duplicating another row they're *next to* and they only differ by sequence number. In that case a ranking function is definitely called for, since you can rank records by order of the relevant columns, letting you identify the duplicates that way. /u/farhil has the right idea.
SQLite is the tool that solves this exact problem.
Thanks. Although it doesn't have to be next to it. But in the same session. I am experimenting with partitioning ;)
If proximity doesn't matter then my original group by solution will work fine. Sounds like you're not sure exactly what you want if you had to do it on paper, so it's important to figure that out first.
Some people charge $20/hr some people charge $100/hr, usually when I hire someone they give me their rate not vice versa. Inbox me your hourly rate and experience and we will take it from there.
With a proper primary key (Company, DateEdited) the correlated subquery will outperform any other option. If the table uses an IDiot key then the execution plans will not differ significantly - you'll have table scans, sorts, and all other types of nonsense.
I'm guessing `ASCII` doesn't support the extended set ... so maybe the underscore is a placeholder. What are you trying to convert it to? Edit: I'm wrong :)
Assuming they can only go from active to inactive once on a given date then use `row_number() over (partition by (Employee, StatusDate order by Status) rnum`, and filter where rnum = 1.
That's the answer! I was headed that direction with partitioning but for some reason I only thought you could partition by one thing and not multiple, so that was my hangup. I was doing something overly complicated where I was adding a 2nd row counter within a subquery that was behaving strangely and that was dumb way to approach it. Learned something new, thanks! I think this is the third time you've provided the correct answer to my little dilemmas. 
I was trying to find out if the string contains extended ASCII. Unfortunately some of the extended ASCIIs are returning incorrect results. For now, I implemented workaround (2nd check in the code below) although I still don't full understand why ASCII isn't returning correct results in some cases. -- find if string contains extended ASCII declare @I int, @str nvarchar(100) = N'yÇeah▄blah', @HasExtAscii varchar(10) select @I = 1 while @I &lt;=len(@str) begin -- first check select @HasExtAscii = case when (patindex('%[' + char(127) + '-' + char(255) + ']%',substring (@str,@I,1) COLLATE Latin1_General_BIN2)) &gt; 0 then ' - yup' else ' - no' end print substring (@str,@I,1)+ ' -- ASCII = ' + cast(ascii(substring (@str,@I,1)) as varchar) + @HasExtAscii -- second check if substring (@str,@I,1) &lt;&gt; char(cast(ascii(substring (@str,@I,1)) as varchar)) begin print ' look at here ----&gt; ''' + substring (@str,@I,1) + ''' &lt;&gt; ''' + char(cast(ascii(substring (@str,@I,1)) as varchar)) + '''' end set @I = @I + 1 end
Do you have a standard format you can ask the vendors to use. I work for a bank and we have data formats we require the vendors to use when sending us data. It helps to build data checks on the format you require, so you build the initial checks but since you have a standard format you can apply the checks consistently and quickly. Otherwise on your import tables you can use not null for columns that have data so if it fails to load you know something is missing. Granted that will cause you to research what is missing but it is a quick way right away. Personally I normally have my import tables and apply required checks on that. Without really knowing more it is difficult to give an answer that will work for you.
Sadly, no, we can't use a standard format; each vendor is different and every vendor has a plethora of data that may or may not be relevant to the situation at hand (what should only be say, 24 columns of relevant info turns into 128 columns of good and gibberish). We have really excellent indexing, and we have some standard data checks that exist at the application level. The problem arises when we force data in as we have nothing else to work with. On top of that, I'd like to say all the analysts can use SQL...but realistically, I'd say only about 5% understand inner joins. I also learned first hand how monstrous of a task it is to try to teach logic to people who has never been exposed in that environment. That leaves me with finding a method that can streamline the process where the analysts do very little tinkering. More button clicking (I feel awful calling them button clickers). 
Sorry, sounds like you have a nightmare. I deal with analysts that are suppose to know SQL too and many have issues with joins, when to use left , full outer etc... Another is not accounting for nulls when using criteria so they under report the full data. A lot of time is wasted.
there's a data profiler in SSIS - you can view the results or xquery output. https://msdn.microsoft.com/en-us/library/bb895310.aspx We've ended up not using it though but writing a test plan with scripts/queries attached to each test item and have a data qa person eyeball the results - we get data from a lot of sources that are terrible in their own unique ways, so once initial quality has been negotiated, it's a manual task to see if the baseline assumptions are terribly out of whack or the source reached a new qualitative level of crappiness. 
This! Pivot is what you're looking for.
Can you change it so that instead of having permissions directly to the tables, they only have permissions to views and stored procedures you create for them, then you control exactly what they're seeing? You can make a stored procedure for each data check query, then all they need to do is execute that stored procedure, then they're not modifying or breaking the queries... or getting into data they don't understand. 
hi, just tested it and it was exactly what I needed and couldn't get my head around it. thank you again! 
Thanks for the help, got what I needed :)
So that's why people religiously put N in front of every string.
Yeah, tells the DB it's to be treated as a Unicode string.
Glad to hear it! If you have any questions about it feel free to ask
Unfortunately this approach won't work in all instances. In this case, it would fail twice (see this [SQLFiddle](http://sqlfiddle.com/#!6/49385/1)): Date | Session | Page | type | sequence :--|:--|:--|:--|:-- 1-Sep | 4 | purchase | 1 | 5 1-Sep | 4 | search | 1 | 4 1-Sep | 4 | purchase | 1 | 3 1-Sep | 4 | promo | 1 | 2 1-Sep | 4 | search | 1 | 1 Now, there may be some unsaid rule that prevents this from happening, but that would be dangerous to assume - especially given the lack of a coherent primary key and the abuse of reserved keywords for column names. When dealing with terrible data structures, one must be extremely careful to understand all of the potential ways the data may be arranged. If you do not, you will inevitably provide an incorrect result and someone will make a decision on flawed data. Depending on the industry, this could be a SOX issue and you could quickly find yourself out of a job.
This sounds like something that would be best handled before loading the data. Insert everything that conforms to the rules, kick everything else out into a set of staging tables that the users could screw with - HOWEVER, you would not let them move the data from the staging area unless it met all of the requirements necessary for production. SSIS and other ETL tools generally make this process easier (and you can create routines to fix problems that don't require review), but do yourself a favor and program and get those requirements translated into constraints in the production environment. The best way to control this problem now and going forward is to prevent junk from being added to the database by enforcing rules in the database itself.
You may want to look at putting in some constraints. If you're pulling in data from other systems, application level checks won't be sufficient. You need to ensure the integrity of your data before it enters the database. Look at tools such as check constraints, triggers, and foreign keys. This will be a lot of up front work for you and your team, but ensuring that invalid data cannot *enter* your database will save you a lot more heartache in the future. /u/muchargh has the right idea with staging tables. 
So just for the record, I think /u/muchargh is on the right track. If, however, you need to do something nightmarish in SQL, I have done the following. select 'Person_ID' as Column_Name , Person_ID as Column_Value , vendorid , fileexportid , File_Date , case when ltrim(coalesce(Person_ID, '')) = '' then 1 else 0 end as Missing_Invalid_Indicator from vendor_cte union all select 'ACTUALHOURS' as Column_Name , ACTUALHOURS as Column_Value , vendorid , fileexportid , File_Date , case when isnumeric(actualhours) = 0 then 1 when len(actualhours) &gt; 6 then 1 else case when cast(actualhours as decimal(8,2)) &lt;= 0 then 1 when isdate(callintime) = 0 or isdate(callouttime) = 0 then 1 when round( datediff(minute , cast(callintime as datetime) , cast(callouttime as datetime) ) * 1. / 60 , 2 ) = cast(actualhours as decimal(8,2)) then 0 else 1 end end as Missing_Invalid_Indicator from vendor_cte ... You can generate a shell of the script to check for missing values by concatenating in Excel, then add in additional checks manually. 
That's a good point, but fortunately [here](https://www.reddit.com/r/SQL/comments/3mzcn7/distinct_with_a_twist_question/cvju2z2) he clarified that the sequence of duplicates doesn't matter, the duplicates should be removed regardless of their location. I personally don't see a benefit to that, as it wouldn't show the entire story, but oh well. You definitely didn't deserve to have your comment downvoted without response though
A note about the performance... If you were to look at the query plan generated for the PIVOT query, you will see SQL server generate CASE expressions for the PIVOT internally. Assuming the case statement logically matches the PIVOT definition, there should be little difference in performance between the two approaches.
This is a good exercise because when you're designing a database, you don't typically have any data to determine what needs to be normalized, and where there are functional dependencies. You have to use your noggin and figure it out. Let me precede with the note that this is the first I've actually looked at functional dependencies, but I gotta learn some time so why not now? That also means take everything I say here with a grain of salt. I'm also going to attempt to give a full answer instead of some tips or insights, because there's a high likelihood that I'm entirely wrong. So first, let's outline all of our (non normalized) functional dependencies **Dependecies** Project_code -&gt; Project_title Project_code -&gt; Project_manager --Since a Project Manager is an employee, there has to be at least one employee for any given project. Therefore: Project_code -&gt; Employee_number Project_code -&gt; Employee_name Project_code -&gt; Employee_city Project_code -&gt; Employee_zip Project_code -&gt; Hourly_rate --Every employee belongs to a department, so there has to be at least one department for any given employee. Therefore: Project_code -&gt; Department_number Project_code -&gt; Department_name As you can see, Project_code is our superkey. (I'm assuming project_code is a unique identifier for a given project). If we normalize this, we get the following: Project_code -&gt; Project_title Project_code -&gt; Project_manager --Since a Project Manager is an employee, there has to be at least one employee for any given project. Therefore: **Project_manger -&gt; Employee_number** Employee_number -&gt; Employee_name Employee_number -&gt; Employee_city --I'm pretty sure that there is only one zip code for a given city. I could be wrong on this though? But generally when normalizing you shouldn't store data that can be derived from one column in another column on the same table, even if they are directly related. **Employee_city -&gt; Employee_zip** Employee_number -&gt; Hourly_rate --Every employee belongs to a department, so there has to be at least one department for any given employee. Therefore: Employee_number -&gt; **Department_number** **Department_number** -&gt; Department_name So the normalized supersets we get would look like this {Project_code, Project_title, (1)Project_manager} {(1)Employee_number, Employee_name, (2)Employee_city, Hourly_rate, (3)Department_Number} {(2)Employee_city, Employee_zip} {(3)Department_Number, Department_Name} I marked functional dependencies between supersets with numbers. If I was to create a data structure with the given column names, that's what it would look like. Does this look right to you? Or am I totally off the mark on what functional dependency/normalization is?
Are you trying to get a single grand total for all of the cost_lines of all of the filled orders? What signifies a filled order? There's not enough information to work with. select sum(l.cost_line) as total_cost_all_orders from order_lines l join orders o on o.order_numb = l.order_numb where &lt;order filled criteria&gt; 
How long would it take to study for that as someone who used SQL occasionally at their job, but really only simple commands. I'm not familiar with what all is covered on the exam, so I'll look at it when I'm at my computer later. Just wondering how long people usually spend studying for it. Thanks! 
Interesting, I didn't know that. Curious to see data on someone trying both approaches, but I'm sure you're right.
&gt; Determining the functional dependencies without the data seems subjective to me. It is. "Recognizing the FDs is part of the process of understanding what the data *means*..." -- CJ Date, *An Introduction to Database Systems*, 7th ed p 355. No data, no meaning. For example, does "dept_no" tell us something about the employee or something about the project manager? Does "hourly_rate" tell us something about the employee or something about the project manager? You can't say without bringing in information from somewhere else. If you had data, though, you might see something like this. project_manager employee_name hourly_rate -- Lana Pratt Doug Haynes $30.00 Lana Pratt Norma Morton $30.00 Gabe Myers Doug Haynes $33.22 Odds are good that hourly rate tells us something about the project manager.
You don't need workbench, but it'll probably help. Some people prefer DBeaver. There are plenty of SQL IDEs. https://dev.mysql.com/doc/refman/5.6/en/osx-installation-pkg.html Just install the dmg, start the server, open terminal and run `sudo mysql`, execute `CREATE USER 'zgterp'@'localhost' IDENTIFIED BY 'password';` and `GRANT ALL PRIVILEGES ON * . * TO 'zgterp'@'localhost';` then connect with workbench or whatever IDE you use to `localhost:3306` with user `zgterp` and password `password`. You don't have to explicitly specify a schema/database. This is what it takes to get a blank slate.
What program are you using to view these schemas? Does the table work, and you just want to change the schema view? Are you writing the SQL to create the tables, or is it being generated/imported from other sources? FYI: It looks like List_Wheels has MySQL style identifier quoting using tick marks, whereas other tables are using the MSSQL square bracket style. I think SQLite follows the SQL99 standard of double-quotes, so neither of these is actually SQLite compatible? 
Well, the "tutorial" I'm following recommended to use Database Browser for SQL or, DB Browser for SQLite. The file format is .SLT. Basically what I'm trying to do, is take one List_Wheels from one DB (Gamedb.slt) into my own customized database. The guy told me to "import the table" and that's all I got. Obviously there's more, because the table definition upon importing doesn't change, and the "format" or "encoding" isn't how it's meant to be. Hopefully I made a bit of sense. :/ I have exactly no knowledge on SQL (as you can probably tell) and the only reason why i'm touching this, is because it's the database 'structure' of a game. 
Text contents? Sorry, not sure what you mean by that. Also, Correct. It is to edit Forza files ( the bulk 'structure' is through Database, or.. .slt - whichever language that is. ). 
I regularly cringe at queries I've written when a few weeks/months later I discover a much easier approach. It's worse when I don't have time to rewrite them or I passed off a code snippet to someone else. I think one of the biggest downsides of working in the data field is you have to deal with the crap other people have made and butchered over the years. If the data integrity is crap/the table design makes no sense/no one thought to build views/etc. you're forced to start smacking things with a hammer. A very large hammer.
you might want to go into a little detail on what your issue is. It sounds like you are about to do something really bad. 
No not really, just wondering. Imagine an application locking a table for whatever reason, is the lock released after you restart the SQL service? 
Locks are bound to transactions. When restarting the service, it will try to cleanly rollback all open transactions. So yes, restarting the service will release all locks, but please tell me you do not intend to restart a SQL Server for that purpose, since there are way less intrusive ways to kill open transactions. 
In most cases you're far better off `KILL`ing the blocking spid. Run `DBCC OPENTRAN` and see what comes up. 
/u/PrezRosslin wrote the code for you but I will explain. A LEFT join doesn't require the entries in the RIGHT side of the query to have a value. Took this snippet from what he did. In the case here w.worktype is on the right. So it will take the values from w.worktype even if they are NULL. left join worktype t on t.worktype = w.worktype 
&gt;Also I am not getting all WTYPEDESCs and OPSSITEIDs Do you have an example of the values in each table for anything you are still missing? The only reason I can think that it would not be present is if it lacks an OPSSITEID.
Open the source database in DB Browser for SQLite On Database Structure tab, right-click upon List_Wheels table block and choose &lt;Copy Create statement&gt; Paste into a blank notepad Make a copy of your "customized" database Open the "copy" in DB Browser for SQLite Same tab &amp; same table as before, right-click and choose &lt;Delete Table&gt; Click over to Execute SQL tab Copy text from notepad and paste into the empty block Click the &lt;Execute SQL&gt; button just above (Or F5 function key or CTRL key + ENTER key) Newly modified "copy" is now a database with modification you desired. 
You only have 1 table with all the data? Or do also have state table? 
a having clause might work instead of a where. Try this: select state, avg(creditlimit) from customer group by state having min(creditlimit) &lt; 7500
Ohey, thanks for the help everyone but I managed to figure it out. I was totally overthinking this. Aside from the code example I gave initially, I also tried to Select state, avg(creditlimit) from customer group by state having (select min(creditlimit) from customer) &lt; 7500 And this worked the same way as the first example I gave. It would average the values correctly, but would continue to show MN, when I needed it to not show MN. The correct code was: Select state, avg(creditlimit) from customer group by state having min(creditlimit) &lt; 7500
Just remember that the having clause is a 2nd "where" after you've grouped. A lot of students get tripped up because they want to put some kind of filter on grouped data but they want to put it in the where clause. Often you'll use both where and having in the same sql statement, but for different reasons, and those reason being filtering before you group, and filtering again after you've grouped. 
Underscores represent a single wildcard character, enclose that in brackets DataTable[_]% and see if it helps, to be honest I stopped reading when I saw that. See if it helps if not I'll look some more.
Omg Thank you! You're a beast! Okay, not quite. Doing so makes me lose all the information from the the List_Wheels. The whole purpose behind this is to import the List_Wheels into my own gamedb.slt. I followed what you said to do, and I lose the data within the table, which beats the Schema problem I was having, now I don't have the data anymore within the table though. 
A good way to debug dynamic SQL is to replace your exec(@sql) with print, so you can see what is being executed. I'll give a shot at refactoring your code but I'm on mobile so I can't guarantee I didn't miss anything. DECLARE @sql VARCHAR(MAX) = ''; SELECT @sql += ' INSERT INTO DataTable_All SELECT [SourceDate] = ( SELECT DateName(month, DateAdd(month, CAST(RIGHT(REVERSE(SUBSTRING(REVERSE(' + c.table_name + '),0,CHARINDEX(''[_]'',REVERSE(' + c.table_name + ')))),2) AS INT), -1)) + '' 20'' + LEFT(REVERSE(SUBSTRING(REVERSE(' + c.table_name + '),0,CHARINDEX(''[_]'',REVERSE(' + c.table_name + ')))),2)), ID, Score1, Score2, Total FROM ' + c.table_name + ' INNER JOIN Certification ON ID = NUM' FROM information_schema.table[_]constraints c WHERE constraint[_]type = 'Primary Key' AND c.TABLE_NAME LIKE 'DataTable[_]%' UNION ALL SELECT 'DataTable'
I tried that before posting and @sql looked just fine. I'll try the brackets around the underscores on Monday when I can look at it again.
I suspect that some of the table names have embedded bracket or space characters. General advice-When building strings of T-SQL commands, try to wrap all references to tables, views, schema, columns and databases with brackets. This will avoid parsing problems, which tend to crop up when you first write such code or when you deploy it to a new database. You might want to look at the QUOTENAME function to get an idea of what I'm talking about. Also, I think the cursor is OK for this purpose. Source: long time DBA who has been telling programmers to avoid cursors since SQL 7.0 came out. Sorry for any formatting weirdness, I'm on my phone.
 CASE WHEN [Lights/Sirens] = 290 THEN 'Lights and Sirens' WHEN [Lights/Sirens] = 490 THEN 'No Lights and Sirens' END 
thanks, i'll try that. it might be the case! 
It's relatively free, ie: the cost of your windows license, etc.
Sql server express is free. You can get server 2012 essentials for 400 on newegg. 
Sounds like you are looking to use prepared statements. Prepared statements and parametrised statements are the same thing just different name. https://msdn.microsoft.com/en-us/library/system.data.sqlclient.sqlcommand.prepare%28v=vs.110%29.aspx?cs-save-lang=1&amp;cs-lang=vb#code-snippet-2 Essentially create the command then assign the username, userpass, and password as the parameters then excecute it.
Thanks for the reply. My book uses the terms entity and attribute interchangeably. So can a PK in one attribute be a PK in another attribute as well? I think that's what's confusing me the most.
Check out http://bobby-tables.com/ for instructions on how to use actual parameterized statements. When you're done, you'll have something like query = "select UserName from users where userName=? and userPass=?" Of course this leads to another question. Why are you storing plaintext passwords in your database? Hash them with something strong like scrypt. Then you hash the incoming password from the user and compare it to the one in the database. Also I am in agreement with /u/MrApep - create stored procedures for doing all of these things. Then you can restrict the permissions of your web app user in the database so that they are only allowed to execute those stored procedures - no querying the schema, no selecting from anything. You'll want the stored procedures to run as creator rather than as the current user. So you're going to need two users, one who creates the stored procs in the database, and that guy has to be different from the web app user (who won't have create procedure privileges). 
What about this table that has two PK's? I see a lot like this. http://imgur.com/O68qLy4
You can't have two primary keys. You *can* have a primary key that's a composite of 2 or more fields.
So even though it says that there are two PK's, there aren't? Just the top one?
No, it's probably a composite key, and each of those fields has a foreign key constraint that references another table. From the image provided, it's impossible to say what's going on there and the poor constraint names are not helping.
You really aren't providing enough context for people to answer properly here. You're throwing words around as they're used in your book, but they aren't consistent with common usage.
Hm, two tables are linked together, how do I know which one is the parent?
Enterprise? You're looking at about 9k per core. Standard is closer to 2.5k per core. But the new community edition of VS is great. Up to 5 devs or 1 million in revenue and it's totally free.
If cost is a concern, you can run something like MariaDB for free (a mysql fork) and linux/apache/etc. Ms does offer free or heavily discounted software (like the free visual studio community edition), but if you grow you're stuck with them and will pay a lot later.
It really depends on the stacks you're comparing here. Going with a managed platform like .NET with its associated tooling will give you a sizeable productivity increase over, say C++. Going with something like rails/python will have much greater infrastructure requirements than .NET though again with increases in (claimed) productivity. Another thing to keep in mind is that not all apps are created equal - it's still easy to create memory/cpu-intensive apps in any language/platform, so you can't just compare platforms to each other like that - it's the applications you're comparing.
Development time is way more expensive than any of your infrastructure. If you invest in quality software developers, then they will be able to create a performant application that can scale easily, and won't require a lot of hardware.
Unless you're already roped into the MS product line, I don't see many good reasons to go with SQL Server over Postgres.
No, but I would still recommend open source alternatives. Your money could be better spent elsewhere.
&gt; EDIT: Doh! Not working. the square brackets will cause an error if you're not using MS SQL Server 
&gt; My book uses the terms entity and attribute interchangeably. then you need a new book, that one is pants 
the one that has the FK is the child
Fucking figures. This is the required one for the course.
An entity is a thing that you want to model, and is created when designing your database structure. These will, usually, result in you creating a table to help store a set of those entities. You could also call it a record or a row, when thinking about the data actually being stored, but an Entity is more abstract. Entities are useful in design as you can model how one Entity interacts with another, in the form of an Entity Relationship Diagram. You can, at a quick glance, determine if an entity has a one to one relationship with another, a many to one relationship, or a many to many relationship. The last of which, you would break up with More Entities! An attribute is something that an Entity has. This will include the usual stuff (For a person, their Name, a unique ID, etc), and links to other entities (by including those other entities unique identifiers). 
I'd probably go with a default of blank. Mostly because string concatenation is a pain with nulls.
Cool! Thank you for your responses.
Thanks again for mentioning this Muchargh, I've already started insuring join elimination is happening and i've seen big performance improvements. I do however have 1 table where I am not utilizing any of the columns, left joining, and joining on the primary keys but join elimination is not happening for the table. Above is the only criteria i've found in my extensive googling so I can't understand why it's not happening. Any ideas? Thanks again 
I flipping LOVE CTE'S. 
No expert by any means here, just offering advice based on what I'm seeing here at Reddit. If you notice while paging in Reddit, there is an "After" clause added to the URL when you start onto a second page - I'm assuming this is some sort of a coded time stamp. This would work for your "New" posts, but might have some difficult on the "Hot" posts as new votes come in. You would essentially be freezing your browsing as the user begins to browse paginated content. I've noticed that even on Reddit as I page through, I get duplicate posts while browsing "Hot" as I page through and posts fluctuate in position.
Hmm, thanks for your comment. I've actually experienced the same couple of times on Reddit. I need some time to think about this and figure out how Reddit made it.
Unique Keys - Enforce that the values in a column are unique 1. Usually applies to one column, but can apply to multiple columns 1. Allows null 2. You can have multiple unique keys in a table 3. You can point foreign keys at unique keys Primary Keys - The primary way to uniquely identify rows in a table 1. Usually applies to one column, but can apply to multiple columns 2. Enforces that the values in its column(s) are **unique and not null** 3. You can only have one primary key per table 4. Every table should have a primary key, else you may end up with duplicates Foreign Keys - Enforce that values in one column exist in another column 1. Usually applies to one column, but can apply to multiple columns 2. Enforces that the values in its column(s) exist in another column(s) 3. Can refer to a primary key OR a unique key 4. You can have multiple foreign keys on a table 5. A foreign key can point to another column in the same table! Regarding your questions &gt; How is parent entity determined? This depends on your design. A "Sales Order" is the parent of a "Sales Order Line Item" because that's the nature of the universe. &gt; Why include a primary key in a child entity? Every table *should* have a primary key &gt; What determines whether a PK in one entity is a PK or FK in another entity? Depends on what you're designing. Give every table a primary key. If a value in a "child" table must exist in the "parent" table use a foreign key. Occasionally, for "one-to-one" relationships, the foreign key and primary key overlap. Also, you don't need foreign keys to have "relationships" between tables. They just enforce the relationships.
Alright, I understand now. Thank you. :)
What are the primary keys for mbsbdreportinggrouptable and product_type_grouping? Also, what is the relation between mbsbdreportinggrouptable and inventtable? One to many, many to one, many to many? It also looks like you're working in the typical data warehouse environment that makes anything that isn't a basic rollup a pain in the ass. Sorry dude.
The thing I was overthinking was the aggregate function portion of the having clause. I assumed, incorrectly, that you could not use the function in the having clause because you cannot use the function in a where clause.
that must be the case. I haven't been able to get it yet. It must be the software I'm using to pull the query, I think it functions on Foxpro somewhat so that may have something to do with it.
Can you give an example of what you want in plain english? I don't understand what your problem statement is. Do you want a query that will join these tables and compute an output or just pick a result from the result table?
Is it possible that there are more than 4 variables in your actual scenario?
Yep, you're on the right track. You need a table that represents the detailed lines of an order with the fields you specified. The fk on that table would be the pk on the orders table. If you copy the data from the orders table to the detail table, using the current order pk for the records as the new fk, then remove the fields from orders, you won't have lost any data. You'd still have all of your one item orders available in the data.
Yes, there will be thousands.
You never said anything about speed, and c# which is the most used .net language for web is also interpreted, not compiled
That depends on what you need the database to do/be. If the database is a shared resource, you stand up an independent SQL Server (or other database server) and point your `connectionstring`s at it. You'll need to work out your security as well - integrated authentication (Active Directory) will make this easier, but you can't give all users in your domain DBO access to the database - you'll want to set up the appropriate roles and preferably attach AD groups to them. If it's for local usage only (each instance of the app has its own independent instance of the database), installing Express is *probably* overkill and you can get away with LocalDB. Asking that a full SQL Server instance be installed on every client workstation would be unusual.
It's massively inefficient if nothing else, which wouldn't lend itself to scalability. I'm not near a PC at the moment to try out some ideas in afraid, but id have thought using ranking and a distinct list of customers, you'd be able to make a loop to handle all this.
Ok so it will automatically do ascending unless desc is in there, and I can do this after each column?
So how do you know which one name you need? If you don't care, you can use something like (sorry for poor formatting, on mobile): SELECT OrgNo, MAX(OrgName) AS OrgName FROM Org GROUP BY OrgNo This would give you all the unique Org#s together with the last name ordered alphabetically (CGREV in case of 209) for each. 
Use http://sqlfiddle.com to build an example schema with data and describe the expected result. Right now, I personally don't have an idea what you want to do. 
See if your local library has anything. Seriously. I took two free SQL courses at home through my local library's website.
I don't know which one name I need. I thought about the max but did not test it. Thank you for your response
Subqueries man. Subqueries.
lynda.com has two courses on sql and access. both were very good. the access one in particular is really detailed for getting you up to speed. some libraries like los angeles public have free access to lynda.com. 
If the number of organizations isn't too unwieldy, you could do a manual scrub, SELECT [Org #], CASE [Org #] WHEN '203' THEN 'CG EXAM' WHEN '209' THEN 'CG REV' WHEN '247' THEN 'CG UTIL' ELSE 'Other' END AS [Org Name] FROM ... You'll have to start with a list of distinct organization numbers and figure out how you want each one to be styled. Then watch your future reports (or have your users alert you) to see when "Other" shows up, and add a case for the new org number.
Let me see if an example SELECT statement helps. I have two tables (A and B). tableA has information about items and tableB as information about costs (item is primary key). There is multiple costs associated with an item in tableB because different warehouses have different costs. I am trying to get to the highest cost but I want it to only look at certain warehouses. However, if I limit the warehouses in the WHERE clause, then I will miss out on any items that have a MAX cost in one of those warehouses, correct? So, I need to calculate the MAX cost only on certain items. SELECT tableA.item ,MAX (tableB.cost) - - I want this MAX function to only be based on records where tableB.warehouse IN (1, 2, 3) FROM tableA LEFT OUTER JOIN tableB ON tableA.item = tableB.item GROUP BY tableA.item 
Microsoft Academy
If you're in MSSQL you could try a windowed function to select the first instance of each number. It'd look something like... select OrgNum, OrgName from table where row_number() over (partition by [OrgNum]) = 1 Look up tsql row_number()
Is this SQL Server? Sounds like you want to cursor through a list of distinct accountNames from mastertemp. It's been a while, but something like: declare c1 as cursor for (select distinct accountname from mastertemp) declare @v varchar(1024) declare @actname varchar(1024) open c1 fetch next from c1 into actname while @@fetch_status = 0 begin set @v = 'insert into ' &amp; @actnname &amp; '.dbo.temp select col1, col2, etc from mastertemp where accountname = ''' &amp; @actname &amp; '''' exec(@v) end That's a quick way of cursoring through the distinct accountNames from mastertemp, creating a string that has your insert statement, then executing that statement as SQL would. A better way, but is probably more full of syntax errors, is using xp_cmdshell and not using cursors. Something like: declare @v varchar(max) declare @actname varchar(1024) set @v ='' select @v = @v &amp; 'insert into ' &amp; @actnname &amp; '.dbo.temp select col1, col2, etc from mastertemp where accountname = ''' &amp; @actname &amp; ''';' from (select distinct acccountname from mastertemp) x xp_cmdshell (@v) 
No worries, just wanted to make sure I wasn't misreading something. If you have questions on the above let me know, would be happy to explain further or provide more feedback if needed.
 OMG!!! it worked!!! ... thank you very much again.... this really means a lot to me ... i love you :'( people on reddit are so nice..God bless you
I think something like this should work. I'll give it a shot tomorrow and let you know. Thanks! 
Let me play with this tomorrow and see what it gives me. I appreciate the suggestions! 
You can also specify ASC for ascending if you want the code to be clear to other less experienced users.
You can't use windowing functions in the where clause in tsql (at least not in 2012). You'd have to wrap the query in a CTE and in that case just using an aggregate function would be the more elegant solution
I'm sure you can try, but I wouldn't expect the publication to replicate until the snapshot is complete. I'm not quite sure why you are publishing the whole database, but presumably you have a reason...
Can't sleep, so thought I'd play with this... You might have variations on Org Name, but surely some of them occur more than others, so why not build a list that gets you the most common variants per Org#? Initially I thought RANK would be the right windowing function, and I worked up a solution with that at work about 11 hours ago, but forgot to post iit before leaving. Good thing, too, because RANK could have left you with duplicates if for example "CG REV" showed up as many times as "CGREV". The correct function to use is simply ROW_NUMBER, so you can end up with a clear winner in the event of a tie. with CTE as (select OrgNum, OrgName, Count(*) Records from TABLE_NAME group by OrgNum, OrgName ) select OrgNum, OrgName, Records from ( select OrgNum, OrgName, Records, ROW_NUMBER() OVER(PARTITION BY OrgNum ORDER BY Records desc) RowNumByOrgNum from CTE) x where RowNumByOrgNum = 1 order by OrgNum;
Wow thanks very much, just what i needed :) 
If you're still learning I'd write the join explicitly. I.e INNER JOIN
stick this in front of it -- 100.0 * 
Why are you casting to int? You are probably losing some precision due to truncation.
&gt; EXCEPT and INTERSECT have an implied SELECT DISTINCT Like with `UNION` vs. `UNION ALL`, you can add `ALL` to `EXCEPT ALL` / `INTERSECT ALL` - at least in some databases... &gt; which means a sort You don't need to sort sets in order to remove duplicates. Hashing works too...
Was just thinking this when I while reading it. They are the "portable" solutions, but often not the best. 
You can beat the Oracle optimizer. You don't need to do it often, but sometimes it does nuts stuff.
My DB course had some pretty crazy brain teasers/convoluted problems using those concepts for weekly assignments. You come to love set theory along the way - or fail I guess. 
 insert into emp_hist (empno,bonus_date,job,sal,comm) select empno, sysdate, job, sal, comm from emp On my phone so formatting won't be any good. But you need a insert select statement, it can insert multiple rows in one go. Sysdate is today's date, it's better than hardcoding it. 
Great article.
Yeah, but usually if you are beating the optimizer, its because you have issues with poor indexing or out-of-date statistics (well, at least on the SQL Server side...not much into Oracle optimization).
as /u/PrezRosslin said, why int? you probably want to cast it as a float, and then do what /r3pr0b8 says and * 100. Casting it as an int will yield either a 1 or 0 without a decimal point. Also, instead of a CASE use a coalesce() or isnull(). 
It's similar on Oracle. Usually the bad plans come from old statistics.
Microsoft has an article when you should give up on the optimizer and just make temp tables: http://blogs.msdn.com/b/sqlcat/archive/2013/09/09/when-to-break-down-complex-queries.aspx
What database are you using? There's a really shady method using all_tab_cols on Oracle.. you probably know where this is going...
Can you point me to a resource/give a brief summary about O(n) and O(n log n)? I've seen people use that when talking about index performance, but I've never seen it explained. What is O? And n? When do they apply?
For SQL Server, every time I have ever needed to use temp tables or hints to beat the optimizer it has always involved searching date ranges. I think it may have also had to do with the dates not being anywhere close to sequential compared to the clustered index.
I really, really dislike this article, but I'm going to try to only give positive criticism. First, using a CTE does **not** automatically make your stored procedure better. This article gives the impression that the presence of a CTE in your stored procedure improves its quality without giving an example of HOW a CTE can improve the quality. All this does is give a simple explanation of how to use one, so it would be better titled "how to use a CTE" or "Syntax for using a CTE". If you want a reader to know how to improve their sp, give a real life example of you improving an sp using a CTE. There are many, many, many grammar and spelling mistakes. Proofread your articles before you publish them, have a friend or family member read them to spot mistakes, do something. Along the same lines, SQL is an acronym/initialism for Structured Query Language, not a word, which means all three letters should be capitalized (SQL not Sql). Consider using better examples in your articles. The last thing I want to read in a stored procedure is every single individual table wrapped in a CTE then joined together at the bottom. Really, the examples you gave would be great to show what **not** to do. Your first example: WITH backup_details(name,backup_type) AS ( SELECT name,backup_type from dbo.Backup_History WHERE Server=’MYTESTSERVER’ ) SELECT * from backup_details where backup_type=’Database’ should be simplified to this: SELECT name, backup_type FROM dbo.Backup_History WHERE backup_type = 'Database' AND Server = 'MYTESTSERVER' To be honest, your example of using a CTE to perform an AND operation is preposterous, to put it kindly. Your second example: ;WITH backup_details(name,backup_type,Server) AS ( SELECT name,backup_type,Server from dbo.Backup_History ), serverlist(servername) AS ( SELECT servername FROM list_of_servers ) SELECT * from backup_details INNER JOIN serverlist servers ON backup_details.Server=serverlist.servername should be simplified to this: SELECT name, backup_type, Server FROM dbo.Backup_History bh INNER JOIN list_of_servers los ON bh.Server = los.servername So what is an example where CTE's come in handy? Well, here's some code I'm working on right now I'll show you in order to demonstrate. WITH serial_signatures_due AS ( SELECT cs.id, cs.object_detail_id, cs.object_id, cs.object_subtype_id, cs.object_key_id, cs.entity_id, cs.due_date, CAST ( COALESCE ( LAG ( IIF(cs.void = 1, NULL, cs.signed_by_id), 1, 1 ) OVER (PARTITION BY cs.object_id, cs.object_subtype_id, cs.object_key_id ORDER BY cs.sequence_number), 0 ) AS BIT ) AS previous_signature_complete, cs.signed_by_id, cs.sequence_type_id, cs.entity_type_id FROM dbo.common_signatures_t cs WHERE cs.cancelled = 0 AND entity_id = 1527 ) SELECT ssd.id, ssd.object_id, ssd.object_subtype_id, ssd.object_key_id, ssd.entity_id, ssd.due_date FROM serial_signatures_due ssd WHERE previous_signature_complete = 1 AND entity_type_id = 1 AND signed_by_id IS NULL AND sequence_type_id = 1 UNION SELECT ssd.id, ssd.object_id, ssd.object_subtype_id, ssd.object_key_id, ssd.entity_id, ssd.due_date FROM serial_signatures_due ssd WHERE entity_type_id = 1 AND signed_by_id IS NULL Now let's break down why this CTE is necessary here. First, if I were to use a derived table, both instances of serial_signatures_due in the final select would be replaced by the contents of the CTE. That would effectively double the length of the query and could hurt the performance. Next, the windowing function. You cannot use a windowing function (LAG in this example) inside of a WHERE statement, and the best way around that is by using a CTE. Finally, refactoring this procedure to use the CTE (and other refactoring) brought the time-to-run for this procedure from ~600ms to between 0 and 3 ms. That's the most important metric to the users. Next on the list is your "Other uses" section. Just remove it. It tells the reader nothing. Same with your "Other SQL commands" section. 
Thats awsome,thanks very much i will appreciate your help, this is just what i was looking for. I never would have thought about doing it this way, but it works very well. 
what have you tried so far?
thanks, i appreciate it! I downloaded the dmg and installed the program. When i opened terminal I typed in sudo mysql and it says: "sudo : mysql : command cannot be found" Im not sure what to do about this? also, what IDE do you recommend using to work with mysql once i get it started up?
It's not homework. Have been out of school for awhile now. It's just work work. I changed it to red green blue because it's the other names are cumbersome. Thanks.
here is how I would approach it... Create a class to hold the "Entry" and store each line's entry Objects into an ArrayList. for each line of the file you have an ArrayList&lt;Entry&gt; (an arraylist of entry objects). Then you can count how many of each type of `Entry` object you have and store that somewhere. Alternately, as you read in each field you can check to see if you already have an `Entry` of that value. If so, increment a counter in that object that says this entry has "X" entries in the row. With the later you have an `ArrayList&lt;Entry&gt;` that contains only unique entries, And each unique entry has a property in the class that tells you how many of it were found in that line. Now all you need is a List of each of the `ArrayList&lt;Entry&gt;`, where each entry is a single line. `ArrayList&lt;ArrayList&lt;Entry&gt;&gt;` is what you need there.
It will keep the separate because it is doing a total on the count per row of each. AKA: (RED NULL NULL NULL NULL) will be (1 0 0 4) from the sub-query, (RED RED NULL NULL NULL) will distinctly be (2 0 0 3).
Great. Thank you!!
Thank you for that, I'm beginning to understand. So, with n being the number of rows, selecting from a table will have a complexity of O(n), while sorting it would be O(log n). However to do both, it would be O(n * log n). And from what I understand, O just denotes that you're calculating the complexity of an algorithm. So basically, if a query with complexity of O(40) runs in .01ms, then O(40000) would run in 10ms? And if a query of complexity of O(40 log 40) runs in .01 ms then a query of O(40000 log 40000) would run in 28.7ms? Or am I way off the mark here?
Itzik Ben-Gan is a shipshape place to start for SQL Server Window Functions. His books (published by Microsoft) are excellent resources, along with being priced at roughly $10-25. He could stand to hire a web developer for his site though: * [Resources](http://tsql.solidq.com/resources.htm) * [Books](http://tsql.solidq.com/resources.htm#Books) * [Book specifically about optimizing Window Functions](http://www.amazon.com/Microsoft-High-Performance-Functions-Developer-Reference-ebook/dp/B00JDMPHKY/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1444239252&amp;sr=1-1&amp;keywords=Microsoft+SQL+Server+2012+High-Performance+T-SQL+Using+Window+Functions)
I was able to make this happen using BCP. Thank you very much for your help.
Glad to help. 
This, [MFW reading OPs post](http://33.media.tumblr.com/3b20d40af08340b9290d1902f9bc8e06/tumblr_nueteuQ1pi1s373hwo1_250.gif). They should be banned general computer use for their disservice to humanity. 
not sure which DBMS you are using. But if you can pull an explain plan it will tell you the execution path differences between the different statements. I could just be that specific date has way more data. But you mentioned a between with the dataset is fast. I would start with the explain plans. 
The issue may not be with your query. Is the date field indexed? If so, when was the index last rebuilt or the statistics updated? Also, as /u/Pork_Taco implied, knowing which RDBMS you're using would be a help.
In addition to /u/Coldchaos's suggestion, have a look at [Kathi Kellenberger's blog](http://auntkathisql.com/category/t-sql-window-functions/) and book. I saw her speak on windowing functions at a SQL Saturday about 2 years ago and her presentation flipped the light bulb on for me.
Good to know. I saw Wayne speak at Summit 2012 but haven't kept up with his blog.
 TSQL and using SSMS 2012. I should also say that it is not just slow but we were unable to pull it up, we tried waiting for 30 minutes but it never came up, it just says executing query and no error messages. 
It is not indexed. The data is really small like 20-100 records per day. TSQL ssms 2012. It won't load the data at all although the longest we've waited is 30 minutes before we have to stop the query. I'm not sure if it will eventually load it though but even if it does, that is still way too long.
Haven't tried that yet. I'll try in a while. 
How many records are generated on the first or after? Could be a lot of end/beginning of month transactions in there. Either way, if that query with the where clause is used frequently, index the column. Try it out on the dev db first of course :)
Check out this link: http://www.tonymarston.net/php-mysql/pagination.html
This was good: http://www.gab.lc/articles/window_functions_postgresql Postgres but should be similar
n*log n has higher growth rate than n, so it dominates. The base of the log doesn't matter. The difference between two log bases is a multiplicative constant.
Not at all justifying this behaviour, but I've run into shit like this so many times that I don't even bother to think about why anymore - I just find a way to make it work because that's my goddamn job. There's more bad design out there than good.
Thanks! I take it this page is more developer orientated then?
It is possible that even with an LDF that you won't be able to recover. When I am doing db work, I sometimes have to insert, update, and delete a bunch of data. The LDF gets full from those changes, and I have to truncate it to continue working. It is possible to truncate it to 0. The LDF is more for tracking short-lived transactions, so that they can be rolled back.
If you're missing the MDF and don't have backups, you're SOL :(
Without example T-SQL and schema, we can only guess as to why this is happening. /u/treebeard901 is probably right - you either don't have an FK in place or the FK isn't trusted. Also, you won't see join elimination on a multi-column FK in older versions of SQL Server. Conor Cunningham covers the bases in [Conor vs Foreign Key Join Eliminations](http://blogs.msdn.com/b/conor_cunningham_msft/archive/2009/11/12/conor-vs-foreign-key-join-elimination.aspx). **note** Conor's examples are for `INNER JOIN` only, you'll need to change them to `OUTER JOIN`s to test appropriately. When I test (SQL Server 2014 SP1 12.0.4213.0), here's what I get: * I do see join elimination in a single column `LEFT JOIN`. * I also see multi-column FK join elimination in a multi-column FK `LEFT JOIN`. * I do **not** see join elimination with a `LEFT JOIN` _and_ when there is a predicate in the `WHERE`.
Select SCO_ID From dbo.PPS_SCOS Where Name like '%python%' Should do the trick. 
this -- SELECT sco_id, name ... GROUP BY sco_id will cause an error in MS SQL
&gt; if this search could be case independent that would be wonderful By default, SQL Server is case-insensitive. But you can set case sensitivity (part of the collation settings) at the database, table or field level. See [this Stack Overflow answer](http://stackoverflow.com/a/1411260/1324345) for how to check the collation of your DB, table and field. If you find that you've got a case-sensitive collation in use, you can force your query to use case-insensitive. See [this blog post](http://blog.sqlauthority.com/2007/04/30/case-sensitive-sql-query-search/) - just put `SQL_Latin1_General_CP1_CI_AS` in where you see `SQL_Latin1_General_CP1_CS_AS` in the example query. If you're making this part of an application or report, [watch out for performance gotchas](https://www.mssqltips.com/sqlservertip/3215/how-column-collation-can-affect-sql-server-query-performance/)
works in MySQL, though at least MS SQL developers will, after getting an error message every time they group incorrectly, learn how grouping actually works MySQL developers, knowing no better, use that actual construction and then wonder why they get indeterminate results
I've read you don't need foreign keys when doing a Left Outer Join. I'll see if I can find the article. In my testing, I am seeing Join elimination occur on tables without foreign keys. I'm not sure I understand your question but it knows it can ignore a table when it's a Left Join because it knows the join isn't going to filter the results. It also sees that the join is to the table's primary key, so it knows the join won't duplicate any rows. It also knows it doesn't need any data from the table if nothing in the query references it. So it ignores it. I believe Inner Joins do require foreign keys 
a bad example because this is Oracle based but I'm hoping the idea is similar MS SQL in the section labeled "outer join table elimination". http://optimizermagic.blogspot.com/2008/06/why-are-some-of-tables-in-my-query.html This one talks more about FK's being necessary for the Inner Join. It doesn't say anything about needing them for Left Outer Joins: https://sahlean.wordpress.com/2014/04/01/join-elimination-reasons-which-prevent-this-optimization/
gave it a shot, no luck 
Odd. Can you post the schemas of the tables product_type_grouping_test, mbsbdreportinggrouptable, inventtable, and custinvoicetrans? Or at least the columns being used/joined to. We don't need the entire schema For the record, I tried hard to make this code not perform the join exclusion on a left join, but I can't. Here's what I have so far DECLARE @parent_table TABLE(id INT PRIMARY KEY IDENTITY(1,1), child_id INT NULL, child_entity INT NULL); DECLARE @child_table TABLE (id INT PRIMARY KEY IDENTITY(1,1), entity INT NULL, value VARCHAR(1)); INSERT INTO @parent_table VALUES(1, 1), (2, 2), (3, 3), (4, 4), (NULL, 5), (5, 6) INSERT INTO @child_table VALUES (1, 'X'), (1, 'X'), (1, 'X'), (2, 'X'), (3, 'X'), (3, 'X') SELECT p.id FROM @parent_table p LEFT JOIN @child_table t ON p.child_id = t.id AND p.child_entity = t.entity
I'm not sure what you mean by schemas? the original post has the schemas for each table, mostly syn which are synonms pointed to another database, and the one local dbo table (product_type_grouping_test) I thought it might have something to do with syn vs. dbo but I created another test table on the dbo schema and it worked fine. It is odd, every other scenario where test &amp; expect the elimination to happen, it works. There has to be one weird "gotcha" i'm missing. 
I wasn't aware of the synonyms being involved. All bets are off on what the optimizer will do in that scenario. Although, one thing comes to mind, if you track down the synonym you are using for the left join, verify the user configured for the linked server has the necessary permissions to read the statistics on the remote table. Even if you are not going across a linked server, permissions could still come into play. Some more info: http://dba.stackexchange.com/questions/58170/sql-server-linked-servers-and-remote-statistics
Sorry, bad terminology on my part. I mean the table definitions.
Yeah I was thinking it had something to do with the synonyms too. The whole query is selecting from synonyms on another database (on the same server). The table that's giving me trouble is the only one on the database the query is run on. I would think the opposite would happen. As a test, I brought one of the synonym tables that I knew Join elimination was working for, from the other database to the local database, and that worked fine. 
Yeah, ignore anything about Oracle - they're not the same product and you can't count on SQL Server and Oracle having the same feature set. On the version of SQL Server that i have (12.0.4213.0), I can get FK join elimination on two tables with or without an FK between them. Honestly, I almost never see FK join elimination happening in the real world. I'm not sure why SQL Server wouldn't perform the join elimination in your case. You could try breaking down the query into smaller and smaller chunks and see which join is causing the optimizer to not perform join elimination.
I'm not sure I understand the question, but I think this might do what you're asking: SELECT EMAIL_ADDRESS_, BIRTH_DATE, Count(distinct to_char(BIRTH_DATE,'MMDD')) FROM $A$ WHERE to_char(BIRTH_DATE,'MMDD') = to_char(sysdate,'MMDD') GROUP BY EMAIL_ADDRESS_, BIRTH_DATE But that seems silly, since the query by definition only has one date possibility so the count would always be one. If that's not it, please expand on what you mean by "date possibility". If you mean you want to count the number of $A$ records for that date it'd be SELECT EMAIL_ADDRESS_, BIRTH_DATE, Count(*) FROM $A$ WHERE to_char(BIRTH_DATE,'MMDD') = to_char(sysdate,'MMDD') GROUP BY EMAIL_ADDRESS_, BIRTH_DATE If you found this tip helpful feel free to gild it!
thanks thats really helpful! Im just trying to get ahead of the client asking silly questions :D you dropped this ) :D 
Sorry that first one was actually dumb, it groups on the email address and I guess the point is that there are a bunch of email addresses. It would actually be better to do this: SELECT EMAIL_ADDRESS_,BIRTH_DATE, Instances from $A$ INNER JOIN (SELECT to_char(sysdate,'MMDD') Day, Count(distinct to_char(sysdate,'MMDD')) Instances FROM $A$ GROUP BY to_char(sysdate,'MMDD') ) x on x.day = $A$.to_char(BIRTH_DATE,'MMDD') where to_char(BIRTH_DATE,'MMDD') = to_char(sysdate,'MMDD') Which is a really longwinded way of creating a count that is guaranteed to be one anyway. (I don't actually use oracle so the syntax may be fucky) 
dang, I should have figured this one out... That's exactly what I need. Thanks man.
 SELECT jobid, teammember, STUFF( (SELECT ','+ teamrole FROM tblJobTeam WHERE teammember = t.teammember FOR XML PATH('') ),1,1,'') AS TeamRoles FROM (SELECT DISTINCT teammember FROM tblJobTeam )t Didn't test, but think the above will work for ya.
Are the collations specified for the different databases involved all the same? 
I just checked and yes
It's basically just a normal connection to the database where excel interprets the output. So if you have permission to create then you'll be able to do that in PQ. In fact, if you have the right priveleges, you can create and drop actual tables, drop databases (I'd assume, haven't tried) and basically do whatever you want. Important if you want to ignorantly send a refreshable version to someone who has a connection to your DB... I wonder if you can escape the DB you're in even though PQ has you declare the DB you're using, sure you can just USE [anotherdb] and do whatever you want. Yeah, more than you asked for but some extra info I guess.
Float might not be the best *practice* (probably won't make a difference in this case) as it's actually less accurate than other similar types. I'd reccoment something like decimal(x,y) or money.
Seriously, life would be so much easier if you had took the time to actually build a schema with an example and expected results. It's not like it takes hours upon hours. Took me like 5 minutes tops to build that schema. [Here you go](http://sqlfiddle.com/#!6/1105a/1). I don't know what RDBMS are you actually using, and since I'm most comfortable with SQL Server, well, I'm not gonna bother. I used a subquery though, instead of a CTE which I normally would, it should make it easier to understand if you're not familiar with the concept. If you're using MySQL, I think the SET clause comes after ON, not before the JOINs. The way it works is that you "first" calculate the desired WeightedCost, using the BatchCost value of either 0 (in case of Analysis = 'X') or the nominal BatchCost, just as you specified in the requirements, for each product (`SELECT ProductID ... GROUP BY ProductID`). Then you simply join this data to the table you want to update via the ProductID column and update the desired column with the value calculated in the subquery. And that's it. If what I'm doing is not exactly what you want, then at least I hope I'm kicking you in the right direction, which most likely will involve a subquery joined to the real table you want to update.
Alternatively, I like to throw this SELECT statement in an OUTER APPLY statement, just for organizing my query. I try to avoid putting sub queries inside the SELECT list. 
 select firstname[FirstName], lastname[LastName], 'Total'[AccTxTypeName], sum(amount)[Amount] from table Group by firstname, lastname PKs for the names would obviously be pref., in case there are 2 "John Doe"s, and I'm not entirely sure if you want to build a new table from this, or just have a summed amount as a "new column" in the result set, but this should get ya started...
Oh boy. Best idea I have is to create separate CTEs for each type of group e.g. CG, AG, whatever, and they would be filtered down with a CONTAINS. Then you'd join them all up and simply concat the column with col_a + " - " + col_b but that's self-explanatory. Someone else might have a more elegant solution because mine would be hard-coded. Edit: ignore my suggestion. A simpler way would most likely just be going with a row number query as stated below.