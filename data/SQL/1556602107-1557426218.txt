How dare you steal my answer!
declare tmp Select tmp = min(age) from TABLE_NAME
Give them a new, unique &amp; unified ID in the new DB and store the old ID as information against the new ID in a table should this information ever be required.
Perfect, was struggling implementing unions and all that. Much appreciated!
Have you tried typing another ' character? Your interpreter probably thinks you're still typing a string literal over multiple lines.
Use coalesce or case statement to have both IDs as one column?
It was fixed when I select control C then Type ;
&gt;Unless you specify a specific type of join (LEFT or RIGHT), an OUTER JOIN on its own is by default a FULL OUTER JOIN That's plain wrong.
Me too, I find SSMS intellisense just gets in the way when I'm working.
To give another perspective: a full outer join is equivalent to a UNION of a left and right join. See this example: [https://rextester.com/RHU50077](https://rextester.com/RHU50077)
i invite you to test a query which says `FROM a OUTER JOIN b ON ...` you will learn that there is no such thing as `OUTER JOIN` you ~must~ specify either `LEFT OUTER JOIN` or `LEFT JOIN` or `RIGHT OUTER JOIN` or `RIGHT JOIN` or `FULL OUTER JOIN` you cannot just say `OUTER JOIN`
I was considering mentioning that SSMS forces the brackets on everything when you select top rows, as well as when you drag and drop columns and tables. I could see how someone would assume they were required because of this behavior. I remembered finding this post years ago, stating that people had been asking for a way to turn it off and Microsoft had refused. It turns out they added an option at some point and I never knew. I know what the first thing I'm doing today is! https://dba.stackexchange.com/questions/21779/sql-server-2012-is-putting-brackets-around-table-and-column-names
Career Benefits of [MS SQL](https://www.ssdntech.com/blog/what-are-the-career-benefits-of-ms-sql-training-and-certification/) Training and Certification
Depends on the type of replication you use i guess. If you use transactional replication you can just create the table at the subscriber db and add the new article to the replication.
Before any analysis is done, I’ve gotta ask why do you want to know how long your people are in the office? It seems like that’s a big trust issue and definitely not an indicator of productivity
I'm using peer to peer publication. I know the method to create the table manually to my subscribers and then add to my publication like an article but I want to create automatically this table to my sub.
Db3 = SQLite? If so: 1) download PowerBI desktop https://powerbi.microsoft.com/en-us/desktop/ 2) connect to db https://community.powerbi.com/t5/Integrations-with-Files-and/connecting-a-SQLite-database/td-p/224205 3) add data and visualizations If you can’t connect directly to the Db, then use the db3 file to restore into your own SQLite container and report from there..
OPs question was about "outer join" vs "full outer join". Reread my reply.
Yep. Its pausing - `WAIT`ing for a `DELAY` of 30 minutes.
But what is it pausing? Everything?
Your statement was: "*an OUTER JOIN on its own*" - and that is not possible. You **have** to specify left, right or full. You can't just write `from a outer join b on ...`
What is the query? Usually a WAIT is used when a loop is being used so it doesn't hammer the system. The WAIT will pause the entire query from completing.
Everything in that batch/process. Not the entire server.
30 minutes is kind of extreme, but I have a couple processes that have wait steps. One example is waiting a few seconds to try a procedure again and another is the overnight cycle waiting on production to finish backing up before triggering, so every 10 minutes it calls out to see if the restore was completed and then triggers once it finishes.
I'm doing reporting - not in a position to create/store any kind of IDs. All I'm doing is pulling data using TSQL for SSRS and Tableau reports.
What I posted is the whole query. I ran sp_whoisactive and I see that all the time. I don't know where it's originating. But I assume based on the other comments that another process is initiating that wait query?
Is there any way to see what process initiated the delay?
I don't have time to dive into your data, but here are some observations: 1. You will need to use something like this: `, row_number() over(partition by user_id, datepart(dy, timestamp) order by timestamp AS) as rn1`, `, row_number() over(partition by user_id, datepart(dy, timestamp) order by timestamp DESC) as rn2`, which will take your data and format it in such a way where each day, the first time a user goes through the door, there will be a 1 for rn1, and the last time they leave will be a 1 for rn2. From there to get the total minutes in the day you would just do a simple datediff(mi, rn1, rn2). 2. You'll then need to do various datediff()'s to calculate the time to subtract from the total datediff() in order to get the minutes inside the office. 3. What are you going to do when two people leave/enter together, and only one of them swipes?
If you're using t-sql, you can download SQL Search and look for procedures using Waitfor .. I'm not a DBA so I don't know if there's a better way to do it, but that's what I do when I'm trying to find what tables/procedures use certain things. There should be a way to see what user initiated it, and just ask them too tho because it could just be a query instead of a stored proc or something
You're saying that the entire query does nothing except make the system wait? I can't think of any legitimate reason for that, but there are others far more qualified than me here that you would want to listen to. I would remove it.
Yup. `EXEC sp_WhoIsActive @get_full_inner_text = 1`. Read the docs on sp_WhoIsActive - it gives you a lot of great info on helpful features besides just the defaults. WAITFOR is pretty cool too. It lets me spid kill myself if I have a query that needs to finish before nightly processing, or if I have to wait for a specific time to run a one-off, but don’t want to schedule it.
If it's in a Stored Procedure you can use the below script. &amp;#x200B; SELECT obj.Name SPName , sc.TEXT SPText , obj.create_date , obj.modify_date FROM sys.syscomments sc INNER JOIN sys.objects obj ON sc.Id = obj.OBJECT_ID WHERE sc.TEXT LIKE '%WAITFOR DELAY '00:30:00'%' AND TYPE = 'P'
Thank you! It looks like it's a scheduled DBATools thing. The hunt for my lockups continues! lol
Do you have proper keys in those "combined" tables or are all of them heaps?
I used MattMC3's query and I found out what was running it. DBATools
Oh, I understand. You have other sprocs which are calling it and causing the wait. Sounds like it was designed to stop processes from hammering the server, but 30 minutes is a really long wait.
Which DBMS product are you using? "SQL" is a query language, not the name of specific DBMS product. Configuring replication is very different for the different database products.
SQL Server 2017 developer edition
You need to double your embedded quotes: &gt; sc.TEXT LIKE '%WAITFOR DELAY ''00:30:00''%'
Yup you're correct!
 SELECT pers.id , pers.name , CASE WHEN proj.project_id = 456 THEN 'YES' ELSE 'NO' END AS works_on_projcet_456 FROM persons AS pers LEFT OUTER JOIN ( SELECT project_id , person1_id AS person_id FROM projects UNION ALL SELECT project_id , person2_id AS person_id FROM projects ) AS proj ON proj.person_id = pers.id ORDER BY pers.id
Our DB is managed by an outside company. I put in a ticket with them to spend a little time and walk me through the scheduled tasks and their functions. From what I'm reading, most delays/waits are only a few minutes.
It's quite a common thing so the internet will be full of good examples. Basically you have to use a For-Each loop Loop through all files in the directory using a wildcard match (BI_Data_Dump_*.csv) Process each file and move on This https://www.mssqltips.com/sqlservertip/2874/loop-through-flat-files-in-sql-server-integration-services/ Is a decent intro, but there are hundreds find one that your happy with.
this is many folks' approach, no doubt.
Doesnt it do so as-is? I've never used it, but schema changes should be enabled on the peer-to-peer and as soon as you add the empty table to the publication it (theoretically) should be created on subscribers (no partial subs either).
So. First I'm going to tell you how to do what you asked for. After, I'm going to tell you what you should do instead. You don't want a new table for this information. Either you'll have to keep it up to date, or it'll get out of sync with the underlying data. Get that out of a query each time. Just make SQL figure it out new each time. ---- Here's how to list all project IDs for, let's say, person 2 select distinct project_id from projects where person1_id = 2 or person2_id = 2; `distinct` means it'll throw away duplicates ---- Now, this isn't a good structure. Every time you want to add a person you have to add a column and modify the queries. Instead, what you should do is keep three tables - one for projects, one for people, and one that lists where they line up. This is generally referred to as a "bridge table," though that's not a technical term. The advantage of this structure is that multiple people can be on one project, multiple projects can be in one person's hands, a mix, whatever, and you don't have to modify queries to keep up. It's always the same "shape." You didn't say which SQL, so, given that `as` notation, I'm going to assume MySQL. create table projects( id integer unsigned auto_increment primary key, name varchar(255), index name ); create table people( id integer unsigned auto_increment primary key, name varchar(255), index name ); create table people_on_projects( id integer unsigned auto_increment primary key, person integer unsigned, project integer unsigned, foreign key(person) references people(id), foreign key(project) references projects(id) ); Then you can just select person.name, person.id, project.name, project.id from people join people_on_projects join projects on people.id = people_on_projects.person and people_on_projects.project = projects.id
firstly, please switch to the ansi-style joins (you'll thank yourself later) (... join ... on ...). secondly, if you need a record regardless if you found corresponding/matching information for it or not, it most likely means an outer join. and thirdly, I always recommend granularity-driven query design: What grain (relevant) the input tables/datasets are? What is the desired output granularity? What is the granularity of other aggregations? In a very simplistic manner, you can think of "group by" as "reduce/set granularity at", i.e. dataset with granularity (A,B,C,D) group by (A,C) gives you granularity (A,C). You can think of joins as "adding granularity" - (A,B) &lt;any&gt; join (C,D) gives granularity of 4 (A,B,C,D) unless your condition culled or made sure some granularities match up, i.e. (A,B) &lt;join&gt; (C,D) on A = C would give 3-member granularity (A,B,D)
This looks nice, but I nead a table with all the names and if they do project 456 for example. Something like this: Name | Project 456 ----|----------- John | Yes Pete | Yes Hank | No
You can do this, but you'll have to change the project and column name for each project: SELECT Name, CASE WHEN EXISTS ( SELECT 1 FROM Projects WHERE Project_ID = 456 AND ( a.id IN ( Person1_ID, Person2_ID ) ) ) THEN 'Yes' ELSE 'No' END as 'Project 456' FROM Persons a
&gt; SELECT &gt; Name, &gt; CASE WHEN &gt; EXISTS &gt; ( &gt; SELECT 1 FROM &gt; Projects WHERE &gt; Project_ID = 456 AND &gt; ( &gt; a.id IN ( &gt; Person1_ID, &gt; Person2_ID &gt; ) &gt; ) &gt; ) &gt; THEN 'Yes' &gt; ELSE 'No' &gt; END as 'Project 456' &gt; FROM Persons a Wonderful, thank you.I want it like this with changing the project by myself.
Perfect, thank you!
I don't capitalize anything. Just looks silly to me.
If you have a decent IDE that highlights the syntax I agree I guess l, I do it out of habit
My $0.02: don't spend too much time and money on certs. I've done quite a bit of hiring/interviewing for SQL heavy positions [nothing open at them moment, sorry :) .] I've never cared what certs the candidate claims to have. I ask practical SQL problems ranging from very basic up to pretty puzzling to gauge a candidate's SQL chops. The main thing I'm looking for is a good mental model of what SQL is actually doing and an ability to apply that model practically. I don't care what fancy pile of probably dbms-specific functions and syntax you know; that's what google is for. I also look for and discuss a practical application of SQL. I'd much prefer your being able to discuss a project you worked on than have you rattle off tests you passed. If you don't have any work experience on this side, do a personal project on an area that interests you, e.g. set up a good sized database with publicly available datasets and develop a range of queries against it (DDL, DML, quick transactional stuff, fancy analytic stuff.) The only positive I can think of with certs is that they *might* help you get past HR and into an actual interview round. This really depends on the recruiter, though. In my experience they will often not know what certs matter or not care. In those cases, a list of programming languages, skills, platforms, etc. would be just as distinguishing. Good luck on the hunt!
I don't capitalize; I prefer structure-on-page to indicate parts of a query. http://poorsql.com/ and the related add-ins are handy to get stranger-code into your preferred reading format.
Canadian here, Winter doesn't seem to want to leave, despite several requests to do so... send help!
Which platform?
https://docs.microsoft.com/en-us/sql/t-sql/queries/from-using-pivot-and-unpivot?view=sql-server-2017 Documentation looks perfectly fine to me, hell it's what I used when I did my first unpivot. How about you first try and write something and then we'll help if you run into issues?
I capitalize all keywords. Columns and tables are generally upper or lower camel case. Aliases are usually lower case.
Lol, I'm wondering whether it's a good idea to add a skill that I don't have any professional work to show for. On second thought, I did actually add Excel as a tool I'm proficient in when right after I graduated. Only now do I realize what a complete noob I was.
I've actually done a bunch of SQL problems on HackerRank and 2 related courses on Udemy. So I won't say I know nothing about it, just that I haven't used it in work so far.
Thanks! Thatwas exactly my question. I might have taken a SQL course or two on Udemy,so I definitely have some knowledge of it. I guess I'll put it up then!
Not really, almost all data that I work on is available through excel spreadsheets. Even when there are databases involved, I'm not allowed to use them directly. What kind of queries do you use in your day to day work? It might give me some idea of what to expect.
Everything or nothing depending on situation
Maybe something like: ;WITH unpivoted AS ( SELECT id, DayOfWeek, Amt FROM yourtable UNPIVOT (Amt for DayOfWeek IN (Sun, Mon, Tue, Wed, Thu, Fri, Sat)) u ) SELECT * FROM yourtable yt CROSS APPLY (SELECT TOP 1 DayOfWeek FROM unpivoted u WHERE u.id = yt.id ORDER BY Amt DESC) dw
I always capitalize SQL clauses SELECT/INSERT INTO/FROM. Everything else is fine so long as it's consistent throughout the database. I have really particular formatting and it also helps me determine if someone updated something in Dev or if later I see something I know I didn't write or do code review for as I will format someone else's query before I commit it to source code.
For me it's context driven. If I'm writing a SQL query in an editor with syntax highlighting I'll leave things lower case, as there doesn't seem to be a benefit for capitalizing keywords. If I'm working without highlighting, I'll capitalize keywords so it's easier to differentiate between those and the rest of the query. I've never capitalized alias or column names unless it matches what a previous developer has written or I'm working with a case sensitive collation.
Same here. My pinky hits shift automatically whenever I write a keyword
This helps a ton! I think I have figured out why I was getting errors when I was running it. Thanks for the help. I'll have to give this method a try
Same here- I'm a bit anal-retentive about my format and definitely going through someone else's code and putting it into my (beautiful) format helps me understand what they're doing.
If it's truly an entry position, I can't imagine it would hurt to have. May help get your foot in the door. It's definitely not necessary though. I, personally, had 0 SQL experience when I got my entry job but I guess they liked my attitude and were willing to build me up. I'm sure that's not super common though.
Happy to help! Hope it works out for you :)
I’m in an entry level data analyst position so I mainly prep Excel files for import into the database. Then I’ll check whether it was actually imported by querying the specific table. SELECT FROM ORDER BY BatchID Simple stuff. The more complicated queries involving Joins are already documented so I just have to copy and paste it in SQLServer.
One of my favorite ways to unpivot data is by using a cross apply method that doesn't involve using an UNPIVOT operator. &amp;#x200B; [https://www.sqlservercentral.com/articles/an-alternative-better-method-to-unpivot-sql-spackle](https://www.sqlservercentral.com/articles/an-alternative-better-method-to-unpivot-sql-spackle)
Thank you! I appreciate you writing that all out :)
I actually posted a [snippet of code](https://i.redd.it/x0fq5x8o6m211.png) from the code I am rewriting now. Imagine having 1200 lines of SQL code where the carriage return is whatever the original developers monitor width and zoom setting put the end of the cursor.
I think we'll need to see the whole command....
From my personal experience, having a cert only gets you the interview. If you aren't getting interviews, then add a cert to your resume. Also, some jobs require them even if they are bs.
CREATE TABLE Sales ( Item_id VARCHAR(255), Unit_price NUMERIC(10,2) );
I like using this site for a quick formatting: https://www.freeformatter.com/sql-formatter.html Set to 4 indents and capitalize SQL keywords. It's one of the top ones in a google search so I can always use it when I'm at a dev's machine or something and they wrote some godawful unreadable mess and I need to debug. :)
&gt; When to capitalize? Never (at least, never in code). There's no good reason other than it's what everyone has always done. It can be helpful in the middle of English sentences when referring keywords like SELECT but other than that, it's hard to type, hard to read, and offers no modern benefit. I have shared my modern SQL style guide here before as a reference for how SQL can look great in lower case. https://gist.github.com/mattmc3/38a85e6a4ca1093816c08d4815fbebfb
I used to capitalize everything, but then I got tired of holding shift all the time, so I went back and uncapitalized all my recent work. Then I decided I should be consistent with my coworker, so I went back and recapitalized everything. Now it depends on the day. But I would like to stop capitalizing again going forward.
I like to capitalize keywords, and have identifiers and functions lower case. Just doesn't feel right otherwise.
I'm on the same boat, thinking of getting MTA 98-364 and building some projects also to go with it.
A is 65 on the ascii and a is 97 (or something?). When the parser reads and converts to bytes, you are doing 30 more iterations for each lowercase letter. Obviously, this is a cached process which makes a difference but code also looks much cleaner when the declarative code is capitalized and the column names are lowercase
I like to capitalize keywords, and have identifiers and functions lower case. Just doesn't feel right otherwise.
Thankfully there are tools to pretty print SQL.
Wrong! Any query tuner always capitalizes declarative words because it's lower in the ascii table. It's always more efficient to capitalize but the process is so cached it barely matter nowadays. But there is an efficiency reasoning behind it.
Oof. That's brutal.
That's... uh... a completely bizarre statement. You might as well write insert statements and push binary data into your query cache system tables if that's the level at which you choose to tune your queries.
holy fuck. sucks to be you! :D you can always run ApexSQL refactor on it (or Redgate SQL Prompt, if you've got it) to at least get a start on reformatting it.
&gt; CREATE TABLE Sales ( Item_id VARCHAR(255), Unit_price NUMERIC(10,2) ); i just tested this in MySQL ver 8 and it works fine what version of MySQL are you on?
Sometimes, if I'm writing a query that nobody will ever see again, I just write it without any capitalization. I'm a bit of a rebel in that sense. If something is going into a repo somewhere, then I cap the keywords.
Er... That's not how it works.
I haven't found one I really like yet. Too often it carriage returns too much especially with joins. INNER JOIN dbo.Table t ON a.id = t.id AND t.bidId = b.bidId AND t.orgId = c.orgId What I want INNER JOIN dbo.table t ON a.id = t.id AND t.bidId = b.bidId AND t.orgId = c.orgId I usually just open a query and to a ton of replace statements and go through each statement indenting the subqueries, putting a if exists and drop table on seperate lines. Commenting that temp table population in some comment blocks. Going line by line really helps me understand the rats nest of crap the sql is doing. No joke I've had times where I've seen blocks of code like this IF OBJECT_ID('tempdb..#temp1') IS NOT NULL DROP TABLE #temp1 SELECT * INTO #temp1 FROM dbo.table IF OBJECT_ID('tempdb..#temp2') IS NOT NULL DROP TABLE #temp2 SELECT DISTINCT pkey INTO #temp2 FROM #temp1 IF OBJECT_ID('tempdb..#temp3') IS NOT NULL DROP TABLE #temp3 SELECT * INTO #temp3 FROM #temp1 INNER JOIN #temp2 on temp1.pkey = temp2.pkey DROP TABLE #temp1 DROP TABLE #temp2 IF(SELECT COUNT(*) &gt; #temp3) &gt; 0 BEGIN DROP TABLE #temp3 END --WHY THE FUCK DID YOU JUST DO ALL THAT!!!?!?!?!?! The original developer: "Oh I was debugging something, I guess I left that code in there." Me: "The analyst who copy/pasta your code copied this into 12 different reports. It adds like 30 seconds of processing to every db call. Don't ever ever ever commit test code to ANYTHING, not even dev."
Have you tried killing a night king or two? Worked a charm for us down here in the southeast US.
No Kings here, just an immortal Queen Mother.
I should really shell out the money for APEXSQL refactor I just don't trust it's going to undo 10 layers of temp tables. I am currently in the process of an entire database rewrite which really isn't that bad as it's a very small database when you remove 310 unnecessary tables from it and the entirety of the database fits in 30 unique objects. What's taking a while is just backwards compatibility. I'm presenting views for the application to use to keep consistency.
ApexSQL refactor is free. SqlPrompt is better, IMHO, but it's not free.
Hmm I will have to give it a go. Last time I looked at the Apex product suite is was costly. Thank you for the info.
&gt; And a guy that got elected for throwing himself down a flight of stairs. Could be a night king, dude. Not to suggest you commit a crime, but if you really are sick of winter . . .
No prob! Good luck to you! :)
poorsql.com dude.
Well, are night kings really good at using a metric fuckton of words to say absolutely nothing relevant to a question they were asked directly?
Kinda the opposite actually, lol. They are not a talkative bunch.
Guess our PM is not a night king then... I'll have to go put away the knives.
\&gt; I'm new in this context of SQL. So new I don't bother indicating what SQL-product I'm using, nor do I bother indicating the style of replication.
I capitalize on the opportunities to work in a field that I enjoy.
Never
As you might with any other language, pick a style guide used by people, companies, projects you like and follow that. Maybe something like this: https://www.sqlstyle.guide/
Read the comments senior
Good god that site increased my query from 85 to 270 lines. I dont need a new line for each select column and group by
Or Caps Lock
\&gt; I'm new So new I tell other people to "read the comments" rather than update my original post.
There are configuration options on the right. You can keep your select columns all on one line by unchecking "expand comma lists" if that's really how you want to live your life :)
Haha I dont mind it too much but I think my senior dev wouldnt like it
yes, you can &amp;#x200B; SELECT somedate + INTERVAL 1 DAY
Thats quite unexpected. So the answer is yes? :o \+ does the same thing DATEADD would?
Up to personal preference. I personally capitalize keywords and format my SQL ry nicely as it makes it easier to read.
Today , people live to make a meme 😑
You do, but you just don't know it yet. As to capitalizing, I capitalized keywords for the first decade of writing SQL, but I stop doing it ages ago because carpal tunnel. Besides, most IDEs have a decent SQL formatter plugin these days, let it do the capitalization if that's the agreed upon standard.
Serious question. Are there jobs that are just writing SQL? I've written tens of thousands of SQL queries, but that is just a small side part of my job as a web developer for a Fortune 100 company.
I'm a BI Analyst, and my employer just wanted to know that I knew the basics. They asked me a handful of questions in the interview about how to approach certain problems within a SQL context. As long as you can convey you have a foundation and are eager to continue to learn, I dont think a cert is needed.
That's 90% of my job. Add in some tableau and SSRS/SSIS on a rare occasion.
Can you tell me how you found your position? I'm trying very hard to move from accounting/finance to BI but not sure I'm going about it right.
I rarely format and use the same tool once I’m done. People... I don’t care how you capitalize things... if I see I right join, I will rewrite everything.
I got lucky, my IT department had an internal job posting for it and I applied. A buddy of mine found a job doing SQL through Indeed.com. He has about 2 years of SQL experience only and is going through a 3rd round of interviews for a Software Engineer position. The posting asked for a lot more than SQL but He was brutally honest in the interview about where his skills were at but made sure they know he was willing to work hard to get to where he needed to be. I think that eagerness to learn goes a long way, or else he wouldnt have gotten as far into the process as he had.
Excuse my while I go throw up
Just format some sample data and paste it in along with whatever your query framework is. Really that's all I look fork. Do you have an example of your data, an example of what you want your data to ultimately look like, and lastly some kind of code. I hate posts where people talk a lot (I do this in mine), and I tend to more probabilistically respond to posts that have an example of their data, and then example of what they want. I'm not good at reading, it takes too much time, just show us what you have, and show us what you want to achieve. At least that's my .02.
&gt; in MySQL ver 8 Do you feel dirty at night when you go home? Does the stink ever wash off?
I agree. I don't care about certs at all, what I care about is hunger and examples. Go try to work for free. Like seriously, go pitch people to try and get them to let you do some SQL projects for them in order to get references. When I meet a candidate like that, I take a step back, and look them over, because they remind me of myself. Can't find anyone to let you work for free? Use SQL for something you like. Do something for fantasy football. Do something for a game you play online. Design a database. Design a scraping tool. ETL it. Write sprocs and do something interesting with the data, and talk to me like a human about the challenges you faced along the way, what you learned, etc. Candidates like that stand out from the pack, and education/certs just don't mean shit in comparison. Just my .02.
90% of my job, add in Tableau, Python, or SSIS on the side.
He does the unspeakable deeds so we dont have to...
https://twitter.com/sqlbob/status/456445106300530688?lang=en
Not the hero we deserve, but the one we need?
My first SQL related job came before I had ever written a single line of SQL, because I was able to quote things I had heard from other programmer types. Certs won't hurt, but entry level SQL seems to have very little barrier to entry. Learning to interview is the best use of time, at least for getting into entry level.
Having interviewed many candidates myself, I agree on the certs comment. Certs are about memorizing something at the time, specific to the test. [u/JayTee1597](https://www.reddit.com/user/JayTee1597), experience is going to matter more than certs ever would. I'm not sure what your background is in. If you've got a degree in elementary education and want to transition from a high school teacher to a SQL developer ,it is going to be near impossible to come in off the street after a cert. If you work a different job in IT, helpdesk maybe, but want to get into the SQL side of it, you're probably going to have to find a position or company that will be willing to let you take on some SQL responsibilities. It's almost impossible to go straight from one IT field to another overnight with just a cert. You'll have to be that guy, JayTee1597 who works help desk but everyone knows is pretty good at SQL and gets some of that work thrown at him. If you're straight out of college with a psychology degree and a SQL cert, again, it will be difficult. It was very common in the past for developers to come from any degree program, but it's becoming less common the way that job requirements are structured by HR. If you're straight out of college with an IT or CompSci degree, a cert might help a very small bit. But you'll have no issues gaining an entry-level job with that type of degree anyways. As far as a SQL interview goes, to stand out, you will need to do more than come in with a cert, and say that you remember what SELECT, FROM, WHERE, GROUP BY and HAVING do. (It's even quite common for recent grads to not remember the difference between WHERE and HAVING). If you can demonstrate critical thinking within a SQL environment, that will go far. As I interview candidates mostly for SQL Server, I am thinking about examples such as speaking about the design of a complex query where you tried different techniques to optimize for performance and used the execution plan as a guide. For example, demonstrating that you know how to use and when to make a choice between subquery/derived tables, CTEs, #temp tables, and @table variables would go far in an interview with me. It just takes practice. Work on a SQL problem, and then work it under a different solution.
I mean at my workplace we don't do a newline for each column because we dont use SQL queries too much, we do modeling. But when we do queries for stuff not replicated in modeling we dont want a 900 line query full of lines of select columns
Most versions of SQL are more or less the same?
In MS-SQL you could use YEAR(GETDATE()) to get the current year? &amp;#x200B; [https://stackoverflow.com/questions/28095314/sql-yeargetdate](https://stackoverflow.com/questions/28095314/sql-yeargetdate)
It depends upon the RDBMS and sometimes even the version of that RDBMS. Let me answer your question with a question - if I add the integer 1 to today's date, what am I adding? Days? Months? Years? Hours? Even if the database _lets_ me do it, how do I know it's going to give me what I want? We have functions for doing date &amp; time math for a reason.
Which RDBMS is that for? Not SQL Server or SQLite, AFAIK
The same? As MS SQL? No, no one else but Microsoft uses T-SQL. It’s way more extensive than SELECT/INSERT/UPDATE/DELETE.
Usually work on postgres/mysql as a software dev (not a dba) i dont really care, the diffs are there but usually they are quite subtle. That said i have never touched mssql so i might be talking out of my ass.
The only fun thing about my job is learning new things. I would take a $10k pay cut for an opportunity to dig into something new!
So are most jobs.
I’ve used MSSQL, IBM DB2, Oracle, and some others. There are subtle changes that you can Google, mostly relating to dates. It was frustrating to troubleshoot. After a few months you get the hang of it. If anything it was a good thing to learn different syntaxes so it forced me to write better, cleaner, and more universal code. I hate my teammates’ spaghetti code. That being said, I’d be terrified to learn Postgres or other SQLs. I’m confident I could learn, it would just hurt my head for a few months.
I don't know what RDBMS even is tbh. We're at some C# programming course and we only learned the basics of SQL. We just built small databases for practice, some had dates but we didnt go trough this + part, we mostly just used DateTime to fetch the current date when some assignment required it.
&gt;It was frustrating to troubleshoot. After a few months you get the hang of it. This comes with a cost. &gt;That being said, I’d be terrified to learn Postgres or other SQLs. I’m confident I could learn, it would just hurt my head for a few months. Like I said, a cost.
Well I assume you could somehow specify which of the 3 values to increase. I don't even understand the question and the entire book doesn't mention it. Honestly this course is terrible and none of us learned anything. Even the questions are bad and weirdly worded.
I know that only Microsoft uses T-SQL and I also know that SQL is more than just SELECT/INSERT/DELETE/UPDATE. Please read what I wrote again. I said "more or less the same", as in nobody is going to give you a significant pay raise because you know both T-SQL and PL/SQL. There are minor syntactical differences and each DBMS operates a little differently, but anyone who knows any flavor of SQL will do fine jumping in to most other versions of SQL because they're so similar. It's not like picking up an entirely new language like Java or C#.
&gt; Well I assume you could somehow specify which of the 3 values to increase. In SQL Server, that's what you use `DATEADD()` for. Basic arithmetic operators lack the ability to specify what you're adding.
This did not work.
I think we're both agreeing with one another. IMO, I don't think there is any reason at all to learn a different version of SQL than what your employer is running in production. If you already know one, picking up another is trivial. I'd hire a data analyst with any SQL experience, even if it was a different version than what we use. The only time I'd look for platform specific skills might be for something like DBA where there are bigger differences.
I understand your point and it makes sense to me. I think so too, but usually only until someone tells me of a new chunk of code that does it anyway, so I never know if this is one of those too or not.
Sure, I was being a little tongue in cheek. I don't actually have carpal tunnel, but I've seen it affect people. So no more all uppercase for me.
That's 90% of my job along with Excel, Tableau, and PowerPoint
Are you serious?
Our EntertainmentAgencyModify database is encountering performance issues because of its size. Archive all Engagements that both started and ended prior to January 1, 2018 into the Engagements_Archive table. After archiving the old Engagements, remove them from the original Engagements table to reduce the size of that table. Remember to use transactions for each of these two queries to protect your data.
We are looking for customer endorsements of the performer "Modern Dance". Provide a list of names and phone numbers for any customers in the EntertainmentAgencyModify database who have ever booked this performer. Remember that some of these engagements may now be archived. Put the list of customers in alphabetical order by last name and first name. (Hint: use a SQL command that will allow you to combine the results of two similar queries, one for Engagements and one for Engagements_Archive, into a single result set.) (8 rows)
Being able to respond to these would be a great help and if there right ill pay YOUuuuuuu
serious about what? Time is ticking.
LOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOL. &amp;#x200B; I hope you flip burgers better than you "study".
Hey, relax. We’re all friends here. You didn’t say much in your first post, and now you’ve explained your thinking more. That’s cool, and I agree that the syntax is similar, but as a hiring manager I have passed on PL/SQL candidates applying for T-SQL positions because when it came to the technical interview, they struggled hard because the database ecosystem is more than just the SQL syntax.
Yeah SQL is a case insensitive language, why waste time pressing shift Even table names should be lower case to avoid using quotes
its an elective class u faggot if i had any interest in it i wouldn't have posted here u dumb fuck. Get off reddit if ur not gonna help. O wait u prob never seen a tit before. Virgin Fuck.
If someone is a teacher wanting to get into SQL development I am sure many schools will need entry level help with systems or data or analytics and leveraging business knowledge + your SQL cert will almost certainly put you first in line for a well paying entry level position.
Curious, other than recursive CTEs, what would you say the difference is between CTEs, #temps, and @table variables?
lol
&gt;if i had any interest in it i wouldn't have posted here u dumb fuck Uh... you posted, which seems to me to imply that you do care; at least enough to post, AND you offered money. Sooo... yeah. \`SELECT COUNT(name) FROM faggots WHERE name = 'PizzaG';\` 1 row found.
good one queer. u got me. get off my post lmao.
what are your thoughts?
I was just using that as a random example of a job. But you have a really good point!
I think capitalizing keywords (or anything) is terrible style, and one of the unfortunate hold-overs from SQL's 70s-era origins. CAPITALIZING KEYWORDS EMPHASIZES THEM (LIKE THEY ARE BEING SHOUTED)! And once you understand the structure of a query, the keywords are actually the least important part of a query. Typing in all caps is a hold-over from the punch card era when lower case simply wasn't available. If you have lowercase on your input device, please feel free to use it.
lol
Might want to work on getting experience by working on a few of your own projects or with friends. When I hire for entry level, I want to know that I can work with you and you are willing to learn. That you are a good fit with the company culture and team. The rest you can learn. When I typically ask technical questions it is to gauge where you would be at. Also, some companies will be willing to invest in getting your certifications. Some of my favourite questions to ask entry level candidates: What is the difference between and relational and non-relational Database? What are the differences between INNER JOIN and OUTER JOIN? What is the difference between temp tables and table variables? That’s all, good luck!
Switched from C# with SQL Server as a backend to web with MySQL and Postgres backends. Didn't even occur to me to consider that transition when taking the job. It's a non-issue.
SQLScript which is used in SAP HANA is pretty similar to T-SQL I think? Its either that or Postgre
The way I process information is that there are more jobs using my flavor of SQL than there are **good** people to fill them, so why should I change unless there's a price tag associated? It's a massive inconvenience.
Yes, it is trivial in the sense of what I'm capable of, but it is an annoyance, and whenever taking a new job there is already a degree of risk, so why entertain an additional risk without there being a significant reward associated?
Add me to the ~90% list (not strictly writing queries, but also some db design and developing ETL tools). The other 10% of my job is making shitty guis.
Share execution plans. No one shares execution plans, yet it is the most insightful aspect of a query. If you do share an execution plan, be sure to tag me. ;)
If your goal is to get a new or better paying job, it's a non-issue. If your goal is to pad out your experience with a particular flavor of SQL in order to get better jobs as an expert, it might make sense to stick to jobs that use it. But if the difficulty of learning a new flavor of SQL is your hang up, it shouldn't be, because it isn't difficult.
Some people learn things for the sake of learning.
I learn for the sake of learning all the time... like machine learning, Python, R, etc. I don't have time to learn another flavor of SQL when I'm still learning MS SQL. Get it, or find another hire, because I can find a job... or pay me extra. It's really as simple as that.
Sometimes I capitalize the keywords, sometimes I lower case everything. It just depends on my mood at the time. I do get a little OCD about consistency in the query/proc so if I start capitalizing, all keywords, must be capitalized. My bigger pet peeve is not properly formatting the query. If you bring me something created by the query builder, I will not even try to start working on it until I've fixed the format/indents.
If I ever leave anything of testing or such, I move it to the bottom and it is all commented out. I also am highly paranoid of commenting out any updates or inserts for a select as well. Cause you have what you said people who copy and paste and run all without looking.
While I know the industry standard is to capitalize, I also do not either. I don’t need my code yelling at me, especially if it is color coded already. That being said, I totally agree with using a formatter for a standard. Have like 10 developers who all do something different, so why not format into something we all hate for a win! I am being honest and truthful, not one person wins and stops the damn my way is better.
For many purposes, #temp tables perform better than @table variables because they have statistics and are better optimized for many rows. You can place indexes if needed on #temp tables. You can break out more control flow if needed with #temp tables over CTEs.
Thank you for your comment! Do you know of any good resources (e.g., decision trees or concept books) that would help with mentally organizing the material and thinking practically/critically? I am currently taking an online course covering the basics of SQL, PL/SQL, and JAVA. I am a very visual and "why" thinker, but there is so many new terms and syntax for me to wade through! I'm the psychology student example you touched on above, btw. I worked in MRI research before becoming a sales analyst. I'm most interested in getting my hands on raw, siloed data, creating reports/dashboards, and measuring the reliability/validity of the data. Sorry for the wall of text, just wanted to give you some context. I appreciate you :)
Sql certs are worthless unless you have the defunct master one.
Ctrl+A Ctrl+Shift+U Never look back.
&gt; if I see I right join, I will rewrite everything. As opposed to....?? :D sql noob here
Try running a bunch of queries with /* or -- in your select, where clause, group by, etc. And have them on one line. It's a nightmare. Gotta separate
Ah okay thank you so much for your input it was really helpful! I don't really have anyone to go to for questions regarding this so this subreddit has been amazing to me and I just want to be able to do the best I can where I'm able to (so just asking questions) haha
I had to google execution plans haha. I'm trying to be an analyst so (hopefully this isn't wrong) so right now I just practice writing SQL queries in CodeWars and HackerRank. I bought a textbook and I know things like normalization 3NF etc but no exectuion plans so far but I will definitely tag you if I ever create one! Thank you so much!
Is there a free version available that I am unaware of?
If you are trying to use this as a time clock, you may not be legally allowed. I know in California whatever you use as a time clock must display the time. Most access control systems do not have a clock display at the door. So you should check your local laws.
Would not use ERD as po for album. What happens when a track/album has multiple collabs? Or is that not supported.
Why are you relating purchases and employees? I see no relation for the store itself. Are you modeling a store that sells records?
Use the INT data type for your key values (primary/foreign). Primary keys should be sequential, auto-incrementing values and as small as possible (as the size directly affects the index page size, i.e. it has performance impacts). Read the below articles on this (specifically the top answers): [https://stackoverflow.com/questions/18708829/binary-datatype-for-primary-keys-vs-int](https://stackoverflow.com/questions/18708829/binary-datatype-for-primary-keys-vs-int) [https://stackoverflow.com/questions/18708829/binary-datatype-for-primary-keys-vs-int](https://stackoverflow.com/questions/18708829/binary-datatype-for-primary-keys-vs-int) &amp;#x200B; Also, your Customers table has a FK of purchaseid#? What happens if a customer has more than 1 purchase? Keeping with normal forms, you could separate the stockquantity and instock columns from the ablums table and make a stock table. Stock describes the number of an album that the store owns. Its a fact about the store and the album, not just the album and as such warrants it's own table. Your missing a LabelID FK on the albums table. Your missing a mediaID FK on the albums table. Your tracks have no direct relationship to an album? Is this by design? What i'd suggest is having a relationship from artist to album and then album to track. Now, not all tracks may belong to an album so keeping the artistid FK on the tracks table is a good idea, and the album ID FK on tracks should be null-able for this reason.
My thought process was that it would be good to know which employee rang up each purchase. That way if there's an error in ringing up a purchase, you know exactly which employee was responsible.
My class is restricted to using Oracle Live SQL so I don't think we can use the INT datatype. I don't think we went over auto-incrementing values. This is an introductory database programming class and we didn't really go over real-world SQL usage and reality-based database construction because our lab didn't even have proper SQL software and we were limited to using Oracle Live SQL. "Also, your Customers table has a FK of purchaseid#? What happens if a customer has more than 1 purchase? What was the reason behind this FK, considering you have the customerid FK on the purchases table?" If a customer has more than one purchase, then another set of records is created in the purchases table for any subsequent purchases. Does that work? "Your missing a mediaID FK on the albums table." I don't get why the Albums table would include the MediaTypeID FK because the same album can exist simultaneously on CD, cassette, vinyl, etc. That's why I made MediaTypes a separate table. Not every album comes in a single media format, was my thinking. "Your tracks have no direct relationship to an album? " UPC is in the Tracks table, and UPC is the PK of the Albums table. I kept Tracks and Albums separate because sometimes the same track can appear on different albums. Y'know, like a song will be on an artist's regular album, and also be on the "Best Of" album. But I really don't know what I'm doing at all so any advice at all is appreciated. Thank you.
Regarding the purchase table, see me 3rd EDIT. You'll need to implement that regardless. Regarding the PurchaseID# on the Customers table, drop this column, its unnecessary and serves no purpose given what i've said in my 3rd EDIT. Regarding MediaTypes, what if a customer wants to know if you have a certain album on a certain mediatype? If you need to support multiple mediatypes for an album, make a relational table such as AlbumMediaType. The PK would be a composite key (a key made of two or more columns) of the FKs UPC &amp; MediaTypeID#. It then links to both Albums and MediaType on the FKs mentioned. Yes correct, i missed the UPC FK on Albums as mentioned in my 1st EDIT.
If the report is hard for your eyes to see the information yet contain all the useful data then it is a poorly formatted report. In this case, to me, this is a poorly formatted report.
Based on your ERD how would a customer purchase a single track? Purchases is related to Albums which is in turn related to Tracks. &amp;#x200B; Will this database be dealing with online transactions only or will people be purchasing in store? I see that every purchase requires a CustomerId, will this always be available?
You should change how the album/track/artist relationship is configured. Step 1. Keep the artist table Step 2. Remove the artistID column from albums and tracks Step 3. Create two new tables: AlbumArtistXref and TrackArtistXref. These two tables will have 3 columns: albumID/TrackID, an ArtistID, and a flag denoting if this is the primary artist (as opposed to a feature) This will allow you to handle albums or tracks with multiple artists
This can be solved with an intermediate table
A left join
This is an old-school record store and there are no online transactions. Pretend it's 1992.
Ok that's cool, so is it possible to sell records without knowing any details about the customer? If so then you need to remove the foreign key constraint, or an alternative is to have an 'Unknown' customer in your customer table.
It depends on the data type, if the type is datetime then it will work, if the date type is date then it won't work. This one works: select cast('2019-05-01' as datetime)+1 This one doesn't: select cast('2019-05-01' as date)+1
No. DATEADD can add milliseconds, seconds, days, weeks, months, years etc. Also '+' will only work on a data type of datetime (not date) and can only add whole days. See my reply with example.
Even if there are advantages to using surrogate keys, it's not really true for primary keys. Foreign keys that have an index on them will consume less space, but come on, we're talking `CHAR(10)` here. There is hardly any performance difference between INT and BIGINT, why should there be from 2 or 4 extra bytes? Honestly I find the enforcing of surrogate keys a very bad practice. If there is a good natural key, why add more data, make it more complex, introduce a ton of problems on the way, make it harder and slower to query based on the natural key? I get that it's already an "id" so probably not really natural, but if it's something that's reflected in another system, it's the best thing if it stays, unless it's too limited for the new use case. Ensuring sequential keys is very difficult. People today turn towards UUID more. Oh, look, even more bytes. Why not just keep CHAR(12) that is at least meaningful?
What Trek said. Any right join can be restructured to a left join. It’s a pain. Sometimes you nearly have to start over but the reward is worth it. With left (and inner) joins, you start with a “main” table and just keep working yourself to the finish line. When you add a right join to a query... it’s like saying “oops... these are actually the things I want and should be the main table.” You’ll get it when you try it. You’ll realized right joins should have never been invented. Good luck!
wut?
We're using Microsoft SQL. But we also use DateTime, not date. I guess the answer is yes after all then. Thanks!
I’d advice every table to have some technical fields: Day of SQL action Time of sql action Type of action Program/script/user which did the action.
May I ask if you used any tool for the ERD, and if yes, what is it called?
Did you install a database server on your desktop so that you have something to connect to?
I installed Oracle 11g,I searched on internet and they tell me to change some files in D:\app\PC\product\11.2.0\dbhome_1\NETWORK\ADMIN...I only got sqlnet.ora and the "SAMPLE" folder. I don't know if this is important but,I installed oracle on a secondary disk,while sqldev is on C:
You may! I use LucidChart.com. So user-friendly that even someone who doesn't know what they're doing can kinda fake it.
You're at about a 7 and we need you at about a 2
Unfortunatly no.
You're album is linked to only one genre, which I think the stroe would find limiting. I think you'd need albumGenres as a link table.
So, with your current ERD, you have the ability to only purchase one record at a time. I'd make an intersection table with association relationship. So from Purchase, I'd remove pricing data, and UPC. &amp;#x200B; Then for a new table, I'd have the PurchaseID as a FK, UPC as a FK, and pricing/quantity data. You can call it Line\_Item or something like that.
&gt; the first four are the days since 1/1/1990 That can't be true: https://www.timeanddate.com/date/dateadded.html?d1=1&amp;m1=1&amp;y1=1990&amp;type=add&amp;ay=&amp;am=&amp;aw=&amp;ad=6228&amp;rec= 1/1/1990 + 6228 days = 20 January 2007 You can ballpark it easily 365 days/yr ~360 days/yr = ~3600 days per 10yrs (round up to 4,000 or down to 3,000). 6228 is ~6000, 6000/3000 is ~20yrs. 6000/4000 is ~15yrs So you know it's somewhere between 15 and 20yrs. 1990+15yrs is 2005, 1990+20yrs is 2010. You need to find out the format of that stored number before you go anywhere.
You're really not giving enough information to be able to help you here. Random searches on the internet aren't going to get you to an answer - you need a start-to-finish tutorial for how to install, configure &amp; connect to Oracle for the first time. Like this one: https://www.databasestar.com/how-to-install-and-set-up-oracle-database/ or this one: https://www.c-sharpcorner.com/article/step-by-step-installation-of-oracle-11g-on-windows-7-64bit2/ . But I can't say for sure if those are enough because I haven't touched Oracle in over 20 years, and never had to install it myself. Is Oracle running? Do you have a user account set up? Have you given sqldev the correct information (credentials, connection string, etc.) to connect to it?
Is disagree with folks saying the certs are worthless. They may not get you a job outright, but they may put you a step above another candidate with a similar skill level. As a data professional who has done a fair amount of hiring, it can show that you are serious about learning and staying current. I didn't NEED to take the 70-761, but I did anyway because it's a small investment and can fill some knowledge gaps you might not be aware of if you're self taught. That being said, the cert alone probably wouldn't get you very far.
Been looking for something like that! Cheers!
😂🖕🏽
IMHO yes, this is a rather poorly formatted report. * The section headings are smaller than the data * Text is too close to the lines, making it harder to read (especially your headings) * There's no explanation of what the percentages are * What's the significance of the colours? And if someone is colorblind or color-deficient, will they lose that meaning because they can't see the colors? * What is "Total Negative Gross Profit" and if that number is negative, does that mean that you actually had profit (think about double negatives in written language - "I can't not do something" means "I must do something") * Grand totals usually go at the end of a report, not the top * If I were to attempt to export this to Excel to perform calculations or produce charts...I probably couldn't
I separate the where clause for each line. I do new lines for select and order by if the query isnt too big, but like I said if I'm pushing something to prod nobody in our group wants a 800 line sql query thats hard to maintain. You can do readability without having new lines for each column
&gt;I've toyed around with the idea of building something like this for myself to generate boilerplate that I can modify as needed, but I don't know that it would save me enough work over just copy/paste/edit from our canned queries to justify all the work of building it. When you say it can be done, How can this be done?
Out of curiosity, why did you opt for connecting genres to the album and not the artist? My guess would be to not create a many-to-many, and probably ease, but I was curious.
UPC while unique for each record can lead with a “0” and is 12 digits in length, that can be pretty heavy when querying. I would just use an auto number for that kind of entry to keep foreign keys similar for speed and reduce the chance of error.
Unless prices are only assigned at checkout shouldn't UnitPrice be in the Album table? If things are frequently marked down, you could add a discount/markdown field in the Purchases table. &amp;#x200B; Also, your design doesn't allow for non-album purchases. Wouldn't an old-school record store sell keychains, guitar picks, posters, CD racks etc.? &amp;#x200B; If so, it would be better to have a somewhat generic Products or InventoryItems table w/ UPC, InStock, StockQuantity fields plus ProductType and UnitPrice fields. Have a separate Albums table for your album-specific properties.
I see myself in the future writing SQL for about 90% of my job
So you know the first digits are DL and the rest is numeric, so that's covered by the where clause After that I'd cast the numeric part as numeric ( not int) and operate with it Also, maybe it would be a good design idea to have 2 columns, one for the vendor and another for order number. That way you'd have an index for each and you can query the max value instead. IDK how much effort would that require for you
I've added the fix to the OP. The problem with my original code where two things. Couldn't increment to the next ordinal position (DL00000009 to DL00000010). Also when I would cast the right 8 as INT 00000009 would just provide 9. I think issue two caused issue 1 somehow.
It's been 15 years for me but I vaguely remember that Oracle does not create a DB on install (the Oracle server process would be running tho). I recall using a command-line utility to create the DB.
Why not just use an update? Great... Still my answer applies Also maybe the datatype should be char and not varchar
I would probably use Access, and we'll ignore the details of the UI but basically, you pick what fields you want to select from a dual list box select, the exact wording that goes in the SELECT statement is stored in a table so you pull out that wording and add it. The FROM statement looks at what fields you used, so it knows what tables are needed and how to join them. Again, the basics are stored in a table. I'd probably have some back end logic to keep the joins from being hardcoded an stupid (like our BI system uses, ugh) The WHERE clause I'd probably just write out manually. I've actually made a UI that could build one (long ago) and it was a giant pain in the ass to get working correctly, and ultimately too limited in capability. Basic aggregations I'd have to think about. Maybe just treat them like any other field we could select, and the GROUP BY clause would get added. But I'd probably ignore those as well and just do them by hand. The tool is really just to make it easier to get some basic "give me these fields" type code, all nicely formatted, that I could build on.
So...Miami, right? ISA245?
I capitalize all, all the time. Makes it look so much cleaner. Although I think proper formatting makes a bigger difference. Started a new job where all the code is all lowercase with no logical consistency in formatting. Bunch of heathens!!
&gt; DBA position to be my "fall back option" if the whole Vice President/Chief Technology Officer I am legit curious what your current position is. You seem to be relatively up there in the corporate ladder if being a DBA is a downgrade.
I'm an analytics architect / "wannabe" statistical modeler and I report directly to a VP. At this point I would consider a DBA position to be a lateral move with a slight pay bump, but having more job security / available jobs. I do full end to end development of ETL solutions to get data in a database, run calculations/models on the data, and then prepare it for &amp; develop the front end interface. I'm kind of a mini-DBA over our database, making sure our sprocs run successfully, fixing them when they break, or rewritting them completely when they run too long. The actual DBA's maintain servers, backups, etc., and I have nothing to do with that at all, nor any inclination. On my non-production server I have more or less free reign and most DBA privileges to do just about anything you can think of except take the database off line, restart, or drop it. I'm also an admin in the environments which sit above the database.
This person has had the same homework/coursework: https://www.reddit.com/r/SQL/comments/bit94x/how_do_i_conduct_statistical_normalization_using/
Yeah i reached out bc I'm fairly certain they're in my class (/r/TwoRedditorsOneCup material) but, no answer yet.
Wondering if this wants a type 2 slowly changing dimension for album prices. When you release an album, you ask full price. After some time, that same album gets a different price. At some late phase in the lifecycle, the record ends up in the discount bin. Presently I'm not seeing the place you're getting the price from.
I'll give it a try This is probably poorly formatted because I wrote this on my smartphone Also I can't stay awake for any feedback since it's 3:30 am where I am sorry SELECT max(c. CourseNo) As CourseNo, max(c. Coursetitle) AS coursetitle,count(*) FROM course c Where exists (SELECT courseNo FROM students s where s. CourseNo=c.courseno) GROUP BY c. Coursetitle, c.courseNo Reason why it count all 12 students In your query is because in your subwuery you count(*) all rows where c. Courseno=s. CourseNo - which is your resultset The exist part was actually the difficult part After this you had all existing courseno which you have to group and then count the rows Please correct me if I'm totally off here PS could you please write the tables and there attributes next time would be easier that way I think
I did that and this time it returned 1 for both courses. So the query counted the number of courses of each course from what I can see, instead of counting the number of students for each course separately. Thanks anyway
I did that and this time it returned 1 for both courses. So the query counted the number of courses of each course from what I can see, instead of counting the number of students for each course separately. Thanks anyway
Quick fix Instead of count(*) can you try insert: (select count(*) from students s1 where s1. Courseno=c. CourseNo) You can actually leave out group by It returns 1 because the table course only holds distinct courses and we need the count over table student, my bad
You have no idea how much this helps, I have been working on this single query for over 17 hours now. It works perfectly, thank you so much I really appreciate it!
Glad it helped I edited my first answer to try give more detail why your query returned 12 on all rows
Wait, do people really post stuff like this to get someone else to do it for their qualifications?
I don't want anyone to do it for me, I was just wanting to know how to incorporate the subqueries as an argument of the function. Just a fundamentals question. My professor didn't give a lecture on this stuff, our instructions just said "do research and teach yourself" basically.
This is the approach I would take too.
I have a SQL fomatting add-on that I have configured to add the square brackets, because I thought it was best practice and why not. It makes it easy to spin out some poorly formatted code really quick, or open up a script of a view that a co-worker designed in the "visual designer" that results in atrocious SQL, press a hot-key, and easily get some standard-formatted code.
I run into a surprising amount of fields already named as key words ("field", "key", "value", "first", "last", "description") and using the SQL formatting add-on I mention in the comment above helps expedite writing the code that references those fields, at least.
Haha yeah, I have it mostly done now.
I think I have the first part of Q1 done, but i have no clue how to query it. Like where the SELECT MAX(Age) FROM X; goes in the argument of the function.
Not really sure what you need or how the “statistical normalisation of value” is really relevant but you can call pl/sql functions in SQL like you’d use expressions: Select my_function(max(age)) from x Maybe that will get you started
I am most familiar with PostgreSQL, so I will describe a schema using Postgres features, namely its `range`-data-type. &gt; Initial setup would indicate a single table... Absolutely not. This design requires several relations ("tables"), if you are to avoid update-anomalies, etc. &gt; The employee would enter most of the information and lieu-time would be populated based on a calculation. I smiled when I saw you mention calculation; I disagree with storing ("populating") the result of that calculation. Let your queries determine the value of lieu-time, otherwise you are effectively "caching" that value, and you must remember to update it when the values that determined the computed-value change (an example of an update-anomaly). I will describe my understanding of the domain you are trying to model: A company has employees. You want to record certain durations ("time") for each employee. Each duration has an associated "context", e.g. working, sick, etc. I do-not comprehend what you mean by "lieu"-time, except that it is calculated based on the notion of a A rough, first-iteration sketch can look like this: ``` Relation-context {context::text} Relation-holiday {duration::tstzrange} Relation-employee {employee_id::integer, employee_name::text} Relation-duration {employee_id::integer, context::text, duration::tstzrange} ``` If you want your database to be accurate, you will want to avoid overlapping-durations. Example, it should-not be possible that an employee both worked hour-x AND was sick hour-X. Postgres range-types allow for the constraint (see the docs). Relations `context` and `holiday` setup what are (sometimes) called "domains". A domain can be thought of as a "pool of conceptual values". They can (ought) be used to explicitly declare the range of valid-values for a specific type. Doing this allows you to constrain the values of foreign-keys. An example would be that relation-`duration` can only ever have exactly 1 of the values located in relation-`context`. With regard to calculating lieu-time, a simple query involving Postgres's range-overlapping-operator, `&amp;&amp;`, will tell you whether an employee has a tuple ("row") whose context equals `working` and overlaps an explicitly-declared holiday. Hope this helps. If you like my style-of-presentation and want to know more, I'm more than happy to help you actually implement this.
I'm getting a ORA-01476 divisor is equal to zero error now
SQL allows using "scalar subqueries" (a query returning 1 column and no more than 1 row) in place of literals of the same datatype. E.g. select sin( (select 1 from dual)) from dual
I agree you should use natural keys when possible. However, in this case I don't see any good options for natural keys that are guaranteed to be unique 100% of the time. Hence, surrogate keys should be used. Best practice for surrogate keys is to use the INT datatype (or a variation such as SMALLINT or BIGINT depending on a reasonable estimate of how many rows you expect to have in your tables). OP didn't make mention of interaction with other systems. Another thing to consider is that if you're using VARCHAR datatype for your primary key (which suggests its a natural key), the natural key can potentially change over time, meaning that your primary key value for a row is subject to changing. This then means that index rebuilds are going to be required to keep your indexes efficient. When using surrogate PKs they should never change over time. Hence, an auto generated INT value in place of a natural key you avoid such issues, as the natural key value (i.e. artist name as an example based on OPs application) can be changed without having an impact on the index. Ensuring sequential keys is very easy, you could use the AUTO\_INCREMENT property in conjunction with making the field not null-able and using the UNIQUE property. You could use the PRIMARY KEY + IDENTITY Properties. IDENTITY allows you to set a starting value (SEED) and a increment value (i.e. IDENTITY(1, 1) ). Here your PK value is automatically inserted for each new row and you do not explicitly insert the key value, hence it will also be sequential so long as there is no manual interface (which can be enforced via a trigger). Easy to implement.
The code you shared has no division in it. Presumably the exception is being raised from within the function? Luckily the error stack will tell you exactly where (so long as it’s not being swallowed). Figure out where the division is occurring then figure out why the divisor is 0. Likely that you are feeding 0 into it as an input argument (possibly its being defaulted). I can’t see your code, it’s hard to say where your issue is without just guessing.
Holy crap. This is a wealth of information! Thank you! I'm just doing this initial reply to say thanks, but I'll have to digest most of this tonight after the kids go to sleep. Regarding the calulcation: What you've said makes total sense! I'm a little irritated it didn't occur to me. As an aside, the basic work I've done has been with MS-SQL and MySQL. But a quick skim of your post leads me to believe I should be ok with what you've written once I get a chance to digest it. Will post again...
&gt;Best practice for surrogate keys is to use the INT datatype Not really. If you're using surrogate keys today, INT might be the most used one, but if you want good software, UUID is the way to go. Too much hassle to ensure uniqueness. &gt;OP didn't make mention of interaction with other systems. There's probably a reason behind using CHAR(12), CHAR(10) (different data types) for different tables. &gt;the natural key can potentially change over time, meaning that your primary key value for a row is subject to changing That is not something specific to VARCHAR. Every other data type can be subject to UPDATE. &gt;This then means that index rebuilds are going to be required to keep your indexes efficient. Just like with surrogate keys. There really isn't any difference. As for indexes being rebuilt - that is true if you also add values, of any type. &gt;When using surrogate PKs they should never change over time Same as the "natural" keys in his example. Same as a lot of stuff. A transaction won't change over time, and neither will many of the other large tables. Surrogate keys might be beneficial for dimension tables in an OLAP scenario, so that people could read the data in an easier fashion, but for operational databases there is no reason to change names of stuff randomly. &gt;Ensuring sequential keys is very easy Only when you're only generating these surrogate key on one server. Imagine a product database, that is ingesting product data from 4 different supplier, or from a distributed computing medium that processes the data exactly the same way, and must generate IDs for different subsets of products. Very real problem. In my company there are several IT departments and each one of them has at least one of such cases.
Update sc Set target = case when colA &gt;= 5 and colA &lt;= 10 then 1 else 0 end From Source Where target is null
Update table Set target = case when x then 1 when b then 2 else null end
Dude my feedback is based on the ERD OP provided for a small record store that wasn't mentioned to have any interaction with other systems. You're making huge assumption about this system/db that have no basis behind them and nit picking at my feedback, which is accurate based on the information given. Your advise is correct in different circumstances, for a different design/spec. Remember, a good DB design follows the spec given. In this case the spec was a small indie music store making sales to customers. Not much to go on, but it what you need to base your suggestions on.
I believe that your understanding of what I'm trying to do is correct. I also understand what you're saying about the overlapping durations. It came up when I did my initial planning, but I didn't realize that there was a mechanism to deal with this. I'll read up on the Postgres range-types and see what the comparable thing is in MSSQL. &gt;They can (ought) be used to explicitly declare the range of valid-values for a specific type. Doing this allows you to constrain the values of foreign-keys. That makes sense. Looking at what you've written, I'm substituting the word 'table' for 'relation' and you've set up 4 tables. The holiday table would define the holidays for the year(s). The employee table would define the employee+the unique employee_id. I don't fully understand the context table or the duration table though. Does the context table simply define possible contexts for the dates? (eg: work_day, holiday, vacation_day, sick_day, etc.) I'm going to go through [this](https://code.tutsplus.com/articles/sql-for-beginners-part-3-database-relationships--net-8561) later and I have a feeling that will clarify a few things for me. Also (now that I'm thinking about it a bit more) I'm thinking that in order to track the time on a weekly basis, I'd need to create a table for each week of the year. That table would hold the data for that week for all employees. Must read more about the tsrange....
i’m more of an SSIS guy but my team uses Informatica. I’m pretty sure it’s possible as we’ve worked a project similar to this. but why don’t you load the XML file into a staging table and then perform the lookup
This was my first thought as well. One purchase should have one or many line items.
CASE works. You could also use OR. SET target = 1 WHERE (cola &gt;= 5 AND cola &lt;= 10 OR cola IS NULL)
Good explanation! Is there a reason you've used the word "relation" here instead of "table"? As a beginner it may be confusing to understand that "relation-employee" means "employee" table.
I think you can do this in PowerCenter. I haven’t used it before, but there is an xml generator transform that can read from other transforms in the pipeline so it should be possible.
&gt; for a small record store At that point I was talking about "best practices". &gt; and nit picking at my feedback I just have an issue with pushing surrogate keys when there are already pretty fine keys in place. Adding a surrogate key in that place would probably mean that you have a new column, and ... why? I haven't seen a good reason yet, in this particular ERD. On the contrary, I see that asking for integer surrogate keys in this case is nit-picking. If you don't have a good reason to do something, other than "because other people do it", then don't do it. &lt;- That's my belief.
- If you must use `CHAR(n)` for your keys, make sure they're of the same length for FK/PK pairs! For example `Artists.ArtistID#` has `CHAR(9)` and the FK `Tracks.ArtistID#` has `CHAR(15)`. This would be less of an issue with `VARCHAR2` or `NUMBER` types, although still a smell. - Normalise further: - `Artists.GroupName` should be a foreign key. And I would expect there to be a many-to-many relationship between `Artists` and `Groups` - `Artists` &lt;-&gt; `MusicGenres` is also a many-to-many relationship. However, I would normalise this and create the link only transitively, via `Tracks` (possibly `Tracks` &lt;-&gt; `MusicGenres` is also a many-to-many relationship) - Once you have a link between `Tracks` and `MusicGenres`, the direct link from `Albums` &lt;-&gt; `MusicGenres` is obsolete. - An `Albums` can have several artists. The same is true for a `Tracks`. The `Albums` &lt;-&gt; `Artists` relationship is unnecessary, because it is redundant with `Tracks` &lt;-&gt; `Artists` - The `Albums.TracksQuantity` column is not really necessary. You can count the tracks per album if needed. For performance reasons, people sometimes pre-aggregate such data (ideally in materialised views), but from a design perspective, you shouldn't have this column. - The same can be said about `Albums.InStock` and `Albums.StockQuantity`. You want to normalise this into a new `Inventory` table. The data does not really belong in `Albums` - Drop `Customers.PurchaseID#`, it has no meaning (or is that the last purchase? But even then, that's not necessary, because you can find it easily) - I suspect you would want to factor out a new `Address` table because not only will you need an address for `Employees`, but also for `Customers`, eventually. - You probably want a `Salary` table and add temporal validity to it, as an `Employee` hourly rate is a value depending on a time period. That might be overkill for your assignment, but is still a good idea. - Your `Purchases` table should really be split into a `Purchase` and `PurchaseItem` table. The `PurchaseItem` table would contain the `UnitPrice`, `Quantity`, `Tax`, `Discount`, `UPC` columns. The `Purchase` table would contain the `PurchaseDateTime`, `PurchaseTotal` (could be dropped), `PurcahseType`, `CustomerID#`, and `EmpID#` columns. I'm currently adding a day to my [SQL training](https://www.jooq.org/training) about database design. This is a really good example that I will take inspiration from :), so thank you for sharing this!
You’ve repeated your first two columns. I think. So you’re calling the same thing twice.
I want to show both teams (the away and home team). Each team has a team\_api\_id on Team table and team\_long\_name on Team table. I can't get how to show both away and home team names without using the column twice.
Select away.team, home.team from match Join team (away) On away_id = team_id Join team (home) on home_id = team_id I’d type the whole thing out but I’m on mobile. This should get you the general idea. The problem with your query is you’re selecting the team name but using the team table only once so it’s always going to be the same team displayed per line.
Yeah I get what you’re going for, but you’re still calling he same thing twice as you’ve only got one join. You can join Teams on itself and then set up a where clause to dictate which team is called from that column. I’m only an Amateur at sql tho, I’d need to play with the data to get a concrete answer
I'm doing something wrong apparently. SELECT DISTINCT Match.away_team_api_id, Match.home_team_api_id, Match.away_team_goal, Match.home_team_goal FROM Match JOIN Team ON Match.away_team_api_id = Team.team_api_id JOIN Team ON Match.home_team_api_id = Team.team_api_id I get : Result: ambiguous column name: Team.team\_api\_id After removing the second Join, I get the Ids of away and Home teams, but how will I show correspond Match.away\_team\_api\_id and Match.home\_team\_api\_id keys with Team.team\_long\_name? I should go like this Match.away\_team\_api\_id -&gt; Team.team\_api\_id -&gt; Team.team\_long\_name
When joining multiply to the same table, it's critical to use table name aliases so you can access each query-instance of the table separately JOIN Team t1 ON Match.away_team_api_id = t1.team_api_id JOIN Team t2 ON Match.home_team_api_id = t2.team_api_id
Didn't know that, thanks for the tip.
Yeah the (away) and (home) were aliases. So you put something like - JOIN Team Home on The Home portion is an alias and you can put Home.column In your select
This guy explained it more eloquently and with a working example.
Didn't oull the data to confirm but I think this should work &amp;#x200B; SELECT DISTINCT a.team\_long\_name, a2.team\_long\_name, Match.home\_team\_goal, Match.away\_team\_goal FROM Team a JOIN Match ON Match.home\_team\_api\_id = Team.team\_api\_id join Team a2 on Match.away\_team\_api\_id = a2.team\_api\_id
There's a typo at line 3, I guess you mean a.team\_api\_id
What is the data type of the column? Try, YEAR(oispv\_staff.a\_attritdat) = YEAR(GETDATE())
No that's what I meant. You have to use pull the team table twice, once for the Home team and then the second time for the Away team.
Oh, I get it. But when I run it I get Result: no such column: Team.team\_api\_id. If replaced with the alias a.team\_api\_id. It works : KRC Genk Beerschot AC 1 1 SV Zulte-Waregem Sporting Lokeren 0 0 KSV Cercle Brugge RSC Anderlecht 0 3
I miscounted the lines whoops lol
ORDER BY name DESC LIMIT 6 OFFSET 9
Is there something special about result 10-15, or after you just looking for a sample?
If you're looking for a full example with working code, that's much more than one should expect from a Reddit question. What have you tried and where did you get stuck? Do you have a way to write to an XLSX file from Java? Do you know how to process multiple resultsets from a single stored procedure execution in Java?
Don't offer to work for free... your time is valuable, and so is this work. Stick to the second recommendation - that's great advice. Find something you care about and build a side project using SQL. Make sure you understand what you're doing well enough that you can explain your project and your choices. And have fun. Also a good opportunity to make sure you like this stuff well enough to be doing it for the next few years. Good luck!
About 5 years or so ago I started working for free to get experience. Making a great salary today and have a senior position in large company. Hell, I'm trying to start my own analytics consulting company and do you know how I'm getting my first clients so that I can fill out a website and showcase that we're a legit group? Offering to work for free / for next to nothing in exchange for promotion &amp; references. Get 5 or 6 logos up on a new website saying they're existing/former clients with testimonials... and then start charging several hundred dollars an hour. See how marketing works? In business you can often sell a product at a loss and end up making more money in the end. I mean if you can get paid, get paid, but if you can't get paid, and you can't get a job without experience... then get that experience, and then get paid. Time is only as valuable as the market allows, and if you have no experience then your time is worthless, and you have all the time in the world. If the market will pay you $200/hr for your time, and you have people lining up to pay you, then you have very little time. It's called free market capitalism. Not a big fan, but you play the cards you're dealt.
This is the solution I am leaning towards, thanks!
Good to know!
I'm currently looking at Apachi POI
A company losing profit to get name recognition is leagues different than somebody who needs to pay bills and put food on the table. There's no reason to devalue yourself by working for free - especially in a market that is under-staffed. Not saying in theory that you're wrong that it would help... it's just entirely unnecessary. Why spend a couple months working for free when you can spend a good long weekend developing a quick project and pitching that in interviews. Especially when it's only to put yourself a little above other applicants. There's value in this work. There's value in OP's time. He shouldn't have to prostrate himself in front of capitalism to get paid - especially for an entry level job.
I was working at a golf course for minimum wage when I was pimping myself out to get some free experience. Went from that to a $60k/year starting job in about a year, maybe two? Less than two? Hard to remember. Not saying don't get paid if you can, but if you can't get paid, then get the experience first, and then get paid.
ask the folks in /r/java ? why would the /r/sql folks know java?
personally, I would argue that SQL do everything itself... no need for java... install the Access Database Engine, use SSIS to run the sproc and pop each resultset into the different tabs... or you could use SQL Agent to push it to Excel using ACE via OpenRowSet/similar. then again, i'm also not planning to write out a complete tutorial for building the SSIS package either.
Some maven package for creating spreadsheets probably
With your own logic, you missed out on more than $60,000 because you devalued yourself, and your employer took advantage of it. Entry level positions should have entry level expectations, and that starting pay is the average salary for entry level SQL developers. Why give your employer $60,000?? That's insane. And not even realistically how the world works. He's looking for entry level work - he should research the average salaries in his area. He should have a range he's willing to accept. Entry level isn't generally looking for loads of experience, they're looking for stand-out applicants. Do something to stand out. You can work for free for a year and show you interviewer you're willing to be taken advantage of, or get your hands dirty with some good old-fashioned self-motivated go-get-em experience. One is a year of tons of overtime for no pay (because you'll need a second, paying job) and one is a week or 2 of something that you can even enjoy and use for fun after you're hired.
I didn't have an employer. These were freelance projects where I went out and pitched people a service they weren't looking for. What's that? You run the corner liqour store where I buy a six pack every week? Ever thought about a database to track inventory? I got you bro. I'll do it for free. Hell I'll pay you to let me do it if I get some cool experience. What's that? You'll give me a couple of cases of beer and a reference? Awesome. I didn't lose out on anything. I created a market for myself.
I got this for processing multiple resultsets from a stored procedure execution in Java. I'm looking into Apache POI
here's an example of the query i used to run for my NFL Pool to determine each player's results select U.userpk , U.username , U.firstname , U.lastname , N.pool , G.gamedate , G.gameno , case when G.vscore + G.hscore = 0 AND G.ot = 0 then cast(null as char(3)) when G.vscore + G.spread &gt; G.hscore then G.vteam else G.hteam end as covering_team , P.pick , W.winners FROM nfl_games as G inner join nfl_picks as P on P.gameno = G.gameno inner JOIN members_nfl as N ON N.userpk = P.userpk inner JOIN members as U ON U.userpk = P.userpk AND U.active=1 inner join ( select userpk , sum(case when nfl_games.vscore + nfl_games.hscore = 0 AND nfl_games.ot = 0 then 0 when nfl_picks.pick = case when nfl_games.vscore + nfl_games.spread &gt; nfl_games.hscore then nfl_games.vteam else nfl_games.hteam end then 1 else 0 end) as winners from nfl_games inner join nfl_picks on nfl_picks.gameno = nfl_games.gameno where nfl_games.weekno = #request.whichweek# group by userpk ) as W on W.userpk = P.userpk WHERE G.weekno = #request.whichweek# AND ( SELECT cutoff FROM nfl_cutoffs WHERE weekno = G.weekno ) = 1 ORDER BY N.pool , CASE WHEN U.userpk = #request.userpk# THEN 0 ELSE 1 END , W.winners desc , U.lastname , U.firstname , G.gamedate , G.gameno
your top level query contains no table called `pricings.charges` instead, it contains a table called `temp`
I have changes `pricings.charges.locl_ntwrk_id` to `locales.networks.locl_ntwrk_id`, but it still doesn't work. If thats what you meant.
nope, that's not it here's your query -- SELECT locales.countries.columns , locales.networks.columns , pricings.charges.colulmns FROM locales.countries LEFT JOIN locales.networks ON locales.networks.locl_ctry_id = locales.countries.locl_ctry_id LEFT OUTER JOIN ( something ) AS temp ON temp.locl_ntwrk_id = locales.networks.locl_ntwrk_id your main query's FROM clause does ~not~ contain a `pricings.charges` table
&gt;LEFT JOIN locales.networks ON locales.networks.locl\_ctry\_id = locales.countries.locl\_ctry\_id &amp;#x200B; If I do a simple `LEFT JOIN` for `pricings.charges`, it will simply list **ALL** prices per `locl_ntwrk_id`. I need **ONE** price which is `pricings.charges.t_valid &lt;= CURRENT_TIMESTAMP(6)`.
[This is on the longer side, but fairly typical in terms of complexity.](https://pastebin.com/UiLS7VQM)
you are correct to use a subquery to do this what you should do is another join, from `temp` to `pricings.charges`, matching `temp.maxdate` to `pricings.charges.t_valid1 as well as the network id then your references to `pricings.charges` in the SELECT clause will work
which section? you forgot to post it
Can you go into more details about the project? What you need help with?
SQL file 1 (order of drop and create statements is IMPORTANT!) Use statement for your group's database Drop statements for all tables Create statements for all tables SQL File 2 Use statement for your group's database Statements that delete all data in the tables in the correct order Statements that insert all data(NOT using the LOAD DATA SQL statement) 5 TV Show Viewings per Team member Include all relationship data SQL File 3 (Comment the file with the statements below) Generate SQL to "Add a TV Show Category". Provide example data. Generate SQL to "Delete a TV Show Category". Provide example data. Generate SQL to "Add A TV Show". Provide example data. Generate SQL to "Delete A TV Show". Provide example data. Generate SQL to "Produce searchable TV Show directory (search all characteristics). Display all characteristic per TV Show in output. Sort ascending by TV Show Name.". Provide example data. Generate SQL to "Add a TV Show Viewing". Provide example data. Generate SQL to "Delete a TV Show Viewing". Provide example data. Generate SQL to "Produce viewing statement between two times of day (if the viewing starts between these times). Show all viewing characteristics in output. Sort by date and time. Calculate the total length of time for these viewings as well as the percentage that this length is of all viewings Generate SQL to "Produce a TV Category report. Show: Category, Number of shows per Category, Total length of time per Category, Percentage of this time per Category are of all-time viewed Sort by Percentage Length of Viewings in each Category. I already have the first two files done,
Here's one of many possible methods to handle this. Posted this in chunks as its too long. This is MSSQL. I'm using temp tables for this. You can use physical tables for your process. I've also created a "Holiday" calendar table assuming the start and end dates of the week as previous Sunday and Saturday respectively. Credits for calendar SQL: [http://zarez.net/?p=2543](http://zarez.net/?p=2543) [https://stackoverflow.com/questions/5635594/how-to-create-a-calendar-table-for-100-years-in-sql](https://stackoverflow.com/questions/5635594/how-to-create-a-calendar-table-for-100-years-in-sql) [https://www.sqlservercentral.com/forums/topic/federal-holiday-function](https://www.sqlservercentral.com/forums/topic/federal-holiday-function) /\* SET NOCOUNT ON; /\*DROPS THE TEMP TABLES\*/ IF OBJECT\_ID('tempdb..#TEMP\_EMPLOYEES') IS NOT NULL DROP TABLE #TEMP\_EMPLOYEES; IF OBJECT\_ID('tempdb..#EMPLOYEE\_HOURS\_SRC') IS NOT NULL DROP TABLE #EMPLOYEE\_HOURS\_SRC; IF OBJECT\_ID('tempdb..#EMPLOYEE\_HOURS\_TGT') IS NOT NULL DROP TABLE #EMPLOYEE\_HOURS\_TGT; IF OBJECT\_ID('tempdb..#TEMP\_EMP\_HOURS') IS NOT NULL DROP TABLE #TEMP\_EMP\_HOURS; IF OBJECT\_ID('tempdb..#CALENDAR') IS NOT NULL DROP TABLE #CALENDAR; &amp;#x200B; /\*CREATE THE EMP TABLE\*/ CREATE TABLE #TEMP\_EMPLOYEES --THIS HOLDS YOUR EMPLOYEE DATA (UNIQUE\_KEY INT IDENTITY(1, 1), EMPLOYEE\_NUMBER INT, EMP\_FIRST\_NAME VARCHAR(64), EMP\_LAST\_NAME VARCHAR(64), EMP\_EMAIL VARCHAR(64), HIRE\_DATE DATE, CURRENT\_FLAG CHAR(1) DEFAULT 'Y', LOAD\_ISRT\_DT DATETIME DEFAULT GETDATE(), LOAD\_UPDT\_DT DATETIME); &amp;#x200B; /\*INSERT TEST VALUES INTO #EMPLOYEES\*/ INSERT INTO #TEMP\_EMPLOYEES (EMPLOYEE\_NUMBER, EMP\_FIRST\_NAME, EMP\_LAST\_NAME, EMP\_EMAIL, HIRE\_DATE) VALUES (123456, 'JOHN', 'DOE', 'JDOE@EMPLOYEE.COM', '01/01/2018'); INSERT INTO #TEMP\_EMPLOYEES (EMPLOYEE\_NUMBER, EMP\_FIRST\_NAME, EMP\_LAST\_NAME, EMP\_EMAIL, HIRE\_DATE) VALUES (789101, 'MARY', 'DOE', 'MDOE@EMPLOYEE.COM', '12/01/2015'); &amp;#x200B; /\*CREATE SOURCE EMP\_HRS TABLE\*/ CREATE TABLE #EMPLOYEE\_HOURS\_SRC --THIS HOLDS YOUR EMPLOYEE HOURS (UNIQUE\_KEY INT IDENTITY(1, 1), EMPLOYEE\_NUMBER INT, WEEK\_START\_DATE DATE, WEEK\_END\_DATE DATE, HOURS\_WORKED DECIMAL(10, 2), SICK\_DAYS\_TAKEN DECIMAL(10, 2), PERSONAL\_DAYS\_TAKEN DECIMAL(10, 2), LOAD\_ISRT\_DT DATETIME DEFAULT GETDATE(), LOAD\_UPDT\_DT DATETIME); &amp;#x200B; /\*INSERT TEST VALUES\*/ INSERT INTO #EMPLOYEE\_HOURS\_SRC (EMPLOYEE\_NUMBER, WEEK\_START\_DATE, WEEK\_END\_DATE, HOURS\_WORKED, SICK\_DAYS\_TAKEN, PERSONAL\_DAYS\_TAKEN) VALUES (123456, '05/26/2019', '06/01/2019', 40, 0, 0); INSERT INTO #EMPLOYEE\_HOURS\_SRC (EMPLOYEE\_NUMBER, WEEK\_START\_DATE, WEEK\_END\_DATE, HOURS\_WORKED, SICK\_DAYS\_TAKEN, PERSONAL\_DAYS\_TAKEN) VALUES (789101, '05/19/2019', '05/25/2019', 40, 0, 0); &amp;#x200B; /\*CREATE TGT EMP\_HRS TABLE\*/ CREATE TABLE #EMPLOYEE\_HOURS\_TGT --THIS IS YOUR TGT TABLE (UNIQUE\_KEY INT IDENTITY(1, 1), EMPLOYEE\_ID INT, WEEK\_START\_DATE DATE, WEEK\_END\_DATE DATE, HOURS\_WORKED DECIMAL(10, 2), SICK\_DAYS\_TAKEN DECIMAL(10, 2), PERSONAL\_DAYS\_TAKEN DECIMAL(10, 2), LIEU\_TIME\_ACCUMULATED DECIMAL(10, 2), LOAD\_ISRT\_DT DATETIME DEFAULT GETDATE(), LOAD\_UPDT\_DT DATETIME);
so it's SQL File 3 you need help with? what have you got so far?
Is there a way for me to send you my 2 files? Or would you rather me just copy and paste it here?
/\*CREATE CALENDAR WITH HOLIDAY. THIS CAN BE CREATED ONCE IN A PHYSICAL TABLE\*/ CREATE TABLE #CALENDAR (CalendarDate DATE, WEEK\_START\_DATE DATE, WEEK\_END\_DATE DATE, IS\_HOLIDAY CHAR(1)); &amp;#x200B; DECLARE @StartDate DATE; DECLARE @EndDate DATE; SET @StartDate = '01/01/2019'; SET @EndDate = DATEADD(d, 365, @StartDate); WHILE @StartDate &lt;= @EndDate BEGIN INSERT INTO #CALENDAR (CalendarDate, WEEK\_START\_DATE, WEEK\_END\_DATE, IS\_HOLIDAY) SELECT @StartDate, DATEADD(wk, 0, DATEADD(DAY, 1 - DATEPART(WEEKDAY, @StartDate), DATEDIFF(dd, 0, @StartDate))), DATEADD(wk, 1, DATEADD(DAY, 0 - DATEPART(WEEKDAY, @StartDate), DATEDIFF(dd, 0, @StartDate))), Holiday = CASE WHEN \[Month\] = 1 AND \[DayOfMonth\] = 1 THEN 'Y' WHEN \[Month\] = 5 AND \[DayOfMonth\] &gt;= 25 AND \[DayName\] = 'Monday' THEN 'Y' WHEN \[Month\] = 7 AND \[DayOfMonth\] = 4 THEN 'Y' WHEN \[Month\] = 9 AND \[DayOfMonth\] &lt;= 7 AND \[DayName\] = 'Monday' THEN 'Y' WHEN \[Month\] = 11 AND \[DayOfMonth\] BETWEEN 22 AND 28 AND \[DayName\] = 'Thursday' THEN 'Y' WHEN \[Month\] = 12 AND \[DayOfMonth\] = 25 THEN 'Y' ELSE 'N' END FROM(SELECT \[Month\] = MONTH(@StartDate), \[DayOfMonth\] = DAY(@StartDate), \[DayName\] = DATENAME(weekday, @StartDate)) c; SET @StartDate = DATEADD(dd, 1, @StartDate); END; &amp;#x200B; /\*YOU CAN MAKE THIS A STORED PROC\*/ /\*IN THE EXAMPLE BELOW, 123456 HAS WORKED ON A WEEK WHERE THERE'S A HOLIDAY (05/27/2019) WHERE AS 789101 HAS NOT WORKED ON A WEEK WITH A HOLIDAY\*/ DECLARE @EMPLOYEE\_NUMBER INT= 123456-- 789101 /\*,@WEEK\_START\_DATE DATE= '05/25/2019' ,@WEEK\_END\_DATE DATE= '06/01/2019';\*/ SELECT B.UNIQUE\_KEY, A.WEEK\_START\_DATE, A.WEEK\_END\_DATE, A.HOURS\_WORKED, A.SICK\_DAYS\_TAKEN, A.PERSONAL\_DAYS\_TAKEN, CASE WHEN C.IS\_HOLIDAY = 'Y' AND A.HOURS\_WORKED = 40 THEN 'DO THE CALCULATION' ELSE NULL END AS LIEU\_TIME\_ACCUMULATED INTO #TEMP\_EMP\_HOURS FROM #EMPLOYEE\_HOURS\_SRC A JOIN #TEMP\_EMPLOYEES B ON A.EMPLOYEE\_NUMBER = B.EMPLOYEE\_NUMBER LEFT JOIN #CALENDAR C ON A.WEEK\_START\_DATE = C.WEEK\_START\_DATE AND A.WEEK\_END\_DATE = C.WEEK\_END\_DATE AND C.IS\_HOLIDAY = 'Y' WHERE A.EMPLOYEE\_NUMBER IN (@EMPLOYEE\_NUMBER) \--AND A.WEEK\_START\_DATE = @WEEK\_START\_DATE \--AND A.WEEK\_END\_DATE = @WEEK\_END\_DATE &amp;#x200B; SELECT B.EMPLOYEE\_NUMBER, B.EMP\_FIRST\_NAME, B.EMP\_LAST\_NAME, B.HIRE\_DATE, B.CURRENT\_FLAG, A.WEEK\_START\_DATE, WEEK\_END\_DATE, HOURS\_WORKED, SICK\_DAYS\_TAKEN, PERSONAL\_DAYS\_TAKEN, LIEU\_TIME\_ACCUMULATED FROM #TEMP\_EMP\_HOURS A JOIN #TEMP\_EMPLOYEES B ON A.UNIQUE\_KEY = B.UNIQUE\_KEY; \--WHERE WEEK\_START\_DATE&gt;=? AND WEEK\_END\_DATE&lt;=?--TO FILTER RECORDS IN THE FUTURE
From my experience, most SQL operations don't get to crazy. &amp;#x200B; It's whenever you get into data that's organized as a tree structure that SQL queries can be unruly. &amp;#x200B; I haven't written recursive SQL (but you can) but I imagine that's as complicated as most SQL operations usually get. Of course, there's probably more complex queries but most of the time (for me) it's SELECT|Rank|CASE|GROUP BY|LIMIT| OFFSET.
Haha thanks for this. Are you joking when you say this is typical length?
It is the 20th largest reporting query out of 200, and is about twice the length of the average query.
I work at a nine figure company. We have a handful of queries this complex. They're not common, but they're not unheard of. As you read that, you'll realize it's not actually very bad. This gives some game data correlated by players, draft picks, and team roster, broken down by week.
He meant that `pricings` table is only visible in `temp` subquery. It is local to it, kinda like variables declared inside function. This is scope issue.
So this is one that is is used over and over again until the foreseeable future? How long did it take to write? Do you do smaller simpler queries as hoc or do you spend your day writing stuff like this?
I had a similar problem not long ago and solved it with [getsheets.py](https://gist.github.com/pacohope/7b01b1f182127a7dfc963aed8befefe1). It will strip out all the tabs and put them into individual CSV or XLSX files.
I would estimate the effort for this query at about 2 focused days, and it is used daily to refresh a report. We'll write smaller queries as part of researching solutions, but our production queries look more like this.
How far you on your work?
I have worked with SQL in my day job for 18 months now. When I went into my role I knew zero SQL but had a grasp of some basic coding. Since then I have had a promotion and now am very comfortable with SQL. As for the complexity, it can vary wildly depending on what sort of place you work and what role within the company you have. I'd say some of what has been posted is slightly on the larger side of queries I have had experience with, but they can build up quickly when you have larger reports to run or a project on. The good thing is, modulisation helps with the larger scripts and when I look back on some of the scripts I've written I take a while to re-interpret them myself (due mainly to my laziness and not leaving annotations). But honestly if you have a decent grasp on SQL currently, its worth a shot. Edit: it will be much easier to develop your sql when you're in a role that provides you with scenarios to do so. Whenever I struggle in work Google is always there to help out
What does this line accomplish? (Brand new to sql) Does it just update the first entry?
1 - execute sproc 2. - winkey + r &gt; excel 3 - create eight sheets 4 - copy/paste each result into said sheet 5 - save ??? profit
I already got it. I was looking for a way to do all these steps in one execution.
I realize that; I was being sarcastic. XD Good job BTW!
I’m still working on writing out 2nd and further tables. I went back home, so I’ll resume tomorrow.
Without sitting down and writing some queries, you could base the first / last names, etc. on the positions of the characters in the user input and parse it out to update the various columns? That’s the first idea that comes to my head.
Build your UPDATE statement string using the values from the parameter list: #! /usr/bin/python3 query_string = "update userTable set " param_count = len(param_list) counter = 1 for item in param_list: query_string += item.key + " = " + item.value if counter &lt; param_count: query_string += ", " counter += 1 query_string += " WHERE id = {}".format(id_value) curr.execute(query_string)
Give me the highest value sale per salesperson as of some date when the client was living in some area as of that date. That's complicated-ish.
The real MVP.
Thank You so much.
Here's a query I wrote today to measure messaging campaign performance. Disclaimer: I'm learning and this is not my main job ;) https://pastebin.com/8ZpiNjBF
Looks like you wrote pretty much the whole thing for me! I can't say I understand the entire thing (yet) but I've got about 60% of it. Thank you so much! This is an amazing way to learn as I go along. Regarding the Employee Hours table: Looks like you set it up with a start date and an end date as columns. I had thought about setting things up as a week # for a particular year and then referencing another table to define the start date and end date for the week #. So data for the employee hours would look like (EMPLOYEE\_NUMBER, WEEK\_NUMBER, HOURS\_WORKED, SICK\_DAYS\_TAKEN, PERSONAL\_DAYS\_TAKEN). &amp;#x200B; Do you think there's any point in that? Or is it over-complicating things? I haven't figured out parts of your second post yet, so it's possible that what I'm suggesting is redundant or just not-workable.
That looks good. Did he ask you to just return one row? If not you can just sort by the max average in descending order and you’re good
OK, let's slow your roll down here and think through this. Here is what your data looks like, and it's coming from a table called MinimumGPA: |cName|major|minGPA| |:---|:---|:---| |OSU|CS|3.75| |OSU|EE|3.5| |OSU|history|2.8| |U of O|CS|3.6| |U of O|biology|3.75| |Cornell|bioengineering|3.8| |Cornell|CS|3.4| |Cornell|EE|3.6| |Cornell|history|3.6| |Cornell|psychology|2.8| |MIT|biology|3.5| |MIT|bioengineering|3.5| |MIT|CS|3.9| |MIT|marine biology|3.5| So now the first thing we need to do is determine the average, which is pretty straight forward, such as: select cName, AVG(minGPA) AS 'avgminGPA' from MinimumGPA group by cName If you were to put this data in Excel to test it, you would see that this will return you with data such as: |cName|avgminGPA| |:---|:---| |OSU|3.35| |U of O|3.675| |Cornell|3.44| |MIT|3.6| Now we can do a few things here to answer your next question about which one of these is the max. You can Google #tables, CTE's, but this is just a generic sub-query which is what you did above: select * from ( select cName, AVG(minGPA) AS 'avgminGPA' from MinimumGPA group by cName ) x Couple of rules to note here, and they may vary depending on what type of SQL you're using, I'm not sure: 1. You must name anything that is unnamed inside of a sub-query, CTE, or before you can put it into a table, temporary (#), or otherwise. That is what the line `AS 'namegoeshere'`. 2. See that little `x` up there? MS SQL doesn't like it if you don't give something a name, and that includes your sub-query. You could rewrite the `select *` to `select x.*` and it would do the same thing, but `x.*` would not be the same as a straight `*` if you were joining to another sub-query, or table. Just adding these in to explain why I have them there, not sure if you're doing this on paper or not. In your example you are doing this: `SELECT MinimumGPA.cName, MAX (avg_min)` in the outer query, but you are no longer selecting from MinimumGPA, you are selecting from a new thing, hence `x`. From here we just need to turn that `*` into the last bit of information you need, and there are a lot of ways to do this, some are rather convoluted and involve joins, others might involve a CTE, or a #table, but the simplest way would be this: select top 1 x.cName, x.avgminGPA from ( select cName, AVG(minGPA) AS 'avgminGPA' from MinimumGPA group by cName ) x order by avgminGPA DESC
&gt; Artists.GroupName should be a foreign key. And I would expect there to be a many-to-many relationship between Artists and Groups Many-to-Many relationships are verboten.
Dope answer, I'll give that a shot. Thanks!
I’m not familiar with the syntax of python, but is this open to injection? Considering it’s a textbox field filled in by the user (rather than predetermined dropdown), protection from SQL injection would be wise. Perhaps I’m missing some intelligence that is inherent in Python though.
thanks! The Error I keep getting: &gt;#1248 - Every derived table must have its own alias
Holy moly. Thank you for the thorough explanation. Much better than my actual instructor.
What I gave you will work perfectly, but there is a slight nuance to look at: order by avgminGPA DESC Here I forgot to add the `x.` but it doesn't matter, because there is only `x`. If you were to join `x` to `y` and if both of them had a column named `avgminGPA` then you would get an error because SQL does not know which of the two you wish to sort by, which is the entire reason for naming it `x` in the first place. This is called *aliasing*. select cName, AVG(minGPA) AS 'avgminGPA' from MinimumGPA group by cName You can see here we didn't use an alias, and we didn't need one, because we only are selecting from a single table. You don't need to use them in the select statement when you're joining so long as the column you're asking for only exists in one of the two tables, if it exists in both then you will get an error telling you that it doesn't know which one you want.
What you gave me wouldn't work with my version of SQL. My tweaked version: SELECT cName, avg_min FROM (SELECT MinimumGPA.cName, AVG(MinimumGPA.minGPA) AS avg_min FROM MinimumGPA GROUP BY MinimumGPA.cName) AS x ORDER BY avg_min DESC LIMIT 1 I am super grateful for your help. I was very helpful! EDIT: Aliasing is confusing!
Well done. Sometimes you can get weird errors if you name a subquery with an `AS`... SQL can be finicky sometimes, and I honestly haven't memorized or learned the how/why/where, I just look at the error, look at the code, play with it a bit, try again.
That's pretty much what I've been doing, on an entirely different lower level though. Learn by trial and error! haha I was getting **Every derived table must have its own alias** error and couldn't figure out what that meant. Our instructor never mentioned alias's to us! Just adding that **AS x** to the FROM fixed it. lol
Dude, my formal programming language education is in COBOL, PERL, RPG, and C++. I treat SQL like my bitch and hit it with a hammer until things work. I don't have time for this new age hippy nonsense. I used to just spam F5 and make little changes when I first started out. You can learn to do this in a 'safe' way, and it's important to learn how to deconstruct your code. For example, highlight the query inside the FROM () statement and run it with a TOP 1, or a LIMIT. Does it work? Ok, problem isn't there. Move on to the next possibility. Eventually you just kind of find your own way. For example I use `X.Field AS 'Name'` in my SELECT/WHERE/GROUP/ORDER sections at all times, and I'm very particular about case sensitivity. When I name a sub-query though I never use AS or '', and I always, always, always give a table an alias, even if there are no joins. This may sound pedantic, but it's my [Brown M&amp;Ms clause](https://www.npr.org/sections/therecord/2012/02/14/146880432/the-truth-about-van-halen-and-those-brown-m-ms). In the beginning my queries look like this: select *, blah, things, morethings from table join things where things And over time, as I polish them to a point where I am satisfied they will scale and work into the future, they turn into very case specifically formatted queries like this: SELECT CAST(X.FieldName AS date) AS 'DateColumn' , Y.Client , CASE WHEN Z1.FieldName IN ('United States', 'Mexico', 'Panama') THEN 'North America' WHEN Z2.FieldName LIKE '%APAC' AND Z2.FieldName NOT LIKE '%Laos%' THEN 'APAC' ELSE 'Non-Coastal' END AS 'Region' , SUM(dbo.(Z.Spend, Z.CUR_CD) AS 'Sales' FROM [DatabaseName].dbo.Table X INNER JOIN [DatabaseName].dbo.Table Y ON Y.ID = X.ID LEFT JOIN [DatabaseName].dbo.Table Z ON Z.ID = Y.ID LEFT JOIN [DatabaseName].dbo.Table ( SELECT n FROM [DatabaseName].dbo.Table ) Z1 ON Z1.ID = Z.ID WHERE X.FieldName &gt;= getdate()-366 AND X.FieldName &lt; getdate-1 AND (Y.FieldName &lt;&gt; 1 OR Y.FieldName2 &lt;&gt; 3) AND Z.FieldName LIKE '%blah' AND Z1.FIeldName IN ('set') GROUP BY CAST(X.FieldName AS date) AS 'DateColumn' , Y.Client , CASE WHEN Z1.FieldName IN ('United States', 'Mexico', 'Panama') THEN 'North America' WHEN Z2.FieldName LIKE '%APAC' AND Az2.FieldName NOT LIKE '%Laos%' THEN 'APAC' ELSE 'Non-Coastal' END The reason I will format specifically like that with certain words, etc., and why I use single LETTER, or LETTER/NUMBER combinations for my aliases is two fold: 1. I will often times have to go look at something I wrote a long time ago, and the moment I see it, if it isn't formatted like that, then I know I can't trust it, or my past self. Secondarily, I didn't use to write my `CASE` statements like that a year ago, that was a fairly new thing I landed on and liked. So if I see it is properly formatted, but the `CASE` is just a little off, then it immediately reminds me of where I was in terms of my skills &amp; learning a year ago, and immediately helps me remember what if any important things I've learned since then which may apply to whatever problem I'm having. This immediately lets me know where and what to look for, and whenever I open up some code that is so fresh n so clean clean... then I trust myself and assume something else is wrong, somewhere else, and that something must have changed because it couldn't possibly be me who was wrong. 2. I treat all queries equally and often times do not understand the underlying data structure. A lot of people prefer, and criticize me for the way I will alias a table, but lots of times I'll have absolutely no idea what `cmpPR2018` means if you want to give a table such as `dbo.CampaignsNA_2018_PR AS cmpPR2018`, and then it just makes my SELECT and WHERE look ugly. What I prefer to know is how X interacts with Y, or A interacts with B, and I write sequentially so that I can see how a LEFT interacts with a RIGHT. Once I can see how that is going on I can begin to comment things out of the SELECT and selecting new things to test to see what if anything has changed, begin playing with the WHERE, and generally debugging/testing the data with a number of hypotheses that eventually narrow down to finding the root cause, and then being able to step back and see how to fix it. So single letter aliases keep things impersonal and make me go through "the process" every time, and I trust that process to help me most efficiently resolve the issue. The first thing I do whenever I inherit someone's code, or have to fix a problem that others can't, I will reformat the code to my standard before even looking at the issue, which is done for me to understand what the code itself is doing, not necessarily what the data looks like. I rarely leave myself comments on code, because I rarely have any idea what the data is like that I'm working on, and any comments I might leave would be something like `--this shit is fucked up, because someone fucked something up, and I'm here having to fix it.`For me the structure of how I format a query is the comment in many ways. Now on code that I develop from the ground up, from scratch, I will comment quite a bit, and leave extensive documentation on what each section does. I never read this ever, but the act of writing it helps me to better understand and memorize the process, and god help anyone who inherits my code once I'm gone so I kind of feel like I owe it to them. Cheers, I've been waiting for jobs to run and working. This was fun. If you want to give it a shot try to calculate the same one row answer using `MAX()` but without using a `TOP` or `LIMIT`. Good example of why using a CTE or a #table is nice compared to joining sub-queries.
My company has 9 stores. One of the queries that I built was to compare sales by item for each store for the last 12 months. So basically the first column was the product. The next 9 columns had the number of that product that was sold at each store. Then the next 9 columns had the gross margin made by each store for that item. Total of 19 columns. Easy to do. Then they want to compare those numbers with the same numbers for the past three years on a yearly basis. So there would be a total of 73 columns. One column for the product, 36 for the counts sold by store by year and 36 for the gross margin by store by year. Each sales order had the date, store and items. The order item table has over 10 million records. Doing 36 joins of the sales table would cause the report to take more than a day to run. Then I changed the query around and it took less than 10 minutes to run. How would you solve this problem?
So in Java you have a workbook object. Just use the createSheet method to create a new sheet for each result set you process.
This would work perfectly for the second question and I had the same solution for the first in mind. But what came to mind for the second one was dumping the results of the first query into a temp table and then selecting the max and grouping university. But as mentioned this works perfectly fine
BI Dev is a common one for what you do. Report Writer emphasizes the writing SQL and working with operations/business. Analyst miiiight be appropriate depending on "working with operations/business to gather requirements looks like" - yes if you're doing things like reading specs, shadowing workflows, etc., no if you're just asking them the proper questions to learn what columns to use. Data Engineer covers some of the automating and pipeline stuff.
You’re leaving information out.
Yea my interaction with Operations is LITERALLY asking the right questions to clarify the request. Thanks, based on your response it seems I’m more of a BI dev, albeit on the lower end it seems
Could you consider the position a Data Analyst?
The first one is not yes because if there are values in the product &lt; 200 or orders &lt;10 that share a pid or cid they can return in the second query. The where clause only applies to the 1values and not the 2s.
Maybe? I still think Analyst should stand in more for the proactive part of immersing yourself in workflow to be the subject matter expert, but I've definitely heard Data Analyst used for small shops where you're kind of doing the whole thing yourself.
I think I added everything necessary. Sorry I forgot the schemas.
any chance you can explain now? I just don't understand. are the joins between x1, y1, z and x2, y2, and z making two tables or one table? how does the filtering of the rows differ in the (i) and (ii) if there is only one table created? and if there are two tables, why are the correct rows selected in (ii) as shouldn't there be a table that has all customers and one that has the correct ones from which we pull z.cid and z.cname?
sorry, but can you maybe explain this more in depth? are two tables created in these joins or one? how does the filtering of the rows differ between the first query and second query if one table is created? if two are created, why are not too many z.cid and z.cname values selected in two as wouldn't one table has all the z.cids and the other have the correctly filtered ones?
 You only ever create one 'table' with a select query. The difference is the joins. It might be easier to look at it like this SELECT DISTINCT z.cid ,z.product FROM product x1 INNER JOIN orders y1 ON x1.pid = y1.pid INNER JOIN customer z ON y1.cid = z.cid INNER JOIN orders y2 ON z.cid = y2.cid INNER JOIN product x2 ON x2.pid = y2.pid WHERE x1.prive &lt; 200 AND y2.qnt &gt; 10 AND z.city = 'Seattle'; You notice that the customer table is kinda like the middle point on a see-saw. It is connecting your x1 y1 join to your x2 y2 join. so when in your were clause you affect one side in one way and another side in another you leave the possibility for multiple joins. In the first picture imagine there is a y1 value greater than 10 and it connects to a different customer than the y1 under ten. You would get a different answer. This isn't possible in the second picture because the where clause in the Q example matches the 2nd picture example. It doesn't matter if you different values in the x2 y2 side. The joins to get to the customer is the same and so are the where conditions.
Soooo helpful omg thank you!!!
Its the local govenment office and in Hungary this is customary. We had problems with people not working their hours so our mayor decided we should use this system. It is legally required we are just behind. A small town.
Its the local govenment office and in Hungary this is customary. We had problems with people not working their hours so our mayor decided we should use this system. It is legally required we are just behind. A small town.
Thanks! I'll try, but I have never written sql code. Lets see where this goes. I have already given them a heads up, that they should use their cards whenever they leave and come back. They usually just use the back door thoughXD. This system needs to be in place mid-may so I still have time. If i cant figure it out I might contact the devs of the software for help. This wasnt cheap by any means and this is a govenrment office so we technically would have the budget, but they dont like IT very much when it comes to money
Hmm thanks for the input. How would that work in powerbi? is it easier than SQLite?
Definitely a BI Developer I would say. I'm doing the exact same thing as a BI Dev. Well I don't use automation tools since most of the reports I have to do are so unique that I think it's not necessary. Also if you're working a lot with specifications and doing design maybe you could say Business Analyst but I doubt that :D
Maybe, possibly, I have no idea. The purpose of the reply was to give OP an idea of how to go about building a dynamic query, not necessarily to provide the verbatim solution to be cut/pasted into production.
PoserBI will just report on the sqlLite data.
Oh - you just gotta add a field name to the average calculation in your sub query (something like “as AvgMinGPA”). Without that the server has no way to reference the field.
Honestly the BI field is so haphazardly slapping together job titles you might as well just pick whichever one sounds best. Having said that, I disagree with the other posts about BI dev being the best fit - I'd consider a BI dev to be a bit more end-to-end: someone who does more pipeline stuff (SSIS in your stack), knows a bit about data modelling, on top of the report development stuff you do. Also, a BI dev doesn't necessarily have the business analysis skills you do: it *can* be purely technical, working in tandem with analysts who build the requirements. My first thought from your description is reporting analyst. But like I said our titles are so inconsistent that there certainly are roles like yours called BI Dev so feel free to put that on your CV.
Thank you for your help! I rebooted my pc and the code finally executed.
I'm a BI Dev and a large part of my job is understanding how the workflow I support works, and showing trending etc. It's a constant source of frustration to me as I was hired for SQL/report writing skills but the program manager I support expects me to do basically data science and make weekly presentations to the outside client I support. Hoping he gets moved to another client soon.
&gt; Honestly the BI field is so haphazardly slapping together job titles you might as well just pick whichever one sounds best. This. I've been doing the same thing for years and every company has called me something different. Was doing BI in the finance department of a F500 for a few years and they said I was a "financial analyst" which got me some funny phone calls. That said, as per OP's question, "Solutions Analyst" means something totally different and I would avoid using it. I think at my current company I'm listed as a Business Systems Analyst, but I would never put that on a resume.
FFS yes! I am a Data Analyst doing what OP described plus ETL dev. Another Data Analyst at my company was doing ticket triage and report validation. I had a job offer for a Data Migration Specialist a couple months ago. WTF. I'll tell you what I can do, you give me money and call me what you want. At some point we'll meet in the middle and standardize this crap.
Report Writer also sounds lower level, so is probably worth avoiding if looking for another job. Data Engineer probably over-eggs the pudding a little, I'd expect more than OP's description from someone with that title
There's no "Rule", but generally table names are pluralised in most systems: so if others need to work with your database, they'll "get into the flow" faster with plural "People" would therefore be the one I'd choose out of your option... although it's usually worth being more specific: what are these people? The most common table name for a table holding people, is probably "Users", or something domain specific like Customers, Teachers, or Students.
You can throw them in a pastebin Generally people on programming forums don't like to solve homework questions for you, though, so the parent commenter's "What have you got so far?" is actually asking for you to give a brief overview of what you understand, and any code you've written. Rather than answer the question for you, people tend to prefer if you show the question, explain your understanding of it, and then show your answer/code so far. We can then point out where you might be going wrong, or explain anything you've misunderstood, and you can finish your answer yourself with a better understanding of the question and technique
Here is one of the really everyday used query (maybe some trouble with encoding) https://github.com/okxjd/sql-examples/blob/master/kn_info_v1.sql
No worries. I'm just trying to help. If you are talking about the destination employee hours table then sure you can normalize even more. I'm not sure how your source table is but if you want to normalize your destination table, you can create an ID on the calendar table and then use that as a FK on the the destination table. You'd need to make some changes to the calendar SQL and the final Select. The way I've created the destination table is not normalized. It's for users who can write a simple select. The way it should be is: (EMPLOYEE\_ID, DATE\_ID, HOURS\_WORKED, SICK\_DAYS\_TAKEN, PERSONAL\_DAYS\_TAKEN) So you'd have an Employee table with an ID and a calendar table with a date ID. And those would be the foreign keys to the destination table. When you would query the table, you'd join the destination table with the employee and calendar table on the ID to get relevant columns. Something like: SELECT \[COLUMNS\] FROM EMPLOYEE\_HOURS A WITH(NOLOCK) INNER JOIN EMPLOYEES B WITH(NOLOCK) ON A.EMPLOYEE - ID = B.EMPLOYEE\_ID INNER JOIN CALENDAR C WITH(NOLOCK) ON A.DATE\_ID = C.DATE\_ID;
Our guy that does what you do is a "Business Information Analyst."
Hi. Can you please elaborate a little? When you say "compute," what is it based on?
The base table contains CustomerId, ItemId, and another table contains ItemReturned, but that isn't required for the requirement here. I should have phrased the question a little more clearly. Simply put: Given the columns CustomerId, ItemId and ItemReturned, compute the column AnyItemReturned. The AnyItemReturned column prints Y if, for the current CustomerId, any value in ItemReturned is Y. Otherwise it prints N.
&gt;Is this what you are looking for? In my example, Customer 1 has Item 20 returned. I've added item 54 for testing. IF OBJECT\_ID('TEMPDB..#TEMP\_BASE\_TABLE') IS NOT NULL DROP TABLE #TEMP\_BASE\_TABLE; IF OBJECT\_ID('TEMPDB..#TEMP\_ITEM\_RETURNED') IS NOT NULL DROP TABLE #TEMP\_ITEM\_RETURNED; &amp;#x200B; CREATE TABLE #TEMP\_BASE\_TABLE (CUSTOMER\_ID INT , ITEM\_ID INT) &amp;#x200B; CREATE TABLE #TEMP\_ITEM\_RETURNED (CUSTOMER\_ID INT , ITEM\_ID INT, ITEM\_RETURNED CHAR(1)) &amp;#x200B; INSERT INTO #TEMP\_BASE\_TABLE VALUES (1, 20), (1, 54), (1, 85) &amp;#x200B; INSERT INTO #TEMP\_ITEM\_RETURNED VALUES (1,20,'Y'), (1,54,'N') &amp;#x200B; \--SELECT \* FROM #TEMP\_BASE\_TABLE &amp;#x200B; \--SELECT \* FROM #TEMP\_ITEM\_RETURNED &amp;#x200B; SELECT A.\*, CASE WHEN ITEM\_RETURNED = 'Y' THEN 'Y' ELSE 'N' END AS ANY\_ITEM\_RETURNED FROM #TEMP\_BASE\_TABLE A LEFT JOIN #TEMP\_ITEM\_RETURNED B ON A.CUSTOMER\_ID = B.CUSTOMER\_ID AND A.ITEM\_ID = B.ITEM\_ID;
Job titles crack me up. Huge variance in responsibilities for job to job with the same title. I've seen BI dev that wants machine learning and big data. Also seen some that want Excel and a little SQL. **To OP:** The best thing is to choose the highest-paying job title. That's probability BI developer. It's a field with so many openings and they pay is great. Just a little under data scientist. If the industry cannot adequately define the job titles, pick the title that benefits you the most! If you are looking to move your career forward, pick up Power BI and you'll likely find a ton of jobs you're qualified for.
Just FYI, you're replying to an article. The article gives some (good) reasons for preferring singular table names, however I feel like it's not very convincing without the arguments found in [this article](https://www.teamten.com/lawrence/programming/use-singular-nouns-for-database-table-names.html).
Thank you for the links,they were helpful and this time I installed oracle correctly...2 times. During the installation it asks me to insert an username and a password,which I simply called admin/admin. Executing sqlplus in the cmd,he asks me then to put the user name and the password but,inserting both it tells me that the data are incorrect...I'm pretty sure the id and pass are correct because I wrote it on a notepad,copied and pasted it for both. I saved my notedpad and copied and pasted it in cmd too, Am I doing something wrong?
Here's a query to order artists by "best sellers" SELECT Artist.ArtistID, SUM(Quantity) AS Quantity FROM Artist INNER JOIN RecordingArtist ON Artist.ArtistID = RecordingArtist.ArtistID INNER JOIN OrderItems ON RecordingArtist.UPC = OrderItems.UPC GROUP BY Artist.ArtistID ORDER BY Quantity DESC
Thank you very much! &amp;#x200B; Do you know a way to do it so that the output shows the full names of the artist instead of just their ID#?
Add Artist.ArtLName to the select and group by
Couple of things, You don't need to do insert into for every insert, INSERT INTO Artist VALUES ('111222333', 'Iron Butterfly', NULL), ('777888999', 'Ramones, The', NULL); Works just as well 2) When you want to pass along a script like that or just test it, http://sqlfiddle.com/#!18/d6172 is your friend. 3) You should mention the SQL Variant you're working with as not all of them work the same way. For instance I had to do a lot of tweaking of your tables to get it to build with MS SQL because VARCHAR2 and NUMBER don't exist in it.
When it comes down to it, though, I feel that convention and consistent code is generally going to work better in a real-world project than theoretical "this is more logical because" I don't much care if "strictly speaking we aren't naming a table, we're naming a relation"... I care that developers don't waste time writing code and then going "Oh ffs, these table names are singular?" Besides which, I'm no convinced by the article you link 1. "Strictly speaking we're naming a relation" - I disagree, a relation is a link between tables. A table is a set of data. A table does not contain a user, it contains users 2. "It reads well everywhere else in the query" so does users, I really don't see this as an issue at all, it's just clutching at straws: making readability worse in half the query in order to make it better in the other half makes no sene. If you want it to read consistently, alias your tables. `from users user` means it now reads properly later too :p 3. Rails etc pluralizing... I don't use Rails, and I've never used an ORM that pluralizes like this. I'm not convinced we should all change our naming conventions to account for an ORM most of us don't use. Go change Rails. 4. "Some relations are already plural." Okay? I don't see any relevance
&gt; I would appreciate it if anyone could show me 3-5 useful joins I could make with these tables. many of us would appreciate you at least giving it a shot this is not /r/domyhomeworkforme
&gt; The article gives some (good) reasons for preferring singular table names i read it, and no, it does not
&gt;Edit: Figured it out myself! Thanks for your help! &gt; &gt;SELECT Artist.ArtistID, SUM(Quantity) AS Quantity FROM Artist INNER JOIN RecordingArtist ON Artist.ArtistID = RecordingArtist.ArtistID INNER JOIN OrderItems ON RecordingArtist.UPC = OrderItems.UPC GROUP BY Artist.ArtistID ORDER BY Quantity DESC
 CREATE TABLE persons
Completely agree here. The titles can be wildly confusing and vary widely. Where I am OP would be a Reporting Analyst. I was a Reporting Analyst and moved to BI Dev. The biggest changes were adding in more data modeling, data management and ETL. Also the general scope of projects/reports is larger and more complex for BI Dev's than Analysts.
Okay, I take it back, there's *one* good reason in the OP article, the ISO standard. The only other good *point* is near the end, where it says "try and stay consistent with the rest of the code and team". The article I linked does a much better job presenting the case for singular-by-default table names.
Apologies if typos exist. /\*HOW MANY TIMES CUSTOMER PURCHASED\*/ SELECT CUSTOMERS.Customer#, COUNT(1) AS Quantity FROM CUSTOMERS INNER JOIN ORDERS ON CUSTOMERS.CUSTOMER# = ORDERS.CUSTOMER# GROUP BY CUSTOMERS.Customer# &amp;#x200B; /\*CUSTOMERS WHO HAVE NOT PURCHASED\*/ SELECT CUSTOMERS.\* FROM CUSTOMERS LEFT JOIN ORDERS ON CUSTOMERS.CUSTOMER# = ORDERS.CUSTOMER# WHERE ORDERS.CUSTOMER# IS NULL &amp;#x200B; /\*RECORDS NOT SOLD\*/ &amp;#x200B; SELECT Records.\* FROM Records LEFT JOIN OrderItems ON Records.AlbumID = OrderItems.Item# WHERE OrderItems.Item# IS NULL &amp;#x200B; /\*WHICH RECORD IS TOP SELLER\*/ SELECT Records.Title, COUNT(1) AS Quantity FROM Records JOIN OrderItems ON Records.AlbumID = OrderItems.Item# GROUP BY Records.Title
“Reporting Developer” would be the most accurate title I would think. Usually BI jobs would require more ETL work than just reporting.
Nah, delete * from world where type = person and Annoying = 1 hehe
&gt; I feel that convention and consistent code is generally going to work better in a real-world project than theoretical "this is more logical because" Totally agree. If someone goes onto a project that uses plural table names and tries to change it to singular "because logic", they're wrong. &gt; I care that developers don't waste time writing code and then going "Oh ffs, these table names are singular?" I don't think this is a valid point, you can switch the word singular to plural to make the exact argument for the other side. I'm working on a project where the person who built the database pluralized every other table name (with the rest being singular) out of spite because the team couldn't agree on this exact topic, and I've had both happen to me often. &gt; A table does not contain a user, it contains users While correct, consider thinking of it from a different perspective. The table "User" defines a single user. A user has an ID, a Name, a DoB, etc... It's implied that there will be more than one user in the table because, well, it's a table. If you are joining to the User table using UserID, you would expect that you are joining to a single record, and that there will be no row duplication as a result of the join. However (leaning into point 4), if you want to create a relationship (perhaps family or friend list functionality), you'll need a linking table between User and User. This would be a plural relationship, because one user can have multiple friends, so the table would be named UserFriends. The plurality of the table name implies that joining to this table using UserId can result in row duplication. Regarding point 2, you're not actually solving any readability issues using aliases. Compare the following: -- Sensible in the FROM clause, but awkward everywhere else SELECT Customers.FirstName, Customers.LastName FROM Customers WHERE Customers.Id = @userId -- Awkward in the from clause, but sensible everywhere else SELECT Customer.FirstName, Customer.LastName FROM Customer WHERE Customer.Id = @userId -- Awkward and wordy in the from clause, but sensible everywhere else SELECT Customer.FirstName, Customer.LastName FROM Customers Customer WHERE Customer.Id = @userId In the first example, you have one place where things are sensible, the FROM clause. In the second example, you have one place where things are *not* sensible, everything else is. That's a massive improvement from plural table names. The third example is worse than it looks. If not everyone aliases Customers to Customer, you're introducing inconsistencies into your code base. And since you're adding an additional step that is strictly optional, it's guaranteed that not everyone will do it. Instead of aliasing it to make it look better, just name it properly in the first place. I agree with you on point 3 however. That sounds like a poor pluralization implementation. I've used Entity Framework a lot and its pluralization does not have that problem.
don't be snarky your post title asked a very specific question, i merely answered it
Msg 102, Level 15, State 1, Line 1 Incorrect syntax near '*'.
that article makes 4 points and some of us don't agree with any of them i will give you consistency, though but good luck when you try to `CREATE TABLE order ( ...`
Actually, it's just an article.
If I really needed to name a table Order, I'd do the same thing as I would if I needed to create a table named References or Values. CREATE TABLE [Order]/"Order" ( Joking, of course. Reserved words obviously shouldn't be used as a table name, but in general I'd encourage people to use table names more descriptive than Orders or Users.
an article that asks a question which i answered
It should be fairly straightforward to do the SQL. PM me if you need further help.
this is why it updates all rows -- WHERE :tracking_num = :tracking_num that's gonna evaluate true for every row, right?
I had a very similar job as a consultant in my first SQL Role. I believe my title was Data Analyst, but me and the whole team felt like we were more aptly titled SQL Report Developers. &amp;#x200B; My next position was 50% of what you described, and 50% supporting and customizing accounting software, and my title was Support Programmer Analyst. &amp;#x200B; If you are job searching, you will want to cast a wide net of search terms because I don't think there is a well defined structure to naming positions at this time.
I would expect a BI Dev to be able to talk about normalization and denormalization, star vs snowflake schemas, slowly changing dimensions, ETL processes (overall process control as well as data flows, index maintenance, etc)... various levels/etc but still... I would probably advise Report Developer.
Yea it’s seems to be the general consensus that there isn’t a specific title for or a set of responsibilities I should for a given title. It feels like responsibilities are sporadic and Yahlieee to what’s needed and not really designed for a title
I do a lot of SQL reporting (maintaining and creating new reports) and my title is Business Intelligence Analyst.
Firstly, use explicit joins; don't just say `from table1, table2, table3`. Make the joins and the join contexts explicit: from table1 inner join table2 on table1.col1 = table2.col1 inner join table 3 on table2.col2 = table3.col3 where table1.name &lt;&gt; table2.name
You can do a sub query in the from where you select CustID, item returned where item returned = y. Then, join that back onto the main table on custody, and select case when CID Exists in that sub query table, return Y, when it does not exist in that sub query table, return N, AS AnyReturn
Sounds like what I do I just through a bit more detail but sound exactly the same. This is probably it
I think there is no golden rule for this. There are some fundamentals, but usually you will have to look at your specific case and optimize for that. For example, if finding one row in **Customer** means that you no longer need to search in **CustomerTable2** then potentially it could be faster to do two queries, with second being optional. If you could be looking at more than 1 match (weird in this case), then you could grab all found CustomerID in first query and exclude those from second one and only use last 2 conditions. I would also create function/procedure for this to get a bit more performance or try using CTE + UNION, could be better.
This is very simple with a window function. A correlated sub-query would work too, but to me window functions are so much easier to understand. DECLARE @Sample TABLE ( CustomerID int not null, ItemID int not null, ItemReturned varchar(1) not null ); INSERT INTO @Sample(CustomerID, ItemID, ItemReturned) VALUES (1, 20, 'N') , (1, 54, 'N') , (1, 85, 'Y') , (1, 1, 'N') , (2, 10, 'N') , (2, 20, 'N') , (2, 85, 'N') , (2, 100, 'N') , (2, 102, 'N') , (3, 1, 'Y') , (3, 1, 'Y') ; SELECT * , CASE WHEN SUM(CASE WHEN ItemReturned = 'Y' THEN 1 ELSE 0 END) OVER (PARTITION BY CustomerID) &gt; 0 THEN 'Y' ELSE 'N' END AS AnyItemReturned FROM @Sample
Generally, your RDBMS will optimise your queries pretty well. You can try to optimise by 1. Putting the "most likely" match first (so that if it is true, the system doesn't need to check the other conditions) 2. Limiting your tables down earlier (eg using a CTE to make your c2 set smaller before joining, or selecting from each table separately and then performing a UNION) Both can help, but it's all situationally dependant and in some circumstances can make things worse (particularly #2) if your changes aren't as good as the RDBMS optimizations would have been
Union together two separate queries, one checking against the first table, and one checking against the second.
Try this: SELECT A.\* FROM ( SELECT CUSTOMERID, CUSTOMERNUMBER FROM CUSTOMER WHERE (CUSTOMERPHONE = @PHONE OR CUSTOMERPHONE2 = @PHONE) ) A INNER JOIN ( SELECT CUSTOMERID FROM CUSTOMERTABLE2 WHERE (CUSTOMERCELL3 = @PHONE OR CUSTOMERCELL2 = @PHONE) ) B ON A.CUSTOMERID = B.CUSTOMERID;
 CREATE INDEX IX_Customer_CustomerPhone ON Customer (CustomerPhone) INCLUDE (CustomerNumber); CREATE INDEX IX_Customer_CustomerPhone2 ON Customer (CustomerPhone2) INCLUDE (CustomerNumber); CREATE INDEX IX_CustomerTable2_CustomerCell2 ON CustomerTable2 (CustomerCell2) INCLUDE (CustomerID); CREATE INDEX IX_CustomerTable2_CustomerCell3 ON CustomerTable2 (CustomerCell3) INCLUDE (CustomerID); CREATE INDEX IX_Customer_CustomerID ON Customer (CustomerID) INCLUDE (CustomerNumber); SELECT c.CustomerNumber FROM Customer c WHERE c.CustomerPhone = @Phone UNION SELECT c.CustomerNumber FROM Customer c WHERE c.CustomerPhone2 = @Phone UNION SELECT c.CustomerNumber INNER JOIN CustomerTable2 c2 ON c2.CustomerID = c1.CustomerID FROM Customer c WHERE c2.CustomerCell2 = @Phone UNION SELECT c.CustomerNumber INNER JOIN CustomerTable2 c2 ON c2.CustomerID = c1.CustomerID FROM Customer c WHERE c2.CustomerCell3 = @Phone;
Okay, will use the explicit joins by defining the 'inner join' s and thank you for providing the code. And yes, I'm trying to get pairs of reviewers who both reviewed the same movie. Sorry, just realized I didn't put it in my post. Why use ratings first instead of directly using the reviewers?
Hi. Can you please elaborate. What do you mean by "require someone to enter a number from 1-5, as a rating?"
Convert each OR statement into a new LEFT JOIN, then wrap that query into a CTE or a subquery, or #table (preferably) and do a CASE statement such as CASE WHEN C.Phone = @Phone THEN 1. Then SUM() all 4 four fields and remove any field where the value is 0. Once had a similar piece of code with multiple OR's that was taking like 19hrs to run. Changed it to multiple lefts and the technique above and it ran in minutes.
Nice approach.
I apologize, for being not being specific. I will update the OP!
Can you post some example data and the query to take a look?
Your schema is an anti-pattern. If you need quick lookups on phone numbers you have should have a phone number table, something like: Create CustomerPhone (CustomerID int, PhoneType varchar(100), Phone(varchar(25)); Your table structure pretty much guarantees poor performance. But this might help a little if there's an index on the phone phone columns: SELECT CustomerNumber FROM Customer c where customer_id in ( select customer_id from Customer c where c.CustomerPhone = @Phone union select customer_id from Customer c where c.CustomerPhone2 = @Phone union select customer_id from CustomerTable2 c2 where c2.CustomerCell3 = @Phone ); That's three index scans and one PK seek. With no indexes this will be worse, though, and you probably can't avoid a table scan. ​
Thanks. Will get back to you in a bit.
More details would help, but here's a question: is the \[name\] column you're filtering on the same column that is used to join your tables? If that's the case, then the inner join is filtering out the nulls...it also explains why your query works when you switched it to a left join.
Yep. I know that the table design is not good. I'll try this and report what the plan looks like. Thanks.
If item.value is a string, you may need to put quotes around the value.
Thanks! Will try this! I got halfway to this idea yesterday minus the CASE.
I tried this yesterday and the optimizer did use the indexes to get rid of my clustered scan. The query ran three times faster but I was worried that I was overthinking it. Thanks. I want to rewrite it again today and look at the plans.
Ditto everything /u/wolf2600 said. Doing SQL that way is horrendous. That said, the reason the third condition reduces the number of rows is because without it Re2 is unconstrained. Like, before the WHERE clause, your subquery is effectively: SELECT * FROM Rating r1 CROSS JOIN Rating r2 CROSS JOIN Reviewer re1 CROSS JOIN Reviewer re2 Which yields 12,544 rows. There are 14 rows in Rating and 8 Rows in Reviewer, so that's 14 * 14 * 8 * 8 = 12,544. Then your first WHERE clause joins r1 to r2 on mID, effectively changing your query to: SELECT * FROM Rating r1 INNER JOIN Rating r2 ON r2.mID = r1.mID CROSS JOIN Reviewer re1 CROSS JOIN Reviewer re2 That returns 2,176 rows. mIDs 101 &amp; 108 have 3 reviews each: 2 * (3 * 3) = 18 and the other 4 mIDs have 2 reviews each: 4 * (2 * 2) = 16. 18 + 16 = 34. 34 * 8 * 8 = 2,176. Your next WHERE clause changes it to effectively: SELECT * FROM Rating r1 INNER JOIN Rating r2 ON r2.mID = r1.mID INNER JOIN Reviewer re1 ON re1.rID = r1.rID CROSS JOIN Reviewer re2 That returns 272 rows, because you only have 1 reviewer in re1 for each row in r1, but still have 8 rows in re2 for everything else, making it 34 * 8. The next WHERE clause further narrows it down by effectively making it: SELECT * FROM Rating r1 INNER JOIN Rating r2 ON r2.mID = r1.mID INNER JOIN Reviewer re1 ON re1.rID = r1.rID INNER JOIN Reviewer re2 ON re2.rID = r2.rID Which, because you now only have 1 reviewer each in re1 and re2, gets you back to the 34. But, without the last WHERE clause, you've got each row in r1 joining to itself in r2. The last WHERE clause effectively makes it SELECT * FROM Rating r1 INNER JOIN Rating r2 ON r2.mID = r1.mID AND r2.rID &lt;&gt; r1.rID INNER JOIN Reviewer re1 ON re1.rID = r1.rID INNER JOIN Reviewer re2 ON re2.rID = r2.rID Yielding 16 rows because you're only multiplying each row in r1 by the number of *other* reviewers for the same movie in r2. Instead of having 2 * (3 * 3) + 4 * (2 * 2) , it's now 2 * (2 * 2) + 4 * (2 * 1). The ORDER BY in the subquery is invalid, so we'll ignore that. Finally, by adding the WHERE on the outer query, you've effectively got: SELECT * FROM Rating r1 INNER JOIN Rating r2 ON r2.mID = r1.mID AND r2.rID &gt; r1.rID INNER JOIN Reviewer re1 ON re1.rID = r1.rID INNER JOIN Reviewer re2 ON re2.rID = r2.rID That removes the duplicates where it's comparing reviewers which were already compared in the opposite order (i.e., of A to B and B to A, exclude B to A). For mIDs 101 and 103 you've got 2 reviews by the same reviewer + a third, and the other 4 movies each have one pair of reviewers. So that yields 2 * 2 + 4 = 8 rows. Then you add the SELECT DISTINCT and that gets rid of the duplicates where it's joining review A^1 to B and review A^2 to B and ones where the same pair of reviewers reviewed multiple movies. Leaving you with 5 rows. Phew! Hope that helped! But, yeah... don't write your queries that way. Not only is it ugly and difficult to read, it tends to generate shitty query plans that are slow as molasses. Also, don't use TEXT for a field like a director or movie name. TEXT is for long strings (like more than 4,000 characters) and doesn't play well with a lot of things. Stick to VARCHAR or NVARCHAR (if you have foreign names) with however many characters is sufficient. For a those I'd expect something like NVARCHAR(128) maybe. Good luck!
Thanks. I started out by thinking about rolling the columns into a CTE and trying to short circuit the OR statements as you mentioned and then I got into a rabbit hole of worrying about why I didn't know how to do this and basically overthinking everything.
Your users should not be entering data directly into your tables in SQL Server, they should be either writing and executing queries, or using an application as an interface between them and the database. You don't "pizzaz a table" because your users are not directly interacting with them like they would a spreadsheet. You can add a **check constraint** on a field to restrict its values to a range of 1-5. Restricting it to a number happens automatically by virtue of defining it as a `tinyint`
Thank you everyone for the comments. I will rewrite my query a few times and see what the plans show. I got into this anxiety of realizing that I need a refresher on the best way to handle conditional statements from a performance angle when I inherit systems with less than optimal design. Really appreciate this community.
The other suggest about using UNION to remove duplicates sounds more elegant to me, but there are multiple ways to skin a cat and you won't know which is faster until you try both.
Another concern is whether SQL can parameterize the query. You'd obviously get one query plan per set of columns being updated, but if you also start getting a plan for every different id in " WHERE id = {}", then you can end up with a bunch of single use plans that start squeezing plans for other queries out of your plan cache. You also pay the overhead of getting a new plan each time the statement is run, instead of reusing an existing plan. Maybe not an issue on a quiet system with plenty of CPU/RAM overhead, but worth considering if your system is stressed.
\&gt; I switched to left join. No dice. I switched to left loop join and I get expect results &amp;#x200B; left loop join???
Is use mostly MSSQL and I have to write these clauses like: NOT( value IS NULL) Maybe give that a shot.
Probably horrifically inefficient, but another approach in T-SQL is along the lines of ...WHERE @PHONE IN (COLUMN1, COLUMN2, ...). Just throwing it into the mix.
Oh yeah, this is so much better! Thank you for this! I also see how illogical this way of solving the problem is. My first step of the sub query was to cross product everything which isn't very direct to the solution. Thanks again for this!
Change your schema so that each phone has its own row. Or You could create a temp table, insert all of the phones into that. Or Create temp table with customer id as a field. Then Insert into #customertable (Id) select customer from customerphone where field = @phone Repeat for each field Then select *, @phone from customer inner join #customertable blah blah That would do it quickly. And sorry it’s not all proper sql, I’m typing on a phone
Why? If you want to know what to call yourself to other technical people, just boil down what you do into 1 sentence. "I wright and maintain reporting as well as BA stuff" works. If you want to know what types of jobs will have similar descriptions, you are looking at BA and report writing/analyst. If you want to know what to call yourself to non technical people, just pick something you like. I disagree with the people saying BI Dev, as that implies a higher level of ETL expertise than you have mentioned.
I agree with what another user commented. Here's how you'd create the table. CREATE TABLE #PAZZAZ (ID INT, PRODUCT VARCHAR(32), RATING TINYINT, CONSTRAINT CHECK\_RATING CHECK (RATING BETWEEN 1 AND 5) ) &amp;#x200B; /TEST\*/ INSERT INTO #PAZZAZ VALUES (1,'PIZZA',6)
&gt; Why use ratings first instead of directly using the reviewers? It doesn't make any difference as long as when you're applying the join context, both tables in the context have already been specified.
Right, got it, thank you.
The value will always be 10 characters so the storage difference is mute.
You can add the condition from the WHERE in the JOIN instead. That will help. Let me know if you have questions.
We probably need to see the query itself to really help but I often use the isnull() function in my where statement instead. I've found I get more consistent results or rather more easily understood maybe.
If you're on Oracle, that approach is how they actually recommend for complex selections. Check out "16.5.3.3 Write Separate SQL Statements for Specific Tasks" https://docs.oracle.com/cd/E11882_01/server.112/e41573/sql_overview.htm#PFGRF94817
Thank you everyone for the feedback. I appreciate it. I found a work around. Was just checking to see if anyone had encountered this phenomenon.
What do you mean a ‘traditional’ date?
 RemindMe! One Week
I will be messaging you on [**2019-05-10 22:30:29 UTC**](http://www.wolframalpha.com/input/?i=2019-05-10 22:30:29 UTC To Local Time) to remind you of [**this link.**](https://www.reddit.com/r/SQL/comments/bk4xaw/what_is_my_job_title_based_on_what_i_do_bi_dev/emg1mqi/) [**CLICK THIS LINK**](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[https://www.reddit.com/r/SQL/comments/bk4xaw/what_is_my_job_title_based_on_what_i_do_bi_dev/emg1mqi/]%0A%0ARemindMe! One Week) to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete Comment&amp;message=Delete! ____id____) _____ |[^(FAQs)](http://np.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/)|[^(Custom)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[LINK INSIDE SQUARE BRACKETS else default to FAQs]%0A%0ANOTE: Don't forget to add the time options after the command.%0A%0ARemindMe!)|[^(Your Reminders)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List Of Reminders&amp;message=MyReminders!)|[^(Feedback)](http://np.reddit.com/message/compose/?to=RemindMeBotWrangler&amp;subject=Feedback)|[^(Code)](https://github.com/SIlver--/remindmebot-reddit)|[^(Browser Extensions)](https://np.reddit.com/r/RemindMeBot/comments/4kldad/remindmebot_extensions/) |-|-|-|-|-|-|
&gt; Was just checking to see if anyone had encountered this phenomenon. ... Your phenomenon is bad code, hence why people were asking for your code.
You don't **style** SQL what you are looking to do is part of UI/UX Design.
I'm sorry I meant an "apple date" I'm not sitting with computer so I can't say the exact language used but I'll get back in a couple hours
I'm sorry I meant an "apple date" I'm not sitting with computer so I can't say the exact language used but I'll get back in a couple hours
Try something like ... WHERE @Phone IN (C.Phone1, C.Phone2, etc). I find that works quite well. Obviously you'll still need the proper indexes.
It's always good put the conditions in the order from the more probable to happen to the less when you use OR. When you use AND, the order should be the inverse.
My idea would be to first create a table filled with the timestamps you're interested in. People have various approaches on stackoverflow. Look up calendar table or date serial for ideas. Then you should be able to left join this to your original table on time BETWEEN start_time AND end_time and group appropriately for your counts.
What’s this about?
Building on this, here's some simple T-SQL to illustrate: `declare @t table (userId int, start_time datetime, end_time datetime);` `insert into @t values (1, '20190501 01:00:00', '20190501 04:00:00')` `, (2, '20190501 02:00:00', '20190501 05:00:00')` `, (3, '20190501 03:00:00', '20190501 06:00:00')` &amp;#x200B; `select` [`h.hr`](https://h.hr)`, count(*)` `from (select 1 [hr] union select 2 union select 3 union select 4 union select 5 union select 6) h` `left join @t t on` [`h.hr`](https://h.hr) `between DATEPART(hour, t.start_time) and DATEPART(hour, t.end_time)` `group by` [`h.hr`](https://h.hr)
Manufacturer here: Most of us don't have dedicated Database people. or if we do, they're not interacting with the floor in enough capacity to really understand the data requirements. So their "veto" is not something that's acceptable or reasonable. If some IT guru is going to try to tell the site leader that we need to restructure our database for hours/days or take down the floor or re-do all of the manufacturing work instructions and manufacturing worksheet interactions with the database, the site leader is going to laugh at you. As a \*very\* self taught SQL person, "store data in violation of the first 3 normal forms" means nothing to me. Sure removing redundancy is good, but also you've got a lot of people who aren't database experts tasked with maintaining this stuff, so making it into a picture perfect database is pretty absurd of an expectation when you're inheriting a system from 3+ generations of self-taught coders. On some level, your understanding of and expectation of databases isn't realistic for every (or probably even most, tbh) site. What if there's not a clear list of all the colors each product can be in? There can be custom colors requested, or other constraints that make it difficult to plan out combinations like that. Also, if I sell it to you in sets of 8 (always), then for me, that's 1 (set). I don't care how many individual components make it up, it's a singular set. How you divide it up is up to you, but that's my base level because it's the smallest grouping of them I sell.
What is interesting to me is that I'm an "IT specialist" sort of, and I break normal form all the time to store data. The difference comes down to transactions. So many people just ignore that part of the process, and everything comes down to whether you are in an OLAP or OLTP environment. It sounds like these people are sending you data that they are recording that makes sense to them in the way that they are entering it. So write a python job, or something, to scrape these files to put the data into a proper form. But then once there, if you want to effectively and efficiently deliver it back to them you may very well have to break normal form. What you need to do is design a solution that gives you what you want, while them not having to change a single thing their doing outside of agreeing to certain technical requirements relative to the way you store data.
Do what works until it doesn't.
They are very explicitly told what our requirements are when they start seeking to us. They just don't give a fuck because they can get away with it
There's definitely a time And place to break the normal forms, such as when you're rarely going to be updating pricing information. Wholesale distributors need not apply.
I am using big query , what changes would I need to make if I am using standard SQL.
My point is that the answer to the problem comes from how they are capturing this data. It sounds like they are providing data to you in a form that they are compiling themselves, yes? So that's where you start.
Could you please also explain what the select statement is doing ? What are multiple union operations doing inside the select statement?
&gt; but you'd think that a manufacturer would want to uniquely identify every product, along with every color that product comes in They probably do, it's just of no use to you. Why would THEIR unique identifier for a product matter to you in the slightest? Most manufacturing companies would assume that your own Inventory Control has the ability to create and uniquely identify and keep track of incoming inventory. &gt; send shipping information based on the size and weight of an individual item because even though you sell that item to us in sets of 8, This isn't how logistics works. When the transporter of the product receives the product from the manufacturer to transport to your company - they usually will receive a Bill of Lading (BOL) documenting the units and total weight. This is standard practice pretty much globally. &gt; which means you bet your ass we don't want to violate one of the first 3 normal forms. You misunderstand what normalization is - normalization is a methodology for **storing data** not communicating information. &gt; just because your lazy ass doesn't want to store weight and dimensions for the individual item. They most likely do - but storing data, again, is completely different from conveying relevant information.
"Apple NSDate" is the exact language it uses.
&gt; but it's of no use to you As a result of their unwillingness to use product ids as unique identifiers we can only carry a single color of each type of item, and they don't pick which, we do, randomly, because that's the kind odd result you get when you ignore common sense and the direction of the wholesaler you sell to.
&gt; their unwillingness to use product ids as unique identifiers we can only carry a single color of each type of item What? Why? If a manufacturer makes a toy car in blue and another in black, to the manufacturer that’s a completely separate product. Why can’t it be a completely separate product in your inventory system? I mean, as an example - product identification for the products that our manufacturer Ed by the company I work for are based around location manufactured, machine manufactured on, color, and how it’s packaged. So let’s say I make a toy car in blue in Timbuktu on Machine A that’s packaged on a pallet of 1500 lbs each - this would be a completely separate product than the product made on Machine Z in the middle of Madagascar. It’s your inventory controls job to determine how they classify and uniquely store their inventory. If your system can’t handle this it’s no fault of the manufacturer, it’s a limitation of your own inventory system.
1500 lbs is 681.0 kg
Take a look at recursion.
I'd suggest running through a pluralsight crash course and a couple SQL Saturday tips &amp; tricks lectures. Eddie is an amazing SQL guy there at force. They have the biggest in the land last I heard.
CPU is in megahertz... That's millions Ram is flash... That's kilo-iops Storage... That's iops... No order of magnitude It's the SAN. Check for cx packet waits
I'd called it "SQL Reports Developer" or something like that. Source: just went thru a job search in semi-related area. Saw a bunch of job descriptions similar to the above. Usually titled "SQL Programmer" or "SQL Reports Developer" or something like that.
I've had to deal with start and stop in a time range before. Try something along the lines of: SELECT
That makes no sense, what do you mean by that?
 select @periodstart, @periodend, count(distinct user_id) from table where @periodstart is between start_time and end_time or @periodend is between start_time and end_time or start_time is between @periodstart and @periodend or end_time is between @periodstart and @periodend; Basically, if the period overlaps with the
Install sql developer for free, download a free populated database to test on. Start learning what works and what doesn’t. Trouble is, there isn’t always a right situation to use, for example, a temp table. Books online and places like Brent Ozar give some advice though for certain things. I’d say, write the queries and then tune them afterwards. It’s a skill and art.
No. Not B. B isn’t even in my mind. If it’s scrambling data that’s huge amounts of IO all at once and maybe even one long transaction. Definitely needs to hit disk for that and boom. IO bottleneck. Look at wait stats and you’ll see
This sounds like a great example of when to use an outer apply. After your FROM , you OUTER APPLY (SELECT TOP 1 fieldname1, fieldname2, etc. FROM second Table b WHERE b.id = a.id ORDER BY value that puts your selection into the correct order) x Then select the fields you need from x (give it a more meaningful alias than x in the real query) and you should be good. Sorry for formatting as I'm on my phone. Let me know if you have further questions.
According to very compelling answers given on [this question on stackoverflow](https://stackoverflow.com/questions/3800551/select-first-row-in-each-group-by-group), it appears that one of the most obvious and fast ways to do a lateral join, or in more standard SQL terms, something like: ``` SELECT t1.whatever (SELECT TOP 1 info FROM B WHERE B.identity=t1.fooId and B.date &lt; A.some_date) AS info FROM t1 ``` it's counter-intuitive that it's fast, and maybe it's only fast on postgresql, but it's worth a try.
I wasn’t able to process this when I first looked at it, but with fresh eyes, you’re right about having a statement per parameter, rather than all inclusive. So the OP has the multi query problem, which is easy to solve, but only if parameterization isn’t done. They use param length and iterate, adding to the @paramClause. While i&lt;paramLength ; When i= 1 @paramClause = SET [firstParam] = ‘suppliedValueByUser1’ i++; with every loop @paramClause += ‘ AND [secondParam] = ‘suppliedValueByUser2’ That yields a single, concise statement. But DOESN’T protect from injection. Can variables, and these parameters be created dynamically? For each loop, a new variable is created using the value of i, and it also uses that count to loop through: Parameters.AddWithValue(‘@param{i}’, [suppliedValueByUser{i}]);
Yes but with an order by B.date DESC so they get the most recent. I like to use row numbers to indicate the most recent record in a derived table and then filter on the row number outside the derived table for this kind of thing, but yeah if they only need one value from B, a subquery would probably be easiest and most efficient
I think you’re stuck on the PK/FK IDs being the only condition allowed in a JOIN, and needing only the most recent one. If I’m understanding your problem right, you can use multiple join conditions and a windowing function to save the day. This is a simple solution: ``` SELECT * FROM ( SELECT a.*, b.*, ROW_NUMBER() OVER (a.id ORDER BY b.date DESC) as rownum FROM table_a AS a LEFT JOIN table_b AS b ON a.id = b.a_id and a.date &gt; b.date ) t WHERE rownum = 1
Makes sense. I already have a database set up for my course with HR data. I'll take a look at that author you recommend, thanks!
Join the table to itself where one table only has letter a and the other has letter b
could you please explain like I am five (well I am actually even way younger in term of coding)?
Select a.id_something, a.numeric_data - b.numeric_data From table a Join table b on a.id_something = b.id_something and b.letter = ‘b’ Where a.letter = ‘a’
Not the original commenter, but I think I understand what they’re suggesting. You want to get the user_id, something_id, and (b-a). The way to do that is to create a table with rows that look like user_id, something_id, a, b, and (b-a), and the way to do that is to self join the table. You’re going to make two copies of the original, one that has user_id, something_id, a, and another that has user_id, something_id, b, and then join *those* on user_id, something_id. Like so: WITH a_side AS ( SELECT user_id, something_id, a FROM my_original_table), b_side AS ( SELECT user_id, something_id, b FROM my_original_table) SELECT user_id, something_id, b-a AS subtraction FROM a_side JOIN b_side USING (user_id, something_id); Alternatively, more compressed: SELECT user_id, something_id, b.b-a.a AS subtraction FROM my_original_table a JOIN my_original_table b USING (user_id, something_id); In each case, you’re using the same information, but in order to connect the pieces, you’re calling the different subsets by different names. I prefer CTEs (the first way, with the parentheses) to the tighter notation below, but that’s mostly about legibility (usually by future me, who will by then have forgotten what is in my_original_table). If your dialect doesn’t have USING, you can join in the usual way: JOIN table_a TO table_b ON table_a.field=table_b.field AND table_a.otherfield=table_b.otherfield. Feel free to dm me if this is unclear or needs more troubleshooting.
so it must be A=storage array?thnx
so it must be A=storage array?thnx
You might be better suited asking in a more DB side sub like r/Database
Ok?
Yes !
Is that a classroom test or regression test, because tests in the RL look just like that. Sounds like your prof is having you do his work😂😂😂
thanx
I don't think there really is. One approach I've seen to this problem is writing your own functions for multiple db engines. You use those functions in your code instead of the native ones, and then you can port your SQL scripts to another db engine with the same functions built in. The obvious problem with that solution is any functions you write aren't going to be as efficient as native ones.
Hadn't thought about that better get a cut haha. At least I know I'm getting the real deal. Written Business Rules, an ERD, indicated 3rd NF on all entities, did a CRUD Diagram with Transaction analysis, a Physical design and now the Test Cases. All I've got left are to write some queries and I'm done. Put 30 working hours into it so far.
In RL those are a part of SDLC. They go by various names but the general structure is called a Requirements Document, Software Design Specification/Data Design Specification, Test Specifications Doc, and Implementation Document. Sometimes there's a Post Implementation document, but usually that's in an Operations Manual.
So you suggest we either use an optional value (color) as part of the primary key or force the primary key to be reliant upon a non-key value (color)?
My suggestion would be to make some arbitrary key so it doesn’t matter. This is why using real world values (like the name of a product) as a key is a bad idea.
&gt;Is there anyway to speed up the the query without adding RAM Maybe. Post the schema (including indexes), the output you get from the Messages tab of SSMS when you use `set statistics time,io on` and query plan. Since you're on AWS, you can scale your instance up and down pretty easily, can't you? Scale it up and see if it works better. If it doesn't, scale it back down. Throwing hardware at the problem may or may not be the solution, it depends upon why the query is slow in the first place. A baseline Azure SQL Database (lowest possible spec, single DB) starts at roughly $5/month.
Depending on what data you're playing with I'd imagine you can optimize a bit; specifically looking at indexes on your join keys and where clauses... ...But on the topic of a cheap database for personal development I have a FreeNAS server in my basement with a database in a Jail (VM like environment if you're not familiar with FreeBSD). It has beastly specs with no hosting costs; but the downsides are it consumes home internet bandwidth and has no uptime guarantee. But for what I use it for there's no better solution. Setting up a server at home may or may not be a good idea for your use case. Now for things that require a bit more dependability there's a bunch of solutions depending on what core technology you want to stick with. AWS, Google Cloud Services and Azure are the major competitors; but they'll have pretty comparable pricing. Though I *think* Google will give new accounts a [$300 credit](https://cloud.google.com/free/docs/gcp-free-tier) for the first year if you want to play around with their platform without committing financially. That would afford you $25/mo worth of services... but would inevitably come to an end.
I am not an expert on the hardware aspects, and not really on cloud based stuff. I think answering your first question might need a little more input? What is the data, how often does it run, what are the results typically from those two tables, does the data change a lot? If the data doesn’t change much, then maybe you can chunk it and not re-match existing. From my understanding though you are really asking the hardware part and that is amazing to see if the difference. I was reading an article about Contains vs Like vs Exist and specs and they did not add up to my own results, which I assume hardware and SQL version are variables are part of the equation. Azure and others (Google and AWS) are pay as you use. Again, not an expert but you will need to know your typical average to see how much it will cost you. I am not positive if it is pay per run or pay per time of the run. So even if it is say a dollar a run, eventually you are going to break even and go below than upgrading the memory I assume? Maybe some others can help here My other suggestion, is why not maybe MySQL? I believe you can purchase a website domain and have a backend SQL for pretty cheap. I had bought one from GoDaddy while in my undergrad to house examples, resume, homework samples from web development. You might be able to set up a simple site or Word Press with a MySQL and it be as fast or faster for maybe cheaper, and have more utility as well?
You could try a partitioning scheme where you would partition both tables by the same (JOIN) key hash. That way you won't be joining 2 big tables, but multiple times joining small tables, since joining tables is O(n log(n)) in the best case scenario, and O(n log(n)) is bigger than O(a * (n/a log(n/a))) = O(n * log(n/a)). It also consumes far fewer RAM. But it might be not the RAM but the IO. 13M rows usually means a lot of data to read from disk in the first place, and the data has to be read either way.
It helps if you provide some sample data: truncate table active_users; insert into active_users values (1, '2019-02-01', '2019-02-01 23:59:59'), (1, '2019-02-02', '2019-02-03'), (1, '2019-03-10', '2019-03-10'), (2, '2019-02-02', '2019-02-07'), (3, '2019-01-15', '2019-03-31'); truncate table periods; insert into periods values ('2019-01-01', '2019-01-02'),--should return 0 ('2019-01-01', '2019-04-15');--should return 3 ('2019-01-15', '2019-01-15 01:00:00'),--should return 1 ('2019-02-01', '2019-03-15'),--should return 3 This is a tough problem, intervals are hard. This should work: select p.period_start, p.period_end, count(distinct a.user_id) as active_users from periods p left join active_users a on p.period_start between a.start_time and a.end_time or p.period_end between a.start_time and a.end_time or a.start_time between p.period_start and p.period_end or a.end_time between p.period_start and p.period_end group by p.period_start, p.period_end order by 1, 2;
Select columns you want instead of *, index joining columns and where predicates, investigate using columnstore indexes. Try using sentry one plan explorer to get an idea of the execution plan.
Yep I agree select only columns that you need. Also if you use indexes it will in theory that will make the query faster, but there is also the thing that if you use indexes, inserting and deleting will be a lot slower because indexes have to update. In my opinion try optimizing the query first and if it doesn't work than buy more Ram.
Check that the indexes in place are actually being utilized properly/as intended. I've seen a lot of cases where an index is added, but the queries being run haven't been written in a way that allows the optimizer to select the index. You should check the execution plans of your queries to confirm this, either way.
Here's the link to what my query looks like. Again, thank you guys! [Screenshots](https://imgur.com/a/uVLzpH7)
Thanks for the incredibly quick response, screenshots can be found above. Curious of your thoughts.
Perhaps this my downfall, I haven't designated PK/FK's yet. Will that impact performance? I'm intrigued by your FreeNAS. I'll DM you in a bit.
You and u/SandManSRB seem to really know what you're talking about, I'd be interested in hearing you're feedback from the screenshots I just posted.
u/jezter24, thanks for the quick response! I was not expecting you guys to respond as thoroughly and as quickly as you did! I attached some screenshots that may be able to explain the problem better than I could!
While I appreciate your response, you appear to be way more intelligent than I... You lost me at key hash... :-(
Great point! Thanks a ton for your feedback! I'll check as soon as I get behind my screen again and report back with findings.
You missed the query plan in the screenshots.
i imagine you could export all of your sprocs and views, and then providing you knew very specific examples of what needs to be changed then you could iterate through each one making changes to change something such as `where $ datefield$ &gt;= getdate()` into something like `where $ datefield$ &gt;= today()`. It's usefulness would be entirely predicated on how consistently the code base is, how many specific changes you need to make, etc. The Python itself would be pretty simple, but you'd then have to test each individual piece of code... although I imagine you'd have to do that anyway. If the changes are more complex such as CTE's no longer being allowed and needing to be converted into #tables, or subqueries... then it would become a much more complex piece of code. Possible, but probably much more difficult/time consuming than simply rewriting things yourself, and probably the type of thing you would want to sell as a solution once built, so not something you just whip up on a weekend.
Thanks for teaching me what this is....... :-) [https://imgur.com/a/IgueFoE](https://imgur.com/a/IgueFoE)
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/5ynNouG.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme)
@mattmc3 Has already given a solid answer. However, just for fun heres another way you could do it: &amp;#x200B; &gt;SELECT [a.date](https://a.date), a.\[identity\], [b.info](https://b.info) &gt; &gt; FROM table\_a AS a &gt; &gt; LEFT JOIN table\_b AS b ON [a.id](https://a.id) = [b.id](https://b.id) &gt; &gt;AND [a.date](https://a.date) \&gt; [b.date](https://b.date) &gt; &gt; WHERE NOT EXISTS (SELECT 1 &gt; &gt;FROM table\_b AS b2 &gt; &gt;WHERE [b.id](https://b.id) = [b2.id](https://b2.id) &gt; &gt;AND [b2.date](https://b2.date) \&gt; [b.date](https://b.date))
Why not wrap the delete in a begin transaction statement ?
I'm a student studying bussines Informatics and we had 2 courses on databases and database design. So I have theoretical knowledge and some practical. I think you should make a third table that will denormalize your database (temporarily) but you should also make a after insert trigger. You can make 2 kinds of tables on could wher you fill it up whit the sum and count and then make a trigger that updates it when you insert something and then you preserve the consistency of your database. The other kind could be a prejoind table where you only use attributes which you need. To be honest I didn't ever use this method but I herd from my proff that it could work.
Now the fun part is learning how to read them!
In even moderately ideal circumstances, that would be a great idea. The core issues here run deeper than what I mentioned. The short explanation is that because of the issues i mention below (and many others), creating our own unique identifier isn't a good idea. It would make communicating about product IDs more complicated, and it would make addressing conflicts (that I mention below) more complicated while only addressing one of the items I mention below. Currently, we attach a string (that is unique to each vendor) to each product id in order to make each item globally unique. It is supposed to work. Your idea is great, but it would make an already shitty situation even more complicated while only addressing one problem. Plus, when a retailer sees one of our products available for purchase, they need to be able to look up these products. So, when I communicate with our sales/purchasing/accounting/customer service departments, I need to communicate by referring to the vendor-provided id number, but when updating, adding, checking products stored in the database, and verifying the products/ids at any point in time, I need to refer to the internal unique ID (while remembering which vendor ID it is associated with)? That's a lot of overhead to address a problem caused by vendors refusing to abide by the guidelines we set forth when they agreed to sell to us. One of the core issues we face as a wholesaler is that on a very fundamental level, we cannot trust vendors to supply us with correct and proper information. I believe that at least 20% of my job consists of addressing conflicts with pricing and product details that are caused by vendors not properly keeping us updated about pricing, descriptions, product status, and shipping information (to say nothing about the normal differences in the formats we use when dealing with data). let me also put aside the normal variations in pricing, product categories, contracts, purchase lines, and sales lines to give you a list of the types of problems I saw vendors create in just the first few months at this job. This list is by no means exhaustive: - vendors changing the product id numbers without telling us at all - product ids are too long. This is 110% the vendor's fault because the allowed product id length is reasonable and the vendor is told about this when they start selling to us. - product ids have invalid special characters in them, such as non-printable ASCII characters. They end up in our system in such a manner that they can't be properly searched for or used by our purchasing/sales teams. - vendors discontinuing items without telling us - vendors giving us one set of pricing information and invoicing us for a different price because they don't keep track of the pricing that they have actually give us. - vendors superseding items (discontinuing one item and replacing it with a newer item) without telling us. This is a distinct issue from discontinuing items because the system treats it very different. - vendors responding to our inquiry about item status without giving the inquiry due diligence and they end up responding with incorrect information about whether they still ship out those items. There are more ways vendors' failure to communicate can cause problems, but these are the most relevant in this case.
Basically you need a partition key that will be common for the same values. I don't have experience with it on MSSQL, but I know that there is this feature there for sure: https://www.sqlshack.com/hash-partition-in-sql-server/ I do it in postgresql manually. Imagine you have IDs from 1 onwards. A hash function would ideally compute the hash, but let's simplify that and make a function that returns `id % n` where `n` is the total number of partitions you want to have. Let's say `n`=4. Such a mapping would mean that partition ID `p` would be: ``` n p 1 1 2 2 3 3 4 0 5 1 6 2 7 3 ``` And since it's the same ID in both tables, and data from the tables are distributed exactly the same across 4 partitions, such as row with `id=6` will be in partition 2 for both tables, a typical join would be: ``` CREATE TABLE t1 (id INT, partition_key INT) PARTIION BY LIST (partition_key); SELECT whatever FROM table1 t1 JOIN table2 t2 ON t1.id=t2.id AND t2.partition_key=compute_chunk(t2.id) WHERE t1.partition_key=compute_chunk(t1.id) ``` where `partition_key` would be a
Since you have so many rows you might look into partitioning. You can partition the table, so then when your query goes to look up a row it will have a smaller data set to query.
[w3c](https://www.w3schools.com/sql/)
&gt; What? Why? If a manufacturer makes a toy car in blue and another in black, to the manufacturer that’s a completely separate product. &gt; &gt; Why can’t it be a completely separate product in your inventory system? technically it can, but the problem with violating the normal forms in the way you as is that a primary key ends up relying on a non-key column, and that creates more complexity in the application that resides outside of the database structure. It's a bad, bad idea. If we allow this violation of the normal forms, what else are we going to allow? What else will we allow to create more complexity and more vulnerability towards update and delete anomalies? When does it end?
You can speed it up by adding criteria in the join instead of the where. This way it reduces the the size of the table before joining it rather than reducing the size of the results after the join.
It looks like the query is doing a full scan on the player table. My guess is it's the 'where nickname = whatever'. Add an index on the nickname column, and obviously any foreign key you're joining on.
In your PlayerList table, you have an index on TournamentID. Try adding the Nickname column to that index as part of the INCLUDES section.
Not sure if I have a correct understanding of your problem or not, but assuming I do, why not have a log table and have a starting and ending record for each segment in your work flows. You'd create the table that would have, for example: log\_id, log\_datetime, workflow\_name, task\_name, task\_description. You then modify your workflows/segments so that a new record is created each time a workflow or segment kicks off and one when it ends. You then use the DATEDIFF() function to calculate run times of each segment/workflow.
Can't use DATEDIFF because it incorporates weekends and holidays. This will screw up the results by overstating the time a project took.
if this is something that you need to do repeatedly... create a view with schemabinding for NickName, SUM(), COUNT() from PlayerList... create a nonclustered index on the view. then, instead of querying the table, use the index instead.
You could keep a table of just holidays and subtract the number of weekends and number of holidays in the range from the date diff.
You think this is something I should do in addition to the partitions?
Correct answer right here.
This is a pretty standard data warehousing problem. The typical answer for this would be to provide a date dimension with one record per day. In that dimension, you could store the number of work seconds per day. Some days the value is zero (weekends and holidays). You could also store the start and end times for work hours making it easy to calculate the first and last day times. Then, you can calculate the TAT by joining your record to the date dimension, and using a simple SUM(), which computers and databases are incredibly fast at doing.
That's exactly what my data is. I'm trying to calculate the time in between each only based on business hours (8am to 9 pm Monday to Friday). If something gets started at 8:45pm and ends 8:15am the next day my TAT would be half an hour, or roughly .04 of a day.
So if I join on the day I'm still missing out on the partial day - if a process runs for 45 minutes, joining on a day won't work. What you're saying about adding start and end times makes sense but I'm not seeing how joining to day would calculate TAT that's less than a day or runs part of one day and part of the next.
because this is Database Development, stay tuned for SQL.
Partitioning is only given performance benefits in enterprise edition. You still get maintenance benefits, but not partition elimination... check the ms docs to confirm.
You still have to calculate the seconds for the start day and end day, and then use your join table to get the sum of the seconds in between those days. It’s a three part problem. For one that’s 45 minutes on a single work day, there is no middle or end. You can make this work with three queries - one for each calculation - joining to the date dimension.
I can’t see screenshots clearly. For the SQL, maybe attach as code and the execution plan as an image
yeah, if you can create PK/FKs on the joined fields then on any fields used as major filters it should really boost performance. That's a lot of rows, but a run time over a minute seems like a bit much.
Relevant Brent Ozar from last month. What do things cost? https://www.brentozar.com/archive/2019/03/pop-quiz-what-do-these-things-cost/
The active user count goes up by 1 at each start\_time and down by 1 at each end\_time. Then prefix sum.
I suggest you find a function that you can commit to the server and generate that for you. I found a function online where you could add the time work starts and ends and it outputted the result. My Google Fu is weak today as I'm from my phone but the search would be "SQL server working time between two dates"
!remindme one week
RemindMe! One Week
I will be messaging you on [**2019-05-12 06:35:41 UTC**](http://www.wolframalpha.com/input/?i=2019-05-12 06:35:41 UTC To Local Time) to remind you of [**this link.**](https://www.reddit.com/r/SQL/comments/bks1c8/anyone_calculating_a_turnaround_time_for_a/emjns4d/) [**CLICK THIS LINK**](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[https://www.reddit.com/r/SQL/comments/bks1c8/anyone_calculating_a_turnaround_time_for_a/emjns4d/]%0A%0ARemindMe! One Week) to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete Comment&amp;message=Delete! ____id____) _____ |[^(FAQs)](http://np.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/)|[^(Custom)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[LINK INSIDE SQUARE BRACKETS else default to FAQs]%0A%0ANOTE: Don't forget to add the time options after the command.%0A%0ARemindMe!)|[^(Your Reminders)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List Of Reminders&amp;message=MyReminders!)|[^(Feedback)](http://np.reddit.com/message/compose/?to=RemindMeBotWrangler&amp;subject=Feedback)|[^(Code)](https://github.com/SIlver--/remindmebot-reddit)|[^(Browser Extensions)](https://np.reddit.com/r/RemindMeBot/comments/4kldad/remindmebot_extensions/) |-|-|-|-|-|-|
Someone just doing it for you would help contribute to the growing number of ill-equipped people claiming to have the relevant skills. They would be doing you a disservice. If you *can't* do it, perhaps now is the ideal opportunity to choose an easier subject. tl;dr just do your own fucking homework
I'd take a look at this post. Should hopefully be able to help. https://stackoverflow.com/questions/5274208/calculate-business-hours-between-two-dates
That's funny, our current version is the top answer copy and pasted. It's fine in small quantities but a million or more of them and it runs very slowly. The code is from 2010 so wondering if there's a better way to do it on newer features of SQL server.
Yeah that's one way to go about it. But due to the efficiency issue pointed out by you, I'd not take this approach. Rather change the syntax to native
Yes this is kinda the objective. The python might be simple for one who's adept at it, in which case it could be a weekend's job. I'm not so good at it so looking for some existing code
Sounds like you’ll need to use a windowing function like row number in a sub query. The row number function will assign an ordered number to the data set in whichever way you order it by. It needs to be in a sub query because you can’t put windowing functions into the WHERE clause. So even though you could get your row number you wouldn’t be able to ONLY get specific ones.
Does your SQL flavor support `ORDER BY whatever LIMIT offset,count`?
how about limit offset?
There is no "middle" of a table. You need to describe the logic that defines how you're choosing these particular records.
Right, like if there is an auto-incrementing IDentitiy you could grab count(*) and the max value to try and determine the middle. It depends on how important it is to be precise and what the true middle means to you.
Nevermind, I just found out about "limit"
Not sure if newer versions provide anything more efficient, but another article that might help. We've had to solve this issue internally in a number of different ways depending on the use case, but this article helped me. https://www.sqlshack.com/how-to-calculate-work-days-and-hours-in-sql-server/
Thanks. I think a combination might work - datediff for the seconds within a day and then joining for the full days. Maybe moving it out of a function might speed it up also. It's a tricky one to do at scale.
In TSQL, you have [OFFSET and FETCH](http://www.sqlservertutorial.net/sql-server-basics/sql-server-offset-fetch/). That's how you can return paged results. So, you can do something like this, and change the PageNum variable to get different paged results, and change the RowsPerPage variable to change how many you return. Or you can do some math to figure out where the middle of your table is: DECLARE @PageNum INT = 0; DECLARE @RowsPerPage INT = 10; DECLARE @OffSetIndex INT = @PageNum * @RowsPerPage; SELECT * FROM Orders ORDER BY DateCreated OFFSET @OffSetIndex ROWS FETCH NEXT @RowsPerPage ROWS ONLY
Records are stored in a table unordered, they're just a "set". So even if you use `limit` to only return 5 records, unless you use `order by`, you could get 5 different records every time the query runs.
So, I know you said you *don't* want a create a calendar table that lists all of the business minutes, so I did that for you and wrote a new function to calculate business minutes and compared it with the WorkTime function you already use. [They are no different](https://imgur.com/a/y4fn2aS). If anything, in my tests, the worktime function executes faster. Additionally, using that table to join on the set and calculate minutes seems to be more resource intensive than using the function. In the end though, you might get better performance results using something else to calculate the business minutes and take that load off your SQL server (if you're using a prod server to run your stats). Otherwise, you might want to consider setting up an [OLAP database](https://www.datawarehouse4u.info/OLTP-vs-OLAP.html) server to run these stats. Once you start growing very large, using your prod OLTP server for reporting and KPIs becomes a huge performance hit. Assuming you haven't already, write some ETL jobs and drop it in an OLAP server for much more performant KPI queries. OLAP servers have much better response times than your OLTP server for production.
Unless it needs rows from all the partitions. In that case, we've made the problem worse. Rarely is partitioning the answer for poor query performance. https://www.google.com/amp/s/www.brentozar.com/archive/2012/03/how-decide-if-should-use-table-partitioning/amp/ It's better suited for speeding up ETL scenarios. When you load tons of data and switch partitions. For query performance issues, exhaust all of the usual suspects first (query plan, indices,table structure, etc ..) before reaching for partitioning.
I was just reading about Azure having double digit growth, but AWS has a huge chunk of the market. A quick search found this: https://www.canalys.com/newsroom/cloud-market-share-q4-2018-and-full-year-2018 As for cloud, there is software as a service, infrastructure as a safer vice, and platform as a service. As these gets cheaper, they grow. Forbes had a good article: https://www.forbes.com/sites/louiscolumbus/2018/09/23/roundup-of-cloud-computing-forecasts-and-market-estimates-2018/#249fdf86507b I would love to see more AWS and Azure myself. Watch Azure YouTube videos of ELT, Data Lakes, and their Data Studio and Factory looks great.
I stack overflow a lot. To me a “programmer” is not someone who reads a book anymore and can just code. You need to Google, Stack Overflow, and then see what they are doing and make it work for you. We use JSP at my work and I really don’t know it, but I can take a previous used one, see how they are doing things, and make it work for myself.
Poorly phrased questions suggest poorly formed thinking.
First do a cte using rownumber(). Then select items where rownumber &lt;= max(rownumber)/3 and items where rownumber &gt;=max(rownumber) - max(rownumber/3 Maybe something like that.
Comments are not loading for me right now, someone tell you it's a HEAP yet? And mention Covering indexes?
Hey there, welcome. If you’re using PL/SQL, you’re most likely joining us as a newish ORACLE Developer. In that case, the free IDE application you’re looking for is Oracle SQL Developer. https://www.oracle.com/database/technologies/appdev/sql-developer.html It’s a free, and quite robust application that runs primarily on Java. Here is one of the primary knowledge based on PL/SQL: https://www.oracle.com/database/technologies/appdev/plsql.html I’d highly recommend learning about and using LiveSQL, it’s a free Oracle 18C Database that you can use to create your own schemes, load data, and essentially write triggers, functions, views, price, etc all from the comfort of a web browser. No licensing needed. Oracle also has a dev gym, which gives you free courses at your own pace. PL/SQL is programmatic vs Transactional, so there are quite a few differences (logical and syntax related) from the vastly popular T-SQL (MSSQL). Hope you’re able to grow your Development career, and we are all around to help.
Have you considered using a sequence object, and building your value using the next vale from the sequence? Also, why is this an nvarchar, rather than a varchar (or if it's always a fixed number of characters, just a char)?
Not such specific language, can you elaborate a bit more in HEAP?
Hopefully your SQL dialect has a way of putting quotes around the table name or some syntactical operator to use so the database interprets it differently than the function.
Put double quotes, backticks or square brackets around it - this depends on which platform you're using.
This and why would you use those as table names? At least add something to it. Left_Stuff, tblLeft, or etc.
Can you explain why you use a CTE instead of a derived table? Should I use always CTEs instead of derived tables? Thanks!
Sorry man, the next you've pasted doesn't make sense, I can't tell what the orders of the roes and columns is
&gt; Now, Is there a way to make a table ("final") like this (place the value of colD at the first appearing row of each group)? just for reference, here is what you said you wanted, reformatted so it's easier to read -- ColA ColB ColC ColD 2018 aaa 100 400 2018 aaa 100 2018 aaa 200 2018 bbb 100 200 2018 bbb 100 2017 bbb 300 600 2017 bbb 300 there's a problem with your requirement -- there is no "first appearing row of each group" because rows in a table do not have any sequence you can, however, put the group totals on *every* row of the group SELECT s.ColA , s.ColB , s.ColC , t.ColD FROM Source AS s INNER JOIN ( SELECT ColA , ColB , SUM(ColC) AS ColD FROM Source GROUP BY ColA , ColB ) AS t ON t.ColA = s.ColA AND t.ColB = s.ColB which gives ColA ColB ColC ColD 2018 aaa 100 400 2018 aaa 100 400 2018 aaa 200 400 2018 bbb 100 200 2018 bbb 100 200 2017 bbb 300 600 2017 bbb 300 600
Select source.colA , source.colB , source.colC , aggregate.colD FROM source JOIN aggregate ON source.colA = aggregate.colA And source.colB = aggregate.colB
Actually it looks like one of the other commenters said this with the aggregate as a subquery in the From clause. That's probably what you want.
My textbook is "Oracle 12c: SQL" by Joan Casteel &amp;#x200B; From my textbook: [https://i.imgur.com/v4GXQvZ.png](https://i.imgur.com/v4GXQvZ.png) &amp;#x200B; "After examining the E-R model in Figure 1-4, you should have noticed the two many-to-many relationships. Before creating the database, all many-to-many relationships must be reduced to a set of one-to-many relationships, as you learn in “Relating Tables in the Database” later in this chapter. Identifying entities and relationships in the database design process is important because entities are usually represented as a table, and relationships can reveal whether additional tables are needed in the database. If the problem arising from the many-to-many relationship in the E-R model isn’t apparent to a designer at this point, it will become clear during the normalization process." &amp;#x200B; ... &amp;#x200B; "**A many-to-many relationship can’t exist in a relational database**. The most common approach to eliminating a many-to-many relationship is to create two one-to-many relationships by adding a bridging entity." &amp;#x200B; ... &amp;#x200B; Further explanation: [https://dzone.com/articles/how-to-handle-a-many-to-many-relationship-in-datab](https://dzone.com/articles/how-to-handle-a-many-to-many-relationship-in-datab)
See my comments from this post the other day: https://old.reddit.com/r/learnSQL/comments/bkfg05/biggest_difference/
Dumb ass memes. Hooray.
Memes : I am inevitable
Thanks. I hate it.
What if I had unique numeric id's in the original table? Would this make it easier?
Shame, the internet must be a horrible place for you if you want to avoid memes! Thanks OP got a good chuckle there.
I would highly recommend looking at the Azure MS training exams. There are a few currently in Beta and will be released shortly. The Microsoft Exam Refs are also a great place to start will learning official best practices. Microsoft launch the exams in beta so that authors can start writing these exam ref books at the same time. You will notice the exam ref books on Amazon launch soon after the exams exit out of beta. They could be touch and go I understand as it all comes down to the author writing it but they are thee most accurate and thee most aligned to the course as possible. Other sources are Udemy and Pluralsight with the latter being slightly more “official”. I’m in the same boat as you and busy up skilling myself before our company enforces all consultants to be somewhat cloud certified. Iv preordered the books on Amazon so long but doing some Pluralsight courses (we thankfully have a company membership) in the interim.
If you or someone you know is contemplating suicide, please do not hesitate to talk to someone. **US:** Call 1-800-273-8255 or text HOME to 741-741 **Non-US:** [https://en.wikipedia.org/wiki/List_of_suicide_crisis_lines](https://en.wikipedia.org/wiki/List_of_suicide_crisis_lines) --- ^^I ^^am ^^a ^^bot. ^^Feedback ^^appreciated.
I prefer CTEs for legibility and extensibility. I find them a lot easier to read when I go back and look at older stuff I’ve written, and if I need to extend or elaborate on my query, I find the concepts easy to call on. It’s probably a hangover from learning Python before SQL; CTEs feel more parallel to the way I would define and then later use classes and functions in Python. There can, however, be performance costs in queries in which you call the same CTE multiple times, because it will scan the CTE-generated table each time separately. If performance is a priority, you may be better served by derived tables.
Bad bot
nope, it wouldn't what's wrong with having the aggregate on every result row? that way, no matter which row you think is the first in each group, the aggregate will be there, and you can just ignore it on the other rows
maybe start with what you have done so far for those 4 questions, or what your thinking is on how to answer it?
Look at lag and lead functions to get either the previous or next sleep time. You can then datediff between them to get the hours.
Unless you explicitly add 'ORDER BY' to your query the order of rows is not guaranteed.
So then it's random how it's ordered? Because you can't add an ' order by' when creating a view
 SELECT p.id, case when o.order\_id is null then "No Match" else "Match" end as order\_table\_match\_flag FROM product p LEFT JOIN orders o on p.id = o.product\_id ~~AND o.order\_id in (2,3)~~
The question is basicly how can i list several data inserted into the table in one column
Thanks this is very helpful. However, it's also providing a order\_table\_match\_flag match in cases where the match is for orders other than #2 or #3. If I leave the 'AND' clause that you crossed out though, it doesn't.
Sorry, you actually need to add the ORDER BY when selecting from the view. Such as... SELECT * FROM MyView ORDER BY EmpID
Oh I thought u stated u wanted to see all matches....not just I'd 2 and 3
Can be done without subqueries, using a window function. SELECT Name, SleepTime, Lead(SleepTime,1) over (partition by Name order by SleepTime) Try that and see what it gets you. You may need to tweak the partition depending on your data. Window functions are very useful once you get the hang of them.
You can also just use SELECT TOP 100 PERCENT so you don’t have to make a number up.
Something like this: SELECT p.id, o.order_id, o.max_product is not null as at_least_one_order FROM product p LEFT JOIN ( select order_id, max(product_id) as max_product from orders o group by order_id ) o on p.id = o.product_id; It's unclear to me if you want the "indicates that there was at least one match" refers to all orders, or just the two from your initial join condition. If you want to limit that flag to only those orders add a `where order_in in (...)` to the derived table (aka sub-select)
Thank you for answering! I saw that last hack somewhere when looking for this. Maybe I phrased my question not quite right, as English isn't my first language, but I was wondering why, without that 'hack', and with completely the same code, the order would be different on different pc's
This sub is not going to do your homework for you. Try it yourself, ask specific questions when you get stuck.
Unless it's in a query for output/reporting, you don't. Comma-separated values in a table breaks all manner of rules &amp; conventions for a relational database, and makes querying that data a big mess later.
Sounds like Group_Concat() is the function you’re looking for.
Partitioning isn't a performance feature, it's a management feature. It can help with certain operations, like loading or dropping large amounts of data very quickly, but it generally is not useful for speeding up queries. Especially when you're only operating on 13M records. That's nothing. People don't usually start partitioning until they get into 10X that or more.
If you're using SQL Server, you can do something like this: `select` `stuff((` `select` `',' + songs` `from table` `for xml path('')` `), 1 ,1,'')` You use for xml path clause to get it into one row and use stuff to remove the first comma.
If you are asking if you can design a table like that, you can but it will be in violation of first normal form.
Do you need any information from the Order table? SELECT p.id FROM product p WHERE EXISTS ( SELECT TOP 1 1 FROM orders o WHERE o.order_id in (2,3) and o.product_id = p.id )
Yes you're right, that's how I stated it. Thanks!
Thanks - for this situation, I just want matches for the two orders. However I'm interested in how to do it for all matches as well, just not for the current problem.
Not for this specific query, but I ma at a future point.
This is the correct answer, you store the data as individual entries and then extract it in the format you want.
String_agg https://docs.microsoft.com/en-us/sql/t-sql/functions/string-agg-transact-sql?view=sql-server-2017 https://www.postgresql.org/docs/devel/functions-aggregate.html Don't what DBMS you are using, so I listed mine
this is what you call a hack "top 100%" is all of it, and logically has no effect the idea that you can then stick an ORDER BY into a view and get it to work is repugnant to some of us
 select p.id, case when o.cnt &gt;= 1 then true else false end case as product_has_orders from product p, left join (select product_id, count(1) as cnt from orders) o on (p.id = o.product_id)
The other solutions here are good, but I'll throw out some more. There are may ways to skin a cat! This will give you the order counts for all the products- SELECT p.id, o.OrderCt FROM product p CROSS APPLY (SELECT OrderCt = COUNT(*) FROM orders WHERE product_id = p.id) o ...or if you just want a binary 1/0 based on whether any were found: SELECT p.id , HasOrder = CASE WHEN EXISTS (SELECT 1 FROM orders o WHERE o.product_id = p.id) THEN 1 ELSE 0 END FROM product p ...or... SELECT p.id , HasOrder = CASE WHEN o.product_id IS NULL THEN 0 ELSE 1 END FROM product p LEFT JOIN (SELECT DISTINCT product_id FROM orders) o ON o.product_id = p.id
The query shown does this for all products/orders. As I said: to limit that for a set of orders, add a WHERE condition to the derived table
You just need to add a spot of aggregation. `SELECT` `p.id, count(distinct o.order_id) as different_matches` `FROM product p` `LEFT JOIN orders o on` [`p.id`](https://p.id) `= o.product_id` `AND o.order_id in (2,3)` `group by` [`p.id`](https://p.id) &amp;#x200B; Will tell you the number of different order\_id values from the orders table. It's grouping by [p.id](https://p.id) so you'll get one row per product
Question - do you have any control over the table schema? I'm just wondering why you'd have multiple rows for a given sleep session with only a single timestamp. This requires two rows to define a complete sleep cycle (Sleep to Wake) and can lead to potential validity errors, such as a Sleep row for a given Person/Category without a subsequent Wake row, or a Wake row without a preceding Sleep row. Seems to me that a better design would be to have separate Sleep_Timestamp and Wake_Timestamp columns on the same row, and a single row per each Person/Category instance of sleeping. BTW, I agree with /u/catelemnis's suggestion below for a query, as well as /u/lepeng's.
only on output, i.e. when pulling from a normalized design, like several others are suggesting
Both of you are wrong and this trick still doesn't guarantee order. Currently it's only working with offset fetch, but it's not by design and you shouldn't count on it. It's all beside the point. Relation database is build upon a set model and sets don't have an order. If you need something ordered do it in the outer query.
The actual data I'm working with has nothing to do with sleep times and is more complex than this. This data structure in that context makes much more sense. I just made up this stupid example to help get a specific technical point across without complicating things too much
I see what you're trying to do and it's going to be horribly inefficient. In `Daily Stats` (please fix this name. spaces in object names may be _allowed_ but they're a pain), create an additional column that will contain the unique identifier of the record from `tblDailyStats` (terrible naming convention, BTW - you _know_ it's a table, don't prefix the name). You'll just insert it like any other column. Then for your daily load process, `insert into [Daily Stats] select &lt;fields&gt; from `Daily Stats` where AUTONUMBER_FIELD &gt; (select max(COPIED_AUTONUMBER_FIELD from [Daily Stats])`. Since this autonumber field is your primary key (I hope) and _probably_ clustered as well, this should run _much_ faster and it will do what you need.
Thanks, good idea.
depending on how robust you want this to be (e.g. regular production run versus a one-off just-copy-it-over-and-we're-done job)... do a left outer join to find the missing data, save into a temp table, then insert from the temp table SELECT t.[OR] , t.[Date] , t.[Short_Date] , t.[Case Number] , t.[Type] , t.[Comment] INTO temptable FROM tblDailyStats AS t LEFT OUTER JOIN [Daily Stats] AS d ON d.[OR] = t.[OR] AND d.[Date] = t.[Date] AND d.[Short_Date] = t.[Short_Date] AND d.[Case Number] = t.[Case Number] AND d.[Type] = t.[Type] AND d.[Comment] = t.[Comment] WHERE d.[OR] IS NULL INSERT INTO [Daily Stats] ( [OR] , [Date] , [Short_Date] , [Case Number] , [Type] , [Comment] ) SELECT * FROM temptable your mistake was writing the INSERT without listing the columns of the target table, therefore explicitly including the auto_increment (which makes me think it shouldn't have worked, because your SELECT has one fewer column)
\[Daily Stats\] is an older version of \[tblDailyStats\], which is currently being tested. During the testing phase, I need to ensure the older version is still updated with all current data until I officially release the new version. I did not develop the original and was trying to clean it up and improve it. During this process, I was trying to follow the RVBA standards, but I suppose that is no longer the standard anymore. I keep your comment regarding the names in mind. &amp;#x200B; There are no primary keys, unfortunately, just a regular autonumber column. Numbers can be replicated in this column. The autonumbers may match up between the two tables, however may contain completely different data. Would this still be the same course of action you'd recommend?
This is giving me a syntax error: Syntax error (missing operator) in query expression 'd.\[OR\] IS NULL INSERT INTO \[Daily Stats\] ( \[OR\] , \[Date\] , \[Short\_Date\] , \[Case Number\] , \[Type\] , \[Comment\] ) SELECT \* FROM temptable'.
We will need to see how you're loading or referencing the file... that's almost certainly your bottleneck. If you can load it into a temp table, create an index on value2 and it should run faster.
MySQL workbench is good for data modeling. You can also use draw.io or sqldbm.com to model as well. As far as answering your questions, never dived that deep into thing probably should, I’ll keep an eye on others responses for my own knowledge
Why not just making testers perform the action(s) in both systems? This will give you your test data / usage data without having to interfere with the production data... what if your new version accidentally overwrites the old system and you have nothing to go back to? Just seems like a really unsafe process to implement.
You'd probably have better luck doing an outer join on the table and checking to see if the value is null to see if it exists, rather than using a subquery
Where I work we just write up logical models (crowfeet etc). The physical might include optimizations you as a developer don't care about like horizontal / vertical partitioning, the type of database engine (InnoDB/MyISAM) etc. None of that may/should matter in writing the query, but it is also a good idea to document it.
um... you have to run those two statements separately, not together in one query
Oh, sorry. My bad. I corrected the issue; however, it sees all of the data as unique and saved them into temptable when the records already exists in \[Daily Stats\].
I create backups prior to updating the data. The reason why we don't want our pilot group to work from both systems is because it'll create double the work for them, affecting their quota. The majority of our staff are using the older system while a selected few are using the newer. I could just manually copy these over to the new system but I wanted to learn the proper way of creating an update query in this scenario.
ER is a tool for translating your business/application need into structured data. items 1 and 2 are pretty squishy and their description and/or usage will likely vary a lot and have a lot of overlap. Basically those are verbal descriptions plus non-rigorous diagrams that serve as a combination of "sales pitch" and high-level motivation for the more rigorous steps 3 and 4. Logical modeling is where what you rigorously define you data objects, their names, and the relations between them. This step doesn't care about what technology you might eventually use to implement the model. Physical modeling is where you take the design from Logical Modeling and formally plan its implementation in some technology (usually a database, but there are other possibilities.) This involves allocating proper storage, hardware/software strategy (e.g., choosing a dbms, planning physical computer/storage types, locations, redundancy, etc.), indexing, and establishing dependencies/triggers/constraints etc. "Database Schema" is semantically overloaded. One usage can refer to a design component from the Physical Model that defines tables, columns, constraints and relations for a given model. Another usage refers to a namespace within a database that groups data objects with usually a set of common permissions and topic, but not necessarily a rigorous structure.
You can do this with a table of date, and joining with operation "ON 1=1" First, your table of dates, I'm using a tally table to generate this. You can make a quick tally table like this: IF OBJECT_ID('[dbo].[Numbers]') IS NOT NULL DROP TABLE Numbers CREATE TABLE Numbers (n INT) CREATE CLUSTERED INDEX IX_Numbers_n ON Numbers ([n] ASC) GO DECLARE @IntsToInsert INT = 1000 DECLARE @Iterator INT = 0 WHILE @Iterator &lt; @IntsToInsert BEGIN INSERT INTO Numbers SELECT @Iterator AS [n] SET @Iterator += 1 END SELECT * FROM Numbers GO Now, use a CTE to make your dates, in this example, all days for 2019. Join this on your table. CREATE TABLE entry_value ( category_id INTEGER NOT NULL, value DECIMAL(12,2) NOT NULL, entry_date DATE NOT NULL ); INSERT INTO entry_value (category_id, value, entry_date) VALUES (2, 100, '2019-01-01'), (3, 100, '2019-01-01'), (4, 100, '2019-01-01'), (1, 125, '2019-01-02'), (2, 150, '2019-01-03'), (3, 110, '2019-01-04'), (4, 167, '2019-01-05'); WITH AllDates AS ( SELECT DATEADD(dd, n, '2019-01-01') AS [Date] FROM Numbers WHERE DATEPART(yy,DATEADD(dd, n, '2019-01-01')) = 2019 ) SELECT * FROM AllDates ad JOIN entry_value ON 1=1
It's not random, but it will depend on how the optimizer executed the query - what join order occured, which indexes were picked, whether parallelism is involved - lots of factors can influence these. Execute the queries on both machines with the actual execution plan turned on, and you'll be able to see the differences.
store "select distinct value from table" into a temporary table for a quick&amp;dirty fix, for example.
I like the optimism of this one "I've been learning SQL basics for the past days. After dealing with queries,..."
are you sure? did you maybe drop the IS NULL condition? please run the left outer join again and verify that it is only selecting rows from `tblDailyStats` which do ~not~ have a matching row in `[Daily Stats]` **based on those 6 columns**
That subquery is likely the culprit. Better off doing an outer join to your main statement on value and changing your case to use where table.value is null
The correct syntax for SQL Server is (removing the string concatenation for this example) UPDATE PuppyInfo SET Adopted = 1 WHERE DogTag = 'some dog tag value' Are you sure your base WHERE clause is actually selecting the row you think it is? Since this looks like it's VB.Net, run it in debug and grab the query string that's built. Paste that into SSMS or Azure Data Studio, and see if it works there. If not, figure out why, then go back and update your code accordingly. That said, building inline SQL with string literals taken directly from input fields without sanitizing them is a bad practice is begging for SQL Injection attacks.
Yeah, this is not remotely enough information to give a useful response. I don't know what "taking too long" means. How much data (row count) is in the component tables? How long is it taking? How long do you think it should take? Why? How are you accessing the file? OPENROWSET()? Have you tried importing it into a staging table? About all I can say is that `i.[value] NOT IN (SELECT value FROM table)` is likely to be a performance bottleneck. `i.[value2] &lt;&gt; 'specific value'` *may* be a performance bottleneck. `some value already in DB = 123529` isn't meaningful enough to tell me what it's trying to do.
If you insist on using a subquery, make the case statement sargeable. https://www.tech-recipes.com/rx/55535/sargable-queries-in-sql-server-with-examples/ In addition to that, try putting the value from table into a temp table instead of searching the entire table.
Okay, so I looked at the queries again. Access automatically changed the code a bit from: SELECT t.[OR], t.[Date], t.[Short_Date], t.[Case Number], t.[Type], t.[Comment] INTO temptable FROM tblDailyStats AS t LEFT OUTER JOIN [Daily Stats] AS d ON d.[OR] = t.[OR] AND d.[Date] = t.[Date] AND d.[Short_Date] = t.[Short_Date] AND d.[Case Number] = t.[Case Number] AND d.[Type] = t.[Type] AND d.[Comment] = t.[Comment] WHERE d.[OR] IS NULL to: SELECT t.[OR], t.[Date], t.[Short_Date], t.[Case Number], t.[Type], t.[Comment] INTO temptable FROM tblDailyStats AS t LEFT JOIN [Daily Stats] AS d ON (d.[Comment]=t.[Comment]) AND (d.[Type]=t.[Type]) AND (d.[Case Number]=t.[Case Number]) AND (d.[Short_Date]=t.[Short_Date]) AND (d.[Date]=t.[Date]) AND (d.[OR]=t.[OR]) WHERE d.[OR] IS NULL; AND INSERT INTO [Daily Stats] ( [OR] , [Date] , [Short_Date] , [Case Number] , [Type] , [Comment] ) SELECT * FROM temptable Into: INSERT INTO [Daily Stats] SELECT * FROM temptable;
Well, the fact that many-to-many relationships don't have a direct representation in SQL databases doesn't mean they don't exist *conceptually*. You'll use a relationship table (as mentioned in those screenshots), but in your mental model, you will still think of it as a many-to-many relationship.
&gt; "Strictly speaking we're naming a relation" - I disagree, a relation is a link between tables. A table is a set of data. A table does not contain a user, it contains users I was confused by that as well; I've tended to use singular or plural nouns to describe the grain of a table, not the table itself, a relationship, nor use a blanket rule. For example, if a table contains a row grain that represents a single user, we'd call it 'user' to accurately identify the meaning of a table's row. A data consumer can utilize the nomenclature to easily identify the entity represented without having to inspect primary keys or unique indexes. A table, in itself, is already implicitly plural (as by it's design, it contains multiple entries) and we gain nothing by re-articulating that in our nomenclature; I'd use a plural noun when a single row describes multiple logical entities. For example, If you have a table that has a grain of multiple users authorized for admin access for a specific company (something like ID, CompanyID, UserID1, UserID2, UserID3), I'd name that something like AuthorizedCompanyUsers since a row actually represents multiple users. I feel a style like this gets a bit more bang for your buck; both developers and data consumers can follow a logical pattern that can provide deeper insight than simply naming everything singular or plural. While I've had great results with this approach, I'd still say that consistency is still top priority when coming into an existing environment.
the first "automatic change" is has only immaterial changes, like dropping the optional keyword OUTER, re-ordering the AND conditions, which makes no difference since they're all ANDs, and adding a bunch of (unnecessary) parentheses the second "automatic change" is the killer, because it loses the list of columns as far as i recall, ms access doesn't actually change a query until you save it try running the INSERT before you save it, and ms access should preserve the column list during execution
Okay, so the first query is still pulling ALL of the data into \[temptable\]. I'll run the second query with the list of columns once I can get the first one working properly. To double check, I went into both tables to compare the existing data and they were there, therefore should not have been picked up by the query. \[Daily Stats\] shows: | 245630 | OR0225189 | 5/3/2019 5:56:32 PM | 5/3/2019 | 400995019 | E-Mail - Regular Priority | RFI | \[tblDailyStats\] shows: | 231085 | OR0225189 | 5/3/2019 5:56:32 PM | 5/3/2019 | 400995019 | E-Mail - Regular Priority | RFI |
I've been taking my cert tests for 70-764 (Passed it) and 70-765 (Failed my first attempt, got two left with the package I bought). 70-765 is 100% completely useless in my environment. We have no interest, whatsoever, in using Azure and that exam is pretty much entirely about using MS SQL on Azure. I have to take it to get the MCSA though. That being said, my boss wants me to get the cert because even though I'm handling my responsibilities fine, it makes the higher-ups nervous that I don't have a piece of paper showing I know what I'm doing. Basically, it makes everyone that has no idea how SQL works feel better about me doing SQL stuff. To be fair though, my experience with SQL prior to accepting the job of DBA was "Installed MySQL on my computer once, and even opened it a couple times." Basically my point is that it CAN be useful, even if you know what you're doing. If for no other reason than to placate the non-technical higher-ups (VPs, COO, CEO, etc.).
The resulting table would have 2 rows.
Maximum number of records that you can get from an inner join is Cartesian product - so # of records in SourceA multiplied by number of records in SourceB
&gt; Okay, so the first query is still pulling ALL of the data into [temptable] i'm sorry, i just do not see how that is possible the WHERE clause ~clearly~ says `WHERE d.[OR] IS NULL` so the only rows from `tblDailyStats` that should get pulled are those that do ~not~ exist in `[Daily Stats]` there's something else going on but i don't have enough info to guess further
Well, thanks for trying. Yeah, I'm not sure why it's doing that. The only thing I haven't mentioned is that the autonumber column is called ID, which is the only one I don't need to pull into the other table.
Assuming @T1 , @T2 and @T3 to be the number of users that completed at least 1 trip , 2 trips and 3 trips respectively . Percentage for Question 1= @T2 * 100/ @T1 Percentage for Question 2 = @T3 * 100/@T2
Great info, thanks a lot! Could you please elaborate on the subject of: redundancy and indexing? I've managed to google rest of the aforementioned terms, however these two are still a little bit unclear for me.
W mysql and a lot of the foss rdbms there are various hosts that will provide free db hosting assuming you're not looking to load a huge amount of data into them
Google that one.
There’s not a lot of details here, but if you have some experience with Terminal, install SQLite3 and do your work there. (Via Homebrew: `brew install sqlite3`). If you need a UI, LibreOffice has Base, but it’s SQL dialect is not really the best. Good luck.
Hard to tell why it’s slow without an execution plan! So let’s make some guesses 🤞 Your query will execute at an absolute crawl if the value column in the not in subquery can return NULL, if that is the case then you really want to filter out those NULLs (almost certainly). If you have lots of rows in your driving table and you expect to cover a lot of ground in your not in subquery table then you would probably benefit from executing that part of the query using bulky methods like outer hash joins, Im not incredibly familiar with MS SQL query transformations but you might have to rewrite the query manually using a left join to your other table and then checking the joined column is not null in your case when.
www.sqlfiddle.com
Those are pretty interesting ideas. Thank you! I don't, personally, watch many movies, though if I did that's a cool idea. I'll have to put some thought in to music database. My hobbies include occasional video games, programming, digital art, and etymology. 🤔 Hmm...
LiveSQL.oracle.com
No need for distinct if the question is whether any of them matched, no? Distinct is expensive
When it comes to physical design, redundancy is maintaining multiple copies of an object (table, database, etc.) on separate hardware stacks. This protects you from losing your objects due to hardware failure, natural disaster, etc. An index is some "extra" data that can be created to speed up finding a set of records in a table. Without an index, if you ask the database for a set of records, it will have to look through the whole table to find what you've asked for. If some set of columns in a table are indexed, and you ask for records based on values in those indexed columns, the index can be used to quickly find those records. As an analogy, suppose you were asked to take a picture of every house with a blue door in your town. To get that done, you would have to travel every street in your town and look at every house. Imagine someone gives you a guidebook where every chapter is a list of house addresses with the same door color. Now to find the blue door houses, you flip to the "Blue Door" chapter in your directory and visit those addresses. The author of the guidebook may have even listed the addresses in an order that lets you visit them efficiently (no back-tracking.) In this analogy, the guidebook is our "index". We can also use this analogy to show some of the extra benefits, limitations and costs of indexing. One benefit is that if you're asked to take a picture of all houses with red doors and white windows, you can use your "index" to speed this up even though the guidebook includes no info about window color. A limitation is that if you're asked to find all houses with yellow windows, the guidebook is useless. There is a cost to indexing though. That guidebook is a data object itself, so you have to store it. There are cases where a table index can be a larger object than the table itself! Another cost is keeping the index up-to-date. Every time a new house is built, it has to be added to the correct chapter of the guidebook. If it's the kind of guidebook with houses listed in optimal routes, then the author might have to totally rearrange a chapter to accomodate the new entry. Same thing happens in a database index when new records are added to a table.
Sure, just felt it was a bit more useful. Distinct is not expensive though, it doesn’t take much more effort (a teeny weeny bit of CPU work) than a normal count, especially when there’s only two possible values. It’s never going to be a deal breaker performance wise.
MySQL workbench is a great starting point.
You cant do a where clause with a table that's left joined, it turns it to an inner join. Do a sub query if needed
Since `@T1`, `@T2` and `@T3` are all integers (you wouldn't have fractional people taking a trip - that would get very messy), this will not produce the correct results because it'll be integer math, not floating-point math. At least one value will have to be floating point to force a type conversion.
 SELECT SUBSTRING("dayofweek",1,3) AS WeekDay, CONCAT(city,'-',state) AS City-State FROM #tablename;
You can do it with arithmetic: datetrunc(‘week’, date - 2) Then Monday becomes Saturday. Not sure if all sql lets you subtract like that so you might have to do something like datetrunc(‘week’, dateadd(day, -2, date)) Or something like that.
`FROM x JOIN y ON 1=1` is a hack you're thinking of `FROM x CROSS JOIN y`
Distinct is very expensive. Not in small queries with 2 values obviously. That's why HyperLogLog gained popularity
Not sure if this is the most efficient way to do it, but it seems to do what you want: with all_dates as ( select ad.day::date as entry_date, c.category_id from generate_series(date '2019-01-01', date '2019-01-06', interval '1 day') as ad(day) cross join ( select distinct category_id from entry_value ) c ) select ad.category_id, ad.entry_date, case when ev.value is null then (select value from entry_value ev2 where ev2.category_id = ad.category_id and ev2.entry_date &lt;= ad.entry_date order by entry_date desc limit 1) else ev.value end as value from all_dates ad left join entry_value ev on ev.entry_date = ad.entry_date and ev.category_id = ad.category_id order by ad.entry_date, ad.category_id; The only thing I didn't understand how you want to "generate" the value for the first row though. In my solution it's NULL. Here is an online example: [https://rextester.com/ODMGU72029](https://rextester.com/ODMGU72029)
Theres no way to know which row should be first. Also, why can't you use a WHERE clause? It seems crazy to try to reach a goal but not use the language feature that exists to reach the goal.
It didn't seem possible to me to use a WHERE clause for this particular situation. Would it be possible with a WHERE clause? For the actual problem I'm working on, which is significantly more complicated than what I outlined above, the "Letter" column represents an event and the "Number" column represents the time the event occurred. So I'm trying isolate the first time a specific event occurred (event "X") and the first time another event (event "a") occurred after "X"
That did the trick, thanks!
I got Lead() to work, thanks!
&gt;For example that Instead of "Wake Me Up" is there, it says "Wake Me Up, Hey Brother, Levels". If you are planning to **store** the data that way: Do not do that, it would violate the most basic principles of database normalization. Just imagine how would you deal with titles that contains commas themselves? If you just want to display the data like that you need string aggregation. Postgres and SQL Server use `string_agg()` for that, Oracle supports the standard `listagg()` and MySQL uses `group_concat()` See here for more details: [https://modern-sql.com/feature/listagg](https://modern-sql.com/feature/listagg)
there is no such thing as "comes after" rows in a table have no sequence
&gt; Would it be possible with a WHERE clause? Yes - you'd likely use a row_number in a subquery, then use WHERE in the outer query and apply it to the output of the row_number. Then you'd either do the right thing and pivot it, or the lazy thing and aggregate a case statement, and bob is your mother's brother. I was going to give you an example, but I realise you haven't told us what 'first' actually means - remember, in a relation (table), there isn't actually a real, defined, guaranteed order to the tuples (records) in it. In your real data, is there a date or a sequence number you would use to identify the first occurrence? Or by first, do you just mean the lowest value of the number field for that combination of user/letter?
Sounds like for the first part you could use: MIN(EventTime) AS \[XEventTime\] and WHERE Event = 'x' will get you what you need. &amp;#x200B; Then maybe a subquery/join to get the second part: MIN(EventTime) AS \[AEventTime\] and WHERE Event = 'a' AND \[AEventTime\] &gt; \[XEventTime\]
Could probably also use first_value(Number) over (partition by Name order by SOMETHING) as FirstNumber
&gt;In your real data, is there a date or a sequence number you would use to identify the first occurrence? First means the earliest time an event occurred for that user. I did a terrible job reflecting this, but in my example table that would mean the "Number" column because in real life that represents a datetime If you have an example, that would be great. Thanks!
select distinct ug.user_group, count(ug.user_id) as members from t_user_group ug group by ug.user_group
What are the columns in t_group_membership?
 SELECT [t_user_group columns], COUNT(u.UserId) FROM t_user_group g LEFT JOIN t_user u ON g.GroupId = u.GroupId GROUP BY [t_user_group_columns]
What you're asking for is basically a tool that works as an expert DBA/consultant. If something like that existed, many of us would be out of a job.
Assuming Number is your ORDER BY... I think I understand. Try this as a starting point. SELECT t1.UserName ,t1.Letter ,Min(t1.Number) as Number ,t2.Letter ,t2.Number FROM table t1 CROSS APPLY( SELECT TOP(1) * FROM table t2 WHERE t2.UserName = t1.UserName AND t2.Letter = 'X' ORDER BY t2.Number ASC ) t2 WHERE t1.Number &gt; t2.Number AND t1.Letter = 'A' GROUP BY t1.UserName, t1.Letter, t2.Letter, t2.Number Probably inefficient and requires an "x" and an "a" for every UserName to show up, but I think it covered every Number scenario. Here's the temp table sample set I used. insert into #temp values ('J','a',63), ('J','a',10), ('J','x',22), ('J','x',11), ('J','a',62), ('J','x',48), ('S','a',15), ('S','x',37), ('S','a',52), ('S','x',94)
Initial thought is that this table design is just plain bad. But given what it is you still might be able gather your desired results by doing some sort of lengthy manipulation through use of programmatic objects. Maybe something that passes in a table variable for each subset of names then loops through to parse out the X and A values you need. Otherwise I just can't think of anyway you could do this through a basic query approach.
We may consider subquery, like following, &amp;#x200B; SELECT \[t\_user\_group columns\] , (SELECT COUNT(\*) FROM \[t\_group\_membership\] WHERE \[t\_group\_membership\].\[user\_groupid\] = \[t\_user\_group\].\[groupid\]) AS \[Count\] FROM \[t\_user\_group\]
I guess you are right. The way I learned assembly was by examining the output of the gcc with the option `-fverbose-asm`. I thought it'd be nice to have something similar to that.
Simply \`group\_fk\` and \`user\_fk\`, foreign keys
Not sure if it's your issue or not but you have too many ends in there. should be Case When Then When Then End
See if this works. &amp;#x200B; select RTRIM(right (mas.id, 11)) as "&lt;NUMBER&gt;" ,CASE WHEN PAY.PAYCLASS = 100 AND MAS.bargunit = '1303' AND MAS.department &lt;&gt; 'PW' THEN pay.payclass = 106 ELSE RTRIM(PAY.PAYCLASS) WHEN PAY.PAYCLASS = 100 AND MAS.bargunit = '1303' AND MAS.department = 'PW' THEN pay.payclass = 107 ELSE RTRIM(PAY.PAYCLASS) END AS '&lt;ROLEID&gt;' FROM table1 mas left join table2 pay on mas.id = pay.id
SELECT DISTINCT User, FIRST\_VALUE(X) OVER (PARTITION BY User ORDER BY X), FIRST\_VALUE(a) OVER (PARTITION BY User ORDER BY a) FROM t1
If you ever sign up for any kind of trial from solarwinds you will NEVER stop getting spam from them.
It could be a configuration issue. What is the bind address set as in the mysql.conf file?
Thanks for sharing!
Learn how to read your database's execution plans and how to improve them by adding indexes or writing more efficient queries. https://use-the-index-luke.com/ is a good start.
I work with software that uses GUIDs almost exclusively for PKs/FKs. I think I understand the purpose of this as being to prevent "collisions" between those people who are working with multiple instances of the software... but this software allows you to create custom fields that are stored in a key/value type table - and each value associated with each person has it's own GUID. This is hundreds of thousands of rows / GUIDs for us in this table alone, and we are a small institution. Is that bad practice?
Thanks. I was able to figure it out. I just needed to step away for a minute. lol
Thanks for the link. I took the 3 min quiz and failed. Looks like I have much to learn.
Backup + shrink log file ([https://www.mssqltips.com/sqlservertutorial/3311/how-to-shrink-the-transaction-log/](https://www.mssqltips.com/sqlservertutorial/3311/how-to-shrink-the-transaction-log/))
Why do you need to add the amount to the project table if it's already in the funding table? You shouldn't have the same data point in two places unless you have some very good reason to do it that way. If you ever need to find out there project funding, then do a query where you join the two tables.
Ensure you gave backup compression turned on.
Okay, I guess it makes sense to not copy values. However, from your experience, do you think this query is good to say "Query for getting full project funding amount for wanted project" ? SELECT Funding.projectID, SUM(Funding.amount) FROM funding GROUP BY Funding.projectID HAVING Funding.projectID = 1000;
There's no way of knowing how large the database is from the transaction log size.
Before you start shrinking the transaction log, make sure you have regularly-scheduled full and transaction log backups running. Your transaction log file will continue to grow unless you take log backups.
You need to really look into what a log file is, and make some decisions. Your recovery model is set to "full", and you are logging every single transaction your database makes in a reversible and reproducible manner. The point of this, is so that if something goes wrong, you can restore 5 minutes ago, 5 hours ago, 5 days ago, or even 5 weeks, 3 days, 17 hours, 6 minutes, 19.23 seconds ago, anytime at all since your last full backup. You're also not backing up the database in a way mssql recognizes, so that transaction log goes back to the beginning.
I would do funding. ProjectID = 1000 as a where clause. It will be a lot more efficient. You can also join the project table if you need project details.
Alright, thank you very much!
 SELECT Name , EmployeeID , EmployeeManagerID , Level FROM Employees WHERE Level IN(1,2,3) AND EmployeeManagerID IN(SELECT EmployeeID FROM Employees WHERE Level = 3)
I really don't need the transaction logs at all. I'm using the database for data analytics. So the log is not important as long as my source data remains intact. Is there a way for me to clear the log and stop logging completely?
I was basing it on the 3x the size in the title.
As others have stated, do not just backup and shrink the log file. Check your **recovery model** of the database. If recovery model is FULL, are you performing scheduled backups? Good info here: https://www.brentozar.com/blitz/full-recovery-mode-without-log-backups/ &gt; "When a database is in Simple Recovery Model, SQL Server does circular logging: it goes back to the beginning of the transaction log and reuses space when it can. Portions of the log are freed up when they’re not covering open transactions... On the other hand, when a database is in Full Recovery Model or Bulk Logged Recovery Model, SQL Server doesn’t free up the log file when your transactions finish. It will continue to grow the transaction log, thinking that you’ll want to back up all of these logs at some point." If recovery model is FULL and you do NOT need it to be full (database backups at night are plenty), set the recovery model to SIMPLE and perform a backup, then proceed to shrink the log file - but ONLY if you've switched it to simple and the log is still huge, you may also need to modify the log file size, but we can help more if you respond. Most common scenario, when I find a database at work with a HUGE transaction log file, it is usually because the model database is set to "FULL" by default, someone created a database inheriting this FULL recovery model and has never taken a transaction log backup. With further investigation, most of these databases don't need to be in FULL recovery, a nightly backup is sufficient because not a lot of data changes throughout the day, so updating the recovery model to SIMPLE and shrinking the log or modifying its default size once solves the problem for good. If, on the other hand you need FULL recovery, are doing transactional log backups, and your log files are still HUGE, they may actually NEED to be HUGE - if you do a lot of data loading or large transactions, and by shrinking it will just a) make it slow down when performing the same daily large transactions/data load because it has to grow the log file back to the size it needs and b) just become that same size over time so you'd have to continually shrink the file, which is bad because of a).
 AND EmployeeManagerID IN(SELECT EmployeeID FROM Employees WHERE Level = 3)
Make sure your database is set to SIMPLE recovery not FULL recovery. The tlog will retain all transactions until backed up. Analytical DBs will be hit hard and unless you require recovery points. Set to SIMPLE and do a nightly or whatever backup. Simple will just retain active transitions in case a rollback is needed from failure.
Could you provide more context, like an error message? Or are you saying that the query returns no results? What RDBMS are you working on?
Just returns no results. If I take that line out I get all Level 3's, 2's and 1's. However the Level 1's populate regardless of their reporting manager obviously. The moment I throw in that line which is pretty similar to what I had as well, it kills all results. I'm working on SSMS
So I changed the databases from FULL to SIMPLE. Then I shrunk the Log files. Freed a whopping 300GB of space. Thanks a lot lads!
Not sure what the problem could be. It's possible that there are no employees reporting to level 3 employees. &amp;#x200B; Have you tried the second query I provided?
The second query is a bit odd to implement with my query I think? Maybe I'm over looking it. I have to inner join two different tables to get all the data I need which is what is making this so damn complicated. SELECT EmFirstName, EmID, EmMgrID, Er.RnkLevel FROM database..tblEmployee (nolock) Em INNER JOIN database..tblJobTitle (nolock) Jt ON Em.EmTltID = Jt.TltID INNER JOIN database..tblplEmployeeRank (nolock) Er ON Jt.TltRnkID = Er.RnkID WHERE Er.RnkLevel IN (1, 2, 3) This pulls all the levels needed. Trying to implement your second query now. This query has been giving me a headache all day... lol
Thank you! So derived tables are scanned only once even is they're used multiple times in the same query? Thanks again!
I'm not going to call it bad practice as I don't know the full story behind the decision... When you say "working with multiple instances of the software", do you mean multiple instances of the same application that are connecting to the same DB? If that's the case, another option is to set up a sequence table and using the INT datatype (or a variation of it). [https://docs.microsoft.com/en-us/sql/t-sql/statements/create-sequence-transact-sql?view=sql-server-2017](https://docs.microsoft.com/en-us/sql/t-sql/statements/create-sequence-transact-sql?view=sql-server-2017) Reason i'd suggest that is that these are the columns that your Clustered indexes are most likely on (being PKs) or possible Non-clustered indexes on the FKs. As such the GUIDs take up a lot more space on pages files, increasing the total size of the index and decreasing the efficiency of it. INT is smaller and as such reduces this issue.
I don't get the reason for this posts existence.
The * is above the 8 on the keyboard.
I always type by accident "select 8 from" instead of "select \* from". I'm trying to see if I'm the only one.
This is why I use a ten-key.
Alter trigger
doing it from head (net near a DB...) select * from (select name, grade, last_update, when_modified, rank() over (partition by name oreder by last_update desc, when_modified desc, rownum) as rnk from table) where rnk = 1
Yes this is what I needed! I will report back, thanks!
Do you have an sql that is used? Post it and maybe I can help. You probably have to use agregation function in the where clause like MAX on the lastDate column
Also you can show us what the result should be, not just what it is
/r/DoesAnybodyElse
If your current trigger specifies the columns then this might make it harder, you could write another foreachdb to query the existing trigger on the table and then send and alter trigger to extend it. You could also write some SQL to generate the trigger based on current table structure.
Unfortunately, there is some identifying information in the actual SQL query, so I can quite post it, I could post a skeleton of it though: SELECT DISTINCT asec.duedate, asec.name, c.course_name, tc.name, sta.name standard, gsec.standardgrade, com.commentvalue, gsec.lastupdated, gsec.whenmodified FROM students s JOIN assignmentscore ags ON ags.studentid = s.id JOIN assignmentsection asec ON asec.assignmentsectionid = ags.assignmentsectionid JOIN sections sec ON sec.dcid = asec.sectionsdcid JOIN terms t ON t.id = sec.termid AND t.schoolid = sec.schoolid JOIN courses c ON c.course_number = sec.course_number JOIN assignmentcategoryassoc aca ON aca.assignmentsectionid = ags.assignmentsectionid JOIN teachercategory tc ON tc.teachercategoryid = aca.teachercategoryid JOIN assignment a ON asec.assignmentid = a.assignmentid JOIN assignmentstandardassoc asta ON a.assignmentid = asta.assignmentid JOIN standard sta ON asta.standardid = sta.standardid JOIN standardgradesection gsec ON sta.standardid = gsec.standardid JOIN assignmentscorecomment com ON ags.assignmentscoreid = com.assignmentscoreid WHERE gsec.standardgrade IS NOT NULL AND s.id = [current student id] AND t.yearid = [current year id] AND (tc.name = 'Term One' OR tc.name = 'Term 1') ORDER BY c.course_name, asec.duedate DESC, gsec.lastupdated DESC, gsec.whenmodified DESC This is a majority of the query with some private information omitted. Please forgive the spaghetti, I didn't build this DB and there are a lot of tables to link to one another.
You want to self join it. Select only the level 3s in one sub query, then self join it to find any employee where the employees from that first sub table is the manager.
SloppyPuppy provided a better solution, you could use max, but partition by is better and probably what you need.
The desired outcome would basically be just the top line, since I have it sorted by the 2 date columns on the right in my query.
and is the most efficient as it scans the table only once. boy do I love analytic functions :)
By multiple instances, I mean separate CRM databases which ultimately both integrate into another database. This is a college admissions software, so I *think* the purpose is to give that flexibility to those schools who have possibly multiple departments, with large admission pools, possible overlap between pools, and which both need to reconcile data into the same singular student information system... if that makes any sense at all. It has been a long day.
oracle pre 12: wm_concat oracle 12 or higher: listagg
How much money do you have? :) jokes aside, there are very very expensive ones like the Oracle golden gate and there are others that are less expensive. but my personal opinion: PG and HA is an oximoron. that DB is anything but HA.
I am a little confused at how I would implement your code above into my query, maybe you could take a look at the query and give some pointers? I have to join 13 tables together to get the information I need to pull onto the page, so I'm not sure how to go about that.
Could you go into more detail why that's the case?
This happens to me daily.
Commit in batches and taken them log backups.
Actually, no, at that point, the author recommends to store the results in an intermediate temp table. Which is probably smart thinking if you have something you’re going to call a bunch of times: don’t calculate it anew each time, stash the results and call those. I’ll update my comment to be clearer.
I'm new to sql, so there may be a better way, but i think this worked for me ALTER TABLE temptable ADD weekday CHAR(3), citystate VARCHAR(22) GO UPDATE temptable SET weekday = SUBSTRING(dayofweek, 1, 3) UPDATE temptable SET citystate = CONCAT(city, '-', state)
I ended up using the alter session set week\_start = 6
It's my pleasure!
Who uses \* anyway?
I use it to multiply things quite often actually... and to select all columns
can you try to OR those two filters instead of AND?
Don't do that. Do not do that to fix your "the log file is growing" problem ! read again, and think it trough what /u/alinroc , /u/auroralai and /u/Intrexa wrote. The reason why your log file is exploding like that is either, that you don't do log backups, which is something really important to take care of, or that you have some seriously VERY bad queries running on that server. Yes, it is possible to have your log explode with sufficiently fucked up queries, even when you are in simple recovery. Anyway, be sure about setting your DB into simple recovery. Simple recovery, is a nice term for "I do not give a shit about if I loose data since my last full &amp; diff backup", as in "I REALLY DO NOT GIVE A SHIT". There are usecases where that is the case, but boy, if you are not comfortable enough, to say those exact words to your manager, don't set your DB into simple recovery. It might seem as a fix, but it also might be a company ending event waiting to happen. Just by you asking that kind of question to the internet, I'd highly advice you to check your backups, and not go for the "yeah fuck it if I loose data" quickfix of setting the recovery model to simple. Really, really think about what you are doing there, and do not rely on stackoverflow for the answer !
Thanks! I ran into issues with this too. Somehow it led me here though which worked. ALTER TABLE temptable ADD weekday char(3) generated always as (substr(dayofweek, 1, 3)), ADD CityState varchar(22) generated always as (concat(city, '-', state));
Well if you query with many tables returns the table you gave as an example you could always select * from around all of it and do what I did.
Upon further inspection, what i did i unrelated to how many joins or tables there are. Just rank by the group you want by the colimns from the tables you need, order bu whatever you need then wrap all of it with where rnk=1
`CURRENT_DATE` returns today's date `LAST_DAY(date)` returns the last day of the month for the given date so `LAST_DAY(CURRENT_DATE)+1` returns the first day of next month then `ADD_MONTHS(LAST_DATE(CURRENT_DATE)+1,-1) returns the **first day of this month** and `ADD_MONTHS(LAST_DATE(CURRENT_DATE)+1,-2) returns the **first day of the previous month** these alst two values are the boundaries we need for an efficient WHERE condition -- WHERE transaction_date &gt;= ADD_MONTHS(LAST_DATE(CURRENT_DATE)+1,-2) AND transaction_date &lt; ADD_MONTHS(LAST_DATE(CURRENT_DATE)+1,-1) vwalah
never happens to me because i never use the \*\*dreaded, evil "select star"\*\*
Okay, thank you. I will have to try this tomorrow, I basically did what you described, but I was wrestling with some syntax errors then I had to go to one of my schools, so throubleshooting will need to wait until another time. Thanks for all the help! I'm sure I'll get it with what you have here.
Transaction logs help you recover that source data. If you're in simple recovery model and have a daily full backup at 9pm, you need to be sure that a business user understands that if that hard drive fails, you will likely lose any data change between when it fails and 9pm the day before. If the users do not accept a potential full day of data loss, you probably need to go back and use a full recovery model and look at how to set up the transaction log backups.
okeeeey..... Do you know what compression will actually do? I'm gonna give a quick run down of row level compression. See, the data is stored what is called a page. A page is 8kb of binary, written to your mdf file, or ndf's if you use multiple filegroups and files. Lets not delve into that one. Lets keep it simple and to the atomic unit, a page, 8kb of "storage". Thats also what "a read" or "a write" is, if you are wondering, so, 8kb is the atomic storage value on SQL Server. Now, lets have some fun here ..... this is for reference : https://www.sqlskills.com/blogs/paul/inside-the-storage-engine-anatomy-of-a-page/ Now, an 8kb page, consists of some meta data (quite a bit of meta data), and "slots". A Slot is pretty much a row in your table. The data of your slot is written as a given binary lengh, based on your datatype, and addressed by "slot 2, offsett 0xAf20" with the given lenght. When you enable row level compression, a 4 byte integer (int32), so standard integer, might well end up as the binary length of 1 byte. Since your actual data, is based on offsets on a reference, the binary data of the column following, your in theory int32, might not start at the offset XX +4byte, but at XX+1Byte. Every other row after still references binary offsets inside of those 8kb. So, yesterday I wrote the value of lets say "5" into that int32 in slot 2, which would be 0000 0000 0000 0000 0000 0000 0000 0101. That can be compressed down quite a bit can't it.... so you will likly end up with some compressd value, containing just 1 byte. Now, ever other row, written to that 8kb page, will still reference offsets inside of those 8kb. Just by pure logic, when you update that single integer in row 2 (slot 2), to not be 5 anymore, but lets say 2147483647.... int32.max.... you will change the offset of EVERYTHING inside the page, coming after slot2 (row2) column that integer, since that integer will then be 4 bytes in length, instead of compressed down to "something" in length. In essence, you will not update 1 record. You will update up to how ever many records 8kb of data can hold. That can be 100's or 1000's of rows. That is what row level compression can and will do to you. Handing out a blanked recommendation of "how about row level compression", If I may say so .... you should proberbly not do that.
You can try the following which might yield better performance. select customerNumber from customer c where exists ( select 1 from customer where customerphone=@phone union all select 1 from customer where customerphone2=@phone union all select 1 from customer where customerphone3=@phone )
Was sarcasm.
Deleting my comment because it was getting brigadier. &gt;you should proberbly not do that. Given the thoroughness of your statement, I find this statement to be in poor taste. You're example while good for certain scenarios, basically doesn't apply as soon as large scale information with character arrays and blobs are taken into account; in fact the opposite is true since compression, while . Which, given that ops database is over 50GB is highly likely. Something you failed to mention, and it is worth mentioning, that backups with compression will take significantly less time when compression is enabled.
&gt; Something you failed to mention, and it is worth mentioning, that backups with compression will take significantly less time when compression is enabled. Backup compression has exactly NOTHING to do with row level compression. Even mentioning both in the same context makes my head hurt. &gt;Given the thoroughness of your statement, I find this statement to be in poor taste. You're example while good for certain scenarios, basically doesn't apply as soon as large scale information with character arrays and blobs are taken into account; in fact the opposite is true since compression, while . Which, given that the database is over 50GB, is highly likely. Poor taste would be saying that you are talking out of your arse, I didn't do that, since I'm working on my social skills... Let me disect that... &gt;basically doesn't apply as soon as large scale information with character arrays and blobs are taken into account I'm talking about 8kb of binary, which is what row level compression is working on. How large scale does an algorythm working on chunks of 8kb get to you, and how does the number of 8kb chunks feed into the algorythm have any god damn meaning to how the algorythm behaves? What do you call large scale ? &gt;character arrays You mean strings? Normally we call char arrays strings, but maybe i'm to new age to not call it a char array.... btw, are you talking fixed length, or variable length, cause when we talk about the 8kb of binary written to disk, or kept in mempory, I'd have some follow up questions to you you'd have trouble googleing the answer to. How large (i binary) is a varchar(20) with the value of "abc"? If you don't imediatly go to the counter question of "whats the table defintiion", you wont answer correctly btw. &gt;blobs Are you kidding me? a blob is a binary array, same as ANYTHING else, a blob dosnt come with "this is how to parse it" thats the only difference. Why how, WHY even go there.... &gt;Which, given that the database is over 50GB, is highly likely Are you fucking kidding me? You call 50gb large scale? You don't really mean that do you? I was talking to you, about how 8kb of 0's and 1's are represented on disk, and in memory... and you throw that kind of bullshit back, are you serious?
Tableau has this functionality, but it's pricey. Microsoft had some 'free' plugin for Excel a few year ago that would do it, but it was somewhat limited.
&gt;Backup compression has exactly NOTHING to do with row level compression. Even mentioning both in the same context makes my head hurt. Besides the fact that the engine can just do a checksum and write the backup without having to do any compression? Nothing to do with it?
I think you replied to the wrong person.... In that case, I'd actually crack this with a self join, then use a row_number to identify the first : SELECT user, x, a FROM ( SELECT t1.user, t1.number AS 'x', t2.number as 'A', row_number() over (PARTITION BY t1.user, t1.letter ORDER BY t1.number, t2.number ASC) rankorder FROM table AS t1 LEFT JOIN table AS t2 ON t1.user = t2.user AND t2.letter = 'A' AND t2.number &gt; t1.number WHERE t1.letter = 'x' ) subquery WHERE subquery.rankorder = 1
row level compression on a god damn table does have NOTHING, NOOOOOTHING to do with backup compression for fucks sake. It also dosnt have ANYTHIGN to do with page validation, those are 3 completely different things. Those are THREE different things !!!
Read the post. I didn't think it made any difference either. But turning on page compression dropped the backup time of one of my primary logging databases by nearly a quarter.
Stop making transactions.
Oh, man. As soon as I saw "My log file" in the post title, I knew this was going to be a wild ride comments section.
I think this might do what you want: [https://www.google.com/earth/outreach/learn/visualize-your-data-on-a-custom-map-using-google-my-maps/](https://www.google.com/earth/outreach/learn/visualize-your-data-on-a-custom-map-using-google-my-maps/) you import your data as a layer on the map, and then monkey with the display to get what you want, and then you can call the map via a link you can embed on a website.
I do tons of geo work as I run analytics for a logistics company. Give me a bit more info. What is it you are trying to map? How do the data points need to aggregate?
*
If you don't want to pay money, I'd use R - take a look at ggmap and ggplot libraries.
Piece of advice, always format your code in your questions. If you can't be bothered to spend a little amount of time to hit few enters and tabs, why should we spend time to find solution to your problem. Programmers are sensitive to unformatted code.
So. Many. Bad. Dba. Idiots.
Imagine a scenario where you have to do something with every row in a result set. Whether it's executing a procedure or an INSERT/UPDATE/DELETE statement. &amp;#x200B; You **could** do it by saving the result set in a temp table/variable, along with a ROW\_NUMBER column, and then iterating over it with a WHILE loop and a index variable... &amp;#x200B; OR, you could use a cursor and do it more elegantly. That's my most commonly used scenario, anyway.
You'd have to partition your dataset and count the row number based on the time. I've used a CTE to demonstrate this as your dataset seems small but you could always use a temptable to achieve the same effect. Using a query like below you'll get your results but not in the same shape as you want to end up with - you'd have to pivot it or something like that. WITH CTE1 AS ( SELECT x.[User], x.[Event], x.[Time], ROW_NUMBER() OVER(PARTITION BY x.[User], x.[Event] ORDER BY x.[Time] ASC) AS 'Event_Time_Check' FROM [Your_Table] x ) AS y SELECT y.[User], y.[Event], y.[Time] FROM y WHERE y.[Event_Time_Check] = 1 ORDER BY y.[User], y.[Event], y.[Time]
Thank you! I managed to get this to work
&gt; is there any other way to do it yeah, use the multiplication operator SELECT width * height AS area ...
you need a date table to use as the left table in a LEFT OUTER JOIN
The correct statement is: select 42 from employees;
You need to see the shrink.
It does provide job security to the guys who actually know what they are doing. Atleast all the comments on this post are asking about recovery point objective before doing anything else. If you type this question in Google you'll find noting but "just put it in SIMPLE and shrink it"...
You are right that set based operations should be favored, but expertise isn’t just knowing the rules, it’s also knowing when to break them. Sometime scenarios are only possible with a cursor. Even Microsoft provides code that uses them. This is their page for migrating logins across servers keeping the same sid: https://support.microsoft.com/en-us/help/918992/how-to-transfer-logins-and-passwords-between-instances-of-sql-server
Granular logging of how each processed record was handled, including exceptions.
Thank you for your reply! As im trying to learn SQL im using Khan Academy and all my coding are in the Khan Academy challenges so im not quite sure if that is SQL Server, MS Access, Oracle or MySQL. Not sure it would work at all. Due to that i dont know if im doing the string\_agg() wrong or if its not supported on KA. Another thing i didnt quite get is how i can add the string\_agg() into this join statement? SELECT Artists.Stage\_name, Artists.Age, Artists.country, Artists.Breakthrough, Songs.title, Likes.food\_name, Likes.to\_do , Likes.pet , Likes.number FROM Artists JOIN Songs ON Artists.Stage\_name = Songs.Artist1 JOIN Likes ON Songs.Artist1 = Likes.Artist2 GROUP BY Stage\_name ORDER BY stage\_name;
My most frequent use of a cursor is in collecting data from multiple databases on the same instance into a temp table to perform aggregation &amp; reporting across all those databases. You can't run a set-based operation over multiple (arbitrary) databases.
Select stored_func(*) from some_table?
Personally, I find the while loop more elegant. Cursors are harder to read, in my opinion.
I love you.
Bigfig nailed why most of them exist. The last shop I was at, the lead developer was a VBA guy who didn't understand set based logic and so he did everything inside cursors. Everything scaled terribly(timewise), but it did work.
I personally think he has a stigma against triggers and he probably didn't read your trigger in detail to understand what it's doing. If it was a select that was not setting variables, this would be a problem because every update triggers a select that scans or seeks tables and indexes to return data over the network for no purpose. Every time an update occurs, the select would too. Your select does have a purpose and is fine. All of this said, triggers are usually a last resort and it generally indicates there is logic you should envelop in code or write the process in another way. Also, sometimes there are good scenarios for triggers. Not every problem is a nail and not all solutions are hammers.
Is this going to work if the trigger specifies the table's columns?
Perfect response, thank you for that! I agree that we could/should capture this behavior in code. Legacy spaghetti vb code. It's due for a rewrite, but so is the database.
Oh can either alter the trigger by rewriting out the whole statement or just do a drop and crest trigger statement to do the same thing. So I’m on mobile.
It would be really messy. You can just use sub queries and grouping Something like: `SELECT User, Min(Time) as EventX FROM evt as evtX` `INNER JOIN (SELECT User, MIN(Time) as time FROM evt as evtA WHERE avtA.Event = 'a' GROUP BY USER ) as FIRSTA` `ON FIRSTA.User = evtX.User AND FIRSTA.time &lt; evtX.Time` `WHERE [Event] = 'X' GROUP BY User` And then repeat for Event A
first query -- SELECT po_hdr.po_no , po_hdr.location_id , po_hdr.supplier_id , po_hdr.division_id , po_hdr.order_date , po_hdr.date_due , po_hdr.requested_by FROM CommerceCenterImport.dbo.po_hdr po_hdr WHERE po_hdr.requested_by='3498' AND po_hdr.order_date &gt;= {ts '2019-01-01 00:00:00'} OR po_hdr.requested_by='11239' AND po_hdr.order_date &gt;= {ts '2019-01-01 00:00:00'} OR po_hdr.requested_by='10622' AND po_hdr.order_date &gt;= {ts '2019-01-01 00:00:00'} OR po_hdr.requested_by='9066' AND po_hdr.order_date &gt;= {ts '2019-01-01 00:00:00'} ORDER BY po_hdr.order_date
Simple utility if you have lat Lon http://www.sgrillo.net/googleearth/gegraph.htm plots data in google earth.
If multiple records are updated, this trigger will not perform properly. One arbitrary record will be pulled into variables, and the rest of the updates will be ignored.
Triggers are not inherently bad or good. They are a tool, and as any too they can be very beneficial when used properly, or they can cause havoc when misused. The main thing people forget about triggers is that they are set based. For example, your particular trigger will most likely produce incorrect results every time more than a single record is updated in dbo.tblForms table. It will not loop through all records, instead the variables @id, @status, and @rev will receive their values from a single random row in the INSERTED structure, and will run the function and stored proc only once for that single row's values. Is this what you intended? Because 99.999% of the times that is now what developer meant to do.
I can’t tell any difference in the two queries I posted to the two that you posted - I was trying to combine the two queries into one, joined on the po_line.po_no and po_hdr.po_no
sorry, i was posting partial results as i worked on your problem my last edit showed the queries **joined**
Thanks for all the replies Ill check them all out. Appreciate the info.
While the select seems fine, you are not handling the possibility of multiple rows in your trigger. The argument of "The application won't allow that" or "No one will ever do more than one at a time" is invalid, because it will happen.
your boss is stupid what else does he put in triggers then?
Use photos as an example.... So when viewing the map you can see "10 photos taken here".. Click on it and view the photos
That's a good catch, OP could rewrite the trigger to utilize a TVP with the stored procedure.
Unix timestamps don't have a timezone.
When I was first learning SQL, it really helped when I realized that you can think of a subquery as if it's a table. So when you see select a.stuff ,b.other_stuff from table_a a join ( some subquery here ) b You can think of "b" as a table containing the results of whatever subquery is in parentheses there. Data-relation-wise, this way of thinking about subqueries is perfectly valid. However, it's worth keeping in mind that an actual database table might have some additional features that subquery results don't, like indexes, partitions, etc. That stuff only matters for query optimization, though.
&gt;but their way of writing the Subqueries is so complicated What are they doing? Is there somewhere in particular you're getting stuck? The easiest way to think of subqueries is to think of them as tables. That's essentially what they produce, tables that you can use in your main query.
While it is pretty much always possible to go multiple routes in SQL, I say why not go the easy route first? So you need all 8,9 "OR" if they are a 6 and have (there EXISTS) a manager of level 9. Ok, so write it that exact same way - look into EXISTS condition if you don't know about it already.
A trick I've used in the past when you can't think of a good way to partition data is to synthetically create a a partition count by using a rolling sum. Here is a quick and dirty version of the query result: ;with d as ( select * from (values ('Bob','a',{t'09:00:00'}) ,('Bob','X',{t'10:00:00'}) ,('Bob','a',{t'11:30:00'}) ,('Bob','X',{t'12:45:00'}) ,('Bob','b',{t'13:15:00'}) ,('Susan','c',{t'14:05:00'}) ,('Susan','a',{t'15:00:00'}) ,('Frank','a',{t'15:20:00'}) ,('Frank','X',{t'16:10:00'}) ,('Frank','X',{t'16:55:00'}) ,('Ashley','X',{t'18:00:00'}) ,('Ashley','X',{t'20:00:00'}) ,('Ashley','a',{t'21:00:00'}))f(name,letter,time)) ,x_counting as ( select * ,x_s=sum(iif(letter='x',1,0)) over (partition by name order by time) from d) select distinct name ,event_x=(select min(time) from x_counting b where x_s=1 and b.name=a.name) ,event_a=(select min(time) from x_counting b where x_s&gt;=1 and letter='a' and b.name=a.name) from x_counting a There are certainly ways to improve this solution but the main trick is the x_counting CTE.
Researching EXISTS now :)
Exactly that. They are essentially views that you define in your query, instead of before. Of course, the more difficult part is understanding why you need a subquery. That just comes with experience.
consider a subquery by itself -- it's just another query and like any query, it has a result when it's executed the result of a query is a number of rows of a number of columns in other words, a table of results you might see a subquery like this -- SELECT ... FROM ... INNER JOIN ( SELECT ... FROM ... ) AS t this is a subquery producing a table of results which is then treated as a table (called `t`) or a subquery like this -- SELECT ... FROM ... WHERE x IN ( SELECT ... FROM ... ) this subquery produces a table of only one column, which acts exactly like a list of values for the syntax of the IN operator and a subquery in the SELECT clause -- SELECT ... , ... , ( SELECT ... FROM ... ) AS c FROM ... this is called a scalar subquery because it produces a single value -- the subquery produces only one row consisting of one column
I think you're most of the way there. I suspect you're running into issues with UNION because you're trying to union two sets that have a different number of columns or different column names. I don't know what dbms you're on so I'll use a description/pseudocode. Let's call your query that gives you level 8 and 9 employees "query89" and your query that give you level 6 employees who report to level 9s "query6". To get your requested result, you can do: select EmFirstName, EmID, EmMgrID, Er.RnkLevel from ( query89 ) UNION select EmFirstName, EmID, EmMgrID, Er.RnkLevel from ( query6 )
Kind of a tangent: it always blows my mind that most SQL courses "emphasize" the set-based nature of sql yet fail to highlight that tables are stored datasets, naturally.
Believe it or not, you've just simplified the whole thing for me, better than anyone, interesting when looking at it this way. Thanks for responding so quickly.
Sorry, should of mentioned I'm doing this from SSMS.
I can't seem to get this to work properly. I gave both SELECT statements the following; SELECT \* INTO Level89 SELECT \* INTO Level6 So that they have a name to pull with the code you provided above but alas it doesn't seem to want me to do it this way. Lol. That or I'm retarded. I'm pretty new to SQL
Here's an older post that shows several ways to do this. [https://sqlperformance.com/2014/08/t-sql-queries/sql-server-grouped-concatenation](https://sqlperformance.com/2014/08/t-sql-queries/sql-server-grouped-concatenation)
Any encrypted data will be deleted - at a minimum this means saved passwords for data sources, etc.
I’m assuming you have latitude and longitude as your underlying geolocation. If so, PowerBI does have a nice tooltip feature that will let you hover above and display different info. I’m not sure if it will store images that will let you directly link from the tooltip, but it’s worth a shot. Excels 3D map feature (I believe it was called power maps or is now power maps and formerly 3D) .... is awesome and free - but the tool tips are static and not clickable
Thanks, man! &amp;#x200B; I just figured it out and am using the following code: &amp;#x200B; WITH CTE ( CategoryId, product\_list, product\_name, length ) AS ( SELECT PROSPECTPLANDIMID, CAST( '' AS VARCHAR(8000) ), CAST( '' AS VARCHAR(8000) ), 0 FROM #PLAN\_REPORT\_SITE GROUP BY PROSPECTPLANDIMID UNION ALL SELECT p.PROSPECTPLANDIMID, CAST( product\_list + CASE WHEN length = 0 THEN '' ELSE ', ' END + SITENAME AS VARCHAR(8000) ), CAST( SITENAME AS VARCHAR(8000)), length + 1 FROM CTE c INNER JOIN #PLAN\_REPORT\_SITE p ON c.CategoryId = p.PROSPECTPLANDIMID WHERE p.SITENAME &gt; c.product\_name ) SELECT PROSPECTPLANDIMID, product\_list into #site\_testing FROM ( SELECT CategoryId, product\_list, RANK() OVER ( PARTITION BY CategoryId ORDER BY length DESC ) FROM CTE ) D ( PROSPECTPLANDIMID, product\_list, rank ) WHERE rank = 1 ;
The most common legitimate use for cursors is for configuration changes or DDL. For example, I've used them before in a stored procedure for an application in SQL Server that uses user-based DB authentication. So each individual user must have a server login, database user, database schema and set of views created. This is part of how the system's security works. The statements that do this sort of thing don't accept variables for the object names, so you have to use dynamic SQL to automate them. The procedure enumerates the members of an Active Directory security group and then uses a cursor to create the necessary objects or disable the login if the user is no longer present. It's an anachronistic system design that predates the web, but it's a financial system so having old designs is extremely common. The other common use for cursors are when you want to minimize the number of rows you're locking at all costs. I've seen systems dump stuff to a temp table to stage the data and then use a cursor to update a live table from the temp table row by row. When I asked why they didn't use a MERGE statement or a combination INSERT/UPDATE, the response was, "This way minimizes locking contention, and we can more easily handle exceptions on a case by case basis". The draw back is that, while you're not risking page or table locks instead of row locks, your processes all take a lot longer and you risk changes being lost due to sequencing if two people are modifying the same item (i.e., you exacerbate the lack row versioning). In practice that doesn't happen in this system due to the number of users and what they modify, but it is a possible issue. IMO, the real reason that cursors are so common is that many developers can't program in anything other than imperative terms.
Thank you guys, everyone of you, your response helped me a lot!
You still need the log file. If you are copying source data down consider looking in to bulk insert over standard insert. As stated elsewhere you need to fix the VLFs from that earlier growth. Pedro Lopes from MS has a good script here that will do the growth iterations for you. https://github.com/Microsoft/tigertoolbox/tree/master/Fixing-VLFs
Glad to hear it works for you. :)
Use Query Designer in SSMS if these people don't have an IT background. I think people find a visual representation of a query easier to understand than just code
Glad to hear it works for you! :)
Ah, Perfect! Thanks! Is there a solution verified
Providing context and assuming nothing is incredibly powerful. As in, does everyone in the room know what SQL is? Not what the acronym stands for, but how it gets used alongside a database and/or BI software? Why it’s such a powerful tool? Are there pros and cons against other data tools (e.g. Spreadsheets, Python, etc.)? This will help you get people on the same page, and could be a good way to introduce your presentation. You’ll be surprised how many people can have an epiphany moment by just slowing down and digging deep on context. After that, possibly just a light introduction on how SQL communicates with a database (e.g. you have to use the word FROM to tell the database where you want pull data FROM), and the basic “rules” of sequence (e.g. you can’t put FROM before SELECT). After that I’d say pick 2-3 SQL tasks that are incredibly common...and allow the group to help you build them live: Filtering, Sorting, and Counting (just a few examples) There’s obviously a lot you “could” work through, but if it’s just one or two meetings, I’d say err on the side of context and simple, but incredibly common techniques. Self-less plug: we’re building out a data education roadmap at sfdataschool.com, where SQL is one of the tools we focus on
I've been working with SSMS for bit, mainly implementing code written by others. I could never understand how people would write these complex queries with wacky syntax/spacing that differed from what I saw them doing live. I had no idea Query Designer was a thing until you posted it here. It answers so many questions and allows me to be much more productive. Thanks!
Running a stored procedure once per row, That’s normally when I see people use them. Also, if you are working on an app with a crappy schema that creates a table every time it’s run for its output and you need to join all of those together but you’re pulling the table names out of a field in another table, that’s one.
UPDATE: I think I've found a good way of doing this but I'm having issues still filtering. SELECT EmFirstName, EmID, EmMgrID, Er.RnkLevel FROM database..tblEmployee (nolock) Em INNER JOIN database..tblJobTitle (nolock) Jt ON Em.EmTltID = Jt.TltID INNER JOIN database..tblplEmployeeRank (nolock) Er ON Jt.TltRnkID = Er.RnkID WHERE Em.EmActive = 1 AND Er.RnkLevel IN (8, 9) UNION ALL SELECT EmFirstName, EmID, EmMgrID, Er.RnkLevel FROM database..tblEmployee (nolock) AS Em INNER JOIN database..tblJobTitle (nolock) Jt ON Em.EmTltID = Jt.TltID INNER JOIN database..tblplEmployeeRank (nolock) Er ON Jt.TltRnkID = Er.RnkID WHERE Em.EmActive = 1 AND Er.RnkLevel = 6 AND EmMgrID IN ( SELECT EmMgrID FROM database..tblEmployee (nolock) WHERE EmMgrID = EmID ) This returns all level 8's and 9's, and if I take out that last AND statement it returns all level 6's as well. Any ideas why this statement isn't just keeping the Level 6's with EmMgrID's that match any EmID?
Tried a few different exists statements and it all ends out the same. No results.. Not sure. :/
This post makes me angry. &amp;#x200B; ...Not because it's a bad post, no. &amp;#x200B; But PEAR. &amp;#x200B; ....PEAR... PEAR TO PEAR COMMUNICATIONS
Here is the deal. Essentially they are going to get access to a few stored procedures. You would really just be teaching them Database basics and fundamentals of the stored procedure. They need to know that it's not just a random number generator and the data is real. That's it.
Do you understand what /u/r3pr0b8 did?
Be careful with it. It can choose some piss poor join operations at times, which can make subsequent debugging of the query a living hell. ex. for whatever reason, I've seen it decide to do the following, vs my correction, more times than I can count. select &lt;columnlist&gt; from tab1 RIGHT OUTER JOIN tab2 INNER JOIN tab 3 LEFT OUTER JOIN tab4 ON tab3.col = tab4.col ON tab2.col = tab3.col ON tab1.col = tab2.COL LEFT OUTER JOIN tab5 LEFT OUTER JOIN tab6 on tab6.col = tab5.col on tab2.col = tab5.col Tab6 &amp;#x200B; the way normal ppl would write it. select &lt;columnlist&gt; FROM tab2 INNER JOIN tab3 ON tab3.col = tab2.col LEFT OUTER JOIN tab4 ON tab4.col = tab3.col LEFT OUTER JOIN tab5 ON tab5.col = tab2.col LEFT OUTER JOIN tab6 ON tab6.col = tab5.col LEFT OUTER JOIN tab1 ON tab1.col = tab2.col
I came here to say this.
Your life will be better if you forget that you ever heard of Query Designer.
Your client handles this, or at least it should. For example, an application or web server from the UK has it's local time at UK time. The SQL result from your database in the US to the application or web server in the UK will (should) use the time information to 'translate' it to local time. You can test this by changing the time on your client and doing the same SQL. &amp;#x200B; This issue CAN cause nightmares when trying to synchronize data over different timezones. Thankfully I now work for a company that only works in UTC (all our servers around the world use UTC).
This is hands down the easiest way to pick up the basics: https://sqlbolt.com/
Recursion!
I find you have to cover what are tables and fields and what is a relational database first. And teach on data they know wherever possible.
I strongly suggest not plunking ad-hoc SQL into PowerBI / Microsoft Query. Much better for dependency tracking to create a view/sproc to then draw the data from into Excel/powerBI
https://www.linkedin.com/learning/search?keywords=Sql Highly recommended
Sum(case(when cond=true then 1 else 0 end))
Running built in commands to reindex/rebuild all indexes that need it across all databases within an instance. Best way for me has been a fast-forward, read-only cursor populated from sys tables.
Why do you find the cursor route more elegant?
Also, would they not be a bit more efficient on a temp table?
I think that it is kinda obvious that **data**base that is **set** based will contain data sets. Or is there something I'm missing here?
The examples you've entered make it seem like you're trying to do this in Excel. Can you clarify if you're talking about Excel, or in SQL running queries against a database?
Kudvenkat SQL tutorial on YouTube.
Where did you go to begin learning?
I had a DBMS course at my university, a couple of udemy courses, and w3schools. I’m comfortable with basic commands, but I’m looking for more difficult exercises/challenges.
I've been looking at [Datacamp](https://www.datacamp.com/tracks/sql-fundamentals?utm_medium=email&amp;utm_source=customerio&amp;utm_campaign=sql_fundamentals) myself, though I haven't had time to do this course. It was recommended to me by someone who saw my code worked, but was pretty messy -- i.e. there are easier ways to do the things I was doing. Don't know how much it might rehash what you've already learned, though.
So I decided to try this route here for the trigger: SELECT @Command = N'ALTER TRIGGER [dbo].[' + @TriggerName + '] ' + 'ON [dbo].[' + @TableName + ']' + 'FOR INSERT, UPDATE, DELETE ' + 'AS ' + 'IF (SELECT Count(*) ' + 'FROM inserted) &gt; 0 ' + ' BEGIN ' + 'IF (SELECT Count(*) ' + 'FROM deleted) &gt; 0 ' + BEGIN ' + --'-- Update ' + 'INSERT ' + @AuditTableName + ' SELECT *, '+ '''U'''+ ' FROM inserted ' + 'END ' + 'ELSE ' + 'BEGIN ' + --'-- Create ' + 'INSERT ' + @AuditTableName + ' SELECT *, ' + 'C' + ' FROM inserted ' + 'END ' + 'END ' + 'ELSE ' + 'BEGIN ' + --'-- Delete ' + 'INSERT ' + @AuditTableName + ' SELECT *, ' + 'D' + ' FROM deleted ' + 'END ' EXEC(@Command) FETCH NEXT FROM @tableCursor INTO @TableName, @AuditTableName, @TriggerName END I put it in a cursors and am looping through the triggers to unify them all. When I run this, it complains 'Column name or number of supplied values does not match table definition.' but if I run it against the table on it's own, it works just fine. Any thoughts?
The Microsoft 70-761 exam reference book is really good for gaining an understanding and helping you become proficient at most (if not all) aspects of querying databases: [https://www.microsoftpressstore.com/store/exam-ref-70-761-querying-data-with-transact-sql-9781509304332](https://www.microsoftpressstore.com/store/exam-ref-70-761-querying-data-with-transact-sql-9781509304332) Once you've read the book and have a good understanding you can always sit the exam as well. The book uses a couple of different open source databases (Adventure Works and Wide World Importers i believe). You can download these (they come with data) and use them to practice/follow along with the book.
https://cs.stanford.edu/people/widom/DB-mooc.html
You're responding in a comment chain started with explanation 'datasets are like tables but not permanent' and you somehow think what you wrote is immediately "obvious" to a regular student? Have you ever seen a description of relational database as "containing datasets" vs "a collection of tables", anywhere?
Thank you for sharing this. This is exactly what I did. Maybe the SQL formulas I did are wrong. I should share the database I am working on and maybe if you could look into it if you some time, that would be awesome: [https://docs.google.com/spreadsheets/d/13BVMqzSp73wl5f7fXxExHM5-ObqKQmNFQHC3qnewxWs/edit#gid=1620227784](https://docs.google.com/spreadsheets/d/13BVMqzSp73wl5f7fXxExHM5-ObqKQmNFQHC3qnewxWs/edit#gid=1620227784) Here are my findings:
Analytical function for rank within gender, and outer join on that?
Isn't this just a count by gender over a partition by order? If you're using Oracle, you can make two columns for this or just concatenate them together: &gt;!count(decode(gender, 'M',1, null)) num_male, count (decode (gender, 'F',1, null)) num_female!&lt;
In addition to this excellent response, I'd also add that if you're looking for specific dates it's usually easier to use a date literal rather than converting back and forth between date/text. DATE 'YYYY-MM-DD' For example, to represent March 1st, 2019: DATE '2019-03-01'
That error suggests that their is a mismatch in columns between your source table and destination table, which is clear because you are appending an extra column in your insert. Dredging back through my memory (thus possibly incorrect) you might be able to correct this by using an AS ColumnName next to the extra column where ColumnName is the name of the column in your audit table that you are inserting into
I was asked to teach a bunch of business people SQL, so I showed then PowerBI in Excel instead. I then showed them various pivot tables, and taught them the SQL needed to replicate what they were seeing. It was reasonably effective.
What's the question?
I don't even know what I'm doing anymore... select 'F: ' + cast(count(*) as varchar) + ' M: ' + coalesce(max(m_count),'0') from [order] o left join ( select orderid, cast(count(*) as varchar) as m_count from [order] where gender = 'M' group by orderid, gender ) as m on m.orderid = o.orderid where o.gender = 'F' group by o.orderid
Need more information. Where will this column go? What will it look like next to all the others?
There has to be more columns. Is there an employee I'd or something? Is it just a column of every possible time?
There are some other columns, but these are all thats important. There are no keys to do a "normal" join on which is the complication here. I have the Time the action was taken in table 1, and I have to match it with the Shift number in table 2. All that the two tables share to compare is that time. So as an example 6am-6pm on Monday might be shift 1, 6pm monday to 6am tuesday might be shift 2. If transaction A happens at 1pm Monday, I need a value of "1" and if something happens 11pm Monday or something I need a "2" The shift schedule is quite complicated which is why we have this table, and there's not really any sort of expression I can use to compute it
Have you heard of the SQL Cookbook?
 declare @time = (select time from ...) declare @shift_number = (select shift_number from table where @t between shift_start_time and shift_end_time) mayhaps?
I did an MSSQL version using a CTE. It was a fun exercise. [SQL FIDDLE](http://sqlfiddle.com/#!18/db0c2e/46)
That might get me on the right track, thanks!
It says I have to log in. Does anyone have the login info??
At least it isn't System.Data.SqlClient**i**
From what I can tell, it sounds like you would have better results and more flexibility handling these dynamic filters or slices on the reporting/dashboard layer, rather than directly in the query. Whether you're using Excel with pivot tables and slicers or a data visualization tool such as Tableau or Qlik, they should allow allow for easy filtering and slicing by one of many dimensions at once.
Just remove the group by and add a distinct
Weirldly enough though, if that ALTER TRIGGER statement is run as a standalone query it executes just fine.
Something like this, I believe... Table_1 thing_that_occurs, time, ... Table_2 Shift_Number, Shift_time_start, Shift_time_end Select t1.thing_that_occurs, t2.shift_number from Table_1 t1 Join Table_2 t2 ON t1.time between t2.shift_time_start and t2.shift_time_end;
You need a LinkedIn account and it will cost $
I second this. Kudvenkat’s videos are so perfect and clear.
I'm thinking this is for something related to a Customer Support/Success business unit? Maybe I'm way off haha, but sounds like exactly something I had to build for a CS business unit (i.e. Shift). Anyway, here's how I accomplish this: SELECT * FROM event_table LEFT JOIN shift_table ON event_table.person_id = shift_table.persion_id AND event_table.time &gt;= shift_table.shift_start_date AND event_table.time &lt;= NVL(shift_table.shift_end_date, GETDATE()) The NVL is used to cover where shift_end_date is NULL (i.e. that person is still current in that shift) Self-less plug: just by this post it sounds like you're SQL prowess is above the curriculum we serve (as of now haha), however check out sfdataschool.com as we're continuing to build-out SQL tutorials and exercises
I've done a number of different Lynda.com (now LinkedIn Learning) courses over the years, and honestly I don't recommend them. They try to pack as much info into as little time as possible, so over the course of three hours you'll learn 50 things with very few examples, no best practices and in some cases no real understanding of when and when not to use them. Especially for OP who's asking for challenges - these are the exact opposite. They make everything sound easy and straight forward, and then you try to apply the concepts and realize you have the barest of knowledge. Honestly I'd say check out MS training or any of the itzik ben-gan books first.
Time of what though? That doesn't make sense. ITime of the shift punch? What are the other columns?
It's possible it's a scoping. Try writing out the commands rather than running then inspect them maybe a wrong value is showing up some where
100%. you will be "forced" to learn the core fundamentals and it will show when you actually do real work. also helps to do 70-762 after to under why or how some things are happening when you query. i think thats the one that talks about indexes/query optimizer. people shit on certifications but they force you to learn the fundamentals, at least the microsoft sql server ones
Try strata scratch! I like the most on this platform is providing expert help and guidance to help us with our technical interview questions.
I'm no longer at work so I don't know for sure if this works but it looks like exactly what I need thanks!
Not really the same, but that helps a little. It's for manufacturing so not really the same, its a bit of a unique situation.
Select count(column\_name) from table where column\_name = uniqueid
if you wanted to use count you could also use: `count(case when cond=true then 1 end)`
Because you don't have to concern yourself with the row index and because the cursor can also be dynamic and reflect the changes, while the aforementioned temp table can only be a cached result set. You can google for various comparisons that show that cursors, when used correctly, actually outperform while loops. Syntaxwise, I would compare it with *for* vs *foreach* loop in other languages.
You cannot do that with stored procedures in SQL server. And functions can't change the database state and are not relevant in my example.
Looks like a simple filtered aggregate: select count(*) filter (where gender = 'M') as male_count, count(*) filter (where gender = 'F') as female_count from orders group by orderid; You can concatenate the prefix `'F:'` or `'M:'` to the result columns.
Without knowing why you need this custom column, we can't really help you. That said, why don't you just count and group them? select count(gender), gender from sampletable group by gender
In standard SQL you can apply a FILTER on an aggregate: count(*) filter (where some_column = 42)
In your first example that "sub-query" is also often referred to as a "derived table".
Udemy.com
Alright, thanks for your explanation.
RemindMe! 1 day
I will be messaging you on [**2019-05-10 10:27:03 UTC**](http://www.wolframalpha.com/input/?i=2019-05-10 10:27:03 UTC To Local Time) to remind you of [**this link.**](https://www.reddit.com/r/SQL/comments/bmhcfj/sql_certificate_london_uk/emwoaui/) [**CLICK THIS LINK**](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[https://www.reddit.com/r/SQL/comments/bmhcfj/sql_certificate_london_uk/emwoaui/]%0A%0ARemindMe! 1 day) to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete Comment&amp;message=Delete! emwoe5a) _____ |[^(FAQs)](http://np.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/)|[^(Custom)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[LINK INSIDE SQUARE BRACKETS else default to FAQs]%0A%0ANOTE: Don't forget to add the time options after the command.%0A%0ARemindMe!)|[^(Your Reminders)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List Of Reminders&amp;message=MyReminders!)|[^(Feedback)](http://np.reddit.com/message/compose/?to=RemindMeBotWrangler&amp;subject=Feedback)|[^(Code)](https://github.com/SIlver--/remindmebot-reddit)|[^(Browser Extensions)](https://np.reddit.com/r/RemindMeBot/comments/4kldad/remindmebot_extensions/) |-|-|-|-|-|-|
I started with SQL Queries for Mere Mortals. It is a "hands on" approach and dialect neutral. The book comes with code samples of very simple databases in several dialects: MS Access, SQL Server, and MySQL. It's a great basics book.
In a table?...
Obviously that's an option. I'm referring more to a server level field, like where the server name, instance name, version information, etc is stored. I didn't want to create a table just for one field if there was another place to put it.
Just a quick thought, if your bare metal or vm's have a standardized nomenclature, you can use SELECT @@SERVERNAME SELECT HOST_NAME()
Maybe set an extended property on something in the `master` or `msdb` database?
I believe so. In some examples I see on SQL it is just a join statement opposed to this inner join. I’m trying to learn all my joins and where and when I would use them - I’ve yet to find a real life example of where I would need say.. an outer join. But yes, inner join is my match to what I would call unique identifier and pull in related results to that. Am I close?
The only certifications that carry _any_ weight in the industry are those offered by the vendors themselves. Both Microsoft and Oracle offer certification exams. You can self-study for both of them, there are books and other materials for it. Finding an in-person class for professionals that only takes place on the weekend would be akin to finding a unicorn pooping rainbows into a pot of gold.
any other approaches are welcomed. But my manager told me to use a CASE clause.
Why not just use [IN (assuming SQL Server)](https://docs.microsoft.com/en-us/sql/t-sql/language-elements/in-transact-sql?view=sql-server-2017)? SELECT abstract FROM tablename WHERE abstract IN ('ABCDE','ABCED','BACDE') Or you could turn up the fanciness with [LIKE](https://docs.microsoft.com/en-us/sql/t-sql/language-elements/like-transact-sql?view=sql-server-2017) OR abstract LIKE 'ABC%' You could also look into regular expresses if your platform supports them.
Thanks very much, this fits my needs exactly. The null first row is completely fine, I can just filter it out with a WHERE.
your manager is dumb. a case is not how this should be approached.
Assuming Typo but search for regular "expression"
I used a simple like command and typed in all the possible combinations, and I got the right output. Manager is kinda weird, I know.
The correct way to do this is to implement Master Data Managment and use synonyms. Then create appropriate rules. So if someone types "Roddit" it will translate to "Reddit" when correctly configured. Quick and dirty way is to put all possible typos in a IN clause, Select * from TableA where Value IN ('ABC','EFG','HIJ') However I would be tempted to place all typos into a table and left join on that to get the correct value. Placing it in a table (or even better MDS) abstracts you data from your code, makes it easier to update and opens the possibility for end users to maintain.
&gt; Why not just use IN (assuming SQL Server) news flash, `IN ( list )` is standard SQL, and works in plenty of other SQL platforms besides the one from Microsoft
I don't think it is possible to use a CASE statement in a WHERE clause. I agree with the others, using a LIKE or IN operator in the WHERE clause is a better approach. However, if you wanted to create a CASE statement to influence a WHERE clause, use a CTE like this: `WITH CaseCheck AS` `(` `SELECT` `CASE Val_to_Check_Col` `WHEN 'USD' THEN 1` `WHEN 'EUR' THEN 0` `WHEN 'BRL' THEN 1` `ELSE 0` `END AS Val_Check,` `Col1, col2,` `FROM` `table_name` `)` `SELECT * FROM CaseCheck WHERE Val_Check = 1;`
I meant the link was only going to be relevant if they were using SQL server, but ok.
SQL Server uses `SELECT TOP N` But remember that just like any other RDBMS, your `TOP N` will be nondeterministic unless you also specify `ORDER BY`.
SELECT TOP 10 bar FROM foo ORDER BY something
\u\alinroc is correct, but to clarify, the query would end up looking like: `SELECT top 10 bar FROM foo ORDER BY something DESC
Additionally if your SQL Server is 2012 or higher you can use OFFSET/FETCH instead of TOP N. It also requires ORDER BY SELECT * FROM Bob ORDER BY Roger OFFSET 0 ROWS FETCH NEXT 10 ROWS ONLY If you change the 0 offset or make it a variable you can change elsewhere you change where there select statement starts pulling from. if you change the fetch 10 or make that a variable you can change elsewhere you change how many rows are returned.
Yes, you can (but agree with others LIKE or IN): SELECT Col1 FROM table WHERE CASE WHEN Col1 = 'ABCDE' THEN 1 WHEN Col1 = 'ABCDEF' THEN 0 WHEN Col1 = 'ABDE' THEN 1 ELSE 0 END = 1
Depends on what you are considering typos. Is it just out-of-order characters, like 'ACBDE'? What if the string is only 'ABCD'? What if it's 'ABCDEF'? What if the user's hands were slightly off-center, so the string is 'SNVFR' instead of 'ABCDE'? There are many combinations/permutations to consider, and I'm not convinced that CASE is the way to go unless you have fairly strict interpretation of what are considered typos.
that looks like it should work to me, but what if you try: &amp;#x200B; WHERE CASE WHEN ci.IMEI\\\_VAL IS NULL THEN 1 ELSE 0 END + CASE WHEN ci.ESN\\\_VAL IS NULL THEN 1 ELSE 0 END = 2
&gt; no matter how I've tried the record selects if ci.IMEI_VAL is NULL regardless of what's in ci.ESN_VAL I would suggest adding both to your select list (along with select c.PGM_ID , c.CNTRCT\_ID , 'Missing IMEI or ESN' RPT and verifying your hypothesis
This might be the closest thing you can do
Didn't read through your entire query and I'm not much of an Oracle guy, but at first glance you are using aliases from your subquery in your order by clause. I know SQL Server will not take that, not sure about Oracle.
Do you mean the "DESC"? That's just the syntax in Oracle to tell your ORDER BY to sort in descending order.
&gt;use this "ORDER BY course\_name, duedate DESC" at the last line
I think he means "asec" is an alias for the table called assignmentsection and is found in your order by clause and he's questioning whether or not your outer query knows about the inner query's alias.
Okay taking the aliases out definitely fixed *something* the error code isn't a wonky column anymore. It now reads this: ORA-00918: column ambiguously defined 00918. 00000 - "column ambiguously defined" *Cause: *Action: Error at Line: 1 Column: 8 Should I define each column in the subquery instead of using a start and see if that works?
Hahaha yeah, it's true.
Yes I was mistaken at first glance, this was part of the issue. It still isn't a fan of the SELECT * though.
no, lest start from the begining. Leave the aliases in the inner query. Then replace the order by with what I suggested. Then tell me what is wrong, and we will continue
This did seem to be the issue, defining each column and using the alias for the columns with the same header *seems* to have fixed the issue. The query runs now, but I may have another error, but it has nothing to do with the subject of this post, so I will say that this is solved. Thanks for the help!
This did seem to be the issue, defining each column and using the alias for the columns with the same header seems to have fixed the issue. The query runs now, but I may have another error, but it has nothing to do with the subject of this post, so I will say that this is solved. Thanks for the help!
You now have two columns named "name". tc.name, sta.name need to be aliased to something unique. I know Oracle hates duplicate names.
It definitely works in "summary" query.
for organization you may want to alias your table (your in line view in your from statement. Can you run the inner select query by itself? If you alias the table as "t", can you select t.somecolumn rather than *? What about t.*? I'm not as my computer so I can't test anything but I'm not seeing anything obvious that jumps out at me, even though there is something.
Lol, I was willing to forgive the typo in the title but then I saw it in the summary and lost my shit, haha.
Yes, this ended being what solved it. I gave the duplicate headers unique aliases and used a hard-coded SELECT instead of a SELECT * and it worked.
At least attempt the question and post questions if you fall short. This sub is not here to do your homework for you.
Since you've not explained what you already tried, I'll just hint that example E is relevant [https://docs.microsoft.com/en-us/sql/t-sql/language-elements/exists-transact-sql?view=sql-server-2017](https://docs.microsoft.com/en-us/sql/t-sql/language-elements/exists-transact-sql?view=sql-server-2017)
FQDN... Hostname.datacenter.company.com
Off-topic, but: Wow. Misspellings galore. Also, the Design Tip makes no sense - why say "vice versa" in that situation? There's never a time a property (or propery) would be sold BECAUSE it was put in the Sales table.
Show them something something they do already, but can do more efficiently and flexibly with SQL. A lot of business people need to join information from one spreadsheet to another and routinely do so in Excel with VLOOKUPS or INDEX and MATCH. When I needed to teach co-workers SQL, I started with this practical problem and then showed them how much better SQL was at joining data, filtering, aggregating and sorting. For instance, VLOOKUP only grabs the first matching value, takes a ton of processing power, you sometimes need to make your own composite keys.
but you are already renaming the columns -- e.g. \` SELECT da.ip\_address AS "IP" \`
 begin select property_id, primary_purpose, locality, nvl(area,'N/A') as 'Area' from Properties a, Localities b, Areas c where a.locality_id = b.locality_id and a.area_id = c.area_id and a.property_id not in ( select distinct 'Y' from sales d where a.property_id = d.property_id ); end; /
You are already renaming the columns in your select when you do the "xxx AS yyy". Also, in your joins...the 2nd join with dim\_site\_asset....you haven't included the alias (which I think you intended to be 'da'). It should look like "JOIN dim\_site\_asset da USING (asset\_id)". I'm just a casual user though....someone in this sub will have the correct answer for you soon enough.
&gt; and a.property_id not exists when 'not in' and 'not exists' collide...
okay, so what can I do about the error I get? &gt;common column name "asset\_id" appears more than once in left table