Well, here are a few things to try: 1. Pay lag: let's say you have an orders table with order date, paid date and order amount. Build a matrix by order month/paid month with total order amount in the cells. What are you going to do when there are no orders in a specific month or no orders paid in a month? 2. Using the prior set as your sample, figure/show statistical probability that an order gets paid within 30 days 3. using the same set, figure out payment probability within 0-14 days, 15-30 days, 31-60 days, 61+ days by the order amounts grouped by quartiles (0-24% of max order amount, 25-49%, 49-74%, 75-100%)
&gt; SELECT &gt; a.[Acct No], &gt; a.Company, &gt; a.[Email Address], &gt; a.[Primary], &gt; a.rn &gt; FROM &gt; ( &gt; SELECT &gt; Jeremy_Portal.ACCT_NO AS [Acct No], &gt; Jeremy_Portal.COMPANY AS Company, &gt; dbo.contact.EMAILADDR1 AS [Email Address], &gt; dbo.contact.[PRIMARY] AS [Primary], &gt; ROW_NUMBER () OVER (PARTITION BY Jeremy_Portal.ACCT_NO, dbo.contact.[PRIMARY] ORDER BY dbo.contact.[PRIMARY]) AS rn &gt; FROM &gt; dbo.Jeremy_Portal AS Jeremy_Portal &gt; INNER JOIN dbo.contact ON Jeremy_Portal.ACCT_NO = dbo.contact.ACCT_NO &gt; WHERE &gt; dbo.contact.EMAILADDR1 &lt;&gt; '' &gt; ) AS a &gt; WHERE &gt; a.rn &gt; 1; --Exclude the first row? After some tinkering with this, I got it to do exactly what I wanted. I took out the Primary field because when I had: Acct No Company Email Address Primary 12345 test test@test.com A 12345 test test@test.com B it would count each of those at 1 row when I needed it to count the second one as row 2, I also switched the a.rn &gt; 1 to a.rn&lt;2 because I just wanted the first row. Thank you very much for your help, it was a lifesaver. 
While we try to get across best practices and how we do things, the purpose of the training sessions to get enough knowledge out there for someone to pass the 70-461. They are not targeted at a specific skill level, they just try to convey the areas covered by the test.
Cool story bro. 
What is your actual question? 
I personally swear by Bactine
I'd throw it back at the user and tell him his requirements are incomplete. Joking aside, 10 independant inputs, and you want to spit a number out the end. It kinda lends itself to a neural network approach if you want to get fancy. Otherwise sure, a series of IF statements would probably do, but expect it to become messy 
Easy way: Calculate each of the products that meets and join them together. Will probably run slow. Hard way: Build dynamic SQL based on the criteria. Will probably run faster.
Thanks!
You can check the execution plan for both the update statements and find out why the the one with the subquery has a higher cost. And also SET STATISTICS ON for your query analyzer and see the data it throws out.
* The first statement likely has a very expensive nested loop join because of the way it is written and having a variable in the sub-query that can't be estimated. While the other one is precalculated only once. * The second query can take advantage of an index searching much better than the first one; first one will probably have to do a full table scan while the second will be limited by an index seek. 
Couple questions.... 1. Depending on your SQL server collation setting you could have an issue with @ChildID 'LowestLevelID' as it is uppercase C here but lower case c in the declare at the beginning. 2. There isn't a CONTINUE before the END statement in your while. I've only ever seen these have a CONTINUE statement, it may be ending early.
Is this being taught in a class? This is /r/SQLWTF material. 
That video would be for anyone who was interested in learning SQLite. I already said that the problem had been solved. 
Ah okay, apologies. So if you add PRINT @ChildId PRINT @LowestLevelName both within the stored procedure and after the SELECT TOP 1 @childId etc. do they print as expected? What happens if you add a WHERE ID = 'something' AND NAME = 'somethingelse' clause to SELECT TOP 1 ... FROM dbo.Entity To guarantee that @ChildID equals something that works? Does the top level of your hierarchy have a NULL parentId? Should your WHILE clause SELECT Top 1 have an order by (at least to guarantee deterministic selection)?
This makes some sense, though I still would never have guessed it would be 200 times as efficient.
Welcome to the [Accidental DBA](https://www.sqlskills.com/help/accidental-dba/) club. P.S. [#SQLHelp](https://twitter.com/hashtag/sqlhelp) on Twitter is a resource not to be underestimated. 
The second statement is likely getting some benefit from caching. You're operating on the same data twice regardless of the query plan. Back to back like this and on a relatively small number of rows it's probably not going back to disk for the second statement.
*"Jorge Bush"*, that is genius.
One major difference between the two is that in the first you're updating the same table that you're querying, so there's likely some contention for resources going on. In the second you've copied your IN clause data to another table, and thus no contention on write.
How are you going to associate all the different customer #s back to the same customer? If customer 1234 has six different entries in a project, how are you going to track that all six of those entries tie back to customer 1234?
I understand that. But some day someone will be searching reddit for a sqllite problem they are having, or a tutorial and they will find your post and they will have to watch a video to get what ammounts to a 10 second answer. I did, and it annoyed me. I don't even use SQLLite (MSSQL and Oracle) but I wanted to know the answer for whenever I end up using it and have the same issue.
I dont know PostgreSQL, but I think you're might need to look into 'alter table' and 'update' statements. 
damn, I was hoping that was a real subreddit :/
I don't need to track all from B. Just need all the records from A populated with a distinct customer number. If there are only 2 projectnumbers in A, I only need 2 Customer numbers from B(even if B has more available)
Will try. Thanks!
Well, I'm going to query off of the zip code, so i want to know which people live at a certain zip code during a certain year... So i just want to grab everyones most recent address for each year. Then narrow that down. 
That's good info. You didn't quite answer my question, so I'm going to assume "no" based on what you did provide. In that case, you want to use a "group by" with "max". SELECT person_identifier, year, max( seqNum ) FROM address_history GROUP BY person_identifier, year
 * Redundancy is always a plus, push for cross-training and it will help you get your foot in the door when someone else takes vacation. * There really shouldn't be a thing as automating yourself out of job, it exists but most companies that remove a position because of this usually regret it the first time and upgrade goes south and those automated systems need to be updated. * Not sure what your environment is like at work, but it would be worth trying to setup after work team-building (go to a wings / beer / lunch / etc). Sometimes that is all it takes to break down some of the silos people build for themselves. 
That problem should never be addressed in the database, databases are for storage and presentation of aggregate information related to the data stored there. Problems should be coded against the database but handled by the application. Edit - In this case the solution to this problem is obviously a loop and calculating the prime factor sums would be another. Doing this in the database will be a hack job at best and very bad practice. AKA an RBAR situation. Edit2 - Databases are designed/optimized for: * Recursive data operations (trees) * Set data operations, especially relational data * Finding / indexing information
outer joins. http://docs.oracle.com/cd/B19306_01/server.102/b14200/queries006.htm
This is pretty neat. I am definitely saving this for later use.
.... and hope they didn't change location twice in one year =)
Generally I don't like to be negative here, but this sounds like trying to put a square peg in a round hole. The *row by agonizing row* nature of **some** of these problems should never be done in a database; others could be done on either. Just because it's possible to do something doesn't necessarily mean you should. SQL isn't a procedural language, it's set-based. It would be more beneficial to expand your knowledge of RDBMS topics such as: * Windowed functions * Aggregates * Indexing * Pivot Operations * Transaction Types / Logs * Backup Types / Differences * Ranking * Execution Plans and how to use them to detect bottlenecks in your queries (how I answered your question below). It will provide you a feel for when a problem should be done on the application or database side. I've made queries that returned analytical data in seconds where our expensive software solutions ran in excess of an hour in some situations. (example: Anything attending to gaps and islands problems). Even with the problem mentioned you could perform the divisor calculations in a procedural language, but store the results and calculate the aggregate information to determine abundant numbers in the database using a SUM and GROUP BY. **TL'DR** - *Learning to use the right tool for the right job is more important that learning to use all the tools to do the same thing.* 
It's Oracle's own join syntax. Imo you should forget you saw it and use ANSI instead for it being the standard.
thanks! 
http://blog.sqlauthority.com/2009/05/21/sql-server-fix-error-provider-named-pipes-provider-error-40-could-not-open-a-connection-to-sql-server-microsoft-sql-server-error/
Lots of possibilities, but the most likely answer is you can't connect to the server. The intermittent part is curious, but the easiest way to check the issue is look at execution logs on the sql instance and see if there are any gaps. 
Funny, for me it was the opposite way around. I only support Oracle Database as of now (and I am most confident using this RDBMS) but in my earlier career days, I used Oracle, SQL Server, DB2, DB2/400.. you name it, I saw it. That's why I advocate the ANSI SQL.
I think this one might have been overthought a tad, no need for multiple queries spread across several branches of an If tree, a single query and some simple case logic will get you what you need. For example: Select ProductSKU , CurrentPrice , case when currentprice &gt; (CompetitorPrice * 1.1) then (CompetitorPrice * 1.09) else currentprice end as CompetitionReduction , case when (currentprice/cost) &gt; 1.5 then cost * 1.49 else CurrentPrice end as MarginReduction Now you know for each of the ten factors how the price should be adjusted if it's to be adjusted at all. You then dump that into a work table, union the various suggested price columns to get the min value, and then give the boss a list of items and suggested prices. You can also mine the work table for a lot of insightful data as to your pricing methods.
Awesome can we get insert into temp tables next, or maybe timezone functions, maybe sql standard table hints, or proper order of operations in aggregate functions? No, none of those, so we are gonna get another fancy substring function that no one asked for... 
&gt; If SQL had variables capable of holding collections, this wouldn't be an issue. It does. SQL doesn't work with arrays and collections like c# and javascript. It works with tables, temp tables, table variables which are like arrays of classes you define. Many programmers long for the ability to query arrays like they do in SQL and that is where LINQ was built which allows developers to query iEnumerable collections built off arrays.
Looking at this particular simplified version of the query with your schema: DECLARE @a INT = 12 UPDATE dbo.Problem_023 SET Is_Summable = 0; IF OBJECT_ID('tempdb..#Problem_023_temp') IS NOT NULL DROP TABLE #Problem_023_Temp SELECT (Number + @a) AS Num INTO #Problem_023_Temp FROM Problem_023 WHERE Category = 'Abundant' UPDATE Problem_023 SET Is_Summable = 1 WHERE Number IN (SELECT Num FROM #Problem_023_Temp) UPDATE dbo.Problem_023 SET Is_Summable = 0 UPDATE Problem_023 SET Is_Summable = 1 WHERE Number IN (SELECT Number + @a AS Num FROM Problem_023 WHERE Category = 'Abundant') The major portion of time is spent on the last statement in a table spool, with it showing 170 million actual rows just in the one iteration. That's 28123 executions (the number of rows in the table), with 6965 rows per execution (the number of rows where Category=Abundant). Putting the data for the IN clause in the temp table avoids the table spool. The table spool appears to go through that many iterations because of the 'number + @a'.
...I'm stunned. I knew I was working on a really basic level, but this demonstrates just how far I have to go to be effective in this role. It's a very different world from my cozy C# bed. If you have any recommendations for exercises or learning materials, I'd love to get started on them. I'm working my way through the MS Virtual Academy, and I'm aiming to get the MCSA cert by June.
There's not much context here and I'm not sure what your asking. Can other dbms have deadlocking? Yes. If you never encountered it, you may not have had an issue of any operations taking too long or the dbms was setup to read uncommitted data. 
All you really need to know is if any [ID,date] pair has a sum of 25+ in the following 5 years. SELECT distinct d.ID FROM ( Select t.ID ,(select sum(t1.value) from Table t1 where t1.ID = t.ID and t1.date between t.date and dateadd(yy,t.date,5) idSum From Table t ) d WHERE d.idSum &gt;= 25 Now I have no idea what proc SQL is, but you should be able to convert the above T-SQL for your tool. 
Thanks! I'll try and convert this into the SAS procedure.
To be specific, we are talking about when two threads block each other in such a way that neither can finish while the other is still active and locking resources. Yes that can happen in any DBMS, because all DBMS use ACID, and the Atomicity and Isolation are the cause of the error. With it being said, non-optimized entity framework code can exacerbate the issue due to the way it's transaction wrappers work. Using the appropriate retry mechanisms and using appropriate sized transactions will generally help lessen the pain.
Thanks everyone... I was able to pull a working example from all of the feedback. I will post a working example once I am sober. :) forgive me... But it has been a long week and I have a three day weekend off without being on call. It just might be tachila Thursday :) 
that
Is now! :)
Yes! I've put up three things from a quick search, but will trawl through code to find more.
No. The deadlock victims process is killed. You are mixing normal data locking with what a deadlock is. In a deadlock, transaction A holds a lock on an object, which transaction B needs to lock exclusively. At the same time, transaction B holds a lock on an object, that transaction A needs to lock exclusively. So A waits for B to release its lock, while B waits for A to release its lock in order to finish the respective transaction (and as such releasing the locks). This situation will never be resolved by any matter other than killing &amp; rolling back either transaction A, or killing &amp; rolling back transaction B. Hence, the deadlock victim do in fact get killed. The application needs to implement a retry logic, but at least SQL Server will not do an automatic retry.
&gt;My coworker believes that asking SQL to evaluate the subset is going to cause more strain on the database. Who is right? That depends. Do you have good indexes on your table? In general, if you have good indexes and your `WHERE` clause has good selectivity which aligns with your indexes, you are much better off doing it in the database. And that ignores the obvious gains of less data going over the network that you've already demonstrated. You can also use included columns in your indexes to reduce the number of reads required. Half a second to get 1600 records seems a bit slow, to be honest. You can probably make that better.
Ah, you've run into a non-[SARGable](http://en.wikipedia.org/wiki/Sargable) predicate. Yes, if you can avoid doing that math in your `where`clause, it will help.
Correct me if i'm wrong, but it appears to me that you are trying to parse together a statement to log. You should be able to do that easily with something like SELECT @Log = @Log + 'Model Type ' + CAST( ModelTypeID AS NVARCHAR(50)) + ' has been inserted into Model List ' + CAST(ModelListCode AS NVARCHAR(50)) + '.' FROM Payload_New_Log That should replace the entire while loop. 
What YOU CAN DO IT LIKE THAT!? AHGGGGGGGGGGGGG Thank you -_- I can't believe I didn't realize that solution.
Thanks for the info /u/svtr; it's been quite a while since we've seen a deadlock in our system, so I'm a little rusty there. I headed up a massive tuning-spree after a deadlock storm, which was quite rewarding to see the performance difference and db size decrease.
What SQL do you have? There are many, I assume Microsoft SQL?
Cases are what have solved it for me, thank you! Once I clean up the query I will post it for others to learn
.5 seconds vs 20 seconds. So your way runs in 2.5% of the time. If this were a sporting event it would be a blowout of epic proportions. That's your answer right there. Besides, if you return all 106k records x 60 fields, something has to get rid of the excess info later. Which adds to the overall time it takes to get to 1,600 x 10. Even if his query was faster, which it isn't, I'd be shocked if the post-processor was able to get rid of the extra data faster than the SQL server could. (Yes, I realize there is more than just time involved in all this, but this result is so lopsided it's not work talking about the other factors.)
Assuming you may have some dates/stores which exist in one table but not the other, try something like this: ;WITH r AS ( select StoreID, Date, Amt = SUM(Amount) from dly_Reductions where Date Between '2/9/2015' and '2/13/2015' and StoreID = 2 and DepartmentID IN (1,5,13) and LineType =2 group by StoreID, date ) , d AS ( select StoreID, Date, Amt = SUM(Amount) from dly_Discounts where Date Between '2/9/2015' and '2/13/2015' and StoreID = 2 and DiscountID = 2 --8. Complaints group by StoreID, Date ) SELECT StoreID = ISNULL(r.StoreID, d.StoreID) , Date = ISNULL(r.Date, d.Date) , Col_H = 8 , Reductions_Amt = r.Amt , Discounts_Amt = d.Amt , Total = ISNULL(r.Amt, 0) + ISNULL(d.Amt, 0) FROM r FULL OUTER JOIN d ON d.StoreID = r.StoreID AND d.Date = r.Date
Well /r/sql is a great place to see real life examples of people trying to accomplish a result and you will see a lot of "Holy crap, I'm stunned you did that with 3 lines of code". We would probably do more community based challenges and be a bit more supportive but the moderators are pretty lenient about outright nonconstructive posting. Heck my first comment to your post was that way because I've been blasted by a few users to where I treat this subreddit differently than I do other programming subreddits. You hung in and kept answering and I felt compelled to help you further. I'd just stick around /r/sql and watch problems come in and see their resolutions.
&gt;Naturally, im now worried that ive automated myself out of a job. [http://threevirtues.com/](http://threevirtues.com/) The first virtue of a good programmer is Laziness. Laziness causes us to write ourselves out of a job. If would be foolish of your employer to let you go as automating business tools do not last forever. If something doesn't break, the business changes. If you are ever suspect of losing your job, go to your bosses boss an let them know ,"I've been told my job is on the block. Since I have been here I have reduced the workload of my department from 3 full time positions to 1 individual. If I'm let go, they may not be able to maintain the automatons I created. I see them eventually falling back into the work they were doing before when one day something breaks." Software rot occurs, lines of business change. Make sure you say this gently you don't want to say it will occur maliciously. Since you didn't provide what tools you are working in (in your post) but lets say if you automated reports in SSRS, one day they will have a windows update to the server or client that could break the report. They could replace their Windows 7 PC's with Windows 10 and Office 2016 and boom, excel doesn't want to open the report. Who are they going to call? Nobody, they go back to doing it by hand. **You are the worker that companies kill for**. All of these automatons should be on your resume. Your company should value you for it, if they aren't, it may be because you walk around with your tail between your legs like you did coming into this post. **Stand up from your seat, stick your chest out and say ,"Dammit, I'm fucking worth more than I give myself credit for!"**
I knocked up an example in MySQL. Porting to Oracle should be fairly trivial First I create some test data create table test_visits (id int primary key auto_increment, starttime datetime, endtime datetime); insert into test_visits (starttime, endtime) values ('2015/01/01 08:00', '2015/01/01 10:30'); insert into test_visits (starttime, endtime) values ('2015/01/01 08:00', '2015/01/01 08:59'); insert into test_visits (starttime, endtime) values ('2015/01/01 10:00', '2015/01/01 12:00'); insert into test_visits (starttime, endtime) values ('2015/01/01 10:00', '2015/01/01 10:59'); insert into test_visits (starttime, endtime) values ('2015/01/01 10:30', '2015/01/01 11:30'); insert into test_visits (starttime, endtime) values ('2015/01/01 11:00', '2015/01/01 15:30'); insert into test_visits (starttime, endtime) values ('2015/01/01 16:00', '2015/01/01 19:00'); Then I create a lookup table to use for hours (theres probably a better way to generate number ranges without having to create a table for it) create table test_hours (hour int primary key); insert into test_hours (hour) values (0), (1), (2), (3), (4), (5), (6), (7), (8), (9), (10), (11), (12), (13), (14), (15), (16), (17), (18), (19), (20), (21), (22), (23); Finally the report... select hour, count(id) from test_hours inner join test_visits on (hour &gt;= HOUR(starttime) and hour &lt;= HOUR(endtime)) where starttime &gt;= '2015/01/01' and starttime &lt; '2015/01/02' group by hour; Output.... http://i.imgur.com/mp1YB1v.png 
=HYPERLINK("\\\public\myattachement.txt","\\\public\myattachement.txt") It wont show blue but it will be clickable. The second box is the display. you could have it =HYPERLINK("\\\public\myattachement.txt","Click Here to Download") =HYPERLINK("\\\public\myattachement.txt","myattachement.txt")
The WHERE clause only restricts the startdate, so you could remove that. The problem is if someone arrives at 11pm and leaves at 2am, the JOIN clause is a bit screwed (because then hour cannot be &gt; starttime AND &lt; endtime). It could probably be reworked a little though. Are you trying to show just a list of hours 0-23, or the actual date alongside it? 
Here's carpii's suggestion with the "test_hours" generated on the fly from a pair of dates. WITH TIMES AS ( SELECT TO_DATE( '01/01/2015 0800', 'MM/DD/YYYY HH24MI' ) START_TIME, TO_DATE( '01/02/2015 1600', 'MM/DD/YYYY HH24MI' )) END_TIME FROM DUAL ), TEST_HOURS AS ( SELECT START_TIME + (ROWNUM-1)/24 HOUR FROM TIMES CONNECT BY LEVEL &lt;= 24*(END_TIME-START_TIME) ) select hour, count(id) from test_hours inner join test_visits on (hour &gt;= TRUNC(starttime,'HH') and hour &lt;= TRUNC(endtime,'HH')) group by hour;
This part of a weekly class we are using to train our Software Engineers at Emergency Reporting. Please keep discussions on Reddit and thanks to those that watched it live yesterday!
The query looks good, for clean coding though, I'd alias the table (and put schema on it) and add the tmp alias to the select columns along with the group by. Could it be you need brackets around your tablename: tble_GU-CourseListings_2013-14?
Hrm, I'm not sure thats going to work? The JOIN clause still says (hour &gt;= TRUNC(starttime,'HH') and hour &lt;= TRUNC(endtime,'HH') So for 11pm to 2am, start hour is 23, end hour is 2 Maybe I misunderstood what TRUNC does, but it seems similar to MySQL's HOUR(), in which case its impossible for 'hour' to ever be &gt;= 23 AND &lt; 2. I think the only way to solve it is to take the date into account, but OP hasn't cleared up his requirements, so I didnt bother attempting it :-)
Where are you based? My company provides exactly this service. 
Here is a cte-based query that should work. Tested in MS SQL but Oracle syntax should be the same: WITH CTE AS ( SELECT ITEM_ID, MAX(ORDER_NUM) as LAST_ORDER_NUM FROM myTable GROUP BY ITEM_ID ) SELECT myTable.ITEM_ID, myTable.ORDER_NUM, myTable.ORDER_DATE, myTable.UNIT_PRICE FROM myTable JOIN CTE ON myTable.ITEM_ID = CTE.ITEM_ID AND myTable.ORDER_NUM = CTE.LAST_ORDER_NUM; 
I am not versed in oracle, but here is what I do if I were trying to do this in sql: 1. Perform a row_number operation where you partition by item_id and order by last order_date desc. You can throw this into a subquery and then select from the subquery where the rownumber is a 1. This is how I would do it in sql CREATE TABLE #salestest ( itemid INT, ordernumber INT, orderdate DATETIME, price DECIMAL(10, 2) ) INSERT INTO #salestest SELECT 98986, 54554, 42005, 244 UNION SELECT 98463, 54555, 42006, 113 UNION SELECT 98514, 54556, 42007, 157 UNION SELECT 98188, 54557, 41913, 85 UNION SELECT 98715, 54558, 41914, 53 UNION SELECT 98893, 54559, 41915, 390 UNION SELECT 98986, 54560, 41916, 330 UNION SELECT 98463, 54561, 42012, 17 UNION SELECT 98514, 54562, 42013, 18 UNION SELECT 98188, 54563, 42014, 322 UNION SELECT 98715, 54564, 42015, 130 UNION SELECT 98844, 54565, 42016, 259 UNION SELECT 98386, 54566, 42017, 6 SELECT * FROM (SELECT ROWNUMBER = Row_number () OVER ( partition BY itemid ORDER BY orderdate DESC), S.itemid, S.ordernumber, S.orderdate, S.price FROM #salestest S) T WHERE rownumber = 1 
Here is an analytic solution using row_number. Disregard the first part as I am just generating a dummy dataset based on your picture. --generate fake data with data(item_id, order_num, order_date, unit_price) as( select 98993, 223721, to_date('29-JAN-15','DD-MON-RR'), 09.13 from dual union all select 98992, 223719, to_date('29-JAN-15','DD-MON-RR'), 10.17 from dual union all select 98991, 223811, to_date('02-FEB-15','DD-MON-RR'), 650.32 from dual union all select 98991, 223085, to_date('13-JAN-15','DD-MON-RR'), 650.32 from dual union all select 97992, 224403, to_date('13-FEB-15','DD-MON-RR'), 32.77 from dual union all select 97992, 224067, to_date('06-FEB-15','DD-MON-RR'), 35.22 from dual union all select 97992, 222353, to_date('17-DEC-15','DD-MON-RR'), 40.14 from dual union all select 97992, 222273, to_date('16-DEC-14','DD-MON-RR'), 40.14 from dual union all select 97991, 222354, to_date('17-DEC-14','DD-MON-RR'), 38.13 from dual union all select 97991, 222274, to_date('16-DEC-14','DD-MON-RR'), 38.13 from dual union all select 96991, 222350, to_date('17-DEC-14','DD-MON-RR'), 38.13 from dual union all select 96991, 222270, to_date('16-DEC-14','DD-MON-RR'), 38.13 from dual union all select 96991, 221507, to_date('19-NOV-14','DD-MON-RR'), 38.13 from dual union all select 96990, 220762, to_date('30-OCT-14','DD-MON-RR'), 01.87 from dual union all select 95990, 224249, to_date('11-FEB-15','DD-MON-RR'), 1427.72 from dual union all select 95990, 223223, to_date('16-JAN-15','DD-MON-RR'), 54.98 from dual union all select 95990, 222714, to_date('29-DEC-14','DD-MON-RR'), 48.92 from dual union all select 95990, 222149, to_date('15-DEC-14','DD-MON-RR'), 48.92 from dual union all select 95990, 221808, to_date('03-DEC-14','DD-MON-RR'), 44.00 from dual union all select 95990, 221408, to_date('17-NOV-14','DD-MON-RR'), 48.92 from dual union all select 95990, 221020, to_date('06-NOV-14','DD-MON-RR'), 48.92 from dual ) select item_id, order_num, order_date, unit_price from( select item_id, order_num, order_date, unit_price, row_number() over(partition by item_id order by order_date desc, order_num desc) r -- rank each record based on when item_id was ordered, incase of a tie fall back to order_num assuming newer orders have a higher order_num. from data ) where r = 1 -- only select the highest ranked records(aka the most recently sold) ITEM_ID| ORDER_NUM | ORDER_DATE | PRICE ---|---|----|---- | 95990 | 224249 | 02/11/2015 | 1427.72 | | 96990 | 220762 | 10/30/2014 | 1.87 | | 96991 | 222350 | 12/17/2014 | 38.13 | | 97991 | 222354 | 12/17/2014 | 38.13 | | 97992 | 222353 | 12/17/2015 | 40.14 | | 98991 | 223811 | 02/02/2015 | 650.32 | | 98992 | 223719 | 01/29/2015 | 10.17 | | 98993 | 223721 | 01/29/2015 | 9.13 | 
The database schema makes no sense because of the "program mapping table" and the "Course/Program Mapping Table". Why would you have relationships between user and the transcript, user and the program and program and the crs_id. I did reporting for student information systems for over 4 years and you are providing just enough information to confuse everyone here. At least provide what you've written so far. 
What you're talking about is SSRS. What about your environment prevents you from using it? That would be relevant if we're looking for a replacement like crystal reports etc.
Because of the hosting environment we're in, we can't utilize our users' AD accounts for anything except their workstation logins, so SSRS is out. It's frustrating, but it's a shared hosting environment and I'm stuck with it. 
I'll admit, I was not aware of that option, so I'll give it a look. Still, any other frontends out there?
Honestly what you're looking for is SSRS with forms authentication. Any other frontend you set up also won't be able to use AD, so you're looking at forms/custom authentication with that as well. Why not use the SSRS you already paid for, with the bonus of it being incredibly simple to set up and easy enough for most end-users to set up their own subscriptions?
Not sure I exactly understand you. Could it be as simple as just using between? Do you want every row in your table that has a date between those dates or do you want every day between those dates to return a row with how many rows have that specific date? SELECT table.dateField FROM table WHERE table.dateField BETWEEN '1/25/2015' AND '2/7/2015' Or SELECT table.dateField, COUNT(*) as 'rows' FROM table WHERE table.dateField BETWEEN '1/25/2015' AND '2/7/2015' GROUP BY table.dateField ORDER BY table.dateField This will not return every day, only days that belong to a row. If you want every day, maybe RIGHT JOIN a temp table with every date in that pay period inserted into it? My syntax is from T-SQL btw. I haven't worked with Oracle.
I did that and to call it I still have to pass a second variable, with that syntax I can now call it like: SELECT myFunction(ColumnA, DEFAULT /*A T-SQL keyword*/) But I still have to have a second parameter in the function call. I'm beginning to think optional parameters are only done in system functions.
Appreciate the help! So my challenge is I don't have a date table anywhere in this subject area except the pay period start and end date as two separate columns. Sort of useless. So I wanted to be able to fill in rows of dates with their corresponding values that fell in between so I could make better time series charts and graphs in excel or Tableau. This may not be possible in OBIEE and only possible if I had direct access to our entire EDW. 
I think your best bet is to create another function instead of trying to reuse the existing one. Not ideal, I know...
maybe I don't understand the complexity of your data structure. SELECT u.user_id , t.crs_id , p.prog_name FROM User u FULL OUTER JOIN Transcript t ON u.user_id = t.user_id FULL OUTER JOIN Course/Program/Mapping cmp ON t.crs_id = cmp.crs_id FULL OUTER JOIN ProgramMapping p ON cmp.crs_id = p.crs_id -- AND cmp.prog_name = p.prog_name -- bad idea joining on text values -- but you have program names instead of program IDs like I say, maybe I'm misunderstanding something but this seems like a straightforward request.
I think you're correct. I did that, I was hoping to avoid it, but I'll get over it.
I'll give it a go, thanks! 
Dude, I was at the end of my wits. It may seem like something easy to you, but I am pretty new to SQL and I am on hour 60 of my 40 hour work week, getting paid salary. I really appreciate your help. Thanks again.
Sorry not a real reply, but your title made me chuckle :)
Sure is, but that's part of the fun.
Unfortunately, I'm getting the same error messages 
Dude is like a gay military porn title. 
I've run this exact query in two instances. One with just: select * from prereq start with course_id = num; connect by course_id= prior prereq_id; and the last with the everything I originally posted with the only difference being I replaced: create or replace procedure print_prereqs with declare. In both cases it worked perfectly and gave me the course prerequisites. Thus, I don't know what the problem is but I'm assured it's not the table name. 
This sort of processing would probably be better to do on the application layer in my opinion as not to gunk up your DB with redundant fields or empty placeholder rows. Any sort of calculation should really happen outside. But I'm not a SQL wizard like some of the people here so It would be great for you to get a more experienced opinion. I drew up some sample data in excel to help explain my solution. Also I believe all, if not most, of this can be done using [Date functions](http://www.w3schools.com/sql/sql_dates.asp) but I haven't had to work with them before so i'm not sure right away how to mess with them without testing. What I would try to do with the date/time stuff is a subquery that returned every valid day between each pay peroid (remember between is inclusive). Also have a look at [this little tutorial about dates](http://sqlblog.com/blogs/aaron_bertrand/archive/2009/10/16/bad-habits-to-kick-mishandling-date-range-queries.aspx). If you have the time it might give you some inspiration. [So Here is some sample data](http://i.imgur.com/liaXIYZ.png). Period is a foreign key to tblPayPeriods. ID in tblEntries would be your primary key. I don't really know what you're using this for so i'm just taking a stab at it. You could use your tblEntries.date field in order to calculate the period field, or if you want, make a table with every single day of the year in it and what payperiod ID that day belongs to. then join the tblEntries.date field to the table that holds every day, and join the tblDay to your tblPayPeriod with another foreign key. Then you could make a query that returns every row in the tblDay (a table that holds every day of the year and the associated pay period) that matches a transaction ID (tblEntries.ID) or belongs to a payperiod (tblPayperiod.ID or one of the associated foreign keys)
The i here is a record of type columns declared in the iteration cursor. You don't need to declare these and in this specific case, i isn't an integer
You need some way of identifying what row(s) that should be updated. This is generally done with a primary key or foreign key. Since you didn't provide that detail it's impossible to give a meaningful answer.
Stop saying 'password' if you want suggestions other than salt and hash. You say this is SFTP passwords, well that's authentication data and you should not store it in plain text at all. If you must do this you have a few options: Encrypt this data before it hits the database and decrypt it outside the database? Ideally using keys that only exist in people's heads, ideally one key per user. Encrypt/decrypt it inside your database if you really believe your data and all backups of it are secure. Throw it in a new database in plaintext with locked down permissions. Decide it's a low risk and just dump it in your existing database in plaintext with no extra protections. Or some combination of the above.
Depends on the DBMS. Just run it and see what happens.
So they are the same. Thanks for the link
I haven't used SolarWinds' products. We've used Idera in the past in our shop (and still use it, in a limited capacity) but we've been primarily a (very happy) SQL Sentry shop for 2-3 years.
cool :) I am on a course so just speed browsing currently I've saved it to Pocket to read later
[You could use a case statement.](https://msdn.microsoft.com/en-us/library/ms181765.aspx)
What DBMS? For SQL server you'd want to do: *** CREATE VIEW oneMonthAgo AS SELECT whatever FROM your_table WHERE your_timestamp &gt;= DATEADD(dd, -30, GETDATE()) GO *** SELECT * FROM oneMonthAgo
sorry I'm new on both reddit and sql so its a little difficult for me. this is what i have right now http://imgur.com/1NtWW8x . also what can I do in order to get just the first login entry per day? I really appreciate the assistance.
select emp_id, convert(date,dateofevent), min(dateofevent) from dbo.tagcomplement where idevento = 1 group by emp_id, convert(date,dateofevent)
The inner join you added looks good to me. Perhaps there are other problems with the query leading to you getting an error message. Can you paste in the entire contents of the query you wrote, including the joins, case statements and table name prefixes (adding table names to your select fields) as well as the exact error message you get upon running it?
http://www.dbforums.com/showthread.php?1635552-How-to-select-the-top-10-rows-from-a-table
If this is an exact copy, you're missing a comma after the first line. That makes the parser think that it's reached the end, but you don't have a FROM in the next line, so you get the error you are getting.
Thanks for the reply. Im not trying to get the top N rows with (FETCH FIRST N* ROW ONLY) I know about this and would like the same idea behind this but for multiple aggregating criteria, such as the top number of purchases for Greg, tony, and jimmy in my example above. I was thinking about inserting into a temp table and ranking with this function(ROW_NUMBER() OVER(PARTITION BY name ORDER BY purchaseid)) and then pulling that row &lt;20 thoughts? 
So, you mean like, top 20 FOR EACH name? Then yea, I'd go with RowNum. No need to throw it into a temp table though, just run it as a subquery. SELECT * FROM (SELECT NAME, RANK, PURCHASEID, ROW_NUMBER() OVER(PARTITION BY NAME ORDER BY something DESC) AS RowNum FROM SCHEMA.TABLE WHERE NAME IN ( GREG,TONY,YIMMY) ) t WHERE t.RowNum&lt;=20
haha nice... duh duh duh. Thanks so much. I get to keep my sanity today. Thanks!
There are no current prices. these are new product.
You may want to start with the data import/export wizards before you try to do this in a stored procedure. You also didn't include the sql server flavor and file type.
Note: This only works for data sets where "being different" is defined as "having only a single record", which might not be accurate if there are any otherwise distinct rows (with no otherwise "different counterparts") OR if there were any multiple-record sets that were "different" from their counterparts (e.g. two "different" rows, fifty "normal" ones).
I'm not being a butthole, I'm pointing out that the OP's problem is poorly defined and your answer "solves" this problem only for a very narrow subset of data. Unless the true data set is as trivial as the example provided, we have no basis for the assumption you've made.
Yeah. I hit post and realized it looked terrible. Took me awhile to get it looking good.
I thino this well definitely get us started. Thank you. I'm the only one in the office who knows 'a little SQL' so I was stuck fixing it.
Use this as a primer (wrap it in a sproc and have the locations and file-names as parameters). It pulls the backup from wherever and restores it to where you choose. It then links up the SIDS all the server logins to the database users (if the names match). Hope this helps. DECLARE @sqlroot VARCHAR(150); DECLARE @sqllog VARCHAR(150); DECLARE @Database_dat VARCHAR(200); DECLARE @Database_log VARCHAR(200); USE master; PRINT 'starting'; PRINT GETDATE(); SET @sqlroot = 'C:\Program Files\Microsoft SQL Server\MSSQL10_50.MSSQLSERVER\MSSQL\DATA\'; SET @Database_dat = @sqlroot + 'Database_data.MDF'; SET @Database_log = @sqlroot + 'Database_log.MDL'; PRINT 'Restoring MyDatabase'; PRINT GETDATE(); ALTER DATABASE [MyDatabase] SET SINGLE_USER WITH ROLLBACK IMMEDIATE; RESTORE DATABASE [MyDatabase] FROM DISK = 'C:\MyDatabase.bak' --THIS CAN BE LOCAL OR NETWORK DRIVE WITH REPLACE, MOVE N'MyDatabase_dat' TO @Database_dat, MOVE N'MyDatabase_log' TO @Database_log; ALTER DATABASE [MyDatabase] SET MULTI_USER WITH ROLLBACK IMMEDIATE; USE MyDatabase; --This re-links the SIDS of the SERVER logins to the DATABASE Users; DECLARE @login NVARCHAR(50); DECLARE logins_cursor CURSOR FOR SELECT sp.name FROM sys.server_principals sp WHERE sp.name IN('Login1','Login2'); OPEN logins_cursor; FETCH NEXT FROM logins_cursor INTO @login; WHILE @@FETCH_STATUS = 0 BEGIN EXEC sys.sp_change_users_login 'Update_One', @login, @login; FETCH NEXT FROM logins_cursor INTO @login; END; CLOSE logins_cursor; DEALLOCATE logins_cursor; GO 
Protip: The downvote button isn't the "I don't like you" button. I recognize you don't like having the validity of your suggestion questioned, but downvoting otherwise useful information simply because you didn't like something someone said isn't particularly helpful to the OP or the subreddit in general.
This is actually for a report that is processed through Access by importing two excel documents. It's something that was inherited by our department, and they, for some reason, want to stick with it. So it is pretty trivial and the database is recreated by the two excel files. Thanks again.
Urlencode the query, then decode it when extracting it from the XML.
And it worked! &lt;![CDATA[&lt;]]&gt; Learn something new everyday. Thank you!!!
Pretty sure there's no difference in efficiency, just readability
Glad to hear that. He argued he could watch "Hours of Netflix" as that much difference. It would be milliseconds at most at this scale. 
We require this convention on my team.
Assuming it's MySQL, you can take both queries and put `EXPLAIN EXTENDED` in front of them to learn about the execution plan. By default you get back the information about which indexes the engine will use to run the query. I mocked up the tables and ran the original query: EXPLAIN extended SELECT c.Status, c.ClassNum FROM Classes C INNER JOIN Announcements A ON C.Classnum=A.Classnum WHERE C.Status='1' AND C.ClassNum ='CS' AND A.Status IN ('YAAAAYYY','sob softly') ...and rewritten this way: EXPLAIN extended SELECT c.Status, c.ClassNum FROM Classes C INNER JOIN Announcements A ON C.Classnum=A.Classnum AND A.Status IN ('YAAAAYYY','sob softly') WHERE C.Status='1' AND C.ClassNum ='CS' In both cases, the output of `EXPLAIN EXTENDED` was the same: | *id* | *select_type* | *table* | *type* | *possible_keys* | *key* | *key_len* | *ref* | *rows* | *filtered* | *Extra* | - | - | - | - | - | - | - | - | - | - | - | 1 | SIMPLE | C | const | PRIMARY | PRIMARY | 21 | const,const | 1 | 100.00 | Using index | | 1 | SIMPLE | A | ref | PRIMARY | PRIMARY | 74 | const | 2 | 100.00 | Using where; Using index | Getting identical results is common for a simple example where there aren't any subqueries or other branching, because there just isn't a lot to optimize. The output will vary depending on how your tables are structured. I gave mine a primary key covering (ClassNum, Status), which were the only columns in each table, and I only added a few rows to each. Numbers for `rows` and `filtered` will definitely look different when run against production data. For a slightly deeper look under the hood, after you run a query with `EXPLAIN EXTENDED`, you can run the query `SHOW WARNINGS` to see what the optimizer is doing. Here are the results for the original query: /* select#1 */ select '1' AS `Status`,'CS' AS `ClassNum` from `tempo`.`classes` `c` join `tempo`.`announcements` `a` where (('1' = '1') and ('CS' = 'CS') and (`tempo`.`a`.`Status` in ('YAAAAYYY','sob softly')) and (`tempo`.`a`.`ClassNum` = 'CS')) ...and for the rewritten query: /* select#1 */ select '1' AS `Status`,'CS' AS `ClassNum` from `tempo`.`classes` `c` join `tempo`.`announcements` `a` where (('1' = '1') and ('CS' = 'CS') and (`tempo`.`a`.`ClassNum` = 'CS') and (`tempo`.`a`.`Status` in ('YAAAAYYY','sob softly'))) Some interesting things to note here. First, the engine knows that we're only going to get '1' back as Status and 'CS' back as ClassNum (because that's all we asked for) and has converted them both to literals. Second, the optimizer has re-ordered parts of the predicate in the rewritten query so that ClassNum comes before Status. That isn't much of a difference, but I suspect that if the predicate order matches the logical order of the columns in the table, you could see a *minor* improvement. In this example it's really six of one, half dozen of the other.
Thanks for your reply
If you have enough connectivity to grab a .bak why not just leverage built in SQL stuff like maybe log shipping?
Start writing every function from right to left. And execute them step by step. It's just functions calling other functions and computing some value. No need to understand all the operations unless debugging. Treat it as black box. It's suffice to know that it's an expression computing a value. **Are you not aware of the principle of functions calling other functions?** Example: ceiling(binary_checksum()) Ceiling takes as parameter the value that binay_checsum evaluates to.
Please tell me you're not building SQL in a browser and passing it back to a web server for execution.
That's a whole lot of calls. Though it gives an idea of scale at least.
Ouch, that's an unfortunate process. If I were you (and I've had to do this before) I'd work my way up the IT ladder with a clear justification of why it would be much more secure to allow access than to carry a full SQL backup around on an external drive. Sounds rife for data breaches if you asked me.
Here is my query that (i believe) meets the condition select distinct * from Rating R1, Rating R2 using (rID,mID) where (R1.ratingDate &lt; R2.ratingDate) and R1.stars &lt; R2.stars Produces the one case which meets the condition ... just dont know how to join that back and get reviewer name and movie title
He said MySQL, not SQL server just FYI.
Look up how to use a subquery in a join or how to use the exists clause. Both of those should get you to your answer, one will let you do it without using distinct.
No, its a form builder. I create a query, add it to an XML that matches aliases to field names on a PDF. When the PDF is generated with the software, it runs the query and pulls the data for a specific file.
 SELECT r.name, m.title FROM Movie m INNER JOIN Rating s ON m.mID = s.mID INNER JOIN Reviewer r on r.rID = s.rID INNER JOIN Rating s2 ON m.mID = s2.mID AND s2.rID = s.rID AND s2.ratingDate &gt; s.ratingDate WHERE s.stars &lt; s2.stars
thank you. will do. 
You probably have, the stanford online database course is pretty popular and people ask for help on it all the time.
Here is an answer using EXISTS. It bugs me since I am not sure what is supposed to happen if they rate it 3 times. Is that supposed to show up twice in the list? Not at all? Its not really explicit about the question, but hey.. as long as you get the right answer?? SELECT Reviewer.name ,Movie.title FROM Rating LEFT JOIN Movie ON Movie.mID = Rating.mID LEFT JOIN Reviewer ON Reviewer.rID = Rating.rID WHERE EXISTS ( SELECT 1 FROM Rating RatingEarlier WHERE Rating.rID = RatingEarlier.rID AND Rating.mID = RatingEarlier.mID AND Rating.ratingDate &gt; RatingEarlier.ratingDate AND Rating.stars &gt; RatingEarlier.stars ) Interestingly the Answer to Question 8 appears to be wrong as it doesn't include the Movies that have a null rating spread, when it doesn't say to exclude them. I guess its just a misleading question but I wasn't really expecting to see that on a Standford Course :)
For the record, CASE statement is perfectly valid in the WHERE clause. It's not recommended for performance reasons though. Use a subquery. select * from ( ... your query here ... ) t where t.InvoiceStatus = @InvoiceStatusVar 
It looks like you only have district and storecount in your SELECT. You should also have storeNum (or whatever the column is) in your SELECT. Don't forget to add it to your GROUP BY as well. You should also rework that inner select into a derived table or CTE. I can help with that if you're interested. 
Or use a CTE (Common Table Expression), which is just as performant an 1% as ugly looking as this would be.
Got it. Thanks for your help. 
What does your Item table look like? Like this? ItemID | ItemLookupCode ------|-------------- 1 | 100S 1 | 100P 2 | 100S 3 | 100S Where ItemID2 and 3 have not been picked up yet because they dont have 100P?
I'll try your suggestions out tomorrow - thanks!
I'm wondering the same. If it is, I wonder if we can assume that itemId can only appear twice? You can only store an item once and pickup once? If so, we could group by the ItemId and assume that if the count is greater than one then the customer has picked up.
It would probably be better to just left join the table twice, once for 100S and the other for 100P. You'd then have two distinct columns for 100S values and 100P values which would make it a lot easier to read and manipulate.
After re-reading your post, my suggestion won't resolve anything. The issue is the select within the case. That's pulling back the same number regardless of the district. I'd try to move that to a derived table or a CTE. 
What platform are you using? I'm assuming MSSQL, but just wanted to make sure.
In order to really help you, we need the table layout, maybe some sample data, and what environment you are using.
just use replace. I don'y use mysql but i would do something like this. You could also chain them together so you do not have to use multiple sets, but the readability is complete shit if you do that. CREATE DEFINER = `lannister`@`%` FUNCTION `WordToNum`(word char) RETURNS char(7) BEGIN DECLARE wordnum CHAR(7); SET wordnum = REPLACE(word, 'A', '2') SET wordnum = REPLACE(wordnum, 'B', '2') SET wordnum = REPLACE(wordnum, 'C', '2') SET wordnum = REPLACE(wordnum, 'D', '3') SET wordnum = REPLACE(wordnum, 'E', '3') SET wordnum = REPLACE(wordnum, 'F', '3') SET wordnum = REPLACE(wordnum, 'G', '4') SET wordnum = REPLACE(wordnum, 'H', '4') SET wordnum = REPLACE(wordnum, 'I', '4') SET wordnum = REPLACE(wordnum, 'J', '5') SET wordnum = REPLACE(wordnum, 'K', '5') SET wordnum = REPLACE(wordnum, 'L', '5') SET wordnum = REPLACE(wordnum, 'M', '6') SET wordnum = REPLACE(wordnum, 'N', '6') SET wordnum = REPLACE(wordnum, 'O', '6') SET wordnum = REPLACE(wordnum, 'P', '7') SET wordnum = REPLACE(wordnum, 'Q', '7') SET wordnum = REPLACE(wordnum, 'R', '7') SET wordnum = REPLACE(wordnum, 'S', '7') SET wordnum = REPLACE(wordnum, 'T', '8') SET wordnum = REPLACE(wordnum, 'U', '8') SET wordnum = REPLACE(wordnum, 'V', '8') SET wordnum = REPLACE(wordnum, 'W', '9') SET wordnum = REPLACE(wordnum, 'X', '9') SET wordnum = REPLACE(wordnum, 'Y', '9') SET wordnum = REPLACE(wordnum, 'Z', '9') RETURN wordnum; END; I come from an oracle background and we can just use translate for this stuff. return translate(word,'ABCDEFGHIJKLMNOPQRSTUVWXYZ','2223334445556667778889999');
your obsession with loops scares me. I don't know how to invoke functions in mysql but i would just do the following. obviously this glosses over logging and error handling. update your_table set word = WordToNum(word);
It's because we have to do this for each letter of a 7 letter word. What you have there will only return a single numeric character because it matches the first case and that's all it needs.
http://dev.mysql.com/doc/refman/5.0/en/string-functions.html#function_replace &gt; Returns the string str with **all occurrences** of the string from_str replaced by the string to_str. REPLACE() performs a case-sensitive match when searching for from_str.
The definition makes sense, but I'm using the function in a simple SELECT statement matched with its word and it's only printing a single digit.
..all that and still nothing. mysql&gt; SELECT word, WordToNum(word) -&gt; FROM tbl_words -&gt; LIMIT 1; +---------+-----------------+ | word | WordToNum(word) | +---------+-----------------+ | AARONIC | 2 | +---------+-----------------+
i dont know what to tell you then. Im going to stick with Oracle. :P
You got me started on a right track I think. I can look up some stuff with Regular expressions and concat but I'm too exhausted for tonight. xD
Hey guys thanks for all the help so far! Sorry for not including enough info, heres the table formats: **Item Table** ItemLookupCode | Description | ItemID | Price --------------|-----------|--|----- 100P | A Pickup | 1| 0.00 100S | A Sale | 2 | 5.00 200P | B Pickup | 3 | 0.00 200S | B Sale | 4 | 6.00 **Transaction Table** TransactionNumber | CustomerID -----------|---------- 100 | 1 101 | 1 102 | 2 103 | 4 **TransactionEntry Table** TransactionNumber|TransactionTime|ItemID|Quantity|Price ---------------|-----------|------|--------|----- 100|2015-02-05 15:32:02|1|2|0.00 100|2015-02-05 15:32:02|2|2|10.00 100|2015-02-05 15:32:02|3|1|0.00 101|2015-02-06 02:15:20|1|1|0.00 102|2015-02-08 06:20:30|2|3|15.00 102|2015-02-08 06:20:30|1|3|0.00 103|2015-02-09 03:03:05|4|1|6.00 103|2015-02-09 03:03:05|3|1|0.00 **Customer Table** ID|FirstName|LastName --|---------|-------- 1|Rod|Smith 2|Bill|Smith 3|Dale|Jones 4|Frank|Anderson I want this query to return transaction 100, item lookup code 200P because customer 1 has never bought 200S before and also return transaction 101, lookup code 100P because he bought 2 of 100S before but already picked up 2 of 100P. It wont return anything from Trans ID 103 OR 104. Btw typed this all with my phone so please excuse any mistakes
Replied with more info lower.
Replied with more info lower.
Replied with more info lower.
Weird, it should not behave that way. have you checked the data type of your returned value? I suspect it is char(1) and the rest of the string is truncated. Use a loop if you must, but don't be surprised if you get negative feedback here. in SQL we work with sets, not loops. The optimizer loops, not us. If the solution requires an explicit loop there are better languages than SQL to use. 
In BIRT I would simply divide the Group's numberOfIncidents by the Table's numberOfIncidents _Total. SSRS might have something similar and this doesn't need to be done in SQL. In SQL: select count(*) incidents, Resolution, count(*) * 100.0 / sum(count(*)) over() percentage from MyTable group by Resolution
I hope this is what you want. There are better ways to do it but this is the most environment agnostic way to do so. Left/Right may need to be converted to substring functions and you may need to re-alias everything. Basically what the query does is it gets a list of all purchases and then attempts to match the transactions against themselves based on the first part of the lookup code. If a given lookup code is missing its partner or the quantity is off, it won't return a row for that transaction. select t.TransactionNumber, c.FirstName, c.LastName, t.TransactionTime, t.ItemID, i.Description, i.ItemLookupCode from CustomerTransaction ct inner join Customer c on c.ID = ct.CustomerID inner join TransactionEntry t on ct.TransactionNumber = t.TransactionNumber inner join Item i on i.ItemID = t.ItemID left join ( select e.*, i.ItemLookupCode from TransactionEntry e inner join Item i on i.ItemID = e.ItemID ) p on t.TransactionNumber = p.TransactionNumber and left(i.ItemLookupCode, len(i.ItemLookupCode) - 1) = left(p.ItemLookupCode, len(p.ItemLookupCode) - 1) and right(i.ItemLookupCode, 1) != right(p.ItemLookupCode, 1) and p.Quantity = t.Quantity where p.TransactionNumber is null TransactionNumber|FirstName|LastName|TransactionTime|ItemID|Description|ItemLookupCode -----------------|------------------------------|------------------------------|-----------------------|-----------|--------------------|-------------- 100|Rod|Smith|2015-02-05 15:32:02.000|3|B Pickup|200P 101|Rod|Smith|2015-02-06 02:15:20.000|1|A Pickup|100P
EDIT: To show a where clause I agree 100% with /u/Anaxandrides. CTE's save lives sometimes. Here's what it would look like. Notice how you don't need to have an alias on the column names when you use a CTS WITH GetInvoiceStatus AS ( SELECT m.ClientGroupID, m.ClientID, m.ClientName, m.MatterCode, m.[Area Of Law], i.InvoiceNo, i.InvoiceTotalAmount, i.InvoiceFees, i.InvoiceCosts, m.mattername, i.invoiceDate, CASE WHEN i.InvoiceTotalAmount &lt;= 0 THEN 'Paid' WHEN i.Total_PaidWithWO = 0 THEN 'Open' WHEN i.Total_PaidWithWO &lt; i.InvoiceTotalAmount THEN 'Partial' WHEN i.Total_PaidWithWO &gt;= i.InvoiceTotalAmount THEN 'Paid' ELSE 'Open' END AS 'InvoiceStatus' FROM [local].[lp2warehouse].dbo.lp2_matters m INNER JOIN [local].[lp2warehouse].dbo.lp2_invoices i ON m.mattercode = i.mattercode WHERE (i.invoicedate BETWEEN ISNULL(@BeginDate,'01/01/1900') and ISNULL(@EndDate,'12/31/9999')) AND isnull(@ClientID, m.ClientID) = m.ClientID AND isnull(@ClientGroupID, m.ClientGroupID) = m.ClientGroupID AND isnull(@InvoiceNo, i.InvoiceNo) = i.InvoiceNo AND isnull(@MatterID, m.MatterCode) = m.MatterCode ORDER BY m.ClientGroupID,m.ClientID ) SELECT ClientGroupID, ClientID, ClientName, MatterCode, [Area Of Law], InvoiceNo, InvoiceTotalAmount, InvoiceFees, InvoiceCosts, mattername, invoiceDate, InvoiceStatus FROM GetInvoiceStatus WHERE InvoiceStatus = 'Partial'
Sounds messy, but if you need to have it structured like that, then create a table somewhere (maybe in a separate DB) that lists all of the DB names you will be using, along with an active flag if you need it so you can retire old DBs with a simple update, and anything else relative to the DBs, like linknames if they are on another server. Then you can create a cursor based on a SELECT from that indexing table so you only use active DBs, putting the DB names and linknames if you need them into a variable(s). Inside the cursor you can then use dynamic SQL with the DB/tablenames hashed into the dynamic sql from the variable(s), with a loop of the cursor per DB. It's a messy way of doing it, but our set-up is kind of similar to that (set up by a 3rd party, we need to use their structure and cant change it), and it's the only way we can accomplish certain tasks, like feeding data from multiple servers/DBs/tables into single tables, or running jobs on everything at once with the functionality of being able to disable certain sources without updating a few hundred SPs, we just need to update the one indexing table. Be careful with the dynamic sql if you don't understand it though, pass variables in as much as possible, only hash data into the dynamic sql when you really have to, like with the DB or table names, never hash in DATETIMEs or INTs or anything like that, pass those as variables instead.
When you say you are on wifi, are you on the same network as your SQL instance? ie. Your company's wifi/network? My guess is you are, and the SQL Browser service is running on the machine where your SQL instance resides, allowing your program (or any program looking for a SQL instance) to find it. You will not be able to access your SQL instance from outside your office without a VPN or rdp.
Your new laptop might be using the same mac address for wifi and Ethernet, which would trick the network into thinking you are always connected over Ethernet. Try running the command prompt and typing ipconfig /all, then check if the physical address for your wifi adapter is the same as your Ethernet adapter. If that doesn't work, do you know how SQL authentication is handled on your server? Is the program network restricted (do you have to VPN/RAS into your companies network)? When you remote in are you remoteing into the server running the program? 
You're probably inside the firewall on you companies WiFi. You would need to access it though a VPN connection from home or away from the office. 
This is the way I would do this. I usually do it by selecting all of the tables i want into a #temp table. Then in a while loop do something like: while exists (select * from #temptable) begin set @dynamicVariable = select top(1) something from #temptable set @sql = 'dynamic sql in string with @dynamicVariable' exec (@sql) delete from #temptable where dynamicVariable = @dynamicVariable return end beautiful enough to make me cry. 
Which RDBMS are you using?
What exactly are you trying to achieve? If the quote status is any of the ones listed on each line, then produce that output? `when QS.Quote_Status in ('New','Preliminary','Feedback Requested') then 'Sales'` BUT...you should be doing this by joining to a lookup table instead, if you're doing it regularly.
Maybe try `OR`.. case WHEN QS.Quote_Status = 'New' OR QS.Quote_Status = 'Preliminary' OR QS.Quote_Status = 'Feedback Requested' THEN 'Sales' WHEN QS.Quote_Status = 'Info Required' OR QS.Quote_Status = 'Quoted' THEN 'Customer' etc.... It's not pretty, though.
I can't actually create any tables in my ERP system. only the Provider has access to do it, so if I create a table it will have to be a temp table. EDIT: and yes I know about normalization :)
what database application? what version of SQL? Have you tried formatting like '1','2','54','1028'
if you enter just one number into the parameter, does it work? 
Yes it does. 
so, what it's doing, is passing data into SQL Like this: IN ('parameter') If you have multiple values that are comma seperated, you are getting IN ('parameter1,parameter2,parameter3') which isn't the right syntax.... Try passing in 1','2','54','1028 and see if that works.
Try using [SQLPing](http://www.sqlsecurity.com/downloads). I'd just make sure your infrastructure guys know what's up. More info on how to accomplish an inventory can be found here: http://www.sqlservercentral.com/articles/64016/
MS SQL will let you separate numbers with just commas, so don't cast the bit in your where to a varchar, leave it as an int.
Using what /r/fozzie33 recommended create the function dbo.CSVtoTable (Call it whatever you like). http://www.codeproject.com/Tips/584680/Using-comma-separated-value-parameter-strings-in-S This will create a temp table of separated values into a column that upon the completion of the session will be deleted. Here's where we put it all back together. We query the temp table while the session is still active and we use the STUFF function plus FOR XML PATH with casting and escaping of single quotes to get the values to look as such '1','2','158','1028'. ---------------------------------------------------------------------------------------------------------- --Copy paste to test DECLARE @STATUS NVARCHAR(255) SET @STATUS = '1,4,6,1028' SELECT STUFF(( SELECT ',' + '''' + CAST(id AS NVARCHAR(255)) + '''' FROM dbo.CSVToTable(@STATUS) FOR XML PATH('') ), 1, 1, '') ---------------------------------------------------------------------------------------------------------- --Now in the format for your original query SELECT * FROM dbo.YourTable WHERE CAST(ps2.valint AS NVARCHAR(255) IN ( SELECT STUFF(( SELECT ',' + '''' + CAST(id AS NVARCHAR(255)) + '''' FROM dbo.CSVToTable(@STATUS) FOR XML PATH('')), 1, 1, '')) --Make sure you have @STATUS declared and set and you should be golden EDIT: Types need to be NVARCHAR(255)
Where EXISTS (SELECT * FROM table1.columnY WHERE table2.columnZ = table1.columnY AND table2.columnX = 'Argo fuck yourself, great movie by the way, Ben Affleck really came into his own as a director although his performance left a lot to be desired onscreen') 
 where concat( ',' ,@status ,',') like concat('%,',psi.valint,',%')
Apparently I'm terrible at formatting on Reddit so my apologies there.
Seems like it should be a subquery and not a CTE, since you only use it once, but otherwise I don't think there's a better way to get the distinct before aggregating. 
Here is a snippit of some of the select statements im dealing with.. ,( CASE WHEN charindex('.',substring([FullName],charindex('.',[FullName],(0))+(1),len([FullName])))&gt;(0) THEN substring(substring([FullName],charindex('.',[FullName],(0)) +(1),len([FullName])),(0),charindex('.',substring([FullName],charindex('.',[FullName],(0))+(1),len([FullName])),(0))) ELSE substring([FullName],charindex('.',[FullName],(0))+(1),len([FullName])) END) ,LEFT(( CASE WHEN charindex('.',substring([FullName],charindex('.',[FullName],(0))+(1),len([FullName])))&gt;(0) THEN substring(substring([FullName],charindex('.',[FullName],(0)) +(1),len([FullName])),charindex('.',substring([FullName],charindex('.',[FullName],(0))+(1),len([FullName])))+(1),len([FullName])) ELSE '' END),5) ,( CASE WHEN charindex(' ',ltrim(rtrim(substring([FullName],(0),charindex('.',[FullName],(0))))),(0))&gt;(0) THEN substring(ltrim(rtrim(substring([FullName],(0),charindex('.',[FullName],(0))))),(0),charindex(' ',ltrim(rtrim(substring([FullName],(0),charindex('.',[FullName],(0))))),(0))) ELSE substring([FullName],(0),charindex('.',[FullName],(0))) END) ,( CASE WHEN charindex(' ',ltrim(rtrim(substring([FullName],(0),charindex('.',[FullName],(0))))),(0))&gt;(0) THEN substring(ltrim(rtrim(substring([FullName],(0),charindex('.',[FullName],(0))))),charindex(' ',ltrim(rtrim(substring([FullName],(0),charindex('.',[FullName],(0))))),(0)),len([FullName])) ELSE '' END) Whenever i crack open a stored procedure, these are all over the place, literally every time a temp table is built for a set based routine, it looks like a christmas tree of functions 
Needing to use DISTINCT is an indicator that the schema design could be improved.
Thanks for the tips. Trying to adopt good practices.
Yes within 30 days. Stackoverflow is your friend. I took a job doing only SQL when I had close to 0 experience. I'm still learning of course, but as soon as you can do a few joins and subqueries you can retrieve most of the information they'll need. 
Is this a meme friendly sub now?
It's a personal preference for readability and it's ms sql specific.
Oh, come on, everybody loves wrapping everything in square brackets or double quotes. :/
This is going to hang in my cubical wall.
Thank you for replying. Based on your answer, I manage to figure this out: isbn VARCHAR(10) UNIQUE CHECK(isbn like '#%') Is this good? 
&gt; isbn VARCHAR(10) UNIQUE CHECK(isbn like '#%') Yes, this should work.
Why? Because he's Batman.
We work with some third party software at work and there are tables and names like [Person Descriptions with Addresses Attached]. Seriously?
This was a request by our data analyst team about 8 months ago. *The following fields need to be added to &lt;SERVER&gt;.&lt;DATABASE&gt;.dbo.&lt;TABLE&gt; ASAP.* [Corrected Form Received Date] DATETIME, NULL [Verified (reviewer initials)] VARCHAR(3), NULL [Appeal Decision Date] DATETIME, NULL [Denial / Reversal / No Change] VARCHAR(10), NULL [Reversal Basis] VARCHAR(50), NULL [Extension Consent] VARCHAR(3), NULL [Withdrawn Date] DATETIME, NULL [Time of Request] TIME, NULL [Time of Evaluation] TIME, NULL When I asked ,"Are those the actual names?", they were dead serious! When I checked the database (this was the first time I went through the tables in this random small database they used) all the column names were just like this. They were so used to bracketing every database, schema, table and column name it just became habit to them and they didn't and still don't understand how absolutely fucked up it looks. Needless to say I took out all the spaces and special character. Also, out of our 30+ home grown databases that have existed prior to me arriving, every column is allow nulls. This is because the staff doesn't know how to handle nulls like WHERE STATUS &lt;&gt; 'D' This wont return nulls because nulls are not lesser or greater than 'D', they are null. This will return all rows where status is not null and status &lt;&gt; 'D'. Since all fields allow nulls they use asci nulls '' to represent nulls and for some reason 1900-01-01 in every null date field. So when they use WHERE STATUS &lt;&gt; 'D' it returns all rows that do not have status 'D'. **Did I mention I'm trying to leave my fucking job?!?!?!**
Honestly sub-queries used as inline queries aren't performance hindering as you may think. Alot of the "eww sub-query" business is related to correlated sub-queries. Select MAX(a.somefield), a.somefield2 FROM ( MIN(somefield3) somefield4, somefield2, somefield1 FROM TABLEA GROUP BY somefield4, somefield2, somefield ) as A GROUP BY somefield2 Inline subqueries are like a log (dataset) moving through a saw mill. In assembly line fashion its outer bark is shaved off, then its run through saw blades and the finished product is 2x4's. Correlated sub-queries would be something like Select MAX(a.somefield), a.somefield2 FROM TABLEA A INNER JOIN ( MIN(somefield3) somefield4, somefield2, somefield1 FROM TABLEA WHERE A.somefield2 = AA.somefield2 GROUP BY somefield4, somefield2, somefield ) as AA ON AA.PKEY = A.PKEY GROUP BY somefield2 Honestly, I don't even know if that would work because I rarely write correlated sub queries. You can also put correlated sub queries in the SELECT statement as well as in the WHERE statement. Why these are bad is because for every row returned in your initial dataset many optimizers will perform a sql execution for that subquerie for a single row. Oracle optimizer from my knowledge is the smartest and will create a hash table and join, MSSQL will not. CTE vs Inline is really up to your preference. As /u/thlycanthrope said CTE's are usually used when you are using a custom dataset multiple times. Such as if you are creating a lookup dataset to resolve a bunch of codes in fields and you are going to join against your primary dataset multiple times. Honestly, you seem to grasp the fundamentals of set based SQL programming and I would pat yourself on the back as it looks like very good code.
Still no go :(
I love SQL!
You got it - you could probably do something like this: SELECT CASE WHEN ISNULL(E.driver_terminal, '') = '' THEN P.owner_type WHEN ... And so on. If E.driver_terminal can be NULL or an empty string, the ISNULL in there makes it a bit easier to compare.
What's everyone preference: TABLE_NAME or TABLENAME?
Which version? Express is here - http://www.microsoft.com/en-us/download/details.aspx?id=30438
If I have a choice I go with TABLENAME, but TABLE_NAME is acceptable too. I just don't want to get smacked by Batman. :)
 join pimpslap on whoopass.name = pimpslap.name where whoopass.name = 'jaynoj'
Every single database I work in has a table named `Case`. It would be such a pain in the ass to come up with a different name just for convenience. Likewise a bunch of tables have column named `Key`. I'd rather just deal with the brackets.
&gt;[Denial / Reversal / No Change] This is making my eye uncontrollably twitch.
TableName
No, but it would be acceptable on /r/sqlwtf.
Yeah, but we've also got systems with tables like: SELECT VerStdPln, ExsCompGen FROM EXS_ST_VER Then you look at the tables and see: EXS_ST_AXE EXS_ST_BYD EXS_ST_PDT EXS_ST_SOR EXS_ST_VER EXS_ST_VSE EXS_ST_WWT And suddenly your SQL queries feel like you're booking a flight using airport designations. Anybody got a data dictionary?
Underscoreres of the world_unite!
The diagram: user -&lt; user_duty &gt;- duty -&lt; priv_duties&gt;- priv &gt;- role I think priv_id is the PK of the "priv" table, so if you'd be just given the priv_id you wouldn't need the role at all, so if i understand your question as "given a user id A, role id B and privilege NAME C does A have C through a duty and is C a privilege under B": select distinct 'Role/Priv is ' || case when priv.role_id is null then 'not ' end || 'valid', 'User ' || case when user_duties.user_id is null then 'does not have' else 'has' end || ' the privilege' from dual left join priv on priv.role_id = &amp;B and priv.name = &amp;C left join priv_duties on priv_duties.priv_id = priv.priv_id left join duty on duty.duty_id = priv_duties.duty_id left join user_duties on user_duties.duty_id = duty.duty_id and user_duties.user_id = &amp;A 
[~xXx__Table Name__xXx~]
TableName. Underscores are appropriate when dealing with categories. For example ModuleA_TableNameA, ModuleA_TableNameB, ModuleB_TableNameA, ModuleB_TableNameB... 
That's dumb (depending on what's reading those values), but these other punters have to protects against two kinds of nullness with every comparison AND every join.
CamelCase, bitches! 
As far as Im concerned access is just a breeding ground for terribly designed databases. At work im currently trying to port a non-critical but still relatively important system from access to sql server. Step 1: translate everything directly from access to sql server and try to fathom the 'logic'. Im about half way through and somehow I've racked up 40 million rows, 30 tables and about as many views. Not a key in sight. Checks? Constraints? What are those? Data types? Just use varchar(255). Nested views 6 layers deep for trivial filtering tasks. The one thing keeping me going is the thought of all those drop statements im going to write when the time comes to mold it in to something less like a train wreck.
I started as a Data Analyst working with T-SQL 6 months ago and have very similar experience to yourself, I'd written some VBA applications and know some HTML / CSS / PHP / Jquery and very basic MySQL. As an entry level analyst you have all the right experience and will be able to pick up SQL quickly. I've still got a lot to learn after 6 months but in 30 days if you work very hard you can pick up enough of the syntax to get a job done. W3schools has excellent examples and was really helpful to me in my first months (and still is occasionally). I'd say go for it, you can get an answer to almost any problem with a quick google search and you'll build up experience quickly. If you need any help with any problems you can PM me and I'll do my best.
The first thing that came to my mind when reading the article was use ROW_NUMBER instead of corraleted and uncorraleted queries, and fair enough the first comments on the article suggested that method instead and RANK
TOP/LIMIT and windowing functions are the two normal solutions. I don't know why they bother showing the wrong ways to do it. Correlated subquery? Really?
From the article, &gt; you can also use database specific feature e.g. TOP, LIMIT or ROW_NUMBER to write SQL query, but you must also provide a generic solution which should work on all database. 
I'm kind of their main reporting guy so I'm the only guy that suffers... Every once in a while someone will ask for everyone within a 50 mile radios of another location and doing a bulk scan/convert to do these calculations is painfully slow. I ended up creating a batch that syncs this data to another table I created that uses the correct data type and is indexed which works a lot better. As for the null situation, I've run into some tables that have both nulls and blanks so I find myself using stuff like ISNULL(derp, '') &lt;&gt; '' for simplicity. Fortunately I don't have to deal with the madness mentioned above. Generally when you let non-engineers design database layouts things turn out... "suboptimal". I had a boss at my last job who's greatest work experience was running a construction company. He insisted on several completely horrible changes to database structure and wouldn't hear anyone's input. Because he was "the boss" he automatically knew more than anyone.
[The most widely used database on the planet](http://sqlite.org/mostdeployed.html) doesn't support them, yet. Access doesn't, but access doesn't support a lot of things. And in the end... this is an interview question. Artificially denying the use of ROW_NUMBER as a way to test other facets of a candidate's knowledge is entirely reasonable.
I do think that those kind of questions are tailored to make the interviewer feel smart about themselves. That aside, for an embedded dB as sql lite you'd want to use a cursor and return n-th record rather than rely on a subquery. These kind of questions and answers are applicable in an academic scenario; for a professional setting they are retrograde, imo.
You'd just use the POST in the source.txt and get value. 
There's no such thing as an "average" query. You need to write the query or queries you need to write to perform the function you are trying to achieve. Use the query explanation tools available in your DB engine of choice (e.g. "EXPLAIN" for MySQL) to help you hone in on an appropriate execution plan. There are many things involved in tuning queries for performance (and the "length" of the query in ASCII characters isn't one of them), but my few rules of thumb would be: * Be sure you are properly using your indexes. * If proper indexes don't exist, create them (or ask a different question of your data). * Don't waste your time with premature optimizations. If performance isn't an issue (and isn't likely to be), don't make it one just for the sake of having something to do. Note: If your data model does not appropriately support answering the questions you need to answer in an efficient manner than you need to transpose the data in a manner that allows you to do so.
I appreciate the sentiment, and theoretically I agree (maybe you're 100% correct). But you could also say there's no "average" if/ else statement. But if you have 10 different `else if`'s, and inside those are some more nested if/ else's then you should usually look to see what you can decompose into methods/ functions, have you made something too complex. But sometimes that's just the code that you need. I'm not asking for what is a "bad" sql statement, more like if you were walking by and glanced at a screen what you make you stop and double check that that's really the best approach.
If we're talking about "code smells", poorly styled SQL statements that are hard to read are a bigger red flag to me than the size of a statement overall. When I see consistently styled SQL statements that are easy to read I generally take that to be a sign of someone who cares enough about their work to hopefully be writing sensible SQL, but the opposite is often even more true. If your SQL is a sloppy mess and hard to read, you are increasing your chances of making a mistake. Compare the differences in this simple statement: select tA.fieldOne, tA.fieldTwo, tB.fieldThree, tC.fieldFour from tTableA tA inner join tTableB tB on tB.tableBId = tB.tableBId left outer join tTableC tC on tC.tableCId = tA.tableCId where tA.fieldOne &gt; 10 order by tA.fieldOne, tB.fieldThree limit 100; vs. SELECT tA.fieldOne, tA.fieldTwo, tB.fieldThree, tC.fieldFour FROM tTableA tA INNER JOIN tTableB tB ON tB.tableBId = tB.tableBId LEFT OUTER JOIN tTableC tC ON tC.tableCId = tA.tableCId WHERE tA.fieldOne &gt; 10 ORDER BY tA.fieldOne, tB.fieldThree LIMIT 100 ; They say the same thing, but I'm going to have more faith in the person who formats their SQL the 2nd way than someone who consistently writes their SQL the first way (I'm also going to have a hell of a lot easier time debugging well formatted code like the 2nd, regardless of how large of a statement it is). For what it's worth, I don't particularly care for the concept of defining "code smells" (and I cringe a bit every time I hear that). To me, the people who throw around that kind of terminology are often the same people who blindly follow "best practices" without having any direct appreciation for why something is a best practice in the first place (and therefore having little idea when it's acceptable or even wise to deviate from those practices).
The following is code smell but not necessarily bad. But seeing this warrants a deeper look for opportunities for optimization. * `DISTINCT`, `COUNT( DISTINCT ... )` , and large number of columns in `GROUP BY` (especially `CASE` statements) usually indicates inefficient data model (i.e. could be optimized by normalizing data) * `UNION` instead of `UNION ALL` * `OFFSET`(especially in the context of paginating results) is basically evil * Functions in the `ON` or `WHERE` clause - should check for non-sargeability * date filters that use `BETWEEN` or `=` instead of inequalities * Subqueries in the `SELECT` clause * Correlated subqueries * `IN (SELECT col FROM tbl...)` - make sure that `tbl.col` is non-nullable or you understand the expected result when col is null * the usage of `INNER JOIN`'s as "filters" - i.e. if you inner join a table, you better be using it in the `SELECT` statement or joining it to some other table. This is just a start.
SQL is for fetch and structuring data. If you are doing more processing it is probably wise to move it up a level to the application layer. Saying that you can still do a lot of processing by using functions and stored procs but more often it is still around data fetching than processing
&gt; Correlated subqueries Why is a correlated subquery a code smell?
The go to answer for all database maintenance questions is to us Ola Hallengren's solutions. They're highly regarded. Or are you doing this for a thought experiment? Because its going to be difficult to get those two dates to be equal.
Look into "like" condition.
It is quite late in the semester for you to be so lost. Also, simply Google. W3Schools will explain it just fine. 
You'll need to look into check constraints and the like function. You'll have to learn it on your own since this is a homework assignment so I'm just going to point you towards https://msdn.microsoft.com/en-us/library/ms187550.aspx so you can figure it out on your own.
Postcode formats &amp; requirements vary from country to country. What have you tried? We aren't going to do your homework for you.
dear OP, please see sidebar, and identify your platform
post_code like '[0-9][0-9][0-9][0-9]-[0-9][0-9][0-9]'
That's it, thank you!
Yes. At best, it makes no difference, and at worst, it can hinder performance. The only reason I would use an `INNER JOIN` this way is to improve readability when I'm **certain** there's a 1:1 relationship between the keys.
My work wants my boss and I to train our Software Engineers weekly so that they can be SQL Queries 70-461 certified and we also get to offer it free to the public. This is a weekly event and if you can make it, the videos and materials will be viewable/downloadable from www.AaronBuma.com I'm not trying to direct discussions away from Reddit, come to the trainings and discuss it here.
What isolation level are you using. SELECT @@ISOLATIONLEVEL 
so its a thought experiment based in reality. The place I work has a backup job that runs over night but does fail sometimes. made me think there has to be a way to check that over night and then run a differential with the time thats left. the second part is that sometimes the .bak files go wrong too, so why not test that too. so thats the reality. I call is a thought experiment because they wouldn't use anything I presented. I am just starting to learn SQL and I work better when there is a problem to solve
You can absolutely use a stored procedure and dynamic SQL inside SSIS. IMO it would probably be the easiest/fastest way to do this. Can you give an example of code in your procedure? I'll see if I can recreate
Depending on the version of Excel and any artificial constraints you might have, maybe consider pulling instead of pushing. That is, use Power Query to get data from the db into Excel, refresh-able and everything. (And all the awesome benefits that come with Power Query PowerPivot (and PowerView/Map))
 SELECT ProductID SUM(sales) FROM table WHERE saleDate BETWEEN IntroDate AND DateAdd(IntroDate,01,MM) GROUP BY ProductID If you're going out to N months after the product has been introduced put that into a loop.
dear OP, please see sidebar... which platform are you using?
This is very simple. In excel, click the data tab, other sources, from sql server. Connect to your sql instance and It's pretty straight forward from there. You can import full tables, use queries, or execute stored procedures. Edit: Forgot to mention that you can even have it refresh on set intervals,or whenever you open the file. 
Join sales to the 'Introduction dates', group by the months between the sales date and the introduction date (look up some date functions for your platforms).
Having never heard of powerquery I'm very happy to learn such a thing exists. Thanks for posting the suggestion.
Seems like a view is the way to go then, honestly I'm not too worried about performance since this is just a hobby project to get a better understanding of SQL so there's not that many tables involved. But the few tables involved I'd rather design according to best practices, thanks for the advice I had already converted the bit to int dunno why I posted it as bit (must be sleepy this morning) and I'm filtering out the false already so that's not a problem.
You really need to learn and start using ANSI 92 and above join syntax. ANSI 89 joins in Oracle are extremely limited which is why you have your problem. Also, move your column based embedded queries to joins, there is a time and place for column based queries, but yours isn't one of them. Nvl-ing columns mean an index can't be used (treated as a function on a column). So check a column value or check is null, so that indexes can be used where possible.
&gt; Isolation Level = 1
you wouldn't need a case, you can simply do a SUM(convert(int, yourBit)). 
I'm not 100% sure what all this means but I will look it up and see if it helps me fix it. Thank you. 
I know all to well about those awful jet drives and yes that has worked for me in the past with putting the largest rows in the top X to fix truncation. I'm curious if /u/ColdJelly 's solution would work for you. I'm not sure if it uses the jet 4.0 drivers or ADO.NET by nature. But it seemed to handle the type casting issues I was having to use Derived Columns for in SSIS between unicode and non-unicode types without any additional work from me. All it required was the query or the direct table select from the database table list.
Given that you said Event Log, I'm assuming SQL Server(see side bar). What happens when you perform it manually in query window instead of the job? Try using a [Copy-Only BACKUP LOG](https://msdn.microsoft.com/en-us/library/ms186865.aspx)
Think of it as sets. You have the set of people a person knows and you have the set of people a person likes. A person likes everyone they know, if the "likes"-set contains all elements of the "knows"-set. To find this out, you can simply subtract one set from the other. So I'd say it should be something like this: SELECT p.id, p.name FROM Persons p WHERE NOT EXISTS ( SELECT personB_id FROM Knows k WHERE p.id = k.personA_id MINUS SELECT personB_id FROM Likes l WHERE p.id = l.personA_id ); EDIT: Sorry, I just realized I did not answer the real part of the question about the symmetric relationship. I'll look into it a little more. How about this? Now the set of people a person knows considers all personA_id and personB_id entries in the "Knows" table. SELECT p.id, p.name FROM Persons p WHERE NOT EXISTS ( ( SELECT personB_id FROM Knows k WHERE p.id = k.personA_id UNION SELECT personA_id FROM Knows k WHERE p.id = k.personB_id ) MINUS SELECT personB_id FROM Likes l WHERE p.id = l.personA_id ); 
I second /u/ziptime's recommendation about the joins and the column-based queries. Did you write this query? In any case, it seems that your posted code is out of order. The first query has the "Coordinates" while the second query does not. But, essentially, that column-based subquery for the coordinates is where you problem lies. Assuming that ssrmeet_bldg_code is null for online courses, an inner join (where g.bldg_code = m.ssrmeet_bldg_code) will only give you the courses that have a building code in ANML.uaa_wapp_bldg_coordinates g. And I'm assuming the online courses do not exist in ANML.uaa_wapp_bldg_coordinates. You need to change that to an outer join.
You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'UNION (SELECT personA_id FROM Knows k WHERE p.id = k.personB_id ))' at line 6 This is my output. It's a bit different I think because its on a site from my university therefore maybe not all commands work?
It worked as a charm! Thanks again!
okay, i totally forgot the reflective part of the data, sorry thanks for the test data, this now works correctly -- SELECT name FROM ( SELECT p.name , k.personb_id AS knows , l.personb_id AS likes FROM persons as p INNER JOIN knows as k ON k.persona_id = p.id LEFT Outer JOIN likes AS l ON l.persona_id = p.id AND l.personb_id = k.personb_id UNION ALL SELECT p.name , k.persona_id AS knows , l.personb_id AS likes FROM persons as p INNER JOIN knows as k ON k.personb_id = p.id LEFT Outer JOIN likes AS l ON l.persona_id = p.id AND l.personb_id = k.persona_id ) AS data GROUP BY name HAVING COUNT(*) = COUNT(likes) the answer is only person **13** likes everyone he knows 
like this? SELECT a.p , b.name , subq.c FROM ( SELECT aid , COUNT(aid) as c FROM r WHERE rstatus = 1 GROUP BY aid ) AS subq INNER JOIN a ON a.id = subq.aid INNER JOIN b ON b.id = a.bid 
There should be no need to use xp_cmdshell to perform a restore from a backup. You'll have to explain your situation more completely if you're certain that you need it.
You should never need xp_cmdshell anymore. There is no reason, outside terrible hacks that should be avoided, to use it over: * Querying it directly (see below) * [Using PowerShell](http://blogs.technet.com/b/heyscriptingguy/archive/2013/08/27/powertip-show-attached-usb-drives-with-powershell.aspx) to pull the information and execute the correct query *** USE master GO CREATE PROCEDURE dbo.RestoreDatabase @filePath VARCHAR(256) = 'Z:\SQLBackups\DatabaseNameHere.bak', @databaseName VARCHAR(128) = 'DatabaseNameHere' AS BEGIN IF @databaseName NOT IN (SELECT name FROM master..sysdatabases WHERE dbid &lt;= 4) BEGIN RESTORE DATABASE @databaseName FROM DISK = @filePath WITH RECOVERY; END ELSE BEGIN SELECT 'DETECTED THAT SYSTEM DATABASE NAME WAS PASSED, RESTORE FAILED' END END *** -- Default: EXEC dbo.RestoreDatabase -- With parameters: EXEC dbo.RestoreDatabase @filePath = 'S:\AdventureWorks.bak', @databaseName = 'AdventureWorks' ***
How would you handle this with a backup filename that always changes?
If you're set on making a SProc, just make the name of the backup file a parameter you can pass into it. There is absolutely no need for `xp_cmdshell` in this case.
Emacs's org-mode is fantastic for this. You can write your [notes, and then code examples together, then make emacs run the code on demand and include the results in your notes too](http://home.fnal.gov/~neilsen/notebook/orgExamples/org-examples.html#sec-13). It's like [a REPL for anything](http://ergoemacs.org/emacs/emacs_org_babel_literate_programing.html).
Pulling data from excel instead of pushing as mentioned a few times here would have been a perfect option. unfortunately the end user access is restricted. anyways, I'm glad I learned this. I worked with MSSQL for several years and I never thought about it. It'd make life easier for some internal projects where DB access isn't the issue. 
upvote for the wiki.
My personal thought is that the scripts should be in a text file, and that text file stored in a VCS. If you want it a stored procedure, then it should be in a text file and that text file should be stored in a VCS. I only partly do this. If the SQL isn't associated to a specific project then it ends up in a text file and no version control... I should go fix that.
The dynamic filename is the only part that using TSQL wouldn't do for you natively so I understand your thoughts around using xp_cmdshell. But as others have pointed out, xp_cmdshell is a huge security hole. Its a bad habit to get into using it at all. I would go right to PowerShell. It really shines for this type of task imho. You could also use SSIS if that's something you are comfortable with. 
Folder on desktop called "scriptsnshit" sub folders in there broke out by product, broke out in there by sub product, etc... Whole thing goes into private stash repository.
Thanks, Yeah it became a habit I think. But it is nicer without it!
Ah yeah I've got a tab open to read up on lateral joins because I believe they would present another way to accomplish the same. Thanks for commenting, if you don't mind I'll just cross-post your comment on my blog so others can find it. **Edit**: updated the post with an example of using a left lateral join. Thanks again for the suggestion.
When you create the data connection, go to properties at the very last step. Then drill into the connection file and you can write a query in there first so it doesn't pull up the entire table. Additionally, you can write a view in the db within SSMS, save it, and then query the view in your connection file. PM me if you want help with this with clarification of where I'm talking about. I'm on mobile now, so can't load up screenshots. 
On Heroku, we have [dataclips](https://devcenter.heroku.com/articles/dataclips). 
Its a bit complicated for me since I flit between clients and they generally don't like me recycling code I wrote for another firm. I use Evernote as my permanent repository and text files on site.
or do you mean... Bat_Man? 
Would it be possible to direct data from several temperature sensors to a SQL database? Ever heard of anything like that? All the literature I read on SQL is business intelligence or finance related.
&gt; Would it be possible to direct data from several temperature sensors to a SQL database? Sure. Depending on the application, you might need some way to identify each sensor. Sensors that are designed with automated logging in mind have simple ways to do that. Some sensors are designed to be used with logging software. They don't write directly to a SQL database. Instead, the logging software either creates a file you can import into a SQL database, or it provides its own database.
At scale, you'll want to incorporate a database. But there isn't any need to reinvent the wheel. For a project like this, you may find that You'd be better off using a Content Management System. PHP with MySQL backend is the current standard in the CMS arena, so it isn't a leap too far if you've got familiarity with either, much less both. My preference is joomla!, but the other leaders in the category - Wordpress and Drupal - receive enough praise that they are all worth considering. Since each is open source and has a fantastic and robust worldwide community, you really cannot go wrong. 
Is heroku worth learning g?
I work there, so I'm probably not the best judge, but if I didn't think Heroku was doing something that was awesome for developers I'd find another job.
I keep them in repository classes that can be executed from our code base. That way, they can be called from the code, under source control, and more easily organized. Having hundreds of stored procs in a single expandable tab in tsql is just agonizing to me.
Yes, it's an Excel limitation. Do you always need the same 500? Go into the SQL and add a WHERE [MyProductField] IN ('all','500'.'products',...) The easiest way to build the list is by taking your existing filtered Excel, copy the column, then paste it into a good text app like Notepad++ where you can use the column editor to add the ',' between every value.
I use Google Keep for my SQL snippets. 
The ideal way or the simple way? The ideal way is to have an API that contains the business logic for the action or query you want performed. The logic would then exist in code, which would connect to various repositories to provide the data or action required. You can then run whatever front end, from a browser, to a Windows Forms app, to a cron job to get at the data or do the thing you need done. The simple way? I use Linqpad and store all of the queries in the My Queries pane. I achieve sorting and querying with organization and memory.
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Bin packing problem**](https://en.wikipedia.org/wiki/Bin%20packing%20problem): [](#sfw) --- &gt; &gt;In the __bin packing problem__, objects of different volumes must be packed into a finite number of bins or containers each of volume *V* in a way that minimizes the number of bins used. In [computational complexity theory](https://en.wikipedia.org/wiki/Computational_complexity_theory), it is a [combinatorial](https://en.wikipedia.org/wiki/Combinatorics) [NP-hard](https://en.wikipedia.org/wiki/NP-hard) problem. &gt;There are many [variations](https://en.wikipedia.org/wiki/Packing_problem) of this problem, such as 2D packing, linear packing, packing by weight, packing by cost, and so on. They have many applications, such as filling up containers, loading trucks with weight capacity constraints, creating file [backups](https://en.wikipedia.org/wiki/Backup) in media and technology mapping in [Field-programmable gate array](https://en.wikipedia.org/wiki/Field-programmable_gate_array) [semiconductor chip](https://en.wikipedia.org/wiki/Semiconductor_chip) design. &gt;The bin packing problem can also be seen as a special case of the [cutting stock problem](https://en.wikipedia.org/wiki/Cutting_stock_problem). When the number of bins is restricted to 1 and each item is characterised by both a volume and a value, the problem of maximising the value of items that can fit in the bin is known as the [knapsack problem](https://en.wikipedia.org/wiki/Knapsack_problem). &gt; --- ^Interesting: [^APX](https://en.wikipedia.org/wiki/APX) ^| [^Approximation ^algorithm](https://en.wikipedia.org/wiki/Approximation_algorithm) ^| [^Knapsack ^problem](https://en.wikipedia.org/wiki/Knapsack_problem) ^| [^Strongly ^NP-complete](https://en.wikipedia.org/wiki/Strongly_NP-complete) ^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cp3reis) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cp3reis)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
IDEs such as Toad/Oracle SQL Developer allow for storing SQL snippets with a name. I use that feature to store them.
Opps, wrong subreddit..again... https://www.reddit.com/r/raspberry_pi
1. Why do you tell them you are recycling code? 2. Do you then do **everything** from scratch every time?
Yes, I'm startled that no one else seems to have mentioned it... Code lives in subversion, not a notes app...
Why not create a table to store SQL code and additional columns to categorize, tag, organize. 
Wait. Don't most SQL databases have console clients so you can *directly* run SQL scripts? So: store it in a file. Duh.
nope, and I know
Have them in some form of executable script, like a set of rake tasks, and put it in version control.
This is the solution you should use. If you need to order the results, you can alias ColumnThatINeed and order by the alias.
This works, many thanks :)
Sadly I can't create anything on the DB, also the people who makes the requests always uses the unfriendly names. The upside is that this DB is used less and that my team doesn't do the transition to the new db :)
In that case you will want to add another column to the recipes take as a primary key and use a previouslyMade table with a timestamp and a foreign key referencing the new primary key. If you wanted to be anal about it, you would also have an ingredientsHistory table that shows when you bought what. Then you could go back in time to any date and see what you had at that time. 
Don't use `select *` in the first place. Select the columns that you need (by name), in the order that you need.
SQL prompt used to save code.
As long as they stick to a naming convention, I agree 100%.
If I understand the problem correctly, this just looks like a complicated programmatic approach to wrap OUTER JOINs and execution plans. 
What's it called in sql developer?
will sp_helptext help you?
Thanks. The imgur link didnt work. It gives a request to log in instead of a picture. But I googled snippets and got this wonderful blog post from the sql developer product manager. http://www.thatjeffsmith.com/archive/2012/08/i-say-snippet-snippet-good-sql-developer-snippets/ 
Oops, my bad. Fixed the image link now. Btw that image is from his blog post. And he's fairly active on reddit too. /u/thatJeffSmith
Oh, he's a redditor. Nice. /u/thatJeffSmith, where do you prefer to have bug reports for SQL Developer filed?
well i did find what i was looking for it wasnt related to tblbatchtransaction but a different table but this does give me some ideas for more overly complicated fun.
Combine some of your queries at least: SELECT @MaxShipID = MAX(ShipID) , @MinShipID = MIN(ShipID) FROM Ships Plus there are ways to fill those gaps. Not to mention collisions or duplicates.
We've considered making a switch to Wordpress but the two problems I have with that is that 1. ~3000+ pages is no small feat to move to a database, and 2. Wordpress is fairly slow, and I'd like to keep my site running as fast as possible. I haven't used Joomla but I'm assuming that if it's as complex in the back-end as Wordpress is, it will be equally as slow. I figured "re-inventing the wheel" would save me both time by not having to migrate to a CMS and save on site speed as well. Thoughts? 
I'd say the idea is in the right direction, but the implementation is overly complicated and has limited adaptability. Learning to read execution plans is a very valuable skill regardless; it's suitable for many people that work with MSSQL. After a bit of time you'll be able to see performance bottlenecks([RBARs](https://www.simple-talk.com/sql/t-sql-programming/rbar--row-by-agonizing-row/)) or shortfalls in what you would expect (ex. A hash join receives two inputs of 10000 rows each and only outputs 40). Resources for Execution Plans: * [Basics Level](https://www.simple-talk.com/sql/performance/execution-plan-basics/) * [Understanding the Graphics](https://www.simple-talk.com/sql/performance/graphical-execution-plans-for-simple-sql-queries/) * [Moderate Level](https://www.simple-talk.com/sql/performance/understanding-more-complex-query-plans/)
It also increments the value by 1000 if the machine improperly shuts down as a safety precaution for pending transactions/etc. If you ever really want numbers in a row, you should use [SEQUENCE](https://msdn.microsoft.com/en-ca/library/ff878091.aspx)s. NOTE: They are also transaction unaware, but the easy workaround it to populate it into a variable before the transaction block then restart it at that point if it fails. ALTER SEQUENCE yourSeq RESTART WITH @ValueBeforeTrans INCREMENT BY 1; *Quick edit - Note that if you are using it in that capacity you should declare it with NO CACHE as well*
Thank you for this! I'm going to start watching these. I'm just starting to study for this myself. Are there any books you would recommend? I'm feeling fairly lost.
I currently have this implemented....we get numerous requests for data so we simply save the SQL string in a table with name and description etc. User interface allows user to select a query and supply any parameters if any and run the SQL string. It's great because you can store documentation for the query in the same table (and part of the same database) and even have a version/change log if you like.
This actually sounds pretty cool. Definitely interested in any specifics of the implementation you are willing to share. I am looking at a situation that is similar where an application will need to get data for various things from different sources and I was trying to think of a good (and "secure") way to make this 100% DB driven.
Care to share the error?
Step 1: Format your queries so they are more readable. Step 2: Your where clause is filtering on a count per-row in the soldvia table. Your problem statements wants you to sum these counts up first and *then* see which sums have more than three. e.g. (untested) SELECT p.productid, p.productname, p.productprice, SUM(s.noofitems) AS TotalNumberOfItems FROM soldvia AS s INNER JOIN product AS p ON p.productid = s.productid GROUP BY p.productid, p.productname, p.productprice HAVING SUM(s.noofitems) &gt; 3 ORDER BY p.productid ; EDIT - Step 3: Stop posting crap like this on StackOverflow. SO (nor this subreddit) is the place you should be going to get your homework done for you. These are straightforward concepts, and I'm sure they're covered in your course material.
The last try gave me this error trying to import it via pgadmin: ERROR: extra data after last expected column CONTEXT: COPY &lt;tablename&gt;, line 1: "-- MySQL dump 10.13 Distrib 5.5.40, for Linux (x86_64)" I'm on a Windows machine
We are currently going through the Joe's 2 Pro's 70-461 series. They might be hard to come by (or will be) because they are going out of business, but it's not for lack of content! Will you be able to watch today's review session? If you can't I'll post when the video is available.
Umm, that looks like a dump straight from mysql. You can't just import that into postgres through pgadmin.
Oops - I'm an idiot. I should have read MySQL. Same principle applies, but I believe you need to explicitly create your tables.
I probably won't be able to watch the review session today. I got onto your site and was just going to start watching from the beginning. It's unfortunate that Joes 2 Pro's is going under, I bought their C# book and enjoyed it. I was going to look into their SQL books as well.
Could you add an intermediate table, and assign each of the new branches an integer code in your order/list? 
The reason we went with alphanumeric is because we use 3 digit branches and they're either being used or are already reserved.
Okay, so you need to convert your existing integer only branches to varchars. If it's called branch: Convert(varchar(3),branch). If it was me I would change the db so that branch is varchar and ensure all code that uses branch will cope. 
The [ISNUMERIC](https://msdn.microsoft.com/en-us/library/ms186272.aspx) function can test if the value is numeric or not. Will the alpha in the alphanumeric be predictable? If so, after testing, if it's not numeric, you could take a substring containing only the numeric part. In either case, numeric or substring-ed, cast or convert to int.
"P01" is not an integer and never will be. If you want it to be, you'll need to figure out some way to map that value to an integer, presumably so that it's still unique in your system. It sounds like some one has a made poor decision to use the alpha numeric values because the database does not want to work with that. You're either going to have to come up with some way to map the alpha numeric codes to integers or change everywhere in your database that's using an int to a string value. I know you've said you use 3 digits for branches but if you're storing that data in an int, then you have lots more digits to use for your mapping. It's then a presentation concern how those values are displayed. All in all, sounds like a couple different bad decision along the way.
I didnt make this decision and I dont think they realized it would be a problem.
I'll look into this more. The branches are C01,P01,K01,N01,T01
you need to create a new surrogate key (int) to map to their branches and treat the alphanumeric branch numbers as legacy. What if they had already used 3 digit ints which overlapped with yours? How would you have handled it? Or, convert your ints to varchars in your primary dataset. 
They werent like that. We converted their data and imported into our database. Since all 3 digits were taken or reserved he decided to go alphanumeric. 
It's simply a CURRENT_TIMESTAMP.
When you are importing it, set the target binding type to datetime type. 
Yes, that will work as well. 
or to your example where you are looking into the future somehow: &gt;between cast(getdate() as date) and cast(getdate()+6 as date)
I tried this but got: Warning: mysql_fetch_array() expects parameter 1 to be resource, boolean given in /index.php on line 193
I missed the mysql tag, my bad. assumed this was for MS SQL, let me look 
Map the new alphanumeric values to integers and go up to four digits or convert your existing columns to varchar.
This would probably be the most reliable way depending on how many numbers you really have... I would map the alpha characters to a value 1-26. so 'P01' would become '1601'. While 'A01' would become '0101' and then convert that into int. which should translate nicely. [edit] - this might be a better way to handle it... or - if you don't /technically/ need those same exact numbers translated... just insert them all into a new table with an identity field. let the database assign them new int values and keep the alphanumeric values as a legacy reference.
Cte( Select top 1 transactiondate From table Where weekday(transactiondate())=4) Order by transactiondate desc) Select * from cte c Join table t on t.transactiondate = c.transactiondate) Where transactiondate &lt; dateadd(day,transactiondate,7) Sorry im on my phone if there is any syntax error let me know will get onto my laptop hope this helps.
Seems like in SQL that your query is ok, but I do not know how JavaScript / node handles returned values. Try *aliasing* the second id column. Like: b.id as id2,
I don't know java script. But in SQL this will fail because you aren't specifying the joins between the two tables. From file_queue a inner join cores b on a.core_id j b.id Where.... Also alias b.id as b.id as core_id. 
Alias your columns. ------------------------------------------------------ a.id would become: a.id AS a_id --------------------------------------------------- and then b.id would become: b.id AS b_id ----------------------------------------- I'm not sure about JavaScript (node) but my best guess is that because you have multiple columns with the same name, it is assuming they are the same column and only showing one. I might be way off with this, but this is a simple fix and certainly worth a try, and multiple columns with the same name should always be aliased anyway.
~~So this one is not working for me, and i think it is because the LVDplus4 and LVDplusAMPD are calculations given aliases, not true columns, here is that secion of my query.~~ select average_miles_per_day, DATEADD(day, 4*30.16, last_visit_date) AS LVDplus4, DATEADD(DAY, (default_minor_mileage/average_miles_per_day), last_visit_date) AS LVDplusDays, delivery_date FROM Table ~~When i try to use the case statement as you posted it, i get invalid column names, when i try to use it using the entire formula,~~ Nevermind, i must have done something wrong, it is working when i use the formulas directly, thank you for your help!
Bug reports officially should go to My Oracle Support. You'll need an active database support agreement to open a Service Request for SQL Dev. You can ask for help on our forums - hosted on the Oracle Technology Network (OTN) too, but don't expect formal bug reporting there. If you're super lazy, you can try tweeting me @thatjeffsmith 
No problem. Worth noting as well, if there is a decimal then that gives the time: 42026 22/01/2015 00:00:00 42026.12345 22/01/2015 02:57:46 The numbers after the decimal give the seconds since midnight... So:
Several people have answered this, but in addition, PLEASE do not build your SQL up like you are doing. This approach is prone to SQL injection, which is probably the biggest reasons for IT security flaws there is. See [here](http://www.reddit.com/r/SQL/comments/2u1q6g/alternatives_to_blacklisting_a_bunch_of_words_and/co4jsek) for an explanation as to why. Node.js supports parameterisation of SQL queries and you should use it! var query = connection.query('SELECT * FROM users WHERE id = ?', [userId], function(err, results) { //query.sql returns SELECT * FROM users WHERE id = '5' });
&gt;`select a.id as a_id, b.id as b_id, a.whatever from fq a, cores b on a_id = b_id` or what have you I believe you have to put tics, so it'd be: Select a.id as 'a_Id', b.id as 'b_id' That would name the columns in your results. 
thanks for chiming in! I found I didn't need the tics after all, for what it's worth.
thanks for addressing my question - I found you were 100% right, but I did keep the 'AS' since it made it easier to read for me and whomever comes after me to look at the code, I think.
thank you so much for this response. I know about injection (though I'm still noobish enough not to know when to worry about it. to be fair, the variable in my query is constructed within the script and has no user input. still, this is really important for best practice it seems. I have done parameterisation elsewhere at times b/c it's just easier to read when you've got a shit ton of columns and values. what I'm curious about is, does using parameters have some built in logic where the parameters passed are verified? I read the link you provided but it wasn't clear what's happening under the hood here. and to just be totally clear (being noobish), are you saying that any case where one might use concatenation one should use parameters? thanks. this is really helpful. I don't want to write stupid code!
Please show us what SQL you've got; we can help you much quicker. Five subqueries is nothing to worry about from a technical standpoint; performance may reduce, though.
Assuming you are SQL2008 or above you should be able to replace '%mydate%' with CONVERT(date, SYSDATETIME()) in your EXEC statement. No quotes, just put the covert after the comma. This returns the current system date. If you are below SQL2008 you should be able to use GETDATE() instead of SYSDATETIME(). Automating the date on the output will take some batch wizardry. This stack overflow article might help: http://stackoverflow.com/questions/19131029/how-to-get-date-in-bat-file Good luck.
Do I pull out the "set mydate=03/03/2015" line? I got this error on the output: Msg 156, Level 15, State 1, Server SERVERNAME, Line 1 Incorrect syntax near the keyword 'CONVERT'. 
Why not a query like: where contains(ingedients, "tomato") and contains(ingredients, "olive oil") and contains(ingredients, "flour")
sho 'nuff
Sorry, I'm stumped then. If I was at work I could have a play. Hopefully someone else has an idea. You might be able to handle it through the batch variable you use now and setting it with set mydate=%date%
The WHERE clause isn't doing any magic. The CTE is doing the magic. The answer is basically CTE's are allowed to be recursive. They don't exist, except in logic, which means they can do things like recurs endlessly. SQL Server has built-in logic to terminate recursive processes after a certain number of recursions (100 is the default) - so that WHERE clause is just preventing it from having SQL kill the process.
Setting that switch got me &gt; Conversion failed when converting date and/or time from character string. Thanks for the help at least, man.
Ok, it looks like it's looking good now. Thanks!
The WHERE clause is what allows the recursion to end and return the results 1-10 to me (eventually num = 10, so the result set becomes null due to the WHERE clause) right? That's what I meant with the "magic" thing... If I can re-write the recursion into a ~~procedural~~ nested, non-recursive version, would it look like this? SELECT 1 AS num UNION ALL SELECT num+1 FROM (SELECT 1 AS num UNION ALL SELECT num+1 FROM ... /* repeat until num &gt; 9 */ ) Am I thinking about this the right way?
If you look at the cte_test bit, you'll see an anchor part SELECT 1 AS num and a recursion part SELECT num+1 FROM cte_test WHERE num &lt;= 9 Notice the second part of the CTE is referring to the CTE itself (cte_test)? Now imagine you replaced the cte_test reference with the contents it refers to, effectively flattening the query, and you did that until you reached the termination clause (WHERE num &lt;= 9, so nine times), you'd get something like this... SELECT 1 AS num UNION ALL SELECT num+1 FROM ( SELECT 1 AS num UNION ALL SELECT num+1 FROM ( SELECT 1 AS num UNION ALL SELECT num+1 FROM ( SELECT num+1 FROM (... [9 nesting levels] ) WHERE num &lt;= 9 Once you expand things, you can probably easily see that every nesting level is one more than the previous, due to the anchor term and that is stopped by the termination clause. 
I might have gotten it backwards from the start... I thought the SELECT num+1 FROM cte_test would be evaluated **first** (as would normally be the case in other programming languages if I recall correctly), rather than the SELECT 1. The "1" is used in the recursive part of the CTE, resulting in the result set (2). Then "2" is used for the next recursive call, until finally num = 10. Finally starting to see the light thanks to this site: [link](http://www.dotmaniac.net/recursive-ctes-for-dummies/)
Thanks. This is helpful and I'm on it!
It would probably look something like this.... SELECT 1 AS num UNION ALL SELECT num + 1 FROM (SELECT 1 AS num UNION ALL SELECT num + 1 FROM (SELECT 1 AS num UNION ALL SELECT num + 1 FROM (SELECT 1 AS num UNION ALL SELECT num + 1 FROM (SELECT 1 AS num UNION ALL SELECT num + 1 FROM (SELECT 1 AS num UNION ALL SELECT num + 1 FROM (SELECT 1 AS num UNION ALL SELECT num + 1 FROM (SELECT 1 AS num UNION ALL SELECT num + 1 FROM (SELECT 1 AS num UNION ALL SELECT num + 1 FROM (SELECT 1 AS num WHERE num &lt;= 9) WHERE num &lt;= 9) WHERE num &lt;= 9) WHERE num &lt;= 9) WHERE num &lt;= 9) WHERE num &lt;= 9) WHERE num &lt;= 9) WHERE num &lt;= 9) WHERE num &lt;= 9) WHERE num &lt;= 9 
I'm not familiar with SQLite, but in T-SQL I think it would look something like this: SELECT [Column] FROM Recipes WHERE Ingredient IN ('ingr1','ingr2','ing3','ingr4','ingr5') Without more information it's hard to say, but that is my best guess.
batch is dead. Do it with PowerShell. $myentity="123"; $myinv = "ABC"; invoke-sqlcmd -serverinstance XXX.XXX.XXX.XXX -database "test" -Username USER -Password PASS -Query "exec test.dbo.sproc_STOREDPROCEDURENAME '$myentity', '$myinv', '$(get-date -Format "MM/dd/yyyy")'" | Export-Csv -NoTypeInformation -Path "c:\output\$(get-date -Format "yyyyMMdd")_ccyexposure.csv";
That doesn't look right. What is in the "Recipe" column? It seems strange that a column called "Recipe" just contains ingredients.
I read somewhere that this is a way to get all the rows with highest values, would group by not cause losing this data?
I ended up getting myself those tables from our admin and it works great. But without this, I would not be able to create any sort of tables on my own.
Yeah sorry I made a mistake in my las post Recipe is a column containing the ingredients. Dont worry about the name of it but that is the column I am trying to query by multiple recipes. I tried: SELECT * FROM Recipe_tbl WHERE Recipe LIKE %ingr1% and %ingr2% and %ing3%....... But it didnt seem to work maybe I just got the syntax wrong?
 SELECT * FROM Recipe_tbl WHERE Recipe LIKE %ingr1% and %ingr2% and %ing3%....... 
but if you want all the ingrediants you'd have to change the OR to AND WHERE Recipie LIKE ('%INGR1%') AND Recipie LIKE ('%INGR2%') AND Recipie LIKE ('%INGRn%')
This question is timed perfectly with an issue I was having with CTE's. I didn't even know this sub existed before today. Thanks for all the great explanations as well.
Can a customer repeat the same purchases in a day? If not: SELECT SUM(purchase_amount) / COUNT(DISTINCT(customer_id)) AS a FROM ( SELECT customer_id, item, MIN(purchase_date) FROM purchases WHERE purchase_date &gt;= '2013-02-01' AND purchase_date &lt; '2013-03-01' GROUP BY customer_id, item ) AS min_p INNER JOIN purchases AS p ON min_p.customer_id = p.customer_id AND min_p.item = p.item AND min_p.purchase_date = p.purchase_date; If they CAN repeat purchases in the same day, then you need to use a window function, which is tougher to understand but here's your query SELECT SUM(purchase_amount) / COUNT(DISTINCT(customer_id)) AS a FROM ( SELECT customer_id, purchase_amount, row_number() OVER ( PARTITION BY customer_id, item ORDER BY purchase_date ASC ) AS rn FROM purchases WHERE purchase_date &gt;= '2013-02-01' AND purchase_date &lt; '2013-03-01' ) AS p WHERE p.rn = 1; 
Thanks for the links, have added to my todo list. 
It definitely helps with readability. Glad to help :)
Using this query: SELECT * FROM Recipe_tbl WHERE Recipe LIKE '%firstingredient%' AND LIKE '%secondingredient%'; I get a syntax error on the second ingredient: " '%secondingredient%' "
I *think* I understand what you're doing here. Nested queries pretty much destroy readability. Edit: Changed `INNER JOIN` to `LEFT JOIN` since I get the feeling you want to default to zero in the event that `[dbo].[Test1]` does not join. Edit 2: Even simpler, left join on `[Key]` *and* `[IsActive] = 1`, then just default to zero without the need for a case statement. UPDATE [T2] SET [IdTest1] = ISNULL([T1].[IdTest1], 0) FROM [inserted] [I] INNER JOIN [dbo].[Test2] [T2] ON [T2].[IdTest2] = [I].[IdTest2] LEFT JOIN [dbo].[Test1] [T1] ON [T1].[Key] = [I].[Key] AND [T1].[IsActive] = 1
You should never use loops in a trigger, and sure as hell shouldn't use cursors. If you can't do a set-based operation in a trigger, the process should be moved into a transactional stored procedure.
Google relational division
I changed the insert statement to this and seemingly are mostly there. insert SERIAL_LOG (ITEM_NUMBER, SERIAL_NUMBER, LOCATION, Current_timestamp) select ITEM_NUMBER, SERIAL_NUMBER, LOCATION from SERIAL_NUMBER This is inserting a row for EVERY existing line in the SERIAL_NUMBER table Also, the timestamp insertion errors out. To get it to work, I actually remove the "CURRENT_TIMESTAMP" from the INSERT. 
Triggers create two "special" tables "inserted" and "deleted" that contain the rows that changed. You want to join to those tables in your select statement in order to get the serial numbers of the rows that were inserted or updated and then you can insert rows to your log for just these serial numbers. Also, the reason you are getting the error is, where you state "Current_timestsmp" you are indicating that you will be providing a value to insert in that column, but then you don't provide the value in the SELECT. I think you want to add a GETUTCDATE() after LOCATION.
 insert SERIAL_LOG (ITEM_NUMBER, SERIAL_NUMBER, LOCATION, Current_timestamp) select ITEM_NUMBER, SERIAL_NUMBER, LOCATION, GETUTCDATE() from inserted
That was it!!! FROM INSERTED!!! Awesome thank you.
StopThinking pointed out the "FROM INSERTED" part of the statement that keeps track of the inserted item and that worked.
The second one worked a treat. Thanks for the help! I'll have to go do some research to better understand window functions and how they work.
Don't use triggers. Use the auditing feature of 2008+.
I managed to learn that what I was looking to do is not possible and I have since redesigned my database to work with full-text searches in order to query it by multiple values.
I will keep that in mind. Thank you.
possibly quite relevant presentation given on Friday (its Sql Server but should still apply) http://sqlblog.com/blogs/adam_machanic/archive/2015/03/07/sqlbits-2015-query-tuning-mastery-clash-of-the-row-goals-demos.aspx
What's wrong with SELECT COUNT(*) AS DupeCount , Name , Number FROM Table GROUP BY Name, Number You can add a HAVING COUNT(*) &gt; 1 if you want just the dupes. Wouldn't this direct set based query be more performative that putting a CTE into memory? 
You could run a VM and still use SSMS. Or, if you don't mind spending a bit of money Jetbrains is coming out with a product they're currently calling 0xDBE, it's pretty good. If you do any Java/Python they also have PyCharm and IDEA, and the full versions of both do include the database functionality from 0xDBE.
That usually works, but in my case, each row is technically unique due to the primary_ID. That query would result in no duplicates. Basically, I need to find dupes in all other columns except primary_ID.
0xDBE works great for connecting to just about every DBMS
Is that an Excel date format?
yep 
What excel gives is days since epoch (Day zero) So: SELECT DATEADD(D, COLUMN, 0) FROM TABLE. (this is where column is the column with the date in, and table is the name of the table holding it.) 
 SELECT DATEADD(d,42072,'1899-12-30') That is using today's date, 42072 
you haven't done anything wrong it's f*%$#$% microsoft access that's wrong
yea had a feeling it was that, solution though? (need to use access)
i've gone many rounds with msaccess, neither one of us has escaped getting bruised... try this -- save the UNION query, then join to the query
&gt; SELECT DATEADD(d,42072,'1899-12-30') &gt;That is using today's date, 42072 Personally I always use zero instead of a quoted date, saves a tiny bit of overhead and is cleaner to read: SELECT DATEADD(d,42072,0)
Have you looked into sql lite? No server, file based solution that let's you make tables and query them. Most of the sql databases out there are a bit intensive as they are trying to set up a full blown server for you.
SELECT NAME, VENDOR, COST, COMPLEXITY FROM LIST_OF_SQL_DB WHERE COST = 0 ORDER BY COMPLEXITY ASC Seriously, SQLite simplest, but MySQL and MSSQL Express are also good because a significant part of DB admin later on is managing the server (index jobs, users, blah blah) and actually installing either of those two DB's isn't that bad and they are widely used, so you could play around with that stuff too. Obviously MSSQL is windows only.
Search "wise owl tutorials" in youtube. Wise Owl does a good set of beginning tutorials if you havent advanced past this point. Also, some others useful for work like vba.
Is that available for Macs?
http://www.reddit.com/r/SQL/comments/2wldbz/working_with_nullsfull_outer_joins/cp9ppja
you can maybe run an emulator? edit- perhaps this? http://razorsql.com/
Is there any reason why all of these languages require the use of Terminal/cmd line?? It's so frustrating and is just a roadblock in learning what should be a simple tool
Most of them run in the terminal because there's a separation between the SQL server and the management gui tool. You may or may not have both of these installed on the same machine. So for a quick one off to make sure everything works the command line makes sense. You know thinking on this more you should really just check out sqlfiddle.com. it's pretty much made for what your looking for. No install... It works in your browser and let's you get down to sql business
I think you'd be more happy w/ MSSQL express if the first thing you want to do is import Excel files. It's a much more GUI friendly application than sqlite. A prebuilt binary is an .exe (in Windows)
Wow. That was actually super helpful, thanks!
You got me - I'm often stuck following the instructions but not seeing the same results on cmd line, and it's very difficult to get help and explain the problem. Lynda.com looks like a great resource! What's MAMP, in layman terms?
I wanted to do a count function in T-SQL, but the only way I knew how to do it is the way I would do it in Python which might not have even been the best way to do it in Python, but the only way I've really known how to do a count. This way is super quick. Can I ask you why this way is better than the loop that I created? EDIT: Also, I wanted to do it because I figure there has to be a better way and I just wanted to learn more. Only thing I was trying to achieve was to optimize and try to get better.
To answer the first question, there is a count() function in SQL, so you can simply do a SELECT COUNT(someField) FROM tbl WHERE {expression} The count function will not count null values of a field you specify, you can use a count(*) if that is not desired. As to why its faster to generate a recordset than running trough the loop : In the loop you insert single row after row, that will always be slower than having one insert of the complete recordset. As a general rule, in SQL you should not use loops or cursors. They have their uses sometimes, and there are no absolutes, but it is quite rare to not have the option of writing the logic set based, and having improved performance with the set based approach. For completness sake, here is 2 other ways of generating a number sequence (you would be suprised how often that is usefull) : DECLARE @min INT = 100; DECLARE @max INT = 741; WITH numberRange AS ( SELECT n = @min UNION ALL SELECT numberRange.n +1 FROM numberRange WHERE numberRange.n &lt; @max -1 ) SELECT * FROM numberRange option(MAXRECURSION 0); WITH n1 AS (SELECT n = 1 UNION ALL SELECT n = 1) ,n2 AS (SELECT n = 1 FROM n1 a,n1 b ) ,n4 AS (SELECT n = 1 FROM n2 a,n2 b) ,n8 AS (SELECT n = 1 FROM n4 a,n4 b) ,n16 AS (SELECT n = 1 FROM n8 a,n8 b) ,numberRange AS (SELECT n = ROW_NUMBER() OVER(ORDER BY (SELECT null)) FROM n16) SELECT TOP (@max - @min -1) @min + n FROM numberRange
Deleted and replaced my post. Sorry about the confusion.
Here are some helpful resources from our Wiki -&gt; http://www.reddit.com/r/SQL/wiki/index
Something like this?: SET DATEFIRST 1; SELECT * FROM Table AS t WHERE YEAR(t.Appointment) = YEAR(CURRENT_TIMESTAMP) AND DATEPART(WEEK, t.Appointment) = DATEPART(WEEK, DATEADD(WEEK, 1, CURRENT_TIMESTAMP));
-- Set first day of week to monday SET DATEFIRST 1 DECLARE @NextMonday DateTime DECLARE @EndOfNextWeek DateTime SET @NextMonday = (select dateadd(week, datediff(week, 0, getdate()), 7)) SET @EndOfNextWeek = DATEADD(day,6,@NextMonday) select Appointment from YourTable where Appointment between @NextMonday and @EndOfNextWeek 
left/right joins simply mean that you want ALL the rows from a TableA even if they don't have any matches in TableB. If there's no match, then the TableB columns will all have NULL values, otherwise it will pair just like a regular Inner Join.
If you want to get into technical tools such as databases, familiarizing yourself with the terminal is a pretty good idea. Besides, if you really want to learn SQL, you should write your SQL yourself, and not have an application do it for you (as a GUI client likely would). 
This is how I've always understood it but then some I read some crazy shit like a self-join and cannot imagine what that could possibly mean. Also there is left outer join and full join.
The temp table approach would be the best, then you can join back onto it to assure you pull through the results you need
How does SQL Server know to "modifiy" the query in the inner most step to not do a UNION ALL? AKA this bit: SELECT 1 AS num UNION ALL SELECT num + 1 FROM (SELECT 1 AS num WHERE num &lt;= 9) /* Where did the UNION ALL go? */ WHERE num &lt;= 9)
Because you've reached the termination clause (9 nestings), so the recursive part is no longer evaluated - i.e. that part becomes an empty set. Any set union all-ed with an empty set is just the original set.
left outer is the same as just saying left, there's no such thing as a left inner, and same with full outer and full. A self join would just be joining a table on itself, so if you had an employee table where the "managerID" is just the ID of another employee, you could join the table on managerID = employeeID to get the manager of that employee.
it was the join, the &amp;%^*^%&amp;%$ join. i had an eureka moment... create table #matchcrs ( [Group ID] nvarchar(255), [Group Name] nvarchar(255), [user id] nvarchar(255), crs_name nvarchar(255) ) insert into #matchcrs select [Group ID], [Group Name], [user id], crs_name from matching left join HAZGRPCRS on matching.[Group Name] = HAZGRPCRS.grp_name --naughty list WITH HazMatComplQry AS ( Select #matchcrs.[user id], #matchcrs.[group name], 'Transcript Status' = case when [Transcript Status] is null then 'NO-ACTIVITY' else [Transcript Status] END, cast(HAZTRAN.[TRANSCRIPT COMPLETION DATE] as date) as 'Transcript Completion Date' from #matchcrs left join HAZTRAN on (#matchcrs.[crs_name] = HAZTRAN.[Training Title] and #matchcrs.[user id] = HAZTRAN.[User ID] and [transcript status] = 'completed') ) ,Numbered AS (SELECT *, ROW_NUMBER() over (partition by [user id], [group name] ORDER BY CASE [TRANSCRIPT STATUS] WHEN 'COMPLETED' THEN 1 ELSE 2 END,[TRANSCRIPT COMPLETION DATE] DESC ) AS Rn FROM HazMatComplQry) SELECT *, datediff(d, Cast([Transcript Completion Date] as date), getdate()) as 'date diff', 'Cert Status' = CASE WHEN datediff(d, Cast([Transcript Completion Date] as date), getdate()) IS NULL Then 'Never Certified' WHEN datediff(d, Cast([Transcript Completion Date] as date), getdate()) &gt; 365 Then 'Expired' Else 'Compliant' END from NUMBERED where RN = '1' order by [group name],[user id] 
OS X 10.4 and up ships with SQLite preinstalled. http://superuser.com/questions/354057/does-sqlite-come-with-mac-os-x Keep in mind that's just the pure sql command line pre-installed. Since you don't want to use the command line you will still need a GUI manager for interacting with it. A quick google search takes me here... https://www.sqlitepro.com/ 
There are great explainations in this thread, but no one has done a visual one yet. Here's an image explaining SQL Joins with VENN diagrams: http://i.stack.imgur.com/GbJ7N.png
Checkout MS SQL Express, (if you are on mac, use WINE to run windows stuff): http://www.microsoft.com/en-us/server-cloud/products/sql-server-editions/sql-server-express.aspx My boss and I train our Software Engineers weekly so that they can be SQL Queries 70-461 certified and we also get to offer it free to the public. This week's training will be on CURSORS and String functions: http://www.aaronbuma.com/2015/03/tsql-cursors-and-string-functions/ This is a weekly event and if you can make it, the videos and materials will be viewable/downloadable from that same post.
`self join` is exactly what it sounds like: a join of a table with itself. Most of the good reasons to use it are complicated, so I'm going to give you a bad reason to use it, because it's straightforward to understand. Please realize that the thing I'm about to describe is stupid and there are better ways to do this. But suppose you have a table which has all your employees listed, and it has ID references from any given employee to their direct manager. You might join the employee table with itself (and you'll have to give the second instance of the table a different label so you can refer to them distinctly) so that you can use it to get the boss' names off of the IDs. create table employees(id integer, name varchar, supervisor integer); select * from employees join employees as boss_labels on boss_labels.id = employees.supervisor; A self join is for when you need to line up data in the table with other data from the same table.
&gt; in terms of relational algebra... please, stop it... "in terms of the way sql works" is a better reason also, the point about full outer join syntax is that you cannot just say OUTER JOIN -- that's invalid, you have to say FULL JOIN, because the OUTER keyword is optional
parse checks syntax before executing... to prevent you from looking like a foo when you hit execute with your buddy standing over your shoulder. Debug - has some cool utility - especially when used to check stored proc's and variable assignments. You can use the step in and out functions to see a sql script that might execute a sp to watch the values as they get passed into the SP and to see the values get assigned as they return. I like the debug feature a lot... there's other uses I'm sure but this is really the main thing I use it for - to make sure I'm returning the values I'm expecting and at which point. saves me from manually recreating the SP in linear form to run one step at a time.
Man, I swear. This is like the typical data analyst query :) I applaud you for using CTE's but the matching table needs to go. It probably is a heap (no clustered index, probably no primary key). I see data analysts do this all the time. They store data in crap tables. It's convenient when querying the subsets of your data but its not something you should put in production. --Never have to populate this table again as it's query driven CREATE VIEW dbo.Matching as SELECT [grpname] as 'Group ID', [grpname] as 'Group Name', [user id], [Action], [Division Ref], cc_code, cc_name, jc_code, jc_name, 'By Codes', [Action Date], [user status] from active_users join Mapping on ((active_users.[cost center ref] = Mapping.cc_code and active_users.[position ref] = Mapping.jc_code) and grp_src not like 'CWID%') where ([Action] = 'Add' and [user status] = 'active') UNION SELECT [grpname] as 'Group ID', [grpname] as 'Group Name', [user id], [Action], [Division Ref], cc_code, cc_name, jc_code, jc_name, 'Specified by CWID', [Action Date], [user status] from Active_Users join mapping on Mapping.grp_src = 'CWID: '+Active_Users.[User ID] where ([Action] = 'Add' and [user status] = 'active') **As far as the 250 results you aren't seeing...** I believe it lies in the CTE [transcript status] = 'completed' or [Transcript Status] IS NULL I say that because in numbering you put ORDER BY CASE [TRANSCRIPT STATUS] WHEN 'COMPLETED' THEN 1 ELSE 2 END This makes me think you want the completed one first if it exists and whatever the other status is second. Although this is very logic less (ie a student could have 2 attempts at a course and both be uncompleted from different years). You have only two transcript status you are allowing 'Completed' and NULL. Are there transcript statuses besides those two that you intend to include?
I'm not exactly understanding your data structure (if you detail it some more I could probably write the query for you), but I think what you're wanting is probably to create a subquery that gives you your values and the names of the columns they belong to, and then use [PIVOT](https://technet.microsoft.com/en-us/library/ms177410%28v=sql.105%29.aspx) to turn the column name values back into columns.
Mr. Thriven, First, thanks for your response. (i'm kinda a hack) Secondly, I used a different query to solve it. but it has CTE still. i will try to digest what you have written, but i need to switch gears to something else. this '--Never have to populate this table again as it's query driven'. i'm trying to get to a view, not there yet. the only keys i can see are [user id], group name (like five versions of this) and training title. i'll see if i can improve this later. here's my solution, i hope it doesn't offend you: WITH HazMatComplQry AS ( Select matching.[user id], matching.cc_code, matching.cc_name, matching.jc_code, matching.jc_name, matching.[Division Ref], Matching.[group name], HAZTRAN.[Training Title], 'Transcript Status' = case when [Transcript Status] is null then 'NO-ACTIVITY' else [Transcript Status] END, cast(HAZTRAN.[TRANSCRIPT COMPLETION DATE] as date) as 'Transcript Completion Date', crs_name, grp_name from matching join hazgrpcrs on matching.[Group Name] = HAZGRPCRS.grp_name left join HAZTRAN on (HAZGRPCRS.[crs_name] = HAZTRAN.[Training Title] and matching.[user id] = HAZTRAN.[User ID] and [transcript status] = 'completed') ) ,Numbered AS (SELECT *, ROW_NUMBER() over (partition by [user id], [group name] ORDER BY CASE [TRANSCRIPT STATUS] WHEN 'COMPLETED' THEN 1 ELSE 2 END,[TRANSCRIPT COMPLETION DATE] DESC ) AS Rn FROM HazMatComplQry) SELECT *, datediff(d, Cast([Transcript Completion Date] as date), getdate()) as 'date diff', 'Cert Status' = CASE WHEN datediff(d, Cast([Transcript Completion Date] as date), getdate()) IS NULL Then 'Never Certified' WHEN datediff(d, Cast([Transcript Completion Date] as date), getdate()) &gt; 365 Then 'Expired' Else 'Compliant' END from NUMBERED where RN = '1' order by [group name],[user id] edit- admission of knowledge level
[SQL Fiddle](http://sqlfiddle.com/) may be a good way to go if you are just trying to learn the query language and don't want to install software.
Both should be using this (IMO). As a developer you need to make sure you're getting results you would expect in order for your application to be functioning properly right? sysadmin's would use it if they find problems and want to isolate it themselves before sending it back to development.
the app doesnt allow for this type of reporting
Export text files (ie csv, delimited txt) or do you have access to the tables of the source system?
just text, no DB access.
Yeah, it could be poor data structure, a lot of these systems/databases are very old and even as stuff is getting updated the data structure hasn't changed much from my understanding (this is a new job for me, been here for ~2 months). I can't post an actual sample as it's sensitive data about employees, but if I query for a single employee here is an obfuscated version of what I see from my testing: FIRST_NAME | LAST_NAME | JOB_DESC | co_cd | ID | DEPT_NAME | STEP | NAMEOFINVESTMENT | ENTITYNAME | AMOUNT | TYPEOFINVESTMENT | COMPLIANCEDECISION | DECISIONDATE | RECORDSTATUS | PURPOSE_NATURE | INTRODUCED_BY | Q10A | Q10B | Q10C | Q10D | Q10E | Q10F | Q10G | Q10H | Q10I | Q10J | Q10K | Q10L | Q10M | COMMENTS --------- | --------- | --------- | --------- | --------- | --------- | --------- | --------- | --------- | --------- | --------- | --------- | --------- | --------- | --------- | --------- | --------- | --------- | --------- | --------- | --------- | --------- | --------- | --------- | --------- | --------- | --------- | --------- | --------- | --------- | archaic | frost | job | ops | id# | dept | 1 | Step1-nameofinvest | Step1-Entity | 1 | Step1-typeinvest | | | | Step1 - description of transaction | Step1 - how introduced | No | No | No | No | No | No | No | No | No | No | No | No | n/a archaic | frost | job | ops | id# | dept | 2 | STEP2-NAMEINVEST | STEP2-NAME | 2 | STEP2-TYPEINVEST | | | | 2-INVESTDESCRIPTION | 12-EXPLAININTRODUCED | No | No | No | No | No | No | No | n/a | n/a | n/a | n/a | n/a | n/a archaic | frost | job | ops | id# | dept | 3 | STEP3-NAMEINVEST | STEP3-ENTITY| 3 | STEP3-TYPEINVEST | | | | 3-PURPOSENATURE | 3-WHOINTRODUCED | No | No | No | No | No | No | No | No | No | No | No | No | Yes Some of the column names are already aliased in this example, but it should matter not. In this case it returns 3 records for the same individual, one for each completed step. The n/a fields mean that question does not exist for that Step. As an example for Step 1, Q10G the underlying question is: "do you have a financial interest in the private security" while Step 2, Q10G is: "will you be providing any services or other business activities" and the Step 3, Q10G is: "have you participated in the solicitation of the private security" and to do what they want I believe it needs to be something like: WHEN the STEP flag is 1, then display Q10G AS "FinancialInterest" WHEN the STEP flag is 2, then display Q10G AS "Services" WHEN the STEP flag is 3, then display Q10G AS "Solicitation" so they have a keyword from the question and can determine which is which. *edit: there are 30 columns and apparently can't view them all on here, trying to figure something else out.
Thank you very much for the explanation. I understand it a little bit more. The other two new examples - not so much yet, but will look into them more. Thanks again.
Your tables design is poor, but it appears not to be your fault, that's the problem with legacy systems. One approach would be to create a table to store the descriptions for the question / step combinations. STEP_NO int not null (PK) QUESTION_NO int not null (PK) DESCRIPTION varchar(100) not null Populate this table and then join to it by manipulating the existing tables into a STEP_NO and QUESTION_NO. 
That does sound like what I'm looking to do. I just updated my main post with some sample data (though it's too wide for Reddit, trying to figure out a way to solve this.) I'll try to obfuscate my query and post that as well.
Well I have a better understanding of what you are asking and yeah its poor design. You can't do WHEN the STEP flag is 1, then display Q10G AS "FinancialInterest" WHEN the STEP flag is 2, then display Q10G AS "Services" WHEN the STEP flag is 3, then display Q10G AS "Solicitation" How would that honestly work? Try to imagine you are returning the result set you posted. Q10G would display as "Services" when on the following row it should be "solicitation". You can create a question category column. So your result set would be: Q10G Q10G_Catagory No FinancialInterest You could do this easily in two ways. First easy way, follow the crappy table design and create a seperate table called Question_Catagories CREATE TABLE [dbo].[QUESTION_CATAGORIES] ( [STEP_ID] [int] IDENTITY(1,1) NOT NULL, [Q10A_Catagory] NVARCHAR(100) NULL, [Q10B_Catagory] NVARCHAR(100) NOT NULL, [Q10C_Catagory] NVARCHAR(100) NOT NULL, [Q10D_Catagory] NVARCHAR(100) NOT NULL, [Q10E_Catagory] NVARCHAR(100) NOT NULL, [Q10F_Catagory] NVARCHAR(100) NOT NULL, [Q10G_Catagory] NVARCHAR(100) NOT NULL, [Q10H_Catagory] NVARCHAR(100) NOT NULL, [Q10I_Catagory] NVARCHAR(100) NOT NULL, [Q10J_Catagory] NVARCHAR(100) NOT NULL, [Q10K_Catagory] NVARCHAR(100) NOT NULL, [Q10L_Catagory] NVARCHAR(100) NOT NULL, [Q10M_Catagory] NVARCHAR(100) NOT NULL, CONSTRAINT [PK_Step] PRIMARY KEY CLUSTERED ( [STEP_ID] ASC )WITH (PAD_INDEX = OFF, STATISTICS_NORECOMPUTE = OFF, IGNORE_DUP_KEY = OFF, ALLOW_ROW_LOCKS = ON, ALLOW_PAGE_LOCKS = ON, FILLFACTOR = 90) ON [PRIMARY] ) ON [PRIMARY] Populate the table with all the catagories for each question. Then on your select statement join question_categories. SELECT FIRST_NAME, LAST_NAME, JOB_DESC, co_cd, ID, DEPT_NAME, STEP, NAMEOFINVESTMENT, ENTITYNAME, AMOUNT, TYPEOFINVESTMENT, COMPLIANCEDECISION, DECISIONDATE, RECORDSTATUS, PURPOSE_NATURE, INTRODUCED_BY, Q10A, Q10A_Catagory, Q10B, Q10B_Catagory Q10C, Q10C_Catagory Q10D, Q10D_Catagory Q10E, Q10E_Catagory Q10F, Q10F_Catagory Q10G, Q10G_Catagory Q10H, Q10H_Catagory Q10I, Q10I_Catagory Q10J, Q10J_Catagory Q10K, Q10K_Catagory Q10L, Q10L_Catagory Q10M, Q10M_Catagory COMMENTS From dbo.Answers A LEFT JOIN dbo.QUESTION_CATAGORIES QC ON A.STEP = QC.STEP_ID The other way, is basically unpivot the original answers (honestly, the answers table is cringe worthy). You can keep the source system untouched but create a DML operation to extract the questions out of the source system and put them into a normalized table. Create an employees table or utilize an existing employees table and do a name lookup. Create answers table with: Employee Key, Step, Question Name (Q10A), Question Result Your primary key would be (Employee Key, Step, Question Name) Then create a question category table: Step, QuestionName, QuestionCatagory. Primary key would be Step, QuestionName. Then you join from answers to question catagory ON answers.step = question_catagories.step AND answers.questionName = question_catagories.questionName. **Why these two options?** You don't hardcode values in your sql case statements. You need to put your data in tables. Ultimately, when you create a very complex CASE Statement, SQL will many times optimize it by creating a hash table from that case statement. SQL will literally do exactly what I'm telling you to do. It's far less work to just build the table and join it to your source. Plus when you revisit the code or someone comes behind you, its easier for them to understand.
So you'd do something like SELECT First_Name, Last_Name, FinancialInterest, Services, Solicitation FROM (SELECT * , ColName = CASE STEP WHEN 1 THEN 'FinancialInterest' WHEN 2 THEN 'Services' WHEN 3 THEN 'Solicitation' END , Val = CASE WHEN Q10G = '1' THEN 'Yes' WHEN Q10G = '0' THEN 'No' WHEN Q10G IS NULL THEN 'n/a' END FROM table) src PIVOT (MAX(Val) FOR ColName IN (FinancialInterest, Services, Solicitation)) pvt Is that what you were wanting to do?
Thanks a lot, I appreciate the thorough answer, this really helped me understand the problem better. I'm not sure what I'm allowed to do in terms of creating new tables, so I'll talk to my boss and go from there.
thanks! i cant automate the txt export, but i do a scheduled job to bulk insert and then have the user open a xcl file with a macro for the data connection to the view and formatting (sorting/pivot) ill take a look at ETL- thanks for giving me some ideas! 
Thanks, I'll try that out.
That's true, you can insert into a temp table or table variable and then left join back to the original. The CTE solution works just fine as well, I always try to avoid CTE's when possible. I worked at a place that forbid them based on the idea that temp/variable tables were generally faster, so they completely ignored CTE's. I guess I was initially biased against the CTE solution based on that experience.
Access does not recognize the SET command. 
&gt;It's too wide You probably don't need all 30 columns to get the concept across. Pare down your example until it's as small as it can be whilst still doing the thing you want it to do. 9 sections need only be 2, etc. etc..
This worked in another query, but I could not run it within Access. Is there a way to convert this to something the MS Access can run?
 select a.* from ( select RECORD_SET, AFTER_STATE, lead(BEFORE_STATE) over (partition by RECORD_SET order by BEFORE_ACTIVITY) as LEAD_STATE from MYTABLE) a where AFTER_STATE &lt;&gt; LEAD_STATE Note : Will ignore records which have no "next record"
Take a look at : http://www.vertabelo.com/blog/notes-from-the-lab/18-best-online-resources-for-learning-sql-and-database I guess w3school is the most popular (it has nice tutorial and possibility to test queires online). Some of my friends also recommended sql tutorials on http://www.tutorialspoint.com/sql/. If Postgres is two hard for you try mysql or sqlite. If you want to learn only the basic syntax, test queries you can always read documentation and test it in http://sqlfiddle.com/.
No Joke but this reminds me that thinking about joins is actually nauseating. Maybe he was working on a complex query when the overwhelming effect struck.
This also may help: http://www.vertabelo.com/blog/notes-from-the-lab/18-best-online-resources-for-learning-sql-and-database
There is a link on the sidebar for how to lean SQL. Honestly, as a language, learning the basics won't take you very long. I'm not sure how much there is to specialize in from company to company. Just dig in and you'll be ahead by the time you start!
The op + this comment sounds remarkably like a conversation I had trying to find the government employee responsible for something :P
Amazon has public data sets you can download and play with. https://aws.amazon.com/datasets
SELECT SUBSTRING(ColumnName, 13, 17) as serials FROM Table t
Something likes this ? This will look for the position of the first # and then the next #, after that he will look for the last # and takes anything in between. declare @varchar varchar(50) = 'pa1234rt1#pa1234rt2#pa1234rt3#part4#part5#pa2134rt6' select SUBSTRING( @varchar -- string , (charindex( '#',@varchar) + charindex( '#',right(@varchar,len(@varchar)-len(left(@varchar,CHARINDEX('#',@varchar)))))) + 1 -- find the first # , len(@varchar) - CHARINDEX('#',reverse(@varchar)) - (charindex( '#',@varchar) + charindex( '#',right(@varchar,len(@varchar)-len(left(@varchar,CHARINDEX('#',@varchar)))))) -- take total length of string remove the lenght till the first # and last # ) 
It was a great suggestion and got me part of the way there, enough to realize that what they're wanting just may not be feasible in SQL alone given the complexity (and poor data structure) of the database. I have a meeting with the front end business logic folks who generate the reports this afternoon to discuss how they can manipulate the data from the query (which I think they should be able to do using flags in the dataset), and if that doesn't work then I get to go back to the requester and let them know we can give them the data they want in 3 reports, but not 1! Thanks a lot though, that did help me approach the problem a bit more effectively!
I set up a meeting with the front end business logic folks today to discuss this further, in fact. After talking to the more senior members on my team they said those folks should be able to use data points as flags to break out the data on the reports, so getting more details on that.
Sorry for the late answer. I think this is want I wanted, I will test it today or tomorrow. I really need to start reading a book about SQL. I am a bit ashamed that I still don't grasp well the basics of joins. Thank you for your help.
As someone who is perusing this subreddit to get a feel for the limitations / capabilities of SQL, I'm a little upset that, judging by the answers here, SQL doesn't seem to natively support regex.
You can upgrade fairly easily: * Load the installation / setup for SQL Server * Click on **Maintenance** on the left * Select **Edition Upgrade** * Enter the new product key * Follow instructions / etc Enterprise is generally more spendy than most companies can swallow without good reason; some questions you might be want to ask first: * What does your "SharePoint guy" need data-driven reporting subscription for? * Can you change the delivery method to avoid this specific feature? (see: Workaround) 
Thank you so much this worked perfectly!! Can you give me a quick explanation of the lead (BEFORE_STATE) and (partition by RECORD_SET order by BEFORE_ACTIVITY)?
Looking at the pricing I can see it is crazy expensive. I will try and get more information on what his plan is. He is a self taught Sharepoint guy and I know very little about the software and I don't step on his toes.
Is your SharePoint guy wanting to host SSRS reports(which allow subscriptions) within his SharePoint? Keep in mind that subscription based reporting can be very taxing on your server depending on how the report is built and the types of query &amp; rows that are created. If this is a shared environment, it could impact other DBs or SQL Instances on your server.
A superficial solution might be a double reverse: Extract suffix -&gt; reverse -&gt; extract suffix -&gt; reverse A larger question is why you're storing 5 pieces of information in one blob field. I don't know anything about the application, but it seems like you should have 5 individual fields + a calculated 6th with the concatenation if you need it. If you're on ms SQL, you could wrote a function in c# like extract_parts(str, seperator, start_seg, end_seg), but I know mixing in CLR stuff at the drop of the hat is frowned on, and potentially impossible depending on your server config. (You could also add one of the standard regex/string processing assemblies out there for the same basic effect.)
So we have a smaller environment. One SQL and one Sharepoint server. It is hosted but virtualized and we can throw additional resources at it if needed. This is what he wrote up he is trying to do. I appreciate all the responses since this is new to me. I’ve been working with SSRS subscription reports lately (allows you to email portal reports). One of the problems is with our standard license, you can only hard code the recipients to a subscription. There’s a more efficient feature called Data Driven Subscriptions where you can use logic to send the report to different recipients (1 subscription with different data to many recipients). From what I’ve read, it’s only available in enterprise and BI editions of SQL.
Not every purchase_date is the same day but there are multiple purchases happening on any given day by different customer_ids. I wish I had some more granular data to use to filter people but the best I've come up with is massaging the data in Excel to have a cohort date (all dates in a given month are rounded down to the first of the month) and a first_purchase date.
I've detached, moved mdfs and ldfs, and attached them to a new server without much trouble. I'd start with that. There's always the dirty solution of dumping all the data to text files. Then you can use powershell to export all your table creation and security rights scripts. This solution sucks but it at least would probably get as much data as possible from the failed drives if it's really that bad 
I will try that during downtime. This particular server needs uptime from 3:00 AM till 11:00 PM, but arrangements can be made. Also, the last backup file gives an error of when the automated backup attempts to start: The backup data at the end of "PATH" is incorrectly formatted. Backup sets on the media might be damaged and unusable. To determine the backup sets on the media, use RESTORE HEADERONLY. To determine the usability of the backup sets, run RESTORE VERIFYONLY. If all of the backup sets are incomplete, reformat the media using BACKUP WITH FORMAT, which destroys all the backup sets. This last full backup is right in line for the filesize and growth, so I am transfering it to the other server right now, and I will try and RESTORE VERIFYONLY to see if the backup is good. I am just scared of excess disk I/O at this time.
Well I knew what I was trying to accomplish and found similar code from googling and used that. I'm trying to understand it now even though it does work. Your answer has really helped me understand this stuff further so thanks. In terms of the code you provided when I try that it returns duplicate values. For example if I had a project ID of 1010 with 2 dates it would return both dates rather than just the most recent. I was originally using very similar code to what you suggested but needed only the most recent values. Can you confirm that I am understanding the logic? So the code first does the Inner Join and joins all the most recent data to table c? Then it would make sure the last 2 statements are correct and only having those records in the table? I think I'm having the biggest issue with the INNER JOIN. Thanks for the link as well you have helped me understand what's going on a bit more. I'm really new to SQL which is why I'm asking such an ambiguous question.
The most difficult part isn't going to be the syntax its going to be learning which tables have what data. It may get a little hate here (or not, not sure) but if you have worked with Access and understand how that works, you can actually view the SQL for your queries in there. It may give you a basic idea of how the syntax works with a visual aid as well. 
Fair enough. Be aware that you may be trying to backup corrupted data. Well, it looks like you are backing up to the same file and it contains multiple backups. You might try changing the destination and see here that gets you.
Which DB?! 
 select (a.HI_COUNT / b.ALL_COUNT) * 100 as HI_PERCENT from (select count(WRITING) HI_COUNT from TABLE1 where WRITING like 'HI%') a join (select count(*) ALL_COUNT from TABLE1) b on 1 = 1
Yes I am using Log Parser 2.2. But I reviewed some Extract_Token code and I do not think it will work because Part1#Part2#Part3#Part4#Part5#Part6 is the data in one column (i.e. ajdsfsdjf#you838sjjs#nnnn0393js#Jjsoe3sfjdsfkdfjk#OlA34#7oh)
 select EXTRACT_PREFIX(EXTRACT_SUFFIX(Strings, 3, '#'), 2, '#') as serials from TABLE01
Is there a reason I should know how to do it all 3 ways? I assume one is going to be more efficient than the other? Or is it just one of those things to show there are multiple ways to achieve the same goal type of things? Thanks for all your help. I am actually learning quite a bit from your examples/explanations. Any good resources that you know of where you've learned your knowledge from? Edit: I noticed in your first response you use the sys.objects. What exactly are the sys.objects 02/03 doing? Wouldn't just the first one suffice?
I rewrote your query with a common table expression (usually referred to as a CTE). It's functionally identical to your query, it's just a different way of organizing it that can be very helpful for understanding complex queries (and also lets you do some things you couldn't with a nested query). It looks like this (I also reformatted a little): ;WITH c2_CTE AS ( SELECT ProjID, MAX(ReportDate) AS Latest FROM QryFTEFYCalculation GROUP BY ProjID ) SELECT c.ProjID, c.ReportDate, c.MaxOfNumberOfFTE FROM QryFTEFYCalculation c INNER JOIN c2_CTE c2 ON c2.Latest = c.ReportDate AND c2.ProjID = c.ProjID Maybe now it's a little easier to understand. The first thing you're doing is finding the max `ReportDate` for every project. Then for every project you found, you want to find the corresponding `MaxNumberOfFTE` value for that most recent `ReportDate`. An `INNER JOIN` only takes rows where the join condition is satisfied so you will only get rows from the table that have the same `ProjID` and `ReportDate` as the one's you found in your CTE. As far as how this gets executed, that's up to the query optimizer that will look at a bunch of different factors, key among them, what indexes are available. If there's no index on `ProjID` or `ReportDate` then you're pretty much stuck looping through every row in the table. Ideally you'd have an index with both those values with `ProjID` ranked first and `ReportDate` second. If you had the `MaxOfNumberOfFTE` column as an included column the whole query could be done with a single index seek and would be very fast. One thing that's interesting about the CTE, if you had some `WHERE` condition in your main `SELECT` like `WHERE c.SomeColumn = 3`, the optimizer might decide it's more efficient to figure out what rows from the table satisfy the `WHERE` then only look at the rows from CTE that would end up being joined. In other words, it won't necessarily execute in the order you wrote it out. 
&gt; It's likely WRITING will be indexed My assumption would have been the exact opposite, I'd assume a column named "writing" would be a large VARCHAR or BLOB/TEXT field. I also assumed the '%HI' would likely be changed to whatever the OP is actually going to be looking for percentages on (i.e. '%WHATEVER%') which would make an index worthless as well. &gt; count(*) for all records on a table on most DBs uses a high water mark Source? Which DB engines are you referring to? Something like MyISAM would be able to use the data stored with the table, but something like InnoDB would have to perform a full table scan or index range scan (and, it's been a while, but iirc Oracle operates in the same way).
He is using a suffixed wildcard, which can use an index, but your solution can't **EVER** take advantage of that. Mine can and would. An index range scan count is fast. I work on production DBs with some tables containing 100s of millions of records. If I run a query like yours, I may as well go home for the week. My query will outperform yours on pretty much any situation, even on a few thousand records. If you don't believe me try it. Almost all enterprise DBs including Oracle store the full table count internally. How do you think it takes 56ms to return 623 million count on one of my tables? 
Does... SELECT * FROM suspect_pages ...return anything? If it doesn't it might be possible for you to just backup the database and move it. 
First, you are right - my query has a mistake. Only group by project ID, not including the other thing. This is why we should test stuff before we suggest stuff =) SELECT ProjID, MaxOfNumberOfFTE, MAX(ReportDate) AS Latest FROM QryFTEFYCalculation GROUP BY ProjID; (Tested in Mysql). ----- So, to answer your question, joins work as manipulating data sets in memory. I'm not sure that helps you understand, so let me elaborate a bit. You probably remember data sets from algebra class. Venn diagrams and the like. [Let's look at this data set.](http://www.gliffy.com/_ui/images/examples/example_venn_pirateNinjaZombie_large.png) The most common join is the INNER join, which in this case would look like this: select n.name from NINJAS n INNER JOIN PIRATES p ON p.name=n.name INNER JOIN ZOMBIES z on z.name=n.name; The result in the IRS. So that is how joins work - by intersection/union. MORE: http://blog.codinghorror.com/a-visual-explanation-of-sql-joins/ 
I'd very much be interested in the results (on different DB engines as well), and I'm not even remotely trying to be snarky when I say that. I was hoping to setup and randomly populate test tables on several DB engines myself to demonstrate the point, but I haven't had a chance to sit down and do so. That said, /u/ziptime isn't wrong that the casting is adding unnecessary work to this (although possibly a trivial amount since the IO costs are going to far exceed the CPU costs), but that aspect of the original query being discussed could easily be pulled out of it. The real crux of this discussion is whether or not joining the same table to itself (to get the records matching a WHERE clause and another to get a COUNT(*)) versus a SUM(CASE WHEN [x] THEN 1 ELSE 0 END) / COUNT(*)) performs better (and how that result varies when the column being filtered is indexed or not). I'm more than willing to accept being incorrect on this, but I think the optimal solution will depend both on the DB engine, the nature of the column in question (and whether or not it's indexed AND whether that index is actually usable). I'm also fairly comfortable assuming that the overhead for a simple CASE statement like this on a full table scan versus having the filtering condition in a WHERE clause on a full table scan will result in very similar performance, particularly for reasonable sized data sets.
You are going about it all wrong from a data standpoint. Creating different tables for each Playlist is inefficient, will require more dynamic coding in your application to interact with the database, and exposes security risks (by allowing your app to create db objects you run the risk of exposing other db actions through your client API). I would do something like: * Party: Table that defines the party. Who created it, other high level meta-data your app needs to function * PartyMembers: List of all party members, includes at least party creator and then anyone added. Many-to-one relationship to Party. * PartySongs: List of the songs included in the party. You could also have the Ratings here as well for voting, but for normalization purposes I'd create a new table for the ratings. In other words, don't think of your Playlists as **database tables**. Think of them as individual records within defined tables.
I'm on holiday at the moment for a week, so can't check on the DBs I have access to at work. I did a simple test on a test DB I have at home (DB2) and the queries gave similar matching performance times for a few 10,000 records. With an index on the WRITING column (200 chars) mine was slightly faster, but I was surprised how little there was in it (1ms difference). DBs are damn fast at doing this sort of thing! Without an index on WRITING or any index on the table, /u/Kthanid query was slightly faster. I think /u/Kthanid might have a point about count(*) on some DBs, I think some do a full table scan, but if it is an index only table or has a usable index then the DB can ignore the table altogether and go with an index full scan, which in most cases will be much faster than a full table scan (more blocks, more I/O). I like my approach as it has the potential to use an index if it can. I have written /u/Kthanid type queries though, many a time (one table scan approach), so perhaps I should be less dogmatic and say there are two ways to skin a cat. 
Best book I've used so far is SQL Database for Beginners by Martin Holzke. It helps you download MySQL, Wampserver, and provides an in interactive overview of terms, querying, database structure, all using clear instructions .
Honestly... If you are trying to create an application you need to take like 10 steps back. It sounds like you don't even understand basic programming. Making an app will only make all your users vulnerable to attacks if you don't know what you are doing. Perhaps you should learn basic programming and then move into app development. I know it sounds harsh... But it's reality and the truth. 
Yes, this. You may have issues with bloat, however. The advantage of a on-the-fly table is that you can dump it easily. There are also performance advantages, perhaps, if you have multiple tables to deal with instead of one. *However*, you're violating some rather basic DB Arch rules by creating tables on the fly like you are. Another question is, why SQL? What's a database doing for you here?
I disagree. The best way to learn how to code is to through yourself fully into a problem. "Making an app will only make all your users vulnerable to attacks..." is rather alarmist. Let him get his app together and working before you knock his legs out from underneath him. Realistically, will it see the light of day? Probably not. Most of our pet projects don't. But, it's more about the journey than the arrival.
I agree. I am a novice developer with intermediate Python experience and for a yearlong class project I decided to do a rather daunting iOS application that uses an external API. Although it's been pretty difficult, I've learned infinitely times faster than I have before by just following a textbook/course. 
OP doesn't necessarily need help with *programming*, but rather the right way to work with a database.
That solution is over complicated and using an antiquated SQL standard. A better version would be: *** SELECT DISTINCT s.sid, s.sname, s.address, p.pid, p.pname, c.cost FROM Suppliers AS s INNER JOIN Catalog AS c ON s.sid = c.sid INNER JOIN Parts AS p ON c.pid = p.pid *** You can think of it like a Venn diagram with two circles, Suppliers and Parts, where they intersect is the Catalog. The above query would give you the purple in this [Picture](http://i.imgur.com/1kSoqT1.png).
* What DBMS? (See right) * Are you sure it's not just a setting your querying program? * If there is actually multiple rows corresponding to a single message, you will need to GROUP BY
Sorry, forgot to mention (I'm new here!) - using MS SQL. Yeah, they're definitely on multiple lines. It's like an email body in the results, so when you copy it out or return it as text, it shows on multiple lines. It's not multiple rows, it's multiple lines in one field... 
But how would that give you the suppliers who supply every part? Isn't that just the Catalog area?
Try this : select replace(replace(AMESSAGE, char(13), ''), char(10), '')
If you have a 2,000GB database, and 1,800 of that is historical, ie: never-if-rarely gets updated, logically you would: * Separate those 2TB's into X file groups * Backup filegroups A1-Y1 filegroups * Continue backing up Z filegroup daily * Once Z reaches a certain Size or date, you archive it (however you decided to create your partitions) * Backup Z * Stop daily backups of Z * Now files are writing to filegroup A2, you backup A2 Optionally choose to do a full backup weekly, etc. Your full database backup is going to be a lot smaller, and your transactional restores a lot easier--assuming you do not do many point in time recoveries.
Yes, the INNER JOIN was attempting to show you how to get additional information out of a RDBMS. Primary keys like **sid** or **pid** are generally useless outside a specific inventory system or for reporting purposes. For example: * The Bolt Supply House (Supplier #7) provides two inch self tapping screws (Part #17) Makes much more sense than: * Supplier #7 provides Part #17 Databases are set based, in my picture I was trying to demonstrate how to look at the overall picture instead of a limited scope: * The blue are parts that do not have suppliers (where a match can't be found in the Catalog table) * The red are suppliers who don't provide any parts (where a match can't be found in the Catalog table) * The purple is the set of suppliers who produce what parts. (where a match can be found in the Catalog table for both suppliers What you were doing in the original post was not efficient and exceptionally difficult to filter. Maintenance would also be difficult as you'd have to add another EXIST or NOT EXIST for every additional criteria. For example, if you only wanted suppliers for two inch self tapping screws: * Original method: *** SELECT C.sid FROM Catalog C WHERE pid NOT EXISTS (SELECT P.pid FROM Parts P WHERE sid NOT EXISTS (SELECT C1.sid FROM Catalog C1 WHERE C1.sid = C.sid AND C1.pid = P.pid ) ) AND sid EXIST (SELECT c.sid FROM Catalog WHERE pid NOT EXIST (SELECT pid FROM part WHERE pname = 'two inch self tapping screws' ) ) *** * Regular join method: *** SELECT s.sid FROM Suppliers AS s INNER JOIN Catalog AS c ON s.sid = c.sid INNER JOIN Parts AS p ON c.pid = p.pid WHERE p.pname = 'two inch self tapping screws' 
That did it - see u/ziptime 's SQL. It did the trick! Thanks everyone
&gt; you need to understand "suppliers who supply every part" better Well duh. That's why I was asking him to explain in more detail.
also watch your spacing and punctuation.
yeah, trying to get the formatting right messed things up a bit
Unrelated to your specific question (which has already been answered by /u/r3pr0b8): Not sure if this is your table or an example you were provided, but generally speaking you'd want your customers in a separate table from the purchases. A more normalized architecture like this will benefit you in the long run, even if it's not immediately obvious to you at this moment. Your customers' should each have a unique identifier (an id primary key field is fine) so that they aren't continually duplicated (and so two customers can have the same name but be different actual people). The purchases table should list the sales being made (and likely you'll have a third table with the details of the items being purchased). In this setup, your query would end up looking something like the following: SELECT tC.customerId, tC.customerFirstName, tC.customerLastName, tI.itemId, tI.itemName, COUNT(tP.purchaseId) purchaseCount FROM tCustomer tC LEFT OUTER JOIN tPurchase tP ON tP.customerId = tC.customerId LEFT OUTER JOIN tItem tI ON tI.itemId = tP.itemId WHERE 1 = 1 -- Filter here as needed by tP.purchaseDate, tI.itemId, etc. GROUP BY tC.customerId, tC.customerFirstName, tC.customerLastName, tI.itemId, tI.itemName HAVING COUNT(tP.purchaseId) &gt; 3 ; 
Yes, the database was just a (purposely poorly made) example given for an assignment! It would be much better organized in separate tables.
So many times I have been asked for help with a query, where the questoin really comes down to the understanding of the difference between INNER and LEFT or RIGHT JOINs. I created this poster a few years ago and I keep it posted on the wall at the office. This way when I am trying to explain JOIN types, I just refer to the poster. I have created the poster below to help describe JOIN types in SQL Server. This had lead to lots of confusion over time, and this is the best way that I have seen to describe them.
A couple of reasons... 1. Clients have execute permissions on the SPs but no read or write on the underlying tables. Good way to lock down the db. 2. Much easier to add business logic or change schema later without changing the app. 
3) Someone came up with an "all database access must be through SProcs" edict a dozen years ago and no one has questioned or reassessed it since.
&gt; I want it to change the DB context of the actual query window I'm running the query in. As if i had just run "use databasename" in there. It sounds like you might want to think about using the DOT NAME to execute the queries. exec [DatabaseName].[dbo].[usp_StoredProcedure] or SELECT * FROM [DatabaseName].[dbo].[TableName] And finally DECLARE @db VARCHAR(20) = 'DatabaseName', @sql VARCHAR(100) exec sp_sqlexec('SELECT * FROM ' + @DatabaseName + '.[dbo].[TableName]') Further reference: https://technet.microsoft.com/en-us/library/ms175170(v=sql.105).aspx &gt; If the executed string contains a USE statement that changes the database context, the change to the database context only lasts until sp_executesql or the EXECUTE statement has finished running. /*Show not having access to variables from the calling batch. */ DECLARE @CharVariable CHAR(3); SET @CharVariable = 'abc'; /* sp_executesql fails because @CharVariable has gone out of scope. */ EXECUTE sp_executesql N'PRINT @CharVariable'; GO /* Show database context resetting after sp_executesql finishes. */ USE master; GO EXECUTE sp_executesql N'USE AdventureWorks2008R2;' GO /* This statement fails because the database context has now returned to master. */ SELECT * FROM Sales.Store; GO
/u/zbignew describes a situation I worked with. We had an Access/SQL solution that had forms that didn't use recordsets (due to the complexity of what it was doing). Instead, each element called a stored procedure to retrieve its value and another to write any changes that were keyed in (if that cell was allowed to). The writing sp only wrote one line at a time. The app could also take data from specifically formatted spreadsheets, and it to would rely on an sp that wrote one line at a time, but the sp also did data integrity checks and would return either an error message if the data element was malformed, or it would return the value that was replaced... allowing for kind of an "undo". Also, an sp can be called with parameters so that you don't have to worry about sql injection.
Just to provide a counterpoint to some of what you've said: - points one and three can be done just as effectively with triggers. Instead-of trigger are particularly powerful this regard - point two: stored procs arguably make schema changes more difficult as there are now additional places in the database and the front end that need to be changed - point four: very true. Much easier to find database objects in your Sql code than in .net or Java
I don't think you can go by code lines in SQL like OOP. In SQL your line count gets bloated because of columns in statements could take one line themselves depending on your formatting. I usually use the following as a rule of thumb: If a portion of code is reusable or will be in the future it should be split out. The chances that code is reusable grows as the stored proc grows. Another problem is when using set based logic it is difficult to pass sets between methods. I'd rather a proc be lengthy than do RBAR passes to another proc.
&gt; I understand from a OOP pattern perspective that if a method or function is longer than X number of lines, then it should be broken up into simpler components. This isn't an OOP pattern, but a general programming pattern (people have made this argument since the days when everything was written in C). But databases work **very** differently from "traditional" programming languages/environments. LOC in SQL does **not** translate to execution time or complexity; something that's "longer" may actually execute *faster* because the engine can optimize the query better. &gt;Is this a valid pattern with SQL stored procs or SQL in general? IMHO, no. I would only break a portion of a SProc into another SProc if that functionality will be used elsewhere. If you have a lot of complex logic going on, you may need to refactor significantly and look into using a "rules" or lookup table to join against instead of doing `IF` and `CASE` statements. Or even step outside SQL completely and look at tools like SSIS (if you're using SQL Server) or rule/decision engines. Anecdote: We have a system that's built the way you describe. SP on top of SP on top of SP, UDFs calling UDFs, etc. Debugging **anything** takes a minimum of an hour because you have to unwind all the spaghetti.
Yum!
You'll find bad things like this all the time--it's only good if you're really only inserting one record at a time (and like 10 records a day max).
I'm totally stealing this quote!
Interesting way to put it.
All credit to SQL Antipatterns by Bill Karwin :)
Plan caching. Maybe not that great a benefit for a single stored proc, but in later versions of the app where additional logic needs to be added, it would help. Also, when strange things start showing up in the database instead of finding the 20 places where inserts happen, you have one place where you can add any needed data validation.
You get plan caching with Prepared Statements too, at least with SQL Server. Don't ever underestimate the power of people doing things **only** because "well, we've always done it this way" - regardless of whether it's the right thing to do or not. I am still seeing **new** code delivered with `NOLOCK` hints where they're both unnecessary and potentially harmful because the developer is stuck in a 2001 mindset.
Yes, that correct
You need to make the group by the same as the select SELECT case when IssueAge between 18 and 35 then '18-35' when IssueAge between 36 and 45 then '36-45' when IssueAge between 46 and 55 then '46-55' when IssueAge between 56 and 64 then '56-64' when IssueAge between 65 and 69 then '65-69' else 'unknown' end age, count (*) FROM [Marketing_Extract_WorkSpcs].[RESULTS].[Apps] where keycode = '15JD005T1A' Group by case when IssueAge between 18 and 35 then '18-35' when IssueAge between 36 and 45 then '36-45' when IssueAge between 46 and 55 then '46-55' when IssueAge between 56 and 64 then '56-64' when IssueAge between 65 and 69 then '65-69' else 'unknown' end Otherwise you're trying to group something less granular than what's in your select statement.
True, but if it's done 20 different ways in 20 different places then you have 20 different cached plans. The other thing is if the SQL is embedded in the app, then the cached plan depends on the luck of the draw as to what parameter was passed on the first call and whether the plan for that value is good for the general case. With a stored proc you can at least add optimize for unknown. Agree with the NOLOCK thing, but if it's in a stored proc at least it can be found and addressed by the DBA/SQL Developer.
Tldr: python string formatting using handlebars for substitutions by name instead of arg index. List of query strings can be passed in to a query function which inserts "union all" between them to return one set.
Hi Steve. I have watched most of your videos and hope you continue to upload new ones. The level of detail, context and speed are ideal for learning. To all of you reading this I would encourage you to check them out as well! 
What /u/r3pr0b8 said about the constraint. &gt; Also, does 'NOT NULL' need a definite constraint statement? Nope. Just like human_id numeric(10) not null,
Nice Ad!
Isn't this just an indication that your schema is incorrect?
Oops. I forgot to add the distinct. Updated. CTE's are nice but I like writing everything out as a series of temp tables first, and then if necessary putting it all into a CTE. Makes it easier to document / update sections as time progresses. The union creates all possible conditions, so by selecting from it all pieces of data are represented.
The difference between UNION and UNION ALL is that UNION performs a distinct over the 2 datasets. You dont need to declare distinct.
I added it in with the edit because I don't have the code in front of me. Not sure if I did or didn't use it, but you're correct. However, it's harmless to include it and as a force of habit I tend to throw in distincts like a priest tosses out blessings. 
The derived table route's a good way to go - that way, if you need to update your case statement (for example), you don't need to change it twice (once in the retrieval, once in the group by statement).
&gt; a UNION is already selecting only distinct pairs sorry, i'm not sure i understand your comment about "distinct pairs" the distinctness of UNION result is accomplished as follows -- first, it retrieves all the rows of all the SELECTs, and then it sorts these query results, *sorting on all columns*, and then compares consecutive rows of the sorted results 
Create a view or as access calls them, a saved query. 
Return Case when number &lt; 0 then 0 else number end
Refer you to my first comment. You don't need to convert them, dump files are valid input to MySQL command line. When you look at the file it should just be lines of valid sql both ddl and dml (structure and data respectively) Also if you google restore MySQL dump file there is a ton of documentation. Edit: First link from stackoverflow for a basic google of your question http://stackoverflow.com/questions/105776/how-do-i-restore-a-mysql-dump-file
The numbering of the clues on your website doesn't match... the PDF seems fine though.
Did you have a chance to look into 'full outer join'?
This has nothing to do with SQL, and more than that, the page you link to has no meaningful content. You are now flagged as a spammer.
No, I am joining on all six conditions.
Well it's a different result set which is good! I'll have to look into the DB and see if it's returning back as it should, I won't be able to confirm that it works as intended until tomorrow but thanks a lot for the help!
LIKE is case sensitive, whereas REGEXP is not, so they aren't equivalent. You could convert to lowercase and compare using LIKE, but that is an overhead. Your problem is parentheses related. Try this : SELECT * FROM websitePage WHERE ((search_content REGEXP '.*lincolnshire.*') OR (title REGEXP '.*lincolnshire.*') OR (search_content REGEXP '.*museums.*') OR (title REGEXP '.*museums.*')) AND live = 'YES' AND parent_id &lt;&gt; '-1' AND inLitterBin &lt;&gt; 'yes' AND site = 'SomeSite' AND hideFromSearch &lt;&gt; 'yes' ORDER BY title ASC Also, definitely consider using parameterised queries, here's [a previous post](http://www.reddit.com/r/SQL/comments/2y42j0/not_even_sure_how_to_ask_the_question_halp/cp6jpq8) of mine outlining why. 
If you're already in the spreadsheet it would be quicker to stay in it. Enter the below into an empty column and drag down to the end of your list (assuming values are in column A) ="'"&amp;A1&amp;"',"
For sure, in some cases it's not going to save you a huge amount of time (I've written that exact formula more times than I can remember, doing as you describe), but in cases where you've got a bunch of filters on and the last thing you want to be doing is inserting columns and writing and copying formulas, it can be handy. Well, hopefully it's of use to someone!
Has anyone done any of these? Are they useful? I have a report writer working for me who is pretty green and could benefit from something like this, I have my MCSA but I did classroom training, are these a good equivalent?
We've gotten lots of positive feedback (not just from our internal team) from our viewers, both from the live feed and people that view the videos afterwards (we both post them to our respective websites) because we understand that someone might not be able to watch them live. As for the content, it will most definitely help a report writer. I have done (and still do) my fair share of SSRS development creating new and tuning old reports and having a fast query makes the difference between a useful report and one that doesn't get ran.The purpose behind this training series is to get all of our software engineers 70-461 - 2012 SQL Queries certified (an MCP certification). You can view all of our previous training materials here: http://www.aaronbuma.com/category/sql-training/ 
I have to get first name and last name from a single column containing both. The problem is, some data is last name, first name while others are first middle last. So after my updates, I wind up with last name correct, and first name containing first&lt;space&gt;middle. The last update, after the whole thing executes, I can re-run it and get row updates. Run it a third time, and there are none. There should be none at the first run. It's as if the last update isn't running (there are no updates past this line). 
Or do it in VIM: %s/.*/'&amp;',/
Ahh gotcha. Thanks! 
That sounds quite cool. I've been meaning to start using Notepad++, I'll give it a go tomorrow at work and see how much of my job I can get it to automate!
It's a different programming paradigm - SQL is [declarative](https://en.wikipedia.org/wiki/Declarative_programming), not [imperative](https://en.wikipedia.org/wiki/Imperative_programming).
Yes, that is the best disclaimer that I have seen in a while.
I use notepad++ for regex. Also, if it's REALLY BIG, I import the spreadsheet and do the IN as a subselect WHERE value IN (select keyvalue from importedTable)
Take an afternoon, read [this book.](http://www.amazon.com/Minutes-Sams-Teach-Yourself-Edition/dp/0672336073/ref=pd_bxgy_b_text_y) Read [this blog post.](http://blog.codinghorror.com/a-visual-explanation-of-sql-joins/) Get a copy of the [Chinook database](https://chinookdatabase.codeplex.com/), start querying random shit to see what comes up. That should get you 80% of the way there.
Pivot Table! is what you want. ID and Period Start would be your Row Names. 
Yeah, this is my method too. It bemuses me that people at my work are so resistant to the idea of using this kind of thing for simple data munging line this. 
Caveat emptor, find and replace in ssms messes up sometimes when replacing \n with comma
A generic "SQL Certification" is worth approximately the paper it's printed on. If you want to work with MS SQL Server, them MS's certs **do** carry value if you're going to be a DBA or serious database developer.
If you are using MS SQL, SSMS has regex built into the find and replace tool.
That's what I thought too. But it works when each cluster is run individually.
Just run them as SELECTS and see what they return. I see nothing as to why it would happen. 
Why would you say a general SQL certification is useless? What I want a certification I can use as proof that I have achieved a certain level of competence and understanding of SQL to employers.
Depending your structure, you could do this with a `join` instead of a subselect and get better performance.
You can do this directly in SSMS as well if you're using SQL Server, just make sure you have regular expressions checked off in the find and replace box and replace \n with ',' 
It took me way more than an afternoon, but I have read that book. Each chapter is supposed to take like 10 minutes to read. I like spending time trying out what I've learned after each chapter, otherwise when I get to the end of the book I wouldn't retain any of it.
I use sublime text to do these kinds of things. The multi cursor mode is quite powerful when combined with the find functionality. 
Wow. Really great reply. I'm not the OP, but I appreciate your recommendations as I'm on the path to learning SQL. Though, admittedly, there's been a lot of trial and error on this path so far. 
Here is the [link to the new poster](http://stevestedman.com/2015/03/sql-server-join-types-poster-version-2/) with bugs fixed and suggested items added. 
The poster has been updated [link to the new poster](http://stevestedman.com/2015/03/sql-server-join-types-poster-version-2/)
&gt; we have changed the type of this column fron [sic] an integer to a navchar over the weekend for some random reason NVARCHAR, not navchar, yes? Not a SQL question, but a SQL Server question, yes? (SQL is a language; SQL Server is a dbms with its own nonstandard idiosyncrasies, like convert().) &gt; the report isn't liking it You might want to be a little clearer about what you mean here. Error message? Crash the server? Data doesn't fit in the space allowed? Report follows you home and tips over your garbage can? Assuming you leave the data alone, changing the data type of a column shouldn't bother a report unless the report makes unwarranted assumptions about the data type. The GROUP BY clause itself doesn't make those kind of assumptions. So if you started with a working SQL statement like select ... from ... group by t1.thing_no; then I'd expect that SQL statement to continue to work after changing the data type of thing_no. &gt; but t1 actually points to a synonym *t1* is almost certainly an alias; *synonym* means something different.
Yeah we're using SSRS, could the issue be the grouping there? I presumed the stored proc did it? I'm super new to SQL
Is there something like this for MySQL?
Digged more into it, got my answer: SELECT emp, dept, row_number() AS rn FROM employees;
Start by reading up on JOINs.
If you think to remember could you reply here with a link to your follow-up posting? 
Thanks very much!
This is brilliant! exactly what I was looking for. Completely surprised and thrilled for the answer - thank you!
I never understood the reason for semi joins or anti semi joins. They accomplish the same thing as a directional join with a is null/is not null statement but they seem to perform worse.
With the over clause SELECT e.emp, e.dept, ROW_NUMBER() OVER(PARTITION BY e.dept) AS rn FROM employees AS e;
Don't be cross, your inner desires will become outer realities.
When i learned i had so much trouble understanding joins. But if i had this poster understood it much quicker. For me it is still worth to check the poster out, because of the triple join. Thanks for sharing.
Two join explanations?
I'm confused - share queries how? I'm assuming you don't mean in the form of scripts, functions, packages, etc.
If you have sharepoint, I'd attempt to utilize it first. Maybe /r/sharepoint might have some pointers for formatting code in the wiki. If you don't have sharepoint, I would implement a wiki that has strong code formatting support. I think you know exactly what your looking for. I wish I had stronger experience in various wikis to help you.
You can also use '=' SELECT e.emp, e.dept, rn = row_number() OVER (PARTITION BY e.dept ORDER BY e.emp) FROM employees AS e;
Thanks for the feedback.
Happy to share. Thanks for the feedback.
Good answer mtger47. Thanks for answering.
At my work, my boss and I are training our Software Engineers for the 70-461 SQL Queries Certification. We are also giving back to the community by broadcasting the sessions live AND posting the videos, scripts and slides on our websites for **FREE.** We do our sessions every Thursday at 9:00 am PST. This week we are covering Time, Logical and User Defined Functions: http://www.aaronbuma.com/2015/03/tsql-time-logical-and-user-defined-functions/ Here are a list of past presentations: http://www.aaronbuma.com/category/sql-training/
A little trick I use is Excel. Past the list in one of the colums and use the CONCATENATE function to generate massive inserts, combine CSVs etc.
Which DB? Oracle or DB2?
I originally thought of both of these as well, we due to regulations at state and federal level we have to be able to report on the previous 7 years, so have to keep them for at least, I had thought about dumping them to a .sql file for backup purposes after 1 year of data but that means a non easily package-able reporting solution would need to be built to accommodate sql files as well as current year from tables. 
You saved it as a CSV, but is it exported as a CSV?
That's the magic of the date range - if the part didn't exist before, or there was no number, there would be nothing to sum :D
if i group between date 1 and date 2, that will just give me the price change of the past month correct? what if i want to use this create a graph showing the history of price changes for 5 years? then that wouldnt work right? thanks so much!
Sounds they're like maybe wanting a code review for SQL queries? Allowing people to comment and make suggestions to the query?
Sounds like an excel issue instead of a sql issue then? Sql doesnt really do formatting its the data retrival layer. Your front end (excel) is responsible for formating. Can you write some vba macros in excel to fix the formatting?
SSIS is a feature of SQL Server, not Windows Server. The version of the underlying operating system should not be an issue. SSIS has shipped with every release of SQL Server since SQL Server 2005. With the proper DSN and security configuration, SSIS can connect to remote SQL servers, regardless of their version. 
And postgres please :)
thank you. 
Nice try, but postgres isn't going to be something that I do.
After I created the SQL Server JOIN Types poster, I received many requests to create something similar for MySQL. This poster is just that. [MySQL version of the poster](http://stevestedman.com/2015/03/mysql-join-types-poster/) Jut to set expectations clear, some have asked for a postgres version of this, but I don't plan on doing that, if someone else wants to make a similar poster for postgres, go for it.
Here you go. [MySQL version of the poster](http://stevestedman.com/2015/03/mysql-join-types-poster/).
thanks! 
Why, honestly?
 // Assuming MSSQL 2008r2 &amp; [month] is an INT in YYYYMM format SELECT curr.Competitor , curr.Product , curr.[Month] , ABS(curr.Price - ISNULL(prev.Price, curr.Price)) AS Movement FROM Sales AS curr LEFT OUTER JOIN Sales AS Prev ON curr.Competitor = prev.Competitor AND curr.Product = prev.Product AND curr.[Month] = (prev.[Month] +1) ORDER BY curr.Competitor , curr.Product , curr.[Month] DESC ;
&gt; where a.personid =null tsk tsk tsk
If you're using MS SQL, you can do this with a CTE and a window function. This example shows current and previous prices, and handles date gaps when the price wasn't checked for a product one month. The WHERE clause restricts it to the latest price, and only records where there has been a price differential. Remove the where clause to get a complete chronological history of prices. Change the INNER JOIN to a LEFT JOIN to include new products that don't have a price history. WITH cte AS ( SELECT ROW_NUMBER() OVER (PARTITION BY Competitor, Product ORDER BY Competitor, Product, DateChecked DESC) AS PriceCounter, CompetitorID, ProductID, Price, DateChecked FROM MyTable ) SELECT t1.CompetitorID, t1.ProductID, t1.DateChecked AS LatestMonth, t1.Price AS LatestPrice, t2.DateChecked AS PreviousMonth, t2.Price AS PreviousPrice FROM cte t1 INNER JOIN cte t2 ON t1.CompetitorID = t2.CompetitorID AND t1.ProductID = t2.ProductID AND t2.PriceCounter = t1.PriceCounter + 1 WHERE t1.PriceCounter = 1 AND t1.Price &lt;&gt; t2.Price
Sharing scripts in a knowledgebase format essentially. Where we could take the SQL from the application interface code and break it down into troubleshooting steps for the implementation folks who aren't as versed in using that SQL for troubleshooting. Eg; "The interface looks at Field X for particular value. Run this query and if it returns a Null, that is why client is not seeing information in application. - select * from table z - "If there is a value there use it in this next query, continue to next troubleshooting step..."
thanks for the feedback. Enjoy!
Which database allows the equal sign? Does that mean you can apply a where clause on a window function? Can you write this: WITH part AS (SELECT emp, dept, row_number() over(PARTITION BY dept) AS rn FROM employees) SELECT emp, dept FROM part WHERE rn &lt;= 1; as (doesn't work on Postgres, you have to use CTE): SELECT emp, dept, rn = row_number() over(PARTITION BY dept) FROM employees WHERE rn &lt;= 1; 
Do you have access visual studio with team foundation server (TFS)? This sounds like a job for Source Control. 
IME this is the best solution. As long as the data format is predictable (which coming from your job, it should be) you can just pre-create a template spreadsheet in excel and then write a quick VBA macro to pull the data from your output .csv and populate the template with it.
From your description and the error, the problem is that you have t1.THING_NO selected in your SELECT statement un-aggregated (i.e. you are not SUMing it or something similar -- aggregating the individual points into one for the GROUP BY) and so when you go to do your GROUP BY for t1.STUFF,t1.OTHERSTUFF SQL Server basically doesn't know what to do with all the data points. The way to fix this is to either add t1.THING_NO to your GROUP BY (if you want your data grouped on THING_NO) or to aggregate it in some way in the select statement, so SQL Server can interpret how you want the aggregated data distributed among the things you see in the GROUP BY.
It turns out that there are some cases where the SEMI JOIN or ANTI SEMI JOIN can perform faster than the IS NULL / IS NOT NULL alternative. As with any performance tuning, it really depends on the actual situation. There are some times I will use a SEMI JOIN, and others the LEFT JOIN with exclusion (IS NULL) to get very different performance numbers.
Because I have never used postgres and I don't have any plans to us it in the near future. SQL Server and MySQL keep me busy enough.
I agree
other viable alternatives? you mean, like re-writing the query so that it doesn't use a subquery in the first place? i'm going to assume that you wouldn't have a forum with an invalid category, so you can rewrite the LEFT JOIN from forums to cats as an inner join even better would be to write it as a LEFT JOIN from cats to forums, to allow for the case where a category has no forums finally, i don't understand why you have the GROUP BY -- i'm fairly certain it's redundant SELECT F.board_id , F.board , F.`desc` , C.cat , T.topic , U.name aname , T.author , T.tname , T.tdate FROM f_cats C LEFT OUTER JOIN f F ON F.bcat = C.cid AND F.plevel &lt;= ? AND F.age &lt;= ? LEFT OUTER JOIN topics T ON T.tboard = F.board_id LEFT OUTER JOIN people U ON U.id = T.author ORDER BY C.cid ASC , T.tupdated
Without the GROUP BY aggregate all threads are displayed (causing the boards to be duplicated for every topic that exists within them). Though that query does sort the topics as indicated it causes the results of each board to appear for each topic that exists on the board... exactly as I just said 
http://stackoverflow.com/questions/1043971/determine-which-user-deleted-a-sql-server-database http://www.sqlservercentral.com/Forums/Topic989304-391-1.aspx Good starting place on reading the default trace. 
let us know when the update is out
I don't think you need to specify TOP 1 with EXISTS. The execution plan is identical if you use SELECT * (in SQL Server anyway).
Sorry, old habit. Nowadays when there is a subquery with EXISTS or NOT EXISTS it should scan just long enough to get an answer. Coincidentally this pattern popped up under the "Senior Developer Answer" on a Brent Ozar blog post today about interview questions for SQL developers: http://www.brentozar.com/archive/2015/03/five-interview-questions-to-ask-sql-server-developers/
uh... board to topic is one-to-many, right? so that's what you get perhaps you could explain what you're actually trying to do
 I'm trying to query all the boards along with the most recent topic (not topics, typo in my original post, sorry). So basically it should look like this: http://i.imgur.com/8PUxQ7O.jpg?1
That's really interesting, thank you! Not sure what you're referring to when you say "differences".. Did you want all the columns for each table or something else?
Going to PM you
group by 
Sorry, I am new to this. I put group by instead of order by and nothing changes.
edit: figured first part out... still stuck on second part. I am trying to combine numbers from all 4 columns. So here is the updated text: SELECT service1, count(service1) AS ser1, sol1 FROM Main1 WHERE service1="address" group by service1, sol1 ORDER BY 'sol1'
here are the steps, 1. Install a separate copy of SQL Server 2014 – The first step is to install a separate instance of SQL Server 2014. This will typically be on a newer OS like Windows Server 2012 or Windows Server 2012 R2. This can be on either a physical system or it can be a VM. If this instance is on the same subnet as the original system if will need a different system name but that can be changed later. 2. Make sure your source SQL Server 2000 is at SP4 – Next you need to be sure that your source system is at Service Pack 4 (SP4). You can check the SP level using the following query: SELECT SERVERPROPERTY('productversion'), SERVERPROPERTY ('productlevel'), SERVERPROPERTY ('edition') If the system is lower than SP4 you need to install it before proceeding. 3. Make a backup of all of the databases that will be migrated – After you have made sure the source system is at SQL Server 2000 SP4 then you can go ahead and make a full backup of all of the databases that will be migrated. 4. Install an interim copy of SQL Server 2008 / R2 – Next go ahead and install an interim copy of SQL Server 2008 or SQL Server 2008 R2. This copy will only be used to convert the database backups from the older SQL Server 2000 format which can’t be restored to SQL Server 2014 to the newer SQL Server 2008 / R2 format which can be. You can install this SQL Server 2008 / R2 instance on any system but it would probably be the most convenient to install the temporary instance on your target SQL Server 2014 system. 5. Install the appropriate service pack for the interim SQL Server 2008 R2 release – If you’re using SQL Server 2008 then you also need to put on SP2. If you’re using SQL Server 2008 R2 then you need to be at SP1. For the record you could also use SQL Server 2005 SP4 but I would recommend using one of the newer releases. 6. Restore the SQL Server 2000 database backup to the interim SQL Server 2008 / R2 system – Once the appropriate service pack has been installed on the interim SQL Server 2008 / R2 system go ahead and restore the SQL Server 2000 SP4 database backups to the interim SQL Server 2008 / R2 server. 7. Backup the SQL Server 2008 / R2 databases – At this point the SQL Server 2000 SP4 database will have been restored to the SQL Server 2008 / R2 system and will have been converted to the newer format. Now you can make a backup of the databases on the interim SQL Server 2008 / R2 system. After this backup has completed you no longer need the interim SQL Server 2008 / R2 system and you can remove the instance. 8. Restore your SQL Server 2008 / R2 to SQL Server 2014 – The next step is to take the SQL Server 2008 / R2 databases are restore them to the target SQL Server 2012 or SQL Server 2014 system. Once the restore has finished the databases will have been successfully migrated to the new SQL Server 2014 format. 9. Move Logins and SQL Agent Jobs – The last step in the migration is to move any logins and or SQL Server Agent jobs from the source SQL Server 2000 SP4 systems. You can get a script that you can use to move the logins to the new SQL Server 2012 or SQL Server 2014 system from How to transfer logins and passwords between instances of SQL Server. You can see how you can use SQL Server 2000 EM to script your SQL Agent jobs at How to script jobs using Transact-SQL (Enterprise Manager). Microsoft also recommends that you run sp_updatestats when you’re ready to begin using the new database.
Upvote for figuring out the solution, then posting it. You're a good man, Charlie Brown.
Teachers_Classes.Teacher_ID should also have a foreign key to Teachers.Teacher_ID. A Primary Key is a column or a combination of columns that uniquely identify a record in the PK's table. So, **one PK value in =&gt; one record from table out.** **Teachers** Teacher_ID| Name ---|--- 1 | Mr Bloggs 2 | Prof Smith 3 | Miss Redmond **Classes** Class_ID| Name ---|--- 1 | English 2 | Maths 3 | Chemistry **Teachers_Classes** Teacher_ID| Class_ID ---|--- 1 | 1 1 | 3 2 | 1 2 | 2 3 | 2 In the Teachers or Classes table, their PK would uniquely identify the record in question, but if Teachers_Classes weren't a composite key, and was say just Teacher_ID, then one PK value in would not give you one record out. Teacher_ID = 1 =&gt; (1, 1), (1, 3). Likewise if Class_id were the PK. Class_ID = 2 =&gt; (2, 2), (3, 2). Not a problem with a composite key Teacher_ID, Class_ID = (1, 3) =&gt; (1, 3).
Thanks. Not worth it for a toy database. ;)
IMO it's only popular because developers who aren't willing to learn anything about databases love it.
BTW, I edited my post to allow not counting on null NETWORK_ID.
~~You don't need the CASE if you use filter it in the WHERE instead. Would result in less rows being piped to the SELECT.~~ Nevermind, morning brain was still on control. 
That and its Flintstone level optimization.
Yep, it's a joke and a fad on any metric.
He is my stab at your issue. From top to bottom part 1 is your first request and part 2 is your second request. **Edit**: Formatting. select b.ProductLine,(count(*) * 100/(SELECT NVEHICLES FROM TABLEA where a.CMAKE=CMAKE))as PercentOfPartsOwned from tablea A inner join tableb B on B.CMake=A.CMake AND B.CMODEL=A.CMODEL GROUP BY b.productline,A.cMAKE,Nvehicles select b.ProductLine,(count(*) * 100/ SUM(nVEHICLES)) as PercentOfPartsOwned from tablea A inner join tableb B on B.CMake=A.CMake AND B.CMODEL=A.CMODEL GROUP BY b.productline,A.cMAKE,Nvehicles 
what i mean was i want to see what the overall % of parts that MY company has a part for in any product line. 
Login in a query?
I think he means logi**c**? 
Yeah, stored procedures are basically like little compiled programs that you can feed input into and get output out of. Not only are they much faster, but you can restrict certain users' database privileges to only being able to interact through stored procedures. Your point is also correct - rather than firing multiple queries back and forth over the wire (which could be happening with 20,000 clients simultaneously) stored procedures allow you to fire one shot, process it, and fire one shot back.
&gt;How would a SQL database benefit from login in a query? Do you mean logi**c**? If so, please edit your post.
Ahhh the 20,000 clients really hammered the concept home. Thank you.
One good reason to use a database server is that you can have many client applications (at a time, or over time). It doesn't have to be just one web application. Is it better to have data logic in five different applications, or centralized in one database?
The problem you are going to have with the above is that the '%'s would allow unprintable characters to sneak through. You'd actually want to do something like this. WHERE PATINDEX('%[^ !-~]%' COLLATE Latin1_General_BIN, your_column_here) &lt;&gt; 0 *Edit Note: This checks for any printable character from space to ~ in the [ASCII table](http://www.asciitable.com/index/asciifull.gif). Which is all valid printable characters.* 
Thanks for the response and info! This is definitely going to come in handy.
I thought that is what views are for, in addition to joins, etc? When I was younger a DBA let one of my scripts access a table directly. We both learned about 'WHERE'/'LIMIT'/'TOP' that week
Sharepoint is core to the MS BI stack, and becomes more so with every new release. For example, I just took the "Administering SQL Server" test for my Data Platform MCSA, and there were a good number of questions on configuring sharepoint for the BI integration with SQL Server included as well.
Great that you got the answer, but I'm curious about why you need to do it here. I try to save any `datetime` to string conversions until the absolute last moment - which tends to not be in SQL, but in the end application/report/output file.
Personally I don't like to use "HAVING" and in your example query if you drop having you can also drop the "GROUP BY". IMO it's best to figure out SELECT FROM WHERE and JOIN if you're there yet before you play with Having. I write SQL for a living, I use HAVING maybe once a month. There are other ways to get the same data that make more sense to me, but those involve temp tables or CTEs which are likely beyond what you're wanting to learn now.
If you are going to rely on logging to be accurate you shouldn't have a "Fire and Forget It" attitude. Resources are going to be allocated no matter what, logging generally has never been that intensive; a stack trace rarely takes up more then a single 8k page anyway.
Do SharePoint. A lot of clients don't refresh SharePoint as often as other tech, what you learn will be relevant longer. SSIS changes all the time, especially now that SQL Server seems to be on a 2 year release cycle.
Do you just do SQL, or do you know other languages? Could you drop a job title?
In this case, a `group by` and `having` are most likely the easiest and most readable way to do this. I love a good CTE, but here it would make the code more complex than it needs to be.
Nice beginner explanation, thanks! Done with my test today and made [this](http://i.imgur.com/Qsz2aHJ.png), thoughts?
You beat me to it, concisely and precisely outlines concerns of mine and many others who work with data. Another huge concern is the fact non SQL DBs lacks ACID guarantees for transactional / data integrity. Which gives the tenet : ***There is nothing wrong with MongoDB (or NoSQL) as long as you always deploy it knowing that it can give you back incorrect answers.*** Well I'm sorry, for me that's the main non starter, MongoDB is a fad and I would always pick a SQL RDBMS over it in any and every scenario.
Agreed, the way the code is now is the simplest way to do it. It's just that I don't like "HAVING". The only time I use it is when I'm asked to verify that some column in a table is unique. For which I'll write. SELECT [column], count(1) FROM Table GROUP BY [column] HAVING count(1) &gt; 1 My goal in posting was to make OP aware that not immediately understanding the HAVING keyword is no reason to give up on SQL.
OK, think about SQL like organizing data into buckets or piles. Every (Select...From) makes a new bucket. The Where decides what can go in the bucket. UNION ALL puts two buckets into the same larger bucket. **If you don't use ALL then it drops duplicates.** As for your question, I think you are just comparing the String "date" to a date literal. Put [date] and let me know if it works. 
You write sql for a living and favor slaughtering your query performance with temp tables and (to a lesser degree, usually) CTEs just to avoid using the HAVING clause? You need to face your HAVING fears man, it's not that scary, just a WHERE that's applied after the GROUP BY. 
:) I don't get asked for stuff where a having clause would make sense often.
In looking at BI Developer job postings, how many prefer SSIS development experience versus sharepoint development/integration with Excel. I'd say go SSIS all the way. 
will a join help you in this case? You will need to rewrite your loop there to detect 'results' id changes though and you might need to rename some columns (just type the column list instead of '*' and use 'r.columnA as r_columnA', for example). select r.*, s.* from results r left join steps s on s.result_id = r.id where r.monitor_id = ? and r.timestamp &gt;= ? order by r.id
Thanks for your reply! So I'm trying some stuff out at the moment. This is a rewritten query using join: select count(*) from monitors m join results r on r.monitor_id = m.id join steps s on s.result_id=r.id where r.monitor_id = 64 and r.completed &gt;= '2014-10-01' and r.completed &lt;= '2014-10-03' This particular query retuns 1728 rows. MySQL's explain for this query gives: select type, table type, key, ref, rows, extra SIMPLE, m, const, PRIMARY, const, 1 SIMPLE, r, range, mon_id_compl null, 575, Using where SIMPLE, s, ref, result_id, db.r.id, 2 However, queries like these take 5-10 seconds. The query uses the best indexes so that can't be the problem? The tables' sizes are as following monitors 133 results 8.441.842 steps 33.738.264 Is this too much to expect a quick query? The server is at the moment running quite an old MySQL, 5.1.73. Is there a possibility to get better performance with updating? Using InnoDB if that matters. 
You're looking for the HAVING clause. In a grouping query the WHERE clause filters the rows that the group by calculation(s) are performed on, and the HAVING clause filters the rows that are performed as a result of that calculation. HAVING can also reference calculations that are not part of the SELECT clause. If you add: HAVING COUNT(*)&gt;=2 as a new line you'll get what you're looking for.
Below is a query to get some information on distribution of results relative to monitors and dates. Given the 8 and 32 million row counts, redesigning the storage looks like a necessity. SELECT MIN( CNT ), MAX( CNT ), AVG( CNT ), COUNT(*), MIN( MAX_COMP - MIN_COMP ) MIN_DT_RNG, MAX( MAX_COMP - MIN_COMP ) MAX_DT_RNG, AVG( MAX_COMP - MIN_COMP ) AVG_DT_RNG FROM ( SELECT r.MONITOR_ID, MIN( COMPLETED ) MIN_COMP, MAX( COMPLETED ) MAX_COMP, COUNT(*) CNT FROM RESULTS r GROUP BY r.MONITOR_ID )
In these sorts of situations, the application usually displays data in a master &lt;-&gt; detail configuration or master &lt;-&gt; detail &lt;-&gt; sub detail. In a Master &lt;-&gt; Detail situation, your user configures the Results filter (monitor, timestamp), applies it and this gives a list of results in a grid (master). The selected row of the results grid, then drives the steps query (detail), to gives a list of steps for that result. This means there aren't huge DB hits, worse case scenario all rows from Results and a filtered list of steps for one given Result
Ahh, that's nice, thx :)
 MIN( CNT ) = 1 MAX( CNT ) = 212836 AVG( CNT ) = 63106.6090 COUNT(*) = 133 MIN_DT_RNG = 0 MAX_DT_RNG = 10200954694 AVG_DT_RNG = 6480396841.8270673752 Thanks for your help so far. 
Thanks for your reply. Steps have 6 columns with 2 varchar(255) and 4 ints, so can't be too long methinks? I will have a look at fragmentation.
I found [Murach's SQL Server For Developers](https://murach.com/books/sq12/index.htm) really handy for CLR developing.
you don't want JOINs, you want UNIONs
So after a bit of reading this is what I came up with. I can't figure out how to insert this into a table though using the same query, and this doesn't feel like the cleanest way to write this. SELECT * FROM Visa_Verde_Course_List UNION ALL SELECT * FROM Fairview_Course_List UNION ALL SELECT * FROM Chualar_Elementary_Course_List UNION ALL SELECT * FROM Chalone_Peaks_Course_List; 
HAVING is mainly just a WHERE clause for aggregate data that you created when you did the GROUP BY. count(subject) was only possible because you had a GROUP BY so the only way to narrow it down is by using a HAVING. FW**G**HSO *count(subject)* created by: GROUP BY Only places you can use *count(subject)*: HAVING; SELECT; ORDER BY
If you're working with MSSQL at work, no reason not to give SQL Server Express a try. http://www.microsoft.com/en-us/server-cloud/products/sql-server-editions/sql-server-express.aspx
If you use http://sqlfiddle.com/ you wont have to download anything.
And if you find yourself needing the features you have on the work instances, then Developer edition license is cheap, installs on non-server versions and has the same stuff as Enterprise
Oh we definitely will!
There's a data type in TSQL called hierarchyid, and supports all kinds of neat functions such as IsParentOf etc to do this sort of thing. Have a look. Recursive CTEs are also your friend.
&gt; What's wrong with the concatenated string from a CTE (and I'm assuming you are using a recursive CTE not the mess you posted)? I'm not using a recursive CTE or concatenated string from a recursive CTE because I need to pivot the ID's out into their respective level columns. Trust me - I've considered this approach, but it always ended up with me having to parse\split the concatenated string to separate them into the separate columns. Of course, the Widget table described above is an arbitrary representation of what I see is the core of the challenge. There is actually much more that I must do which requires the widgetID's to be separated out into individual columns. Ironically, the end result of this (later down the line) is that there will be a concatenated string. However, I have to perform multiple transforms and mapping activities on these pivoted ID's and associated data before that can happen.
I glanced over this and it looks interesting. However, I'm not yet sure how promising it is for my application. I'll have to dig deeper into this. Answer me this... I got the idea from [this MSDN article](https://msdn.microsoft.com/en-us/library/bb677173.aspx) that you **either** use a parent/child relationship (as in where I'm pulling data from and the arbitrary widget example above) OR you use hierarchyid. Is this your experience? Am I understanding this correctly?
For 4 &amp; 5 I would use HAVING clause. Having count(horror)&gt;=2
You are running in a DOS terminal - the example is using a Unix shell. The equivalent command in DOS is 'dir'.
Basically ls -l is a Linux command, while you are currently using Windows. You can try ‘dir‘ for the same result on Windows.
but 'dir' doesn't give the same result
Important. UNION vs UNION ALL. If you use UNION ALL, the query will run faster but this is because its garbage in garbage out. Or in this case, garbage out garbage in. If the values match the data types, the data goes in. Including duplicates. If you use UNION, you suffer a performance hit, but this is because its scanning for duplicates. 
I would abstract it more and say it's a Unix command... as it works on Macs and every other non-linux Unix command line.
Can't you use Recursive CTE?
&gt; Is the following correct and if it is, is it the most efficient, SQLish way to respond to the query above? It returns the empty set. Looks good to me. Dunno the data so don't know why it doesn't return any data. It's a little odd that *customers.salesRepEmployeeNumber* is *int(50)* while *employees.employeeNumber* is *int(11)* but it says that's the right relation.
&gt; in this case all the tables are identical column headers in identical orders Form the habit of specifying your columns **now**. It's easier than having to unlearn it later.
Maybe create the above sql as a view, then just use Excel's native ability to connect to data sources and put the result in a worksheet. Then maybe write some vba to refresh the data and export to csv. You might also look int SSIS to automate this task entirely, skipping Excel completely.
Simple: Download an ODBC driver and then write it as a .dqy file where it automatically connects to the data warehouse server and pulls it....so you can click -&gt; open -&gt; auto run/fill into a CSV sheet. That, or you can get an Apache server on your machine, then use PHP to do the above and use a PHPExcel library to auto-export.
2. Not sure if 2004 is going to be a number in that case. Probably better to use '2004' But yeah, go to office hours.
CSV is NOT Excel. CSV is plain text and Excel can open it as a single Sheet document. What are you doing with the data after you've dumped it into CSV? Excel can query Oracle directly. Or you can get a reporting tool like BIRT to do the query and output to your desired format. 
1. Set up a passthrough query in Access using connection string to the SQL database and use the SQL as written. Create a macro that exports said query to csv using TransferText (or TransferSpreadsheet if you actually want it in xlsx format). Have Windows Scheduler run a .bat script that runs the Access macro in the middle of the night. 1. In Excel create a data connection directly to your SQL database. Enter the SQL as written. Excel file will now auto-refresh whenever you open it. 
How long does it take to just get the resutls for a monitor_id and timestamp range - SELECT * FROM RESULTS r WHERE r.MONITOR_ID = 64 AND r.COMPLETED &gt;= '2014-10-01' AND r.COMPLETED &lt;= '2014-10-03'
Nothing in the manipulation portion that needs to be rolled back. It's all reads from the db into temp tables for manipulation, so only shared locks, if I understand correctly. 
I'm not sure how to benchmark a locking issue... The stored procedure, when run on my test environment with our (very slow) test server, runs fine, but when run in the production environment on a brand new server from the application that calls it, running for 10+ minutes and timing out (and not always). We're leaning towards a lock happening somewhere but can't seem find the process that it's conflicting with.
You need one more quote at the end to close the one that started before Exec. 'Exec LALALADW.dbo.DW_YPOP @CustString = '96Chin''
Nice. At pub now. Will test tomorrow. 
I'm not telling you whether code will crash your production environment or not, I'm not that bold :P
Speaking on personal preference I find wrapping only related operations together. If the conditional updates can fail or are independent of the bulk data load(or vice-versa) then they can be wrapped in difference transactional blocks. If they can't, they should be in the same block.
Whats odd is it works if the custcode is pure letters or letters then numbers. Either way. Gna try the suggestions and see how it goes. 
As I understand it, the (nolock) hint only applies to SELECT statements, as in, not waiting to get a shared lock to read data. The hint will not affect INSERT and UPDATE statements, which require locks.
This is correct. Normalize your data, use group by to specify your output columns. 
What do you do with this data? That's a super important factor in picking a design.
I would say you should try to organize the data so it's easiest to analyze since that's the thing you'll be doing most. You want that thing to be easy as possible, even if that makes a bit more work for you in the parts you do infrequently, like writing the data. The other thing to consider is how much time are you going to spend maintaining this thing? If there's going to be a lot flux in the meta-schema of the incoming data (lots of new devices, old devices changing the way they report data), you'll want to optimize for easy maintenance. I think I'd be tempted to extract out the common data points into one (or maybe a few if there are few different common sets of data among the devices) and then have some additional satellite tables to hold the data that's more specific. This way you can easily report on the common data points. I think trying to enforce data integrity in the database for a large number of these different meta-schemas is going to be more trouble than it's worth. Since you're only inserting once, and from the sounds of it, via some front end, it will probably be a lot easier and more maintainable to do that stuff just once up front when you pull the data in.
Since it seems you main priority is reporting/analysis, see if a dimensional model would make sense in your case? I think a device_measurement_dim (combining deviceType, device and types tables), a date_dim (covering the date and the relevant rollups), a hhmmss_dim (covering the time including seconds, if needed) and a sub_second_dim (if you need sub-second analysis) should cover you case. The main fact table (values) in your case then would have device_measurement_key, date_key, hhmmss_key, sub_second_key and state (the measurement, I assume).
I can't speak to 2014 vs 2012, but the AdventureWorks database is a sample DB with data about a bike store, there are tables such as customer, orders etc. that are filled with rows of data. MS uses the AdventureWorks DB in any examples, explanations of queries from their training material.
You've asked a rather open ended question on a subreddit for more SQL programming sorts of things. But anyway, I'll still give you a basic runthrough: How the database is set up is determined by what host you're on. It goes from easy (such as with Bluehost here: https://my.bluehost.com/cgi/help/6 ) to difficult, such as with Amazon AWS here: https://www.digitalocean.com/community/tutorials/how-to-install-linux-apache-mysql-php-lamp-stack-on-centos-6 I am going to recommend that what you do is search for whomever you chose as your host along with "Mysql." You'll probably pick up what you need to do! One last thing of note for you! You're getting into a world of tons and tons of options, and they do change pretty quickly, so it's quite exciting. But if you want to get started with a basic understanding of everything, I recommend you look up what a "LAMP stack" is. It's sorta the underpinning of a lot of web sites out there, and if you really learn this stuff you can make a lot of money and have a pretty fun job! Take care, -Chris
Thanks for your reply. So I have to set it up through my host? My website is being currently hosted by Dream Host. I went on their website and found [this](http://wiki.dreamhost.com/MySQL) documentation. My understanding from what I've read and what you've wrote is that I enter in basic information on their website and they make the database for me. Is that correct? Also, I'll definitely be checking out that LAMP stack. I'm always looking for new things to learn!
Yes, most of these major hosts tend to sorta hide the nitty gritty and give you a simple interface where they make the database for you. It's not really worse for a small website, so go for it! If you want things really easy, it looks like they support one click install on phpbb, which back when I was on forums was super popular. You might want to go with it instead! http://wiki.dreamhost.com/Available_One_Click_Installs
you need a subquery which pulls the last date for each customer then join that back to your main table on customer and month
i like a guy that can take a broad outline and write his own sql from it! please post the query when you're done so others may learn the technique
You're taking the horse to water and teaching him to fish!
I'm struggling a little bit with this. I can see why when I did a brief test it worked and why upon further examination it does not. If my data looks as below: &amp;nbsp; customer, MONTH(dated), amount a01, 05, 100 a01, 05, 150 a01, 07, 100 a01, 07, 125 b01, 04, 100 b01, 05, 50 b01, 05, 50 b01, 08, 25 &amp;nbsp; I want the query to select the MAX(month) for each customer, which my initial query was not doing: SELECT customer, SUM(amount) FROM tbl.sales_analysis WHERE MONTH(dated) = ( SELECT MAX(MONTH(dated) FROM tbl.sales_analysis t01 INNER JOIN tbl.sales_analysis t02 ON t01.customer = t02.customer ) GROUP BY customer &amp;nbsp; This will essentially give me the sales value for December. I think I may need to create a temp table holding *customer, MAX(MONTH(dated) * then use that as the sub select. 
You don't need to practice cursors.
Can you give this a try? select SUM(Amount) as AmountLastMonth, CustomerID from CustomerSalesFromLastMonth group by CustomerID, Dated having Dated in ( select MAX(Dated) from CustomerSalesFromLastMonth group by CustomerID)
what about windowing functions? and why don't I need to practice cursors? Is it because that they cause severe performance lag and nobody uses them ?
Just a few tips here. Be sure you aren't locking tables on read, and make your transactions as small as possible. Usually what I do is, grab and stage the data, do my processing and updates on the staged data, and then at the end have the transaction and do whatever insert/update as needed. For big updates (10,000+) consider using a looped batch update, say 1000 of records at a time, to allow other transactions through. So like this: Try [Stage data for each table to be updated] [manipulate] begin tran [update table1 from temp1] [update table2 from temp2] [update table3 from temp3] (batch process e.g. update only 5000k records at a time if needed) commit tran end try 
 Alter trigger [dbo].[LabelFlagsupdate] on [dbo].[Table1] for update as Update Table1 set Field1 = (Case When (user_roles.role_id = '2' and USER_ROLES.record_status &lt; '3' and Vendor_Item.V_ID = '5613') or (user_roles.role_id = '3' and USER_ROLES.record_status &lt; '3' and Vendor_Item.V_ID = '5613') or (Vendor_Item.V_ID &lt;&gt; '5613') Then inserted.ING_GLUTEN_FREE Else Deleted.ING_GLUTEN_FREE --raiserror ('You do not have permission to alter Field1',1,16) end), Field2 = (Case When (user_roles.role_id = '2' and USER_ROLES.record_status &lt; '3' and Vendor_Item.V_ID = '5613') or (user_roles.role_id = '3' and USER_ROLES.record_status &lt; '3' and Vendor_Item.V_ID = '5613') or (Vendor_Item.V_ID &lt;&gt; '5613') Then inserted.ING_ORGANIC Else Deleted.ING_ORGANIC --raiserror ('You do not have permission to alter Field2',1,16) end), Field3 = (Case When (user_roles.role_id = '2' and USER_ROLES.record_status &lt; '3' and Vendor_Item.V_ID = '5613') or (user_roles.role_id = '3' and USER_ROLES.record_status &lt; '3' and Vendor_Item.V_ID = '5613') or (Vendor_Item.V_ID &lt;&gt; '5613') Then inserted.PRIVATE_LABEL_ID Else Deleted.PRIVATE_LABEL_ID --raiserror ('You do not have permission to alter Field3',1,16) end) FROM Table1 join inserted on inserted.item_id = item_master.item_id join deleted on inserted.item_id = deleted.item_id join vendor_item on vendor_item.item_id = item_master.item_id join users on users.user_id = inserted.Change_ID join user_roles on users.user_id = user_roles.user_id
Few things that I would comment on. Use lots of parentheses with OR so you are sure it does what you want. The query analyzer does not always process boolean left to right like other languages. Store your values in variables then you can error check them. You can rollback the transaction in the trigger based on the field name so that you don't have to manually roll it back. Edit: Moved common filter to the WHERE clause. ALTER TRIGGER dbo.LabelFlagsupdate ON dbo.item_master AFTER UPDATE AS BEGIN DECLARE @DeniedColumn VARCHAR(100); SELECT @DeniedColumn = CASE WHEN ISNULL(i.ING_GLUTEN_FREE, 0) &lt;&gt; ISNULL(d.ING_GLUTEN_FREE, 0) THEN 'ING_GLUTEN_FREE' WHEN ISNULL(i.ING_ORGANIC, 0) &lt;&gt; ISNULL(d.ING_ORGANIC, 0) THEN 'ING_ORGANIC' WHEN ISNULL(i.PRIVATE_LABEL_ID, 0) &lt;&gt; ISNULL(d.PRIVATE_LABEL_ID, 0) THEN 'PRIVATE_LABEL_ID' ELSE NULL -- Allow END FROM INSERTED AS i INNER JOIN DELETED AS d ON d.item_id = i.item_id INNER JOIN vendor_item AS vi ON vi.item_id = i.item_id INNER JOIN users AS u ON u.user_id = i.Change_ID INNER JOIN user_roles AS ur ON ur.user_id = u.user_id WHERE ( (ur.role_id = '2' AND ur.record_status &lt; '3' AND vi.V_ID = '5613') OR (ur.role_id = '3' AND ur.record_status &lt; '3' AND vi.V_ID = '5613') OR vi.V_ID &lt;&gt; '5613') ) IF @DeniedColumn IS NOT NULL AND @DeniedColumn &lt;&gt; '' BEGIN IF @@TRANCOUNT &gt; 0 ROLLBACK TRANSACTION; RAISERROR('You do not have permission to alter field %s', 16, 1, @DeniedColumn); RETURN; END; END; GO 
Nailed it! Thank you very much! TIL you can use a sub-query in a FROM statement! Nice one!
Cursors don't cause any performance lag unless they are used incorrectly, like almost any aspect of SQL. They have their place, and if you use them when you shouldn't then things will slow down. Personally, given the DB structure I work with, there's no way around using cursors with dynamic sql, a lot of people would say straight off the bat that this is wrong, it isn't, it is one of the few times when any other solution is either slower (and a lot more difficult to maintain), or just wouldn't work. Cursors definitely have their place. That said, you'd be better getting familiar with other areas of SQL, that will be used more frequently, if you do want to understand cursors more though, learn about set theory. Once you understand set theory, look into cursors and figure out how they differ, this will help a lot more than just learning about cursors, and you'll end up understanding a lot more than just cursors too.
Don't use triggers for this. Keep your logic in the stored procedure.
As long as all your apps and users only use stored procedures this is sound advice. If you have anyone meddling around in your database then triggers might be a better option as they catch all points of modification.
a multi-billion dollar company with somebody reaching out to reddit for help and posting parts of their gluten free database schema. rigghhhttt.
I'm curious what about this that you find so unbelievable? I work with some massive companies that employ some ridiculously incompetent people (not saying that OP is). I would applaud those people making 2% of the effort OP is rather than just do some nonsensical crap they came up with themselves after thinking no more than 3 seconds about the problem.
Try to be more security conscious.
One thing that will separate you from many is an ability to write fast SQL. I have taken code that runs in hours and make it run in seconds. Unfortunately that won't happen unless you work with a large amount of data and deal with complex problems. edit: just wanted to add to my last point. there're often times that you'll need look into optimizing code that's already running fast. For example, a SP takes about 3 seconds to run (seemed fast) but is possible to optimize to make run in a second. if this is called a thousand times from another app, that's 30 mins saved.
Plans at the moment are to shard the data by client (1-3 devices per client), into different databases and potentially different servers. The billions of rows question was for "could that handle dumping everything into one table without becoming very, very slow and painful". I'm not sure splitting timestamp,device_id into another table makes sense if the duplication isn't excessive and already expensive queries (show me the most recent value of variable 12) then require a join and a sort of a virtual table... The priority is storing the data, jobs to check consistency and sanity checks on retrieved data are fine (that's how it's currently done). You're correct, only new and changed values will be stored. This deduplication is handled by the embedded system reading the values from the actual equipment, so the server won't have to do the hundreds of queries per device per second for that. You're correct that skipped reports would cause incorrect data as well, but that's currently the case (this won't be a regression). It hasn't been a problem so far because the only time that would happen is if the server is inaccessible for long enough that the (ample) on-device memory fills up. Thanks for your perspective!
The only reason I suggested the extra dataset table was that it would allow you to index the values table much more efficiently. But that was because I was assuming that each dataset would be a complete readout and so returning the most recent reading would be the same as returning the most recent dataset. In the case when you only store a value if it changes then having the timestamp and device id in the values table will be much faster for finding the most recent readings etc. So yeah... carry on ;)
 SELECT * FROM "ORDER DETAILS" JOIN "PRODUCTS" on "PRODUCTS".ProductID = "ORDER DETAILS"Product.ID WHERE "ORDER DETAILS".quantity is NOT NULL; The double quotations are definitely out of the ordinary.
Welp, guess the double-quotes are needed - maybe place the field name also in quotes ("PRODUCTS"."ProductID"). Reading the docs, it seems that any field or table name with either a space or are not in ALL CAPS must be enclosed in double-quotes. 
This didnt work for me even in brackets, although apparently open database like brackets instead of double quotes.
Ugh, why did i take my class again, this is due at midnight and this is just my first of 7 SQL's i gotta write. urgh, ive been pulling my hair out for the last week over this fucking class.
The main reason for the dimensional modeling is to provide ease-of-use (aka aggregating/relating/slicing/etc), rather than straight-away performance benefits. There are some special join techniques (for example star join or bitmap hash join for medium selectivity joins) supported by some SQL DB servers (I do not know much about MySQL specifically, sorry). Performance-wise, switching from 2 INT ids to just one will save you up to 25% of your fact table size. Everything depends on your specific circumstances and use cases though. I do not have visibility into your specific environment, but given a client's date reference ('we became aware of such-and-such at this time') my immediate instinct is to ask 'well, that's great, when was that available to me'. This stance might be too paranoid in your environment though. 
 Select * From "Order Details" o Join "Products" p on o.productid = p.productid -- not sure if you should use an inner here or not. Where o.quantity is not null -- I think you could use where o.quantity &gt; 0 here as well.
SQL isn't too bad, your struggling with the syntax which will happen with any new language. I think your join syntax is wrong, im on the same page as Trollfailbot and omegatheory that you need to specify the table and column from both sides of the join. can you do a simple select * from "order details"? if yes then can you do? SELECT "ORDER DETAILS".ProductID FROM "ORDER DETAILS" if yes then can you do SELECT * FROM "PRODUCTS" if yes can you do SELECT "PRODUCTS".ProductID FROM "PRODUCTS" if yes to all above then the following should work SELECT * FROM "ORDER DETAILS" JOIN "PRODUCTS" ON "ORDER DETAILS".ProductID = "PRODUCTS".ProductID WHERE "ORDER DETAILS".quantity IS NOT NULL
Yea, what would he use here though? I think there are a few ways to script this one. A left outer would work since he really only needs the information from the left table... OK so this was a total brainfart moment here and trying to look at the question he wrote at the bottom. &gt; im trying to display products ordered in however much quantity. i have a feeling ive got it completely backwards. Couldn't this just be select productid, quantity from order details? 
https://www.ibm.com/developerworks/mydeveloperworks/blogs/SQLTips4DB2LUW/entry/aggregating_strings42?lang=en
Also it says I can't use the column name, I would of liked to say When &lt;columname&gt; = 1 Then '&lt;mydescription&gt;' and repeat that for all the columns that needed the result changed to the description of my choice
you should read [this](http://stackoverflow.com/questions/194852/concatenate-many-rows-into-a-single-text-string)
 declare deptName cursor for select distinct ID from YourTableName declare @deptCount int select @deptCount = count(distinct ID) from YourTableName declare @deptName varchar(500) while @deptCount &gt; 0 begin open deptName fetch next from deptName into @deptName DECLARE @empName nvarchar(max); SET @empName = N''; SELECT @empName+=Value+N',' FROM dbo.YourTableName where ID = @deptName; insert into ##AggregatedDepartments values ( @deptName, @empName) set @deptCount=@deptCount -1 end
You need to create a temp table ##AggregatedDepartments first.
for now, make it a habit to write good code. that's the most important. everything else will follow as you gain more experience through practice or more reading.depending on job description, you may not even be using many of the tools you have learned (although still good to learn them). here's my personal coding standard/best practice. I'm sure many of it sounds simple but you'd surprised how many people do not follow this: 1. clean and consistent code. I format my code like C#. people you work with in the future will greatly appreciate this. your employers/clients will love you. 2. test every single statement for performance. pay extra attention to statements in the loops (goes to my point above under edit section in previous post). double check to make sure loop is needed (i have seen a lot of loops that aren't needed and can be replaced by a statement that's maybe just little more complex). 3. no more than 5 joins. maybe only one or two more joins if it is known that only small amount of data is involved 4. in big statements with many joins, break it into temp tables but keep into mind I/O takes time. so don't go storing 10 columns in the temp tables if you can just store one column (and later pull data from 9 columns from permanent tables as needed). 5. look for existing functions before writing it on your own. there's a pretty good chance the function is already written by very smart people or came with newer version. for example, when I started working with SQL 2000, [this](http://sqlandme.com/2011/04/27/tsql-concatenate-rows-using-for-xml-path/) was not available...and now I use it all the time 6. always keep in mind that loops, cursors, triggers, recursive can be used only if makes sense. they can be costly. 7. know about locking, lock escalation, deadlock...this can kill your project in production. keep in mind when you unit test your code, it may be okay but when there're thousand people executing your code at the same time, will it work? in my code, I only update &lt;5000 rows in the same table per transaction. multiple table may be updated in the same transaction though. this is a complex topic. you have to balance between perfornance and playing safe. you can find a lot of resource on this on internet. if you want to work with large data, maybe you can use data generator or download sample database. I have used it many years ago. I don't know what's out there now. I know it's difficult to learn to become an advance SQL developer without a project to work on. most of it comes with experience just like all other types of programming. however it is rewarding once you're established. At one point I was a .net guy and had .net certification...I'd never go back to that career route (personal preference). good luck. edit: that's less than 5000 rows (so update 4999 rows or less). [lock escalation will occur at 5000](https://technet.microsoft.com/en-us/library/ms184286%28v=sql.105%29.aspx)
Thank you! That's all I can say for taking your time to write that :)
"I think shes looking to just stick us with a bad grade." If you did the parts up to question 80 by yourself, I can't imagine why these should be stumping you. If you didn't, &amp; got someone else to do it for you, then your current question makes a lot more sense. The point of doing this work is not to 'pass', but to 'learn'. You need to burn a few brain cells to do that. If you aren't willing - perhaps a career in fast food might interest you. Sorry for preaching - but the lesson will come much harder from someone else.
You can nest `CASE` statements, if your concern is using multiple fields to infer a certain description. CASE ColumnA WHEN 1 THEN CASE ColumnB WHEN 1 THEN 'This Description' WHEN 0 THEN 'That Description' END WHEN 0 THEN 'Other Description' ELSE 'Default Description' END Basically, this could be represented in C-like syntax as: if(ColumnA == 1) { if(ColumnB == 1) result = "This Description"; else if(ColumnB == 0) result = "That Description"; } else if(ColumnA == 0) result = "Other Description"; else result = "Default Description";
Whenever you do get the answers to the questions, would you mind posting a link to them? I am going to use this assignment as practice but would be nice to know final outcome. As far as your questions go your best bet is definitely your book and the web.
Could you tell me what `AS [text()]` does in the example you gave? I can't find it anywhere
My browser shows all the links to your diagrams as broken links. Is that just me?
MAX(INT(Value)) maybe
Have you tried running the commands?
Thank you so much... I saw it at work the other day. I don't remember there being xml involved, but I'll look again. Thanks! 
i tried removing them, but it still is doing the same thing, Im going to add screenshots to my post, take a look and let me know what you think 
My work wants my boss and I to train our Software Engineers weekly so that they can be SQL Queries 70-461 certified and we also get to offer it free to the public. This is a weekly event and if you can make it, the videos and materials will be viewable/downloadable from http://www.aaronbuma.com/2015/03/eleven-join-types-of-tsql/ I'm not trying to direct discussions away from Reddit, come to the trainings and discuss it here.
There really isn't a good reason to be running a browser that doesn't support it. The latest version of [every major browser allows it except Opera Mini](http://caniuse.com/#feat=transforms2d).
showing the same diagram for FULL OUTER JOIN and CROSS JOIN is ~so~ confusing imagine if you were new to sql
A Venn diagram has multiple sets AND a question being asked of those sets. When you did the author and books question, you asked the very silly question "Which books are also authors?" That is not the question the join is answering. The question the join is answering is "Which authors wrote these books?" or "What books were written by these authors?" - in both cases "these" refers to the books/authors we've recorded. Let's take a different Venn example - Cats and Houses. We can ask the silly question "Which houses are also cats?" and arrive at the empty set, or we can ask questions like "Which houses do these cats live in?" "What is the cat population of each house?" "Which cats don't live in a house?". For each of these we can point to some part(s) of the Cat/House Venn diagram to identify the set that answers the question and it's easy to understand what is and isn't included in the answer. Furthermore, it is intuitive that we'd get information about the cats from the veteranarians and about the houses from the municaplity and not the other way around. Venn diagrams are a set theory tool and sets are abstractions. Before you can start using the Venn diagram to demonstrate anything, you have to communicate what a set is in a concrete way. Venn diagrams are also an abstraction - their purpose is to create a new set from existing sets which answers a particular question. Leaving the question off the Venn diagram allows it to answer a lot of questions without having to change it - the sign of a good abstraction.
So, Venn diagram with 3 or more tables were understood? I just think that when we consider table relations we speak about one-to-many, many-to-many relations etc. And Venn diagram has a lack of expression to describe this things. While we have so much opportunities to develop great interactive examples, it's really sad to use such a "muted" method.
&gt; Venn diagram with 3 or more tables were understood? Yes, much more so than trying to explain it in alternate terms. People are terrible at keeping track of more then a few conditions on the top of their head (roughly seven), especially those who normally operate in a procedural manner. Explaining [multiple JOIN or conditions is much easier](http://i.imgur.com/SGxX9I5.png). One-to-many is very easy to reflect in a regular Venn Diagram. Many-to-many not so much without the addition of another dimensional space; granting people that need to utilize an understanding of many-to-many relations should be well past needing basic diagram explanations. You mentioned in the article that a CROSS JOIN is difficult to represent, but that is only in the case when a WHERE isn't used (a Full Cartesian Product has much better alternates in every situation), and when a WHERE is used there is a direct equivalence to regular JOINs).
I prefer http://mockaroo.com myself
Been waiting for this. Thank you :)
Off the cuff you COULD do it like this. There is probably a much better way. WHERE c.DESCRIPTION = 'Filed' AND (SELECT COUNT(*) FROM PATENTACTIONS p2 WHERE pm.PATENTMASTERID = p2.PATENTMASTERID AND p2.COMPLETEDDATE IS NULL) IN ( (SELECT COUNT(*) FROM PATENTACTIONS p3 WHERE pm.PATENTMASTERID = p3.PATENTMASTERID), -- ALL 0 -- OR NOTHING )
acode and lcode are codes entered at different times on the tasks. The codes are values that the employees put on the tasks. Max wouldn't work here. Instead, i'm thinking we need to sort by date entered and use top 1 to show the latest acode and lcode entered on the task. Maybe if i create a subquery as described above with an alias. Then, reference the alias and columnname. Ugh, i don't think i can specify the taskid in the sub query. Also, where do i put the subquery though? sub query (select top 1 charvalue from TaskAttrChar tac where t.taskid = tac.taskid and t.attribid = tac.attribid)
Total might be a reserved word in access. Try renaming that table to something else.
I think you need to re-evaluate what you are doing as this seems like you are trying to hack a solution onto something that could be much simpler. Show some sample data at least the stuff up above is a mess.
i installed it how do you see the tables you created i cant find the area
&gt; edit - the values for acode and lcode are held in the charvalue column on taskAttribChar Still not enough information, how do you tell the difference between a lcode and an acode if they are stored in the same column? Edit - Also what defines "highest" for the attribid?
Since you used TOP in the above, I'm assuming your on MSSQL, which means you'll want to use a windowed function to calculate this. ;WITH cte AS ( SELECT rn = ROW_NUMBER() OVER (PARTITION BY taskid ORDER BY attribid DESC), taskid, attribid, charval, entereddate FROM taskAttribChar ) SELECT * FROM cte WHERE rn = 1 
It looks like it's generic SQL and the problems are multiple choice and fill in the blank.
So if I just wanted to do it for one case could I do something like this: CASE c.HasChair WHEN 1 THEN 'Chair Installed' END
I tried it but does't seem to like this bit: ON MainTable.BitType = DescriptionTable.BitType
I don't have Access in front of me, but does Access recognize "JOIN"? You might need to specify "INNER JOIN." Only other thing I can think of is it doesn't like the hyphen in "Main-Table."
Paste the entirety of your query. I'll see if I can fix it.
What is your actual question? What are you trying to accomplish? Is speed, accuracy or storage more important? What database are you using? If you don't care just use varchar(2) on all fields and call it a day.
&gt;If you don't care just use varchar(2) on all fields and call it a day. The overflow handling might get annoying though.
I meant varchar/varchar2 depending on your database. Set it to maximum allowable size and go to town
All good now I was missing and end to my case statement, appreciate your help that worked
Thank you for this. Applying for a promotion soon and this should jump start my knowledge of SQL. 
I don't think they would cover window functions. They mentioned that they would just cover basic things like create tables, databases, update statements , dropping tables etc. Pretty basic stuff 
Call it MAIN_TABLE. Table names, like all object names must not be a reserved word, and generally contain letters, numbers and underscores, unless qualified. Qualified in some DBs is double quoted, others square bracketed. But my advice is to stick to letters, numbers and underscores
the screenshot is from phpmyadmin, thats where its added, and yea it supposed to just pull from a table i think, i dont know much about this stuff
I signed up and went through the first section. Very good information for beginners. The process you use when having the student write their own sql statements is a little confusing. The first time I kept saying, no, that is exactly how you write that statement. Then I realized that the criteria you were requesting had changed. Once I got the hang of that part it was easier the second and third time around. So far, and I've only completed the first section, I love this. The presenter is easy to follow and understand. Most of the time the presenters in videos like these barely speak English or come across as boring. 
I'm not a DBA but I was definitely looking at this with a mix of wonder and rage 
Yeah the only thing is the db has over 3000 entries and i need about 200 of them this time. Also the list would change weekly and i didn't want to have to put the 200 ids in manually every time. 
This sounds like a job for: SCRIPTING! Know any?
how many columns are in that table? can you do a SHOW CREATE TABLE please 
Listen to /u/tippo_sam. I would do one of two thigs: * Script the loading of a table from that data file (trunc &amp; fill), then join to that table from your main table to get your result set * Write a script that reads the file and constructs your query for you The first will likely be faster; in MS SQL Server anyway, you can do a bulk load to populate that table, and (assuming you've indexed your foreign keys) the `JOIN` will be pretty speedy.
CROSS JOIN is equivalent to a FULL OUTER JOIN with an always true ON condition (ex 1=1). Conversely using a WHERE with CROSS JOIN has equivalency to a FULL OUTER JOIN. 
go to the SQL tab, and in the box, type this query and run it -- SHOW CREATE TABLE pp3_poll_options this is also where you can run your query -- SELECT * FROM pp3_poll_options and confirm that it's showing you all the columns in the table 
In MS SQL Server, you can set up a "linked server" to point to a CSV file, then read from it like you would a regular SQL table. If the text file isn't nicely formatted, things get trickier. For small files that you read weekly, performance is fine. You can use the same method to read data from Excel files, Access files, dbase files. The exact syntax of the SELECT statements and connection strings for setting up the linked server can be found via Google.
And PG and SQLite and Teradata and...
yeah, zero rows is pretty definitive of a problem in loading the table
My file is located on the server. basically it is a .txt file with a list of id's that i parsed using a curl script from a webpage. Basically i would like to create a Select statement were it would be something like. SELECT * from DB WHERE id= (the id's from my txt file). but i finding that may not be possible. 
Also, Access!
At my place of employment we use Ultra Edit to format orders that we receive in bulk. Its essentially a tool that will help you build the scripts after you copy and paste the UserIds. whenever you use an IN statement you have to surround the context in single quotes. This can be accomplished much more quickly using UltraEdit, for example.
This worked spectacularly. Thanks so much. I just needed to get it out in a CSV and it seems to have done the trick.
What's nice is that we use an add-on for SSMS called "SQL Complete": http://www.devart.com/dbforge/sql/sqlcomplete/ It can do some pretty extensive auto-formatting of your code and it lets you define what that format should be so I build a format file and have the rest of my team use the same file and BAM! everyone's code is formatted nice and clean and uniform :)
I don't get the zero thing? Did I miss where he explained it?
I like. 
page is down. I'm happy with PoorSQL.com
Store the db in a hash? Care to expand on that. 
Please don't use EAV in the future :)
&gt; I always remove the 0 before promoting script. i love you even more now 
A Venn diagram takes 2 or more sets as input and produces one set as output. "1) each house contains one cat" is a singular set and "2) each house contains a lot of cats (could be dozens of them)" is also a single set. The cardinality (size) of the sets are different and its certainly accurate to state that calculating cardinality of a result set using a Venn diagram is ambiguous. I would ask, why are you trying to use a Venn diagram to calculate cardinality? Sets are an abstract concept and Venn diagrams are a tool for showing how set operations act on sets. When using it as a teaching aid for databases, the purpose is to show a thought process for handling ANY question/query - i.e. "to think in sets". The mapping of Venn diagram shadings to various join semantics is so that you can convert between concrete semantics (joins, tables) and abstract semantics (Venn diagrams, sets). Another good use of Venn diagrams is migration. Lets say you have 2 databases that have very different designs and you are tasked with migrating a query from one database to the other database. Take the existing query and create a Venn diagram for it; then map the sets in Venn diagram to the target database's tables; and finally write the target query by converting the Venn diagram operations into SQL join operations. 
You could use a HAVING clause. This is similar to the WHERE clause, but works on aggregates. Select customer, prodline, modno, Count(*) from MyDB.dbo.archive group by customer, prodline, modno HAVING count(*) &gt; 10 This would only return groups where the count is larger than 10.
Thank you for this, it works! :D
That would definitely count as being more elegant, thanks! And I figured out my error, so the counts match... Outstanding!
What you probably want is the following subquery (select sum(order_details.quantity * order_details.price) FROM order_details WHERE order_details.order_id = orders.order_id ) 
We also tried to add the service account with DENY permissions on the database, for some reason that didn't work and the table was just truncated again at 7:30PM. Also, if I wasn't clear above, the job is most likely coming from another server.
^This! It'll give you the SPID of every connection to your server and what the connection is doing.
It is supposed to bring forward 2 points: - that a join condition ('alpha') is ANY valid condition on AxB (cartesian or a cross join), including, for example, conditions that fully ignore one side (i.e A.id &gt; 7 is a valid JOIN condition - I know people who have found this surprising). - that 'outer' parts of the join are complements of the original data sets more so than a negation of the join condition, which might be hard to construct (such as function-based ones, or the ones relying on case expressions, or ones with a lot of NULL-able fields tested, etc.)
https://www.linkedin.com/pulse/20140918135022-2867075-how-to-find-out-who-truncated-your-table SELECT [Current LSN],[Operation],[Context], [Transaction ID],[Transaction Name],SUSER_SNAME ([Transaction SID])[Culprit] FROM fn_dblog (NULL,NULL) WHERE [Transaction Name]='TRUNCATE TABLE'
I suppose one would have to check out the execution plan for a specific system and query to be sure. I only brought it up because I was chastised by a developer for `count(*)`. That said, I've only used it in dat exploration and not part of a query or stored procedure without explicitly picking the field by name I was counting.
I would also like to hear if they COUNT(1) is faster. I was told it's faster by my boss so I switched, but I'd like to know if it actually is.
Good idea, that may need to be our next move. We thought granting deny permissions to the service account in question would do this, but the truncation still occurred.
it's not so much filtering out GROUP BY is used to "collapse" multiple detail rows into a single group row thus when grouping my user_id, the MAX function will take the largest value in the detail column, *ignoring NULLs* -- because all aggregate functions ignore NULLs and of course MAX works on all kinds of datatypes including strings
One more thing learned today! Thanks!! I was under the impression that MAX works only with int and datetime types. Thank you so much.
Its there, you just dont see it. Its in the last line of the sample data. taskAttribDef sets attribid and name. Im thinking i need a correlated query for this. i tried putting on in the select clause something like (select charvalue from taskAttribChar tac where attribid =1 and t.taskid = tac.taskid) However, this returns nulls. But if i remove t.taskid=tac.taskid. itll return the same value for all records. Ignore highest attribid //entered date for now. im thinking taskAttirbChar will only have a single record for each attrib
Thanks for the suggestions! I came in to SQL by the back door. I was asked to be the guy that translates 'developer' to 'business' and back again, with a secondary job of helping the developers find issues with the data they output since I was on the business side for many years and understand what they need. In order to better understand, I started teaching myself SQL so I could run basic queries, learn WHY they can/cannot do something, and I am sure that there is a LOT I do not know yet. I am good with Top, Into, joins, insert, update, most common select statements but there are a lot of little things I haven't run in to yet since I am just playing with other people's data, not building or running the DB myself. I have zero formal training, everything I have learned so far has been by searching the web when I had a question. Sometimes that isn't as helpful when you don't know what you don't know. In the OP, I found the right answer, but I knew.. KNEW there had to be a better way, and that HAVING statement was just the ticket. Now that I know this place exists, I will be hanging out here a lot!
Try using a derived column 
~~What about using~~ ~~Or, better yet~~ ~~to catch any that might be longer.~~ My fault was pointed out to me! I completely spaced that it is strictly integer (not varchar like a phone number with dashes) LEN is not used with numbers. Try this instead where len(convert(varchar(10), phone)) &lt; 10
Then you'll want something like this: -- Note, the ORDER BY entereddate DESC will pick the most recent date SELECT t.taskid, MAX(CASE WHEN t.attribid = 1 THEN t.charval ELSE '' END) AS acode MAX(CASE WHEN t.attribid = 2 THEN t.charval ELSE '' END) AS lcode FROM ( SELECT taskid, rn = ROW_Number() OVER (PARTITION BY taskid, attribid ORDER BY entereddate DESC), attribid, charval ) AS t WHERE t.rn = 1 GROUP BY t.taskid 
Depends on what "free text" you are trying to insert. Is it dependent on the data inside the text file? If so, use derived columns. 
That's part of the reason I am using that, to filter out international numbers and extensions (creating data set for automatic marketing campaign in the US). Thanks for the heads up though :)
Not sure what platform but you will always have to make update 5 statements. Cause you cant update multiple tables at the same time. I suggest not using an IF or Case but using the where statement in the update to determine where to update . Something like. --update 1 update a set x = X from LLP a join master_table t on a.id = t.id where t.Area_of_Law = ' LLP --update 2 update a set x = X from Corporate a join master_table t on a.id = t.id where t.Area_of_Law = ' Corporate' edit: formatting
So I will have to do it this way. For some reason I was thinking I would need to use a Case or IF then statement. Thank you for responding! This is extremely helpful.
Check and see how many records you're getting rid of before doing this update. If it's a significant amount, and worth your time you get some extra numbers, you might be able to salvage some of them by using regular expressions to strip '-', 'x' and anything after it, and leading '1'. Also, please place me on your do not call list. Thanks!
You can use variables and use VB to fill them. Place them in the file with the sql.
Good to know, thanks
I have a REPLACE string that removed the typical "-", "(", ")" and spaces from the phone numbers. Do you think it would be better to then use your WHERE clause since the numbers are more or less "normalized?"
Depends on what the desired result is. The isnumeric(Phone) &lt;&gt; 1 will catch any weird thing that slipped trough your replace statement. 
Thanks guys...I'll give it a try when I get at my computer and let you know
I took a general assembly class in san francisco. It starts off with the basics, moves into creating tables, and using the like function. I'm now working on joins, groupby, and extract. There's other stuff like w3school and udemy has an interesting one for marketers. and sqlzoo is pretty good so far. What about you?
This is a significant misuse of the concept of `null`. Those phone numbers should be in a separate table, `JOIN`ed in, and incorrect phone numbers should simply be deleted. You will not be able to right or left join correctly across this table until this misuse is repaired. This should be handled in a trigger or a check constraint. This will blow up in your face when you try to go international. Even US numbers aren't ten digits internationally in some countries.
Sweet, thanks
you can use case expression: select count( case when absenceType = 'Present' then 1 end) as CountOfPresent, count( case when absenceType = 'Authorized Absence' then 1 end) as CountOfAuthorizedAbsent from attendance where studentID = 1 and absenceType in ('Present', 'Authorized Absence') 
Seems unnecessary to make those subqueries?
w3school worked awesome for getting me up and running. Refer to Microsoft's website for syntax help if your version differs from what w3 teaches (mine did). Also, if you're not able to get your hands dirty with a live database it will probably take you much longer to get up and running. I was using w3 alongside a production environment. Birth by fire.
Sorry friend, Thanks for reminding me. I Will add the PostgreSQL here also. PostgreSQL has also very good feature list. Will sure add this database also. 
Having cells full of no information is a design failure. That is a less problematic wrong encoding, but it's still a wrong encoding.
Pedantic note: it's "column", not "field" :)
Can you go into a little more depth about why having a NULL in a cell is such a bad thing? The reason I ask is that I am working on a database that is a clearing house for application access. Yearly, this database has: 300,000 employees who generate 6.5 million incoming requests which have to be validated, sorted, translated, assigned, transformed or otherwise manipulated for.. 6500 applications with more than 1 million individual entitlements resulting in an average of 13 million outgoing requests. And we have nulls in almost every table that I can think of....
I would probably use NULLIF instead of a WHERE. 
Ok, I can see where that makes sense... but in a table that stores data from multiple input systems and we have only partial control of what comes in (beyond the key required data) it is inevitable (or so it seems). I am guessing that is why they created ISNULL and NULLIF...
I know. I only said field because 'area of law' is a field on a form that gets exported to Excel to be updated by a user and sent back to update the database. 
Better to establish an explicit placeholder. Better still to write a data model where the absence can be treated as the absence of a row. Yes, there are situations where doing the wrong thing is the least awful way, but they're rare, and with some thought, there's usually a way through that maintains your data model's integrity.
Dude, use aliases!
You can import directly from csv using SQL developer or Toad as well
&gt; I can get each time frame individually but I don't know how to include them all in the same results SELECT t.DateOpened , t.SalesType , t.ItemCode , y.yesterday , w.wtd , m.mtd , x.ytd FROM daTable AS t LEFT OUTER JOIN ( SELECT Date , SalesType , ItemCode , SUM(Quantity) AS yesterday FROM daTable WHERE -- logic for yesterday GROUP BY Date , SalesType , ItemCode ) AS y ON y.Date = t.Date AND y.SalesTyp = t.SalesTyp AND y.ItemCode = t.ItemCode LEFT OUTER JOIN ( SELECT Date , SalesType , ItemCode , SUM(Quantity) AS wtd FROM daTable WHERE -- logic for week-to-date GROUP BY Date , SalesType , ItemCode ) AS w ON w.Date = t.Date AND w.SalesTyp = t.SalesTyp AND w.ItemCode = t.ItemCode /* similar code for mtd and ytd */ 
Edit: I think I figured it out. I didn't join the whole table and just did the Left Join with the WTD Select statement first. I removed the Group By date from the WTD statement and it worked. Thanks again! Thanks! I'm glad my code already looked somewhat like this. I was missing the initial Join of the entire table so everything would be included. Would it work if I started with yearly sales query and joined everything to that instead to avoid joining the entire table? Also, after implementing just the yesterday and WTD portions, I'm still getting two lines for items sold in both time frames. One line has the yesterday total in both columns and other has Null in yesterday and the WTD total in WTD.
https://msdn.microsoft.com/en-us/library/ms189081.aspx
Thanks, I wish we could. This service account is used in a LOT of places and a lot of different jobs. I changed the table name tonight and the truncation didn't occur (obviously) so now we wait til Monday for the teams to sift through their emails for, hopefully, a job failure email. Thanks everyone! Not sure what else we can do now. 
 Expanding one subquery as an example: "yesterday","WTD","MTD" and ""YTD" would refer to the relevant amount column(s) in the target table(s) select a.salestype ,a.itemcode ,b.amount as yesterday ,0 as WTD ,0 as MTD ,0 as YTD from sometable a ,someothertable b where a.primarykey = b.primarykey and to_date(b.date,'YY-MM-DD') = sysdate-1 ...and so on
It has been some years since I used MySQL, but I believe you can use variables to track this in a single select statement. I'm writing this on my phone, but the concept is create variables to store 7 days worth of sales quantity, another to store the last item number. Your data set is grouped and ordered by date and item number, joined against a date table so every day is represented. If item number is new, initialize variables to zero and set the last one to the current row quantity. If it is the same as the previous item, rotate values through the 7 variables (a=b, b=c, etc.) and set the last one to the current row's value. Add them all together and you have the 7 day sliding window total. In a different database software you could just use window functions like LAG() to accomplish this, so also maybe search for tricks to simulate LAG in MySQL if my idea above doesn't sound good. Good luck! 
I think you should be able to do it with some combination of window functions and a date dimension. It's too late for me to pull out my laptop and give it a shot but maybe tomorrow or Sunday I'll have something. 
Try http://www.sql-server-performance.com/ I don't use SQL Server anymore, but I was a forum mod there for a few years and learnt a huge amount from the site
ATM im using access to help me get the jist of all this. Im thinking this may be a reason why no ones suggestion seems to be working?
are you looking to argue with me or are you trying to be helpful? WTD was meant as a placeholder. 
you really are a pedantic POS aren't you?
Y U NO GIVE UP ALREDDY
obviously, which is why there's a group by on the outer query
so there is... but that makes the UNION query a lot more inefficient by not using SUM in each subselect
Thank you so much. I think what was throwing me off the most is the DBMS_OUTPUT strings. Those being gone helps this make the most sense. I really appreciate the help
The WHERE clause won't necessarily be evaluated in the order it appears in your statement. The optimizer will try to determine the fastest execution plan. Here's some basic info on how the optimizer works: http://dev.mysql.com/doc/refman/5.0/en/where-optimizations.html
The documentation is wrong and the error "ORA-00979: not a GROUP BY expression" is correct. It can't be used in a group by because it is not part of the grouping. LISTAGG is a special kind of function, hence why the syntax differs in regards to "Within group". Here's another approach to your problem: with qryRNum as ( select CUSTOMER_ID, ORDER_DATE, row_number() over (partition by CUSTOMER_ID order by ORDER_DATE desc) RNUM from TEST_TABLE ) select CUSTOMER_ID, max(ORDER_DATE) LAST_ORDER_DATE, min(ORDER_DATE) NEXT_TO_LAST_ORDER_DATE from qryRNum where RNUM &lt; 3 group by CUSTOMER_ID
Access is the devil. I suggest downloading WINSQL, it's free, and querying the access DB from there. You can set it to the DB file pretty easily.
I am pretty sure that the full evaluation is preformed because the where is not a conditional, but more like a filter. In TSQL, the where clause is performed after FROM, JOIN, and ON (the original table and the Cartesian products). http://tsql.solidq.com/books/insidetsql2008/Logical%20Query%20Processing%20Poster.pdf I'm no sure how MySQL handles it, but the very slow function might be evaluated on every single row because it's in the where clause. Consider using a derived table that has all other where conditions applied and then the bar=some_very_slow_function('foo') on this will run as few amount as timed as needed: http://sqlfiddle.com/#!9/97d7c/9 Additional response: http://stackoverflow.com/questions/22347065/does-mysql-run-all-where-conditions-if-first-is-false EDIT: this was supposed to be a response to OP, didn't mean or know I had hit reply to yours &gt;.&lt;
 So something like this? CREATE VIEW TEST_III WITH SCHEMABINDING AS SELECT VendorName AS Name, MAX(InvoiceDate) AS LastInvoice, SUM(InvoiceTotal) AS SumOfInvoices FROM dbo.Vendors V JOIN dbo.Invoices I ON V.VendorID = I.VendorID WHERE PaymentDate IS NOT NULL GROUP BY VendorName; SELECT TOP 10 SumOfInvoices, Name FROM TEST_III ORDER BY SumOFInvoices desc; 
I don't see a CTE anywhere. They're creating a View with schemabinding. 
As it is, that code won't work to create a view for two reasons: 1. Views cannot have &gt;1 result sets/outputs 2. That second select is referencing an object that doesn't exist yet (TEST_III), creating a circular reference. Here's what I think it should be (a combination of your two select statements): CREATE VIEW Top10PaidInvoices WITH SCHEMABINDING AS SELECT TOP 10 VendorName AS Name, MAX(InvoiceDate) AS LastInvoice, SUM(InvoiceTotal) AS SumOfInvoices FROM dbo.Vendors V JOIN dbo.Invoices I ON V.VendorID = I.VendorID WHERE PaymentDate IS NOT NULL GROUP BY VendorName ORDER BY SumOFInvoices desc; 
By the ANSI standard, [short-circuit evaluation is implementation-dependent](http://stackoverflow.com/questions/789231/is-the-sql-where-clause-short-circuit-evaluated). I can't find anything about MySQL, but the [MariaDB doc](https://mariadb.com/kb/en/mariadb/operator-precedence/) talks about short-circuit evaluation. Relying on it like they suggest there with `SELECT some_function() OR log_error();` seems like a great way to shoot yourself in the face, however.
True.. My bad, I just saw the WITH and the 2 SELECTs in a view and my brain turned it into a CTE. All I was saying was that it should all be in the one statement.
That dang schemabinding :)
Damn it! That totally worked. Thanks! 
MySQL - sorry!
[Cumulative sum](http://stackoverflow.com/questions/2563918/create-a-cumulative-sum-column-in-mysql) approach is given in the answers section of that link.
That query gives us min = 1 max = 14, average = 4.003 count = 8666022 
MySQL doesn't support windowed analytic functions such as ROW_NUMBER.
This means that getting the steps for each result is the slow part ...
What's your budget? I like Business Objects but I don't know how much auto generation of code there is.
This part of a weekly class we are using to train our Software Engineers at [Emergency Reporting](http://www.EmergencyReporting.com). Please keep discussions on Reddit and thanks to those that watched it live!
If you're using MSSQL already, SSRS should be your first stop, and look elsewhere if there's something it lacks. Be very cautious about letting clients create their own reports; one bad query in a report and they can wreak all kinds of havoc. Look into shared datasets and give them access to *those* to generate reports from. That way you can control &amp; tune the queries.
Petaho or Jasper 
You have to connect before you issue a query; try reversing the order of the first two statements.
i think you got the bad/good tables backwards in your query try this -- SELECT * FROM information_schema.TABLES good LEFT JOIN information_schema.TABLES bad ON bad.TABLE_SCHEMA = good.TABLE_SCHEMA WHERE good.TABLE_NAME = 'foo_table' AND bad.TABLE_NAME IS NULL; 
or maybe i don't understand what you're doing...
Thanks !
buy the book "Learn SQL in 10 Minutes". It's a pretty quick and good reference for beginners
The OVER(PARTITION BY ...) clause should do what you want. Take a look at an example over here: http://www.tutorialized.com/tutorial/How-to-Use-the-OVER-Clause-in-SQL-Server-2008-R2/68289
Unfortunately, it looks like MySQL doesn't support that.
 select t.*, s.SUM_AMOUNT from TABLE t join (select LOCATION_ID, SUM(AMOUNT) as SUM_AMOUNT from TABLE group by LOCATION_ID) s on s.LOCATION_ID = t.LOCATION_ID
I learned the leading comma working in Chartio. A great BI tool. They offer drag and drop query creation for simpler tables and will show the auto generated sql, and that's how I noticed it . Now when I write my code, a leading comma is always used, love it.
Thanks. Such a weird query for something that should be easy.
Did you make sure to specify RESTORE WITH RECOVERY on the first one?
Thanks, I'll have to experiment later. I'm aware of cross joins just have never needed one so have no exp with them. I'll let you know how it goes.
You're partly right, and thanks for the shot. I had tried every possible combination of joins and clauses in my tool belt but all came up nil, I wasn't going to bother listing them all for sake of brevity. I think the problem is more complex than it seems at a glance. Going to try cross joining later.
I am not too familiar with sqlcmd. I just right clicked on the 'databases' folder under my instance I wanted to restore to and used the 'Restore Database...' wizard. 
Alternatively, you can get the same information with this one: select location,amount,sum(amount) as sum_amount from table_1 group by location,amount; But the id won't be related so the output is different. So if the id column is important, you need to join the tables like /u/ziptime recommends. 
It means that your subquery is returning more than 1 row for some value of column1 in tableA. The below query will help you identify which value that is. SELECT A.COLUMN1, COUNT(*) FROM TABLEA A, TABLEB B WHERE A.COLUMN1 = B.COLUMN3 GROUP BY A.COLUMN1 HAVING COUNT(*) &gt; 1
Success! Thanks a ton. 
I'm not going to flatout give you the sql, since this is homework, but here's some tips. ____ For the specific errors you're getting (they are both pretty common for new SQL users): &gt; keep getting the error "ORA-00904: "STUDENTS_SERVED": invalid identifier" You've assigned the alias "STUDENTS_SERVED" in the `SELECT` clause and are trying to use in the `GROUP BY`. `GROUP BY` is evaluated before `SELECT`, so Oracle doesn't know what "STUDENTS_SERVED" means yet. You can't use an alias there, or in almost any part of the query for that matter. It only affects how the column is named in the result set, or how the column is referenced in the parent query when used in a subquery. &gt; if I change the GROUP BY to something else like INSTRUCTOR_LAST_NAME I get the error "ORA-00979: not a GROUP BY expression" Any field that's not listed in the `GROUP BY` needs to be aggregated. Otherwise, there column be more than one value for the group, but only one record would be returned. How would Oracle know which one to pick without aggregation? ____ Having said that, you should be grouping by INSTRUCTOR_ID, not STUDENTS_SERVED. Think of it like this: you want to sum ENROLLMENT for all rows that have *what* in common? INSTRUCTOR_ID, because it uniquely identifies an instructor. But since we're using GROUP BY INSTRUCTOR_ID and we want to return other non-aggregated fields (name, department), we can't just stick it all in a single query (you'll get the "not a GROUP BY expression" error). Instead: 1. Write a query that returns INSTRUCTOR_ID and sum of ENROLLMENT (aliased as STUDENTS_SERVED). 2. Make that a subquery (in the `FROM` clause) and join it to INSTRUCTOR, then select all the fields you need to return. A Because STUDENTS_SERVED is in a subquery, you can use that name in the parent query and Oracle will understand it. And because the GROUP BY is handled in the subquery, you can select the fields you need without worrying about aggregation. 
I understood most of what you instructed me. But I do not understand or really know what you mentioned in the item listed in 2. on how to create a subquery in the FROM clause then *join* it to INSTRUCTOR. You then go on mention I need to select all fields I need to return. I'm a bit lost with this statement. 
 select --insert all the output fields here from INSTRUCTOR table1 join (select --insert the subquery here ) table2 on table1.INSTRUCTOR_ID = table2.INSTRUCTOR_ID The only thing that the subquery should return is INSTRUCTOR_ID and STUDENTS_SERVED.
Do you need to keep the RecID? If not, then you could simply delete the old data and load the table from the spreadsheet.
What I would use is a Merge statement. https://msdn.microsoft.com/en-us/library/bb510625.aspx 
What are you trying to see? The owner and security settings on the files?
Sometimes the dumbest way to do things is also the easiest. If this is a one-time data load sort of thing, I'd just use the CONCATENATE function in Excel to build the UPDATE statements, then copy &amp; paste those into SSMS. So in your example, if the data is in columns A through D and starts at row 2, I'd paste this function into cell E2: =CONCATENATE("UPDATE MyTable SET FirstName = '",B2,"', LastName = '",C2,"', Address = '",D2,"' WHERE SID = ",A2) Then double-click the bottom-right corner of that cell to generate the UPDATE statements for the rest of the rows. Not an elegant solution, and if there are data integrity concerns (e.g. you need to make sure you don't overwrite anything already present) I wouldn't use this method. But, it's quick and easy.
 Select frequency, maxdata From capture_data cd inner join (select max(data) as maxdata FROM capture_data WHERE captureid = 41203 AND frequency BETWEEN 946 AND 956) as a on a.maxdata = cd.data WHERE captureid = 41203 This is going to return extra rows if there is more than one matching data value with the captureID, but I guess the frequency column means that the data has already been summarised so that captureID and data are unique. I'm Maria-illiterate so the correct use of quotation marks has been left as an exercise for the reader.
Thanks for the gold!
I'm using the 70-462 Training kit book (comes with a discount voucher for the exam and a pdf version included) in conjunction with the virtual acadamy Microsoft has at [Virtual Acadamy 70-462 Jump Start](http://www.microsoftvirtualacademy.com/training-courses/administering-microsoft-sql-server-2012-jump-start). I've done the same for the 70-461 and recently passed it so now moving on to this, I felt that this is enough to get you above the passing mark for the exams. Just know that the practice exams with the book don't even come close to the real exam anymore.
Both queries will scan the Dept table, and for each Dept the _second_ query will need to look @ every row in the employee table, whereas the first query will only need to scan all employee records for departments with no employees, otherwise it'll stop at the first department employee.
The theoretical reason EXISTS would perform better is: * The sub-query will produce only the DISTINCT results because the Departments are already a distinct set, the only thing you are doing by using EXISTS is filtering the departments that have employees. * The sub-query by its design has a FOREIGN KEY to the department, so it doesn't require a temporary table to figure out the unique/DISTINCT set which will make it faster. * The FOREIGN KEY will almost always have a non-clustered index on it which means it will do a seek over a scan, which is also more efficient. *Edit - As an additional note, this will ONLY be more efficient/possible for for one-to-many relationships*.
Firstly, please practice to use the new ANSI join syntax ( [type] join &lt;set&gt; [on &lt;condition&gt;]) - it is much cleaner and easier to manipulate. Secondly, for this task you want to look into subqueries and NOT IN/NOT EXISTS conditions.
&gt; Oracle's update is not very advanced, to say the least I don't agree with that at all, Oracle's update is advanced and more feature rich than most RDBMS and supports ... * Update - all rows * Update - filtered rows * Update - based on a single queried value * Update - based on a query returning multiple values * Update - based on the results of a SELECT statement * Update - correlated, single column * Update - correlated, multi column * Update - Nested table * Update - object table * Update - based on record (set Row = ...) * Update - partitioned I'm a fan of merge, but your query can be written as an update and it will perform better than your merge... update WAREHOUSE.PAID_CLAIMS c set C.CODE_POS = (select max(CDE_POS) from ICN_UPDATE U where U.NUM_ICN = trim(c.NUM_ICN) group by NUM_ICN) where c.CODE_POS is null Remember, merge will fire any insert and update triggers, even if it is only performing an update.
Oh that makes sense! thanks you !
Thanks
 SELECT writer , COUNT(*) FROM articleViews GROUP BY writer 
 SELECT F.title , COUNT(X.film_id) as actors FROM film F LEFT OUTER JOIN film_actor X ON X.film_id = F.film_id GROUP BY F.title ORDER BY actors DESC it's important to count an X column -- this allows for a count of 0 
I realized I had some mistakes on this post. I edited it. Can you please take a look again? Becuase it's not working.
You are right, I tried SQL Fiddle and it worked. I am getting an error on my site though, probably becuase I am trying to convert it into JSON. I will figure it out. Thanks guys!
I'm going to be lazy and copy and paste from your query, but as /u/ichp notes you should probably be using the INNER JOIN method of adding tables. Lose the artist name filters in your query. This won't have the desired effect as it will remove just those tracks from the playlist, not the playlist from your selection. Replace with something like: AND playlist.playlistID NOT IN (select distinct playlist.playlistID from playlist, playlisttrack, track, album, artist where playlist.playlistid = playlisttrack.playlistid AND track.trackid = playlisttrack.trackid AND album.albumid = track.albumid AND artist.artistid = album.artistid AND artist.name = 'Chico Buarque' AND artist.name = 'Black Sabbath')
If you want the results from a single table, then select from that table and exclude records via filter conditions. It reads better and in many cases it performs better too. Why does it perform better? Because joins take work! Also, DISTINCT typically uses a sort operation (or a hash) which typically takes O(n log n) - i.e. doubling the records takes more than double the effort. In the original example, we want all the departments that have at least one employee. "All the departments ..." SELECT * FROM DEPT d "... that have at least one employee" WHERE EXISTS ( SELECT NULL FROM EMPLOYEE e WHERE e.DEPT = d.DEPT )
&gt;select DISTINCT on a join is a universal hack and is bad because it implies you're selecting too many rows, and then going to do extra work to get rid of the rows you shouldn't have selected in the first place. That depends on the situation, there are occasions where pre-filtering too many data sets will cause huge performance overheads when using many sub-queries. As long as you are filtering to your final set before performing DISTINCT that won't always be the case. EXISTS can also cause locking problems with serialized transactions; where DISTINCT utilizes additional space, generally, will not. &gt;1→∞, 1→1, ∞→1 These have equivalences or are subsets of 1→∞, so that would be expected. &gt; ∞→∞ Absolutely not, the worst case runtime of DISTINCT is O(nlogn) with memory overhead of n (this is usually the case, and why EXISTS is faster for 1-to-x relations), the worst case for utilizing EXISTS for this situation is O(n²). Not to mention the huge memory overhead for maintaining the sub-queries of an addition n². Now you also have to consider the data in this case, sparse data sets will probably still be faster. &gt;What's more legible?.... That you can see the intent of the query by looking at it, the person obviously is looking for a unique set when utilizing DISTINCT, where using EXISTS could be utilized for many other purposes. Is it best practice or the most efficient? Maybe not, but for maintenance purposes it is much easier to see the intended result. &gt;Meticulous? Ha ha ha, NO, that's called good programming discipline and is central to ALL development. &gt; Good programming disciple Now it's my turn to laugh. When there are jobs that span several thousand lines and dozens of stored procedures, you have to be meticulous. I'm not sure what magical land you live in, but your average developer does not know, nor cares/wants to know about efficient or correct querying unless it directly effects them. They want the correct data, and as long as it's not abysmally slow they don't care how it is pulled. This is infinitely more relevant when dealing with external vendor software that couldn't be bothered to use good practices. 
Define "rank". What is it based upon? What does your data look like, and what do you want the results to look like? If a value in a table can change based upon other data in the same table, you probably should not be doing this in the table itself, but rather when you access the data as part of the query.
I have data like Id | name | sales | rank I want that every time when I update sales value, rank value would change (just like in excel) Is this possible ? Rank is depended from sale values. The higher sales - the lower rank. A row with highest sales value would have 1 in rank field.
Unfortunately, your query is also not equivalent to OP's because notice that he only updates those rows where the CLM_ICN field has a corresponding value in ICN of the same table. Your merge however will update all rows that are found in ICN_UPDATE which completely disregards that filter.
I think we need to see the rest of the existing query/view and need to know the datatype of specificflag and most importantly what variant/platform you are using. There's an implicit conversion happening somewhere for some reason. e: does CASE WHEN cast(specificflag as varchar) = '1' THEN 'Yes' WHEN cast(specificflag as varchar) = '0' THEN 'No' ELSE cast(specificflag as varchar) END AS columnalias work?
You would want to look at a Trigger to do this.
Turns out it was being caused by that else statement since it would have returned an integer, removing it (since there is no else, everything is either 1 or 0) or changing it to something else (else 'blank' for example) allowed the query to run without an error. Thanks for taking a moment to offer some help, it's greatly appreciated!
It sounds like the more appropriate solution would be to create a view containing a calculated field for rank instead of storing it. We really need to know more about the data and the platform to give you specific advice.
Your database is not Excel. Don't treat it or think of it the same way. /u/Adventux suggests a trigger, and that's *a* way to achieve this, but there are performance penalties and other gotchas that come with triggers. What would be better is to **not** keep the rank in the table at all - IMHO it's not appropriate to keep it in a transactional table (think about it - every time you perform an insert or an update, you're having to perform at least **one more** update on the table - including doing a recalculation of **everyone's** ranking. This will not scale well to thousands (or more) of records on a busy system). Instead, calculate your ranking only when you need it - when you query the table. You can do this very easily (and in the query that defines the view, if you go with /u/fauxmosexual's suggestion there which isn't a bad way to go at all) with a [window function](http://en.wikipedia.org/wiki/Select_\(SQL\)#RANK.28.29_window_function): select id, name, sales, rank() over (order by sales desc) as sales_ranking; Keeping the ranking on the table would be more appropriate if this was an analytics/reporting database/table, where it's historical data that has been pre-calculated by some other process and doesn't change.
 select * from @sample where isnumeric (@mycolumn)&lt;&gt;1
&gt;Just know that the practice exams with the book don't even come close to the real exam anymore. I'm soon to take this exam as I'm doing pretty well on the book's practice exam. Is the real thing much more difficult?
&gt; the worst case for utilizing EXISTS for this situation is O(n²) AFAIK a many to many EXISTS would use a sort hash merge join which has a big O notation of O(n log n + m log m). Have you got a link to where you got O(n²)? &gt;That you can see the intent of the query by looking at it, the person obviously is looking for a unique set when utilizing DISTINCT, No, you are referring to a table you aren't selecting from, and just as /u/in8nirvana and many of the more respected DB article writers online (e.g. Thomas Kyte, Vijaya Kadiyala et al) would concur, EXISTS is the better approach. &gt; When there are jobs that span several thousand lines and dozens of stored procedures, you have to be meticulous. Yes I know, I head up a team on enterprise level financial DBs (100s of TB), 100,000s of lines of code and business objects, billions of records. No-one I work with would advocate a DISTINCT join, because it's a poor approach, simple as. We need to be disciplined because performance, efficiency and correct approaches matter. You are promoting bad practices for people on here who may not know any better.
No you would not.
It's a process of elimination. 1) Try removing the order by and see what sort of difference that makes. select * from #temp; OUTPUT to {DATADIR}\{CODE}{PULLDATE}TKT{FILETYPE}.TXT DELIMITED BY '\x09' QUOTE ''; It could be that the new server doesn't have enough in memory sort space and is having to page a lot. Also VM servers can have poor IO if not configured correctly. 2) Get the vendor to run the output locally on the SQL Anywhere instance, this will determine if it is the ODBC connection. 3) Try a different version of the ODBC driver, like an older one or a 32 bit one. 4) Get a packet sniffer and watch the rate of the packets through the ODBC connection. 
Creating a view would do the trick. It would look exactly like a table as far as accessing it would go, as the server itself would be doing all the ordering and numbering. From your point of view in your java app you would still be saying "show me records 10 and 100", you'd just be taking them from the view instead of the table. Think of a view like a virtual table that can include calculated fields. e: this assumes you are able to create objects in the database you are taking the data from If you wanted the field to be updated on a scheduled basis you'd be better to use the SQL Agent (in MSSQL anyway) to run an update statement as a job. 
Thank you. 
Okay this is an ugly, ugly hack and if nobody can do better I'll be disappointed in /r/sql: SELECT Name FROM Researcher WHERE Name in (SELECT R.Name FROM Researcher R INNER Join Author A on A.Name = R.Name INNER JOIN Paper P on P.Title = A.Title WHERE Year = 2005) AND NAME in (SELECT R.Name FROM Researcher R INNER Join Author A on A.Name = R.Name INNER JOIN Paper P on P.Title = A.Title WHERE Year = 2006) AND NAME in (SELECT R.Name FROM Researcher R INNER Join Author A on A.Name = R.Name INNER JOIN Paper P on P.Title = A.Title WHERE Year = 2007) AND NAME in (SELECT R.Name FROM Researcher R INNER Join Author A on A.Name = R.Name INNER JOIN Paper P on P.Title = A.Title WHERE Year = 2008) AND NAME in (SELECT R.Name FROM Researcher R INNER Join Author A on A.Name = R.Name INNER JOIN Paper P on P.Title = A.Title WHERE Year = 2009) AND NAME in (SELECT R.Name FROM Researcher R INNER Join Author A on A.Name = R.Name INNER JOIN Paper P on P.Title = A.Title WHERE Year = 2010) AND NAME in (SELECT R.Name FROM Researcher R INNER Join Author A on A.Name = R.Name INNER JOIN Paper P on P.Title = A.Title WHERE Year = 2011) AND NAME in (SELECT R.Name FROM Researcher R INNER Join Author A on A.Name = R.Name INNER JOIN Paper P on P.Title = A.Title WHERE Year = 2012) AND NAME in (SELECT R.Name FROM Researcher R INNER Join Author A on A.Name = R.Name INNER JOIN Paper P on P.Title = A.Title WHERE Year = 2013) AND NAME in (SELECT R.Name FROM Researcher R INNER Join Author A on A.Name = R.Name INNER JOIN Paper P on P.Title = A.Title WHERE Year = 2014) AND NAME in (SELECT R.Name FROM Researcher R INNER Join Author A on A.Name = R.Name INNER JOIN Paper P on P.Title = A.Title WHERE Year = 2015) Honestly I'm a little embarrassed about posting that, there must be a better way. e: I'm leaving this here under /u/mikeyd85 much better solution as a testament to bad SQL. Don't actually do this.
You just need to do your filtering in a subquery: Select a.* From tablename a INNER JOIN (select ID, Criteria from tablename where criteria = @yourcriteria) b on b.id = a.id
Something like this.... (untested) select R.NAME FROM RESEARCHER R where 10 = (select count(count(null)) from PAPER P join AUTHOR A on P.TITLE = A.TITLE where A.NAME = R.NAME and P.YEAR between 2005 and 2015 group by P.YEAR) The count(null) will return a 0 if at least one record exists for that year, so count(count(null)) will count the number of those. So 10 is at least one for each year. Alternatively, 10 = (select count(distinct P.YEAR) ...
Yeah, but he put: *"Author.name is a foreign key for Researcher and Author.title is a foreign key for Paper."* in the op.
Awesome, thanks so much. I tried a subquery but it seems I got the order wrong and swapped the ID and criteria around. That would explain it.
[honest question] why do I always see mysql users doing joins to select statements but rarely in other DB's?
Dunno, I'm not a MySQL user at all. I'm self-taught so I probably have a weird approach to things.
I prefer the "in (select" approach here because you aren't selecting anything from the joined table.
Well, with WAMP can't you build a database and then screw around with it?
What version of SQL are you using regularly? You could install the SQL Server 2012 Express edition free, and then get the adventureworks database