I'm assuming you've already looked at Microsoft's comparison chart, and are wondering what features are most interesting. I think it's relative to your business, but the ones I like are the performance improvements, column store indexes, new SSIS deployment model, in-memory tables and new monitoring tools. Others will probably chime in with their favorites.
The longer support lifecycle is kind of a big deal. Mainstream support for 2008R2 **ended two years ago** and Extended Support will end in three years - will this system be around in 3 years (my money's on yes)? Extended Events are better than Profiler. The new Cardinality Estimator in 2014+ has made many queries faster (caveat: it's also ruined others). 2014+ Standard Edition supports 128GB RAM vs. 64GB in 2012 and earlier Beyond that, we'd need to know the specifics of your usage/requirements to pinpoint exact features that you'll need/want. Personally, I see little to no value in starting a *new* implementation with a database platform that's four versions behind current.
On Windows, I use PowerShell for this sort of thing. It's easy to switch between databases or servers and stitch the multiple results eta into a single report.
You've piqued my interest. Can you point me to a good example/tutorial by chance?
Hello again /u/eStonez. Yes, I think I now understand that using a solution like ours we are not going to see more than one resource changing the same data. However, it is not possible to do more than one user insert in the 'Booking in progress' table at the same time? For example, let's say that we got just one ticket left. If two users do the conditional at the same time both of them are going to be inserted into the 'Booking in progress' table.
You can do that by using the pending table suggestion that /u/notasqlstar and I are recommending. Put everything in the pending table with the transaction create date. If they take too long to complete the transaction release the tickets back into the pool by cancelling their pending reservation. /u/notasqlstar suggests you delete them from the pending table. I personally would just create an (isActive) indicator that gets changed to 0 after 5-10 minutes in that table and the transaction not being completed. Doing it this way saves the data so you can see how long it takes people to actually complete the transaction on average and tweak the reservation hold period to offer the best user experience.
I recommended he deleted them after inserting them into a housing table for transactional history. If the pending table is going to be used by multiple users with a possibility that two users are accessing the same record then I wouldn't want that table to ever get bigger than at best a million rows. Something in the 10k range is sexier. 
I see. It is a good point. I thinking about periodically moving this to a historical table.
That sounds about perfect. I'd probably append whenever the event closes. Leave them in the table temporarily so you could quickly flag it back to 1 allowing them to resubmit if a reservation opens up due to another time-out or a cancelled reservation.
My only fear would be selling more tickets that the batch can offer.
As long as you process oldest to newest and 1 transaction at a time. The only thing that matters is Available count -(Finalized Count+Requested Count) As long as it is &gt;=0 you can attempt to process the sale. If it results in a negative number,set the IsActive flag to 0 return a message saying "Sorry only X tickets are available." Then move onto the next line.
Hello again! I was too dumb and lazy to explain the whole scenario. What I'm facing in this process is the ticket reservation. The payment will happen after the reservation occurs. The user will be then redirected to a page to authorize the payment (inputting credit card information, for example). In case the user close the page or cancel the process the order will not finish, the transaction will expire and the ticket slot will be open again. The issue that I'm having is that I need to confirm that I got a ticket (or tickets) available for the customer before redirecting him to a payment page. Like I said in my previous comment, my fear would be authorize the user to something that we don't got. I'm using a payment API little limited, so I don't have features like the [authorization from Paypal](http://stackoverflow.com/a/36052599/1459499) that would allow me to try to do the transaction after the user leave the page.
If you use SSIS the 2012 version was a major overhaul, and is much easier to use, deploy, and debug in dev/qa/production environments.
No they won't, that's why I said "Conditional Insert". Insert 1 tickect to reservationTable (inner join with your ticket table) where (dynamic available ticket quantity value) &gt;= 1 AFAIK, no matter how hard you try to post two insert at the same time from two different sources, first query will insert and second query won't because the (dynamic available ticket quantity value) for the second person will be zero, it is impossible for the RDBMS to insert the data and not recalculating at dynamic value.
In 2014 the managed backup to azure blob storage is pretty awesome if you use azure. 
Something like this: $servers = "prod1\MSSQLServer","prod2" foreach ($server in $servers) { invoke-sqlcommand -query "select blabla" -serverinstance $server }
I don't see how dynamic sql would help you when having to run on multiple databases. Are these on the same server? 
SQLComplete by devart. Great product, both free and paid versions. 
I think they've improved intellisense a lot on the spring release of SSMS 2016. You should give it a shot. Edit: I just used it for the first time today via VPN instead of being plugged in and it's worthless again.
This was the answer. Im such a fool! Thanks /u/nvarscar
brute force SELECT [ASSIGNEE_FULLNAME] , SUM ( CASE WHEN [TITLE] LIKE '#%' THEN 1 ELSE 0 END ) AdjustedCount COUNT(*) TotalCount FROM [Rptg_Test].[dbo].[_Extract_export_b$] GROUP BY [ASSIGNEE_FULLNAME] 
This worked perfectly, thank you.
You've got some 'wrong' symbols in the query: “” and ‘’. Probably because of being copied from Word. Change them to " and ', that might help.
Your tblowners has the postal code set to 7 characters but this value - RG12 8JK is 8 characters include the space. 
I did find someone experiencing a similar situation - not able to delete a maintenance plan - here were the steps they used to remove the plan: 1. Select the ID with the select statement select * from sysmaintplan_plans 2. Replace with the selected ID and run the delete statements delete from sysmaintplan_log where plan_id = '' delete from sysmaintplan_subplans where plan_id = '' delete from sysmaintplan_plans where id = '' 3. Delete the SQL Server Jobs with the Management Studio
need to specify the msdb to use these but this is a good way to remove an orphaned plan. select * from msdb.dbo.sysmaintplan_plans
Changed it, still unfortunately not working :c
This comment has been overwritten by an open source script to protect this user&amp;apos;s privacy. It was created to help protect users from doxing, stalking, harassment, and profiling for the purposes of censorship. If you would also like to protect yourself, add the Chrome extension [TamperMonkey](https://chrome.google.com/webstore/detail/tampermonkey/dhdgffkkebhmkfjojejmpbldmpobfkfo), or the Firefox extension [GreaseMonkey](https://addons.mozilla.org/en-us/firefox/addon/greasemonkey/) and add [this open source script](https://greasyfork.org/en/scripts/10380-reddit-overwrite). Then simply click on your username on Reddit, go to the comments tab, scroll down as far as possible (hint:use [RES](http://www.redditenhancementsuite.com/)), and hit the new OVERWRITE button at the top.
/u/nvarscar like this? http://imgur.com/ayXOsat
You still have those curvy (") symbols. Try to replace them with regular ones using replace function in the editor.
trying to add this : http://imgur.com/K3pLxie
This comment has been overwritten by an open source script to protect this user&amp;apos;s privacy. It was created to help protect users from doxing, stalking, harassment, and profiling for the purposes of censorship. If you would also like to protect yourself, add the Chrome extension [TamperMonkey](https://chrome.google.com/webstore/detail/tampermonkey/dhdgffkkebhmkfjojejmpbldmpobfkfo), or the Firefox extension [GreaseMonkey](https://addons.mozilla.org/en-us/firefox/addon/greasemonkey/) and add [this open source script](https://greasyfork.org/en/scripts/10380-reddit-overwrite). Then simply click on your username on Reddit, go to the comments tab, scroll down as far as possible (hint:use [RES](http://www.redditenhancementsuite.com/)), and hit the new OVERWRITE button at the top.
This is actually can be replaced by WHERE CAST(DateTimeField AS date) = CAST(GETDATE()-1 AS date) , but yes, that would give you the result set for yesterday. But it's better to use a SARGable condition instead: WHERE DateTimeField &gt;= CAST(GETDATE()-1 AS date) AND DateTimeField &lt; CAST(GETDATE() AS date) In this case, the query can utilize indexes.
WHERE DATEDIFF(day,SentDate,LogDate) &gt;=1 AND LogDate &gt;= '20160101'
This seemed to do exactly what I needed! Thank you. 
Excellent. Glad I could assist.
Some queries might get a new execution plan after upgrade and utilize more disk resources. You may check sys.dm_exec_query_stats and search for queries with biggest i/o. Also, check perfmon - Phisical disk stats, especially, Avg Queue Length, Disk sec/read, Disk bytes/sec and make sure that the throughput is decent and queue and latency are indeed higher than usual. Check active sessions and make sure none of them is the culprit - some rogue session might be loading the disk: untimely maintenance/poorly written query/bad query plan.
 SELECT Person, MIN(ID) FROM Table GROUP BY Person
DrTrunks has got it right, but lemme expand on that a little bit. These queries are contrived and dumb, and so are my server names. Of course, you could replace the "SELECT blahblahblah" with "execute sp_something 'foo' bar' ", as part of some utility script. &lt;# First, get powershell configured. #&gt; &lt;# Second, make sure that you have a nice utility script to use to interact with SQL invoke-sqlcmd comes with the SQL admin tools, IIRC You can also use Chad Miller's Invoke-SQLCmd2. IIRC, that will work without having all of the SQL tools in the world installed. I'll use invoke-sqlcmd2 for these examples; it's parameter list is similar to invoke-sqlcmd #&gt; &lt;# Example #1: Something super-simple #&gt; Invoke-Sqlcmd2 -ServerInstance:moe -Database:master -Query:"select name from sys.databases" &lt;# Example #2: Still simple, just parameterized #&gt; $srv = "moe" $dbn = "master" $qry = "select name from sys.databases" Invoke-Sqlcmd2 -ServerInstance:$srv -Database:$dbn -Query:$qry &lt;# Example #3: Slighly fancy, more than one server. Make sure that you put the server name in the query, so you know where you are #&gt; $servers= @("moe", "milo", "morpheus","morpheus\r2") $dbn = "master" $qry = "select @@servername ServerInstance, SERVERPROPERTY ('ProductVersion') as ProductMajorVersion " foreach ($srv in $servers) { Invoke-Sqlcmd2 -ServerInstance:$srv -Database:$dbn -Query:$qry } &lt;# Example #4: The same, but different. Just one server, more than one database. Make sure that you put the db name in the query, so you know where you are #&gt; $servers= @("morpheus") $dbs = @("master", "AdventureWorks2008","ReportServer") $qry = "select @@servername ServerInstance, db_name() DatabaseName, SERVERPROPERTY ('ProductVersion') as ProductMajorVersion " foreach ($srv in $servers) { foreach ($dbn in $dbs) { Invoke-Sqlcmd2 -ServerInstance:$srv -Database:$dbn -Query:$qry } } &lt;# That should get you started. In the end, it would probably be be better to read servername-databasename pairs from a CSV file (or a SQL table) and just loop through those. #&gt; 
Going by your expected results, it's not a GROUP BY you're after but an ORDER BY. GROUP BYs are used with aggregate functions like max, or avg, but I don't think you need one of them as that would mean each person only appeared once. SELECT Person, ID FROM table ORDER BY Person, ID; So it sorts the result first alphabetically by name (grouping the lm together) then by the ID. 
I'll start with a few of mine (I code in MSSQL/T-SQL, with SSMS and [SSMS Tools Pack](http://www.ssmstoolspack.com/Download) addon): Note, SSMS Tools reads the {C} part as where it leaves your cursor after text replacement, so it makes a bit more sense. **IIV** - "Insert into values" - Just a quick little shortcut so I don't screw up the parentheses. insert into {C}() VALUES () **LOOP** - Sets up a while loop that I can then modify quickly. declare @max int = 1 declare @i int = 1 while @i &lt;= @max begin {C} set @i = @i+1 end **XMLC** - Sets up a "for xml" group concatenation column for my most common usage of it: to get a comma separated list of columns from a table. I can of course change it to suit my actual table usages but it gets the syntax close. ,xmlc = substring((select ','+CAST(B. as varchar(max)) from {C} B where B.TABLE_SCHEMA = A.TABLE_SCHEMA and B.TABLE_NAME = A.TABLE_NAME order by B. for XML path ('')),2,999999) **BAK** - Runs a custom made table backup Stored Procedure, which basically just runs a select * into and appends the db, schema, and date to the tablename, placing it in our "play" schema for safekeeping in case I screw something up. exec dbo.BAK 'DestinationSchema','SourceSchema','SourceTable','AppendSuffix'; **RUNSQL** - Sets up a cursor to execute dynamic sql generated from a select statement... People who work in production environments will look at me like I have three heads but it works for my job, so nya. declare runsql cursor for {C} open runsql declare @sqlexec varchar(max) fetch next from runsql into @sqlexec while (@@FETCH_STATUS = 0) begin exec (@sqlexec) PRINT 'EXECUTED "'+@sqlexec+'"' fetch next from runsql into @sqlexec end close runsql deallocate runsql go 
If this were PostgreSQL you could use an array. https://www.postgresql.org/docs/9.5/static/arrays.html You should be able to mimic this behavior with a JSON data type http://dev.mysql.com/doc/refman/5.7/en/json.html But if I understand what you are trying to do it would be something like this create table Users ( UserKey int PRIMARY KEY, UserName varchar(32) ); create table Combinations ( UserKey int, SubCombinationKey int, SequenceNumber int ); create table SubCombinations ( SubCombinationKey int, SequenceNumber int, IntValue int ); select u.UserName, c.sequencenumber, sc.sequencenumber, sc.intvalue from testing.Users u inner join testing.Combinations c on u.UserKey = c.UserKey inner join testing.SubCombinations sc on c.SubCombinationKey = sc.SubCombinationKey order by u.UserName, c.sequencenumber, sc.sequencenumber; you end up with something like this. user p1 p2 val Bob 1 1 55 Bob 1 2 78 Bob 1 3 78 Bob 2 1 7896 Bob 2 2 563 Bob 3 1 32 If you are familiar with arrays it would be be this Bob[1][1] = 55 Bob[3][1] = 32 Bob[2][2] = 563 ps. did i do your homework? 
I'm familiar with arrays from another context; I see what you're suggesting and its the concept I had nagging at the back of my mind. I'll go read those links now. ps. nope, not homework. I volunteered to rescue someone's website and then got asked 'hey, can you maybe add these features'. And since it's been years since I did *any* code, I don't trust that I am not missing the obvious, sensible approach to a problem.
Shows how much you know. Idiot.
also check the memory limits on the 2012r2 SQL Server. High Disk IO could also be a problem with caching. Have a look at the average page lifetime and buffer flushes for that.
Without knowing the full JD, I'd say also look into database normalization.
Your appreciation tastes funny.
&gt; try_convert This sounds worth it. My manager likes to pay as little as possible for anything.
My users want to calculate an weekly/monthly average for [value], but sometimes the sources are not complete. They needed to fill in the gaps.
Step 1. Create a master calendar table with the fields DATE, VALUE_A, VALUE_B VALUE_A is the field VALUE from source_A and VALUE_B is the field VALUE from source_B Step 2. Use the LEAD/LAG functions to brute force 10 days before and 10 days down and with some ugly use of COALESCE you can make some nasty spaghetti code that will get you what you want. LEAD(VALUE_A,1) OVER (ORDER BY DATE) will give you the value of VALUE_A for the next day. LEAD(VALUE_B,-1) OVER (ORDER BY DATE) will give you the value of VALUE_B for the previous day. Put all the above in a common table expression. Step 3. Query the common table expression. If you named it DATA then you can do something like SELECT DATE ,COALESCE(VALUE_A, VALUE_A1, VALUE_A2 ... VALUE_A9) VALUE_A ,COALESCE(VALUE_B, VALUE_B1, VALUE_B2 ... VALUE_B9) VALUE_B ,CASE WHEN VALUE_A IS NOT NULL THEN 0 -- Today WHEN VALUE_A1 IS NOT NULL THEN 1 -- Tomorrow ... WHEN VALUE_A9 IS NOT NULL THEN 9 -- 9 days out ELSE NULL END VALUE_A_TEST ,CASE WHEN VALUE_B IS NOT NULL THEN 0 -- Today WHEN VALUE_B1 IS NOT NULL THEN 1 -- Yesterday ... WHEN VALUE_B9 IS NOT NULL THEN 9 -- 9 days prior END VALUE_B_TEST FROM DATA Step 4. Wrap everything above in its own CTE call it DATA2 then SELECT * ,CASE WHEN VALUE_A_TEST &lt;= VALUE_B_TEST THEN VALUE_A ELSE VALUE_B END FINAL_VALUE
Which flavor of SQL are we talking here? SQL Server, MySQL, Oracle?
MS SQL I think the server is running 2005.
For free and for beginners. With online exercises. http://www.studybyyourself.com/seminar/sql/course/?lang=eng. 
I'm still learning about databases so I wasn't even sure if something like that was possible. 
Thanks. The export was a developer request for that specific number of records. I'm inserting the results into several tables then using expdp to pass the data along.
Like what everyone else is writing, use the coalesce function. Read up on it, its made for situations like this. 
Correct, in order to use CPTs you have to pay some licensing fees to the AMA. [More Info on that here](http://www.ama-assn.org/ama/pub/physician-resources/solutions-managing-your-practice/coding-billing-insurance/cpt/cpt-products-services/licensing.page?) as I don't really deal with that. Edit: You can still find plenty of information about what CPTs are and how they are used when billing IP/OP services (Professional/Physician services aside, they're more common on OP services btw. IP typically is billed by a [DRG Code](https://en.wikipedia.org/wiki/Diagnosis-related_group) by the hospital. Also, while I work with Healthcare data, I'm not a medical coder and don't have any data/analyst certification relating to that [such as this one](http://www.ahima.org/certification/chda) so I defer to someone more knowledgeable if they feel like correcting me.
Cool. Try generating your SQL in this style: WITH c AS ( SELECT template=N' --Look ma I can even have comments! SELECT * FROM ~db~.[dbo].~t~ GO ' ) SELECT REPLACE(REPLACE(c.template ,'~db~',[name]) ,'~t~','Mytable') FROM sys.databases So the point here is to be able to copy nicely formatted SQL into a string literal, keeping all the carriage returns and indentation. We can then swap out some parts for tokens, I have used ~token~ as my style. All you have to do is do search/replace single quote for 2 single quotes. You can use comments, terminators, GO, USE or anything else you can think of, because now your output will have proper formatting. If you are in SQLCMD mode, use SET NOCOUNT ON then an :Out command to send the output to an .sql file.
 running total in sql server
Was there anything useful in this article? I went through two paragraphs of PR-speak and gave up.
While I am very happy with SQL Server as a dev, this is a typical case of "Microsoft says".
What you're proposing there (addition of the `PreferenceNum` column) sounds totally reasonable. If you want to create a new table per preference, go ahead! You can do whatever you want, it's your database. A new table per preference sounds a little overkill though (without knowing exactly what you're building...), so another column called `PrefernceNum` should work just fine. Edit: one other think to maybe consider, is instead of using Integers to store the PreferenceNumber and SelectedValues, maybe use human-readable strings. That way, you can safely re-order things on the front end without being tied down to an "order" of preferences or selections.
I have learned it may not be working because I am using trunc for the date field. Not sure what to use in it's place. 
Try using the [STR](https://msdn.microsoft.com/en-us/library/ms189527.aspx) function
Can you show us the WHERE clause?
I agree with the number thing in general, but by the time the user is done filling out the preferences page, the next page is going to generate a field based on an algorithm that takes integer data from the preferences the user just submit, so it makes the front end a bit more complicated to deal with but it makes the algorithm possible. I really appreciate all of your help. I've been reading about databases and how to interact with them, and I've built a test page that lets me input the data and retrieve it in different ways, and it seems to be working exactly how I want it to. I'm very excited about this project!
Thanks for the completely worthless, content free garbage. Here's a fucking hint to the people posting this shit to r/SQL - we already know how to pronounce "SQL". Fucking bullshit.
You may want to keep a close eye on your senior developer. A guy like that, I wouldn't let anywhere near one of my systems. PCI, SOX, HIPAA - that attitude won't fly with auditors, either.
I remember running into this a few years ago with a similar data set. 1. Convert NVARCHAR to VARCHAR first in case you have weird hidden characters 2. Use REPLACE(REPLACE(REPLACE to get rid of tabs (char 10), carriage returns (char 13), and question marks (your conversion from NVARCHAR to VARCHAR will convert the weird hidden blocks to question marks) 3. Then do a LTRIM(RTRIM to get rid of heading/trailing spaces 4. Now convert it to Float (I had to convert it to Real if memory serves) 5. Find out whoever decided it would be a good idea to store super long numeric fields as a fucking string with special characters and go kick em in the shins as hard as you can
what a completely useless 2-page "article"
/u/mcnabbaldo, I'd be interested to learn why you felt this two page trite article was worth posting to /r/SQL ... can you elaborate? For now I have marked it as spam as it is so, well, devoid of useful content in any way.
 SELECT MONTH(invoice_date) AS invoice_month , SUM(purchase_amount) AS total FROM invoices GROUP BY MONTH(invoice_date)
Most RDBMSs support a datetime, timestamp or date datatype depending on the RDBMS. That gives you access to functions like YEAR(), MONTH() and DAY(), which most RDBMSs support. Take a look at your table. Are you using that, or is the table using char or varchar fields? And which RDBMS are you using? The question is about 10 times easier to answer when you know that. 
if you don't want to install your own server, use http://sqlfiddle.com
well, duh..... OP: "I need to see total purchase amount (total) listed by month" month -- not year and month, just month 
Yeah I've noticed that too. The articles don't even seem to contain any real substance, just a bunch of marketing bullshit. Any thoughts on what we should do? Just downvote? Ask for stricter moderation?
you guessed correctly that WHAT HE SAID IS NOT WHAT HE MEANT i prefer to first answer the question as asked, and if that doesn't work out, then wait to get called out by kind folks like you
Airtable.com
PostgreSQL, MySQL, SQLite, Oracle XE, MS SQL Server Developer Edition, ... 
personal use? Microsoft Access by a friggin landslide
Oh god the datascience subreddit is terrible for blog spam 
Access is good if you want to build a gui, or have embedded code in your single database file, and you're OK with vba. But as a database, access is really pretty weak 
He is learning, so I didn't guess. I knew that he didn't describe well what he meant. Not to mention that summary of sales per month without taking years into account is plain useless (not to say stupid)
That entirely depends on what you want to use it for, and which platform you are running. 
actual serious data store != personal use database 
&gt; not to say stupid i would kindly ask you not to disparage organizations looking for seasonal trends by month over a multi-year period
^^^ This is the answer. This thread is over.
postgree on linux, sql server express on windows, would be my vote. To me it would actually come down php vs asp.net. And yes I actually would put up with IIS to not use php.
I believe I have removed the offending posts. If I missed any, please use the report button or message the mods 
Yes, OP definitely works for one of them.
Hm?
or why wouldn't ORDER BY ListPrice DESC work instead?
It is Production.Product
I'd think this should give you the results you want... SELECT ProductName, ListPrice FROM Production.Product ORDER BY ProductCategoryID ASC, ListPrice DESC
&gt; 2.You can't include a field in the order by that isn't in the query itself. Yes you can... DECLARE @tbl TABLE (col1 INT,col2 INT) INSERT INTO @tbl (col1,col2) VALUES (1,10),(2,20),(3,30) SELECT col1 FROM @tbl ORDER BY col2 DESC
EDIT: FIXED. Syntaxing error. Don't worry about it guys but now I've got a different issue here now, https://www.reddit.com/r/SQL/comments/4q472v/where_clauses/ I get no results for some odd reason but i'm not sure which would be the correct 2 syntax to use. 
WHERE YEAR(OrderDate) = 2015 AND MONTH(OrderDate) BETWEEN 1 AND 3 ? The "in" clause is only going to look for those dates mentioned. Your 'orderdate between' is only filtering on 4 days, and I can imagine there weren't any orders at the new years :)
I don't quite understand this answer, could you expand further?We're looking for the total number of items sold in the first quarter EDIT: now i understand, UK/US Date difference had me confused, that would bring up the results for only 4 days so that answer is OUT. The top answer is correct, but there is also another one I'm still trying to work out
Double check those dates are in the table. I've got the AdventureWorks2016CTP3 db setup and the latest OrderDate in that table is 6/30/2014. Changing the dates in your query to use 2014 works just fine, and using the BETWEEN clause will get you what you want.
Honestly, I thought the same thing, but I double checked when I first responded to OP. I guess TIL :)
@random_tx_user Thanks for the link, really useful. You're right as well. Onto the next question. 
The LIKE clause is a very cost one, specially with wild cards and the OR is also costly, you are using all of those things, your query will be slow for sure.
Dude what's with all the homework help? We are happy to help but this is your third super basic post this morning. Go figure it out and ask a question after you try a few solutions.
you need to hold off on posting so many homework assignments people are going to stop helping you until you learn how to learn
Consider making a minimal working example that provides the same error next time. 
Problem is, Table 1 has 3 columns whilst Table 2 has 5 columns
created a "newlot" column and set it = to str(lot,10,0) and this worked-unfortunately, though when I ran the update with newlot in lot's place it updated 0 rows despite their being matches that I can see
Can you upload a sample file of each table? 
This might be a stupid question, but how would I export the table to a file to upload?
HAVING MIN(Amount) &gt; (SELECT AVG (Amount) FROM Sales.vSalesOrders) 
PM
Is this homework? You really should be doing it yourself instead of bribing the internet.
What I meant was create one query with the 2 tables joined to return all promotions, and a 2nd query with the 2 tables joined to return all products, then using UNION to combine the 2 distinct results. 
This isn't homework, its an obsolete thing I'm being forced to do otherwise my job is at risk considering i'm disabled and i'll never use it in my job. I think asking for help isn't too far. I believe what you call bribing the internet is also called 'tutoring' 
Understood, but I see no reason why I have to learn this, in all the tests I've done in their in office lectures I've just about passed (pass mark was 70% and I achieved 71%) so I need to pass this final examination online just to get it out of the way so that my boss gets off my ass (I never have and never will have to use SQL in my line of work, but our management wants to ensure we 'continue to learn new things everyday.' I haven't struggled with any other courses, but this one has certainly tested me. I just need to answer 3 more questions to pass the the whole thing. So if you could be of any help, as I said, I'd be more than happy to tip. Kind Regards 
I'm not sure what the other table you're trying to join on is but you need to alias your CONCAT statement in the CTE. Also, I'm pretty sure you can just include ' ' as one of the items in your CONCAT statement instead of nesting CONCAT statements.
&gt; I personally thought it was option 1 because of the Union all function UNION\UNION ALL is used to combine sets as opposed to join sets, which is what you want here. Both component queries of UNION(ALL) must have the same number of columns which each corresponding columns having the same data type for it to work as well. For practicality purposes, you really don't need the CTE for what you've described as your requirements. IMHO, it doesn't add any value to the execution, does adds additional complexity. I would have used Option 3, but fixed the group by, if that was an option. But, this post is laid out like it was from a quiz or something like that. Homework, maybe? Anyway... Here's my take on it: * Option 1 won't work because UNION ALL must follow another select statement. The CTE doesn't count. * Option 2 Should like it will work. * Option 3 won't work period as it stands because of the "group by CustomerName". It would work if the group by instead referenced c.FirstName and c.LastName. It doesn't invoke the CTE no matter which way you swing it. * Option 4 won't work because it's omitting the join Sales.SalesOrder which is implied to include the Amount column. So... Option 2 is the most correct answer, if not the correct answer.
You seem like you're hustling to learn something, so I'll give you a tip that might allow you to understand what /u/tsuhg is doing. This can be applied to any query when you aren't understanding how the WHERE predicate is working - especially when you're just learning how to work with sets of data like this. &gt; WHERE YEAR(OrderDate) = 2015 AND MONTH(OrderDate) BETWEEN 1 AND 3 Try adding the YEAR(OrderDate) and MONTH(OrderDate) date to the select in your original main query so that you get something like this: SELECT SalesOrderID, OrderDate, Amount, YEAR(OrderDate) as OrderDateYear, MONTH(OrderDate) as OrderDateMonth FROM Sales.SalesOrderHeader --WHERE YEAR(OrderDate) = 2015 AND MONTH(OrderDate) BETWEEN 1 AND 3 I commented out the WHERE so that I could include it and you could uncomment it to try to run it with and without it. You need to understand that when using the WHERE clause, ALL conditions must be true for a record to be returned. So, in this case, both YEAR(OrderDate) must equal 2015 and MONTH(OrderDate) must be including or between 1 and 3. You should be able to see these values in the results if you add them to the SELECT like I showed you above and leave the WHERE commented out.
Occasionally. Usually for blogspam. In all honesty, the other mod stays a lot more on top of the moderation queue and cleanup than I do. He also set up AutoModerator which takes care of a lot of the junk automatically. I'm happy to have seen this post and been able to act on it before he did for once. Personally, my preference is to not remove anything. We are a relatively small community, most posts are self posts of people simply having questions. My personal reddit philosophy really is that people should use the voting buttons to voice their opinion of whether a post has merit and then the junk posts would automatically fall to the bottom. But in cases like this, where it was clearly just new accounts advertising and they linger on the front page for too long because we don't get that many posts, I will use my discretion and remove them. 
Thank you! I was going to add on it today as I didn't have a lot of time yesterday :)
Try to make sure you are processing as few rows as possible when you introduce your LIKE phrase. Maybe perform your date-based data selection up front (on userreviewdata) and load these date-constrained results into a temporary table. Then, do your LIKE work on the temp. table. This will give you a small(er) result set with which to perform your JOIN to userreview table. If it's available, you could try using " d.reviewtext LIKE ANY ('% choco %','% air %') " if your implementation permits it - there may be optimizations applied internally to manage the CPU use. HTH
Sorry i reposted same
I'm right there with you. I'm an applications analyst who learned SQL on the job, got decent at it, and now I'm looking to break into more languages. I'm personally gearing toward Javascript, mainly because the software I admin uses it heavily. Good luck on your choice!
You'd only need the lot column from each table right?
See pms. Thanks!
Ok, so all of these are coming over as varchar(50), which is fine, just wanted to let you know. You're also missing the ITEM &amp; LOCATION column which are JOIN and WHERE conditions. 
Yeah I was unsure if it would be converted when exported as CSV. I intentionally left those out because they are working (the update works when I take out the lot part, but is updating on more than I want it to obviously) and to not share too much of our data.
Additional piece of info:I am importing the lotupdate$ table from an an excel sheet using the wizard and that is only letting me import as float or int. I am currently investigating other ways to import to see if that changes anything.
for lot update(update from table): lot-float, on_hand_qty -float, newlot-nvarchar(50) for location inventory(table to be updated and as an aside we can't change this one):lot-nvarchar(25), on_hand_qty-numeric (19,5)
I was able to work around the issue(thanks to you asking) by exporting the entire lotupdate$ table again as CSV, and then re-importing everything in it as nvarchars. If you find a real solution I am all ears, but am certainly relieved the workaround exists. Thanks a bunch.
Thanks - I thought it mattered with Oracle. 
Whatever cast you do on this must be viable for all values in the column. Try selecting from each table also returning your cast for the join column. If any values aren't castable then you will get the error. So test casting your nvarchar to float and your float to nvarchar. Also try casting both to int. I am guessing the $ table came from excel? All numbers are floats when obtained from excel. Quantities are usually integers so if you can confirm that then you might find a common integer datatype that works both ways. You haven't stated your platform. Please read the sidebar! If you are using later versions of SQLS you can use the TRY_CONVERT function instead of CAST. Instead of returning an error it will return uncastable values as NULL making it much easier to find the dodgy values. If you have any control over these datatypes then go back to your design and choose appropriate ones for the attribute. If not find who does. I've actually seen companies fail audits for bad practise like that! 
There are edge cases where a poor optimiser in some platforms will behave differently according to how you write your SQL, but you should not form a habit around them.
having is where for aggregates Having AVG(price) &gt; 100
&gt; All that is affected is readability by other humans. this ~cannot~ be overstated my suggestion is this -- there is almost always a "driving" condition that governs what a query will return examples: - list sales for the previous month - show all friends of a given user - count likes and shares for a given post - return all shops within a radius of a given location each of these types of query have one thing in common -- they have one or more conditions which will end up in the WHERE clause so my strategy is, the "driving" condition of a query or report, which goes in the WHERE clause, determines the first table in the FROM clause when you start the FROM clause with that table, and then add joins from there, the "logic" of the query becomes far more obvious and making the purpose of a report obvious is very, very important to recap, ~don't~ do this -- SELECT ... FROM table1 INNER JOIN table2 ON table2.foo = table1.bar INNER JOIN table3 ON table3.bat = table2.qux WHERE table2.dingus = 42 
Some notes other people have not touched on. A way to optimize your joins: - Join on an index for both tables. - Can you limit the larger population with a where clause and join to that subquery or possibly store the subquery into a temp or variable table? (This depends on hardware. Some circumstances storing in memory vs physical is better or worse. There are times I'm working with so much data I have to limit the large table by inserting filtered results into a physical table I created for the purpose.) 
Is the account_ID field indexed in both tables? And when you want it to show only one username, what do you want to have happen to the logoff and on time? SELECT Accounts.Username, max(p_login_logs.time_login), max(p_login_logs.time_logoff) \ FROM `p_login_logs` JOIN `accounts` ON accounts.Account_ID = p_login_logs.Account_ID WHERE p_login_logs.ip = '127.0.0.1' GROUP BY Accounts.Username ORDER BY max(p_login_logs.time_login) DESC LIMIT 0,5 Will get you only the last login and logoff time per account, but is that what you're looking for?
I'm a SQL newbie, too. The Mode Analytics tutorials have thus far been the most in-depth and understandable for me. Here's the overview of [Group By](https://community.modeanalytics.com/sql/tutorial/sql-group-by/) and [Having](https://community.modeanalytics.com/sql/tutorial/sql-having/). Maybe the tutorials will help!
There is a primary key on Account ID in Accounts, and an index on Account ID on the logins table, there is no primary key in the login table tho. The only activity on the table are inserts, whenever a player logs in, and an update when the player logs out. Select queries come manually when an administration uses this query.
To piggyback on the comment about using an index - If both tables have an index on a column and that is sufficient for the join don't use more columns. Select * from A join B on a.JobIDKey = b.JobIDKey If you also add JobNumber [to be extra sure] but there isn't an index on JobNumber the join will have to go back to the base table instead of just using the index. So Select * from A join B on a.jobIDKey = b.jobIDKey AND a.jobNumber = b.jobNumber may preform worse even through the results are the same. 
Wow this is awesome. Thanks for the links.
My biggest suggestion is to use views. Views are wonderful because they are "pre-compiled", if you will, which makes them more optimized than querying with a stored procedure on a join of two or more tables. You can specify a view per each role to lock down the data set that is appropriate for each role, then use stored procedure to filter data as needed. One thing to note is that you can't create a parameterized view, but you can create a parameterized stored procedure. If you combine views, stored procedures, and indexes on commonly queries columns, you'll have a fairly optimized backend without doing much work. 
It's exactly what I wanted! Thanks man!
Hm, interesting. Never thought that it would work that way, good to know.
My experience in this particular area is specific to data mining on MSSQL, so your results on Oracle may differ. I've found a lot of instances where CROSS APPLY/OUTER APPLY perform far far far better than a normal join or a subquery on MSSQL server when querying well indexed extremely large data sets. I haven't fully looked at the query optimizer to explain why (perhaps someone on this reddit can give more detail) but i've seen some query runtimes go from hours to minutes by using these.
Use IIF or CASE to do a logic check. E.g If it's null (not matched) N else Y
I always start with the smaller tables first and work my way up to the larger ones. I've seen a vast improvement in how quick the queries return since doing so. 
I thought the query Ive written would give "Y" only if the records are joined. Should I use the case in the first select statement?
Can you post the explain for your query?
Why are you doing a union all join in the first query? Seems you would want an inner join of some type.
SSIS is made for this. If that's not an option, a scheduled task (either SQL Agent or Windows Scheduler) that runs the query, dumps the result to a file and then does the file transfer. My drug of choice is PowerShell, but anything that's more modern than a `.bat` file will do the trick. Side note: FTP should have been dragged out back and shot a decade and a half ago. It is a horrible, insecure, ancient protocol and has no place in the modern world. Ask for SFTP instead (FTPS is a troublesome stopgap). All of this goes double if you're transferring to an external server, and triply if the data is even *remotely* sensitive.
Got stuffed by this one yesterday.
Great! Care to enlighten us? Datafiles, logfiles, tempdb or other?
SSIS is one alternative. Another, BCP out a query with the results to a CSV/DAT/Text/ or whatever file into the desired location. It's much less hassle than SSIS, and just a job that runs at the given time. 
http://dev.mysql.com/doc/refman/5.7/en/mysqldump.html https://aws.amazon.com/getting-started/tutorials/backup-to-s3-cli/
probably also for those who do not use SQL everyday. i support all kinds of software and may go for a week or two working with other languages and hardly touching SQL... at first i found myself googling a lot just to get a refresher, but now it comes back to me quicker.
Right, and I get that. I do that too. But this is too high level IMO and seems to me to be a eclectic mix of syntax and functions that doesn't really cover anything.
I'm a BI developer, so YMMV. We have analysts who specify the purpose of an SSRS report, the parameters and the expected outcome of the report. It will have documentation for chart objects, table objects, drill through behaviour (we launch a new report in a modal dialogue box for this) and Excel export behaviour. The actual SQL code for the report - both to populate the table for that report / family of reports, and the code behind it we leave well commented.
After the from statement use CROSS APPLY ( select isnull(b.flag, 'n') as flag ) as dt Then in the query itself you can just reference dt.flag instead.
Lets try that again.. my first answer wasn't based on a clear enough read of the problem and realizing that your example table is an intermediate result. For convenience, lets call the number ID and your date as EventDate Start ID EventDate 1234 2016-06-01 1234 2016-06-04 1234 2016-06-30 12345 2016-05-20 12345 2016-05-28 12345 2016-06-30 20134 2016-02-05 20134 2016-06-12 20134 2016-06-30 21332 2016-04-12 21332 2016-06-12 21332 2016-06-30 Intermediate table you presented ID MinEventDate MaxEventDate 1234 2016-06-01 2016-06-30 12345 2016-05-20 2016-06-30 20134 2016-02-05 2016-06-30 21332 2016-04-12 2016-06-30 Goal: &gt; see how many of the records minimum date falls between a range of time and then how many of the records maximum date falls between the range of time SELECT SUM( CASE WHEN MinEventDate BETWEEN '2016-06-01' AND '2016-06-30' THEN 1 ELSE 0 END ) as Start , SUM( CASE WHEN MaxEventDate = '2016-06-30' THEN 1 ELSE 0 END ) as End FROM ( SELECT ID, Min( EventDate), Max( EventDate) FROM YourTable GROUP BY ID ) Intermediate
You can do it all with the help of this tool http://mysqlbackupftp.com/
Can you explain what you mean by ANSII SQL knowlege please?
Cheat sheets are always arbitrary :)
Ouch. First, add a customer first purchase date to your customer table. Having that inner query (x) tablescan the entire transactions table to gather EVERY customer's first transaction, no matter if they had a transaction during the time period in question or not is going to be a HUGE time suck. That simplifies things quite a bit.
I'll mention another method no one has suggested, I don't recommend in comparison because it's not as secure as SSIS. (Storing login information in plain text. If the information is not risky and you can create a read only account to a specific view for the data you need, this may not be bad.) Powershell. You can use powershell to connect to a DB, execute a cmd, return the results to a file, move the file, connect to the SFTP, and disconnect. I did something similar, except it also encrypted the file, and pushed to an encrypted drive. 
Because the FROM just specifies the table sources and maybe JOINS. If you are using JOINS, some of the WHERE clause can be optimized into the JOINS. Also, the SELECT does the projection depending on the step. The reason 4-7 are listed is if you utilize everything in one query. It is important to know that a GROUP BY and HAVING clause happen before OLAP and QUALIFY, and then the SAMPLE.
Totally agree. Those are things which I would find very useful.
Well, for example: select bidID from Bids b where exists( select * from PeopleClassifications pc join EventClassifications ec on ec.CategoryID = pc.Category ID where pc.PersonID = b.PersonID and ec.EventID = e.EventID) The choice to use CategoryID in the join condition is pretty arbitrary. If it's easier to internalize - use CROSS JOIN instead of INNER and pile all relevant conditions into the "where" clause, like this: select bidID from Bids b where exists( select * from PeopleClassifications pc cross join EventClassifications ec where ec.CategoryID = pc.Category ID and pc.PersonID = b.PersonID and ec.EventID = b.EventID) 
Its important to note that overindexing a table can slow down update and delete operations.
sounds like you might enjoy either a report developer position (sql and ssrs heavy) or a full blown sql development position. What market are you in?
I live in Arkansas near Little Rock. I just want out of Higher Education and out of AR. Well, maybe not out of higher education, because I'd love to get my master's, but at least out of AR.
If you're looking to stay/get in the database world on the technical side, I'd focus on certs and experience over an advanced degree, tbh. They're worth more than a masters 
I'm just saying, ANSI sql (ty autocorrect) really impresses me when j see it in an interview. 
Nice 
no, I'm using remote connections for now. Don't want the game server and the db on the same IP. Nobody knows where the db is that way as I said before "There is a primary key on Account ID in Accounts, and an foreign key index on Account ID on the logins table, there is no primary key in the login table tho.". Is that good enough? 
I like a challenge and I love solving problems. I studied math and that's what got me interested in DB work. It's a really cool application of logic. Like I said, I'm trying to teach myself SSIS, though that seems really situational. Like, learning it is more just learning to solve problems as they arise instead of there being a path to follow. I don't know much about SSAS, but I'm looking into it. I want to know every piece of the stack and have a thorough understanding of database administration. But, I just wish I could find a job with some support in the meantime. It's frustrating to work alone all the time. No one to run ideas by. No one to ask questions. It can get tedious. I'd also like to learn powershell. Seems to be a really valuable skill for DBA stuff.
I meant tagging in the sense of having data in the result set that identifies record lineage - normally you wouldnt need any conditionals around those tags; this is helpful when you have multiple recursive parts UNION ALL'd together: with cte as ( -- anchor select '' sourceBranch, 'A' branchTag, &lt;mycolumns&gt; from.... union all -- 'up' branch select cte.branchTag, 'U' branchTag, &lt;mycolumns&gt; from cte join .... -- 'down' branch select cte.branchTag, 'D' branchTag, &lt;mycolumns&gt; from cte join .... ) select * from cte
The percent is the [modulo operator](https://en.wikipedia.org/wiki/Modulo_operation). So the where clause is limiting the result set to those rows where dividing the year by two leaves a remainder of zero (i.e., even years).
use 'unicode' as the code page, not cp1252
You can store images in DBs as BLOBs. You could also do what you suggest in your last sentence. This discussion is a good starting point: http://dba.stackexchange.com/questions/2445/files-in-the-database-or-not
Indeed, I've made the same criticism in the past. In addition to writing the cross join with a comma, you could also use the `CROSS JOIN` keywords to make it explicit. The comma is (still) often used by people to form an implicit cross join, but then they're putting join predicates in the `WHERE` clause, pre-ANSI JOIN style.
&gt; In some versions of SQL Server, storing image data in a table will affect query operations in that table. You can put the image blobs in their own table, no?
You can store images as binary large objects (BLOBs). However it isn't always a good idea unless they are small as they can screw with space management. However there are advantages in having everything together and in one place. The usual approach is to use the host file system to store images with some form of folder structure. The database contains searchable tags and the file location. This is the approach chosen by Adobe Lightroom. This keeps the database efficient and letting the file system do what it is good at.
Thanks for the gold!
I use the import feature when i have a large amount of data in a spreadsheet that I want to put into the DB [Link to Microsoft's page](https://msdn.microsoft.com/en-us/library/ms140052.aspx)
Yup, this hangs in every cubicle I get.
If there's only one link in each record, you may use CHARINDEX to search for `&lt;a href="` and `&lt;/a&gt;`, or http:\/\/ and "\&gt; and after that apply SUBSTRING and STUFF to modify and change the string correspondingly. But if there's more than one occurrence, then it becomes more difficult since you have to find each and every position of the link within one record. Usually, it can be done by cross applying numeric table and searching through every symbol.
You should stick with express. Your company should consider upgrading to Standard when the reach a point of having a server with 8+ core processors over 3.0 ghz and g4 gb ram . When it comes to hardware your current machine is well above Microsoft's recommended for ms standard. May i ask have you done any database analysis of performance? Indexes, views, stored procs? Edit: I guess my suggestion is for your company to entertain the idea of paying a way lesser sum to get a DBA contractor in to do some basic Database analysis rather than investing money in the hardware/software that could just be an indexing issue. Could you elaborate more on what you are querying on this data
https://blog.jooq.org/ https://modern-sql.com/ Joe Celko's SQL for Smarties and Trees and Hierarchies
Make sure your text columns are nvarchar In import "wizard": Uncheck Unicode, set Code Page to 65001 UTF-8 Make sure DataType is DT_WSTR (for unicode columns)
Select *, NULL as Newfield, NULL as NewField2 into NewTable from OldTable
This is the answer, thanks. I had this confirmed on a Stack overflow thread earlier today.
Search for "ETL" tutorials, include the name of the database platform that you're using to get results more tailored to it (for example, SQL Server has a tool called SSIS - SQL Server Integration Services, which makes tasks like this very simple).
Thanks! Can you tell me whats the problem with &amp; _? I attach it to the end of the line like "Select * Into NewTable" &amp; _ "From OldTable" but it doesn't work. It only works when I remove the &amp; _ and make it into one continuous line. EDIT: found it. supposed to add a space before " i.e. NewTable " &amp; _
That's because you get "Select * Into NewTableFrom OldTable" in the end. Add proper spaces when you do the concatenation :)
Sorry. I'm using SQL within VBA, some rules to follow I think. Thanks for your help, anyway. :)
Sorry I typed this on my phone and it mangled some important syntax. I would ignore the above for now and just try substituting 'a' for NULL and seeing if that works.
Here is a copy of a word doc that I send out to people that request reports from me... (Formatting is bad here, but you can use this to get a list of questions to ask... Requestor : Name: Role: Location Department: Location: Contact info Phone: Email: Request Basics Report name New or Modified? Is there another report that you want to use as a base report or example? If yes, identify the original report. If reporting workbench, include template name. If Clarity include the Epic-released base report. New or Modified? Is there another report that you want to use as a base report or example? If yes, identify the original report. If reporting workbench, include template name. If Clarity include the Epic-released base report. Tracker Reference # Report Type (once known) Report Writer (if different) Functional Information Report Description and Purpose Consumers Audience/consumers (role, group; rarely in tdividuals) Short Description (what) What questions is the report going to answer? Purpose (why) Benefits, Operational need; workflows supported; Reporting Tool analytical vs. operational; is template modification likely? Frequency (when) For example, annually, quarterly, monthly, weekly, daily, ad hoc, other – include day of week or month where applicable General Report Layout and Content Info to display (or summary of modifications) Which fields or columns should be included in the report? For example, patient name, diagnosis, visit date, account balance What visualizations, such as graphs or charts, are needed? Group by How should results be sorted/grouped? If multiple grouping levels, list from less specific to more specific. For example, service area, bill area, provider Include summaries? If so, using which fields? Which type of summary? For example, count/sum/average subtotal. If thresholds are requested ensure compliance to standards Inclusion/Exclusion Criteria Which records qualify for the report – and should be excluded? For example, all payments posted through electronic remittance by week Parameters Which criteria should you be able to select and change? For example, departments to select, date ranges; Are dynamic parameters required? Date Criteria What are the date items, and where are they derived (what kinds of dates)? For example, encounter date or post date 
I make $33k at the moment so...pretty much anything is a step up.
Depending on geography, maybe with those skills and experience you could find a higher paying job. I just did a quick search and came up with [this](http://www.payscale.com/research/US/Job=SQL_Developer/Salary) 
I live in an impoverished and high crime area in Arkansas
One thing you should quickly learn is that there will always be an exception or quirk or alternate way of doing things. Personally I think you are over analyzing the relationship concept single there only three types. One to one. One to many. Many to many You've got those ideas down stop stressing over the edge cases.
I'd like to pipe up here and ask that you please try to avoid SELECT * INTO if you can. Instead, create the temp table first then use INSERT INTO .. SELECT .. When you use SELECT * INTO, the SQL engine has to interrogate the entire FROM statement and make determinations for the data types. This is an unnecessary expense on your SQL server, and can sometimes generate errors (it might think a string field needs to be 5 characters wide when it really needs to be 50, or identify an INT when you really need a DECIMAL) Plus, using * isn't scalable or future facing. If a developer adds additional fields to a table your query could wind up failing to execute properly. If you articulate the fields you watch to select, you avoid problems when someone adds new fields later. Plus the whole need to articulate NULL AS field1, NULL AS field2 etc becomes unnecessary .. the NULL is implied during your insert. Your DBAs and future coworkers will thank you later if you expend a bit of energy on your code now
Dang thats great. I'd hate to be one filling it out but must make it super easy for you to get started. 
Wait, this guy's getting paid to do data entry? I'm in the wrong business. Seriously, look into SSIS packages like the other guy said. Not entirely sure about AWS. If it's your db, why wouldn't you have permissions? Have you tried to insert the data? Make a backup and try.
Going to stand on a limb and guess MySQL / MariaDB.
Oh dear God this. I can't begin to talk about the BS projects we started that the mid levels dreamed up on the toilet and came to our desks and asked for, claimed they needed it that day, and then seemed to forget about it until two weeks later. We have since implemented a ticket system, which is a simple for to fill out with about 5 fields are so. We have all but eliminated those type requests. 
As KSledneck suggested, it's more than enough. The reason your Queries are slow could be due to a lot of things - indexes(nonclutered, clustered, heap table)?, data compression, hdd bottleneck? One thing is for sure, your hdd is 5400 RP, which is bad for DB. A lot of the commercial machines use Fusion IO which goes into the PCI slot and is uber fast, but you don't need that. Leave the RAM out of the equation atm. Bottlenecks should be -&gt; clustered -&gt; index -&gt; data compression -&gt; hdd (roughly) P.S. I simplified it in case someone goes ballistic, it's a lot of variables, but for simple local machine.
Thanks for all of the responses! I don't understand much of what was said (clustering, indexing, etc.), but now that I "know what I don't know," I have a place to start learning. Much appreciated!
Clob or blob
Be sure to have a document version control page and an executive sponsor sign off page. These are crucial in the event of an audit. 
I would suggest checking out SMART Goals as a basis. Some of what you had listed would be pretty difficult to measure effort as it relates to specific people. For instance, "Functionality" is basically a "yes/no" metric. What if you have 2 developers and one was assigned a huge piece of work and the other 10 small pieces. They may have ultimately contributed evenly on work, but the second dev seems like they did more from a functionality perspective. Speed is another one... what if the environment being deployed is run off a Dell Inspiron in the back office? Should that really be measured against an enterprise level solution? I don't think "Speed" is a good metric name either, you likely want Performance or Implementation Quality or something to that effect. It seems like these metrics you've listed focus less on the individual and more on the product. If you want to retain individuals who do great work, the first step is understanding what each person is working on and their timeline. If you don't understand the work they are performing, as their manager, then there is a significant disconnect that needs to be addressed. Understanding the work your employees are doing so that you can understand their challenges and their accomplishments is a large part of being able to recognize employees for their contributions. Your star employees will be the ones taking risks on larger projects and delivering a good/great product/solution almost every time, then using that knowledge on the next project. Your average employees will be the ones taking the easy tasks and completing them by their due date and checking all the boxes. It sounds as though your business is maturing at a good rate though. Are you leveraging any source control options? If so, you could use metrics from those systems to help you (Emphasis on HELP... things like velocity should never be the sole factor) determine how much people are actually contributing, though this comes with its own set of challenges. If you have a PM (or someone with a PMP certification) you could reach out to them as this is more their specialty area. Probably not a perfect answer, but should help in some way I hope.
I'm not sure if the "where" is kosher here as I only know Oracle SQL syntax well but seems to me that what you're trying to get at is: SELECT a.column1, a.column2 FROM tableA AS a INNER JOIN tableB as b ON a.columnX = b.columnX WHERE a.value1&lt;0 AND b.value1-abs(a.value1)&gt;z (or should the two values from the last line be added to see if their sum is greater than z?)
Good points! 
This is the whole database: https://i.gyazo.com/2766a7ff4bdeb879cd60a2e4c8095c4a.png
Is the supervisors cut off above the employee table? 
Thank you!
What's a PASS chapter?
I am now deployed from within my company as a jr sql programmer.. Ive 100% learned from YouTube so i can help you here! My old job was in collections and now programming and all youtube 100% self taught. Check out venkat - hes got alot of the basics for query writing... Over 130 videos. After you understand the basics check out Brent Ozman... I think that's his name. He has tons of vodeos for sql server administration and optimization its awesome! 
It sounds like you want to create an index performance. I do this for our call center agents such as: | Agent | Leads | Submits | Ratio | Index | | :--- | :--- | :--- | :--- | :--- | | 0001 | 125 | 5 | Submit/Leads | (Submit/Leads) / (TotalSubmits / TotalLeads) | | 0002 | 25 | 1 | Submit/Leads | (Submit/Leads) / (TotalSubmits / TotalLeads) | | 0003 | 200 | 6 | Submit/Leads | (Submit/Leads) / (TotalSubmits / TotalLeads) | | 0004 | 138 | 9 | Submit/Leads | (Submit/Leads) / (TotalSubmits / TotalLeads) | | 0005 | 16 | 3 | Submit/Leads | (Submit/Leads) / (TotalSubmits / TotalLeads) | It will spit out a ratio where 100% is average, anything above 100% is above average, and anything below 100% is below average. 
A lot of tutorials use adventure works database so I'd start there. You can download sql server express edition for free and install adventure works then you can get started with writing queries. Lynda and pluralsight are two tutorial websites that have videos on sql that I liked personally. Those sites aren't free though. 
Sure it is, just come up with a ratio that's meaningful, e.g., total projects assigned, and total projects completed with a 5 star rating based on KPI's such as speed, client satisfaction, etc. Then rank each individual across the pack.
http://www.bkent.net/Doc/simple5.htm
Mnemonic: The key, the whole key, and nothing but the key. "The key" : to be in 1NF, you have to have a key and all columns have to depend on this key. Repeating columns is a no-no. "The whole key" : every non-key column has to depend on the whole key, and not only parts of it. If you have a single column key, congrats, if you are 1NF you automatically qualify for 2NF. "Nothing but the key" : All non-key columns have to only depend on the key, and can't have any dependencies between each other. The "why!?" of normal forms is to reduce redundancy. By following these rules, you only state a fact once, and avoid running into the problem of having contradictions between tables/rows/columns about the same datum. Don't know any five year olds that would ask about normal forms, so I'm not sure I was successful here... :)
&gt; Brent Ozman Pretty sure you're talking about Brent Ozar, he's awesome, and pretty funny. Seen him speak at a couple of SQL Saturdays. His website is a great resource for all things SQL https://www.brentozar.com
it certainly does. It is there for a reason. 
Useful for stripey tables :)
What do you mean? I'm new to all of this. 
If you do a ROW_NUMBER() % 2 you get an alternating true/false column. When you're in a report writer package you can use that to get alternate row shading, a popular table decoration.
You nailed it! Brent Ozar! 
&gt; Computer programmers assemble data, as if they were legos. This reminds me of Chris Saxon's explanation about [different SQL table types](https://www.youtube.com/watch?v=x2NNIo6riUI)
That was cool. Thank you.
The key is the unique identifier - just as a key could be a text column with multiple characters, it can be multiple columns. For example, the unique (dependent) key for an order line-item might consist of order number plus line number, like the unique key for a specific hour in something tracking events would be the date plus the hour, since hours repeat between dates..
Again, no. You can easily create a KPI which weights complexity of the work. Also, as a manager, the OP should have discretion over who to assign which project to in order to distribute the workload efficiently. The trick is coming up with something meaningful to your environment. There are lots of ways to measure employee performance. As far as statistics and analytics go, this is the method you want. If you want to create something which is less tangible and more based on your 'gut feelings,' or 'co-worker/client feedback' then you can do that instead... but in either scenario you can just as easily create a KPI and index team members against the team as a whole --&gt; i.e., individual index performance is ranked against the performance of the whole. Building this out in SQL isn't easy, though. It's far simpler to track in Excel but it's completely doable in SQL and fun, too.
Look for job listings that interest you. Read the job description and requirements section. Become knowledgeable and well acquainted with whatever skills and experience the jobs require. Apply. 
As your needs grow, so can your system. Say you at first only need to process x transactions a day. Then your company hits a grow spurt - suddenly you need to process 100x transactions a day. Well, a scalable system would be capable of "growing" to meet that capability in some way(s) - like adding nodes, or processors, or memory, to name a few possibilities.
There's two types of scalability that people typically talk about: Scaling out -- This typically refers to adding additional nodes. That is to say, additional points where the data are accessible. For example, if you make your database accessible at corporate HQ and at a branch site in another country, you've scaled it out. You might be using log shipping, mirroring, or replication (one or two way) to enable the same database to be quickly accessible at both locations. Being able to add an arbitrary number of nodes lets you minimize bandwidth costs because you're only sending data over expensive lines one time, and it also eliminates or mitigates latency because your users are geographically closer to the nodes. Factors here include how difficult it is to administer such a system, how network-intensive such systems are, and how many nodes can reasonably be supported. Adding a network load balancer to a web server so that requests get divided amongst 5 identical servers in the back end is an example of scaling out. Scaling up -- This refers to making each node capable of doing more. It might be executing more transactions per second (probably the most common), storing more data, or adding new features. Factors here typically include how powerful the hardware is, how well the software takes advantage of the software (a single-threaded application doesn't scale up, for example), and how easy the software is to maintain and update to add additional features to. Adding new features can fit in either bucket depending on the functions being added, but I tend to put it in scaling up. A web server with a good cache is something that scales up fairly well, because it can typically handle several requests and execute multiple threads fairly easily since sessions are typically independent of each other. Adding more RAM, more CPUs, upgrading to 16Gbps fibre channel SAN on arrays of SSDs, re-coding a specific procedure or algorithm to take advantage of multithreading or optimizing code execution or memory usage. Throwing money at the problem in terms of bigger and better hardware only gets you so far. Eventually scalablity comes down to how your software is coded and what the load you need to support looks like. Historically, it's very, very difficult to scale both up and out, and you should beware of any system that claims that it can do both easily. Simply put, I'd expect to get a system that scales both ways very well very easily for a very narrow set of criteria that the developers think are very common, but may not apply to your application at all. 
NoSQL queries tend to be better suited to distribution than purely SQL queries. This means they can be more easily farmed out to multiple nodes (scaled out) rather than scaled up.
Changing the HAVING part to the following wouldn't solve it? HAVING ([Steve Live$Item Ledger Entry].[Entry Type] = 5) AND (YEAR([Steve Live$Item Ledger Entry].[Posting Date]) = YEAR (GETDATE())) AND ([Steve Live$Item Ledger Entry].[Posting Date] &gt;= getdate() -15) It will bring the data from the last 15 days, even if it mix the data from different months, like today (7/4/2016) it would bring data since 6/19/2016.
Ugh, so simple. I didn't need the month at all. Argh! This worked, thanks so much.
Hmm, I will try this, thanks!
Just because you *can* do something doesn't mean that it's a good idea to do it all over the place. * `select *` means you're pulling back more data than you really need. * It's more or less impossible to have a covering index to reduce I/O requirements. * If for some reason someone reorders the fields on the table and your application references the fields by ordinal instead of by name, you're going to have a bad time. * If you're joining 2 tables and you have columns with the same name in both, what do you do? http://sqlblog.com/blogs/aaron_bertrand/archive/2009/10/10/bad-habits-to-kick-using-select-omitting-the-column-list.aspx
nah, man, that's good if it doesn't return any rows, check the data values!
Use &gt;= (greater than or equal to, not =&gt;). Make sure DueDate is actually a date field, if not convert it in the query.
&gt; Make sure DueDate is actually a date field, if not convert it in the query. And then find whoever decided storing dates with an inappropriate data type was a good idea and take away their keyboard.
This gets an incorrect syntax near WHERE clause: WHERE [Steve Live$Prod_ Order Line].[Due Date] &gt;= 'Getdate'() Obviously wrote it completely wrong. Any suggestions?
GETDATE is a function name, shouldn't be in apostrophes. Other than that, and the wonky table name that looks like an Excel sheet imported to Access, it should be OK. 
Keep in mind that GETDATE returns a datetime. If your DueDate column is also datetime, your where &gt;= will only return rows that are later than the exact time you run the query. So if you have dates without time in that column, they'd be something like '2016-07-04 00:00:00' and these records won't be in the result set, because they are earlier than now. Your safest bet would be to cast GETDATE to date, like `CAST(GETDATE() AS DATE)`.
Same error still. I think I tried that. Got me stumped!
Sorry. SELECT [Steve Live$Prod_ Order Line].[Item No_] ,[Steve Live$Prod_ Order Line].Description ,[Steve Live$Prod_ Order Line].Quantity ,[Steve Live$Prod_ Order Line].[Due Date] FROM [Steve Live$Prod_ Order Line] GROUP BY [Steve Live$Prod_ Order Line].[Item No_] ,[Steve Live$Prod_ Order Line].Description ,[Steve Live$Prod_ Order Line].Quantity ,[Steve Live$Prod_ Order Line].[Due Date] WHERE [Steve Live$Prod_ Order Line].[Due Date] &gt;= Getdate() ORDER BY "Steve Live$Prod_ Order Line"."Due Date" DESC 
See? I told you it could be in another place :) You changed the order of the WHERE and the GROUP BY. it should be: SELECT [Steve Live$Prod_ Order Line].[Item No_] ,[Steve Live$Prod_ Order Line].Description ,[Steve Live$Prod_ Order Line].Quantity ,[Steve Live$Prod_ Order Line].[Due Date] FROM [Steve Live$Prod_ Order Line] WHERE [Steve Live$Prod_ Order Line].[Due Date] &gt;= Getdate() GROUP BY [Steve Live$Prod_ Order Line].[Item No_] ,[Steve Live$Prod_ Order Line].Description ,[Steve Live$Prod_ Order Line].Quantity ,[Steve Live$Prod_ Order Line].[Due Date] ORDER BY [Steve Live$Prod_ Order Line].[Due Date] DESC
DOH! That's so obvious now, forgot the obvious logical order. I'm so horrible at this, need to seriously take a course.
The short version is "because they heard it was." None of the benchmarks back them up. Postgres scales circles around the various NoSQL databases, in both NoSQL and SQL modes, both horizontally and vertically. So do MySQL and Oracle.
You should have a 2nd table to store those Ids, but since the damage is done and you can't change it ... you can make a search in parts, something like: WHERE ids = '42' or ids like '%,42,%' or ids like '42,%' or ids like '%,42' That way it will find the number if it is in the beginning, end or middle of the string or if it is the only number. May not be the best solution, but is easy enough :P
Thanks for the help. That makes sense. I haven't actually started storing the IDs like this yet, but the reason I was thinking of it was because I need to be able to search through and paginate multiple tables worth of data. The plan was to build this single table to use an an index. These are category IDs that for example will belong to many articles, videos and so on. The main point is to avoid me having to query 4 separate tables and somehow combine all the results into one stream of content. Maybe that's not the right way to go about it though?
OK, please describe one of these "true use cases." Because I've yet to see any where a `select *` was more appropriate than specifying fields in the query.
The best way to workaround that is to use the [Split](http://www.sqlservercentral.com/blogs/querying-microsoft-sql-server/2013/09/19/how-to-split-a-string-by-delimited-char-in-sql-server/) function, which would transform your string into table with single column and multiple rows. SELECT * FROM table WHERE ids IN (SELECT * FROM dbo.fnSplitString('12,545,3,2342',',')) There are some other ways of dealing with it without creating DB objects, but having function to do that seems to me like more elegant solution.
Oh great, in that case try to do it the right way :D Make another table with at least 2 fields and those fields will be the primary key: 1) The ID of the table that you already have. 2) The ids you want to store. So when you select it you make a JOIN, and you can make an index in that 2nd table and the JOIN will use it properly, the LIKE operator will kill the index if you use a single table, even with FULL-TEXT enable the performance would be worse than a JOIN and a simple "WHERE ids = number" EDIT: your query will be something like: SELECT 1stTable.* FROM 1stTable JOIN 2ndTable ON 1stTable.ID = 2ndTable.ID WHERE 2ndTable.ids = 42 instead of that ridiculous WHERE I posted before. EDIT2: If you are using SQL Server, you can use the command OFFSET/FETCH to make pagination
Wouldn't that have the same problem as '%42%' though, i.e., it'd also pick up 142. 
yes, on the split function, very big no on the implementation. 
you store data on disk. Yes I know of SSD's, but I also know what SSD's cost in a SAN. So disk. How does "noSQL" store data on disk, where you just wave your magic wand and "add a new tag in the set of data and you'll be able to retrieve it", that in any way differs from "ALTER TABLE add [magic] varchar(255)" ? If you are talking about schema less datamodeling, let me tell you, I can add a XML, JSON, or just Binary column to any table, and go "fuck it, let the frontend make sense of it" in any rdbms you care to name as well. We do datamodeling for type safety and constraints. We do it to have not null, to have foreign keys, primary keys, to have check constraints, index defintions, unique indexes, filtered indexes, cost based statistical query optimization, data partitioning.... those things. if you want to not have that, then you can just as well dump your data on google docs in csv format and call it a noSQL database. And Google Doc'ing a bunch of CSV's is pretty close to a good number of noSQL "databases" as far as I read many specs.
Several solutions were given which will work, but understand that the performance of this is going to be bad. You're always going to force a full table scan, and then a substring search of each row.
This is actually a pretty good way to introduce NN to sql professionals because it's easier to read code in your own language than in some language you don't use every day or maths formulae you might have never seen. I might try this out for in SQL Server 2016 with inmemory tables, natively compiled procedures and table types and clr functions. It is not entirely silly because the data you are learning on might be in sql server and not in hdfs/blob storage anyway. 
I like how you explained to /r/sql what SQL stood for.
because I've seen people making threads with the name SQL and people end up asking if they were asking about Microsoft SQL Server or SQL the language. I am asking this question in another subreddit as well, so the redundancy is there so people will know with 100% certainty what I'm referring to without having to read the entire thing for its context. Did you think I was being patronising for saying that? If so, I assure you that was not my purpose. If anything, I did that out of consideration, for those who do not have the time nor the inclination to read the entire post.
FYI the query that runs in 11s that I mentioned in the post: select gpe_card.gpe_Cardid, Min(Gpe_CheckOutDate) as MenorData from Gpe_card as Gpe_card with (nolock) inner join Gpe_reservation as Gpe_reservation with (nolock) on gpe_card.Gpe_cardId = Gpe_reservation.gpe_cardid where Gpe_reservation.Gpe_expired = 0 and Gpe_card.statecode = 0 and gpe_reservation.statecode = 0 and gpe_card.gpe_cardprogramid = '889c972a-f760-e011-990c-00155d001110' and (Gpe_roompointsamount is not null and Gpe_FBPointsAmount is not null and Gpe_OtherPointsAmount is not null) and (gpe_card.Gpe_ExpiringPoints is NULL OR gpe_card.Gpe_ExpiringPoints = 0) group by Gpe_card.gpe_cardid Union select gpe_card.gpe_Cardid, Min(Gpe_CheckOutDate) as MenorData from Gpe_card as Gpe_card with (nolock) inner join Gpe_reservation as Gpe_reservation with (nolock) on gpe_card.Gpe_cardId = Gpe_reservation.gpe_cardid where Gpe_reservation.Gpe_expired = 0 and Gpe_card.statecode = 0 and gpe_reservation.statecode = 0 and gpe_card.gpe_cardprogramid = '889c972a-f760-e011-990c-00155d001110' and (Gpe_roompointsamount is not null and Gpe_FBPointsAmount is not null and Gpe_OtherPointsAmount is not null) and ((select COUNT(gpe_eventid) from Gpe_event as Gpe_event with (nolock) where Gpe_Type = 1 and Gpe_event.createdon &gt; '2016-06-04' and Gpe_event.Gpe_cardId = gpe_Card.Gpe_cardId) &gt; 0) group by Gpe_card.gpe_cardid edit: I suck at reddit formatting... how do you guys do those clear and beautiful posts? :)
 DECLARE @datevariable DATE SET @datevariable = '2016-01-03' -- six months -- SET @datevariable = '2016-06-04' -- one month DATEADD, learn it and learn to love it. DECLARE @datevariable DATE SET @datevariable = DATEADD(MM, -6,GETDATE()) I use it when I need to set date ranges as well declare @SDate date declare @EDate date SET @EDate = DATEADD(DD,-1,GETDATE()) SET @SDate = DATEADD(MM,-3,@EDate) This for instance sets an end date of yesterday and a start date of 3 months before yesterday. 
This is excellent. Thank you.
When building software that reads databases through an ODBC or similar connection, using a select * query as a proxy for a describe operation or a read of INFORMATION_SCHEMA is often the best way to obtain object metadata. When building row level security views that filter tables with no projection, maintaining the column list for those is a heavy, pointless task. Even worse if you are generating thousands of them. The * is a valid operation for metadata passthrough in a number of circumstances. I would certainly never recommend it for a high volume or schema bound application, but not every application is that way inclined, just the majority.
^ this is definitely more scalable. 
SQL 2016 version: SELECT * FROM table WHERE ids IN (SELECT * FROM STRING_SPLIT('12,545,3,2342',','))
Ya buncha pedantic self-righteous twits. lol The Venn is perfectly fine.
Use Venn diagram but explain that it may be a Cartesian product? I think it's still useful, the other graphs you cannot draw on a board really quick.
I'm not understanding exactly what you want to achieve, here are some doubts: In the 1st line, the MemberID_Previous 1298 has the MemberID_Current 1237848 but none of those IDs appear anywhere else in the Data set, what that info means? The ID 502432011 in the line 13, it shows that Current and Previous MemberID are the same and in the line 14 it changed to 502471011 and that new ID doesn't show up anymore, what is the result you want in this case? Can you give a couple examples of the expected result of your query?
Hi rbardy, your example mentioned with ID 502432011 would have a end node at 502471011. At this point, ID 502432011 would be the Master Id, and all dependencies would have a hierarchy linking to it. 502471011 would be the 3rd child of ID 502432011. The Expected output would be Master ID Child ID (Last record in path) Steps (branches to reach Child ID)
Good thought, master MemberId is the edge link for this chain of changes. I would consider all chains a) loops (a&gt;b,b&gt;a as individuals. Both A and B would be Master ID since they didn't follow the specified logic. For B) Both A and C would be Master Ids since the node stopped at B, regardless of them sharing prior Ids. For C) I'd consider A, C as masters, since B is a subset of C, with D and E dependencies. Does this make sense slightly?
Are you working with JDE? I don't know of any other system that uses that kind of date format..
Only slightly :) What I'm saying is that your 'definition' need to cover the 'exceptional' cases. E.g. 'dont follow the specified logic' - what logic is that? It's a loop in the dependency graph, how do you deal with those on 'definition' level? BTW graph loop detection in SQL is as hard if not harder than your original task. Anywho, if you only need to detect left 'leaves'/'roots', and you've dealt with the loops somehow (otherwise the below would discard all members of loops), just do select left from myAdjacencyList where left not in( select right from myAdjacencyList) PS. Adjacency list is not the best structure for edge detection/traversing. Just FYI.
As soon as I get home, I will give it ago, and use it for my new blog post, interesting setup. As my SQLprompt expired, will give that one a go. Also SQL Assistant is neat. 
You can create a dataset with required dates first (dynamically or dedicate a separate table for that) and then join existing data to it. If it was SQL Server, i would use LAG (or LEAD) function to determine windows for the join statement, or use something like [this](https://blog.jooq.org/2015/11/07/how-to-find-the-longest-consecutive-series-of-events-in-sql/) to find min and max values of each range. EDIT: now that I think of it, - ROW_NUMBER() solution in the link wouldn't fit here, since we need to find gaps between ranges, not ranges themselves. So yeah, LEAD function and then join ON date &lt; lead_date and date &gt;= actual_date or date &gt;= actual_date and lead_date is NULL
if I do this, I'll still have data gaps in the new dataset though, right? So one way or another I'll have to apply some logic that says, if entry(x) is null, entry(x) should take the last, non-null value in the list. I think applying this logic is the bottleneck. or, does the join somehow take care of this in a way that i do not understand?
This is the power of autism
I think you're overthinking it, he just needs to use the theater table for (a).
Slam dunk, that's right. So lets put it all together. SELECT TNum , Address , Seating FROM Theatre WHERE Seating &gt; 300 ORDER BY Seating ASC Ascending order is something you can Google but in simple terms it will rank your data starting off small and then getting higher. `ORDER BY Seating DESC` will do the opposite and start with the highest value and then descend. Do you understand all of this? Lets move on. In English: List the name of any Company which has ever performed any play by Shakespeare.
You're going to need to do a little better on this one. Read the question carefully: (b) List the name of any Company which has ever performed any play by Shakespeare. You only have to return (1) thing, not three. And where what Shakespeare? Remember on the test you're going to have to write the syntax specifically and be able to know what each part does. 
SELECT: COMPANY FROM :CID: 001
Lets look at your answer from from question A: &gt; SELECT Theatre FROM:TNAME WHERE: **Seating** &gt;300 You need to add the equivalent of that "seating" in your answer 
i dont get what a and b is for?
Looking for a resource for PostgreSQL also. The official documentation if free and has TONS of material btw.
I don't have those levels of permissions unfortunately 
Create a table which is every possible date, then left join it to your other tables, and use some kind of isnull() for the empty vlaues. Also, optimized for space? Digital storage space is literally the cheapest thing in business.
You're trying to reference an alias in your WHERE clause, which MS SQL doesn't support. This will work: select VirtualMachineID, MountPoint, ((100.0 * FreeSpace / Capacity )) AS PercentFree from dbo.VIM_VirtualMachineVolumes where ((100.0 * FreeSpace / Capacity )) &lt;= 10 
probably performance and not space. I don't really know. I'm kinda on the bleeding edge of my own technical limits as is. Thank you for the help.
replace that entire formula with this keyword -- CURRENT_DATE 
Something like this in the where clause might work WHERE field NOT IN ( 'list item 1' ,'list item 2' )
I did that, but I exceeded the character limit. There's 171 items and all of them have really long names.
Thanks, this helped me out a lot.
&gt; ... put all of your filtering within the JOIN statement (prompting an index seek) rather than the WHERE statement (sometimes prompting a table scan)... Is that true for INNER JOINs? It sounds off to me and quick Googling tells me otherwise. This is coming from T-SQL land too, maybe it's different in the DBMS you're working in?
http://www.w3schools.com/sql/ is a good start
Pluralsight.com and Lynda.com are what I use. I love both sites.
Poking around the spiceworks website and googleing "spikeworks sql" gives clues that that Spiceworks has SQL Server monitoring software. I assume that's what you're seeing. So, what do you know about Spiceworks and in what context do you use it? Both SQL Server Management Studio and SQL Server developer edition are now downloadable for free. https://blogs.technet.microsoft.com/dataplatforminsider/2016/03/31/microsoft-sql-server-developer-edition-is-now-free/ https://msdn.microsoft.com/en-us/library/mt238290.aspx Its difficult to say what you need without understanding more context. 
http://www.codecademy.com has some beginner sql stuff. It's not really in depth but it's interactive and very fast to learn.
Maybe this is what you are looking for: SELECT Month(order_date) as OrderMonth, count(*) FROM OrdersTable WHERE order_date &lt;= end_date and order_complete_day &gt; end_date GROUP BY Month(order_date)
Honestly with newer versions of MS SQL, the query plan is determined for you so it's a moot point. But I've observed faster execution when using the ON/AND vs WHERE However, WHERE is more readable and compliments changes to code (convert INNER to LEFT join for example)
A few problems with this solution... This does not account for orders that are still open. The order_complete_day could still be null. This does not account for orders that span more than one month. Suppose an order was entered on June 29th and was closed on July 3rd. It should be counted in both June and July. The *Month* function will not work in some databases.
You can check out our wiki as well /r/SQL/wiki/
Here is one solution: with month_list as (select to_char(add_months(&amp;start_date, level - 1), 'YYYY (MM) Month') the_month, trunc(last_day(add_months(&amp;start_date, level - 2)) + 1) first_day_of_month, trunc(last_day(add_months(&amp;start_date, level - 1))) last_day_of_month from (select &amp;start_date, &amp;end_date from dual) connect by level &lt;= months_between(trunc(&amp;end_date, 'MM'), trunc(&amp;start_date, 'MM')) + 1) select the_month, (select count(*) from order_table o where order_date &lt;= m.last_day_of_month and (order_complete_date is null or order_complete_date &gt;= m.first_day_of_month)) order_count from month_list m; So for this input: ORDER_DATE ORDER_STATUS ORDER_COMPLETE_DATE 1 1/24/2016 4:24:48 PM C 1/26/2016 4:24:48 PM 2 4/3/2016 3:40:41 PM C 4/8/2016 3:40:41 PM 3 5/23/2016 3:40:31 PM C 5/25/2016 3:40:31 PM 4 6/22/2016 3:40:23 PM C 6/24/2016 3:40:23 PM 5 6/26/2016 4:08:32 PM C 7/5/2016 4:08:32 PM 6 7/2/2016 3:40:12 PM C 7/5/2016 3:40:12 PM 7 7/4/2016 3:40:06 PM C 7/5/2016 3:40:06 PM 8 7/5/2016 3:40:57 PM O I get this output: THE_MONTH ORDER_COUNT 1 2016 (01) January 1 2 2016 (02) February 0 3 2016 (03) March 0 4 2016 (04) April 1 5 2016 (05) May 1 6 2016 (06) June 2 7 2016 (07) July 4 Note that row 5 is counted in both June and July because it was opened in June and closed in July. Also note that line 8 is counted in July even though it is still open.
This is the perfect answer. What's your opinion on using except instead of not in?
Personally I find union queries to be a bit ugly, I feel they detract from the core ways in which SQL works. Additionally, I suspect [but it probably depends on database] that NOT IN can be easily optimised by the query planner where appropriate indices exist, wheras set unions might require a temporary b-tree. 
I said CURRENT_DATE doesn't work in JDE. That doesn't mean there's another way in JDE? Sorry? lol
[TechOnTheNet](https://techonthenet.com)
why did i not think just format it like that... oh my. it's been a long day. thank you. that should work perfectly haha. cheers to learning sql 
[SQL Queries for Mere Mortals](https://www.amazon.com/SQL-Queries-Mere-Mortals-Hands/dp/0321992474) is an excellent beginner's read front to back. The same guy wrote a database design book, which I did not find as helpful. As for software, it depends on what database system you end up using. I use SQL Server and a little MySQL, but I know PostgreSQL, SQLite, and many others are popular. If you're learning indipendantly, I'd start with MySQL , as it is free (open source) and you will be able to learn the basics of SQL in any SQL database. It also has a good free GUI development tool, MySQL Workbench, and a robust online community. Like any programming language, the best way to learn is to build something with it. So start thinking of something you want to build, so you'll have something to do once you know enough to play around.
[removed]
But then I get two result rows for each value, a row for good and a row for bad. Which means that I still have to export it to find make calculations and such. This is a very simplified version of the data I'm working with. 
Are you trying to do something like this? DECLARE @datevariable DATE SET @datevariable = 'whatever' DECLARE @badcount INT SET @badcount = SELECT COUNT (*) FROM [table] WHERE DateField = @datevariable AND status = 'bad' DECLARE @goodcount INT SET @goodcount = SELECT COUNT (*) FROM [table] WHERE DateField = @datevariable AND status = 'good' SELECT @datevariable, @badcount, @good count *** Or does it need to all be in one query like: SELECT good.goodcount , bad.badcount , @datevariable FROM ( SELECT DateField, COUNT (*) AS goodcount FROM [table] WHERE DateField = @datevariable AND status = 'good' GROUP BY DateField) good INNER JOIN ( SELECT DateField, COUNT (*) AS badcount FROM [table] WHERE DateField = @datevariable AND status = 'bad' GROUP BY DateField) bad ON good.DateField = bad.DateField 
You can count only the goods and bads. select identifyingvalue ,count(case when status = 'good' then 1 else 0 end) as good ,count(case when status = 'bad' then 1 else 0 end) as bad from table where date='date' group by identifyingvalue
Hi. I used to work at Home Depot and had to create a similar data model to find "an assortment of like products" based on all the shitty different ways different distributors would type out (for example) "3 inch brown deck screws" I had to do this so I could then create a pricing variance report to identify which vendors fucked up and put stupid or bad prices in (nails at $100 each or 6" 20'x20' mahogany boards for $3) First thing you do is get to cracking on assembling a reference table with every single product in it. Organize it by category. Include individual attributes. Have a text descriptor comprised of every single attribute. Get that shit as detailed/verbose as possible. Then, study the Levenshtein distance model so you know what you need to accomplish. Then, steal someone's code off of stack overflow or something and fine tune it to meet your needs. Filter, filter, filter. When you run this thing as is with no filters it compares every character in your data set against every character in your data set. The DBAs are gonna kick you in the shins if you don't filter by category, attribute etc and have other filters designed to clear out the obvious chaff. This will take you a few weeks. It's beautifully scalable and pretty efficient if done right. Good luck bud
No, just... fucking... no... there is no goddamn reason why the Levenshtein distance model should be used for a goddamn SEEK AND REPLACE of parts of a description. That's like hunting for ducks with a goddamn tank. The hell is going on here today? First I get some guy who tells me to use an equal sign to find a word in a sentence when that's not how the equal sign works and now its "make an overly complicated equation with millions of records worth of reference table" 
Well I mean, I know saying levenshtein sounds wild &amp; crazy and all, but it's just a formula. You can scale it down. I did when doing dev testing for that project I talked about. You have a table with string one, and one with string two. Or table with two fields. Careful about that though. Indexing is a bitch. You set the number of characters to search for every record in string one, then string two. You set comparison length and criteria, and you assign ranking. Store that separately. It scales up. Just in case managers love the data and they want a lot more of it more frequently. It scales up better than other models. Alternatively you could make a quick and dirty rules engine/table. One field says like "distributor = x" based on the different shitty ways they consistently fuck up when entering data. One field could say like "category = x" and is based on the way basically everyone fucks up for a given category. Third field just contains a rule: replace(replace('D/C','dc'),etc..) But this model requires a lot of hands on love and attention if the model becomes popular. But I guess it's less complex.
those counts will always give the same result!!!
I am guessing you want want something like this: SELECT Identifyingvalue , COUNT(case when status = 'good' then 1 else null end) Good_Count , COUNT(case when status = 'bad' then 1 else null end) Bad_Count FROM table GROUP BY Identifyingvalue
That's not the "first" Brad. It's all people named (Brad, &lt;LastName&gt;, &lt;MiddleInitial&gt;) grouped together. What are you expecting [group] to do?
What are you trying to do? If you GROUP BY all the columns it's the same as doing SELECT DISTINCT (which you are doing as well for some reason). Your GROUP BY here is redundant.
Hey, show some respect to people who actually take the time to go over your less-than-optimally described problems. :-) It was getting really late here at Central European Time so yeah I didn't go back and change my sql after your reply. In any case, your solution will involve a translation table of some sort and tbh I have no idea what level of competence you are at; from your answers you seem to know what you're doing and setting up and using such a table really shouldn't be too much of a problem. What level of answer are you actually looking for? 
its actually quite simple to do this in a query. I actually encourage people to not use the OE - especially if they need to look at the same thing regularly. If you figure out how to do something via a query, you can save the query as an .SQL file and have something to come back to later. Plus, its good practice for writing SQL. Take a look at the results of these queries in your database: select * from INFORMATION_SCHEMA.TABLES select * from INFORMATION_SCHEMA.COLUMNS You can add a WHERE clause to these to filter them however you please. Furthermore, you can look for columns in specific tables. For instance, you can write them like this: select * from INFORMATION_SCHEMA.TABLES where TABLE_NAME not like 'STATIC_RECS_%' select * from INFORMATION_SCHEMA.COLUMNS where TABLE_NAME not like 'STATIC_RECS_%' Also... another tip: Say you want to quickly get the meta data (info about it) of a table while in SSMS. Type the name of the table (or highlight it in the query window) and press ALT+F1. This executes sp_help with the name of the table being the parameter value. It will give you almost anything you might need to know about the table including columns, column data types\lengths, indexes, constraints, foreign key references, etc. I use it all the time. 
&gt; I'm clearly dumb Also, your write-up and question were well-stated. If you're dumb, it's not very clear ;)
"I'm clearly dumb" Also, your write-up and question were well-stated. If you're dumb, it's not very clear ;)
- I'm clearly dumb Also, your write-up and question were well-stated. If you're dumb, it's not very clear ;)
i've never even heard of JDE, so i can't do that how about if you explain exactly which format you want? also, **please identify the DBMS you're using** (based on DAYOFYEAR i'm going to guess MySQL, but you need to confirm this)
Wow, thank you, that's really interesting. I'm doing some experimenting with it now!
DISTINCT is exactly the same as GROUP BY with all columns, and when I say they are the same I really mean it, the SQL execution plan is the same for both.
This is a good example to when you use trigger in a table. In your table you can create a trigger for Insert and Update and check if the new Field_A value meets your requirements, it will look something like this: CREATE TRIGGER [TableName_IU] ON [TableName] FOR INSERT, UPDATE AS IF EXISTS(SELECT Field_A FROM INSERTED WHERE Field_A = X) BEGIN &lt;new task procedure here&gt; END in the trigger you can do whatever you want, call a procedure, make inserts, define variable and do the logics you need and so on. EDIT: Some clarification, the "table" INSERTED is a temporaty system table that gets all the new values of the table the trigger is running, in this case INSERTED = new values of [TableName], if you want to get the old values you use the "table" DELETED (this is great to create detailed logs)
Haha I'm on mobile. I was getting an error when submitting my comment so i tried reformatting several times. I'm just surprised that there was no spam filter! I will leave the other iterations. Maybe catch a few extra downvotes :P
Polling is generally not an efficient/scalable method for databases. If you find that your system relies on it often, you should really consider switching to the observer pattern and using queues to communicate changes.
You a real 1
/r/LearnSQL
Hey again: select ( (select sum(sample2) from table1 where date = 116188 and var2 = 'H' and var3 &lt;&gt; 0 and var3 in (select type from table1 where var4 &lt;&gt; 0 and var5 not like 'doesn't matter') + (select sum(sample2) from table1 where date = 116188 and var2 = 'H' and var3 = 0) ) / CAST(( (select sum(sample) from table1 where date = 116188) - (select sum(sample) from table1 where date = 116188 and var = 0) ) AS float) That should work.
Need some brackets on your OR clause ( d.dt &gt;= PriceDate AND d.dt &lt; LeadDate ) OR ( d.dt &gt;= PriceDate AND LeadDate is NULL ) 
SELECT * From tblSupplierProducts p1 JOIN tblDescChange d1 ON LEN(p1.ProductDescription) &lt;&gt; LEN(REPLACE(d1.ProductDescription,p1.ProductDescription,'')) Something like this maybe?
Let me start off by saying your phrasing of the question is certainly confusing. You begin the question without context and seemingly exclude some vital information for us. What I've interpreted is that you're scraping data from Wordpress and want to save it to a SQL database to mine later. You seem to be asking whether each Wordpress site that you mine should get its own table or whether you should use one table to describe multiple properties of multiple sites. I believe that the topic at hand here is database normalization. Read up a bit on this as it forms the backbone for relational database use and will help you understand how good database form is practiced. The idea is to reduce duplicate data entries by describing different "levels" of data [detail] in different tables, with relationships between properties to tie these different levels of data into the end user's desired view. For instance, if we are storing sales data for a company, we will see that some customers place many orders over time. Rather than store that customer's name every time they order, we create a customer table to store information about that customer and file it as customer "123". Each time that customer orders, we store "123" along with the quantity and price of the goods ordered, which keeps us from needing to store duplicates of the customer's name, address, phone, e-mail, ... When it comes time to display sales data, we join the customers table onto the orders table and display "Johnny Appleseed of 1 Apple Way, Appleton, NY ordered 16 widgets". You need to find a way to normalize the dataset as you store it so that you may extract it in the future in an intuitive manner without bloating your database size. Personally, I would rather have 1 table with many rows than many tables with few rows however the goal is to normalize everything. You might have a "site" table which describes the Wordpress site itself, a "Users" table which describes user accounts, and a "Site User" table which stores relationships describing who has access to which site. A little rambley, but I hope that helps. EDIT: Look up "Xref" or "Crossreference" tables
Wow; let me start off by saying thank you for the detailed response. I mentioned a wordpress database simply because I've seen them have decently large sets of data in thouse types of tables. What Im scraping is overwatch game data. Each Hero (21) has upwards of 12 tables. I would just keep all the hero data in one place but I also... hmm, I could keep them all together and create another table that that says what fields are in what table. Anyways - based on the way I was doing it; 21 heroes with 10 tables, with 10 stats. I was considering ... arg, now my auto generated sql is causing a php error. And Im going to dinner. From what I could understand, either way I was going to do things the tables would have been in third normal form. All one to one relationships with no redundant data, and anything that can be calculated isn't stored. I could be missing something in that but that's what I remember of it. It would have just either been a really tall database, or a really fat one. Thank you! I will probably respond again to this unless I get home and fiddling with my other idea works better.
I'm reading the entire thing as a "large datavolume", so that is why i'm nitpicking. &gt;When a client connects to the database, they will be issued a SimulationID via something like "SELECT MIN(SimulationID) FROM Jobs WHERE JobStatus = 'ToRun'". make that a foreign key integer (not int 32) and normalize out the jobstatus. 5 chars will be at least 5 bytes, and you want to keep indexes small. If you want to filter on that, you want to index, keep index size small for the sake of the caching. My guess is that 8bit will do fine. &gt;AvailableControllers I simply don't understand what a controller is supposed to be. Since most of the other stuff relates to something similar to the thing I don't understand .... well, I don't understand your datamodel. I'm left with giving you general advice. 1. Datasize matters a lot when talking large volume. Int16 is half of int32, and on 1bn rows, that makes a difference. So think really hard about the datatypes you use. Its not c# or java running on the client, its the database backend, where those things still matter. 2. Think about your dataaccess paths. What table gets queried with what filters? Index accordingly. Also index every foreign key you have, I know this sounds like a given, sadly it is not. Also keep in mind, you can't just nilly willy index everything, inserts and updates will be slower when the DBMS has to also update the indexes. You really got to put some thought in that one. 3. 100M records is not 100M records. Rowsize of 20Bytes... 100m*20 Bytes = 2000 Megabyte = nothing. Throw in a couple of nvarchar(max) columns, numerics, there such, and you are gonna start feeling some pain. 4. Its all about the SQL you write against your model. Read best practice, only use sargable expressions, look at the execution plans. If you run table scans left and right, its gonna hurt, no matter how you design the tables. Btw, what DBMS are you using? I might have a 5th and 6th if I knew what you are actually running that on.
Whew. Thanks for the feedback! It's actually simulated; the system is modeled by a set of differential equations, the output of which is put through a "noise maker" that messes up the input to simulate a sensor. This is the sensor value that the controller uses to decide what to do next. The main goal is to pretty much run all possible permutations of controllers, sensor error models, inputs to fuck up the system, and initial conditions to get a Monte Carlo analysis of sorts on the final dataset. Other posters had...words to say about all of the tables I'm making. What do you think? I want each controller to have its own table to log controller specific data and couldn't figure out how to accomplish that in one table. 
Also are there any other DBMS you'd personally use for this purpose? And why Postgres of the options?
40ms per row, 15 clients ~ 750 row per second .... I'd say yes, that is the point where you'd want to consume those clients in a service, put it in a buffer, and write off the buffer into the DB. Everytime you write to the DB, the DB will start a transactional context, to make sure that if something goes wrong, everything done is rolled back cleanly. That does produce quite some overhead. Writing 1000 rows is far faster than writing 1000 * 1 row (depends on the dbms, but bulking 1k rows can be as little as 4-5 times slower than writing a single row (give it a take, i don't run benchmarks this time of day)). Everything else taken into consideration, you really don't care that much for analysis performance I think. You will most likely run a test, and after that, run an analysis. So read performance is not your concern. If that is the case, that is a very rare case as far as databases are concerned, so, well, don't believe everything you read on stack overflow if I'm on the right track. Dump the data into the db, as fast and as non blocking as you can, to deal with the data volume generated, as I said, I'd recommend a buffering service as an intermediate. To evaluate the data, you can always at night or on the weekend, take the data you have in the db, and transform it into a more efficient model designed to run analysis off, if need be that is
Since you said you might be running a dev edition, I'm not gonna delve to deep into that ;) In any case, that is exactly my point, I don't think that you care much about the performance of the analysis you run on the data. You can focus on the writing of said data. Write it the fasted way you can, consume the most data points you can. Create a database per test you are running, why not, you can script that quite easy. For the analysis, create a seperate database, where you select the data of your "datadump" into, seperate the two, and you will have a lot less problems in the long run. You have the possibility to seperate the write concern from the analysis concern, if you can do that, do it. You don't have to care about the downsides of "I can write fast, but the performance of the queries suck" vs "I can run fancy analysis, but I have trouble dealing with the data sent". That is a luxery you should take advantage of tbh. 
So in a denormalized table, would I need to have Controller and Model inputs in the same amounts? I usually log about 5 Model rows for every 1 Controller row, to minimize data duplication, as the controller only iterates about once every 200ms as compared to the 40ms model, but for the read only database it could just be duplicated as many as I need to fill in those spaces. By the way, thanks for all your input. It's really valuable to me to have someone to bounce ideas off of!
We use a combination trigger and broker for this setup. The logic needed on the new rows is too complex for a stored procedure alone, so the trigger adds an entry to a broker queue, then the processing application just asks the broker for the next action item. Cool thing is, if you can process your items in parallel, this scales out well, you could have 3-4 or 200 of these applications standing by for new queue items from the broker, at any time, and that could even change dynamically.
This is the worst "How To" with no information. You're showing us some random application you don't link, reference, or mention to the viewers and nothing you do is in PostgreSQL. Crap.
I'll take a crack at this, let me know if I'm missing something. Devices Table. DeviceId, DeviceName columns. (1 row per hardware device) DevicePorts Table. DevicePortId, DevicePortNumber, DeviceId columns. (If a device has 10 ports , 10 rows) DevicePortConnections Table. DevicePortConnectionId , DevicePortConnectionName columns. (1 row for each connection) DevicePortConnectionsPorts table, DevicePortConnectionId, DevicePortId columns. (2 rows per connection, 1 for each port) A query to see on what ports 2 devices are connected? Select d.devicename, dp.deviceportnumber from devices d Join deviceports dp on d.deviceid = dp.deviceid Join DevicePortConnectionsPorts dpcp on dpcp.deviceportid = dp.deviceportid Where d.devicename in ('device1','device2') (Sorry done on phone so I hope that makes sense AND works)
I originally posted this in /php as it is aimed more at web designers who I feel are, in general, not particularly well-versed with db's &amp; sql. I would like to hear opinions *backed with links to docs, etc* where appropriate, as I suspect at times our opinions are not necessarily up to date with the latest releases.
You have to define the table and mapping to a file as well, e.g., Create table patients (firstname,lastname); Map patients TO 'patients.csv' DELIMITER ','; and then you query. It will create a bash script that executes your query on the csv file directly.
Sorting on a thing cast to a string is terrible performance and hard to index usefully. Enums also are keeping stuff that should be data (field values) in the schema. The other solution to this whole problem is to have an fk onto a "lexicon" table with a unique index. Better for performance. You also did your research. Enums don't work everywhere and make portability to other dbs a problem. Really, there's no reason not to use a separate table, and lots of reasons not to use Enums. So as don't use Enums in my work 
Ok, well that is nice and interesting.
Posts from brand new Redditors require moderation to avoid spam.
You will still use it when you're an old, grey (and inevitably grumpy) DBA
Take a look at the query plans. One will have nested loops everywhere and will never finish. Update your stats.
It was tagged though... we're getting there slowly!
The option is now called just "Table..."
Thanks for the quick reply but that's not what's going on here. The closest option I have to "Table..." is "File Table..." and selecting that only opens up a query window with the Create FileTable template. No designer :( Right clicking Tables only provides me with the following options: - File Table... - Filter &gt; - Start Powershell - Reports - Refresh 
For free. With online exercises. http://www.studybyyourself.com/seminar/sql/course/?lang=eng.
You wouldn't happen to have any idea if it's possible to download the database they use in their examples somehow, would you? I tried searching around, but I'm not having much luck.
For MSSQL it's possible to pay to do just the certification exam without doing the course. Buy a study guide and look around for free practice exams and you can get certified for a few hundred dollars. [Here's a link](https://www.microsoft.com/en-ca/learning/exam-70-461.aspx)
This is definitely the route I'll probably go. That's really cheap and it works toward something formal I can put on my resume. Thanks for taking the time to reply.
Thanks for all the feedback everyone. Honestly wasn't expecting a response.
You have a lot of options, this is one typical pattern for using a passed in parameter: Case when @param = 'All' then industry else @param end = industry If you pass null as a proxy for All you can just do Coalesce(@param, industry) = industry Depending on how ubiquitous the industry filter is you could create an indexed view that is basically Select industry, other field, other field Union all Select 'All', other field, other field And then just return Select * from view where industry = @param Anyway, depends on how baked in the new request needs to be.
Save yourself time and money. First pick the SQL database you want to get certified in. Then go to amazon and buy the highest rating learning guide and go through. Then buy the actual training materials from the database company and go through it. You do NOT have to attend a class. The classes tend to be a week or so and that doesn't cater to lifelong learning. Go through the manual over a month and then take the exam. You saved yourself thousands of dollars and got your certification.
At first read, I thought this was a question about infidelity.
Absolutely. I usually do have multiple versions. As I mentioned, this is a new machine so I'm still whacking away at everything. Anyways, I ended up rolling back to the June 2016 update and everything works fine. Thanks so much for the help again.
I've done this for Microsoft certifications. If you're fine with learning things on your own and have the motivation to stay on top of the material on a regular basis, I recommend this as well!
I think [this](http://stackoverflow.com/questions/10337944/sql-unique-constraint-across-multiple-tables) is what you're looking for.
I haven't touched MySQL in a couple of months...but are you using &amp; instead of % for wildcard? So you're trying to find first names that contain the letters 'an' right? whether that be Ryan or rihanna?
Oops, yeah I was using the wrong sign here. I'm a dummy. Thanks!
True, I guess I just got over zealous. The number of times I have had people wondering why their results for a query like below aren't what they expect is just too many to count. Q:"Why does it return 2,3 when I specified num1 = 1" A: "Brackets." But you are right, in your case it's perfectly fine ;) CREATE TABLE #numbers ( num1 int, num2 int ); INSERT INTO #numbers VALUES (1, 1), (2, 2), (1, 3), (2, 1), (1, 2), (2, 3); SELECT * FROM #numbers WHERE num1 = 1 AND num2 = 1 OR num2 = 3 
thank you.. i'll try that... I felt everything was right and debugged it the whole way through lol.. i knew it had to be something simple that i overlooked.
This is a really well written guide for beginners to understand how SQL works, with some really sweet diagrams :).
This is pretty good! Although the thing I primarily struggled with when starting out were the different kinds of JOIN.
Not unless you've stored that association somewhere. A guid is just a huge random number. Guid means Globally Unique IDentifier. The whole idea is that the number space is so big that any one, anywhere can pick a random number from that space and the chance of some one else picking that same number are ridiculously tiny. The key to this all working is that the number is truly random and has nothing to do with anything like machine name or anything else. Fundamentally there should be no way to deterministically figure out how the number was picked. That's not to say people don't use guids completely wrong all the time and do things like produce sequential series of guids. If you're doing that, you probably shouldn't be using a guid in the first place.
Do you mean the objectGUID property of the Active Directory object? If so, there is a way to create a linked server connection to the AD catalog and query it from your SQL Server. EXEC sp_addlinkedserver 'ADSI', 'Active Directory Services 2.5', 'ADSDSOObject', 'adsdatasource' GO SELECT * FROM OPENQUERY(ADSI, ' SELECT name FROM ''LDAP://DC=contoso,DC=com'' WHERE objectClass=''computer'' AND objectGUID = ''your_guid_here'' ') AS tblADSI
Looks like a great guide. I recommend using ISO8601 for the date format just so folks new to SQL can start seeing that and perhaps know to use it in their database. https://xkcd.com/1179/
Hey, thanks for the input. I'm in IT and work very closely with the DBA. We're always on the lookout for ways to save time and reduce work so that we can move onto bigger projects. I'll have to check out ELK and look into RedGate more. This script at https://www.brentozar.com/blitz/ is something I've found so far among several other resources. Haven't put them into use yet. Also trying to move onto extended events in SSMS and break the habit of using profiler. 
You could sub query to find if there are nulls then use a case statement. SELECT x, y, CASE WHEN MAX(z) = 1 THEN NULL ELSE MAX(d) END AS d FROM ( SELECT x, y, d, CASE WHEN d IS NULL THEN 1 ELSE 0 END AS z FROM table ) AS S GROUP BY x, y (edit: fixxed formatting)
I think I understand. SQL is weird. I'll try it tomorrow when I'm back at work and let you know!
Yes, thanks im horrible at explaining stuff. Looking for the client names in AD indeed, but Im pretty new at SQL so I'm not sure my query did find the client name from AD though. (I put the solution Query in the post edit, it was all googling and testing until it worked.) The GUIDS were part of the error messages the SCCM guys got as output when something didnt work and they wanted to know which clients who were the problem so they could test a few things.
What are you hoping to accomplish with this? It's not bad per se to have 2 statements for this.
MySQL Documentation page for the lower_case_table_names parameter: http://dev.mysql.com/doc/refman/5.7/en/identifier-case-sensitivity.html
You'll want a recursive query. [This article](http://blog.sqlauthority.com/2008/07/28/sql-server-simple-example-of-recursive-cte/) has as a good explanation. 
Basically, this means that you'll have to wrap whole query into the EXEC statement and insert parameters by either sp_executesql or directly into the query string.
Being on MS SQL, you could really see if you can utilize hierarchyID: your table would shrink to the (hID, Node) and the query similar to one you are asking for would be written as select p.hID, p.Node, levelsRemoved = (c.hID.GetLevel() - p.hID.GetLevel()), c.hID, c.Node from (select hID, Node from myHTable where Node = @Param) p join myHTable c on c.hID.IsDescendantOf( p.hID) = 1 Here's an example how to populate hierarchyID based on your data: create table #tH( hID hierarchyid, node varchar(20) not null, primary key (hID)); insert into #tH (hID, node) values ( '/1/', 'Alpha'), ('/1/2/', 'Hotel'), ('/2/', 'Bravo'), ('/2/1/','Papa'), ('/3/','Charlie'), ('/3/1/','Romeo'), ('/3/1/4/', 'Delta'), ('/3/1/4/1/', 'Zebra'), ('/1/2/1/', 'India'), ('/1/2/2/','Kilo'), ('/2/1/1/','Lima'), ('/2/1/2/','Nancy') ;
Yup. Annoying as hell, but that was the only way it worked. I was doing a much easier query than yours, I was making a WHERE date &gt;= '2016-1-1' in the OPENQUERY, but I have to concatenate the quote symbols and that was enough to make it not work anymore, I had to wrap it and execute the string. BTW, those 2x quotes will also make your query to fail, instead of: '...AND AttemptComplete__c = ''true''... you'll need to do '...AND AttemptComplete__c = ' + char(39) + 'true' + char(39) + '...
Hey thank for the reply. I came up with a simpler solution than what you gave me. I'm just making the form that populates the database have only SelectFields (I'm working in Flask) with all the same options. Not technically what I was asking for, but it accomplishes what I was going for, and probably easier to implement than what's in that link. Thanks again!
Are there any joins in your query? That's usually where my duplicates come from.
Quick breakdown of what its doing in English terms: The inner SELECT is setting a flag 'z' = 1 if d is Null (else it sets it to 0). The Outer select checks if MAX(Z) = 1 meaning there is a null value of d. Since we default for all other cases of d a side from null to 0 and 1 if null, MAX(Z) will be 1 if there is a null somewhere in the tuples x,y,d it then sets the field to NULL otherwise it populates with MAX(d) .
Thanks! What I was missing was that z is set for every record, and then MAX is being used basically as a 'if there is ever a z == 1'. Really neat usage there.
what is hitting the table while you try and run it? If a lot of other stuff is waiting in line to read or write to the table, SQL server is going to keep sending yours to the back of the line. If you have to update a table in runtime, you can try updating them 4999 rows at a time: create table @id_list (unique_id int) while exists (select columnX from big_table with (nolocK) where columnX !=1) begin insert into @id_list (unique_id) select top 4999 unique_id from big_table with (nolock) where columnX!=1 update big_table set columnX = 1 where unique_id in (select unique_id from @id_list) delete from @id_list end 
brent ozar's stuff is great. follow his blitz results and your database will be humming along in no time. A good step 1 is to check that your temp DB and prod database have 1 file per cpu core, then find out the default size of your transaction log - if that has to grow during the day, that will cause slowness. I found the max size of the log at the end of the day, doubled it, and just gave it that much space - now it never has to grow and it never slows things down. Next, If you google it, there are scripts to find missing indexes. Third, I also had great results by getting rid of all the heaps. 4th - play with your threshold for parralelism. If it is too low, queries will get stuck in line waiting for 2+ cores to become available. If it is too high, some of your cores will work harder than others.
I'll give that a shot. Thanks!
I'm just trying to clean up some of my code and this one is sorta the messiest. 
Go to the Keys, select the PK, script it out to a create as. What does it look like? IGNORE_DUP_KEY = On / Off? What does the PK reference column wise? Also, check this: http://www.sql-server-performance.com/forum/threads/duplicate-rows-found-on-primary-key-table.9130/#post-50957
Any knowledge resources that you've found especially useful for DMVs? That's been on my radar for a while. And thanks! 
My brief spiel on the wait types. You have pin point troubleshooting and you have overall server baseline troubleshooting. For pin point, find the SQL guid you are trying to fix. Find the wait types associated with it, then examine the query plan. From there, research the wait types and figure out where to relieve the pain. For baseline, take a snapshot of queries and overall server wait type times. Do it again periodically. Red Gate and DPA do this every second and compile a list. Here are you wait types, here's the amount of time each query is associated to per type. The thought process is, tune the highest time query so you can make the biggest difference. In addition to looking at base time for wait types, I like to look at CPU / Memory / Disk / Network speeds. I historically archive and run sp_whoisactive along with some disk and memory/ cpu queries so I can correlate events on the server to the queries and look for patterns. Things get more complicated on VM's due to noisy neighbor VM's and not always having views into the Host stats. Some links: https://www.simple-talk.com/sql/database-administration/baselining-with-sql-server-dynamic-management-views/ http://www.sqlskills.com/blogs/paul/advanced-performance-troubleshooting-waits-latches-spinlocks/ http://www.sqlskills.com/blogs/paul/worrying-wait-type/ https://sqlserverperformance.wordpress.com/2016/06/08/sql-server-diagnostic-information-queries-for-june-2016/ https://www.mssqltips.com/sql-server-tip-category/31/dynamic-management-views-and-functions/ https://www.brentozar.com/sql-server-training-videos/playing-doctor-with-dmvs/ 
&gt;prior coding experience is SAS and a quarter of C++ Can you learn those in 2 days? Sorry, but that's your answer. You can learn some though and it's a simple enough language if you can work in set based logic.
Make a point to look at different functions. Just google sql functions. Have a vague idea of what the more common ones do. Even if you dont remember the proper syntax you can at least say you would use the function to solve the problem. 
Not really - that was just one example of one efficiency. Think also about storage space - is it better to have the word `Rocks` 500 times, or the number `1` 500 times (and the word `Rocks` once). It might be interesting to do some reading on [Database Design](https://en.wikipedia.org/wiki/Database_design)
These don't seem so bad. Thanks for the help.
I don't agree at all with this person. I accepted a job as a data analyst with an insurance company with 0 SQL experience and absolutely taught myself 90% of the skills I use on a daily basis in... about 2 days. It's stupid simple [e: for a programmer]. The test will probably be harder than normal if you aren't familiar with their specific syntax. I'd make a note to say that in the interview and ask to use Google unless you chance learn the same version that they're using. I'd recommend something like 2008R2 or newer, but something Microsoft. If you want you can PM me and we can hop on a IM client or something and I can walk you through the basics. edit: background in C, COBOL, RPG, Perl, etc.
Thank you for the optimism, sir(or madam). Instead of taking up all your time could you perhaps let me know if these links are a good starting point? http://www.sohamkamani.com/blog/2016/07/07/a-beginners-guide-to-sql/ http://www.w3schools.com/sql/default.asp If I have any specific questions I would be more than happy to send you a PM if you didn't mind? Thanks again for the offer.
Nope, don't use those guides. What you need to do is download MS SQL, build yourself a database and import some tables in through Excel. And then learn how to query it and then learn how to do all the 'common' types of commands. Trust me, your SAS knowledge is worth 10x what your SQL knowledge is for this job. They'll likely have something like SPSS which is similar but dumber, and you'll have to apply your statistics in a different way. SQL can do a lot of things (all actually) that any other language can, but for your purposes as an analyst you can think of it more like a command prompt. You aren't really learning how to program in it as much as you are learning how to navigate data and think in sets (which you should already know how to do). So for your purposes here you're really just learning simple things like cd directory, cd.., etc.
I have a few data tables that I can use to play around with, so I will be spending time tomorrow playing around with them. I will let you know if I have any problems. Thanks.
His second post explains it a bit better - instead of storing 'rock' 500 times, you store '1' and then reference in another table. When you start getting into large datasets, this can cut down on storage significantly. There are other performance benefits that go along with this as well, as a more advanced topic.
As another poster mentioned you can't learn c++ in 2 days, but you could learn for loops, switch statements, types, variable declaration, etc. This is basically what your goal is right now with SQL. You can't learn enough in a few days to jump into a new job and be productive, but you CAN learn enough to pass a general SQL test (I'm guessing that it's general knowledge since they didn't mention what syntax or DBMS you'd be using). As long as you do well on that quiz you should have plenty of time before you start to improve your SQL skills. Sidenote, coincidentally I work as a senior analyst with an undergrad in stats and I've found that my math background has made it extremely easy to pickup SQL/general programming. You'll be absolutely fine as long as the quiz isn't unreasonable.
Having a separate table for types is the way to go for the reason BFG_9000 mentioned. If you want to have the table be human-readable, you can make a view that joins them together on the type ID but shows the type name. In general that's the way you should always handle a situation like that- the table should be optimized for space/speed (and strings are slower/bulkier than numbers, for the most part) then make a view that's optimized for human readability if necessary. 
It says to 3) Stop the server, set lower_case_table_names, and restart the server. I'm having trouble finding out where I change this. I want to change it to be equal to '2' instead of '1'. 
Khan Academy has a great (free) interactive SQL tutorial - you're not going to become an expert in two days, but with your SAS background, you should at least be able to learn some of the similarities and basics.
My first reaction is your insane, but after reading your post, you have a background in writing code and doing analysis so two days and you can probably get by to the point where your convincing people you know what your doing. The most important thing in your situation is having some kind of mentor at your job, someone who can show you the ropes, someone to bounce ideas off because your going to get stuck many times. If you have someone there to help you in the beginning, it makes all the difference. Your enviroment is going to play a big role in your success since your new to SQL and you'll probably be expected to perform right away. With hard work and a good solid mentor, i'd say in six months you'll being a great job.
I'm going to agree with you, I have an OK understanding of VBA, and R and taught myself enough SQL (by RTFM only) to complete a SQL exam and get the job offer (which I declined). I think that if you're highly technical by nature you should be able to learn enough to dabble and have a good enough understanding of how to do what you want, it may not be the prettiest... 
I would use whatever requires the least amount of memory **and makes sense**. Anyway, you can create views which are basically simulated tables via `SELECT` statements. For example, say you have some tables that look like this. ##pokemon id | name -:|:- 1 | Bulbasaur 2 | Ivysaur 3 | Venusaur 4 | Charmander ##pokemon_pokemon_type pokemon_id | type_id -:|-: 1 | 1 1 | 2 2 | 1 2 | 2 3 | 1 3 | 2 4 | 3 ##pokemon_types id | name -:|:- 1 | Grass 2 | Poison 3 | Fire Note that one Pokémon can have multiple types and one type can be used for multiple Pokémon. Therefore, you have a many-to-many relationship. Because of this, the standard SQL solution is to have an intermediary table (in this case, `pokemon_pokemon_type` (`&lt;table1_singular&gt;_&lt;table2_singular&gt;`)). With that, you could create a view as follows: CREATE VIEW pokemon_with_types AS SELECT pokemon.*, GROUP_CONCAT(pokemon_types.name SEPARATOR ', ') AS type_names FROM pokemon LEFT JOIN pokemon_pokemon_type ON pokemon_pokemon_type.pokemon_id = pokemon.id LEFT JOIN pokemon_types ON pokemon_pokemon_type.type_id = pokemon_types.id GROUP BY pokemon.id; I haven't tested the above query, so there's likely something wrong with it. Then, you could query the view as if it were a normal table. SELECT * FROM pokemon_with_types; id | name | type_names -:|:-|:- 1 | Bulbasaur | Grass, Poison 2 | Ivysaur | Grass, Poison 3 | Venusaur | Grass, Poison 4 | Charmander | Fire Unfortunately, since an aggregate function (specifically `GROUP_CONCAT`) is used in the view definition, the view wouldn't be updatable. However, it should be fairly clear how to update it to your liking.
It sounds like the position has more to do with running reports, so I'd focus your study on joins and groups. Totally possible to learn that stuff in a few days. Whether you remember it a week later is another question...
You are in for a lot of studying. I second the idea of doing exercises off a book, like the Sam's 10 Min. That's probably the only thing you have time for. Of course, a small quiz can't test everything either. As a statistics major you are probably well versed in set theory. SQL and databases have a lot in common with set theory and a propper database textbook or course will cover that. Look into relational algebra (but probably hold off until after Friday since it likely won't be the fastest way to learn basic sql). For now, it might just be helpful to think of SQL operations as set operations, especially when you do more complicated things. Of course if you got more time on your hands... Database design and normal forms might be another topic to look into if you expect data definition might be important and you are looking for a deeper understanding. And then there is the whole db administration side of things as well as performance tuning. I don't expect it to be part of the quiz. If you are looking for a book, I liked Database Systems Concepts by Silberschatz et al. It is a textbook that has a good balance between detail and the broader picture. Oh, and you should probably be frank with the company that you are new to SQL (but a fast learner, ya da ya da...)
IGNORE_DUP_KEY = OFF
You might also want to check out this video, which uses a correlated subquery with the MAX() function as an example: https://www.youtube.com/watch?v=ueULSny-sWU
 ALTER procedure [dbo].[usp_rpt_NewHireMailingList] (@startdate datetime,@enddate datetime, @companyid int) AS SELECT ei3.employeeID, ei.firstName, ei.lastName, benefitClassName, ei.addressLine1, ei.addressLine2, ei.city , ei.stateCode, ei.zipCode, CONVERT(VARCHAR(10),ei3.hireDate,101) [Hire Date], CASE WHEN CONVERT(VARCHAR(10),ei3.reHireDate,101) = '01/01/1900' THEN '' ELSE CONVERT(VARCHAR(10),ei3.reHireDate,101) END [Rehire Date], CONVERT(VARCHAR(10),EI2.benefitEffectiveDate,101) [Effective Date] FROM Employee_DemographicInfo AS DI INNER JOIN view_EmployeeInformation AS EI ON DI.userID = EI.userID INNER JOIN Employee_EnrollmentInfo AS EI2 ON EI.userID = EI2.userID INNER JOIN Employee_EmploymentInfo AS EI3 ON EI2.userID = EI3.userID WHERE di.insertdate BETWEEN CONVERT(DATE, @startdate) AND CONVERT(DATE, @enddate) AND EI3.mostRecentHireDate BETWEEN CONVERT(DATE, @startdate) AND CONVERT(DATE, @enddate) AND DI.openEnrollYN = 0 AND DI.companyid = @companyid
Oh, that's great. I never thought to try KA. Thanks!!
Thank you for the advice. I'm good at making friends and self-learning on the job, so hopefully good things will come down the road. Thanks for the reply!
You could, but you're storing data in 1NF which involves more general overheard and additional performance hits than using more normalized forms of storage.
I'm still trying to reply to everybody but I want to thank you all for your outpouring of support. I am overwhelmed by your kindness, and I know that even if I don't get this job I have the tools and faculties to get the next one, guaranteed. I am spending as much time today practicing as I can. I will let you all know how the interview goes.
Still haven't been able to figure out the recursive part :(
I was re-searching and figured out SSIS was the way to go
Have you tried SQLBolt? Not necessarily a workbook, but an online interactive tutorial I found helpful. http://sqlbolt.com/ 
I didn't know about SQLZOO. This is great! Thanks.
Aware of that now. At the time though I was looking at it in the context of what data will actually be there. There's only like 800 pokemon so we're talking a small db. 
&gt;Natural keys work well when connecting two systems with two different primary key formats Not sure about that.
You want a LEFT join rather than the (INNER) JOIN you are using now.
No problem! It's an awesome site that helped me get started.
Do you mean "if it doesn't exist, return nothing from the second table but still return results from the first table (Class)"?
I believe that narrows the field, is it possible data got imported with ignoring identity insert? That's definitely one way it could happen.
Adventureworks and the overstackflow DB's are great ones to use. Adventureworks is free from microsoft, you can get the overstackflow from brent ozar's site. The WE3 schools is a good site for the basics, practice it in your RDBM on your downloaded DB though. 
I've always found sample problems and tutorial databases to be horrendously boring, it's hard to be interested at all in the outcome. My favourite way to practice is to use real world data from various "open data" sets. A lot of municipal governments do this (Toronto has a good list of datasets you could play with) and the advantage is that at the end of the day you have *real* data that you can draw *real* conclusions from through your analysis. Makes a difference, I find.
Would I be able to do that if I need to output one file for each companyid I have?
The answer would depend where the hierarchy lives. I would assume it belongs to the site rather than the page? So a site has a hierarchy of pages, the same pages can be used in many site/hierarchies? An hierarchy id is absolutely going to help you with accessing root nodes, ancestors, decendants etc. You then just reference your page Ids via your hierarchy table which should function as an associative entity. This all assumes no loops (acyclic graph).
Ok, OP, I missed this when you first posted it. Here are my 2¢ worth. You said the position is "**marketing** and database analyst". That indicates to me that it is mix of technical and (non-technical) business responsibility, which may mean lighter tech requirements. So my thought is that if you can demonstrate a fair understanding of basic to intermediate querying, you've got a shot. Since it is too late for advice to be much help, though, the main reason I'm here is to ask: How did it go? 
Thanks for your response m8
Sure if you want to talk rates, we can do it in Private Messaging. Otherwise I would suggest something like a *AMP stack (WAMP for Windows or LAMP for Linux). If you've not got much experience with web development I would suggest the W3Schools tutorials because while they aren't always perfect, they generally work well. They have tutorials for learning php and it's MySQL related functionality as well as basic HTML.
Sorry for not being especially clear, I believe the job mostly deals with analyzing marketing data, not especially in creating it but I could be wrong. Important characters: Sam: HR rep, Frodo: hiring manager So the bad news is that I haven't heard back from Frodo. I had the phone interview on Wednesday, Frodo expressed interest in having me come in on Friday (today), but he first had to meet with his team and it's possible that instead the interview would be next week. It really sounded like he wanted to do it Friday, so I buckled down and spent most of those two days studying my ass off. Turns out SQL is pretty cool and now I can do everything with code instead of the GUI in access, plus a lot more. Hella sweet. Thursday rolls around and by 1pm I hadn't heard anything. I speak to Sam who states that Frodo told him the same thing that he told me, but by this point Sam still hadn't heard anything back yet. Sam then drops the bomb on me, saying that Frodo would also be out Monday and Tuesday of next week. I'm pretty confused at this point because Frodo did not mention this at all during the phone interview, but I tell Sam that if he hears anything just let me know and I'll do whatever I can to rush over there as soon as I can (I currently commute 50 miles each way, part of the reason why I'm looking for another job). Well, it's Friday 1pm currently (I live in Rivendell), and I haven't heard anything. I checked in with Sam a couple hours ago and he hasn't heard anything either. It's unfortunate that I was unable to get that interview today. The emotional roller coaster was the hardest part, and I felt like shit studying SQL at work instead of working. I drew some questioning looks from supervisors but mostly I'm left to my own devices anyway as long as I get my stuff done. The plus side of all this is that SQL is dirt easy and I really like using code instead of the GUI interface of MS Access. So much easier to do everything right there instead of export it into excel and then do all the data manipulation. I mean, all I'm doing are joins and simple queries and stuff so at most all my queries are like 4 or 5 lines but it's fun at least! I'm going to keep practicing and if I ever have that in person interview I'll let y'all know how it goes. I again can't thank you all enough for helping me out in my time of panic. I had such a strain headache from practicing so much but at this point I know that even if not now, it will pay off eventually.
You're looking for a "join" and subquery, however it is expressed in sqlalchemy.
What is the pure SQL or pseudocode way of structuring a query that has a condition on its own result, conditional on both the number of results and the value of some column? Can you walk me through that in the context of this toy problem?
Yeah just like rbardy said, it's pretty context specific. I work in healthcare, and every month I do a report of what we're getting paid for and what we're not getting paid for. Superficially it's simplistic, but what happens is that every health insurance company has a contract with us stating they will pay X amount for X procedure, and there are restrictions in place - age, family history, clinical evidence of a depression and alcohol screening, having labs done and diabetic education for diabetics, and then there are exceptions and "carve outs" for a billion other things. So, we could have two patients that look exactly the same but when it comes time to bill the insurance there are like 20 variables to sort through. Luckily my job is to just get the data ready and verify that it's correct in our system ;) Something that you can do if you are interested - you can get data sets from like, the census, or city-data.com which is a really good resource, and you can try to create some kind of "insight report" from them if you are so inclined. I've done this in the past for my job and it's worked pretty well. I wanted to analyze how our patient population compares to populations from the same zip code, so I looked at demographics, income, job status, stuff like that and compared them to the data we have.
Use MS Access or LibreOffice Base as the front-end to your MySQL database
I cannot quite understand your problem statement - so i'm going to restate it the way i understand for example, let say I have a course and a coursebook tables with relevant IDs, atr1 and in course, attr2 and attr3 are columns in the coursebook, your (2) is restated the way I understand it as return coursebook records if (attr2 is null and the relevant course has attr1 = false) OR (attr2 is not null and records with attr2=null do not exist for the same course); here's the SQL for this: select cc.*, cb.* from coursebook cb join course cc on cc.course_id = cb.course_id where cb.attr3 = 'good' and -- (1) ( ( cb.attr2 is null and exists (select * from course c where c.course_id = cb.course_id and c.attr1 = false) ) OR ( cb.attr2 is not null and not exists (select * from coursebook cb2 where cb2.course_id = cb.course_id and attr2 is null ) ) 
how do i use libre office?
Ok, thanks Ill try.
Yes, thats exactly what I want.
Thanks, This graphic helps a lot.
There are plenty of examples of you Google SSRS report parameter and SSRS subscription.
 DECLARE @WEEK INT SET @WEEK = 1 DECLARE @YEAR INT SET @YEAR = 2007 SELECT DATEADD(wk, @WEEK, DATEADD(yy, @YEAR-1900, 0)) - 4 - DATEPART(dw, DATEADD(wk, @WEEK, DATEADD(YY, @YEAR-1900, 0)) - 4) + 1 AS 'WeekDate' That should give you the date, and from there you can add some logic in the where clause where the datediff !&gt; X EDIT: You can use this in the where clause such as: WHERE DATEADD(wk, 13, WeekDate) !&gt; WeekDate For: DECLARE @CurrentDay date = '2016-05-01' DECLARE @CurrentWeek date = DATEADD(dd, -(DATEPART(dw, @CurrentDay)-1), @CurrentDay) SELECT * FROM ( SELECT DATEADD(wk, A.Week, DATEADD(yy, A.Year-1900, 0)) - 4 - DATEPART(dw, DATEADD(wk, A.Week, DATEADD(YY, A.Year-1900, 0)) - 4) + 1 AS 'WeekDate' , * FROM Table A ) Z WHERE DATEADD(wk, 12, @CurrentWeek) !&gt; Z.WeekDate AND @CurrentWeek &lt;! Z.WeekDate
&gt; stion. Is it faster to start a transaction and do a bunch of inserts before committing, or just do a large insert statement eg insert into table (columns) values (row1),(row2),... the fastest way is a bulk insert. You can do this way : https://msdn.microsoft.com/de-de/library/ms188365.aspx or using bcp, or by using for example ado.net : https://msdn.microsoft.com/en-us/library/1y8tb169(v=vs.100).aspx Generally speaking, it will be faster to write a couple of thousand rows per transaction thou, so even insert into table (columns) values (row1),(row2) will be faster than doing two seperate inserts. Bulk inserts will be a hell of a lot faster however. The SqlBulkCopy class is quite nice to use, you can feed it a datareader, or a .net datatable, you can throw pretty much anything into that, just takes a bit of coding to read the data as a stream and feed it in. Nothing to terribly hard to do thou
For #1, you forgot to also group by Account. But your syntax looks pretty good otherwise. Better than I would expect from a beginner.
Thanks, those classes are awesome. I decided on PostgreSQL. When creating columns, should only one column be SERIAL in the original table? My thoughts are that if I use a SERIAL as the primary key linking tables, it needs to be the same everywhere, so it should only be defined in one place, with the rest of them referring back to that.
Assuming fpg is ranked by position (ie you have 1 through 12 for WR, 1 through 12 for QB in the table) you can do SELECT * FROM tbl1 INNER JOIN ( SELECT NAME, SEASON# FROM tbl2 WHERE fpg &lt;13 GROUP BY NAME, SEASON#) as tbl2 ON tbl1.NAME = tbl2.NAME and tbl1.SEASON# BETWEEN tbl2.SEASON# and tbl2.SEASON# + 1 This'll dupe years for some cases (player is consecutively in top 12) but you can group by to get rid of that.
Thanks! This works!
You are close. Remove &gt;a."Year" - b."FirstYear" + 1 = 2 and change to a."Year" between c."Year" and c."Year" + 1 
Look for it in my.ini, http://stackoverflow.com/questions/14597884/mysql-my-ini-location
Ahhh that makes it so simple, never would have caught that. Thanks again!
Thanks. I've since found the my.ini file, but every time I try to add "lower_case_table_names=2", it won't let me save the txt file and says access is denied. Even using the options tab in MySQL Workbench 6.3 to change it to 2 doesn't make a difference. 
Try opening a notepad.exe as an administrator.
Bummer it looks like that doesn't work. 'And a."Year" = c."Year"' by itself doesn't actually filter anything (213 rows with or without conditions), and a."Year" = c."Year" + 1 yields no results. I think I've got it down with a subquery though 
Agree with u/eleventhousand well done
So any update on how the test/interview went?
Unfortunately they ended up wanting to reschedule for next week. Gives me more time to study although honestly I would have been interested to see what would have happened if I had taken it friday. I'll let everyone know how it goes.
Smells like homework 
In the example above, the last result for Apples has a price value of 125. An acceptable match for apples for the classifier would be 131.25 or 118.75. Thanks! 
/shrug something like this, maybe (assuming product type and date are a logical key, prices are positive, the record itself counts in '5 occurrences'): select a.type, a.price, a.date, case when count(b.type) &gt; 3 and min( b.price) &gt;= a.price * 0.95 and max( b.price) &lt;= a.price * 1.05 then 1 else 0 end as classifier from exampleTable a left join exampleTable b on b.type = a.type and b.date &lt; a.date and b.date &gt; dateadd( dd, -365, a.date) group by a.type, a.price, a.date 
What are you planning on using SQL for? DBA? Generating reports? Manipulation and cleansing of data for analysis?
I'll just say what happened with me and it might generalize to you (esp because it looks like you want an analyst role; if not, then ignore the rest of this). I learned a little bit of postgres from pgexercises.com. I already knew a ton of Python but had no formal experience besides doing QA for a small startup. I found a startup-ish type company [60 - 80 people in our branch] and joined them as a "data engineer". I didn't have the formal experience required but in the interview they told me it didn't matter much. I did the exercises, they liked me, they hired me. The job was actually less data engineering and more just making complex queries (which was awesome), documentation, and analysis for marketing + engineering. Don't worry too much about the concrete requirements. For you, though, if you have the stomach to go to a startup, a System Admin + SQL person would be a great way to market yourself. That way you do a little of both, get your formal experience, and be able to go whichever way you want after. Leverage what you already have to get where you want to be!
I was going to say the same thing. *** A junior level analyst becomes one of two things - a developer or an analyst. Developers will focus on code performance; analysts will focus on accuracy. The next level is Architect or Scientist *** You could segue from systems/OS guy, to DBA, to Architect. You could bypass engineer. Maybe. Read up on backups, source control, permissions. Read more on performance settings. Your systems experience is valuable here. Also, you better learn SQL
The five whys are cheesy but useful. Try to maintain a consistent interface. When your users get accustomed to where filters are and stuff they bug you less. Make sure there's a good data dictionary and if not make one.
Just a basic report is all you need. Try [this](https://msdn.microsoft.com/en-CA/library/ms167305.aspx)
I did a mix of sys admin / sql admin / developer as an entry level wear all hats position. (No SQL experience, they were wowed by my Excel spreadsheets and work ethic / progress. I had been with the company three years prior doing tons of varied work, starting by doing call center phone tech support.) A year later I got moved into sys admin for two years and did very little to minor sql admin. (Very minor, almost non-existent SQL.) Then for a year and half I worked for a software company as a DBA for them for a year and half doing performance tuning. Lucky break there, they couldn't find enough technically sound people and I was surrounded by super tenured DBA's to learn from. Plus paid training and events. Afterwards, now a Sr DBA with a major company. Sys Admin have a ton of transferable skills that will correlate with SQL Admins, but if you go through the 461 / 462 / 463, you'll be able to land something junior level. Job availability where you're at though will be the main question. Around here, most people can land a junior gig easy. Start connecting with recruiters, go to SQL meetups, go to SQL Saturday. Begin making connections in the industry now and get a finger on the pulse for your location. SQL Admin -&gt; Data Architect is probably the route you would end up going unless you decide to go SQL Developer. It seems like SQL Admin jobs are drying up in comparison to Developers, the automation and advances in cloud make the administration easier and thusly you need less people. (Just my own speculation.)
That's pretty much what I'm doing now. I was just able to get the IDs. The part I don't understand is the script task that is running the report. But I'm thinking that is where I just change the input to look at my stored procedure? 
Thank you! This will be great to go on. Now when I try to post a new entry, I get an error that Entry.personId doesn't exist. I've had this problem when I have made triggers in the past, where I create a trigger and the error that I get doesn't seem at all related to a trigger, but when I drop it, the error is gone. I'll keep tinkering away with your example though. Cheers
Is there something like that to run my stored procedure and have it save a one file for each id to a specific path? That is what I'm stuck on now, and I think it's the last step.
Good luck. I haven't worked with MySQL in years so what I wrote was just a stab in the dark. But it should hopefully give you an idea of how to go about it.
I don't know what did wrong, but this is the query to undo your update: UPDATE dbo.DiagWin_SOFTWARE_MST SET dbo.DiagWin_SOFTWARE_MST.ProductName = Right(dbo.DiagWin_SOFTWARE_MST.ProductName,LEN(dbo.DiagWin_SOFTWARE_MST.ProductName) - 11) FROM dbo.DiagWin_SOFTWARE INNER JOIN dbo.DiagWin_SOFTWARE_MST ON dbo.DiagWin_SOFTWARE.SoftwareId = dbo.DiagWin_SOFTWARE_MST.SoftwareId INNER JOIN dbo.DiagWin_KEYMASTER ON dbo.DiagWin_SOFTWARE.AssetKey = dbo.DiagWin_KEYMASTER.AssetKey WHERE dbo.DiagWin_Keymaster.AssetName = 'GGDT005' After that, please try first to do a SELECT, if the result is what you want then you run the UPDATE. EDIT: My code works even if some of the items you updated was already marked as "RECYCLED"
You are going to need to assemble a few pieces. I'm going to give you some google prompts as it has been a long day. Come back if you are lost. Sys.databases metadata Backup metadata tables and views Ssis file task Installing ssdt and visual studio (bids) You should make an ssis package as your main executable element. Note that ssis has 'transfer database' tasks - hint that backup/restore isnt the only way. It can also do backup/restore tasks. You can join and query the meradata views to see backup history and what files/sets etc. are involved. File task will copy the files for you. Good luck! 
What's the value of your ResultSet property? 
Would it be possible to have a batch file run the sql query and use the result as a parameter? The result of the query gives the location that I would need the files from. Just curious if this was possible. Thank you for the advice though. Greatly appreciated. Edit: I'm only concerned about running the query to get the result and copying the existing backups to a new location 
[Here's another example that's more concise. I found it after I had learned joins but it seemed good.](http://i.imgur.com/2mlaF1M.jpg)
Powershell is probably what you want then. It can easily get the result from sqls and do the file operation.
I'm in my fourth year doing what you have so eloquently described above. What advice can you give, if any, for moving this career path forward? 
Use a sub-query that returns your field with the ; concatenated on, then at the end of the sub-query add FOR XML PATH('') 
Disregard the post comment from before. I went for the good old join and then the xml path got clear for me. I saw few posts about it, but your post helped me to keep on digging into it. Thanks.
Without knowing the database's schema, I can't say for certain what went wrong, but my first guess is that you have a many-to-many relationship between [dbo].[DiagWin_Keymaster] and [dbo].[DiagWin_SOFTWARE_MST], with [dbo].[DiagWin_SOFTWARE] acting as your junction table. If that is the case, then you should not change ProductName in [dbo].[DiagWin_SOFTWARE_MST], as this affect all computers with the installed software.
Yes--it'll be easier to just insert a record with the foreign key id, a value, and a date--and then from there you can select the "newest" date. it will be a lot easier than always scanning/seeking a table to find the newest date to append.
@this_commutes Thank you for the information, you comment has been very informative and helpful.
You're going to need to use expressions and pass the expressions on in the data flow task. I'd store the procedure in the stored procedures and have the data flow execute the stored procedure via a string combined from your expressions to declare the variables dynamically. You can do it this way too which may be easier. http://stackoverflow.com/questions/7610491/how-to-pass-variable-as-a-parameter-in-execute-sql-task-ssis
NOOOOOOOOOOOOOOOOOOOOOOOOOO OP do not run this. This *dangerously* assumes that the product name has (recycled) in the front of it. Throw in AND dbo.DiagWin_SOFTWARE_MST.ProductName like '(recycled)%' just to be safe....
So if i want getdate() - 7 for one, and getdate() + 7 for the other that is where I create the expression? 
Yup, you would have two expressions. One for a week ago and one for a week in the future. So in their example, you would have two variables being populated by two expressions. There is a setting in expressions to make them evaluate immediately, that can help during testing. This setting can impact performance / the values placed depending on how it is configured. 
Nice, sounds doable. For my last variable what steps would I take to pass one company id at a time?
You're going to probably end up using a for each loop container. I would anticipate if you have a lot of files, you'll have a first step that gathers the names or what you want to name files, then it proceeds to a for each loop container. This is a bit of a different idea: http://stackoverflow.com/questions/13257068/how-do-i-loop-through-date-values-stored-as-numbers-within-for-loop-container Basically three expressions, min value, max value, and current value. The expressions get adjusted based on conditions. (Condition being that it has been passed already. Thoughts on that is a stored proc that logs to a table saying, oh hey, we just processed ID 1, then the min and max value determine, hey, you got more values to pass, then a proc looks and says, well, here's the min, max, and what we've done, meaning we have X left, let's process one of those.) You can take that sort of logic from the overflow and apply it to your issue here. That's the off the top of my head approach, I'm sure there's a simpler method though if you search around for passing a list of dynamic id's into a FEL process. 
Yeah, that is why I made the same suggestion a bit below :)
Cherish this code, it took me a few weeks to get this right. Why? Because it works dynamically in SSIS and doesn't care about EOM or EOY. If you play enough in SSIS, you'll realize how much of a pain the ass it is. I almost made a date time table in all instances because of that. Formatteddate: RIGHT("0" +(DT_STR,4,1252)MONTH(DATEADD("dd",0,@[User::OffsetDate])),2)+"-"+ RIGHT("0" +(DT_STR,4,1252)DAY(DATEADD("dd",0,@[User::OffsetDate])) ,2) + "-" + RIGHT((DT_STR,4,1252)YEAR(DATEADD("dd",0, @[User::OffsetDate] )),4) Formatteddate2: RIGHT("0" +(DT_STR,4,1252)MONTH(DATEADD("dd",0,@[User::OffsetDate2])),2)+"-"+ RIGHT("0" +(DT_STR,4,1252)DAY(DATEADD("dd",0,@[User::OffsetDate2])) ,2) + "-" + RIGHT((DT_STR,4,1252)YEAR(DATEADD("dd",0, @[User::OffsetDate2] )),4) OffsetDate: DATEADD( "dd", @[User::OffsetValue] , @[User::TodaysDate] ) OffsetDate2: DATEADD( "dd", @[User::OffsetValue2] , @[User::TodaysDate] ) OffsetValue: -7 OffsetValue2: 7 TodaysDate: getdate() 7 expressions to get your before and after date. 
XLSX if you're doing SSIS and they use it for Excel in reports. Excel and SSIS do not play nice however.
Offsetdate and offsetdate2 get calculated, this adds the -7 or 7 to the current date which is the todaysdate expression. We now dateadd the offsetdate three times, then convert to month, day, and year. The right string portion basically forces the date to look like MM-DD-YYYY and converts to string so it can be concatenated. Why the DD,0,OffsetDate, I don't remember. I know it has to do something with handling previous month or previous year though. It's been a year since I got this fixed up, so I haven't poked it since I fixed it then.
Is there a way to do this without having to create a temp table?
Not sure if I misunderstand your query but getting all contact ids that have more than one email address. Select Contact.ContactID, count(Email.Email_address) from Contact Inner join Email on Email.ContactID = Contact.ContactID group by Contact.ContactID having count(Email.Email_address) &gt; 1 If you want all email addresses that have more than one contact, you can use this: SELECT count(Contact.ContactID), Email.Email_address FROM Contact INNER JOIN Email on Email.ContactID = Contact.ContactId group by Email.Email_address having count(Contact.ContactID) &gt; 1
I dont think you've misunderstood it, but just to clarify: The query is trying to see if a user has more than one email that they're using, and if so, identify that they are through a count (greater than 1). The second query is less important, but is good to check to see if email addresses can be registered to more than one person. Thanks for your help; I'm on mobile at the moment so will have to play around with this later.
ok then the query should work
&gt; There are multiple attributes which need to be monitored for changes, each in specific related tables in the relational model. Are you saying that all of these possible changes are going to be reflected in the SCD? And is the problem just how to detect these changes?
I'm having trouble getting my variables to display correctly. Right now I have two global variables (start and end date). Is that how it needs to be? I keep getting different error messages when I try to evaluate the SQL part of it. 
Sounds right to me, not sure on the messages without reading them. One thing to note, you may have to change some settings. I'd say for the expressions you're using, set them to evaluateasexpressions to true. For the SQL, set delayvalidation to true. This basically makes your variables set themselves right now and SQL doesn't check to see if everything is good before it fires. When I do testing, I usually set stuff like that. When it goes live, I switch it back to how it should be. (After a solid run through, it places temporary cached values in place so it validates itself each run through.) Or it does something similar like that. 
Tested both queries out and they both work. Thanks for your help!
If you get stuck, just set the SQLSourceType to variable, set the sourcevariable, and have an expression that concats the exec proc + start + end + id syntax together so it dynamically compiles at run and then you have to set the result set options and expression still. Just another roundabout method if the wall becomes immovable.
When I parse the query in SSIS the batch could not be analyzed because of compile errors. The result set is blank on the execute SQL task where I'm trying to execute my SP, and my result set would be more than one row. Does that mean my result set on the primary page needs to be set to full result set? I'm 99.9% sure the connection is working as expected.
One of your subqueries is returning more than one result, which isn't allowed. It looks like you should be joining prov_org and prov_id_num, but it's hard to tell. If you could explain what you're trying to do, someone might be able to help.
Both subqueries in your code can return more than 1 register, and that can not happen to a subquery. What you can do is use "SELECT TOP 1" instead of "SELECT DISTINCT"
Confused? If they are in the ods surely they are only there because they changed anyway? Find the change date then use Lag or Lead as appropriate to derive the end date from the start date of the next record. 
So what I'm trying to do is get a list of people that have been added for the first time into the database. If I enter the above in the command line it will exec my sp?
When you're in T-SQL, you can do exec proc @variable and it will run your stored procedure with the parameter. In the data flow, you are using an OLE DB Source Editor. You tell it to access the DB and get the data access mode from sql cmd from variable. By combining the SQL Syntax executing the proc into a single expression, you can hit preview and it should compile the expression, validate, and it would turn into the exec proc @variables, then return your output. In the data flow, you can then direct the output to your excel / csv file. 
I'd opt for data flow, looks solid from my thought process so far. You will probably need a step outside the FEL or inside the FEL that creates the files and names the files you want to export the data in. You'll need expressions that handle the name change of the files too, changing the value inside the FEL.
https://dev.mysql.com/doc/refman/5.7/en/json.html#json-values Native json plus d3.js or similar, as a Web Dev I'm guessing you'll like this. 
I got it to run then it errored out with [Execute SQL Task] Error: An error occurred while assigning a value to variable "startdate": "Unable to find column startdate in the result set.". I'm not sure where this would be coming from? 
&gt; I got it to run then it errored out with [Execute SQL Task] Error: An error occurred while assigning a value to variable "startdate": "Unable to find column startdate in the result set.". &gt; I'm not sure where this would be coming from? That's based in the result set tab. I would assume you need more variables in the result set (one for each column returned by SQL) or it needs a ETL loop to handle the Exec sql task. Again though, for more than 1 result back, I never used the exec t-sql task, so that will be tricky. 
So first pre-join from the leaf table, through all the parent levels you need, so you have a big denormalised lump. Now use the GREATEST() function to collapse all the change dates from all of the levels. This will be your StartDate. Now use that with LEAD to get the StartDate of the next record, by the leaf primary key. This will be your EndDate. When you are at the end of the stack and lead doesn't return a date, you can set it to your high/open date.
It's all good, SSIS is always a learning process. I still think data flow is your easiest way out of a headache.
So get rid of the execute sql task where I'm trying to run my file and replace that with a data flow task?
Do it! Lol Seriously though, that kind of job can lead to great career advancement, especially if you drive a successful initative to get better tools and techniques in play. Guess what. There are very few sexy industries, or at least the 'sexy' wears off very quickly. Sometimes the most interesting work is found in the most boring seeming industries. And you will make new friends. It is work after all not a social club.
If you want the equivalent there is also https://technet.microsoft.com/en-us/library/ms161551(v=sql.90).aspx I use this everywhere. Super fast and easy.
How do I set-up the data flow task to get it to run?
Maybe it would help if you reflected on your motivation for applying for this position and why you thought about making a career change in the first place. 
For growth and with your degree, it sounds like you could end up in a data analysis or scientist role which is pretty rad if you like numbers and like SQL. IT work in availability differs by region, hard to say for advancement for your location. You can still stay in touch with your current co-workers. I still talk and hang out with people from the last four jobs over the last 10 years. I find many jobs with SQL are not sexy. The last one was pretty sexy, god do I miss that job. I do not miss driving 100-140 miles a day or having worse benefits / pay however. Evaluate your motivation for applying as Cletus mentioned, sometimes it's worth jumping. My biggest fear is stagnating. 
I have the OLE DB and it is set to SQL cmd from variable, where do I input the variable expression? It's not letting me type anywhere. 
If there is nothing in the drop down what do I need to update?
Which is the name of the package? Data Type is string? When you evaluate the expression, what is the value? Is the expression set to evaluate as expression = true? 
When I change my id to string it shows in the drop down list. But it's not letting me change the date ones. Should I just delete and make them again?
You need a final expression in a string that concatenates the exec proc text with the parameters. That will be the expression in the drop down you'll use. 
There's not a great course but I would recommend Stanfords database course to gain a strong theoretical grasp of relational algebra. Then practice using a sample database. I heard code academy has a Sql course now...maybe try giving that a shot. 
Can you elaborate on how you are defining your load window possibly with a few more examples. This is hard to decipher. &gt; year to date + last fiscal(financial) year i.e. 2015-07-01 to 2016-06-30 + year to date (ymd) I would right off suggest you store this in a variable with it populated up top. Then use the variable in the where clause instead. This will ease up the amount of calculations the optimizer has to do at execution time and make it easier to read. 
I'd suggest checking your estimated execution plan as the query is now, and then try declaring the date as a variable, and re-evaluating. It may also be worthwhile dropping a Non-Clustered index on the .[DATE] column if you can. DECLARE @FiscalYear DATE SET @FiscalYear = DATEADD(yy, - 1, DATEADD (MONTH,(MONTH(GETDATE()) - 1) / 6 * 12 - 6, CAST(CAST(YEAR(GETDATE()) AS VARCHAR) AS DATE) Then your where clause would be Where ([Schema].[Table].[DATE] &gt;= @FiscalYear) or Where (CAST([Schema].[Table].[DATE] AS DATE) &gt;= @FiscalYear) If you wanted to make it more flexible, you could turn it into a sproc and accept an input variable to replace the getdate() value in your dateadd, with a default of GETDATE() if the input variable is null.
What version of SQL Server are you running? Here is the code I would use. Essentially resetting our start date to the fiscal start date this year and depending on if today is before or after that subtracting 1 or 2 years. Does that sound right or am I staying up too late coding? DECLARE @FiscalStartDate DATE = '20100701' DECLARE @EndDate DATE = '20160605'; --GETDATE(); DECLARE @CurrentYearFiscalStartDate DATE = DATEFROMPARTS(YEAR(@EndDate), MONTH(@FiscalStartDate), DAY(@FiscalStartDate)) DECLARE @EndDate DATE = '20160605'; --GETDATE(); DECLARE @StartDate DATE = DATEFROMPARTS(YEAR(@EndDate), MONTH(@FiscalStartDate), DAY(@FiscalStartDate)); IF @EndDate &gt;= @CurrentYearFiscalStartDate BEGIN SET @StartDate = DATEADD(YEAR, -1, @StartDate) END ELSE BEGIN SET @StartDate = DATEADD(YEAR, -2, @StartDate) END SELECT @StartDate --Or without using variables: SELECT CASE WHEN @EndDate &gt;= DATEFROMPARTS(YEAR(@EndDate), MONTH(@FiscalStartDate), DAY(@FiscalStartDate)) THEN DATEFROMPARTS(YEAR(DATEADD(YEAR, -1, @EndDate)), MONTH(CONVERT(DATE, '20100701')), DAY(CONVERT(DATE, '20100701'))) ELSE DATEFROMPARTS(YEAR(DATEADD(YEAR, -2, @EndDate)), MONTH(CONVERT(DATE, '20100701')), DAY(CONVERT(DATE, '20100701'))) END --OR OLDER SQL SELECT CASE WHEN @EndDate &gt;= CONVERT(VARCHAR(10), YEAR(@EndDate)) + RIGHT(CONVERT(VARCHAR(10), @FiscalStartDate, 112), 4) THEN DATEADD(YEAR, -1, CONVERT(DATE, CONVERT(VARCHAR(10), YEAR(@EndDate)) + RIGHT(CONVERT(VARCHAR(10), @FiscalStartDate, 112), 4))) ELSE DATEADD(YEAR, -2, CONVERT(DATE, CONVERT(VARCHAR(10), YEAR(@EndDate)) + RIGHT(CONVERT(VARCHAR(10), @FiscalStartDate, 112), 4))) END
&gt; Do I start out with MySQL or SQL Server? Purely on this point: it doesn't have to be either. SQLite is good for no-effort set up or administration, and databases are standalone files. For more complete setups whilst still being free I would personally recommend postgres for compliance, freedom, community, range of functionality and A+ documentation.
&gt;we currently deal with mssql, oracle, sybase, and db2 You poor soul. :( 
&gt;&gt;we currently deal with mssql, oracle, sybase, and db2 &gt;You poor soul. :( Yeah. Did you know there isn't a functional ODBC connector that can be used with ssis for Sybase? We have to use a link server connection from the target server to the Sybase box and call a sproc to pull the data. Sooo fucking painful. The DBAs on both sides fought us tooth and nail over that one but they didn't have an alternative.
I've done very little with sybase and db2, I really just remember it was an uphill battle to do anything and at the end of the day, you didn't feel accomplished. It felt like you only got halfway to where you wanted to be after compromising. Oracle does the hard things easy and easy things hard, pain in my ass but still enjoyable. SQL Server fan though. 
The HR system contains some of the most confidential company data. In my opinion, it does not belong in excel but should be stored in a secure database server. I had my identity stolen once by an employee at a former employer because the owner didn't care about guarding our personal data. Depending on the kinds of HR data you collect, you may be subject to HIPAA and/or PCI regulations.
If you want to change career tracks, do it. It can be hard to get someone to give you that first chance in a new field, so don't piss it away. Don't worry about the industry. Worry about the job duties. I work for a big ass bank. I'll probably move to an insurance company next year. So what. It's all about what *I'm* doing. Financial/Data Analysis has a lot of job offers. Especially for someone with an actual financial or stats background. You'll meet new coworkers, and the ones you know now will go their own ways eventually anyway. You'll either keep touch or you won't. That is the way of things.
I was trying to give you some ammo to use to make the case. Don't underestimate the amount of work involved in creating even a basic HRIS. It's difficult to suggest reinventing the wheel when you can spin up a cloud based HRIS for a lot less money than it would have taken to do it on-premise even a few years ago. So that's another option and maybe a good middle ground. They would still need you to help implement and maintain it.
Oh don't misunderstand! I greatly appreciate the ammo! To be perfectly honest, I do not understand why they are even looking for the option of an interim excel system for this. They have yet to share what HRIS system they are planning on implementing or if its not being done for 2 years because of budgetary reasons, or whatever reason they may have.
Nevermind I think I was just now able to get GETDATE() - 7
Glad you got it! Were you able to use the exec proc as a string variable to preview / pull data from the source db?
Ah so exec proc is also a variable I need to make? 
It depends how you want it, but in the end, the variable has to be a string. The string when evaluated should be the T-SQL text of exec storedprocedure @parameter1, @parameter2, @parameter3. Param 1 would be the variable of your final start date, param 2 would be the variable of your end date, param 3 would be your ID variable. The exec storedprocedure can be from an expression or hardcoded into this string based on preference. 
I'm having trouble getting it to evaluate when I create it as it's own variable. What other steps do I have that could execute it?
I'm having trouble getting it to evaluate when I create it as it's own variable. What other steps do I have that could execute it?
&gt;interim excel system You mean permanent if it actually works. If it did work, they'll never leave it. It's not a bad idea, it's a fucking terrible idea. They could be held liable. They need a SaaS (software as a solution, hosted externally) and have someone enter in all the data there. You can get reports and trends and the costs are dirt cheap in comparison to writing it yourself. 
 EXEC [dbo].[usp_rpt_NewHireMailingList] (@startdate,@enddate, @companyid) That is what I'm putting into the expression.
Your reservations are reasonable, but you should take the job. The unknown can be intimidating, but it sounds like an awesome opportunity. 
Thank you! I would have never gotten to that. I'll play around and see if I can get that to run. 
Should get you closer! Hit evaluate expression to see the variables populate in the expression to see how it looks. It should be runnable in T-SQL to run in SSIS.
MS Access front end with PostgreSQL backend. 
How large is this company? I have to believe there are systems you can buy off the shelf that will be cheaper than building your own.
I just downloaded the .accdb version of this yesterday! For PowerBI classes.
Thanks for checking in! Yes and no =) I got it to where lastStatus would become what I wanted it to when I created a status with a startDate, but I've run into a problem where once there is an endDate for a status (when, for my purposes, it should no longer be considered the lastStatus) the lastStatus does not change properly. Here's the trigger (pretty close to yours): DROP TRIGGER IF EXISTS updateLastStatus DELIMITER $$ CREATE TRIGGER updateLastStatus AFTER INSERT ON Entry FOR EACH ROW BEGIN UPDATE Person p SET lastStatus = ( SELECT `status` FROM Entry e WHERE e.personId = p.id AND e.endDate IS NULL AND e.startDate IS NOT NULL ORDER BY e.startDate DESC LIMIT 1 ) WHERE p.id = NEW.personId; END $$ DELIMITER ; So, the 'AND e.endDate IS NULL' part is not working? At first I thought it was that lastStatus was getting put in my data model on the front end and then being put back into the database, overriding what the trigger was doing, but now I'm not sure. My boss (I am working a half-intern/half-jr. dev position at a start up) kind of got frustrated and took it over today, so maybe I should check in with him first (maybe it was his back end code!). He just tossed me on this sql trigger task and I have never done much work with sql, so it was frustrating to say the least.
I would like to see this https://youtrack.jetbrains.com/issue/DBE-77 
You're welcome! It gives me something to do in my downtime. So, if you haven't asked your boss yet, then maybe you should ask him about the data in the table. E.g. What is the business logic driving it? What are the constraints, if any, on it? Because the trigger you have now has to make a lot of assumptions about the logical integrity of the data. If left unconstrained, that Entry table might be prone to inconsistencies. I would ask are there check constraints to ensure startDate is always less than endDate? Or to ensure that consecutive startDate and endDate ranges don't overlap? To ensure that there's always a null endDate on the last record per personId? Etc. Hopefully your boss won't feel threatened by those questions. If he/she is worth their title then they will be able to answer those questions and prove to you that everything is kosher.
I'd like to be able to bring a console window into my second monitor and have it bring the results pane as well, anyone know if this is possible?
I think you can use current_timestamp instead of getdate(). Or just use plain old get date()? When I have to do weird stuff like this, I prefer to set variables, which I feel helps keep my where clauses legible. I would find a way to rewrite it to look like:. Declare @last_fiscal_year date. Set @last_fiscal_year = {whatever logic you feel like}. Select blah from table Where date &gt;= @last_fiscal_year
So, that code doesn't run when I copy and paste it into a query window (after adding an initial declare). In either case, nesting dynamic sql like you're attempting to do is RARELY needed. DECLARE @command VARCHAR(5000) if OBJECT_ID('tempdb..#DBInfo') is not null begin drop table #DBInfo end create table #DBInfo ( ServerName VARCHAR(100) , Name varchar(100) , FileSizeMB INT , FreeSpaceMB INT , FreeSpacePct VARCHAR(7) , FreeSpacePages INT , PollDate datetime ) set @command=N'Use [' + '?' + '] SELECT @@servername as ServerName, Name, CAST(sysfiles.size/128.0 AS int) AS FileSize, CAST(sysfiles.size/128.0 - CAST(FILEPROPERTY(sysfiles.name, ' + '''' + 'SpaceUsed' + '''' + ' ) AS int)/128.0 AS int) AS FreeSpaceMB, CAST(100 * (CAST (((sysfiles.size/128.0 -CAST(FILEPROPERTY(sysfiles.name, ' + '''' + 'SpaceUsed' + '''' + ' ) AS int)/128.0)/(sysfiles.size/128.0)) AS decimal(4,2))) AS varchar(8)) AS FreeSpacePct, GETDATE() as PollDate FROM dbo.sysfiles' INSERT INTO #DBInfo (ServerName,Name, FileSizeMB, FreeSpaceMB,FreeSpacePct, PollDate) EXEC sp_MSForEachDB @command select * from #DBInfo DECLARE @tableHTML NVARCHAR(MAX) ; set @tableHTML = cast( ( select td = Name + '&lt;/td&gt;&lt;td' + Case when convert(decimal(8,0),FreeSpacePct) &lt; 20 then ' bgcolor="red"' else '' end + '&gt;' + FreeSpacePct + '&lt;/td&gt;&lt;td&gt;' + cast(PollDate as varchar(30)) from (SELECT Name,FreeSpacePct, Polldate FROM #DBInfo) as d for xml path( 'tr' ), type ) as varchar(max) ) set @tableHTML = '&lt;table cellpadding="2" cellspacing="2" border="1"&gt;' + '&lt;tr&gt;&lt;th&gt;Database Name&lt;/th&gt;&lt;th&gt;FreeSpacePct&lt;/th&gt;&lt;th&gt;PollDate&lt;/th&gt;&lt;/tr&gt;' + replace( replace( @tableHTML, '&amp;lt;', '&lt;' ), '&amp;gt;', '&gt;' ) + '&lt;/table&gt;' print @tableHTML Note that I removed the % sign because you can't cast a varchar to a numeric with it. declare @test varchar(10) = '100.00%' , @test2 varchar(10) = '100.00' select CAST(@test2 as decimal(8,2)) select CAST(@test as decimal(8,2))
... and you can remove that PARTITION BY 1 completely, BTW.
Are you looking to have the actual values in the 'RowDesc' columned ordered in descending order? If so, you'd just need an order by clause at the end of the select statement: ORDER BY [RowDesc] DESC.
I like your answers.. And your username!
Thanks! :)
["nifty"](http://sqlperformance.com/2012/08/t-sql-queries/median)
The article i linked outlines other ways of doing similar things (there is another post for grouped data sets, it is in the comments, also done by Aaron Bertrand) and provides a performance analysis for each method. My "nifty" was simply meant to point out that there are faster/alternate ways to accomplish the same thing, granted it is a sqlserver article so ymmv. Some of these (such as the "paging trick") are incredibly interesting regardless of your server resources or purpose. Granted you're on 2008R2 but hey, maybe someday, right? Edit: [Here's the other article I mentioned](http://www.sqlperformance.com/2014/02/t-sql-queries/grouped-median)
One datasource — one database for now. Please, vote for this: https://youtrack.jetbrains.com/issue/DBE-2287
I've run into this before and have a snippet of sql that fixes it. I'll try to remember which laptop it's on. Edit: make sure you have the needed permissions defined in the "Securables" tab as well. 
I'm not familiar with SAS but I'd try putting the subquery outside of the select list and in the FROM list instead - this might stop it from re-running the whole query for each row, like so: PROC SQL; CREATE TABLE WORK.QUERY_FOR_F3_WORKLOAD_COUNTS1 AS SELECT DISTINCT t1.'Claim Number'n, t1.'Claim Assignee Key'n, t1.'Assignment Date'n, t1.'Exposure Assignee Key'n, t1.'Exposure ID'n, t1.'Coverage ID'n, t1.Claimant_Id_Nb, t1.claimant_full_nm, t2.'Employee Name'n, t2.'User ID'n, t2.'Job Title'n, t2.'Manager Name'n, t2.'Manager ID'n, t2.'Manager Title'n, t2.'Director Name'n, t2.'Director ID'n, t2.'Director Title'n, t2.'AVP Name'n, t2.'AVP ID'n, t2.'AVP Title'n, t2.'Org Name'n, t2.'Org ID'n, t2.'Org Title'n, x.'Min Asgn Date Exp Assgnee' FROM WORK.QUERY_FOR_ROLLUP_WITH_WORK__0000 t2 INNER JOIN WORK.QUERY_FOR_F3_WORKLOAD_COUNTS t1 on (t2.'User ID'n = t1.'User ID'n AND t2.'As Of Year'n = t1.'Transaction Year'n AND t2.'As Of Month'n = t1.'Transaction Month'n) INNER JOIN (SELECT MIN(t3.'Assignment Date'n) AS 'Min Asgn Date Exp Assgnee', t3.'Exposure Assignee Key'n FROM WORK.QUERY_FOR_F3_WORKLOAD_COUNTS t3 GROUP BY t3.'Exposure Assignee Key'n) x ON x.'Exposure Assignee Key'n = t1.'Exposure Assignee Key'n WHERE AND t2.'Org ID'n = 'HILYARK' ORDER BY t1.'Exposure ID'n; The way I'd do this in MSSQL is to do this as a windowed query and avoid the performance-hurting subquery altogether. Not sure how you'd do that in your environment, and my syntax here is probably a bit off. Also, just guessing here, if WORK.QUERY_FOR_ROLLUP_WITH_WORK__0000 and WORK.QUERY_FOR_F3_WORKLOAD_COUNTS are views, you could make them subqueries here and possibly improve the performance by doing it all in one, depending on what they're actually doing.
[Instructions](https://msdn.microsoft.com/en-nz/library/ms188670.aspx)
As a reader I would also like to know if this worked lol
Sorry! No dice. I have the Enterprise Guide version of SAS which doesn't declare joins in the code, at least from what I can tell (see original code). I am supposed to get SQL server on my machine in a week or so though so I will def try it then. FOr now I just created a separate table in the GUI that pulls in Assignee Exposure ID and the min Assignment Date for each then joined that table to the original. Essentially the same thing you did but not nearly as slick or efficient.
Isn't it easier to create a view using SUM()? Something like this: SELECT Item, SUM(quantity) as quantity, lot FROM TableName GROUP BY Item, lot
Sorry if you just left this out of your copy paste, but did you end your proc sql with a quit statement? All proc sql code needs to end with quit; or it will just keep on running. If you did... I agree with the other post about how the query should be structured. And Enterprise Guide can definitely utilize explicit joints. Have you tried removing the select distinct and order by and instead following up with a proc sort and nodupkey? On large datasets, select distinct can be nasty. So use fauxmosexual's query without the distinct and order by statement. Then follow up with: proc sort data=WORK.QUERY_FOR_F3_WORKLOAD_COUNTS1 nodupkey; by whatever vars you want; run; 
Typically this would be part of a purge/archive job which would run on a periodic basis (daily, weekly, monthly, etc). And you would need some kind of date/timestamp field to identify when the record was created. 
How would you determine the age of a record without a field that indicates its age?
over time, a sql page will acquire /*///'s. You can drill into your hard drive and examine it under the microscope. Each / past the first indicates 100 days.
I've tried that but I can't get it to work in the context of the actual larger query, hence the entire code.
The primary key is how the table is physically structured, and its best if the values are unique (else it adds a hidden uniquifier). Having gaps isn't a problem as it'll just move to the next item when writing down data. Adding data / deleting in the middle afterwards may cause page splits unless the fill factor is low enough to handle the adds. Of course empty space causes more disk IO to read the actual data. If you are munging up the data you'll want to do some maintenance like reorganizing the table so its happy again. Could an alternative be a surrogate key like an auto-increment ID and just set the permutation column as unique? 
ideally this table will have a primary key containing the serial column and all of the permutation columns to guarantee uniqueness. You're proposing having a serial ID as a primary key, with a UNIQUE constraint on the permutation columns?
So, basically, what I'm thinking could solve your issue is to encapsulate the entire SQL statement into a variable (like @SQLVariable) which allows you to pass in a variable into your openquery object, then exec that @SQLVariable... That's why I pasted in that example for you. I show you how I pass in a parameter to an openquery object... (I vaguely recall trying multiple iterations of things, and this was the only thing I found to work when I built this last year.) 
Periodically, unfortunately. I'll definitely be investigating that solution on Monday though. Thanks!
I haven't used your monstrous query since I don't have that dataset, but with the script below you'll get those variables into your CTE'd OpenQuery pretty easily. DECLARE @RemoteQuery nvarchar(max) DECLARE @sql nvarchar(max) DECLARE @LSC varchar(max) DECLARE @STARTDATE datetime DECLARE @ENDDATE datetime DECLARE @LinkedServer varchar(128) SET @RemoteQuery = ' WITH CTE AS ( SELECT object_id AS LSC, create_date AS SFDCCreatedDate FROM sys.objects ) SELECT * FROM CTE WHERE SFDCCreatedDate BETWEEN CAST(@STARTDATE AS date) AND CAST(CAST(@ENDDATE AS datetime)+1 AS date) AND LSC IN (@LSC) ' SELECT TOP 1 @LinkedServer = name from sys.servers WHERE server_id &gt; 0 ORDER BY server_id SET @LSC = '3,5,7,9,12' SET @STARTDATE = getdate()-10000 SET @ENDDATE = getdate() --Replace variables in the statement SET @sql = REPLACE( REPLACE( REPLACE(@RemoteQuery, '@LSC', @LSC) , '@STARTDATE', QUOTENAME(convert(varchar(30),@STARTDATE,21),'''')) , '@ENDDATE', QUOTENAME(convert(varchar(30),@ENDDATE,21),'''') ) --Prepare the remote statement SET @sql = 'SELECT * FROM OPENQUERY(' + Quotename(@LinkedServer) + ', ''' + replace(@sql,'''', replicate('''',2)) + ''')' EXEC (@sql) 
I think you are looking to do something similar to what is explained in the following link: http://stackoverflow.com/questions/7901416/best-way-to-update-table-with-values-calculated-from-same-table
Will you have a reference table that captures the limits for each col1 and max col2 value if these limits can change? If so, are you able to join your cross join results to this ref table and on the join to col2 you can use &lt;= in the join?
ah so you mean like having a `limits` table and for each id, having a 'limit' column? I suppose that would be possible but I'd need to really think about how to make that work. I started with the cross join as it's simply the easiest way to do what I want. validation be damned!
I'm somewhat new to sql, any chance you could walk us through that? (No rush)
Assuming you don't have any other columns, like identity ids, you could go with a stored procedure that would do: 1. Select the desired sum into a #temp table 1. Delete all rows from your source table 1. Insert into source from #temp Something like: SELECT item, lot, SUM(qty) AS qty INTO #sourceAggregated FROM SourceTable GROUP BY item, lot TRUNCATE TABLE SourceTable INSERT INTO SourceTable (item, lot, qty) SELECT item, lot, qty FROM #sourceAggregated This of course will be troublesome if you have a lot of data, there would be some performance and other issues (e.g. Transaction log growth if you're using full recovery model with an always on availability group). If you had some unique ids that would let you distinct between rows, you could use MERGE, or update only the first row as denoted by the Id and then delete other rows. I'd need more information to propose a better solution. 
How can i work dependent ctes in? I want the OQ to go and pull data,then do the bulk of the subsequent work on my main server.
Can you explain what cross apply is doing in English? I grasp the mathematical concept of whats going on here, but have never felt comfortable with it.
The simplest way that I can think about it is that it is a correlated subquery with the benefits of a join, often resulting in an execution plan similar to that of an inner join and (typically) outperforming the correlated subquery.
Sounds like you want to make a numbers table. Many ways of doing it. Personally I make a function that does a Cartesian product and returns a table made on the fly with the number of required rows. Another way is the link below http://sqlblog.com/blogs/adam_machanic/archive/2006/07/12/you-require-a-numbers-table.aspx
Use DATEADD(year,5,date) &lt;= getdate() What you suggest will do what you expect. It might not be the best ever way of doing it though. Normally a scheduled job with overlap protection would be the way. 
Thank you very much for walking me through that. I learned something!
This would be better, but you are right the datediff as written would have some unexpected results. date &lt;= DATEADD(YEAR, -5, GETDATE()) 
OP is ok there if the default implicit transactions is off. Still would be better with explicit transaction. 
No because it's all a part of the same batch 
Oh, didn't even notice that. Thanks. So your suggestion would be to do it using PHP (which the script is written in, that are using the database)? Or am I misinterpreting it? I've never worked with views before - actually I thought they were just for visual use, not something that I could fetch data from using PHP etc. The value is crucial to the script, but the way it's created doesn't matter to me.
A VIEW is basically a saved query. When you SELECT from it, it behaves exactly like a table. It's not saved data; it's generated dynamically when the query is run. So, if you do this: CREATE VIEW netinv_interfaces_count AS SELECT id, COUNT(*) ifCount FROM netinv_interfaces GROUP BY id; You can then do this when you need the count field: SELECT d.*, ic.ifCount FROM netinv_devices d LEFT JOIN netinv_interfaces_count ic ON ic.id = d.id; Internally, it's identical to running this: SELECT d.*, ic.ifCount FROM netinv_devices d LEFT JOIN ( SELECT id, COUNT(*) ifCount FROM netinv_interfaces GROUP BY id) ic ON ic.id = d.id; I'm not quite sure if that syntax works on MySQL, but it should. Most of my day is on MS SQL Server, DB2, and PostgreSQL. I'm LEFT JOINing here because I have no way of knowing if netinv_devices and netinv_interfaces is one to zero or more relationship, or one to one or more relationship. If it's the latter, then a plain INNER JOIN is better. 
Ok, that's gonna be a project for tonight. Just to be sure - you do suggest keeping the function in MySQL (if done like you just wrote) rather than having PHP doing it?
What if I used your method to insert the data into a @table? I'm hesitant to use #tables because this is for an SSRS report that could have multiple users executing it simultaneously. I don't understand how to use your method and integrate it into a larger set of instructions.
It should work in any situation As long as whole query is sent to the remote server. You cannot integrate CTE from one server into CTE of another.
Like this: Select uniq_id, user_id, q1, q2, q3, q4, q5, q6, q7, q8, q9 from question_table where q2 is null or q3 is null or q4 is null or q5 is null or q6 is null or q7 is null or q8 is null or q9 is null It will show if any of those questions were not answered, if you want to show only the ones that question 2 to 9 are empty, change the OR to AND
Is your string ALWAYS in the same pattern like this? "OrdRevenuePay " + number1 + " -&gt; " + number2
CHARINDEX is the answer: DECLARE @str VARCHAR(128) SET @str = 'OrdRevenuePay 1275.00 -&gt; 600.00' SELECT LTRIM(RTRIM(SUBSTRING(@str,CHARINDEX(' ',@str,1)+1,CHARINDEX('-&gt;',@str,1)-CHARINDEX(' ',@str,1)-1))) as col1 , LTRIM(RTRIM(SUBSTRING(@str,CHARINDEX('-&gt;',@str,1)+2,LEN(@str)-CHARINDEX('-&gt;',@str,1)-1))) as col2
hmm. I'll look into unpivoting....
yea, but I'm looking for rows with 8 Null values in any combination.
Yep, as I said below, INSERT EXEC would do the trick.
not pretty but for a quick and dirty query it ought to work - WHERE ( case q1 is null then 1 else 0 end + case q2 is null then 1 else 0 end + case q3 is null then 1 else 0 end + ..... case q9 is null then 1 else 0 end ) = 8
I feel as though setting up a proc to check if it's still running and then execute every 30 seconds is better practice, plausibly with a T-Log space check and verification that can trigger an earlier T-Log backup if possible. To answer OP's question: [Error: 9002](https://msdn.microsoft.com/en-us/library/ms175495.aspx) Also, I've seen queries so taxing on the server the server literally just falls down and restarts. That was a fun day.
&gt; one of the columns in tbl_Link has the column name from tbl_Data this is an extremely bad idea never store meta-data -- it is a symptom of failing to design the database properly
This is bad design.. but the only way I see you can get what you want is SELECT tbl_Data.Group , tbl_Data.Category , tbl_Data.Item , CASE WHEN tbl_Link.Column_Name = 'ThisColumn' THEN tbl_Data.ThisColumn WHEN tbl_Link.Column_Name = 'ThatColumn' THEN tbl_Data.ThatColumn ELSE 'Nothing' END as Value FROM tbl_Data INNER JOIN tbl_Link ON tbl_Data.Group = tbl_Link.Group AND tbl_Data.Category = tbl_Link.Category AND tbl_Data.Item = tbl_Link.Item 
the operative word is "seems" query the meta-data table, then use the retrieved column name to format an sql string, then submit that i'm pretty sure it cannot be done in one step, not without a CASE statement that compares the retrieved column name to every possible column name string 
Yet another topic for today where unpivot might be helpful. You can unpivot all known columns and then choose the one you need within the WHERE clause. Other option, as it has been already suggested, is to use CASE, which would basically do the same thing: unpivot data.
You can unpivot the whole tbl_Data using column names as the resulting rows keys (you'll need to cast all the columns to the same datatype) and join to the unpivoted result set from (select [group], [category], [item], colName1 = cast( colName1 as varchar(xxx)), ..... colNameN = cast( colNameN as varchar(xxx))) unpivot( colValue for colName in (&lt;tblData columns&gt;) as unpivotData
Well, it's kind of a mess here. You're trying to subquery the same group statement inside SELECT, but this subquery has references to the parent statement (A.CreatedDate, A.LSC), that's why you get the error. Roughly speaking, your subquery is not filtered right now, because non of the WHERE clauses in it refers to the scope of the subquery. Anyways, the values that you're trying to get from the subqueries would be the same each time, right? So why don't you put them into some variables before you run this statement? DECLARE @Total int SELECT @Total = CASE WHEN SUM([Leads]) = 0 THEN 0 ELSE SUM([Submits]) / SUM([Leads]) END FROM [Digital].DSU.AgentIndex A WHERE A.CreatedDate BETWEEN @STARTDATE AND @ENDDATE AND A.LSC IN (@LSC)) AS 'SubmitIndex' SELECT A.Agent , (CASE WHEN SUM(A.[Leads]) = 0 THEN 0 ELSE SUM(A.[Submits]) / SUM(A.[Leads]) END) / @Total FROM [Digital].DSU.AgentIndex A WHERE A.CreatedDate BETWEEN @STARTDATE AND @ENDDATE AND A.AgentID IN (@AGENT) AND A.LSC IN (@LSC) GROUP BY A.[Agent] HAVING SUM(A.[Production]) &gt; 0
The calculation is how I want/need it to come out.
Same error, it's still telling me I need to group on; and I'm not trying to tie the inner to the outer, I'm trying to get the total sum from the table just between the dates and lead sources.
In simple terms are you saying that I'm not defining the @parameters inside the subquery which is why it doesn't know what to do and thinks I'm trying to reference the outer query? I understand how to rewrite it as/if necessary, and would probably take an approach similar to the one you proposed, but I am not realy sure why it won't pass a parameter into both the inner and outer subquery and give me the answer I'm looking for.
Yes the gaps are fine and in many cases they are unavoidable as well.
Then shouldn't the inner at least be summing across only the agents in the outer? Or is it really every agent in the table? SELECT AgentIndex.Agent , (CASE WHEN SUM(AgentIndex.[Leads]) = 0 THEN 0 ELSE SUM(AgentIndex.[Submits]) / SUM(AgentIndex.[Leads]) END ) / ( SELECT CASE WHEN SUM(AInd.[Leads]) = 0 THEN 0 ELSE SUM(AInd.[Submits]) / SUM(AInd.[Leads]) END FROM [Digital].DSU.AgentIndex AInd WHERE AInd.CreatedDate BETWEEN @STARTDATE AND @ENDDATE -- inner query must be in same date range AND AInd.LSC IN (@LSC) -- and same LSC range ) AS 'SubmitIndex' FROM [Digital].DSU.AgentIndex AgentIndex WHERE AgentIndex.CreatedDate BETWEEN @STARTDATE AND @ENDDATE AND AgentIndex.AgentID IN (@AGENT) AND AgentIndex.LSC IN (@LSC) GROUP BY AgentIndex.[Agent] HAVING SUM(AgentIndex.[Production]) &gt; 0
No, the inner should be summing across all agents regardless of which agents you want to look at. E.g, if a manager wants to only select his agents, it will only show him his agents, but it will calculate the index across everyone for the same date range &amp; source. 
Try encapsulating the whole thing in parenthesis without the nested query since its the same data. ((CASE WHEN SUM(A.[Leads]) = 0 THEN 0 ELSE SUM(A.[Submits]) / SUM(A.[Leads]) END) / (CASE WHEN SUM(A.[Leads]) = 0 THEN 0 ELSE SUM(A.[Submits]) / SUM(A.[Leads]) END)) as 'Calculation' 
Running into something a little weird: DECLARE @STARTDATE date = '2015-01-01' DECLARE @ENDDATE date = '2016-01-01' DECLARE @TOTAL float ; SELECT @Total = 0 --CASE -- WHEN SUM([Leads]) = 0 -- THEN 0 -- ELSE SUM([Submits]) / SUM([Leads]) -- END --FROM [Digital].DSU.AgentIndex A --WHERE -- A.CreatedDate BETWEEN @STARTDATE AND @ENDDATE -- -- AND A.LSC IN (@LSC) --##Indexer SELECT A.[Agent] , SUM(A.[Production]) AS 'Production' , (CASE WHEN SUM(A.[Leads]) = 0 THEN 0 WHEN @Total = 0 THEN 0 ELSE SUM(A.[Submits]) / SUM(A.[Leads]) END) / @Total FROM [Digital].DSU.AgentIndex AS A WHERE A.CreatedDate BETWEEN @STARTDATE AND @ENDDATE GROUP BY A.[Agent] HAVING SUM(A.[Production]) &gt; 0 This gives me a divide by zero error and I'm not really sure why. I imagine that it doesn't matter because if `SUM(Leads) = 0` then it will give a zero, and the only way `@Total` could be equal to 0 is if `SUM(Leads) = 0`, but I'm curious why the case logic can't reference the parameter.
Okay.. and does the above (which will sum across all agents) still give you the group by error? Does it now show AInd.CreateDate, or does it still reference the fully attributed table name?
Just concatenate. Because null+something = null WHERE q1+q2+q3+q4+q5+q6+q7+q8+q9 IS NULL
Sorry it's a Monday. It works now. I seem to understand what you've done. Was it as simple as thinking I was referencing the outer table's created date and telling me I had to group by it in the outer query?
Oy, do I feel foolish. Thanks for the kick in the ass.
I think you are going for percentage of leads that are assigned? Try making the entire thing in the nest a parameter declare @denominator decimal(10,4) select set @denominator = (case WHEN SUM(A.[Leads]) = 0 THEN 0 ELSE SUM(A.[Submits]) / SUM(A.[Leads]) END) FROM [Digital].DSU.AgentIndex WHERE A.CreatedDate BETWEEN @STARTDATE AND @ENDDATE AND A.LSC IN (@LSC)) AS 'SubmitIndex' SELECT AgentIndex.Agent , (CASE WHEN SUM(AgentIndex.[Leads]) = 0 THEN 0 ELSE SUM(AgentIndex.[Submits]) / SUM(AgentIndex.[Leads]) END ) / @denominator FROM [Digital].DSU.AgentIndex A WHERE A.CreatedDate BETWEEN @STARTDATE AND @ENDDATE AND A.AgentID IN (@AGENT) AND A.LSC IN (@LSC) GROUP BY A.[Agent] HAVING SUM(A.[Production]) &gt; 0
You are setting @Total to 0 on Line 6
Division by @Total is happening outside of the CASE statement.
&gt; why it won't pass a parameter into both the inner and outer subquery You can do that of course, just need to fix the syntax by removing A. from the subquery.
/r/SQLServer may be able to help more with SSIS
Right, but I'm also saying WHEN @Total = 0 THEN 0.
So this is always 0
No, sometimes it could be 0, other times it won't be 0. I'm asking why the case statement isn't picking up that it is = 0 to and tossing a divide by zero error.
What you have there is setting it to zero, then you are dividing by it. Take the = 0 out and do set @total = (case statement) Also you should use decimal for the data type.
I understand. I'm selecting = instead of set =, been a long day.
There are lots of NLQ products around in the BI space and have been for many years. They just don't have a great uptake. Maybe this one will be 'the one'.
 select ... WHERE q1 IS NOT NULL AND q2 IS NULL AND q3 IS NULL ... AND q9 IS NULL It doesn't have to be pretty it just has to ask the right question ;) Edit: Just spotted /u/rbardy beat me to this answer. Upboats for him!
I die a little bit inside every time I hear this. &gt; The column where this data is stored is a text datatype
I'd recommend doing some really basic modelling courses. It won't take you long before the basics click into place and you'll find the answers obvious. Here are some basic tips though: * You are an account holder and a person. That makes you an entity, so you should have a row in a table. * Your database might need to work for other persons too * people have an age at a given point in time. They also have a date of birth which never changes. Which seems easier to store/update/use? * how would your bank statement look? Is there anything you can learn about the structure of bank accounts from it? * some values e.g. retirement age, have a very limited set of valid values. How can you keep a 'list' of these and what they mean? It is tough at first to break out of programming type thinking. SQL is much more like your spreadsheet than a program. You stick things in cells, that are in a structure and the whole thing shows you a current state. You don't really 'pass variables' or 'loop' in a spreadsheet. Check out the sidebar or the inevitable posts about the best places to learn SQL and modelling. Just promise you'll come back and tell us when you get the 'AH-HA!' Moment ;)
Take a look at set operators, MINUS/EXCEPT, INTERSECT and UNION The MINUS specifically might help if you have a reference of invalid permutations. Otherwise hold tables that have min/max values for say, col2 related to col1 and then join to those using a BETWEEN predicate to limit your cross joins. Don't really understand what you mean by a 'serial column' sorry.
It sounds like the SAS optimiser just isn't very good at such queries. Get 1 work table which gets all the min dates by Exposure Assignee Key, then join that result to your main query, avoiding subqueries altogether.
Take a look at sp_prepexec It is a bit old school but it has constructs for binding and passing parameters in a type safe manner. You'll need a linked server but you might find that is easier overall then OPENQUERY
Ahh ok, a Sequence
Totally thought this was the subredditsimulator
You need to find the function that will locate a string within a string. Then use the returned index as the start of the substring function.
Try going to codecadedmy.com or just google the Microsoft SQL for dummies free pdf
Okay, I worked on this some today and here's what I have so far. I created a stored procedure and a function (for calculating tax brackets). Now, I still need to do some logic with the Traditional IRA and the 401k, since I need to tax those accounts at the retirement age. Additionally, I need to add in compounding interest for the retirement accounts. I may want to add in inflation too. But this is a good start I think! Stored procedure: CREATE PROCEDURE [dbo].[personalfinanceTest] @Input_AgeCurrent INT ,@Input_AgeRetire INT ,@Input_Income DECIMAL(10,2) ,@Input_IncomeIncreasePct DECIMAL(10,6) --@PostTaxSalarySavingsPct decimal(10,2), ----Investment ,@Input_401k_PersonalPct DECIMAL(10,2) ,@Input_401k_EmployerPct DECIMAL(10,2) ,@Input_IRA_TradAmt DECIMAL(10,2) ,@Input_IRA_RothAmt DECIMAL(10,2) ----Market --@Inflation decimal(10,2), --@SavingsInterest decimal(10,2) AS CREATE TABLE #Table ([Age] INT NOT NULL ,[Years] INT NULL ,[Income] DECIMAL(10,2) NULL ,[401k Personal] DECIMAL(10,2) NULL ,[IRA Traditional] DECIMAL(10,2) NULL ,[Taxable Income] DECIMAL(10,2) NULL ,[Tax] DECIMAL(10,2) NULL ,[PostTax Income] DECIMAL(10,2) NULL ,[Savings] DECIMAL(10,2) NULL ,[401k Employer] DECIMAL(10,2) NULL ,[401k Total] DECIMAL(10,2) NULL ,[IRA Roth] DECIMAL(10,2) NULL ,[IRA Total] DECIMAL(10,2) NULL ,[Cumulative Savings] DECIMAL(10,2) NULL ,[Cumulative Retirement] DECIMAL(10,2) NULL ) DECLARE @age INT = @Input_AgeCurrent DECLARE @prev_savings DECIMAL(10,2) = 0 DECLARE @prev_retirement DECIMAL(10,2) = 0 WHILE @age &lt;= @Input_AgeRetire BEGIN DECLARE @years INT = @age - @Input_AgeCurrent DECLARE @income DECIMAL(10,2) = @Input_Income * POWER(@Input_IncomeIncreasePct + 1,@years) DECLARE @401k_personal DECIMAL(10,2) = @Input_401k_PersonalPct * @income DECLARE @401k_employer DECIMAL(10,2) = @Input_401k_EmployerPct * @income DECLARE @tax DECIMAL(10,2) = dbo.calculateTax(@income - @401k_personal) DECLARE @IRA_trad DECIMAL(10,2) = @Input_IRA_TradAmt DECLARE @IRA_roth DECIMAL(10,2) = @Input_IRA_RothAmt DECLARE @income_taxable DECIMAL(10,2) = @income - @401k_personal DECLARE @income_posttax DECIMAL(10,2) = @income - @tax DECLARE @401kTotal DECIMAL(10,2) = @401k_personal + @401k_employer DECLARE @IRATotal DECIMAL(10,2) = @IRA_trad + @IRA_roth DECLARE @retirement DECIMAL(10,2) = @401kTotal + @IRATotal DECLARE @savings DECIMAL(10,2) = @income_posttax - @retirement INSERT INTO #Table([Age] ,[Years] ,[Income] ,[401k Personal] ,[IRA Traditional] ,[Taxable Income] ,[Tax] ,[PostTax Income] ,[Savings] ,[401k Employer] ,[401k Total] ,[IRA Roth] ,[IRA Total] ,[Cumulative Savings] ,[Cumulative Retirement]) SELECT @age ,@years ,@income ,@401k_personal ,@IRA_trad ,@income_taxable ,@tax ,@income_posttax ,@savings ,@401k_employer ,@401kTotal ,@IRA_roth ,@IRATotal ,@prev_savings + @savings ,@prev_retirement + @retirement SET @age += 1 SET @prev_savings += @savings SET @prev_retirement += @retirement END SELECT * FROM #Table And the function: CREATE FUNCTION [dbo].[calculateTax] (@salary AS money) RETURNS money AS BEGIN DECLARE @tax money DECLARE @10high money = 9275 ,@15low money = 9276 ,@15high money = 37650 ,@25low money = 37651 ,@25high money = 91150 ,@28low money = 91151 ,@28high money = 190150 ,@33low money = 190151 ,@33high money = 413350 ,@35low money = 413351 IF @salary &lt; @10high SET @tax = @salary * 0.1 ELSE IF @salary &lt; @15high SET @tax = @10high * 0.1 + (@salary - @15low)*0.15 ELSE IF @salary &lt; @25high SET @tax = @10high * 0.1 + (@15high - @15low)*0.15 + (@salary - @25low)*0.25 ELSE SET @tax = @10high * 0.1 + (@15high - @15low)*0.15 + (@25high - @25low)*0.25 + (@salary - @35low)*0.35 RETURN @tax END; GO Open to any advice! EDIT: And in case you want to test it: EXEC @return_value = [dbo].[personalfinanceTest] @Input_AgeCurrent = 23, @Input_AgeRetire = 60, @Input_Income = 50000, @Input_IncomeIncreasePct = 0.04, @Input_401k_PersonalPct = 0.06, @Input_401k_EmployerPct = 0.03, @Input_IRA_TradAmt = 2000, @Input_IRA_RothAmt = 3500
I am taking an online course from www.ed2go.com pretty affordable.
I'll try and paste the errors tomorrow. Without the dynamic SQL it runs with a full table pull open query in about 30 seconds. Another commenter suggested using #tables with dynamically generated names instead of a cte so two users wouldn't crash into each other.
I keep repeating myself on this sub-reddit but.... I highly recommend the [Mode Analytics SQL Tutorial](https://community.modeanalytics.com/sql/tutorial/introduction-to-sql/). It was recommended to me when I was interviewing with a large tech company and it was the easiest for me to understand and was actionable. I've used the O'Reilly books (too face paced / skips around), Coursera (too fast paced), Codecademy (doesn't help you retain knowledge), and paid tutorials from Udemy. Mode was by far the best. 
Why not Where timestamp - %number% &lt;= 12
Agreed. Looping through and inserting rows is procedural and goes against set theory. Phunky is right, good start but keep trying to move into a purely SQL basis if you really want to learn RDBMS. Otherwise you're just implementing your Excel techniques in a less optimal place
&gt; Original statement cleaned up SELECT MAX( TimeStamp) -- Never name a variable a datatype name. Reserved words are reserved words for a reason. , Description FROM T_Table WHERE Art = 'isAlive' GROUP BY Description &gt; but I now kind of want to only select rows where the difference between the TimeStamp and a given number is equals or less than 12. Assuming Timestamp actually is a timestamp containing full year, month, day, hour, minute, seconds, milliseconds WHICH of these values are you looking for a difference of 12? Just saying you want the difference between a TIMESTAMP and a given NUMBER is not useful since you don't compare timestamps and integers. Lets say you have a timestamp value @Check_by that the time stamp must be within 12 hours of. SELECT MAX( TimeStamp) -- Never name a variable a datatype name. Reserved words are reserved words for a reason. , Description FROM T_Table WHERE Art = 'isAlive' AND ABS( DATEDIFF( HOUR, @Check_by, Timestamp)) &lt; 12 -- absolute value.. you're not specifying that it has to be less than 12 hours before so I went with within 12 hours to either side. GROUP BY Description Define the problem more clearly and we can be more help
&gt;Is there any way to loop through tables in an access database and create the dataflow on the fly so the metadata is correct? BIML or C# is the only way you can really accomplish this. SSIS Packages are pretty static and typed processes. It really makes it so you have to use some type of scripting language to dynamically build the SSIS package itself at runtime and then run it.
The link you posted has the solution, you just need to follow the SQL Server examples, since that is what you are using ...
Packaging data into a propriety container format isn't really SQL.
You are likely using a version of SQL Studio that doesn't perfectly match the version on the server, which sometimes leads to very annoying bugs in the client.
I am upgrading the application now. 
Ahhh. I was following an app on iPhone called 'Learn SQL.' As such, I didn't realize there was a difference between MySQL and MS SQL. 
dear OP, please confirm your database platform because people are giving you MS SQL solutions, which is a waste of their time if you're not running MS SQL
&gt; w3schools isn't the best resource full stop
T-SQL: SELECT RIGHT(SUBSTRING([AddressName],CHARINDEX('HRG = ',AddressName),LEN('HRG = ')+2),2) The 'HRG = '-values (there are 2) could be replaced by a variable instead to make the method easier to work with, which could also be done for the number of characters you are looking for (again, because this might vary. If using SAS (although it's been a while since I've used it and I appear to no longer have a working license), but doing this in a data step should provide a similar result (Essentially, the two expressions are mostly identical): AddressName='HRG = XX'; OutputValue=right(substr(AddressName,find(AddressName,'HRG = '),length('HRG = )+2),2);
[I typed your exact question into Google.](https://www.google.com/#newwindow=1&amp;q=I+need+to+create+a+ssis+package+that+will+create+an+excel+file+with+the+sql+result+inside.) [First result](https://sqljourney.wordpress.com/2013/01/12/ssis-create-new-excel-file-dynamically-to-export-data/)
Thanks! Unfortunately I don't think this will work for me, as I will still need the RowID and JobType for each record. I'm thinking my best bet might be to order by JobType, AccountID.
Thank you! I believe something like this will work for me. 
Where does the error occur? You refer to a GUID, are you sure it is not GUI?
&gt; Ahhh. I was following an app on iPhone called 'Learn SQL.' As such, I didn't realize there was a difference between MySQL and MS SQL. Uh... there are a LOT of differences...
It occurs within the menus of the application. You click on the menu item and navigate away and the error occurs. 
SQL is a data query language. There are various implementations by major manufacturers that comply with the standard and extend it to various degrees. MSSQL is Microsoft product, MySQL is owned by Oracle, etc. The basic language of SQL is the same for queries. Anything related to advanced management is usually proprietary. There is no product that is fully standard because the standard doesn't describe an actual DB product but an abstract idea. NoSQL refers to all database products that don't support SQL querying such as MongoDb. You have to read into it to understand what exactly that means.
where to start... Some simple googling will teach you this. * SQL is a language * RDBMS (Relational Database Management Systems) use SQL to get data from their datastores. These are Mysql , Oracle , Microsoft SQL server just to name a few.. there are tons. * you typically administer only one of these. But in some cases can do many. * If I were you I would leave NoSQL out of the equation until you have learned a bit more about What i've mentioned above.
They don't do recursive ones though. It would make a few problems simpler if they did.
Recursive CTEs are quite a bit different than what you're describing. 
Relying on an existing engine called AlaSQL (http://alasql.org/) - looking into expanding it to be a little nicer to work with. 
GUI*
This is bare bones VM that I moved the database too. I have completey reinstalled the application, upgraded, etc. Other databases work just fine with the application where this one bombs out with one menu item.
It still sounds like your application isn't properly handling something it's getting back from the database. As no one here has your database or application source code, you're not going to make much progress here. You're going to have to run the application in a debugger to capture what's happening.
What debugger do you recommend?
This is the link from the error message i'm trying to install MS SQL and when im installing the functions it gives a error at database engine services and this comes
read this: https://en.wikipedia.org/wiki/Comparison_of_relational_database_management_systems
Thanks for pointing it out! Edited it in
Yeah, in retrospect, using joins makes the code look like such a convoluted clusterfuck that using functions and inline subqueries (e.g. WHERE x IN [subquery]) is going to be way more intuitive than what I've proposed. Though in my defense, I'm showing the verbose version, with the hope that other people can see that it only *looks* like a clusterfuck, but it's fairly organized if you can compartmentalize things, such as mentally condensing the visual clutter of a subquery into its table alias. My main rebuttal was going to be that "XYZ (including inline subqueries) are slower than join statements"...but I just checked with the latest version of MySQL, and the optimizer is quite good at knowing how to keep track of indexes and so forth. In fact, JOINs seemed to be less optimized. However, thinking of the bigger picture: I think subqueries (of the correlated type) provide more flexibility and expressiveness when it comes to doing JOINs involving more than one or two tables, at the cost of significant upfront boilerplate. 
Different data (or missing data) in this particular database. 
This web site is well suited for beginners. You can do online exercise there. http://www.studybyyourself.com/seminar/sql/course/?lang=eng.
This web site http://www.studybyyourself.com/seminar/sql/course/?lang=eng may be interesting for you. You have a basic course and an advanced one. You can submit exercises online too.
Usually I'm better off with separate query objects to organise big queries. CTEs don't get flattened by the optimiser so it only helps in the rare occasion that the optimiser hasn't got the right info for a good plan for whatever reason.
Not going to do your homework for you, but I can tell you that you'll need the following concepts to solve this: 1) aggregate functions and group by 2) where clause 3) having clause
Great point about perms and temp tables.
Whoops, bug! I meant to just join to the CTE in this bit, `JOIN clients c on c.id = d.client_id`
Depending on what you want to use it for, there's the Ms sql server express, doesnt have agent jobs, and etl requires either pure tsql or ssis, or a combination of them, for ssis you need a full sql server to get integration services. You can use trial version for 180 days before having to rearm. Maybe you should add what your purpose is, learning? Production? Homelab? 
The management studio only works with SQL Server. However, there are a lot free databases for personal use. Oracle and DB2 have free limited versions, MySQL/MariaDB are free as well as Postgres (my favorite actually). You could also use something server less like SQLite depending on your requirements. 
Ima upvote cause of the amount of effort that went into this answer. I wasn't bold enough to provide this kind of feedback. I'll say though that I wonder if dispatch_date is really the field to be using for date filters. I'd say it's probably better to use available_from (but what's up with the data type there? 12.07? Really?) Lastly, this table is so badly designed! If the rest of the instruction is this shoddy it's no wonder OP doesn't know how to write a very simple query.
 select datapoint1 ,datapoint2 ... ,datapoint5 ,sum(total) as total ,sum(total2) as total2 from ( {your query here} ) group by datapoint1 ... ,datapoint5
If I had someone in my employ that designed schema's like that I would have to encourage them to seek opportunity elsewhere. well the if it happens to be Available_From and it happens to be a date datatype then figure out how to use "DateDiff()" coupled with "Now()" to get a value in terms of months and shove that into the where clause. Also I wonder how many downloads I'm going to get because people are reading it on mobile in the text formatting is stripped out.
Other comments have hopefully resolved the difference between DBMSs, so the question is "which to learn?" Per http://db-engines.com/en/ranking , Oracle, MySQL and MS SQL are probably the ones to learn first. That said, don't try to learn them all at once. If you get good at one, you'll mostly know the other two and just have to learn the small ways in which they are different. MySQL is pretty good for beginners, cause you can get a full installation for free, meaning you can install it wherever you want to play around with it. Do you have an industry that you want to end up in? If so, Google around to see which DBMSs are most commonly used in that industry.
Thanks for the advice. I think you're right with a lot of this. I have more data sources integrated and I think my next steps are to get more users to adopt my dashboards and make dashboards for a wider variety of business teams. My biggest concern is that the numbers won't match since I don't know the exact logic/filters they're using. My role in the company is on the marketing team, I'm the marketing ops manager. 
 Well, I'd do this using mysql and writing a few scripts, and then slapping a website on top of it for him to manage the data. Ssis is very overkill for what you are proposing imho.
You've gotten good advice already. But for the "not enterprise ready" argument against Tableau maybe this will help put some arrows in the quiver: https://www.tableau.com/about/customers
That is a good point. My initial questions are not intended to support two data warehouses, but to understand how this situation came about. Even if there is a single data source, the existence of two dashboards also risks confusion if either dashboard differs in terms of filtering, aggregation, etc. 
+1 for Joe having the same answer in both posts 
mysql is free
Save yourself time and trouble, SQL Server back end and use Access Forms for the front end. Easy.
I like to build things in chunks, then put them together. Kind of like a module. You start with your base, then add layers. Let's get the base. Update Table Set column = value Ok, now let's look at IF. https://msdn.microsoft.com/en-us/library/ms182587.aspx IF Boolean_expression { sql_statement | statement_block } [ ELSE { sql_statement | statement_block } ] So you want to run the proc if a value coming to the proc is the same as a value in the column? Your proc needs to have variables to pass. Create Procedure dbo.proc (@variable varchar(255)) as begin ... end Rememeber, to stop parameter sniffing you need to declare the variable in the procedure and set the passed variable to it. DECLARE @StopTheSniffing VARCHAR(255) SET @StopTheSniffing = @variable So now when you pass your value, the proc sets the value to @variable, then to @StopTheSniffing. Ok, so now let's look at the if again. IF @StopTheSniffing = (select top 1 column from table where column = @StopTheSniffing) Begin Update Table Set column = value where column = @StopTheSniffing Else Print 'Nothing to see here, move along' End The above takes the parameter passed, and looks for it, then the IF evaluates if that is true and begins your update, your update is based on the parameter passed. If nothing is found, it ends the query. 
Generally for stored procs I only use #tables. I get into @tables for reports that have the possibility of being run concurrently. This applies to CTE's as well. The threshold is when you have a process that is taking too long to run, which is subjective. Generally in my environment I want everything to run in less than 30 seconds unless there is a good reason otherwise. 
Yup, I have set up quite a few DB's using the new SQL Server 2016. Long as the SQL Service agent and the other services necessary are installed, running, and have proper setup and access, it's just like 2005 (even 2000) and up in creating DB's. 
Holy shit I'm an idiot. Thank you!!
&gt;I can't alter the table after the update using a trigger Not sure about MySQL, but I was under the impression that triggers are designed to do exactly this.
You need to get rid of the M:N relationships on that one table with a couple of intermediary tables. 
According to /u/BrentOzar, you should never use table variables in stored procedures. This is mostly because the optimizer estimates the row counts incorrectly. Mostly it estimates 1 row, but that can vary depending on situation (e.g. if you use OPTION(RECOMPILE) it gets more accurate.) This can and does throw off prediction on which join type (Nested Loops vs. Merge vs. Hash Match) is better in different conditions. This can also cause the optimizer to not allocate enough memory to a query, which in turn leads to TempDB spills. Also, temp tables get statistics, while table variables do not. Statistics are invaluable for optimizer to come up with a good execution plan. More info [here](https://www.brentozar.com/archive/2014/04/table-variables-good-temp-tables-sql-2014/). [Edit] Watch [this video](https://www.youtube.com/watch?v=Fi2k9ullztQ) as well, it was very helpful to me.
Usualy, optimizer don't have any information about the table variable statistics and assumes that it always has only 1 row, which in some cases leads to ineffective execution plans. Temporary table do have row statistics and the plan have better chances to be more effective when you're using temporary table. Usually I use table variables, if I need to loop through data one row at a time, or if the dataset is relatively small and is used only once or twice. I switch to temporary tables, once I start doing aggregations, or start to think that the performance might benefit from an index created on that temporary table.
Hmm, I am sometimes getting this error: &gt;Error Code: 1362. Updating of NEW row is not allowed in after trigger Other times I get a runtime error saying that the table that I'm inserting into doesn't exist. Here's the trigger: DELIMITER // CREATE TRIGGER incrementPersonVersion AFTER UPDATE ON Person FOR EACH ROW BEGIN IF OLD.modifiedAt &lt;&gt; NEW.modifiedAt THEN INSERT INTO Person.revision VALUES (NEW.revision + 1); END IF; END // Where it say's 'INSERT INTO Person.revision, I've tried using 'revision', NEW.revision, NEW.Person.revision, and perhaps other variations I can't think of now. I don't get a syntax error when I create the trigger, but I get an error in Eclipse saying that the table does not exist. I've been assuming that perhaps it was related to error 1362? Perhaps it's telling me the table doesn't exist because for all intents and purposes it is just not accessible to me at that time? I'll keep tinkering, but any insights would be greatly appreciated. Thanks EDIT - I tested a different statement between BEGIN and END to try to make what was happening after update more explicit (at least in my untrained eyes): DELIMITER // CREATE TRIGGER incrementPersonVersion AFTER UPDATE ON Person FOR EACH ROW BEGIN IF OLD.modifiedAt &lt;&gt; NEW.modifiedAt THEN UPDATE Person p SET p.revision = p.revision + 1 WHERE p.id = NEW.id; END IF; END // And I got this error in Eclipse: &gt; Can't update table 'Person' in stored function/trigger because it is already used by statement which invoked this stored function/trigger.
SQL syntax for INSERT is the following: INSERT INTO Person (revision) VALUES (NEW.revision + 1); But I assume that you simply want to update the row, not insert another one, right? Note that the query below assumes that you have some Id field in your table. If not, you would have to use BEFORE trigger. UPDATE Person SET revision = NEW.revision + 1 WHERE Id = NEW.Id
Hey thanks for the response! Maybe my response to /u/nvarscar could explain some. I'm not familiar with the term upsert. When a Person page is saved on the front-end (I built the front-end) all of the info is resent and the backend (which I didn't build) sends it to the SQL to update regardless of whether it's all the same info. For presentation purpose he wants to be able to have a display where there is: Version # | Timestamp | Perhaps a button to retrieve all the associated info from this version. The timestamp only updates if an actual value changes and I want the version column to imitate this behavior. The easiest way seemed to be to tie incrementing the version number to when the timestamp updates (thus verifying that information has changed). All of this may be off topic from your comment... If the row exists with all of the information about a specific Person, we won't insert a new row, but update that same row (whether or not a value has actually changed).
That still gave me this error: &gt;Can't update table 'Person' in stored function/trigger because it is already used by statement which invoked this stored function/trigger. If I could do this BEFORE I think that would solve this problem. Except I think it presents a different problem, being that, how can I tell if any actual change will be made to the row before update and that the table is not just being updated with the exact same data it already has. I'll have a go at trying out a BEFORE trigger again though. EDIT - I tried making it a BEFORE trigger, but the line IF OLD.modifiedAt &lt;&gt; NEW.modifiedAt THEN Does not seem to work. The timestamp updates, but the version does not... I think that's because the timestamp is not changing until the update on the table is already performed. So the condition doesn't work. 
Or a CURSOR (/s) They did it that way because they need to exec a couple of OTHER procs for each row in the @Table. I have no doubt that a total refactoring of the whole shebang would result in a single query/update... but you know how it is: There's never time to do it right, but there's always time to do it over. Thanks!
Ouch.
It seems that MySQL doesn't allow you to work with the same table :/ I believe, you can use this in the BEFORE UPDATE trigger: FOR EACH ROW BEGIN IF OLD.modifiedAt &lt;&gt; NEW.modifiedAt THEN SET NEW.revision = NEW.revision + 1; END IF; END
That CSV is not a CSV, so there is no elegant solution. The elegant solution is to ask the CSV provider to send you a valid CSV. 
This is the answer. SAS represents blank/missing/null values with a dot (not-fun fact: SAS ALSO uses .A-.Z and ._ as "special" missing values). "WHERE [variable name] IS/IS NOT NULL" is ANSI-standard SQL, so that's your best option.
Thanks for the clarification and no problem! It's definitely a stupid issue to have to resolve. Playing around with the import format a bit more, it seems that specifying the field as a SQLVARCHAR in the XML file is forcing the int to a char. I don't know why I haven't seen that explained anywhere, but maybe my google-fu just isn't that strong! Thank you for taking the time to help!
Ok, in that case I don't think there is any other option except OLD.someValue &lt;&gt; NEW.someValue || OLD.someOtherValue &lt;&gt; NEW.someOtherValue || etc. EDIT: and consider using &lt;=&gt; to avoid comparing possible NULL values: IF !(OLD.a &lt;=&gt; NEW.a AND OLD.b &lt;=&gt; NEW.b)
The legacy '89 syntax works in 12g.
I'm not working that job anymore, but I distinctly remember something rejecting that syntax. Maybe it was just the Oracle Thin or DataDirect JDBC driver.
Ahh, okay. Thanks for the help!
still don't see the need to ever to do all that
&gt;they need to exec a couple of OTHER procs No they don't. There is a simple set based redesign somewhere.
Looks like you're working really hard to avoid a join. Just join table 1 and 2 together then you will have the total values there ready to sum. If the join is outer, uae COALESCE() on the datapoint columns to ensure a value is always returned.
Sql server express is free and comes with SSRS reporting and full text search https://www.microsoft.com/en-gb/server-cloud/products/sql-server-editions/sql-server-express.aspx Developer is completely free and every feature is available, you're just not supposed to use it for production. Postgres is free and comes with a GUI tool. MySql Is free and comes with workbench GUI That's just a small selection.
JDBC driver thing sound sensible, I would be surprised if a driver supported that syntax, but Oracle itself has been around so long it has all kinds of legacy support and proprietary extensions.
Bingo. The change I'm pushing (ok, BEGGING) them to make is to add a PK. They're hung up on finding "root cause", however. Grrrr... So I was thinking I could just tell them that SQL handles @Tables differently based on # of rows. Thanks for the reply
Right. Hence my "refactoring" comment. 
You will find this particular syntax a lot with shops using certain versions of Microsoft devstudio and BusinessStudio as a front end for developing queries to Oracle databases. There was a bug in the UI of the Microsoft product that displayed the query in visual layout form. For some reason it converted outer joins in queries against oracle databases that were displayed in the visual layout window into this older format. If the query was left in text format it was fine. I was on a team converting University of North Carolina campus reports from an old version of Cognos into Sql Server Reporting Services reports against an Oracle db. That's where we ran into and we left queries like this all throughout the code. We reported the problem to Microsoft and kept moving.
I can't speak for any other DBMS but I know for a fact that in Oracle, ANSI join syntax has the exact same performance as old Oracle style joins.
Sorry that should be "like" not "="
Using the PIVOT command: https://technet.microsoft.com/en-us/library/ms177410(v=sql.105).aspx
Another consideration is duration. Table vars go away at the end of the current statement. Temp tables go away at the end of the current session.
I see your point, but isn't it more of a hassle to go in and update the SSRS report each time a new column is added? I'm just trying to understand. Wouldn't it be easier to just let SSRS do the work for you by using a matrix to dynamically pivot out the columns you need? I used to feel the same way about SSRS, but this tool can do so much, way more than you can imagine. Its just not user friendly sometimes with more complex tasks. 
I disagree. PL/SQL has many features that T-SQL does not. Yes, the naming is different but that's only on the surface. In PL/SQL cursors are your go-to structure, whereas cursors in T-SQL are a performance nightmare. Both try to add iterative and procedural logic to SQL but do so with different methods and implementations. Going from one to the other is like going from Java to C#: you'll recognise a lot but writing highly optimised code will not be an exercise in copy-pasting. 
Having worked in multiple RDBS's (MSSQL, PL/SQL, PostgreSQL, MySQL) my opinion is that the basic syntax of MSSQL is more similar to PL/SQL than to the others. So, if all RDBS's are cars, then MSSQL and PL/SQL are sedans while the others are a van or pickup truck. The basic relationships are the same, but they handle differently by degrees.
The first two things you'll run into is that NVL() is now IsNull() and Decode() is now handled via if statements in the select list.
&gt; Table vars go away at the end of the current statement. If by "current statement" you mean current **batch**, then you would be absolutely correct. This has a side-effect of being unable to reference a table variable inside of dynamic SQL, if you declared it outside of that dynamic SQL. But OP probably knew that already. The only use case I can think of where you'd want to use a table variable over a temp table is if you need to do logging, and there's a transaction in the middle of your code. This is because data in table variables survives a rollback, but data in temp tables does not. If you *want* to see the log entries that were inserted during a rolled back transaction, table variable is the way to go.
Show them an execution plan where the Estimated Vs. Actual Rows amounts are wildly different (e.g. 1 estimated, 80k actual.) That'd be the root cause - optimizer cannot get an accurate estimation on the table variable, and is doing things like choosing inefficient join type, or not granting enough memory to the query which causes TempDB spills.
Welll.... you're right of course, except that this behavior just started a month ago. The (badly written) SELECT has been running that way for a long time with no performance problems (compared to the other instance, that is -- I'm sure BOTH of them will speed up once they fix the @Table). There's been no increase in volume. My contention is that they've just been getting lucky all this time, but they're not buying it. Well, they've finally (yesterday) agreed to the @Table fix, so maybe there's light at the end of the tunnel.
Thank you!!! After further research I realized I could use the SYS_CONNECT_BY_PATH function. This allows me to dynamically join all of the rows, and then I can just do a rank partition by to get the latest. Awesome solution this was incredibly helpful
Unfortunately, no. Parameter sniffing is one of my favorite "hidden" problems to look for. Good thought, though. 
 select (your_shit FROM your_table) as x Pivot (SUM(x.Shit) for Pivot_Column IN ([Criteria1], [Criteria2], [Etc]) 
If or case? Sorry, couldn't help myself. 
T-SQL is very "set-based" where Oracle PL/SQL is often iterative.. * Cursors that perform well in Oracle are very bad in SQL Server. * If I recall, update triggers in Oracle fire "once per row" where in SQL Server you get one trigger for the whole set of updated records, with the data made available in two virtual tables "inserted" and "deleted" that can be operated on in a single pass. * Empty strings ARE NOT NULL! '' = '' is a true expression. (EDIT: As it should be!) * Common Table Expressions (CTEs) perform much better in SQL Server than in Oracle (this is my subjective experience) * SQL Server is way, *way* better to work with (also my subjective experience)
No need to apologize - you are correct. Both get used to replace the Decode() function. FWIW, I do prefer T-SQL's syntax for it - complicated decode statements get very messy very quickly.
It's because your SELECT clause includes an aggregate function (SUM). This tells the engine to SUM that value across all records returned by your query, which results in only one row returned In contrast, when used in conjunction with GROUP BY, SUM produces the sum of all records for each group, so you get a result record for each group
Please add examples of both queries to your post, one with group (present already) and the one without. I have a feeling that you're using an aggregate function in a query without a group by clause, which (given that you don't have any other columns in SELECT) implicitly aggregates all the rows automatically.
Different database engines behave differently for this. MSSQLServer will refuse to execute a partially aggregated query, but MySQL does it just fine, providing intuitive (but not precise) results (e.g, picking the first value from the result set for the director column, and then the aggregate sum for the 2nd column)
Read Itzik Ben Gan's TSQL books to ramp up on how to solve most of the common problems you'll face.
I'm not familiar with SAS's proc sql, but in general terms, I would LEFT JOIN to a subquery that selects only the providernumber and a specialist/pcp flag. Depending on your data, it might be enough to calculate one (PCP or specialist), and then say the negation of one indicates the other. Select p.providernumber, p.name, o.address, s.specialtyname , case when PCP.provider is not null then 'PCP' else 'Specialist' end PROVIDER_TYPE From p.provider Left Join office o on o.providernumber = p.providernumber Left Join specialty s on s.providernumber = p.providernumber left join ( select providernumber from ... join ... left join.. where ... and ... ) PCP on o.provider = PCP.PROVIDER 
Also, careful with this part: Left Join specialty s on s.providernumber = p.providernumber Where s.primary_specialty = "True" Again, I don't know anything about proc sql, but in most RMDBs, that's the same as an inner join, because the records that don't exist on `specialty` will have `primary_specialty = null`. You probably want: Left Join specialty s on s.providernumber = p.providernumber and s.primary_specialty = "True"
I'm a developer, not a DBA, so we likely spend time focusing on different aspects of these systems. In any case, it's good to hear other views.
You really need to step through the normal forms to fix up your schema. You need to go to 4th normal form at the minimum. 
Well, you made a mistake copy-pasting or preparing an example somewhere, because what you have works fine - see this [sqlfiddle (using MySQL syntax)](http://sqlfiddle.com/#!9/9cab7e7/2). Now if you consider the situation /u/digicow described - that your `OrderDate` column is actually `DATETIME` - you will indeed get no results, if the time is not 00:00:00 - [sqlfiddle](http://sqlfiddle.com/#!9/9f0b7/1). Now depending on the database you're using, there are different options. The easiest one that should work for all major RDBMS is to cast the `OrderDate` column to `DATE` SELECT o.OrderNum FROM Orders o INNER JOIN Trackers t ON o.AcctID = t.AcctID WHERE CAST(t.OrderDate AS DATE) = '2016-08-01'; This should work with MySQL, PostgreSQL and SQL Server. Oracle has a bit more convoluted way of dealing with dates, so you'd probably need something like `WHERE TRUNC(t.OrderDate) = TO_DATE('2016-08-01', 'yyyy-MM-dd')` - that is truncate the `TIMESTAMP` value first down to date only and then comparing to the date, converted from a string. Other engines do the conversions implicitly. 
Thanks for the reccomendation!
This really belongs in /r/php, is this going to be an actual public facing site or does it just need to work for class? You can use the code from [here](http://www.w3schools.com/php/php_mysql_insert.asp) no problem but you should really use prepared statements in lieu of mysqli directly. If someone enters "; drop organization;" in one of those fields, you'll lose your whole table as is.
Got it, I don't want to do your work for you then but you can copy this code almost line for line to get this working. Just access your variables as follows to populate the pdo statement: filter_input(INPUT_POST, 'orgUsername'); http://www.w3schools.com/php/php_mysql_prepared_statements.asp
The columns in `wp_options` include a `varchar(64)` and a `longtext`. The table is too big to display on screen. Try a query analyzer that uses a grid view. MySQL Workbench does, IIRC.
&gt; can I only use SELECT, FROM, WHERE only once within a very long script? You can use them as many times as you want, even within the same individual SQL statement. Everything that is the result of a select statement is a table that itself can be selected from. Subqueries is the term usually used to describe this. &gt; is it possible to invoke the UNION statement to in order to use the SELECT function more than once using the same database? Yes. UNION combines the results of two or more SELECT statements.
As other commenter mentioned sub queries. But imo use temporary tables before making huge queries... 
Could you explain what you need in a general sense. Maybe talk about the output of the query and what tables you have access to?
 SELECT * FROM TableA SELECT * FROM TableB SELECT * FROM TableC You'll get 3 sets of results. From this one piece of code, but technically it is 3 queries.
OP, that's a pretty odd question. Once you are a bit more experienced you'll realize that =p. Anyways, if you shed some light on what your objectives are, I'm sure myself and /r/SQL would be happy to help. Also, the answer to question 1 is no and question 2 is yes
If all septors have 4 sockets, then likely each has a field name (socket1, socket2, socket3, socket4), but without knowing the database architecture, I don't think anyone here will be much help.
You can join the table to itself on the condition that date on the left is greater than date on the right. Then check for the max difference between rank on the left (the more recent rank) and rank on the right (the older rank). If your table is called Ranks, something like: SELECT MAX(A.rank - B.rank) FROM Ranks A INNER JOIN Ranks B ON A.date &gt; B.date You'll need to add some additional logic to make sure you are comparing the same types of games, seeing the max difference for each game rather than the max of all games, etc
This is **really** great! I didn't know and never considered joining a table to itself. This query gets the same results as the one I came up with, but yours is much more simple and easy to understand. Thanks!
You need to provide a more concrete example of what you want to accomplish. Yes you can use multiple SELECTS, you can also use UNION, but the question is, do you need to use those? Like the example /u/pythor gave: select id, name, desc from TableA where id = 5 union all select id, name, desc from TableA where id = 10 Can be written as: select id, name, desc from TableA where id = 5 or id = 10
Don't forget PL/SQL's exception handling. SQL Server has TRY...CATCH, but I don't see it used much.
From the way the question is presented to me, the DISTINCT seems to be desired but it isn't clear. Also, your query compares the reviewers' ids to get rid of the duplicate and the question states it needs to happen based on the actual names. 
Even if they don't have the same name, the pairing can be incorrect by checking: AND R1.rID &gt; R2.rID The problem requires that "[fo]r each pair, return the names in the pair in alphabetical order." I read this as for a pair of reviewers John and Sally, return John as the First and Sally as the Second. rID|name ----|---- 50|John 75|Sally With the check above, you would get nameFirst of Sally and nameSecond of John. The request is that nameFirst of John and nameSecond of Sally.
Yes, you're right. Here's the updated query: SELECT R1.Name, R2.Name FROM Reviewer R1, Reviewer R2, Rating Ra1, Rating Ra2 WHERE R1.rId = Ra1.rId AND R2.rId = Ra2.rId AND Ra1.mID = Ra2.mId AND R1.Name &lt;= R2.Name AND R1.rID &lt;&gt; R2.rID ORDER BY R1.Name, R2.Name 
Be specific. What do you need to know? Want to practice? Get a book on MSSQL (focusing at using the commandline) and play with it at the commandline to see what each thing does. When you get to using the GUI, everything will seem easy, but you'll understand what goes on under the hood. 
I'd recommend reviewing your book's explanation on what the different columns represent. Your SQL seems fine, except I'd add an ORDER BY clause to match the results. The issue is the formula inside your SUM. You are calculating the item prices with the discount field being a percentage discount. The formula that appears to be used is: UnitPrice * Quantity - Discount This presumes that the example solution represents the actual solution and not just an example. Refer to what each column actually represents in your documentation.
I'm not sure what your question is. Are you just wondering if your answer is right? Here's what I came up with, which seems to provide the answer in your first link: SELECT ProductID, SUM(UnitPrice*Quantity*(1-Discount)) AS OrderTotal FROM ProductsOrders GROUP BY 1 ORDER BY 1;
Thanks for your helpful response. This runs SQLite 
Yea ur right. My bad I wasn't thinking straight
The discount column is in decimal form. So it should be (1-discount)
This statement GROUP BY 1.......I understand what GROUP by does but what does the 1 stand for?
I know. And I agree that your formula would be the correct one, except it doesn't match the results you provided as the solution. Discount has three values in the sample data: 0, .15, .2. This would imply to me they should represent a percentage. If you calculate the normal formula you would use, which you did, it doesn't match the answer set you provide to mirror. 
&gt; help me revise for a test Huh?
Unfortunately I'm on a mac and the only quick way I found to use your database and convert it to .csv was through a free tool that deletes the 2nd half of every table. So I believe the answer I provided was right. However, since I don't have every row of every table, my end results not perfectly match those in your screen shot. If you copy and paste my answer (which I think is the same as your answer) on your end, do you get a different result as your screenshot? I.e. the amount for order 1 doesn't equal 1450.65?
You can easily access row data in statement level triggers in Oracle using a compound trigger (before / after row - store values in a collection or GTT and before / after statement access it). It's not difficult and can be achieved in a few lines of code. Row level triggers are useful, so the fact SQL Server doesn't support them is equally annoying - even if they can be achieved using inserted / deleted tables. No, I'm a developer who has to work on both platforms. It's not sales speech, I have no affiliation to either, I've just many years of day to day experience working on both RDBMS platforms and have a good grasp of the functionality each provide. From my experience, Oracle destroys SQL Server on functionality. There's no comparison - Oracle is leagues ahead. When I hear developers say they prefer SQL Server or think it's better than Oracle, then it just just suggests to me they don't develop on both in their day to day job. Oracle's null / empty string duality may not adhere to ANSI specs, but it makes development less verbose and bug prone. I've lost count over the years on the number of times I've interfaced with SQL Server systems that get caught out by null &lt;=&gt; empty string bugs. Where a developer has accounted for one and not the other, or hasn't coded for null propagation, when they have CONCAT_NULL_YIELDS_NULL on or aren't using CONCAT. 
Yes, it's pretty weak to be honest.
 Insert targeTable (field1, field2) Select FieldX , FieldY From tableSource 
yes, what I'm asking is that, for example CREATE TABLE Table1 (field1, field2, field3) INSERT INTO Table1 SELECT field1, field2 from Table2 Now, I don't plan to use field3 because it is where I'll put updated data on. It is not in any table. I need to edit field2, do some joins, and then from there get values for field3. I'm using SQL on VBA, and it doesn't let me update unless I specify something for field3.
If you read under the title you'll see that I said I am not looking for the answers. I want someone knowledgeable to walk through it with me and help me understand.
The answer from `da_chicken` is fully correct. You should use MySQL Workbench which you can download here : http://dev.mysql.com/downloads/workbench/
You need to give a better outline of your tables and some sample data.
I don't know what the context is, and I say this in jest, but are you the guy I work with that adds distinct to every query you come across? If so, schedule a meeting at the flag pole for 5 o'clock tomorrow. 
How does that column assignment work in your example there? I've seen variable assignment using select x = y, but not columns. Is there a resource you could point me to for this? is this in MSSQL? is this similar to a "SELECT X INTO TABLE"
I add it when I'm not grouping, want distinct values, and don't know the table contents. I'm more curious about the `bit` being interpreted as *FALSE* vs *0*. In my environment it would be a 0.
For 2k I did the Oracle courses on Oracle 11g SQL. Would highly recommend it. The format of my course was two nights for 8 weeks in a classroom setting. I learned a lot more than how to write SQL; I also learned how the database works, how to make efficient queries and statements. These skills have given me a huge advantage on the job and landed me a better job. They have 12c available now and would be similar content.
That's generally what I do, Remember to do a LEFT join or maybe even a FULL OUTER JOIN depending on what your data looks like.
[removed]
Admittedly I've not used MS SQL for a few years now, but my recollection is that FALSE is just an alias for 0. I could be wrong, though. It happened once before.
Lazy answer because this question sounds like homework. Select '&lt;employees&gt;' as emprow UNION ALL Select '&lt;employee&gt;&lt;empid&gt;'+ empid +'&lt;/empid&gt;&lt;empname&gt;'+ empname +'&lt;/empname&gt;&lt;/employee&gt;' as emprow From HR.Employees UNION ALL Select '&lt;/employees&gt;' as emprow 
I really like your syntax for the first 3 lines. Im stealing it :)
Nobody here knows what a 'group' or 'account' is. I'd advise that you find a record that gets duplicated when you join and look at that record in each table individually. That should then explain why you're getting a duplicate.
[Hire me :)](http://www.jooq.org/training/)
If you're using Java, JDBC, JPA, Hibernate, or jOOQ, you can also check out my [High-Performance Java Persistence](https://github.com/vladmihalcea/high-performance-java-persistence) book which is around 25$ and you get over 400 pages on data access best practices.
You may have seen something like: insert into #table1(field1,field2) values(val1,val2) Instead of supplying the value set directly, you can pipe in the result set of a query. You can read a little more about this here: https://en.wikipedia.org/wiki/Insert_(SQL) I think this is ANSI SQL, so it should work in MSSQL.
Question for you. It has come to my attention that some of the rooms don't really belong to anyone, thus I need to create a general faculty/general student user. This is gonna cause the room_id on the user table to not work. Can I fix this by adding two tables, a user_room table?
Looking at the data you provided you may want to do a rank first. Having said that pivot may not be your best option if there are a varying number of output columns. Assuming allocation_id always increments in order and there are always 9 columns to be output.... ;with cte as (SELECT * --you should type these out...I'm too lazy , RANK() OVER(PARTITION BY PROJECT_CODE ORDER BY ALLOCATION_ID) as rnk FROM PROJECTS ) SELECT PROJECT_CODE ,TASK_CODE ,[1] as lineseq1 ,[2] as lineseq2 ,[3] as lineseq3 ,[4] as lineseq4 ,[5] as lineseq5 ,[6] as lineseq6 ,[7] as lineseq7 ,[8] as lineseq8 ,[9] as lineseq9 FROM ( SELECT PROJECT_CODE ,Rnk ,lineseq ,task_code FROM Cte ) piv PIVOT ( MAX(lineseq) FOR rnk IN ([1],[2],[3],[4],[5],[6],[7],[8],[9]) ) Sorry for the formatting...using my phone EDIT: fixed column names
Your first statement is still a join. Anytime you select data from two or more tables you're joining. The syntax you used in the first statement is form the 1989 SQL standard and you shouldn't use it anymore, it's too ambiguous. Instead, since SQL '92, there is the explicit join syntax in the FROM clause instead of a condition in the WHERE clause. Since the new syntax is less ambiguous (think of it this way, if you didn't realize you were doing a join in your first example, it's going to be more difficult for anyone reading your SQL to see that you were doing a join) so you should familiarize yourself with it. Note that the syntax is JOIN table ON ... And not JOIN table WHERE ... the WHERE clause is a separate clause. select b.Title, B.Isbn from Book b inner join Publisher p on p.ID = b.Publisher_ID where p.Name like 'O''Reilly' ;
Your first example is an outdated syntax. Prior to the introduction of the JOIN keyword, joins were done as cartesian products of multiple tables (such as in your first example) using the WHERE clause to define join conditions. There are a number of advantages to the new syntax, despite its verbosity and it is certainly preferred at my organization as well as many others. You can find plenty of lists of reasons that the newer syntax is preferred by googling for it, but here's a place to start: http://www.orafaq.com/node/2618
Have you looked for online tutorials?
So you're trying to have Project 1 become 1 row with 9 columns for each lineseq, then Project 2 is one row with n numbers of columns, etc?
Thank you for the response. Yours was the easiest to understand. Can you explain what the join statement does that makes it so much better?
Corrected version of your second example SELECT b.Title , b.Isbn FROM Book as b INNER JOIN Publisher as p ON p.ID = b.Publisher_id WHERE p.Name = "O'Reilly" This is clear and well written. Anyone reading it knows what elements are required to connect the tables vs what elements are specific to the problem being researched. SELECT b.Title , b.Isbn FROM Book as b , Publisher as p WHERE p.ID = b.Publisher_id AND p.Name = "O'Reilly" is equally valid but is no where near as clear. It's not terrible when you are joining two tables, but when you start joining 10-15 tables it makes a HUGE difference., 
Which RDBMS are you using? SQL Server, MySQL, Oracle ... ? What you are looking for is called PIVOT, but it changes depending of the DB.
I haven't been using DBs for a very long, so I'm not exactly sure, but I think this is the answer to what your asking: I'm using [sql.js](https://github.com/kripken/sql.js). Edit: Actually, I think PIVOT game me the right google search that helped me find answer on stack-overflow. Thank You!
For future reference, you are using SQLlite. Check this link: http://stackoverflow.com/questions/1237068/pivot-in-sqlite , sadly SQLlite doesn't have a direct way to do a PIVOT so follow that link to reproduce the same result using temp table.
thanks, knowing "pivot" gave me the google search I needed and I just found that page.
I meant using JOIN the TSQL command
No, this has all been really helpful. That makes sense, thank you.
The % is used for a like (pattern match) operation. It won't work in a IN clause. Try it without the IN. (field1 like 'A%' or field 1 like 'B%')
I just did and I think I broke it. Btw, I need more than those conditions in my WHERE clause. when I do WHERE field1 LIKE 'A%' It's fast, but when I add OR field1 LIKE 'B%' then it doesn't go anywhere. EDIT oh wait I think I need parentheses.. EDIT 2: Yey it worked! Thanks a bunch! :)
It looks like you're looking for values in field1 that start with a particular value. You could try this: WHERE LEFT(field1,1) IN ('A','B','C')
Don't have a lot of information but check the uniqueness of the field you're joining on, not the uniqueness of all of the fields. Running select distinct a,b,c on a table with 5 million duplicate a columns, but completely unique b and c fields, you're going to bring back 5 million rows. And if you're joining on column a, you just got your output set multiplied by 5M rows.
Not as good. Since you're evaluating the start of the string, using LIKE will likely take advantage of indexes on the field (if the exist). Any time you put the field into a function, though, like LEFT(), you eliminate the possibility of using an index. I do think using RIGHT(field, 3) = 'foo' is better than LIKE '%foo', though.
wildly off: the between condition should be on the date_table, imo. I'm also not sure I understand the relationship of your period (and what is that period - 10/1/2015 to 9/30/2016?) with the P.BegSID and P.EndSID. 
Looks like you need a sub select on the main table. Using a where clause element on the left join table won't work. So you have Left join (select * from main_table where st_f=9 and endsid &lt;&gt;999999) p on t.time_date_dim_sid between p.begsid and p.endsid Where t.actual_date&gt;date'2015-10-01'
If the function is deterministic and the rules of determinism (or whatever it's called) are followed, can't indexes still be used?
I don't have an oracle instance handy, but try this: select t.actual_date ,count(p.primary_key) from main_table p join (select p.primary_key ,t1.actual_date as "begin_date" ,nvl(t2.actual_date, sysdate) as "end_date" from main_table p join time_table t1 on t1.time_date_dim_sid = p.begsid --assumes begin date is never null left join time_table t2 on t2.time_date_dim_sid = p.endsid) as "maintbl" on maintbl.primary_key = p.primary_key left join time_table t on maintbl.begin_date &lt;= t.actual_date and maintbl.end_date &gt;= t.actual_date where t.actual_date &gt;= '01-OCT-2015' group by t.actual_date order by t.actual_date
Keep in mind implicit joins do run just fine on many flavors of sql but they are not ansi compliant and some versions of sql will create a cartesian product of the two tables then filter the results. It's my recommendation that unless otherwise required by the rdbms, always join tables using JOIN explicitly.
You might be thinking of something like WHERE left('food', 3) = 'foo' There isn't any performance hit because it's a static string that's being put through the deterministic function so it only gets evaluated once. That doesn't have anything to do with indexing though, if you put an indexed field through a deterministic function (e.g. in a join), the result of that function is not indexed.
It is meant to be a schedule for sending customer certain stuff in a weekly base. Like company will send customer Smith a turkey every Mon, Tue, Fri.
That should still be 3 distinct records imho. 
++
I will second bitwise operations if you intend to keep things in one column. Bitwise does a great job of storing a lot of information with a small file size profile. I have deployed bitwise in a day-of-week scenario and it works wonderfully, though can be quite complex to understand and establish. Great tool to have in the toolbelt nonetheless
&gt; I need to restrict to October 1, 2015 and forward (FY16) for a daily summary count of records still pending. &gt; I know the date the record was submitted and date it was completed (these are the DIM SIDS values), so for any day in between those dates it is pending. This makes no sense. Why would a record that has not been completed (is still pending) have a date completed? Why the hell would you name the submitted date Main_table.BegSid? And when there's a perfectly good datetime datatype, why would you screw yourself over by creating a date table with SIDS for any given date? Okay, I'm taking your example and renaming some things for clarity. How many records, submitted in this Fiscal Year are still pending today? SELECT Date_table.Actual_date , COUNT(*) FROM Main_table INNER JOIN Date_Table ON Main_table.SubmittedDateSID = Date_Table.Time_Date_Dim_SID WHERE Main_table.ST_F = 9 -- Pending Flag? -- Order Submitted in this FY AND Date_table.Actual_date &gt; TO_DATE( '2015-10-01 00:00:00', 'YYYY-MM-DD HH24:MI:SS') AND Main_table.CompletedDateSID != 999999 -- Default value for not complete??? For each day in this Fiscal Year, how many orders were pending that day? SELECT Date_Table.Actual_date , ( SELECT COUNT(*) FROM Main_Table WHERE ( SELECT SubDate.Actual_date FROM Date_Table SubDate -- Submitted before the date being reported. WHERE SubDate.Time_Date_Dim_SID = Main_table.SubmittedDateSid) &lt; Date_Table.Actual_Date AND ( SELECT ComDate.Actual_date FROM Date_Table ComDate -- Completed On/After the date being reported. WHERE ComDate.Time_Date_Dim_SID = Main_table.SubmittedDateSid) &gt;= Date_Table.Actual_Date ) As PendingOnDate FROM Date_Table WHERE Date_table.Actual_date &gt; TO_DATE( '2015-10-01 00:00:00', 'YYYY-MM-DD HH24:MI:SS') That's just insanely complicated.
Seems like you know what you need to do (Google PIVOT syntax) -- what is your question? Now that that is out of the way, yes -- use the PIVOT function. Note that you will have to either use a defined (static) list of project numbers in the pivot function or use dynamic SQL whereby you produce your pivot by passing in the project list dynamically (in the FOR clause of the PIVOT). One alternative would be copying into Excel and then Transpose pasting within Excel, which is arguably easier for a sole user who sparingly accesses this info. Could probably write a VBA macro to do the transpose paste, but I digress. The best approach would be re-assessing the use of the data or re-setting expectations with users wanting data in this format. Either change the way data is entered into the database to suit your organization's use of data on the output or get users accustomed to seeing outputs in a way that doesn't require extensive work on your end. Post here again if you are having trouble with PIVOT syntax -- it's notoriously a pain in the butt and I never learned it by heart, always had to google syntax when I needed to use it.
Pivot is the way to go. How many distinct values of RN do you have? How often do the values of RN change?
This is the greatest explanation of all time and a significant contributor to me landing a job today.
Yes, and i complain about the bad design of those as well.... You notice I then tried to work within those constraints instead of brushing off the possibility of working with such a bad design?
Unpivot was what solved it. I updated the post with a link to the solution. I don't know how often they change, I was trying to finish a solution for another users question.
Here is how you do it dynamically. Was pretty simple actually once I took my head out of my ass: DECLARE @DynamicPivotQuery NVARCHAR(MAX) DECLARE @ColumnName NVARCHAR(MAX) SELECT @ColumnName = ISNULL(@ColumnName + ',','') + QUOTENAME([RN]) FROM ( SELECT DISTINCT ROW_NUMBER() OVER(PARTITION BY Project_Code ORDER BY LineSeq) AS 'RN' FROM [WorkSpc].[dbo].[BP_SampleData] ) A ORDER BY [RN] SET @DynamicPivotQuery = ' SELECT Project_Code , ' + @ColumnName + ' FROM ( SELECT Project_Code , LineSeq , ROW_NUMBER() OVER(PARTITION BY Project_Code ORDER BY LineSeq) AS RN FROM [WorkSpc].[dbo].[BP_SampleData] ) X PIVOT(SUM(Lineseq) FOR RN IN (' + @ColumnName + ')) AS Y ' EXEC sp_executesql @DynamicPivotQuery Now it won't matter how many open lineseq's there are, the query will auto-adjust the maximum number of columns around it.
My example will work with non-integers. I modified an old dynamic pivot I had which was using state abbreviations. I do need to learn more about FOR XML and STUFF and a few other weird commands.
Oh, yes that's correct, however if you look at the [sample data ](http://www.filedropper.com/sampledata) and original [post](https://www.reddit.com/r/SQL/comments/4vnz0p/looking_to_do_a_simple_pivot_without_aggregate/) you'll see that was the necessary method. You could probably add an affix like "Open Item" before the integer to make it look pretty.
 SELECT Project_Code , CAST(LineSeq AS float) AS LineSeq , ROW_NUMBER() OVER(PARTITION BY Project_Code ORDER BY LineSeq) AS RN FROM [WorkSpc].[dbo].[BP_SampleData] Does that work?
That isn't the best approach, and I can think of reasons you might have problems in the future if you have lineseq's that start with a 0. The solution might be to have a table that ranks the lineseqs or has an incremental key or whatever, then do the process and afterwards join to the table for each column and translate the key to the lineseq.
Actually that's a good point. It doesn't necessarily solve my problem, but I do need to include distinct in order to make my note true about rptdef_id and number_value being a unique combo. Some of them aren't unique, but I know why they aren't and don't care which one is used.
for long memos, yes. For names almost certainly not. You did not include what DBMS you are talking about, and that makes a big difference here. Here is the argument for MSSQL: nvarchar(MAX) is a so called LOB datatype, Large Object B... i guess Binary. Anyway, the important part is this, LOB types are not saved as part of the row. They are saved as a pointer to somewhere else. This means that reading LOB data will cause additional IO, since SQL Server will read the page the row is written too, AND the page the LOB data is written to. Might not seem important, but this will also affect the caching and your queries will be slightly slower as well. This is one of those things you should keep in mind when modeling your data. You are better served with using varchar(200) or whatever is appropriate for you. After all, your frontend won't be to happy being presented with a name spanning more than 4000 chars. On other database systems, your millage will vary, but to some extend they all behave in a similar fashion. 
Something like this might work for you: select Row, rptdef_id, number_value, numval_count = (select count(Row) from #results as r2 where r2.number_value = r1.number_value) into #tempresults from #results as r1 select * from #tempresults where numval_count &gt; 1
Okay this intrigues me, but there are things going on here that I've never done before. I think I can bang around and figure it out, but I have one follow up question. Where does my query fit into all of this? **EDIT** Never mind, I see where you're going. I'll give this a whirl. I may have more questions.
 SELECT f.* FROM ( SELECT frd.description, frdt.rptdef_id, ftr.number_value, ftr.result_table, count(ftr.number_value) over () nv_count FROM fas_test_result ftr JOIN fas_rptdef_to_test frdt on frdt.test_id = ftr.test_id JOIN fas_rpt_definition frd on frd.rptdef_id = frdt.rptdef_id WHERE 1 = 1 AND frd.year = 2015 AND upper(ftr.result_column) LIKE '%ROW%KEY%' ) f WHERE nv_count &gt; 1 ORDER BY ftr.number_value, frdt.rptdef_id
Being able to solve this problem depends entirely on how familiar you are with aggregate functions. If the answer is "very little" or "not at all" then you ought to start by going through the documentation and taking on some easy examples. [Oracle AGGREGATE Functions documentation](https://docs.oracle.com/database/121/SQLRF/functions003.htm#SQLRF20035) Basically what you need here is to find the duplicates. You can do that from a sub-query and then use the results to filter out the rows from the main table (fas_test_result in your case) by using an IN condition. The broad syntax for finding duplicates is SELECT COUNT(...)...FROM ... GROUP BY....HAVING COUNT() &gt; 1 
Nevermind, I re-read the question. I wasn't completely understanding the problem. How about UNPIVOT() assuming this is MSSQL? The same could be accomplished by loading the data into Excel and using powerquery.
I'm not a dba but if your attributes may grow, I've worked for several shops that made basically a key value attribute table so you can add new properties without schema changes.
I've never seen a count without a group by. That's weird. At this point I'm at home and on mobile, but thanks! 
Ha, I was just mentioning in another comment that I'd never seen a count without a group by. Then it's mentioned in the first couple paragraphs of your link. So I guess the answer would be "very little". Looks like I've got some light reading ahead of me. Thanks! 
It's an analytic count. http://www.orafaq.com/node/55
Generally, 'top 5' means 'the five **rows**' that meet some criteria after some kind of ordering is done, not 'a situation where five columns in each row' meet some criteria. Perhaps you're looking for the top five rows ordered by a column which contains the Sum of the numbers in the other columns, from largest to smallest?
I hope I'm understanding you correctly - you want one record from PE_DATA_08012016 for each combination of record number and exam date, and you don't care which record it is?. I assume you have a unique identifier/primary key (PK) in PE_DATA_08012016. So in your first query, take the count out of the select list (you don't need it) and put in the max or min of the PK: SELECT [medical record number], [ct exam date], MAX(PK) as PKToJoin INTO #DUPEEXAMDATES FROM PE_DATA_08012016 GROUP BY [medical record number], [ct exam date] HAVING COUNT(*)&gt;1 then just join the next query onto that min/maxed PK: SELECT B.* INTO #DUPESCANINFO FROM #DUPEEXAMDATES A INNER JOIN PE_DATA_08012016 B ON A.PKToJoin on b.PK btw there's probably a much more elegant and performance-friendly way of doing this in a single windowed query but if this is a once off job it's probably not worth the effort to optimise. e: also, did I just help defraud a medical insurance company out of CT scan payments?
It's unclear to me whether you want the top five column names or the top 5 values. Either way, I'd start with making a detailed view of the data by da11 and language like so: Select da11,'English' as lang,English as val from table Union all Select da11,'French',French from table Union all ... All the way down. You could use this as a CTE or create it as a separate view. Your final select would be something like Select da11,lang,val,row_number() over(partition by da11 order by val) LangRank You could then CTE or view this and do: Select * from cte2 where LangRank&lt;=5 order by da11,langrank If you wanted to build this back into a "one row per da11", you'd then have something like: Select da11,r1.lang lang1,r1.val val1,r2.lang lang2,r 2.val val2.... From (select * from cte2 where LangRank=1) r1 Left join (select * from cte2 where LangRank=2) r2 on r2.da11=r1.da11 Left join (select * from cte2 where LangRank=3) r3 on r3.da11=r1.da11 ... This is just from memory, but something like this should work assuming SQL server 
throw it in a pivot http://stackoverflow.com/questions/14694691/sql-server-pivot-table-with-multiple-column-aggregates
This would work given this is likely going to be a super small set of data. Writing queries against a name value pair generic data model against a decent volume of data is pure torture. 
Just use a simple create view command: CREATE VIEW ViewName AS &lt;your query here&gt; 
Great advice, live and learn :D
some kind of unpivot, maybe?
Recursion will help you there: DECLARE @String nvarchar(max) = 'sdfs ssd wwe sdfsdf fg' ;WITH CTE as ( SELECT CHARINDEX(' ',@String) as SpaceIndex UNION ALL SELECT CHARINDEX(' ',@String,SpaceIndex+1) FROM CTE WHERE CHARINDEX(' ',@String,SpaceIndex+1) &gt; 0 ) , CTE2 as ( SELECT * , ROW_NUMBER() OVER (ORDER BY SpaceIndex) AS RN FROM CTE ) SELECT SpaceIndex FROM CTE2 WHERE RN = 4
AWESOME, THAT WORKS, THANK U VERY MUCH
this does work as well, thank u!
thank u! all the solutions posted seem to work, thank u for ur help
the recursion one is awesome, very simple yet effective
Here we go: SELECT [LOCATION], ABS(Checksum(NewId())) % 9999999 as 'random' NewID() will generate an unique number. CHECKSUN() returns an integer hash value based on a value ABS() just to make sure those are positive numbers % 9999999 will limit the result from 1 to 9999999, 7 digits
I don't know LINQ, but a quick search showed me this: http://www.sqltolinq.com/ , maybe this can help you.
Nice approach, but there's really nothing guaranteeing that the number is unique.
I've never heard of Nvarchar(max) taking up more space. Nvarchar(max) like other blob datatype (image,xml,binary) are stored in blob storage table. You may find advantages and disadvantages to this because adding too many columns with long lengths like Nvarchar(2000) or Nvarchar(4000) can grow your record sizes and decrease the amount of rows per page causing more disk reads when doing table scans. I've seen where a client had so many columns of type Nvarchar(1000) each page contained exactly 1 row. Honestly, I find it very rare one could find a need for more than 1 or 2 blob types (different types if two) in a table and the rest of the database columns should be small or foreign keys.
HAHA! I could kiss you right now. I had no idea how to google the syntax for that... I was close, but still so far off. Thank you so very, very much 
Wow. That's clever. If it was a one off thing to do that would be a cool thing to use. If it was part of something that should live for many years and be maintained by others I'd be hesitant to use it. It's not entirely obvious from a quick glance that the cartesian product H,H A,H B,H C must be &gt;= than the i &lt; 10000000 terminating condition. If that i &lt; 10000000 were ever to change to something larger you'd need to look into the cartesian product as well. But still, I'm impressed. :)
Using suggestions from /u/reallyserious and /u/aplato I created this monster: DECLARE @Location TABLE ([Location] int, [random] int) ; INSERT INTO @Location ([Location], [random]) VALUES (1, 1234567), (2, 1234567), (3, 1234567) ; WITH H AS ( --3000 rows SELECT i=1 UNION ALL SELECT i+1 FROM H WHERE i&lt;3000 ) ,Numbers AS ( SELECT ROW_NUMBER() OVER (ORDER BY (SELECT NULL)) as row_num FROM H CROSS JOIN H A --3000*3000 rows ) , RandomNumbers AS ( SELECT row_num , 999999 + ROW_NUMBER() OVER (ORDER BY NEWID()) as random_num FROM Numbers ) UPDATE l SET l.random = r.random_num FROM @Location l INNER JOIN RandomNumbers r ON r.row_num = l.Location OPTION (MAXRECURSION 3000) SELECT * FROM @Location It worked for whole 5 minutes, because of generating 9000000 pseudorandom rows, but in the end it gives you a table, that you can join to your existing one and perform the update in a single transaction.
You mean LOB not BLOB 
 SELECT * INTO NewTableName FROM (WITH a AS ( SELECT type, SUM (column) b FROM table GROUP BY type ) SELECT column FROM a UNION ALL SELECT SUM(b) FROM a ) C 
I started to write a formula, then stumbled upon [this](http://pragmaticworks.com/Training/Resources/Cheat-Sheets/SSRS-Expression-Cheat-Sheet). First row: =DateAdd("d",-DatePart(DateInterval.WeekDay,Today,0,0)+1,Today) 
 =DateAdd(DateInterval.Day, 2-WeekDay(Today), DateAdd(DateInterval.Day, -2, Today)) --Last Saturday =DateAdd(DateInterval.Day, 2-WeekDay(Today), DateAdd(DateInterval.Day, -8, Today)) --Sunday 1 Week Ago =DateAdd(DateInterval.Day, 2-WeekDay(Today), DateAdd(DateInterval.Day, -15, Today)) --Sunday 2 Weeks Ago
Will this work no matter what day of the week its executed on?
Yes.
TY. I saw a few different solutions when Googling but wasn't quite sure if they would work dynamically like this so I thought I'd ask here. 
Thanks ! What exactly do these portions of what you edited do? WITH CTE AS ( SELECT * , ROW_NUMBER() OVER (PARTITION BY a.[NCPDP Number], a. [Product/Service ID], a. [Prescription/Service Reference Number], a.[Fill Number] ORDER BY a.[Date/Time Switched] DESC) AS RN FROM table a ) Then AND a.RN = 1
Instead of calculating MAX for each single group of rows, I used windowed function ROW_NUMBER() to calculate ranks of each such group; **a.RN = 1** condition in the end ensures that only highest ranks are returned.
Ahh that makes sense and will be way more efficient then what I had. Thank you for your response. I really appreciate it. I need to look into windowed functions more, you gave me something to learn more about :D! 
Okay, so if you're looking for optimization, don't go with the rownumber. That's going to tell the server to use an order which is very expensive. You're still going to want to use the max aggregate in that CTE and then join that back up. It's difficult to say for sure how to get it running quicker than that since I don't know the purpose and what's involved in each table.
Essentially it is finding the last time an item was purchased from table A based on a few columns. Then it matches those results to table B so I can calculate the different in pricing from the two. I will give both a try. Maybe the CTE itself will speed everything up and hopefully not bomb out my TempDB folder! I will run it both ways and let you know how it performs. 
Okay, I'm just telling you that you do not want to get into a habit of using row_numbers unless you absolutely have to.
Because you'd have to have a different level of privileges (I think). Not sure if they can just isolate that and give it to you. If you can't view the plan, I'd also experiment with switching out the CTE for a temp table. Overall it's extremely hard to optimize something without knowing like how many records you're bringing back for instance, but that should be a good start.
Okay, thanks for the tip! Is there a guide or anything that shows what functions are more taxing on the server then others, or at least how taxing they are?
The structure is POOR and the data volume is massive! When I came to this position I had to request indexes be made on tables that are 1+ million lines. Thanks for all your input. It is very much appreciated.
ofc but still the allocation units are for ROW DATA, ROW OVERFLOW DATA and LOB DATA. Varchar(max) is not really binary since it is ASCII text in contrast to image or binary data type ofc. It is however arguable since even ASCII represantion of course is binary in the very end. Still in CS you decide between binary and non binary data on a higher level :) I just wanted to point out correct terminology of SQL Server.
Position of columns in a table makes no difference.
Thank you. I didnt clarify so i edited my original post, but how do I write the query for this new column? I am not sure how to write it.
You can use an `IF` for this such as: IF getdate() =&gt; date_conversion_here REPLACE my_table; ELSE REPLACE my_table;
this doesnt work.
Well if you are interested read further: Data Pages are "maintained" in an IAM PAge. For each allocation unit type there is one IAM Page (chain). So for normal ROW DATA you have one IAM Page maintaining all the addresses of the data pages storing ordinary row data. Then you have one IAM PAge maintaining all the addresses of the data pages storing LOB data such as image, text varchar(max) and so on. Then you have one IAM Page maintaining all the addresses of data pages storing ROW OVERFLOW Data. ROW OVERFLOW has nothing to do with page splits. A page split occurs when you want to insert a row to a page and that page is already full i.e. it cannot take your row anymore BUT it has to go there since index key defines that particular position. Then the page gets split that means 50% of the rows stay there and 50% get copied on another data page (still talking about ROW DATA - data pages). That is a page split. ROW OVERFLOW data however is different. A data page is always 8KB in size. Imagine a row that has 50 VARCHAR(200) columns but the data in that row for each column is only 1 character so we have 50 byte row data length. This can be easily stored. Now imagine you update all 50 columns with 200 character strings. Then you have 50 x 200 &gt; 8 KB and you cannot store that row anymore in one page. Now that part that makes the row "overflow" of that page gets cut and put on another page. In you original page there will be a pointer to the ROW OVERFLOW data page where you can find data that got moved since it flew over a normal DATA PAGE. You my find this interesting. :) 
An alter table statement can be used to add the new column. If the new value is always required, regardless of where the insert takes place from, then you could use a persisted computed column: https://voiceofthedba.wordpress.com/2011/08/23/computed-columns-and-case/ That would automatically populate your new column without changing any stored procedures/insert logic - it's a change to the definition of the table
Is this a one to many issue? Sounds like you have many orders to one receipt. Which is causing duplication when joining the tables. Can we see your query and an example of what you're hoping to get?
 Select Internal ID, Order #, email, Order Subtotal, Flavour = Case when (Order Subtotal &gt;= 199 and order Subtotal &lt; 299) 'Chocolate' Case when (Order Subtotal &gt;= 299 and order Subtotal &lt; 399) 'Vanilla' Case when (Order Subtotal &gt;= 399) 'Butter' where .......... I believe this, or similar, is what you are looking for. Sadly I don't have the Time to test it. But you can do and give feedback :) Edit: My SQL Statement didn't work as I thought. Looked into it and Case WHEN seems to be better. Edit2: I guess you could also have it like my first attempt; Instead of 'chocolate' you coult run another select.
Then make the WITH part as another View and then your query won't need it Like: CREATE VIEW A AS SELECT type, SUM (column) b FROM table GROUP BY type CREATE VIEW B AS SELECT column FROM a UNION ALL SELECT SUM(b) FROM a 
I don't know why that approach wouldn't work, but you could do a second pass at this. select * from (SELECT * FROM table_x WHERE column_b = 'Z') where column_a = 'X'; The downside of this approach would be if your result set of the inside query is very large. It will require more time than making two comparisons in one pass.
Since you haven't specified any data that might help one to write query, I can only advise you to use GROUP BY or DISTINCT for your orders in a subquery, or use ROW_NUMBER() to get only first rows for each order.
You are using the AND operator in your 3rd query, that doesn't result in the combined result of the previous 2, it result in the intersection of them. If you use OR then it will result in the combination.
SQL Server process the query in the following order: FROM -&gt; WHERE -&gt; GROUP BY -&gt; HAVING -&gt; SELECT -&gt; ORDER BY When you build the CASE in the ORDER BY it will recalculate the information and may cause undesirable results, when you use the alias you added in the SELECT the ORDER BY works properly because the CASE was already calculated.
If you're getting some other result than this, you're sql on the last example has to have a typo table_x_ID column_a column_b 1 X Y 2 Z X 3 X Z 4 A T 5 X Z 6 N Z SELECT * FROM table_x WHERE column_a = 'X' 1 X Y 3 X Z 5 X Z SELECT * FROM table_x WHERE column_B = 'Z' 3 X Z 5 X Z 6 N Z SELECT * FROM table_x WHERE column_a = 'X' AND column_b = 'Z' 3 X Z 5 X Z 
For data conversions, this is exactly what I did rather than crosswalking values for some other third party to work with. Case statement works great.
&gt; ...I'm not getting the row(s) back I'm expecting, even though those rows BOTH have X in column_a, and Z in column_b. Did you miss this part of the OP or did the OP stealth edit?
See, CURDATE gets a not recognized function error?
What DBMS? Pseudocode: Is it the last week of the month? IF Month( Current_date) != Month ( Current_date + 7 days) then True ELSE False END IF Is the month divisible by 3? IF modulus( Month( Current_date), 3) = 0 then True ELSE False END IF If both true, quarter end data set, else other data set
Wow, tons here, thank you! w3 looks like a great guide, think I'll print off and post the references on my wall! The denominator actually could be an issue so I will have to handle that so it its excluded or ignored I suppose. This might be a bit of a side project for me unfortunately.. haha. 
Use the OR operator. Unless all rows with X in columnA also have Z as a value in columnB you won't get the results you're looking for. by using ANd, you're searching for rows where columnA is X and columnB *in the same row* is Z.
So 's' and 't' are completely arbitrary here, right? I don't quite understand what the SELECT t.* FROM dbo.CustomPollerStatus AS t Does there. Can you explain that a bit?
Since it is a class, I won't give direct answer, so ... I would search about how RIGHT JOIN and LEFT JOIN works for that one.
The 't' is just an alias for the CustomPollerStatus table. It is used just to make the query easier to write, your example could be written as: SELECT dbo.CustomPollerStatus.* FROM dbo.CustomPollerStatus But that alias can be anything, the following is also valid: SELECT YOLOsicNameBRAH.* FROM dbo.CustomPollerStatus AS YOLOsicNameBRAH
From an analysis standpoint, it might be better to NULL out the denominator in case of zero, so it's clear that we have no data, as opposed to 0% order completion: numerator/NULLIF(denominator,0) Re: the date issue, you can use T-SQL's DATEDIFF(inc,start,end) function to look only for dates that are today, or 1-day ago. Incorporating this in with LTD's great suggestions above into code, we might end up with something like this: SELECT HEAD.DATE_DELIV, DETA.QTY_ORDER, DETA.QTY_DELIV, (isnull(DETA.QTY_DELIV,0) * 1.0)/nullif(DETA.QTY_ORDER,0) --auto-cast numerator to decimal FROM SQLRMLIVE.dbo.INVODETA DETA INNER JOIN SQLRMLIVE.dbo.INVOHEAD HEAD ON DETA.NUMBER = HEAD.NUMBER AND DETA.TRANSAC = HEAD.TRANSAC WHERE datediff(day,HEAD.DATE_DELIV,getdate()) &lt;= 1 --today or yesterday EDIT: you could have the datediff in the WHERE clause restrict to values IN (0,1) to protect against weird issues with future dates in the header table: WHERE datediff(day,HEAD.DATE_DELIV,getdate()) in (0,1) --today or yesterday 
Technically it is a valid statement, and will eliminate class statuses that happen to have the same ids as students. But it isn't a valid answer to any kind of question, just a list of numbers that may or may not overlap...
Nope, not unless the class status is the exact same data type as the student ID And even if it *is* the same data type the results would have absolutely no meaning because it would list all your student IDs down the page, then list all your student class statuses down the page. I would suspect that you're looking for a JOIN, not a union SELECT ss.studentid FROM dbo.student_schedules as ss INNER JOIN sc.ClassStatus on ss.studentId = sc.studentid And some other qualifiers, like what students class status is some value... ?
As narayanis said, Union will remove duplicates, Union All retains them.
To clarify, if historic then order by data order, if actual then order by data type, if forecast then order by leads.
 SELECT cps.* FROM CustomPollerStatus cps, CustomPollerAssignment cpa WHERE cps.CustomPollerAssignmentID = cpa.CustomPollerAssignmentID AND cpa.AssignmentName like 'DPS_216_%' How does this do for you?
Yeah, this is like unioning people's names and their addresses. You'd never want that data in a single column.
I'm terribly not fond of the comma usage in the FROM clause of that statement. Mostly because it doesn't play nice if I have to modify the query in the future to use OUTER JOINs. At that point I'll need to do much more work to get the code working again.
Please don't do other people 's homework. 
First question. .. Why? If your query works then rewriting the same logic in a different form isn't going to change your result. Next question... What is the relationship between those 2 tables? If it is 1..n then many answers here are going to give you a bum result.
If you use CASE like this it can't hurt to include ELSE 0 Any zeros in the result are then obvious errors. But the real answer is where is your DataType table? Don't code data into SQL. 
Row number is safer if multiple identical dates may be present
Not always true. SQLS will try to fit the value in the row if it can. There are also lots of options. https://technet.microsoft.com/en-us/library/ms189087(v=SQL.105).aspx
First query should determine the campaign for the visitor id Next query should count visits and payments by visitor id Then final query contains the first 2 queries. You don't need Excel to do that. 
Alright, how about an example?
 INNER JOIN RIGHT JOIN LEFT JOIN They're almost the same width, so the join conditions line up nicely. If you type out LEFT OUTER JOIN the join conditions are all over the place.
Stick to SSIS. It can be a pain comparing versions in a multi developer environment, but the benefits outweigh the drawbacks IMO (debugging, logging and visualising data flows compared to huge stored procedures). Additionally if you are consuming multiple data sources the TSQL route can be a real pain. 
I don't understand how to do this? Everything is set up in Excel and works fine but the tracking is broken and it won't show up until they fix it. 
What a terrible site. so much ads and hardly anything new on the subject. 
thanks everyone for the responses. sounds like SSIS is the way to go.i appreciate the responses. 
Yeah what you have is how you would want to write it. I've seen more complex queries benefit from something similar before but that was because there were some table scans happening which were killing the run time.
They (a very generic 'they' in this case) should make 'pivot' and 'unpivot' to be basic operations they teach/explain for SQL courses/classes. These actually are quite common now that a lot of folks use entity/attribute/value-like storage. Anywho, first, you have values/attributes in rows - cs-uri-stem works as an attribute name, campaign will work as the 'value' for the campaign attribute, makepayment.asp is going to be a binary flag based on presence test. So, make a pivot (a 'combined' record for a visitorID) that will become your 'base' result set, then get the report for the Campaign attribute (no need for distinct since our base is at 'VisitorID' granularity): with VisitorRecord( VisitorID, Campaign, Payment_Ind) as ( select VisitorID, Campaign = max( Campaign), Payment_Ind = max( case when cs-uri-stem = '/makepayment.asp' then 1 end) from [table] group by Visitor ID ) select Campaign, Visits = count(*), Payments = count(Payment_Ind) from VisitorRecord group by Campaign 
That's some Advanced SQL shit, right there. I wonder if Kim Delaney knows.
If you wanted to go outside the box, SAP makes an ETL tool called Data Services. It's quite a nice tool, and is much more visually appealing then SSIS, in my opinion. Not without its quirks, but it gets the job done and it can connect to a wide variety of platforms that SSIS may have a harder time with. Just throwing that out there. 
Interesting.. Will check it out. Thanks for the heads up. 
Using TFS for source/version control helps quite a bit
it's for my own reference not homework
I got the code to work but modified it a bit to only look at the campaigns I want to: with VisitorRecord( VisitorID, Campaign, Payment_Ind) as ( select VisitorID, Campaign = max(Campaign), Payment_Ind = max( case when [cs-uri-stem] like '%eservices%pay%confirm%' then 1 end) from [OPSDB].dbo.Weblogs_Parsed group by VisitorID ) select Campaign, Visits = count(*), Payments = count(Payment_Ind) from VisitorRecord a INNER JOIN [Digital].[Params].[eServiceEmails] B on A.Campaign = B.CampaignCode group by Campaign And it looks good! But I tried another method I read about and it gives me slightly different results. Do you know why? Which one is right? Other method: SELECT A.VisitorID, B.CampaignCode INTO #VID FROM [OPSDB].dbo.Weblogs_Parsed A INNER JOIN [Digital].[Params].[eServiceEmails] B on A.Campaign = B.CampaignCode WHERE A.VisitorID IS NOT NULL SELECT DISTINCT VisitorID, CampaignCode INTO #DVID FROM #VID SELECT DISTINCT A.VisitorID INTO #PAYMENTS FROM [OPSDB].dbo.Weblogs_Parsed A INNER JOIN #DVID B on B.VisitorID = A.VisitorID WHERE A.[cs-uri-stem] IN ( SELECT DISTINCT [cs-uri-stem] FROM [OPSDB].dbo.Weblogs_Parsed WHERE [cs-uri-stem] LIKE '%eservices%pay%confirm%') SELECT CampaignCode , COUNT(DISTINCT A.VisitorID) AS TotalVisits , COUNT(DISTINCT B.VisitorID) AS TotalPayments FROM #DVID A LEFT JOIN #PAYMENTS B ON B.VisitorID = A.VisitorID GROUP BY CampaignCode order by campaigncode
All this does is duplicate the data from CLUB_FEE. I need to get the average from the whole CLUB_FEE column and put the result in the next column.