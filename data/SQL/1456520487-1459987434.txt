/u/rbardy's solution is [probably the best way to do this](https://www.reddit.com/r/SQL/comments/47ldnd/ms_sql_finding_the_median_value_in_ms_access/d0ehn2h) and pretty much the way I do it myself.
Nope :( gave it the position to go to instead of the length. Tried to make it too complicated anyway by stripping off the extension. select reverse(substring(reverse('/path/to/file.txt'), charindex('.', reverse('/path/to/file.txt')) + 1, charindex('/', reverse('/path/to/file.txt')) - charindex('.', reverse('/path/to/file.txt')) - 1))
So ... you make new tables every time a new type of dog is added? Use the code I posted, just copy and paste and it will work.
In SQL Server it is faster to put the filtering in the JOIN and have no WHERE clause at all. The reason seems to be something along the line of the filter is applied to the small table [States] then that is JOINED to [Address] giving a filtered subset. Then that is used to JOIN to [SalesOrder]. I have sped up a lot of queries using this technique.
these are honestly just awful this is a bunch of random trivia that has little or nothing to do with actual field-relevant work
"16. What data type is used to store graphics and images." Anyone else cringe at this question? The answer is NEVER and your data model is fucked if you are doing this.
That's why I was careful to say I don't check for syntax. When you add the fact that c# has different order of parameters and behaviors. I'm not counting commas. It's just to demonstrate that you know string manipulation. 
You're making it more complicated than it needs to be: SELECT RIGHT(@FilePath,CHARINDEX('\', REVERSE(@FilePath)) - 1) Edit: I see -- you're taking off the extension as well. That's not how I understood the problem.
Assuming that there's a table which holds product information, I'd select the products from there and do a Top 1 cross/outer apply or top 1 correlated subquery to pull the sales data.
I'm sorry man but that's definitely not a hard and fast rule. It comes down to how the optimizer decides to rewrite your query. Technically, they both filter the same way.
Can you give me an example, please?
I feel like I shouldn't have to say this in 2016 but don't correlate your sub-queries unless your put in a situation of either you must aggregating greatly over a massive data set without a true need or use a correlated sub query. Correlated sub queries are the crutch of bad SQL programmers. They aren't set based and cause recursion and most of the time SQL rewrites your query to prevent them. Did I say that meanly enough? :D In your first example SELECT ProductID, descr, PricePerLB, EffectiveDate FROM #inventory AS i1 WHERE i1.EffectiveDate = (SELECT Max(i2.EffectiveDate) FROM #inventory AS i2 WHERE i1.ProductID = i2.ProductID) Most likely the MSSQL or Oracle optimizer isn't going to correlate your subquery and will hash a list of Product and MAX over EffectiveDate and then INNER join that to your initial dataset. **If it doesn't, what will happen is your query will go through every row in #inventory and then it will execute the correlated sub-query for each row, even it it's obvious that the effectivedate is not the greatest.** If you are running it against a table with 478 million rows, the code will not work. Correlated queries slow down exponentially and code you write as a consultant today may not run 2 years from now as tables grow. So SQL sees your code and is essentially rewriting it to better code. To something like code is below. SELECT ProductID, descr, PricePerLB, EffectiveDate FROM #inventory i1 INNER JOIN (SELECT ProductID, Max(EffectiveDate) as MAX_EffectiveDate FROM #inventory) i2 ON i1.EffectiveDate = i2.EffectiveDate AND i1.ProductID = i2.ProductID The sub-query (I prefer to call them sub-select) that is being inner join executes first and gives us a productID and it's maxed effectivedate. We then inner join our small subset of data back to the main table. This is a query that executes 2 reads, not a read per row in the table. If the SQL optimizer is smart enough to perform the hash for you, get in the habit of writing code the SQL optimizer does not have to rewrite as there will be times where the SQL optimizer may think that the correlated query may be faster with parallelism and it blows up in the optimizers face. The optimizer isn't smarter than you, it does what it's told to do and it will drive your query off of a cliff.
Sure. On mobile so formatting will be bad. This is assuming there is a table called product with each product and sales for each time a product is sold. SELECT p.ProductID ,p.ProductDesc ,(SELECT TOP 1 SaleDate From Sales s Where s.ProductID = p.ProductID Order By SaleDate DESC) FROM Product p SELECT p.ProductID ,p.ProductDesc ,s.SaleDate ,s.OrderNumber From Product p CROSS APPLY (SELECT TOP 1 SaleDate ,OrderNumber From Sales s WHERE p.ProductID = s.ProductID Order By SaleDate DESC) s To note, you can think of a cross APPLY as an inner join, whereas an Outer APPLY is like a left join.
Single table inheritance is where an entire type hierarchy is represented by one table. Common attributes that would be in a base class are the only ones that can be NOT NULL, all other attributes may or may not apply to each entry so they must be nullable. Think of a bank customer, there is some common information that all customers will have, but while one have only a savings account, another might have a savings account, two checking accounts and four investment accounts. In single table inheritance, both customers would be fully represented by records in the same table, but the first one would have null values in the checking and investment account fields. Also, of you only allow four investment accounts and a customer wants to open a fifth, you'd have to add another nullable field to the table for all customers. It's about the worst possible design you could have.
Make sure your groups don't have the page break before/after set. 
Table Inheritance lets you point a foreign key at things of slightly different type. It lets you avoid "xor constraints". For example, on a Sales Order Line Item, you might want a foreign key to a `goods` (physical products) table. But say now you want to sell services too. So you have an abstract type `products` and two concrete types `goods` and `services`. You could make a table `products` with just the primary key (which you point your order item foreign key at), and two tables `products_goods` and `products_services` with the same primary key, and a foreign key to `products`. This is class-table inheritance (CTI). Single table inheritance usually requires you to use nulls, which some people don't like, but is faster and simpler than CTI as CTI requires several left joins. 
Is it erroring out when you enter a specific visibility condition? If so, what's the condition? If you don't already know, you can use the Count property of a parameter to check whether it has any selected values. So your visibility condition could be: Iif(Parameters!A.Count = 0,False,True)
it errors out on any condition. that is interesting, i will try that on monday and see what happens
For that matter, `SELECT` on the data in production before you run the `UPDATE` or `DELETE`, wrap the actual change in a transaction, and check the number of updated records before committing to make sure that it's what you're expecting.
 FOREIGN KEY (b, c) REFERENCES B ( name, c )
Happy Reddit birthday. B can't be identified with just its own name. It's a weak entity that's dependent on C, so its name is not a primary key it needs C to identify it. It also needs to exist independently of A.
Same, I just had to use Google to look up the 'translate' function, and even now I'm not sure. I've never had to use it in the last eight years of using SQL daily at work, so unless I find it in someone else's code I'm modifying I doubt it's going to come up.
That depends. I have use it with smashing success for my aggregated results table which is populated with ~1300 lines of code and blends data across multiple sources (schemas, hierarchies, etc.) that can contain millions and millions of rows. I have a daily SP which refreshes the data that takes about 30 minutes to run... but what it does for me is create a repository that's only about 500K rows / 12 columns with indexes on almost every field. I can connect that table to any of the raw production tables to answer nuanced questions in a very short amount of time, and create SSRS/SSIS deliverables that execute in seconds as opposed to 10-20 minutes if they had to function in the production environment. All that at the expense of less than 5GB. Also creates a perfect environment for new analysts to use where they don't need to learn all of the nuanced rules for pulling data from source X and joining it to data from source Y. The data is clean and simple. 
Your sample data set does not make sense, but if it's trivial then all You need to do is something like this: select a.description , b.answer_id , b._impact from a inner join b on b.id = a.id
 select a.description , b.answer_id , b._impact from a left join b on b.id = a.id This should give you what you want.
Following your example I get the rows that are filled in returned, but it doesn't display the null rows. I know the rows are technically empty, and I thank you for your continued help, but do you know why the null rows aren't being returned? 
A left join should give you null data. I think you're either confusing your requirement, or confusing the table. Can you give me some of the data to look at?
What would be the best way to put the data up?
Need to make the file public so I can access it.
Done, sorry
OK, and you don't care about the other 6-10 IDs? Also... you want it so if you pick answer_id = 1 it gives you the corresponding ID's, but what happens if answer = 1 and corresponds to an ID that you don't have a description for?
This should work: select a.description, b.impact from b left join a on a.id = b.id where answer_id = 1
In the first example you might have been able to do a `full outer join` in order to get the desired results.
&gt; Is it possible to achieve this with the current table? &gt; Something like &gt; SELECT Order # FROM Sheet1 &gt; WHERE 2/16/16 is between Fab Start and Fab end yes, just like that, assuming you fix up the syntax errors for instance, dates have to be given as quoted strings '2/16/2016', and column names containing special characters or spaces have to be delimited with brackets [Order #], [Fab Start]
Oh my god, thank you so much. That worked perfectly
This is for a project that i'm working on, so not for school. Would you mind explaining where the b.description comes from? I'm assuming its the result of the inner query, but honestly I don't know.
 left join ( select a.description, b.impact from a inner join b on b.id = a.id where answer_id = 1 ) b So here you are saying this: 1. `LEFT JOIN` to everything starting here `(` 2. `SELECT` the descriptions and impacts `WHERE answer_id = 1` 3. Closing the section with a `)` 4. Naming the section `b` and then saying `b.description` (which you just pulled for everything `where answer_id=1`) is now `= a.description`. Above you have the line `from a` but it's really like this `from table1 a` or `from table 1 AS a` but you can just as easily do it like this: select x.description, y.impact from a x left join ( select a.description, b.impact from a inner join b on b.id = a.id where answer_id = 1 ) y on y.description = x.description
Thank you! It worked. I removed the other CASE completely. So I am guessing just like math, the statements inside the parentheses are computed first? Which is why the original query above wouldn't work? 
You're grouping by the wrong column. In your SELECT statement you're using the table dbo.DividendsPaid, however in your GROUP BY, you're using the dbo.PurchaseDetails. You can alias your tables, which is especially useful in join statements. Example syntax below. SELECT dip.AssetID , SUM(dip.DividendPaid * pur.UnitsPurchased) AS TotalDividends FROM dbo.DividendsPaid dip INNER JOIN dbo.PurchaseDetails pur ON dip.AssetID = pur.AssetID GROUP BY pur.AssetID ORDER BY TotalDividends DESC; I'm not near a test environment right now, but that should work. 
It has to do with whether you include missing spots on either side of a join. To understand this, you need to think about a join where there are gaps on either side. So, let's talk about an apartment building's table describing `pets`, and a table describing `tenants` as the owners of pets. Pets have a `null`able field pointing at their tenant; this means that pets can only have one tenant (owner) in this model. (Yes, that's wrong. It's good enough for this tutorial.) So let's say we have a `foo join` of `tenant.id on pets.tenantid`. That is, roughly, going to give us the pets matched up with their tenants. Here's the thing: some of the tenants won't have pets, and some of the pets won't be owned by a tenant (like the fish in the front office, or the parking lot guard dog.) So the kind of join is going to tell you how you get this data back. Do you want the answer to include tenants that have no pets? Do you want it to include pets that have no owners? (This is why people sometimes try to explain this with a Venn Diagram, which is ... just an awful, awful choice. Really.) The names are for set union logic. An `inner join` is just the things that have matches on both sides. It's the inner union. An `outer join` is all rows, whether they have a match or not. It's the outer union. A `left` or a `right join` are effectively the same thing - things that have no match on one side, but not the other. The difference between `left` and `right` is just which side is included, and you could get around only having one or the other by just writing tables in the alternate order. So. You want pet owners to pay for carpet insurance? `inner join` will exclude owners with no pets, and pets with no owners. You want to make a list of all animals for the animal control department, with a list of their owners, but including pets with no owner? `left` join if the pets' table is written first; `right` if it's second. This will leave humans with no pets off the list, which is appropriate. You want to make a list of all living creatures in the building, cross referenced by pet ownership where appropriate? `outer join` will include people with no pets, and pets with no people.
First of all, in any real world situation, do not use 'text' as your primary keys - that's bad for many reasons. Secondly, if you're implying that there's a many to many relationship between A and B, it's irrelevant that there's A-&gt;C relationship. The domain (C) could be the same but actual values are different (i.e. in your given f(A)-&gt;C, Choose(A,B,C-sub)-&gt;true, f(A)=C does not need to be equal to C-sub). If you're implying that in your relationship "Choose" A implies C, then "choose" is superfluous to begin with, simply replace C by A in the table B.
Thank you for this explanation, really helped to visualize the data and makes more sense.
Yay! That one came from [mine](https://imgur.com/2mlaF1M)! Edit: some more explanation for OP: Imagine the table on the left is about Books, with the corresponding Authors and editorial information, and the table on the right is about Writers (authors), with their corresponding postal address and phone numbers. Each color in the diagram represents a different author. You can see how INNER JOIN will show you the complete information only for those rows where every table has info, but you probably want the LEFT JOIN which will list all the books and fill in the author information whenever available. The RIGHT JOIN would list all authors and fill in the book information where available. FULL OUTER JOIN provides all the available info, leaving many blank spaces (NULLs). And Cross join (or listing tables with commas in your queries) makes a big mess by duplicating records and mixing things that should not be mixed... but you can turn it into an INNER JOIN by adding a "WHERE B.AUTHOR = W.NAME" clause, which would filter out the "wrong" records.
[edit] even simpler. Add b to A, have two FKs to C and B.
Nice job!
Ftfy: SUM(dip.dividendpaid*pur.unitspurchased) (sorry, on mobile) 
&gt;generally for performance JOINS are done on the seam which is the A.FIELD = B.FIELD BUT you can perform JOINS on columns that are not named the same so you could see A.FIELD = B.AFIELD I don't understand what you're trying to say here. In the vast majority of cases when using a relational database, you will join on specific fields because that's what defines the relationship between the tables (without relationships, a relational database is just a pile of data, like the junk drawer in your kitchen). Field *names* don't mean anything when performing joins, unless you're relying upon natural joins which, IMHO, is a *terrible* idea - always explicitly state your joins. What *is* important is having a foreign key relationship between tables and joining with those fields so that you don't end up with more data than you expect, with a chunk of it being either wrong. Performance gains when doing joins come from properly indexing all fields involved in the join. I've never heard of anyone referring to a "seam" with tables or joins.
Yeah, that's probably what I should do. Thanks!
This is a very small and quick tip, but it's helpful. I use it everywhere I go just as an extra precaution to prevent a mistake by running a statement in the wrong server. I usually do green from development, yellow for staging, and red for production.
Great! Yes, I assumed I would need to do that too. Thanks.
Brilliant, thank you!
Why are we joining on ID = ID when the OP said &gt; So essentially I need the descriptions in A to be substituted in for the score_attribute_id in table B Wouldn't SELECT A.Description, B.Impact FROM DescTable A LEFT JOIN DetailTable B ON ( A.ID = B.Score_attribute_ID) WHERE b.Answer = 2 Do the job?
 USE FINANCES GO SELECT dbo.DividendsPaid.AssetID, (dbo.DividendsPaid.DividendPaid * dbo.PurchaseDetails.UnitsPurchased) AS TotalDividends FROM dbo.DividendsPaid JOIN dbo.PurchaseDetails ON dbo.DividendsPaid.AssetID = dbo.PurchaseDetails.AssetID; USE FINANCES GO SELECT Div.AssetID, -- Always select the same table and column you're going to group on SUM( (Div.DividendPaid * Pur.UnitsPurchased)) AS TotalDividends -- need a sum there are mutltiple rows being grouped FROM dbo.DividendsPaid Div JOIN dbo.PurchaseDetails pur ON Div.AssetID = Pur.AssetID GROUP BY Div.AssetID -- matches selected table/column ORDER BY TotalDividends DESC;
Awesome this is very helpful, thanks!
This isn't a computer language, but you should be really take some time to learn how to explain data to people who don't care about what kind of code went into getting the underlying data from a database. I am a BI developer, and I can code all day long. When it comes to painting an effective picture with words/visuals to explain the data, I have found that to be much more challenging. People that work within our realm will understand it all easily, but sales people/managers/execs often need you to speak the data on their level.
&gt; but understanding how queries are written, executed, and optimized will likely come in VERY useful to you at some point. **Not** understanding these things will make your users, sysadmins &amp; DBAs **very** upset with you when you release a slow-as-hell application update that consumes vast amounts of CPU &amp; memory resources because your queries suck.
Yeah I'd be careful with that one - it works fine when you always connect from the registered servers interface, but if you happen to change the connection afterwards the color will NOT update. I believe I raised it as a bug years back (when I still thought MS would fix bugs with their dev/management tools), but I could be wrong and I may have already given up by the time I discovered that one.
A possible use case would be to create a windowed function for something like linear regression, similar to what [Oracle](http://docs.oracle.com/cd/B10501_01/server.920/a96540/functions101a.htm#85925) has. Or just as the computational engine behind something like [this](http://nbviewer.jupyter.org/github/agconti/kaggle-titanic/blob/master/Titanic.ipynb). I don't know R, I personally use python - which has been a huge productivity booster for me. But I'm considering learning R to be able to do computations closer to the data when 2016 becomes available. Having to do those computations outside of the database engine adds quite a lot of overhead, and you often can't harness the same processing power locally as you'd have available on your BI servers. MS also recently acquired [Revolution Analytics](http://www.revolutionanalytics.com/) and has added multi-core support to the Revolution R OPEN environment as well, which is another compelling reason for me. On the down side, I've found that MS products are horribly under supported (both on the MSDN forums and with connect bugs) when compared to other offerings - and as such I'm very skeptical. We're going to have 2016 deployed anyways, so I'll try it when I have it available to me, and make a decision then. Overall though, if you're doing any sort of analytical work then you could probably benefit greatly from learning R (or python!), and it's quite easy to learn it a bit at a time and leverage what you know as you go along. What you do on a regular basis is going to vary a lot from what I (or someone else) does regularly, obviously - but since I personally started learning/using python I now use it in conjunction with TSQL for almost everything I do.
Yep.. you are being hamstrung by the jet engine implementation of count. for anyone that wants more information, [this](http://stackoverflow.com/questions/17994380/count-distinct-in-a-group-by-aggregate-function-in-access-2007-sql) seems like a good article.
Excellent, thank you! Select from a Select. I Queried the the query to get my summary data and that worked. The nested SELECT reduces the number of queries I need for my report.
What is in '**y**', because that sounds like the conditions/etc of the operation lend themselves to being performed in a stored procedure or in code (For example, the user changing the value of updatestatus would start a stored procedure that does the update and the rest of the process required). Not as a trigger. 
okay, you may be missing something important here... using COUNT without a GROUP BY always returns just one row, because the whole table (subject to filtering by the WHERE clause) is considered a single group then adding DISTINCT in front of it is kind of redundant, because there are no duplicate counts to resolve, since there's only one so your first query returns the count of all rows that satisfy the WHERE clause but your second query uses DISTINCT too, and if it returns a different number of rows, one of the reasons could be that you have multiple rows with the same Rec_ID
UPDATE XYZ SET X = (CASE WHEN X LIKE '%ABCDEFG%' THEN 'DDD')...etc. Use a where clause to contain what you're looking for to prevent it from scanning every record. 
What version of SQL? Whichever you're working with will look something like this: SELECT CASE WHEN [Column] like '%stringx%' THEN 'A' WHEN [Column] like '%stringy%' THEN 'B' ELSE [Column] END AS 'ColumnName' , * FROM Table WHERE [Column] like '%string%' Now if you're asking how to take that data and then update the table, that's slightly different than it would be if you simply want the data, or if you simply want to put the data somewhere new.
Do you want to replace the entire text with A if it has X in it, or just the part of the text that matches A? (same question for Y,B)? So If string contains 'Apple' replace with 'Banana' So 'I love Apples' becomes 'I love Bananas' ? Or the whole thing ('I love Apples' becomes 'Banana')
This is almost there, but don't forget a trigger should employ set based logic. That is why INSERTED and DELETED are pseudo tables. 
This would replace his text with a fixed string. He needs to replace the occurences of one substring with another substring. Not replace the whole text. So where you have 'ddd' it should be something like replace('ABCDEFG','DDD',X)
SoundEx is pretty useful for discovering possible typos from data entry when dealing with specific words. [SoundEX has issues with non-english languages](http://stackoverflow.com/questions/299949/sql-servers-soundex-function-on-non-latin-character-sets) so its application is somewhat limited/restricted in real world use.
Works right up until there are 2 identical dates too.
Don't use triggers. I can't believe you guys aren't telling him off. 
I wrote with CASE because it makes much easier to add a 3rd, 4th ... condition to replace. But yeah, nested REPLACE will also do the trick and removes the need of a CASE.
Thank you for the info. I'll see what I can do when I get to the office, tomorrow.
Copied from a comment I made in /r/sqlserver on the same subject: This looks like you're trying to do a fairly standard operation which is usually solved with a correlated subquery. &gt;SELECT * &gt;FROM Mytable &gt;WHERE value = ( &gt;SELECT min(value) minValue &gt;FROM Mytable t2 &gt;WHERE t2.SecondaryKey = Mytable.SecondaryKey &gt;) The problem with this approach is when there are many rows with the same value, you will get all of those rows returned. If you wish to pick off just 1 row for every SecondaryKey value, regardless of if there are many rows then you can do something slightly different: &gt;SELECT * &gt;FROM Mytable &gt;WHERE GlobalPrimaryKey = ( &gt;SELECT TOP 1 GlobalPrimaryKey &gt;FROM Mytable t2 &gt;WHERE t2.SecondaryKey = Mytable.SecondaryKey &gt; ORDER BY Value desc &gt;) If you want to get really efficient then windowing functions are your friend &gt; SELECT * &gt; FROM Mytable &gt; JOIN ( &gt; SELECT SecondaryKey,GlobalPrimaryKey &gt; ,ROW_NUMBER() OVER (PARTITION BY SecondaryKey ORDER BY Value DESC) seq &gt; FROM Mytable &gt; ) t2 &gt; ON t2.SecondaryKey = Mytable.SecondaryKey &gt; AND t2.GlobalPrimaryKey = Mytable.GlobalPrimaryKey &gt; AND t2.seq = 1 Using window functions to pick off 1 row is a good way to protect against the '&gt; 1 matching row' issue. 
Something like... select p.UserID, p.FirstName, p.LastName, count(*) from People p join PeopleCars c on c.UserID = p.UserID where p.FirstName = 'Andrew' group by p.UserID, p.FirstName, p.LastName 
 SELECT A.FirstName, A.UserID, Count(B.UserID)"carsOwned" FROM A INNER JOIN B ON A.UserID = B.UserID WHERE A.FirstName = 'Andrew' GROUP BY A.FirstName, A.UserID 
Ok well....I think this is what you're looking for but honestly I'm not sure. Also, is there a post time column? I just assumed there was in the query below SELECT * FROM (SELECT ROW_NUMBER() OVER (PARTITION BY Odds.PoolID ORDER BY PostTime DESC) "rowNum", * FROM Odds) Odds INNER JOIN Pools ON Odds.PoolID = Pools.PoolID WHERE rowNum = &lt; 5 This was done in MSSQL btw. 
Try taking backup using "Cloudbacko software" i have experience of using CloudBacko software for my MYSQL database backup. Currently, i am using the same software because it gives me full security and protection to my data. Backup of a large 100GB MySQL Database can be finished overnight.
I started with wiseowl sql server tutorials on YouTube . They have really good basics and some intermediate stuff. Sorry on my phone otherwise I'd paste the link. Edit: sorry you said oracle they may have some oracle specific stuff too not sure
Who is the crash course through? I wouldn't mind checking out what they have to offer. I found w3schools.net and the codecademy SQL intro courses very, very helpful!
ill check 'em out, thx!
You need to add the field [DeliveryTime] in the GROUP BY: GROUP BY DeliveryTime, DATEPART(year,deliverytime), DATEPART(month,deliverytime) EDIT: I think you should add both DATEPARTs in the select. EDIT2: If you want to present the Revenue values by month/year, you need to remove the DeliveryTime from the SELECT and add the DATEPARTs. (I need to start reading the topics better before replying &gt;_&gt; )
Do you remember any basic SET math from school, or did you take any Logic courses in college? SQL is based on sets with limits defined by and/or/not logic, so reviewing that will be a great help to staying with the flow in a 3 day crash course. 
You've now chosen the worst of both worlds by implementing both options 1 and 2. Try SELECT DATEPART( year, DeliveryTime), DATEPART( month, Deliverytime), SUM(TransactionRevenueGross), SUM(TransactionRevenueNet), SUM(ALREVENUEGROSS) FROM [dbo].[BigDealEmailRevenue] as R with (nolock) WHERE deliverytime BETWEEN '02/29/2011 00:00:00' AND '03/01/2016 23:59:59' GROUP BY DATEPART( year, DeliveryTime), DATEPART( month, Deliverytime) 
Here is a pastebin since the formating didn't work above (even with four spaces). https://paste.ee/p/IC2yB
I don't think there is a hard limit of only being 6 deep, but in practice it only 6 deep. I tried checking the path 7 levels deep and had no results. An example of bad results is that I have an assessment named ASUP at the top level of the first folder. But it repeats in every level of the path: It should only return a row at this level of the path: "AUP --&gt; " but it returns redundant results at levels like this: "AUP --&gt; AUP-Dublin --&gt; Consistency Test --&gt; " That is because of how I'm doing the join, but I'm not sure how to fix it.
What determine's who's the team's max closer? From your example I don't see why you're closing Tracy from Team B as you have two closers who've both closed 1 item and both with the same cut? For A and C it's at least clear that you're selecting Molly because she closed the greatest number of items, but where there are only 2 closers to pick from the criteria is clear as mud.
I hate to say it, I'm really rough with SQL, I usually work in Access - i.e. with the query builder. Can you walk me through how that might look?
I'll look into that link. Another way I might end up doing it is simply collecting all the data I need over whatever range and comparing the dates myself (Via perl or php or whatever) I was just hoping there was a quick and easy way to simply select what I need!
It looks like you are on SQL Server. If so, you can use a [recursive CTE](https://technet.microsoft.com/en-us/library/ms186243\(v=sql.105\).aspx). Make sure you list what platform in the future.
did you see my reply about how to return a count of 0?
Yep yep. Definitely try a visibility function. Potentially combine that with some IF statements at the top of your queries. IF Parameter A is null {do nothing} Else {Run the query} Its a bit janky, but might be easy to implement.
I only have a suggestion for 2. Item level ought to have it's primary key as item at location, so both fields make up your primary key. Or at least a Unique key and you create a item_stock_level_ID that's your primary key. I'm not sure how User ID figures in unless that's just to indicate who updated the row?
you should be using `with recursive`
You're correct, User ID is simply tracking who updated the row. I figured a compound key is the correct choice, but I wanted to be sure. The tool I am using doesn't support them though, so I may have to look at using a unique key. I think I need to brush up on normalization. Edit: I was thinking about storing revision history in the item_stock_level table, but I think it would be easier to use triggers to update a different table. That will keep the original from getting monumentally large. Note to self: read up on how MySQL handles transactions.
Thanks for this!! I was Used to this since developing with Toad a few years back and hated the fact that SSMS "never did it." This helps tremendously as I'm always switching back and forth. 
If you have rows that are full duplicates, why don't you just put a DISTINCT on your main query? 
Sqlzoo over w3schools imo. 
You can make a saved query in Access and then reference it instead of the actual table. The saved query would just be `select distinct * from table_with_dupes`.
Recursive CTEs are the right way to solve this problem in SQL.
Distinct on the main query was still returning duplicate values, I don't know why, maybe because they have an indexed term? But I wasn't using the indexed term to select... I'm not sure.
The simplest approach is to make three queries in Access. The first calculates the SUM of % CUT per CLOSER per TEAM (total_cut_per_closer_team). The next calculates the MAX of those SUMs per TEAM (max_total_cut_per_team). The last one inner joins together those two queries together on TEAM and SUM/MAX% CUT to see who has the MAX SUM per TEAM (Results). Pretty straight forward in the Access GUI, but SQL below. You could combine these all into a mega query, but I find this easier to understand and keep track of, especially if you plan on adding some weird tie breakers. Query 1 (total_cut_per_closer_team): SELECT Sales.TEAM, Sales.CLOSER, Sum(Sales.[% CUT]) AS [SumOf% CUT] FROM Sales GROUP BY Sales.TEAM, Sales.CLOSER; Query 2 (max_total_cut_per_team): SELECT total_cut_per_closer_team.TEAM, Max(total_cut_per_closer_team.[SumOf% CUT]) AS [MaxOfSumOf% CUT] FROM total_cut_per_closer_team GROUP BY total_cut_per_closer_team.TEAM; Query 3 (Results): SELECT total_cut_per_closer_team.TEAM, total_cut_per_closer_team.CLOSER, total_cut_per_closer_team.[SumOf% CUT] FROM max_total_cut_per_team INNER JOIN total_cut_per_closer_team ON (max_total_cut_per_team.TEAM = total_cut_per_closer_team.TEAM) AND (max_total_cut_per_team.[MaxOfSumOf% CUT] = total_cut_per_closer_team.[SumOf% CUT]); 
SQL is a high level programming language. Therefore it may be self-taught without any barrier. I self-taught SQL with w3schools and the following web site http://www.studybyyourself.com/seminar/sql/course/?lang=eng.
You might consider creating a table with `string_to_find` and `string_to_replace` so that this solution is more extensible. That would require a slightly different approach in the SQL though.
Oracle allows date literals in the form of date 'yyyy-mm-dd' or you can format a literal to your desired date format. Some examples.... to_date('12/30/2015', 'MM/DD/YYYY') to_date('12-JAN-2015', 'DD-MON-YYYY') -- This one is language specific because JAN is english. Your solution is relying on implicit string to date conversion, but this depends on your language settings, so is likely to fail. Always do date comparisons to dates, not strings. Here's a few ways to do what you want, take your pick : case when column_date &lt; date '1900-01-01' then null else column_date end as column_date -or- case when column_date &gt;= date '1900-01-01' then column_date end as column_date -or- case when column_date &lt; to_date('01/01/1900', 'dd/mm/yyyy') then null else column_date end as column_date
Don't get a certification for the sake of getting it. If your employer wants you certified, get them to pay for it (the exam, any courses, and giving you time to prep/study), and get certified for the platform(s) your organization uses. If you use Oracle, get **Oracle's** certification. SQL Server, get **Microsoft's**. A "certification" from some random company that just makes money off certifying people isn't worth getting here.
Have you tried Amazon's own Data Pipeline? It's what we used at my last job to automate our AWS tasks, including loading data into and running scripts in Redshift.
&gt; I hope you have documentation lying around the office. nope. I'm pretty sure I'm going to suggest we drop this idea. I can see the fruitless horror show that awaits me otherwise.
Or Having Count(SomeKey) = 1 Row Number(Over) is expensive
Something like [this] (https://docs.oracle.com/cd/B19306_01/server.102/b14357/ch3.htm) should do: connect hr@sales-server:1521/sales.us.acme.com 
In where col1 LIKE '%/_%'ESCAPE '/'; the part ESCAPE '/' Defines the escape character used in the expression that precedes it. As an example, this statement does exactly the same: where col1 LIKE '%@_%'ESCAPE '@';
Not really. All it says is that you passed the test. Most of the people I know who have passed MS's certifications, especially the first of the 3 exams, say they rarely if ever use most of what they were tested on in their daily work. Heck, some of the most prominent people in the SQL Server community, people who have MVP status, people who are speaking *internationally* about SQL Server, don't have an MS certification. And it's definitely not hurting them.
Sorry... SQL Server 2014 Enterprise. Within the student table, a given student might have a 'W' ethnicity value for White, 'A' for Asian, and so on. Thanks.
So the result you want is something like: Student | W| A| B :--|:--|:--|:-- Name1 | Y | NULL | NULL Name2 | NULL | NULL | Y Name3 | Y | NULL | NULL Name4 | NULL | Y | NULL If so, what you are looking for is the PIVOT command, give me a sec to write what you would need. EDIT: Here is the query SELECT * FROM( SELECT STUDENT, ETHNICITY, 'Y' AS RESULT FROM Students )AS PivotBase PIVOT (MAX(RESULT) FOR ETHNICITY in ([W],[A],[B]) ) AS PivotResult The Result will be exactly like the edited table above
This will split each student's ethnicity into a different column. Just add another CASE WHEN for each ethnicity. SELECT stu.id ,CASE WHEN stu.ethnicity = 'W' THEN 'Y' ELSE 'N' END AS [StudentEthnicityWhite] ,CASE WHEN stu.ethnicity = 'A' THEN 'Y' ELSE 'N' END AS [StudentEthnicityAsian] FROM stu If instead you're trying to summarize the number of students by ethnicity, you might want to use PIVOT. The first [answer to this StackOverflow question](http://stackoverflow.com/questions/15931607/convert-rows-to-columns-using-pivot-in-sql-server) might be helpful.
Yes !
You can download and use Oracle SQL Developer here: http://www.oracle.com/technetwork/developer-tools/sql-developer/downloads/index.html after creating a new connection to your database there is a data modeler option where you can drag your tables into a pane and it will build a data model based on the metadata of those tables. here is a video showing what you are looking for: https://www.youtube.com/watch?v=f80xWJYKJFQ
Ok thanks
You want to look at SSIS, which is another Microsoft product. It is somewhat similar to the process flow in SAS and it will let you visually represent steps for each temp table, etc.
Thanks! I'll look into it! Since I'm not IT a lot of the googling I was doing was either not returning anything useful or flying right over my head. So far this looks promising!
I agree with /u/notasqlstar. SSIS is usually tied to the SQL Server, there you'll be able to do the ETL process you need with a friendlier interface, quite similar to SAS. SSRS will be the equivalent of the SAS report. If you don't have it yet, look at the MS SQL installation disk/ISO and look for those components, if you don't have the license look for SQL Server BI tools
Thanks but I actually wanted to have the data displayed like below: Student_ID|School_1|School_2|School_3 :--|:--|:--|:-- 69|School A|School B|School C 70|School X|School Y|School Z 
Here it is: SELECT * FROM( SELECT Student_ID, School_Name, ROW_NUMBER() OVER (PARTITION BY Student_ID ORDER BY Student_ID) AS COLUNM FROM Students )AS PivotBase PIVOT (MAX(School_Name) FOR COLUNM in ([1],[2],[3]) ) AS PivotResult The only difference is that instead of "School_1" and "School_2" in the header it will show "1", "2", "3" and so on.
You should spend some time really getting to master the T-SQL language. Rather than building a bunch of temporary tables, you should really look into leveraging CTEs and/or Table Variables. CTEs and Table Variables are both very easy to debug, and you can re-run your scripts again and again without having to reset the state of your database.
**The first one,** &gt;create table l_employees_2 &gt;insert into l_employees_2 &gt;select employee_id, &gt;first_name, &gt;last_name, &gt;dept_code, &gt;hire_date, &gt;credit_limit, &gt;phone_number, &gt;manager_id &gt;from l_employees; **The second one,** &gt;alter table sec0611_departments &gt;add column "text", &gt;add column 4; If you can explain to me how each is done, I'd very much appreciate it
Please help me out fellas! 
Tab, yikes! CLS? You can parse it neatly into CSV using Powershell and then pipe it in to SQL using execute SQL command. Make sure your define the tab delimited with `t and define days types. Example: &lt;?xml version="1.0"?&gt; &lt;BCPFORMAT xmlns="http://schemas.microsoft.com/sqlserver/2004/bulkload/format" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"&gt; &lt;RECORD&gt; &lt;FIELD ID="1" xsi:type="NativeFixed" LENGTH="2"/&gt; &lt;FIELD ID="2" xsi:type="NCharPrefix" PREFIX_LENGTH="2" MAX_LENGTH="100" COLLATION="SQL_Latin1_General_CP1_CI_AS"/&gt; &lt;FIELD ID="3" xsi:type="NCharPrefix" PREFIX_LENGTH="2" MAX_LENGTH="100" COLLATION="SQL_Latin1_General_CP1_CI_AS"/&gt; &lt;FIELD ID="4" xsi:type="NativeFixed" LENGTH="8"/&gt; &lt;/RECORD&gt; &lt;ROW&gt; &lt;COLUMN SOURCE="1" NAME="DepartmentID" xsi:type="SQLSMALLINT"/&gt; &lt;COLUMN SOURCE="2" NAME="Name" xsi:type="SQLNVARCHAR"/&gt; &lt;COLUMN SOURCE="3" NAME="GroupName" xsi:type="SQLNVARCHAR"/&gt; &lt;COLUMN SOURCE="4" NAME="ModifiedDate" xsi:type="SQLDATETIME"/&gt; &lt;/ROW&gt; &lt;/BCPFORMAT&gt;
That is the reason, I said interview question, not real time questions. These are the questions for freshers or 2-3 years of experience guy s
Sorry for the delayed reply, tried it today and this worked! Thank you!
Do something like this for each line SUM(Case when TransTypeCode ='1111' then Revenue else 0 end) as 'A' Then only group by statecode.
Pretty sure you want a pivot query, something I'm not familiar with in mysql: http://stackoverflow.com/questions/7674786/mysql-pivot-table
People on this sub are always happy to help solve problems, but you need to make an attempt first. If you're trying to learn, having someone else do your practice won't help. 
Thanks. I found that page too and one of the comments worked for me, cause I couldn't quite wrap my head around the pivot table. Never done them either.
Some random thoughts that will hopefully be of some help. I've dealt with Korean Characters and I know they can be frustrating. First step be sure all the fields you're trying to load to as well as any fields in any staging/processing are NVARCHAR. If they're varchar it'll load the foreign characters as NULL IIRC, but may be an empty string. What does your sql file do? I don't follow. Also make sure when it's in excel as a .csv it's in UNICODE. If you're not going to get these regularly, it's likely simplest to import it manually. Foreign characters can be a total drag.
&gt; First step be sure all the fields you're trying to load to as well as any fields in any staging/processing are NVARCHAR. I don't really know how to do that to an already-existing database, unfortunately. There's no option in here for NVARCHAR. &gt; What does your sql file do? I don't follow. It just inserts data from the excel file to the database. That's actually it, to be honest.
Is this a microsoft SQL server database? assuming so you can use ALTER TABLE dbo.YourTable ALTER COLUMN YourColumnName NVARCHAR(100)--OR whatever length you need **I'm not a DBA and I don't understand how databases use disk storage. This could impact it, if you have a lot of data in that table already.** You can't get Japanese characters into a VARCHAR or CHAR data type. Just as you can't get the letter 'F' into an int field. The only difference is it won't throw an error, it'll just load nothing.
You DELETEd without a WHERE clause. Tsk tsk.
I'm getting an error. I don't think NVARCHAR is recognized by my php. ALTER TABLE user_tbl ALTER COLUMN comp_name NVARCHAR(10) ;
Nah, it's good. I am in no way an expert in anything anyway. It's why I'm asking this question anyway. I'm not setting datatypes in the SQL file though, because I don't see the need if the table is already in the database. I'm using MySQL. For the excel, it's from another database, retrieved and converted into a csv file with PHP methinks. All Japanese characters are broken, probably converted to single-bytes or something. &gt; Do you happen to have a knowledgeable colleague? I wish I have one.
Tip: don't say "I'm getting an error" without also posting the text of the error. 
here is a script, that will take this bad decision of a datamodel, cut out the dates, convert them to dates, add a year, and convert it back to the bad decision : DECLARE @data TABLE (id INT IDENTITY, dateRange VARCHAR(255)) INSERT INTO @data VALUES ('&lt;timespan start="Apr-2015" end="May-2015"/&gt;') ,('&lt;timespan start="Jun-2015" end="Jul-2015"/&gt;') ,('&lt;timespan start="Aug-2015" end="Sep-2015"/&gt;') ,('&lt;timespan start="Aug-2015" end="Sep-2015"/&gt;') ,('&lt;timespan start="Dec-2015" end="Jan-2016"/&gt;') ;WITH numberrange AS ( SELECT n =1 UNION ALL SELECT n = r.n+1 FROM [numberrange] r WHERE r.n &lt; 99 ) SELECT CONCAT('&lt;timespan start="', sq.[dateStart] , '" end="', sq.dateEnd, '"/&gt;') FROM ( SELECT sq.[dateRange] ,dateStart = FORMAT(DATEADD(YEAR, 1, PARSE(SUBSTRING(sq.[dateRange], [1]+1,[2]-[1]-1) AS DATETIME USING 'en-US')), 'MMM-yyyy') --sql 2014 for these functions ,dateEnd = FORMAT(DATEADD(YEAR, 1, PARSE(SUBSTRING(sq.[dateRange], [3]+1,[4]-[3]-1) AS DATETIME USING 'en-US')), 'MMM-yyyy') FROM ( SELECT * FROM ( SELECT * , idx = ROW_NUMBER() OVER(PARTITION BY id ORDER BY n) FROM @data d LEFT JOIN numberrange n ON SUBSTRING(d.[dateRange], n.n,1) = '"' )data PIVOT ( MAX(n) FOR idx IN([1],[2],[3],[4]) )pvt )sq )sq it should even scale to quite large numbers of rows
Thanks, it's exactly what I needed.
Seems convoluted. This is my favorite visual representation which someone posted here, and I can't find the source so I just took a picture of it . http://imgur.com/US3f93z
Select Tradeid, etc, ect from tablename TP inner join (select MAX(refid) refid, tradeid from tablename group by tradeid) TP1 on TP.tradeid=TP.tradeid and TP.refid=TP1.refid
Thank you, this worked! I think not referencing the tables was causing an issue. 
Apparently not. lol. First time ever asking for help on reddit, looks like it'll be my last. And I did try, just that nothing out of those tries has worked. It is why I decided to come here, try my luck.
Erm, more to the point would be that a tvf will be inlined and is not executed as its own process, which leads to you being able to use it in a query. 
Vendor lock-in? Maybe some performance tuning for features which are specific to the platform?
SQL has some inherent limitations. Most have been addressed over the years by either ANSI SQL or vendor-specific implementations. Most 'non standard' APIs form a obect-relational layer to either a target language (e.g. c#) or a 4GL/5GL. In almost all cases this is to assist application programmers with building database software easily. In recent years, databases have much better support for OR functions, BLOBs, security, and transport formats like XML or JSON. So, the likes of ABAP are almost a legacy of having to be able to support mutiple base databases from different vendors whilst still being able to have complex features. Even with today's databases, there are considerable differences in how one implements advanced functions. A few reads on using XML in SQL Server versus ORACLE or other engines is a real eye-opener! And finally of course, once your Enterprise system has proprietary languages, you achieve a higher degree of lock-in, and your Education ecosystem generates higher revenues.
How often you do full backup and TL backups? For now, try to change your database recovery from Full to Simple, it will solve the symptom but you need to check your backup policies to full solve your problem. If you want to do by query, this is the command, run on Master. ALTER DATABASE [Database] SET RECOVERY SIMPLE
If your data isn't critical and can lose some data if something happens, then you can keep recovery as Simple and do a backup as needed. You can find scripts to do it and then create a Job in the Server Agent to make it automatically.
 the project has 6 views, I need help with normalizing to 3nf three of the views, I thought I knew how to do it but apparently not, its a group project so nobody can really do it for me haha I see what your saying though
Best detailed answer you'll get: http://stackoverflow.com/questions/56628/how-do-you-clear-the-sql-server-transaction-log
&gt; Use a create table statement to create a new copy of the l_employees table, with a new name of course. &gt; create table l_employees_2 The problem with this is that you're missing the entire list of columns from the l_employee table in order to create a new version of it. CREATE TABLE i_employees_2 ( employee_id *&lt;data type used in original table&gt;*, first_name *&lt;data type used in original table&gt;*, . . . ) GO &gt; Then use an insert statement with a select clause to copy all the data from the l_employees table to your new copy of the table. Pretty good, but, I think you either need to list the columns or use * INSERT INTO l_employees_2 SELECT * FROM l_employees GO or INSERT INTO l_employees_2 ( employee_id, first_name, last_name, dept_code, hire_date, credit_limit, phone_number, manager_id) SELECT employee_id, first_name, last_name, dept_code, hire_date, credit_limit, phone_number, manager_id FROM l_employees GO &gt; Add two new columns to a copy of the departments table, sec0611_departments. One new column, a text column, will be for the name of the manager of the department. The other new column, a numeric column, is for the annual budget of the department. You're not even trying on the alter.. ALTER TABLE sec0611_departments ADD COLUMN manager_name *&lt;data type for a name&gt;* ADD COLUMN annual_budget *&lt;data type for a dollar amount&gt;*;
I took screens of the tables I have to normalize, after I normalize me and my partner will merge and draw erds https://imgur.com/a/I05Me
I just looked at it and it seems incredibly simple. However, there will be an added step in my process (which isn't the end of the world). This is due to AWS having options to export from DynamoDB -&gt; S3 Bucket and then S3 Bucket -&gt; Redshift. They do not have a built in functionality that goes from DynamoDB -&gt; Redshift. I suppose I could whip something up using the CLI, but I've never been involved with anything of that nature. I will do some research and figure it out. Thank you for your help and guidance!
Another thought which may be helpful: If you don't want to convert the column type, the other option is to replace the unicode characters with unicode escape sequences (\u followed by 4 hex chars). How you would go about doing this in PHP is beyond me, but I'm sure there's some way to do that more or less out-of-the-box.
Fastest way, usually better performance then with a ROW_NUMBER(): SELECT TRADEID, etc, etc FROM TABLENAME TP CROSS APPLY ( SELECT TOP 1 REFID FROM TABLENAME SUB WHERE SUB.TRADEID = TP.TRADEID ORDER BY REDIF DESC ) tt
As you have been given some good advice I'll allow this post to stay, but in general, this is not a good format for a question to /r/SQL. You didn't actually provide any information to work with, and this subreddit is not for buying or selling services.
I apologize
This is a common problem when querying log tables where each activity is recorded in a new row but with the same identifier (TradeID in your case). I'd use the EXISTS clause like so: SELECT TradeID, * FROM TABLENAME TP WHERE NOT EXISTS ( SELECT 1 -- what you select here doesn't really matter FROM TABLENAME TPINNER WHERE TPINNER.TradeID = TP.TradeID AND TPINNER.RefID &gt; TP.RefID ) Note that this is also a correlated sub query (like D_W_Hunter's answer). 
Oh that's interesting. It is about 2 million rows I work in the health insurance industry so it's a lot of data. I'll try this tomorrow at work. Thank you for your advice.
You made me curious to see what I could find. It seems inconclusive... http://stackoverflow.com/questions/13057663/what-is-the-difference-between-a-row-record-and-tuple http://stackoverflow.com/questions/4212265/tuples-vs-records/679449
Here is the result... collation_connection utf8_general_ci collation_database utf8_general_ci collation_server latin1_swedish_ci Variable_name Value character_set_client utf8 character_set_connection utf8 character_set_database utf8 character_set_filesystem binary character_set_results utf8 character_set_server latin1 character_set_system utf8 character_sets_dir C:\\xampp\\mysql\\share\\charsets\\
Yeah, I'm really designing something very similar to this. Thanks for the input. This really helped! 
the junction table should not have its own sequence number PK instead, it should look like this -- CREATE TABLE Monsters_in_Films ( MonsterID INTEGER NOT NULL , FilmID INTEGER NOT NULL , FOREIGN KEY MonsterID REFERENCES Monsters ( MonsterID ) , FOREIGN KEY FilmID REFERENCES Films ( FilmID ) , PRIMARY KEY ( MonsterID , FilmID ) );
Use a many-to-many relationship table between groups and artists. Depending on whether you need history of who was a member of what, you may or may not want to delete the records from the mtm table (instead, you have a column like DateJoined NOT NULL and DateLeft NULL, when a member leaves, you set the DateLeft value). Outside of the mtm table, groups and artists don't know about each other. The group table should also have a DateFormed, DateDisbanded columns and instead of deleting the group, you just set the DateDisbanded and mark all the active records in mtm table with the same date in DateLeft (you can do that with a trigger, or have a procedure that disbands a group).
I recently 'got' Joins using left/right inner/outer, where before I would just use the **WHERE** statement to filter. What clicked for me was when I realised that a **LEFT JOIN** only returns rows from the left table when it has a match in the right hand table, and a **LEFT OUTER JOIN** also returns the rows with-OUTER (without a) match. From getting that the rest fell into place.
If you wanted a single column you could use a case statement like the following: Select Stu.Name As [Student Name], CASE WHEN Stu.Ethnicity = 'w' THEN 'White' ELSE WHEN Stu.Ethnicity = 'b' THEN 'Black' ELSE WHEN Stu.Ethnicity = '' OR Stu.Ethnicity IS NULL THEN 'Not Provided' END As [Ethnicity] FROM table
DrTrunks I really appreciate this, it works as expected. I see that LAG() accesses the previous row but I am a little confused as to how the rest of it functions, could you break it down in laymans terms for me? I think the confusing part is the ordering by clothing.
&gt;I see that LAG() accesses the previous row but I am a little confused as to **how** the rest of **it functions**, could you break it down in laymans terms for me? &gt;I think the confusing part is the **ordering by clothing**. Exactly that. In SQL there is no order, SQL does not "know" what is in the "previous row". There is no previous row, unless you order by something. So I ordered lag() by your other column, but you could order it by whatever you want if you just have at least 1 column in it. As always [MSDN](https://msdn.microsoft.com/en-us/library/hh231256.aspx) has an article about it.
I agree. Thanks for the correction.
Thank you once again! Despite a tough day at work, learning this nugget of information has me feeling good inside. It looked like the ordering was affecting the ordering outside of the CASE statement, so I have now managed to order it in a way which is great for me.
haha, no doubt
Essentially a soft delete: http://stackoverflow.com/questions/2549839/are-soft-deletes-a-good-idea
I saw the problem, the POWER() function returns the same type as the number you put in the 1st argument, to fix I just used CAST to force it be a BIGINT instead, this is the ALTER FUNCTION: ALTER FUNCTION [dbo].[OctToBin](@s VARCHAR(255)) RETURNS varchar(255) AS BEGIN DECLARE @i bigint, @temp char(1), @result bigint, @Output varchar(255) = '' SET @i=1 SET @result=0 WHILE (@i&lt;=LEN(@s)) BEGIN SET @temp=SUBSTRING(@s,@i,1) SET @result=@result+ (ASCII(@temp)-48)*POWER(CAST(8 AS BIGINT),LEN(@s)-@i) SET @i=@i+1 END WHILE @result &gt; 0 BEGIN SET @Output = @Output + CAST((@result % 2) AS varchar) SET @result = @result / 2 END RETURN REVERSE(@Output) END Test: print dbo.OctToBin(0001401215170046) Result: 1100000001010001101001111000000100110
Seems like that did the trick! Thanks again for your help. 
There are a lot of disadvantages to using non-standard APIs. You're basically "hard-coding" your software to work with that specific vendor's platform and version, and you are setting yourself up for difficulty in the future. That being said, there are advantages to non-ANSI standards. They can enable functionality that hasn't been standardized yet, or may be more preferable to the standard. My personal anecdote involves SQL Server and the "UPDATE FROM" syntax. I think it's superior to MERGE syntax and I hope (but doubt) it will be adopted into the standard. In any case, it's bad practice for me to use UPDATE FROM, because I can't take those queries with me if the software I support ends up using a different database in the next release. Another example I can think of involve result set paging, which was a pain to do across different SQL implementations. SQL Server did "TOP X" records, which is limited in usefulness. Eventually, SQL:2008 standardized this with the OFFSET &amp; FETCH clauses, which are now widely available across most recent RDBMS platforms (including SQL Server 2012). Ultimately, it's a bad idea to commit yourself to a database platform that may be bought, sold or bankrupt in the future. If ANSI does what you need, you should try to use it whenever possible. You need to be aware that any time you use a non-ANSI feature, you may find yourself in a situation down the road where you have to re-write it. My $0.02 after working on a lot of different database platforms.
Holy crap, when did BCP start supporting XML files? We had to migrate *tons* of business data in a very short time span, we ended up having to use BCP for its raw performance. But XML wasn't an option back then, we had to use CSV.
LISTAGG: http://docs.oracle.com/cd/E11882_01/server.112/e41084/functions089.htm
Good man! Worked like a charm.
Wrong * Wrong number of records (more by 3) I'm completly lost.
You followed my advice on the insert, but not on the table create. As written here you haven't put any columns for your table create. I they didn't require that you use a create table command you'd accomplish it with SELECT * INTO l_employees_2 FROM l_employees; But, from what you posted the other day, they are requiring that you use a create table statement.
Depending on platform there's usually a way to display a table's create statement. All you'd have to do is use that on the original table and change the name.
[Microsoft PowerBI Desktop](https://powerbi.microsoft.com/en-us/desktop/) might be worth looking into. It's a free download.
Worked beautifully. Also, I don't think it matters. As long as it works. Thanks again! 
You can not directly delete database, However you can move or remove the data files. Take a look on here https://support.microsoft.com/en-us/kb/224071
I don't like subqueries. I'd recommend doing something like: IF OBJECT_ID('tempdb..#UserID') IS NOT NULL DROP TABLE #UserID CREATE TABLE #UserID (UserField varchar(10) PRIMARY KEY) INSERT INTO #UserID SELECT DISTINCT UserField1 FROM staging WHERE UserField1 is not NULL INSERT INTO #UserID SELECT DISTINCT UserField2 FROM staging s LEFT JOIN #UserID u ON S.userfield2 = s.UserField WHERE UserField2 is not NULL AND u.userfield is null -- NOT IN #UserID ALREADY INSERT INTO #UserID SELECT DISTINCT UserField3 FROM staging s LEFT JOIN #UserID u ON S.userfield3 = s.UserField WHERE UserField3 is not NULL AND u.userfield is null -- NOT IN #UserID ALREADY --Remove The records that match on Userx fields from #userID DELETE U FROM #UserID u JOIN staging s ON u.userField = s.User1 OR u.userField = s.User2 OR u.userField = s.User3 SELECT UserField FROM #UserID edit added DISTINCT 
The most amazing thing is that it's less confusing and more readable than the sql code my former coworkers would write.
Hehe yea
Love this!
Try this out: select sum(x.p) as 'dollar amount' from ( select a.itemid, b.costperitem * c.quantity as 'p' from ( select i.itemid, min(i.receiveddate) as 'mindate' from inventory i group by itemid ) a inner join inventory b on b.mindate = a.receiveddate and b.itemid = a.itemid inner join redemption c on c.itemid = a.itemid ) x
If the dates are guaranteed to be different, the results produced by both queries will be the same, they just use a different way to extract the latest cost by item. About the only thing I can think of that would give you unexpected results is that you have a DATETIME column that you apply a BETWEEN condition to with dates. Remember that '2016-02-22' expands to '2016-02-22 00:00:00', so you are not including the data from even a minute into Feb 22nd.
You don't think you need something more theorical before going directly to SQL? I mean, teach about how databases works, what's a table, a row, etc.? 
I was actually worried about the propensity for duplication. Your approach is cleaner and more eloquent. Thank you for expanding. 
Like the /u/pooerh was saying, you probably want to use: `where cast(receiveddate as date) between cast(date as date) between cast(date as date)`.
The reason you need a group by clause is because you want to group your orders by customer ...
Is the id auto computed for the row, or do you assign it? 
I definitely wouldn't do it that way, since you're reinventing identity fields and employing a long lock and risk of a race condition in the process. There may be a streamlined upsert-style syntax for it, but barring that I'd probably do: select @team_id = t.team_id from teams t where t.team_name = @whatever if @team_id is null then begin insert... select @team_id = @@identity end Or whatever. Another approach is: insert into teams (team_name) select @team_name where not exists (select t.team_id from teams t where t.team_name = @team_name) select @new_team_id = t.team_id from teams t where t.team_name = @team_name Slightly worse performance, but it avoids a tiny race condition in the first version.
The table has an intTeamID (PK) and strTeam. So when a team is added it'll get the next available intTeamID and insert the Team at that ID. Then it returns the ID back to the code.
That seems like it would work, but i'm not super experienced and am having trouble factoring the ID back into it. Like in what i provided it generates the next available id, because that needs to be inserted along with the Team (if the team does not exist already).
Have you tried 2012 instead? If not, I without go ahead and install it. Once installed you can determine if the issues are legit or were just cautionary since not every program had been evaluated for Windows 10. Sometimes it is hit or miss.
So, I'd copy the table that I want to update, update the copy, then merge it back with the original? That's about what I figure MERGE does from a quick lookup, but thinking that way, I still see it taking the same amount of SQL... I think.
Er, what's wrong with adding the `IDENTITY` property to the `intTeamID` column, and then when you want to add teams just using: INSERT INTO TTeams (strTeam) OUTPUT INSERTED.intTeamID VALUES ('Hungarian notation sucks'); Then you don't need a stored procedure at all. If you *really* want a stored proc, then you should *still* use the `IDENTITY` property and just combine it with `SCOPE_IDENTITY()`: CREATE PROCEDURE uspAddTeam @strTeam varchar(50), @intTeamID int OUTPUT AS SET NOCOUNT ON; INSERT INTO TTeams (strTeam) VALUES (@strTeam); SET @intTeamID = SCOPE_IDENTITY(); SET NOCOUNT OFF; And viola! No need for exclusive table locks!
If you don't need to run an instance of SQL Server on your desktop, just pick up the [SSMS 2016 CTP](https://msdn.microsoft.com/en-us/library/mt238290.aspx?f=255&amp;MSPPError=-2147217396) It'll alert you when the next release comes out so you can upgrade to the final bits when 2016 is out.
So I have edited the [my.cnf] file and added this line of code... character_set_server=utf8 And now, the output is... Variable_name Value collation_connection utf8_general_ci collation_database utf8_general_ci collation_server utf8_general_ci Variable_name Value character_set_client utf8 character_set_connection utf8 character_set_database utf8 character_set_filesystem binary character_set_results utf8 character_set_server utf8 character_set_system utf8 character_sets_dir C:\\xampp\\mysql\\share\\charsets\\ But the it's still blank. From the excel file, 'broken Japanese character' is like this... æ¤œç´¢ ...which should have been 検索.
Thank you for answer! :) 
Consider using `SCOPE_IDENTITY()` instead of `@@IDENTITY`, it's safer in such a situation. Imagine you're inserting into a table t1 that has an identity, but there's an insert trigger on t1 that will insert into another table t2, with another identity. `@@IDENTITY` will return the identity value from t2, `SCOPE_IDENTITY()` from t1. The more you know! [Documentation for SCOPE_IDENTITY](https://msdn.microsoft.com/en-us/library/ms190315.aspx), see example A.
Forms in C#
This is really incredible, thank you. Hard to believe this is free when I look at what other services are charging. 
I've tried access and it really blows as a front end to MSSQL. Just super glitchy-- I don't feel like I can depend on it from a consistency standpoint. I ended up settling on Visual studio lightswitch forms. A little more setup work but a much more usable product in the end.
Build a CUBE and they can connect to that. But I'm guessing that is an unrealistic suggestion.
Forms are a part of the access application tools that let you create drag and drop Windows forms and bind them to data., as for references, not really, my daily job it to move my business away from access because you can't source control the code, I can tell you that a fair amount of programming in VBA can be involved depending on the data complexity and use cases.
I absolutely did not believe you, but holy shit! It's really a preview!! I love open source, love Linux, hate just about everything when it comes to Windows... Everything except SQL Server. Man, this could be really promising! Link: https://blogs.microsoft.com/blog/2016/03/07/announcing-sql-server-on-linux/
I want in on the preview!!! :(
I think what you are looking for is something like this: SELECT Customer.CustId, count(*) FROM Customer INNER JOIN Orders ON Customer.CustId = Orders.CustId WHERE (Prov = 'ON' AND YEAR( Orders.OrderDate) = '2014') GROUP BY Customer.CustId HAVING COUNT(*) &gt; 2;
&gt; core relational database capabilities to preview today, and are targeting availability in mid-2017 Meh
Thanks :)
Does Linux had the equivalent of Windows Powershell? Being able to script on Powershell for some SQL scripts was an absolute Godsend for our development team. 
No unfortunately Linux is extremely limited when it comes to any kind of scripting and programming in general, you better stick to Windows. 
Well said... You're not the only one utterly confused by his reaction. Suddenly my skill set is transferable and I've potentially got the chance for some exposure to Linux professionally. Can't be a bad thing. 
Differential backups are backups of the changes that have occurred since your last FULL database backup. This way, you can’t run a differential backup process without first having a FULL backup in place. The most common cause for this issue occurring is if you have another backup software (NTBackup, BE, Bacula etc. ) which takes a snapshot (VSS copies – Volume Shadow Copy) of your database, then this can cause the chain of backups to be broken and invalidate your previous DIFFERENTIAL backups, which is why when your current DIFFERENTIAL backup process starts, it will fail as it can’t see that a initial FULL backup or another DIFFERENTIAL backup has occurred before it. Find more here - http://sqlbackupandftp.com/blog/cannot-perform-a-differential-backup-database-current-database-backup-exist/
Simple: Rebuild is recommended when your fragmentation is above 30%. Rebuild blocks queries. Rebuild can run on multiple CPUs (multi threaded). Rebuild is faster. Only in Enterprise Edition you can rebuild with an online index that then does not block. Reorganize is always single threaded so it uses only one core. It is an online operation meaning it doesn't block anything but it can take long. Only recommended if your index fragmentation is below 30%
I THINK I DID Q1,Q2, BUT Q3 AND Q4 ARE DEFINITELY WRONG....... I DONT KNOW HOW TO DO THEM!!!
Okay, first you need to format your code properly. You need 4 spaces in front of each line so it formats correctly. You also need to prefix your title so we know what version of SQL you are using. Please take a look at the side bar. Next think about your problem logically. 
In case you're not joking Linux has had the bash shell since forever and a whole bunch of other shells. 
First of all, calm down with the CAPS haha. ok, lets go. Q3: Which tank has the fewest fish? You need to use COUNT on the fish_id grouping by the tank id, so this is the query: SELECT tank_number, COUNT(fish_id) AS NumFish FROM tank_fish GROUP BY tank_number ORDER BY NumFish From that we see that tank 1 and 5 has 1 fish each. Q4: Which tank is worth the most and how much is it worth? For that you need to JOIN the fish table with tank table and SUM the fish prices grouping by the tank_number SELECT tank_number, SUM(price) AS TotalValue FROM tank_fish AS T join fish AS F ON T.fish_id = F.fish_id GROUP BY tank_number ORDER BY TotalValue DESC Running it we see that tank 10 is worth 144,97, making it the most valuable.
I mean where the actual data is stored. To give you a scenario, I would want to do something like this: * Download a data set as a postgres or .csv file * Ingest it into a database * Use something like [SQL Tabs](http://www.sqltabs.com/) to query that database * Export that manipulated data as a CSV and load it into Python Typically I do all of this in SQLite but I'm looking to transition something that's a bit more powerful. This is only for personal use, so I'm guess I'm looking for an end-to-end tutorial or explanation on how to set this up on my local machine.
You have the full backup, which is self explanatory. There are the transaction logs, that only save the changes made between the last full backup and now, so every transaction backup you make, the file will get bigger until you make another full backup. There is the log backup, it stores the changes since the last backup, any backup, for restoring, you'll need to have all the log files and the full backup and the transactions if you have it. I particularly make a full backup daily and a log backup every hour, with that set up properly you'll lose at most 1 hour of work if everything breaks.
I'll wait till I see the license costs. 
PowerShell drew a lot of influence from Bash and other shells, but it's quite different in implementation. Slinging objects around makes a many things *much* easier than passing everything as text and having to parse that.
I think we're going to see PowerShell come to Linux before too long. DSC is already there. Large portions of .NET are going (or have gone) Open Source. Snover's a smart guy, I have to believe it's on his roadmap.
A comment about transaction log backups: they won't actually get larger (excepting normal activity) as the inactive portion of the transaction log will be removed once the backup is complete. So restore you will need all transaction log backups since the last full/differential backups (I.e a complete LSN chain) - not just the latest. 
There are more than just three most popular backup types like full, differential and transaction log backups, but let's start with them: Full Database Backup (http://sqlbak.com/academy/full-backup/) A full database backup is the simplest kind of SQL Server backup that does not depend on the recovery model. It contains all data in a particular database and enough log that is required to recover the database. To execute full backup use the following command: BACKUP DATABASE Adventureworks TO DISK = 'full.bak' Differential Database Backup (http://sqlbak.com/academy/differential-backup/) A differential database backup is related to the last full backup and contains all changes that have been made since the last full backup. You can perform a differential backup in the following way: BACKUP DATABASE Adventureworks TO DISK = 'diff.bak' WITH DIFFERENTIAL Transaction Log Backup(http://sqlbak.com/academy/transaction-log-backup/) This SQL Server backup type is possible only with full or bulk-logged recovery models. A transaction log backup contains all log records that have not been included in the last transaction log backup. To apply a transaction log backup to your database use the following command: BACKUP LOG Adventureworks TO DISK = 'log.bak' COPY_ONLY Backup(http://sqlbak.com/academy/copy_only_backup/) Use COPY_ONLY option if you need to make an additional full or transaction log backups which will occur beyond the regular sequence of SQL Server backups. To perform copy-only backup simply add “COPY_ONLY” clause: BACKUP DATABASE Adventureworks TO DISK = 'full.bak' WITH COPY_ONLY File and Filegroup Backups(http://sqlbak.com/academy/file-backups/) These backup types allow you to backup one or more database files or filegroups. To execute file backup use the following command: BACKUP DATABASE Adventureworks FILE = 'File' TO DISK = 'File.bck' Use this command to perform filegroup backup: BACKUP DATABASE Adventureworks FILEGROUP = 'Group' TO DISK = 'Group.bck' Partial Database Backup(http://sqlbak.com/academy/partial-backup/) Typically partial backups are used in simple recovery model to make backups of very large databases that have one or mode read-only filegroups. However, SQL Server also allows making partial backups with full or bulk-logged recovery models. Use the following T-SQL command to execute a partial backup: BACKUP DATABASE Adventureworks READ_WRITE_FILEGROUPS TO DISK = 'partial_backup.bak'
any previews on performance with the preview build?
in that case, you probably want to use the [`psql`](http://www.postgresql.org/docs/current/static/app-psql.html) command line client and the `\copy` command, or [`copy`](http://www.postgresql.org/docs/current/static/sql-copy.html) command in queries you can even do it all in one step: create table foo ( id int primary key, name text ); copy foo from program 'curl "http://example.com/my.csv"' with (format csv, header true);
If you are a dev use [this](http://www.microsoftstore.com/store/msusa/en_US/pdp/SQL-Server-2014-Developer-Edition/productID.298540400) license, it's way cheaper than the Azure license costs! &gt;How does this work ? Even professional Microsoft license experts argue with each other over this. You usually pay PER server PER (virtual?!) core PER call PER edition... &gt;Does MS audit somehow ? They can audit. &gt;SQL Azure You're sure compliant if you use it!
I just use an evaluation for myself.. and another evaluation.. and another.. I reformat my computer every 30 days.. When do they audit ? It is so confusing but I mentioned it to the manager as a 'By the way..'.. People are scared of using the cloud hear but I am hoping it will prevail.. We are suppose to be 'Socks Compliant' whatever that means.. and I think Azure has some features related to that
I have recently found that the %'s are applicable to the "it depends" law of database administration. Recently found out, we have a gigantic index. Doing a reorganize on it at 25% takes over 2 hours, it's that crazy. This is a DB we can NEVER do a rebuild and having some lag time is better than a complete inaccessible mode. (Without the index, everything lights on fire. It's stupid.) So I've started setting the % to 5% reorg and this takes 10-20 min and keeps it up day to day. 
I wasn't! I don't have knowledge of Linux at all, as my experience has been in SQL Server/C#/Powershell! Thanks for the knowledge, maybe someday I'll use a Linux box at my job.
You should not be copying C# programs *or* batch scripts to your SQL Server. Just move the tables and data. You might need to *keep* those programs and scripts to help with maintenance, conversions, export routines, etc. But don't put them on your server. In fact, don't put anything on your server but SQL Server itself.
Lots and lots of money, depending on where you live: &gt;Under federal law in the United States, each violation carries a potential fine of up to $150,000 per software title copied illegally. Individuals prosecuted for criminal copyright infringement face up to $250,000 in fines and imprisonment of up to five years. The BSA offers a lot of cash: &gt;BSA offers rewards of up to $1 million USD for qualified reports of software license violations. In other words, your co-workers could really cash in by reporting you. If you want to make a quick buck of your workplace... I'm not responsible if you get fired for making such report. But if they offer up to a million, they'd have to make more back from the fines.
partition?
It's going to be expensive, gimped, or both. I remember when SQL Server used to be both a real bargain and a high-performance database and thinking "that price is going to change real soon." Sure enough by the time we did our next upgrade, licensing jumped over 30%.
Are you saying that you want file.mdf to be accessed by two connections simultaneously, to move it completely, or do you want redundancy?
&gt;I cannot download SQL server here and have already competed code academy. How about you download it at home and practice with the adventure works database? 
There are any number of very powerful shells available on linux, and you'll find that powershell has a number of aliases to make it feel a little more like a linux shell (e.g. grep, ls, cat, man, clear, cp, mv, rm, pwd, echo, ps, etc). However, the paradigm is a little different. In a shell like bash, you don't generally work with objects, everything is text and you pipe text around and parse it with various tools (grep, sed, awk, tr, cut, tail, sort, uniq, cut, etc).
You might want to correct your URL to http://sqlzoo.net
Currently it is attached with the least optimal IP address/NIC/routing. So I want to change the UNC path. I figure there must be a "trick" to do this, maybe altering msdb directly etc.
where do you go to figure out if your software is valid ? 
Microsoft has a course on edX (called Querying with Transact-SQL) and while I have no idea if the course is any good or not, it provides instructions on how to set up an Azure SQL Server instance and use it for free for a period of time. When the time runs out, just create a new Azure account (it's not cheating the system, Microsoft explicitly recommends doing this). Not only do you get a legit SQL Server instance and database (pretty sure the instructions are for adventureworks, but you can load or build your own) database), but you also get the chance to play around with Azure and get that all important "cloud computing" experience that's all the rage right now. Agree with others that sqlzoo is awesome, but I think it misses part of the equation in that while you're definitely learning SQL, it's not the same as starting from scratch. [Link instructions telling you how to create Azure SQL Server instance](https://raw.githubusercontent.com/MicrosoftLearning/QueryingT-SQL/master/Getting%20Started.pdf) [Link to edX course](https://www.edx.org/course/querying-transact-sql-microsoft-dat201x-0)
That's what I did. I used Google remote desktop at that job because everything was locked down pretty hard, but for some reason Google extensions were AOK.
This gives away that you are trying to learn the skills for the next job while doing the current job. I learned a great deal just trying to get around this problem awhile back. 90% of my time was unutilized. But I had to pretend like I was giving 110%. Weird job.
Have you taken a look at Blockspring? It can do exactly what you're asking plus connect to other platforms too. You'll need someone with some technical background to set it up, but thats pretty evident of most systems.
You need to work out whether you have an issue that is being caused by fragmented indexes/out of date statistics to start with. In my experience, most databases have this as an issue if nobody's been looking at maintenance in these areas before. Have you looked at [Ola Hallengren's Scripts](https://ola.hallengren.com/) before? He's got a great solution for rebuilding/reorganising indexes based upon levels you set, it will also keep your statistics updated if you want it to, it's pretty flexible. For reference, my job is to help maintain hundreds of customer databases remotely so we need scripts that will just run on their own with minimal input, these scripts have been invaluable. The recommendations that we work with is Rebuild over 80% and reorganise at over 50%, but you'll have to see what works best for your database. 
Install SQL Server at home. Remote into your home machine. Profit.
The 'repeat forever' part is unclear - in a general case you have a tree structure, and relational DB/SQL is sub-optimal for tree traversal/retrieval. In the specific example though, there are only 2 levels in the tree structure and there appears to be only 3 distinct node types - 'Type 1' with 1:N children of 'Type 2' (4 kinds of data) with 0:N children of Type 3 (4 kinds of data, no children allowed). If this is the case, while (most likely) very verbose, the access to the data should be relatively straightforward.
If you have access to chrome browser, there is a Secure Shell extension that provides a reasonably useful terminal client. From there you can connect to a leased server.
Thank you, I was confused
&gt; select dbms_metadata.get_ddl('TABLE',table_name) &gt; from user_tables &gt; ;
True, it's a good start. 
Although GROUP BY is deductible, SQL stupidly demands it appear in the statement. So, add a GROUP BY clause. Another note, although it doesn't really matter here, using functions (that are not part of an index) is not very efficient. You ought to change the YEAR clause to Orders.OrderDate &gt;= DATE '2014-01-01'
There are tools that will do it for you, and that is the recommended method. Regardless, you can first start with the table names which can be gotten from sys.tables.
Assuming it's SQL server, here's one method: SELECT TOP 1 tank_fish.tank_number, SUM(fish.price * tank_fish.quantity) FROM tank_fish, fish WHERE fish.fish_id = tank_fish.fish_id GROUP BY tank_fish.tank_number ORDER BY SUM(price) DESC;
MS SQL Server Express, PostgreSQL, MySQL, anything with an ODBC layer you can tie into. 
SQL Server express will be enough for sure, 10GB of data is a lot, and you'll have several tools that will make the Access -&gt; MSSQL transfer pretty easy. MySQL is very similar to MS SQL, if you write your queries in MS access I don't think you'll have any trouble with MySQL Postgre allows you to store a lot of data, and I mean A LOT, like 1 GB per field, 1.6 TB per row and 32 TB per table [source](http://www.postgresql.org/about/), and the query is also similar. 
Why not just use SQL Express. Microsoft can swoop and audit at any time. I have seen it happen and it can be ruinous.
Since you are trying to get other (I am assuming less technical) analysts involved you may want to consider how well it will play with Excel. MySQL has [MySQL for Excel](https://www.mysql.com/why-mysql/windows/excel/) and SQL Server has [Power Pivot](https://support.office.com/en-gb/article/Power-Pivot-Add-in-a9c2c6e2-cc49-4976-a7d7-40896795d045)
By repeat forever, I mean one could have 50 levels deep of data if one wanted. Effectively ayes a tree structure with unlimited levels of branches. Yes, I know sql isn't designed well for that. 
they gotta be planning an express version to compete with mysql Right?
Which db isn't relevant, since it's just a sql data design, which should fit into any sql database. 
It's not really a secret. It's just that the design will need to work on multiple platforms.
Yeah, I'm well aware. Just a bit of sarcasm friend. 
So firstly let's deal with recursion. All you need is a table with a primary key, and a nullable/optional foreign key that joins back to the same table. This is sometimes called a 'pig's ear' and it is how you make trees in an RDB. Next your 'any of 4 kinds of data'. Truly vague and probing for more info has been a bit like pulling teeth. I am going to assume you have in effect 4 'tables' with a set of columns in each for your 'kind of data'. There is nothing wrong with having all of these columns in 1 big wide row and leaving them NULL if they are not applicable. You can have a type column containing 1-4 to indicate which set of columns are going to be filled. This is a 'sparse column' style design, slightly denormalised but much easier to traverse. So now you have 1 table that can perform your full requirement. The problem is you must query it recursively, and recursive queries differ by DB engine. Ask yourself how strong the 'platform independent' requirement really is. Finally, if your data is truly hierarchical or document structured, the a Relational DB could be a poor choice. You could use MongoDB or similar, or intersystems Cache as a HDB example.
Very helpful. Thank you!
Well, if you're working with MS SQL, there's hierarchyid type for these kind of structures. It performs ok, but is absolutely a proprietary solution. https://msdn.microsoft.com/en-us/library/bb677213.aspx 
First, terminology: I am almost certain you mean tables, not databases. Tables are spreadsheet-like collections of rows and columns. As to what you are actually attempting to do, you need to join those tables together to retrieve values from each of them. I am guessing you have something like: - Students table with name, etc - Subjects representing coursework or whatever - Entries being a table to express that many-to-many relationship between students and subjects. You would write something like: select students.first_name, entries.date_of_entry, subjects.subject_name from students inner join entries on entries.student_id = students.student_id inner join subjects on subjects.subject_id = entries.subject_id where subject.exam_board = 'OCR'; Obviously I'm guessing about what belongs where. Anyway you really need to look into joins and learn them because you aren't going to be able to pass any tests without them. Check out the resources in the [wiki](http://www.reddit.com/r/SQL/wiki/index).
Thanks! I'm the only one in my department who even knows what SSIS is (to be generous to myself), so troubleshooting things as I learn has been a challenge. I did try googling the error code, but all the forum posts and stuff I was finding was either unintelligable to me or suggested things I didn't know how to do :(. As for the query code, I'd be happy to edit it in tomorrow morning when I get back into work. For context, I didn't originally add it since I tested it in SQL server first, and SSIS will generate a preview of it before I attempt to run the package just fine. From what I can understand at least, it just seems like SSIS is just...crapping out from being overloaded?
At a guess I'd say that the foreign key is indexed, and the extra time is updating the index. I'm not much of a MySQL user, but what I'd try for shits and giggles if you're loading the text file by odbc is to import the data into its own table first (without an indexed foreign key), and then insert it from there into the final table. MySQL may be able to optimise the index updating better if the data is coming from another table rather than via odbc. 
&gt; What's wrong with this? you're creating a DATETIME column called `date` -- that'll confuse someone down the road (possibly even yourself) 
You can join them with + . So c1+ ' ' + c2. Or you can create a null column between them. 
table_name meaning the specific name of the table right? Is there a way to run it for all tables ?
Try SELECT Column_1 + " " + Column_2 FROM Table_1 Or SELECT Column_1, " ", Column_2 FROM Table_1
the plus sign as a string concatenator is not standard SQL, sorry
Are you doing a [bulk load/insert](https://dev.mysql.com/doc/refman/5.7/en/load-data.html) (exact usage/terms may depend upon your MySQL version)? If you *really* want to get nuts, load into a standalone table with the same structure as your destination table, and do a [partition swap](https://dev.mysql.com/doc/refman/5.7/en/partitioning-overview.html) if your setup allows. As for the "is it normal" question - standard DBA answer: it depends. You could have some import operations run significantly faster than this with more records, and others that run slower with fewer records. Depends on your import method, indexes, constraint checking, storage subsystem and system memory, among other factors.
I always liked CONCAT: SELECT CONCAT(col1, ' ', col2) 'Address'
Running totals of what? These are dates, not amounts you can sum. Brute force method.. There has to be something more elegant than this but I'm on one cup of coffee so far this morning and brute force is all I got: SELECT DISTINCT tDate, CountinDate1, CountInDate2, CountInDate3, CountinDate1 + CountInDate2 + CountInDate3S AS CountInAll FROM ( SELECT Date1 AS tDate, ( SELECT COUNT(*) FROM Events d1 WHERE d1.date1 = Events.date1 ) AS CountInDate1, ( SELECT COUNT(*) FROM Events d2 WHERE d2.date2 = Events.date1 ) AS CountInDate2, ( SELECT COUNT(*) FROM Events d3 WHERE d3.date1 = Events.date1 ) AS CountInDate3 FROM Events GROUP BY Date1 UNION SELECT Date2 AS tDate, ( SELECT COUNT(*) FROM Events d1 WHERE d1.date1 = Events.date2 ) AS CountInDate1, ( SELECT COUNT(*) FROM Events d2 WHERE d2.date2 = Events.date2 ) AS CountInDate2, ( SELECT COUNT(*) FROM Events d3 WHERE d3.date1 = Events.date2 ) AS CountInDate3 FROM Events GROUP BY Date2 UNION SELECT Date3 AS tDate, ( SELECT COUNT(*) FROM Events d1 WHERE d1.date1 = Events.date3 ) AS CountInDate1, ( SELECT COUNT(*) FROM Events d2 WHERE d2.date2 = Events.date3 ) AS CountInDate2, ( SELECT COUNT(*) FROM Events d3 WHERE d3.date1 = Events.date3 ) AS CountInDate3 FROM Events GROUP BY Date3 ) 
 ;WITH CTE AS ( SELECT 'Date1' as SourceColumn, Date1 as DateValue, count(*) FROM [Events] GROUP BY Date1 UNION ALL SELECT 'Date2' as SourceColumn, Date2 as DateValue, count(*) FROM [Events] GROUP BY Date2 UNION ALL SELECT 'Date3' as SourceColumn, Date3 as DateValue, count(*) FROM [Events] GROUP BY Date3 ) SELECT * FROM CTE UNION ALL SELECT 'AllDateTotals' as SourceColumn, DateValue, COUNT(*) FROM CTE GROUP BY DateValue
Hi DW - running total of the counts. Basically, when ordered by date, the running total would tell me what the year-to-date count of that event occurrence was.
This is the perfect place to use UNPIVOT, it is so rare for me to use it. ;WITH CTE AS ( SELECT [EventID], [Date], [DateValue] FROM (SELECT [EventID], [Date1],[Date2],[Date3] FROM [Events]) E UNPIVOT (DateValue FOR Date IN ([Date1],[Date2],[Date3]) )AS BaseValues) SELECT DISTINCT DateValue, (SELECT COUNT(*) FROM CTE as C WHERE C.DateValue = CTE.DateValue and Date = 'Date1') as CountDate1, (SELECT COUNT(*) FROM CTE as C WHERE C.DateValue = CTE.DateValue and Date = 'Date2') as CountDate2, (SELECT COUNT(*) FROM CTE as C WHERE C.DateValue = CTE.DateValue and Date = 'Date3') as CountDate3 FROM CTE Here is the result using the data you provide: DateValue | CountDate1 | CountDate2 | CountDate3 :-- | :-- | :-- | :-- 1912-05-24 00:00:00.000 | 0 | 0 | 1 1913-05-16 00:00:00.000 | 0 | 0 | 1 1917-04-13 00:00:00.000 | 0 | 0 | 1 1922-10-26 00:00:00.000 | 0 | 0 | 1 1925-07-14 00:00:00.000 | 1 | 0 | 0 1931-01-31 00:00:00.000 | 1 | 0 | 0 1932-10-27 00:00:00.000 | 0 | 0 | 1 1939-05-22 00:00:00.000 | 0 | 1 | 0 1939-06-30 00:00:00.000 | 1 | 0 | 0 1945-10-02 00:00:00.000 | 0 | 1 | 0 1951-09-23 00:00:00.000 | 1 | 0 | 0 1951-12-15 00:00:00.000 | 1 | 0 | 0 1972-04-29 00:00:00.000 | 0 | 1 | 0 1975-09-02 00:00:00.000 | 0 | 1 | 0 1975-11-22 00:00:00.000 | 1 | 0 | 0 1981-06-14 00:00:00.000 | 0 | 1 | 0 1982-12-19 00:00:00.000 | 0 | 2 | 0 1984-12-29 00:00:00.000 | 0 | 1 | 0 1994-12-28 00:00:00.000 | 1 | 0 | 0 1998-10-12 00:00:00.000 | 2 | 1 | 0 2002-11-08 00:00:00.000 | 0 | 0 | 1 2005-02-26 00:00:00.000 | 0 | 0 | 1 2007-05-15 00:00:00.000 | 0 | 1 | 2 2009-09-16 00:00:00.000 | 1 | 0 | 0 2013-10-03 00:00:00.000 | 0 | 0 | 1 I really don't get the "running totals" part, isn't that a sum of the count columns?
&gt; None the less it's accurate if you are using SQL Server and it's wrong if you are using anything else what's your point? this is /r/SQL, not /r/SQLServer 
We discovered the performance impact of SET NOCOUNT when a developer noticed that code with many INSERTs in a tight loop would run with *much* better performance at work than at home over VPN. We discovered the functional impact of SET NOCOUNT when we added a trigger to a primary table in our ERP system. Those extra "(x) rows affected" messages were unexpected by the ERP system and it caused some fairly serious system errors. SET NOCOUNT ON; -- Just Use It.
Yes? And why would you loop? Of course with NOCOUNT OFF it will take extra CPU cycles to return the COUNT data. You would see an even bigger performance gain if you didn't use a hideous loop and bent your poor SQL server in ways it's not really meant to do
If you read the article you'd see the problem is related to network latency *not* CPU cycles..
Did you feel the need to inform me that remote data is transported through a network? I thought it was sent by magic and fairydust If you wrote a proper insert query without unecessary logic operations you would not ever need to think about this
And your complaints and sorry insert logic adds even less Be at peace 
Select customerid, order date,..., Row_number() over (partition by customerid, orderdate order by customerid, orderdate) as rownum Then wrap that in an outer select * From... Where rownum=1 gives you each customer's first order then check if the month/year of the order is equal to the month/year of the relevant time period using datepart or possibly a datediff function. 
It's difficult to make jokes when people take the Internet *so seriously* as they do today.. I miss the time 10 years ago 
Good luck with that
Elegant. 
...start with select null from dual; This is a basic noop - it will tell you whether the connection is being successfully established or not, returning a single row with a single null result.
Oracle doesn't support LIMIT clause. You should be using ANSI 92 and above join syntax. ANSI 89 syntax is dead, hardly anyone uses it. Distinct works on all columns in the select clause, you can't apply it to one using parentheses. Your second answer doesn't need it anyway... select r.FIRST_NAME, r.LAST_NAME, r.REP_NUM from REP r where exists (select null from CUSTOMER c where c.REP_NUM = r.REP_NUM and c.CREDIT_LIMIT = 10000) I'll leave you to sort out your 3rd query.
Seconded
Do a division by 1000, apply ceil then multiply by 1000. 
ceil(&lt;YOUR_NUMBER&gt; / 1000.0) * 1000
thank you
Thanks. I don't know why that didn't occur to me.
Your from needs to be only one source followed by a join (second table/source). Try this: SELECT OFSAPCOMMSDB_PROD.MCOMM_SODET.MATERIAL, OFSAPCOMMSDB_PROD.MCOMM_SODET.MAT_DESC_DOC, AVG(OFSAPCOMMSDB_PROD.MCOMM_SODET.NET_PRICE_USD) AS Avg_NET_PRICE_USD &gt;FROM OFSAPCOMMSDB_PROD.MCOMM_SODET Left join OFSAPCOMMSDB_PROD.MCOMM_SOHDR On OFSAPCOMMSDB_PROD.MCOMM_SODET.SALES_ORDER_NR = OFSAPCOMMSDB_PROD.MCOMM_SOHDR.SALES_ORDER_NR &gt;WHERE OFSAPCOMMSDB_PROD.MCOMM_SOHDR.ORDER_DATE BETWEEN '01-JAN-2014' AND '31-DEC-2014' AND &gt;GROUP BY OFSAPCOMMSDB_PROD.MCOMM_SODET.MATERIAL, OFSAPCOMMSDB_PROD.MCOMM_SODET.MAT_DESC_DOC, OFSAPCOMMSDB_PROD.MCOMM_SODET.SALES_ORDER_NR, OFSAPCOMMSDB_PROD.MCOMM_SOHDR.SALES_ORDER_NR, OFSAPCOMMSDB_PROD.MCOMM_SOHDR.ORDER_DATE &gt; 
For Clarity... I am logging into SSMS using my creds from DomainA. The SQL servers exist in DomainB. There is a 1 way trust between DomainA and DomainB SQL1 has a linked server pointing to SQL2. WHen logged into SQL1 with my DomainA creds, I can not query the linked server. I can log into and query each server individually, but double hop authentication is not working when using credentials from a trusted domain.
No, you can't do that. Assuming you have separate fields for UPS, FedEx &amp; DHL and they're nullable, this should work: INSERT INTO Shipping (UPS,FedEx,DHL) SELECT CASE WHEN Address1 LIKE '%UPS%' THEN Customers.Address1 ELSE NULL END AS UPS CASE WHEN Address1 LIKE '%FED%' THEN Customers.Address1 ELSE NULL END AS FEDEX CASE WHEN Address1 LIKE '%DHL%' THEN Customers.Address1 ELSE NULL END AS DHL FROM Customers But this smells like a denormalized data model. If you need it denormalized, that's fine. Otherwise, you should have this broken out into a few tables - something like one for customers, one for addresses (including the type of carrier), and one for carriers. With FKs linking customers to addresses and addresses to carriers.
insert into X (ups, fedex, dhl) select case when Address1 like '%UPS%' then Address1 else null UPS when Address1 like '%FED%' then Address1 else null FEDEX when Address1 like '%DHL%' then Address1 else null DHL END FROM Customers
You are adding too many fields in the group by. You usually only add the fields you are selecting in the group by, yours should be: GROUP BY OFSAPCOMMSDB_PROD.MCOMM_SODET.MATERIAL, OFSAPCOMMSDB_PROD.MCOMM_SODET.MAT_DESC_DOC 
Unfortunately, I did not create the data model, it just falls to me to fix the mess that was made of it. In any case, thank you for clearing that up, that clarifies a lot. 
It looks like you are missing something to join those tables together. Can you give us more of an idea of the tables' structure? Are you trying to get a view that tells you how many rows are in your tables? 
First, don't use comma joins. The INNER JOIN syntax was introduced in 1992 because comma joins are hard to read, hard to maintain, and when you want an OUTER join you had to use vendor specific syntax. Second, use some table aliases. That also makes your query easier to read. That said, you're getting duplicates because you're grouping by more columns than you're selecting. You only have to GROUP BY the fields that actually appear in the SELECT clause that aren't in an aggregate function. The query engine doesn't need to group fields in the JOIN or WHERE clause. If you want to see why it's acutally duplicating, run this and it should be obvious: SELECT hdr.ORDER_DATE ,hdr.SALES_ORDER_NR ,det.MATERIAL ,det.MAT_DESC_DOC ,AVG(det.NET_PRICE_USD) AS Avg_NET_PRICE_USD FROM OFSAPCOMMSDB_PROD.MCOMM_SOHDR hdr INNER JOIN OFSAPCOMMSDB_PROD.MCOMM_SODET det ON det.SALES_ORDER_NR = hdr.SALES_ORDER_NR WHERE hdr.ORDER_DATE BETWEEN '01-JAN-2014' AND '31-DEC-2014' GROUP BY det.MATERIAL ,det.MAT_DESC_DOC ,hdr.SALES_ORDER_NR ,hdr.ORDER_DATE If you just want to know the overall average by material, then just modify your GROUP BY: SELECT det.MATERIAL ,det.MAT_DESC_DOC ,AVG(det.NET_PRICE_USD) AS Avg_NET_PRICE_USD FROM OFSAPCOMMSDB_PROD.MCOMM_SOHDR hdr INNER JOIN OFSAPCOMMSDB_PROD.MCOMM_SODET det ON det.SALES_ORDER_NR = hdr.SALES_ORDER_NR WHERE hdr.ORDER_DATE BETWEEN '01-JAN-2014' AND '31-DEC-2014' GROUP BY det.MATERIAL ,det.MAT_DESC_DOC 
Thanks for the post! I am fairly new and had never really found a reason to use exists, but I run these left outer join queries frequently. I just set statistics on including io and the improvement there is incredible, too--in one test on an indexed column, my logical reads went down by a factor of three. This is some pretty crazy performance improvement! Out of curiosity, is there a case where utilizing exists is potentially worse, and not necessarily in terms of performance (i.e. are there cases where a left outer join with null vs where not exists would create different results sets?) Thanks again!
Probably don't want to LEFT JOIN and then specify MCOMM_SOHDR.ORDER_DATE in the WHERE clause. That's an implicit INNER JOIN. It also probably doesn't make sense to outer join the header table to the detail table, anyhow. Usually the header is exactly one, and the detail table is 0, 1 or many.
Good tip, thanks!
We can only assume by Robotcell you mean one of these https://www.robots.com/faq/show/what-is-a-robot-cell &gt;I have sort of understood that it is possible to make an application like the one I want in Visual studio. However, I do not understand what the end product would look like. Will the end product just be a program running in the background doing what it should do? Is it something running on the server? Is it a program with a UI? I have some problems visualizing what the end product should look like. It depends on the integration with the PLS software. **Will you be developing the PLS software?** **If so**, simply add a reference to a SQL library (dlls) that will load a SQL driver and accept a connection string. After that you are basically sending command strings like "INSERT INTO dbo.RobotMovement blah blah blah my data" to the SQL Server database. You are now inserting records into SQL from inside the PLS software. **If not**, you need to get the data from the pls software. Does the pls software have a software development kit? A software development kit which may allow you to create a (console) application that talks to the instance of the PLS software running. Similar to how I mentioned earlier we had a reference to the SQL Library, but we'll also add a reference to the PLS software development kit (dlls). You can then have your application act as an in between application that makes calls and monitors changes or events in the PLS software and then updates SQL just as we did earlier "INSERT INTO dbo.RobotMovement blah blah blah my data blah blah". **If you are not writing your own PLS software nor does the PLS offer a software development kit for interfacing**, does the PLS software may log data to files? You'll have to write an application to periodically pick up log files and parse the data in them *(hate to say it but these systems usually create malformed logs but it can be done)*. You'll parse the logs into separate sql commands and just like earlier, you'll add a reference to a SQL library and send commands to the SQL Server Database. If the PLS software does not have any of those options you are kind of screwed. &gt; I have been in touch with an expert who is helping me, and he proposed to use table adapters. You could use table adapters. However, depending on the flavor of SQL and if you are required to write the software in Java as opposed to c#, you may not have access to table adapters. &gt;To provide communication with the database. I have tried googling for solutions and have looked at some things, like LINQ to SQL for example. But, I don’t fully understand where I should start this sort of thing. Any tips on where I should start? LINQ is an awesome tool in many languages but wont help you get data into a SQL server. LINQ allows you to query objects in c# similar to the way you query objects in SQL. It makes very concise and readable queries rather than looping through objects and their children with if/then statements that obfuscate the programmers original intent. LINQ is very readable and saves time in development and support.
Good answer. This is what I see in my environment. You can query msdb to see if there are snapshot backups This will show you all the backups on a server in the last 24 hours. Look for a 1 in the in_snapshot column and a 0 in the is_copy_only column SELECT CONVERT(CHAR(100), SERVERPROPERTY('Servername')) AS Server, bs.user_name, bs.database_name, bs.backup_start_date, bs.backup_finish_date, bs.expiration_date, CASE bs.type WHEN 'D' THEN 'DB Full' WHEN 'L' THEN 'Log' WHEN 'I' THEN 'Differential' ELSE bs.type END AS backup_type, bs.is_snapshot, bs.is_copy_only, bs.backup_size, bmf.logical_device_name, bmf.physical_device_name, bs.name AS backupset_name, bs.first_lsn, bs.last_lsn, bs.database_backup_lsn, bs.checkpoint_lsn FROM msdb.dbo.backupmediafamily AS bmf INNER JOIN msdb.dbo.backupset AS bs ON bmf.media_set_id = bs.media_set_id WHERE (CONVERT(datetime, bs.backup_start_date, 102) &gt;= GETDATE() - 1) ORDER BY bs.database_name, bs.backup_finish_date DESC 
The problem with your situation is that you never want to alter schema. So you can't have a conditional AS. [If you think about it, it's literally impossible](http://i.imgur.com/d6EzmBI.png) You can transpose your data and give you multiple column outputs. SELECT ID, Name, Address1, CASE WHEN Address1 LIKE '%UPS%' THEN 'UPS' WHEN Address1 LIKE '%FED%' THEN 'FED' WHEN Address1 LIKE '%DHL%' THEN 'DHL' ELSE 'NONE' END AS SHIPPER, CASE WHEN Address1 like '%UPS%' THEN Address1 ELSE NULL as UPS, CASE WHEN Address1 like '%DHL%' THEN Address1 ELSE NULL as DHL, CASE WHEN Address1 like '%FED%' THEN Address1 ELSE NULL as FEDEX FROM Customers Your output will be like this http://i.imgur.com/iwArr7H.png 
Exactly what I needed, thanks! :)
How you are going to load the data into the database depends on what format is your data source is in? Check this out for list of online databases. https://en.wikipedia.org/wiki/List_of_online_databases
"use exists always given these queries and this database" would satisfy you? I agree of course, but I feel that this is stretching the hand holding a bit. Anyone who is capable of understanding the article also understands what you say implicitly. And absolute beginners have a lot more problems.. 
While some people tend to cleverly avoid `NOT EXISTS` in favour of the exposed `LEFT JOIN` solution, they often also use `(LEFT) JOIN` instead of `EXISTS` [which is often just wrong](http://blog.jooq.org/2016/03/09/sql-join-or-exists-chances-are-youre-doing-it-wrong/).
How did they get around the problem of mobile units not being on the same network or dmz as the reporting server? I'd hate to have to use vpn to get mobile reports working 
Actually, all the data is cached once initially connected so reports can be generated in offline mode. I saw the 2016 presentation release Thursday.
Okay my phrasing wasn't good, it's true about it not being a product problem, our initial issue with it is that if we had public facing ssrs, then we just moved the issue in food chain, then it wouldnt be mobile units inability to contact the ssrs service, it would be the ssrs services inability to get data. As vpn is for units but not for system users, would have to punch a hole in the firewall, which is not permitted . I sound like an idiot, I'll let the comment stand for a bit I think. 
I'm curious about why and how optimization is such a controversial topic? Are most of you DBAs? As a 'power user' that works in analytics... I pretty much have my own server. Most of my work is one-off analyses where I normally weigh the time it would take to rewrite something efficiently against how long it will take to execute in the first place. Our DBA's and admins used to give me dirty looks for writing things that would take hours and hours to run, or which could crash the server. I've improved a lot over time as I've learned how mad SQL is compared to something I'm more intimately familiar with such as COBOL, but that's one of my fundamental core complaints with SQL. My attitude has always been along the lines of, "if a query can crash your server, then your server sucks, I need something better," or, "why can I get this to execute on my own home Linux server with cheap commercial available SSDs, but your commercial Windows blade server is running out of space, taking longer to execute, etc." [This](https://www.reddit.com/r/SQL/comments/33ixsi/question_about_how_a_query_executes_with_multiple/) post does a fairly good job of illustrating what I'm talking about. The code is very straightforward from my point of view as a programmer, but it just fucks SQL up. It can be easily solved by simply breaking the join into multiple lefts and then unioning the columns into one. Mathematically I can understand how this limits the maximum size of the set and therefore dramatically increases execution time/space required, but at the same time I feel like the server client should also know this and not punish the user (in this case a new user) by requiring this intermediary step. Even still, with some complaining I was able to lock the server and resources down for 16 hrs or so and run it, and it ran fine. To be perfectly honest with that project I spent close to 16 hrs trying to solve this problem. From a job performance standpoint, if I were a manager, I don't think I'd want my analyst spending his or her time like that... but then perhaps a proper environment would have a layer of DBA's assisting in the development of complex queries but that would only increase time/money as well.
With SQL server you can just schedule a job go run a stored procedure. I don't have any experience with teradata or Oracle though unfortunately. I'm sure both have their own scheduling assistants though.
Nah, you don't sound like an idiot at all. Your concern makes sense. I think everyone who needs to do this has different requirements and security needs. In some cases a reverse proxy through the firewall might make sense, for others a replicated environment in a DMZ where you are exposing a copy of the data might be the right choice. 
I work in finance and we do extremely heavy aggregation and transformation jobs. The time of the day simply runs out before everything is done unless we make very performant queries 
MySQL has an event scheduler and SQL Server has a job scheduler.
Another poster recommended the Adventure Works database, which is awesome. I don't know how "big" you want to go - but here is a post to grow some of the tables in the DB. http://sqlblog.com/blogs/adam_machanic/archive/2011/10/17/thinking-big-adventure.aspx Good luck!
Dude...the bigger the better. Thanks. Absolutely gonna check it out.
That's much better. What was OP's link then, a summarised version? 
Not a bad article, but I'm not at all a fan of even showing comma joins anymore. Simply put, that syntax should never be used for teaching. And I'm similarly against `JOIN ... USING` for much the same reason that `NATURAL JOIN` is bad. Encouraging the use of potentially ambiguous syntax is just a bad idea.
While not the focus of this article, it's worth noting that some RDBMSs (for example, PG^*) will optimize `SELECT...FROM tbl1 LEFT JOIN tbl2 WHERE...IS NULL` to the appropriate anti semi join. ^* Annoyingly though, PG won't optimize `WHERE col IN (SELECT...)` to the appropriate semi join though, which I've never understood.
"Windows NT CRT DLL" is usually `C:\Windows\System32\msvcrt.dll`. It's the Microsoft Visual C++ runtime library for pre-Visual Studio 2005 software and it ships on all versions of Windows. This particular one is not one of the supplemental runtime libraries a core OS file. It's also possible that it's the same file in a different location. Possible causes? An unhandled exception in application code or disk corruption would be my only guesses. You haven't given enough information to say any more. 
There are a couple of things you can do here. Firstly, make sure that row is as NARROW as it can possibly be. So, instead of storing a whole DATETIME value on every row, consider a seperate TIME field, with a key to a 'Day' table. That key could potentially be much thinner that a DATE value, if you dataset will contain a limited number of days, so TINYINT or SMALLINT might do. Make sure your Temp and Humidity columns are as tight as possible. If you know the valid range of you data will be quite small (e.g. the range of temperatures possible for what you're measuring) then you can use a 'thinner' data type. Only store the precision you are going to use. So for example, instead of keeping a FLOAT, you might use an integer with a scaling factor and offset. (EDIT: Don't use decimal - most wasteful of the numerics!) (EDIT: extra para) Furthermore, you can mess with FILL FACTOR settings of the table and any indexes to eliminate any free space. you know that your data are arriving in order, and there will be no need to insert data out of order or modify it within the same page. This will allow your DB engine to get at more of your rows in every read. Now you have nice tight base data, you can begin to limit and aggregate. I've just seen the 'choose 1st minute' comment - that is a great way of sampling ever 'n' row for use. You could also use an average() over every minute, rolling 5 minutes etc. Now calculate this and store it in another table - that will be the source for your graphing and analysis. 
With all the 'this is the fastest variant of query X' posts lately, I thought it would be worth one on WHY we should NOT HAVE TO rewrite queries for performance, and why we SOMETIMES DO. 
OK that's cool - that's exactly the kind of scenario where this type of thing is neccessary. PIVOT is your friend!
Automate the crap out of everything, have a reliable person as your backup so it's not 100% on you (which also means you **document** everything), and configure your alerts to only alert when there's a **true** emergency. Proactively manage your environment so that things that could generate an alert don't happen.
Thanks for the reply! It's a bit above my SQL level though. Some things you say make absolutely no sense to me, sorry! I did try and make the columns as small as possible which I think worked. I will look into the datetime thing, was thinking about splitting date and time to separate columns, would that make any sense? I ended up using a few views based on /u/r_kive his post with various date ranges. This works as it seems the bottleneck was the fact that the initial query returned so many rows the data alone was about 5MB. It had to load that for every object (graph etc) which took some time. Now quering the views returns way less data so it's much faster.
I had hoped to pop in some examples but formatting on mobile is a nightmare! Splitting date and time makes sense most when you wish to compare time-of-day values over many days. In my prior comment it was m ore to save a few bytes per row, but both purposes are served. 
He wasn't using them *at random*, and neither were *you*.
If I TOLD you then I'd have to KILL you.
I don't see why not.. Give it a try!
SELECT LCASE(Comment) As LComment FROM tblReddit WHERE PermaLink = 'https://www.reddit.com/r/SQL/comments/4a9q78/rewriting_sql_for_faster_performance_and_why_you/d0yijym'
yeah... that's what I figured a large company would do. We're a smaller company (100-200ish people....maybe 20 people outside of our tech team writes queries, I'd guess 12 months ago that number was....less than 10. I'm in the process of writing a wiki for some of our other data systems right now, and started thinking about the SQL issue when someone asked me to teach them some basics of SQL and i started thinking about all the corner cases I've learned over the past few months, stuff that isn't written down anywhere. 
Nailed it! Nice. No other comments needed. This thread is closed. Move along folks, nothing else to see here.
Im looking at code academy and lynda atm. Have you heard of any from your peers?
Tableau is a great tool, but it can also be prone to some of the issues you're having. Tableau users still need to have an idea of how to interpret the data, what fields to join on, what filters are needed, how to handle NULLs, etc. It's a good tool for business users, but it won't solve the problems the problem of missing documentation. If anything, Tableau makes it really easy to write a report with bad numbers.
&gt; What is SQL? Structured Query Language. A standardized method for accessing data in a relational database. &gt; I am looking to make a career change to a financial analyst role and see them asking for SQL experience. What is the best way for me to learn SQL? How do you learn best? There are youtube video series, there are online classes available, there are books and of course real live classes. There are free downloadable database's (and query tools) you can put on your home machine to just do some free form learning by trying various things without instruction. What's your current role, what experience do you have and what tools are you already familiar with?
I don't know if that actually is a problem of the DB, or just something that new users need to know.
Would that also include some sort of examples or things you should know, like "if you want to count processed ordered, make sure you use these filters and these joins, instead of just looking at the count of orders_ids for the day."
I am an Oracle Applications DBA, and it's a constant challenge to get people to extract the correct data from the ERP system. This DB was designed by Oracle so of course it is very standardized in 3NF. There are a lot of tables. The average meaningful query uses about 10 tables or more. As you can imagine, this gets people into trouble, especially because the business users/analysts do not understand joins. A lot of them use inner joins only and they drop off a lot of data unintentionally. They are also not careful with their filters (WHERE clause). They drop off a lot of data because they didn't realize what they were limiting. Example, a user wants to eliminate a transaction type called "Misc" so they say NOT LIKE M% which eliminated all other transaction types starting with M, but the user never realized that there was a transaction called "Move Order" and they inadvertently excluded it. Even though we have a formal data dictionary as provided by Oracle, I publish and maintain a "cliff notes" version for my users. In a sea of literally 1000s of tables, they only really need to use about 20 of the main base tables, and some of these tables have 300 columns, but they probably only need 5 of them. I've simplified the problem by linking in these tables over ODBC via Access. That way they can drag and drop tables and use the readable cliff notes version of the data dictionary to know what to join on, when to left join to the table, etc. On my version of the data dictionary, I list a lot of tips, for example "If you need x, please include y and set it to 0". Or I say "If you join Purchase_Orders to Issued_Checks, change the join to left join because we might have Purchase Orders that have not been paid yet". I try to use common language and not speak in terms of set theory. The bottom line is that users HAVE to know the data, and they have to understand where the data can cause trouble when using the data for decision making and especially reporting. Blindly connecting tables and betting on data is NEVER good enough. So if I were you, I'd write a wiki and host a recurring training session where you talk about where data comes from, what joins do, how joins and filtering can get users into trouble, etc. Education and practice is what it's all about. Too often I get the finger pointed at me when a user was called out on bad data that they had presented in some meeting. But when looking at their query, it always does exactly what the user defined it to do: Not show the whole story! It all comes back to the old saying "Computers don't do what you want it to do, only what you ask it to do" 
Yep.. that's what sample queries in step 2 should cover.
ELI5: The database is mommy. Mommy has learned a lot of things in her life. When you want to know something, you ask mommy a question. The way you word your question is important in order to get the answer you are looking for. SQL (Structured Query Language) is the language for the questions you ask a database.
You can create views for them with simplified versions of tables. Your best bet, imo, though, is to have people QA each other's queries. Also, get them training, and make yourself available to help them write queries until they get the hang of it.
Depends... you can use the databases own scheduling tool (each one usually has something that'll do this, SQL Agent for SQL Server, Oracle Scheduler or Teradata Query Scheduler) or you can use an external scheduling tool for all of them if each one has a way to run a query from the command line (SQL Server has iSQL, Oracle has SQL-Plus, teradata looks like it's tdsql) and then run things against all databases from a single scheduling tool. Which you use depends on your priorities and needs. Using the native scheduling tools will likely give you more robust error reporting.
Do you know what database software your company uses? There are several free database tools &amp; databases, but if you can get a free version of what you company uses you will be an extra step ahead.
I'll ask my IT department. But im looking to use SQL as a way to enter a new company as a Financial Analyst.
Okay great thanks. Website is blocked at work, I'll give it a shot when I get home. How much variation would you say there is between all the variants of SQL? Btw thanks for the help, I really appreciate it.
Awesome thanks. Now to just sit down and learn it. thanks for help again 
Those are both good resources. I'll add: http://sqlzoo.net Just be careful with any in person class or "camp" because they can be very costly and often will task you with quite a bit of busy work that focuses on database management vs. straight querying a database using SQL. If you learn better in an academic environment I know that Stanford, Harvard, and MIT have database courses you can take for free online (lectures/notes/quizzes are all posted publicly).
I just learnt it on the Job. I had a brief introduction and whenever I had a problem I just asked one of the experts. IMO, the most difficult part is where exactly to find the stuff you seek, that is in which goddamn table did they put the info. It's not how to write a query.
Yeah, never tell a DBAs that their data is "wrong". They will turn on you like a pack of rabid dogs. "The business group input it incorrectly" or "there is an unexpected result set when we make this query" 
luckily i said it to a admin that was a friend in person and not by email.
Do you know which courses specifically? 
Write that kind of stuff as stored procedures, then document it.
You said elsewhere in this thread that you use Excel. I'd like to build a more detailed answer on that, despite that this is ELI5. In the way that excel has an equation system, where by example `sum()` roughly means what math calls ∑, databases have a query system. The purpose of the equation system in excel is to allow you to express complex math which the machine will solve for you, according to other information that it has (like when you sum the contents of a column.) Query systems are similar: they give the database a question to answer, according to the information it has. SQL does not operate on the metaphor of 2d sheets of cells, but rather named tables full of data, which can be correlated arbitrarily. Otherwise, though, it's actually pretty similar to what you expect. So like, you might make a "table of employees," a "table of jobs," and a "table of locations," in order to represent the organizational structure of a company. Each table is a series of columns (first name, last name, location they're at, salary, and so on, for the employee table, for example.) SQL is a way to ask the database questions like "would you please tell me the top 5 salaries at this location" which might be something like select * from employees join locations on employees.location_id = locations.id where locations.name like "Pittsburgh" order by salary desc limit 5; SQL is where all those things, like `order by` and `where ... like`, are defined. This allows a bunch of different databases to speak (sort of) the same language to outside software, which increases replaceability, modularity, and competition, which is good for everyone. As far as programming languages go, SQL is one of the harder ones to learn, because each of the different databases does a varying quality of job sticking to the standard. My opinion is that you would be best off picking one, getting a book about it, and sticking to that one for six months while you get comfortable. PostgreSQL and MySQL are both free and powerful, and good choices. Both are well explained by sites like "Use the Index, Luke," slide shows by Bill Karwin and Joe Celko, and so on.
&gt; ANSI SQL ... but the core should be identical no matter where you go. you seem to be new to earth databases postgres, oracle, and mysql have three different and incompatible approaches to basic things like aliasing columns and tables select from dual is required in oracle, banned in mysql, and means something different with and without in postgres (neither what it means in oracle) sql makes javascript look coherent and universally well implemented
Oracle DBA: Spotted.
lol - actually I *much* prefer Sybase MySQL, and even Ingres/Postgres on functional grounds, but since Oracle bought MySQL, this is a MySQL thing too...
Whoa, weird behavior in MySQL! SELECT 1 FROM DUAL; -- "1" SELECT * FROM DUAL; -- "Error: No tables used"
Plausible. I've regularly used SQL Server, Informix, Oracle, Sybase SQL and DB2, I haven't touched postgres and only dabbled in mysql. Aliasing in the databases I'm familiar with works just fine with AS syntax, which is the way I'm used to using it. I'm a little annoyed by oracles's from dual where I can just select in SQL Server but it's not a huge issue for me so I don't notice it.
KhanAcademy provides a pretty engaging introductory course. It's free. Search "Intro to SQL: Querying and managing data" on their website. 
In MySQL you can definitely just "select 1;" This works fine, yet if you tried to "select *;" you would expect the error. The problem with "dual" is that it's a nice band-aid in some situations, but not all. One could almost argue the same about "select *" in all dialects, but I think that's a harder case to make.
Well, I never said ours was up to date ;) Hell, some of our data goes back to the 90s in some convoluted lookup application that runs on our mainframe (yes, mainframe... we still run z/OS on actual mainframes). For the commonly used tables, however, the reference data is usually pretty good.
Do most places have legit documentation? I switched shops a few months back and am quite frustrated that there is 0 documentation. Is this normal? Previous shop had some documentation, but not a ton. The upside was there was a legit training program. Current employer was/is lacking there.
Try this web site http://www.studybyyourself.com/seminar/sql/course/?lang=eng. There you can submit exercises and get a feedback right away. It is free and they provide the solution in case you cannot solve one exercise.
You need to create a data dictionary that explains how the data model relates. Explain what values etc mean and how tables should be joined. 
The official term you are looking for is Database Normalization. The two primary reasons are to reduce redundancy in the data and also speeds up performance. Let's use a quick example for a DVD. Let's say you want to track the information contained in a DVD, specifically the Title, Director, Year, and all the actors. If you tried to do this one one table, and there were 10 Actors, you'd have 10 records for each actor and the Title, Director, Year would all be duplicated. So the same piece of data is duplicated 10 times. Now imagine the Year is wrong, and needs to be updated. You have to update it 10 times. The "proper" way to do this is to split the data into two tables: DVD and Actors. The fields in these tables would be: **DVD:** dvd_id Title Director Year **Actor:** actor_id dvd_id ActorName The Title, Director, and Year values only appear once, on a single record, in the DVD table. The dvd_id is a key field, and is used in the Actor table as the foreign key. The dvd_id is the only data that appears multiple times, in the Actor table. This is the fundamental concept of normalization. It's important to note that Database Normalization has several levels (called forms). Databases generally conform to a certain level depending on the intended use. Getting into large data structures and this model actually goes out the window since it isn't as efficient in certain circumstances (data warehousing, for example). [Here's a good link that gives a more deep dive into it with examples.](https://support.microsoft.com/en-us/kb/283878)
Sure thing. To take it a step further, normalization is typically done in OLTP databases (On Line Transaction Processing). An OLTP database is typically the production system that has constant transactions being posted. Think of a Point of sale system at a store, the ordering system at your favorite online web store, etc. The way you would query this data WITHOUT normalization would be: SELECT * FROM dvd On the other hand, in my example with one level of normalization, you'd have to do a little extra work to bring the two data sets together: SELECT * FROM dvd JOIN actor ON dvd.dvd_id = actor.dvd_id In most cases, reporting databases (where data is primarily read only and used for business intelligence or other reporting purposes), the data is usually flattened for ease of use. That means data is redundant, but requires less traversing the database schema to find it which improves the speed of reporting. 
Because they use it via a Semantic Map stored in the BI tool.
Yes, as long as your definition of 'First' and 'Last' is 'In Date Order'. If it is first/last to arrive in your system or some similar logic then that is something else. 
To expand on this... The film_category and film_actor tables are *bridge tables*, they are used to model a many-to-many relationship. M2M relationships can be tricky to work with, you can very easily end up with a quite a bit of redundant data, which can lead to anomalies in an OLTP system if the app layer doesn't properly account for it. With a bridge table, you create that relationship into a table such that the logical relationship is 1-M, M-1 while the modeled one is an M-M. Also, I wouldn't really say normalization increases performance...quite the opposite in fact. It introduces complexity which the engine and the developer both have to deal with. Normalization is about trading performance for consistency. Incidentally, this is why warehouses are denormalized: They don't have to worry about consistency (a warehouse is populated through ETL processes, which pull from an application/OLTP database or an ODS which has already vetted the consistency of the data). The higher the degree of normalization, the harder it's going to be for someone to introduce inconsistent data into your system. I'm also not so sure about the DVD/Actor schema Chris_Pdx modeled above...unless he has another table in which he's keeping actor demographics, aside from the DVD link, he's going to have actors repeating and will have introduced a vulnerability for inconsistency here (i.e., people like me who get "Jack Nicholson" mixed up with "Jack Nicholas" on Mondays before I have enough coffee might use both names to indicate the same person). To that end, the model you have pictured is certainly the best way in terms of normalization. And fwiw, even in a dimensional warehouse, I'd usually implement a similar bridge structure when dealing with an M-M relationship like this. Most of the OLAP tools out there will be able to understand this in a pretty efficient way, so there really isn't a need to denormalize it, especially since it isn't going to hit you very hard in terms of performance. Anything you gain there might well be lost by the massive increase in the sizes of your indices, which the engine now has to sort through. 
There might be a more complicated way to arrive at the solution, but I think you might have a solid entry or an obfuscated SQL contest right there.
&gt; The concept/reason to having to create essentially two tables to be able to connect say multiple film categories to one film in another table and same for having two actor tables for multiple actors in one film in this example database for a dvd rental store. * Multiple film categories in the Category table, linked to a Film table via a linking table film_category. This is what is used when you have a many to many relationship. Any given film can be in one or more categories, so you can't just have two tables with a single category field in the film table. That would lave you no way to represent a film that is both a Romance, and Comedy. Or Western, Drama, Action, and Science Fiction (cowboys and aliens). In the other direction any one of those categories is associated with many films. * two actor tables, or more appropriately an Actor table, a linking Film_actor table and the main Film table. Again you've got a many to many relationship. Daniel Craig is in Cowboys and Aliens, Skyfall and Star Wars: Episode VII - The Force Awakens to name a few. In the other direction each of those movies have many more actors. 
 SELECT forum_posts.sub_id, Max( forum_posts.post_id) FROM forum_posts GROUP BY forum_posts.sub_id Unless you assign the post_id when the poster begins composing (which burns unique IDs every time someone writes out a message and hits cancel, so would be poor design) In which case you actually do get a tiny bit more complicated. SELECT forum_posts.sub_id, ( SELECT MAX( latest_posts.post_id) -- Get the highest post ID, since you could theoretically have multiple posts at the exact same publish datetime FROM forum_posts latest_posts WHERE MAX( forum_posts.date_time) -- terrible column name, should be published_dt, or saved_datetime if you want to be verbose = latest_posts.date_time - for the most recently publised post AND forum_posts.sub_id = latest_posts.sub_id ) FROM forum_posts GROUP BY forum_posts.sub_id 
Reading up, it looks like you're using the values syntax from conditional multi table inserts though you are trying to do an unconditional multi table insert. You *could* use something like INSERT FIRST WHEN 1 = 1 THEN INTO t1 (id, l_name) VALUES ( employees.employee_id, employees.last_name) WHEN 2 = 2 THEN INTO t2 ( id, l_name) VALUES ( employees.employee_id, employees.last_name) WHEN 3 = 3 THEN INTO t3( id, l_name) VALUES ( employees.employee_id, employees.last_name) SELECT employees.employee_id, employees.last_name FROM employees; But I don't know why you'd want to.
 SELECT ObjectTable.ObjectID, ObjectTable.[Description] FROM ObjectTable WHERE EXISTS ( SELECT 1 -- Is there an active attribute 4? FROM AttributeTable WHERE ObjectTable.ObjectId= AttributeTable.ObjectId AND AttributeTypeID = 4 AND IsActive = 1 ) AND NOT EXISTS ( SELECT 1 -- are there NO other active attributes? FROM AttributeTable WHERE ObjectTable.ObjectId= AttributeTable.ObjectId AND AttributeTypeID != 4 AND IsActive = 1 )
If I'm understanding your question correctly you need a Where condition (where AttributeTable.AttributeTypeId = 4 and AttributeTable.IsActive = 1. Like this: select ObjectTable.ObjectId, ObjectTable.[Description], AttributeTable.AttributeTypeId, AttributeTable.IsActive from ObjectTable left join AttributeTable on ObjectTable.ObjectId= AttributeTable.ObjectId Where Attributetypeid = 4 and IsActive = 1. This returns only 2 rows for Object 1 and Object 2 - both having AttributeTypeID of 4 and IsActive set to 1. 
Yep.. I scrolled right past that. I don't know why you'd need the values clause if you're adding the same thing in each place, but it doesn't seem to hurt.
Ahh i see, tbh i'd probably have gone with 'timestamp' which is no better :D but that just happened to be the column name from a shoutbox script that i used to understand the priniciples and stuck with that rather than use 'date_time', 'datetime' and 'timestamp' i just carried on using that one as a uniform label. Sounds like i need to do an update on my end, im still using a 2yr old version of Xampp for local testing, and was just saying to someone i could do with getting this project done and then upgrade and see whats faulty (something will be!) and then fix it. The site will only be used by a dozen or so people and doubt it'll get much traffic beyond that either, so ive got time to go back and tidy up when its done, but i need to get it up in a little over a month before our current hosting ends. Okay, i'm now googling temp tables :D I seems like it'd be easier to just use the previous results as array data, and then use that to do a search where id = x, or id = y, or id = Z, and i cant imagine it'd be any (or much) difference in terms of performance hit. In this particular instance, its an insignificant site and wont ever be dealing with many requests from different users at the same time, but it obviously makes sense to do things properly if possible. Thanks for your time and help. oh, whats the column name for the result, surely i dont have to call it by ( SELECT MAX......) etc? n/m [0] &amp; [1] etc will do :)
I hear that a lot from people at work but honestly I don't see what's difficult about the approach. 
That's exactly what I needed - the challenge was that I didn't know how to set up the where WHERE clause correctly. /u/D_W_Hunter showed me in his example and now I have a new topic to read up on, select 1. Thank you for the help!
An inelegant, but effective, solution could something like: SELECT F1.friends_friend FROM (SELECT * FROM friends WHERE friends_id = [id_1]) F1 INNER JOIN (SELECT * FROM friends WHERE friends_id = [id_2]) F2 ON F1.friends_friend = F2.friends_friend This will restrict your query results to the intersection of the set of friends of your first user, and the set of friends of your second user. There are likely more efficient solutions.
A more streamlined approach would be: WITH Studennt_Enrollments AS ( SELECT DISTINCT StudentID , EnrollmentID , CourseID FROM Enrollments ), Points_Earned AS ( SELECT EnrollmentID , SUM(PointsEarned) AS 'PointsEarned' FROM AssignmentGrades GROUP BY EnrollmentID ), Student_Points_Earned AS ( SELECT a.StudentID , a.CourseID , SUM(b.PointsEarned) AS 'PointsEarned' FROM Studennt_Enrollments a INNER JOIN Points_Earned b ON b.EnrollmentID = a.EnrollmentID GROUP BY a.StudentID , a.CourseID ), Points_Possible AS ( SELECT CourseID , SUM(PointsPossible) AS 'PointsPossible' FROM Assignments GROUP BY CourseID ) SELECT x.FirstName , x.LastName , x.CourseID , y.PointsEarned / z.PointsPossible FROM Students x INNER JOIN Student_Points_Earned y ON y.StudentID = x.StudentID INNER JOIN PointsPossible z ON z.CourseID = y.CourseID Also took longer to write. Using and indexing temp tables would be the fastest way to accomplish the task.
Can you provide more specific column information? I cannot understand this description. 
&gt;More importantly queries like this are maintenance nightmares because it takes someone unfamiliar with the overall objective a huge chunk of time to figure out what the hell you're trying to do before they can even begin to fix the reported problem with it. That's why I document my code in a format such as: --Section 1: See subquery a. Section compiles a distinct list of all students, courses, and enrollments. FROM ( --Section 1: START HERE SELECT DISTINCT StudentID , EnrollmentID , CourseID FROM Enrollments --Section 1: END HERE ) a
&gt; I've seen it as SELECT *, I've seen it as SELECT 'X', I've seen it as SELECT 0. It's a programmer idiosyncrasy, I'm not aware of any positive argument for one thing vs another. I think SQL Server is smart enough to optimize them all to the simplest (fastest) method regardless.
Ok. A couple of things I left out. We should alias the max(date_time) statement (so write " as latest " just after it. This gives that column a name we can use. We also need to alias the select statement ( so write " as x1" directly after the closing.parenthesis). Now we can join the select back to forums_posts. What you've missed is joining both the sub_id and the date columns, so it will now be: Join: (Select max(date_time) as latest, sub_ID from forums_posts group by sub_ID) as x1 INNER JOIN forum_posts on forum_posts.sub_id = x1.sub_id and forum_posts.date_time = x1.latest The inline select is giving us each sub_id and its latest post time, and were using the join to restrict the forum_posts table. Then we can join on the sub forums table knowing that the forum_posts table will only return the rows for the latest. 
i'll definitely use the relabelling bit, i think ive got everything how i need it now, and ive thrown in some extra dummy threads &amp; posts to make sure, and its looking fine. I've packaged the sql array data slightly differently, not sure it'll make much difference, but i didnt like each row being its own array, meaning its arrays in an array, i'd rather have it as 1 array... maybe i'll go back but im not seeing any downside. Thanks for the help
That stinks. Well all you are going to have to do in this case is make sure you insert in the correct order. Ironically with this data model you can create ratings independent of movies, but the sequence that makes the most sense is to populate movie and cast, then MovieCast and Ratings, then associate the rating with the movie. What are the possible ratings? Are you supposed to have like 1-10 and then you can associate them to multiple movies? That is what your schema (which is terrible) suggests.
SQL query: Documentation UPDATE employee SET Emp_CreditLimit = Emp_CreditLimit * 1.08 WHERE Emp_CreditLimit &lt; (SELECT AVG(Emp_CreditLimit) FROM employee as E) MySQL said: Documentation #1093 - You can't specify target table 'employee' for update in FROM clause Hmm maybe it's something to do with MySQL? Thank you for your help!
So given variables Session_user Another_user You want a list from this table of those people that are friends with both of these people? SELECT friends.Friend_id FROM friends WHERE EXISTS ( SELECT 1 -- this person is friends with the session user FROM friends Ses WHERE Ses.friend_id = Session_User AND Ses.friends_user = friends.friends_user ) AND EXISTS ( SELECT 1 -- this person is friends with another user FROM friends Another WHERE Another.friend_id = Another_user AND Another.friends_user = friends.friends_user ) Assuming friends_friend is friends of friend_user, it's a really bad idea to have here. The whole table design is going to create more headaches than anything because any person can be friends with more than one person. Suggestion CREATE TABLE person ( Person_id int(11) NOT NULL, First_name VARCHAR (20) NOT NULL, Last_name VARCHAR (20) NULL, -- in case you don't know their last name Open_to_friendship VARCHAR(15) NOT NULL DEFAULT 'enabled' ) ENGINE=InnoDB DEFAULT CHARSET=latin1; CREATE TABLE friendships ( Person_ID int(11) NOT NULL, Friend_person_id int(11) NOT NULL, Friends_as_of datetime NOT NULL ) SELECT Person.Person_id, Person.First_Name, Person.Last_Name FROM Person WHERE EXISTS ( SELECT 1 -- this person is friends with the session user FROM friendships Ses WHERE Ses.Person_id = Session_User AND Ses.Friend_person_id = Person.Person_id ) AND EXISTS ( SELECT 1 -- this person is friends with another user FROM friendships Another WHERE Another.Person_id = Another_user AND Another.Friend_person_id = Person.Person_id )
I don't know the schema but lets talk this through. You will need to do the following: 1. Select all ID's and all names together. 2. Select the most recent purchase for each id. 3. Select the sum of all purchases. 4. Put everything back together into one return. So your top line select should look like this: select a.id, a.name, b.latest_purchase, c.total_quanity from ( select distinct id, name from table ) a inner join ( select id, max(purchase) as 'latest_purchase' from table group by id ) b on b.id = a.id inner join ( select id, sum(allpurchases) as 'total_quantity' from table group by id ) c on c.id = a.id Depending on your schema you might be able to do this without a sub-query in the `FROM` but at its core that is what you're trying to replicate and you could use a variety of techniques (#tables, @tables, etc.) to achieve it.
First solution, but make TimeIn and TimeOut datetime variables not just time. That way you have a way to report at the end of the week what a users hours were. Things start to get complicated when you've got to attribute time to cost centers, but hopefully that's beyond even your managers definition of rudimentary since it changes the whole perspective from a clock in/clock out view to a x hours for cost center a, y hours for cost center b. 
&gt; our final is just making 5 tables and assigning Primary/foreign keys This diagram only has 4 tables? &gt; I was wondering if I can somehow just fill out all my foreign keys with the data from the primary keys. I'm confused by this. &gt; Like, fill out my RatingID column in MovieRating and use a code to populate RatingID wherever it occurs in my DB with that data. You'd add rows to the MovieRating table like: RatingID Rating 1 G: General audiences – all ages admitted. 2 PG: Parental guidance suggested – some material may not be suitable for children. 3 PG-13: Parents strongly cautioned – some material may be inappropriate for children under 13. 4 R: Restricted – under 17 requires accompanying parent or adult guardian. Unless you allow your movie table to have null RatingID, your Movie table isn't populated yet, so there is nothing to update. You can then populate your Movie table. Your Cast table, like your MovieRating table can be populated at the beginning. then your MovieCast table can't be populated until all three of the other tables are populated.
It depends on the usage of the books really. If they are likely to be used frequently a book case is a useful way of having ready access to them. If they are archived, putting them in boxes might be a more efficient use of space.
You haven't specified a from statement in your update query. Employee is in you subquery
Cheers... posting now
Ha...yeah
You're missing a semi colon after :new.idproduct
Thank you so much to all.
Maybe make a date table with the date range you want, then left join to the table you've got and coalesce(count, 0).
 DECLARE @StartDate date = '2016-01-01'; DECLARE @EndDate date = GETDATE(); DECLARE @Loop int = 1; DECLARE @Rows int; DECLARE @DateToInsert date; SET @Rows = DATEDIFF(DAY,@StartDate,@EndDate); SET @DateToInsert = @StartDate; IF ( OBJECT_ID('tempdb..#days') IS NOT NULL ) DROP TABLE #days; CREATE TABLE #days ( [Id] int IDENTITY(1,1) NOT NULL , [MasterDate] date NOT NULL ); INSERT INTO #days ( [Date] ) VALUES ( @DateToInsert ); WHILE ( @Loop &lt;= @Rows ) BEGIN SET @DateToInsert = DATEADD(DAY,@Loop,@StartDate); INSERT INTO #days ( [MasterDate] ) VALUES ( @DateToInsert ); SET @Loop += 1; END SELECT DATE(D.MasterDate), count(*) FROM #days D LEFT JOIN table T ON D.MasterDate = DATE(T.date) GROUP BY DATE(D.MasterDate); Disclaimer: I don't know MySQL. There may be a more elegant solution but this is how I would do it in MSSQL. Basically all I'm doing is taking a date range and creating a row in a temp table for each date in that range. Then, do a left join to your table but group by the master date. If there are no matching dates in your table, the count will be 0. I hope this helps.
I found this : SET @startDate := ( SELECT Date FROM table group by Date order by Date asc LIMIT 1); SET @endDate := ( SELECT Date FROM table group by Date order by Date desc LIMIT 1); call make_intervals(@startDate, @endDate,1,'DAY'); select date(interval_start) from time_intervals ; With the following procedure : CREATE PROCEDURE make_intervals(startdate timestamp, enddate timestamp, intval integer, unitval varchar(10)) BEGIN -- ************************************************************************* -- Procedure: make_intervals() -- Author: Ron Savage -- Date: 02/03/2009 -- -- Description: -- This procedure creates a temporary table named time_intervals with the -- interval_start and interval_end fields specifed from the startdate and -- enddate arguments, at intervals of intval (unitval) size. -- ************************************************************************* declare thisDate timestamp; declare nextDate timestamp; set thisDate = startdate; -- ************************************************************************* -- Drop / create the temp table -- ************************************************************************* drop temporary table if exists time_intervals; create temporary table if not exists time_intervals ( interval_start timestamp, interval_end timestamp ); -- ************************************************************************* -- Loop through the startdate adding each intval interval until enddate -- ************************************************************************* repeat select case unitval when 'MICROSECOND' then timestampadd(MICROSECOND, intval, thisDate) when 'SECOND' then timestampadd(SECOND, intval, thisDate) when 'MINUTE' then timestampadd(MINUTE, intval, thisDate) when 'HOUR' then timestampadd(HOUR, intval, thisDate) when 'DAY' then timestampadd(DAY, intval, thisDate) when 'WEEK' then timestampadd(WEEK, intval, thisDate) when 'MONTH' then timestampadd(MONTH, intval, thisDate) when 'QUARTER' then timestampadd(QUARTER, intval, thisDate) when 'YEAR' then timestampadd(YEAR, intval, thisDate) end into nextDate; insert into time_intervals select thisDate, timestampadd(MICROSECOND, -1, nextDate); set thisDate = nextDate; until thisDate &gt;= enddate end repeat; END; I just have to join now, ty.
I would have never thought that manga n SQL could merge to create a book. LOL
I think our cases are the same - in my case, AttributeTypeId was in a different table as well. I think the solution would look almost the same as well: SELECT ObjectTable.ObjectID, ObjectTable.[Description] FROM ObjectTable WHERE EXISTS ( SELECT 1 -- Is at least one attribute of ID 1- 4 active? FROM AttributeTable WHERE ObjectTable.ObjectId= AttributeTable.ObjectId AND (AttributeTypeID = 1 AND IsActive = 1) OR (AttributeTypeID = 2 AND IsActive = 1) OR (AttributeTypeID = 3 AND IsActive = 1) OR (AttributeTypeID = 4 AND IsActive = 1) ) AND NOT EXISTS ( SELECT 1 -- Is Attribute 5 inactive? FROM AttributeTable WHERE ObjectTable.ObjectId= AttributeTable.ObjectId AND AttributeTypeID = 5 AND IsActive = 1 ) Try this out and see if it works? EDIT: This might be better: SELECT ObjectTable.ObjectID, ObjectTable.[Description] FROM ObjectTable WHERE EXISTS ( SELECT 1 -- Is at least one attribute of ID 1- 4 active? FROM AttributeTable WHERE ObjectTable.ObjectId= AttributeTable.ObjectId AND (AttributeTypeID IN (1, 2, 3, 4) AND IsActive = 1) ) AND NOT EXISTS ( SELECT 1 -- Is Attribute 5 inactive? FROM AttributeTable WHERE ObjectTable.ObjectId= AttributeTable.ObjectId AND AttributeTypeID = 5 AND IsActive = 1 ) TL;DR: Forgot how to IN
Ya, i was kind of blown away that i found this thread and it was like 90% identical to my issue... I'm going to give this a shot now;
Here it is: SELECT M1.ID, M1.Name, M2.Role, M2.Name AS Children FROM Members AS M1 JOIN children AS C ON M1.ID = C.parentID JOIN Members AS M2 ON C.childrenID = M2.ID OBS: M1 = Members with role 1 (has a value in the parentID column) M2 = Members with role 2 (has a value in the childrenID column) If you can, just normalize those tables, make a Parent table, a Child table and the Members list would be a simple union between them.
You'll have a lot of work with that CSV mate. For start, you'll need to make an UNPIVOT, using your example, I got something like this: SELECT Store, [Date], [Time] FROM (select * from schedule) p UNPIVOT ([Time] FOR [Date] IN ([2/29/2016], [3/1/2016], [3/2/2016], [3/3/2016]) )AS unpvt this is the result: Store | Date | Time :--| :--| :-- 266 | 3/1/2016 | 9:00 to 17:00 266 | 3/2/2016 | 9:00 to 17:00 266 | 3/3/2016 | 9:00 to 17:00 270 | 2/29/2016 | 12:00 to 20:00 270 | 3/1/2016 | 12:00 to 20:00 270 | 3/2/2016 | 12:00 to 20:00 270 | 3/3/2016 | 12:00 to 20:00 272 | 2/29/2016 | 10:00 to 18:00 272 | 3/1/2016 | 10:00 to 18:00 272 | 3/3/2016 | 10:00 to 18:00 After that you'll need to make string treatment in the [Time] column, but that I'm sure you'll be able to handle.
Thanks! This is really helpful. I was thinking of having to do an UNPIVOT but i didn't know the correct syntax. Now I can load this data into a temp table and do some more fun things. 
gotta admit I'm still early on in the learning process and I have a tendency to try "advanced" stuff before I properly understand the foundation. The help is much appriciated tho, gonna have to play around with the basics abit more I've been using this site as a lookup for [different functions/commands](http://www.tutorialspoint.com/sqlite/sqlite_commands.htm) You have any other good recourses to recommend for beginners?
I use SQL Server, so all the material I ever studied was for it, but I know that most of the query works with any relational DB. Just keep practicing and training, most of my knowledge I got from my work day and desperate searches when I needed it haha.
Depends a lot. If your browser application directly execute a query using form fields as parameter then the browser can be the point where SQL injection will be made. If your application send the parameters to the server and then the server creates a query based on that but has a poor treatment, then the injection will be in the DB server. NOS won't help much because most of the times injection isn't result of hacking or brute force, it is the result of exploit of a bad designed system.
Realized LINE wasn't even a table I'm using. Here is my more updated code that still doesn't work. In short I'm trying to make a trigger that takes away sold items from data base. CREATE OR REPLACE TRIGGER TRG_UPDATE_INVENTORY AFTER INSERT ON BB_PRODUCT FOR EACH ROW BEGIN UPDATE BB_PRODUCT SET STOCK = STOCK - BB_SALES_SUM WHERE BB_PRODUCT.IDPRODUCT = :BB_PRODUCT.IDPRODUCT; end; Still getting bad bind for BB_product.idproduct
And what are those 5 columns in the Lines table? Because then yes, you had the original trigger correct, but you need a way to link the row in Lines back to the inventory table.
But which one has the mechanism to ultimately detect an attempt injection and able to stop it. Are you saying that proper browser query validations can stop the injection (with form validation or use of sproc) and not necessary just depend on db server?
Here's the problem I see. BB_Product is the inventory table, it does not contain sale information. Thus an insert into this table is just adding another item to inventory and shouldn't effect any other rows in inventory. Some other table in your list of tables *has* to be the transaction table that records items being sold (leaving inventory) and *that* is the table you want to put the trigger on.
Yes, a bad implemented application can destroy a well implemented DB server, it is just like add a bio-metric bullet proof door in a wall made of plywood. SQL injection is treated with good developing practices allied with well defined permissions in the DB side. Other types of invasions may require additional tools like NOS, Firewalls and so on, but SQL Injection itself, is not the case. An example of SQL Injection is the following: Imagine a login page, in the backgroud of the page it runs the query: "SELECT * FROM Accounts WHERE user = '" &amp; txtUser.ToString &amp; "' and password = '" &amp; txtPassword.ToString &amp; "'" And then has an IF that checks the existence of user and password to validate it and so on ... And then the "hacker" types that: User: ' /* Password: */ go TRUNCATE TABLE Accounts -- In that case the "hacker" just deleted every user from your DB, and he didn't break any security, he just exploited a bad designed application that had too much privilege in the DB server.
Since SQL injection is valid SQL, it cannot be done by the database. Must be done by the application layer, on the server. There are some database firewalls that try to detect and block SQL injection. See http://www.hexatier.com/ etc You could also try using Snort: http://www.symantec.com/connect/articles/detection-sql-injection-and-cross-site-scripting-attacks
That and a bunch of other reasons. Excel's also better at complex math, does lots of reporting stuff, graphs, is more familiar to accountants because of the grid metaphor, blah blah blah.
Basic outline... Get the table and column names from the system tables along with the column's data type. If that datatype is a string, use the table and column names to construct a dynamic SQL statement to search that field for your character, and then perform a replace on those fields that have the character in question. It's a fairly advanced concept, a bit of a pain to perform and getting it wrong could make a real mess. Why did this problem end up in your lap, and what DBMS are you doing this in?
Thanks for the reply. Fortunately I only have to run it once as a "purge all" sort of scenario. &gt; Why did this problem end up in your lap, and what DBMS are you doing this in? I've inherited a work project that's almost as old as I am (WinForms, almost 20 years old, talks to database over the Internet, no security, everything is hardcoded, Turkish Hackers Union etc). Very fun to trawl through... cough. It's all on SQL Express / SQL Server Management Studio 2014.
SELECT county, city, state FROM table WHERE (SELECT COUNT(county) FROM table) &gt; 1
I'm getting errors even after I plug in my table name and column names. However, wouldn't this return a column for each city which has more than 1 county? Basically, My Cities table is this; City County State Portland Clackamas OR Portland Multnomah OR Portland Wachington OR And If I was searching for Portland, OR, I would want my County column to return... County Clackamas, Multnomah, Washington
Well, it's no longer the original [County] column, but you can get something similar via [CountyAgg] = (select ', ' + county as [text()] from table t where (t.fieldOne = 'myValue') for xml path ('')) You can use "stuff" to kill initial divider comma if you want. FYI, people who are used to data dump-type reports meant for human consumption seem to like this format, but for subsequent processing it's usually not as convenient. 
MS Sql does not have a string aggregate function out of the box, but it does have an option to format result set as an xml fragment - that's the "for xml path" piece. The subquery makes individual records in the result set defined as a simple xml text() and xml path is blank, so individual records are dumped into a single text buffer, thus getting us the the string aggregation across the subquery result set. There's no 'group by', so you'll need to take care of the grouping on your own.
Look into COALESCE. It should look something like: SELECT COALESCE(counties, ', ') FROM table WHERE city = @city
Any chance you're familiar with SSIS? If it were me, I'd simply create an SSIS package that reads the CSV in its given format, uses C#or VB script task to convert each value into the desired format, and then exports to CSV. With a package, it would be reusable and modular enough to change one component (like column order, data types, etc.) without having to change much else.
Do it. Not worth taking a stand on.
Yes, this is SQL Server based. Interesting links on indexes, but my data is coalesced from multiple sources in multiple formats, Oracle, flat files, and other SQL server tables. This indexing trick may help, but it will not be the silver bullet to make this run in minutes. I would love that if it were the case!!! This may shave several minutes though. I will give it a try on our test server. Thanks!
What tool are you utilizing for your data merge process?
What part of the process is taking 3 hours? Because that's not a reasonable amount of time for 500k rows of data to be inserted to ANY SQL Server instance really... so I have to think the problem lies in producing some of the data you're inserting, and not in the destination SQL Server.
Just another blog spammer. Posts with ?'s in the title that are not selfposts should be held for review.
Just do it. Who cares?
Will selecting MAX(...) not work? You might have to move your entire select into a subquery for it to work. 
 ORDER BY Age DESC LIMIT 1 
I can't use DESC LIMIT 1.
In addition to what /u/shaunc mentioned with disabling indexes, can I ask why it takes three hours to load the data? Is it coming from a complicated query or something? If so, then you can `SELECT` into a temp table and then either `MERGE` or `TRUNCATE/INSERT`.
That sounds like a perfect use case for a staging table, possibly as a temp table. I have a similar process involving loading lots of data from a variety of sources, and I ended up just using temp tables, loading the raw data into them, creating indexes as needed, preparing the final data in another temp table, and then disabling indexes, doing a `TRUNCATE/INSERT` to populate the final table, and rebuilding the indexes. That allowed us minimize the time during which the data was unavailable. Depending on your particular setup, there may be some advantages of using fixed staging tables instead of temp tables. For example, you can assign them to a filegroup on a different physical drive then the final data table and tempdb to cut down on the performance overhead.
First, at the very least the Visits table should include a date of the visit. Then the sql join is pretty simple. Select * from persons join visits on persons.key1 = visits.key1 join movies on visits.key2 = movies.key2
And is there an other way without the JOIN word?
You're welcome! I'm just happy that my question benefited more than just me :D
This guy needs a good mic and to tell whoevers doing laundry behind him to knock it off.
why? when you tagged your post with [MySQ] i thought you were using MySQL -- are you not?
How is the week columns named? Depending how it is done, you can use the command datepart(wk,[date field]) to get the week of the year value and filter by it.
too bad i can't read what's on the screen your next tutorial should be text-based 
it's named as "Week XX". So I need this to be automatic to skip what's in common in the source and destination and only pick what is unique in Source. 
Get the MAX week, verify it isn't in the destination table already, and then move that week. It really depends on how you store the data, "Week 1", "Week 2", "Week 3" vs 1,2,3
This was what had thought about, but disregarded it ad it used a loop, but I see there's not really a way around that. Thanks for the solution! 
This is the equivalent of asking 100 people in the room in serial to calculate their age by taking the number of days from their birth then dividing it by 365.25. After throwing away the remainder, go back through all those people and then ask them for their age based on those calculations and if its &gt; than the current oldest person. Not only is the query inaccurate, it's really inefficient and will cause the column to be completely reread into member even if it's indexed. OP said in another message he couldn't do Select TRUNCATE(DATEDIFF(CURDATE(), Birthday)/365.25, 0) as BirthdayEdit From members Where Gender = "F" ORDER by Birthday DESC LIMIT 1 If he can't use "Limit 1" it sounds like a homework assignment because it exactly fulfills his intentions. He could do Select TRUNCATE(DATEDIFF(CURDATE(), Min(Birthday))/365.25, 0) as BirthdayEdit From members Where Gender = "F" This would aggregate Birthday first, then it would perform the DATEDIFF/TRUNCATE. &gt;But its too early in the morning for my brain to work this out. The SQL programmers at my job write crappy code all day long, not just in the morning.
I don't think you need purchasemenuitem ID in the purchase or menu item tables. One purchase is going to have many items, so you don't want to put one purchasemenuitemid in the purchase table. One menu item is going to be in many purchases so you don't want to put one purchasemenuitemid in the menuitem table.
It's a homework question. I'm not allowed to introduce syntax that is not covered in lectures. 
jeeebuz kriste, man, you should've said so in the first post some shoddy OP you are
It's an option, it works both ways. Frankly string parsing is more easily written in a programming language so given all I know about your skills and use case so far you are probably best using c#. Could you give us a few examples of what the data might look like? Edit: all of my suggestions bar SQL itself I learned from the docs available online. 
This question isn't really a SQL issue, it is a C# issue ... but anyway. You created the MySQL connection but you didn't open it in your code. After you learn how to open it, you'll need to create objects that will receive the data from the MySQL (DataSet, RecordSource ...) and those objects is what will store the result of your query.
Yes, its called SSIS. https://msdn.microsoft.com/en-us/library/ms141026.aspx
Yep you can but you have to jump through hoops. Basically you need the c# exchange web service libraries. You have to add them to the GAC. After that you can use the dataflow script component to read the mailbox and create row outputs, or the control flow script component to chuck out files that you subsequently load.
Thank you!
Thank you!
 DELETE l FROM sysssislog l JOIN ( SELECT source, DATEADD(DD,-30,max(starttime)) [threshold] FROM sysssislog GROUP BY source ) t ON l.Source = t.Source AND l.starttime&lt; t.threshold on mobile and not at a db. Hope that works for you. No loops required. Don't forget Source column is not the same as package in sysssislog! Edit: starttime not timestamp
Thanks for that mate :) 
If you change your column names to generic ones then your UNPIVOT stays dynamic. You may have to read the header line seperately though and unpivot that too. That will give you a x ref table.
 SELECT Person.Name, Movie.Name FROM Person INNER JOIN visits ON visits.person_id = person.person_id INNER JOIN movies ON movies.movie_id = visits.movie_id INSERT INTO comments ( person_id, movie_id, comment) SELECT Person_id, Movie_Id, 'This would be a comment' FROM Person INNER JOIN visits ON visits.person_id = person.person_id INNER JOIN movies ON movies.movie_id = visits.movie_id
 SELECT Gender, LastName, Birthday, TRUNCATE(DATEDIFF(CURDATE(), Birthday)/365.25, 0) AS 'AGE' FROM Members WHERE Gender = "F" AND Birthday &lt;= ( SELECT MIN( Birthday) FROM Members WHERE Gender = "F" );
Ah, right. Sorry, missing detail. I only use package start and end events, I simply use it for monitoring and build execution history.
You might be better off with the new SSISDB logging. It has a more 'sane' structure including simple tables for executions, package ids and other nice things ;
MSDN is full of good articles and examples. I hadn't used table adapters before, they look quite good. https://msdn.microsoft.com/en-us/library/ms233812.aspx 
Try to create FKs with DELETE CASCADE, SQL Server will handle the children tables much faster and it will be guaranteed to work.
Homework questions are pretty boring. Might be better asking in /r/databases? 
Depending on your situation, it sounds like you may just have a bad starting point (the application). That said, you probably can't just start over from scratch. Generally speaking, I recommend against "IN" statements, unless you are providing a list of constants, or able to make outer references from within the IN statement. Also, reducing the nested state of the queries will probably help reduce the overhead. A nice substitute for "IN" is "EXISTS", which works like a JOIN without affecting your rowcount. An example of EXISTS given the above code: DELETE t3 FROM childtable3 t3 WHERE EXISTS( SELECT 0 FROM childtable2 t2 JOIN childtable1 t1 ON t2.t1Id = t1.t1Id JOIN Account tA ON t1.accountId = tA.accountId WHERE t2.t2Id = t3.t2Id AND &lt;condition&gt; ); Another possible solution is, as you said, Batching Deletes. This only really helps in large volume deletes (100k or more rows per delete). Depending on how big your cache is, each record being deleted must be stored in cache and then written to log. If insufficient cache is found, then it spills to tempdb. This is where batching to the upper limit of your cache would reduce the execution times, as well as IO to tempdb. You would need to test this as every system has a different tipping point on cache allocation, especially depending on the concurrent workloads. 
Collation has a bunch of rules that I was unaware of until just a few months ago. By default, MSSQL is lax on case and trailing spaces, but there are ways to check against the value. Using OP's specific example: DECLARE @table TABLE ( id int ,value1 varchar(5) ,value2 varchar(5) ); INSERT INTO @table VALUES (1, '', ' '); SELECT [valuesEqual] = CASE WHEN CAST(HASHBYTES('SHA1', COALESCE(value1, '')) as bigint) = CAST(HASHBYTES('SHA1', (value2 )) as bigint) THEN 'true' ELSE 'false' END ,value1 = COALESCE(value1, '') ,value2 FROM @table ... WHEN PATINDEX(value1, value2) = PATINDEX(value2, value1) EDIT: Another simpler version of the above comparison method. 
Sorry ill elborate. The columns are Name, Score, TimeStamp The name will be inputted by the user as User_name and score will be another variable. I want the oldest score deleted by using the time stamp for the smallest time (which would be the older time) and delete that whole record. 
A suggestion, if data size is not a concern, is Partition Switching. You could create the table as a partition, and add some identifier to the Clustered Key per partition (Maybe date, since this is a daily swap). Each day you create a new partition and load data with the new partitioned key. Once you have Today's partition and Yesterday's partition, you can either keep Yesterday until you load Tomorrow (no more than 3 partitions) or drop Yesterday on completion of Today. The nice part about Partition Switching is that it works like a Truncate for all data in the partition that is being deallocated. Also, this allows for a live-swap of data simply by referencing a different partition in your query. From a SQL syntax perspective, the user querying sees no change, other than a reference to the value the table is partitioned by. https://technet.microsoft.com/en-us/library/ms188730(v=sql.105).aspx Critical note: Clustered Key should have the partitioning value set as the first column in the Cluster to avoid physical data fragmentation, as partitioning tables is a physical operation in addition to logically separating.
Thanks for this, this really helps ease my mind in attempting this as a possible solution.
Thank you for the suggestions. You are on point, there are a number of UDF's that are part of this process, and the collection process is the true bottleneck. Thank you for your time in responding. I do appreciate the help and insight.
Is there a way to partition your data and drop whole partitions?
You should look into talend open studio - it is what we use for all our ETL processes. I'm not saying you can't do it, but brew your own ETL in c# is going to take a lot of work to get running stable and fast. Since you are doing the ETL inside a custom program, I'm not sure how much help can be offered...
The way that I would do this is to use a looping iterative approach. Even though it may take longer than a set-based approach, it should reduce the risk of blocking and you may be able to just let it run and monitor it a few times a day for as long as it takes. So, from your example you apparently need to delete data for specific accounts. I would create a purge\queue tracking table that contains the account PK values and a status column. I would also create a stored proc that loops through this table and sets the status to a "processing" status, lets say 2. One iteration of the while loop would process one account. The logic in the loop calls another stored proc that does the actual deleting. This deletion proc should start from the top and select the PK and FK values into temp tables - working all the way down to all possible child tables. I would avoid all of the nested IN sub-selects as that becomes extremely cumbersome. Besides, temp tables don't create that much overhead since SELECT...INTO is minimally logged. But the space allocated to TempDB may need to be considered and make sure TempDB is on your fasted spindles. You may or may not need to index these temp tables - it depends on how much time you want to spend tuning vs what you think the performance gains for the deletes may be. After all of the temp tables are created start from the bottom child tables deleting rows based on what is in the temp tables (using IN or EXISTS). you may need to loop through deletes on each table using SET ROWCOUNT 100 to limit the number of rows deleted per iteration. The table deletion loop ends when @@ROWCOUNT returns zero. I picked the 100 rowcount value arbitrarily. Just beware of your lock escalation threshold, which I think is 1000 by default, to avoid locking the entire table. Work your way up to the top level from here. The proc marks that account as completed status after that's done and the loop tries to select a new account to act on. The only down side is that all of this will be logged, but I'm assuming you're expecting that anyway. Also, if the DB is properly indexed, then I don't see why this would take too much longer. Edit: clarifying a few things
Always delete in small batches, eg. Delete top (1000) from table...where Then you could have something like WAITFOR Delay '00:00:200' Followed by GO 100; This would help prevent blocking and is a simple method of deleting a lot of records in small chunks.
First ... I am curious why you are deleting anything from a database? Is there no historical importance to them? Why not just mark the records "inactive"? All tables I create have an "inactive" field. This way, if a user or process "deletes" (marks the row inactive=1), it is very easy to get it back. Plus, if the row REALLY needs to be deleted, "delete from the_table where inactive=1". As a side benefit, it also helps identify what records need to be moved to an archive database. Lots of deletes = lots of locks and transactions = lots of time. One thing you can control is the time. You could spread the time out. When I do have to do gobs of deletes but more often gobs of updates, I do them in batches. Similar to what /u/gentile_ben said. If you can, put in an "inactive" field, update which records need to be inactivated (set inactive=1), then delete in batches, (where inactive=1). The hardest part will probably be putting in the inactive field. But look at it this way, it can make your job easier in the future. You can also use the inactive field to determine what records need to be moved to an archive database. Some people I know use the column title of "archive" instead of "inactive". 
Thanks for the video. I enjoyed it.
I don't think it supports it, but it would be much easier if you could use the LAG function - something like SELECT engine, pointtime, status FROM ( SELECT s.engine, s.pointtime, s.status, LAG(s.status, 1) OVER (PARTITION BY s.engine ORDER BY s.pointtime) AS PreviousStatus FROM enginestatus s WHERE ... ) sub WHERE sub.status &lt;&gt; sub.PreviousStatus If not, you could use ROW_NUMBER() - it'll work but may be slow if it has to sort the entire dataset. You could possibly have an index help you out here though: WITH cte AS ( SELECT s.engine, s.pointtime, s.status, ROW_NUMBER() OVER (PARTITION BY s.engine ORDER BY s.pointtime) AS r FROM enginestatus s ) SELECT c1.engine, c1.pointtime, c1.status FROM cte c1 INNER JOIN cte c2 ON c1.engine=c2.engine AND c1.r+1=c2.r WHERE c1.status&lt;&gt;c2.status You'd need to double check that join and which direction to compare the rows in (maybe it should be c1.r-1=c2.r) but the gist of the idea is there. This is also assuming you can use WITH statements in OpenAccess - never used it so I don't know. You could put that subquery in the FROM if you needed to
Yeah, ROW_NUMBER() or ROW_ID() isn't supported from what I can tell. Partition is not supported either, from what I can tell. Options are extremely limited. Other ideas?
How would the MAX() know to continue dropping down to the next maximum value? If you could give me a start I'll work on it. In reality I am going to run this daily so it may not take as long.
Thanks. This makes alot of sense to me. Theres really no timescale on it. So it could even just gradually purge data over a long time. My idea is essentially the same except I would write a small application or service which would select the data store it. Then batch delete them itself. This would lower the impact on the database as there would be no temp tables or tempdb considerations. I will look into writing a stored procedure to do it though as it would be nice for the db to be able to manage itself.
Im going to look up partitions as Im not a hundred percent familiar with them. The database is SQL azure what i'm not sure if this changes anything as some SQl Server features are not implemented.
Ironically, I was just looking at that. It looks like the version we are using is extremely limiting in what we can do. The amount of workarounds I have to do is insane and slow. The database gets 10,000 new data points a minute but I have to work with this crap version. I am going to press on them to upgrade! Thanks for the help.
Im going to give this a read. Thank You.
Thanks I have just looked this up. I was unaware of the perfomance gains from using exists instead of in. I will try and change the output to this format instead.
Thanks everyone so it seems to me that the way to go is batching of deletes. Whether that be in temporary tables that are then iterated through by a stored procedure or creating a separate application which can manage them itself. Then with an adequate delay between deletes and ran over a long period of time it should cause minimal disruption.
Maybe its because I'm a database dev as opposed to an C#\Java type dev, but making an app for this seems like overkill. To each his own.
Not 100% sure on the syntax for OpenAccess but this should work. Also I wasn't sure if status was value or status. I'm assuming "value" because your initial query had value. SELECT es.pointidlong ,es.pointtime ,es.value --other values go here FROM enginestatus es INNER JOIN ( SELECT MIN(pointidlong) as pointidlong FROM enginestatus WHERE pointtime &gt; Now() - 30 AND es.pointidlong = 'engine1' GROUP BY engine, value ) as esAgg on es.pointidlong= esAgg.pointidlong ORDER BY es.pointidlong ,es.pointtime The subquery should execute first and grab all the records entered in the last 30 days. Then you join it back to the table assuming you want to get more fields from enginestatus. It will aggregate and only return the records for first pointtime when the status changed for the engine. So rather than looking to see if the row's next value has changed, the much faster aggregate does it "set based" rather than recursively. Just for future reference, if you are writing correlated queries that use TOP functions.... **don't**. It causes each row to execute another query just for that row. So rather than doing 1 or 2 executions and returning data "set based" you end up doing (1 || 2) + n. So it could be 432,000,000 times if it received 10,000 rows per minute over a months time. That execution that occurs millions of times is getting a list of all the point times that occurred before that point time and then sorting that list in descending order. Unfortunately, you even chose pointtime to sort desc rather than pointidlong as pointidlong is sequential and also is probably the primary key and the data is sorted physically by pointidlong. A sort on an already sorted list wouldn't be that bad but since the sort was done on the pointtime, it would resort the list by pointtime not realizing the end product would be exactly the same. The only issue with sorting on pointidlong would be if pointtimes could be updated externly. I highly doubt considering the dataset they would be changed. Anyways, don't correlate your subqueries. They serve a purpose, 1% of the time, but people use them 99% of the time. I've have approved 1 dba review that included a correlated subquery and it was because I wrote the initial code of the query for the developer which happened to have a correlated subquery because there were so many functions being called in the select statement that adding a very large one to many table so we could aggregate was causing too much overhead because we were calling functions against columns in rows that ultimately were being aggregated out of the query. It's so rare for correlated queries to be necessary.
Figures, guess ill go back to having multiple tabs. 
I have not, but thanks for bringing it to my attention
You can try this: http://ssmsexecutor.codeplex.com/ Also, the Express (free) edition of SQL Complete supports this as well: https://www.devart.com/dbforge/sql/sqlcomplete/ IMX, however, "execute current statement" functionality causes more problems than it solves because it's trivial to execute the wrong statement by mistake. Forcing you to highlight exactly what you want to execute is a much better design.
Ugh, I hate these offshoot SQL engines (I left my last job because I was forced to use kbsql). Looking at the pdf, Open access doesn't support aggregates in subqueries. Also I don't think **AND vhs1.pointidlong = 'WM23683_SCOMP'** is right, that probably should be engine?? I would think point**idLONG** would be a long integer identifier and not a varchar. We may be able to get away with using views. Run the create view once (it creates a permanent view) CREATE VIEW houston_vhs.houstonvalues_last30days AS SELECT MIN(pointidlong) as minpointidlong ,engine ,value FROM houston_vhs.historicalvalues WHERE pointtime &gt; Now() - 30 GROUP BY pointidlong, value Then anytime you need to run your query SELECT vhs.pointidlong ,vhs.pointtime ,vhs.value FROM houston_vhs.historicalvalues vhs INNER JOIN houston_vhs.houstonvalues_last30days vhs30 on (vhs.pointidlong = vhs30.minpointidlong) WHERE vhs30.engine = 'WM23683_SCOMP' ORDER BY vhs.pointidlong ,vhs.pointtime I've seen some lousy sql engines think the table and the view are seperate objects on disk and run them parallel and immediately hang because of disk contention. 
This is only true for the + operator (and || in Postgres/Oracle). If you want to concat fields where one might be null, you need to use the CONCAT function. It'll handle this situation correctly.
Not only is CONCAT more portable, it doesn't null your entire string if any of the fields you're concatenating happens to be null. Unless you're specifically after that behavior, don't use + or ||.
AND, OR and LIKE are not 'clauses' they are operators. Edit: and poor old NOT feels really left out now.
My suggestion would be to avoid triggers and use a stored procedure to perform the insert into candy sales and update store within the same transaction instead. 
Here's a good description of how to do the trigger you want. http://www.codeproject.com/Articles/25600/Triggers-SQL-Server 
This is absolutely correct, but is OP asking for help on an exercise in an online course? Personally I don't get the structure. Why doesn't store have a product id? Where is the product table? Is this one of half baked online courses? OP could you link to it?
Not exactly the same, but in SSMS, you can drag the mouse to highlight (select) a statement and click execute in the toolbar and it will only execute the statement(s) highlighted. 
you can use GO to delimit a query batch, all batches get executed, or you can use RETURN to have the batch terminated at the keyword, nothing gets executed after the return. in any case, you shouldnt depend on delimiting to not have queries executed, its a recepie for desaster.
upvoted /u/Eldarial Have you only got Transact SQL at your disposal (i.e. not using SSIS)? Have a look at the MERGE statement to INSERT OR UPDATE new and changed records. Let us know if you need more direction. 
Not only is the topic only marginally relevant to this subreddit, but it just a link to an abstract which says "click here for the article" - if you're going to link to something, link to the actual substance.
Currently, my schemas are created with FluentMigrator. I then run unit tests to see if the migrations created or modified the correct table structure, constraints, relations, views, stored procedures, etc. by calling a class that can make just about any assertions on a specific database. In some places, I also deactivate some of the constraints and insert garbage, run the migrations and see if the migrations can restore integrity or put garbage values in a jail so that it can be acted upon and that the application recovers gracefully with minimal trouble.
I've spent some time looking in to [tSqlt](http://tsqlt.org/sql-test/) and plan to play with it at some point in the future.
I'd second using SSIS for this although you might hit the learning curve pretty hard. You were on the right track with select into distinct, you may just need to persevere. Using IDENTITY is a good idea when going into final tables, but to build up your schema you would be better to identify any natural/ business keys present in your csv. This will save you the lookup step as you go from parent-child-child. If you'd like to post the header line from your csv we could probably help!
this is not working
Like /u/edarial stated above, you set a column (usually your primary key) as IDENTITY when you create the table. When you INSERT simply omit that column and it will get automatically filled with a sequential number. Tbh I would concentrate on your normalisation first. Get a set of Select queries ready that extract your new tables.
&gt; I believe there is a SQL law to never pluralize table name you believe incorrectly, plural tables names are preferred 
I'm not familiar with using that but logically it seems like it would limit by a random number instead of providing a set number of random records. Ordering by random seems to be the way to go.
There is no "SQL law" about table names being singular or plural. It's just a matter of local naming convention. Choose one approach and use it consistently.
I would approach this differently: 1. Create new table as select rows I want to keep from old table. 2. Drop old table. 3. Rename new table to old table. Obviously, things like indexes and foreign keys complicate things, but they can be managed.
To clarify the first scenario, you would have separate columns for player 1 and player 2, both would be foreign keys to the players table. Like atrizzle said, if you might ever have more players, the second option is better. 
In my experience, anything more than one is Many. Sure, there might only be two now. After it is in production, there will be a need for #3 and it will be easier to change.
Oh I see. I didn't realize an out parameter was an option for procedures.
it's many to many
NATIONAL VARCHAR I think, but I am more of a MS SQL expert.
i will increase font size 
I can't find it in phpmyadmin too. When I want to create a table, this is all I can choose for string types: http://imgur.com/2OainFw I looked up and found my mysql version (5.7) supports national varchar. I don't understand why I can't use it though :/
This is how I usually do it
This. Logically you have 2 tables with 2 relationships between them, each with a different role. They are mutually exclusive I.e. if a player is in the role 'player1' they cannot also be in 'player2'. Physically you have 2 foreign keys to the same table, and a constraint to enforce the mutex. When querying, each role becomes an alias.
On SQL Server the LENGTH command return the number of characters in a string, so the query would be: SELECT Category, Brand FROM PRODUCTS WHERE LENGTH(Category) = 3
GROUP BY State,gender
For old versions they called it BIDS (BI dev studio). They now call it SSDT (SQL server data tools). You can download it separately. I'm using the 2013 version. 
Yes, that should be the VS shell you need.
Thank you so much!
&gt;WHERE First LIKE 'D' OR lastName LIKE 'D' &gt;HAVING (state) &gt; 20; WHERE First LIKE 'D%' OR LastName LIKE 'D%' HAVING COUNT(state) &gt;20; Why is First not FirstName? *I am just looking at your code and picking up what I think is wrong.*
Given all the questions you have regarding basic SQL commands, I suggest you read through [this site](http://www.w3schools.com/sql/) as it appears either your professor is not clearly communicating the material to you or you are not properly understanding the material being presented. Either way, supplementing your existing knowledge from a site such as the one provided will serve you better in the long run rather than rushing to complete your assignment right before it is due (I believe this is your third post in the past 3 or so hours...)
Did you at least attempt your homework? Edit: What is your specific problem when attempting to answer the problem you posted?
I do know SSIS, I was thinking that I could do this in SSIS as well. I'll have to investigate a little further. My extent with SSIS is using it to automate importing a csv file into a server. Thanks!
Yup, my first thought would have been to use the subquery as well, but then I decided that if it's a question about basic aggregation functions, they're probably not expecting a subquery. 
 select p.picID from tb_Pic p inner join tb_shape s on s.shapeID = p.shapeID where s.shapeType = 'square' minus select p.picID from tb_Pic p inner join tb_shape s on s.shapeID = p.shapeID where s.shapeType &lt;&gt; 'black' `minus` is a set operator, so I don't think it will return duplicates, but experiment.
You're not going to learn this stuff unless you at least attempt to answer the question yourself. Let us know what your thoughts are and what attempts you've already made to solve this. Hint: break it down into each separate part; *highest *lowest *average *renaming column 
It's why I said I'd start with yours if clarification can not be gotten because honestly, I'm a terrible going by the verbiage I'm given and arguing it until proven wrong.
Yeah /u/r3pr0b8 really trounced me on this one.
~~Your query would replicate records as your join is on state and there is no distinct.~~ It would likely be better to use a correlated subquery and exists instead of a join to filter the states that don't have enough members. Edit: Could also use an "IN" statement... I just prefer EXISTS Edit2: I missed the group by in the join subquery, my bad. Additionally, I interpret the problem as * Find all states with more than 20 members * List the Total Credit Limit for that state * Restrict the List to people with D as the first letter in their First or Last Name. It is important because your answer and mine result in different results. From the below data set, my query which assumes the above is true returns $21500, yours would return $23500 create table #Members ( memberid int identity(1,1) primary key, state nvarchar(2), First nvarchar(100), LastName nvarchar(100), CreditLimit money ) insert into #Members (state, First, LastName, CreditLimit) values ('AZ', 'Joe', 'Smith', 10000) , ('AZ', 'John', 'Smoth', 1000) , ('AZ', 'Lane', 'Joley', 10500) , ('AZ', 'Alice', 'DoeSmith', 2000) , ('OR', 'Ben', 'Jerg', 3000) , ('WA', 'Frank', 'Toli', 15000) , ('IL', 'Lisa', 'Vern', 20000) select a.state, sum(a.CreditLimit) as TotalCreditLimit from #Members a where exists ( select 1 from #Members z where a.state = z.state group by z.state having count(*) &gt; 2) and (a.First like 'J%' or a.LastName like 'J%') group by a.state
I'm not sure of the efficiency difference on group by vs exist, but this is just easier for me to conceptualize than the group by. SELECT Pic.picID FROM tb_pic Pic WHERE EXISTS ( SELECT 1 FROM tb_shape Shape WHERE shape.shapeType = 'square' AND shape.PicID = Pic.picID ) AND NOT EXISTS ( SELECT 1 FROM tb_shape Shape WHERE shape.shapeType =! 'black' AND shape.PicID = Pic.picID )
I think MSSQL supports CONCAT, which behaves similar to VBA, so you could SET @ClientName = CONCAT( @Surname, ', ', @Forename); (I think I can't recall if that implementation will work that way or require) SET @ClientName = CONCAT( @Surname, CONCAT(', ', @Forename)); 
EXISTS performs better since it doesn't bring back the whole data set, it only tries to get 1 register, if it does then the SQL stop searching and return true or false instead of the whole select would bring from the group by.
Shouldn't that be LEN( TRIM( Category))?
Since it's an academic exercise I'll consider it possible that we actually just want to record how many times a given user accesses a given page over time. This doesn't accomplish anything resembling "users currently accessing the website" since there's no time stamp anywhere. you didn't specify which dbms so this is more of advanced pseudocode than it is something you can just cut and paste and expect to work.. DECLARE @UserCount INTEGER DECLARE @PageCount INTEGER SELECT COUNT(*) INTO @UserCount FROM Users WHERE uname = @uname; SELECT COUNT(*) INTO @PageCount FROM Users WHERE pname = @pname; IF @UserCount = 0 BEGIN IF @PageCount &gt; 0 BEGIN INSERT -- Page exists, user doesn't, insert. INTO Users ( uname, pname, access_count) VALUES( @uname, @pname, 1) END -- No instructions for what to do if page does not exist, do nothing. END ELSE BEGIN IF @PageCount &gt; 0 BEGIN SELECT COUNT(*) -- Page exists, but is there already a row for this user at this page? INTO @UserCount -- strictly speaking this should be its own variable, but I'm lazy. FROM Users WHERE pname = @pname AND uname = @uname; IF @UserCount &gt; 0 BEGIN UPDATE Users -- User at Page exists, increment count. SET access_count = access_count + 1 WHERE uname = @uname AND pname = @pname END ELSE BEGIN -- page exists, but no row for user at that page INSERT INTO Users ( uname, pname, access_count) VALUES( @uname, @pname, 1) END END END 
That's what i came up with, sans the INNER JOIN keyword (so ugly) and the ELSE NULL which is redundant: SELECT tb_Pic.picID FROM tb_Pic, tb_shape WHERE tb_shape.shapeID = tb_Pic.shapeID GROUP BY tb_Pic.picID HAVING COUNT(CASE WHEN tb_shape.shapeType = 'square' THEN 1 END) &gt; 0 AND COUNT(CASE WHEN tb_shape.color &lt;&gt; 'black' THEN 1 END) = 0;
That is sheer nonsense. What is the "brings back"? To where? To the database? To the application? NOT EXISTS will read the same data a GROUP BY would read, that is, the same table or index (unless the query plans are different and one reads a table the other, an index) meaning the same amount of data is read with the same amount of reads. The EXISTS might be able to stop early, but in a small table, depending on how much room it takes up, it might all get read anyway.
Please stop coding joins like this unless your RDBMS doesn't support the keywords (looking at you older versions of Oracle). It is ancient and makes reading/updating that code (obviously not this example, but more complex queries) harder than it needs to be and leaves much more room for error. Edit: Added some words because the keywords don't exist in older versions of Oracle which some people may still be using.
&gt; List the total amount of credit limit for each state with more than 20 members. Restrict the list to only members with a name starts with letter “D”. (Use Members Table) SELECT First, -- Does not list total amount of credit limit for that state but actually lists each row. LastName, State, COUNT (State), -- you need to group by state so count( state) isn't going to help since you're counting the thing that is the same. CreditLimit, COUNT (CreditLimit) -- you're counting how many credit limits there are, not totalling them? FROM MEMBERS WHERE First LIKE 'D' OR lastName LIKE 'D' -- you're indicating the entire string has to be 'D', D*anything* is indicated by D% HAVING (state) &gt; 20; -- You don't have a group by, you can't have a having without a group by. While I have no idea why you'd ever sum people's credit limit by state, that's how I interpret 'total amount of credit limit for each state' SELECT State, SUM( CreditLimit) FROM MEMBERS WHERE First LIKE 'D%' OR LastName LIKE 'D%' GROUP BY State HAVING COUNT(*) &gt; 20 -- Don't count a specific field unless it's a unique ID 
So yes I guess I screwed up and did not provide you with the outline of the second table which would have been Pages ( pname, entry_count, exit_count) I screwed up on that. Its the syntax of this coding that I am not 100% on. I am not a programmer I only know just a lil bit of code. Thats kinda of why in my shameless way I was asking for a lil help or someone do to do a problem for me so that I can deduce / figure out how to write for the other problems.
Unfortunately this is not a problem from the text book. this is a made up project by the professor. The text book does not do a good job going over pl/sql and stored procedure very well. This class overall has been nothing but a struggle due to the poor teaching methods of the teacher. 
The DB is what "brings back" any data, even if an application is making the call, the DB is the one who process and retrieve the data, the destination doesn't make much difference in this case. I totally agree with the NOT EXISTS, but the EXISTS sure has an improvement just GROUP BY, even if it is just a minimal one.
Yes this is much more advanced then what I was expecting. Very thorough I must say. I don't know what you mean by DBMS? This is what I was able to come up with from help from a friend set serveroutput on size 1000000 format wrap create or replace function enter_page ( uname in users.uname%type, pname in pages.pname%type) as user_count number, page_name varchar2; begin select count(*) into page_name from pages where pages.pname = pname; if page_name = 0 then (dbms error message) else select count(*) into user_count from users where users.uname = uname; if user_count = 0 then insert into users (uname, pname, 1) else update users set users.pname = pname; access_count = access_count + 1 where users.uname = uname; update pages entry_count = entry_count + 1 where pages.pname = pname; end if; end; / show errors;
DBMS - Database Management System i.e. what flavor of database you're writing this for, there are syntactical differences depending on who's database it is. Examples: Oracle, MS SQL Server, Informix, DB2, MySQL select count(*) into page_name from pages -- you've declared page_name as a character field and you're trying to put a number into it. where pages.pname = pname; -- Possible Problem.. user exists, but the row doesn't have the same pname value. -- So you update the user's row in the table to the current page?!? update users set users.pname = pname; access_count = access_count + 1 where users.uname = uname; And you didn't mention this table at all.. update pages entry_count = entry_count + 1 where pages.pname = pname; 
Both the pname and uname counters should be numbers. You did it just fine with the uname counter, I'm not sure why you made the pname counter a varchar2. 
Actually, i have a very hard time reading the ANSI style joins. When asked to fix queries, i nearly always rewrite them away. [ANSI join syntax is just plain wrong] (https://slashdot.org/journal/135346) A well written where clause is an easily traversal chain that explains what is being done on which table, in order. Separating the clauses into two different clauses makes it terribly hard to follow.
I doubt it. In order for EXISTS to stop a read early it would need to find the data before all the data is read, and that means the block size being read and the amount of blocks being read, allow for there to be unread data. Practically, thhis is only going to happen on larger tables. Even then, this assumes the GROUP BY isn't optimized to read less of the data, which the optimizer often rewrites. Being the optimizer ought to have an easier time with a single GROUP BY then multiple EXISTS()s, the GROUP BY might do less reading anyway.
Is that the RIGHT OUTER SIDE or the LEFT INNER SIDE? :)
I know each case is a case and the environment may change the result but as seem [here](http://sqlperformance.com/2012/12/t-sql-queries/left-anti-semi-join) NOT EXISTS runs faster with and without index. But yeah, if you can make a single GROUP BY that gives the same result of multiple EXISTS then go for the GROUP BY for sure, but usually the relation is 1 to 1.
Just a bunch of opinions... However, the keywords are being implemented by most every new/recent major distribution including Oracle, which is the only developers I see using the old join style to date likely because they held out for so long or because people were taught by one of the aforementioned developers. This move is likely to support the newest ANSI standard for SQL (ANSI-92 vs ANSI-89) going forward.
SqliteStudio is what you're looking for.
Oracle supports the standard because some company who pays likely asked for it. :) SQL Server deprecated the syntax so much that you need to set a special option (database version or something) to use it. Quite the heavy handed approach. In other databases it is used by programmers, who have no grasp of good SQL anyway. :-P In any case, an "inner join" is just a fancy name for a where clause that confuses people. Much like that idea of teaching people joins via Venn diagrams, it causes more confusion, and some of the most horribly written queries i have seen. Anyway, as you said, it's an opinion. I expressed it in just two words when showing off what i came up with, and got strong responses. So funny.
They said that Venn Diagrams are bad at teaching people joins... You know, the one mechanism for effectively demonstrating how data sets interact with one another? :( Lost cause.
Can you post the assignment or question? 
Alright Hunter you have been a big help so far. I am hoping that you can help me once more. In one of the problems it wants me to delete the links. If the links don't exist I just need it to respond with a Doesn't Exist error message. The table format is as such Links (pfrom, pto, count). create or replace procedure remove_linke ( pfrom in links.pfrom%type, pto in links.pto%type) as pfrom number, pto number; begin select count(*) into pfrom from links where links.pfrom = pfrom and links.pto = pto; if pfrom = 0 then dbms_output.put_line('This Link Doesn't Exists'); else delete from links where links.pfrom = pfrom and links.pto = pto; end; / show errors; 
Oh alright, I'll try to do so in that case, thank you very much!
There is client side and server side Power Bi. If you want to get started today you can look up some tutorials on how to use Power Pivot and Power Query. If you have Excel 2013 you already have what you need.
Sure, but you can use a script task once you import the CSV to write it to a temp table in your desired format, then export that temp table to another CSV if you wish. 
Well, since its MSSQL, it would be LEN(RTRIM(LTRIM(Category)))=3 since there's no straight TRIM function in T-SQL. I would only do this if I thought there was considerable risk of there being leading and trailing white space chars. If it was data set by an application or process, I probably wouldn't use a trim function. But, if its user entered data, then that would be a good idea.
Just curious as I've never worked in this flavor of SQL - what's the difference between a natural join and an inner join?
I have limited experience with deployment (have a couple of teams running dashboards on it, including our own team), so I'll fill in what I know. Disclaimer: MS are trucking out updates quite frequently and I might be a few weeks or so behind the current feature set as I've been a bit busy with some other stuff. * 9.99 is per month, and if you want the enterprise features (more storage on powerbi.com, data gateway connections, live refresh etc., more connectors). * There is a free tier if you want to play around with it. We have found that as we have office365 and organisational accounts set up, anyone with a domain account can log in and set up a free account and have a go. We use a data gateway connection to SSAS tabular for a small data model and we need the paid tier for that (as will anyone who wants to consume the data to the best of my knowledge). * If you upload, it will sit against your own personal workspace, but you can share this with users in your organisation or move it to somewher emore common such as a group or whatnot. * I do believe that they have recently opened up access to outsiders, if you opt to share with someone you know, but you do have to invite them and they do have to sign up. There are some capabilities for exposing data more publicly but as we're using it for internal enterprise stuff, we haven't really looked at it. There is group functionality too but we haven't played with this very much. * FWIW if you do use the paid tier, the data gateway does honour user access permissions, so if you have row level filters on SSAS or a user can't access the BISM then this is honoured through the gateway. One caveat to this is if you share a dashboard with this type of data in it, it does inherit your view of it. The [free course on edX](https://www.edx.org/course/analyzing-visualizing-data-power-bi-microsoft-dat207x-0) is quite straightforward but does a good job of covering some of your questions off. I've taken it and learned a few things, despite using it on and off for a while.
From my understanding, Natural Join is the same as an Inner Join with the caveat that the Natural Join has an implicit ON clause that uses all of the columns in common between the left hand side and the right hand side. From the example tables in my assignment: SELECT * FROM ships NATURAL JOIN classes; Will return: name | class | launched | type | country | numGuns | bore | displacement ---|---|----|----|----|----|----|----|---- California | Tennessee | 1921 | bb | USA | 12 | 14 | 32000 Haruna | Kongo | 1915 | bc | Japan | 8 | 14 | 32000 Hiei | Kongo | 1914 | bc | Japan | 8 | 14 | 32000 Iowa | Iowa | 1943 | bb | USA | 9 | 16 | 46000 Kirishima | Kongo | 1915 | bc | Japan | 8 | 14 | 32000 Kongo | Kongo | 1913 | bc | Japan | 8 | 14 | 32000 Missouri | Iowa | 1944 | bb | USA | 9 | 16 | 46000 Mushashi | Yamato | 1942 | bb | Japan | 9 | 18 | 65000 New Jersey | Iowa | 1943 | bb | USA | 9 | 16 | 46000 North Carolina | North Carolina | 1941 | bb | USA | 9 | 16 | 37000 Ramillies | Revenge | 1917 | bb | Gt. Britain | 8 | 15 | 29000 Renown | Renown | 1916 | bc | Gt. Britain | 6 | 15 | 32000 Repulse | Renown | 1916 | bc | Gt. Britain | 6 | 15 | 32000 Resolution | Revenge | 1916 | bb | Gt. Britain | 8 | 15 | 29000 Revenge | Revenge | 1916 | bb | Gt. Britain | 8 | 15 | 29000 Royal Oak | Revenge | 1916 | bb | Gt. Britain | 8 | 15 | 29000 Royal Sovereign | Revenge | 1916 | bb | Gt. Britain | 8 | 15 | 29000 Tennessee | Tennessee | 1920 | bb | USA | 12 | 14 | 32000 Washington | North Carolina | 1941 | bb | USA | 9 | 16 | 37000 Wisconsin | Iowa | 1944 | bb | USA | 9 | 16 | 46000 Yamato | Yamato | 1941 | bb | Japan | 9 | 18 | 65000 As you can see, every column from both tables is present with the class column only appearing once because they have that column in common.
tb_shape does not contain picID.
I haven't used group by often mainly because those select clause I wrote usually contained 20-30 column names, and group required everyone of them in the group by clause, even though picID is the only thing I wanted to group by. It looks messy, should I just live with it?
WHERE first LIKE 'D*' OR lastname LIKE 'D*' I don't think I've seen a WHERE statement with a 'D*' type code. Why wouldn't he just use the 'D%' and '%D' I'm guessing he wants last names that start with D and end with D. Look up aggregate SQL functions. Group By functions would also be helpful but I don't know if you are there yet. 
Hello, you could play around with the "order by" function
Nifty open source tool if you want to poke around: http://sqlitebrowser.org - Not sure if it has all the functionality you need, but it has been enough for me on my dev machine. I usually manipulate schema in my code and do very little actual admin from the database side. 
The subscription (i.e pro plus) will give you power pivot (which the developer tools needed for the Excel data model) and Power Bi desktop which is a stand alone that allows you to create Bi solutions outside of Excel. Your team will all need Excel 2013 + but just regular Excel is enough to use the data model you have built.
SQLite was designed to be the database that doesn't need to be managed. What exactly are you trying to do?
I've only ever needed to look into sqlite DBs a couple of times, and I used: http://dbeaver.jkiss.org/
I believe he is referring to current iterations of the release candidate for SQL 2016
Ah, sorry, i misread the question, thought he was trying to get the user with the earliest timestamp. In that case I'd probably use this: SELECT a1.UserID, a1.Timestamp FROM TableA a1 WHERE a1.Timestamp = (SELECT TOP 1 a2.Timestamp FROM TableA a2 WHERE a1.UserID = a2.UserID ORDER BY a2.Timestamp ASC )
All of this and read reddit in the can. 
See this is why I tell people Reddit isn't full of trolls. That's really well written. I also get where you see the problem that way. Honestly I think there is some clarification needed as it can be interpreted a few ways. I personally like your way of going about it but I went withe above one because a lot of people are trying to figure out homework and they don't usually get that in depth. 
Use the `sqlite3` binary. It allows you to interactively talk to a sqlite3 database and allows you to run SQL. It also has a couple of other features, like dumping sqlite3 databases.
Could you elaborate on his method? What he can do better?
For sure coffee. News sites. Reddit. Check Jira for any tickets. Fix data issues, mostly caused by migrations sometimes by level 2 support not fixing issue correctly the first time. ReviewBoard. Verify co-workers code for upcoming builds. Facebook. Start working on development tickets. Attention span is about 40 minutes an hour. I work 9 or 10 hour days. When time, loads of clean up in local environments. Cleaning up utility scripts that help us do typical tasks. 
Where did this data go to? Where did this data come from? Can you fix this data? Can you delete this data? My computer is slow. etc.
That is correct. For OP: It's basically a full featured enterprise copy of SQL Server with a 180 day license.
I might be doing that right now..
Perhaps! Maybe we even work at the same company! haha!
Download the 461 brain dumps. Watch the Microsoft Fill in the Gaps video on YouTube. A month with those resources, and you'll be fine. These two will show you the areas you need to focus on.
* Customer: "I need sales data for the last three years. When can I have it?" * Me: "What are you trying to accomplish?" * Customer: "Just doing some analysis for the next fiscal year. Just gimmie the data." * Me: "Well I really want to help you with your business need and be able to answer your question in the best possible way. Is this needed for a report? Is the data needed just once or ongoing? If so, how often does it need to be refreshed? Do you need any sort of self-service report like SSRS or Power BI? Is this data going to be shown at an executive level presentations or just for scratch analysis? Maybe we already have a report for what you need." * Customer: "I don't know. Give me the data" * Me: "Okay, but you should read this dictionary doc on how the fields are calculated and used by the business..." * Customer: "I don't need that. Gimmie the data." * Me: "Okay, here you go" **Several weeks pass** * Customer: "Hey, I have been working with the data you gave me and it is wrong. It doesn't match what I have in my files. You need to fix this because I am going to meet with the VP tomorrow and it needs to be perfect." * Me: "But did you read the dictionary doc on how to do this?" * Customer: "Nope but I need the report now so I can test the metric." * Me: "Which metric?" * Customer: "FY17 Forecast Sales for the US." * Me: "Umm, you mean this report that is currently being used by the business on a monthly basis for reselling purposes? This one has gone through deep business requirements and was signed off on by your group last year." * Customer: "I guess so. Still you should look into fixing that data you gave me because it confused me and almost made me report incorrect numbers to management." * Me: **Sigh**
All too familiar, man.
Unhelpful answer: This is very hard to do in MySQL. The only ~~great~~non-hacky way to do it in SQL at all is with a recursive query, but MySQL doesn't support that. If you can script at all this would be trivial to solve in Python or Perl. edit: [lol nevermind](http://stackoverflow.com/questions/662207/mysql-results-as-comma-separated-list)
Oh. God. I'm having some serious deja vu.
Hey, I know you have a lot on your plate today, but I just found this SQL on the internet, ran it without a WHERE clause, and now all our customer statuses are wrong. Can you fix it? ^ Literally happened 10 minutes ago.
Log in, hey this report is wrong, well it works for everyone else, did you check the parameters? Oh you were running by the wrong date set. OK good. Every. Damn. Day. 
This is about as accurate an answer as there ever was.
I work in SQL maybe, 25% of the time. A lot of what I do is pulling data from multiple sources and combining it in ways that make our internal customers happy using Excel, Access, &amp; SAS. Run some canned queries where all we do is change dates or customer numbers. Write new queries for the Ad-Hoc stuff or for new processes and reports. Refactor old queries because they're fucked up and nobody even remembers exactly what they do anymore, we just keep running them. It beats working for a living.
My job title has the word SQL in it but lately I'd say only 30% of my time is directly doing SQL stuff at the moment (it should be 70%+). Either way, recent requests which are kind of typical : - Modifying an interface so our website can offer a new service to customers. This involves some light database "design" (new tables / stored procedures) and modifying the SSIS package (ugh) to produce files for a separate ETL process to consume. This involves working with the business area to understand exactly what they want as an output (fortunately I know the domain I work in quite well). - Generating a daily email (using SQL Server) which confirms that two systems we have are reconciled. One system is a SQL Server hosted within our general network, the other system is a MySQL database in our DMZ. - Pulling the event logs from 40 servers using Powershell and putting the results into SQL Server so I can query any Errors and look at trends. - Investigating why an interface / process has failed meaning certain new accounts weren't being returned when a specific process was used by staff (in the end, not a database issue as such - service account authentication issue on part of the middleware). - Using SQLite to process a really big log file so we can work out which machines on our domain are hitting a specific DNS server. - Improving the general maintenance scripts that we run on our SQL instances. - Writing a process to pull our Active Directory user list into a SQL database. We've been trying to move people towards a self-service reporting system so I don't get asked many "Can you tell me how many accounts have a balance over £500" (or whatever) any more - these things people can find themselves - usually. You still get some queries where the requirements are too weirdly specific to be covered by the reporting system but it's quite rare. We have some dedicated report writers, and I support them when I can. I am very fortunate in that a high proportion of my work is self-directed. I don't really get micromanaged or given individual tasks to do - more work that I approach (with colleagues) in the way we find works best for everyone. I "buy" this semi-autonomy by working pretty damn hard. 
I do SSRS reporting for the ID division of our company...I write the sql queries to run the reports and make them pretty for them to use.... Get to work check my email Check my personal email Go to the bathroom for 30 minutes Come back and eat breakfast Pretend to work Maybe a question comes up...make a report or point them to where the data is Go for a walk Shop on slickdeals Eat lunch Repeat morning ritual I wish my team was more of a team I talk to no one and feel left out of everything Good thing is my companies health benefits makes it hard to even think about leaving...about to have a baby and only paying 50 dollars out of pocket 
As others have suggested: read the book cover to cover, watch the Fill In The Gaps videos on YouTube, and find yourself some free brain dumps. Focus on what's new in 2012 if that's the version you'll be focusing on. Also, FOR XML, windowing functions, UDFs, views... Knowing no SQL... Hope you don't plan on having any free time this month. Study every free minute you have. 
I was asked at work to import 100,000 transactions from an auxillary income source into our production database, when there are no matching fields/keys, and the data is extremely dirty - im talking data is in the wrong fields, misspelt names, the postcode is combined with the city for some reason (some of the time). I got told "Just get it done." from my manager who is a marketer. Thats the one peice of work I find really annoying. Oh yeah im the only employee at the organization who knows SQL, Apart from that. Writing Ad-hoc reports, importing data, exporting and proccessing data for a number of Quarterly mailings, modifying/Developing Access programs, writing some Stored Procs for Statistical Analysis reporting. (We have a in house program that reads SQL and dumps out a report for the end user, I have it as a Stored Proc so I can edit the reports from my desk without having to resupply which can be frustrating). So a bit of the ETL proccess. I have a bunch of segmentation information and serial IDs that I keep in tables which I load in from our previous mailings, my reports compare transactions that come in with this list and then spits out stats. Would you guys call that a DataMart? 
Out of curiosity, what did you major in (or where did you learn the necessary skills)? 
CTEs, most often used by programmers learning how to use CTEs, and then by programmers showing off that they know how to use CTEs. 
How do you even do this.
The best part is this guy tells all his co-workers that the DBA team are a bunch of pricks. 
What would you use instead, are they not a valuable tool for staging data?
I believe you need this line in your select clause , count(sku) as color_count And a GROUP BY color clause at the end 
Yes, what you describe is the behavior I would expect to see. You are granting access to that user differently for each step, but still granting access.
I love it when they ask for 'stats of $whatever' without saying what they actually want.
I guess the error you're seeing is ambiguous column name? When you join tables and select a column that appears in more than one table with the same name; you have to specify which table you want the value from (even if they contain the same value) As a side note when joining tables it's best practice to alias them. SELECT sks.sku, sks.store, str.city, (sks.price-sks.cost) as Profit FROM sksinfo sks INNER JOIN strinfo str ON sks.store=str.store ORDER BY Profit DESC
There are often other options, including temporary tables, table variables, or views. CTEs have their uses, but in my experience are a maintenance nightmare. They also can be a black box performance wise. I've seen queries written with CTEs run horribly slow, and when broken out into temp tables the performance has improved by several orders of magnitude. It all depends, of course. Sometimes temp tables are a horrible idea, too. I just find that whenever a co-worker says "we can use a CTE for that!" I end up cursing their name 4 months down the road.
[w3schools](http://www.w3schools.com/sql/) is a good resource to start with, it starts from the beginning and builds your knowledge base as it progresses through the different functions etc. Just to give you some basics on the object explorer window (the one you're looking at); * The first level is your server (this can contain multiple databases). * The next level is lots of things that you don't need to worry about at this point other than the Databases folder. * Within the Databases is a list of all of the databases on the server (ignore System Databases and Database Snapshots at this point). * If you expand a server you'll see a list of the objects that are available in that server, only concern yourself with the tables for now. * Expand Tables and you'll see a list of tables that hold database on your server. You can expand the tables to see what columns they contain if you like.
I've learned to basically so no until I get reasonable requirements.
First of all, view makes no difference in any way, it doesn't improve or make any query slower, view is only an alias to a query, so make a view from a superset and then filter the data from the view won't change anything. View is a good way to call common queries, like imagine an standard company with Clients, Products, Sellers and so on, so a pretty common query is a list of Products bought by Clients per Seller, so you make a view with those joins and when you need to call it you just do a SELECT * FROM VIEW and done, other thing that view allows you is that you can DENY the access to those tables but ALLOW access to the view, it is a security benefit, but no performance change at all. Stored Procedures and Functions is great when you do a certain task in a query often, for example, I have a lot of tables with a DATE field and it is pretty common for me to make reports grouped and filtered by month and year so I made a Function that returns the 1st day of the month of the data I send (if I send 2016-03-24 or 2016-03-15, both will return 2016-03-01), so I group by the result of that, instead of every time I do the query I write a code for that.
I don't know that they even need a WHERE 
Not to state the obvious but you are aliasing your fields when calculating them?
Yes I am. I'm sorry, I was trying to simplify the problem enough to make it quick, but I may have simplified it too much. :(
No problem, just checking the basics first. if you take out your ISNULL statements from your sub-selects do you still get the same error?
I'm glad it helped you figure it out. That's perfectly normal for "this **should** work, what bug did I discover" situations. It's usually something simple you overlooked.
I got it figured out. The Nulls are actual values from another table. As in, the fields were filled in with the word NULL, so ISNULL would not pick them up. Thank you for your help though!
Using an ordinal in the ORDER BY is a terrible practice. What happens if the columns in the SELECT change? It's okay for ad-hoc queries, but for queries that are used in production or on a repetitive basis you should always list out the column name.
Oracle won't let you use an alias name in the order by. Thus in those cases I use the number or the actual formula being aliased.
I didn't know that. Sorry if I came off as a douche about it. Can you do something like this in Oracle? ORDER BY (retail - cost)
Yes.. .and that's indeed what i'll often do for production code that's not a one off.
It seems like everyone I talk to says they had no plans on getting into a database type of position, which is interesting. I'm actually studying supply chain right now too but also with a MIS major and a CS minor, so i think I'll probably end up in a similar position eventually. Glad to hear it's worthwhile!
I don't think so either, but it is a good habit to start with that IMO.
I actually think my business education has helped me as a developer. I kind of speak both languages if that makes any sense. I think a big reason people don't set out to work in databases from the get-go is because they're kind of abstract, unsexy, and always behind the scenes (as opposed to front-end development that's visual/flashy and interactive). 
I will try that, but will that only work if the field is null? What happens when something is entered into the field and removed and it is blank but not null?
I'm pretty sure that casting a non-numeric as any numeric value returns 0
Perfect, I'll give it a try. Thank you!
I like cleaning this up in my dataset by using isnull instead of making SSRS deal with it but that's just me.
I tried isnull but it skips over blanks. Maybe nullif is better? It looks like the following may do the trick to make nulls and blanks change to 0: select coalesce(nullif(field1, ''), '0') from t1 left join t2 on t2.field1 = t1.field1
I believe you can just cast the blanks to your desired field type and they'll fill with zero's.
 select case when field1 is null or field1 = '' then '0' else field1 end newfield1 from t1 left join t2 on coalesce(t2.field1,'0') = coalesce(t1.field1,'0') 
I don't know what platforms it works on or not, but stuff() is what you're looking for on MSSQL. An example: SELECT [VehicleID] , [Name] , (STUFF((SELECT CAST(', ' + [City] AS VARCHAR(MAX)) FROM [Location] WHERE (VehicleID = Vehicle.VehicleID) FOR XML PATH ('')), 1, 2, '')) AS Locations FROM [Vehicle] Replace the ',' bit with ' '
Thanks, this looks like it works as well!
 SELECT GROUP_CONCAT(dacolumn, SEPARATOR ' ') AS result FROM datable
Xml path came in with SQL 2005. I don't really know the background to it, but it's part of the back-end systems that build up the full xml support. I think this function was built for the general idea of taking a record set and forming xml from it, but of course it does very handy string concatenation. Great for making csv strings as well.
It really depends on what metro area you're in
Taking a wild guess here... are you in Texas? If so... you could be doing better, you could be doing worse.
Austin. Tech industry is exploding here...but so is COL. 
That seems in line with what I've seen at the three Fortune 500 retailers I've worked for in the North Jersey/NYC market, assuming you don't have a ton of other experience you're not listing. I'm in CRM/Analytics but have always worked closely with DBAs. Just my experience! 
We'll need more info. What is the primary key?
Yes...Texas, how did you know lol? So I guess that puts me at average pay?
haha, I totally missed that. Good catch.
did you reorder the WHERE after the JOIN like mentioned above? That's my bad - I should've caught that.
I noticed in your edit that you have a 'desc' in your GROUP BY. That only belongs in an ORDER BY, so I'd drop the 'desc' in the GROUP BY
you have a desc in your GROUP BY...
If the code in your post is what you're using, you still have the WHERE before the JOIN and I don't see brackets around returns. Also, you're misunderstanding the GROUP BY. That is where you list your non-aggregate columns (so store, city, state), not your aggregated column
One approach would be to extract the target data into a temp table and then repopulate your source table. As always, make a backup of your table before running anything you find here. IF OBJECT_ID(N'tempdb..#footemp') IS NOT NULL BEGIN DROP TABLE #footemp END CREATE TABLE #footemp ( A INT NOT NULL DEFAULT 0, B INT NOT NULL DEFAULT 0, C INT NOT NULL DEFAULT 0, PRIMARY KEY CLUSTERED (A,B,C) ) BEGIN TRAN BEGIN TRY INSERT #footemp SELECT DISTINCT A ,CASE WHEN B=1 THEN 2 ELSE B END AS B ,C FROM your_table TRUNCATE TABLE your_table INSERT your_table SELECT * FROM #footemp END TRY BEGIN CATCH IF @@TRANCOUNT &gt; 0 ROLLBACK TRAN END CATCH COMMIT TRAN I was in the midst of making a sqlfiddle but their site went down, [this link](http://sqlfiddle.com/#!3/230bd/2) may work later.
They said one sentence about not feeling like part of the team, then said they're just sticking around for the health benefits. Sorry to be so blunt. If there is more to the story I figured they'd share. I don't walk on eggshells and carry a box of tissues on Reddit comment threads. I'm always open for a dialogue if there is one to be had though. I'm this case, I was told to eat a dick. That pretty much confirmed my thoughts. 
 I concur. Definitely underpaid, bank up experience and shop around.
ops, lost the order by in the middle of the copy and paste and formating :D
When you are grouping, you need to include everything except the field (or fields) you are aggregating. Your group by should be: GROUP BY st.store, st.city, st.state Once you do that, you shouldn't get the error about non-aggregated values needing to be part of the associated group.
Can you not use merge
There are a few ways to do this. If it is a one-off with no groups: Declare @s varchar(max) Select @s = @s + column From mytable The xml trick you have already on another comment. There is also a free example on MSDN of a CLR aggregate function: Select mygroup, dbo.Concatenate(column) From mytable Group by mygroup This last method has so many advantages, it is worth jumping through the few hoops to get it into your DB.
3-5 of exp? It'd be pretty average. If you hunted around you could find something better. Take into account of your current workload/happiness level as well.
Too quiet against the background sound of the video for my liking.
You do quite a bit more as a lot of places have the Datawarehouse under a BI team while the main DBA/DBE's handle the transactional databases. In the Detroit area for around that, I've seen it go from $50000 for small companies to larger companies paying in the $100,000 range. Thing is the higher the pay they are willing to give, the more of a workload/work time that they will expect.
also the stack overflow ask is here: http://stackoverflow.com/questions/36208685/sql-cant-join-summed-metrics-from-various-tables
Aw that's so insightful ^_^
Do you want to try an convince your current employer that you are underpaid and deserve more, or are you trying to decide if you should look for a position with a different employer that will pay you more?
Would prefer the former over the latter, but not opposed to looking outside the company if I have to. My manager is really awesome and has taught me a lot, but I think his hands may be tied by higher ups when it comes to more money. I posted this just to more or less get confirmation if I am underpaid for what I do or am I just starting to get money hungry and think I am worth more then I actually am. 
Kind of what the other guy responded. I got a hit on LinkedIn that led to a new position, but if you wanted to stay you could try to get a counter offer instead. It helps to have alternatives even if you don't necessarily want to take them. Edit: also in Austin
I'm not sure about MySQL but most of RDBMSs and i think MySQL as well have join elimination. so if the RDBMS is sure that the join has no impact on the result of the query it will eliminate it. A join does not influence the result if: * no column on that table is used on the select * the number of records with or without the join will be the same. You can check the execution plans to see if your views/queries make use of join elimination.
Thanks! That totally works! One more question though: This is a simplified example of my query. I have two metrics on the opportunity object, all opportunities, and closed won opportunities. Bookings have a stage of 'Closed Won' and they should count as both opportunities and bookings. Any idea how to separate them since they both share oppty IDs? 
Just give them a different rType, the fact they tie to the same table and it's ID doesn't prevent using that field to categorize them.
I can't point you in the exact direction you want (working with the adventureworks dataset), but when I was learning I used sql-ex.ru. I've probably spent over 40 hours doing their questions and have maybe half done. 
No way he is leaving soon. I think he plans on hanging around for a while. Also, if he were to leave, I would expect the manager position to go to one of the more Sr. people. I am talking people with 10+ years of experience. I do have the things you mentioned, debt and kids, and while money is not ridiculously tight atm, I feel like if I could be up in the 80-85k range, it would give us a lot more breathing room. 
I used a case statement and gave the qualifying records a value of 1 and used sum in the final table on that field. It works! I think you're using tsql? Either way thanks for your help. 
Sorry for not responding guys, I ended up getting help from a friend and totally forgot abouut this post. We did indeed end up using IDENTITY to assign a number to the id fields. Thanks all.
Honestly, I've been working with SQL for years, and those questions make me want to download AdventureWorks just to have some test data for trying it. Side note: Though there is no practical limit on how many objects you can JOIN, can you think of a practical reason? I think the most absurd query I did was a 7-table, 2-anchor, 2-recursive member query, which totals 28 tables, and maybe a couple extra at the end for group names, etc.
1) Primary Key is specific to the current table, and a Foreign Key is derived from the Primary Key of a different table. By definition, a Primary Key is that which defines a row as *unique* within your table. It is always a good idea to have a Primary Key in your table. 2) Depending on your implementation of SQL the syntax may vary. Typically, you apply a Foreign Key Constraint to a specific column, and identify what Table &amp; Column you are referencing. In your case, the second code sample is an incorrectly formatted attempt to *name* your Foreign Key "Deptno", rather than calling the Primary Key *named* "Deptno". Hope this helps
It is always a good idea to keep a coding portfolio accessible for times, such as interviews, in which someone might need to review your capabilities outside the realm of a technical interview. That said, it is best to use code-snippets that are not core to a product, as this can be frowned upon amongst colleagues, or even illegal potentially. As far as getting jobs, it's the same with any field. Often it is more about *who* you know than *what* you know. As a result, getting to know people in the companies you are interested in is your best bet at getting a job, and it will take time. In my personal experience, my last two jobs were had by way of an internal reference by a friend who vouched for my skills, despite my short work history in software. Edit: Recruiters are also a big plus, if any happen to be in your area. You sell your talents to them, and in return they connect you with clients. 
Thanks for your reply. Can you suggest some sample projects which helps to extend my knowledge and programming skills? 
Thanks for your reply. I understand the importance of networking that's why I joined local sql server community. Can you suggest any ideas to work on some sample projects related to real time scenarios?
I know this does not answer your question...but I would like to propose looking at the healthcare industry for work...many many third party administrators and smaller insurance companies and vendors alike need competent IT professionals. A lot of TPAs and vendors are moving away from having IT in house and are more relying on PBMs and major healthcare companies for there IT support though...This is not the right answer in my opinion... My point being....show that you can provide a service in saving the company time and money on transition projects via IT support...Many of these companies are in desperate need of developer work that you can provide. Just my perspective from someone who works in a PBM.
Arg, two days. I finally got it by looking at it on reddit instead of sql workbench. I created a dentist table but uploaded data to my client table. Still, I wish I had an error message to work with!
I think his point is not necessarily whats practical, but what unusual scenarios can you think of that may not make sense and then try to tune those scenarios. I find queries all the time that don't necessarily make sense - at first - because I may not understand the data its pulling. But, after working with the query for a bit and getting to know the data, you start to get a general idea of whats going on. At that point, you could ask, "how can I make this more efficient?" and "will there be a return for putting in the time to do so?"
There is another problem in your code that is creating no error or problem, but it might bite you later, and may be clouding understanding. Lots of your literal *int* values have those leading zeroes. There is no need for them and they will disappear following each operation. Also for 'duration' why not just store an integer number of 'minutes'. You could later use these values to plan day schedules and the like. Fees too could use a money or decimal type. And you need to normalise 'allergy'
Make your table narrow. Use the smallest possible data types to hold your data. Use a surrogate key for repeating details e.g. date You might consider holding start and end times, I.e. Only store a new row if the value changes How long do you need detailed data for? After a year, could you reduce to holding only end-of-day values? 
Great tips. I plan to only hold detailed data for the last 30 days. Then it can be reduced to end of day, and finally, end of month.
15m rows is very do-able. You should be ok.
The Round Robin is a simplistic example of how Postgres 'likes' to handle data especially when it's write-heavy. Further down the page they discuss using arrays to fufil this purpose and then separating the functions out to increase time granularity and to add concurrent transactions (e.g. prices for all stocks within that interval). Given that you know how large one day of prices will be you could round-robin the data on an &lt;arbitrary time period&gt; basis to optimise for writes and then shunt that data to an 'archival' table which is better structured for querying. That would mean you couldn't query realtime data without writing separate queries though, it depends how far down that rabbit hole you want to go. There are probably trading platforms that will do this already for you that would let you do portfolio comparisons if you wanted to do options trading.
Your first example is how to do it IIRC. Normally though you're avoiding a divide by zero error so in your example checking the value of field2 is prudent. I think it relies on a quirk of VB that the 'else' expression is not evaluated at all unless the condition is false. The best example I saw though was a custom VB function to allow this logic to be re-used throughout an Ssrs project. Google it. .. I recall the are lots of good blog posts around on this.
Stanford's MOOC. https://lagunita.stanford.edu/courses/DB/2014/SelfPaced/about and Hackerrank
Decimals have preset thresholds where they go 'to the next byte'. I can't remember what they are... my best recollection is 9 digits. If I'm right then as you only ever need 2 digits to the right of the decimal, you could get up to decimal(9,2). That would give you up to 9999999.99 You don't have to be quite this fussy about the storage. Basically count the maximum digits you need (precision) and how many of those are to the right of the decimal point (scale)
DBA's, imo, have no business giving anyone data. Get an analytics team. We don't give data out, we consume it and tell end users what the data means. We also interface with DBAs to make sure our servers and databases function as expected, stay up to to date, etc. -- basically only when we crash the server, or need access to raw tables for one task or another. I deal with the, "it doesn't match my records," guy and either work backwards from his dataset to figure out what the hell he has been doing and reporting on for the last year, then validate to see if the methodology is significantly flawed. I may include a raw dataset attached to an analysis so other users can verify my findings, but unwashed data is never given to the masses.
So I'm not sure if this is purely academic, or if you'll be trying to implement this is real life. If you really must do this and subscribing to a service isn't an option, Postgres is probably not the best choice as it does not maintain a clustered index. There are benefits and trade-offs when it comes to heaps, but when working with time-series data your read performance will suffer if the data is not stored by stock and then date/time. You can get around this somewhat by partitioning your tables and starting a fresh partition each day so you're not having to make as many reads to check if values change, etc. But it's a level of overhead you probably wouldn't have to deal with (at least not at the same level) if you had a clustered index. That said, a sufficiently narrow table won't see huge performance impacts until you hit a few million rows - if you've got the hardware to handle it. Also, I would say "StockHistory" probably isn't what you want this table to be - this is StockPrice. Just because there is a time dimension involved doesn't necessarily make it a history table. A stock history table would traditionally be along the lines of: (Stock, Date, OpeningPrice, ClosingPrice, AvgPrice, MinPrice, MaxPrice) which is quite different than the table you would use to store the price: (Stock, Datetime, Price).
I think what I was looking for was an error within the select dealing with the appointment table. The other selects worked just fine, but when I went to the last table that was pulling information from the other tables, there were no results. Because of that, I couldn't tell where the problem was. On a 13 inch screen on mySQL Workbench, I had only enough real esate to see part of the code. On reddit using the browser, I saw the issue right away because I could see CLIENT used 5 times instead of 3.
Yes, circumventing the whole concept of domains could save two bytes per record. But insert performance and read performance now pay a price: Inserts (using datetime field): 1. Check prior value before executing insert 2. Begin insert 3. Foreign key check on stock 4. Commit insert Inserts (with surrogate): 1. Check prior value 2. Check surrogate exists for date value, insert if it does not 3. Begin insert 4. Foreign key check on stock 5. Foreign key check on date surrogate 6. Commit insert Reads (no surrogate): 1. Return rows Reads (with surrogate): 1. Join to date table 2. Return rows Joins are generally efficient and don't incur a great cost - but they *are not free* and if you're looking to have the best performance then you'll gladly sacrifice a few records per page to avoid the operation altogether. **Also, your database is now super-annoying to work with.**
1. each table should always have a primary key. They are unrelated to foreign keys. Use it to uniquely identify rows. 1. foreign key just means "make sure a value in this column exists in another column" 1. you don't need the "foreign key" part of the statement
How performant does it need to be? Cassandra might be a better option for quick writing and retrieval 
Simple, I think? Try this: SELECT a.ItemName , a.QuantitySold , b.Cost FROM ( PASTE SOLD ITEMS QUERY ) a INNER JOIN ( PASTE ITEM COST QUERY ) b ON b.ItemName = a.ItemName Might want a `LEFT JOIN` if you have a chance that you have items that don't have a cost. You can also do this, which is a bit cleaner: WITH Items AS ( PASTE HERE ), Cost AS ( PASTE HERE ) SELECT a.ItemName , a.QuantitySold , b.Cost FROM Item INNER JOIN Cost b ON b.ItemName = a.ItemName The cleanest way if you have a lot of data would be to: CREATE #ITEM Table ( [Columns] [datatype] (length) ) CREATE #COST Table ( [Columns] [datatype] (length) ) CREATE INDEX IX_ItemName ON #Item([Column]) CREATE INDEX IX_ItemName ON #COST([Column]) INSERT INTO #Item PASTE HERE INSERT INTO #Cost PASTE HERE SELECT a.ItemName , a.QuantitySold , b.Cost FROM #Item INNER JOIN #Cost b ON b.ItemName = a.ItemName DROP TABLE #Item DROP TABLE #Cost If dealing with a lot of data/tables across servers you can use `OPEN QUERY` when `INSERTING` and that should speed the process up. If not dealing with a lot of data you can skip `INSERTING` and just do a simple `SELECT * INTO #Table` and skip creating/indexing the #Tables.
Then gussy up that resume because your are a wee but underpaid. You can make six figure easily in Houston, but it's not Austin. 
Just curious. Would finishing up my mcsa add any notable amount of money I could potentially get?
Not sure if it's the problem (as I don't use the database you're using much), but you are using "LIKE", which is typically used with wildcard operators (for example, you would say "LIKE '%Computing%"). I wonder if the database is expecting those? Without wildcards, you would probably be better off using an equal sign. Also, most databases will default to ascending when an order is called out, but not defined as ASC or DESC, but maybe that could be an issue? Maybe try putting ASC after it?
&gt; I wonder if the database is expecting those? no.... LIKE something without wildcard characters is perfectly okay, at least syntactically 
"Syntax error (missing operator)" is not a MySQL error message, it's a Microsoft Access error you're running Access, aren't you? Access needs to have parentheses around its joined tables -- don't ask why SELECT Patron.Name , Loan.DateDue , Book.Subject FROM ( ( Patron INNER JOIN Loan ON Loan.UserID = Patron.UserID ) INNER JOIN Book ON Book.CallNo=Loan.CallNo ) WHERE Book.Subject IN ( 'Computing' , 'Mathematics' ) ORDER BY Patron.Name; 
I don't dispute what you've found, but I would be looking into whether there is actually access or not. When a SQLS database undergoes restore, all 'Database Objects' go with it. This includes permissions (grants) and USER objects. USER objects are no use on their own, they are just a pigeon hole to link the GRANT objects to. To make use of a USER object there must be a LOGIN object attached to it. LOGIN objects are Server Objects and belong to the instance not the database. Quite often in this kind of scenario, even when both the USER and LOGIN already exist, they will end up disconected from each other, and you have to do: ALTER USER myuser WITH LOGIN DOMAIN\mylogin (used to be sp_change_user_login) So if you have ound a scanario where post-restore, you can use a foreign Login to connect via a restored User to a restored DB, then indeed you've found a potential hole! EDIT: PLEASE PREFIX YOUR POST!
Happy to help, but please post it, don't hide it. That way everyne benefits from learning from it! 
I'm not sure but this might be similar to the 'Broken LSN chain' problem I saw recently. I was exploring the management views to see why this was happening. There are lots of guides for these around. https://www.mssqltips.com/sqlservertip/1601/script-to-retrieve-sql-server-database-backup-history-and-no-backups/ When I looked why my chain was being broken, I found 'foreign' backup entries from the 'NT AUTHORITY\Local System' account appearing, linked to a 'Virtual' backup device. Turned out the admin had both SQL Backups (Disk-Disk-Tape) AND a backup agent making a full image of the server WITH the backup files. As this process invokes the VSS (Volume Shadow Service), it counts as a database backup, breaking any carefully prepared Full-Diff setup. I've no idea what this would do with a Tran Log backup. Note to self to experiement with that. Hope that helps - I'm probably barking up the wrong tree though. 
What environment are you using? In Oracle you would use Row_number() over (partition by id order by created_at) as rnk in a subselect. Then in the outer select you select where rnk = 1.
Row_number() would also work with SQL Server. Other alternative (if above solution isn't available) would be make a view with MAX() field from quotelog and then JOIN the view instead of use the quotelog directly.
Sorry, I forget to add MYSQL in the title, and sadly mysql doesn't have Row_number()
Was there already a login on server2 for AD/group1? I wonder if it found the equivalent login and bound the user on restore?
No, there was not. I suspect during the migration to Server2 there SHOULD have been AD/Group1 added. Another apparent side effect: In SSMS, when viewing the properties of the User AD/Group1 , it shows the associated login as "AD/Group1". We theorized that since it knew from User info that it was an AD group, it went straight to AD with the SID to get the name. But there was definitely NO AD/Group1 login on Server2. 
Yeah I'm going to have to look that one up. Perhaps as you say because it is windows and a trusted domain it was happy to make a login. Great PSA mate... I'll be looking out for that one in future!
this will work in all platforms, including MySQL (although ROW_NUMBER is obviously better, if you gots it) SELECT quote.id , quote.createdAt , status.status FROM quote INNER JOIN ( SELECT quote , MAX(createdAt) AS latest FROM quotelog GROUP BY quote ) AS q ON q.quote = quote.id INNER JOIN quotelog ON quotelog.quote = q.quote AND quotelog.createdAt = q.latest INNER JOIN status ON status.id = quotelog.status
I gotta guess he was using it as an alias for 'rank' but that's a different function so ¯\_(ツ)_/¯.
The HAVING clause is similar to the WHERE clause. Where the WHERE clause allows you to filter on basic criteria, the HAVING clause lets you filter by aggregates.
you're right, the data OP showed seems, ahem, misleading but OP definitely says "the latest quotelog for the quote" and OP also says "I want to order by qoutelog.createdAt" to find the latest again, i'm certain the status table is just a lookup table for the chosen quotelog
Udemy has a course called SQL for Newbs or something that focuses on MySQL and is a pretty good intro. Done by Udemy staff so it's higher quality than the other stuff I've seen. Costs money but there are coupon codes everywhere to make it &amp;lt;$10..
I am an Oracle dev so the only advice I could give at this point would be to keep in mind the columns that you always filter on and those you sometimes filter on. It might be worth while to prepare multi column indexes for some of the more popular queries column combos. In this case you could make an index that includes both first and last name instead of two separate indexes. The other thing would be to try and partition the data in some way and look at it from the perspective of the partition, for instance partition by state and focus your queries on a per state basis. If you want anything inclusive of all states then you would have to prod other partitions. I don't know much about SQLite but 12 million rows with basic filters should be screaming fast. Maybe I am accustomed to more horsepower.
yes you could manually partition using multiple tables. Oracle Enterprise has a partition clause which you can prune in the query so it knows which bucket of data to look in. essentially it is many different tables with a controller as you are describing but it is just handled for you
w3schools should really just be used like a dictionary in that you dont use it to learn how to speak English. I used http://sqlzoo.net/ a few years ago and I picked up enough to be dangerous in an interview (which I landed).
That seems like a very useful tool to have. One of the main issues that I keep running into with this project is that the user is not sure what queries they will run. So I have been trying to account for all this possibilities. I might even be losing some performance from using LIKE instead of '=' for non-wildcard searches.
Dangerous is just what I'm going for. Thanks!
I haven't heard of Cassandra, I'll have a look. Thanks.
I'm using the Django framework, I don't know enough about how it handles joins to comment, but I do plan on extending to those function you mentioned in the future
here is a free one, http://www.sqlcourse.com/ 
The big things i notice, is you have a lot of erroneous data types that are adding needless bulk, which will affect your search/pulldown times. Dates into dates, Gender/Race char(1) instead of varchar, state char(2). The next thing is you might swap your likes for matches and wildcards via full text indexing. Third, i'd swap out your city/state/county into a seperate table. This will accomplish the same thing as your single-county table, but without having to make seperate logic. Since in any query it can narrow it down, it'll use the index from the city/county table to break up your big table into smaller search chunks. Also, make sure your indexes are in the proper order when they're multi-column (and they should be). For example a Lastname, firstname index is much more useful than just a lone Lastname index and a lone Firstname index. Since you already have 12 indexes, I'd say it's safe to assume insert overhead isn't a big deal and you have plenty of space. So I'd grow each of your indexes to include an additional column for their most common usage. Lastname includes first, First includes last. State includes city, etc. If the query doesn't use them, no extra work added, but if it does it'll speed up immensely.
I am surprised you didn't get errors on the unique key violations after trying to insert a new 1, 2 and 3 when client already had those keys in place. What did client look like after the second set of inserts into it? Did you overwrite your previous 1, 2 and 3? I am also surprised that you didn't get a foreign key error when you inserted appointments with dentist ids that were not in the dentist table.
Why would you *average* cost per item when you're trying to get the current cost per item, which would seem to be the most recent cost per item rather than the average? (Or, potentially the most recent cost value that is less than the date ordered end range?)
No, not today. But you can submit your idea to our Exchange - sqldeveloper.oracle.com 
OK thank you for your answer.
I can see why you have the username you have... In this case MySQL has alternatives to a joining table, including ENUM's and SET's for value objects. I beleive ENUM to be superior to a table relationship in this case
ENUM is the spawn of the devil among many other disastrous properties, you need administrator privileges in order to add a new value, and $deity help you if you have to delete a value and then add one
&gt; ) This is massively helpful, thank you so much!! Just one more question that may or may not be a deal-breaker with me (wish I'd have mentioned this originally!) Do you know if STUFF is supported with natively compiled SP's? I wasn't able to find anything definitive online and haven't had a chance to test yet. Unfortunately, the project I'm on requires NCSP's, so I've been really grasping for functionality...
You can PM it me. I'll look at it. 
True about the data types. I am pulling the information from a tab delimited text file, so I just took the shortcut of making everything strings. But I see that I need to fix that. Not sure what you mean for the second point. So do you mean pull city, state and county into its one separate table, or each have its own table? I should probably get rid of state since it is all for one state anyway. The data was just provided with that column. Not sure what you mean by the order. I was looking this up on the documentation a bit. It makes sense that multi-column indices should be in the order of the WHERE conditions. But how does SQL use partial multi-column indices? Such as: CREATE INDEX Idx1 ON VOTERS('FirstName', 'LastName', 'CountyCode'); SELECT * FROM VOTERS WHERE FirstName = 'X' AND LastName = 'Y'; Would it still use that one given index? Thank you for the comments.
in MySQL, this is a feature (your example would be stored as 1960-06-00) note: you didn't say which platform you're using -- see sidebar
`STUFF` is not, per the documentation here: https://msdn.microsoft.com/en-us/library/dn452279.aspx#bfncsp That said, `STUFF` is just a quick way of trimming our string. The same effect can be achieved with `SUBSTRING` and a subselect with a little bit of conniving. SELECT [VehicleID] , [Name] , (substring((SELECT CAST(', ' + [City] AS VARCHAR(MAX)) FROM [Location] WHERE (VehicleID = Vehicle.VehicleID) FOR XML PATH ('')), 3, len(SELECT CAST(', ' + [City] AS VARCHAR(MAX)) FROM [Location] WHERE (VehicleID = Vehicle.VehicleID) FOR XML PATH ('')) - 2) AS Locations FROM [Vehicle] Essentially, we have to repeat the subselect that's generating most of our text so we can pass it to the `LEN` function which gives us the last argument for `SUBSTRING`. We tell `SUBSTRING` to start at the third character and grab a string the length of our entire compiled string - 2, since we want to trim 2 characters.
Which RDBMS are you using?
Googled it. http://tunweb.teradata.ws/tunstudent/TeradataUserManuals/SQL_Reference_--_Functions_Operators_Expressions_Predicates.pdf Search for "Binary Arithmetic Result Data Types" 
9 out of 10 blog readers can correctly identify click bait headlines. 8 out of 9 readers cannot resist anyway. You won't BELIEVE what reader #7 discovered!
What you can do is create a view of all the joined tables in the GUI and then run queries off of that view. If it is MS access then create a query in access with all of those tables joined together and run queries off of that.
Yes rn is probably a better alias. Sorry I was on my mobile. [Row_number()](https://docs.oracle.com/database/121/SQLRF/functions170.htm#SQLRF06100) and [rank()](https://docs.oracle.com/cd/B19306_01/server.102/b14200/functions123.htm) are very similar functions. Row_number() does not allow ties, rank() does allow ties. EDIT: Removed a sentence. 
Honestly, allowing a 0 value for the month or day seems like a really good solution to this problem. The alternate solutions are: * Store year, month, and day in separate columns. This has the concomitant complexity of storing what is naturally a single value in multiple columns. * Use some default value (first of the month, first of the year) with a separate field for accuracy, as suggested in the stack exchange post linked to by u/el_chief. Besides the (once again) added complexity of adding another field to your database, you now also have values that appear to be something they're not, and so could produce false equalities. * The 00 method doesn't require extra field(s), and don't produce values that appear to be equal to values they are logically different from. What's not to like here?
If you want the "Average" but you know that the data isn't correct and also don't want to clean the data, you need to try to get the "Median" instead. Median means to get the mid value of a list, that may be more precise for your need, I can't think of a query right now, I can try later, but start by trying to make a function that brings the median if you send the name of the item.
I can help you get median to work. I'm not sure you want median. You need to decide what works best because average takes some extremes into account, while median doesn't. You may want everything between quartiles 1-3 for example, while choosing to omit quartile 4, or only including data up to the median of q4. All of it is accomplishable but your inner join is going to need to include that logic in order for it to work. You may want this as a stored procedure that updates at daily intervals which would then allow the parent query to run periodically throughout the day to give you prompt results. edit: You may to look at a sample of the past *n* months of data to determine exactly what range you want to exclude and then rephrase your question. Box plots would work well for that.
Can you give us some examples of what specifically you do that you are hitting the ceiling on? What type of tasks do you consider the most advance in your day to day duries?
Join your tables or finish your non ANSI join if you must. Do you even need the table product categories? Also.....alias your count and order by alias asc How are the names stored??? Show sample data
All depends on how you treat those cases, if there is only one data point as a reference then it doesn't matter if you call it the mean, median, mode, upper quartile, bottom quartile, etc. That's the only reference you have and it should be reported, but probably notated based on a count of *n*. Seems like you need a lot more business rules for your question.
I may have built a CTE the other day that then selects into a temp table 👀
would SAS be a good program to learn more advanced SQL? thats what most at my company use
It would all depend on what you'd consider 'advanced'. Joining 10 tables in a select could actually still be pretty basic in principle. If you're trying to get your head around 10 joins then I wouldn't worry too much about advanced stuff right now, make sure you've got the basics straight in your own mind otherwise you're going to be building a house of cards with a wobbly foundation. Personally I used SAS before I used SSMS but now only use SSMS (company I am at don't have SAS) and I now prefer it. As you've been recommended previously, start with the base table and join tables one at a time and build from there. Don't try and jump in and do it all at once, you'll only make things harder for yourself.
comma joins are the devil
Run it with an Actual Execution Plan then examine this, it'll point you to where your performance issues are. If you've not looked at execution plans before then I'd suggest you watch some youtube videos of how they work. I'd also suggest downloading SQL Sentry Plan Explorer, it will give you a lot more information than the tool built into SSMS.
There is a bunch of stuff that you don't tell us so exact answers are going to be hard but here are some places to look. Are there any indexes on these tables? Try adding an index that supports your query. Look at the WHERE clauses in the two separate queries. You don't say what type of RDBMS you are using. If it is SQL Server there a number of query tuning helps available. Turn on the option to show the execution plan. You are likely to find that it is joining every row in the first table with every row in the second table. This results in a record set that is then filtered by the WHERE clause. This behavior wastes a lot of time.
Awesome, thanks so much for he suggestions! I think your lady paragraph is probably my problem. Adding indexes will help too--I think that's why the 40 second query takes so long on its own. 
Thanks, hadn't thought of that. Seems like a good starting point to search from.
Please never write the title or parts of it in all caps. That's the easiest way to get no answer.
What you have probably installed is the SQL Server piece, so now you need a SQL client unless you want to do everything from a terminal; which I would not recommend. I use a program called HeidiSQL quite a bit, but there are probably several SQL client programs you can choose from that would be Mac compatible. Edit: A coworker of mine uses Sequel Pro on his Mac if you want to check it out: http://www.sequelpro.com/
well my bad! Was typing on my phone and kept Caps Lock on by mistake, sorry about that
thanks, yeah that's probably the case, the tutorial on the website kept leading me back to using Terminal! I'll check it out, thanks!
Apology accepted.
To add to this, I would suggest using [MySQL Workbench](https://dev.mysql.com/downloads/workbench/). It is a great tool that is from Oracle and it is what I used for my schooling. It also has the great feature of being able to create EER or Enhanced Entity-Relationship Diagrams. This tool is awesome and really beneficial for learning how to model databases and giving you a visual representation of the database to see how everything relates. If that's not something you need than like /u/tylerhipp said, HeidiSQL does what you'll need. I believe this is correct but if someone can correct me if I am wrong, I'd appreciate it. I am not sure how to do this on mac but what you need to figure out is how to run MySQL service on mac. That will be the server running and storing the database. You then need the client, i.e. workbench or heidisql like we said before to connect and work with the data easily using a GUI. 
 You aren't guaranteed to have *just one* outlier, and even median could fail to help on a small enough sample set with outliers. There may be a fancy way to do this (because I assume you're no the first to run into dirty data you can't clean) but the brute force method I'm coming up with is to average twice. The first time a straight up average like you've listed, the second one (that you use on your report) does not include any rows in the average where the curcostperitem is more than 50% percent different from the first average. increase or decrease that percentage as needed to get the cleanest results you can. DECLARE @ReportingPeriodBegin DATETIME DECLARE @ReportingPeriodEnd DATETIME SET @ReportingPeriodBegin = '03/21/2016 05:00:00' SET @ReportingPeriodEnd = '03/28/2016 4:59:00' SELECT Items.strmerchItemName , Items.total , BestAvg.Cost FROM ( SELECT tblMerchItem.strmerchitemname , SUM( tblredemptionacctitemdetail.lngquantity) AS 'Total' FROM tblRedemptionAcctItemDetail INNER JOIN tblM erchItem ON tblRedemptionAcctItemDetail.lngMerchItemID = tblMerchItem.lngMerchItemID WHERE dateOrdered BETWEEN @ReportingPeriodBegin AND @ReportingPeriodEnd AND tblRedemptionAcctItemDetail.lngMerchItemID IS NOT NULL GROUP BY tblMerchItem.strMerchItemName ) Items INNER JOIN ( SELECT AVG( curcostperitem) AS 'Cost' , tblMerchItem.strMerchItemName FROM tblInventory INNER JOIN tblMerchItem ON tblInventory.lngMerchItemID = tblMerchItem.lngMerchItemID WHERE tblInventory.dateReceived &gt; DATEADD(yyyy, -100, GETDATE()) GROUP BY tblMerchItem.strMerchItemName ) BadAvg ON BadAvg.strMerchItemName = Items.strMerchItemName INNER JOIN ( SELECT AVG( curcostperitem) AS 'Cost' , tblMerchItem.strMerchItemName FROM tblInventory INNER JOIN tblMerchItem ON tblInventory.lngMerchItemID = tblMerchItem.lngMerchItemID WHERE tblInventory.dateReceived &gt; DATEADD(yyyy, -100, GETDATE()) AND curcostperitem BETWEEN ( BadAvg.Cost - (BadAvg.cost / 2)) AND ( BadAvg.Cost + (BadAvg.cost / 2)) GROUP BY tblMerchItem.strMerchItemName ) BestAvg ON BestAvg.strMerchItemName = Items.strMerchItemName ORDER BY a.strMerchItemName 
This looks like it may work out and cover most of the bases. Thanks for your help
No worries, it's easy to over-think these things. Obviously, beware that if you keep running this then your quantity figure is just going to keep increasing each time.
Thanks, I'll try and check this out when I'm at home in the evening and let you guys know if I have any questions!
This definitely seemed like click bait at first but based on comments I decided to click. I've read some things by lukaseder before and I've liked his tutorials. I've barely been in the professional field a little less than a year but I've had massive exposure to sql. They've thrown me at the mainframe and rewriting old Ideal(it's like Cobol) programs with sql and java. I struggled hard at first and even had doubts I could do the job. But I stuck it through and researched and read and read in my time at home. Everything mr lukaseder says here is what I've learned on the job. I wish I could have seen this when I first started!! I cant stress enough, if you are a beginner, read this article! Many beginners think that their statements begin with the select clause, etc. But it's totally not what you would think (at first). 
The second point i just meant use the match operator, which can utilize full text indexes better than like, depending on your DB system, which iirc yours supports. Look up full text indexing when you have to do lots of text searches. I did mean pull out city/state/county into one, and that way you only need a cityID(int) column in your table, which will reduce size, and make any lookup that has city county or state specified able to use a much smaller table and index (city code) to do their seeks. Multi-column index order means what order you specify the column. For example, the index ('First', 'Last', 'County') is entirely different thant he index ('County', 'First', 'Last'). The way multi-column indexes are used is left to right. Meaning in your (First/Last/County) index, you -must- have a first to use the index, and if you have a last then it makes it even better. But if you only have a first and a county, it's no different than just using first. So in this case, say we wanted All Last name Johns with a first name starting with m. If we had an index Last, first, it would allow us to use both and seek very quickly to the M section of Johns. Likewise, if we wanted all first name Susans with a last name starting with T, a First/last index would allow that. Now, this has a pretty interesting property. If you somehow know every value an index covers, you can use the column in the index even though you're still saying you don't know what you want. So for example, If you had the following multi-column index: CityID/LastName/FirstName Normally you'd think you can't use that index unless you know the city id. However, since (with the second suggestion) you have a table full of every CityID, you can join it and it will use the data from that to use the cityID column in the index. So if you only had a last name, you could actully use the city/last/first index by doing: &gt; Select * from People P join Cities C on P.CityID = C.CityID where lastname = 'John' This is very useful when you have a column you normally use a lot, but sometimes don't. In your case, since you use counties a lot, This lets you reduce counties in the city table, then use a multi-column index in your main table utilizing the cityID as part of it. Keep in mind bigger/more indexes have a direct trade off on space and insert times, but if you need speed over both of those, proper multicolumn indexes will do wonders for you
I'm not sure a trigger will work because the business wants to be able to schedule the sproc to run in advance. For example, today is 3/30 and they want it to run on 4/2 at 8am. 
The issue is that they won't know when they'll want it to run until a few days beforehand. They may tell me today "we want it run tomorrow at 8p." So, I'd update (or insert, haven't figured that part out yet) the datetime to tomorrow at 8p. Then, when the job runs at 8p it'll exec the sproc.
Right I see, so really you just want them to be able to set a schedule for the job. if the table only has one row you should be fine to run once a minute, but have a look to see if you can give them a front end to a proc (Ssrs, excel) that can update sysjobschedules
I've created a Job that check if certain conditions are meet, ie getdate being between certain Dates and then execution a stored procedure. For instance service windows, is all handled by the system because we get notified via email excel when service windows are for our 3rd party system, we then have a via procedure on the email dump the excel sheet, and a ssis package that loads the dates, and a job that checks when the service window is and then disables all our 3rd party jobs 15 min prior to the 3rd party service window begins, and then starts all 3rd party jobs again 15 mind after service window End. Works wonders. 
That status column is a freaking awesome catch. QA would've had a field day with that one :) You hit the nail on the head with the dynamic sql. I'm basically going to have the job exec a sproc that dynamically loops through all the sprocs in my process. &gt; I'd check once a day at a designated time (usually beginning of business or end of business) and if the job is scheduled, run it How would this work if they want it scheduled at 230p one time and 8a the next time and xx:xx the next time. If I can convince them to always have it occur at the same time, your solution is perfect..
Here we go: WITH CTE AS( SELECT Invoice, Date, Sum(Amount) OVER (PARTITION BY Invoice) as SumAmount FROM TableA) SELECT * FROM CTE WHERE SumAmount &lt;&gt; 0
Push back on running it during the business day as that could interfere with actual production system use. There's almost never a good business reason for running a 2-3 time yearly process at a given time of day. Find out *when* it needs to be complete by when it's run and work from there. When I've seen something like this implemented it either needed to be done before batch started in the evening (end of day run) or it needed to be done before people logged in the morning (Beginning of day run) 
[Alternatively](http://sqlfiddle.com/#!9/38dd92/3) if you don't or can't use CTE's for whatever reason (stuck on another platform): select * from TableA join (SELECT Invoice, Sum(Amount) FROM TableA group by invoice) as SubQ on Subq.invoice = tableA.invoice and subq &lt;&gt; 0
[HAVING (Transact-SQL)](https://msdn.microsoft.com/en-us/library/ms180199.aspx) Write your query like normal and add this to the bottom: HAVING SUM(Amount) &gt; 0
If he uses the group by and also wants to show the Data field, he'll need to add the date in the group by and that way SUM won't work. SELECT Invoice, Date, Sum(Amount) as SumAmount FROM TableA GROUP BY Invoice, Date HAVING Sum(Amount) &gt; 0 This query won't SUM anything using OP's example, that is why CTE or subquey is needed. But if he removes the date, this will work: SELECT Invoice, Sum(Amount) as SumAmount FROM TableA GROUP BY Invoice HAVING Sum(Amount) &gt; 0
&gt; WITH CTE AS( &gt; SELECT Invoice, &gt; Date, &gt; Sum(Amount) OVER (PARTITION BY Invoice) as SumAmount &gt; FROM TableA) &gt; SELECT * FROM CTE &gt; WHERE SumAmount &lt;&gt; 0 This worked perfectly! Thanks. I dont know anything about CTE at all. Someone showed it to me a while back, but it didn't stick. I'll look into learning this. Thanks again!
Can't you just use a `HAVING` clause such as: select invoice, sum(amount) from source group by invoice having sum(amount) &lt;&gt; 0
Maybe on your once (possibly twice?) day check to see if they added anything in the table. If there is the new entry, try doing an [sp_add_schedule](https://msdn.microsoft.com/en-us/library/ms187320.aspx?f=255&amp;MSPPError=-2147217396) to add that time as a schedule to your job. Then delete all schedules from the job once it runs.
Just an FYI, you are doing this just to learn, try XAMPP. It installs the LAMP stack even on windows. (Linux), Apache, MySQL, PhP. Installing this stack on your own can be painstaking and was very discouraging especially on Windows my first time ages ago. Xampp includes MyPHPAdmin which can get you by. Although I agree with /u/stevenporte1 you should get the MySQL workbench. A good GUI makes starting out much easier.
Thanks for the suggestion! Tried out MySQL WorkBench and it's exactly what I'm looking for!
Just read closer, you're correct. No matter what you need a subquery to include the date, but your method isn't necessarily going to do that depending on how that table functions with dates. Complex dates with multiple +/- transactions could give bad data for the date if it was being used for analysis. Not saying you're wrong, only that OP should probably ask these questions of the dataset first.
A simple total solution for both cases would look like this: select a.invoice, a.sum, min(b.date) as 'date' from ( select invoice, sum(amount) as 'sum' from source group by invoice having sum(amount) &lt;&gt; 0 ) a inner join source b on b.invoice = a.voice group by a.invoice, a.sum
Sure why not? Just select your subquery and then start up the query designer to edit the subquery itself.
Running every minute is overkill IMNSHO. Look at the AT command in Windows. You can set up your command to run at a specific date and time in the future. It uses the Task Scheduler which you are already using to do your once-a-minute thing.
Here is a very strange idea: Consult the ICAO alphabet guide. It gives pronunciation help for numbers. Translate your digits into the associated words. Now run your Soundex on the result.
Look into SQLite. I know that it works well on Android. Yes you will have to bone up on the SQL language and concepts. But since there SQLite implementations on Windows, Android, and IOS you can sandbox your learning in your favorite environment. [This](https://www.google.com/search?q=sqlite+android&amp;oq=sqlite&amp;gs_l=serp.1.2.0i71l8.0.0.0.34219.0.0.0.0.0.0.0.0..0.0....0...1c..64.serp..0.0.0.yNd_oKq9fwU) Google search will give you some starting points.
It is if you highlight your query then click the designer control. but it doesn't show the table as a linkable object in the designer but the subquery is still present in the where clause
Ok, so assuming the table you put at the top is the data set you're working with, the easiest way to do this is a subquery with MAX like /u/willwriteforpie said. So something like SELECT eventID, name, eventDate FROM yourTable WHERE memberInLab = (SELECT MAX(memberInLab) FROM yourTable) Make sense? Here I'm assuming "yourTable" is the table or query that generated the data you have in your question. No need for that outside HAVING from what I can tell.
I would reach for NTILE here.
First off you need a GROUP BY statement. Probably your unique ID for the gaming session. Also you'll need to put some parenthesis in your WHERE clause. WHERE ( did_spend= "yes" AND (Datefield &lt; "upper date value" AND Datefield &gt; "lower date value" ) AND mode= "campaign") Try that and see where you get. I'm on mobile, so sorry for bad formatting. 
Because you told it which table to associate with which alias... select a.col1, b.col2 from tableA as a join tableB as b on a.keyID = b.keyID
It will depend of the query, SQL doesn't know if you don't tell :D For example: SELECT customer FROM Clients That will return the customer from the table Clients, now see this example: SELECT ID FROM Clients JOIN Sells ON Clients.ID = Sells.ID That will return an error, because SQL won't know from which table that ID is, that is why you make and use alias, so you could make this: SELECT Clients.ID FROM Clients JOIN Sells ON Clients.ID = Sells.ID Or SELECT C.ID FROM Clients as C JOIN Sells as S ON C.ID = S.ID The 2nd example uses Alias to give a temporary name for the tables. 
You alias it yourself. I'm on mobile, so pardon the formatting: SELECT c.ingredients, d.shape FROM CookieTable AS C LEFT JOIN CloudTable D ON C.sharedcolumn=D.sharedcolumn You'll notice "AS C" denoted the CookieTable as C. But you don't have to use AS. The CloudTable is denoted by just adding the letter after it. 
I see that you defined it in there. Maybe I overlooked the first line on the from clause before. I could have sworn I have seen queries where they don't define the table though. I took a class using MySQL and I thought I had seen a query or two without defining the tables in the from statement.
It would on his date fuel at the very least, would it not? His BETWEEN function did not have parenthesis, which left the second argument out alone, correct?
Can you provide a screenshot? I've just tested your code and it worked fine for me: Created temp table with 4 columns, based on the info provided: CREATE TABLE #game_data (sessionid INT IDENTITY(1,1) NOT NULL, userid INT NOT NULL, did_spend BIT NOT NULL,event_time DATETIME NOT NULL) INSERT INTO #game_data (userid, did_spend, event_time) VALUES (1, 1, '20160101 00:00:00'),(2, 0, '20160101 00:00:00'),(3, 1, '20160101 00:00:00'), (4, 0, '20160101 00:00:00'),(5, 1, '20160101 00:00:00'),(6, 0, '20160101 00:00:00'), (7, 1, '20160101 00:00:00'),(8, 0, '20160101 00:00:00'),(9, 1, '20160101 00:00:00'), (10, 0, '20160101 00:00:00'),(1, 0, '20160101 00:00:00'),(2, 1, '20160101 00:00:00'), (3, 0, '20160111 00:00:00'),(4, 1, '20160111 00:00:00'),(5, 0, '20160111 00:00:00'), (6, 1, '20160111 00:00:00'),(7, 0, '20160111 00:00:00'),(8, 1, '20160111 00:00:00'), (9, 0, '20160111 00:00:00'),(10, 1, '20160111 00:00:00'),(1, 0, '20160130 00:00:00'), (2, 1, '20160130 00:00:00'),(3, 0, '20160130 00:00:00'),(4, 1, '20160130 00:00:00'), (5, 0, '20160130 00:00:00'),(6, 1, '20160130 00:00:00'),(7, 0, '20160130 00:00:00'), (8, 1, '20160130 00:00:00'),(9, 0, '20160130 00:00:00'),(10, 1, '20160130 00:00:00') There's 30 records 15 did_spend = 1 (yes) 15 did_spend = 0 (no) Ran this script and got the correct count of 15: SELECT COUNT(did_spend) AS 'did_spend' FROM #game_data WHERE did_spend = 1 AND event_time BETWEEN '2016-01-01' AND '2016-01-31' Edited to add: try using 1 instead of 'yes', like I did, if the did_spend column is BIT data type then yes won't work as the data is either 1 or 0
very good, thanks!
I would suggest to make more tables. 1 - Places, but without the category/subcategory fields 2 - Category, just to register the categories 3 - Subcategory, to register the subcategories 4 - Place_Category, with the placeID and categoryID 5 - Place_Subcategory, with the placeID and subcategoryID That way the Place_Category for example could store the Place several times with different categories, so you can add as many as you want, and same idea for the Place_Subcategory table In the app, for each Place registered the user will have a sub form listing the Category and Subcategory and can add as many as he wants.
That actually makes a lot of sense. Then have my data entry form break the data down and insert to a few different tables?
How do I initially make sure the PlaceID gets populated into all of the other tables? Make that happen when I initially create a place? Or whenever I do something with a place, have it pull Places.PlaceID and use that to fill a column? I'd imagine I want PlaceID/CatID/SubID to all be auto increment, too.
Yup, this solved it. Thank you and thanks /u/willwriteforpie as well. Can't believe it was this simple.
Depends on what role the interview is for? You're going to want to show completely different skill sets for DBA, Business Intelligence or Data Architect roles. 
Here is the query, with proper JOINs made, try to avoid implicit JOIN like in your 2nd query, it is a bad habit to have for queries. SELECT TG.intGameID, TG.strPlayerName, TG.blnComputerMovesFirst, TGD.strGameDifficulty, TGO.strGameOutcome, TG.dtmPlayed, COUNT(dbo.TGameMoves.intMoveIndex) AS MoveCount FROM TGames AS TG JOIN TGameDifficulties AS TGD ON TG.intGameDifficultyID = TGD.intGameDifficultyID JOIN TGameOutcomes AS TGO ON TG.intGameOutcomeID = TGO.intGameOutcomeID JOIN TGameMoves ON TG.intGameID = TGameMoves.intGameID GROUP BY TG.intGameID, TG.strPlayerName, TGD.strGameDifficulty, TGO.strGameOutcome, TG.dtmPlayed, TG.blnComputerMovesFirst
 SELECT REQ.Skill ,CASE WHEN MY.Skill IS NOT NULL THEN 'I got this' ELSE 'I learn fast' END AS Status FROM RequiredSkills REQ LEFT JOIN MySkills MY ON MY.Skill = REQ.Skill
Most SQL code generators and most SQL developers I know (myself included) never use AS for the table alias, and almost always use AS for the column aliases. Just a little quirk like many other things. I tend to prefer verbosity myself as opposed to simplicity (eg using AS even when its not *required*) but for some reason I NEVER do it with table aliases. EDIT: And for the sanity of anyone who comes after you, OR even yourself - please don't use table aliases like a, b, c, d, e.... at least put SOMETHING in the alias to hint at the table name. Example: SELECT coolstuff.*, emptystuff.* FROM my_schema.mytablefullofcoolstuff coolstuff INNER JOIN my_schema.mytablefullofemptystuff emptystuff ON (coolstuff.pkey = emptystuff.fkey)
Jobs are going to be executed in a different context, so store the result set in a permanent table or a queue. FWIW, I believe, sp_start_job returns control right away and does not guarantee job (or any specific job step for that matter) completion.
I took my 70-461 exam yesterday, and even though I put in a fair few hours of revision, I didn't pass. I found it very difficult. I didn't have any geography data type questions. I am going to study way more and take a more practical, hands on approach to my studying and ensure that everything is completely solidified in my brain, and that I understand everything before I take the exam again!
Are you planning on truncating logs after each delete iteration or do you need them?
I don't need them but I think logs are generally stored, I don't believe I have access to truncate them. From what I've been told I'm under the impression that this is a temp issue as I'm doing ~100million rows in one transaction and commiting it/checkpointing it every so often moves the log out of cache to somewhere permanent and will avoid the issue?
I do it all the time. it is way better than table triggers.
You question isn't very clear; can you elaborate? Can you give an example of what the result you desire will look like?
This. My test started off with about 5 questions on XML querying.
Ok what questions did you struggle on or were unprepared for? Were there questions you had no clue. Do you remember some of them?
Fair enough. My rows are sequentially numbered within the reference number. So the record 123 would read sequence #1, then the next line would be #2, then for record 125, sequence 1 and then 2 again 
Like /u/rbardy, I'd prefer proper joins instead of implicit But to make the least changes to your starting point i'd just add a subselect SELECT TG.intGameID, TG.strPlayerName, TG.blnComputerMovesFirst, TGD.strGameDifficulty, TGO.strGameOutcome, ( SELECT COUNT( TGM.intMoveIndex) FROM TGameMoves as TGM WHERE TGM.intGameID = TG.intGameID ) as MoveCount, dtmPlayed FROM TGames AS TG ,TGameDifficulties AS TGD ,TGameOutcomes AS TGO WHERE TGD.intGameDifficultyID = TG.intGameDifficultyID AND TGO.intGameOutcomeID = TG.intGameOutcomeID
If you're on SQL server 2012 or later you could see if you can get the LAG command to work for you. You could also apply a cursor here but please don't do that, SQL is set based for a reason, your performance will drop through the floor.
I'm having trouble envisioning this from the description, can you put up a table layout? From how I'm interpreting it SELECT cfRecord -- gets the "record" (I just hate using actual reserved words as fields) FROM cfTable -- with the higher sequence. WHERE EXISTS ( SELECT 1 FROM cfTable inCF WHERE cfTable.cfRecord = inCF.cfRecord -- where it's the same record AND cfTable.cfSequence &gt; inCF.cfSequence -- the subselect's sequence is lower AND cfTable.A != inCF.B -- the second A isn't the same as the first B )
Awake or not the full description is still fairly silly, especially with the other column names.. it sounds like you just need SELECT ProdData.ProdID , ProdData.ProdDate -- , ProdData.LotNo -- No reason for any of this other crap.. -- , LotData.LotNo -- , ProdData.GoodQty -- , ProdData.NoGoodQty , SUM( LotData.Quantity) -- , LotData.NoGood FROM LotData INNER JOIN ProdData ON ProdData.ProdID = LotData.ProdID GROUP BY ProdData.ProdID , ProdData.ProdDate Order By ProdID Desc 
I'd work on automating the common requests (as you've stated you've done) and then you could either dive deeper into the analytics (using Python or R) or work on the presentation of the data by doing some front-end work and making some kind of reporting panel they can look at. Could do all of that at the same time, your call.
I've not yet but will do next week as the database is temporarily unavailable, due to the full transaction logs.
Thanks, that looks good from my basic understanding, I'm still very new to this though so how do I integrate it with the above and get my values into those parameters?
yea, I really wasn't impressed with the exam. It seems like it concentrated on a small set of topics and the stuff I use daily I wasn't really quizzed on.
&gt; My rows are sequentially numbered within the reference number. um, no i see two rows with 123, and two rows with 125 how do you determine which ones come first, before other rows with the same number?
You haven't told the database how to do the join. Also, LIKE doesn't do anything special without a wildcard. SELECT s.Seller_Name ,c.Customer_Name ,p.Product_Name ,o.Sale_Date FROM Orders o INNER JOIN Seller s ON s.SellerId = o.SellerId INNER JOIN Customer c ON c.CustomerId = o.CustomerId INNER JOIN Product_Orders po ON po.OrderId = o.OrderId INNER JOIN Products p ON p.ProductId = po.ProductId WHERE p.Product_Name LIKE '%digger%'; 
Well, check your data. If the product wasn't on an Order, or the Order doesn't have a Seller and a Customer, the INNER JOINs will eliminate that.
thanks that worked. only thing is that its showing up yyyy-mm-dd i can convert that to mm-dd-yyyy in excel but is there a way in sql. im getting a conversion error when i try any style except 101 Conversion failed when converting date and/or time from character string.
Thanks, so that still does the cross referencing for my where clause, and that's the row count out my main data table, and not the number of rows from my lookup table? Thanks.
hmm. im on sql server 2012. is there any reason the format function isnt working?
Your join to Products doesn't actually link on anything, it's acting like a cross join. you need the intermediate table because you don't have anything to join on otherwise.
you could do something like recreate the basics of UBER or facebook 
It's no trouble, my Mrs is working nights and I'm skint so can't afford to go out. Anyway, I've quickly knocked up a copy of your data, maybe a few mistakes but it's good enough for government work This script gives me 16 rows (I did say my data might be off a bit): SELECT DISTINCT s.SellerName, c.CustomerName, p.ProductName, o.SaleDate FROM Orders_Table AS o INNER JOIN Seller_Table AS s ON o.SellerNo = s.SellerNo INNER JOIN Customers_Table AS c ON o.CustomerId = o.CustomerId INNER JOIN Product_Orders_Table AS po ON o.OrderNo = po.OrderNo INNER JOIN Products_Table AS p ON po.ProductId = p.ProductId WHERE p.ProductName = 'Digger' I then used MAX() on o.SaleDate and then GROUP BY on s.SellerName, c.CustomerName, and p.ProductName This returned 8 rows I noticed that customers have bought a digger from more than one seller. So, because you are including the seller's name in your query it is returning a record for each digger they have sold. e.g. in my data Bill and Mike both sold a digger to each customer 2 * 4 = 8 
I'm a photographer, so I suppose I could create a database of images or gear.
&gt; BOS-JFK Sequence 1, JFK-BOS Sequence 2 all within the record locator the data you presented -- 123 and 125 -- does not show this 
You could do biomes! Or a single biome. I'm feeling a savanna.
[SQL Server Express](https://www.microsoft.com/en-us/download/details.aspx?id=42299) is probably what you're looking for. It's the free version and will run fine on consumer-level hardware Windows versions. 
SQL Server Developer Edition is now free: https://blogs.technet.microsoft.com/dataplatforminsider/2016/03/31/microsoft-sql-server-developer-edition-is-now-free/ 
I hate to call you out but encourage you to use the DATETIME data type for date/time columns rather than VARCHAR. SQL Server is surprisingly good at taking various string formatted dates and converting them to DATETIME nicely. For display you can specify the format as shown elsewhere. If you ***must*** use VARCHAR then let your app convert user input into ISO-8601 format (yyyy-mm-dd hh:mm:ss and I scrub out the delimiters and space in line with the specification) and then let the app format for display.
What's the difference between SQL server express and SQL server developer edition?
Express is a pretty limited version of MSSQL, the developer edition is full-featured. It's the same software as the Enterprise edition, it's just licensed differently (i.e., you can only use it for development). So, things like database size limits and feature limitations are gone. If you're running it on your machine, though...make sure you know how to configure it. You need to adjust max memory settings, for example...otherwise whatever data you put into that database will live in your machine's RAM. Turn it down to something reasonable (if you have 16 gbs of RAM, I'd set the Max Mem to 2gb at most). I'd also have a separate hard drive (if you have one laying around...you don't need to buy one) for the files related to the database. Doesn't need to be a server set up, with an individual drive for logs, data and tempdb...unless you're really putting a lot against it. But if you're just using it for typical dev tasks, put all that on one drive and you'll be fine. 
Thanks for your thorough response - seems like the developer edition is the way to go but I had some trouble with the download (some error kept coming up about SQL BootStrap not being installed). I installed SQL server express instead and things have been running fine so far. As far as the configurations - in Management Studio I right clicked on my server, then clicked on properties, memory and then set the maximum server memory in MB to 2000 which is 2 GB. Is that the correct way to do it? 
Yep, that's the one. Should take effect immediately, you don't have to restart the services with max memory settings. 
get MS SQL Express. The limitations are at points you will likely not hit. Mostly in the multi-user arena will you hit limitations. You can also spin up a server in the free tier or just above on amazon AWS.
Yep! You can follow the guide for SQL 2012 [here](https://www.mssqltips.com/sqlservertip/2694/getting-started-with-sql-server-2012-express-localdb/) or read more about it [here](http://www.asp.net/mvc/overview/getting-started/introduction/creating-a-connection-string). I think there is a 2014 version now, but I'm still using 2012 and it works great. After install, you should have a default instance set up at either `(LocalDB)\v11.0` or `(LocalDB)\ProjectsV12`. One of the VS updates changed my path so I can't be sure which one it is. You don't have to follow the CLI commands in the guide, unless you want a specific named instance. If you don't already have SQL Management Studio installed, you'll want to install that so you have access to manage everything (unless you are a CLI junkie). That's pretty much it -- the SQL instance doesn't run until it receives a connection request, either in SSMS or Visual Studio. Much, much, lighter load on your resources. If you are using VS or EF (probably not, but this works at enterprise level too), your connection string will look like this: &gt; &lt;add name="MyContext" connectionString="Data Source=(LocalDB)\v11.0;Initial Catalog=foo;Integrated Security=True" providerName="System.Data.SqlClient" /&gt; 
Find the median using row_number over(partition()), find how to get a running count (so you can find a distribution), find how to get a row total, column total, and a grand total, find how to pivot. 
You don't need the group by in your subquery, you don't even need a sub-subquery, just select avg(capacity - booked seats) from your joined tables. And this query returns destinations with less empty seats than the average, not more.
This should work: select a.destination from departure a inner join airplane b on b.airplaneid = a.airplaneid inner join airplane c on c.modelid = b.modelid and c.capacity &gt; a.bookedseats group by a.destination having avg(c.capacity - a.bookedseats) &gt; (c.capacity - a.bookedseats) order by a.destination edit: ~~It should also be a lot more efficient than your other examples because it's only joining on rows where the capacity is greater than the booking, and then from there only selecting the destinations where the average of all is greater. At least I think that is how the execution plan would work.~~ See comments below. Your approaches should work as well so long as you have the same equation for how to derive the destinations based on the average.
I assume this would be for a mid level analyst position, so here are some basics I'd generally ask in an interview: •CREATE/ALTER •SELECT •UPDATE •DELETE •Transactions, Different joins, INNER/OUTER (Left and FULL). •TRUNCATE vs DELETE vs DROP. •CTE, tempdb, InformationSchema •ROW_NUMBER vs RANK vs DENSERANK, etc. Since its BI, I would also ask you a few SSRS and SSIS questions to see if you can take data from Excel/csv form all the way to tables/views/reports. Hope that helps a bit.
This isn't right it will only show average empty seats by destination, not departure. Also you can't mix aggregates and non aggregates fields in a having clause.
Yes this is correct although since the problem says list all destinations, you might want a DISTINCT in your select to avoid repeating destinations. Also you have a typo, in your subquery it should be joining to a1.modelid not a.modelid Oh, and you have your two values backwards, it should be capacity - bookedseats, not bookedseats - capacity (that's a negative number) EDIT: [SqlFiddle](http://sqlfiddle.com/#!9/30cd8/2 ) See how Chicago has 1 empty seat, and Boston has 3. Average empty seats across all departures is 2. Boston has more empty seats than that and is returned.
Early on a Sunday while having my coffee. Even still I would ask for clarification on that point as the question seems to imply it is only interested in flights which have an empty seat in the first place. I would submit the question like this `--and c.capacity &gt; a.bookedseats` and then circle it and write a note next to it asking for the intent of the question.
First thing, actually practice explain, out loud (to your dog or a rubber duck or whatever), the basics. What's a database? How do records, tables, and dbs all relate? Explain selects, joins, where clauses, unions, updates, drops, etc. You'll be surprised how hard it is to put in words all this stuff you know, so practicing it once it valuable. * How are VIEWS used? How about HAVING? * What's the purpose of an index? How do you decide on your indexes? * Some language-specific questions (e.g. I'd ask a Hive candidate about partitions and clusters). Find out what flavors of SQL they use and read up a bit on what makes it different from the ones you know 
Using postgresql I've used recursive function which is then fed to QT treeview widget
&gt; 3) Give me an example of using the partition by statement As an interviewee, this question would make me cringe. Good question.
It's Oracle's Hyperion Essbase, which is a multidimensional database engine. I'm taking flat files from their ERP and developing a new analytic financial system (budgeting, forecasting, reporting, etc.) with customized rollups and metadata properties. The administration console to maintain the hierarchies is all cloud-based now and although it shows the dimension hierarchies in a nice format, it's so much faster and easier to review and collaborate the edits in an Excel file. 
You can do this with recursive CTE if your DB (which you haven't mentioned) supports it. Here's an Oracle solution (without using Oracle's proprietary connect by syntax). To use on another DB, you'd have to replace the lpad for the equivalent and maybe the string concatenation operator ||. Notes : Your test data isn't correct. FRUIT, NUTS should have records themselves as CHILD with a null PARENT, so my example fixes that. with TEST (PARENT, CHILD) as ( select 'FRUIT', 'APPLES' from dual union all select 'FRUIT', 'PEARS' from dual union all select 'FRUIT', 'BERRIES' from dual union all select 'BERRIES', 'BLUEBERRIES' from dual union all select 'BERRIES', 'BLACKBERRIES' from dual union all select 'BERRIES', 'STRAWBERRIES' from dual union all select 'NUTS', 'ALMONDS' from dual union all select 'NUTS', 'PISTACCHIOS' from dual ) ,FIXTEST as ( select * from TEST union all select distinct null, PARENT from TEST t where not exists (select null from TEST t2 where t2.CHILD = t.PARENT) ) , TREEWALK (PARENT, CHILD, DESCR, PATH, LVL) as ( select PARENT, CHILD, CHILD, CHILD, 1 from FIXTEST where PARENT is null union all select f.PARENT, f.CHILD, lpad(' ', lvl * 3, ' ')||f.CHILD, PATH||'/'||f.CHILD, LVL + 1 from TREEWALK t join FIXTEST f on f.PARENT = t.CHILD ) select DESCR from TREEWALK order by PATH DESCR FRUIT APPLES BERRIES BLACKBERRIES BLUEBERRIES STRAWBERRIES PEARS NUTS ALMONDS PISTACCHIOS 
For simple activities/tests, sqlfiddle.com may be a good fit. MySQL is pretty straight-forward to install though, so you may want to consider just installing it locally if the intent is just to practice.
First, it's specified that you need to use a variable to hold the "customer ID" marked for deletion with a default value of 8. Variable declarations are formatted like this: DECLARE @&lt;variable name&gt; &lt;DATATYPE&gt; = &lt;default value&gt;; Delete statements are formatted like this: DELETE FROM &lt;table name&gt; WHERE &lt;condition&gt;; Queries with automated rollback/commits are formatted using try/catch blocks rather than if/else statements, like so: BEGIN TRAN; BEGIN TRY &lt;query&gt; COMMIT TRAN; END TRY BEGIN CATCH ROLLBACK TRAN; END CATCH
Hi,Thank you for help. I also needed @@ROWCOUNT &gt; 1. Sorry for bothering you but can you help me on this one too? Write a set of statements coded as a transaction block to move deleted rows from the Customer table to a CustomerArchive table. Delete records from the Customer table that do not have any orders in the Orders table. The related Address records must be deleted before the Customer records. Use the statements you already coded for #1 and add the selection of Customers with no orders as the delete records statements. Insert the customer records that will be deleted into the CustomerArchive table first. 
Unless it's a typo you can have a field reference itself, managerid to managerid. Otherwise you'd never be able to add values. You can reference another field in the same table though. E.g. all employees have an employee ID, and a manager ID which is the employee ID of their manager. This is called an adjacency list if you want to look up how to manage it. Another hierarchy model is a directed acyclic graph. Both require some amount of customised maintenance procedures.
A foreign key where a field references itself does not usually make much sense. Not sure exactly what you are trying to do but when looking the table definition, it seems you can define a multi-column foreign key referencing the same table like this: ALTER TABLE Assignment ADD CONSTRAINT Assignment_Manager_FK FOREIGN KEY ( MANAGERID, ManagerProjID ) REFERENCES Assignment ( RescID, RProjID )
But you understand the concept and could answer interview questions about it, no?
I highly doubt @@ROWCOUNT &gt; 1 is what you needed for four reasons: 1) The most obvious reason is that if there was only one row affected, it would evaluate as false. You would want @@ROWCOUNT &gt;= 1 or @@ROWCOUNT &gt; 0 2) Assuming you meant to put @@ROWCOUNT &gt; 0, and @@ROWCOUNT worked the way you thought it did, it would still be incorrect (or at least pointless), because if the row count was less than 1, there would be no reason to rollback/commit because there wouldn't have been any changes 3) @@ROWCOUNT resets after each query, so if there were records in Addresses that were deleted @@ROWCOUNT would be set to a value greater than 0, but then if there were no records in the customer table that were deleted, @@ROWCOUNT would be set to 0 again. In this scenario, your query would incorrectly rollback a transaction leaving orphaned records in the Addresses table. 4) Having 0 records affected doesn't mean that it executed unsuccessfully Let's look at this scenario: I created two tables, Customers and Addresses, and Addresses has a foreign key constraint referencing Customers.ID: CREATE TABLE Customers ( ID INT IDENTITY(1,1) PRIMARY KEY, Value INT ) CREATE TABLE Addresses ( ID INT IDENTITY(1,1) PRIMARY KEY, CustomerID INT REFERENCES Customers(ID) ) I inserted some data: INSERT INTO Customers VALUES(1), (2), (3) INSERT INTO Addresses VALUES(1), (2), (3) I then ran this query: BEGIN TRAN DELETE FROM Addresses WHERE CustomerID = 2 WAITFOR DELAY '00:00:15' DELETE FROM Customers WHERE ID = 2 ROLLBACK TRAN And immediately switched to a new query window and ran this query: INSERT INTO Addresses VALUES(2) If you follow the same steps correctly, you'll have inserted a value into 'Addresses' for the customer ID of 2 before the record in 'Customers' has been deleted. This means that once the delete query reaches "DELETE FROM Customers WHERE ID = 2" it will fail due to the foreign key constraint. I believe this is the scenario your teacher is wanting you to prevent, but your query does not handle it correctly. Source: Am Microsoft Certified Professional SQL Developer (70-461), and this is also a common situation I (and most of SQL devs) have to handle in the workplace
W3 schools for the fundamentals. Once you start googling things that are beyond the fundamentals you'll be seeing a lot of Stack Overflow
Sure. Feel free to dm me
No, it doesn't need to be a query, and this is great! I figured that once I gave up on trying to do this in SQL, that I'd just make it happen in VBA.
are both databases postgres? A db link could be your solution. 
This sounds like a situation where a CTE (Common Table Expression) might be useful. I believe you can wrap your first query in a CTE and then join the second table to that. Something like this: with CTE AS ( SELECT forums_posts.sub_id, ( SELECT MAX(latest_posts.thread_id) FROM forums_posts latest_posts WHERE MAX(forums_posts.submitted_dt) = latest_posts.submitted_dt AND forums_posts.sub_id = latest_posts.sub_id ) AS thread_id FROM forums_posts GROUP BY forums_posts.sub_id ) select * from CTE JOIN forums_threads ON forums_threads.thread_id = cte.thread_id
You can install sql server express and adventure works database for practicing as well. 
Thank you both - yeah I figured this much, but I'm just wondering why there are so few new innovative resources for SQL, when there is so much for other programming stuff (I'm thinking interactive sites, learning games, moocs etc.)
the term is database link. It really depends on what type of access you have etc... Depending on the # of records you are working with. This might be a nice solution for just plain old excel. I'm an oracle guy so my thoughts would be database links or external tables. 
Hello again. It will be easier to help you if you give us an example of what you want. For example, do you want this dataset: |Production ID |Production Date |Macola Part Number |Part Name |Lot Number |Die Cast GoodQty |Die Cast NoGoodQty |Trim Shop GoodQty |Trim Shop NoGoodQty |GoodQtyDifference |NoGoodDifference |1 |2016-04-04 |1 |Part 1 |1 |10 |1 |10 |1 |0 |0 |1 |2016-04-04 |1 |Part 1 |2 |17 |3 |16 |4 |1 |-1 |1 |2016-04-04 |1 |Part 1 |3 |13 |1 |14 |0 |-1 |1 To look like this dataset: |Production ID |Production Date |Macola Part Number |Part Name |Lot Number |Die Cast GoodQty |Die Cast NoGoodQty |Trim Shop GoodQty |Trim Shop NoGoodQty |GoodQtyDifference |NoGoodDifference |1 |2016-04-04 |1 |Part 1 |1,2,3 |10 |1 |10 |1 |0 |0 |1 |2016-04-04 |1 |Part 1 |1,2,3 |17 |3 |16 |4 |1 |-1 |1 |2016-04-04 |1 |Part 1 |1,2,3 |13 |1 |14 |0 |-1 |1 Because that's what I'm reading, but it doesn't make sense to me.
&gt; Your example is correct, I would like the Lot Numbers to be displayed in that way.
isnull(substring((SELECT Distinct( ', ' + xsa.column_name) from table_name sa join table_name2 xsa on xsa.id = sa.id where sa.extra_id = xss.extra_id ORDER BY 1 FOR XML PATH( '' ) ), 3, 1000 ), '') 'Column Name' This is a basic example of a substring. It is separating by ', '. It can hold 1000 characters and it gives them each a length of 3.
Well.. are there other columns for those rows that give you the ability to match them up? customer id etc..? 
Okay, so first things first: The "nested searches", which I'm assuming you're referring to the "JOIN ( SELECT...", is what's called a correlated subquery, or an inline view depending on who you ask. "Inline view" is a good name because it explains exactly what it is: It's a view that you declare and use inside the query. So we could create a view from that subquery like this: CREATE VIEW latest_posts AS SELECT latest_posts.sub_id, latest_posts.submitted_dt, MAX(latest_posts.thread_id) AS X1 FROM forums_posts latest_posts GROUP BY latest_posts.sub_id, latest_posts.submitted_dt And then use it like this: SELECT forums_posts.sub_id, latest_posts.X1 FROM forums_posts JOIN latest_posts ON forums_posts.sub_id = latest_posts.sub_id JOIN forums_threads ON latest_posts.X1 = forums_threads.thread_id GROUP BY forums_posts.sub_id, latest_posts.X1, latest_posts.submitted_dt HAVING MAX(forums_posts.submitted_dt) = latest_posts.submitted_dt Also, the last query I sent you was embarrassingly wrong, I'm sorry. The "HAVING" clause is quite simple really. It's just a where clause that allows you to use aggregate functions inside of them. The normal where clause does not allow this because behind the scenes, the "WHERE" clause is performed before the "GROUP BY". 
It sounds like you'll need to use getdate() if you want to this to be a dynamic report that lasts into the future. You may look into something like - dateadd(mm, -1, getdate()) dictates the date range. Without adding more complex elements and trying to keep it simple perhaps a table that stores date ranges into the 2020's. If the getdate() element falls between a certain date range on that table then it calls the previous month's date range, also stored on that table. That date range would be used as the overall range of that data being pulled in SSRS. Here's a much better option (but much harder): Create a hardcoded report that is run at the end of each month and grabs the previous month's data. As you've found this is detailed in Stack Overflow. Set a SQL job agent to run this report. It will first drop the table, create the table then insert the previous month's data into it. Then on the SSRS side just link the simple select from that table you're inserting data into. Not only does this work flawlessly no matter what day they run the report you've also just put all the resources into a monthly task instead of the database being resourced each time they would run the report.
Assuming two tables Ingredients IngredientName varchar(50) UnitPrice decimal(8,2) TempStorage IngredientName varchar(50) Qty decimal(5,2) Price decimal(10,2) You can use a query along the lines of update TempStorage TS inner join Ingredients I on TS.IngredientName = I.IngredientName set Price = Qty * UnitPrice The exact syntax depends on the particular database you're using; what I provided is T-SQL syntax (SQL Server). [Here's a Stack Overflow bit that shows the syntax for some other flavors of SQL](http://stackoverflow.com/questions/1293330/how-can-i-do-an-update-statement-with-join-in-sql)
It's not necessarily hard - just like SQL or SSRS aren't really hard, but they are all broad topics that take years to master. It's a Lille like asking 'If you know a little Java, should you write a production application in C#?' Just because you may be able to doesn't mean that it will perform well, scale etc. Can you grab a Kimball book and look up what SCD is? Sure. Will you know when, how, and if you need to implement? Not without some practice. Or MDX is a query language that is deceptively like T-SQL that has its own learning curve. Can you pick it up? Of course. Will you make mistakes? Of course. How willing is your company to allow for mistakes, rewrites, and a long timeline? Learning curves are usually a bit longer if there isn't a mentor or guide, as well. The cube UI is friendly enough that you can create a 'hello world' cube without too much work. And depending on your data, that may be enough. But properly designing a data warehouse takes a fair amount of practice and work. If you're interested Kimball's books are always a good place to get the theory (not SQL specific) behind data warehouses. 
Start with tabular models with 1 fact table and a few dimensions. Limit your use cases to this solution until you feel more comfortable with dimensions and facts. You can evenow pull this in via power pivot (excel plug-in), and try to write some DAX statements. You will find this much easier than mdx and multidimensional models.
See, i tried to do something like this myself, but it was upset that both ingredient fields were memos. I remember now why i shy away from SQL whenever possible, lol. I don't even need to do math with it yet, i just need to get Price to pass from ingredient to TempStorage and cant even manage that, lol. 
Yes, you had a version of the inline view in your query, it was this part: ( SELECT MAX(latest_posts.thread_id) FROM forums_posts latest_posts WHERE MAX(forums_posts.submitted_dt) = latest_posts.submitted_dt AND forums_posts.sub_id = latest_posts.sub_id ) AS X1 Except the way you used it, it was a subquery, not an inline view. The difference between a subquery and an inline view is that the subquery is on a 'column' level while an inline view is at the 'table' level. The subquery is the easier of the two to write, and comes most naturally to new SQL developers because it's easier to think about individual column instead of sets of data. However, a subquery will execute for every single row in the returned result set, which means they're slow as a dog. An inline view, however, will only execute once, and so they're usually incredibly faster than a subquery. I'm glad to have introduced you to views, however I'll advise you to use them extremely sparingly. I would avoid referencing views within views. Your coworkers will hate you if you do that. SQL isn't an OOP language, avoid nesting when you can. If it helps you get your head around it, think of views as computed tables. &gt; So this is a living, breathing search with accurate information each time i look at it? Meaning that now this view exists, i can do a query for all the static stuff like sub-forum names and descriptions, and the stuff ive been trying to do based upon latest data, i can get that from the view in the same way i would a table, just join it and we're golden? Basically, yes. In MS SQL you can even update views like you would a table, I'm not sure if it's the same with MySQL/MariaDB though. 
So, i am a COLOSSAL moron. I could have sworn TS.Ingredient and I.Ingredients were both short text. Nope, one was long, one was short, the query works now. However, i'll still need what you gave me above once i try to do math between the two, so i didnt fully waste your time XD
Ah! Memo fields are BLOB fields and cannot be compared. Change their type to an appropriate length varchar field
my boss is a "get it done its so easy why can't you just get it to work" now type guy so i'm not sure how much they'll accept mistakes and doing things over and over which is probably what will happen. 
Came here to say something similar. Tabular is pretty straight forward once you spend a few hours in it. 
I'll assume that the Vehicle table has the field CustomerID, if that is correct then the SQL would be: SELECT C.* FROM Customer as C JOIN Vehicle as V ON C.CustomerID = V.CustomerID WHERE V.Make = "Toyota" 
Yeah, I've been there. You'll figure it out. The big difference with experience is doing it, or doing it right the first time, like any technology. Good luck. 
So at the outset I would say you're probably not looking at it correctly from a relational database perspective. If you're creating a new table for each recipe (or whatever the collection of ingredients is if it's not a recipe), then you're not leveraging the benefits of a relational database. Tell us specifically what you're trying to do, not *how* you're trying to do it and we might be able to provide better assistance. Based on what you've provided so far, it's quite likely that you need 5 tables at least: Ingredients, Recipes, RecipeIngeredients (for the many-to-many relationship between the two), Users, and UserRecipes (also many-to-many). Beyond that, we'd probably need to know more about your specific application.
select substring(MothersName,1,instr(String," ") -1) Fname, substring(MothersName,instr(String,",") +1) Lname from tablename
Ahh, ok I'll give that a go. Im unemployed basically, i've had medical issues since i was 17 which put an end to further education and make employment complicated enough that its not worth it, and the medical situation required some exaggeration to claim benefits before i had a transplant and so now the side-effects definitely arent vocal or glamorous enough to get any assistance, even claiming unemployment doesnt last long, so i havent done that in over 10yrs. Im basically positioned in a spot where nobody is obligated to give me a hand financially, so im living with my folks, skint and do as i please, counting the days pretty much. I've tried doing stuff self employed and only ended up with debts, and my web development skills are too weak and sprawled over multiple aspects to allow me to build what i want to build, but not good enough by far to say im capable at any one thing, and im a terrible learner, i absorb so little its a PITA. These days i just do whatever keeps me occupied and about a month ago i decided to bite the bullet and build the small gaming clan im in a new website and get us off the CMS site we're on now (enjin.com) and i'd build us the same sort of thing, but without the restrictions that have stopped me doing things i'd wanted to do, and means we have no bills etc. So thats what the forum is for, and i have 14 days before our account is due a payment. Things had been going along at a really positive pace, and then 2wk ago i got the cold from hell, and i'd got next to nothing done during that time. The 24/7 headaches are gone now, so im back to work now basically :).
this is what i have always used. 
I've had to tackle some similar problems before but ran into a few issues where a person's last name was two words or their first name was two words or something. Then there may be suffixes like Jr. Sr. and prefixes etc - you know your dataset, think about if anything like that is in there. Also, maybe some dont have middle names (or two middle names).
I'd also suggest this route, Power Pivot has made SSAS Tabular very accessible. You can easily prototype using it and then upgrade to an actual SSAS model when (and if) it makes sense. On the DW side of things - start small, maybe even with just some CSV files extracted from your OLTP system on a schedule, and work up to a datamart for specific entities that can be logically grouped into related analysis. When you start trying to build out the fact and dimension portions of the models in PP or SSAS then you'll start to get a picture of how structures should maybe look in the DW. Provided you have some domain experts on your team (which may be more important than SQL or SSIS) then you should be able to get something up and running fairly quickly. Of course the SSIS/SQL knowledge is key as well for ETL, but I'd say until you have some clear direction defined for your DW then you're likely not going to be getting into any complex SSIS. Inevitably you'll have to learn DAX (especially for any time intelligence), and maybe some even a bit of MDX if you plan to query your cubes outside of Excel. Hope that helps!
I was laying in bed after I got my origional idea to work correctly and actually thought "You know what, i don't need a table for reach recipe, just a master recipe table where i store all the ingredients related to each recipe and do a sum of the cost". I was trying to think how i was going to move around and do math with all the different tables through my app and it just seemed too messy the way i had it, lol. I may still keep the other tables, so i could say, quickly glance at one and see the cost of each individual ingredient is...but it might just be to justify keeping the work i've already done in tact, lol. 
Table A has the columns for Parcel ID and Land Use, Table B has the columns for Parcel ID and Premise ID, Table C has the columns for Premise ID and Consumption. The problem is that in Table B each Parcel ID typically has multiple Premise IDs The link between Table A and B is the Parcel ID, and the link between Table B and C is the Premise ID. Each database has indexed columns but none of the tables share any of their respective indexed columns. So yes I want the Parcel ID and corresponding Land Use from Table A and the sum total of the consumptions for all Premise IDs for a particular Parcel ID. I want it so that the final table will have list the Parcel ID, the land use, and the sum of all the consumptions of the Premise IDs that lie within the Parcel ID. I know its confusing, I've been banging my head against it for a week now.
try 'delete' instead of 'select'?
Inner Join + Inner Join
 &lt;= CURRENT_DATE - INTERVAL 7 DAY
Thanks! for the inner join lines, what is "join teams as away"? I normally learned the approach of "join teams on" so the "as away" part is new to me. Thanks!
&gt;`sum(c.Consumption) Consumption` Will this work with the `group by b.PremiseID` ? I may have been overthinking it, I had a subquery to total `Consumption`.
syntax is INNER JOIN tablename AS aliasname ON ... so the "AS" in the FROM clause assigns a table alias notice how the table aliases are used in the SELECT clause 
Rather than use an inner join across linked servers, would it be possible for you to use SSIS to move data between servers so that it is centralized to one location? You would still have potential failures with the SSIS package (from network disconnects) but if you run the job frequently enough (perhaps once an hour?), your data should be fresh enough. This would theoretically eliminate the need for a linked server.
 DELETE FROM historical_table WHERE historical_table.recordCreated &lt;= DATEADD(DAY, -7, GETDATE())
I'm sure there's 100 ways of doing this better, but this is legacy code my predecessor wrote and i don't want to try to re-build the wheel; It's been in use for over 3 years now... The job is scheduled to run once an hour though, always has. It's also a critical process for our Accounts receivable; I don't get paid enough to build stuff like that :P I just want to see if i can figure out why the run time went up by so much and if it's because of whatever they did to fix the network issue...
Could you pull the data you need into a table on the local server then perhaps just update that with any data that has changed since the last import. If you have or can insert a column that contains the last modified date stamp then a simple merge function should update the data for you. Might be an option if the network speed isnt great. Alternatively, download SQL Sentry Plan Explorer and look at the tab for 'top operators', see if any one process is taking up the bulk of the operator cost.
Not sure if it works in Access, but try count(distinct user_id)
Make one query that is `select distinct login_date, user_id` then query that one `select login_date, count(user_id) from query1 group by login_date`
I'm not OP but did just recently learn joins and subqueries myself. Thanks!
in case anyone here is wondering i figured out a work around. i ended up doing this CONVERT(VARCHAR(10),CONVERT(date,MODDATE.FieldValue,101),101) and it gave me this: 09/10/2014 probably just a one time work around since my dates are stored weird.
I don't know enough about your data to have an educated opinion over whether this is a good idea or not, but as I said above if you want 1000 in the control group then you want 2440 that have a score between 660-749, etc. What criteria you use to select these 2400 is the interesting thing. I imagine a non-random sample would be superior to a random sample. I might take all accounts with a score of 660-749 and then look at the distribution by date/age or some other relevant metric, and then select the 2400 so that the internal quartiles mimicked the quartiles of the full set. Depending on how important this is I might even quartile the quartiles. Selecting your 2400 from each segment would be easy using Excel's `randnumber()` but you'd have to do it by hand.
&gt; what I'm doing wrong Post what you have tried and what results it returned. 
Assuming Table B has AT LEAST one record for each record in Table A, and assuming Table C has AT LEAST one record for each record in Table B: SELECT tbl_a.ParcelID, tbl_a.LandUse, sum(tbl_c.Consumption) AS total_consumption FROM tbl_a INNER JOIN tbl_b ON(tbl_b.ParcelID = tbl_b.ParcelID) INNER JOIN tbl_c ON(tbl_c.PremiseID = tbl_b.PremiseID) GROUP BY tbl_a.ParcelID, tbl_a.LandUse However, if it is possible for Table C, or Table B to be empty, you will need to use LEFT JOINS (and probably coalesce the value): SELECT tbl_a.ParcelID, tbl_a.LandUse, Nz(sum(tbl_c.Consumption),0) AS total_consumption --Access uses "Nz" to coalesce NULLs. FROM tbl_a LEFT JOIN tbl_b ON(tbl_b.ParcelID = tbl_b.ParcelID) LEFT JOIN tbl_c ON(tbl_c.PremiseID = tbl_b.PremiseID) GROUP BY tbl_a.ParcelID, tbl_a.LandUse
Not entirely sure on the syntax there but I think your use of the table is the problem. Is the table being recreated each time in the loop with that syntax? Why use a table unless your are inserting multiple records? You just need a single variable to hold the random number.
The only mistake I see is: IF (SELECT * FROM @TempCustomer) = 5 That doesn't check if @TempCustomer has 5 numbers, it checks if the selected number is 5, the correct would be IF (SELECT COUNT(*) FROM @TempCustomer) = 5 Same with the SET @xx
It LIVES http://i.imgur.com/tGTHf1o.jpg
Along with what Campes said about the table being recreated very time, this may be a better way to go about it... declare @i int WHILE (@i &lt;&gt; 5) BEGIN set @i = ( select CAST(RAND() * 10 AS INT) ) END That should be a lot more concise and should work, although I don't have access to a server to be check right now. You don't need the table variable at all, and you don't need to put an explicit break/continue in the loop, as the WHILE states the logic to exit it when @i = 5.
Good job man! Only suggestion I'd have is that the ID be an integer identity column because it's 1) best practice (string primary/foreign key columns are strongly frowned upon), 2) cleaner, and 3) it's faster to join integers on integers. Those are nitpicks. Good stuff though.
That's clever, I'll have to remember that one.
Prezrosslin is correct. The select distinct query needs to be the sub query of your count query
Not sure, but I think any db where the performance would matter, this data would be normalized 99% of the time. It should be minimal though.
Thanks!
Does this do what you're looking for? WHERE status_changed &gt; DATEADD(year,-1,GETDATE()) AND status = 3
Cool what date range do you want for non-cancelled events? You'd want the query to look something like WHERE (status_changed &gt; DATEADD(year,-1,GETDATE()) AND status = 3) OR (status_changed &gt; DATEADD(year,-1,GETDATE() AND status &lt;&gt; 3) but with the date range modified in the second clause
Thanks again, the date range for non cancelled events could be 5 years Unfortunately i'm getting the following error Incorrect syntax near the keyword 'AND'.
Cool. Presuming you copied the code segment directly I missed a parenthesis near the second AND statement. Either way the logic behind what I'm doing should hopefully make sense at this point. Try: WHERE (status_changed &gt; DATEADD(year,-1,GETDATE()) AND status = 3) OR (status_changed &gt; DATEADD(year,-5,GETDATE()) AND status &lt;&gt; 3)
Excellent, you have been a real help And after actually digesting it, it certainly does make sense Once again, thanks so much for your help Really appreciate it
I am going to go against the trend and say dont use any sort of design view. I used to rely on this and it became a crutch and stopped me developing further. Unfortunately (although lucky but didnt realise) a bug in our SW meant that i had to stop using the design view. Boy did i struggle at first but in the long run it helped loads. I began understanding my joins much better and also began to get my own style of writing a query 
I'm not too familiar with access, but as his comment says Nz is Access' coalesce function you can think of it as a nested isnull() function. Basically, coalesce(item1, item2, item3, 0) says, if item1 is null use item2, if item2 is still null use item3, and finally if item3 isnull then just use 0. Very useful if you are doing arithmetic functions as if a value is null your result will be null.
Glad it worked for you.
Is the field called brand_name? Then just search the databases for that particular field name. 
We have continous deployment, so in our case I'd just search and replace in the rdl files and then deploy it all 
Is it not pulling from your stored procedure? Makes your life a million times easier if you use a stored procedure because if it fails you know exactly where it's failing and how to fix. Tables and fields change so it's best to use a SP. 
I'm not sure what will be the use of that information, but of all the scenarios I can imagine, temp tables wouldn't be necessary. If it will be used for a report, I would make a view for each query , make 5 reports and use them as subreport of a main one. Can you elaborate a bit more about what those query will do or what will use them.
I'll make a query later, I'm about to go home from work :D, but I would like to ask a few things. Your table really only have 2 fields? If so, how do you differentiate if a customer bought 2 items together or if he bought 1 item and later bought another item? 
Edited for clarity. There is no limit to the items, I just want to order by Count desc
So you want as many columns as needed as long as there are more than 1 item? To make my life easier, can you write a sample table with items and customers for me to test the query, like 8~10 rows will be more than enough? Don't need to use reddit table layout, write separating by ';' or '|' or whatever.
Yes. http://sqlfiddle.com/#!9/d185cb
Look up the CASE statement. 