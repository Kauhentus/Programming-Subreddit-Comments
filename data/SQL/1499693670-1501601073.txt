Aren't you making this an inner join by adding the 3 requirements from the joined table? Try changing that to WHERE clauses instead.
Even by removing those year / term / session join conditions (ie searching for *any* point in time), I only get 2 / 3 of the items (an additional from many years prior).
Can you give the table structure and some sample rows to test?
Your issue is your WHERE clause. It does not account for any NULL values in your left joined table, effectively making it an inner join. I don't know what RDMS you are using, but in SQL Server I would use the ISNULL() function to fix this. 
Correcting the first block LEFT JOIN [ACTIONSCHEDULE] [asched] ON actions.ACTION_ID = [asched].[ACTION_ID] WHERE actions.action_id IS NULL OR ([asched].[ACADEMIC_YEAR] = '2017' AND [asched].[ACADEMIC_TERM] = 'FALL' AND [asched].[ACADEMIC_SESSION] = 'MAIN' ) This means that you'll still have the filtered results, but also have the results where no match was found. Another way to do it would be to use a subquery for ACTIONSCHEDULE
This is actually an interesting homework problem. CREATE TABLE Prerequisite ( CourseID CHAR(7) NOT NULL, PrerequisiteID CHAR(7) NOT NULL, PrerequisiteGroup INT NOT NULL, CONSTRAINT PK_Prerequisite PRIMARY KEY (CourseID, PrerequisiteID) ) So this is a many to many relationship with the bonus column PrerequisiteGroup. PrerequisiteGroup is just an arbitrary number (1,2,3,4) which indicates that prerequisites with the same group number have an OR condition. This could accommodate all courses of a certain level, but I think a potentially better idea is to create special courses for that. So you would have actual course record COMP300, worth 0 points, that is added for any student that completes any other 300 level COMP class. Then you can use this special course as the single prerequisite. However, I don't think you can use this trick for the COMP307 and one further COMP 300 level course (because COMP307 would automatically grant COMP300). You'd either have to create another special course (COMP3??), or individually list all the qualifying courses.
https://social.msdn.microsoft.com/Forums/sqlserver/en-US/490579bc-966a-4f16-8ad7-69c55dbe3afe/emonth-equivalent-function-in-sql-2008 Try this: select dateadd(SECOND,-1,DATEADD(MONTH,datediff(MONTH,0,'2012-12-05')+1,0))
To take this one step further I think your syntax would be this: between DATEADD(MONTH, -3,DATEADD(MONTH,datediff(MONTH,0,GetDate())+1,0)) and dateadd(SECOND,-1,DATEADD(MONTH,datediff(MONTH,0,GetDate())+1,0)) This will calculate to be "between '2017-05-01 00:00:00.000' and '2017-07-31 23:59:59.000'" 
 SELECT employees.employee_id , employees.last_name , ( CASE WHEN employees.department_id = ( SELECT departments.department_id FROM departments WHERE departments.location_id = 1800 ) THEN 'Canada' ELSE 'USA' END ) location FROM employees; You would normally do something like this if 1. The location ID is not stored in the employees table 1. It's possible that the department ID for that location might change. Like many who replied, I like the left join version better SELECT employees.employee_id , employees.last_name , ( CASE WHEN departments.location_id = 1800 THEN 'Canada' ELSE 'USA' END ) location FROM employees LEFT JOIN departments on employees.department_id = departments.department_id; It's most likely workable as a join, but if it is possible to have an employee with a department id that isn't in the departments table, only the left join will duplicate what the subquery will do.
 SELECT * FROM ( SELECT 'ADAPSSCA' [ACTION_ID] UNION SELECT 'ADAPBCER' UNION SELECT 'ADAPHTRN' ) [actions] LEFT JOIN [ACTIONSCHEDULE] [asched] ON actions.ACTION_ID = [asched].[ACTION_ID] AND [asched].[ACADEMIC_YEAR] = '2017' AND [asched].[ACADEMIC_TERM] = 'FALL' AND [asched].[ACADEMIC_SESSION] = 'MAIN' WHERE [asched].PEOPLE_ORG_CODE_ID = 'P000011550' &lt;-- This totally defeats your left join. Only a row where the action id exists in your action schedule will return a row. Maybe you want SELECT * FROM ( SELECT 'ADAPSSCA' [ACTION_ID] UNION SELECT 'ADAPBCER' UNION SELECT 'ADAPHTRN' ) [actions] LEFT JOIN [ACTIONSCHEDULE] [asched] ON actions.ACTION_ID = [asched].[ACTION_ID] AND [asched].[ACADEMIC_YEAR] = '2017' AND [asched].[ACADEMIC_TERM] = 'FALL' AND [asched].[ACADEMIC_SESSION] = 'MAIN' AND [asched].[PEOPLE_ORG_CODE_ID] = 'P000011550' 
You've got it backwards. You want filters of OUTER JOINs in the ON clause. Putting them in the WHERE clause turns it into an INNER JOIN.
Thank you, sir !
You're right. Too early on a Monday...
Worked for a private, for profit college for 4.5 years. The real world version of this absolutely stinks! If I recall, we had scripts that, for those electives which weren't "pick anything" but rather "pick two from this list of 8", we had to codify all of it brute force.
You might find value in abstracting a course from an instance of the intersection of a course, section, trimester/semester/quarter/instructor. Then you can run prereqs off the course master list. This gets more complicated when you add in changing prereqs for versions of a program/degree.
wait. what? and furthermore, this is actually in an 'sql fundamentals' course/book from oracle? This query will actually fail if there are more than one departments with location_id 1800. I'm not sure if this is 'ELI5', but queries return result sets. If a result set has a single record returned it is called a 'singleton'; if a result set has a single column it is sometimes called a 'list'; if a result set returns a single row with a single column it used to be called 'scalar' (with the advent of object and user-defined types I'm not sure what it is called anymore). In major sql implementations, 'scalars' can be used as an expression of the returned column type wherever those expressions are allowed. I.e. if you can use "(2+2)" some place in the syntax you should be able to use (select &lt;integer&gt; from &lt;sometable&gt;) there as well). Unless it is part of the defined syntax, it is very rare that a specific approach/method is required. What is better in each specific case is sometimes clear but majority of the time the execution plan and cost would be the real way to compare your options.
People here will happily help you, but I doubt you'll find someone here to do it for you.
There's plenty of people who would do it, but I highly doubt a random college student would pay a professional rate.
p.s. since the original query treats the subquery as a singleton, you would want to default to a cross join to a "select max" here instead of a left join. A quick test gave me estimated cost of 8 for a straight select * from the table, 10 for a cross join to a 'select max' from a reference table and 17 for a left join to the reference table and a non-index-covered column from a reference table in the select list.
We'll be glad to help you figure it out. But, I'm never going to condone doing it for you.
And just need to convert it to DATE to make it equivalent to EOMONTH function: SELECT CONVERT(DATE,DATEADD(SECOND,-1,DATEADD(MONTH,DATEDIFF(MONTH,0,'2012-12-05')+1,0)))
I'm not sure what you mean by restrictions, so I implemented a GPA requirement. If there are other restrictions hopefully they will be easy enough to implement. I can't tell how complex the requirements can get, so I'll solve the general case. No matter how complex, the equation for requirements can always be written in this form using some basic unit of requirements, R. Think of it as expanding an arbitrary nested algebraic expression of addition, multiplication and parentheses such that no parentheses are left: Requirement = R\_1 AND R\_2 AND ... AND R\_n OR R\_(n+1) AND R\_(n+2) AND ... AND R\_(n+k) OR ... OR R\_(m+1) AND R\_(m+2) AND ... AND R\_(m+j) The most basic requirement unit, R, is either a class (QUAN201) or a class level (Level 200 QUAN). Each AND group can have an arbitrary number of R's, and an arbitrary number of groups can be joined by OR's. So we need three tables, one for each type: - Basic requirement unit - another table to group R unites using AND, a one to many relationship with R. Let's call it ANDGroup. - Requirement, a one to many relationship with ANDGroup I suggest you split your CourseId in Courses table into two columns (This will help with querying for Level requirements): CourseLetter VARCHAR(4) CourseNumber INT Your RequirementUnit table will look like this: CREATE TABLE RequirementUnit ( RequirementUnitId INT NOT NULL PRIMARY KEY, CourseLetter VARCHAR(4), CourseNumber INT, IsLevel BIT NOT NULL, Corequisite BIT NOT NULL, MinGPA DECIMAL(3,2) NOT NULL ) The IsLevel bit indicates whether the requirement is not a specific course but a course level. Corequisite bit indicates whether the course must already be completed or merely registered. MinGPA is the minimum required GPA in the course. Now the ANDGroup: CREATE TABLE ANDGroup ( ANDGroupId INT NOT NULL, RequirementUnitId INT NOT NULL, PRIMARY KEY (ANDGroupId, RequirementUnitId) ) And the Requirement: CREATE TABLE Requirement ( RequirementId INT NOT NULL, ANDGroupId INT NOT NULL, PRIMARY KEY(RequirementId, ANDGroupId) ) Let's create a simple example. Let's say COMP333 requires a Level 300 COMP course, AND (COMP 301 OR COMP310). RequirementUnit: RequirementId|CourseLetter|CourseNumber|Level|Corequisite|MinGPA -|-|-|-|-|- 1|COMP|300|1|0|0 2|COMP|301|0|0|0 3|COMP|310|0|0|0 We need to expand the parentheses to create ANDGroups: 300 level COMP AND COMP301 OR 300 level COMP AND COMP310 ANDGroup table: ANDGroupId|RequirementUnitId -|- 1|1 1|2 2|1 2|3 Requirement table: RequirementId|ANDGroupId -|- 1|1 1|2 RequirementId goes into your Courses table. If the requirements are the same for multiple courses then the same RequirementId can be reused. Let's say we have a StudentCourses table which has all the courses a student completed or is registered for, along with the GPA. CREATE TABLE StudentCourses ( StudentId INT NOT NULL, CourseLetter VARCHAR(4) NOT NULL, CourseNumber INT NOT NULL, GPA DECIMAL(3,2) NOT NULL, Registered BIT NOT NULL, Completed BIT NOT NULL ) Here is a query that returns all CourseIds a student can register for: DECLARE @StudentId INT = 1337; SELECT C.CourseId FROM Courses C WHERE EXISTS /* We use EXISTS because at least one row from the Requirement table needs to be returned */ ( SELECT * FROM Requirement R WHERE C.RequirementId = R.RequirementId /* Here we check whether the list of RequirementUnits is a subset of student's courses. Essentially we go through each requirement and remove it from the return set if conditions are met. If all conditions are met then an empty set is returned, which means the NOT EXISTS statement is true. */ AND NOT EXISTS ( SELECT RU.CourseLetter, RU.CourseNumber FROM ANDGroup A INNER JOIN RequirementUnit RU ON RU.RequirementUnitId = A.RequirementUnitId WHERE A.ANDGroupId = R.ANDGroupId EXCEPT SELECT SC.CourseLetter, SC.CourseNumber FROM ANDGroup A INNER JOIN RequirementUnit RU ON RU.RequirementUnitId = A.RequirementUnitId CROSS JOIN StudentCourses SC /* Let's use a CROSS JOIN and put all the logic in the WHERE clause for clarity */ WHERE A.ANDGroupId = R.ANDGroupId AND SC.StudentId = @StudentId AND SC.CourseLetter = RU.CourseLetter AND SC.GPA &gt;= RU.MinGPA AND ( RU.Corequisite = 0 AND SC.Completed = 1 OR RU.Corequisite = 1 /* If a course exists in the StudentCourses table, then it's either registered for or completed */ ) AND ( RU.IsLevel = 0 AND SC.CourseNumber = RU.CourseNumber OR RU.IsLevel = 1 AND SC.CourseNumber/100*100 = RU.CourseNumber ) ) ) I did not test the code so there are likely errors, though the code should be good, broadly speaking. Note that with some modification this model can handle more complex requirements. For example, if a course requires a COMP 301 and at least one other 300-level COMP course, then you'd need to add one more flag to the RequirementUnit table, something like IsExclusive, and modify the query as well.
The ELI5 version is that a clustered index means the rows in the table are stored on the disk in the order of the index. A non-clustered index doesn't affect the order rows are stored in. nb: this is an oversimplification to the point of being misleading but it's as close to an ELI5 as you can get without getting into B trees and their leaves, and how disk is organised into pages. It's also what a not insignificant number of people who should know better actually believe anyway so believing it might make you wrong but puts you in good company.
Clustered index is how the data itself is sorted when stored. It does not require additional space. This is why you can't have more than one clustered index. Nonclustered indexes make a copy of the indexed column and use a lookup (store a pointer) to the actual data.
&gt; Clustered index is how the data itself is sorted when stored Important point here: Even if it's "sorted when stored", there is no guarantee of how the data will be ordered when you query it unless you use `ORDER BY`.
To add to what others have said, because a clustered index is stored on disk that way, it can provide fast reads. A clustered index is often wasted on primary keys which are simply auto-increment style keys and where you rarely pull out bulk loads of data _in that order_. I use clustered indexes for the fields I want to use the most.
&gt; I use clustered indexes for the fields I want to use the most. If the table is write-heavy, I lean towards clustering on a key that's going *generally* ascend over time, as opposed to something that's going to be more random. Else page splits can become a performance hit.
Yeah, very true. It's going to definitely come down to how your database is used.
Think of a library, the way the books are physically stored on the shelf is the clustered index. Now think of the card catalog that helps you find the books, it only has snippets of information on each card, and there are different card catalogs ordered by different info right? Those are your non-clustered indexes. 
&gt; It does not require additional space. This is why you can't have more than one clustered index. that's totally not the reason why 
which is why you don't assign ~any~ indexes when first designing the database -- it's premature
Yeah, this is the best ELI5. The clustered index *is* the data. Nonclustered indexes just point to the data.
Oh god that sounds boring. I'm doing this for fun :( Because I'm insane, I'm thinking about doing it in java and just using multiple lists or trees to do the same job given how annoying it is proving to be. 
Haha, yeah... that is a bit nutty! I think this "industry leading" software was written in Power builder or Delphi, client-server style using terminal server. The software had some limited functionality exposed via an api for web front end. If I were to write one now, I'd probably want to do it all via api and html 5. Also, when working in a place where there's a chance of multiple people editing a record simultaneously, semaphores are your friend.
 SELECT CASE WHEN something THEN 'Equilateral' WHEN somethingelse THEN 'Isoceles' ELSE 'Fubar' END AS result FROM ... 
I was referring to the first sentence. You can't write the data sorted in two different ways. Well you could, but that would be a nonclustered index. Way to quote me out of context.
&gt; Way to quote me out of context way to write in a non-linear fashion 
Nonclustered are copies of the data really. The primary key will point back to the book, but the data on the card catalog entry is a copy, just like a nonclustered index
Awesome thank you!
I like indexing my foreign key fields out of the gate to help w/ join performance, but once things start getting used those may get adjusted. Adding/changing your clustered key after you've got a few million rows in the table can be time-consuming, so I try to choose that carefully at initial design.
&gt; Nonclustered are copies of the data really. Yes, but not a complete copy, and getting into the details of it might confuse the 5-year-old.
I ~~have~~ almost never see an index on just a FK help a real query. edit: Maybe because it's because I live in the OLAP world. I can see indexing CustomerID on an Order table, but I've never understood indexing all/most FKs as a best practice. Like a StatusID for example would likely find way too many records for bookmark lookups to be desirable.
Exactly, keeping it simple for the 5 yr olds.
Seems fine to me. We don't have any other information to go on about your database though, or even a lot of information about these tables (data types, indexes). What is your concern? 
&gt; I'm looking for advice and opinions on what I am doing right and wrong. have you considered not using surrogate primary keys?
I've always thought the old school library card catalogs were a good analogy. The really old ones with actual cards. The books on the shelves are the physically sorted (clustered) by a key, usually Library of Congress or some use Dewey Decimal. You can't also sort the books by date or author - there's only one way to physically order the books. The card catalogs with little cards allow ways of sorting that information by author or title with a pointer to where the book actually is - like an index. These function like non clustered indexes. Sometimes quicker to search, and lightning quick if all the info you need is on the card (like the publish date or the authors name) and you don't need to go the the actual book. You can have as many card catalogs as you want, but be careful, if you add or remove a lot of books from your library, the effort to maintain those card catalogs becomes too much work and slows things down. 
I would have been more explicit with my data types. I think I'm missing how relational databases work in general. I guess I'm just having a hard time understanding foreign keys and how to use them
What is a surrogate primary key? Edit: I looked it up. It seems to be a non-int primary key such as an abbreviation etc. I was under the impression that SQL operated faster when the primary key was an int.
I actually didn't notice that your compartment is just a combination of your keys. That kind of defeats the purpose of having the keys in a different table (except in some edge cases). Are there other fields in the compartment table? 
&gt; I'm just having a hard time understanding foreign keys and how to use them you completely ignore them when writing queries their purpose is to guarantee **relational integrity** the FK in the compartments table referencing the container id will ensure that you **cannot** insert a compartment for a container that doesn't exist 
&gt; I was under the impression that SQL operated faster when the primary key was an int. that's not the main reason people use surrogate keys... in fact it's not even always true
Here: / (public.money2numeric(maintable.total_dollars))) * public.money2numeric(mainextratable.extra_fee)), 2) Are you checking for these values to be non-null or non-zero values? Because if you multiple a value by zero...
I see it now. It did have to do with the divisor being zero. Ultimately this comes down to an order of operations issue. Normally in most situations, an RDBMS will perform "short circuit evaluation" of a case statement. My beleif was that because of the first two WHEN clauses evaluating as TRUE, the third when clause would never be evaluated for rows where the amount of the divisor was zero. HOWEVER, the reality is, at least when you are utilizing windowed aggregates (or I guess any aggregates, really) inside your case statement, these aggregates or windowed aggregates are evaluated FIRST. So in this case, the sum() OVER () is being evaluated BEFORE the case statement, thus no amount of short circuit evaluation is going to avoid a divide by zero. The solution then is to nest a case statement INSIDE the window (or aggregate) to check for a zero-value divisor prior to performing the math inside the window (or aggregate). So with that, the above third case changes to the following code: WHEN max(detailtable_line.detail_sequence) OVER (PARTITION BY detailtable_line.main_primarykey) = detailtable_line.detail_sequence THEN public.money2numeric(mainextratable.extra_fee) - sum(CASE WHEN public.money2numeric(maintable.total_dollars) = 0 THEN 0 ELSE round(((public.money2numeric(detailtable_line.detail_dollars) / public.money2numeric(maintable.total_dollars))) * public.money2numeric(mainextratable.extra_fee), 2) END) OVER (PARTITION BY detailtable_line.main_primarykey ORDER BY detailtable_line.detail_sequence) + round(((public.money2numeric(detailtable_line.detail_dollars) / (public.money2numeric(maintable.total_dollars))) * public.money2numeric(mainextratable.extra_fee)), 2) Thus resolving our divide by zero.
Yea, if you mult by 0 you get 0, which is fine in this case. thats what we want. But I solved the issue. Posted a comment.
No those are the only columns in the compartments table. Do you see a better way to organize the data? If it makes a difference I have shrunk the amount of data to create the sample in the OP. There are more compartments and containers
Oh wow. Should I put the abbreviation or name as the primary key in the compartments table then? They are both unique columns.
Truncate my friend. Unless you *really* need more speed by not having the indexes.
Ok thanks for that information
I don't think I would ever do something like that. We have some legacy processes that do that, but really it is best to not touch a table itself. Keep the structure and any indexes intact. I mean, I guess its fine while doing development, but once you are using it in a production setting that shouldn't ever happen in my opinion. All it takes is one malformed column to change a datatype on a select into and then... blow up downstreams. Plus it takes the table offline.
To add to this, my preference is to use the ansi comliant coalesce() rather than mssql specific ISNULL(). coalesce(possible_null_column,'some_default_value') IN(value list)
Maybe if you spent less time posting in videogame subreddits and more time actually trying to learn your course materials you wouldn't be in such a bind. Just sayin. Listen, been there, done that. Self control is hard. Hopefully you find it sooner rather than later, because otherwise you will waste a decade like I did.
Just as a note: those are an extra library (table_func) that has to be imported. Crosstab isn't included by default as i recall.
&gt; Should I put the abbreviation or name as the primary key in the compartments table then? no... because that would allow only one compartment per container you can make the abbreviation the PK in the containers table, assuming that you would never have two containers with the same abbreviation then use the abbreviation as the FK in the compartments table the PK of the compartments table would be a composite key, consisting of container abbreviation and compartment 
I always thought of clustered indexes like the index in a book, but instead of which page a term is one, it holds references to the combination of fields that you index
I use SELECT INTO quite a bit. It's a really clean way to get bulk logged inserts.
It still references the pages (literally), but the index is ordered by the terms/columns in the index, just like the index in a book is ordered by the terms, but references physical pages.
Too basic for /r/SQL - try /r/LearnSQL
Thank you for your help. I have a better idea on how to structure things
Useful when you have a table with a self reference. Like say you have items, but items can be part of a kit. Say you have a table with ID and "ComponentID" that references the ID. Each item in the table could have a component. Each component could have it's own components. Using a recursive CTE you can get a query that has a hierarchy of items with components (which are also items in the same table).
I usually need it if i have stuff which nested somehow or could be represented as a tree. I already encountered the example from /u/Cal1gula in real life. A customer has "recipes" for a product: component A is made from 2 pieces of component B and 1 piece of component C. But component C is made of 1 piece component D and 1 piece component B. In my example these compents are metal-bars, nuts, bolts, washers, screws, etc. If you now want to know that you need to produce 120 pieces of Product A you need to walk down this tree and sum it all up. Example 2: A file/directory tree.
I've been writing SQL full time for 5 years. I've never written a recursive cte for work. I did one for fun to solve a puzzle that got posted here about a year ago. The only recursive cte I've seen in the job was one that was flattening an accounting hirearchy into a relational database table. 
Employee contract organizational chart: *EmployeeContract(StaffNum, Role, PayRate, SupervisorStaffNum)* 
After using SQL on a daily basis for years, I actually just wrote my first one for work yesterday. We're moving reporting into Tableau and wanted to be able to easily filter data down so that you'd only see items for people that report up through you in the employee hierarchy. The idea is that a employee with no direct reports only sees their data. That employee's supervisor would see data for themselves and everyone that reports to them, but no one else. On and on, all the way up the company. Tableau has a function that gives you the username for the current employee, so I needed a list of all of the NT logins for everyone directly above every employee. Using a recursive CTE, I'm able to generate this list for all active employees in about 4 seconds. The result is then used as a filter in Tableau. I used a modified version of the solution in this post to get what I wanted: https://stackoverflow.com/questions/40696473/sql-server-how-to-show-list-of-manager-name-separated-by-comma
Here is an example (geopolitical hierarchies): http://blog.databasepatterns.com/2014/02/trees-paths-recursive-cte-postgresql.html 
Yeah I would like to note that I've been using SQL for ~7 years and I've only used a recursive CTE in that specific example. Needed to query/update the costs of parts used in making packaging materials. So we needed to sum up the costs of the sub-components to update the costs of the finished goods. But all the parts were stored in 1 table with the key/component pairing. It's a useful thing, but not that common in my experience.
Your eyes are going to be fucking *streaming* if you ever have to read enough to implement a decent indexing strategy. Fundamentally, clustered defines physical ordering (though fragmentation still occurs). Non-clustered does not. The library shelf/card catalogue analogy others have mentioned seems really good.
Do you have a date table? If so, I'd try using that. Join to a subquery containing the year, month number, and last day in that month (by extracting year/month number from your left hand side data).
Match up? Are you using aligned identities to establish a relationship or something? If so, please don't ever do that. Use foreign keys. It would be awful if identities updated when older records were deleted. It'd break all of your joins to other tables. You can totally fuck with and reseed indents. IDENTITY_INSERT and DBCC CHECKIDENT respectively for MSSQL. But that should be required so rarely it'd make me think carefully about my design.
The actual value of the identity column is arbitrary. They could just as well be GUIDs if those somehow required less disk space than integers. The point is to generate guaranteed unique identifiers.
https://www.reddit.com/r/SQL/comments/6mufhg/does_anyone_have_a_good_example_or_an_eli5_for/
I actually figured out how to make it work by using a cast. I'll post my solution tomorrow. 
Lol. I was working on this same one today. We should be friends. 
What are you asking? Are you asking for validation or alternatives?
How can I select the data to the right of the value I'm looking for. 
Maybe use a cte to rename the column. SELECT 1 AS ID, 'StartDate' AS MandatedColumnName, GETDATE() AS Crapname INTO #tmp; WITH cte (ID, MandatedColumnName, NewColumn) AS (SELECT * FROM #tmp) SELECT c.ID, c.MandatedColumnName, c.NewColumn FROM cte AS c; GO DROP TABLE #tmp; GO SELECT 1 AS ID, 'StartDate' AS MandatedColumnName, GETDATE() AS DifferentCrapname INTO #tmp; WITH cte (ID, MandatedColumnName, NewColumn) AS (SELECT * FROM #tmp) SELECT c.ID, c.MandatedColumnName, c.NewColumn FROM cte AS c; 
You silly goose. I don't know the names of the columns in advance. They will just get sucked up from SSIS and dumped into a table that could grow to *n* columns. I know *one* of the columns will have the value I'm looking for, and then need to take the data to the right of that.
I understand; I consumed two differently named #tmp tables with the same query. If you don't know anything about the table you're probably stuck generating dynamic sql from sys.columns.
[I found an XQuery way to do it.](http://www.sqlservercentral.com/blogs/rocks/2013/03/13/retrieving-the-next-or-previous-element-node-value-in-an-xml-blob-using-xquery/) SELECT TOP (10) *, 'SomeDate' AS MandatedColumnName, DATEADD(SECOND,10*RAND(CHECKSUM(NEWID())),'20000101') AS SomethingNew INTO #tmp FROM sys.tables AS t; WITH cte1 AS (SELECT (SELECT * FROM #tmp FOR XML PATH, TYPE) AS x) , cte2 AS (SELECT r.r.query('.') AS r FROM cte1 AS x CROSS APPLY x.x.nodes('row') r(r)) SELECT r.r.value('(/row/MandatedColumnName/text())[1]','varchar(100)') AS MandatedColumnName, r.r.value('((/row/*)[. &gt;&gt; (/row/MandatedColumnName)[1]]/text())[1]','varchar(100)') AS NewColumn FROM cte2 AS r;
Dayum.
In Oracle there is also [decode](https://docs.oracle.com/cd/B19306_01/server.102/b14200/functions040.htm)
Haha, I was actually stumped on an earlier "Easy" one that needed several subqueries. The handful after were cake compared to that one. Which was this one: https://www.hackerrank.com/challenges/weather-observation-station-5
Do you have control over the format of the files? Create a standard and document it. Post it online. Send it to your business partners sending you files and tell them nicely ,"fix your crap or it will fail." BizTalk does data validation and processing but I'm not suggesting you go down that route. However, I'd automate pulling the file and processing. Send the failure information directly to the client. They will fix their end quickly.
I'll check this out. This is going to be a long term concept so we'll see. 
I do not. For example 10 of the 100 files might join on ID. 15 of the 100 might join on last name + first name + last four of phone number, etc. So we will have to write these pieces separately after the files are loaded. The major pain point is that 25 of the 100 might suddenly decide to start capturing a certain date, or event, or new piece of data. These columns will just suddenly appear, and might not have any similarity in terms of how they are named. So to avoid making 25 phone calls, etc., I am proposing that the users must label the new column to the left... and in a perfect world this naming convention would be specific and rigid. Then after all the tables are loaded into their own homes, a repository table will be generated which will stitch the pieces of data from all the sources. Not sure if that makes sense?
The only thing that doesn't make sense is why as the recipient this can be more rigid and set in stone. Are they clients? 
Thanks! I ended up using something similar. Here's what I did that worked: DateRun &gt;= CAST(DATEADD(month, DATEDIFF(month,0,GETDATE()) +1,0)-3 as date)
Sort of. Internal clients from different countries / different companies / who use different software systems. So one system MIGHT let you rename the column to SALE_ID, but another system won't. Currently someone is literally stitching these different sources together into an Excel file. Some would like this practice to continue and then for us to just suck the Excel file up into a database, but this seems to be less than less than optimal.
Yeah I would *not* make a different table for each product. That is why you are having trouble making a query I assume. You would have to do a bunch of dynamic SQL and it will be gross. What is stopping you from making one assembly table with a 1:N relationship and inserting a record there for each step in the process?
I'm not very good with access but from what I understand, the columns are different from products to products. For example one product will need the serials of the optics used while another product needs the sn of the control card instead. If I made a single table and left empty the cells for another product, the table would become huge because of the amount of different products (20 ish cells for each products with over 15 different products).
I would still consider a different approach. Like an "attributes" table where you can list the unique aspects of each product and relate them back to the product. Or attributetype/attribute so you can have a unique set of attribute types per product, then another table with the values. 
&gt; CAST(DATEADD(month, DATEDIFF(month,0,GETDATE()) +1,0)-3 as date) You sure that works? My logic is showing that will look for DateRun to be on or after "2 days before the current end of month", so on or after "2017-7-29". You can put the logic is a select statement to check what the date is returning: select CAST(DATEADD(month, DATEDIFF(month,0,GETDATE()) +1,0)-3 as date)
If possible I would recommend going with SQL Server instead of Access. I agree with the previous commenter that you should not have a different table for each product. You should have a table called "Products" with one line for each product. A separate table could be called "Inventory" where you could track the assembly status of each individual item (tied back to "Products" with a "Product Key" column).
There are probably simpler ways, but I like using `REVERSE` to parse values before a separator. SELECT c.Amount, REVERSE(SUBSTRING(REVERSE(c.Amount),CHARINDEX(' &gt;- ',REVERSE(c.Amount)) + 4, CHARINDEX(' ',REVERSE(c.Amount),CHARINDEX(' &gt;- ',REVERSE(c.Amount)) + 4) - CHARINDEX(' &gt;- ',REVERSE(c.Amount)) - 4)) AS First, SUBSTRING(c.Amount,CHARINDEX(' -&gt; ',c.Amount) + 4,99) AS Last FROM (SELECT 'Amount 1600.00 -&gt; 1650.00' AS Amount) AS c;
Hmm, shit. I'll double check but I'm using it in a where clause and it was working. 
I like to do something like this: with test as ( select 'Amount 1600.00 -&gt; 1650.00'::text as change ) select split_part(change, ' ', 2) as old_amount , split_part(change, ' ', 4) as new_amount from test
Thanks for the reply! This worked as intended, but i'm unable to actually compare the 2 values in SQL...i'm trying to cast/convert them to a int or decimal so i can compare them ( Need to only return values where "Last" &gt; "Old".
Do you work in healthcare? I have the same problems as you. Clients with different systems that send us excel sheets with different columns on every sheet, every time a different format. Currently building a system that uses excel XML documents (like the other response you received using xQuery), to validate and normalize the data and then integrate it with our internal systems.
 SELECT c.Amount, CONVERT(money,REVERSE(SUBSTRING(REVERSE(c.Amount),CHARINDEX(' &gt;- ',REVERSE(c.Amount)) + 4, CHARINDEX(' ',REVERSE(c.Amount),CHARINDEX(' &gt;- ',REVERSE(c.Amount)) + 4) - CHARINDEX(' &gt;- ',REVERSE(c.Amount)) - 4))) AS First, CONVERT(money,SUBSTRING(c.Amount,CHARINDEX(' -&gt; ',c.Amount) + 4,99)) AS Last FROM (SELECT 'Amount 1600.00 -&gt; 1650.00' AS Amount) AS c WHERE CONVERT(money,SUBSTRING(c.Amount,CHARINDEX(' -&gt; ',c.Amount) + 4,99)) &gt; CONVERT(money,REVERSE(SUBSTRING(REVERSE(c.Amount),CHARINDEX(' &gt;- ',REVERSE(c.Amount)) + 4, CHARINDEX(' ',REVERSE(c.Amount),CHARINDEX(' &gt;- ',REVERSE(c.Amount)) + 4) - CHARINDEX(' &gt;- ',REVERSE(c.Amount)) - 4)));
FYI, [this site](http://www.databaseanswers.org/data_models/) is worth browsing for ideas.
Could you convert each CSV into a JSON file and then process them?
[Database.NET](http://fishcodelib.com/database.htm)
[This](http://www.databaseanswers.org/data_models/) might help.
why'd you link to this post? this is the post we're currently in.
i've used recursive CTEs so i could have a table of dates for a report, because our database doesn't have a calendar table. for example, in MS-SQL/T-SQL: WITH REPORT_DATES AS ( SELECT @START_DATE AS REPORT_DATE, 1 AS REPORT_DATE_NUM UNION ALL SELECT DATEADD(DAY,1,REPORT_DATES.REPORT_DATE) AS REPORT_DATE, REPORT_DATE_NUM + 1 AS REPORT_DATE_NUM FROM REPORT_DATES WHERE REPORT_DATES.REPORT_DATE &lt; @END_DATE ) this gives you a table of all dates between @START_DATE and @END_DATE.
AQT - it's the only tool I don't have running in Linux. There is one feature I absolutely miss in other tools - dragging column headers to the window bar to do automatic group bys without having to chop and change SQL all the time. Id investigate other tools if they did this but everyone I've looked at (up to a year ago) didn't offer it. 
I could. I was actually just saying CSV as a placeholder. What would be the benefit here?
Nope. Staffing :) Thanks for your comment.
Ya, i tried different CONVERT and CASTS but still the same thing :S I think this only works for when the string is defined in the query like you did in the FROM line...in my case c.amount is coming from a table in which the column i'm parsing is a VARCHAR(255)...This is what my query looks like: select distinct e.ord_hdrnumber,e.updated_by, e.updated_dt, REVERSE(SUBSTRING(REVERSE(e.update_note),CHARINDEX(' &gt;- ',REVERSE(e.update_note)) + 4, CHARINDEX(' ',REVERSE(e.update_note),CHARINDEX(' &gt;- ',REVERSE(e.update_note)) + 4) - CHARINDEX(' &gt;- ',REVERSE(e.update_note)) - 4)) AS First, SUBSTRING(e.update_note,CHARINDEX(' -&gt; ',e.update_note) + 4,99) AS Last, e.key_Value,i.cht_itemcode from expedite_audit_tbl e join invoicedetail i on e.key_value=i.ivd_number where e.activity='InvoiceDetail Update' and e.update_note like 'Amount%' and e.updated_dt&gt;'1/1/2016' AND SUBSTRING(e.update_note,CHARINDEX(' -&gt; ',e.update_note) + 4,99) &gt; REVERSE(SUBSTRING(REVERSE(e.update_note),CHARINDEX(' &gt;- ',REVERSE(e.update_note)) + 4, CHARINDEX(' ',REVERSE(e.update_note),CHARINDEX(' &gt;- ',REVERSE(e.update_note)) + 4) - CHARINDEX(' &gt;- ',REVERSE(e.update_note)) - 4)) order by e.updated_Dt asc This brings back results, but it's not comparing the values properly in the WHERE clause so i'm getting negatives, which i don't want. Tried doing something like "Last" - "First" &gt; 0 but i get that same "Cannot convert varchar to numeric" or w/e. 
What's an example that isn't working?
Toad
Dbeaver.
No problem. This article (and included sproc) was *extremely* helpful. It may be of use to you as well. http://www.sqlservercentral.com/articles/ETL/69339/
Say in the WHERE clause i want to compare the 2 values so they're &gt;0, i get the conversion error. As is, the query works, but i'm finding some records where the "Last" value is less than the "First" value. This 1 for example is coming up in my results, but it shouldn't: Amount 1100.00 -&gt; 646.95
You have to convert them to compare them. What's a record giving a conversion error?
Sorry if I was coming off very "HOW IS THIS POSSIBLE!?" Personally, when being one of your internal clients a lot of times I get requests like ,"We need your users extract." and I say ,"Ok, give me a file spec." Now if you have a file spec and they can't meet it, then you have to determine a spec with them and they have to stick with it. The whole "dropping a column" crap does not fly. It sounds like they are hand making these extracts. If they are internal clients, it may be best to handle things as you are now for the moment but I would encourage you to find a way to push these internal clients to automate their end of the process and match the spec. If you create a process that someone has to hand import and monitor, you are creating a process that requires a partial or full time employee. Someone who can error. Someone who can call in sick. Someone who can quit. Someone who wont train the next person in the position. Someone who needs medical insurance. Someone who could slip on the way into the building and break their neck and sue the living shit out of your company. That someone may end up being you and you may get stuck doing this shit for waaaaaay to long and one day you'll be like ,"why the hell am I doing this process?" and you'll find a way to automate it. Do it now, time is finite.
 Thanks for the link!
They aren't hand making them. They own an instance in a third party tool and use it however they see fit. If they create a module (which may create multiple columns), then the columns will simply appear in their data file. If they suddenly decide to remove the module, then those columns will suddenly disappear. I can cope with that. What I can't figure out how to cope with yet is how to join columns with dissimilar names -- but I believe I can make sure they have to add a column with a label (e.g. SALE_DT, etc.) 
Plain old Notepad ++ is pretty helpful. Even for little things like finding those random unicode characters in your data using "Show All Characters".
You could make two columns: ID | ProductType | ProductIdentifier 1 | serial | hdg64478 2 | sn | 1234 3 | pID | A-654 You could even create another table that lists the product types and then refer to their ID with a ProductType_ID instead of ProductType for the first table. 
JSON files are self-describing, so parsing files with different formats would be much easier.
The format isn't the problem, the naming schema is. They will have the same format of datetime, but not the same names.
[DataGrip](https://www.jetbrains.com/datagrip/) cross platform multi-DB intellisense is pretty killer, and it does a whole whack more.
SQL just isn't your best option for cleaning data. I would create a script in some language (powershell, bash, vbs, python) to clean the data prior to import. Here is an example saleid,sale_amount,sale_comment,salesperson 1,100,nothinghere,Tom Hardy 2,200,notheingwhere?,Russel Crowe will become SaleId,SaleAmount,SalesPerson,SalesComment 1,100,Tom Hardy,nothinghere 2,200,Russel Crowe,notheingwhere? I wrote it in Powershell because I've been writing a ton of it lately and thought what the heck. cls $sourceFileDelimiter = "," $sourceFileExtention = ".csv" $sourceFolder = "c:\my imports\" $errorFolder = "c:\my imports\errors\" $completedFolder = "c:\my imports\completed\" $archiveFolder = "c:\my imports\archive\" dir $sourceFolder | foreach { $file = $_ Write-host "Processing "$file.FullName if($file.Extension -eq $sourceFileExtention){ $fileContents = Get-Content $file.FullName $newFile = $null $saleIDPosition = $null $saleAmountPosition = $null $salespersonPosition = $null $salescommentPosition = $null foreach($line in $fileContents){ $lineSplit = $line.Split($sourceFileDelimiter) if($newFile -eq $null -and $fileContents.IndexOf($line) -eq 0){ foreach($headerColumn in $lineSplit){ if($headerColumn -like "*sale*" -and $headerColumn -like "*id*"){ $saleIDPosition = $lineSplit.indexOf($headerColumn) } if($headerColumn -like "*sale*" -and $headerColumn -like "*amount*"){ $saleAmountPosition = $lineSplit.indexOf($headerColumn) } if($headerColumn -like "*sales*" -and $headerColumn -like "*person*"){ $salesPersonPosition = $lineSplit.indexOf($headerColumn) } if($headerColumn -like "*sale*" -and $headerColumn -like "*comment*"){ $salesCommentPosition = $lineSplit.indexOf($headerColumn) } } if($saleID -eq $null -and $saleAmount -eq $null){ #abort and write error as it is a critical field $message = "SaleID = $saleID saleAmount = $salAmount" new-item -Path "$errorFolder $file" -Value $message -ItemType file -force } else { $newHeader = "SaleId,SaleAmount,SalesPerson,SalesComment`r`n" $newFile = new-item -Path "$completedFolder $file" -Value $newHeader -ItemType file -force } } else { $saleID = $lineSplit[$saleIDPosition] $saleAmount = $lineSplit[$saleAmountPosition] $salesPerson = $lineSplit[$salesPersonPosition] $salesComment = $lineSplit[$salesCommentPosition] $newLine = "$saleID,$saleAmount,$salesPerson,$salesComment" Add-Content $newFile $newLine } } Move-Item $file.FullName -Destination $archiveFolder } }
https://www.reddit.com/r/SQL/comments/6mufhg/does_anyone_have_a_good_example_or_an_eli5_for/dk6ephp/
hahaha i'm dumb, i get it now!
 -- i'm a sucker for regex -- EDIT: Added casting with test_tbl as ( select 'Amount 1600.00 -&gt; 1650.00' as test_str from dual -- Oracle standard test table ) select cast(regexp_replace(test_str,'Amount ([0-9]*\.[0-9]{2}) -&gt; ([0-9]*\.[0-9]{2})','\1') as decimal(18,2)) as old_amount, cast(regexp_replace(test_str,'Amount ([0-9]*\.[0-9]{2}) -&gt; ([0-9]*\.[0-9]{2})','\2') as decimal(18,2)) as new_amount from test_tbl; 
Sounds like a job for Python
Try `count(distinct case when action_id = '1' then user_id else null end)`
YES. That worked. I see. Guess i wasn't *that* far. 
Really interested in this but it's expensive! Worth it? 
SoftTree SQL Assistant 
Adam Machanic's [sp_whoisactive](http://whoisactive.com) I don't know what I'd do without it
 with test_tbl as ( select TO_TIMESTAMP('2017-03-01','YYYY-MM-DD') as login_ts, cast('1' as varchar(1)) as action_id, 'user_x' as login_id from dual -- Oracle standard test table union select TO_TIMESTAMP('2017-04-01','YYYY-MM-DD') as login_ts, cast('2' as varchar(1)) as action_id, 'user_y' as login_id from dual union select TO_TIMESTAMP('2017-04-02','YYYY-MM-DD') as login_ts, cast('2' as varchar(1)) as action_id, 'user_y' as login_id from dual union select -- two people are going to login on action_id 2 this month TO_TIMESTAMP('2017-04-10','YYYY-MM-DD') as login_ts, cast('2' as varchar(1)) as action_id, 'user_z' as login_id from dual union select TO_TIMESTAMP('2017-05-01','YYYY-MM-DD') as login_ts, cast('2' as varchar(1)) as action_id, 'user_y' as login_id from dual union select TO_TIMESTAMP('2017-06-01','YYYY-MM-DD') as login_ts, cast('3' as varchar(1)) as action_id, 'user_z' as login_id from dual ) select x.login_month, sum(case when x.action_id = '1' then 1 else 0 end) as Login_Type_A, sum(case when x.action_id = '2' then 1 else 0 end) as Login_Type_B, sum(case when x.action_id = '3' then 1 else 0 end) as Login_Type_C from( select to_char(login_ts, 'YYYY/MM') as login_month, action_id, login_id from test_tbl where login_ts between add_months(trunc(sysdate,'mm'),-12) and add_months(trunc(sysdate,'mm'),0) and action_id in ('1','2','3') group by to_char(login_ts, 'YYYY/MM'), action_id, login_id)x group by x.login_month order by x.login_month asc; 
Brent Ozar &amp; Team's first responders kit is great. The red gate products are also awesome.
Microsoft SQL Server 2012 Performance Dashboard Reports
Ozar*
Yep. No matter how many times I visit that site, my phone refuses to learn it. Do you listen to/watch office hours?
I listen to the podcast in the background at work. Sometimes I wish I’d paid more attention to it. Kendra Little has her own podcast that I’ve been catching up on lately. 
Haha same! Is it on LittleKenda.com? I guess I'll look for it on itunes. richie's podcast (AFK) is pretty good as well.
Probably is. I subscribed from Apple. It’s called Dear SQL DBA. It’s not quite as rapid fire as Brent’s. She goes more in depth on one question over 15-20 minutes. I’ll give richies a try now!
Subscribed, thanks! I started richie's from the first episode just to get a feel for the format, but I think it took them a bit to get into the groove. I'm working my way back now instead, starting with the latest episode.
Depends - we make heavy use of Python and Node.js so we also had interest in the WebStorm and PyCharm products, so for us the all products pack was a no brainer. Most of the functionality of DataGrip is also within a database plugin for PyCharm as well - so if you happen to need a Python IDE you can get the database IDE as a bit of a freebie. I think it also comes with the PHPStorm and RubyMine as well, but can't be 100% sure. It's not quite the same IDE setup as DataGrip, but the majority of the functionality is there I believe. We also use SQL Server, MySQL, SQLite and PostgreSQL all in our environment - and having a single tool to query all of those from Windows/Mac/Linux is very much worth it for us. There's a ton of other integrations they do within their IDEs as well, like support for Docker, remote Python, etc. So - tl;dr, if you have an interest in their other products in any way then you'll probably find a license of some sort that is worthwhile for you. They also have [discounts and freebies](https://www.jetbrains.com/datagrip/buy/#edition=discounts) for certain situations.
Huh I did not know PyCharm had database functionality. I usually just write python in vim (and jupyter notebooks) but I'd be curious to try pycharm for the integration. Thanks for the thorough answer! 
:) 
Favourite, no but many of us are using SQL Developer (the DB is usually Oracle) as we have locked down environments and our companies are too tight to get us anything else.
Favourite- by far SQLPrompt. A free one - SQL Sentry &lt;3
&gt; self joined to only the TOP 1 match so how to you determine which row is the "TOP 1 match"? also, please see sidebar about posting your platform
Yeah i don't know there's probably a better way. Sorry, I'm doing this in Access.
We use Navicat at work, I really like the interface. There's a list of saved queries down the left of the screen that can be sorted into folders. My only gripe is that you can't have sub-folders.
It sounds like you're asking for Select prod, date, count (*) From tablename Group by prod, date Having count (*) = 1 Just saw that you said you're doing this in Access. My solution is for SQL server, so I'm not sure if it translates. 
Case statements evaluate the "when" conditions sequentially and only perform the first "true" line. If you want to execute multiple when's then you will have to separate them into multiple case statements.
&gt;Does anyone know if this is wrong or right, or how to rectify the above for I can have lots of WHENs that are working correctly? You're `CASE` statement is working as intended. `CASE` stops evaluating at the first condition which is true.
This inspired me to create a recursive CTE to calculate the week of the tax year (it will start failing in about 628 years time): WITH Weeks (WeekNumber, Monday, YearIncrement) AS ( SELECT WeekNumber = 1 , Monday = CAST('20150406' as date) , YearID = 2016 UNION ALL SELECT WeekNumber = CASE WHEN DATEPART(DAY,DATEADD(ww,1,Monday)) &gt; 5 AND DATEPART(MONTH,DATEADD(ww,1,Monday)) &gt; 3 AND DATEPART(YEAR,DATEADD(ww,1,Monday)) &gt;= YearIncrement THEN 1 ELSE WeekNumber + 1 END , Monday = DATEADD(ww,1,Monday) , YearIncrement = CASE WHEN DATEPART(DAY,DATEADD(ww,1,Monday)) &gt; 5 AND DATEPART(MONTH,DATEADD(ww,1,Monday)) &gt; 3 AND DATEPART(YEAR,DATEADD(ww,1,Monday)) &gt;= YearIncrement THEN YearIncrement + 1 ELSE YearIncrement END FROM Weeks WHERE DATEADD(ww,1,Monday) &lt; GETUTCDATE() ) SELECT WeekNumber , Monday , YearIncrement FROM Weeks ORDER BY Monday DESC OPTION (maxrecursion 32767)
TSql translate almost 100% Query itself works but its not what i was looking for as it groups and only returns rows =1. I need every single row returned from those first 2 columns that table thus no grouping allowed in the main query.
Have you looked at using row_number()over(.....)?
So this isn't working at the row by row level? But then there are rows that don't contain any text from the first, but do from the second, that are not affected by the REGEX_REPLACE clause
So, and this is not gonna be pretty, you'll have to nest both CASE statements. 
works in Access? 
&gt; Here is an example: something is wrong because that should've given a syntax error you cannot say OR THEN 
Row_number() doesn't work in access unfortunately, have to use TOP 1 but I still don't know how the code would look like.
apparently not :(
SELECT prod , date , CASE WHEN count (*) = 1 THEN "1" ELSE "" END AS Count FROM tablename GROUP BY prod, date
Is there a column (or columns) in the table which is unique?
No, there are no PKs unfortunately as its a view linked from SQL Server into Access.
Edit: Inline sub-selects work too. I like this solution. SELECT (SELECT CASE WHEN Cond1 THEN Regex1(c.Col) ELSE c.Col END AS Col FROM (SELECT CASE WHEN Cond2 THEN Regex2(b.Col) ELSE b.Col END AS Col FROM (SELECT CASE WHEN Cond3 THEN Regex3(a.Col) ELSE a.Col END AS Col) AS b ) AS c ) AS Col FROM table AS a --- ~~A couple options.~~ ~~SELECT CASE WHEN Cond1 THEN Regex1~~ ~~FROM (SELECT CASE WHEN Cond2 THEN Regex2~~ ~~FROM (SELECT CASE WHEN Cond3 THEN Regex3~~ ~~FROM table)))~~ --- ~~SELECT CASE WHEN Cond1~~ ~~THEN CASE WHEN Cond2~~ ~~THEN CASE WHEN Cond3~~ ~~THEN Regex1,Regex2,Regex3~~ ~~ELSE Regex1,Regex2 END~~ ~~ELSE CASE WHEN Cond3~~ ~~THEN Regex1,Regex3~~ ~~ELSE Regex1 END END~~ ~~ELSE CASE WHEN Cond2~~ ~~THEN CASE WHEN Cond3~~ ~~THEN Regex2,Regex3~~ ~~ELSE Regex2 END~~ ~~ELSE CASE WHEN Cond3~~ ~~THEN Regex3~~ ~~ELSE Nothing END END END~~ 
If you have SQL Server linked there and just need to query, why not insert into a local table (temp?) and run a valid SQL query on it? Especially if you don't need any to do any DML/DDL.
Is there any way to distinguish the records? If there is nothing to distinguish the records, there is no way to pick one (without ranking functions).
Maybe this picture can illustrate better the query I'm trying to achieve. https://m.imgur.com/kiawE01
Yeah I've inserted into a local table but i still dont know how to achieve the query i need. Maybe this pic can help: https://m.imgur.com/kiawE01
Thanks! I'll give it a try.
If you have the data above in SQL Server why aren't you looking into SQL query options? You told the user above that the query wouldn't work because [it's Access](https://www.reddit.com/r/SQL/comments/6n828p/identifying_uniques_based_on_two_columns/dk7oec9/). It's difficult to give you assistance when you have conflicting information. Do you have the data in a SQL Server table or Access only? Can you write a TSQL query against the data?
 SELECT t.Prod, t.Date, t.Other1, t.Other2, CASE WHEN a.Others IS NOT NULL THEN '1' ELSE '' END AS Uniques FROM Table AS t LEFT OUTER JOIN (SELECT Prod, Date, MIN(Other1+Other2) AS Others FROM Table GROUP BY Prod, Date) AS a ON a.Prod = t.Prod AND a.Date = t.Date AND a.Others = t.Other1+t.Other2
They are stored on pages in the database just like other data. Just like a book. Makes it easier for SQL to look up (usually key) values. If you have a book with 10,000 pages and an index with 2 pages it's far quicker to go to the 2 page index to find the page with the data than look through 10,000 pages individually.
They're stored on disk along with the rest of the table... a clustered index is the actual underlying table... non-clustered indexes are basically copies of the data sorted in different ways. Keep in mind that either of those index types _can be cached_ in memory if they're accessed frequently enough. For example, if you have an ID on your table as your clustered index... the data is sorted by that ID: ID|Name|Date --:|--:|--: 0|Test|2015-01-01 1|Testing|2016-01-02 2|Testing 2|2017-01-03 This is fine... if you're looking up things by ID... but if you wanted to find row by date, specifically the "newest" rows, you can create a non-clustered index on the date (ordered by desc): ID|Name|Date --:|--:|--: 2|Testing 2|2017-01-03 1|Testing|2016-01-02 0|Test|2015-01-01 Now the data is sorted so the newest dates are on top, so SQL can quickly find the needed rows. (Note: this isn't _actually_ how it works... it creates a little "lookup" part so it knows that dates in 2017 are at position X, date in 2016 are at position Y, and dates in 2015 are at Z... so then if you give it a date in 2016, it just scans that part for the necessary rows... not the entire table). This actually explains it pretty well and goes into how the b-trees work: https://www.essentialsql.com/what-is-a-database-index/
What is the uniques column signifying? I'm not sure I'm understanding what that data should be. What is being counted and why is it assigned only to one row?
First, let's understand what a page is. A page (or block) is a fixed size chunk of data on the disk that the database is optimized to read into memory and process at one time. Basically bite sized chunks of data. Indexes are stored on disk, separate from the table. Indexes contain two parts. Leaf nodes and the tree. Leaf nodes (each node is a page) store the indexed column(s) in sorted order with a reference to the table record (usually a PK). Tree nodes are created to link to the location of every lower node, and are created in levels until a single top level node is able to contain all references to the next lower level. Unlike vanilla indexes, clustered indexes are the table (are not stored separately). The clustered index sorts the table on the indexed column (usually the PK) and creates a tree structure like a normal index. Below is an example diagram where each page holds a limited amount of data. In reality they hold A LOT more, but the example would be too big if I did that. Legend: page = record1data1:record1data2 record2data1:record2data2 CREATE TABLE t (ID,Letter,DOW) CREATE UNIQUE CLUSTERED INDEX cix ON t (ID) --Table Leaf Nodes t01 = 1:Q:Mon 2:W:Tue t02 = 3:E:Wed 4:R:Thu t03 = 5:T:Fri 6:Y:Sat t04 = 7:U:Sun 8:I:Mon t05 = 9:O:Tue 10:P:Wed t06 = 11:A:Thu 12:S:Fri t07 = 13:D:Sat 14:F:Sun t08 = 15:G:Mon 16:H:Tue t09 = 17:J:Wed 18:K:Thu t10 = 19:L:Fri 20:Z:Sat t11 = 21:X:Sun 22:C:Mon t12 = 23:V:Tue 24:B:Wed t13 = 25:N:Thu 26:M:Fri --Clustered Index Level 2 cix14 = 1:t01 3:t02 5:t03 7:t04 cix15 = 9:t05 11:t06 13:t07 15:t08 cix16 = 17:t09 19:t10 21:t11 23:t12 cix17 = 25:t13 --Clustered Index Level 1 cix18 = 1:cix14 9:cix15 17:cix16 25:cix17 CREATE INDEX ix ON t (Letter) --Index Leaf Nodes ix01 = A:11 B:24 C:22 D:13 ix02 = E:3 F:14 G:15 H:16 ix03 = I:8 J:17 K:18 L:19 ix04 = M:26 N:25 O:9 P:10 ix05 = Q:1 R:4 S:12 T:5 ix06 = U:7 V:23 W:2 X:21 ix07 = Y:6 Z:20 --Index Level 2 ix08 = A:ix01 E:ix02 I:ix03 M:ix04 ix09 = Q:ix05 U:ix09 Y:ix07 --Index Level 1 ix10 = A:ix08 Q:ix09 Let's consider the query `SELECT * FROM t WHERE Letter = 'K';`. This query could go through the following steps to complete. 1. Read ix10. A &lt;= K &lt; Q so... 2. Read ix08. I &lt;= K &lt; M so... 3. Read ix03. K = ID 18 so... 4. Read cix18. 17 &lt;= 18 &lt; 25 so... 5. Read cix16. 17 &lt;= 18 &lt; 19 so... 6. Read t09. Return "18:K:Thu". Reading a total of 6 pages read. Alternatively a table scan would read t1 through t13, or 13 reads.
&gt; DB queries are expensive according to the web It's usually a good idea to write the simple, readable version of your code first. Then, you can test how slow / fast the code is. If one of your queries turns out to be too expensive, then you should optimize it further. My 2c: use two queries.
Left outer join links instead of the inner join? The inner join will only return users with one or more links records.
Just tried that, still 0 results. Hmmm. Makes sense though, guess it probably has nothing to do with the SUM or COALESCE then..
Got it! SELECT u.firstname, u.lastname, u.email, u.created_on, SUM(l.votes) AS sum FROM users u LEFT JOIN links l ON u.id = l.created_by AND l.created_by = 50 WHERE u.id = 50 GROUP BY u.firstname, u.lastname, u.email, u.created_on; Thanks :D
No problem, happy to help.
&gt; When you left join, join on the user and move l.created_by = 50 to WHERE no no no any WHERE condition on a column from the right table like that, and you are **throwing out** the unmatched rows (i.e. where that column is NULL)
if you're going to require `l.created.by`to be equal to `u.id` then you do ~not~ need to specify `l.created_by = 50` and yes, you want COALESCE as well SELECT users.firstname , users.lastname , users.email , users.created_on , COALESCE(SUM(links.votes),0) AS sum FROM users LEFT OUTER JOIN links ON links.created_by = users.id WHERE users.id = 50 GROUP BY users.firstname , users.lastname , users.email , users.created_on
&gt; My 2c: use two queries. ON DUPLICATE KEY works just fine, no need for two queries (which would require a TRANSACTION BLOCK to prevent race conditions) however, OP's problem is that the key is being misused 
drag and drop interfaces are ~greatly~ simplified if your "position" column is FLOAT when "moving" a row, just re-calculate a new position name position initially -- curly 1.00000 larry 2.00000 moe 3.00000 insert shemp after curly -- curly 1.00000 larry 2.00000 moe 3.00000 shemp 1.50000 move moe between shemp and larry -- curly 1.00000 larry 2.00000 moe 1.75000 shemp 1.50000 with FLOAT, you can keep doing this hundreds of times, simply by averaging the position values of the two rows between which the new row is to be inserted 
Thanks for the input. I'll give this a try and see how it works. Currently, my setup doesn't feel that complicated though. Using the HTML5 drag and drop api, I just call a set_position function in a drop_handler function (when I drop the element into it's new place) function set_position() { var lessons = document.getElementsByClassName('lesson_position'); for (i = 0; i &lt; lessons.length; i++) { lessons[i].value = i + 1; console.log(lessons[i]); } } Not sure if there's anything wrong with my current approach yet, but it appears to be working just fine. I was surprised it ordered the elements for me automatically when I call getElementsByClassName. Edit: Oh. Just realized that's because I initialize the var within the function.
Thanks! Yeah.. Two queries made it much much easier, and my function did a lot of junk trying to just do one query, especially when creating the sql string.
Yes! This is what I was aiming for - I blame it on the tiredness mindset in the middle of the night. He would not need l.created_by. Removed my previous comment to avoid any confusion for OP or any other readers!
No problem! Glad that worked. Just for funsies, MySQL 5.7.8+ does have a JSON type. One representation of this that might work would be: Table: sections_lessons Columns: `section_id INT PRIMARY KEY AUTOINCREMENT`, `lesson_ids JSON` Then you could do: REPLACE INTO sections_lessons (section_id, lesson_ids) VALUES (1, '[101, 201, 301]'); Not sure I matched your schema exactly, but you get the idea. (tbh, I haven't used this feature of MySQL at all, it could turn out to be a mess :) )
I've found that comparing indexes to the Dewey Decimal system is the way to go. If you want to find all the books written by an author, would you prefer to look through every single book in a library, or just go to the card catalog, look your author up, and then get a list of exactly where all of his books (rows of data) are located?
I like that a lot. May start using that from now on.
[removed]
Not really a SQL question. You might want to try posting this in /r/SQLServer/
I like it because you can ask them what would be faster, having the computer search through every book in a library, or having the computer look at the card catalog. But here's the twist. What if the library only has 2 books? Well now it might be faster to NOT use an index. Indexes aren't magical and don't always improve performance. Sometimes they make it slower because creating one takes up time and space.
Oh! Very cool! It's definitely worth playing around with at the very least. Thanks x2
Im so glad it helped! Thanks for the +1 
To combine the tables, you want to `UNION` them. select * from table1 UNION ALL select * from table2 UNION ALL select * from table3 ... You can easily create a view this way: create view combined as ( select * from table 1 UNION ALL ... ) Your question about primary keys doesn't make sense to me, can you clarify?
Thanks for the reply. 1. which approach would you recommend? 2. re primary keys - if for instance I wanted to insert to the table one more record but the value of it's primary key would already be in the table, I would get an error. It's required to have distinct values in the primary column. Hope this makes sense?
Also, given the tables are large, how do I 'save' the combined table so I can reference it from another source? What I mean is that I'd like to run the 'combination' process once, and give it a name, and then query the combined table - that is, when I run a query it wouldn't need to combine them all over again per each query.
In that case you want a SELECT INTO query. It'll make a new table from the result sets of the above. If your source data changes though, that won't update the table you made.
check on memory caching. i dont know about this sqlite3, but ms sql caches its results in memory. This prolly the best thing you can do to speed up retrival. You would need a lot of RAM for a large data set. At an old job of mine, we ran a linux app/db server that had 16 gb of ram but prolly really only needed 2 gb to run. Linux and windows likes to cache their info in memory too.
CREATE VIEW is going to do exactly that; it's like a "virtual" table.
Creating a view will utilize a virtual table. There's nothing wrong with that, but it can slow down processing time depending on how many records there are and what you do with that view afterwords. If you're going to run a lot of calculations, you're better off writing that view to it's own table and then referencing it from there. I do want to back up a step though. You mentioned there are some duplicate records. I think you're going to want to remove those. There's plenty of ways to do that, but an easy way to build off of /u/cwmcarthur 's solution. You can put that union statement as virtual table (in parentheses) and select distinct from it. You can then insert those records into a table and you will have removed the duplicate records and combined all available data into one table. The catch here is, I only know T-SQL, but I think the overall premise is the same. Perhaps others here can convert some of my syntax if it's not correct. Select Distinct Temp.Column1, Temp.Column2, Temp.Coumn3 --make sure to select to reference the alias column names INTO CombinedTable --Use this line to insert the data from the select statement into a table. FROM (select * from table1 --Instead of * it might be a good idea to list out each column name unless there are too many UNION ALL select * from table 2 UNION ALL select * from table 3) Temp -- Make sure to add an alias to the temp table so you can refer to it in your select statement edit: it helps when I mark code as code... 
That formatted so effing bad
edit: You got it!
Thank You!
What assumptions can you make? Are there only one punch in and one punch out per day? Are some punch in's before midnight or some punch out's after midnight? Do any teachers forget to punch out ever? If so, do we need to assume they left at the end of the day? A simple solution that will likely fail as soon as any real data is encountered, might go like this. SELECT TCH.Firstname, TCH.lastname, CMP.CampusName, DATE(CP.Punchtime) AS Punch_Date DATE_FORMAT(MIN(CP.Punchtime),'%m/%d/%Y') 'Punch_In', DATE_FORMAT(MAX(CP.Punchtime), '%h:%i %p') 'Punch_Out' FROM Teachers TCH LEFT JOIN ClockPunches CP ON TCH.teacherid = CP.idnumber INNER JOIN Campuses CMP ON TCH.CampusCode = CMP.CampusCode LEFT JOIN TeacherAttendance TRA ON CP.userid = TRA.teacherid WHERE DATE(CP.punchTime) &gt;= DATE_SUB(CURRENT_DATE, INTERVAL 7 DAY) AND TCH.ADMINID GROUP BY DATE(CP.punchTime)
Yea I agree is stupid af not to have a value assigned with an in or out punch. Teachers do forget, there will more than likely never be a punch after midnight as this is for a beauty school. I really don't know how to work this.
Sometimes there are multiple punch in and outs for 1 day, but sometimes not but some teachers come in at 8 am and some at 3pm.
If you don't have to worry about shifts crossing midnight, you might be able to group them into pairs like this. SELECT idnumber, DATE(punchtime) AS punch_date, MIN(punchtime) as punch_in, MAX(punchtime) AS punch_out FROM (SELECT idnumber, punchtime, (ROW_NUMBER() OVER (PARTITION BY idnumber, DATE(punchtime) ORDER BY punchtime)-1)/2 AS segment FROM ClockPunches GROUP BY idnumber, punchtime) AS x GROUP BY idnumber, DATE(punchtime), segment
Use the movieid fields to join. There are multiple films with that title made in different years with different lead actors.
Julie Andrews didn't co-star with Shirley Temple in 1934.
That sounds like it makes sense. I'll try it once I'm home. Thank you! 
Index on job_id and start_time (DESC) is going to be the most optimal way: CREATE NONCLUSTERED INDEX [idx_JOB_ID_START_TIME] ON [dbo].[job_history] ( [job_id] ASC, [start_time] DESC )WITH (PAD_INDEX = OFF, STATISTICS_NORECOMPUTE = OFF, SORT_IN_TEMPDB = OFF, DROP_EXISTING = OFF, ONLINE = OFF, ALLOW_ROW_LOCKS = ON, ALLOW_PAGE_LOCKS = ON) ON [PRIMARY] GO It runs in ~12 seconds for 7 million rows (3 million returned) for me. You could also try this: SELECT job_id, start_time FROM ( SELECT ROW_NUMBER() OVER (PARTITION BY job_id ORDER BY start_time DESC) as RowNum, job_id, start_time FROM job_history ) j WHERE RowNum = 1 to see if that performs any better. Uses the same index, but will start with a filter and might finish MUCH faster if you have less unique IDs.
Thanks. I really appreciate you taking the time! The index is what I already had. The second query didn't perform any better/worse. 
No problem. When you are joining tables, you want to use the keys to join since those will join the data correctly assuming you don't need to filter the join as well. You'll notice them in the diagrams at the top and either be underlined or marked as a primary key (PK) or foreign key (fk). Primary keys should identify a unique tuple if the data is normalized. Foreign keys will are not primary keys for their table but are primary keys to tables you wish to join to from that table.
One thing to note is that 5 million rows isn't really that much. You could dump the data into a temp table and index it however you like. This isn't a competing solution to what /u/ihaxr posted, but rather complimentary to what he posted.
Sure. We've had some server performance problems, so did some analysis to look for jobs that were eating up CPU time, and this popped up on the list as something that runs often, and takes &gt;1000ms to run. I can put on any indexes you suggest... would it be faster to dump the data into a temp table and index that? Or just directly index the table itself? I'd guess the latter, but I really don't know.
Try dumping it into a temp table, just the 2 columns you need and running the query off it without an index. If it runs longer than normal kill it.
What's interesting is that 5M rows just isn't a lot. So something else is going on here. If this is what you're looking to optimize then you have other problems. Is this is a table that is being constantly inserted into, and where you always need to know the max value for other dependencies? If so you could modify the inserts to include a hardcoded 1 into a column such as `[isNew] bit`, and then have a process run nightly which sets anything which isn't `max(date)` to 0... this way you can index the column and then include in your query `where isNew = 1`. I'm wondering if your tables are being locked up by other processes and how it performs if you just dump it into a #table and test. 
&gt; this popped up on the list as something that runs often, and takes &gt;1000ms to run. What is the frequency of "often"? Several times a second or several times an hour. That makes a big difference. Also, 1000ms is just one second. Its not that bad. In order to really wrap your head around the issue, a profiler or x-events trace would need to be run to understand what is taking so long and maybe determine if there is any blocking occurring. I hate to suggest this because we don't have any context about the usage of the data and your retention policies is that you may not need all 5 million rows in this history table. You could look into off loading the data to a holding table. But, I still say that 5mil rows isn't a lot.
One more thought. Is that query inside of a sub-query such as: left join ( select job_id, MAX(start_time) from job_history group by job_id ) on job_id = job_id
If you have a table with unique job_ids you can do something like this. It will do 1 record seeks on job_history for as many job_ids as the "job" table has. CREATE INDEX ix ON job_history (job_id, start_time DESC) --DESC is important SELECT j.job_id, jh.start_time FROM job AS j CROSS APPLY (SELECT TOP(1) jh.start_time FROM job_history AS jh WHERE jh.job_id = j.job_id ORDER BY jh.start_time DESC) AS jh
trigger *function*?
Nope. 
post the query plan please. https://www.brentozar.com/pastetheplan/ 
Table users has a sequence number and a username, but we forgot the unique constraint on username, and now we need to delete all the dupes, leaving the row with the lowest id ID | Username 1 Homer 2 Moe 3 Marge 4 Moe 5 Moe Write a SELECT gives us the sequence numbers of the dupes we want to delete would return 4 and 5 in the example above. If you need to draw, ask if you can have some extra paper or use a whiteboard. Unless this is for a very senior position, you will probably be fine. They will probably be getting a lot of people who don't know SQL well at all, so if you can answer most of the questions you have a good chance. Because it is a paper test, I don't think they could require too much advanced SQL, but I would definitely want to know the basics of aggregate functions, group by and having, and the various joins. If they mentioned a specific dialect they may be looking for some more advanced abilities though. Good luck, you'll gain something whether or not you get the job.
join, self-join, group by vs. having.
Maybe a sub query
I resent over those. Group by vs having is a good one!
I was screwing around with the online helpers and realized I have a tendency to make joins and extra requirements over subqueries because I like to make it harder on myself. :) I think mainly this is because, as I analyze the info, I'm used to being asked about the data before the layer that was asked for. 
This one is perfect! although, I was more comfortable building a table and making sure I was right. Select Distinct(ID) From users Where username = 'Moe' and ID &lt;&gt; (Select min(id) From users Where username = 'Moe')
How about a generic solution?
my take SELECT id FROM users WHERE id NOT IN (SELECT min(id) AS id FROM users GROUP BY username)
Not sure how I got stuck on Moe. This is what happens when I panic during a test.
Another alternative. Row_number partition by id where rowid != 1. On phone but I hope you know what I mean
What is the difference between clustered and non-clustered indexes?
That's a nifty alternative, but it works 2-3 times slower, than /u/shif's solution.
&gt;group by vs. having I'm a bit lost. These clauses are undoubtedly related, but serve different purposes. How am I supposed to compare them?
or check out this blog as well: http://bi-solutions.gaussling.com You find lots of tutorials on SQL.
having is when you're selecting an aggregate function SELECT COUNT(ID), COUNTRY FROM CUSTOMER WHERE COUNTRY &lt;&gt; 'USA' GROUP BY COUNTRY HAVING COUNT(ID) &gt;= 9 ORDER BY COUNT(ID) DESC
In SQL Server you cam use a CTE with Row_Number and the Delete in the same query. Just make sure to BEGIN TRAN first and commit once verifies
SQL can handle XML pretty well, but xQuery is really one of the more complicated methods of querying data so I wouldn't expect someone new to be able to pick it up really quickly. Have you tried something like this site? http://www.convertcsv.com/xml-to-csv.htm
I get a syntax error for over 
MySql?
The difference between getting coffee and going to lunch when getting your results? Sorry, couldn't resist it. :)
Yea
Common Table Expressions, analytical functions, building your own functions, and potentially cursors
Am I dead in the water on this one?
ROW_NUMBER is not an option, but there's always a solution. You can take a stab at [the MySql way to do ROW_NUMBER](http://www.mysqltutorial.org/mysql-row_number/), or I will later.
When to use
Write a recursive CTE to generate a row for each date between @DateStart and @DateFinish.
So, you've got multiple rows that are the same item NCM Code but different product no's?? And All the rows with the same item NCM Code have the same taxes?? If so, you could do this to bring the taxes up to the NCM code level SELECT DISTINCT it.[NCM Code] as Item , CASE WHEN EXISTS ( SELECT 1 FROM [Table_1] in_PrdDim INNER JOIN [Table_2] PrdTax ON in_PrdDim.Product_No PrdTax.Product_code -- Why aren't these columns named the same thing?? WHERE PrdDim.[NCM_Code] = in_PrdDim.[NCM_Code] AND PrdTax.[uf_origem] = 'SP' ) THEN 'Yes' ELSE 'No' END AS [Taxes Configured State 1] , CASE WHEN EXISTS ( SELECT 1 FROM [Table_1] in_PrdDim INNER JOIN [Table_2] PrdTax ON in_PrdDim.Product_No PrdTax.Product_code -- Why aren't these columns named the same thing?? WHERE PrdDim.[NCM_Code] = in_PrdDim.[NCM_Code] AND PrdTax.[uf_origem] = 'RS' ) THEN 'Yes' ELSE 'No' END AS [Taxes Configured State 2] , CASE WHEN EXISTS ( SELECT 1 FROM [Table_1] in_PrdDim INNER JOIN [Table_2] PrdTax ON in_PrdDim.Product_No PrdTax.Product_code -- Why aren't these columns named the same thing?? WHERE PrdDim.[NCM_Code] = in_PrdDim.[NCM_Code] AND PrdTax.[uf_origem] = 'AM' ) THEN 'Yes' ELSE 'No' END AS [Taxes Configured State 3] FROM [Table_1] PrdDim 
Union vs Union all
Try (FORMAT(SUM(ATD.duration)/SUM(CLS.lessonDuration)*100, 1))+'%' As 'Attendance Percentage' I was able to do a select '100'+'%' and get 100%
A left outer join is the correct solution. Don't accept "doesn't seem to like that" as a result. Keep questioning to understand why something didn't work.
WINNER! Execution goes from 20K physical reads and 1.5 seconds of CPU time to 120 physical reads and 0ms CPU time. Now to read about CROSS APPLY... thanks! 
`DISTINCT` is a crutch, and is overused used to remove duplicates. It's usually better to structure the query so that it doesn't return duplicates. SELECT it.[NCM Code] as Item , MAX(tax.[Taxes Configured State 1]) AS [Taxes Configured State 1] , MAX(tax.[Taxes Configured State 1]) AS [Taxes Configured State 2] , MAX(tax.[Taxes Configured State 1]) AS [Taxes Configured State 3] FROM [Table_1] it INNER JOIN (SELECT Product_Code, , MAX(CASE WHEN uf_origem = 'SP' THEN 'Yes' ELSE 'No' END) AS [Taxes Configured State 1] , MAX(CASE WHEN uf_origem = 'RS' THEN 'Yes' ELSE 'No' END) AS [Taxes Configured State 2] , MAX(CASE WHEN uf_origem = 'AM' THEN 'Yes' ELSE 'No' END) AS [Taxes Configured State 3] FROM [Table_2].dbo.tab_regras_icms GROUP BY Product_Code) AS tax ON tax.Product_Code = it.[Product No] GROUP BY it.[NCM Code]
Ok so I changed it a little bit to this &gt;SELECT c.customer#, COUNT(*) FROM customers c LEFT OUTER JOIN orders o USING(customer#) GROUP BY c.customer# ORDER BY c.customer#; now the problem is its telling me the column part of the USING clause cannot have qualifier. I'm a little confused by this as I feel like its right and cant find anything explaining the error edit:thanks for the help btw i appreciate it
What database brand are you using?
oracle SQL developer 
USING threw me for a loop too. Never have used oracle. Kinda neat. Will cause issues if the tables don't have matching column names.
Actually, I think I figured it out, I had to take out the table abbreviations in order to get the outer join to work. Then I had to change the count from * to just order# as it as was counting all the customers as an order too. So now I have this and it seems to be working. &gt;SELECT customer#, COUNT(order#) FROM customers LEFT OUTER JOIN orders USING(customer#) GROUP BY customer# ORDER BY customer#; Thank you for the help though I just had to think about it in a different way. 
Unlike normal sub-queries, `CROSS APPLY` and `OUTER APPLY`can include outside references, like `j.job_id` in this example. I think of them more like in line sub-queries with the bonus that they can return multiple rows and columns. In fact, since this example returns a single value it could be rewritten as: SELECT j.job_id , (SELECT TOP(1) jh.start_time FROM job_history AS jh WHERE jh.job_id = j.job_id ORDER BY jh.start_time DESC) AS start_time FROM job AS j But I prefer `CROSS APPLY` because I like all my data access in the `FROM` clause.
Maybe because the output of the sum is a number and you are adding a string to it? Try converting the output of the sum to a string in order for it to allow you to concat it with another string. 
Yea for some reason it still does not work.
I guessed COUNT(*) would be your next hurdle, and I couldn't be more pleased that you figured it out on your own.
Yeah it was a little confusing for sure, I understand where I messed up there though. The toughest thing was the COUNT though as I was using the * for null values, when in reality the table I was counting from just didnt have those numbers at all instead of a null space.
What if I had two keys? Not just the most recent job_id, but the most recent pair of "job_id" and "parameter"? Thanks so much for your help!
I don't think you are using FORMAT correctly. You can't pass an integer for argument #2. You pass a string representing the format you want. This works for me in SQL Server 2014: `FORMAT(SUM(ATD.duration)/SUM(CLS.lessonDuration)*100, '100%') As 'Attendance Percentage'` Rextester test: http://rextester.com/RFRDZC92093 This also displays the percentage including decimals: `FORMAT(SUM(ATD.duration)/SUM(CLS.lessonDuration)*100, 'p') As 'Attendance Percentage'` 
&gt; I was using the * for null values COUNT(\*) counts **rows** in the result set a row consisting of column values from the left table along with NULLS in the columns which would've come from the right table **is still a row**
yeah I understand that now the problem was I needed all the customer#'s even if they weren't in my orders table and that is where I was getting hung up if that makes sense.
I thought it made sense but I don't think I fully understand. I am joining movieid on casting id as I need to use the casting table to make the connection between both tables, right?
I love this question! but to the person asking this, do you know the tipping point.. when query optimizer uses a clustered index vs your other index vs full table scan? And when to use Hints?
Yes. Your answer would look something like this... SELECT m.title , a.name FROM casting c JOIN movie m ON m.id = c.movieid JOIN actor a ON a.id = c.actorid WHERE c.ord = 1 AND c.movieid IN ( SELECT casting.movieid FROM casting JOIN actor ON actor.id = casting.actorid WHERE actor.name = 'Julie Andrews');
Wow, never thought of it that way. Thank you very much, that is awesome! 
Great feedback. Avoid jokes as they only make sense to me. Got it. :)
I doubt that asking for a recursive CTE on a written test a good idea unless it's a really high level position where expert knowledge of SQL is required. I've never had to use a recursive CTE.
Why are you formatting in your query? Do it at the presentation layer (web page, report, what have you) and save your DB server CPU cycles for doing work that is best done by the database.
Try this: CAST(SUM(ATD.duration)/SUM([CLS.lessonDuration])*100 AS NVARCHAR(10)) + '%' As 'Attendance Percentage' I think [FORMAT](https://docs.microsoft.com/en-us/sql/t-sql/functions/format-transact-sql) was being used incorrectly so I replaced it with [CAST](https://docs.microsoft.com/en-us/sql/t-sql/functions/cast-and-convert-transact-sql) and converted the value to NVARCHAR(10)
They aren't. College isn't even required. Source: never went to college, have a sql job.
They aren't, but a basic understanding of set theory will help you tremendously.
If you are working with SQL Server.. then What are the statistics in SQL Server?
Oh boy, now that is a sinister question. If there was a easily accessible list of parameter values (getting the distinct list from job_history defeats the purpose) you could use the same CROSS APPLY method. But I doubt that. So if the parameter values only exist in job_history, the only sensible thing to do is the simple MAX aggregate. But something being sensible is not the goal. A fast query is the goal, so let's entertain the non-nonsensical. We know that we're only interested in a few hundred out of millions of records, but we don't know their parameter values to pick them out individually. Let's start with an index that will be useful: `CREATE INDEX ix ON job_history (job_id, parameter, start_date DESC)` That seems like pretty sweet index. Now we can really easily find at least one record that we wan't. `SELECT TOP(1) * FROM job_history ORDER BY job_id, parameter, start_date DESC` Wait a second. Maybe we can get the next record also? Let's use the @job_id and @parameter we just pulled. `SELECT TOP(1) * FROM job_history WHERE job_id = @job_id AND parameter &gt; @parameter ORDER BY parameter, start_date DESC` Sweet, but what if what if there are no more of this @job_id with different parameters? No problem. `SELECT TOP(1) * FROM job_history WHERE job_id &gt; @job_id ORDER BY job_id, parameter, start_date DESC` Now see where we're headed with this? That's right. Recursion. But before we get to far let's combine the last two queries and establish that it's a bad idea, because it doesn't use the index very well. SELECT TOP(1) * FROM job_history WHERE (job_id = @job_id AND parameter &gt; @parameter) OR job_id &gt; @job_id ORDER BY job_id, parameter, start_date DESC` Solution time. We want to pull the last execution of the first job/parameter, then recursively pull the last execution of the next job/parameter. This would actually be a pretty slick use for cursors, but fuck cursors. The next time I see a cursor will be too soon. That leaves us with [recursive ctes.](https://technet.microsoft.com/en-us/library/ms186243(v=sql.105\).aspx) First draft. With our knowledge of OUTER APPLY and intuition to separate searches for the next parameter/job_id, we side stepped errors `Recursive member of a common table expression 'cte' has multiple recursive references` and `Outer join is not allowed in the recursive part of a recursive common table expression 'cte'` due to multiple cte references or LEFT OUTER JOIN use respectively. Leaving us with this: WITH cte AS (SELECT TOP (1) jh.job_id, jh.parameter, jh.start_date FROM job_history AS jh ORDER BY jh.job_id, jh.parameter, jh.start_date DESC UNION ALL SELECT ISNULL(nextparam.job_id,nextjob.job_id) AS job_id , ISNULL(nextparam.parameter,nextjob.parameter) AS parameter , ISNULL(nextparam.start_date,nextjob.start_date) AS start_date FROM cte AS c OUTER APPLY (SELECT TOP (1) jh.job_id, jh.parameter, jh.start_date FROM job_history AS jh WHERE jh.job_id = c.job_id AND jh.parameter &gt; c.parameter ORDER BY jh.parameter, jh.start_date DESC) AS nextparam OUTER APPLY (SELECT TOP (1) jh.job_id, jh.parameter, jh.start_date FROM job_history AS jh WHERE jh.job_id &gt; c.job_id ORDER BY jh.job_id, jh.parameter, jh.start_date DESC) AS nextjob WHERE nextparam.job_id IS NOT NULL OR nextjob.job_id IS NOT NULL) SELECT job_id, parameter, start_date FROM cte ORDER BY job_id, parameter OPTION (MAXRECURSION 32767); But we still get the error `The TOP or OFFSET operator is not allowed in the recursive part of a recursive common table expression 'cte'.` The entire solution is based on skipping over hundreds of thousands of useless records, and seeking straight to the single next record with relevant data. Time to give up and settle on the sensible solution. Never; breaking Microsoft technology is too much fun. Let's try one more thing: WITH cte AS (SELECT TOP (1) jh.job_id, jh.parameter, jh.start_date FROM job_history AS jh ORDER BY jh.job_id, jh.parameter, jh.start_date DESC UNION ALL SELECT ISNULL(nextparam.job_id,nextjob.job_id) AS job_id , ISNULL(nextparam.parameter,nextjob.parameter) AS parameter , ISNULL(nextparam.start_date,nextjob.start_date) AS start_date FROM cte AS c OUTER APPLY (SELECT jh.job_id, jh.parameter, jh.start_date FROM (SELECT jh.job_id, jh.parameter, jh.start_date , ROW_NUMBER() OVER (ORDER BY jh.parameter, jh.start_date DESC) AS top1 FROM job_history AS jh WHERE jh.job_id = c.job_id AND jh.parameter &gt; c.parameter) AS jh WHERE jh.top1 = 1) AS nextparam OUTER APPLY (SELECT jh.job_id, jh.parameter, jh.start_date FROM (SELECT jh.job_id, jh.parameter, jh.start_date , ROW_NUMBER() OVER (ORDER BY jh.job_id, jh.parameter, jh.start_date DESC) AS top1 FROM job_history AS jh WHERE jh.job_id &gt; c.job_id) AS jh WHERE jh.top1 = 1) AS nextjob WHERE nextparam.job_id IS NOT NULL OR nextjob.job_id IS NOT NULL) SELECT job_id, parameter, start_date FROM cte ORDER BY job_id, parameter OPTION (MAXRECURSION 32767); SUCCESS! Now that was dirty. We have OR conditions without OR conditions, OUTER JOINs without OUTER JOINs, TOP(1) without TOP(1), and have utterly mutilated the simplest of queries. But it's faster ain't it. R.I.P. sensible query SELECT job_id, parameter, MAX(start_date) AS start_date FROM job_history GROUP BY job_id, parameter
Try enabling 'force protocol encryption' server side and seeing if it still works.
I've had all 3 classes and discrete math alone will get you far (you really have to understand simple logic math). I don't use calculus 3 with SQL and I don't think I really use linear algebra either.
Lies. Set theory is really important, but that's easy (so far as it informs basic SQL anyway).
Me too but learning set theory really helped my "conceptual" understanding of how querys work 
&gt;In the collection several may add the same column in one week only to delete the column the next week. ... &gt;So sucking these up through SSIS is no problem. That's trivial enough. I really wanna see your SSIS packages - that doesn't sound trivial. Were you able to handle dynamic metadata somehow without going to pure dynamic SQL/script? Anyway, with regard to identifying columns to the 'right', you can't without a supporting framework of some kind. Could pick up column_id from sys.columns but that will probably fuck you eventually as they're not guaranteed sequential and are likely recycled. The data model doesn't really put much importance on ordinal column positions. Actual suggestion, are you altering tables in your ETL to align to the new schemas? If so, log the columns added/removed in those changes and use that. Or retain a 'base' list of columns per source and look for changes from that.
Any value containing a % symbol in SQL must be of a string type. Cast that to one and concatenate the symbol.
ETL it into a normalized SQL dataset-- then join it back together as needed. I bet that cures your performance ails in both power query and lets you write some SQL by hand for the really heavy lifting.
I'd probably just load WideWorldImporters and ask them to do random, everyday stuff on it e.g. rebuild those indexes, create one to support this query, write me a query to return x's sales per calendar year in absolute terms and as a % of total year's sales book, output a t-log backup to C:\, why would &lt;whatever&gt; datatype be inappropriate for that field, troubleshoot this awfully running query for me... Really depends what the job is though. If it's non-DBA I'm not going to ask about index maintenance (though support is still very relevant).
try to visualize what the left outer join produces in the FROM clause this is ~before~ GROUP BY -- cust# ord# 401 70001 401 70002 401 70003 577 88821 577 88822 666 NULL -- this is a row produced by the LEFT OUTER JOIN 808 32201 808 32202 808 32203 808 32204 as i said, COUNT(*) counts rows however, COUNT(foo) counts **non-NULL** values in the foo column so, when we GROUP BY with COUNT(*), we get cust# COUNT(*) 401 3 577 2 666 1 808 4 however, when we GROUP BY with COUNT(ord#), we get cust# COUNT(ord#) 401 3 577 2 666 0 808 4 COUNT(\*) is the only exception to the general rule that **aggregate functions ignore nulls** 
Well, my question was more along the lines of it appears to be working, but I never set anything on the server side...I didn't enable that option nor did I configure a cert. But it still works (at least I think so according to Wireshark?)
&gt; Were you able to handle dynamic metadata somehow without going to pure dynamic SQL/script? Was just planning on using pure SQL in the SSIS package to be honest. It hasn't been built yet but I know some friends who have built packages like this in the past... just dump each new file into a holding table, and then use a SPROC or something to dump it into the main table if it doesn't already exist there, or update what already exists with new values. &gt;Anyway, with regard to identifying columns to the 'right', you can't without a supporting framework of some kind. Could pick up column_id from sys.columns but that will probably fuck you eventually as they're not guaranteed sequential and are likely recycled. The data model doesn't really put much importance on ordinal column positions. I know. This is a horrible situation where I'm just trying to throw out an idea that will work "most of the time" -- and one where if it doesn't work then its the users fault. &gt;Actual suggestion, are you altering tables in your ETL to align to the new schemas? If so, log the columns added/removed in those changes and use that. Or retain a 'base' list of columns per source and look for changes from that. At the moment this entire discussion is theoretical. Ultimately I am hoping that they will scrap the request because it is too flimsy to ever be of significant use, because of how disparate the datasources are.
Yeah sure, that setting should either confirm it's working or 'break' it - as you say, it doesn't seem like it should be working as is.
Oh sure I get you, so it's more like orchestrating SQL via SSIS. Which is totally fine, I was just wondering if there were any possibilities I was missing for handling dynamic sources. At least there's BIML. I'm all for things being the user's fault - that was the problem with column_id and is the problem with ordinal positions in general unfortunately. I'd expect column_id to work at least most of the time, if that's good enough, but would want to test it out properly with lots of additions/removals. Can't see a proper (i.e. I'd think abbout putting it into prod for a client) solution except for logging and handling schema changes directly though. It's not anything like an insoluble problem, SQL just hates dynamic data formats so isn't really a good tool for this. Though if you're not going to get much valid data out of it then yeah, it's a fairly pointless exercise in any tool.
In SSIS I try to use as much pure SQL as possible, at least that was the direction I was steered by the person who taught me.
Nothing wrong with that, I write a lot of SQL for SSIS packages too. Just worth considering whether something really needs to be built in there, or could be written as actual pure SQL and orchestrated by Agent (lots of potential reasons why not but worth considering IMHO).
I'd say it's less of needing the actual calculations you learn in these courses, and more about developing "math mind." By this I mean developing a "feeling" for logic, operators/functions, and step-by-step thinking. Discrete math will especially develop this kind of thing. Some of the computational math is going to be useful too. Calc will give you knowledge of a lot of useful functions and algebraic chops (ability to manipulate equations into simpler or new forms.) Depending on what you're doing with SQL, matrix operations (covered in linear algebra) can happen. As others have mentioned, there's a whole mathematics for the algebra of relational data, but honestly that doesn't come up a lot in practice unless you get into something like normalizing data models.
Without more information I think you're not going to get very definitive answers. What are your table names? What are the column names? At this point all we can really say is that you need `WHERE statuscolumn = 'PAY'` or `WHERE statuscolumn = 'ACT'`
Thank you for responding. I think what I'm asking for is how to grab accounts with different statuses which have the same overall unique "packet id". Everything I need is in one table and just a few fields, so if I were to hack this out it would be something like: SELECT accountid, packetid, status FROM accounttable WHERE status = 'ACT' AND --here's where I need help: there is another account with the same unique packetid that has a status = 'PAY'
Maybe I need an INTERSECT statement to accomplish what I want?
I'd start with an installer, EnterpriseDB usually puts out pretty good ones. You can get them from the postgres website [here](https://www.postgresql.org/download/windows/). After that, it's going to depend a lot on how your local network is set up. Simplest solution will be just having the postgres instance available via password (md5 access in pg_hba) to machines on your network. **edit:** Also, depending on what your goal is with running postgres it wouldn't be totally insane to dual boot CentOS (or any flavor of linux, really) and Windows so you can run postgres on the linux box. It's not that it doesn't work on Windows, but IMHO it's really a database designed to run best on linux.
It's just a home network connected to the same router. I'm terrible at network stuff, but had to set some similar stuff up on VMs at work last year. Would virtualization be a good route? I'd prefer not to dual boot
What obstacle? Paying for Enterprise?
In that case, I'd virtualize the server in VirtualBox (free and pretty damn good hypervisor, all told). You can set up a VM to be your postgres server there. I'm a little rusty as it's been a few years since I did this last, but if you put the network adapter on the VM into bridge mode it should be accessible in your network. Then, check the firewall rules on the linux VM to make sure it'll accept tcp traffic over port 5432 (default for postgres). Once you've done that, you should be able to connect to the DB server using the admin software of your choice.
Yes
We need some sample data. If I understand correctly, you want to return PacketID values which have accounts with both 'PAY' and 'ACT' statuses against them? If so, INTERSECT (or EXCEPT with the inverse condition) does indeed sound suitable.
I'm thinking no. 
I don't think any of us are going to read an entire paper, so you'll have to elaborate on the requirement of 'posting into an ID number' or at least point us to the relevant figure/s.
Preface: why?
Apologies. I know a lot of users are afraid of long winded posts so I tried to summarize the issue and offer the documentation as a supplement to further describe the problem. I'll make an edit.
be sure to know all your joins buddy, good luck!!!
No worries, it's probably hard to win here. Just tricky without more background as that ERD really looks like it wants to be a star schema.
mostly because I want to use web services and I don't want to require to send 3 PATCH requests instead of one. PATCH record/E instead of PATCH record/E PATCH record/C PATCH record/D
Ah. Aside from something sketchy like leaving gaps between Order values (and then managing those gaps very carefully), I cannot think of any way around that - especially (not familiar with web interface) if Order cannot be expressed logically.
GROUP BY is for defining how to aggregate HAVING is like WHERE but for aggregate results
SSL or TLS usually means PKI, but it doesn't have to. Apps that implement these can decide quite granularly how much of the cert chain to check. That's how your browser can still show you a site it warned you about. So the server can give you any public key it wants. My guess is you'll either find a self-signed cert in the machine store or the SQL service's store.
Same here. No college. Do a ton of SQL/SSIS/SSAS/SSRS. 
Not necessary. but a good understanding of set theory and combinatorics can help you from doing really dumb things, like cross joining two large tables, and help formulate how to get from here to there. But I don't think it should be required curricula.
Maybe do something like make the sorting done by large binary numbers. Like max out an integer field, divide the next highest value in half and use that as your new order id. You would probably need to keep a local copy in sync and also reset the values of all the records from time to time. It wouldn't be pretty.
I'd use two tables for this: One for defining the screening types and type specific attributes, presumably in a hierarchical way, (id, type, min, max, values, parent_id etc.) and one for linking the respective screening type ids and attribute values to the customer id. Creating a spreadsheet for all needed screening types with their attributes and value types might help classifying them.
Assuming you're only concerned about ease of management, just build the reorder logic into a stored procedure.
PIVOT is the simplest syntax. MAX(CASE WHEN Attribute = 'ColumnName' THEN Value ELSE NULL END) AS ColumnName does the same thing but is more flexible (for example assigning specific data types).
I mean... There isn't anything I've seen in SSIS that I can't just write in SQL -- whether the scheduler can execute things in the way I want, or in an optimal way, or whether I would have to get in the habit of writing SQL scripts to run which run other scripts, etc. becomes the question. I can see a use for SSIS (obviously) but I was taught to do as much of it in SQL as possible and just think of SSIS as containers for the most part. You can certainly use some of the functionality along the way... but I'm pretty sure I can just do that in SQL.
A couple thoughts. Why is the data passing through Access at all? If you're creating a .csv file, why not just create it from the medical database? How many of these things actually have many-many data? Do you actually have patient data with multiple assigned genders? If many-many relationships don't exist in your data, those extra tables can go away and become normal columns on your patient table. If for some reason Access is necessary, and the excessive flexibility is desirable, just let them exist without defining foreign keys or indexes. If Access is just means to an end, it doesn't have to be pristine.
You could try a piece such as "Order By case when Record_ID = 'E' then 'BA' else Record_ID end" at the end of your query
then i have to store the order query somewhere and be able to parse it/edit it for next time the end user wants to edit the order
The medical database program unfortunately lacks any capacity to export any data to excel or access. Not to mention that given my position in the organization, I'm locked out of the bulk of the software that said medical database program offers. The other issue is that the information that I do have is primarily medical and lab results. Which I have to transcribe from the medical database to MS Access (originally used Excel but I ended up with too many files to properly track when management/doctors requested information). While I could just type in each ID number and make the CSV file from scratch each time. The purpose of using Access (or even excel to a point) is to make the data do the work for me so I wouldn't have to go through thousands of patients information and type each section keystroke by keystroke. So while I don't HAVE to use Access, my workplace doesn't exactly give me the tools to actually do my job efficiently. So I'm trying to make the best out of a poor situation. EDIT: I should also clarify why I treat this information as Many to Many data. Since the grant is based on patient information for each reporting period, which in this case is the year, I need to be able to query the information based on just the year. So my two options would be just have it be one to many which would require me to constantly update patient records, or have it be many to many and have as an additional field the year. The reason I treat it as many to many is that when it comes to some of the information needing reporting that will change such as a patient's HIV status from HIV to AIDS or if a patient becomes transgender or if a patient becomes pregnant. So having it parsed by a year and when management or the auditors come knocking, I'll be able to have that information still available in the database file.
I'm afraid I don't fully understand and lack the capacity to visualize your response. As for the IDs, unfortunately, in the document there are very few that actually contain the same type of response that goes with that ID number. They are all very specific to their own situation as the data ranges from something simple like ethnicity to prescribed anti-retro viral medication. Maybe if I could see a chart of what you mean then I might understand it better.
My current style settings give this: SELECT people.id , people.name , places.country FROM people INNER JOIN places ON places.person_id = people.id WHERE country = 'United States' AND people.age = 42 OR people.age &lt; 30 ORDER BY people.name ASC , people.country ASC;
I tried several styles and settled on this one. It's a compromise between readability and spending too much time formatting the code. It's sufficiently readable with syntax highlighting. SELECT people.id, people.name, places.country FROM people INNER JOIN places ON places.person_id = people.id WHERE ( country = 'United States' AND people.age = 42 OR people.age &lt; 30 ) AND people.name LIKE 'P%' AND places.country NOT IN ('Syria','Australia') ORDER BY people.name ASC, people.country ASC; I always put a group of OR statements into their own block. 
You won't be using an iota of calculus in SQL. The other two are more useful.
Why is DESC sorting important for the index? I know sorting can matter on a clustered index, but this isn't one.
Original sort order: Record ID | Order ------------- | ------- A | 100 B | 200 C | 300 D | 400 E | 500 Updated sort order: Record ID | Order ------------- | ------- A | 100 B | 200 C | 300 D | 400 E | 250 You can use pretty much any data type for this, although something like INT or BIGINT is probably going to be easier. You basically stick it in between two records by averaging their sort orders. Just make sure you're able to handle the case when you're trying to insert between two consecutive integers. On a final note, whatever it is you're trying to do there is probably an easier way to do it.
You can nest using OUTER APPLY or CTEs: CREATE TABLE #V ( Value VARCHAR(100) NOT NULL ) INSERT INTO #V VALUES ('Amount 1600.00 -&gt; 1650.00'), ('Amount 1550.00 -&gt; 900.00') SELECT V3.Old, V3.Last FROM #V V OUTER APPLY ( SELECT REPLACE(REPLACE(V.Value,'Amount ',''),' -&gt; ',' ') ) V2(Value) OUTER APPLY ( SELECT CONVERT(DECIMAL(10,2),LEFT(V2.Value,CHARINDEX(' ',V2.Value))), CONVERT(DECIMAL(10,2),RIGHT(V2.Value,LEN(V2.Value) - CHARINDEX(' ',V2.Value))) ) V3(Old,Last) WHERE V3.Last &gt; V3.Old This would return: Old|Last -|- 1600.00|1650.00
I've never seen right aligned SQL across my team and the various contractors we have used/other DBA teams we use/legacy reports we work with. But I also accept ours is working SQL not ideal SQL.
I personally prefer right-aligned style as more readable. Having to manually align keywords is only a matter of writing custom indentation rules for popular editors, I think.
&gt; But it's a real pain in the ass to manually align keywords after writing the query. tip: align them ~while~ writing the query
the parentheses you added simply make explicit the potentially horrible coding error of mixing AND and OR improperly i say *potentially horrible*, because it's not clear what was actually intended by the original query is this what was intended -- WHERE country = 'United States' AND ( people.age = 42 OR people.age &lt; 30 ) or this -- WHERE ( country = 'United States' AND people.age = 42 ) OR people.age &lt; 30 but just slapping parentheses around everything adds nothing
not mentioned yet -- the **leading comma convention** very valuable SELECT people.id , people.name , places.country FROM people INNER JOIN places ON places.person_id = people.id WHERE country = 'United States' AND people.age = 42 OR people.age &lt; 30 ORDER BY people.name ASC , people.country ASC
Later clauses, with longer keywords, require the adjustment of all preceding clauses.
ur doin it wrong
I like the leading commas a lot. When you need to comment things out it's especially helpful not to have to put random, in-line, multi-line comments to cover the commas. Still on the fence about the right alignment. I think I prefer left.
Yes this is what I suggested! Nice table to show how it works. Even make it multiples of two so it fits nicely into regular int/bigint columns and you can divide by 2 easily. Like such: http://www.timwappat.info/post/2016/01/13/Generating-SEQ-(LNITEMSEQ-etc)-column-values-in-Dynamics-GP I have seen a system do this with sales order transaction lines. It works well since the transaction have a limited amount of lines, so the numbers "reset" for each order. OP will have to figure out how to deal with that problem if they have no similar mechanism. 
OP said it's a web service. They don't have direct SQL access most likely. So they probably can't build their own sproc to do handle the reordering (hence why he has to make a single call for each record).
i've never really understood this argument for leading commas. if you're not using leading commas, the only instance where you need to use in-line commenting is if the last element in the list is being commented out: SELECT people.id, people.name/*, places.country*/ FROM people if you are using leading commas, you still need to use in-line commenting if the first element in the list is being commented out: SELECT /*people.id ,*/ people.name , places.country FROM people on this point, to me, both styles are basically equivalent.
&gt; Still on the fence about the right alignment. I think I prefer left. you mean, like this? SELECT people.id, people.name, places.country FROM people INNER JOIN places ON places.person_id = people.id WHERE country = 'United States' AND people.age = 42 OR people.age &lt; 30 ORDER BY people.name ASC, people.country ASC frankly, that's pretty useless
&gt; on this point, to me, both styles are basically equivalent. agreed... on that point **only**
i'm curious, what do you see as the benefit to leading commas?
No, I mean something like this: SELECT people.id ,people.name ,places.country FROM people INNER JOIN places ON places.person_id = people.id WHERE country = 'United States' AND people.age = 42 OR people.age &lt; 30 ORDER BY people.name ASC ,people.country ASC Don't really want to spend too much time mucking with markdown formatting but hopefully that gives you the idea.
The difference is it's much more common to want to comment out the last column.
Or literally any column except the first.
Sure, but leading vs trailing commas make no difference other than the first and last.
Yeah that looks as good to me while being a bit easier to adjust ad-hoc (without a special formatter), compared to right-align. My main reluctance is that it goes against conventions in other languages, such as Python's PEP8 on alignment of variable assignment: https://www.python.org/dev/peps/pep-0008/#pet-peeves Yes yes, I know, it's a different language. But a lot of PEP8's visual layout conventions transfer well to SQL (and many other modern scripting languages), so I'd like to minimize the cognitive load in memorizing styles. Edit: wanted to see what left-align would look like with some indentation. I added a non-sensical conditional to the `ON` clause to show where I anticipate indentation helping out, in terms of visually differentiating boolean operators from query clauses: SELECT people.id ,people.name ,places.country FROM people INNER JOIN places ON places.person_id = people.id AND places.country = people.nationality WHERE country = 'United States' AND people.age = 42 OR people.age &lt; 30 ORDER BY people.name ASC ,people.country ASC 
This. This right here.
In school last term we were taught to use a new line for each function and to indent each line one more than the previous SELECT * FROM table WHERE etc. = 'etcetera'
there are many, but i'll give you just one what is the syntax error in this query? -- INSERT INTO companies (`name`, `company_type`, `language`, `contact`, `address`, `telephone`, `mobile`, `email`, `website`, `date_setup`, `intro`) VALUES ('{$_POST['name']}', '{$_POST['company_type']}', '{$_POST['language']}' '{$_POST['contact']}', '{$_POST['address']}', '{$_POST['telephone']}', '{$_POST['mobile']}', '{$_POST['email']}', '{$_POST['website']}', '{$time}', '{$_POST['intro']}' )"; what about this one -- "SELECT ". "authors.name, ". "articles.id, ". "articles.name, ". "articles.usrfrndname, ". "articles.addedon, ". "articles.intro, ". "FROM authors ". "LEFT JOIN articles ". "ON authors.id=articles.authorid ". "ORDER BY articles.id DESC ". "LIMIT 4",
https://pastebin.com/VmcmB7QB - since this messed up the layout. 
 SELECT Translate2.Value2 , Translate2.Value2B , Translate1.Value1 FROM ( SELECT ID1 , ID2 , MAX(TimeStamp) AS latest FROM Changes GROUP BY ID1 , ID2 ) AS subquery INNER JOIN Changes ON Changes.ID1 = subquery.ID1 AND Changes.ID2 = subquery.ID2 AND Changes.TimeStamp = subquery.latest LEFT OUTER JOIN Translate1 ON Translate1.ID1 = Changes.ID1 LEFT OUTER JOIN Translate2 ON Translate2.ID2 = Changes.ID2
holy that was fast. I guess it's pulling unique combos of ID1 and ID2, I only need that last iteration of ID2. Would you happen to know how I could achieve that ? I tried removing ID1 from line 4,9, and 13 but still ending up with multiple ID2s. THANKS!
&gt; whatever it is you're trying to do there is probably an easier way to do it. which is that way? i made this post because i want to know if there is an easier way to do it.
Don't know if you got it figured out, but did you try coalescing inside the sum?
to me, that says that there are multiple rows with the max timestamp in `Changes` for each ID2 i'm sure if you look at your data it will become clear run the subquery by itself -- my version and yours, without lines 4 and 9 -- and check the results
Ctrl+Shift+R will refresh your query window so intellisense will know about the column, but that's not going to do anything to help your 0 rows affected issue. You've checked that supplierid 1, 2, 3 exists in dbo.suppliers and dbo.Products? 
EDIT: the supplier id column in my products table is null, and i want to change them to match the suppliers table. Pretty much want products with categoryid 1 and 2 to have the supplier id of 1 Here are the results http://imgur.com/a/WIioY
Change WHERE supplierid IN ('1', '2', '3'); to WHERE categoryid IN ('1', '2', '3'); SupplierID is null at the start of the query so there are now rows for the update to act on.
Wow thanks man, I should have seen that -.- Thanks for help and quick reply
I came into this thread very anti-leading-commas, but you have convinced me to repent.
I'd have to know the broader context of your problem to give a better answer.
exactly as described above.
If you change the LEFT JOIN in the second query to a RIGHT JOIN then they should produce the same results. With LEFT JOIN there will be a row for every row in the table on the left, even if there is no corresponding row that matches the join condition from the table on the right (columns from table b will be NULL). So, by changing the order of the tables, but using LEFT JOIN in each case, you are changing which table's rows are used to produce the resultset rows.
On a left outer join you take all the data from table A and supplement it with the data from table B. All the rows from column A will be shown, with the matching data from column B behind it. If you switch the columns, you'll get all the results from column B with data from column A behind it. So if you start with the larger column, there usually will be more NULLS than when you start with the smaller column. 
I apologise if its a stupid question, but I can't visualise correctly. So in the first one, the number of rows is the number of rows in PROD_MASTER? I want the number of rows to show how many times each PROD_ID(product id) turns up in the FACT_TBL, grouped according to the PRODUCT(product type) The main Fact Table consists of over 50 million, while on the other hand PROD_MASTER is 50000, because some of the products work like a recurring payment, meaning if the same account has multiple users, there'll be a ton of recurring rows with the same PROD_ID, which is the reason for this entire exercise. But the numbers don't ever add up, so I'm confused as to what I'm doing wrong here
Before you start using LEFT/RIGHT JOINS you need to understand the concept of JOINS itself. There is a lot of information and tutorials on the net about it. For example, look here: https://www.w3schools.com/sql/sql_join.asp
check out these two blog posts regarding INNER- and OUTER JOINs: http://bi-solutions.gaussling.com/inner-joins-explained/ http://bi-solutions.gaussling.com/outer-joins-explained/
i used w3 schools as my basic tutorial, but I didn't understand how the 'ON' part of the statement works. does that mean it will simply cross reference? And what happens when the data doesn't match on one of the two sides in the LEFT and RIGHT joins? Inner join was just a simple intersection, like in set theory, while the full join was just a union. It's the left and right that confuses me completely
okay, that made sense. So it nulls so many things because the smaller dataset (usually) doesn't have enough levels in the column which match that from the larger one. Since all the rows of the larger one MUST be represented, it just puts NULL where it can't see the match
That's about it, yes
This one actually really cleared a lot out, thanks a ton
Personal formatting: select people.id , people.name , places.country , case when places.country = '####' then '1' when places.country = '####' then '2' else '3' end as 'criteria' from table name as 'people' left join second table as 'places' on people.id = places.id inner join ( select a.field1 , a.field2 from table as 'a' where a.field1 = ## and a.field2 = ## ) as data on places.## = data.## where country in ('United States' , 'United Kingdom' , 'United Arab Emirates' , 'United Airlines' , 'Manchester United') and people.age &gt;= 30 order by people.name asc , people.country asc A lot is personal preference. I hate tabbing. Spaces allow for everything to be properly aligned. Indentations with spaces allow for me to see sub-queries and case statements easier.
If you hold alt and drag your mouse over a number of lines you can type on all lines at once. Leading comma's help with this. Need to add a different alias to a number of fields in your select statement? Alt drag all the way down before them, type "alias.", all done. Same for commenting out those rows, alt drag all the way down, type "--", done.
First. This is incredibly cool. Second. In real world queries there are a lot of edge cases like Oracle's ROWNUM pseudocolumn, analytical functions, WITH clauses etc. Does the big db vendors have something similar? It would sure be helpful when you're rewriting a big difficult query to know that it does indeed return exactly the same result as before. 
from a SQL perspective, it's nice. from a developer and mathematics perspective, this is damn amazing... it boils down to leveraging how SQL can be expressed as algebraic expressions, then analyzing those expressions for equality, and in the case of inequality, determining edge case conditions to be converted back to a SQL statement and executed to show you the specific records demonstrating the inequality. Or at least, that's what it appears based on the documentation. I'd written something years ago to test query equality... but it was all focused on executing both queries/procedures and validating the output (column count, column types, row count, and finally cell values)... it worked well enough for rewriting sucky report queries, but wouldn't be able to touch large datasets involved in ETL and such... their approach is far more impressive. microsoft should totally purchase this and include it in Management Studio and Visual Studio.
Well, it's not like all the RDBMS didn't already have these tools built in in order to rewrite queries for optimisation. They're just not publicly available to developers.
 CREATE TABLE orders_v2 SELECT docnr , itemnr , CASE WHEN COUNT(*) &gt; 1 THEN 'YES' ELSE 'NO ' END AS multiple_hits FROM orders GROUP BY docnr , itemnr 
 SELECT c.CustomerID , CONVERT(varchar(100),STUFF(MAX(RIGHT('0000000000'+CONVERT(varchar(10),1000000000-s.[Priority]),10)+CONVERT(char(10),c.MonthDate,120)+CustomerName),1,20,'')) AS CustomerName , CONVERT(varchar(100),STUFF(MAX(RIGHT('0000000000'+CONVERT(varchar(10),1000000000-s.[Priority]),10)+CONVERT(char(10),c.MonthDate,120)+CustomerEmail),1,20,'')) AS CustomerEmail FROM Customer AS c INNER JOIN [Source] AS s ON s.SourceID = c.SourceID GROUP BY CustomerID; What this is doing is prefixing each field with a string that sorts alphabetically based on priority and date. The MAX aggregate selects the best option and eliminates NULLs. Then the prefix is stripped off, and the column can be recast as the correct datatype.
Is it really that practical? I'd wage a pretty penny half my query optimizations would fail this sort of equivalency test. Equivalency of the output is what really matters and is simple enough to test. SELECT COUNT(*), CHECKSUM_AGG(CHECKSUM(*)) FROM (&lt;query1&gt;) AS x; SELECT COUNT(*), CHECKSUM_AGG(CHECKSUM(*)) FROM (&lt;query2&gt;) AS x; 
Forgot to mention in the original post (my bad) I'm using Netezza. Not sure If I can use CONVERT or STUFF on there. Will give this a go though..
We've discussed this a few times recently. The general consensus is that certs won't get you an interview, experience will. There are a good deal of training materials on the sidebar here. If you are learning SQL and queries and you can demonstrate this to your supervisors, are there any positions open internally that you can apply for? If not, you will probably need to build some knowledge until you can take a position at another company where you can claim X years of SQL experience. That's how it's worked for me in the past decade. Glad you like SQL though, welcome to the club. My wife still thinks I'm nuts because I like writing it. That passion alone will open some doors for you.
The good news is I can easily claim a certain number of years of experience with it since I do currently use it, just not quite to the extent I would with the mcsa. Thanks for the response! Definitely should have checked the side bar first!
The only thing I pay attention to on a resume is experience. I recommend installing and using a database and using SQL at work (even if it's not necessary) to gain experience. Just dump your Excel data into a table, and write queries to ask it question. Even non-work experience can be benificial. I've had a couple candidates put interesting personal projects on their resume. I'm not even sure what I would ask a candidate with a cert but nothing else. I don't speak for the industry though. I see what it takes to ace interviews, not get them.
If you are using SQL Server 2008 or later, you could use a hierarchyid to do this. The computer would auto generate hierarchyids inbetween two numbers. For example: declare @h hierarchyid = '/' select @h.GetDescendant('/2/','/3/').ToString() --Returns /1.1/, @h.GetDescendant('/2.234/','/3/').ToString() --Returns /1.235/ I had to do something similar for shoing hierarchies for contracts with using a web api using POST rather than patch; however, .net core didn't know what a hierarchyid was so I used a stored procedure. 
Skip the MTA. Darn near useless.
Only reason I was considering it is because I'm pretty much a complete noob. If I just jump into the mcsa with the prep will i be ok?
If anyone looks at this in the future i think I've solved it. I will show the line you need for one column. each column will just be a repeat of this: FIRST_VALUE(customer.customerName IGNORE NULLS) OVER(PARTITION BY p.CustomerID ORDER BY reference.Priority, p.monthDate ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS Name
this is in the select, obviously the two tables will be joined on sourceID
the solution I found ended up being slightly easier than this. I've added it as a new comment on this thread.
That works, but I suspect it's not as efficient if you end up grouping by CustomerID.
 INSERT INTO table (name, description, number) SELECT name, description, 3 as number FROM othertable WHERE id=1 If you needed to pull this number from another table you would just join the other table to the select in order to grab the number you wanted.
I found a number of optimizations around outer apply vs join subselects vs join + conditions... the queries generate wildly different execution plans, with up to 100x performance improvements due to streaming and memory consumption. And in terms of "simple enough to check", that requires crunching through data... when one of my tables has 100m rows, another has 3b rows... "just run the query" isn't really practical
I would question this... i can't speak to "all the RDBMS", but certainly on the ones I use heavily, there are a number of queries that are designed poorly, that can't be rewritten without questioning the impact. Further, for the tool to identify the impact, users can be informed of edge cases that they may not have been aware of.
SELECT id FROM users WHERE username IN ( select a.username from ( select count(username), username from users group by username having count(username) &gt; 2)a ) limit 10 offset 1 
1 duplicate user is easy enough. how would i write a query with multiple duplicate users eliminated the first id from each duplicate??
I get it. I usually kick off the original query and let it run for 12 hours or whatever. Then at the end of the day when I've finished working through a dozen optimizations, I can compare the aggregate validation to check if anything I did changed the results. There's another trick I use which deterministically selects random matching subsets of linked tables (yeah it sounds like a contradiction). DECLARE @newid uniqueidentifier = NEWID(); SELECT * FROM FactCustomer WHERE RAND(CHECKSUM(RAND(CustomerID),@newid)) &lt; 0.002; SELECT * FROM FactOrder WHERE RAND(CHECKSUM(RAND(CustomerID),@newid)) &lt; 0.002;
That is not quite true. First of all, what you're doing is totally valid, but also totally different: given two procedures `q1`, `q2`, you throw a finite number of probes at them and see whether the resultsets `r1`, `r2` are identical (or at least equivalent, depending on your testing criteria). That is fine and basically what everyone does in TDD (only the timing is different in TDD; typically, you have a formulation `q1` that produces some `r1` for a given set of probes; those results have been checked to contain no errors. Then, when someone commits a change (optimization or whatever) `q1 -&gt; q2`, you run the new version `q2` against the same set of data; if none of the results have changed, you may accept the change. The problem with that procedure is that with most functions you can never exhaust the practical space of legal input values (let alone their *theoretical* space, which is infinite most of the time, numbers going to infinitely large and infinitely close to zero, strings allowing an infinite number of characters, etc.). Now, in case (1) a formal equivalence check between two queries can be successfully done, and (2) should fail for two samples `query1`, `query2` provided by you, while you (3) point at your test results that indicate a pass for equivalance—you still failed the task. All you did was a demonstration that the two queries behave the same for a certain subset of possible input values. You logically don't get out of this; if you now claim that the counterexamples use combinations of values that could never appear in the data, then this is amounts to saying that the constraints of the system were not properly described to start with, which invalidates both your tests and the formal checks alike. 
SELECT DISTINCT A.ID FROM TBL A INNER JOIN TBL B ON A.USERNAME = B.USERNAME AND A.ID &gt; B.ID; Thoughts?
I understand the difference between the two approaches, but I fail to see how a mathematical equivalency is a particularly useful test for optimizing queries. If it's often going to fail for perfectly legitimate changes, what practical use is it? If nothing else, a significant portion of query optimization is taking clever advantage of relational constraints, which mathematically equivalent syntax evaluation will never recognize. There theoretically exists cases which fail, but such cases physically cannot exist in a particular database. --- Take this example: SELECT j.job_id, jh.start_time FROM job AS j CROSS APPLY (SELECT TOP(1) jh.start_time FROM job_history AS jh WHERE jh.job_id = j.job_id ORDER BY jh.start_time DESC) AS jh with an appropriate index is a significant optimization of SELECT jh.job_id, MAX(jh.start_time) AS start_time FROM job_history AS jh GROUP BY jh.job_id and is guaranteed to return identical results, given a FK between job and job_history.
Need &gt; 1 not &gt; 2, and no need to limit/offset.
Thanks!
To be fair, the OP never advertised "find equivalent queries that run faster", it only advertised "check whether two queries are mathematically / logically equivalent". But what makes you so sure that 'there are some constraints that equivalence checks will never cover', and given you're so sure that those 'theoretically failing cases can never physically exist in the DB', shouldn't those cases be exactly those that are ruled out by formal constraints (unique, not null, fk, greater than zero...)?
Thanks! I think this should work
I have no problem with the OP. It's an admirable academic exercise. I was mostly replying to: &gt; Microsoft should totally purchase this and include it in Management Studio and Visual Studio. I suspect that it would fail the majority of optimized queries, burdening users to further justify their work beyond equivalent output. Mathematically equivalent queries already get identical execution plans more often than not, so it'd be rare that such a query actually performs better. &gt; Given you're so sure that those 'theoretically failing cases can never physically exist in the DB', shouldn't those cases be exactly those that are ruled out by formal constraints? I think the counter examples will be trivially discounted, even in cases where legitimate discrepancies exist. Things like multiple CustomerIDs in the Customer table, or that every join is potentially many-many desensitizing users to truly problematic joins. Then there's optimizations like this: --Original SELECT * FROM Customer AS c INNER JOIN Order AS o ON c.CustomerID = o.CustomerID WHERE c.CreateDate &gt;= @Filter --New SELECT * FROM Customer AS c INNER JOIN Order AS o ON c.CustomerID = o.CustomerID WHERE c.CreateDate &gt;= @Filter AND o.CreateDate &gt;= @Filter Here counter examples can exist but procedurally won't, so it's a legitimate optimization. For all these reasons, I suggest that evaluating the output (and the entire output) of a query is superior, easier, and sufficient for establishing that a changed query is valid.
Easier to spot dropped commas/not drop when they're aligned, I suppose.
You're going to need a column in "NAME_AND_ADDRESS" that matches a column in "STUD_CRS_HIST". You want to use a join: https://www.w3schools.com/sql/sql_join_inner.asp And a where clause: https://www.w3schools.com/sql/sql_where.asp
Having done the 2012/2014 courses myself I would heartily recommend them. Even if you just do the first exam (Querying SQL Server), you will learn a vast amount, and it will get you thinking like an SQL dev. They even cover a bit of performance tuning, which is always in high demand. Contrary to what some have said, I would always offer an interview to someone who has paid and studied their own way through. It shows a passion for the subject, and you can teach skills but you can't teach passion! Best of luck!
I have 0 certs, but 20 years of experience keeps me employed.
 CONCAT( '&lt;font color="red"&gt;' , ROUND(100*SUM(ATD.duration)/SUM(CLS.lessonDuration),2) , '&lt;/font&gt;')
Understood. I am looking to the certification as a means to get my foot in the door.
When you sign up for the exams, do they include or recommend training material?
Yes.
Try looking for how you can cast sysdate into a timezone with offset.
unpopular opinion, but i prefer Lower camelCase for all SQL objects. Much more legible than underscores if other naming conventions (i.e. indexes) make use of underscores. 
Easy.
If you are looking for the whole experience I recommend CBT Nugget's videos and for books 'Murach's SQL Server 2008 for Developers' or another version from the Murach list. SQL Express is free as long as you are under certain criteria.
Oracle has built-in functions for time zone conversion. For Oracle 10g look here: https://docs.oracle.com/cd/B19306_01/server.102/b14225/ch4datetime.htm#i1007699 For Oracle 12c look here: https://docs.oracle.com/database/121/NLSPG/ch4datetime.htm#GUID-D8C7A7EB-A507-42A2-9B10-5301E822A7F2 An example from there: SELECT FROM_TZ(CAST(TO_DATE('1999-12-01 11:00:00', 'YYYY-MM-DD HH:MI:SS') AS TIMESTAMP), 'America/New_York') AT TIME ZONE 'America/Los_Angeles' "West Coast Time" FROM DUAL;
A couple things. Having SQL on your resume is enough you don't need it imo. If you can query in access you can query mysql mssql etc... now you may want to brush up on dynamic sql and dynamic sql programming if you want to work as as a developer, but if you're wanting to do administrative tasks like backing up, scripting, security, performance, jobs etc I think you can do fine without a cert. You should look into NoSQL databases like MongoDB and make sure you have an understanding of how that works. Lucene is another. Also check out Pluralsight that's a great way to get your tech chops where they need to be. You should also have an understanding of how the database runs in the cloud and how you deploy. For instance publishing and applying DACPACs are a bit different on a local box vs azure when you create them in visual studio. You can also create them in management studio if you install data tier application framework. You may want to brush up on Powershell, look at the SQLPS module and it's syntax look at how you can combine this with a dacpac to publish new databases. If you want to do anything on the Microsoft side of the house and you're serious about it, you must know how to use Powershell. Honestly any tech cert you get right now should be cloud focused companies are moving away from on prem and are paying premium for people with cloud and scripting skills in powershell or Python or Ruby etc... I'm a DevOps engineer with a good amount of experience if you need any advice let me know I'll do my best to steer you in the right direction if you're wanting to move into a technologist role. I think there are a lot of dead end places you can end up having a good understanding of what the market wants is key and how your current skills fit into that so you can market yourself accordingly. It's a lot more than a cert but if you get one the only one I could see paying off right now w database would be like a certified cloud architect or something in Azure since you're Microsoft oriented.
This is MSSQL but I imagine Oracle is similar: SELECT position , emp_id FROM ( SELECT position , emp_id , row_number() over (partition by position order by salary asc) as x FROM employees ) y WHERE x = 1
 SELECT c.Date , SUM(p.Price) AS Price , COUNT(*) AS Count FROM ProductionTable AS p INNER JOIN Calendar AS c ON p.ProductionStartDate &lt;= c.Date AND c.Date &lt; p.ProductionEndDate GROUP BY c.Date; Be warned, this could require a huge intermediate data set. Supose we have 100k jobs which are active for an average of 100 days. 100k x 100 = 10 million instances of a job being counted. These kinds of queries can get very expensive.
Thank you
You can either do just the exam, or you can attend a course to do the training as well. I did the exam along with the official book (if you type the exam number in Amazon you'll see them). The official training books are good as they come with a CD with sample exams on too.
That's the whole *point* of a database, no?
That said, don't use MySQL, use PostgreSQL instead. There's plenty of videos and articles out there that detail why; I can supply some pointers in case you insist.
Welcome to the [delete]-o-rama once more. Why.
 &gt;SELECT Count(Machine.[MachineType]) AS 111, Count(Machine.[MachineType]) AS 133 &gt;FROM Machine ... I am assuming you want to count different machinetype? You could try something like Select Sum(case when machinetype = 'condition 1' then 1 else 0 end) as 111, Sum(case when machinetype = 'condition 2' then 1 else 0 end) as 133 From machine...
I'm not sure I follow...if you want to do counts like that, you don't need a where, you'd use GROUP BY and HAVING
You are right you cant index it in a convetional way. You could use fancy stuff like [Trigram indices](https://www.postgresql.org/docs/current/static/pgtrgm.html)
Then whatever you do, work some proper dimensional design into your studies. That and SQL will open doors for you.
http://www.codeatglance.com/stack-using-array-implementation-push-and-pop/ you may have a look on our c programming code how to implement stack using array. 
If the number might differ between calls, you can also first declare a variable like so: DECLARE @SOMENUMBER INT; SET @SOMENUMBER = 3; And then insert the variable instead of a literal number.
You're likely better off using DATEADD(YEAR,DateField,6) CONVERT(Varchar(8),DateField,112) Will get you YYYYMMDD Text from a column to an INT is a bit trickier unless the only thing there is a number. I believe it's an implicit conversion meaning the SQL engine will do it for you. Else CAST(Field AS INT)
&gt; INNER JOIN Calendar AS c &gt; ON p.ProductionStartDate &lt;= c.Date &gt; AND c.Date &lt; p.ProductionEndDate For readability I prefer INNER JOIN Calendar AS c ON c.Date BETWEEN p.ProductionStartDate AND p.ProductionEndDate but it works the same Also be aware that if the StartDate has a time element in there, you'll want to strip that out or you'll miss records.
Thank you!
BETWEEN is inclusive, but OP doesn't want to count jobs on their EndDate. To use BETWEEN you want to subtract a day from the EndDate. 
Would like to add to this that if you are doing the comparison in a WHERE clause you are possibly killing your performance. 
What would be the better option? Grab them all and do the comparison on the smaller set?
Would need to see the full query. It really depends. Working with smaller sets is usually better performance, yes. 
This is awesome! Thank you so much! It works perfectly except for one thing... ProductionEndDate can be null. This ends up being important, because stuff that isn't null shows in the historical backlog fine. But as the date gets closer to now() we only see jobs that are completed as that day's backlog. I tried doing SELECT c.Date , SUM(p.Price) AS Price , COUNT(*) AS Count FROM ProductionTable AS p INNER JOIN Calendar AS c ON p.ProductionStartDate &lt;= c.Date AND (c.Date &lt; p.ProductionEndDate or p.ProductionEndDate is null) GROUP BY c.Date; But that obviously didn't work. Thanks again for taking the time to think about this.
10-4 on the amount of records. We are talking about 1000 jobs a month that take 2-4 days or around 60 active jobs/day. (We *think*... which is why we need to report on it.)
Awesome thank you so much!
That's a good point, you'd get way better performance if you compared the table field to a variable that was get date minus 6 years. To op: using functions in joins or where clause will slow performance, once you use a function you lose the use of any indexes on that field.
To get them in the same row (total added by me); SELECT COUNT(MachineTypeID]) AllMachines, COUNT(CASE MachineType WHEN 111 THEN 1 ELSE NULL END) 111, COUNT(CASE MachineType WHEN 133 THEN 1 ELSE NULL END) 133 FROM Machine I would do it like this: SELECT Machine.MachineType, COUNT(MachineTypeID) CountOfMachineType FROM Machine GROUP BY Machine.MachineType HAVING Machine.MachineType IN (111,133,...etc)
Use ... &lt; ISNULL(productionEndDate, getdate())
your HAVING clause should be a WHERE clause filter before grouping, not after
correct. Brain fart.
You want to replace NULL EndDates with the last day you want open jobs to be counted. The current day, or in the future if you want to project things.
How could i insert something like that into the query to alter the time range being searched for? I'm trying to automate some reports, so i would only want that individual query to be affected
Thanks for your help!
what company is this for so i can remember to NEVER DO BUSINESS WITH THEM
You need to change the application as soon as possible. Storing passwords in plaintext is horribly insecure and could very easily jeopardize your customers or employees, leaving the company liable. The application needs to take the password field, put it through a hash function such as SHA256 (ideally with some sort of salt), and then store that value in the database. When a user logs in - same process. Encrypt, then query the database to see if the text matches. As for the migration process, you're going to want to rewrite the application and create all of the password storage/retrieval functions, then write a small additional function to read, hash, and then update all passwords. Make a copy of the database beforehand and test thoroughly before deleting, but you *must* delete that database with plaintext passwords ASAP. Please start doing research [here](https://stackoverflow.com/questions/947618/how-to-best-store-user-information-and-user-login-and-password) and then [here](https://stackoverflow.com/questions/674904/salting-your-password-best-practices).
Additionally, if this concerns systems that are financial, medical, or contain other sensitive information: all users or employees should change their passwords once the new system is ready. If that's not the case, this step is less imperative, but still something you should strongly consider. This needs to be somewhere in your top priorities, and I don't say that lightly. Yes, I know. Security is a hassle and no doubt upper management is going to hate to hear you recommending a password reset. But it's a lot better than a data breach and fending off what lawsuits may come.
I forgot to mention Youtube channel Kudvenkat. His accent sometimes makes it hard to follow but I found his [videos](https://www.youtube.com/playlist?list=PL08903FB7ACA1C2FB) on Merge and Triggers helpful. 
This is a pretty straightforward method: https://www.mssqltips.com/sqlservertip/4037/storing-passwords-in-a-secure-way-in-a-sql-server-database/ You will need to change the .NET application as well to handle the new query.
If this concerns **any real people or any valuable information**, all users should change passwords after the issue is corrected. FTFY
Change the table to a view and to a binary string. Limit access to the original table. That's a quick and dirty way. 
Great use of unique salts, great guide in general. +1
I disagree, I think it's more situational. As an example, if this were a product forum (which concerns real people but not particularly 'valuable' information) and there were no evidence of a prior breach, while I would still suggest a password reset for all users I would not mandate it. The unfortunate consequence of password changes is that the new password is often not significantly different than the old one, so the security gained is usually small. That said, if this were a security-imperative situation (financial, medical) I would make it absolutely mandatory, since the less secure option really can't be an option at all.
I don't think you can do that in sqlite.
Very bad idea. Any time passwords are compromised blacks hats routinely find that password is being used in other locations; i.e. people reuse passwords all over hell and creation. Might as well do away with them altogether for your site, that way when you are hacked your customers' passwords won't be compromised. /s edit: a word 
haha! This is a joke, right?
Yes, I am aware. Password reuse is extremely common. So if the database is already leaked, people are already pretty well screwed. In a non-critical case, advise all customers to change their passwords on that site and any site they reuse that password on, esp. sites that are more sensitive. And this is what I would recommend in *all cases*. But *mandating* the password change is where I disagree.
It's not a long term fix, but it's an immediate solution 
One thing I do not like about that guide is that it returns invalid password vs invalid login. By returning invalid password you are basically telling any would be hackers that they have found a legitimate username. If an invalid pass is supplied it should return the same error response as invalid user: "invalid username or password" 
And pepper! But pepper is a bit more than most people are willing to do.
Good point, didn't read that closely. My b. +1
FYI, you don't store them "encrypted". That is a reversible function (ie: you can decrypt them). Instead you need to store a salted hash instead. In layman's terms, it's like a one-way encryption. That way the admins, or hackers, or anyone else with access to the database can never discover the actual password. User types in the password when registering. It's stored as a salted hash. User logs in later, and the password they supply is again run through the same salted hash function. Compare the 2 salted hashes, not the actual passwords. You'll need to add columns to your table, something like "hashed_pw" and "salt". Use MSSQL built in HASH functions (HASHBYTES()?) to fill the new hashed_pw column based on the plaintext password. Each user should have a different salt. Once all the new hashed PWs are filled in, test it. Then test it more. When you're 100% sure everything is working, you can delete the plaintext passwords.
Help me understand this. Salting is done to make it difficult to bruteforce passwords even if the hash gets leaked. The salt is stored in the same table as the hash. If an attacker gets their hands on the database, they'll have both the hash and the salt. The advantage of salting passwords in this way seems dubious. Am I missing something or is there a flaw in the article?
wonder if it could not optimized in one way or another.
It's not a solution at all. The whole problem is that the passwords are stored somewhere in the database in plain text. Using a view does not correct the fact that the table is still there, with the plain text passwords.
Even with the salt they still have to brute force the password but now they have to use the salt for each and every user password. The salt is different for every user so this protects against using rainbow tables. At best you can get one password at a time in however long it takes you to crack a single salted hash. Basically salt helps protect against anyone building a precomputed lookup table.
They just don't know about the [benefits](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2825034/).
Say my password is hunter2 and I use that fucker everywhere. Say our h4x0r friend gets a hold of a list of known passwords and accounts right. Pretty easy to do if you know what you are doing (these kind of lists are everywhere). Say they also have generated a hash table containing hunter2 also. Since I have been using hunter2 for so long, it's already been part of a previous data breach. hunter2 has already been hacked before I even enter it into this new application. Everyone knows to check for hunter2. hunter2 might as well be no password. If we don't salt our password, whoever has our data can use a hash table to figure out our password. Say we salt hunter2 with a GUID. Now our password is hunter266311780-6eec-11e7-9598-0800200c9a66. Now there's no way that someone has a hash table to compare this to already because hunter266311780-6eec-11e7-9598-0800200c9a66 isn't a common password, hasn't been breached. Not part of an existing hash or rainbow table. Since they only know "266311780-6eec-11e7-9598-0800200c9a66" and not the "hunter2" part, they can't use a simple hash table to crack our encryption. There is probably other reasons and a better explanation but hopefully this makes sense. Here's a SO post that might help you too: https://stackoverflow.com/questions/16891729/best-practices-salting-peppering-passwords
That worked perfectly! I had been thinking i could set the null dates to something in the future. But this worked great.
Nulls in SQL can be a pain, especially when you don't expect them. 
Most local or smaller companies ever.
As everyone has stated you want one way hashing not encryption. Something not mentioned though is CPU complexity. There al algorithms and libraries that handle a lot of this for you. Look up bCrypt. It is a hashing function that can auto generate the salt for you. It also outputs a string that encodes the hashing algorithm and salt in the single string so you don't need to deal with multiple columns. The CPU complexity part basically runs the hash function N times which makes it exponentially harder to brute force. bCrypt comes with a logarithmic iteration scale and every year or two you are supposed to bump the number of iterations up to account for faster CPUs.
Pick a dataset you're interested in, like a sport or a game or stocks or something. Try [Kaggle](https://www.kaggle.com/datasets), [Amazon](https://aws.amazon.com/datasets/), [Google](https://cloud.google.com/bigquery/public-data/), or /r/datasets/ for inspiration. If you're interested in the data, the questions (and hurdles) will come naturally.
One of my regular responsibilities is to optimize queries which run for 8+ hours. They usually have some complex business logic with comparisons between several data sources not naturally linked. There are also occasionally mass updates which can take all weekend, but they're not run in a single query.
The column `cat_hidden` doesn't exist in table `mw_category`.
My longest running query is only a few hours, and that was comparing one list of drugs with another list of drugs using MDS. My longest non-MDS query was generating bulk data for presentation purposes within SSRS. To note, we did consider just hard-coding some values for the SSRS reports, we figured you can do better demos with a real dataset. That took a couple of hours to run (on a very fast dev box), but much longer to write!
Ah, password reuse and rainbow tables. I see!
I know this post is a little old, but check out [SeekWell](http://www.seekwell.io/reddit). It enables you to connect to databases and write SQL queries directly in Sheets. A few other key features: * Quickly view all tables and columns in a database and **get summary stats on a column with one click** * Query from the sidebar, a large pop-out window, or even from within a cell * **Results can be sent to a specific cell, scratch sheet or directly to a pivot table** * Your query history is saved and viewable if you need to re-execute an older query * You can save a set of queries on a “Run Sheet” to **update multiple reports at once** Check it out the [add-on](https://gsuite.google.com/marketplace/app/seekwell/108156487752) or our [site](http://www.seekwell.io/) and please let me know if you have any questions!
 UPDATE TableName SET points = 9000 WHERE steamId = '76561198125971713';
Ended up completing in 2 days, 6 hours, some odd minutes. 50+ hours... crazy.
Since I am a MS SQL guy, I would recommend their free version called SQL Express. IMO, it is a far more marketable db platform than SQLite. 
It depends - what do you want to do? For example, I learned that I had more fun analyzing the data itself, and went to grad school for it. SQL is still a major and necessary part of my field, but I'm not doing any common DBA tasks. 
That's the thing I've done only development stuff not much analysis. Looking at the Microsoft Data Science course to learn more. Not sure what other options I should look at? 
You can take the next step by updating your resume and talking to recruiters. As far as what might be next, OLTP work tends to lead into DBA responsibilities. OLAP is more ETL heavy and get's into cubes and MDX. Data Science is another lucrative option.
Do you have an execution plan by any chance?
Within data science? Tons, it's a huge field. I also haven't done the Microsoft course, but I've heard pretty good things about it. Let me know if you want some resources for going down that route. For other options, going into the DBA route, data engineer, management, ETL specialist, etc. It's a pretty flexible field :)
Broad or deep? If you want to be an architect you need to go broad, go get a job in pure BI and learn the ropes, then pivot to event streaming or the Hadoop ecosystem, then try your hand at machine learning. Download every tool and kick the tires, do every demo you come across, subscribe to every blog and StackOverflow tag and Quora digest you can. The key is to build a large toolset so you can knowledgably tell someone the best way to solve their data problem. If you're going deep just focus on one area and master it completely. You can be a consultant in the field and pick and choose your clients and interests. The goal is to be the first name in every CIOs vendor list for your skill area.
thanks bro!
WTF is 2:1? 
 SELECT numbers.n AS Character , SUBSTRING(words.word FROM n FOR 1) AS Letter , COUNT(*) AS Count , 'EFTS' AS Frequency FROM words CROSS JOIN ( SELECT 1 AS n, 2, 3, 4 ) AS numbers WHERE words.length = 4 GROUP BY numbers.n , SUBSTRING(words.word FROM n FOR 1) 
No GROUP_CONCAT sorcery for frequency?
EFTS means exercise for the student... term used by my teachers when i went to school, admittedly a long time ago... similar to QED quod erat demonstrandum, except i couldn't find an internet reference for EFTS... must have been a ~long~ time ago
I was wondering what EFTS meant! Thank you for that, it's still going to take me a while to understand what you wrote and what it's actually doing, though! Just for initial sample, I have 54 rows of data. This query produces 34 results with a total count of 54. So it looks like it's doing exactly what I wanted with just the first character. it looks like Line 7 (`JOIN ( SELECT 1 AS n ) AS numbers`) is where character is selected by position in the string? I'm also seeing that it counts `b` and `B` as the same character, so I need to look up case sensitivity. But yes, like I said, I probably could have plodded through this in PHP, but this is far more efficient! MY hat is off to you, thank you so much! :)
Hi, Is this an acceptable answer? SELECT ID, Username FROM table AS t WHERE ID = ( SELECT ID FROM table WHERE t.username = table.username AND t.ID &gt; table.id); Thanks for posting this question.
Is this correct? SELECT ID, Username FROM table AS t WHERE ID = ( SELECT ID FROM table WHERE t.username = table.username AND t.ID &gt; table.id); 
Not quite. This will only select the first dupe for each username. But a simple change could make this work better (albeit slower than other solutions because of the correlated subquery). It's a simple change in the outer select statement. Probably the best way to play with puzzles like this offline is to make your own little SQLite database, fill it with 20 or so usernames, add another 15-16 duplicates, triplicates and quadruplicates, and see if you can solve the problem for real.
Thanks for the reply and the advice.
What's you query? This is usually fixed by making join criteria unique.
sorry since my query quite long and for security reason just make my image results as table. so let's call i have a table with that data and want to get results like image no 2. how to do that?
Change your join conditions so that they all use unique criteria.
It's a column store instead of a row store. This is the biggest difference between the two.
i don't think there's a unique criteria. this is for general situation my post only for example. the real situation is thousand data. above only example when i found this double reservation number. it should be that 1 reservation has/in the 1 unit number. in my case 1 reservation number in the 2 unit number while they have/in same memno.
They're not duplicates though. Look at the incrementation in the "reserve_numb" column. If you just want the most recent row, SELECT MAX(Reserve_numb) for the column. 
As an analyst, the biggest thing you'll see is the change in data throughput. Vertica is designed for reporting/data warehousing so your SELECT queries will be much more performant. The biggest difference to me as a user/administrator is that there is currently no way to create Stored Procedures, though in my department most of the processing is done in SQL Server beforehand so it wasn't a big issue.
What version and DBMS of SQL? If you are using SQL Server 2016 (or Oracle) try `LEAD` and `LAG` functions. They return the next and previous row data.
 SELECT r.ReservationNo , um.UnitNo , um.Memo FROM reservation AS r INNER JOIN (SELECT ISNULL(u.ReservationNo,m.ReservationNo) AS ReservationNo , u.UnitNo , m.Memo FROM (SELECT u.ReservationNo , u.UnitNo , ROW_NUMBER() OVER (PARTITION BY u.ReservationNo ORDER BY u.UnitNo) AS RowNo FROM unit AS u) AS u FULL OUTER JOIN (SELECT m.ReservationNo , m.Memo , ROW_NUMBER() OVER (PARTITION BY m.ReservationNo ORDER BY u.Memo) AS RowNo FROM memo AS m) AS m ON u.ReservationNo = m.ReservationNo AND u.RowNo = m.RowNo ) AS um ON r.ReservationNo = um.ReservationNo 
Hey! Thanks for your answer. I tried Lead and Lag functions, but it doesn't seem to work, as it gives me 'random' week values (I don't even see a pattern to be honest.) CONCAT(cast(Weekday AS string),'-',cast(lag(week) OVER (partition by platform, region, campaign order by platform, region, campaign) as string)
Try with some ordering.
&gt; order by platform, region, campaign This ordering?
Since you are only able to post random piece of code and not provide sample data, it's difficult to see what you are doing here. Does that ordering give you unique values? You also stated you are looking to get the previous week without campaign. I'm not 100% what you mean by this, but you're including it in the partition and ordering. Not sure if that is intentional.
My 'Campaign' column defines periods, labelled as 'No Campaign 0', 'No Campaign 1',... and 'Campaign 0', 'Campaign 1',... My 'DayCode' column defines a day code based on the day of the week, and the week of the year (e.g. 3-3 for Wednesday of the 3rd week of the year). it's a concatenation of the columns 'WeekDay' and 'Week'. I want a column which would be able to tell me the daycode of the previous period for the same day of the week. For example, let's say a campaign is running on 3-3, I'd like for the code to seek for the daycode of the same day of the week when a campaign was not running. Sorry if I'm not clear, I'm a bit new to sql. That ordering does not give me unique values, it gives me random week numbers. We can jump on a quick skype call and I'll share my screen with you, I just don't want it to be available on the internet for everyone to see. 
I'd go with a sub query of some sort, the following should get you started, there are a couple of ways to do it. Subquery in select section: SELECT subject , (SELECT COUNT(*) FROM Course Inside WHERE Inside.subject = Outside.subject] AS [# of courses] FROM Course AS Outside GROUP BY subject; Subquery as a data source: SELECT subject , CourseCount.[# of courses] FROM Course left join (SELECT subject, COUNT(course_id) AS [# of courses] FROM Course GROUP BY subject) AS CourseCount ON Course.subject = CourseCount.subject GROUP BY subject, CourseCount.[# of courses]; I don't have the time to go into too much detail and don't really want to do the homework myself... but hopefully the above pair of examples can steer you in the right direction.
I think you want to get counts by course_id, join them to Course, then group by subject and sum your counts.
In the UK (and likely other places...) university degrees usually come with honours and these rank from 1st (the highest class of honours) to upper second (2:1), to lower second (2:2) to a 3rd and finally a degree without honours. There are many ways of writing upper and lower second class honours...
The issue is that last comparison: case when 'Count' &gt; 150 then 'Check' This is comparing the string 'Count' to the integer value 150, which isn't possible. To resolve the issue, all you should need to do is: case when count(UserActionName) &gt; 150 then 'Check'
Awesome ty
 MAX(CASE WHEN camapign='No' THEN week ELSE NULL END) OVER (PARTITION BY weekday ORDER BY week RANGE UNBOUND PRECEDING) + '-' + CAST(weekday AS varchar)
Hi ! Thanks for your reply. Unfortunately, your code gives me the week corresponding to the date :/ EDIT : Thanks to you, I tried my initial idea : min(Week) over (partition by platform, region, campaign)-1 and added a partition by weekday. It finally worked! Thank you soverymuch! 
Hi everyone! Thanks for your help. I finally succeeded to make it work thanks to /u/jck4hokies. My final solution is to just add a layer of partition by to my initial idea : CONCAT( cast(Weekday AS string),'-', Cast(min(Week) over (partition by platform, region, campaign,weekday) as string)-1) Thanks again!
As /u/Erudition303 pointed out, your error is happening because you're comparing the string 'Count' to the number 150, and 'Count' can't be converted to an integer. Also, I have no clue what you're trying to accomplish. You're grouping by UserActionName, which (depending on the DBMS which you fail to mention) will cause COUNT( UserActionName) to return 1. Because for that row there's only one UserActionName. Then you're looking for all sorts of contents to the UserActionName and having your case return that 1 And then in a separate case you're trying to find out if the result of that count is greater than 150??? None of that makes any sense, and all those LIKE statements are going to suck for performance. Also.. your from doesn't identify what table this is all supposed to come from??? Reformatting your code, I get SELECT UserActionName AS 'User Action Name', CASE WHEN UserActionName LIKE '%Req%' THEN COUNT( UserActionName) WHEN UserActionName LIKE '%Up%' THEN COUNT( UserActionName) WHEN UserActionName LIKE '%App%' THEN COUNT( UserActionName) WHEN UserActionName LIKE '%Crea%' THEN COUNT( UserActionName) WHEN UserActionName LIKE '%Imple%' THEN COUNT( UserActionName) WHEN UserActionName LIKE '%Rej%' THEN COUNT( UserActionName) WHEN UserActionName LIKE '%Under%' THEN COUNT( UserActionName) WHEN UserActionName LIKE '%No%' THEN COUNT( UserActionName) WHEN UserActionName LIKE '%ancel%' THEN COUNT( UserActionName) WHEN UserActionName LIKE '%ndo%' THEN COUNT( UserActionName) WHEN UserActionName LIKE '%heck%' THEN COUNT( UserActionName) WHEN UserActionName LIKE '%dded%' THEN COUNT( UserActionName) END AS 'Count' -- Bad form, do not name columns after reserved words. , CASE WHEN 'Count' &gt; 150 THEN 'Check' END AS Comment FROM - SOME TABLE NAME HERE GROUP BY UserActionName My wild guess to your intended query... SELECT [User Action Name] , [ActnCount] , CASE WHEN ActnCount IS NOT NULL AND ActnCount &gt; 150 THEN 'Check' ELSE ' ' -- Or null, or whatever, I just like having an explicit else covered. END AS [Comment] FROM ( SELECT UserActionName AS [User Action Name], CASE WHEN UserActionName LIKE '%Req%' OR UserActionName LIKE '%Up%' OR UserActionName LIKE '%App%' OR UserActionName LIKE '%Crea%' OR UserActionName LIKE '%Imple%' OR UserActionName LIKE '%Rej%' OR UserActionName LIKE '%Under%' OR UserActionName LIKE '%No%' OR UserActionName LIKE '%ancel%' OR UserActionName LIKE '%ndo%' OR UserActionName LIKE '%heck%' OR UserActionName LIKE '%dded%' THEN COUNT( *) -- if this UserActionName matches -- one of the conditions in this list -- count how many rows have this particular value,. ELSE NULL -- If it doesn't match one of the conditions in this list do not return a count. END AS [ActnCount] FROM - SOME TABLE NAME HERE GROUP BY UserActionName ) Edit: That'll still suck for performance, possibly even more so with the ORs, but it seems to actually server a purpose.
&gt; WHEN ActnCount IS NOT NULL AND ActnCount &gt; 150 FYI you don't need the IS NOT NULL check here 
True, but I always include it. For a long time, many dbms's would return goofy results when you compared a null to anything, so I always limit my comparisons to when not null.
NVM think I got it, SELECT CMP.CampusName, sum(case WHEN REG.enrollmentType = '1.0' then 1 else 0 end) AS 'Full Time', sum(case WHEN REG.enrollmentType = '0.75' then 1 else 0 end) AS '3/4Time', sum(case WHEN REG.enrollmentType = '0.5' then 1 else 0 end) AS 'Part Time' FROM Students STD INNER JOIN Registrations REG ON REG.studentID = STD.studentID AND STD.isactive = 1 INNER JOIN Campuses CMP ON CMP.CampusCode = REG.studentCampus AND CMP.isactive = 1 GROUP BY CMP.campusname 
NVM is there any way to remove duplicates for this?
select distinct, or just do a partition and choose 1 row
I think you want `SUM(CASE WHEN REG.enrollmentType = '0.5' THEN 1 ELSE 0 END) AS 'Part Time'`.
I did select distinct but because its off the first name/ last name it does not fall under REG.enrollmentType. I cannot have names in the query as well. As in its only going off +1 or 0 for Reg.EnrollmentType.
You can test here: http://sqlfiddle.com CREATE TABLE IF NOT EXISTS users ( id int(6) NOT NULL, username char(10) NOT NULL ) ; INSERT INTO users (id, username) VALUES ('1', 'Homer'), ('2', 'Moe'), ('3', 'Marge'), ('4', 'Moe'), ('5', 'Moe') ;
Seems like **COUNT(DISTINCT [column_name])** should help a lot here. Do you have to write hand-write these queries without access to test data? That's pretty rough...
"many dbms's would return goofy results" LOL pics or it didn't happen 
This gives the correct answer. but i'm not sure what A.ID &gt; B.ID is doing to make it work. 
You don't say what software... I primarily work in Oracle. I'd write a query first that joins up all the columns needed, then count/sum them and group by subject. Something along these lines: SELECT SUBJECT, count(distinct COURSE_ID) as NUM_COURSES, count(distinct CLASS_SECTION_ID) as NUM_SECTIONS, count(distinct ri.INSTRUCTOR_ID NUM_INSTRUCTORS, COUNT(distinct rs.STUDENT_ID) as NUM_STUDENTS, sum(c.CREDIT_HOURS) FROM (SELECT c.SUBJECT, c.COURSE_ID, ri.CLASS_SECTION_ID, ri.INSTRUCTOR_ID, rs.STUDENT_ID, c.CREDIT_HOURS FROM COURSE c INNER JOIN REGISTRATION_INSTRUCTOR ri on c.COURSE_ID=ri.COURSE_ID INNER JOIN REGISTRATION_STUDENT rs on c.COURSE_ID=rs.COURSE_ID) GROUP BY SUBJECT
&gt; Personally, I'm convinced of the superiority of leading commas. which is why i still love you but wait... when a better idea comes along, who wants just to stick to the old way of thinking, so that the precious snowflakes who are acclimated to it don't throw a hissy fit? since when did "but we've always done it this way, so it's better" become something people want to agree with? 
leading commas are even more beneficial if you write SQL in a text editor where you use the keyboard more often than the mouse for example, using Shift along with the arrow keys works like your alt-drag idea (i think)... i hardly ever use the mouse when working in my text editor
We use vertica at my place of business. It is apparently cheaper that keeping up with teradata development. What I do know is that our SAS users can't really connect to it through SAS so we essentially spent 6 million bucks trying to integrate a bunch of disparate systems from different lines of business for nothing. We all prefer to use teradata because the whole change was stupid to say the least. And they need to use td sql a to connect and view. I know that there are more functions for analysis in vertical than teradata. Apparently like ASTER, vertica is a noSQL environment. Don't quote me on that though 
I think the trailing commas mostly come from developers previous experience with lists before the became developers. Shopping lists, English homework etc. However leading commas as a best practice are much easier to maintain and are a habit I'm trying to form. The number of times I get 'unexpected.. ' because I've commented out an item and lost a comma or removed the last item and forgot the previous line ended in a comma is frustrating I think if you are in the habit of leading commas you should do them everywhere and for Collab and teaching purposes explain why they are better then more people will be using them anyway 
As I said in the previous post, I'm in the middle of writing a SQL guide for novices. Leading commas are undoubtedly going to be more confusing for them than trailing, if for nothing else because of how trailing commas looks more like normal writing and grammar... ...**However**, maybe it is worth exposing novices to leading commas early on. One of the most fundamental things that novices have trouble grokking is **insignificant whitespace** -- which is important to all kinds of programming but, like trailing commas, is unusual in normal English/grammar writing. Learning the leading commas style is one way to push them out of their comfort zone. Of course, everything is about tradeoffs and return on investment; grokking leading commas is hardly a huge cognitive load, so overall, getting used to the style may be worthwhile in the short-term **and** the long-term. But in terms of professional collaboration (e.g. working with folks on a project), I'd stick to whatever their styleguide is, and most seem to be silent on leading commas, or argue for trailing commas. I've already compromised on not converting camelcase in JS projects to snakecase, so I'll learn to live with trailing vs. leading commas :). 
That was my first thought too http://quoteinvestigator.com/2014/11/27/always-done/
i agree with your logic here
It's giving you all results that have an ID strictly less than itself.
Lol holy shit. this confuses me. 
When you restore a backup, you have to specify "I will be done restoring after this one" (`RECOVERY`/`NORECOVERY`) or it will stay in that state. This is so that you can restore a full backup followed by some number of incremental backups. I'm not familiar with Barracuda, but maybe there's some setting to control this behavior.
Yea what you're describing is what I'm used to. I'm no too familiar with this software he's using either. It literally just has a spot for him to pick the date / databases then he clicks restore. Sounds like an issue with Barracuda. 
thank you mate appreciate
I like leading commas on schemas that require heavy aliasing to avoid ambiguous column references. Otherwise I'm down with trailing commas.
What does that mean exactly?
I appreciate the utility of leading commas, but prefer the aesthetics of trailing commas. The SQL engines don't care.
In you're desired result set, I'm not sure where 4/21 7am is coming from. That date doesn't seem to be anywhere in your tables. I'm not exactly sure I understand the problem, but I grouped OrderDate by Order so that it uniquely joins to OpenOrder, and added logic to override the OpenOrder.EnteredDate if OrderDate.EnteredDate was greater. SELECT T.[Date] , T.[Time] , COUNT(DISTINCT O.[Order]) 'Open' FROM #Time AS T LEFT JOIN (#OpenOrder AS O LEFT OUTER JOIN (SELECT od.[Order] , MAX(od.EnteredDate) AS EnteredDate FROM #OrderDate AS od GROUP BY od.[Order]) AS od ON od.[Order] = O.[Order] AND od.EnteredDate &gt; O.EnteredDate) ON T.[DateTime] BETWEEN ISNULL(od.EnteredDate,O.EnteredDate) AND O.EndDate WHERE T.[Date] BETWEEN '4-20-17' AND '4-21-17' GROUP BY T.[Date] , T.[Time] ORDER BY T.[Date] , T.[Time] ASC;
That is a good way. Another option is this: SELECT OrderId , STUFF(MAX(CONVERT(char(23),EventDate,121)+Event),1,23,'') AS Event FROM OrderEvents GROUP BY OrderId You can test which way is faster.
ROW_NUMBER() OVER (PARTITION BY OrderID ORDER BY EventDate DESC) is absolutely the right way to go. You shouldn't be looking for anything else. Good job!
The most important thing I learned in college was K.I.S.S. Keep It Simple, Stupid. Please keep in mind I'm using SQL Server, but this should be compatible. I'm going to do a simple join between the two tables. Aliases A and B, and then I'm going to use a join to a subquery as a filter by using Max(). Hope this helps! select a.InternalOrderNumber, b.Event from Order a join OrderEvents b on a.OrderID = b.OrderID join (select OrderID, max(EventDate) as Max_EventDate from OrderEvents group by OrderID) c on a.OrderID = c.OrderID and b.EventDate = c.Max_EventDate 
Leading commas for in-dev code for rapid comment ability, trailing commas for archival and production code for flow and readability.
Hear hear on leading commas! I've never met anyone else who uses leading commas, so I'm glad to hear I'm not insane. Or at least there are other crazies out there.
https://oracle-base.com/articles/misc/sql-trace-10046-trcsess-and-tkprof For most of these you must have a user with dba role (access to some v$ tables). Also, if client has oracle enterprise version, you can make a snapshot before test and second snapshot after test, and after that generate a AWR report which gives you a lot of info to bedug and trace SQLs. https://oracle-base.com/articles/10g/automatic-workload-repository-10g Execute an AWR snapshot: EXEC DBMS_WORKLOAD_REPOSITORY.create_snapshot; select max(snap_id) from dba_hist_snapshot; Generate AWR from console: @?/rdbms/admin/awrrpt.sql For Oracle standard version there is statspack (the father of AWR) which gives similar information. https://docs.oracle.com/cd/B10501_01/server.920/a96533/statspac.htm Generate statspack snapshot: EXECUTE statspack.snap; Generate statspack from console: @?/rdbms/admin/spreport Also, if you have a PL/SQL Developer connected to the db, go to Tools -&gt; Sessions. Filter the active sessions -&gt; start your query -&gt; If the query runs long enough, you can see it the active session list. Select that row and select 'SQL text' view from the bottom menu. There you have the full SQL. All sql's are saved in v$sql and v$sqlarea tables, so you can find them also there (see column 'last_execution_time' or something similar).
dude... dude... STUFF is not an Oracle function, neither is CONVERT using the 121 style option, neither is string concatenation with the ~abominable~ plus sign you can test which way is faster
"flow and readability"
 SELECT ORDER_ID , SUBSTR(MAX(CONCAT(TO_CHAR(EVENT_DATE,'YYYYMMDDHH24MISSFF'),EVENT)),21) AS EVENT FROM ORDER_EVENTS GROUP BY ORDER_ID; SELECT ORDER_ID , MAX(EVENT) KEEP (DENSE_RANK LAST ORDER BY EVENT_DATE) AS EVENT FROM ORDER_EVENTS GROUP BY ORDER_ID; SELECT ORDER_ID , EVENT FROM (SELECT ORDER_ID , EVENT , ROW_NUMBER() OVER (PARTITION BY ORDER_ID ORDER BY EVENT_DATE DESC) AS ROW_NO FROM ORDER_EVENTS) ORDER_EVENTS WHERE ROW_NO = 1; Does that look more like oracle? Either group by solution guarantees that the join condition is unique, which can have internal advantages. And the string ordered aggregate can use hash grouping, so it doesn't need to sort.
I guess I don't need to quickly comment parts of my SQL that often. To me, the main advantage of leading commas is the readability, including reducing the number of errors that come from missing a comma.
I haven't found anything Free/Cheap. I'm expecting to spend about 600 US for study materials and the test.
Online? Duke University is offering some free MySQL courses apparently. Found out today so I'm going to check that out
[removed]
Code academy now has a sql course. I used a course like that and ucertify.com. If you can pass the ucertify tests, you will slay the actual test. 3 years ago i spent about 200 total for the test and materials. Best investement of my life!
Next I'd be curious if anyone besides me uses Alias = column rather than column as Alias I prefer the former because then all my column names are at the front, which is especially useful with long formulas.
Thanks guys. Really appreciate the help. I think I see the main reason why I am having such difficulty with this. I don't have access to test data so I created the tables in Access and then connected by query tool to that data to write SQL statements on it. The problem I discovered is Access doesn't allow the count distinct function so I kept getting syntax errors! I need to create these as SQL tables, but I don't how to set up a test environment. My SQL management studio wants to me to connect to a real server, and I'm too uncomfortable creating dummy tables on our live server at work. Any tips on I can create a test environment? I played around with sqlfiddle yesterday but I'm not sure if that website saves your tables after you close out the browser. Thanks again.
You can learn SQL for free or cheap, you have to pay for the certificates. I'd recommend studying oracle -- [cert](https://education.oracle.com/pls/web_prod-plq-dad/db_pages.getpage?page_id=654&amp;get_params=p_id:457&amp;p_org_id=1001&amp;lang=US#tabs-1-1) as it's a bigger market, although MS SQL also has a large market and their exams are cheaper. I don't know if I'd call general SQL 'impressive knowledge'. I'm certainly going to be impressed for example if your proficient with most of this book [Troubleshooting MySQL](http://freepdf-books.com/download/?file=6232) or have one of oracle's expert certifications (or RHEL or CCIE). The advantage of learning SQL, I think, is that it's everywhere. [reddit](https://github.com/reddit/reddit/wiki/Architecture-Overview) and most other websites use a database, most application software as well.
Just run MySQL locally on your own computer.
I'm also trying to build this habit but it's tough for the reason you mentioned: years of using trailing commas everywhere *not* at work. However, I'm always glad I used leading commas when I make changes to queries or functions and that provides some nice positive reinforcement that I'm hoping will solidify the habit over time. Working with SQL that has leading commas is definitely easier.
Thanks for the response! The 7:00am on 4/21 date comes into play because the order was in open status at 4/20/2017 15:16 and the activitydate at that time was 4/21. So both criteria matched. At 4/21/2017 7:00am that was between the OpenOrder date range of 4/20/2017 15:16 and 4/21/2017 16:06 AND the lookup date 4/21/2017 matched the activity at that time from the Order Date table. See Below: Date | Time | Open | ActividtyDate | Desired Result :-- | --: | --: | --: | --: 04/20/2017 | 07:00 | 0 | 04/20/2017 | 0 04/20/2017 | 08:00 | 0 | 04/20/2017 | 0 04/20/2017 | 09:00 | 0 | 04/20/2017 | 0 04/20/2017 | 10:00 | 0 | 04/20/2017 | 0 04/20/2017 | 11:00 | 0 | 04/20/2017 | 0 04/20/2017 | 12:00 | 0 | 04/20/2017 | 0 04/20/2017 | 13:00 | 0 | 04/20/2017 | 0 04/20/2017 | 14:00 | 0 | 04/21/2017 | 0 04/20/2017 | 15:00 | 0 | 04/21/2017 | 0 04/20/2017 | 16:00 | 1 | 04/21/2017 | 0 04/20/2017 | 17:00 | 1 | 04/21/2017 | 0 04/20/2017 | 18:00 | 1 | 04/21/2017 | 0 04/21/2017 | 07:00 | 1 | 04/21/2017 | 1 04/21/2017 | 08:00 | 1 | 04/21/2017 | 1 04/21/2017 | 09:00 | 1 | 04/21/2017 | 1 04/21/2017 | 10:00 | 1 | 04/21/2017 | 1 04/21/2017 | 11:00 | 1 | 04/21/2017 | 1 04/21/2017 | 12:00 | 1 | 04/21/2017 | 1 04/21/2017 | 13:00 | 1 | 04/21/2017 | 1 04/21/2017 | 14:00 | 1 | 04/24/2017 | 0 04/21/2017 | 15:00 | 1 | 04/24/2017 | 0 04/21/2017 | 16:00 | 1 | 04/24/2017 | 0 04/21/2017 | 17:00 | 0 | 04/24/2017 | 0 04/21/2017 | 18:00 | 0 | 04/24/2017 | 0 For the desired result the order had to fall in between the OpenOrder times and the lookup date has to match the activitydate from the OrderDate table at the specific time. 
dense rank rulez
A random certificate (especially from a free/cheap place online) isn't going to count for much in your studies or job search. Generally speaking, if you go for a certification in anything SQL-related, it should be from the vendor of the database platform you're working with (Oracle, Microsoft) and don't get it until you have a job that requires it (or you'll get something from your employer for getting it). I know a **lot** of very knowledgeable people, internationally-recognized experts, who haven't bothered with getting certified. It really just means you can pass the test and know nitpicky things/features that the vendor is trying to highlight in part for marketing purposes. It *doesn't* mean you know how to do the job.
Ha! It wasn't until I saw it in 24h time that it clicked. You're only reporting business hours. When it went 5 PM, 6 PM, 7 AM, 8 AM it just looked like consecutive hours to my brain. That's really funny. See if this logic works for your scenario. It selects the OrderDate record most recently entered after the OpenOrder, and prefers the OldOrderDate to EnteredDate date range if found. SELECT T.[Date] , T.[Time] , COUNT(O.[Order]) 'Open' FROM #OpenOrder AS O OUTER APPLY (SELECT TOP (1) * FROM #OrderDate AS od WHERE od.[Order] = O.[Order] AND od.EnteredDate &gt; O.EnteredDate ORDER BY od.EnteredDate) AS od RIGHT OUTER JOIN #Time AS T ON T.[DateTime] BETWEEN ISNULL(od.OldOrderDate,O.EnteredDate) AND ISNULL(od.EnteredDate,O.EndDate) WHERE T.[Date] BETWEEN '4-20-17' AND '4-21-17' GROUP BY T.[Date] , T.[Time] ORDER BY T.[Date] , T.[Time] ASC;
Underappreciated reply.
I had never even seen that style. It doesn't seem to be accepted by SQLite (which I assume means that it also won't work for PostgreSQL). I did find a blog post by someone who uses SQL Server who agrees with you: http://sqlblog.com/blogs/aaron_bertrand/archive/2012/01/23/bad-habits-to-kick-using-as-instead-of-for-column-aliases.aspx Like leading commas, I do see the practical use of this `=` style. But the fact that it seems not to be acceptable syntax among all major flavors of SQL is enough for me to not use it. The other issue is that a single equals sign in SQL -- unlike in the majority of today's scripting languages -- is the operator for equality comparison. A little off-topic: as someone who studied the usual CS curriculum of C/C++/Java/JS, using equals sign as the assignment operator is now habit. But in my time teaching programming to beginners, I've come to appreciate how goddamn confusing that is to the most reasonable non-programmer who has spent years seeing the `=` used in mathematical equations, e.g. it's hard to explain on an intellectual level why `=` is assignment and, unlike a math equation, why it goes in one direction. I've almost considered just teaching R which has `&lt;-` for assignment. It's kind of nice that SQL uses the single equals sign for equality, so I think using it for assignment/aliasing in the `SELECT` clause adds unnecessary cognitive load, even if it does allow for nicer alignment of derived columns, e.g. SELECT year , country , total = SUM(apples_sold) + SUM(oranges_sold) FROM mytable GROUP BY year , country; 
Join the table to itself on the day of the week equal to the previous day of the week. I.e., SELECT Today.Cars, Today.Date, Yesterday.Date, Yesterday.Cars FROM Table AS Today INNER JOIN Table AS Yesterday ON (Today.Date = Yesterday.Date)
Hi there, thank you for your reply. I should have precised that I have other factors that I need to take into account, for example the location of the store, the salesman,... Can I add them after the 'on'? Like today.location=yesterday.location,... ? Because when I try in my sql statement it seems to give random numbers, with no real patterns.
When you say plsql do you really mean PL/SQL (the procedural part) or you mean Oracle flavoured SQL? In PL/SQL i would try this: create or replace function isdate(p_text in varchar2 ,p_date_format varchar2 default 'YYYY-MM-DD' ,p_time_format varchar2 default 'HH24:MI:SS' ) return integer is v_date date; begin begin v_date := to_date(p_text,p_date_format||' '||p_time_format); exception when others then begin v_date := to_date(p_text,p_date_format); exception when others then return 0; end; end; return 1; end; / select isdate('bla') "isdate", 'not a date' "remark" from dual union all select isdate('2017-02-01') "isdate", 'date' "remark" from dual union all select isdate('2017-02-01 13:13:13') "isdate", 'date' "remark" from dual ; isdate remark ---------- ---------- 0 not a date 1 date 1 date 
Yes, if you have other join criteria, you should include them in the ON clause. It would help if you can post your table definition.
Thank you for your answer. I meant oracle sql.
You should have a look at analytic functions. Here is a nice overview: http://www.orafaq.com/node/55 You can use "lag" to access the previous row. I am not sure how you want to handle gaps in your data.
It worked! thank you :)
Thanks! I'll have a look
Can ID2 and ID3 be null in Both tables?
Yes
 Table A: ID1 Int, ID2 Int, ID3 Int, Blob Varbinary Table B: ID1 Int, ID2 Int, ID3 Int Did I get your tables right? Your wording is confusing so I guessed what you wanted. Select Blob From TableA a Left Join TableB b ON a.ID2 = b.ID2 AND a.ID3 = ISNULL(b.ID3,a.ID3)
That's exactly it. I was struggling with syntax
Ok you can left join on multiple conditions, but as I am guessing you have encountered, null = null creates a disastrous results. So typically you can write LEFT JOIN table2 ON a.x = b.x and a.y = b.y and a.z = b.z But I would probably write this as: SELECT binary FROM Table1 a LEFT JOIN Table2 b ON a.ID1 = b.ID1 AND (a.ID2 IS NULL OR a.ID2 = b.ID2) AND (a.ID3 IS NULL OR a.ID3 = b.ID3) Those parenthesis are important in this case.. make sure you dont remove those
One of three ways: select * into #table from query select * from #table where new_query Or: with query as ( select * from table ) select * from query where new_query Or: select * from ( select * from table ) a where new_query
The issue is that ID2 and ID3 can be null. In both and it's not a disqualifying event. 
After testing, neither of the proposed solutions work. Went with a concatenated key option and that seemed to work. Was just hoping for a more elegant solution. 
If you are on 12c you can even put the function in a with-clause/CTE: with function isdate2(p_text in varchar2 ,p_date_format varchar2 default 'fxYYYY-MM-DD' ,p_time_format varchar2 default 'fxHH24:MI:SS' ) return integer is v_date date; begin begin v_date := to_date(p_text,p_date_format||' '||p_time_format); exception when others then begin v_date := to_date(p_text,p_date_format); exception when others then return 0; end; end; return 1; end; select isdate2('bla') "isdate", 'not a date' "remark" from dual union all select isdate2('2017-02-01') "isdate", 'date' "remark" from dual union all select isdate2('2017-02-01 13:13:13') "isdate", 'date' "remark" from dual 
You really really want to avoid using OR in a join. It can go really bad. A trick is to do something like this: select * , case when condition1 is not null then condition1 when condition2 is not null then condition2 when condition3 is not null then condition3 else condition4 end as desired_results from table left join condition1 left join condition2 left join condition3 left join condition4 Or better still for performance you can put the query in a #table and then perform the case on that.
I was thinking a temp table would work. 
Once when I was getting started I wrote a fairly complex process for attribution modeling that took a bunch of columns and unioned the results into an ID column. I then tried to join this metacolumn to multiple other columns, such as ON A.ID = B.Email OR A.ID = B.Phone OR A.ID = B.Phone2 OR A.ID = B.Phone3 I was using a modest sized data source but it took about 19+ hrs to run the first time, and upon running it a second time it crashed the server. A guy who I worked with had a Masters in Mathematics and worked out the numbers. According to him the logic was creating a possible data set that was ~12.5 *trillion* rows. We broke it down into two steps which used multiple joins and then a case to merge the multiple columns. It ran in less than a minute. TLDR: Don't use OR's unless you have to.
If your table is small, that should be alright, but there can be performance issues if you are dealing with a large amount of rows. What about the following option: Select Blob From TableA a Left Join TableB b ON ISNULL(a.ID2,0) = ISNULL(b.ID2,0) AND ISNULL(a.ID3,0) = ISNULL(b.ID3,0)
Common Table Expressions (option 2) are the best approach for readability over option 3; however, temp tables have a place too if you want to index the results or will repeatedly use the table.
In terms of performance option 1 will usually be far superior than CTE's, or any other type of sub-query. This goes twice for more complex queries that have multiple steps.
Not mentioned so far: you can use views or materialized views. Views are basically just named queries; they're fully dynamic in that their results are not stored on disks; because all rows will be retrieved live, they are always current (within the limits of the current select statement). Materialized views can be defined similar to (dynamic, ordinary) views; they are basically tables that are defined by a query. Because storage-wise materialized views are just tables, they can be just as fast as querying from a table; OTOH, when the data sources mentioned in the select statement has changed, you have to issue a `refresh materialized view x` explicitly (manually or using a trigger). For this reason, the data in materialized views may have become stale. Personally I tend to write quite a few views when starting out: they are quite flexible and as long as the data set is not too large and the queries are not too complicated, they can still be sufficiently performant without sacrificing flexibility. (NB the above was written with PostgreSQL 9.5 and up in mind; most other RDBs should provide similar facilities.) So... when re-reading your question it occurs to me I didn't answer it... OK so one way to go is to create a (mat.) view that pulls together all the data from the different sources you have; when you have that 'grand view' on all the data, you can then select something specific in a single, short and simple (!) query (IOW it's probably better to leave the *complicated* queries in the DB, and only use simple `select * from v where v.x = 42`-style queries in your application). Most of the time you'll want to do joins. Absolutely go educate yourself on joins (there was a good intro on CodingHorrors some time ago. As a first approximation, you can then do this: ``` create view orders_and_parts as ( select * from db1.foo.orders as o left join db2.bar.parts as p on ( o.part_id = p.part_id ) ); ``` This gives you a view with *all* the orders, and for each order's `part_id` field, you'll get all the fields from the matching row in the `parts` table (your query will be more involved b/c normally you don't have one `part_id` per order, but you get the idea). Then, you can query ``` select * from orders_and_parts where order_id = '2017-1234-A'; ``` (In general, always use the `*` star syntax to peek inside, then pick what you need and explicitly name those choices. 'Never' use star syntax in production code.) Hope that helps. 
I'll try it tommorow. Thanks!
Certainly valid points. Depends on who is going to be looking at your code, too. I've been using it for 10 years and never had anyone complain, in fact most people seem to like it. And of course I don't have any non-programmers looking at my SQL either. You're right that I'm pretty sure it only works in SQL Server, which I strongly prefer to the other SQL-based databases I've used.
I'm currently using Vertabelo to learn about SQL. It has been really helpful in learning how to join tables, build queries, etc. 
Udemy sometimes have a few sales on courses. Got oracle sql and postgre for 10$ each. 
if you do find please link it here. The only thing i was able to find was this - https://www.coursera.org/learn/analytics-mysql - duke univ partner up with coursera
They're vastly different. You could argue that Access isn't even a "real" rdbms: there is no Access "server", simply a file that is sitting on an ordinary file system, with rudimentary locking handled by the clients writing their own locks to a separate file. Because of this it's comparatively inefficient: the filesystem isn't reading the SQL submitted and sending you back the data you need for your query, rather you're pulling mass amounts of data back to the client so the SQL can be applied locally. All of this adds up to a system that performs poorly across a network, with large volumes of data, or with multiple concurrent users. It's also quite vulnerable to various forms of corruption because it's relying entirely on the clients to make sure the data stays consistent, with no server side transaction control it isn't very ACID if the client has a flaky connection or is prone to crashes etc. What Access does have though is a simple way of creating user interface forms and reports, and if you're good with VBA you can make small apps entirely within Access. The sweet spot for Access is small projects, on a local area network, that don't require integration with other systems, using a small amount of data with very few users, where absolute durability of data can be compromised on, which are to be created/maintained by Office power users rather than developers.
Yeah the MySQL class is free for 7 days, it's $49 a month. So yeah, not free. Says it takes 8-10 hours a week for 5 weeks so if you can get it done in a month it's $49 for some certification from a great school. I'll probably do it
http://sqlzoo.net/w/index.php?title=SQL_Tutorial&amp;printable=yes
I agree 100%. My experience working as a DB Developer in the past, and now as a DBA, is that what letters you can put on your resume matter **a lot less** than the ability to solve problems and write good SQL.
Yes, but in this situation I would use a subquery [I am an Oracle person, so apologies if this is not SQL Server friendly, but I'd be surprised if you couldn't do this in SQL Server]. SELECT subquery.binary FROM ( SELECT binary as binary FROM Table1 a LEFT JOIN Table2 b ON a.ID1 = b.ID1 WHERE (a.ID2 IS NULL OR a.ID2 = b.ID2) AND (a.ID3 IS NULL OR a.ID3 = b.ID3) ) as subquery This avoids using an OR statement in the join. 
I'm not sure why you would write a subquery for this. The two queries appear to be functionally the same. 
&gt; after putting a one_to_many relationship between the two this has nothing to do with querying foreign keys simply enforce **relational integrity** before you can "see if the player is active in the player_info table" you have to know which player it is, right? all you need is a single query on the player_info table the game status table isn't needed at all for that
Since they arnt linked wouldn't they not belong in the database in the first place? or at least have a missing table between them? It makes no sense to set up a relational database for this type of data when there are so many other methods that could be used to store it. (there are rare situations where you could/should use extraneous tables but based on the information I would not call this one of them) Anyway, in all 3 of these options you really have set up a relationship between the tables (you cant reference another table without a FK) but from the wording I think you mean the PK FK constraints arnt placed on the tables in the first 2 which is expensive because the columns arnt indexed. The 3rd option is the least expensive since it has the columns indexed. One note to make is one_to_one is really the same as one_to_many but many_to_many will kill your database and/or any added efficiency you where looking to gain from using a relational database
PL/SQL is a proprietary Oracle language that makes SQL more programmatic. Access is a very simple database included with some MS Office suites. The scale of projects used for Oracle vs Access are very different and because of that they are usually not considered for the same project. Can you give a bit of background on what you and possibly your team will be building?
I was assuming this was a part of a larger project or larger query. If this is the end result, you're right - the subquery isn't really needed. Putting it in a subquery lets it be a more compact unit that you can reuse in other queries. If you wanted to join in other tables, you could join them to the subquery.
So it has no influence on select queries, then the whole purpose of foreign keys is to avoid duplicates upon creation?
Hmm, is this the reason why people choose to have 30 tables with only 3-4 of them having foreign keys? I am used to using an ORM for dealing with the sql, so I thought it made sense to use relational tables, but you're saying it's better if we don't, because it makes things slower? Also, instead of having one_to_many or one_to_one, you can choose to index your table keys to have the same advantages of faster queries?
You could get a SAS certification. SAS is widely used, is well-recognized, their "proc sql" is really a wrapper for SQL, and the exam costs $180. Sometimes there are discounts. You can study from reading material online for free. Let me know if you want to talk more about it.
&gt;is this the reason why people choose to have 30 tables with only 3-4 of them having foreign keys? The tables without relationships are look up tables. Sometimes used by the application connecting to it or the database itself. &gt;but you're saying it's better if we don't, because it makes things slower Really depends on your data. If your storing enough data to excuse a database then you could probably break your 2 tables into multiple tables along with some intersection tables (I am unsure of your knowledge of database normalization so I will provide [this](https://en.wikipedia.org/wiki/Database_normalization)) if your working with unrelational data you are better off using a non relational database (in which case your on the wrong subreddit because these databases are non-SQL) like MongoDB. Heres a [article](https://www.ignoredbydinosaurs.com/posts/210-explaining-non-relational-databases-my-mom) explaining more of that &gt;instead of having one_to_many or one_to_one, you can choose to index your table keys to have the same advantages of faster queries When you put a primary key constraint on a column, it is automatically given a clustered index and foreign key constraints automatically get a non-clustered index. basically what these constraints do is colligate other constraints that are used in PK or FK tables. I hope this better answers your question
Try to merge your interests with SQL. Like for example I'm into Hockey which could turn into a project where you predict some stat about a player. Or find some insight about how someone might perform in the future. The point is to make it personal and apply a professional approach. Start a blog.
If you use SQL and want to get a SQL job, put it on your resume and apply for SQL jobs. 
I guess I ment what does that look like, is it just queries?
I was a business analyst and wanted a DBA job. I made enough bullets on the resume and put 'Junior DBA' as my former title. A friend reviewed it and said don't bother with the Jr. So I took it off and applied for Jr positions. I was able to get a few interviews and with practice (interviewing) was able to get through one with the experience I had. Pro tip: Those moments after a conversation or argument where your brain clicks and says oh shit! I should said X! Write that down in your interview notes and review it before your next one. Say X. Repeat until you're a SQL Developer! 
Yeah I've done that much and some
Sharding seems like the best word to research what you described. Others might be database distribution or replication. 
Propose a question about something that really interests you. Movies, games, music, money. Whatever. Then attempt to answer those questions with data. Build a database. Dimensionalize it. Organize it. Apply requirements to it. Visualize it. For example, I watch a lot of movies. How many films do I watch of a specific genre? How many hours on average do I spend a week watching movies? What are my favorite genres? Who are my most common actors/actresses? If I am in February, can I predict the amount of hours I will spend watching movies to the end of the year? Show the process of shepherding the data through the process. How did you construct your dataset? How did you model your dataset? What normalization techniques did you use? Do you handle any slowly changing dimensions? How do you visualize the data to answer your questions. If someone wanted to know all about your hobby, how would you explain it to them using data? For me, if I were a hiring manager, I want to know that you can think large scale about the data. Not just write complex queries to solve a single problem. I want to know you are organizing your data to answer a multitude of questions.
Thanks. 
Oh, I didn't see a ping for this :) I'm reworking my numbers and findings - please stay tuned! (I'll announce on https://medium.com/@hoffa or https://twitter.com/felipehoffa)
I was unable to get a job for a while, spent that time working on 3 certifications, got the job immediately after. The Investments firm I joined didn't know much about the certs, but they did like the fact that I did the certs and increased my knowledge. It demonstrated my aptitude to learn. None of the other candidates had them.
Sharding most likely, but in Azure sometimes also called [geo-replication](https://azure.microsoft.com/nl-nl/blog/azure-sql-database-now-supports-powerful-geo-replication-features-on-all-service-tiers/) depending on the details.
Option 1 will not always be superior to CTE's. Option 1 will always have to be written to disk in TempDB which is slower than using the results straight from the bufferpool like with a CTE. A couple of weeks ago I changed a query at my workplace from using a temp table to using a CTE, because it was taking up to a minute to fill these 2 temp tables. The query then took only 8 seconds instead 70 seconds total and ended up using less memory. Best way here (what no one has mentioned?) is to just build a join between these 2 tables.
Why do you prefer PL/SQL over Access? I think that answers halve your question.
&gt; the whole purpose of foreign keys is to avoid duplicates upon creation? no, that's what UNIQUE constraints do FOREIGN KEY constraints are different
I have two suggestions. Firstly, is it realistic for you to add another skill to your toolkit, such as web development? If so, I'd recommend learning a nice readable programming language such as Python and building some database-driven web applications you can share or blog about. You needn't worry about doing any front end "look and feel" stuff (unless that appeals to you), and you'll show that you can pick up new skills. An alternative is to think of a current affairs question you can answer with data. Ideally nothing too controversial. Then you can write a blog post about sourcing the data, building a database with it and querying the results. Good luck!
I would first alias your tables and use the aliases in your references for readability and other purposes. Second, you could be making this an inner join by adding those where clauses. If you need distinct Meeting_Description values you will probably need to exclude the time, room, etc. from your query and use DISTINCT since DISTINCT will give you unique rows and those time/date stamps are probably unique.
Yes, Foreign Key makes it so you can't have records in the game table without a player, because how could a player, who does not exist, play a game?
But what are you actually trying to do here? Sample data, table structure and results would be extremely helpful.
Ok, forgot about that.
Ok, thanks.
Don't know what you are trying to do, it sounds like you are getting the correct data but you are asking for the wrong information. Can you post the column names instead of a wildcard? 
this query is messed up as it stands see /u/Cal1gula's comment about using aliases in your references "for readability and other purposes" in this particular case, the "other purposes" apply --- we cannot tell which table your colulmns belong top, consequently we cannot fix the query properly please identify where each column in your query comes from 
as soon as you identify your columns, sure... please see my other comment
Dude... what *are* they? Like open up the view and tell us... lol. We can't see your screen or read your mind.
&gt; The columns are coming from the view; VMeetingRooms if that's true, then you do not need a join at all obviously, you're joining to `special_code` for a reason... what is it?
With any type of large data set, or any type of complex operation that involves multiple sub-queries... Option 1 will almost 100% of the time outperform a CTE.
Join a SQL User Group and ask around
Thanks for taking the time to explain it so thoroughly and the Coding Horror mention. I'm pretty new to SQL and appreciate all the help I can get!
In case you missed it - SQL Fiddle is working much better now. Worth giving it another look.
How do you currently use SQL. What do you query? Do you build reports? Create stored procedures? Let us get an idea on what you know so we can point you in the right direction.
I had read about it being fixed up but have not tried it. Thank you. 
Easiest way for me to describe APPLY: It's a JOIN except it will work with a function output (a JOIN won't). Here's a short example from StackOverflow: (myTableFun is a TVF) Example(will return an error): select F.* from sys.objects O inner join dbo.myTableFun(O.name) F on F.schema_id= O.schema_id This is a syntax error, because, when used with inner join, table functions can only take variables or constants as parameters. (I.e., the table function parameter cannot depend on another table's column.) This is legal: select F.* from sys.objects O cross apply dbo.myTableFun(O.name) F where F.schema_id= O.schema_id The function will be executed once per row and then joined to the results.
For each row on the left side, call a table-valued function on the right side (a subquery, usually correlated, can be used as a table-valued function) and cross join the result to the row on the left side of the 'apply'. 'Outer apply' keeps the row from the left side even if the table-valued function (the right side) returned 0 rows. Say your table (T) has 2 rows with values 'A' and 'B'. Your table function F returns 3 rows with values for A (F1, F2 and F3) and no rows for B. so T cross apply F(T.row) will give you AF1 AF2 AF3 on the other hand, T outer apply F(T.row) will give you one extra row: AF1 AF2 AF3 B[NULL] 
Do you mean Alias name? I prefer a short, meaningful abbreviation. Someone on this forum tried to convince me the a,b,c,d way was the best and it was absurd. He had to go through and comment every query to denote which letter applied to which query in what way and it seemed awful. Anyone who has to support the code (including your future self that has to re-learn the entire query) will hate you. I like Aaron Bertrand's blog: http://sqlblog.com/blogs/aaron_bertrand/archive/2009/10/08/bad-habits-to-kick-using-table-aliases-like-a-b-c-or-t1-t2-t3.aspx In your case, I would use Cust, Con, Add, Svc.
I generally use one or two letters derived from the table name. C - Customer table A - Address S - Sales... So it's a combination of the two. 
Oh, I made it a point to never use a, b, c, ... I always use something that makes sense, when reading the SQL. I was just curious how common of an occurrence this is.
This is exclusively how I work, and I really dislike code which tries to abbreviate tables (e.g. dbo.TotalSales AS tsal instead of AS A) I think it is a result of my programming background, but (especially when you work with multiple recursive sub-queries) it just doesn't make sense to me to name them an abbreviation, whereas I can imply meaning by naming them such as A, B, C (one then two then three), or A1, A2, B, C1, C2 (A, product of A, B, C, product of C). The only time I am more descriptive is when I use a CTE such as with data as ( ), total_sales as ( ) select * from data a join total_sales b Luckily my first SQL job also had this as a best practice. So it wasn't an issue.
&gt; Someone on this forum tried to convince me the a,b,c,d way was the best and it was absurd. totally agree... it's absurd... not to mention frustrating when you're trying to read and understand someone else's query &gt; In your case, I would use Cust, Con, Add, Svc. cus, con, add, svc -- three char each for consistency also, lowercase for identifiers, please
I honestly disagree with you. It makes it so much harder for me to understand a long complex query that goes across multiple screens when they use an abbreviation instead of a simplified alias. This goes twice if it involves multiple sub-queries which derive from one another. What are you going to do here: select * from ( ) ? join ( select * from sales_table AS stab join ( select * from ( ) ? join ( ) ? ) ? join ( ) ? ) ? join ( ) ? This becomes doubly confusing if you have a variety of tables that all have similar names, e.g., sales_history, total_sales, model_sales, model_total_sales, etc. All of your abbreviations end up nearly the same, so it becomes amazingly frustrating to start trying to diagram it if you're looking for ways to add new dimensions, joins, remove duplicates, etc. On the other hand if I name them all arbitrary values then there is no possibility of confusing one for the other. A is A, B is B... data comes in at B1 and is deduped by a join on B2, this forms X which is part of a three step join such as X, Y, Z, etc. Just for me anyway it feels so much easier to rip someone else's work apart by rewriting their query like this. It's the first step I take. edit: I think of it like this. If I sit down to write an equation, which is what a query is to me, then I don't say `stab =` and work forward, but I do say `x =` and then comment what x represents.
I can totally see your point. Especially when it comes to handling multiple nested queries, that have the same base joins. The way you described it has changed my opinion a bit (but not all the way :P).
&gt; What are you going to do here: did i say 3-char everywhere? no, i did not -- just for that one example for this case, with all those subqueries, i'd name them something more relevant, and 3 characters is not going to do it if you would care to fully expand your sql, i'll give it a shot
people pick between 1-3 letters and make them ref the table names, if it's just sales and customers i'll use C and S, but if it's shops and ships ill use shi and sho... you get the idea.
It just becomes too cluttered with similar but different things.
Not trying to, but it isn't absurd and I genuinely prefer working on code formatted that way.
Nope, those aliases are not meaningful at all. I mean, nothing there is unreadable and should be able to be deciphered by anyone familiar with writing sql queries, but I prefer making it meaningful to the tables I'm referring to. E.g: Response_Master_Incident = RMI, ResponseEdit = REd, Problem = Pro, Priority = Pri 2 extra letters, sure. But, just like commenting, extra care makes good re-usable code. 
We'll still have to disagree on this. "a" has no meaning at all except that it is a letter in the alphabet. "sls" implies a meaning of "sales" and the underlying data. You can still do sls1 as the product of sales in the same method. Except you know it's a product of sales just by looking at the abbreviation. Whereas if it's a1 you have to go to a comment or read the entire query to figure that out. There is no way the latter more efficient or easier to support.
A has meaning if it precedes B.
I mostly use it to pull data and export it to excel. The company that I work for calls alumni for colleges asking for donations, I query anything from that. I also use it to update fields. They have there own in-house developed reporting system, it does use sql tho. We still load the data into the databases with control and batch files. Let me know if I need to be more specific
shit, i never thought of that!!! if hadn't already spent all my pension this month on crackers and cat food, i'd gold that comment
Check the types of all the columns, especially the dates. Are they the same type? Datediff needs them to be the same type/format. Next thing I notice is that part of your WHERE clause might need to be in Parenthesis so it evaluates in the right order. I would also check to ensure that rowid is an int or another value that can have evaluations performed on it. WHERE t2.rowid = (rowid -1) I would suggest breaking out the select statement in this update to try and isolate what's going wrong. SELECT datediff(ss,t.EndTime, t2.StartTime) FROM #ttCorrectionTimes t INNER JOIN #ttCorrectionTimes t2 on (t.rowid -1) = t2.rowid
on my phone. ill provide a clear example later of what I mean. I dont just cycle letters, A B C are a set to form X, D E F may form Y, and Z may form from A1 C1 and G. All of that may then form U which then joins to A B and H. So you can absolutely discern meaning. Stop being an asshole because someone disagrees with you.
So what kind of jobs have you interviewed for? How did you feel at the interviews? Were you asked questions you had trouble answering? If you're getting interviews but not offers I'd focus on how you interview. Look for gaps in experience or knowledge you can identify from the interviews and fill them. Also learn to show how you can bring your experience into the role even if not it's not directly related. For instance if you've automated things at work or solved some problems on your own initiative those are things you can apply to many roles. 
&gt; Stop being an asshole because someone disagrees with you. reported
Have you looked at the LAG and LEAD functions? Then you wouldn't need a self join. https://docs.microsoft.com/en-us/sql/t-sql/functions/lag-transact-sql 
Do it twice.
I'd rephrase that to "statements", not "transactions" 
You're correct. The word statements wasn't coming to mind for some reason. Thanks! 
I'll say that when righting subqueries adhoc joined against same table, sometimes I see a, b, c used...but in stored procs, I find actively unhelpful.
You can DENY users privileges to objects. Like deny selecting from certain tables 
Can you deny users from specific actions? Say for instance there's an application using a database. The database should only be able to access exact data it's requesting for, with error validation to prevent a user from access all information. For increased protection can you also have such protection on the database level? 
What your describing is a huge benefit of using views. Create a view that only displays exact what the user should see, then give the user permission to that view. 
Idk much about plesk or sequel pro, but your hostname might also be an IP address. I can't see much from your post, but with most clients typically you will need to provide a hostname/IP address port Database name Username Password If your database is remote, where is it? Is it a work db? Contact an admin for connect info. Did you set up a database in the cloud somewhere? Check your cloud configuration. Sometimes it can be really helpful to just start by listing information that you know, and it helps you discover the gaps of things you still need. 
I manage an active/passive cluster hosting a number of DBs that's an order of magnitude larger, but about 25% as much data. I wouldn't be concerned with the memory usage *unless* you're constrained enough that you're under memory pressure and have a lot of thrashing - pages getting dumped out of the buffer pool quickly because more data needs to be read. SQL Server will take every byte you give it (well, up to a point, but you have enough data that you haven't hit that point) and that's a good thing - data pages have to be in memory to be operated on, so the longer you can keep a page in memory, the less time you'll spend waiting to read it in from disk. Find specific areas of concern and addressing them. Are your backups and `checkdb`s completing in an adequate amount of time? Are you able to meet your RPO and RTO objectives? Do you have a baseline that you can use in tracking your rate of growth for capacity planning? Any duplicate indexes using up extra space? How do your users &amp; applications access the data? Stored procs? Ad-hoc queries? Prepared Statements? What do your wait stats look like? IOW, what (if anything) is bogging your system down? Have you done any tuning of your configuration, including changing [some of the outdated defaults](https://www.brentozar.com/archive/2013/09/five-sql-server-settings-to-change/)? Why do you have so many databases? Is this growth intentional, or are people creating databases "because they can?" Is this cluster the company "junk drawer" where **everything** goes into it, instead of putting databases on different instances/clusters based upon their usage? Or are all of these databases for one application/department? Could you move some of the databases to another instance/cluster altogether? Do you have a decision-making process for managing this growth? Can you archive data to another server (or even another database on the same cluster, maybe on a lower tier of storage)? Are you partitioning your tables (could make moving that data around easier)? Have you done an evaluation of your environment with Brent Ozar Unlimited's [`sp_blitz*`](http://firstresponderkit.org/) scripts (do **not** run `sp_blitz` with `@bringthepain=1`, you will regret it) to find hotspots (including wait stats), pain-inducing queries in your plan cache, etc.? What, if any, monitoring tools do you have in place?
Security. Restrict access to stored procedures with parameter filters
Let's work our way into CROSS APPLY and OUTER APPLY. First we have joins. `INNER JOIN CoolTable AS c ON c.CoolID = a.CoolID` Joins can return multiple rows and multiple columns. Joins can only have outside references as part of the join condition. Second we have in-line queries and scalar functions. `SELECT a.CoolID, (SELECT TOP(1) c.CoolName FROM CoolTable c WHERE c.CoolID = a.CoolID) AS CoolName` or `SELECT a.CoolID, CoolFunction(a.CoolID) AS CoolResult` These can only return a single row and a single value, but can have outside references inside them. Third we have CROSS and OUTER APPLY. These can return multiple rows and multiple values, and can use external references in their logic. They're like a hybrid of the other concepts. My favorite uses are joining to tables with effective dates, and custom UNPIVOTs. -- Effective Dates SELECT o.OrderID, c.CampaignName FROM [Order] AS o OUTER APPLY (SELECT TOP(1) c.CampaignName FROM Campaign AS c WHERE c.CampaignCode = mt.CampaignCode AND c.EndDate &gt;= o.OrderDate ORDER BY c.EndDate) AS c; -- Custom unpivot SELECT st.StatType, st.StatTime, st.RowsProcessed FROM JobStats AS js CROSS APPLY (SELECT 'Start' AS StatType , js.StartTime AS StatTime , 0 AS RowsProcessed UNION ALL SELECT 'Step1' AS StatType , js.Step1Time AS StatTime , js.Step1Rows AS RowsProcessed UNION ALL SELECT 'Step2' AS StatType , js.Step1Time AS StatTime , js.Step1Rows AS RowsProcessed UNION ALL SELECT 'Step3' AS StatType , js.Step3Time AS StatTime , js.Step3Rows AS RowsProcessed UNION ALL SELECT 'End' AS StatType , js.EndTime AS StatTime , js.FinalRows AS RowsProcessed) AS st; 
Similarly, I try to use the first three consonants of the table name, unless it's a reserved word. Examples above would be CST for Customer, CNT for Contract, ADR for Address, SRV for Service.
Are there any indexes on the tables? Possibly the additional columns result a useful index to not be used.
Wow thanks for your reply ☺️ I don't know which IP address it is. Is it the one for my server or for my laptop? I tried looking for a way to make my laptop an access host but couldn't work out how to do that. I'll have another go today. Thanks again!
I'm not talking about correlated subqueries or derived tables here, talking about CTE's. If you write your joins as nested loops, you're going to get bad performance on large data sets.
And the SQL engine will give no f at all in which order your write it and will join your tables whichever way is fastest. Have fun reading your execution plans.
&gt;RAM usage is always peaked (I've limited SQL to use 90% max RAM usage). That still leaves ~26GB for the OS? If you're not running anything else, you could easily leave only 6 or 8GB for the OS. Aside from sp_blitz, take a look at your [wait statistics](https://www.sqlskills.com/blogs/paul/wait-statistics-or-please-tell-me-where-it-hurts/). 
Lag function, where have you been all my life! Thank you /u/yooter! 
As in "Select *" is prohibited but "select top 1000 *" not? You could maybe do that via pools &amp; the Resource Governer in SQL Server. I'm not sure if Oracle has an equivelent, but it probably costs extra licensing (har har).
If I'm writing production code, I'll use readable aliases. This can be anything from an abbreviation to a full word, depending on how unique the table / view / type is. If I'm writing a quick and dirty query that'll be used once and not likely ever again, I'll use a, b, c etc.
Don't use single quotes for aliases, use double quotes or brackets []. You or someone else will get in trouble with [QUOTED_IDENTIFIERS](https://docs.microsoft.com/en-us/sql/odbc/reference/develop-app/quoted-identifiers) later on. If you want to use sum or count, you have to use [GROUP BY](https://docs.microsoft.com/en-us/sql/t-sql/queries/select-group-by-transact-sql) for all the other columns. If you don't want to do that, you can use a [windowed function](https://docs.microsoft.com/en-us/sql/t-sql/queries/select-over-clause-transact-sql), but you'll have to specify what you want to partition over.
What you want is a window function. Essentially, a window function steps out of the current window of your query to recompute some column operation in a new window that you specify. For instance, let's say I have a table orders with columns amount, salesperson, date and I want to find the average amount per salesperson per day, but also the total amount per salesperson for each row. I could write: SELECT sum("amount") as "daily_amount", sum("amount") OVER (PARTITION BY "salesperson") as "total_amount", "salesperson", "date", FROM orders GROUP BY "salesperson", "date" The SUM window function there steps out of my groupby salesperson and date to partition by salesperson only, then compute the sum of each salesperson's orders. This is what you're looking for. Alternatively, I suppose you could calculate the sum in a subquery then join it back in, but this seems more convoluted. I'm most familiar with postgres but MS SQL has window functions as well. Window functions are of the form FUNCTION() OVER([PARTITION BY] &lt;grouping columns&gt; [ORDER BY] &lt;ordering columns&gt; &lt;asc, desc&gt; [ROWS BETWEEN &lt;frame&gt;]) in postgres. Look up the MS SQL documentation to see the syntax there.
superb analysis... really well done... this is genius -- SELECT count(*), count(length), count(language_id), count(rating), count(length + language_id), count(length + rating), count(language_id + rating), count(length + language_id + rating) FROM ( SELECT CASE WHEN length BETWEEN 120 AND 150 THEN 1 END length, CASE WHEN language_id = 1 THEN 1 END language_id, CASE WHEN rating = 'PG' THEN 1 END rating FROM film ) film
I think your customer information is probably in another table not another database. A simple join should do the job.
remove datetime from SELECT and GROUP BY clauses
Thank you very much, this worked. Now I have to get it working in HTML. 
Thank you very much, this worked. Now I have to get it working in HTML. 
Thank you, this worked. Now I get to figure out how to do this in HTML as well.
yeah, well this returns the total jobs (count). now it's how to filter them with dates (aka, if there was more that 1 job in that date), it should be one only. so, now - how do i get only days_active (now i'm getting job count, but if there was 1+ jobs per day, it should count as one).
GROUP BY will give you the records with the same datetime value. So if they are all unique, GROUP BY effectively does nothing at all. Maybe you are looking to GROUP BY day only? You can do that with `GROUP BY DATEPART(dd, datetime)`. And maybe you also want to include only those jobs with count &gt;1 so you would also add `HAVING COUNT(user_id) &gt; 1` perhaps?
temp tables?? whatever for?? can't you just do `INSERT INTO subcommittee SELECT ... FROM employee`? we might be able to **edit** (forgot to finish my sentence) we might be able to help more if we could see the table layouts 
You can make a key out of 2 columns. https://stackoverflow.com/questions/3474291/unique-constraint-on-multiple-columns Where is Subc Code coming from? As /u/r3pr0b8 said, you can probably just do a join here and insert into the table. No multiple queries + temp tables seem to be needed.
which platform are you using? you forgot to mention date functions vary ~wildly~ from one to the next you want to extract the date from the datetime column, and then use `COUNT(DISTINCT date)`
&gt; You can do that with GROUP BY DATEPART(dd, datetime) that won't work correctly because June 27th's data will get counted along with July 27th's, etc. you were probably thinking of this -- SELECT DATEADD(dd, 0, DATEDIFF(dd, 0, mydate)) assuming this is SQL Server, which is risky bidness in /r/SQL
You're right, could also just cast as date instead of datetime. Have to make some assumptions about the engine when the user provides none!
you're grouping by the email before replacement try this -- SELECT REPLACE([emailaddr],'@email1','@email2') AS duplicateemail , COUNT(*) AS amount FROM users WHERE emailaddr LIKE '%@email1' OR emailaddr LIKE '%@email2' GROUP BY duplicateemail ORDER BY amount DESC
Sorry, so employees can be in more than one SubC, there needs to be a line for each SubC that the employee is in without duplicate rows. I'm self taught so I find that I have problems asking questions with the right verbiage and description. 
now I get "invalid column name 'duplicateemail'. select replace([emailaddr],'@email1','@email2')as duplicateemail, Count(*) as amount from users where emailaddr like '%@email1' or emailaddr like '%@email2' group by duplicateemail order by amount desc SQL is indicating the problem is the group by duplicateemail Thank you for your help!
First, you need a list that's user_id and date, not user_id and datetime. Then, you can group by user and count of date and you'll have the number of days that a given person had at least one job. If you want to do that with a single SELECT statement, you'll have to select from a select something like: SELECT User_ID , COUNT( Unq_Day) FROM ( SELECT User_Id , &lt; appropirate function for your dbms to return day given datetime&gt; FROM jobs_new -- add WHERE datetime range to filter down GROUP BY User_ID , &lt; appropirate function for your dbms to return day given datetime&gt; ) GROUP BY User_ID 
If I recall its because MSSQL doesn't let you group by aliased columns. You would have to change it to `GROUP BY REPLACE([emailaddr],'@email1','@email2')`
Happy to help! I would have written out an example, but I mainly comment on mobile.. I don't like writing SQL with thumbs haha.
No need. I looked it up and put it in place. Works like a charm.
&gt; there needs to be a line for each SubC that the employee is in without duplicate rows. yes, that's how you do it sorry, what was your question?
thanks /u/ramse, i forgot about that... and offhand, can't recall if a column alias is allowed in the ORDER BY, but i suspect it is sometimes expressions can get really messy, so copying them into the GROUP BY clause gets messy too you could, as an alternative, push the messiness down one level of subquery -- SELECT duplicateemail , COUNT(*) AS amount FROM ( SELECT REPLACE([emailaddr],'@email1','@email2') AS duplicateemail FROM users WHERE emailaddr LIKE '%@email1' OR emailaddr LIKE '%@email2' ) AS subquery GROUP BY duplicateemail ORDER BY amount DESC
This worked perfectly! Thank you, both! Am poor but please accept this [Reddit Silver!](http://imgur.com/f0Iu0xE)
It looks like the index contains 4 out of the 6 columns, so 65% sounds about right. edit: You might consider to make data_type_id + datetime + sequence_number the PK and get rid of data_id. Then you don't need a separate index, and save by getting rid of data_id.
Indeed. I've been thinking of doing this. Those three indexes combined should together be unique. But it seems that while undesired a couple of duplicates have snuck in. And I've been trying to get rid of them in the dev table to try that out.
Like this? SELECT a.AnsweredDate , COUNT(a.AnswerID) AS DateCount , SUM(COUNT(a.AnswerID)) OVER () AS GrandTotal FROM tblAnswers AS a WHERE a.AnsweredDate&gt;=@Start_Date AND a.AnsweredDate&lt;=@End_Date GROUP BY a.AnsweredDate;
&gt; SELECT a.AnsweredDate &gt; , COUNT(a.AnswerID) AS DateCount &gt; , SUM(COUNT(a.AnswerID)) OVER () AS GrandTotal &gt; FROM tblAnswers AS a &gt; WHERE a.AnsweredDate&gt;=@Start_Date &gt; AND a.AnsweredDate&lt;=@End_Date &gt; GROUP BY a.AnsweredDate; This totally helps. I really just need one row with the Grand Total of the Sum(Count) if that's possible. Thank you for helping!!
Add With rollup To the query? 
 SELECT a.AnsweredDate , COUNT(a.AnswerID) AS DateCount FROM tblAnswers AS a WHERE a.AnsweredDate&gt;=@Start_Date AND a.AnsweredDate&lt;=@End_Date GROUP BY a.AnsweredDate WITH ROLLUP;
My brain turned off so I forget this is set logic and not procedural logic. Thanks for your help.
Yes, if you have a chain of CTE's and a large data set then a #table will almost always outperform.
You should be able to do something like: SELECT DISTINCT d.name, d.surname, d.id, d.account_id, d.mailbox_id FROM table c LEFT JOIN table d WHERE d.id = c.d_id ORDER BY d.name
Are you looking to enforce uniqueness? 
Lots of complicated answers here. If it's me in postgres I do this: SELECT User_id, Count (distinct datetime) as active_days From Table Where unit_of_work is not NULL GROUP BY user_id Assuming your datetime column doesn't have a timepart (reading your post you seem to indicate it's just a date). If it does you will need to truncate it inside the count (distinct) call
Why would you want an outer join? You want a list of players that belong to a team, if you did an outer join you would get players that might not be on a team or a team without any players, which you do not want. You'll want an inner join. You would basically select the top 10 team.name and average of player.height from team, join that on players using the proper id, group it by the team name, then order it by the average height descending. Here's a SQL Fiddle for you to try it out on: http://sqlfiddle.com/#!6/4552b
Hey there, thanks for getting back to me. Outer join was a typo, meant inner join. Would you mind showing me what the final product would look like?
Select sum(thisIsACount.count), AnsweredDate From tblAnswers tblA cross apply (select count(AnswerId) as 'Count' from tblAnswers where AnsweredDate = tblA.AnsweredDate ) thisIsACount where AnsweredDate&gt;=@Start_Date and AnsweredDate&lt;=@End_Date Group by AnsweredDate I think this should work.
Sum by what? Do you mean just sum all the counts? That's just counting them all? Like this: &gt; Select COUNT(AnswerID) FROM &lt;query&gt; Side note: in MSSQL you can use BETWEEN instead of &lt;= and &gt;= &gt; AnsweredDate&gt;=@Start_Date and AnsweredDate&lt;=@End_Date Like this (Between is inclusive) &gt; AnsweredDate BETWEEN @Start_Date AND @End_Date
http://sqlfiddle.com/#!6/4552b/4 
Hi, Thanks so much for all the help thus far – this has been really helpful. What you’ve sent is really close to what I need. I’ve tried to tweak it a little bit, but to no avail yet. There are two main criteria that need to match DateTime from Time table: First the DateTime has to fall in between the EnteredDate and EndDate from the OpenOrder Table - Then the Date from the Time table has to match the corresponding OrderDate from the OrderDate table when the Time table DateTime is between EnteredDate and EndDate. I’ve change the OrderDate table to hopefully make this easier. Here is a new example (the time table above is still the same) CREATE TABLE #OrderDate ( [Order] INT ,EnteredDate smalldatetime ,EndDate smalldatetime ,OrderDate smalldatetime ,LocNum int) INSERT INTO #OrderDate ([Order],[EnteredDate],[EndDate],[OrderDate],[LocNum]) VALUES (678910, '2017-04-18 13:58:00', '2017-04-19 08:01:00', '2017-04-20', 0), (678910, '2017-04-19 08:01:00', '2017-04-19 08:19:00', '2017-04-17', 1), (678910, '2017-04-19 08:19:00', '2017-04-20 13:49:00', '2017-04-20', 2), (678910, '2017-04-20 13:49:00', NULL, '2017-04-21', 3) CREATE TABLE #OpenOrder ( [Order] INT ,EnteredDate smalldatetime ,EndDate smalldatetime) INSERT INTO #OpenOrder ([Order],[EnteredDate],[EndDate]) VALUES (678910, '2017-04-20 10:01:00', '2017-04-20 13:40:00') Here is what the result should be for this example: Date | TIME | Open :-- | --: | --: 04/20/2017 | 7:00 AM | 0 04/20/2017 | 8:00 AM | 0 04/20/2017 | 9:00 AM | 0 04/20/2017 | 10:00 AM | 0 04/20/2017 | 11:00 AM | 1 04/20/2017 | 12:00 PM | 1 04/20/2017 | 1:00 PM | 1 04/20/2017 | 2:00 PM | 0 04/20/2017 | 3:00 PM | 0 04/20/2017 | 4:00 PM | 0 04/20/2017 | 5:00 PM | 0 04/20/2017 | 6:00 PM | 0 04/21/2017 | 7:00 AM | 0 04/21/2017 | 8:00 AM | 0 04/21/2017 | 9:00 AM | 0 04/21/2017 | 10:00 AM | 0 04/21/2017 | 11:00 AM | 0 04/21/2017 | 12:00 PM | 0 04/21/2017 | 1:00 PM | 0 04/21/2017 | 2:00 PM | 0 04/21/2017 | 3:00 PM | 0 04/21/2017 | 4:00 PM | 0 04/21/2017 | 5:00 PM | 0 04/21/2017 | 6:00 PM | 0 Any additional suggestions are greatly appreciated! 
Reported
Here's the article (and yes, you probably did get it from the Brent Ozar Unlimited newsletter): https://www.procuresql.com/blog/2017/07/five-reasons-why-log-shipping-should-be-used/
Agreed, learned something that might come in really useful. 
I think I got it now. Here's a query for the original version of OrderDate: SELECT t.[DateTime] , COUNT(oo.[Order]) AS Orders FROM #OpenOrder AS oo CROSS APPLY (SELECT MIN(x.OrderDate) AS MinOrderDate , DATEADD(MINUTE,-1,DATEADD(DAY,1,MAX(x.OrderDate))) AS MaxOrderDate FROM #OrderDate AS od CROSS APPLY (SELECT od.NewOrderDate AS OrderDate WHERE od.EnteredDate &lt;= oo.EnteredDate UNION ALL SELECT od.OldOrderDate AS OrderDate WHERE od.EnteredDate &gt;= oo.EnteredDate) AS x WHERE oo.[Order] = od.[Order]) AS od RIGHT OUTER JOIN #Time AS t ON t.[DateTime] BETWEEN CASE WHEN oo.EnteredDate &lt; od.MinOrderDate THEN od.MinOrderDate ELSE oo.EnteredDate END AND CASE WHEN oo.EndDate &gt; od.MaxOrderDate THEN od.MaxOrderDate ELSE oo.EndDate END GROUP BY t.[DateTime]; Here's a query for the new version of OrderDate. SELECT t.[DateTime] , COUNT(oo.[Order]) AS Orders FROM #OpenOrder AS oo CROSS APPLY (SELECT MIN(od.OrderDate) AS MinOrderDate , DATEADD(MINUTE,-1,DATEADD(DAY,1,MAX(od.OrderDate))) AS MaxOrderDate FROM #OrderDate AS od WHERE od.[Order] = oo.[Order] AND od.EnteredDate &lt;= oo.EndDate AND od.EndDate &gt;= oo.EnteredDate) AS od RIGHT OUTER JOIN #Time AS t ON t.[DateTime] BETWEEN CASE WHEN oo.EnteredDate &lt; od.MinOrderDate THEN od.MinOrderDate ELSE oo.EnteredDate END AND CASE WHEN oo.EndDate &gt; od.MaxOrderDate THEN od.MaxOrderDate ELSE oo.EndDate END GROUP BY t.[DateTime]; Now that I ^^*think* ^^*I* fully understand, I have some questions. What business events populates OrderDate? Why does it mater analytically to filter order inventory by OrderDates in addition to OpenOrder duration?
This is it! Thanks! 
All data in #temp table will have to be written to disk which is painfully slow compared to RAM. So it's better to use what's in your bufferpool directly with a CTE (and a hash or merge join).
Not quite sure if this works or not, but are you joining two tables in one join @ that Inner join(users and creators)? "INNER JOIN [users] [creators] ON [tickets].[created_by] = [creators].[id]" You are saying you are trying to pull data from 2 tables, but i see 3.
Usually I just do this: SELECT SUM(CASE WHEN language_id = 1 THEN 1 END) FROM Film 
okay, the SELECT clause in the query posted above contains 8 counts... how would you do each of those 8? 
Just add the SUMs. I've never had a query complex enough to justify putting the case statements in a subquery.
Looks like for the Inner Join creators is an alias of users. A bit confusing since the left users join isn't aliased. 
Have you confirmed that tickets.assignedto and users.id are the same data type?
Of course, seems right :P
You're outer joining [users], but also inner joining on another parameter. try outer joining [user][creators].
Is this article highlighting the benefits of using Log Shipping vs doing log backups as part of a Data Maintenance plan? Or are both equally beneficial?
You are joining to the same table twice, you can take all of that info (including the WHERE clause) and put it in a single JOIN. Edit - Good point /u/r3pr0b8. Two joins would be needed for two different users, I didn't look at the SELECT clause close enough. I removed my query so it wouldn't confuse OP. 
Any time I see left joins with WHERE I assume the problem is there. Confirm that rows you want meet the conditions in your WHERE clause. That could be the problem (maybe status is NULL for rows from [users] or no rows from [users] were created after your date range?). The LEFT OUTER should return to you the outer join from the [tickets] table with any matching rows from [users]. Are you sure that is the correct ID column? You then inner join on users again, but on a different ID column in tickets, so it seems unlikely that [users].[id] is the wrong ID column but you never know. This is a hard question to ask without being able to see the data since your query looks fine, other than being formatted poorly :P 
/r/badcode would love this ... Maybe it's come from the SQL Server 2000 days when [things were different](https://technet.microsoft.com/en-us/library/ms190193(v=sql.105\).aspx) Today we have RAISERROR and the newer THROW, which handle errors much better.
You can't just return @@error because @@error won't be -1. For example the divide by zero error `SELECT 1/0; SELECT @@ERROR;` is 8134. It's often useful to save @@error to another variable so you can reference it more than once, but in this case it's not necessary.
&gt; @@error won't be -1 Yes, I am aware of that. Which is why I said, this code hides the actual error reason by making everything -1 (which is a bad thing if you actually need to debug stuff). Mind, I am in control of both the consumer as well as the DB, and the consumer also just checks for != 0.
No, it just discusses the benefits of using log shipping as disaster recovery and as a means to validate tlog backups. If you're asking if tlogs backups are just as beneficial I'd say not only yes, but they are crucial to keeping your db operational. If that really is what you're asking, I'd take a look at this series: http://www.sqlservercentral.com/stairway/73776/ If not then I think I just misunderstood! 
&gt; SQL Server 2000 days very much likely, this project has been around for at least 10 years. I'll look into RAISERROR and THROW ... as is probably evident I am not an SQL person myself, but it's good enough for implementing some basic things here and there ;)
That's fine and good, but for a long process it just doesn't work out that because of how SQL tries to execute the query. With a CTE you may not get sequential execution, but with a #table I can force it to execute one step at a time. This is hugely faster than a CTE for lots of cases. Here is a simple obvious example: I have a 3000 line sproc that uses about two dozen #tables to create a data cube. I could rewrite this as a single CTE if I wanted to. Are you saying that this is going to be faster?
So the purpose of the code is apparently just there to suppress errors. Maybe someone was debugging and forgot to remove it? Is there a custom message on the front end somewhere? THROW is nice because you can make your own error messages right in line.
The error isn't suppressed. It's just overriding the return value (which is not the same as the error value).
&gt; Maybe someone was debugging and forgot to remove it? Too prevalent for that ... it really looks like some antipattern that caught on in the codebase. Guess it's a good point to create a story to fix it :D &gt; Is there a custom message on the front end somewhere? Most of that code is internal (admin tool), and just goes `if (result != 0) throw new Exception()` (without message or anything of course). I'm currently adding some new functionality and looking around, which is why I found that.
your SELECT clause references columns from the [creators] table, but your FROM clause omits to mention iy definitely needs two joins to the users table -- one for the user who created the ticket, and one for the user it's assigned to -- they aren't the same person
but the point is, we're not adding sums!!! we're counting rows where more than one condition exists in the same row dude, i think you missed what the query is supposed to do i'm wondering if you even read the article
How about a module on query optimization and best practices? That would be helpful I think. 
I've come to the conclusion that there are no optimization best practices, and that every query is a case by case basis. For every case a "best practice" improves performance, there are many scenarios that it doesn't. Optimization is just learning a hundred examples (and understanding why they worked) to have more tricks to try in the future.
Query optimization is a massive topic. I plan on a separate course dedicated to this!
True, for pretty much every SQL optimization "best practice" you can come up with a case where it makes things worse. Understanding the underlying principles is more important.
The two are equivalent, except one uaes a subquery.
i'm wondering if you even read the article and you still have not shown how you would do the 8 expressions in the SELECT clause -- with or without a subquery 
NULLs and 3-valued logic is ~not~ intermediate/advanced should be taught in SQL 101 
shit, thousands and thousands of us have ~always~ used horses and buggies... these newfangled motorized carriages are only used by a few nutjobs which is better? hell, that should be obvious!! just count them!!!
FYI almost all of the book's reviews got removed after I drew attention to it.
Nice! I still can't believe that people argued they weren't fake! 
Absolutely. I was just thinking earlier about how the reviewers also all reviewed the same 8-10 other items. Fake as fuck.
Thank you all for good suggestions. Originally, I didn't want to provide the DBMS, since I didn't want people to write code in my place. I've checked out the provided samples and have hacked together a solution. I've also took time to read and finish the book: Sams Teach Yourself SQL in 10 Minutes, Fourth Edition - I found it very helpful to fasttrack me into SQL.
Fit me perfectly, and from this construction I can add the filters the way I know and it will work. Much appreciated.
Yeah, didn't provide the DBMS, since I wanted to write code myself. The suggestions above were very helpful on getting direction on how to work with dates. I work with Postgres database. Dates are pretty simple overall - I have datetime available (with no time), the iso_date too, so shouldn't be too hard. 
Thanks for illustrating this with subqueries. 
Yeah, that ultimately works the same way. Further down, the article shows a variant that doesn't pre-calculate the `CASE` expressions in a derived table (using PostgreSQL `FILTER`, which is a fancy `CASE`): SELECT count(*), count(*) FILTER (WHERE length BETWEEN 120 AND 150), count(*) FILTER (WHERE language_id = 1), count(*) FILTER (WHERE rating = 'PG'), count(*) FILTER (WHERE length BETWEEN 120 AND 150 AND language_id = 1), count(*) FILTER (WHERE length BETWEEN 120 AND 150 AND rating = 'PG'), count(*) FILTER (WHERE language_id = 1 AND rating = 'PG'), count(*) FILTER (WHERE length BETWEEN 120 AND 150 AND language_id = 1 AND rating = 'PG') FROM film I just personally find count more readable than sum when we're really counting, but ultimately, the two approaches are equivalent.
It can be done with SUMs just as well...
This is amazing! Thank you so much! For your questions, Order Dates fluctuated for internal reasons and external reasons (mostly external). The primary goal for this audit log is to aid workflow. It’s possible in the end, it smooths out a particular scale, but his could aid staffing scheduling in the future. Much more testing/QA to do, but a huge leap forward. Again, very much appreciated! 
No problem. Thanks for the gold. :)
Gotta agree with this. 
&gt;I work for Oracle. You seem to be a brilliant guy I think you need to think about switching to MS T-SQL. /S
i think we need to do some regression analysis to be sure!!!!!!!
What is the old vs new argument you're comparing to? The syntax in that the highest number of results just indicate age of conception?
It looks good, but I'd start with a quick review of *beginner*'s material. When I teach SQL, I always have to go back a bit to refresh their minds. I'd also include triggers, sequences and nextval.
I work with and teach T-SQL, but I prefer Oracle or even SQLite3.
&gt; shit, thousands ... hell, that err, I think you meant to write: &gt; shit &gt; &gt; , thousands &gt; &gt; hell &gt; &gt;, that
Do we need fucking beautify for fucking sql now?
Yes, code should always look nice. That's why at work we use http://poorsql.com/ .
Commas should be at the end, not the beginning.
SO efficient! Die hard comma first fan over here. 
I don't see any problem. Are you concerned about two bots, or two versions of bots accessing the same database? There is the potential for minor blocking with a handful of users, but only during writes. Avoid `SELECT *` and specify column names when inserting, and your database should generally be backwards compatible with earlier version bots until they are updated.
This really isn't a good use case for sqlite. If both clients were using the db as readonly it would be ok but usage over a network and also updating could cause a problem. Since you have two clients it is probably ok but keep in mind the entire file is locked when a single client is writing to it. Consider using mysql or postgres if you need to scale.
Fair point, but I didn't cover it in the beginner course so need to do it now!
The beginners material is in the first Databases for Developers course; I'll point anyone needing that there! ;)
Good suggestions, thanks. Nulls certainly need a lot of discussion, which is why they get a whole class to themselves! Joins are in the [beginner's class](https://devgym.oracle.com/devgym/database-for-developers.html). Haven't nailed down the specifics for each class, good point about self-join hierarchies. "Standard" CTEs - fits under WITH clause in week 1. Dual - definitely worth covering, though I don't think it deserves a whole class to itself... I'll think about how to fold it into something else.
Thanks, but I think I'm good with Oracle! ;)
You always have to refresh their memories especially if they don't use SQL on a dally basis.
With the example image wouldn't leading be more efficient in case you need to comment out the code? 
shit, i never thought of that!!! if hadn't already spent all my pension this month on crackers and cat food, i'd gold that comment
Ew.
Also easier to add/remove in bulk using Alt+Shift since they're all in a vertical line. Can't do that with trailing commas. 
Sounds like it used to be an issue before SP2 was released, now just reading based off this [forum thread](https://social.msdn.microsoft.com/Forums/en-US/ba7245fb-2533-4cc8-bccd-889d77497a27/msg-9105-the-provided-statistics-stream-is-corrupt-when-i-use-a-union-query-in-my-db?forum=transactsql) that you should make sure it's not corrupted by running DBCC CHECKDB(db) WITH DATA_PURITY 
$715
&gt; DBCC CHECKDB(db) WITH DATA_PURITY Thanks! This unfortunately didn't help, but I did run it with repair_allow_data_loss and it partially fixed the issue. I am operating off a of a backup for now but I deleted the statistics directly from the table and now I'm making progress.
$699 (cost) + $15 (registration fee)
Sum(case when own=1 then 1 else 0 end) as total_owned
 SELECT s.series_name, COUNT(f.name) AS FigureCount, SUM(CASE WHEN f.own = 'yes' THEN 1 ELSE 0 END) AS Owned FROM figures AS f RIGHT OUTER JOIN series AS s ON s.short_name = f.series GROUP BY s.series, s.sort ORDER BY s.sort
This works perfectly! Thank you so much. In all my reading, I never came across the CASE statements. I've studies C so I'm familiar with CASE, but didn't know it was in SQL. This is amazing. I can't thank you enough.
This is great. I wasn't aware CASE was a thing in SQL. This is a great help and give me yet something new to read up on. Thank you.
Anytime :)
How dare u/tasha4life be off by a dollar on your spam post!
&gt;Two types are joins are exist in SQL as given below: God damn it. 
Except there is a CROSS JOIN as well.
It's articles like these that remind me to go easy on our self-taught juniors, because they're out there trying to learn from sources like this one. **Article's tl;dr:** You're answering your interview questions wrong. Talk about how a Venn Diagram works.
This [Coding Horror article](https://blog.codinghorror.com/a-visual-explanation-of-sql-joins/) explains all this with actual examples.
I've never liked the Venn Diagram approach to explain joins. It is misleading. Just show what the damn thing actually does. Give two simple tables with a common field, show a simple query joining them, and show the output. Explain what happened to create the output. 
or... you know... you can just google one of the million different venn diagram descriptions... https://duckduckgo.com/?q=sql+join+venn+diagrams&amp;iar=images&amp;iax=1&amp;ia=images 
Yet you duckduckgoed.
This post can't be serious.
that article is crap
I verbed it. Google gets enough of our money.
I'm going to assume you're using Microsoft SQL Server. 1\. I prefer to make a stored procedure that accepts @parameters for the values you want to insert/update in a table. A simple example might look like this. CREATE PROCEDURE dbo.Table_Update ( @TableID int NULL, --NULL for inserts; value for updates @ColumnA int NULL, @ColumnB varchar(50) NULL, @ColumnC datetime NULL ) IF @TableID IS NULL BEGIN; INSERT INTO dbo.Table (ColumnA, ColumnB, ColumnC) VALUES (@ColumnA, @ColumnB, @ColumnC); END; ELSE BEGIN; UPDATE dbo.Table SET ColumnA = @ColumnA, ColumnB = @ColumnB, ColumnC = @ColumnC WHERE TableID = @TableID; END; 2\. You want to create a UNIQUE CONSTRAINT on such columns. Then you will perform an UPDATE command on the record instead of an INSERT command into the table. 3\. It's a good idea to validate both on client side (for user experience) and server side (for users hacking your JS). 4\. The database will return an error and not insert/update the record if a NULL goes into a NOT NULL column. 5\. Passwords are best stored as salted hash values. To verify a logon, the application would salt and hash the supplied password, and compare it to the stored DB hash.
In regards to question 2, here's what I have right now: protected void buttonSubmit_Click(object sender, EventArgs e) { string insertString = "INSERT INTO profilestable (PlayerName, FriendId, UnitName) values ('" + playerName_textbox.Text + "','" + friendID_textbox.Text + "','" + unitname.SelectedValue + "')"; string deleteString = "DELETE FROM profilestable WHERE FriendId = friendID_textbox.Text"; string query = "IF NOT EXISTS ( SELECT * FROM profilestable WHERE FriendId = friendID_textbox.Text ) " + insertString + " ELSE " + deleteString + " " + insertString; SqlConnection con = new SqlConnection(connString); using (con) { SqlCommand comd = new SqlCommand(query, con); using (comd) { con.Open(); comd.ExecuteNonQuery(); con.Close(); } } } The error I'm getting is "The multi-part identifier "friendID_textbox.Text" could not be bound", twice. Once for each "insertString" reference in the "query" string. It seems to be fine in the "deleteString" string however. Also I know that this insertString has worked before, I have used this exact string during my first iteration for simply inputting items into the table, but now I'm trying to do what I mentioned above by replacing a row if it already has the same "FriendId" column value.
This is the simple fix, but I'd recommend a stored procedure using parameters. This approach is vulnerable to SQL injection attacks. protected void buttonSubmit_Click(object sender, EventArgs e) { string insertString = "INSERT INTO profilestable (PlayerName, FriendId, UnitName) values ('" + playerName_textbox.Text + "','" + friendID_textbox.Text + "','" + unitname.SelectedValue + "')"; string deleteString = "DELETE FROM profilestable WHERE FriendId = '" + playerName_textbox.Text + "'"; string query = "IF EXISTS ( SELECT * FROM profilestable WHERE FriendId = '" + playerName_textbox.Text + "' ) BEGIN " + deleteString + " END " + insertString; SqlConnection con = new SqlConnection(connString); using (con) { SqlCommand comd = new SqlCommand(query, con); using (comd) { con.Open(); comd.ExecuteNonQuery(); con.Close(); } } }
&gt; I don't want to use check constraints, because that seems like a hidden dependency, and that would feel like a nightmare to manage. the main problem with OTLT is that it doesn't let you declare **foreign keys** if you don't think that using FKs to ensure integrity of credit card types or address types or whatever else you have, then you're missing the whole point of "types" and the important concept of **integrity** and ~you~ don't have to manage FKs, the database does that for you
I should post the diagram, because I am absolutely using foreign keys, just that (for example) the credit_card_type column has a foreign key reference to the common_lookup table. Maybe I don't understand an OTLT enough. so my credit_card table has a credit_card_type column which has a foreign key reference to the common_lookup table's common_lookup_id column, and the credit_card_type can be inferred based on the common_lookup_table and common_lookup_column values, which (in this case) would be credit_card and credit_card_type respectively.
I'll give it a shot, but a few things I noticed. First, you replaced "IF NOT EXISTS" with "IF EXISTS". Why is that? If it does not exist, I want it to be added to the table like normal. If it does exist, I want to replace with the new one from the form. Second, it seems you're looking for playerName_textbox.Text, which is not what I'm searching for. I want to find an entry in the table where the value in FriendId (the column in the table) matches the input of "friendID_textbox.Text". If there are no matching entries, add tot he table like usual. If there is a matching entry, delete it and insert the new one freshly afterwards. Thanks a ton for walking through this with me though, I really appreciate it! **EDIT:** I tried running it, it didn't seem to work. It doesn't delete anything, but it does correctly add a new row. It just doesn't delete the old entry. **EDIT 2:** Made some minor changes and it worked flawlessly. Thank you so much for your help! I pretty much understood the rest of your answers too, but I'll have to give them a shot. Thanks again mate, I really appreciate it!
yeah you're going to have to show the exact table layouts and some sample values you seem to imply that `common_lookup_id` contains credit card type values which, since they're referenced by a FK in the credit card table, can't be used by any other type because they're unique which begs the question, how do you know which `common_lookup_id` values refer to which entities so yeah, table definitions and sample values, please
Here is the version I used in the database development course I took. I know it has issues, but it wasn't meant to be production-ready, it was just meant as educational material: https://github.com/maclochlainn/web-video-store/blob/master/create_web_video_store.sql This example is oracle, but afaik that shouldn't matter. I can get you a diagram in a minute.
&gt; I can get you a diagram in a minute. no need, i got what i wanted to see from the CREATE TABLE stuff there's nothing to prevent adding a credit card with a credit card type that is actually a rental item type in other words, it looks like relational integrity, but it isn't
If you want to learn about SQL Injection type this into the Player Name textbox. Buh','Bye','!') DROP TABLE profilestable --
so how would you ensure that only the proper values can be inserted into telephone_type, credit_card_type, etc? IMHO they are too short to make tables out of them, and I'm not sure about using check constraints. the telephone_type table would litterally have 2 rows, and the credit_card_type table would have up to, what.... a dozen rows? seems kinda ridiculous to me.
While I recognize that it's an issue, I am still a total newb at this stuff and don't know if I have the knowledge to do anything about it at the present time. But also the project I'm doing this for is not very large and is only going to be used by a small select group of people that respect the resource.
&gt; IMHO they are too short to make tables out of them i've seen tables with one row and one column, i.e. one value size of table has no bearing on its ~semantic~ purpose if your intention is to avoid small tables, then, as the kids say, **ur doing it wrong**
&gt; I don't think there are enough credit card types or address types or telephone types for them to need separate tables What does it cost you in comparison to the benefits of having FK constraints and the corresponding referential integrity? It's OK to have small tables. Really. You said elsewhere: &gt;telephone_type table would litterally have 2 rows You sure about that? Home, work, mobile, mobile 2, fax, work fax, other?
&gt; It's OK to have small tables. Really. Thank you! I'm glad for the perspective.
Assuming you're using DBA_AUDIT_TRAIL... Technically you can join to DBA _ OBJECTS and go by the Last _ DDL _ date _ time or Created_Date_Time column, would that work? Edit: underscore formatting is lame and I'm too lazy to fix.
For a simpler safe approach than stored procedures, take a look at query parameters via the [SqlCommand.Parameters](https://msdn.microsoft.com/en-us/library/system.data.sqlclient.sqlcommand.parameters\(v=vs.110\).aspx) object. It's not much different from what you're already doing. So your delete string changes from this: ```string deleteString = "DELETE FROM profilestable WHERE FriendId = '" + playerName_textbox.Text + "'"; ``` To this: string deleteString = "DELETE FROM profilestable WHERE FriendId = @FriendID"; SqlCommand comd = new SqlCommand(deleteString , con); comd.Parameters.Add("@FriendID", SqlDbType.NVarChar); comd.Parameters["@FriendID"].Value = playerName_textbox.Text; In general, string concatenation while building any SQL statement is a big no-no. Best to learn that up front, and develop good habits from the start. There are also multiple options to deal with the issue - such as the above, or stored procedures with parameters, Object Relational Mappers (ORMs) like [Entity Framework](https://msdn.microsoft.com/en-us/library/aa937723\(v=vs.113\).aspx) and [LINQ to SQL](https://docs.microsoft.com/en-us/dotnet/framework/data/adonet/sql/linq/), and even simpler Query Builder libraries (don't have an example of that off-hand, sorry). Here's some good [discussion on using parameterized queries](https://stackoverflow.com/questions/4892166/how-does-sqlparameter-prevent-sql-injection) as well. The two top answers briefly go over a bit of what can happen if you use string concatenation, and also what using query parameterization does to protect you.
I've used shared lookups before, with valid FKs, in a case when users could dynamically define their own lookups. Standard lookups still had dedicated tables. It looked something like this: CREATE TABLE dbo.LookupType ( LookupTypeID int NOT NULL IDENTITY(1,1) CONSTRAINT PK_LookupType PRIMARY KEY CLUSTERED , LookupTypeName varchar(50) NOT NULL CONSTRAINT UK_LookupType UNIQUE ); CREATE TABLE dbo.Lookup ( LookypTypeID int NOT NULL CONSTRAINT FK_Lookup_LookupType FOREIGN KEY REFERENCES dbo.LookupType , LookupID int IDENTITY(1,1) NOT NULL , LookupName varchar(50) NOT NULL , CONSTRAINT PK_Lookup PRIMARY KEY CLUSTERED (LookypTypeID,LookupID) , CONSTRAINT UK_Lookup UNIQUE (LookypTypeID, LookupName) ); CREATE TABLE dbo.Customer ( CustomerID int NOT NULL IDENTITY(1,1) CONSTRAINT PK_Customer PRIMARY KEY CLUSTERED --Standard Customer Data ); CREATE TABLE dbo.CustomerLookup ( CustomerID int NOT NULL , LookupTypeID int NOT NULL , LookupID int NOT NULL , CONSTRAINT PK_CustomerLookup PRIMARY KEY CLUSTERED (CustomerID,LookupTypeID) , CONSTRAINT FK_CustomerLookup_Customer FOREIGN KEY (CustomerID) REFERENCES dbo.Customer , CONSTRAINT FK_CustomerLookup_Lookup FOREIGN KEY (LookupTypeID,LookupID) REFERENCES dbo.Lookup );
So I've been looking around and trying to learn about what SqlCommand.Parameters are, but I'm not having much luck understanding. From what I understand, it prevents erroneous characters from being added to a string the SQL query, causing bad things to happen such as dropping your table. What I don't understand is what's going on in the code you posted above. I see that the string is the same up until the point where you say FriendId = @FriendID, but I don't understand much past that. I would like to address this issue using this method, but I'm afraid I fundamentally don't understand what's going on. If I were to attempt it in my own words: You create a query string doing whatever you want to do, but you do not include variables and direct data in the string. Instead, you somehow reference it with a "@". Then, you declare a new command, but I don't understand why you pass in the previous "deleteString" and "con" connection variable at this point. What does each argument do in the SqlCommand() method? After that, you use some thing to do with the "comd" command variable as though it were acting like an object that you perform a method on, but I do not really understand anything past this point. Would you be able to better explain it to me, or show me a YouTube video/source that could do a good job? Thank you so much for your help, after playing around with my own form for a bit I realize the importance of doing this. I really want to come to an understanding of how this works, and your advice of "the sooner the better" seems spot on. Thank you immensely for your time!
The main difference to focus on is that in the string concatenation version you're putting the variable (your text entry field) into place yourself, whereas the parameterized version is asking SQL Server to do the substitution (and it's using a much more robust solution for this than string concatenation). @ in SQL Server terms denotes a variable. In straight TSQL it'd be something along the lines of: DECLARE @FriendID NVARCHAR(MAX); -- Declaring a variable SET @FriendID = 'JoeSmith'; -- Setting a variable DELETE FROM profilestable WHERE FriendId = @FriendID; -- Using a variable The SqlCommand.Parameters.Add method declares the variable, and the ["@FriendID"].Value portion sets it. [SqlCommand](https://msdn.microsoft.com/en-us/library/system.data.sqlclient.sqlcommand\(v=vs.110\).aspx) has some overloaded constructors, [one of which](https://msdn.microsoft.com/en-us/library/877h0y3a\(v=vs.110\).aspx) takes a string (the query) and a SqlConnection as parameters. You could also just as easily create the SqlCommand with no arguments, as there is also a [constructor for that](https://msdn.microsoft.com/en-us/library/9s8ekk5c\(v=vs.110\).aspx), and instead set the CommandText and Connection properties afterwards. The gist of it, is that you're creating an object that tells your .NET app that you want to issue command, defined by a query (the string), against a specific server (the connection), using the parameters (your text field) that you have added to it. .NET is really not my area of expertise - maybe someone else can chime in with some suggested training resources from Youtube. If you have a few bucks (and time) to spend then places like [Pluralsight](https://www.pluralsight.com) can be very much worth the money, and they also have a 10 day trial. Look into these: https://www.pluralsight.com/courses/full-stack-dot-net-developer-fundamentals https://www.pluralsight.com/courses/adodotnet-fundamentals And pretty much anything by this guy: https://www.pluralsight.com/authors/scott-allen
Dude this is super helpful! Thank you! I was wondering though, wouldn't it be easier just to do an ASP.NET RegularExpressionValidator with C# in the code itself to even prevent it from being sent in the first place? Although to be honest I'd need help to figure that out too, I'm a slow learner unfortunately and I ask a lot of questions as you can probably tell. Thank you again for your help and advice!
Sequential execution is also slow. I rather have multiple processors working on my problem than only 1. If you're aiming for disk-speed, [3000 line sprocs](https://thedailywtf.com/articles/The-Query-of-Despair) and single core performance, just keep doing what you're doing. I won't try to convince you to try another method, because you already believe what you want to believe. I'd almost say you're a troll... See [this article on UNIONs](https://sqlsunday.com/2017/07/26/prioritizing-rows-in-a-union/)
I winced at that too, I don't use them very often but he forgot about Cartesian Product / Cross Join which is useful in some very specific cases. You really can't easily explain a Cartesian product in terms of inner and outer joins.
No, using parameterized commands is the Right Thing. If you try to do it by sanitising all the inputs then everywhere that ever becomes an input is a potential threat and you are more or less bound to miss one somewhere. This way only the database code needs to handle the problem, so you only have to get it right in one part of the system. Also, if you insist on sanitising the inputs you end up giving away to the user/ customer what's going on. Some of them will be slightly inconvenienced (e.g. "Why can't my name have an apostrophe in it? I'm O'Connor, not just Connor stupid program") but some will recognise that this means you're using poor techniques and may look for ways to attack.
Off topic: For passwords you don't want "encryption" you want a Password Hash, which is a one-way function that lets you do two things: Make a password into a new hash, and Compare an entered password to a hash, but does not provide a way to turn a hash back into a password. https://stackoverflow.com/questions/4181198/how-to-hash-a-password/10402129#10402129 The password hash used should be pessimised (opposite of optimised, you are only going to use it once per log in, but bad guys are going to be running it a lot, so pessimisation makes that harder for them), and salted (if two people have the same password they should still get different hashes so nobody can tell). The above link ticks those boxes, there are arguably better options, but if you do what that link says you're better than most web sites by a considerable margin.
I'm sorry but are you suggesting that a query only uses multiple cores when it is written as a CTE, and that all daisy-chained SELECTS between BEGIN/ENDs do not, and will only use a single core?
Why not have both inserts in a single stored procedure?
you need a calendar table it is not necessary to store "prev" and "next" for values that can be located with window functions
I think you're looking to define prev/next based on contiguous segments. Try something like this. SELECT MIN(c.Week) OVER (PARTITION BY c.Segment)-1 , MAX(c.Week) OVER (PARTITION BY c.Segment)+1 FROM (SELECT c.Campaign , c.Week , c.WeekDay , ROW_NUMBER() OVER (ORDER BY c.Campaign, c.Week, c.WeekDay) - ROW_NUMBER() OVER (PARTITION BY c.Campaign ORDER BY c.Week, c.WeekDay) AS Segment FROM CampaignRun AS c) AS c;
This seems like the most logical choice given the information we have. DECLARE @newuserid int BEGIN TRANSACTION BEGIN INSERT INTO users (col1, col2...) VALUES (val1, val2...) SET @newuserid = SCOPE_IDENTITY() INSERT INTO group (useridcol, col2...) VALUES (@newuserid, val2...) END COMMIT TRANSACTION Could also include a try/catch block and/or additional logic for error trapping and handling. OUTPUT is a newer function than SCOPE_IDENTITY and is a bit more flexible so that can be a consideration as well. 
I agree, with the caveat that you may have to institute a process to report on users w/o their default group, or logic in the application to handle this condition. This is to handle the situation where you have users created by a rogue developer (or DBA) who has side-stepped the stored procedure.
Depends on how you write the CTE (recursive will always be single core. For all other normal select statements (regardless of CTE, derived, correlated subquery or whatever) it depends on MAXDOP, [cost threshold for parallelism](https://technet.microsoft.com/nl-nl/library/ms188603(v=sql.105\).aspx) and the amount of pages that has to be read.
Right, so, as I've said since the beginning: For any type of complexity or large data set you are highly likely to get much better performance not using a CTE and utilizing #tables. You can use permanent tables, too. Forcing SQL to execute things in sequence can often be the difference between a process that takes hours and minutes. Most of the processes I'm referring to started as a CTE and were rewrote when their execution times exploded. I'm not talking about a trivial process that does light work, I'm talking about intensive ETL types of calculations which may require many joins (self -&gt; other) and many different functions/cases/etc across tens or hundreds of millions of rows. 
And thus you say so, and deliver no proof at all. #temptables always write to disk, disk is slower than ram. What more do you need explained?
OK, here is just a simple example which I can't even believe is necessary: with data as ( select * from table join a join b join c join d ), sub_data as ( select *, dbo.function([column]) from data a join data b join data c join data d ), cleanse as ( select * from sub_data a join data b join c join d join e where not exists () ) select * from data b join cleanse a So before we get on with the test can you tell me how your indexes are going to function when I join data and sub_data?
Have you even read this [article](https://sqlsunday.com/2017/07/26/prioritizing-rows-in-a-union/) ? It's exactly about this kind of situation only a whole lot less vague. And your aliasing is a horror show.
I wrote that in 10 seconds as an example. I'm not really getting a clear answer from you. If you want I can run real test data and show you results: &gt;The temp table solution has merit in situations where you want a little more control and don’t want to blow up your plan with lots and lots of source tables and operators. This is particularly true if your indexes aren’t perfectly aligned as they are here, or if you’re using views as data sources. Many database professionals will argue that a temp table in and of itself is a performance bottleneck, and this may well be true – you might actually want to create a regular table for this purpose. Complex way of saying that when shit gets real, you use #tables or dbo.tables (as you would in SSIS) or you get fucked. Basically you've been going on and on about my exact point, and your resource agrees precisely with what I'm saying.
Hey /u/jc4hokies ! Thanks for your help (again!) There seems to be a solution. However, I have a little issue : On [this image](http://imgur.com/V3BdIxB), you can see that the pdc for the daycode 2-16 is 2-14, whereas it should be 2-15. Do you have an idea on how I could solve this? In any case, thank you very much for your help :)
Hi /u/r3pr0b8 ! Thanks for your help. Can you elaborate a bit about window functions, calendar table? I'm a bit new to SQL...
Try this: MIN(c.Week) OVER (PARTITION BY c.Segment, c.WeekDay)-1 , MAX(c.Week) OVER (PARTITION BY c.Segment, c.WeekDay)-1 or this: ROW_NUMBER() OVER (ORDER BY c.Campaign, c.WeekDay, c.Week) - ROW_NUMBER() OVER (PARTITION BY c.Campaign, c.WeekDay ORDER BY c.Week) AS Segment 
You are going about this in the wrong manner I think. I understand you are a newbie, but you need to research using data sets and not doing things programmatically in loops. Since you are already splitting your dates into parts of dates and putting those parts into individual columns you are already going down a path of storing duplicate data in the same table, which is rarely a good practice. I think maybe it is time to take a step back and say "what am I trying to do here?" and not "how do I split each date component into 4 date parts with corresponding parts indicating the next and previous date and also that is part of my key value for my campaign".
&gt;when shit gets real ...Data spills (spools) to disk anyway or you're gonna need a whole lot of RAM if you want to prevent that.
What I'm saying is that large SSIS packages don't use CTEs for a reason. They write to permanent tables for a reason. I am, here, talking about #tables for two reasons: 1. The performance difference between a #table and a dbo.table is often negligible at best, for example, it does not differ by an order of magnitude. 2. You often cannot create permanent tables on production databases and must use #tables if you want to get something done in a reasonable amount of time. I write CTEs often. I rewrite them to use #tables when they become too big for their own good and no longer execute efficiently. Very simple concept.
Your first formula did work! Thank you very much for your help! Another problem arised, though (Since I started to learn sql, I always feel like it's one step forward, two steps backwards). When [this kind of situation arise](http://imgur.com/bvRoCNl), do you have any idea about how I can look for the previous day code with no campaign ...? I'm sorry to bother you like this, my colleague/SQL teacher is out of office and I have some deadlines to meet which I will not meet if I stay stuck on this freaking problem. 
I'm trying to replicate an excel file into SQL, which probably leads me to have a reasoning which might not be the best fit for SQL. I'm trying to calculate the [CAGR](http://www.investopedia.com/terms/c/cagr.asp) of the marketing impact of my campaigns per day. Therefore, I'm forced to take in consideration the day of the week, as well as the week of the year when I do my calculation... Do you have an idea about how I could achieve this calculation without giving each date a daycode, to identify the previous day code / next day code ? I'd gladly go in that direction!
&gt;The performance difference between a #table and a dbo.table is often negligible at best, for example, it does not differ by an order of magnitude. Azure VMs with SQL Server often have a blazing fast TempDB on the local storage and the databases on attached storage which is slower. So it depends on your environment. &gt;You often cannot create permanent tables on production databases Can't talk for your environment, but I usually make an import schema. It's going to cost IO somewhere right? I'd rather not recreate the table structure and indexes the whole time. &gt;Very simple concept. Sure
Hi Everyone, I wrote the blog post mentioned by Alinroc. Transactional log backups are crucial to any disaster recovery plan that needs to include recovery up to point of failure or the past last full or differential backups. If you use Log Shipping it should be the process that takes your transactional log backups. If you want to learn more about Backups and Recovery I co-authored a book that was purposely cheap so more people could learn the fundamentals. Link to Book -&gt; (https://www.amazon.com/SQL-Server-2014-Backup-Recovery/dp/150257389X/ref=sr_1_1?ie=UTF8&amp;qid=1501517929&amp;sr=8-1&amp;keywords=John+Sterrett) Also, if you are interested in learning more about Disaster Recovery I will be blogging more at https://www.procuresql.com/blog You can also catch free online content at http://hadrvc.pass.org/
&gt;Azure VMs with SQL Server often have a blazing fast TempDB on the local storage and the databases on attached storage which is slower. So it depends on your environment. Show me a SQL server environment where a #table executes faster than a dbo.table by an order of magnitude please. I would be willing to bet a considerable amount of money that such a thing does not exist, and that if it does it is such a highly specialized example that it essentially doesn't exist in real world applications. &gt;Can't talk for your environment, but I usually make an import schema. It's going to cost IO somewhere right? I'd rather not recreate the table structure and indexes the whole time. I often find that dropping and recreating indexes can greatly increase performance of some SSIS packages. Regarding my question above this example isn't 100% of the time but maybe close to 49% of time and I can provide examples if you persist. Hell you can often improve performance greatly simply by doing away with updates/inserts and simply rerunning your entire data set and inserting it into an empty table and then recreating your indexes. My basic point -- and forgive me but I'm pretty confident that it stands -- is that CTEs and sub-queries are great... to a point. At that point you want to use tables, and for a variety of reasons you may want to use #tables instead of permanent ones. 
I don't understand why the highlighted value is incorrect.
When there is a campaign, I need the previous day code of a day without campaign edit : if it can be a help, I do have a column displaying 'yes' if there is a campaign, and 'no' when there's no campaign.
Wait what? Brb googling sql keyboard shortcuts. 
It works in SQL Server Management Studio. Not sure if that is a common shortcut across other platforms though. I've only used SSMS. 
I'll give it a good readover and let you know if I have questions! Thanks for the info mate!
Understood. So sanitize some basic stuff but then go and use parameterized commands proper on the backend. I can do that :)
In all the ORDER BY / PARTITION BY change Campaign to IsCampaign (Yes/No column).
I just wanted to say that I was pulled away from this particular task, but I'm revisiting it now. I'm so, SO appreciative of the work that you've done here. I'll let you know if I'm able to successfully implement it myself. &lt;3
&gt;Show me a SQL server environment where a #table executes faster than a dbo.table by an order of magnitude please. I would be willing to bet a considerable amount of money that such a thing does not exist, and that if it does it is such a highly specialized example that it essentially doesn't exist in real world applications. Sure: 1. go to Azure (get a subscription, free $170,-) 2. Create *any* VM with SQL (there are templates) 3. Create TempDB files on the D: drive, remove the standard from the C: drive 4. Create disk storage, SSD or HDD doesn't matter. 5. Attach it to the VM, put your DB on it 6. Test it! :)
Why don't you show me a video of a query that runs an order of magnitude different when you use a #table vs a real table. Or just the raw output. Screen shots will work. 
It's not magic, it's just the temporary storage on an Azure VM being way faster than (not crazy expensive) persistent storage which is slower. You'll see it when you use Azure VM's. [Using SSDs in Azure VMs to store SQL Server TempDB](https://blogs.technet.microsoft.com/dataplatforminsider/2014/09/25/using-ssds-in-azure-vms-to-store-sql-server-tempdb-and-buffer-pool-extensions/) [Performance best practices for SQL Server in Azure Virtual Machines](https://docs.microsoft.com/en-gb/azure/virtual-machines/windows/sql/virtual-machines-windows-sql-performance) [Complaints about persistent storage being slow in azure](https://serverfault.com/questions/725459/why-is-my-azure-vm-d3-disk-i-o-crazy-slow) If I were to place my TempDB on a NVME drive and a normal table on a USB 1.1 disk you wouldn't frown right?
I'm not seeing anything about order of magnitudes (not being difficult) but will point out you're really wandering into a niche area where it is sufficiently different than what I was specifically commenting on.
&gt; While I recognize that it's an issue, I am still a total newb at this stuff and don't know if I have the knowledge to do anything about it at the present time. You're looking at it backwards. Because you're a "total newb", this is the **perfect** time to form good habits. Don't do it the wrong way and then struggle un-learning that - put the time into learning it right first.
The problem with writing your own validators/scrubbers is that you will **always** miss something and that **will** eventually be exploited. Using parameterized queries solves 99% of this problem because the underlying code was written by people far more talented than you (and more people as well!), more stringent checking is performed and appropriate errors will be thrown when necessary. Also, parameterized queries will put less load on your database server and your DBA will thank you.
You shouldn't have to sanitize anything yourself. You can use validators to make sure that someone isn't putting their name in a phone number field, but using a parameterized query will take care of all the sanitizing for you.
&gt;a niche area A niche area? Let me paraphrase my boss: the cloud's the next big thing and everythings going into cloud and damn the consequences! Azure is every DBA's pain and consider yourself lucky you don't have to deal with this "niche area".
Let me paraphrase what I told my boss, and their bosses: *If you want to do serious analytics and modeling, I need a server on premise. You can keep whatever bullshit you want in the cloud and we can bring it down as needed at intervals which are reasonable. Fuck the cloud and tell your salespeople I don't give a shit.* Cloud's great for lots of things, but I'm not a DBA, and that aside you may well be correct about #tables verse dbo.tables for your niche example -- but I highly doubt by an order of magnitude. That smells like a very suspect statement. Admittedly it's possible... but it deviates from the point I was making re: CTEs that get involved, are join or function intensive, etc.
 Select ROUND(SUM(ATD.duration)) as 'Hours Attended',CMP.campusname, Teachers_Hours FROM STudents STD INNER JOIN Campuses CMP ON CMP.CampusCode = STD.StudentCampus AND STD.isactive = 1 Inner Join Attendance ATD ON STD.studentId = ATD.StudentID AND ATD.IsActive = 1 Inner Join ( Select ROUND(SUM(TCA.duration)) as Teachers_Hours FROM Teachers TCH Inner Join Campuses CMP ON CMP.campuscode = TCH.CampusCode AND TCH.isactive = 1 INNER JOIN TeacherAttendance TCA ON TCH.teacherID = TCA.teacherID AND TCA.isactive = 1 ) AS Teachers_Hours WHERE STD.ADMINID AND CMP.isactive =1 Group By CMP.campusname, Teachers_Hours I tried something like this but it returns the same value for all 3 campuses under teachers hours.
Chill out, homes. You need to provide more detail about these tables, how big they are, what your current output looks like, and what you expect your output to look like. 
 Hours Attended Campus Teacher Hours 42259 Brooksville 9162 49179 Spring Hill 9162 50476 New Port Richey 9162 This is the current output The teacher hours should be different. Tables are small, around 1000 rows each. This output is my 2nd query, the first one just runs forever.
They should be different how? Do you want to continue to see them the way they are or see total hours. You need to break things down more specifically. You have a query in the description which outputs what? You want to change it how? Which table has the information you need and why isn't it currently functioning as you expect? 
It's important to group first and join later to avoid many-many queries (students-teachers here). The modified query is campus-campus and is much easier to manage. SELECT s.duration AS 'HoursAttended' , t.duration AS 'Teachers Hours Attended' , CMP.campusname FROM Campuses CMP INNER JOIN (SELECT STD.StudentCampus , ROUND(SUM(ATD.duration)) AS duration FROM Students STD INNER JOIN Attendance ATD ON STD.studentId = ATD.StudentID AND ATD.IsActive = 1 WHERE STD.isactive = 1 GROUP BY STD.StudentCampus) AS s ON CMP.CampusCode = s.StudentCampus INNER JOIN (SELECT TCH.CampusCode , ROUND(SUM(TCA.duration)) AS duration FROM Teachers TCH INNER JOIN TeacherAttendance TCA ON TCH.teacherID = TCA.teacherID AND TCA.isactive = 1 WHERE TCH.isactive = 1) AS t ON CMP.campuscode = t.CampusCode WHERE CMP.isactive =1;
Because each campus is not going to have the same amount of teacher hours, there are different teachers, classes ect. I want to change it so I can receive the right output of teacher hours. Everything functions good until I add TeacherAttendance TCA, then it will never return the query as in the original query I posted or for query 2, it will returns 9162 for all teachers. I apologize for me being a little all over the place. IF I do 1 query for student hours and a separate for teacher hours it works fine but when I try to combine them, it goes bonkers.
 HoursAttended, Teachers Hours Attended, campusname 42259 9162 Brooksville We lose the other 2 campuses with this query. 
Then left outer join.
I think jc4hokies is giving you an example of how you might do it. I'm still not entirely sure what you're trying to look at. Multiple teachers work at multiple campuses and the houses are different and you want to come up with a total by campus of every hour teachers attended and every hour students attended?
Yes exactly.
We have 3 campuses I want to get the overall hours for students and teachers at each campus Should be something like this Total Student Hours Campus Total Teacher Hours 42259 Brooksville 9162 49179 Spring Hill 5000 50476 New Port Richey 1600 Whenever I bring in the teacher hours shit goes wrong, and Jc's pulling this with left outers. No matter what the teachers hour are jacked, unless I only do teacher hours or students hours totally by themselves. Total Student Hours Campus Total Teacher Hours 42259 Brooksville 9162 49179 Spring Hill NULL 50476 New Port Richey NULL NULL NULL NULL Like I said though this thing has been beating me up, it seems so easy but I cannot do anything to get it to work.
Right so you need to calculate these separately (most likely) and then join them, so something like this: select campus, sum(hours) thrs into #teacher_hours from table group by campus select campus, sum(hours) shrs into #student_hours from table group by campus select campus, sum(shrs), sum(thrs) from #teacher_hours join #student_hours group by campus You could do this with a subquery, your tables are small enough.
Well you can use month(date) and year(date) function in your query. &gt; Select t1.count (1) from table1 t1, table2 t2 where t1.unique_id = t2.unique_id and (month(t1.date_1) = month(t2.date_2) and year(t1.date_1) = year(t2.date_2) ) Typed on a mobile excuse the formatting.
If you're applying for something that doesn't actually require SQL... being able to read and understand a SQL query and being able to write queries are resume worthy. This includes joins and CTEs, but not necessarily things like stored procs, loops, and cursors. If you're looking to become a DBA... you should be familiar with pretty much all DML, decent knowledge about clustered/non-clustered indexes, foreign keys / constraints, primary keys, database normalization/denormalization, and stored procs.
Basically, the answer to any "is a trigger the solution" question is: No, a trigger is not the solution.
Can you explain the difference between a left join and an inner join? Tell me how you could use "having" in a query. Does NULL equal a blank? What do you think of ANSI SQL, is it important, and why? To me, SQL is something of a language, not a skill set to check off on a resume. And I'll know if you get that.
IMO, to qualify yourself as a solid beginner, you should feel reasonably fluent writing Insert, Update, and Delete statements, including Inserts with more than 1 table. You should be able to write Select statements with 2-3 tables joined. You should be able to create a basic Create Table statement to store the data you are working with. I also think you should be able to do Inner and Left joins. Also, learn the BOL (Books On Line) or equivalent for your platform, and official documentation of these statements well enough, so that by "reasonably fluent", you can write these on the fly, with nothing more than a quick glance at official documentation to help if you encounter a minor obstacle. Beyond that would be Intermediate territory. You can get by with even less than all this if your job is interpreting SQL or otherwise minor interactions with it, without being responsible for development yourself.
I disagree... you could be fluent in 40 different languages, paradigms and methodologies, but still struggle with SQL, declarative languages, and set-based operations. Aggregation, collation, isolation modes, and normalization are beyond "basic SQL" in terms of a resume... they are into intermediate/advanced topics, or a place to start for people starting a long-term track beginning with database theory.
Not sure if OP got their answer, but your solution looks to be the most efficient, assuming that 0 does not occur within the dataset for ID2 &amp; ID3 (which is a valid assumption for an id). Just need to add in ID1 into the start of the join: “a.ID1 = b.ID1 AND”
So I went and gave it a shot, here's what I've got but it's giving me some "Input string was not in a correct format" errors. Probably syntax but can you take a look at this if it's not too much trouble? using System; using System.Collections.Generic; using System.Linq; using System.Web; using System.Web.UI; using System.Web.UI.WebControls; using System.Data; using System.Configuration; using System.Data.SqlClient; public partial class _addme : System.Web.UI.Page { string connString = "Data Source = (omitting this for obvious reasons)"; protected void Page_Load(object sender, EventArgs e) { } //If the Friend ID has already been used in the table it will remove the old entry and add the new one. This allows the //user to "update" their entry in the list without having to manually remove their old one. protected void buttonSubmit_Click(object sender, EventArgs e) { { string insertString = "INSERT INTO profilestable (PlayerName, FriendId, UnitName, UnitLevel, HP_stat, MP_stat, ATK_stat, MAG_stat, DEF_stat, SPR_stat, LB_stat, Right_hand, Left_hand, Head_piece, Chest_piece, Acc_1, Acc_2, Materia_1, Materia_2, Materia_3, Materia_4, Remove_after, Save_to_profile, Add_notes) values (@playerName, @friendID, @unitname, @unitlevel, @hp_stat, @mp_stat, @atk_stat, @mag_stat, @def_stat, @spr_stat, @lb_stat, @righthand, @lefthand, @headpiece, @chestpiece, @acc1, @acc2, @materia1, @materia2, @materia3, @materia4, @remove_after, @save_to_profile, @addnotes)"; string deleteString = "DELETE FROM profilestable WHERE FriendId = @friendID"; string query = "IF EXISTS ( SELECT * FROM profilestable WHERE FriendId = @friendID ) BEGIN " + deleteString + " END " + insertString; SqlConnection con = new SqlConnection(connString); using (con) { SqlCommand comd = new SqlCommand(query, con); using (comd) { con.Open(); comd.Parameters.Add("@playerName", SqlDbType.NVarChar).Value = playerName_textbox.Text; comd.Parameters.Add("@friendID", SqlDbType.NVarChar).Value = friendID_textbox.Text; comd.Parameters.Add("@unitname", SqlDbType.NVarChar).Value = unitname.Text; comd.Parameters.Add("@unitlevel", SqlDbType.Int).Value = unitlevel_textbox.Text; comd.Parameters.Add("@hp_stat", SqlDbType.Int).Value = hp_stat_textbox.Text; comd.Parameters.Add("@mp_stat", SqlDbType.Int).Value = mp_stat_textbox.Text; comd.Parameters.Add("@atk_stat", SqlDbType.Int).Value = atk_stat_textbox.Text; comd.Parameters.Add("@mag_stat", SqlDbType.Int).Value = mag_stat_textbox.Text; comd.Parameters.Add("@def_stat", SqlDbType.Int).Value = def_stat_textbox.Text; comd.Parameters.Add("@spr_stat", SqlDbType.Int).Value = spr_stat_textbox.Text; comd.Parameters.Add("@lb_stat", SqlDbType.Int).Value = lb_stat_textbox.Text; comd.Parameters.Add("@righthand", SqlDbType.NVarChar).Value = righthand_textbox.Text; comd.Parameters.Add("@lefthand", SqlDbType.NVarChar).Value = lefthand_textbox.Text; comd.Parameters.Add("@headpiece", SqlDbType.NVarChar).Value = headpiece_textbox.Text; comd.Parameters.Add("@chestpiece", SqlDbType.NVarChar).Value = chestpiece_textbox.Text; comd.Parameters.Add("@acc1", SqlDbType.NVarChar).Value = acc1_textbox.Text; comd.Parameters.Add("@acc2", SqlDbType.NVarChar).Value = acc2_textbox.Text; comd.Parameters.Add("@materia1", SqlDbType.NVarChar).Value = materia1_textbox.Text; comd.Parameters.Add("@materia2", SqlDbType.NVarChar).Value = materia2_textbox.Text; comd.Parameters.Add("@materia3", SqlDbType.NVarChar).Value = materia3_textbox.Text; comd.Parameters.Add("@materia4", SqlDbType.NVarChar).Value = materia4_textbox.Text; comd.Parameters.Add("@remove_after", SqlDbType.Bit).Value = remove_after.Checked; comd.Parameters.Add("@save_to_profile", SqlDbType.Bit).Value = save_to_profile.Checked; comd.Parameters.Add("@addnotes", SqlDbType.NVarChar).Value = addnotes_textbox.Text; comd.ExecuteNonQuery(); con.Close(); } } } ScriptManager.RegisterClientScriptBlock(this, this.GetType(), "alertMessage", "alert('Update Successful!')", true); } }
There is an awful lot in between "jobs that don't actually require SQL" and "DBA" .
Step 1: Put it on the resume Step 2: BS the interview (sorta. You have to know some stuff) Step 3: Make sure you always have access to stackoverflow. Congrats, you're just like the rest of us. 
Go on then...
...Except if another DBA is interviewing you, you can forget it, they will see clean through your BS. I've interviewed dozens of potential new starters, the bullshitters stand out clear as day, and they never get the job; it's far better to be honest about skills, and for things you lack, turn them into goals you want to achieve. Do NOT try and BS an interview for a SQL job, you're only chance of getting it is if the company doesn't already have a DBA and it's a random manager that's taking the interview. 
...anyone who uses a database and doesn't maintain it?
CASE WHEN [ToLose] IS NULL THEN [Apply] END AS [ItsJustAnInterview]
If someone is writing joins &amp; CTEs, I'm going to expect them to know when &amp; how to use loops/cursors properly as well.
 trunc(t1.date_1,'MONTH') = trunc(t2.date_2,'MONTH') would also work
&gt; here's what I've got but it's giving me some "Input string was not in a correct format" errors. Probably syntax but can you take a look at this if it's not too much trouble? Rule #1 for asking for help with code: **Always** point out the exact line where it's erroring out. We can't compile your code because we don't have the full project. Rule #2: Distill your issue down to the smallest possible amount of code that demonstrates the problem. On Stack Exchange, this is known as the [Minimal, Complete, and Verifiable example](https://stackoverflow.com/help/mcve).
Yupp .. In fact it wont need year function as I believe it'll change the date to (?) first of the month. 
Yep, that's how querying a normalised database works. It's very convoluted for a typical user to have to query and join so many tables, which is why Data Warehouses are quite happy to create some redundancy and make the querying a little easier/friendlier. (among many many other reasons) 
A JOIN is a simple keyword that anyone with an understanding of SQL should know. A CURSOR is the stuff of dreams of the devil. I can stare at cursor based code long enough and generally figure out what it's up to, I'd refuse to write anything that used a cursor. There is always another way to do it, and 95% of the time that other way will result in quicker queries. Let the database work its database magic.
I would consider two sides to entry level: data loading and data extraction. If I tell you to delete duplicate rows in a table with a unique ID column, and column user id, timestamp, and purchase amount, how can you do that? I want to select all rows from that table from users who made more than one purchase a week. How do I do this? I need addresses. Can you join this table to another table with unique integer ID and columns user id, street address, and an update date timestamp for users who update their info at the time of purchase. Do you know how to make sure you're getting the right address for a given purchase? It's not as simple as joining those two tables on user id (I want their address at time of purchase. What if I wanted the most current address for a user? This is a more advanced question). These are questions I would ask. I don't care if you know that NULLS and blanks are different, or how to optimise a query for performance. At entry level I assume I will teach you lots of this. I need to see a basic working understanding of data and how to use SQL to store and get it. 
Agreed, I've known people that were really bright that couldn't wrap their head around set based logic. It required a different way of thinking vs other programming languages. 
i've seen database design like this before -- sucks big time a foreign key for city, and another foreign key for country? seriously, i would like somebody to explain in detail why you do this and at the same time, please explain why you do ~not~ assign a foreign key for first name in the persons table it's the exact same scenario, except nobody thinks "hey, there's a lot of "redundancy" with so many people having first names like John and Mary, so let's split them out and use a foreign key" i've said it many times before, the fact that John repeats a lot in the persons table **is not redundancy** so why do people feel the need to split out city and country? i do wish people would understand normalization 
IMO put that you're learning SQL. The fact that you are trying to learn new things should be a green light to any interviewer. IMO people who want to learn new skills make better employees than people who don't. As far as getting a job doing SQL, call up a tech staffing firm. Talk to a recruiter there. See what the job market in your area is like. Sometimes there will be super high demand and places will hire people with lower skills. That's how I got hired. In my 6 month contract I made sure to learn as much as I could from the other developers. It paid off big time, I got offered a FT position with that company and have been writing SQL full time for 4-5 years now. At my first interview they wanted to know that I knew the difference between and INNER and a LEFT JOIN. IIRC they asked for an example when I've used each. Then they wanted to be sure I understood the GROUP BY logic.
IF there's no DBA to begin with, you're going to have a bad time. But, it's an ooportunity to learn a lot on the job if they don't ask for a lot of minimum skill requirement. But you still need to know how to carry your own weight. Can't spend 3 days on stackoverflow figuring how to use joins in queries lol.
There *are* situations where a cursor is unavoidable. In the post I was replying to, it was really the CTE that was the breaking point for me as far as whether or not I expect them to know the appropriate application(s) for a cursor. But I've seen a fair amount of CTE abuse as well, and poorly-constructed JOINs from people who should know better.
I'll not pretend I've seen it all, but IMO cursors are evil tools created for people who can't figure out set based logic and are to be avoided. I'll admit there are times when they're faster to write, and if the code will seldom be executed aren't terrible. 
Can you tell me how one can execute the same query or stored proc against a few hundred databases (the list determined by the output of another query) without use of a cursor? Don't say `sp_msforeachdb` because that uses a cursor internally. &gt; I'd refuse to write anything that used a cursor. Then you'd be refusing to do a bunch of things. To say that there is **never** a valid use for a cursor is just as incorrect as using them for everything you can do with a set-based operation.
I can only think that someone wanted to have picklists for every single item on the front end maybe? So they have a unique record for each city, state, country, etc. I certainly have seen country code tables, which serve as a translation from US to United States, etc. Which are quite useful. But there are a lot of fucking cities in the world... that should be a text field.
touché I work at a shop where a senior guy uses cursors to do SUM and COUNT of million row tables. I'm not a fan and I see them used poorly daily.
If someone wrote Entry Level I would expect that they can write A query - ANY query, not necessarily the best performing one - to get a basic result out of a dataset. I usually think of my accountant and other non IT friends who could do this against their datasets. It was rarely pretty but they (sort of) understood joins, filtering, and maybe some conditional logic. And like a foreign language you should be able to read some SQL and tell me what it's doing functionally, even if you wouldn't have known how to write it. And you should at least understand the concepts of a database: tables, views, rows, columns, indexes, etc. If you understand the why (why are seeks better than scans, why normalization is important, why referential integrity matters, etc.) the how becomes a lot easier.
You can do this with a PIVOT: CREATE TABLE #Company ([company id] int, name varchar(max), job varchar(max)) INSERT INTO #Company VALUES(1, 'Bob', 'CEO') ,(1, 'John', 'CFO') SELECT * FROM (SELECT [company id], name, job FROM #Company) AS C PIVOT (MAX(name) FOR job IN ([CEO], [CFO])) AS Pvt Since PIVOT requires an aggregate you kind of fake it with a MAX on the name column. Add other job to the IN clause (need the square brackets) to add more columns to the list.
&gt; I work at a shop where a senior guy uses cursors to do SUM and COUNT of million row tables. Yeah, that's a bad move right there. Aggregates and especially window functions have obviated the need for a *lot* of the cursors that are out there. I'm guessing he cut his teeth on SQL Server 2000 (maybe 2005) and hasn't kept up with new features.
https://en.wikipedia.org/wiki/Database_normalization well studied and important subject.
**Database normalization** Database normalization, or simply normalization, is the process of organizing the columns (attributes) and tables (relations) of a relational database to reduce data redundancy and improve data integrity. Normalization is also the process of simplifying the design of a database so that it achieves the optimal structure.It was first proposed by Dr. Edgar F. Codd, as an integral part of a relational model. Normalization involves arranging attributes in relations based on dependencies between attributes, ensuring that the dependencies are properly enforced by database integrity constraints. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/SQL/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.24
Yeah I don't need a wiki link. My google fu is top notch and I know what database normalization is. What is your point here?
well you said they did it because you think " someone wanted to have picklists for every single item on the front end maybe" what i was trying to say is that the reason you think they did it maybe valid but they probably do it for the sake of it being normalized just because thats the correct way to do it. wasnt try to imply you dont know how to google. just instead of me explaining all the reasons to normalize a DB - I just linked the well explained wiki article.
With countries you already have a fixed number of countries beforehand. Sure one might be added every now and then (almost never), but the list is pretty much fixed. With first names there are a million of possibilities. To map every option to it's own key would generate more data than to just save all the first names separately. Conversely, with countries, the table containing the foreign key would just need to store 1,2,3 etc etc for US,NL,UK.
But that doesn't make sense at all. Are they going to make a table with every city on earth? And if so, are they going to maintain that beast when names change? You would need a team of people to do so. Plus, it's not even normalized. There are at dozens of duplicate city names in the USA. So linking City to Country isn't correct, so this fails normalization tests anyway. You would need territory/state. What if there's a duplicate city in a territory? What smaller piece do you break that down to now? Zip Code? What if there's a duplicate *in the same zip*? Now the model is entirely broken and unusable and you have a database with thousands or hundreds of thousands of records that are basically useless. It's just bad. Full stop. There is absolutely no need to put every piece of data in a separate table.
How do you handle the fact that there are [eight cities named Portsmouth in the USA](http://us.geotargit.com/called.php?qcity=Portsmouth) when your records are relating to country, how do you identify them? Normalization broken.
&gt; To map every option to it's own key would generate more data than to just save all the first names separately. please explain why cities requires a separate table 
Thank you for your help. It looks like Oracle doesn't have the MONTH and YEAR functions, but I adapted your query to use the EXTRACT(MONTH) and EXTRACT(YEAR) functions to, I believe, the same effect. However, I'm getting different results with this query than I do with the original query. **New query:** Select count(distinct t1.unique_id) from table1 t1, table2 t2 where t1.unique_id = t2.unique_id and ((extract (month from t1.date_1)) = (extract (month from t2.date_2)) and (extract (year from t1.date_1)) = (extract (year from t2.date_2))) and t1.date_1 like '%-16'; **Old query:** SELECT COUNT(DISTINCT UNIQUE_ID) A FROM TABLE1 where DATE_1 like '%-16' and UNIQUE_ID in (SELECT DISTINCT UNIQUE_ID FROM TABLE2 where DATE_2 like '%-16');
Instead of taking count, try selecting the records (select * from) and compare output from both queries and see what's not matching? [Edit] Also, use the approach suggested by /u/maggikpunkt so you won't have to extract month and year individually. 
If you're a logistics company, you can do route optimization on the cities table instead of aggregating address every time. Also, zipcode tables to lookup cities.
Beginner in SQL here. I started about six months ago because the organization I work for had a database we could query for one of our largest (if not the largest) systems we have, but had been left completely unused. I started studying the schema, what information it contained, what it did not, etc. Eventually I got access and started running simple queries. Since then I have incorporated a dozen or so scripts into our workflows in addition to dozens more that I built either for experimental purposes, as one offs, or just for fun. On my next job hunt, I will say I am a beginner. If the interviewers ask me about it I'll say something like the above. I can describe the structure of my queries, what they return, and anything else they ask. Whether they consider me a beginner (I definitely am right now) or not is up to them. But I am confident I will live up to expectations while remaining honest.
I was about to spout off about normalization as well until I read the rest of your post. I suppose the better solution would be to have users each get address, city, state(if applicable), country, etc fields, right?
I had someone apply for a Junior DBA/SQL Developer role once, she was about 40 years old, first question I asked was can you write on the white board a SELECT statement for me from a table called Customers. She didn't even know where to start, clearly had never seen SQL before. The recruiter who sent her to us got a pretty good talking to.
&gt; But you still need to know how to carry your own weight. Can't spend 3 days on stackoverflow figuring how to use joins in queries lol. If you're spending 3 days on stack overflow looking up joins, it is definitely time to consider a new career! :P
Yeah, I was in the process of doing that but wasn't seeing the disconnect at first. I think I've figured it out. My initial query was only matching records that both had dates of 2016, because I didn't take the month in to account unless I was checking one month at a time with that query. The new query is what I *actually* want to see, which is all records for 2016 that match both the month *and* the year. Thank you both for your help... this one was giving me more trouble than it probably should have.
Yep, thats exactly why i mentioned it. You can imagine a date like a number and if you truncate the decimal portion you basically round it.
Dynamic SQL