It may be your best decision and I would suggest you to practice using Strata Scratch ([www.stratascratch.com](https://www.stratascratch.com)) the best for practicing SQL. The questions come from technical interviews taken from companies so all the questions are relevant to working on a job. You can also post for SQL help on their subreddit r/stratascratch. I found it really helpful when I was practicing for my SQL interviews.
Learning SQL is a fantastic decision. I practiced using strata scratch ([www.stratascratch.com](https://www.stratascratch.com)). They have datasets pre-loaded with questions and answers you can practice with. They source their questions from technical interviews from companies so I found it helpful to use for interview practice. They also have a subreddit r/StrataScratch where you can ask questions if you need help with SQL. Otherwise, I found datacamp useful too but it's more expensive.
I like Strata Scratch ([www.stratascratch.com](https://www.stratascratch.com)) the best for practicing SQL. The questions come from technical interviews taken from companies so all the questions are relevant to working on a job. You can also post for SQL help on their subreddit r/stratascratch. I found it really helpful when I was practicing for my SQL interviews. I also like sqlzoo, datacamp, hackerrank, and leetcode. Tried them all and out of those 4, I think leetcode was the most helpful because they also have a discussion board to get help from others. Datacamp was cool for very specific niches.
I practiced using strata scratch ([www.stratascratch.com](https://www.stratascratch.com)) and would suggest you too. They have datasets pre-loaded with questions and answers you can practice with. They source their questions from technical interviews from companies so I found it helpful to use for interview practice. They also have a subreddit r/StrataScratch where you can ask questions if you need help with SQL. Otherwise, datacamp is also useful but it's more expensive. &amp;#x200B;
Run each of your sub selects alone and verify that they work independently of the main query and return what you expect. Your second query, for example, has filtering in the select clause that should be in the where clause. 
It's better to aggregate before joining. SELECT us.customer , us.spend_us , us.spend_us*c.EUR AS Spend_EU , us.spend_us*c.CND AS Spend_CND FROM (SELECT SUM(CASE WHEN xfc.currency = 'EUR' THEN xfc.usdsfc ELSE 0 END) AS EUR , SUM(CASE WHEN xfc.currency = 'CND' THEN xfc.usdsfc ELSE 0 END) AS CND FROM table_with_currency_and_USDXFC xfc) c CROSS JOIN table_with_cust_and_spend_us us;
You can union the two together
Thank you for sending this. I will work on it in the evening. Its interessting to find out how they formulate these questions in an interview.
It's our job to burn our cycles. In this example here we have a large Tableau workbook which has several million rows, and several dozen columns, where one of the columns is the [SPEND_US], which is calculated by taking the domestic currency of the transaction and converting it into US dollars based on rates given to us by the business. This works fine, and there is no issue. The issue is that the business would like to see the visualization and have the ability to convert those US dollars into 9 other currencies. Our business has transactions in dozens, and dozens of currencies, but these 9 represent the more 'standard' ones that our global business partners will use internally. Presently in Tableau we are "joining" to a currency conversion table which is then, upon request, converting the USD to one of the other eight. This takes forever and the workbook is very slow. Easiest way to boost performance is to do that work in advance, in SQL, before the data goes up to Tableau server. Whether it does or does not break normal form is really irrelevant to me here. This logic is being saved as a view in SQL, so it isn't going to take up much space, and Tableau can spin letting it do the work and suck the data up at its leisure.
When I first read this I thought you called me, "sugar," and I thought, "wow, our relationship has really progressed."
mysql workbench should work on mariadb. this might be of use as well: https://mariadb.com/kb/en/library/monyog-sqlyog/
OPENQUERY must use a string constant, so no variables or dynamic sql. You can dynamically create the sql in a prior step, but not inside a CROSS APPLY. You're best bet for performance is if you can pull the linked server records into a local temp table, that you can index. I'd expect a Products lookup should be reasonable to pull everything, but sometimes filtering the linked server pull is required.
Thanks for the reply! &amp;#x200B; \&gt; You are basically looking at a Triangle relationship. Retailer to product Retailer to crumb Crumb to product This allows you to start with any one subject and move to another or all. Having a table for crumbs, retailers, and products that are unique and creating relationships between them in a separate entity is probably what you are looking for &amp;#x200B; Since a particular crumb trail can be used by many products, does this mean I should create an intersection table? For example: PRODUCT |ID|Name|Country of Origin| |:-|:-|:-| |1|Samsung Televsion|Korea| |2|LG Television|Korea| &amp;#x200B; PRODUCT\_CRUMBS |PRODUCT\_ID|CRUMB\_ID| |:-|:-| |1|2| |2|1| &amp;#x200B; CRUMBS |ID|Crumb\_ID|Position|Crumb| |:-|:-|:-|:-| |1|1|1|Home Entertainment| |2|1|2|Televisions| |3|2|1|Electronics| |4|2|2|Home Entertainment| |5|2|3|Televisions| &amp;#x200B; And then I would do something similar with RETAILER?
I have a pdf if more complicated questions if you want me to send it across?
SQL Server Data Tools has a schema compare mode which will also create a state-based migration script. I don't know if it does data compares. Red Gate SQLCompare can do schema compares &amp; generate change scripts, and Red Gate also has a data compare tool. SentryOne's Pragmaticworks DBAxPress can do both schema and data compares (I'm trialing this as soon as my schedule frees up (HA!)) and generate scripts as well.
There are tools like RedGate SQL Compare available that can show differences between databases, but I don't know if that is the best tool for deploying to prod. It kinda sounds like you're doing development in Test, which makes it a lot more difficult to ensure reproducibility of your deployment process.
&gt;Whether it does or does not break normal form is really irrelevant to me here. In that case, dynamic SQL solution should work. I'm just saying be mindful of the drawbacks.
Thanks. I'll try the temp table route.
I will look into doing this. &amp;#x200B; I do agree, it is helpful to learn and practice having an creating a mind where you think about these problems. Programming in general requires this kind of thought process and I want to improve on that aspect.
Agreed. This is much more efficient to execute.
Almost. Sounds like you are doing transactions or something. Let CRUMBS be a unique list not associated with anything. (Dont have televisions twice) Let PRODUCTS be a unique list not associated with anything Let RETAILERS be a unique list not associated with anything Let TRANSACTIONS be a header record, possibly related to retailer Let TRANSACTION_Details contain an intersection between TRANSACTION, PRODUCT, CRUMB and contain a sequence. What I am recommending isnt even fully normalized. Try and think about what you are processing. Maybe its not a transaction.... but.. the item you are splitting up (file, table, w/e) is a concept. It needs a table. The properties of this concept (retailer,product,crumb) also need their own tables.
You could make another table containing the maximum possible boolean values for each date and join sales to that instead of your original campaign table. To create the table you join your campaign table to a table of all dates where date between campaign start and stop (you could also use a table of integers and a dateadd to do this), group by product id, taking the max of each boolean. You'd also want to create an index on date. 
&gt;Let CRUMBS be a unique list not associated with anything. (Dont have televisions twice) &gt; &gt;... &gt; &gt;Let TRANSACTION\_Details contain an intersection between TRANSACTION, PRODUCT, CRUMB \*\*and contain a sequence.\*\* Do you mean a crumb sequence here? I'm not sure how I can create a sequence which is atomic. &amp;#x200B;
Yes. Sorry. I meant your position. It is atomic in the case that the sequence is the _detail's extended property of the Transaction header table, and the relationships represent the rest of the data. - normalizing farther In my world we take it one step further, where there is only one transaction to X per row. You have to add a unique number to every table (that's never in another table, but the same for every record in that table) Then you have a header Transaction Then you have TransactionReference which points to Transaction and has 3 values. This Specificdatasetnumber I explained above, the primary key of that table, and a sequence. Then you get the same sequence for multiple items, but they are separated by the SpecificDataSetNumber and PK. Example Transaction (SpecificDataSetNumber, pk) 7262, 2 Crumbs (SpecificDataSetNumber, pk, value) 887, 1, television 887, 2, ps4 Product (SpecificDataSetNumber, pk, value) 998, 1, something 998, 2, else TransactionReference(transactionid, SpecificDataSetNumber, RowId, sequence) 2, 887, 1, 1 2, 998, 1, 1 2, 887, 2 , 2 
Yeah, we are doing development in Test and the biggest pain point is moving changes up to Production. A previous company I worked for had really great in-house data diff tools that allowed us to push things up but we are doing things manually here. I want to find a “cheap” solution to build out a proof of concept. 
Depends on how you define "cheap", but Redgate SQL Compare is around $500, I think. That's a lot less than paying for employee time dealing with all the problems you're facing, and can even be sold to management as a long term "check" tool for other processes: "We can use it to verify our build results match expectations against fresh production copies" and "we can cut back on missing modifications by using it as a tool to identify potentially conflicting changes between Dev and Test".
SSDT. You code your entire DB into a single project, which is packaged into a single unit called a DACPAC. When you deploy this DACPAC, it compares it to the target database and tries to only script needed changes. This works with existing data, barring some changes that can cause indeterminacy. e.g. Adding a non-nullable column with no default. Additionally, it can detect 'drift,' where objects have changed since the last deployment, allowing you to fail for the sake of maintaining consistency. Every job I've worked in with SQLServer uses SSDT, since it can give ETL (SSIS based) the same treatment, and be leveraged in a CI/CD pipeline. Further, it means your DB is source controlled very easily, if it wasn't before. We also leverage SSDT and tSQLt to unit test our DB during build. Best of luck. Feel free to hit me with any questions. Happy to help.
Homework?
MySQL is a product. There is one. 2 editions, many versions. ... Did your even try to understand what SQL is before posting here? Because it looks a lot like you just want a magic button you can press and "make shit work". If that's so, let me just stop you and say that you're wasting your time, and everyone else's. Until you know what to ask, just don't ask. There is no such magic button.
I struggled with understanding your question, and it would have been nice to have some staging data to examine, but I eventually came up with a few solutions: --MSSQL /*STAGE DATA*/ drop table if exists #sales drop table if exists #campaigns create table #sales ( product int , sales_dt date ) create table #campaigns ( product int , [start] date , [end] date , boolean1 bit , boolean2 bit ) insert into #sales (product, sales_dt) values (1, '2019-01-15') insert into #campaigns (product, [start], [end], boolean1, boolean2) values (1, '2019-01-01', '2019-03-01', 1, 0), (1, '2019-01-01', '2019-02-01', 0, 1), (1, '2019-01-01', '2019-01-15', 0, 0) /*QUERIES*/ --original select s.product , s.sales_dt , c.boolean1 , c.boolean2 from #sales s left join #campaigns c on c.product = s.product and s.sales_dt between c.[start] and c.[end] -- Option 1 | group by select s.product , s.sales_dt , max(cast(isnull(c.boolean1, 0) as int)) as boolean1 , max(cast(isnull(c.boolean2, 0) as int)) as boolean2 from #sales s left join #campaigns c on c.product = s.product and s.sales_dt between c.[start] and c.[end] group by s.product, s.sales_dt --Option 2 | case w/ exists select * , case when exists ( select 1 from #campaigns where boolean1 = 1 and s.sales_dt between [start] and [end] and product = s.product ) then 1 else 0 end as boolean1 , case when exists ( select 1 from #campaigns where boolean2 = 1 and s.sales_dt between [start] and [end] and product = s.product ) then 1 else 0 end as boolean2 from #sales s --Option 3 | cross apply ("Could there be option to do that on join level?") select * from #sales s cross apply ( select max(cast(isnull(boolean1, 0) as int)) boolean1 from #campaigns where s.sales_dt between [start] and [end] and product = s.product ) bool1 cross apply ( select max(cast(isnull(boolean2, 0) as int)) as boolean2 from #campaigns where s.sales_dt between [start] and [end] and product = s.product ) bool2 There could also be other solutions here, but I can't really optimize that for you since I can't see the data and you're working with.
Cross apply would typically be my go-to for this, but considering all of the functions you needed, makes me curious as to what the issue is he is trying to solve. I didn’t understand much, but you have definitely pointed him in the right direction.
because with explicit JOIN syntax it's more obvious what's going on, especially if multiple tables are involved, because the join conditions are isolated against their respective joins, whereas the old way all conditions for all joins are all mixed up together with filter conditions in one humoungous WHERE clause besides, explicit JOIN syntax is ~way~ better when specifying outer joins -- i can never remember which side of the equal sign the asterisk is supposed to go SELECT * FROM TableA , TableB WHERE t1.Name *= t2.Name is this a left outer join or right outer join? and how do you write a full outer join? 
Keeping joins separate improves readability, and is necessary for join types other than inner. 
https://stackoverflow.com/questions/1599050/will-ansi-join-vs-non-ansi-join-queries-perform-differently Read over the accepted answer. :)
Okay, let me think on this and get back to you in a bit. 
Yes that helped thanks : )
I was doing some testing and inner join is gave the correct answer to it rather than [x.id=y.id](https://x.id=y.id)....etc I do see the purpose now thanks.
I'd definitely try to get records locally first into a temp table and then index on relevant columns. Also, how many rows you have in that @main table variable? Generally table variables aren't great in SQL server and for most use cases temp tables perform better. I don't ever use them except when I need to pass rows around stored procedures.
I'm not sure how the redditors on this subreddit will respond, but what you may need are proper checklists and procedures. By procedures, I do not mean stored procedures. I mean ensuring when performing code deploys across Dev - QA - UAT - PRD, the steps are reproducible and documented. Tables are clearly defined, and loads are consistent (i.e. always loading the full file). That way, you avoid any institutional failure that occurs as a missed batch job or file input. You can immediately respond to it. 
What a specific components of ssdt other than SSIS is it you're referring to? We ci/CD with Git and Jenkins our ssis code.. but what other tools would help?
@main is up to around 500 rows. Do you think I should switch to temp table?
&gt;"where" I think I'm misunderstanding you - if you've done a bootcamp you must have used a WHERE clause? For the other flow control / parameterising / programmability questions, you're moving away from generic ISO SQL and into platform-specific syntax. Is there a specific platform you're working with currently? Also, you don't need a whole lot of that aspect of SQL for data analysis. If you've already got a statistics background I'd suggest diving into R or doing some stats/data-science Python courses. They'd leverage your existing stats knowledge much better than getting into the nuts and bolts of SQL development.
There's a stigma against 89 because older people tend to use it and they are dinosaurs who must not know any better, and 92 has to be better because it is the higher number and it's newer. I use 89-style because I prefer it. So do Oracle devs who work for directly for Oracle. Use what works for you and those who read your code. I would use 92 style joins for outer joins. 89 is quicker for everything else, at least for me. YMMV. Don't forget to downvote! 
The problem here is that someone who is trying to cheat their way into a certificate is the same kind of person who would try to cheat their way out of paying.
Well he mentioned DACPACs but there's also the SchemaCompare, which I find extremely useful.
Do your own fucking homework/project or quit the program, you don't deserve a certificate.
I do, but it won't change the performance you're getting right now. 500 is tiny and the remote connection is your bottleneck for sure.
Its a project for my 12th grade. And im asking cuz im tired of trying and cant do it. But thank you.
As said in the comment above, im tired of trying. And no, im just trying to finish my project. But thank you.
The main advantage to 92 syntax is the join and condition is a single continuous block of code. This makes complex queries much more composable, portable, and understandable. * You can comment out a join in one place instead of two * You can easily copy and paste your join logic to and from other queries * Others can move through your code linearly to understand it 
I do not suggest looping or overly-fancy use of variables in SQL for your career path. "Looping" and other procedural-type activities will vary greatly from platform to platform. That means that if you spend a lot of time learning Oracle's PLSQL procedural stuff, it will be basically wasted if you take a job/role where they're using only MySQL databases. You might want to learn how to pass variables into a given flavor of SQL script from an outside process, as that comes in handy, but don't get into setting/changing variable values on the fly during SQL execution. In general, while you're learning SQL, try to stick to standard SQL, i.e., avoid learning too much platform specific SQL that won't port well. If you know how the following things work, and know it well, you can solve nearly any SQL query: select, aggregates(sum, min, max), case, from, join(left, inner, cross), where, group by, having, logical operators(and, not, or), comparison operators(&lt;,&gt;,=,&lt;&gt;,between,in, etc.) and subqueries. All the rest is googling to solve the problem at hand. MySQL, Oracle, PostgreSQL are good to learn on as they hew pretty close to the standard. Note: MS Access has especially terrible syntax (as far as portability) and unfortunately many beginners get their intro to SQL through it. If you want to learn a procedural language, learn Python. It is great for data science and analysis, plus it is a well supported and widely used general purpose scripting/programming language. With SQL and Python you can do pretty much anything in the near horizon of your career. 
SSDT, to the best of my knowledge, includes: * CLI and DLL's for deployment * SQLServer template for VS * Integration Services template for VS * Build integration via MSBuild for these project types * DACfx framework for managing Data-tier applications (a DB) This means, on a build machine with MSBuild, you can automate the DACPAC build, as well as package it (albeit manually using NuGet and a nuspec file) for deployment elsewhere. This applies to SSIS projects (as ISPACs, but using the same DLLs) as well. SchemaCompare is great, but my favorite feature of SSDT comes when you have experience in a .NET language, like C# or PS. SSDT will do all kinds of cool things, like expose the tSQL AST for analysis (think linting for tSQL, with custom rules) and even let you enforce coding standards. I'm not super familiar with Jenkins, so I'm not sure how it goes about deployments. Assuming it allows you to write your own deployment scripts, you should be able to latch into those same DLL's and deploy DB projects in an automated fashion. Hope that helps!
If your on windows you can use power shell and task scheduler. But just about any programming language can connect to a sql dB and run your script. 
Simplest would be a shell script. There are ETL applications which can do a lot of sophisticated things, some free and some not. Pentaho, Informatica, SSIS, DataStage, etc
Yes, we use Windows. I’m wondering what’s the simplest one click process like. So far I have a sql file that I call on Heidisql. That creates a table, loads data, remove invalid data, then put the valid data onto another table.
Can I do this shell script on Windows? Can it download a file from a targeted website, like 6gb size, unzip it, rename the file, then call on a sql script that loads data from text file, creates tables, filter out invalid data? How would I tell the SQL script what file to load data from? I’ve been using LOAD DATA LOCAL INFILE command with a set file name. What ETL windows program do you recommend?
SSIS would work 
In addition to what's been mentioned, using the explicit join syntax also makes it easier to understand what the where clause is doing to filter queries. You can have complicated subqueries in the where clause for example. It'll make it much harder to figure out what's going on in the where clause if you have a bunch of unnecessary filters in there that could be accomplished by joins.
Use whatever tools you'd like for the external processing. Once the data is ready to be loaded, use SSIS to accomplish the rest. No sense in reinventing the wheel.
SQL Server? Are you using an edition that has the SQL Agent? The typical way to do this is via a SQL Agent job.
Mariadb
What SQL do you use? Microsoft SQL Server? If so, what version?
Mariadb
Depending on what your ETL needs are, SQL and PowerShell may be enough- although I imagine Python would also work. You can use PowerShell to download files to a directory and rename them. From your above comment it sounds like your sql script can already manipulate the data the way you want, so you can have Powershell download the files and execute the sql script. Then, you can use Task Scheduler which should already be on your computer to run the Powershell script whenever you want. See here for how to download files with PowerShell: https://www.addictivetips.com/windows-tips/download-files-from-powershell-windows-10/ You can google how to do the rest but it's fairly straightforward.
You might want to look into pentaho. You can setup a process flow, much like I do with Microsoft sql Server, sql Server jobs, and SSIS. It should allow you to download files and transform the data as needed before loading it into your mariadb tables. 
Try /r/ETL As far as what you're asking yes it is all possible. You'll either have to roll up your sleeves and figure out how to make Powershell do your bidding, or download Pentaho data Integration Community Edition (free) to do it. For running it regularly you'd use Windows scheduler if its on your own computer. I suggest running it on a server though so it doesn't stop running if your computer is turned off. 
I’ll start tomorrow. I would love to make progress. Been limited because I have to ask IT to do every little thing.
I just realized this is Oracle, so I may not be much help, but try changing out OVER(ORDER BY to OVER(PARTITION BY ID ORDER BY for all occurrences and giving it a shot.
Whatever the mariadb equivalent of a “awl job” is, use that. SQL jobs are meant explicitly for execution at certain intervals or times.
What’s awl?
I don't understand what you're asking. Can you provide a diagram of the tables and what you want to pull from them
If you add @start_date to the where statement in you SQL query It will automatically add the parapet to the report. SELECT * FROM your_table_name WHERE abc.termination_date BETWEEN @Start_Date AND @End_Date 
I only need to pull the number of rows in each of the tables and use the data and join it with those from the first table.
I get: Lookup Error ORA-00936: missing expression 
 Can you post your SQL Query you’re using? 
&gt; each row has a corresponding table. tbh you just need to burn down the entire schema and any hardware that it has ever touched. This isn't just wrong, it's a grave affront to the entire field of database design.
I messaged you.
Just get a job where it's entry level SQL stuff you use day in, day out, and you'll learn more than you ever dreamed. Loops, static variables, I've never needed in my 7 years as analyst at several firms. You could aspire to be a SQL or ETL developer if desired, to get some deep knowledge of stored procedures and whatnot, and you might pick up some more advanced stuff that most people in data never learn
I know, but it was done that way. Can you help? The only way I know is do a get statement and take the tablename and do a foreach and do another get statement which will result in x+y GET requests, which is 500+500, because I have 500 rows and 500 tables, each rows has 1 table.
Updated, thanks!
Also, how would you have done it, each table contains all of the transactions for a given product (row). &amp;#x200B; Should there have been a table called transaction with an id called productid and a table called product with an id called id that's connected to the productid?
What database platform are you using?
Yeah, that's about right: two tables with a shared product ID.
How would you do a count in that situation? And do you need a join?
It would be simple as: SELECT product, count(*) FROM product p INNER JOIN transaction t on p.product_id = t.product_id
Depends on the DB and OS technology. In terms of MSSQL/Windows I've used powershell, batch and VB through task scheduler when I was working with SQL Express. With higher versions I use agent in combination with the above or just directly against built on stores procedures. You could also use SSIS but I've never really been a fan. 
mysql
MariaDB's job scheduler? https://mariadb.com/kb/en/library/events/
MariaDB's job scheduler? https://mariadb.com/kb/en/library/events/
I'm not a mysql expert, but this might possibly work SELECT p.name, p.price, p.ID, p.tablename, t.table_rows as Count FROM products p INNER JOIN (SELECT table_name, table_rows FROM INFORMATION_SCHEMA.TABLES) t on t.table_name = p.tablename 
What kind of field is the date? An actual date, or an int, string, etc? Declare @maxDate int; Select @maxDate = (select max(MYDATECOLUMN) from MYTABLE); Select * from MYTABLE where date = @maxDate; Or you could just do: Select * from MYTABLE where date = (select max(MYDATECOLUMN) from MYTABLE) Idk anything about the technologies you are talking about though. Just sql. 
Paste your current statement. 
wow thanks!
Did it work?
Yeah I think INFORMATION_SCHEMA.TABLES will be key here - that will give you a place to get total rows per table from.
its a string
here's the where clause: where snapshotdatehour = '2019022223' then it updates to be 2019022323 etc. in the next day, i wanted to find a way how to not need to manually change the date every time i run it
What version of Oracle? Also there are several session and db parameters to adjust how the optimizer behaves. Just take a second and think about your situation. You state that you are new to something and then immediately complain about the thing you don't know. The CBO is an impressive piece of software but it can only use the information it had available.
Are you gathering table and index statistics? The stats inform the cost part of the “cost based optimizer”
&gt;What version of Oracle? 12C. Not looking for help, I've got a solution. I'm just wondering whether the comparitive experience is just me, or a genuine area where MS SQL is better than Oracle.
So you're using SSRS with an Oracle database then I'm assuming? Is your Data Set calling a stored procedure in your SSRS project or is it just inline SQL?
As the picture shows, I'm having some trouble getting onto another line. I kinda want the system to execute the query (and fail and tell me there's a syntax error), so I can try type it in again. But for some reason... it's not :/ &amp;#x200B; Only been doing this for about 30 minutes, so I'm still very very new to this. &amp;#x200B; FYI The multiple enteries of Claire was just a test to see if the Auto Increment function would work: What I did: CREATE TABLE student ( student\_id INT AUTO\_INCREMENT, name VARCHAR (20), major VARCHAR (20) DEFAULT 'Undecided', PRIMARY KEY(student\_id) ); &amp;#x200B; Any help would be greatly appreciated and thank you for reading :D &amp;#x200B;
Which version of 12c? The optimizer for 12.1.0.2 is notorious for the adaptive features causing issues. Generally best to disable it in non olap environments, in my experience
Ah, that might explain a whole lot - it is in fact 12.1.0.2.0. 
Oracle, use job scheduler. I don’t have much professional experience with other DB’s, but I assume most have something to make this happen. If not, use chron and run a bash script to login to the DB and yada yada. 
Would it be possible to also download a large text file from a website and load data from there too as part of the automatic data load process?
Yeah, try disabling optimizer_adaptive_features and see if that works. You can also disabling using a hint in the SQL in question
Also, play with the materialize and inline hints for your CTEs as well
I use Visual Studio and am using Toad Data Point - so yes? &amp;#x200B; its just a view, there may be a procedure behind the view.
Thanks a ton! Optimizer_adaptive_features improved it hugely, and I think the materialise hint is the blindingly obvious thing that I'm missing - almost all of my solutions have involved hand-materialising subqueries into tables. 
It just depends on what type of SQL you are using and if you can get server access so the job isn't dependant on your computer being up. You said your company is using windows, so if you can get task scheduled or a job scheduled on the SQL server if you are using microsoft SQL server it's easy. Assuming you wrote this script you should fins it not too hard to build something in powershell or a SSIS package. If the servers are some unix variation you want it to be added to the cron tasks and and it needs to be a bash script. What you want, assuming you are on a all microsoft stack is some ssisuser role on a small server if you can justify how much time completely automating the task is worth you are working in the right direction. &amp;#x200B; &amp;#x200B;
SQL agent probably the easiest way, but also powershell used with task scheduler
I've learned something new. Have an upvote. I've been writing bash script with cron for years. This can be kinda frustrating especially coming from a sql server background. 
Look up kettle. It may help. 
This is the real answer. Pentaho/Kettle can be a great tool to use. 
Oh, I use dynamic SQL quite a bit, but here for this case it seems like just having 9 left joins is the most simple way to accomplish the task.
How long does it take to set the variable to the date formatted max of a string to date for your data? If it's negligible, that's all you need to do. SELECT DATE_FORMAT(MAX(STR_TO_DATE(blah.Date,'%Y%m%d%H')),'%Y%m%d%H') FROM blah INTO @MyVariable; SELECT blah FROM blah WHERE blah.Date = @MyVariable 
CURDATE() date formatted?
Clode the midding `'` then `;`
Assuming you mean ISA as in IS-A subtype relationship in OO (if not, please clarify), then there are a few ways, but I would typically do it like this: One table is your "base" type, with all the common columns, and then you have a separate table with the columns of your subtype, which has the same primary key as the base table (known as a subtype table). This will often include a "type" column in the base table to determine which subtype it belongs to, and you may also use this and the key as a composite foreign key between the two. Alternatively, you can include all the columns of all the subtypes in one table, and leave them NULL when they are inapplicable (I'm not a great fan of this approach). 
Talend -&gt; Export job to JAR -&gt; Windows Task Scheduler Python + Pandas|Numpy library -&gt; Windows Task Scheduler / AWS Lambda (if you want to do this in cloud) Eg - Setup a windows machine which would run 24/7 and windows task schedule will trigger python script / JAR file.
&gt; This would give you exactly the output you describe above, but you have to know each currency ahead of time and update if you add more. I actually really like this approach and find it very elegant.
Thanks for the support! &amp;#x200B; As the main table has about 800k rows and the campaigns has about 375M rows, i wanted to find as low cost query as possible.
Hi, thank you, i will try them out, the 3 option won't play for me, cause i'm using vertica, but that's my mistake, didn't let you guys know about the db. What else should i have shared in the OP.
So first you need to come up with a list of sale items (SalePrice * Quantity), e.g.: select item, saleprice * quantity from table Then you need to wrap it into a subquery to find the max value. You can use MAX() or ROW_NUMBER() to find the solution. This is pretty basic.
Thank you, but as there are 375M rows in campagin table this won't play too well i think, i even tried, failed cause memory got full :D
that's the part i'm confuse about, i'm not sure how it would look like
I have that, but I don't see how to come up with name associated with it: select MAX(SalePrice\*Quantity) as LargestItemSale from SaleItem
I don't mean to be a dick, but this is clearly a homework question, and if you aren't sure how it would look then you aren't paying attention in class, or your teacher sucks. It's probably you not paying attention. This is a very basic concept as it relates to using sub-queries. I don't mind helping you, and I come here for help all the time, but this is something you can learn about with a simple Google. select something from ( select *, max() something from ( select * from table ) x ) y
no worries, Yes it is one question that I am blocking on i am not ashamed to admit it, you wouldn't believe me if i tell you that my professor isn't good at SQL and makes the class very confusing, in fact half of the class does not even attend his lecture because they are lost even more, I'm a front row student, I participate and all, he even admitted that he would struggle as it is his first time teaching...
You want to do something like this: select max() as LargestItemSale from ( select saleprice*quantity ) x Before you can utilize the max() function you need to do the math.
No need to be ashamed. Sometimes teachers suck, and this is a good place to learn. But it's March which means you're midway through a semester and if you're only just not covering basic sub-queries then it means you're taking a 101 class. If your professor is that bad, then that's a problem. On the other hand, you should be able to figure this out with a simple Google on how sub-queries work. You need to do the math before you can use the MAX(). At a minimum it is partially your fault and partially your teachers fault. At a maximum it is all your fault. Again, not trying to be a dick. Happy to help.
oh that was a slip of writing, silly me! Yeah I know, but it is kind of a problem as there are places where I have to run a similar query for 50 similar features. There I need to know how to do looping and other things. You will say that do it in python or R or some other language like scala,julia etc, but sometimes people want to have these arguments done in sql platform then I get stuck. That's why I asked about how to loop use variables and statics and etc 
Hey is That book good for fairly beginners? Caught my eye because the guy is also a geologist thought that was interesting
No problem 👍 I'm a big fan of DBMS job schedulers. The rich, query-able metadata available for reporting and job monitoring is nice. Its personally one of the features I look for first when evaluating new databases. 
Maybe you want to try a GUI SQL client like [HeidiSQL](https://www.heidisql.com/)?
&gt; tbh you just need to burn down the entire schema and any hardware that it has ever touched. This isn't just wrong, it's a grave affront to the entire field of database design. upvote for this 
You can use windows scheduler
Oh I'm using POPSQL, as my GUI SQL. I'm curious whether there's any merit in doing it into the command line promt? compared to the GUI? &amp;#x200B; Thank you for replying btw :)
Can it also download a file that constantly changes its name, such as the filename starts with the name of the month? 
Honestly, I have been doing SQL development for over 3 years now, having to write scripts that do what you mentioned,sub queries, pivoting, etc... My go to is always google. Asking my question and seeing results from sites like StackOverflow or DBA Stack Exchange is extremely helpful. Same with sites like SQL Server Central and such...
Oh ohkk. Yeah even I use the same online resources but I am trying to prepare for an interview hence I thought of taking advise on the web!
No merit other than you're taking a class and the instructor wants you to use a CLI.
RDBMS?
I am using Toad Data Point. I believe it is RDBMS.
Toad data point is and IDE available for a number of RDBMS. Oracle, SQL Server, MySQL, PostgreSQL?
I use visual studio to prepare my reports &amp;#x200B; I use TDP/Oracle to write the scripts.
Understood. RDBMS references the Database Engine. I’m going to assume Oracle here. If so, then you’ll want to create a new column using the To_Char function. https://docs.oracle.com/cd/B19306_01/server.102/b14200/functions180.htm
Generally, getting your SQL chops up isn’t a case of just reading an article or something like that. It’s more something that is built over a number of years from having encountered numerous problems in a live environment and being under time pressure to solve them. In terms of general learning, Itzik Ben-Gan’s ‘T-SQL Querying’ is good for the actual DDL side of things, I’d recommend reading CJ Date’s ‘An Introduction to Database Systems’ and dabble with Joe Celko’s books (the ‘Puzzles’ books are good for scenario-based stuff). Trying to bluff these sort of things for an interview isn’t massively recommended. If you’re presented with a problem and lack the knowledge of the requisite techniques or database architecture, it’s quite easy to be found out and fall on your ass. Best of luck, though!
Yeah, it's more a book on strategy than a cookbook on advanced solutions, some knowledge of sql and how it works is required, but just about anyone who has the basics can benefit from it.
Yeah Thank you for the sarcasm but I have been using sql for 4-5 years. I was just looking for resources where I can revise and refine my skills. I did get a sql cookbook by O'Reilly which I hope will serve the purpose. When you spend so much of your time and energy in writing a comment, try to be constructive next time and a little less bitchy!
Most technology skills are relative, so I think you could put SQL on your resume as a skill assuming you can perform basic tasks with SQL. Just don't lie about your ability (much) in your interviews. Likewise, if they decide to give you a SQL test, you'd probably want to be able to pass it. I've been in interviews where I failed the SQL test hard, and it was a super excruciating and cringeful experience. That's why you constantly interview though, you do it to get better. 
the frustrating part is that it doesn't seem very acceptible to say you'd google for similar issues and try to solve from there. They want you to know exactly how to do it off the top of your head. I failed a sql test in a recent interview for this reason. The questions were pretty simple, I just hadn't done anything similar in my current role recently and didn't know off the top of my head. Would have taken me 2 minutes of googling to solve the questions i failed. 
Yeah Thank you for the sarcasm but I have been using sql for 4-5 years. I was just looking for resources where I can revise and refine my skills. I did get a sql cookbook by O'Reilly which I hope will serve the purpose. When you spend so much of your time and energy in writing a comment, try to be constructive next time and a little less bitchy!
Oh ohkk. Yeah even I use the same online resources but I am trying to prepare for an interview hence I thought of taking advise from the web!
This works but when my user exports this it is going to not be as sortable as I would like. Is it possible to keep the date variable unit/whatever you call it? &amp;#x200B; instead of a character value it is a date value?
DATE datatype values are only stored in one way.
I hadn’t meant to convey sarcasm or bitchiness - apologies if that is how it came across. I fail to see what parts weren’t constructive - the books I recommended aren’t light, introductory reading - they cover everything you wish to brush up on and more. Date’s book is particularly heavy going, despite its title. Honestly, wasn’t trying to undermine you - I’ve just come across so many people that consider SQL as ‘easy’ because they understand the syntax of ‘SELECT’ and think they can master it in a weekend...I’ve been using SQL pretty much daily for about 15 years and get tripped up by something every other day! So apologies, may have assumed you were more of a novice than you are but still tried to help. The cookbooks from O’Reilly are definitely a good resource (have a few in different languages), so that’s money well-spent there. In terms of an online resource to play with, this may be of use: https://www.windowfunctions.com Again, tried to be helpful; sorry if it came across badly!
Thank you for your help! &amp;#x200B; If thats the answer thats the answer. &amp;#x200B; Is it possible to do this with convert/cast and have more luck or is this what I should be using? 
The importance of knowing how to ask the right questions is as valuable as knowing the right answer sometimes. Thing is, if you’re attuned to a particular variant, you can be caught up on the simplest of things that differ from the ANSI (such as T-SQL using ‘TOP’ and MySQL using ‘LIMIT’, for example). Like you say, a two-minute Google search can put you on the right track!
Sorting is your issue. I imagine your data looks like this when you sort? 1 10 11 12 13 ... 19 2 20 21 If so, there is no way around it unless you leave it as a date and sort by that. You can also do a To_CHAR(employee.hire_date, ‘YYYYMMDD’) It will be sortable, but won’t look as aesthetically pleasing.
exactly this. Recruiters look for keywords, you want it on there. Then when you get in the interview you talk about how much you actually know and it is either enough or it isn't. 
I'm not familiar with MariaDB, but a quick look at the documentation revealed [Events](https://mariadb.com/kb/en/library/events/) which can be scheduled. That's probably what you want but you should also be familiar with the [limitations](https://mariadb.com/kb/en/library/event-limitations/) However, for DL from a source outside the database you'll need another tool, VBS, Python or Powershell for example, combined with Windows task scheduler
[Here's a 10 year old blog post](https://sqlblog.org/2009/10/08/bad-habits-to-kick-using-old-style-joins) that provides some good examples and a detailed why. 
I made a thing! I have a 11x17 PDF for printing if anyone is interested.
I’m interested!
Thanks for that suggestion. I switch it to a temp table, and then really evaluated my indexed. Cut run time by 25%. I haven't even started on the linked server part yet, so I should see some big gains there also.
I got around this by putting the original field in but not including it in the visual presentation. &amp;#x200B; Thanks for your help! I spent silly time trying to figure out the date format!
Huh, just realized that I never use exclusive joins.
You're using a flavour of SQL I'm not familiar with, but subtracting 1 day \` - 1 DAY \` from NOW() could certainly put you into the past.
....I know that. I'm wondering why is the first row with the date 2019-04-01 appearing even though I'm only going back by an interval of one day.
It's because of your `&gt;=` in your criteria. Do you want LESS THAN instead `&lt;=`? NOW() = 2019-03-08 NOW() - 1 Day = 2019-03-07 2019-04-01 &gt;= NOW - 1 == TRUE VS 2019-04-01 &lt;= NOW - 1 == FALSE 
I am interested!
You can set granularity of the output (1 row per a combination of...) by using "group by"
Put it down if your comfortable answering random sql questions. When to use FULL OUTER INNER JOINS, How to see long running transactions, how to find unused indexes, When to use a specific ISOLATION LEVEL or when to use a specific recovery model. If you are comfortable answering those basic interview questions for SQL, then yes, do it. Anytime someone sees SQL on your skill list, they will assume you're a long bearded DBA wearing a wizard hat and a wand so be ready to answer any specific DBA / SQL related common questions. GL
I love that you added the exclusive/inclusive aspect. Its never covered in SQL tutorials, but putting something in the where clause vs. the join claise on left joins makes a huge difference, and a lot of people dont realize it.
I don't think INTERVAL does what you believe it does. http://sqlfiddle.com/#!9/6bbc45c/13 You probably want something closer to: SELECT * FROM t WHERE a BETWEEN DATE_SUB(NOW(), INTERVAL 1 DAY) AND NOW() 
https://drive.google.com/file/d/1lrDLax_GwyiN3nvlcHHS9Nb0wmbhoVku/view?usp=drivesdk There is the PDF.
Now you know for next time! Time well spent!
I'd put familiarity with SQL. 
Very nice...
Wow, that website is pure gold, thanks so much!
I was taught not to use them because sometimes the results are a little unpredictable, especially in complex queries.
Glad to help, brother!
https://drive.google.com/file/d/0B4NOhs9cPX2BejZWY09tS0ZhbjVOT2xCQjBWNjJtb1VVRTBr/view?usp=drivesdk The center table was not centered, now it is at least more so.
I'm using MS SQL. 
I have been working with SQL daily since 2006 and never used them either...
Me too!! Thanks!!!
There have been a few situations where it's been necessary. Left join where the right table's value is NULL. Finding entities that don't have entries in the other table. I think I used it more than full outer join. I basically never use right joins though, most of the time can just rephrase the query to make it easier to read and conveniently (most of the time) using all left joins. 
This won't work, since it'll basically be running a DISTINCT on the output set, but that won't change the logic. OP, unfortunately you've run into the oddities of querying SQL, in that a CASE statement is evaluated on a row-by-row basis. but you have multiple rows. There are a couple of options you can take here, but you really shouldn't solve this in the SELECT part of your statement. If you want to do so, you can replace your statement with... CASE WHEN EXISTS(SELECT ComputerID FROM dbo.YourComputerTable yct WHERE yct.ComputerID = p.ComputerID AND p.PolicyCompliance NOT IN ('HasPolicy', 'NotNeeded')) THEN 'Compliant' ELSE 'Non-Compliant' END This will have performance implications, in that the subselect there will run for each row being output, but you may not care for a simple report like this, and that depends on your row count anyway. If you want to dig deeper and rebuild your query, you can do that, just bear in mind that some statements, in some places, (like CASE in your return column list) are evaluated at the row level and applied only after results are already generated.
I usually write them as subqueries. "NOT IN" with a subquery can be called an anti-join in execution plans with SQL Server, but that term isn't in any syntax.
Nice. Just last week, I was playing with these circles in Paint. Your presentation is very clear and useful.
Where is CROSS JOIN?
/facepalm you obviously need to write a very slightly more involved aggregate function
Right, I'd personally probably jam all of this into an APPLY somewhere and let it rip, but we only saw a microcosm of the overall query, so there's only so much a person can recommend.
[removed]
Cross join is best join. Maximum data!!
... that's not it. In his particular case, simply taking max() of his case expression, in more involved situation there are other things need to be done.
Also when i do this in my first code example (second block of code of my post): HAVING overall_score &gt; (SELECT AVG(overall_score) FROM table1) I get that the table doesn't exist or isn't defined, which is false, it exists I even do a join on it
Group By does work, but it's less than half of the puzzle. Exists in case is a great solution for the other part. SELECT p.ComputerID , CASE WHEN EXISTS( SELECT ComputerID FROM dbo.YourComputerTable yct WHERE yct.ComputerID = p.ComputerID AND p.PolicyCompliance = 'MissingPolicy' ) THEN 'Non-Compliant' ELSE 'Compliant' END FROM dbo.YourComputerTable p GROUP BY p.ComputerID The two key components here are 1) a method to get down to a single row per computerID and 2) a method to define how to condense multiple rows values into a single value. For 1) GROUP BY can work, DISTINCT can work, or starting from a different table (like the one in which ComputerID is defined) could be an option. For 2) an aggregate is the most common tool to pair with GROUP BY, but isn't really appropriate here. EXISTS in CASE is a great tool in this context. Logically it's working almost like a custom aggregate (although under the hood things are quite different).
If you're just looking for a list of computerIDs that have any amount of non-compliance, you could do something like: SELECT ComputerID, SUM(CASE WHEN p.PolicyCompliance IN ('HasPolicy','NotNeeded') THEN 0 ELSE 1 END) as NonCompliantCount FROM YourTableName GROUP BY ComputerID This should give you one row per computerID and the number of policies that are non-compliant for that computer.
So, technical sidenote: results of the queries we use in the "FROM" clause are technically called "derived tables" (and that's a feature what you are using in your statements already). Informally, this usage does get often called 'subquery' too. results of the queries used everywhere - these are actually called subqueries. And you generally have 1) 'scalar' subqueries - returning a single column of a specific type and no more than 1 row back. These can be used in place of an expression of the same type (e.g. 1+(select max(order_id) from orders) ). Various Sql engines have specific restrictions on where the scalar subqueries can be used. 2) subqueries returning more than one row and/or more than one column. You can use these with "IN", "NOT IN", "EXISTS" conditions. &amp;nbsp; Soo, given that you need "the average score of all production houses" and that can be calculated as (SELECT AVG(movie.imdb_score) AS overall_score FROM Movie) and you can use this expression as a number, can you modify your very first statement to get your result?
I’ve only used it a handful of times (in fact I think the last time I did was off the back of a reddit post on the very same subject), but unless I’m looking at a really small set of data, I seem to run into spool issues... which is a shame really as it did exactly what I needed it to do on some work I had recently when looking at a single account on my data set, when trying to run it against a few hundred thousand accounts for several years of data to build a base data set for further analysis however, it literally killed my machine.... are there any ways to improve the efficiency of them, or is it just the nature of the beast?
Thank you very much! I appreciate the technical sidenote, help me search the problem even more narrowly. So if I understand I have a subquery which use a scalar subquery which use a derived table. Now as for your select thank you it works I'm very happy and i understand why thanks to you. However, AVG(movie.imdb_score) = 2,17 and AVG(overall_score) = 5.75 Which I find weird. My foreign key to production_house.id_prod in my MOVIE table is not nullable. imdb_score though is nullable. I just did some research and it seems that AVG automatically exclude null value? I do feel like the question ask for the comparison with the average of average (5.75), here it is: &gt; List the production house_production_id, with average scores higher than the &gt; total average scores of all houses 
It's helpful for troubleshooting. Say your main table has 1,000 records and you join to another and the output query (with a join) has 999 records but you didn't expect to lose any data. Showing what's excluded will make it really easy to troubleshoot why that record isn't being joined to the other table. 
Are the left/right exclusive diagrams backwards?
My only wish would be that the labels ("LEFT INCLUSIVE") were the actual syntax ("LEFT OUTER JOIN" / "LEFT JOIN")
Most of aggregate functions ignore nulls, I'm not sure I understand the concern. I generally think that an average of averages is non-nonsensical statistic, but if you feel otherwise - find that and use it in your comparison. 
https://blog.jooq.org/2016/07/05/say-no-to-venn-diagrams-when-explaining-joins/
I didn't feel like writing LEFT JOIN WHERE NULL B or some title like that for some of them, so I got creative. LEFT OUTER JOIN and LEFT JOIN are the same after all.
me neither. Not sure why I should though.
Kimbal (dimensional): https://www.amazon.com/Data-Warehouse-Toolkit-Complete-Dimensional/dp/0471200247/ref=pd_sim_14_3/141-6970840-8546854?_encoding=UTF8&amp;pd_rd_i=0471200247&amp;pd_rd_r=3f2aca2d-4226-11e9-a824-df4daa8f2bd1&amp;pd_rd_w=fSKlT&amp;pd_rd_wg=9BcWm&amp;pf_rd_p=90485860-83e9-4fd9-b838-b28a9b7fda30&amp;pf_rd_r=GXZ46BJJM675PM83BR71&amp;psc=1&amp;refRID=GXZ46BJJM675PM83BR71 https://www.amazon.com/Data-Warehouse-Lifecycle-Toolkit-2nd/dp/0470149779/ref=pd_sim_14_2/141-6970840-8546854?_encoding=UTF8&amp;pd_rd_i=0470149779&amp;pd_rd_r=3f2aca2d-4226-11e9-a824-df4daa8f2bd1&amp;pd_rd_w=fSKlT&amp;pd_rd_wg=9BcWm&amp;pf_rd_p=90485860-83e9-4fd9-b838-b28a9b7fda30&amp;pf_rd_r=GXZ46BJJM675PM83BR71&amp;psc=1&amp;refRID=GXZ46BJJM675PM83BR71 https://www.amazon.com/Data-Warehouse-ETL-Toolkit-Techniques-Extracting/dp/0764567578/ref=pd_sim_14_4/141-6970840-8546854?_encoding=UTF8&amp;pd_rd_i=0764567578&amp;pd_rd_r=3f2aca2d-4226-11e9-a824-df4daa8f2bd1&amp;pd_rd_w=fSKlT&amp;pd_rd_wg=9BcWm&amp;pf_rd_p=90485860-83e9-4fd9-b838-b28a9b7fda30&amp;pf_rd_r=GXZ46BJJM675PM83BR71&amp;psc=1&amp;refRID=GXZ46BJJM675PM83BR71 Inmon (3NF+timeinvariant): https://www.amazon.com/Building-Data-Warehouse-W-Inmon/dp/0764599445 Data vault (agile ingest+data mart): https://www.amazon.com/Building-Scalable-Data-Warehouse-Vault/dp/0128025107/ref=sr_1_1?crid=3CLX12LHYQ7L4&amp;keywords=data+vault+2.0&amp;qid=1552106923&amp;s=books&amp;sprefix=data+vau%2Cstripbooks%2C120&amp;sr=1-1 
Thank you, great explanation. But... what is the point of the left inclusive? Would not be the same result if you just select all from a and ignore b?
your results could indicate that you're joining on a wrong field or on not on all fields that are needed. try posting your data structures and some sample data?
Awww that makes sense!
Thanks for looking. It's using Oracle. I am pretty sure it's joined together correctly (although could be wrong) since I can get the results I am looking for - but the date on date wrangling across the two tables needs some logic (I think) which might be solved by some kind of grouping and logic. I will post on Monday if I haven't resolved since I don't have access to the data source right now.
SELECT site, username, last name, logindate From users LEFT JOIN sessions on users.site = sessions.site WHERE logindate = (select max(logindate) from users); 
SELECT Sales.ID, Sales.Date, Sales.Amount, Sales.Qty, Sales.ItemID, Items.Name, Items.UnitPrice FROM Sales LEFT JOIN Items on Items.ItemID = Sales.ItemID Assume that there are some sales orders that have a NULL item ID because a sale is made with something not in the item list. Inner join would essentially filter both tables on rows that have matching ItemIDs in both tables, but through the above join, you're getting all sales regardless of whether there is a matching item.
Well, now I just want to remake this poster. Thx...
How the heck do you format SQL in the mobile client. XD
 SELECT c.site , c.username , c.lastname , MAX(s.login_date) AS login_date FROM client AS c INNER JOIN session AS s ON s.site = c.site AND s.username = c.username GROUP BY c.site , c.username 
start by removing the dangling comma in front of the closing parenthesis
Can we remove the right join? There’s no sense teaching people such things exist. 
"==" isn't a thing in SQL. Try just "="
Error when I changed it too. &amp;#x200B; Update Movies set Director = "John Lasseter" where Director = "El Directore" &amp;#x200B; Thank you tho. &amp;#x200B; &amp;#x200B; &amp;#x200B;
Are you using double quotes? Because they are for column names. Use single quotes for text values.
Do a subquery and group by on the outside. Boom. 
If you're getting an error message, please include the text of the error
Hey man I appreciate the help. Do you mind elaborating a little bit?
I can’t copy and paste it on mobile, but do this: SELECT [column names, don’t use *] FROM ( [copy/paste code here]) GROUP BY [column you want to group]
 constantly interview - nice point!
For real. The server really does tell you exactly what is wrong.
querying with transact-sql at edX.org and DataCamp.com SQL courses may be a good start - helped me to get fully prepared to work with sql on daily basis. 
If I do this: `SELECT EmpID, JOB, ITEM, PITEM, PROJ, [02/24/2019],[02/25/2019],[02/26/2019],[02/27/2019],[02/28/2019],[03/01/2019],[03/02/2019], NOTE` `FROM ProcessTable` `GROUP BY JOB, ITEM, PITEM` I get this error: Column 'ProcessTable.EmpID' is invalid in the select list because it is not contained in either an aggregate function or the GROUP BY clause. &amp;#x200B; &amp;#x200B;
Thank you!!!
If you were to put your datediff calculation in your select statement, you’d likely see that they are all coming back negative because birthday comes before inactive date. Swap those two in your datediff and you should get the results you want.
Any automated process that generates SQL would have a hard time with letting go of right joins. Right Joins are just stupid Left Joins though.
Interesting... I’ve never run into anything that makes right joins except for lazy humans. 
I had just tried that and saw the negatives! This seems to work: SELECT UPR00100.FRSTNAME as 'First Name', UPR00100.LASTNAME as 'Last Name', UPR00102.Phone1 as 'Phone', UPR00100.BRTHDATE as 'Birthday', UPR00100.DEMPINAC as 'Inactive Date', upr00100.INACTIVE as 'Inactive', CAST(DATEDIFF(DD, UPR00100.BRTHDATE, UPR00100.DEMPINAC) /365.25 AS INT) as [Age at Temination] FROM UPR00100 LEFT JOIN UPR00102 ON UPR00102.EMPLOYID = UPR00100.EMPLOYID WHERE UPR00100.EMPLOYID like '%E20%' AND UPR00100.INACTIVE = 1 ORDER BY BRTHDATE I wasn't certain it was 100% so I spot checked with: DECLARE @DateOfBirth date = '1999-07-11 00:00:00.000' DECLARE @CurrentDate date = '2018-06-15 00:00:00.000' SELECT (YEAR(@CurrentDate) * 10000 + MONTH(@CurrentDate) * 100 + DAY(@CurrentDate) - YEAR(@DateOfBirth) * 10000 - MONTH(@DateOfBirth) * 100 - DAY(@DateOfBirth) ) / 10000 Not sure this is accounting for leap years though. I have one person for whom this might be a factor.
Are you ending the statement with a semicolon? Some of the online practice things are pretty strict about that.
Go full cartesian... you mad man
I work in RPA, and we automatically generate sql and I am both succesful at my trade and never had a robot build a right join
Better practice is to get the identity column load them to a #table update from a join to that list then you can validate the updates.
Nope they should be handled with care
You should add... Cross join, outer and cross apply, intersect, except, and union. Maybe natural just in case. I never see a complete list, only partials like outer inner and full.
You're trying to set the Director column value(s) to a column named John Lasseter, where the Director column value(s) equals the value(s) in column named El Directore. Try using single quotes instead.
&gt;the date needs to be like 3/6/2019 which I don't know how to implement in the DATE. Because you don't. Dates should be stored as a `DATE` data type, which is not text. It's formatted as text for displaying to the user, but that's done by the client and the formatting is dependent upon the user's locale and preference settings. Is 3/6/2019 March 6th or June 3rd? The interpretation is based upon the reader's own assumptions. If you store as a text data type, an assumption is built into the database design - not a good place to be. ISO8601 is the one true date/time format anyway.
Sorry for a late reply, but when I put this on reddit I noticed it and removed it but it didnt work. I forgot to edit the post and remove it.
SAP Business Objects is a different monster.
Lazy posts should get no comments or even deleted. They piss me off just as much as lazy tickets at my job. “I get an error tell us how long until it’s fixed”. Give me the fucking error at minimum!
Strongly disagree. Anything that could go wrong with OP's query \[when correctly written\] would apply to this solution as well, and with this solution you'd have to account for locking, race conditions, and deadlocks yourself. &amp;#x200B;
Ah, I see. In that case, trying passing whichever value is more important for both joins. &gt; LEFT JOIN tblPhonebook ON phone = (select COALESCE(msgRecipient, msgSender) from tblMsgs ...) as thread
should be &amp;#x200B; PRIMARY KEY ( messageNum , username )
Because users and messages share a one-to-many relationship, I do not think that you should have the sends table. Instead, simply use the user id as the foreign key to the messages table.
So if a user is deleted you want all messages that user sent to be deleted? If so you would need to redesign your tables so that messages are related to a user.
Or you could make sends the parent table for messages CREATE TABLE User( username varchar(30), password varchar(15), email varchar(50), PRIMARY KEY (username)); CREATE TABLE Message( messageNum int, content varchar(750), FOREIGN KEY (messageNum) REFERENCES sentMessages(messageNum) ON DELETE CASCADE, PRIMARY KEY (messageNum)); CREATE TABLE sentMessages ( messageNum int, username varchar(30), PRIMARY KEY (messageNum), FOREIGN KEY (username) REFERENCES User(username)) ON DELETE CASCADE ON UPDATE CASCADE; &amp;#x200B; &amp;#x200B;
As far as the leap year issue with the division by 365.25 method goes, I never liked the inherent issues with precision. Even the examples I've seen around the net that use hours and division by some other number. I have been using the following code block. It's convoluted, but it's always accurate. Basically, it tests to see if the difference in years between DoB and the comparison date, when added to the comparison date, yields a date greater than the comparison date, and if so, subtract 1 from years. SELECT DATEDIFF(YEAR, [date_of_birth], [comparison_date]) - CASE WHEN DATEDIFF(DAY, [comparison_date], DATEADD(YEAR, DATEDIFF(YEAR, [date_of_birth], [comparison_date]), [date_of_birth])) &gt; 0 THEN 1 ELSE 0 END
Ok so of that's the case, which relationships get their own table and which relationships can be represented without new tables?
Okay, thats great. Thanks! But I've also just realised i've got another table called users &amp;#x200B; users site, username, enabled A, client1, Y A, client2, N A, client3, Y B, client1, Y I wan't to include only rows that are marked "Y" Can you help?
&gt; which relationships get their own table many-to-many &gt; and which relationships can be represented without new tables? one-to-many
so if you delete a user, their messages become complete orphans, messages that weren't sent by anyone? what would be the benefit of such a design?
And how about a one-to-one relationship?
No, checkout the design, if you delete a user it will delete the sentMessage and cascade down to the message table.
Sorry I see, I am assuming a message can't exist without being sent. I don't know the meaning of message. Is it a predefined set of messages? Can they be created without sending them? These questions would need to be answered by op. Anyways with my design it wouldn't be possible to have a message not sent by a user, it would violate the FKs
single table is what i would do but that's just me
You could do something like this: INNER JOIN users as U ON U.username = C.username Then have a WHERE clause before your GROUP BY: &amp;#x200B; WHERE U.enabled = "Y"
just do another join SELECT c.site , c.username , c.lastname , MAX(s.login_date) AS login_date FROM client AS c INNER JOIN users AS u ON u.site = c.site AND u.username = c.username AND u.enables = 'Y' LEFT OUTER JOIN session AS s ON s.site = c.site AND s.username = c.username GROUP BY c.site , c.username 
Yes and foreign key would have to be unique 
I know his sounds silly, but, after recreating the cluster did you disable and re-enable Always-On? In theory, you should have taken everything down and started them one by one. That's all I have. :/ 
Congrats!! I’ve seen the crazy amazing stuff you do on youtube and I’m a huge fan!! If this test was a nightmare for you, what hope do the rest of us have!?
Succeed where others dont. I scheduled the exam for today. Friday night I got the ref book, read what I wanted to, passed. I dont really study. I'm not a test taker. I learn by doing. I have been messaging someone who took 5 sql certs in 5 months, whose tech background was HTML dev. Study and be confident. Ps (I paid for the confidence one, test and 2 retakes, 230$). Used today as a live practice test because of where the subject matter was going. I recommend doing that so the first one isnt so stressful. 
with a one-to-one relationship stored as a single table, there is no foreign key
Now that you've reached the MCSA milestone, what's next?
MCSE
I was checking out Elric's youtube and it looks like he is already into BI Dev 70-767 b/c his channel has some rich Data Quality Solutions content. 
Which book are you taking about?
The green books! https://m.barnesandnoble.com/w/exam-ref-70-762-developing-sql-databases-louis-davidson/1124307367?ean=9781509304912 
Mcse and silver partner (AI in june)
=) Yep, that's the next one. Dax mdx and cubes. Msce.
100%
Was thinking about MCT also. Being a trainer on the side. Youtube helps me get that practice in.
Do you happen to know any book which just focuses on writing sql queries ? I mean these are also queries but I am looking for something that can taste my sql skills to retrieve data. Ajd what's your YouTube channel?
https://m.youtube.com/c/elricsims-dataarchitect Get dev edition. Follow the 70761 series. It's just about getting in there and doing something. You like fortnite? Find dataset downloads, load guns and stats, calc upgrade costs. Just do something in sql that you like... that's not sql.
Shout out to r/datasets helped me get some random stuff just to play around with.
Not gonna lie, this took me a few reads, so pardon my idiotity. Can you edit the function to have a WHERE NOT EXISTS clause for your 'checking' parameters? 
Taking it Monday. I bought the triple-shot deal, so if I fail I have two more chances. I'd like to do it once and get it over with, but this one seems a lot harder than 761. Wish me luck!
Good luck! Know your transactions and index optimizations! I did the same. 3 for the price of 1.5. Treated the first one like a practice test.
We should all start a leaderboard and do a race to see who can clear the most certifications by the end of the year ;) 
Passed the 762 last year and I do agree it was definitely more challenging than the 761 imo. So rewarding after though! Congrats!
That sounds mentally painful, expensive, and stressful. Let's do it. Get Microsoft to put up a 100000 cash prize.
When your heart stops and your eyes adjust and see PASS... the you sink into your chair and say... ima go do something just for me. Yakisoba.
&gt; WHERE NOT EXISTS Sorry I knew it is confusing... Just curious, what would `WHERE NOT EXISTS` do in this case? By saying checking parameters you probably meant `Networkid` I assume?
Here is a rough draft idea: Assuming you have no instances of crossing the year boundary (2019Q3-2020Q2), you can create a table QUARTERS with the quarters, start dt, end dt, num_days_in_qtr (90/91/92). This might be easily done using the system calendar, check your db sys cal. Then JOIN your campaign info to that table ON: campaign_start &lt;= qtr_end AND campaign_end &gt;= qtr_start so that each quarter that the campaign falls into has a row. Then in the SELECT : CASE when campaign_start between qtr_start and qtr_end then NUM_DAYS(qtr_end-campaign_start) When campaign_end between qtr_start and qtr_end then NUM_DAYS( campaign_end -qtr_start) Else num_days_in_qtr END as campaign_days Now you should have the num days per quarter for a campaign. Then you can do a WINDOW expression to get the proportion of time the campaign is in that quarter like campaign_days / sum(qtr_days) over (partition by campaign) This is a fast sketch, might be missing somethings. And also didn’t figure out your db implementation of num_days or window. But you can figure it out , good luck!
I think I misunderstood your question at first. I thought you wanted to only call networkids which have not been called regardless of the date range (which is where the WHERE NOT EIXSTS comes into play). Now it seems you want to see a list of networkid's (+ date ranges) which were/are passed often. Are you concerned with pulling resources away from other objects in the database? You're wanting to hit a cache if the already existing networkid and date range exists (and potentially narrow inside the result set) versus constantly querying per each request? How many transactions/s for this particular process is happening (and how many records pulled back on average)? A stored proc would help as they can be cached, but blah blah "are tricky with parameter sniffing". I need a little more understanding of your data set. &amp;#x200B;
Sorry for the confusion. Yeah you are right about the "cache". About your questions: No this is the only TVF I'm pulling out from, at least for now as the requirement is only on cancellation. Number of transactions: In current db it ranges from a 300-500K to 5-10 millions depending on Networkid for a whole year. Clients may query for a full record (i.e. at least 3 years) Yeah I can write a stored procedure too, if it's more beneficial. The data source is a bit complicated. I only have read permission, but BI gave our team a separate db that we can do anything about it, so I stored the TVF there and call other databases. The cancellation data is in a single table, one line for one cancellation event. Count\_Cancel is usually 1 but somehow a few of them are more than 1 so I'm ignoring them. ## Id | Date | NetworkId | Platform | Count_Cancel | And there are a few other columns that I can JOIN other tables to grab e.g. the traffic source, original transaction, e.g. So the TVF just consumes the parameters and, say, output a table of cancellation of a specific Network within the given date range. I believe it would be better to do any cleaning/aggregation in Python so I use Pandas for that and Dash for the dashboard. The concern is: I'm not sure if this kind of "caching" is meaningful, it seems to be as a query that hits multiple years can really take more than 15 seconds.
The problem with a TVF is it's (to my knowledge) not cached by any RDBMS whereas a stored proc will be cached. Perhaps a re-write of the TVF into a sproc and testing its speed compared to the TVF would be a fantastic start. I don't know if [redis](https://redis.io/topics/quickstart). can help if a TVF is the only way, but, definitely look into redis. Index networkid and date (which I am sure you do). However, you may be able to take it to an orthodox way of indexing such as (forgive me for this xD) &gt;&gt; &gt; create index ix\_date\_2018 on dbo.myTable ( &gt; &gt; \[date\] asc &gt; &gt; ) where \[date\] &gt;= '2018-01-01 00:00:00.000' and \[date\] &lt;= '2018-12-31 00:00:00.000' Lastly, is speed a concern (15s) or are you trying to squeeze every which bit out of it? Is blocking/deadlocking occurring? &amp;#x200B;
IMO, It depends on if you need to need a full batch to be successful versus the latter or if you truly need to handle the exception at your discretion. 
Then have a retest at the beginning of the year to see how much truly stuck with j00. :P 
Its weird, I learn and maintain information better under stress. I know I'll have to take transition and non existent exams to maintain my mcsa/e /partnership. Helps when you're doing it at work 5 days a week too.
What do you mean by silver eligible developer?
This link shows you what a company can receive for 2k a year. (For silver, 5 visual studio licenses, 365, support, and more) https://partner.microsoft.com/en-us/membership/core-benefits This is what you have to do, so the company you work for can do that. (Competancy requirements drop down.. 761,762 473). Note that 473 will be retired in june and replaced with something else. https://partner.microsoft.com/en-us/membership/data-platform-competency
After reading through the problem a couple of times I have a small idea of what you're doing. &amp;#x200B; Not sure if you know since your example didn't specify if you did; CASE statements can have multiple WHEN statements within them to cover all scenarios you're looking for. Additionally, you can do nested CASE statements for a bit of "flow" to the rule / logic structure you want.
It depends on the direction in which the constraints were established. I'm assuming your user table with their ID contains the FOREIGN KEY referencing the key in the other table? If so, that should be fine. If the relationship is the other way around, referential integrity would be lost and you would probably get an error if you tried. &amp;#x200B; Assuming the keys were set up the first way I mentioned, you could create a TRIGGER to perform the desired operations on the key table ON DELETE on a record in the user table.
I actually didn't recreate the cluster, just the Always-on. However, I rebooted the servers the other night, and the maintenance plan ran fine tonight. Guess it was really that simple. Couldn't try it sooner due to needing the databases available during the week.
Took this about a month ago and passed with about a 75 or 80%. I used the green book and the kaplan online prep. I was only scoring about 50-60% on the practice tests but felt I knew material well enough to pass. I didn't think the green book was as good as others I have used for other tests. I sort of disagree about reading till you can recite. There were lots of things that I elected to learn well enough to scrape by. Practically, I found the index stuff most helpful in my day to day life, as my understanding prior wasn't as deep as I would like. I'm deferring the last test until it is replaced since there isn't a book and a lot of the study material is out dated. Two colleagues both passed but neither was confident going in. I've been a MCP/MCSA for about 7 or 8 years but have never had all the required tests for MCSE. It would be nice to achieve that.
Your normalized example is the way to go, I don't even understand your single column idea. How are you going to join them? Based on row number??
I've read it 3 times, and still don't know what you're doing. Next time try to put source data, a description of the operation you want, and an example output from that source data. But anyway, whenever you have long case statements with tons of "WHEN" clauses that refer to the same columns, you might want to just use a JOIN with a temporary table or with a CTE.
Let's say, for argument's sake, that you've decided that `Category` is an `ncharvarchar(100)` field. Your transaction table is now 200 bytes larger *for each record*. Not just 200 bytes per record on disk, but in memory on your database server and your application server or client application (something like this has actually led to out of memory problems on my production instance that has 768GB of RAM). With the properly normalized approach, you're adding only 4 bytes (if it's a regular `int`) to each record. But you can probably get by with a `smallint` (2 bytes) or even `tinyint` (1 byte). What if you need to change the spelling of a category? With the "expected" approach, you update a single record in a table and you're done. With your proposed approach, you have to update *two* tables, and one of those updates might be quite large. And you've now duplicated data - this is counter to the goals of normalization. Or what if you need to do internationalization/localization, and need different names entirely for each category? There are also situations where foreign keys, assuming they're also indexed, may improve query performance. &gt; What bothers me is that ID column in `Categories` table doesn't add any value, and simple `select * from transactions` shows some cryptic numbers instead of meaningful labels In a production application, you wouldn't be running `select * from transactions` in the first place. If you don't need the `Category` column, you just don't query for it. Then you don't see those "cryptic numbers" that are bothering you. This is the challenge with school projects - they often don't see the reality of how databases and applications have to work in production scenarios. If the joins are that much of a burden, write views to perform them for you.
No, OP is going to join based on the text of the `Category` column on `transactions`.
&gt; But I don't like it. First of all, it's more work, Doing things properly often is more work at the start, but less overall compared to trying to clean up messes made by taking shortcuts in the beginning. &gt; What bothers me is that ID column in Categories table doesn't add any value It lets you rename things without needing to also modify every pointing table. Also sooner or later you'll want to be able to soft-delete (keep them, but mark inactive) things like categories. &gt; I have to play around with joins, which isn't that much difficult, but it slows me down and grows inconvenient quickly as more such tables and cryptic foreign keys are added. Use SQL VIEWs. They're great for quick debugging, and actual production usage. I rarely run read queries directly on tables these days, I set up useful VIEWs with all the relevant info joined for most read operations. &gt; Perhaps simply because you quickly came up with additional columns to add to Categories table (or any other table really) Yep, that's another reason to do things properly. I've found that after a while, even simple linking tables you thought would only ever need two columns often need additional metadata added. So for this reason I also recommend against multi-column PRIMARY KEYs. 
The table cannot be normalized. The category is not really an issue because it adds value to the entry, because each transaction must have a category. If the table had another column called CategoryIndustry with a 1 - 1 mapping to Category (e.g Food - Groceries), that can be normalized since the industry column would be repeated unnecessarily every time. That could be normalized by using a lookup table with CategoryId, Category and CategoryIndustry. Concerning you’re example, I’d recommend using a lookup table since it’s cleaner to work with and to join on INT’s rather than VARCHAR’s. Also, consider that the categories names might change, so if “Food” needs to be changed to “Eatables”, you’d have to go update all the tables where you have a Category column, instead of just updating the lookup table. Hope it helps. 
Normalization is optimal for tables that are frequently modified. So, if you expect that someone will often change names of categories, you should normalize them - create a lookup table. More likely, you will simply add rows to your data table and then query it often. Denormalization is optimal for reading. You do not want to do unnecessary joins. However, size of the data table may become issue. If you are having very large number of rows, and your categories are long, you might be wasting lot of space on them. (Instead of having 2(date)+4(amount)+1(surrogate key) per record you might have 2+4+25(average length of category). In that case you are wasting 80% of your table. If size is not an issue. ok, but if it is, you could create view on top of your data and lookup table and you could have best of both worlds.
this should be really easy to test
\&gt; But this is bad design, right? &amp;#x200B; no, it isn't &amp;#x200B; replacing a natural key with a surrogate key and forcing a table join in order to translate the integer into the key is \*not\*\* what normalization means &amp;#x200B; you might want to use an integer for other reasons, but in this particular example it is needless over-design 
&gt; What if you need to change the spelling of a category? With the properly normalized approach, you update a single record in a table and you're done. With your proposed approach, you have to update &gt; two &gt; tables, and one of those updates might be quite large. &gt; And you've now duplicated data - this is counter to the goals of normalization. i am afraid you, too, have misunderstood normalization using the category name as a foreign key with ON UPDATE CASCADE means you get just what you say you want -- changing a category name with one update and you're done 
&gt; More likely, you will simply add rows to your data table and then query it often. Denormalization is optimal for reading. You do not want to do unnecessary joins. upvote for this
Oh great.
Ah, I misread that portion (no wonder). With Always-On if you ever have to do any type of maintenance I would always suggest a start/stop. Curious to know why such a critical aspect of the environment couldn't have been restarted during production? Seems like a failover + reboot xNth scenario would have worked with least impact. BTW welcome to the Accidental DBA Role :P 
Not everyone uses `on update cascade`
I had strong suspicion there will be performance difference, just couldn't say how much. All your other points are valid and interesting too. Thank you, this was exactly kind of reply I was hoping for :)
&gt; If the table had another column called CategoryIndustry with a 1 - 1 mapping to Category (e.g Food - Groceries), that can be normalized since the industry column would be repeated unnecessarily every time. Thank you for bringing this up. I feel like it's something I knew, but forgot about. 
My knowledge is basic, but I agree. Normalisation is essentially future proofing the data. You might not need to reference the category now but there's a good chance you will do later in life. I haven't seen the about how byte size could effect performance, very interesting.
The size of the table as alinroc mentioned (storing a small number in each transaction record takes less space/better query performance than a large text field). Also it aids in updating the descriptions of the categories (update one record in your dimension table instead of updating every record in your transaction table which has that value). And it's better for scalability.... If an actual company will be using this DB for their work, do it the right way and create a separate dimension table. If you're doing this for a school project and you'll lose points if the schema isn't normalized, do it right. If you're doing it for a personal project and you would rather no have to join to a bunch of dimension tables, do it however you want. Basically, if you're building the DB for yourself, what what you want. If you're building it for someone else, do it the right way.
T-SQL fundamentals by Itzuk Ben Ganan. Fantastic. https://www.amazon.co.uk/T-SQL-Fundamentals-Itzik-Ben-Gan/dp/150930200X/ref=mp_s_a_1_1?adgrpid=54211224718&amp;hvadid=259075482711&amp;hvdev=m&amp;hvlocphy=9046782&amp;hvnetw=g&amp;hvpos=1t1&amp;hvqmt=e&amp;hvrand=13718182345747793088&amp;hvtargid=aud-614677023298%3Akwd-300368370891&amp;keywords=t-sql+fundamentals&amp;qid=1552227846&amp;s=gateway&amp;sr=8-1&amp;tag=hydrukspg-21
Like everything it depends. Leaving the real values in there makes retrieval easier and that may be ok. The data space used is negligible, disk is cheap. About the single column table, you could do that but once a value is entered you are no longer allowed to change it that's a rule for a primary key. So if you entered Food and wanted it to be groceries later then too bad, you need to add a category, or do stuff like dropping keys and updating data. Try telling your boss I need to kick the users out of the system so I can drop keys and change the data because I designed this wrong... The issues with having real values in the table: First, if you need to rename a category you now have an update anomaly since you need to update many rows. Second issue, and this is a big one, if you want to query this by a particular category you have no guarantee they are all spelled the same. This design takes away your data integrity. You WILL have multiple spellings for each category within your data at some point. Adding a category table with an ID as the pk is the best route. It's all about scalability and flexibility. It's true your query will be more complex since you need to add the joins but this gives you the data integrity and flexibility you'll need, and that's the benefit. 
Thanks a lot for the help! Makes sense, I'll try out Stored Procedure on Monday. I cannot check the other stuffs (index/query plan) though, as I only have read permission on that database (we have a separate placeholder database with all permissions so I put the TVF there). 15s is just average as sometimes it goes up to 20-25s, which could be an issue but I'll discuss with the client to see what she thinks.
sure, you make great points but using a surrogate key is ~still~ not what normalization is about i am just defending a term that is horribly misused in the industry
I cant explain the how in a few sentences... but I have about 1000 'category' tables that come in groups of 3 or 4. I call them Taxonomy. You have to normalize the data itself, not just the table. I would not have Food or Travel as a Transaction taxonomy. I would have something like FINANCIAL,EXPENDITURE,COMMON (3 degrees, Broad to specific, Class-&gt;Family-&gt;Type) FINANCIAL,EXPENDITURE,RARE (new tires) FINANCIAL,INCOME,COMMON (paycheck) FINANCIAL,INCOME,RARE (lottery) CONTAINER,COMMON,COMMON These represent the transaction. In a transaction table you can create a Container record and some common expenditure records and then relate the container to the expenditures in a relationship table (yes also has a taxonomy for direct, descendant, ancestor relationship types) to build a receipt (transaction 2 transaction relationship) I would follow this pattern for Food also. FoodClass,FoodFamily,FoodType,Food. A container could represent a recipe (food 2 food relationship), other types for actual food. You could create a relationship type of (transaction to food) and relate your line items accordingly, but everything is isolated so you dont need to SELECT all this other information about food. It gets really abstract from here (as if it wasn't already), but you can give measurements (tsp, cup, USD, YEN, Quantity) to food and transaction. When put together you can be very specific: Under a transaction container, $5 was spent on 4oz of Cinnamon... or $5 spices, or $5 common, Or... just $5 food. TLDR, when used in a system, categories are almost like dimensions that allow for varying degrees of intelligence. But you must go beyond table normalization (1,2,3,dknf) and into data normalization. It is through the use of these taxonomys that I am able to build complex json, purely data driven calculations, parameterized language translations, and so much more in the system itself... that the intelligence is just an added bonus. 
This is very interesting. I won't follow these ideas all the way in this simple project of mine, as it obviously leads to very complex tables (though offering interesting possibilities!). Though I was thinking about dissecting categories into more categories (heh), and your answer gives me lot to think about. Thank you very much! 
If You mean to run it for the previous month then it would be something in the lines of extract(month from datefield) = extract(month from getdate()) -1. That is for redshift. For MySQL there is a month function so month(datefield) = month(curdate)-1. Please make sure you also have the year in your where because it will pull from all years you have data for otherwise
How do i make sure the year is in place?
I think their entire thought process was around foreign keys and data validation, rather than optimization. 
WHERE MONTH(DATE) = MONTH(CURDATE)-1 AND YEAR(DATE)=YEAR(CURDATE). 
That a the best part. There are no complex tables. 99% of the tables look the same. Its data normalization, not normal form others are familiar with. I could do this whole thing with the same 4-5 columns in every table, except the actual relationship table.
Eh, I expressed myself poorly. I meant lots of tables with complex relationships.
Yea. It took some time to fully get an implied relationship (no actual keys together). The food 2 food, food 2 transaction, transaction 2 transaction, people to transaction, people 2 people... etc. That's all 1 table. That was easy to get once you started using it. Instead of pk to pk relationships, you add a value that represents a table. Then you have table and pk to table and pk.. any record to any record. The relationship type allows you to avoid recursion in live code, and only use recursion when a relationship changes (to update that type). The speed is all about normalizing the files properly, with the right clustered index.
Generally speaking, I love when machine tells me: "Hey, this makes no sense. You made mistake here, correct it." Strongly, statically typed languages. Data constraints. Things like that. So, you are right that I am mostly concerned about foreign keys and data validation. But of course, performance is something that has to be taken into account, and as was pointed to me in some of the answers here, the design I proposed *has* some performance issues. I am glad for these answers, although I had my own suspicion, it's nice to have it confirmed and expanded upon.
&gt; Doing things properly often is more work at the start, but less overall compared to trying to clean up messes made by taking shortcuts in the beginning. You are, of course, completely right. I was thinking more about how to do it right without doing unnecessary work, than doing it simplest way possible. So I was thinking "putting these data into separate table is good idea, but are numerical PKs needed?" (turns out yes). I don't mind doing work if it has clear benefits, just don't want to do work that's pointless.
They are called window functions. Example Select sum() over (order by year, quarter ROWS between CURRENT ROW and 1 FOLLOWING) From table No group by. This sums q1 with q2 (next row), q2 with q3... etc. Here's a video. End of video goes over range and row. https://youtu.be/XJnaRG59Jss
that's not going to work real well in December, when `MONTH(CURDATE)-1` gives **zero**
TDP?? &amp;#x200B;
True, I thought you needed a one time thing :)
what you want is WHERE a_start &gt;= /* first day of previous month */ AND a_start &lt; /* first day of current month */ depending on which database platform you're on, the date functions to determine those two values will vary placing only the column on one side of the comparison operator, and only scalar values on the other side, ensures that the conditions are **sargable** -- when you apply a function like MONTH(), they aren't
Usually for simple CRUD you don't have to worry too much about TRY / CATCH and transaction usage. You can typically do enough input sanity checks before the CRUD to avoid issues that single statements can be performed safely by the time you get there. If you have batches of CRUD, like inserts into a primary key table for a new ID, and then subsequent inserts into a table that uses that ID as a foreign key, and you want to abort the whole batch on one failure, it's best to use TRY / CATCH and a named transaction with COMMIT and ROLLBACK. There can also be a consideration for who is going to see the error, even from a simple, single CRUD op. For the application I work with at work, I don't want a user to get a raw SQL error (the application has kind of sub-standard error reporting, and it's not my department to change that). So I wrap most everything in TRY / CATCH to report "nice" error messages that they can take a screenshot of and send it to support and when it finds its way to me, it tells me everything I need to know. 
This is cool. So, I use sql 2016 and JSON. Using a table driven model, a Json key (datelessThan, dategreaterthan, networkid) would relate to the proper operator. = &gt; &lt; etc I do something similar where I dynamically nest where ins for pagination and ordering for search JSON {networkid:96,StartGT:1/1/2019,endLT:1/7/2019},{networkid:96,StartGT:1/2/2019,endLT:1/5/2019} Now you can dynamically build a filter Select id From x + @filter @filter (parsing the json) = For each element, extract column and operator from table to build networkid =96 and start and end... Where Id in (select id where 96, 1/1 and 1/7 And where id in (select id where 96 and 1/2 and 1/5) Use a covering index and you should be good.
I've heard the green book for 70-762 wasn't that great so I picked up Pro SQL Server Internals which was awesome for explaining indexing and query tuning. Congrats on passing! I have my test on Friday, currently trying to cram as much as possible and am reading through the green book again now. Also, for those looking for references, SQL Server Central's Stairway to Columnstore Indexes is a great one, and I feel like Red Gate's Simpletalk articles on a lot of the exam material for 70-762 help break things down well.
You should be able to do this: CASE WHEN ((Type\_of\_Flight = 'Not-Scheduled') AND (c.Quarter\_start = 1) and(c.quarter\_end = 4)) then (Days-Passed\_Since\_Start \* $daily\_spend) WHEN ((Type\_of\_Flight = 'Not-Scheduled') AND (c.Quarter\_start = 1) and(c.quarter\_end = 3)) then (Days-Passed\_Since\_Start \* $daily\_spend) WHEN ((Type\_of\_Flight = 'Not-Scheduled') AND (c.Quarter\_start = 1) and(c.quarter\_end = 2)) then (Days-Passed\_Since\_Start \* $daily\_spend) WHEN ((Type\_of\_Flight = 'Not-Scheduled') AND (c.Quarter\_start = 1) and(c.quarter\_end = 1)) then (Days-Passed\_Since\_Start \* $daily\_spend) END AS Whatever 
The green book got me everything except one section. Querying dm_* views, what columns they have and their purpose for waits latches locks etc. Solid resource on indexes and transactions. 
Sorry, no this would report would be a monthly report moving forward.
So just wanted to get back to you on this one, I have tried various tools and so far Azure data studio is the only thing that has worked. It takes a bit of tinkering though. `# Start by installing the deb file;` `wget` [`https://go.microsoft.com/fwlink/?linkid=2072744`](https://go.microsoft.com/fwlink/?linkid=2072744) `mv &lt;strangeNameItDowloadsWith&gt; azuredatastudio.deb` `sudo dpkg -i azuredatastudio.deb` `# Then you need to get the prerequisites` `sudo apt-get -f install` `# it now appears to work but crashed on launch because it needs 2 more libraries` `sudo apt install libxss1 # for libXss.so.1` `sudo apt install libasound2 # for libasound.so.2` `# then you can launch with` `azuredatastudio` &amp;#x200B; Hopefully that helps if anyone else has issues. &amp;#x200B; Going to keep trying with the other tools because GUI is really inconvenient.
&gt;I wrote this: and OISPV\_STAFF.A\_HIREDATE &gt;= CURRENT\_DATE - INTERVAL DAYOFMONTH(CURRENT\_DATE)-1 DAY \- INTERVAL 1 MONTH AND OISPV\_STAFF.A\_HIREDATE &lt; CURRENT\_DATE - INTERVAL DAYOFMONTH(CURRENT\_DATE)-1 DAY &amp;#x200B; This gives me a syntax error.
Found this online: https://stackoverflow.com/questions/12375888/oracle-date-function-for-the-previous-month and creation_date &gt;= add_months(trunc(sysdate,'mm'),-1) and creation_date &lt; trunc(sysdate, 'mm')
Thanks for the recommendation!
&gt;creation\_date &gt;= add\_months(trunc(sysdate,'mm'),-1) and creation\_date &lt; trunc(sysdate, 'mm') This works. &amp;#x200B; I dont understand what the add\_months does?
Adds months as the name implies but in this case adds -1
So I finally got sqlcmd working, took embarrassingly long. Apparently it's best NOT to provide a port number, because it doesn't work when I do that, it's happy with just the ip though. Very cryptic error message too, no error code or explanation, just said connection failed.
For all my efforts I cannot get this one to work, which is a shame because it looks like I could go straight to python with this. Very terse error messages that don't give many clues about how to proceed. 
Right on, and to be clear, I wasn’t criticizing. Your post was well thought out and provided concrete examples of what you wanted to do, including why. Not often we get such well though out posts here, especially from someone willing to learn. 
unfortunately my Microsoft™ CrystalBall® application is down at the moment... would you mind pasting the exact error message here? also, in case i can't identify it from the error message, please indicate which database platform you're on
Which linux distro are you using and did you try freetds yet? Have a look at this guide: https://zend18.zendesk.com/hc/en-us/articles/218197897-Configuring-a-Linux-Server-to-Connect-to-an-MSSQL-Database-Using-ODBC?mobile_site=true
No I haven't tried that, so far I was looking at pyodbc with this; [https://docs.microsoft.com/en-us/sql/connect/odbc/linux-mac/installing-the-microsoft-odbc-driver-for-sql-server?view=sql-server-2017](https://docs.microsoft.com/en-us/sql/connect/odbc/linux-mac/installing-the-microsoft-odbc-driver-for-sql-server?view=sql-server-2017). &amp;#x200B; I will look through those, perhaps pyodbc doesn't care where the odbc came from? Thanks for the links
Please don't do this.
Sorry for not being clear! I ended up using creation\_date &gt;= add\_months(trunc(sysdate,'mm'),-1) and creation\_date &lt; trunc(sysdate, 'mm') 
Add_months adds x months to the date I. Question. In this case, you are adding -1 months to the beginning of this month. https://docs.oracle.com/cd/B19306_01/server.102/b14200/functions004.htm
Well, even though I'm an accidental DBA my boss knows less about this stuff than I do. So when everything shit the bed, he went into a full panic. Like he's probably still panicking although I haven't heard from him today. So once we got everything functioning he was hesitant to do anything else lest it break again. So he wanted to wait till the end of the week after the stores closed to do anything.
sqlfiddle.com maybe?
Try sqldbm.com 
Toad Data Point IIRC
Is there a place I can create a free DB to share? The data I'm using is contained within my company. I'd love to share an example DB since that might make things way easier. 
[PostgreSQL | macOS](https://www.postgresql.org/download/macosx/) [MySQL Community Server 8 | macOS](https://dev.mysql.com/downloads/mysql/) \- If macOS download is not shown by default, select macOS from the "Select Operating System" drop-down list.
Thanks for following my weird question. The problem I have with the nested CASE is that I'm only able to declare one THEN. What I'd like to have happen is when the second CASE condition is met, it'll perform additional actions, multiple THEN scenarios. &amp;#x200B; This is not valid code but writing out what I'm trying to do. When the condition is met, perform the following calculations. define macro q1 90; define macro q2 91; define macro q3 92; define macro q4 92; CASE WHEN (c.budget_schedule2 = 'Not-Scheduled') THEN CASE WHEN (c.Quarter_start = 1) and(c.quarter_end = 4) THEN ( (Days_Passed_Since_Start * Daily_Spend_Needed ) as q1_temp // this calculate Q1 spend ($Q2 * Daily_Spend_Needed) as Q2_temp ($Q3 * Daily_Spend_Needed) as Q3_temp ($Q4 * Daily_Spend_Needed) as Q4_temp ) WHEN (c.Quarter_start = 2) and(c.quarter_end = 4) THEN ( (Days_Passed_Since_Start * Daily_Spend_Needed ) as q2_temp // this calculate Q1 spend ($Q3 * Daily_Spend_Needed) as Q3_temp ($Q4 * Daily_Spend_Needed) as Q4_temp ) &amp;#x200B; &amp;#x200B;
Thanks for following my weird question. The problem I have with the nested CASE is that I'm only able to declare one THEN. What I'd like to have happen is when the second CASE condition is met, it'll perform additional actions, multiple THEN scenarios. &amp;#x200B; This is not valid code but writing out what I'm trying to do. When the condition is met, perform the following calculations. define macro q1 90; define macro q2 91; define macro q3 92; define macro q4 92; CASE WHEN (c.budget_schedule2 = 'Not-Scheduled') THEN CASE WHEN (c.Quarter_start = 1) and(c.quarter_end = 4) THEN ( (Days_Passed_Since_Start * Daily_Spend_Needed ) as q1_temp // this calculate Q1 spend ($Q2 * Daily_Spend_Needed) as Q2_temp ($Q3 * Daily_Spend_Needed) as Q3_temp ($Q4 * Daily_Spend_Needed) as Q4_temp ) WHEN (c.Quarter_start = 2) and(c.quarter_end = 4) THEN ( (Days_Passed_Since_Start * Daily_Spend_Needed ) as q2_temp // this calculate Q2 spend ($Q3 * Daily_Spend_Needed) as Q3_temp ($Q4 * Daily_Spend_Needed) as Q4_temp ) &amp;#x200B;
Thank you, I'll try this route and will report back!
Is your db in Oracle, SQL Server, or something else? You can also have an ELSE statement at the end. Correct, you should only be able to specify one THEN per WHEN, unless you mean you can only do one THEN per whole Case statement? What syntax errors are you getting? &amp;#x200B; Feel free to PM me with more questions or clarification
My DB is in BigQuery. Hmm, since it is only one THEN per WHEN then CASE won't fit what I'm trying to do... which also eliminates IF/THEN since that is literal CASE. &amp;#x200B; I'll try and revise what I'm trying to do in a Google Sheet to try and make it easier to read, I'll PM you shortly. &amp;#x200B; Thank you! &amp;#x200B; &amp;#x200B;
MySQL and Postgres both have native ports for macOS. One or both of them might even be installed on your system already. If not, you can download installers from their respective sites (see below), or install via Homebrew. https://www.postgresql.org/download/macosx/ https://dev.mysql.com/downloads/mysql/
I just made a Google Sheet to try and show what I'm trying to do with expected results. &amp;#x200B; [https://docs.google.com/spreadsheets/d/1Dq-4VcA\_RgZlYjso79hwr4LwDnVjGuzw7Q3NK4mQIv4/edit?usp=sharing](https://docs.google.com/spreadsheets/d/1Dq-4VcA_RgZlYjso79hwr4LwDnVjGuzw7Q3NK4mQIv4/edit?usp=sharing)
http://www.sqlfiddle.com/
Pyodbc should work with freetds. Here is another guide I found: https://gist.github.com/rduplain/1293636
Your index will perform much faster with an int than with a string variable.
How're you supposed to find the true issue without access to the query plan(s)? :P Cheers m8; hope it works out!
Unfortunately, this seems more typical than atypical. :/
Yeah, what /u/AbstractSqlEngineer said too. XD 
You could install Linux in a VM and do Mysql and Postgresql that way: https://blog.macsales.com/40342-tech-tip-how-to-install-and-run-linux-on-a-mac You could also set up a free Amazon AWS account and run it "in the cloud".
Not sure if you mean that the programmers are mocking the companies, stupidly parroting the companies, or honestly saying that they want "real" big data instead of the companies' "big data."
Yes.
"We need big data. We need business intelligence solutions..." Me: "You need a couple indexes first. Like a clustered index for starters."
Lol I died reading this 
But is it web scale?
CFO says "You want to spend how much? But the current database built ad hoc, with no documentation, ten years ago still works." 
How do you get this BIG data? 
Where does this BIG data come from? 
Another redditor was able to solve this, the issue was that I used 'is' instead of '='.
A column always has a default value. If you don't specify one, it's NULL. So without a column default, ON DELETE SET DEFAULT is essentially the same as ON DELETE SET NULL.
Yes, that's possible. It's called cascading replication. [https://www.postgresql.org/docs/current/warm-standby.html#CASCADING-REPLICATION](https://www.postgresql.org/docs/current/warm-standby.html#CASCADING-REPLICATION)
That should absolutely work if you use = instead of == [https://rextester.com/DHPIC80481](https://rextester.com/DHPIC80481)
Spying on people through one gillion and two "data points" obtained from the user by hook, crook, or correlation? 
Please send your sample data to my mail id balamurali.k86@gmail.com I will help you with the design
Makes sense, thank you very much. 
AFAIK you're going to need a scripting (or non) language to read the API. Within the code you'll need to parse the requests into SQL statements for CRUD statements. 
1. Scheduled task (script/program) calls API to fetch data 2. Scheduled task parses, (optionally) formats the data 3. Scheduled task inserts data into database That's about the best you'll get here because you've not provided any real requirements. What database platform? What does the data look like? What does the API look like? How does the data need to be stored in the database? _Why_ are you moving this data from A to B? What will you be doing with the data once it's in SQL that the current home for that data doesn't allow for? How often is this running, and how much data will you be processing on each iteration? I'm sure there are lots of videos and tutorials once you have the right questions to ask. Where to start? * Decide what database platform you're using * Find out what methods there are for getting data into it - both basic inserts and bulk loading * Understand how to call the API and then transform the data accordingly * Understand the scripting environment you're working in and what facilities it has for the above tasks * Decide what the data should look like in the database &gt; I guess this question is not extremely hard When approaching people asking for help with something you know you don't know much about, please don't make statements like this. Depending on the environment and requirements, it could very well be a difficult task. Or, it may be entirely unnecessary.
Depending on the volume of data, bulk copies may be a lot better than individual `insert` statements.
Our big data is on burned CDs from the 90s which are in a file cabinet in Bob's office. 
&gt;Is naming column 2 value good practice or shall I call them voltage, current, timems? No. Use meaningful names for your columns. It doesn't cost you extra. &gt;Do I need a index? It depends on how you're using the data. Probably, but what that index is depends on usage. Are these values all coming in at the same time, and getting split into separate tables? Are these values all related to one another? If that's the case, I don't see the value in having three tables here, just write all of this to a single table with all three values represented as columns.
If you need 100% Postgres compatibility, CockroachDB does not seem to be an option: From: [https://www.cockroachlabs.com/docs/stable/porting-postgres.html](https://www.cockroachlabs.com/docs/stable/porting-postgres.html) &gt;Although CockroachDB supports PostgreSQL syntax and drivers, it does not offer exact compatibility &amp;#x200B;
&gt;Use meaningful names for your columns Thanks, I will. &gt;at the same time? They will be extracted from a Labview application once every minute &gt;split into separate tables? That's currently the plan. &gt;related to one another Each measurement comes from a system. There will be 1 to 15 systems that needs to be logged. This needs to be inserted too. &gt;to a single table &amp;#x200B; Would it look like this? create table 1 name: measurements column 1: tstz column 2: system.index (from table called system) column 3: terminal_voltage_v column 4: average_current_a column 5: ping_time_ms &amp;#x200B;
&gt; Each measurement comes from a system. There will be 1 to 15 systems that needs to be logged. This needs to be inserted too. In your original schema, there's no way to link the terminal voltage, average current, and ping time from a single measurement. You _could_ use the timestamp (or whatever `tstz` is - what did we just say about meaningful column names?) but what if you get two measurements at the same time from two different systems? You can't tell the difference anymore. If you're recording these same three fields from each system, I would do this: Table 1 (the systems you're measuring): SystemID (identity) SystemName Table 2 (measurements): SystemID MeasurementTimestamp terminal_voltage_v average_current_a ping_time_ms
Yes, you are absolutely right about losing the connection to system in my original question. This seems like the way forward. Thank you for your advice. 
&gt; Would MeasurementTimestamp or measurement_timestamp be considered good practice? That depends on the convention for your RDBMS and other systems (databases) that may be in play. Personally I loathe underscores and CamelCase or pascalCase are the preferences in the languages and systems I work with.
&gt;randomly select students for an event &gt;We want to select these 44 students based on their demographics Pick one. You need to define what you want better. E.g. 10 people. 5 different demographics with 2 people each. Do you want, 5 people 1 from each demographic? Do the percentages need to be accurate? E.g. 10% of people are in demographic A so 10% of 'randomly' selected people must be in demographic A? 
I've learned a ton from everything that I've read from Itzik Ben-Gan. Highly recommend.
I've somehow never heard or seen the term "CRUD" in reference to SQL operations. Huh... learned something today. :)
I'm dealing with an investor right now who keeps saying machine learning. About everything. It's too painful.
Legends say that the robot in the sky, known as machine learning, can generate said big data.
Hi, All. I figured this out. I had two issues. &amp;#x200B; 1. The EXEC needed to be wrapped like this &amp;#8203; INSERT INTO @Results EXEC(@SQLQuery) &amp;#x200B;
\&gt; \~\~ten\~\~ thirty years ago With this correction, you have my company
SVP: "We need better data to help us grow to $1 billion in sales." SQL Dude: "Can you give us some money to build an actual performant database, so that we may provide you with actionable insights?" SVP: "Nah."
Cloud machines that learn. I've heard about this.
This sounds to me like the perfect reason to use postgresql. You can use pg until your system gets big enough that scaling matters, then one avenue available to you is to switch to cockroach as an easy way to scale. Postgres also has a longevity and track record (ie, battle testing) that few other systems can claim 
I cant wait till I understand what this means soo I can die reading it as well. &amp;#x200B;
Does the Silver membership include free licenses to Power BI? I tried searching but couldn't find anything. 
I'm not sure to be honest.
Hue does this. You just need to make the right connections 
Is there any reason you can't repeat your case logic for each quarter? It'd be a lot of copy-pasting, and I'm not sure how BigQuery is at optimizing, but MSSQL would probably recognize the repeated logic and make it relatively efficient. I think you're making a mountain out of a mole hill. &amp;#x200B; Instead of expecting your case/when to do multiple thens (not what SQL does) - have each Column (q1\_temp, q2\_temp, etc) do the same case/when statement then your then would be the calculation. &amp;#x200B; &amp;#x200B; But....honestly...looking at your example... you only need 4 case/whens, and just return NULL for each temp quarter less than Quarter\_start. You seem to be stuck on flawed logic for how to do what you want.
r/InclusiveOr
simplified: case when schedule2 = 'not-scheduled' then case when q-start &lt;= 1 then [your q1_temp logic] else null end else null --guessing null - you don't have logic for 'scheduled' end as q1_temp ,case when schedule2 = 'not-scheduled' then case when q-start &lt;= 2 then [your q2_temp logic] else null end else null --guessing null - you don't have logic for 'scheduled' end as q2_temp ,case when schedule2 = 'not-scheduled' then case when q-start &lt;= 3 then [your q3_temp logic] else null end else null --guessing null - you don't have logic for 'scheduled' end as q3_temp ,case when schedule2 = 'not-scheduled' then case when q-start &lt;= 4 then [your q4_temp logic] else null end else null --guessing null - you don't have logic for 'scheduled' end as q4_temp might be a little more complicated if you have scheduled logic, or if you need something other than 'c.budget_schedule2'.. I'm not sure if you need that to increment as well. 
More like: &gt;SVP: "We need better data to help us grow to $1 billion in sales." &gt; &gt;SQL Dude: "Can we hire competent sales people who don't spend all day 'networking' on Faecesbook and playing table tennis?" &gt; &gt;SVP: "...." &amp;#x200B; &amp;#x200B;
.... This too!
Your understanding is wrong. &amp;#x200B; Whenever rows are inserted into/deleted from an indexed table, its index(es) are maintained automatically.
Perfect. And that's the case under any platform? For example, it's a MySQL database that sits online and I'm interfacing with the data on a platform called phpMyAdmin
Scaling is one benefit, but there are also other benefits running cockroachdb, eg. high availability.
use a CTE? &amp;#x200B; with CTE as -- your base table ( SELECT A, B FROM (SELECT DISTINCT A, B, C, D FROM Table t1 GROUP BY A, B, C, D) SELECT \* FROM CTE GROUP BY a,b &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B;
People who don’t fully understand DB architecture don’t appreciate how important the simple things, such as indexes, are to the performance aspect of a DB. And then they go wanting to do things like BI and big data processing (OLAP), when their OLTP systems aren’t even designed properly. Also, I got a “Rome wasn’t built in a day” vibe from it as well lol
Too real, man. Too. Real.
You can get ha in traditional sql databases, too. Remember before aws, when websites were down all the time, and notice that nothing ever goes down nowadays? No, me either. 
i don't know the answer to the multi-part problem, but your DISTINCT is redundant
i died. 
Sure. But it is included in cockroachdb out of the box. No one is stopping you from setting up health checking, self-healing, rebalancing, replication for postgresql. You are just reinventing the wheel, which have less features and are less maintained.
Window functions will likely be your friend here. As far as selecting random rows, I based my sample code below on the discussion at [https://stackoverflow.com/questions/191342/random-record-from-a-database-table-t-sql](https://stackoverflow.com/questions/191342/random-record-from-a-database-table-t-sql). I wasn't exactly sure what your specific requirements are and you'll probably want define that more clearly for yourself, but these options below should cover a few variations. (Breaking this code out into two separate comments because including the staging data because character limit) --MSSQL declare @top int = 44 --Option 01 - random subset, demographic agnostic select top (@top) * from #students order by newId() --Option 02 - random subset, with no more than 3 per state select top (@top) * from ( select *, row_number() over (partition by State order by newId()) as rn from #students ) x where rn &lt;= 3 order by newId() --Option 03 - random subset, with at least one per state (up to 44) and a union to supplement declare @rc int = 0 declare @1perState table ( Id int ) insert into @1perState select top (@top) Id from ( select *, row_number() over (partition by State order by newId()) as rn from #students ) x where rn = 1 order by newId() set @rc = @@rowcount select * from ( select * from #students where Id in (select Id from @1perState) union select top (@top-@rc) * from #students where Id not in (select Id from @1perState) ) x &amp;#x200B;
Thanks. This worked like a charm. 
https://archive.codeplex.com/?p=OlapPivotTableExtend will let you see the MDX query
you have a condition on the right table in the WHERE clause -- move it to the ON clause &amp;#x200B; SELECT a.table\_name , b.constraint\_name FROM information\_schema.tables a LEFT OUTER JOIN information\_schema.table\_constraints b ON b.table\_name = a.table\_name AND b.constraint\_type = 'FOREIGN KEY' WHERE a.table\_schema = 'world' 
so is that a difference between sql server and mysql? or am i just being silly
I know, I needed to shorten this portion to show as an example. I had aggregate functions as well appended to the distinct statements. I'll update that. 
I cannot think of any RDBMS that would behave differently. MySQL certainly keeps tables and their indexes in-sync. &amp;#x200B; (phpMyAdmin merely provides a frontend to the backend database server — it is the latter who implements tables and indexes.)
You're comparing the string `'PartSourceKey'` with the string `'842'`. You are not comparing the column `PartSourceKey` at all.
no, that is exactly the same in all databases it has to do with left outer joins using NULLs for column values in the right table row where the left table row has no match left outer joins work this way in every database system so let's say you have a row from `information_schema.tables` that has no match in `information_schema.table_constraints` according to the way left outer joins work, all the "b" columns for the joined result will be set to NULL however, in your query, you have a WHERE condition that says `b.constraint_type` has to be equal to 'FOREIGN KEY' well, it can't be, because it's NULL from the left outer join therefore, that row from `information_schema.tables` is dropped, as you discovered when you move that condition into the ON clause, it becomes a condition on matching the row thus, when a match isn't found, you still see that row from `information_schema.tables` in the results 
D'oh. I see that now. I feel silly. Thanks. &amp;#x200B; So, how does one compare the column value here? I originally tried putting the "select pp.part\_source\_key ... etc" statement in the case conditional, but I got an error indicating that "Cannot perform an aggregate function on an expression containing an aggregate or a subquery."
You'd join to `part_part` and reference `pp.part_source_key` in the case statement. In line selects aka correlated sub queries, like you are currently doing for `PartSourceKey`, are not typically a good way to do things. They can often lead to performance and readability problems.
Take a look at Pentaho Data Integration. Free/FOSS. GUI. Java. Designed to move data into databases
Woot! Already joined that table in a separate join, but today I learned I can join the same table on different joins. What a wonderful way to end a Monday. Thanks friend. Your help is greatly appreciated.
Company: we need to utelise 30 years of our data. Analysis, BI, trends, graphs Dev: but your data has never followed any convention, dates are stored as text... Company: DaTeS!!! YES!!! that will look great on the graphs. I’ll allocate 3 days of effort.
SVP: Get a SAS team on it. like if they knew how to covert all the data into something useful. True story.
Hahahaha.... I've heard similar. People with no data experience try to solve your data problems for. "We can't give you this answer. The data is too dirty." "Can't you just run a linear regression or something?" Lol
Take some Udemy courses. Get an entry level SQL job. Work into it. The last thing you want to do is step into a job you're unqualified technically for. Or to exaggerate your skills. They'll find out quickly, and you'll be out on the street
I have taken a beginner's SQL course and am comfortable with the basics of it. I actually have an interview for a data analyst position, but I just feel wholely underqualified for it. The position requires 50% SQL usage and I only just took a bootcamp course on it. I had a phone interview with the manager and was able to answer his SQL based questions, but I am still very unconfident about it all. 
That sounds about right. I was first hired as data analyst having zero SQL experience, so depending on the firm's needs, you may be fine. I then advanced within a year to BI analyst as SQL honestly is very easy to learn. 
I also got a vibe that's like, all those people think their hundred thousand row (or hell even a few million row) DB is "big data".
But I'm sure you have other programming experience? I have a business background and don't have any other technical experience except from using Excel. The requirements asked for 2 years of SQL server and Data mining experience which I don't have....which is why I'm so confused as to why they want me to interview...
You also can learn sql pretty fast on the fly imo. Obviously it won’t be as theoretically informed (like why tables are normalized) but you could get queries going. The stuff that really helped me with sql magic were subqueries and ctes. When I learned those a few months ago, it became much easier to do most of the data transformation stuff I wanted to do in sql (as opposed to dumping a partially transformed dataset into r or python and going from there.)
Believe it or not, only Excel is what I had. As far as experience. I had some things which looked good on a resume, such as working for an investment bank, a MBA, and some international experience
I wanted to put big data on my resume, so I started using CLOB and Varchar(max).
A great way to mitigate being uncomfortable with learning SQL is to become comfortable querying the information scheme and various sys schemes. If the company you’re working for allows you to access them, trawling through them, learning the foreign keys, etc. can go a long way towards feeling more confident with the language.
Hi there, I was wondering if you can list some entry level SQL positions. I’m on the same boat as OP. I had recently begun practicing with dummy databases. (thanks to some friends) for the past 2 months. I’m currently in healthcare. 
its SAP you'll struggle to do on your own. ERP systems are something you'll need to experience in a job using one.
I’m on the same boat as you buddy. Looking to dive into a new career path. My friends stress the importance of relational databases, SELECT/JOIN statements and aggregates. Where have you been learning and practicing? I took 2 courses of LinkedIn learning, solving on w3schools and hackerank. 
I took the course "The Complete SQL Bootcamp" on Udemy. It uses Postgres as the tool and gives you a database to practice queries on. I'm pretty comfortable with the basics after this course, so I'm trying to look for more advanced stuff. 
Gotcha. I’m going to give that boot camp a try too. What’s your professional background in? 
Read all the Celko books. Done
&gt; why does it return an error when I try to simply select the UnitPrice without an average because without the AVG, the subquery will return all UnitPrice values for that product this is a column of data, which you cannot shoehorn into a single row returned by the SELECT clause technically speaking, a subquery in the SELECT clause must be **scalar**, i.e. produce one row of one column value a subquery in the FROM clause is different, this is known as a **derived table** and can have multiple columns, multiple rows
Thank you for your response, I really appreciate it! So if the AVG is simply to keep the query scalar, would functions such as MAX, MIN, etc. work in its place or is there a specific reason AVG is used? 
This is where things become difficult. There's a huge gap IMO between learning SQL and becoming advanced. It gets pretty old learning SQL from all these various sites as they bore you to death with the beginings of SQL. It's like watching every Batman movie and being forced to see how Bruces parents die. Fuck, we get it. We've seen it over and over. Gimme the good stuff. The advanced stuff requires SPROCS, indexes, triggers and a front end app which is hard to put into a course. You want to see advanced SQL, get the SQL Server training book from Itzik Ben-Gan and he has a sample database along with labs in the 70-461 course. I highly recommend you try and learn the Microsoft Training Kit for the 70-461 or 70-764 (good luck finding a decent book on this one though) - That IMO is the best advanced SQL you will ever find. Explains every kind of JOIN with examples and specific pit falls DBAs or Devs fall into. You're not going to find anything decent besides beginner courses online and how many times can you select with an inner join before you want to strangle yourself with the keyboard. 
You could use any aggregation that would only return one value, including TOP (1), but you’ll get different results from each one, so you need to know what you want to see... lowest price the product sold for? highest price the product sold for? most recent price the product sold for? average price? weighted average price? In this case the objective was to see average unit price across all orders, so that’s why AVG was used.
Oh okay, thank you for clarifying! 
Literal conversation I had. We need to get this new big data stuff, we need Scala, we need Hadoop, we need R and we need Mongo... Ok, why though, what do we have planned that calls for it? We can’t let our technology stack stagnate! Right, but what projects are coming in that require those tools? ...
Datacamp has been my favorite site so far, thanks again for the suggestion
While not directly related to the cited technology, I started to read “SQL Performance Explained” by Marcus Winand who also has a great website (https://modern-sql.com). This book is about indexing, and it is going great. My next book to tackle is “SQL Antipatterns” by Bill Karwin from The Pragmatic Programmers series. I will link to them below. I’ve done some of the SQL Hacker Rank stuff and found that challenging and fun. https://sql-performance-explained.com https://pragprog.com/book/bksqla/sql-antipatterns 
Data camp ha some great courses. 
Sorry, I missed your last message. Have any luck? 
Damn didn’t even look at it from that perspective but wow that’s funny too lol
Learn the business processes. Spend a little time after working hours and try to parse the source query of at least one new report query every day. Find out how to see which ones get called the most and which ones the front office call learn those two set first think about how they work. If there is an ETL team (guys who move data around from the transactional databases to report data lakes and the integrated data warehouse) read what they've put out or do lunch with them sometime and find out what moves where, when and how it affects stuff. You will be fine on the SQL skills you evidently know how to learn and from there it's time effort and [ten thousand mistakes](http://thecodelesscode.com/case/100)
Google Adventure works Codedrops usually has good exercises using it. 
Many thanks. I should also add that I'm using a MacBook. I've only recently started lurking on this subreddit and I'm positive there's plenty of resources from all of you people that are contributing.
Found a free pdf online. Will be by new book to read during my commute. Thanks!!
I should caveat that adventure works is tsql the Microsoft SQL dialect and designed for sequal server management studio. Its what most shops I've been in use but Oracle and MySQL are also options. 
In Postgres you may utilize [Aggregate functions](https://www.postgresql.org/docs/9.5/functions-aggregate.html).
Safaribooksonline has free trial and most of his books
It sounds like what you need is a [recursive CTE](https://www.codeproject.com/Articles/21082/Concatenate-Field-Values-in-One-String-Using-CTE-i) with string concatenation.
I agree with the above, as I been looking for “advanced” SQL classes a year ago. I found an interesting reply on Stack Overflow and the reason you see beginner classes and not advanced, is because advanced is different for everyone. Stored procedures could be advanced, and personally in my Undergrad was never taught. It was all Oracle 9, and manual tracking of space. Now I use stored procedures all the time. I agree with the above post, on classes and training, but I would also check out some blogs. I was reading one on the difference between like, exists, and two other options; fascinating and their results were different from when I tried it on my own. Which is another issue of advanced, the database, server, and system play parts in that formula.
Copy all first names to a temp table ordered by random, middle, and last separately also, combine and overwrite back to main table, if you have family linking you can reassign the family last name based on the main person. 
Thanks i'll give it a go.
How are you with things like window functions, changing data types, subqueries, CTE's and temp tables? If you can work with all of those, then you shouldn't feel underqualified at all. If one or more of the above is new to you, then that's what you should seek out resources on. The good thing is that all of those can be learned with practice.
I don’t know those things yet because the Intro course didn’t explain it, but I’m definitely interested in them. 
As are MySQL, Postgre, DB2... Whilst most dialects don’t deviate too far from ANSI, it’s probably worth trying to focus on one flavour as you’re learning and then adapt to the different environments as you grow. 
Here you go - http://www.postgresqltutorial.com/ Covers all of the things I mentioned above, plus more. 
Ooh thank you this is great. The course I took uses Postgres too so I already have a database to practice on. The job I applied for uses MSSQL though, so do you think I should continue practicing on Postgres?
There aren’t really generic ‘entry-level’ positions per se, it’s worth just looking into data roles where it’s secondary. If you’re searching for roles with SQL in the title, you’re going to be in way over your head if you’ve dabbled with courses for a couple of months without professional experience. Most job titles with SQL will expect performance tuning, database maintenance and a lot of other things which are far removed from knowing how to write basic queries. If you look for low-level analyst roles (those with junior or similar implying it’s entry-level), those will likely involve SQL (and likely Python and R, but there should be some which don’t) and be a good way to accumulate experience on your CV of actually working with SQL. When working with SQL to solve problems professionally, your skills should soar. Even after 15 years, I get caught out most days!
At your level I don't think there's too much difference between Postgres and T-SQL (which is what MSSQL uses), but someone else might want to chime in if I'm wrong. CTE's actually perform better with MSSQL than Postgres, so that's nice. Are you expecting a coding challenge or technical interview? Because if they're going to do that in MSSQL it'd make sense to practice with MSSQL. Otherwise, the concepts are the same, you'd just be making some slight syntax adjustments. No big deal.
SQL is a standard. When it comes to writing select queries, there's little to no difference. When doing more admin stuff (functions, procedures, data types), there is significant difference. But it's ready to learn and not expected of an analyst (still nice to learn, do that on the job). 
I read a course _Advanced SQL_ here at U Tübingen. Material and slides to be found here: https://db.inf.uni-tuebingen.de/teaching/AdvancedSQLSS2017.html A rough syllabus would be: - Fundamentals of the tabular (relational) data model - The Core of SQL - Standard and Non-Standard Data Types - Type casts (in particular from type text) - The variety of types of values that may be stored in table cells: text and numeric data - ranges - user-defined enumerated types - dates, times, timestamps, and intervals - bit strings - binary large objects (BLOBs) - geometric objects - JSON and XML documents - sequences - Arrays and User-Defined Functions - The type `τ[]` (or `τ array`) - Computation over arrays - Array unnesting and aggregation - Table-generating functions - User-defined SQL functions (UDFs) - `LATERAL` (sideways row variable passing) - Sample problem (Finding Seats) - Window Functions - Window Frames - `ROWS` and `RANGE` frames - `WINDOW` clause - Partitioning - Sample problem (Weekend Weather) - Sample problem (Visibility in the Hills) - Scans - Window Functions - Sessionization - Run-Length Encoding - Landscape Features - Numbering and Ranking Rows - Consecutive Ranges - Piecewise Linear Approximations - Recursion - Expressive power of SQL and recursion - `WITH RECURSIVE` - Self-referential queries - Set vs. bag semantics (`UNION` vs. `UNION ALL`) - Home-made `generate_series()` - Tree traversals - (Non-)termination - Connected components in a graph - Recursive text processing (regular expression matching) - Bag semantics and termination - Recursive array processing (Sudoku) - Loose index scans - K-Means clustering - Marching squares (control flow to data flow) - Cellular automata (Game of Life, Liquid flow) - Parsing context-free grammars (CYK algorithm) - Procedural SQL - PL/SQL = Scripting + SQL - Saving turnarounds - Blocks, Statements, Expressions - Invoking Queries, populating tables - Implementation of a spreadsheet core (JSON-based formula representation, dependency extraction, topological sorting, recursive formula evaluation)
You may be confusing indexes with statistics, statistics exist for both the table and index and these are not always updates automatically. 
I practiced myself using strata scratch ([stratascratch.com](https://www.stratascratch.com)). They have datasets pre-loaded with questions and answers you can practice with. Otherwise, datacamp is also useful for practicing 
Usually the answer is RDBMS specific.
`GROUP_CONCAT` !!! 
Very true and you can transition I'm mostly TSQL and Oracle at present but I think I see my company shifting to an open source solution in the next 5 years and plan to adapt. 
Very wise - I’d recommend at least a basic familiarity with NoSQL sources, too. Spending a couple of hours here and there getting wise to unfamiliar data constructs is far more preferable than having it land on your desk and scrambling at it in a live, time-constrained situation..,I speak from experience! Ha. 
&gt; which is why I'm so confused as to why they want me to interview It's because of this: &gt; I have a business background You can teach anyone to code passable SQL, and there are a ton of us making dashboards who are good at SQL and at making charts, but don't really quite get the business side of things that would drive actual insight.
Entry level Data Analyst in Health care
 Select bigtable.ID, name From bigtable INNER JOIN smalltable On bigtable.ID = smalltable.ID Or: Select smalltable.ID, name From bigtable INNER JOIN smalltable On bigtable.ID = smalltable.ID 
It means that these tables contain same column name “ID”, you need to specific which “ID” you want to select in your select statement. For example, SELECT bigtable.ID, name FROM bigtable INNER JOIN smalltable ON bigtable.ID = smalltable.ID
Healthcare industry hires analysts all the time. Set up job alerts for "SQL", "data analyst", "business analyst" or just "analyst" on Indeed
Select bigtable.ID From bigtable Inner join blah blah blah
&gt; Best practice is to always qualify &gt; all &gt; your field names in a query with a join regret i have only one upvote for this
Bigtable and smalltable both have identically the same columns: ID, name, city Will this work? Create table want as Select bigtable.id, bigtable.name, smalltable.id, smalltable.name From bigtable Inner join bigtable On bigtable.id = smalltable.id 
are you maybe trying to UNION them instead? 
Should be fine, but since you're joining on ID, you don't really need to select it from both tables.
Oracle (and many other DB vendors) use a fast-load api. Instead of inserting rows one at a time, they generally write directly to database tablespace, bypassing logs and constraints, and then build any necessary table structures (indexes, primary key etc.) after the load completes.
Hmm.. so to emulate that I'd want to create a table in temp tablespace and insert to that table first and then at the end insert to my true destination from the temp table?
Not exactly, because the second step would still have to go through the conventional database API. Oracle defines [direct load](https://docs.oracle.com/cd/B10500_01/server.920/a96652/ch09.htm) this way: &gt;A direct path load eliminates much of the Oracle database overhead by formatting Oracle data blocks and writing the data blocks directly to the database files. A direct load does not compete with other users for database resources, so it can usually load data at near disk speed. Essentially, the conventional database api is bypassed altogether. The data is written directly to the disk where the table data would be located. This can happen at very close to the maximum disk throughput speed. Once completed, all the necessary table structures (indexes, constraints etc.) are built. (Some db vendors can actually load indexes using a direct mode as well - not sure if Oracle does that). It's kind of the opposite of a TRUNCATE command. The TRUNCATE just resets the table data pointer to unoccupied space - it leaves all the table data on the disk to be potentially overwritten at a later date. That's why it's so quick. A direct load write table data directly to unoccupied space, and then updates the table data pointer to reference the newly created pages.
The relational-operator you want is `UNION`. ``` SELECT first_name, last_name FROM customer UNION SELECT first_name, last_name FROM employee; ``` Note, the attribute-names don't have to be equal, just their types.
INSERT INTO NewTable SELECT * FROM (SELECT * FROM Table1 UNION SELECT * FROM Table2) results
`INSERT INTO NewTable SELECT * FROM (SELECT * FROM Table1 UNION SELECT * FROM Table2) results`
SSIS?
its not any part of sql. completely different program. Like a dumbed down version of sql
&gt; Create tableC as Select * from tableA, TableB I sadistically want to see you do this, and witness your reaction when you figure out what it does.
SQL is a query language, it's not an application.
Crystal Reports?
That all sounds like normal work for a dba. 
1. Doesn't do what OP wants to do. 2. Likely to error due trying to create a table with duplicate column names. 3. Oracle style joins are lame.
MongoDB?
tl/dr: use UNION ALL by default, use UNION _only_ if you know that you will need to get rid of the duplicate records. Here's the problem with this thinking: sql engine/optimizer code does NOT know whether rows will be duplicated or not, so if you use "UNION" without ALL it is forced to run "DISTINCT" command on the result regardless. What this means is that sql engine cannot execute any parts of the subquery from your example in a different sequence. For example, if you join "(SELECT * FROM Table1 UNION SELECT * FROM Table2)" to a table that has a common ID with both table1 and table2 and the index, that index might not be used as there's no way for the optimizer to apply this to individual tables inside your subquery. 
think i found it. it's called essbase. appreciate all the comments!
the select part of the first command executes across both servers, the select part of the second one executes only on the [linked] server and optimizer on your side (not having access to statistics on the other server) picks a wrong way to deal with it. Try moving your temporary table to the [linked] server (via "insert into") and running it that way. 
How it played out in my head. Oh? Duplicate column names? Let's run it without the create. Oh. It does pull the columns from both tables. Oh? Why are there so may duplicates. Oh. The columns from the other table aren't duplicated. Huh? ... OH, shit! Dramatic lessons stick with you the best.
I like to setup my SSRS server on its own SQL instance with a bunch of linked servers on it to the instances with the actual data. Then just use 4 part naming to reference the other servers. Works very well for MSSQL Server, gets a little messy running against DB2 / Informix, but the really bad ones get converted into SSIS jobs to pull in the data and run on MSSQL
I guess I'm confused on how to move it to the linked server.
Load a flat file of forenames and surnames, cross join one to the other, sort by NEWID() and get as many as you need.
I'm curious about this as well, look forward to hearing some feedback. Good question.
With any luck the field names are the same across tables, therefore you can query the system tables to find which tables contains the same fields. SELECT c.name AS ColName ,t.name AS TableName FROM sys.columns c JOIN sys.tables t ON c.object_id = t.object_id WHERE c.name LIKE '%MyCol%'; -- MyCol would be where you would put the name of the column you are looking for.
Quick, buy an espresso machine and get an Rx for anxiety meds. XD Thoughts and prayers to our fallen brethren. 
A hearty chuckle was had. Thank you kind sir.
I mean SQL isn't by nature a cloud hosted Database.
My best advice is to NEVER give friendly names to servers as a rule of thumb. Correct me if I'm wrong, but, shouldn't you be connecting to the server before opening any file to execute against the server? I think I'm already preaching to the choir, but just in case you may be able to build a solution using these tables ... select \* msdb.dbo.sysmanagement\_shared\_registered\_servers\_internal (server\_name) Do you have multiple groups in the list? If you do, you will need to also join msdb.dbo.sysmanagement\_shared\_server\_groups\_internal 
Because hosting costs money and hosting companies are in no way required to give you free shit just because other smaller companies are trying to get market share so they can try to sell you on their paid services. It's already extremely easy and free to get SQL and a web service setup on a local box for testing.
I wouldn't really call Google a small company m8 
"tbl_invoices" may have foreign keys that point from the ID columns you are looking at to the tables that you are looking for.
Have you tried `initiationtimestamp::timestamp`?
Kudos to dbas - im a bi dev with a timeline of about 2 weeks. Since I’m not a dba this is not my normal scope of work - this is extra workload since my daily “normal” responsibilities are still there lol 
Union all
Was talking specifically about mongodb. Google already has a way of recouping the costs of you using their platform since they can tie in everything you and whoever accesses your site into their massive advertisement machine.
How are you creating your SQL? If you aren't using bind variables, you will need to parse each SQL statement separately Are you doing batch commits? Committing after each insert will increase overhead. Commit only every 1000 statements or so (actually number depends on row size, available memory and other activity). Use the /* +APPEND */ hint in your insert statement. That will attempt direct-path inserts (Oracle's version of the fast-load like AngelOfLight described.)
4 part naming referencing can cause performance issues, we use open queries since that part of the work load is done on the “foreign” server (splits workload) instead of everything being doing on the “primary” server. But this would make maintenance much easier 💡 
Still mongoDB is for sure not a small company i'd even argue they are better than many SQL "versions" that aren't the biggest ones like Postgre or Server, i don't know, its just seems weird when compared to other modern databases systems
Yeah that's not going to happen. Raise that "unreasonable expectation" flag asap.
Yea management did today, this is coming from the top of the pyramid. 
That did it!!! Thanks! I can't believe it was that simple!!!!
I will try these things. tyvm
 With A_CTE as ( ) , B_CTE AS ( )
\`UNION ALL\` will yield duplicate tuples (an oxymoron, but this is SQL). I doubt that's what anyone wants.
How would I then pull information from both cte’s? Where would the join be (if at all)?
Are the 2 tables related? You can always update in a JOIN construction. UPDATE O SET Name = N.full\_name FROM Table\_New N JOIN Table\_Old O ON O.ID=N.&lt;insert join column&gt;
how would you pull information from one CTE? where would you joing two tables normally?
To use MS SQL Server in that way, Google would have to pay Microsoft ridiculous licensing fees. They're not going to open that sort of service up to you for free just for the hell of it.
A basic 2GB Azure SQL Database will cost you about $5/month. You can spin up a VM on Digital Ocean or Linode for $5/month and put anything you want on it. You can set up a VM on Azure with SQL Server or PostGres in it and as long as you have it powered off and deallocated when you're not using it, it'll only cost you a few bucks a month (storage cost + compute while it's running).
Yup been searching for a little bit now. It’s a little challenging because I don’t have the professional exp with SQL
Yup I’ve been searching for a little bit now. It’s just a bit challenging without the professional exp using SQL and reporting programs. 
Yeah, we need to use openquery 100% on the DB2 (System i) DBs otherwise we're pulling 100+million rows into MSSQL for a query that returns 1 row
That's what I've seen used most often. And yeah, I most often need to literally stick one series with another series. Not a set union operation
Use array binds for your insert statements you’ll be even faster as your network round trips will drop massively. This is what any client side import tool should be doing. Google how to do it for whatever language and driver you are using. This white paper about high volume loads has an example using python on page 18 (you should read the whole thing though) https://www.oracle.com/technetwork/database/in-memory/overview/twp-bp-for-iot-with-12c-042017-3679918.html Insert statements generate barely any UNDO, you can probably have millions of rows inserted in one transaction without reaching any limits of your UNDO tablespace. Memory has nothing to do with this limit, Oracle will happily flush things to disk (even if it’s not committed, and that’s okay!) There is usually never a need to use direct path load (it will cause serialisation that can really mess with your day). 
Thanks for the advice, however I’m not the DBA of these servers so I don’t have permission to those tables. My team often switches between Dev, Test, Stage, and Prod servers from 4 data applications (meaning that we have 16 servers - I know... I am not the architect). I give my new employees a .regsrvr file to import into the Registered Servers so they can see the English description for each server. For example, the Registered Servers explorer would say: EDW Prod - H##-77777XXX. The issue is if I give my user a .sql and say to connect to the EDW Prod server, they either have to remember it is H##-77777XXX, or go reference the Registered Servers window. I was just hoping that since the Change Connection window (right click-&gt;Connection-&gt;Change Connection) retains a running log of ACTUAL server names, that there was a way for it to recall friendly names as well. Either that... or allow you to change a specific query window’s connection from the Registered Servers window where you can see the friendly name. 
Many companies underutilize data and analytics or have none at all. So, companies willing to hire you will either be just starting out, or willing to groom you for internal roles with their data team over a longer period 
I see. AFAIK, they are going to have to manually click the name each time, unless you were able to create a sproc/function which they could call at the beginning of each script by passing the "friendlyName" and thus connecting their current session. Above my pay grade. :) /u/AbstractSqlEngineer XD
this is a pretty simple example: select AirNo, AirName, count(\*) as FlightNo from table group by AirNo, AirName
No worries, I appreciate the help and suggestions!
What's the count of #PIProducts?
I am not sure if SQLCMD allows for friendly names; try it out. Query &gt; SQLCMDMODE then in a query window &gt;:connect &lt;friendyName&gt; There are some nuances with SQLCMD, but, it may work in your case. Sorry I couldn't be any more halp. :/ 
Stuff tsql (or stringagg if 2017+) Listagg db2 and Oracle (some DB version requirements) I don't think there's really a completely db independent solution. 
The space after the first airplane. ? 
No dice, but SQLCMD is something I’ve been meaning to look into so that’s for the remind!
I tried to fix that but its still giving me the same error. I'm not sure if I did this assignment completely wrong or if there is something i'm missing here.
That’s exactly the idea. You couldn’t have phrased it any better. I guess my own personal challenge is composing a resume and cover letter that highlights that I’m growing passionate about this career route willing to learn &amp; continuously ask questions. 
What do you do with your "duplicates"? I guess without an example that's a bad question on my part. I see things "in the wild" I think can be avoided by a relationally-sound schema, which is not something I've ever encountered, in my limited experience. I assume the duplicates come into play because so many people choose to rely on an auto-incrementing-integer. In Postgres, I have adopted using a prefixed-`SERIAL`, e.g. for an order-number, something like `ON12345`, for customer-number, `CN12345`. Now, if I were to print those 2 values on a sheet of paper and leave it on a co-workers desk, they could immediately differentiate between the 2. Using pure integers, this can't happen. `12345 = 12345`. `ON12345 &lt;&gt; CN12345`. Etc.
If it's not the space mentioned I'd guess it's the hyphen. Fully qualify that name ([]s if tsql quotes if a bunch of others)
The error will be a syntax issue. Try square brackets around the column that has a - in the name 
Thanks for pointing that out I realized that there wasn’t supposed to be a hyphen there but I’m still running into the error: “Invalid use of ‘.’,’!’,or ‘()’ in query expression ‘Airplane.ManufacturerID=Manufacture.ManufacturerI’.
Thanks for pointing it out realizes that the hyphen wasn’t supposed to be there so I fixed it but I’m still running into an error
So update I realized I also had a spacing issue in the join but now I’m receiving syntax error in join operations 
So update I realized I also had a spacing issue in the join but now I’m receiving syntax error in join operations 
U/TheOriginalSuperman There is a feature under the main menu. SQLCMD MODE under Query. If enabled you get some additional scripting. You can use :Connect server user pass. You can also create SQL Templates (View -&gt; template explorer) Combine the two. Make a template with a list of servers in a /**/ comment block or something. But.. you can either directly call the server if its linked, or use the sqlcmd connect feature. Is that what you were looking for?
I think you're missing an s in airplanes after the on 
Damn. :/ What about EXEC (select * from msdb.dbo.backupfile) AT &lt;friendName&gt; I'm out of options, hopefully tha gawd abstract chimes in for some guidance. :P 
Bless your soul It made me recheck everything and I discovered some spacing errors on my end 😭😭🙏 thanks
You're welcome. A good habit is to copy and paste words that are meant to be the same. Minimises these kind of issues 👍
&lt;Insert join column&gt; is this a command or does it require the actual name?
Without further context or table setup: Select Airplane Number, Airplane Model, Airport ID, Airport Code from table
Thanks I appreciate it, might have been overthinking LOL
\^ This. Do you not have access to a data dictionary? 
95 rows. Trying to insert around 2600 rows into the Trans table.
How does this scale to legacy VBA reports? XD
How does this scale to VBA reports? XD
Access isn't a good choice to learn SQL. Consider installing SQL Server Express and SQL Server Management Studio. 
I'm dumb and misread which query was taking longer. The first query it seems you are trying to compare a data set on server A to the linked server B. Perhaps comparing the execution plans and/or checking the wait type running [whoisactive](http://whoisactive.com/downloads/) will provide some better insight. 
What's the plan of the first query? Lack of statistics on linked server table or wrong estimation on size of temp table can make SqlServer retrieve all data from remote table and join locally, instead of querying only for needed ids. &amp;#x200B; You can probably try adding `with (forceseek)` hint right after `#PIProducts p` , or mb after linked table (you may probably need to reverse position of from and join statements in query for that, I do not remember syntax limitations exactly)
Well let's say I'm getting 2 results that I know are definitely different. Why make a redundant expensive operation? Or how about records about sold products, with product ID, date, and measures like quantity and stuff, taken from different shops? Or when doing some window function magic with duplicated inverted values. Anyway, of all the cases where UNION would work, I actually would be better off using DISTINCT or even DISTINCT ON in Postgresql. Because if I want deduplicated values, I want them as such across the entire dataset, not just if they're duplicated in different sources.
Honestly, their question scares the hell out of me. Management. Airlines? I hope this is a homework assignment. XD
zucccc
"Big Data"? I think you meant, "Bigly Data". 
&gt; USE OF JOIN REQUIRED
Ugh words are hard. "WHERE" is a clause. Statement is the whole executable statement (select ... from... where...). Anywho, most of sql engines execute statements sequentially within one connection. To execute multiple statements parallelly would require then multiple connections. parallel options in oracle govern execution of a single/standalone statement - each sql statement is compiled into an execution plan and steps of this plan can execute in parallel or certain step might be processed in parallel fashion.
Why are you trying to stop parallelism? Are you on an intel p4?
hint: left. Go left. It helps deliver UPS save gas.
create table [linked].#temp with the same columns as your #PIProducts, insert [linked].#temp select * from #PIProducts
I am bad at explaining myself, maybe this will make more sense. This is for a live connection on tableau. For most queries that tableau sends over they run in about 6 seconds serially but 100+ seconds in parallel, but when adding filters tableau sends a select statement that doesn't include any of the other filters(kinda), this takes about 85 seconds serially but only 13 seconds in parallel. This is because of a data source filter I have for row level security, so the query goes and connects to the fact table and then to the security table and filters on the security table. The security table and the fact table are about 1,000 and 1,000,000 times larger than the query itself. It returns about 121 million rows and them groups them, giving me the 40 or whatever rows I actually want back. I'd like to run most queries serially, but I want to run the queries that tableau makes to create filters in parallel.
It's an exadata.
Thanks will give it a try in the morning
View the table's DDL to see if the columns have foreign key relations to another table.
Annoyingly, that syntax isn't universally valid: I believe Oracle, for example, won't accept it. 
If you want help with homework just say so.
I created a free 12 month trial Azure account and made a small SQL Server and Database for my own project in about 5 minutes. Costs nothing for a year, then bills 5 bucks a month after. You have $200 in credit to put toward the paid Azure services as well so you can get a good idea how much things actually cost when your free trial runs out. So far my little database, server, keyvault, container registry, container group, and logic app have cost me about $2 of my $200 credit in about a month. Super inexpensive
why in the world are you running a live query on 121M rows? do an extract on your result (aggregate to whaterver grain is needed) or preagg on the sql side 
You didn't list the actual error notification. Also, as said before, Access not great platform to learn SQL. 
Unfortunately the data source almost the entire schema, and our dba's have restricted the size of the temp tablespace. So no go on an extract. The data source is just too wide, even when I try some tricks with only extracting a single row and then having tableau server fill in the rest. I'm also just an intern and I've been told I can't make materialized views. But I can make an extract that acts like one. Unless you're saying to make the aggregate filters and then save those as an extract and have that small extract as a second data source. That sounds potentially promising, but I don't know how well it'll actually work in practice. Something to look into at least.
SELECT A.USER AS “User” ,GROUP_CONCAT(B.CENTER) AS “Responsibility Centers” ,GROUP_CONCAT(C.MANAGER) AS “Managers” FROM USER A LEFT JOIN CENTER B ON A.ASSIGNED_CENTER = B.ID LEFT JOIN MANAGER C ON B.CENTER_MANAGER = C.ID WHERE A.STATUS = ‘ACTIVE’ GROUP BY A.USER ORDER BY A.USER DESC LIMIT 100000; Something like this?
Take this as an opportunity to get your team to ditch the SSRS and adopt Power BI instead. You will be thankful in future migrations. 
We’re making a slow crawl towards it, problem is there is a lot of shit to complete, or as I would say, get out of the way, before we fully transition
“Please grant me write permissions for chumbawumba.tubthumping.net for 48 hours.” *(Hands you a CD-RW, stamps a library card)*
&gt; duplicated inverted values I have no idea what a referent for that looks like. I'm vaguely familiar with PG's `DISTINCT`, I get the general idea; I have read about `DISTINCT ON`, but never used it. I have a lot to learn. Thank-you for your time.
group\_concat doesnt work in my system... &amp;#x200B; I'm using TDP
Sql server has a developer license on azure that is free. You just pay VM cost which could be minimal depending on what size you need. Just search sql server 2016 developer in azure 
All performance benefits from creature comforts like statistics usually go out the window when you’re using a linked server. Specifically because you might not have permissions on the target server from which the data is derived to use features like statistics. This is probably why your query isn’t performant. If your dba hooked is only giving you db_datareader access to the data in that linked server, performance will drag. Check out [this article](https://thomaslarock.com/2013/05/top-3-performance-killers-for-linked-server-queries/) for more information on it. 
After manufacture. Manufacturename also have a space
I was able to fix it ! Thanks though haha as the person below suggested I should just copy and paste same words cause I was making a lot of misspelling errors 
Try this. Create #table table ( ,productid bigint , intPhysicalModelId bigint default(876) , index ci_physical clustered on (intphysicalmodelid, productid) , index ix_productid nonclustered on (productid) )
Basically, since your in temp db, the physical Id makes sure all of your data is on the same page/extent. The cluster forces your data to be very close, the nonclustered will get your join. If it's still slow... add a noncluster to the other table. Create index ix_othertable on (joincolumn) include (selected, columns) I'm curious to know if you have one temp file per core, if not... do that as soon as you can. 
Slivered. That was amazing.
1. Make lots of back ups. 2. Aniexty and heartburn meds 3. Copious amount of coffee. I love power BI over SSRS. I did a 14 hour work day today because our audit season has started and the damn SSIS jobs are being a little bitch. This is our last import that uses CSV files over NESSUS XML. CSV files are good until about 50 MB our files are 2OO mb which require VBA scripts to split them. Yeah I need some tequila shots
Not sure if someone already mentioned this but on your join it seems you’re missing an s at the end of Airplane. The table name you used is Airplanes without an alias of Airplane, which is what you used instead of Airplanes 
Select statement at the end
Assuming the old and new table are related, they must have a joinable column. You should insert that column here. 
Oh, I did not know this. How would this work in Oracle then?
What's a dba? /s Sometimes it feels like the way we manage data systems is regressing.
Use aliases
&gt; have no idea what a referent for that looks like. It's just a crappy bad name that I use for this: Say you want to calculate a customer's total revenue from the last year, for any day. Like you want to say how much revenue the customer generated in the past year in February 2018. I mean you can just make a query for that customer, but it's not compatible with reporting software that only does SELECT columns FROM table WHERE condition. To do that, you select all order dates and values, and UNION ALL with order date+1 year, and value * -1. And compute a running sum. You get a list of dates where the customer state changes, and the customer total revenue the last year for that date. You can then easily query on the result without making any aggregation. There can be a situation where a customer buys stuff for 100$ on June 1 2017, so you also generate a record with -100$ on June 1 2018. But then the customer makes another purchase say on May 25 2018 for another 100$ and returns the items on June 1 2018 so you register that return as a negative, -100$. A UNION is logically incorrect in this problem, and in this case it will lead to an incorrect solution that will also perform slower.
Looks like typo for me. Airplane != Airplanes
Sounds like you want the \`OFFSET\` clause.
You can also spin a machine, install docker and pull a free SQL Server docker image. Done
The subject line of this is misleading. e.g. Oracle: https://docs.oracle.com/cd/E18283_01/server.112/e17118/statements_10007.htm &gt; You cannot roll back a TRUNCATE TABLE statement, nor can you use a FLASHBACK TABLE statement to retrieve the contents of a table that has been truncated. It may be true for SQL Server that it CAN be rolled back, but it definitely isn't for other DBMS.
I believe there's no OFFSET in ACCESS mdb. Actually this should work but it doesn't for some reason: SELECT \* FROM (SELECT \* FROM tblSection WHERE sub\_section = 2) WHERE [tblSection.ID](https://tblsekcija.ID) NOT IN (SELECT TOP 3 [tblSection.ID](https://tblsekcija.ID) FROM tblSection ORDER BY date DESC) ORDER BY date DESC; It works without that "WHERE sub\_section = 2", but I need this to select the right rows. DAMN!! 
I use DBeaver, which you can. I’m 95% sure you can in SQL Developer, but I can check once I’m at work and let you know if nobody else has.
Exactly. I'm in an SQL Server shop, but even I know this to be true. God I miss Oracle. 
Where is the comparison to a blanket delete from?
You could try [ScaiPlatform](https://aws.amazon.com/marketplace/pp/B07NPPSPJ1?ref=_pntr_web_pricing) which is free on AWS and it is a sql gui that integrates well with SQLServer. Once you connect it to your db, you can find the values of your ids through foreign keys ( it displays the original values) and create some reports. At the same time, you could also take a look at the **information\_schema.columns** and filter for these column names.
It's been in BOL since 2013. Honestly, the entire article in OP could be replaced with a link to [https://docs.microsoft.com/en-us/sql/t-sql/statements/truncate-table-transact-sql?view=sql-server-2017#remarks](https://docs.microsoft.com/en-us/sql/t-sql/statements/truncate-table-transact-sql?view=sql-server-2017#remarks) .
I've seen silly statements that in SQL Server TRUNCATE is non-logged, but Microsoft never said this. They always say TRUNCATE is *minimally* logged. [https://docs.microsoft.com/en-us/sql/t-sql/statements/truncate-table-transact-sql?view=sql-server-2017#remarks](https://docs.microsoft.com/en-us/sql/t-sql/statements/truncate-table-transact-sql?view=sql-server-2017#remarks) 
[ScaiPlatform](https://aws.amazon.com/marketplace/pp/B07NPPSPJ1?ref=_pntr_web_pricing) starts with s and it is free in the cloud. It is a real-time SQL gui for data analytics and sql database management. You can create dashboards, reports and views, import spreadsheet data without writing code and it integrates well Amazon Redshift and Aurora. 
Create a sql user, not windows, that has permissions specific to the role you are requiring. You can use GRANT and REVOKE to fine tune those permissions. Some even go as far as creating a reader role (app uses to connect) and a writer role for procedures that change data (EXECUTE AS), and a separate one for dbowner. I have seen AD accounts take chunks of the system offline.
PS Look into a secure password vault. You dont want to lose these pws.
It looks like I have to run the "With CTE" statment every time. Is there a way to run it once and save the temporary table in memory during the session? 
Tableau is not like BO or Microstrategy - it cannot trim queries to remove unused pieces and you don't want a data source covering fields and streams because once in a blue moon someone might explore an option. You also might need to start writing custom SQL to make sure you dont pull too much data into the client as well.
&gt;Tableau is not like BO or Microstrategy - it cannot trim queries to remove unused pieces Yes it can. Google join culling.
Truncate absolutely is unlogged in Oracle and can not be rolled back.
I think you want a second CROSS APPLY to parse out the LINEITEMs. SELECT NULLIF(REQUISITION.x.value('(REQ_NUM)[1]', 'NVARCHAR(10)'),'') AS 'REQ_NUM' , NULLIF(REQUISITION.x.value('(REQ_DATE)[1]', 'DATETIME'),'') AS 'REQ_DATE' , NULLIF(REQUISITION.x.value('(SUPPLIER)[1]', 'NVARCHAR(50)'),'') AS 'SUPPLIER' , NULLIF(REQUISITION.x.value('(REQUESTED_BY)[1]', 'NVARCHAR(50)'),'') AS 'REQUESTED_BY' , NULLIF(REQUISITION.x.value('(REQUESTED_DATE)[1]', 'DATETIME'),'') AS 'REQUESTED_DATE' , NULLIF(REQUISITION.x.value('(APPROVED_BY)[1]', 'NVARCHAR(50)'),'') AS 'APPROVED_BY' , NULLIF(REQUISITION.x.value('(PODATE)[1]', 'DATETIME'),'') AS 'PODATE' , 'R' AS 'STATUS' , NULLIF(REQUISITION.x.value('(QUOTE_NO)[1]', 'VARCHAR(50)'),'') AS 'QUOTE_NO' , NULLIF(REQUISITION.x.value('(TOTAL_PO_COST)[1]', 'NVARCHAR(21)'),'') AS 'TOTAL_PO_COST' , NULLIF(LINEITEM.x.value('(LINE_NO)[1]', 'NVARCHAR(21)'),'') AS 'LINE_NO' , NULLIF(LINEITEM.x.value('(PART)[1]', 'NVARCHAR(21)'),'') AS 'PART' , NULLIF(LINEITEM.x.value('(WHSE)[1]', 'NVARCHAR(10)'),'') AS 'WHSE' , NULLIF(LINEITEM.x.value('(QTY)[1]', 'INT'),'') AS 'QTY' , NULLIF(LINEITEM.x.value('(UNIT_CST)[1]', 'NVARCHAR(21)'),'') AS 'UNIT_CST' , NULLIF(LINEITEM.x.value('(REQUIRED_DATE)[1]', 'DATETIME'),'') AS 'REQUIRED_DATE' , NULLIF(LINEITEM.x.value('(INVENTORY_FLAG)[1]', 'BIT'),'') AS 'INVENTORY_FLAG' , NULLIF(LINEITEM.x.value('(REQUISTION)[1]', 'NVARCHAR(21)'),'') AS 'REQUISTION' FROM @xml.nodes('REQUISITIONS/REQUSITION') AS REQUISITION(x) OUTER APPLY REQUISITION.x.nodes('LINEITEM') AS LINEITEM(x)
Like... ripping it apart? Are you on 2016? Because it's pretty easy with 2016. You can do something like.. Select item1, item2 From openjson(@json, '$.deep.path.go.real.deep') With ( Item1 nvarchar(100) '$.not.so.deep' , item2 nvarchar(100) '$.shallow' )
Not sure if it matters but I’m using SQLite 3 
I think you need to tell it *what* to select. For example: `SELECT * FROM Movies WHERE Director = ‘X’` to return all columns. Or: `SELECT MovieName FROM Movies WHERE Director = ‘X’` (if your Movie table had a column called MovieName) &amp;#x200B;
Yes, set up a service account specifically for sql server. 
Is this verbatim what you're running? nta.ContractID in your SELECT has a transposed `.`: `,.nta[ContractID]` should be `,nta.[ContractID]` Otherwise it looks fine... assuming you can leave out the word `INTO` after `INSERT`... (news to me!)
Never use some user's account for services. People leave. Create service accounts for your services and processes where PW never expires and grant permissions as needed.
Well this might be where I'm confused. I know not to do that with services. Do you consider a maintenance plan a service? 
How are you able to tell the portfolio has re-entered at a later date/time?
Each security has a unique identifier. A quantity column shows how many shares we own and when we’ve sold it completely it hits zero and disappears from the database. So if I select the unique identifier from the table I can see there are two tranches of data ranges.
If you already know how to separate the two groups distinctly, then just use MIN() OVER(PARTITION BE [field that makes it separate]) or MAX() OVER(PARTITION BY…
Chances are you have left/outer joins in your data source and then, my understanding, seeing join culling is very unlikely.
if it's a service or plan you don't want interrupted when someone leaves....yes
It's all inner joins and no custom SQL, the join culling is consistent . I'm literally looking at the tabprotosrv log files live. My issue isn't that I'm getting bad queries, the queries are looking exactly as expected. It's that I want only a fraction of said queries to run in parallel and the rest serial. I have no idea how to do this in practice, other than possibly some really janky PL/SQL, was wondering if anyone here had heard of doing anything like this before.
Parameter-ize all your variables, replace special chars that don't make sense with spaces or blank strings, and log your variables inputs so you can see what your attackers are trying. Then adjust your replace (s) to remove other stuff.
I don't know much about this, I want to know whole procedure like how to build a website vulnerable to sqli and then I will attack my own website and then I will secure it from it.
`PL/SQL` is Oracle's flavor of ANSI SQL. `T-SQL` is Microsoft's flavor of ANSI SQL. Ultimately you'll earn more with Oracle, but the path is harder.
&gt; schema binding view with clustered unique index Since you didn't specify the specific RDBMS, I'm just going to assume MS SQL Server. The big issue with schema binding is that you can no longer change the underlying table - adding a column, adjusting a varchar width, etc. - unless you drop the view first. Why didn't you apply the index directly to the table?
 \*\*\*\* Section Times in Video \*\*\*\* \#1: Open the SQL Server Missing Indexes Blog Post [0:55](https://www.youtube.com/watch?v=3IF_MjmmG6g&amp;t=55s) \#2: First Process: Brent Ozar's sp\_BlitzIndex SP [2:19](https://www.youtube.com/watch?v=3IF_MjmmG6g&amp;t=139s) \#3: Second Process: Customized Missing Index Script [6:17](https://www.youtube.com/watch?v=3IF_MjmmG6g&amp;t=377s) \#4: Determine which Indexes Should be Created [12:26](https://www.youtube.com/watch?v=3IF_MjmmG6g&amp;t=746s) \#5: Monitor Usage [14:57](https://www.youtube.com/watch?v=3IF_MjmmG6g&amp;t=897s) \#6: Leave Comments! [16:36](https://www.youtube.com/watch?v=3IF_MjmmG6g&amp;t=996s) \#7: Question of the Day! (I will reply) [17:11](https://www.youtube.com/watch?v=3IF_MjmmG6g&amp;t=1031s)
You could try using A or B as a foreign key in another table that stores all the distinct values. But perhaps a simple nonclustered index on the big table is better.
&gt; It's an exadata, so just a bit faster than a p4. Not by much! :) &gt;that tableau spits out Found the problem.
It's called a service account because of what it does, not a special type of account. A service account is just a normal user on ad set up with a very strong password that has a longer reset time than normal users, that is known by only a subset of people. This account shouldn't be used to login into a machine as a normal user would, and should have its rights limited to what task it's performing. Protip: in AD, list what the account is for, which vendors might use it if any, etc. This will be important later down the line when troubleshooting.
Honestly for the kinds of queries that I'm seeing I don't think there's really a better way to write them than how Tableau's doing it. It's not that complex or anything.
Oracle is the way to go.
The problem is that Tableau - at least in my experience - writes queries as agnostic as it can, often ignoring language specifics that can improve performance. You can get those specifics by writing custom SQL, and the Tableau generated queries are a good place to start. We have a Tableau environment that connects to Teradata (don't get me started...), Oracle, SQL Server, and several others. The generated queries all have issues at the outset until we tune them.
Maybe a GUID could work for you? The NewId() function can make this do. &amp;#x200B; Examples: AE1C61F9-4C7B-4694-A0D0-EF9951FD43FE 9229F864-40C8-43FD-9F54-439A22D179B2 FE2FC579-5DED-4BDF-B442-4814255F6EF8 &amp;#x200B;
How would I go about that? I dont want letters, only numbers Alter table want Add idcol varchar(80); Update table want Set idcol = NewId() ???
Select * From client as cc Where Email in ( Select email From Client Group by email Having count(distinct postaladdress) &gt; 1 ) Is that what you want?
Sorry I mistyped it the first time. So I'm using where it's coming from. Specifically I do this: SELECT Title FROM Movies WHERE Director = 'example'; and I get nothing however when I do: SELECT Title FROM Movies WHERE Director = '#5'; It selects all the movies where the director is the 5th person in my director list. Is that the right way? Should I not be able to ask for director 'example' rather than his number? 
You'd have to do it with a correlated subquery, e.g., UPDATE O SET NAME = (SELECT full_name FROM Table_New N WHERE N.ID = O.ID)
I dont think there's any other way than having hints added directly to the statement. I'd refresh statistics first and what other settings your system runs now. IMO, if the execution plan seems to be wrong find out the root cause rather than try to hint your way to the right execution path. 
Can you give some reasons as to why Oracle is preferred?
Can you give some reasons as to why Oracle is preferred?
I was just talking about this with a co-worker. He had an API call that said was like var variable = queryStringResults["id"] var query = "Select column1, column2 FROM table where id = '" + variable + "'"; db.Raw(query); This would run a query like Select column1, column2 FROM table where id = '4' Paramaterizing your query handles it like this in our nodejs project var variable = queryStringResults["id"] var params = {id: variable} var query = "Select column1, column2 FROM table where id = :id"; db.Raw(query,params); So your statement will look like this //This will be declared but not executed DECLARE @id = '4' //This will be executed Select column1, column2 FROM table where id = @id The id parameter will not be executed but passed in as data and will be declared as a variable. Thus if you put some code in it like ";GO DROP TABLE dbo.Students" all of that will be treated as text and will not affect the execution of the query. The query will fail but it will fail because there is either not an id that equals that value or it will fail converting string to int.
Thanks for the suggestions. I have tried those along with everything else in this thread and could not get the run time below 6-7 minutes. As a "hack" I ended up turning the the column of IDs into a string and then ran EXECUTE(@tsql) and it runs in 3 seconds now. I'm not happy with that solution, but it works.
How are you counting 'first'? Strictly speaking from a relational algebra perspective, records in a table don't actually have an order. If by sequential ID, the SQL might go something like: SELECT ColA, ID_col FROM ( SELECT ColA, ID_col, row_number() OVER (partition by ColA order by ID_col ASC) RowOrder FROM table_source ) x WHERE x.RowOrder = 1
You're really close. Assuming you're sure you've got one record of each type, you could do: select uid, max(case when key=30 then value else '' end) as 'Desc1' max(case when key=31 then value else '' end) as 'Desc2' from T1 left join t2 on t2.key=t1.key left join t3 on t3.type=t1.type group by uid
Will this still work if I want to select more columns (but only consider duplicates being strictly in colA)?
Depends on the industry and type of work you're looking to do. Oracle gets more use at very large enterprise-scale organisations who have long-standing data infrastructure. Otherwise, MS SQL is more prevalent.
Yes, you can add any other columns (add them to both SELECT lists). It will still return you the first record from table_source for each ColA - just don't add the columns to the partition by part, and you'll be fine.
Nevermind, fixed! I had to use joint statements . Thanks for the help !
 SELECT ColA, colB, colC, ID_col FROM ( SELECT ColA, ID_col, row_number() OVER (partition by ColA order by ID_col ASC) RowOrder FROM table_source ) x WHERE x.RowOrder = 1 Like this?
Like this: SELECT ColA, ColB, ColC, ID_col FROM ( SELECT ColA, ColB, ColC ID_col, row_number() OVER (partition by ColA order by ID_col ASC) RowOrder FROM table_source ) x WHERE x.RowOrder = 1
Woot! That actually worked. So today I learned about 'max'. Always nice to learn something new.
Alright I think I get it now. Thank you so much! 
openjson or azure data factory
You should check your regional listings for jobs related to both Oracle and SQL Server and see which one is more prevalent in your area. I spent the first half of my career doing PL/SQL development, however I had to make the change to T-SQL due to the jobs for Oracle being more and more scarce in my area. It went from having a moderate number of jobs with Oracle as being the defacto RDBMS in the area to being pretty much taken over by SQL Server. If traveling/moving isn't a limitation for you - then anything in Oracle is going to pay more. 
&gt; Ultimately you'll earn more with Oracle, but the path is harder. There's no preference about it. Any Enterprise is going to decide what RDBMS to use for their purpose, and users have to live with it. If your goal is to have a high salary, Oracle is the way to go - the certification path is much harder, there are far more interconnected systems that you have to learn - `RMAN`,`Data Pump`, etc. - on the certification path than there is with SQL Server. If you just want a job, then go with SQL Server.
How are you preparing ... what materials did you read before taking exam .. I have done OCP certification in oracle 9i, 10g and 11g. May be I can suggest you what you can read in addition, if you tell me how you prepared.
I’ve bought 2 courses on Udemy by imtiaz ahmed https://www.udemy.com/user/imtiazahmad4/ 
What have you tried so far?
[I would highly recommend you use the official Oracle Press exam guide instead](https://www.amazon.com/Oracle-Database-Guide-1Z0-071-Press/dp/1259585492/ref=sr_1_1?keywords=1Z0-071&amp;qid=1552508697&amp;s=gateway&amp;sr=8-1). Oracle exams aren't really a joke - and having a book in front of you that you can sit and actually study and contemplate what you're doing instead of a video that's continually running may serve you better. 
SELECT AIRPLANE NUMBER, Airplane model, airplane id, airplane code, FROM airplanes
Excellent post, lazy way .... SSIS powerpack from Zappysys. I'm not affiliated to then in any way but have been using their product for about 3 years and it brings simplicity to do many SQL processes from API calls to JSON and XML.
If you don't know **anything** about this, then why on earth are you working on it?
...that's a tall order for a beginner. Are you setting it up local, or are you wanting to do this live, "Honeypot" style? I'm on mobile right now, but when I get time tonight I'll try to google-fu you some blogs about setting up a website with a SQL back end. Do you have a VM to use? Or multiple? Are you gonna do this in Azure free trial? Linux? This is like going to Baskin Robbins and saying "I'll have the cold one".
I feel like I'm getting there, thank you so much for the quick reply! It does give me a result populated with multiple queries done with the same e-mail address, but the results still show up but even if they are all at the same postal address. Meaning I have five different rows with the same email that are all at 123 smith st, they all show up. I would like to exclude those. &amp;#x200B; I would like to see only results where the same email is used at multiple postal adresses (123 smith st, 456 gator st, 879 motor pl, etc.).
Thank you very much! I will try this!
Hard to tell, depends a little on the kind of transactions. If the users mostly read, it's not worth it. Do you experience bad performance on the environment? And it is related to log writes? Based on the amount of users, I would guess is is not worth it unless the users do a lot of inserts or updates. Check your wait types to see if you see wait types related to log writes. 
I work with very large data sets (hundreds of millions of records) and SQL Developer is what my company prefers. We do use SQL Server but it is not preferred for the main data we house. As someone mentioned above, it is easier to find a job with SQL Server but in my experience, I find that SQL Dev is a more logical language and is able to handle larger data more efficiently. 
instead of ref_period in your transaction table, just use date then you can join to the product table `ON trans_date BETWEEN start_date AND end_date`
I work with very large data sets (hundreds of millions of records) and SQL Developer is what my company prefers. We do use SQL Server but it is not preferred for the main data we house. As someone mentioned above, it is easier to find a job with SQL Server but in my experience, I find that SQL Dev uses a more logical syntax and is able to handle larger data more efficiently.
If you are using SQL server, you could look into system versioning on tables. https://docs.microsoft.com/en-us/sql/relational-databases/tables/temporal-tables?view=sql-server-2017
'''SELECT Airplanes.AirplaneNumber, Airplanes.AirplaneModel, Airplanes.AirplaneEndofLifeDate , Manufacture.ManufacturerID, Manufacture.Manufacturername FROM Airplanes INNER JOIN Manufacture ON Airplane.ManufacturerID=Manufacture.ManufacturerID ;'''
https://mode.com/sql-tutorial/introduction-to-sql/
&gt; Since you didn't specify the specific RDBMS, I'm just going to assume MS SQL Server. yup, MSSQL &gt; The big issue with schema binding is that you can no longer change the underlying table - adding a column, adjusting a varchar width, etc. - unless you drop the view first. I'm fine with that &gt; Why didn't you apply the index directly to the table? Oh, I should have specified. There are some joins that need to be done. Thank you for the reply. 
When you say 'API calls' do you mean making SQL server an end point?
What do you mean by "pin"?
&gt; Check your wait types Not sure how to do that. I'm pretty raw at SQL, just started this a year ago.
Check out w3schools sql section. It has a great run down of all the main basic functions like max/min etc really helped me when I was new to sql. 
Can’t you just connect to the SQL Server (not the Analysis Services) and query directly against that?
If you're going to ask someone to do your homework for you, the least you could do is format your SQL. I dunno what engine you're using, but, in Postgres, 20160123 isn't a date, it's an integer.
sorry, i tried using the 4 spaces before each line how would you format then? or what would you suggest 
You also have an errant space in front of [FlightDestAirportCode], don't know if that matters for your SQL.
that's just on reddit here lol 
Dude, you never know, the table could be setup that way... it would be amazingly gross, but I put nothing past anyone. 
I also have a feeling you’re asking questions and then deleting them when you get the answers so there’s no trace of your antics. Shady move.
RAND() is pretty common across many languages... MySQL has it, Maria has it, SQL Server has it...
nah man 
Can you recommend how I would go about it?
Ahh. Yeah, when putting code in here put it on it's own lines and then code escape it and make sure it looks exactly like how you're running it. (It's part of the editor for me... is that not standard? I know RES adds nice things but I've never used reddit without RES.)
Then someone else of doing the same type of assignment this week and being shady like that. But Occam’s Razor and all...
I almost used LMGTFY... https://stackoverflow.com/questions/7878287/generate-random-int-value-from-3-to-6
What RDBMS are you using? SQL Server?
You can use openrowset to query ssas (using mdx) from the database engineside. Mdx and Dax are their own 'languages' within mssql. That is am interesting idea to use ssas for extraction, youd think to use ssis/pure sql to get your measures dims and facts up first. 
Oracle and Microsoft SQL have different strengths. Oracle handles transactions differently. You have to commit more often, and oracle has some very powerful features relating to procedure groups and additional algorithms for analytics (without ssas) Microsoft Sql is more common because of the .net stack and price. Downgrading to mssql is a common thing. With mssql 2019s polyBase additions... I can only see that happening more.
Space is your disadvantage. 500k isnt a lot, I'm pulling 100mil no problems.. follow this pattern. Nonclustered index. Create index ixname on table(join/where column) include (selected columns) Clustered index... you have to imagine every clustered index you have on one database file in one xls sheet, and sorted. On top of your nonclustered indexes if on the same file. 1. Make a new file for non clustered indexes. 2. Your clustered index should start with a value that is unique to the table, this will physically organize your data.
Yep. 2014.
If you have the cash, pick up a copy of [SQL Server Query Performance Tuning](https://www.amazon.com/Server-2017-Query-Performance-Tuning/dp/1484238877/ref=sr_1_1?keywords=sql+server+performance+tuning&amp;qid=1552531142&amp;s=gateway&amp;sr=8-1), it goes over disk issues in the 3rd chapter and generally is a go-to reference for what you're asking. (It doesn't look like Apress made one for 2014 but almost everything in it should still be valid). Very rarely is performance issues directly related to hardware setup, and given the very low number of concurrent users you have if you're users are complaining about performance it's most likely going to be a T-SQL or indexing related issue. 
You could also create your own by casting date time to int and doing some kind of math operation on it to generate a random number. Or you could just use RAND()
&gt; I almost used LMGTFY... &gt; &gt; He's bringing it back!!!
I've seen SQL Server more often than oracle, but that's not scientific at all. 
&gt; SQL Developer It cannot possibly be buggier than SSMS, that's for sure. :) 
Actually this is a college project, we can do it locally. I have few months to learn things needed for this project. I just want to know how can I do it on local. We can use some shitty website which is vulnerable to SQLI, then we will modify the code to detect and stop attacks. 
Yes, if you're looking for unique values, you'll either want a primary key or if it truly has to be unique across everything (or unpredictable), generate guids.
PSA: You can put data in tables on Reddit pretty simply: | ColName1 | ColName2 | ColName3 | | :--- | :--- | :--- | | val1 | null | val2 | | val7 | val42 | val9 | | val4 | val8 | null | Becomes: | ColName1 | ColName2 | ColName3 | | :--- | :--- | :--- | | val1 | null | val2 | | val7 | val42 | val9 | | val4 | val8 | null | 
Can you statically declare the values represented in T3? Or do you need it dynamic... PIVOT could be a solution, you could remove all of your CASE statements.
No, the opposite I'd say, we use it to connect and interact with 3rd party API points.
Probably a school project. 
If you only use it as an identifier, why not use a identity column? Random is not unique, therefore not advised as an identifier. https://docs.microsoft.com/en-us/sql/t-sql/statements/create-table-transact-sql-identity-property?view=sql-server-2017
Thank you!
I can tell you where I practiced from. I found sqlzoo, datacamp, hackerrank, stratascratch and leetcode. Tried them all and found [stratascratch.com](https://www.stratascratch.com) more useful out of those 5. They have datasets pre-loaded with questions and answers you can practice with. They source their questions from technical interviews from companies so I found it helpful to use for interview practice. Datacamp was cool for very specific niches. 
&gt;You have to commit more often, That is nonsense. Actually Oracle is faster if you commit less often i.e. only as often as your transaction boundaries require it.
PL/SQL is **NOT** Oracle's flavor of ANSI SQL! PL/SQL is only used for procedural code, it has nothing to do with the query language "SQL" (apart from the fact that you can embed SQL statements in PL/SQL). If at all, PL/SQL is Oracle's flavor of ANSI SQL/PSM ("Persistent Stored Modules") Unlike Microsoft, Oracle makes a clear distinction between the query language "SQL" and the procedural language PL/SQL &amp;#x200B;
If you are using Postgres, you could put those dates in a [daterange](https://www.postgresql.org/docs/current/rangetypes.html) column. That can be part of the primary key (and thus can be referenced from other tables). A range type would also allow you to define a [constraint](https://www.postgresql.org/docs/current/ddl-constraints.html#DDL-CONSTRAINTS-EXCLUSION) that prevents overlapping ranges
| That | is | neat | | :-- | :-- | :-- | |Thanks | my | friend | 
Replace ProdDate in both instances with CAST(ProdDate AS DATE) to truncate off the timestamp from a datetime field.
I tried out your test case in db-fiddle and got the correct result. Could you take a look and correct any assumptions I've got wrong. https://www.db-fiddle.com/f/bS6S1q3cgL3Xpaicg9ZmZ8/0
my guess would be that your ProdTime column has date and time. If you need to look at the date portion, you could use one of the date functions available on your sql platform (mysql, oracle, etc.) or use the cast function to convert ProdTime values to a simple date.
Hi there! I know this is open-source, but I found a sql gui tool on AWS that is free. It is called [ScaiPlatform](https://aws.amazon.com/marketplace/pp/B07NPPSPJ1?ref=_pntr_red_ps_red_sql_promo_q119) and it is good for sql data exploration, reporting, aggregations, data visualization and it has built-in sql console. You can also see the foreign keys with the original values of the referenced table. 
SELECT t1.\* from Table t1 inner join ( select max(ProdTime) as ProdTime from Table 1) as t2 on t1.ProdTime = t2.ProdTime it should work now. &amp;#x200B;
Is the foreign key relation between the two tables defined as "ON UPDATE CASCADE"?
I have the exact same thing going on - I fought with this for so long that I did a full re-build of WSUS, and I am still getting a count of 55 when I do that database check. I think something might be up with the command on step 4 at the above link, to delete files from tbFile. When I run the following query, I get zero results: &amp;#x200B; **select FileDigest from tbFile where FileName like '%.esd%' except select FileDigest from tbFileForRevision** &amp;#x200B; So to me it looks like nothing is being put in @NotNeededFiles, so nothing is being removed from the database. I've got all the relevant updates installed on the server, I've got the MIME type set up right in IIS, etc. I will update if I figure out what's going on, please let me know if you've made any headway on this.
Whew! That makes things easier. So, in basic terms, I'd set up a website that uses a SQL back end to host a list of names and emails in a table named Accounts or something easy to guess. Then a simple query on a webpage in your front-end language of choice that looks up an email and displays (found/not found)
It's been a long while since I used oracle. From what I remember, it was hard to get out of the implicit transaction ideology. I want to say it was oracle 7 or 9.
Hey, I am just interested in querying database Exam 70-461 , just to want add an asset that I can query so is it mandatory to also take 462 and 463? and any expiration date for those certs? &amp;#x200B;
Yeahhhhh.. That's it. But how to make SQLI attacks on website which is on our PC and not on server, and how to detect and stop sqli attacks. Thanks for replies BTW, you're so helpful.
No, you dont have to take the others. I do not recommend 461, because its 2012 and 2014. I recommend 761. Same thing but its 2016+. Not much has changed in regards to the subject matter. Temporal tables have been added, json support too. But to be honest, I didnt get any json or XML related questions so I'm not sure if they are on 761. Retiring, is the term. They do not expire. We dont know when 461 will get retired. And when it gets retired they offer a transition test for a few. After you pass, you want to head over to youracclaim.com to get your badge and link it to linkedin. When you do that, acclaim explicitly states to mark your cert as 'does not expire'.
I think what you're looking for is to create a row for every month that a given datelog is valid? For this we're going to want to determine a start and end date specifying when the record was active. We can get the "start date" from Status_Since and the "end date" of a datelog row by obtaining the Status_Since of the next row chronologically. Once we have our start and end date span, we can join that to our date table using a between to relate all of the months a row was active, to the valid rows in the date table. Here's an example with some cte's to mock the data you're using: With dates as ( --Faking the Date table SELECT cast('2008-01-01' as date) as MonthStartDate, EOMONTH(cast('2013-01-01' as date)) as MonthEndDate, datename(month, cast('2013-01-01' as date)) as MonthName UNION ALL SELECT dateadd(month , 1, MonthStartDate) AS MonthStartDate, eomonth(dateadd(month , 1, MonthStartDate)) as MonthEndDate, datename(month, dateadd(month , 1, MonthStartDate)) as MonthName FROM dates WHERE dateadd (month, 1, MonthStartDate) &lt; getdate() ), datelog as ( --Faking the Datelog table select '45' as ID, 'Active' as Status, Cast('2008-02-29' as Date) as Status_Since union all select '45' as ID, 'Discontinued' as Status, Cast('2012-06-14' as date) as Status_Since union all select '45' as ID, 'InActive' as Status, Cast('2016-04-5' as date) as Status_Since ), datelog_with_enddate as ( --Demonstrating how to get the date of the next row of a given id chronologically to generate an EndDate for our datelog row Select dl1.ID, dl1.Status, dl1.Status_Since as StartDate, isnull(dl2.Status_Since, '1/1/2070') as EndDate --Arbitrary far future date to represent records with no enddate (aka current or last row), allows us to use a clean join later From datelog dl1 OUTER APPLY ( select TOP 1 Status_Since from datelog dl2 where dl1.id = dl2.id and dl1.Status_Since &lt; dl2.Status_since ORDER BY Status_Since asc ) dl2 ) SELECT * FROM dates d join datelog_with_enddate dl on d.MonthStartDate between dl.StartDate and dl.EndDate order by d.MonthStartDate OPTION (MAXRECURSION 200) 
Comment out the 'AND operatorname =' and add a GROUP BY operatorname.
It is quite easy using python. &amp;#x200B; Integrate google sheet with Python export data as for example csv and then import to SQL table. There are many tutorials about it :)
If you have a while, I'm actually working on a blog post this weekend about sqli, so I'm going to kind of do a walk-through. I'll be using Linux and cloud servers, but the premise will be close. Remind me next week and I'll link you to it. Attacking a local website is just like a remote one. Use the webpage to input SQL code into the search/action box and see what happens. For defense, log what goes into the box, and update your search box validation according to what you see.
Thank you, im just starting with SQL, my idea was to start with it, learn a bit and then move to Python. Should i just move to Python faster? 
Extras: They gave me server paths like example-dz-ysbs827\example_DZ_BAD
&gt;A client just gave me a list of servers and corresponding environments but I cant see any of them... &gt; &gt;I dont want to look like an idiot to the client (though I am) so what am I missing here? that looks like a path.. what kind of server are you trying to connect to?
DB engine, Im thinking it has to do with my access though?
Ok, SQL Server normally uses that syntax for databases, computername/sqlserverinstance, but that would imply connecting locally to it. &amp;#x200B; Server name: computername/sqlserverinstance (default) Authentication: Windows authentication (default for local instances) &amp;#x200B; You can connect through SSMS and it will choose the port automatically for you &amp;#x200B; If it's an external database you're going to need to specify an ip or url.
Why not just save the Google sheet as a .csv flat file and import the .csv file into sql? Why waste time using Python as the middle man? Doesn't make sense to me when every single database engine supports csv files. Python is a great learning tool though so I guess it can be beneficial but I wouldn't see why you would need it for something so simple.
What if we want to have the connection live, where any updates to the sheet automatically updates the database as well. Your method of downloading as a .csv file and importing that won't work here. 
Use a spreadsheet that supports ODBC connections. Simple. 
&gt; Only thing I had to do different is make the table alias unique Take it or leave it, but `CROSS APPLY` aliases are in the form `AS TableAlias(ColumnName1,ColumnName2)`. So for xml nodes, I prefer the form `AS RootElement(GenericColumn)` rather than `GenericAlias(RootElement)`. Or even `AS RootElement(RootElement)` if you're so inclined. So in your example: CROSS APPLY MY_XML.nodes('REQUISITIONS/REQUSITION') AS REQUISITIONS(XMLDATA) OUTER APPLY MY_XML.REQUISITIONS.nodes('LINEITEM') AS LINEITEM(XMLDATA)
Can you just download SSMS 18 ? It has the option for Dark in Tools =&gt; Options =&gt; General =&gt; Theme (at least for me... I'm 99% sure I haven't customized it in any way)
If you don't want something to access your data, then stop it from accessing it. If another system / application / user is using the same credentials, then fix that.
1. learn the business workflow 2. map business workflow data/outcomes to the data used/captured 3. locate/map data used/captured to the DB structures 4. profit 
There isn't a question here... you should do your own homework... but also you can't have a column in your SELECT that isn't using an aggregate function or in your `group by` clause... so... add the `STUDENT.STUDENT_NAME` to your `group by` and see if it's working.
WHAT IS EVERYONE YELLING ABOUT?
Any particular database platform? MS? Oracle? MySql? Postgres? DB2? Is it a database that is designed and maintained in-house? Is there an application front end attached to it? If there is an application, did your company write it internally or is it a 3rd party vendor? If it's a vendor's app and/or db, you should be able to get a field descriptions manual describing all of the tables and the fields, and what their purpose. If it's an internal database/application written by your company, someone must have documented something, you'd hope! But if you want to start now without getting any documentation, start looking at the data dictionary and see what tables have owners that are different from an out of box install of the seeded database. Then query your non-seeded tables by size so you can see which of those tables are the largest. Check out indexes on some of the larger tables. Those are probably the big dogs. You could start capturing some statistics on table sizes so you can see which ones are growing. You could also start documenting primary keys and foreign keys. You can infer how to join tables by finding alike keys, but that doesn't guarantee anything because it depends on how it was designed... unless you are the author, you'd need documentation for that. Anything else is simply an educated guess. If it is truly very normalized, most of the time it's a software/db package written by a third party and not in-house. At least that's been my experience. Everywhere that has a "data warehouse" that I've worked at is more like a "data landfill", a lawless place with no integrity with a banner message that reads "Abandon all hope ye who enter here". Good luck! 
I would look at fixing the query or creating proper indexes so it doesn't cause performance problems... you could always just write TSQL to get the session_id of the broken query and kill it... then just run this every 1 minute or something... just need to update the program name (shown in `sp_who2`) and part of the SQL query (being sure it doesn't overlap with stuff you do not want to kill): DECLARE @t TABLE (session_id int) INSERT INTO @t SELECT ses.session_id FROM sys.dm_exec_requests req INNER JOIN sys.dm_exec_sessions ses on req.session_id = ses.session_id CROSS APPLY sys.dm_exec_sql_text(sql_handle) AS sqltext WHERE ses.program_name = 'Microsoft SQL Server Management Studio - Query' AND sqltext.text LIKE 'SELECT%' DECLARE @n int, @i int= 0, @s int, @kill nvarchar(20)= 'kill ', @sql nvarchar (255) SELECT @n = COUNT(*) FROM @t WHILE @i &lt; @n BEGIN SELECT TOP 1 @s = session_id from @t SET @sql = @kill + cast(@s as nvarchar(10)) EXECUTE sp_executesql @sql delete from @t where session_id = @s SET @i = @i + 1 END Borrowed some TSQL from here: https://stackoverflow.com/questions/37252753/automatically-kill-session-of-some-specific-task
https://i.imgur.com/OnPB4RF.gif
Join your key in the subselect you are using. Basically whatever the value is that is causing you to need a max date. Select mt.\* From Table mt Where mt.ProdTime = (Select Max(t1.ProdTime) From Table t1 WHERE t1."SOME ID IN Table" = mt."SOME ID IN Table"
Agree on the upgrade to the 18 beta. It is pretty nice and dark mode is unhidden. The contextual menu colors for the object explorer are still messed up though. Dark gray text on black background...
So... technically, yes this is possible. I'm building an application for a client that does this at the moment. However, in practice there are a lot of pitfalls. For example, what happens when someone inputs data that doens't conform to what should be in the database? Or is outside the boundaries of the table? How are you going to handle conflicts between the database and the spreadsheets? It also isn't a cakewalk to get past the OAuth authentication in whatever language you decide to use fwiw. If you are not an experienced developer, I would highly suggest another workaround for the problem you are trying to solve.
Dark theme still sucks in SSMS 18... Just go to DbName --&gt; Tasks... The contrast is all off!
Start with biggest tables, and follow foreign keys.
This is going to find every session of SSMS that is running a SELECT and kill it. Effective if you have always on set up and want to enforce users not connecting to the primary node... but I would definately add some logging to a table.
You need to understand what this query is, and why it's being sent. If it's a reasonable query being used for legitimate purposes by an entity you trust... optimize a bit. &amp;#x200B; If it's some wildcard coming out of nowhere, block the hell out of that user and tidy up your administration.
Devil's advocate: Is your SQL DB underprovisioned?
As always, Brent Ozar with the recent &amp; relevant blog post: https://www.brentozar.com/archive/2019/03/pop-quiz-what-do-these-things-cost/
I think this would work. I also think it's a terrible solution. SELECT * from TABLE where year(ProdTime) = year((select max(ProdTime) from TABLE)) and month(ProdTime) = month ((select max(ProdTime) from TABLE)) and day(ProdTime) = day((select max(ProdTime) from TABLE)) 
Fantastic info, thanks again! Very new to querying XML. 
Seems a lot of posts recently (to me) with homework questions. Listen-up, cheater. SQL is awesome, and you'd do well for yourself to learn it. Unfortunately, I imagine you're being taught SQL, and not the relational-theory it's predicated on. If you're serious about SQL, learn RT first, or in tandem. I suggest "SQL and Relational Theory", by Chris Date. If you're not interested in learning SQL, well... less competition for me.
As a general thing, I'm pretty sure that you should really only be keeping these values in one place and this whole thing is ill-advised, but I only vaguely understand this language (and your specific situation even less) I think maybe this will work? UPDATE TABLEONE SET product_group = (select top(1) product_group from TABLETWO where TABLETWO.product_code = TABLEONE.product_code)
You can use SQL (or a form of it) directly in Google Sheets. https://www.benlcollins.com/spreadsheets/google-sheets-query-sql/
aye, assuming OP doesn't modify the TSQL as I indicated :P 
First off all, run a trace to see who the user that's calling the query is. If the user is specific to that app /SP call, then remove that user from the DB/instance (for an instance, it's a login; for a DB, it's a user, they map together). If this isn't possible due to other factors, like the login/user being used by multiple apps and connection strings can't be safely changed, then comment out the code within the rouge SP and replace it with "PRINT 'SP disabled'" (or whatever you want to stop the code running without causing errors every minutes). 
I usually wrap one-off scripts like this against production databases in a begin transaction/rollback, verify the results, and then run them again with a commit.
In my experience it's helpful to write a `SELECT` first, verify the relation it produces is correct, then use that same `SELECT` as the basis of your update, e.g. ``` UPDATE ... FROM (SELECT ...) AS r WHERE ... ```
I've done that. rollback; &amp;#x200B; or if it's committed, truncate the table and select into table (select \* from table as of timestamp - 10 minutes) , if you have flashback enabled (or temporal tables in newer versions of MS). &amp;#x200B; Flashback has saved my ass for same-day changes numerous times. 
I have mixed feelings about this. I've seen it cause more production outages than botched updates. The risk is a begin transaction accidentally gets run without a commit, even on just a select statement. Next thing you know everything in the database has been blocked for 20 minutes.
Hi. Generally in SQL, `UPDATE`s and `DELETE`s can be thought of as kinds of `SELECT`s. In other words, if you can write a `SELECT` that produces the relation you want, then it's trivial to write an `UPDATE` or `DELETE`. So I'd focus on writing a `SELECT` that produces the relation ("table") you want first, then shift your focus to writing the `UPDATE`. Example syntax: ``` UPDATE relationA SET attribute1 = r.attribute1 FROM ( SELECT ... ) AS r WHERE relationA.attribute1 = r.attribute1 ; ``` The sub-query in `FROM` can be arbitrarily-complex. You can use `JOIN`, etc., in there, and that result of that sub-query becomes available to your `UPDATE`. Hope that helps.
I've never once had that happen, but I could see how it could be an issue.
*waits for someone to truncate that doesn't have any of that...* 😂 We always do a full table backup to a temp before making a manual change.
Thanks for that, just to clarify though, when i insert them back into the table will they overwrite the names against the PersonID in the new order? 
For sure!! When I have to do an insert or update, if I'm not 100% positive, I'll make it a select and see the results, or stick a "begin Tran" in front 
The scrambling part worked but it doesn't like me trying to insert it back into the Main table?!? --RAN table scrambled Select FirstName Into Person_ScrambleFirstname2 From Person ORDER BY RAND() --Ran table scrambled Select Surname Into Person_scrambleSurname2 From Person ORDER by SURNAME DESC; --Ran table scrambled Select Middlename Into Person_scrambleMiddlename2 From Person ORDER by Middlename ASC; --recombine into Main table Insert into Person Select A.Firstname,B.Surname,C.Middlename From Person_ScrambleFirstname2 A, Person_scrambleSurname2 B, Person_scrambleMiddlename2 C Msg 213, Level 16, State 1, Line 25 Column name or number of supplied values does not match table definition. 
I'm not really sure how it would happen if you just put BEGIN at the beginning and then ROLLBACK or COMMIT at the end. I mean, yeah, if you forget the second part it *could* cause problems, but by that logic, you could screw up the UPDATE itself so you shouldn't do *that* either.
Man with prod data I never do an update without first doing a rollback so i can see exactly how many rows are changed. I begin transaction; select count(\*) first on the data then do my update select count(\*) rollback; That way i always know how many records are going to be edited. If something looks off i can fix it until it looks right, then i comment out the rollback and run the commit. &amp;#x200B; &amp;#x200B;
Do rollbacks they have saved my hide many times.
If the update, delete is manegeable in terms of size, I save the data to be updated/deleted into a separate table with same name and append _Staging. I run the update/delete on final table and validate all is ok before getting rid of staging table. This method was initially for PROD critical tables only, but I apply it all the time as best practice.
&gt;70-761 761 makes sense , I found this only affordable study resource [https://www.udemy.com/ms-sql-server-70-761/](https://www.udemy.com/ms-sql-server-70-761/) Can you provide with more if you have?
Same. Also selects before and after to be sure the result is expected.
Exactly what I do.
There should be a test database that refreshed against production on a needed frequency, and you do your testing there. Something being committed to Production should always be a quick transaction. And every transaction should be joining on a backup table you just created. At least this is how it's done where I work.
https://www.youtube.com/playlist?list=PLPI9hmrj2Vd8vkFnf-dwWU3W85pct-tLj These are mine, only a few more videos left and I'll be creating a few practice exams as an outro. I tried watching what you linked... it was 75% power point. I highly recommend the 'green book'. I snagged 762 recently, worked for me, I'll be getting 767 and 768 soon. https://www.amazon.com/gp/aw/d/1509304339/ref=tmm_pap_title_0?ie=UTF8&amp;qid=&amp;sr=
How the fuck does anyone do any kind of dangerous query without begin...rollback tran? It's the best feature of the entire language imo. I'd refuse to do my job if it wasn't there lol. I get anxiety just thinking about it. 
I always surround OR statements with parentheses for this very reason. UPDATE TABLE WHERE EMPLOYEEID = 'somedude' and ( PAYCODE = 'bla1' or PAYCODE = 'bla2" or PAYCODE = 'bla3' ) SET INACTIVE = 1
It's also helpful to get someone else to do it. So you, you know, don't have to worry about screwing it up.
I would suggest creating a temp table from the update table just never do it with your transaction tables as they're usually very large. You can always recover from a bad query if you copy the table you're updating. Yeah the over head might be large but guarantees are not usually free. You need to ensure you clean up your temp table once you're sure the query executed successfully, gotta hate wasted disk space. 
I like your alias. You may not have to worry about screwing-it-up \*yourself\*...
How does one even write an update statement like that? :P Luckily you were spared. 
Try this, &amp;#x200B; SELECT OPSNAME,MONTH,SUM(COUNT) FROM (select 'Operators Name' AS OPSNAME, 1 as COUNT, CASE NVL(month(DT),0) WHEN 12 then 'DEC' WHEN 11 then 'NOV' WHEN 10 then 'OCT' WHEN 9 then 'SEP' WHEN 8 then 'AUG' WHEN 7 then 'JUL' WHEN 6 then 'JUN' WHEN 5 then 'MAY' WHEN 4 then 'APR' WHEN 3 then 'MAR' WHEN 2 then 'FEB' WHEN 1 then 'JAN' ELSE 0 END as MONTH from MyTable where Status = 'Complete' AND operatorname = 'name') GROUP BY OPSNAME,MONTH; &amp;#x200B; You don't really need an NVL function on a CASE with an ELSE as it should fall into the else if NULL. &amp;#x200B; Your issue, which is a common one, you have a column for each month, you want 1 column for month and to group by that column and operator name. 
It's always worth having the transaction logs on a separate disk (and keeping the ALU sizes proper). At the same time, I would highly look at your SLA. If it's not in dollars/minute or something similar, it may not be worth the cost. 
Sorry it happened to you, but hey, it’s in these situations that we learn some things. My general rule of thumb is anything over 10 records I wrap it in a BEGIN TRAN and COMMIT / ROLLBACK with a select count * depending on if it had the desired result or not. Some DBAs always wrap their code in BEGIN/COMMIT but if it’s a low number of records I personally don’t. Just remember to either commit or rollback once done or you’ll cause deadlocks :-) 
Depends on the isolation level the other accessors use.
As I got to the end of this my heart started thumping for you
As I got to the end of this my heart started thumping for you
How can I view the specific foreign keys? 
What is the DDL? 
Unfortunately the field names are not the same :/ In some tables the responsible manager is in the column „manager“ and in others it is called „owner“
Use those parenthesis whenever you're tossing an `or` into the query.
DDL is basically the `create table` statement used to create your tables. It will show the primary key to foreign key relations. Try this: https://www.mssqltips.com/sqlservertip/4753/list-dependencies-for-sql-server-foreign-keys/
I love Oracle Do whatever you want, it just changes when you're committing :D
My MAN! That is amazing, thank you. I've only had to change d.MonthStartDate in the last query to MonthEndDate (or in my case, DateValue WHERE IsLastDayOfMonth = 1) because I want to know the status on the last day of the month and not measured on the first day of the next month. Enjoy some gold!
Dude, Always write any query that updates or changes data (DML) as a transaction So something like BEGIN TRANS UPDATE ... WHERE .. --ROLLBACK --COMMIT When you run, you can check if your data looks correct in the same window. If you are not happy run the ROLLBACK, if you are run the COMMIT 
It doesn't take much to run ssms. Is a 32 bit application, so it will only use 2 GB of RAM anyway. 
Simply run it on a test database first and see how it performs. My procedure it this: * Write SELECT query that fetches the relevant data * Transform it to an UPDATE query * Wrap the UPDATE statement in the SELECT query before and after, and wrap the whole thing in BEGIN TRAN / ROLLBACK TRAN * Run it on a test database, verify the before/after SELECTs, verify number of rows affected * Run it on a test database and COMMIT the transaction (or just amit the BEGIN/COMMIT thing) * Run the wrapped query on production, verify the before/after SELECTs, verify number of rows affected * Run it on production and commit If you want to be even more safe, take a backup of production just before executing it.
If you're serious about learning SQL, learn the theory upon which it is based: relational theory. &amp;#x200B; A book I recommend "SQL and Relational Theory", by Chris Date. &amp;#x200B; As for actually using SQL, I recommend PostgreSQL. It is free, has powerful features, and closely follows the SQL specification. &amp;#x200B; Any desktop/laptop computer made in the last 6-8 years will easily run PostgreSQL.
I learned it the hard way, I deleted a company's database and was applied by my senior, from them on I always select then delete /update
If you are operating in a system where you're expected to be johnny badass in production this is what I have done: 1) Run a select query to pull your records where clause prior to running any insert/update/delete. ALWAYS 2) Start a transaction after you confirmed your select works 3) run the update/insert/delete 4) run the select again If your select doesn't look the way you expect and/or the returned record counts didn't work ROLLBACK. This has saved my ass plenty of times. The cost of doing this forever is WAY WAY lower than the pain of recovering from one failure. Not to mention you are not going to lose face to all the people you would have screwed over in an unprotected scenario. NEVER let yourself cowboy a transaction. Protect yourself to protect your company. If management complains about 60 seconds of care and you can't explain this effectively you are in the wrong job find a new one.
Note: all initial values of table_A.name_im are NULL
Thanks a lot. Appreciate! 
Thanks. Appreciate!
Check on MERGE Statement. When matched then update is what you're looking for. 
Begin Tran/commit/roll back is my friend.
Update table_a Set a.name_im = b.name_comp where a.id = b.id; Hope this works
Do I need a condition to specify Where a.id = b.id AND a.name_im = null ?
Not needed as all the columns are NULL and we are updating whole column info together
If you wan't to update values ONLY where name\_im in table A are null, then yes. Othervise, you don't need to, but it will be updated for all rows.
SQL Server (and others) supports [update from](https://www.techonthenet.com/sql_server/update.php). Oracle uses [merg into](https://docs.oracle.com/cd/B28359_01/server.111/b28286/statements_9016.htm#SQLRF01606) for the same purpose.
*Always* always write in a SELECT statement first, then just comment it out once you're reeeally sure you're going to get your intended results with the UPDATE. When I first stated writing SQL a senior level engineer told me about that and it's saved me so many times! I rearranged your code a little and added an alias to your table but this is the gist. UPDATE a SET INACTIVE = 1 --SELECT * FROM TABLE a WHERE EMPLOYEEID = 'somedude' and PAYCODE = 'bla1' or PAYCODE = 'bla2" or PAYCODE = 'bla3' &amp;#x200B;
 SELECT NAMES.NAME ,TITLES.TITLE ,MIN(TITLES.AVERAGERATING) FROM TITLES JOIN KNOWN_FOR ON KNOWN_FOR.TITLEID = TITLES.ID JOIN NAMES ON KNOWN_FOR.KNOWN_FOR = NAMES.ID JOIN PROFESSIONS ON PROFESSIONS.NAMESID = NAMES.ID WHERE PROFESSIONS.TITLE = 'director' GROUP BY NAMES.NAME, TITLES.TITLE HAVING MIN(TITLES.AVERAGERATING) &lt; 4.6 ORDER BY TITLES.AVERAGERATING DESC; This should get you what you're looking for. 
Thanks, I'll give this a shot.
I bet your task/homework is around subquery usage. Look into that.
Here is how I write updates: `SELECT *` `-- UPDATE T SET field = 'BLAH'` `FROM TABLE T` `WHERE BLAH = BLAH` &amp;#x200B; Run it in SELECT mode and however you copy and paste it, it's NON-VOLATILE until you select starting with the update. If you don't get EXACTLY what you want to update during the SELECT portion, then don't run the update.
Something like this? UPDATE TableA SET TableA.Name=TableB.Name FROM TableA INNER JOIN TableB ON TableA.ID=TableB.ID 
UPDATE a_table SET a_table.name_im = b_table.name_comp FROM actual_table_name a_table JOIN actual_B_table b_table on a_table.ID = b_table.ID WHERE a_table.name is null
Windows at least 8GB RAM and 4 CPU's
Do you also have any youtube videos you recommend for learning relational theory as it applies to SQL?
may not have known about it? it took four years of work experience (two job changes) before someone showed me that feature. my mind was blown, fwiw, and would have completely saved this one guy's job that I ended up taking on that got me into the industry. he ran a delete statement that wiped out YEARS of transactions when he was only modifying one person's account and was asked not to come back after it was fixed later that day.
Yeah it was more just surprise that people actually issue dangerous statements without knowing about that. Poor co-worker of yours that had no idea...man that would suck. I was obviously very fortunate in having a great mentor who taught me the transaction syntax the first time I asked about doing any of the non-select DML statements. If I ever become someone's mentor I will make sure to do the same lol 
Thanks !
lol
Thats what I said internally!
On YouTube? Not any that come to mind. O'Reilly has a paid subscription, and therein are videos by Chris Date; basically video versions of his book. I imagine a search on YT for "relational theory" will get you *somewhere*. "Functional dependencies", "normal forms", "domains", "keys", etc., are some of the core concepts, and therefore search-terms, you might try using. Quality on YT will vary greatly; I'm willing to believe at least 1 creator will know what he's talking about, but you will probably have to cut the wheat from the chaff, and since you don't know what you don't know, that may be difficult. Sorry I can't provide an easy "watch this video" vis-a-vis YT.
changed this part CAST('''' AS XML) - Did not have enough single quotes. &amp;#x200B;
I'm not certain what you mean by &gt; I have complete control over the values the software passes in the query and there are about 4 other fields that can be used as conditions. ​As that should mean you have access to the code which would make things simple enough if you know how to code. Without access to the code of the software to change the actual query though, not really as you'd have to perform some sort of SQL Injection and if your system allows THAT you have bigger problems.
I don't have access to the code (I certainly wish I did) but I do have complete access to the database. Items can be configured through XML, and the software uses values provided in specific elements to execute the query. That's what I mean by having control over the values passed. &amp;#x200B; It seems like I will just have to insert a few hundred thousand records into the pricing table, which is what I was trying to avoid.
can do either &gt; select top 1 then add &gt; order by Activities.ActivityID desc at the end. could also just go with &gt; select max(activities.activitiyID)
 SELECT max(Activities.ActivityId) ,Businesses.PersonId FROM dbo.Activities INNER JOIN dbo.Businesses ON Activities.PersonId = Businesses.PersonId GROUP BY Businesses.PersonId
SELECT TOP 1 WITH TIES * FROM dbo.Activities ORDER BY ROWNUMBER() OVER (PARTITION BY PersonId ORDER BY ActivityId DESC)
Use an ETL : these softwares are specifically designed to address your issues. You can try Pentaho Data Integration. The Commmunity Edition is free, open-source, and pretty easy to use.
I use PDI (Pentaho Data Integration), community ed. It's FOSS. Works well w CSVs. Talks to Oracle. Runs everywhere (except BSDs for some reason). There are probably more light-weight solutions, but it's a good general tool
Do you mean similar to an identity column in msSQL server? For that you would use a sequence.
 --- Select from Update tblproducts set upccode = '', Flag = 'T' -- where pruduct ID in (*small list of productids*)
Wow, my formatting is ruined. Anyone know how to fix this? 
Bulk load everything. Fastest way is to load it in a temporary table that is identical to the CSV structure, then, in 1 query per operation, move the data you need in the place you need. I'll take Postgresql as an example because I know it best. * Use `psql`, and `COPY mytable FROM STDIN WITH (FORMAT CSV)`, and simply pipe the CSV data to the process. If you need to split the data horizontally (ex. by month) you can make the target table partitioned, and pre-create the needed partitions. Otherwise you just copy and then move the data where needed. You can also declare the table as UNLOGGED for better speed. * After loading the data into a staging area (temporary tables), you can make queries that move it into a permanent table, with ON CONFLICT DO NOTHING clause. * Why would you need the key? You can just pre-generate the keys I guess. * Use string manipulation functions to split the strings when you move the data from the staging area to its destination. The benefits of this approach is that it's all code (you can version control it on git), it's compact, it's easy to read and understand even from a phone and doesn't require special software. And the best advantage is that it's fast. If you load the data, record-level, into some software, then process it and create transactions to push the data, it's going to be orders of magnitude slower than bulk operations and/or stream processing. Another option that I've mentioned is stream processing. Input stream is CSV text, output is CSV text that goes to the database. Avoid creating complex objects and making it about transactions. Python makes this easy by offering fast CSV processing in a back end written in C, and you can do basic manipulations and stream splitting there (redirect one stream to 2 or more target streams to 2 different db connections). It's not just Postgresql that supports bulk data loading like that. MSSQL has BCP, which is very hard to use but much faster, and a bit buggy at times. MySQL has at least fast CSV export but I'd just stay away from MySQL if possible. If you want comparisons in speed: In 30 minutes I wrote the code to move a table with 130 columns and 40 million rows from one database to a database engine that's completely new to me (zero prior experience). In 10 minutes the data was copied. Same data being copied to Tableau on a similar hardware (tableau uses cursors, transactions, row level operations) takes 2 hours. I tried to make a very fast data mover using ADO.NET, with batch operations, prepared statements, object reuse where possible and the least garbage collection, and no complex object creation, really just barebones, it was 10 times slower than using sqsh, and 100 times slower than BCP. I made a hiring test with some ETL task. The same data amount processed in bulk takes around 5 minutes on my laptop. A candidate came with a row-level processing solution in Talend, execution time 4 hours for just a part of the task. You'd be amazed how easily you can lose your performance on basic stuff like this, and then you have to wait for 5 hours for nothing. SQL is fast, indexes are fast, streams are fast, bulk is fast
yup.. have had to fix a few of those in my times. they get probationary read only access after that for a while.
Yeah, fuck those people. 
&gt; Anyone know how to fix this? put four spaces at the front of each line of code I have a table called "source" with : ColA ColB ColC Red null usa Red john null Red null null Blue alan uk Blue null null With the end goal being : ColA ColB ColC Red john usa Red john usa Red john usa Blue alan uk Blue alan uk
 ColA ColB 4 4 
Let me test this ColA ColB ColC Test test test Test test test 
you neglected to mention which database you're on, and the syntax for joined updates can vary try this -- UPDATE source INNER JOIN ( SELECT ColA , MAX(ColB) AS MaxB , MAX(ColC) AS MaxC FROM source GROUP BY ColA ) AS maxes ON maxes.ColA = source.ColA SET source.ColB = maxes.MaxB , source.ColC = maxes.MaxC 
Let me test this ColA ColB ColC Test test test Test test test 
Test &gt; Anyone know how to fix this? ColA ColB ColC Test test1 test2 Test3 test4 test5 
Oh I'm so sorry! Its netezza 
Will this still work in netezza?
i have no idea what happened when you tested it? ™
I'm the one that did it and I'm the one with admin access :|
You have a backup?
of course; flat files to one backup server, veam replication to a rolling set of daily and weekly removable discs along with another flat file version and an offsite removable disc.
Select * From activity a1 Where activityid&gt;= all ( Select acticityid From activity a2 Where a1.group = a2.group )
Nice. Sounds like you’re set up. I’m lucky in that I have the day priors data already up and ready to go in case I need to do something quickly because we use it for various feeds and stuff so we don’t have to read from production tables albeit a day old it’s usually good enough.
Flashback table tablename to before drop or FLASHBACK TABLE tablename TO TIMESTAMP (SYSTIMESTAMP - INTERVAL 'x' minute); (where x is how many minutes ago you did it) &lt;3 oracle
What rule are you using to make this record set Red null USA Into Red John USA ?
not gonna lie i've definitely done: delete from budget_table --where date = ____ and gl_code = ____ and account_number = _____
Where that backup at Doe!? 
Never had the opportunity to do db admin work but I do appreciate the ban hammer powers they give.
Yeah unfortunately without entering into the realm of SQL Injection you probably won't be able to do it then.
I too do this. It has saved my bacon a time or two. 
"BCP simulation"
My safe-ish pattern is SELECT Col1, Col2, Col3, ... Coln, -- UPDATE t SET ColToUpdate = NewValueExpression FROM MyTable AS t WHERE &lt;logical predicate goes here&gt; This has served me well and saved me from making stupid mistakes because it allows to to simulate an update by seeing what the new value would be so I can validate it. When I'm ready to run the update, I highlight the statement beginning with UPDATE on through the predicate. Also, BEGIN TRAN &lt;statement&gt; ROLLBACK helps, too.
In prod always Begin Transaction Query Commit **Rollback**
If you need more of a boost, try TRUNCATE instead!
After a problematic rails migration this week, I now hold the team record for dropping tables in prod. Goood times.
I would.... Die. Or rather die.
Thanks for the recommendation, it worked!
Yep, did this the other day...was definitely awake after that. Luckily nightly backups and very little use that morning saved my ass.
Hahahahahahahahahah this is so not funny cause it happened to me and I still feel the hot sweats....
&gt;Could someone explain what my attempt is actually saying so I can know Well, for starters, for every Reviewer (R) you're getting the EXACTLY SAME person from Reviewer (R2) by doing r.rID = r2.rID (assuming rID is the key to that table). And then you add more joins to that. &gt; Logic: I know I need to have a table with 2 Reviewer Name columns and 2 Movie columns. I apply the condition so that the movies have to equal each other and the condition that the Id's cannot be the same as the question says I'm not sure what exactly you mean by the above and if taken literally, it seems to be an incorrect rephrasing of the task. The question rather clearly asks for some information based on "pairs of reviewers [...that...] gave a rating to the same movie". Assuming rID is a proper key to the Reviewer table, that base information should come solely from the Rating data set. That's not all that is being asked, but it is where you could start. Alternatively, you can notice the granularity of your desired output (pairs of reviewers) and get all of these first. 
What do you mean by "database development"? Are you talking about using Java or C++ to build a SQL-engine? Or extending an existing engine? Or using Java or C++ to build an application that uses a database? I'm under the impression you can write very procedural code in C++. I dunno about Java.
If you want to be "top of the class", you can do no better than to read Chris Date's "Normal Forms and All That Jazz". This is NOT a simple book. But if you can digest the first, I think it was 3 or 4 chapters that go up to BCNF, you WILL be at the top of your class, and probably above most SQL "professionals".
And I am curious, what does 1NF mean to you?
Not sure which is worse... Bad update or initializing replication with the wrong database... 
See if this is over your head: &amp;#x200B; [https://www.youtube.com/watch?v=NNjUhvvwOrk](https://www.youtube.com/watch?v=NNjUhvvwOrk)
My usual approach is to write a Perl script that reads the data file using Text::CSV, do whatever needs to be done, and inserts the cleaned up data in the database using the appropriate DBI driver.
Thanks for giving me flashbacks on a Friday night. More bourbon, ahoy! 
Select Sum(Case when datediff(month, date, current_timestamp) between 1 and 12 then column_value else 0. End)/12 as avg_column_value, Sum(Case when datediff(month, date, current_timestamp) = 0 then column_value else 0 end) as sum_current_column_value From &lt;table&gt; Where datediff(month, date, current_timestamp between) 0 and 12
I’ll raise you an update query, if you forget to put a where statement on the end in a live environment. That right there is a game over.
Design, develop, and implementing database systems based on varying needs. Creating complex functions, scripts, stored procedures and triggers to support application development, etc. I dunno. I suppose it's more of a comfort-level thing, because I already am familiar with Python--in a relatively vague sense--and, therefore prefer it to OOP languages: and it's farther away from the hardware, making it significantly easier to learn. I'm sure you can write procedural code with C++, as you can also write very object oriented code with Python, but... My main concern is ease of learning, for my first language, I suppose.
&gt; ave been asked to find Temne course is name of assessment couyld you rephrase this please? also, show the table layout
Ok sorry for the confusion I’m not entirely sure what I have written there but I’ve fixed the issue now. I had used a left join where I needed to use an inner join. All good now thank you though 
Java is too slow to develop db engine, most likely you ll be programming to access db and implement business logic on data. Go with Java as it gives you skills which has more economic potential however Python is also oop language.
Language and syntax is easy. It takes a couple weeks or so to just be familiar with the different common functions and what the different JOIN conditions do. Writing basic queries on an existing data structure will be easy. You can learn efficiency and how to read execution plans as you go. If you're building a database, then figuring out the proper level of normalization for an efficient and maintainable data structure is the harder part. That takes a few months of practice to get competent for smaller projects, and for larger projects you probably won't ever stop learning.
to be honest, it's kind of like chess you can learn the basic syntax (how the pieces move) in under an hour, but it takes a lifetime to really master it 
Is the problem solving side to it different to regular programming?
Are you learning software development? On an application of any size, you wouldn't write your own SQL. That's handled by the Data Mapper layer, often called ORM. In .net this is called Entity Framework. Not every app uses a data mapper so ymmv. I only mention this in case it helps you have a full picture, it's certainly good to have some familiarity with SQL syntax. 
Join
How should I go about this problem? Table A is bigger than Table B
Left join
select a.id\_a, b.id\_b from \[table a\] a inner join \[table b\] b on [a.name](https://a.name) = b.name
Ooooo, that horrible leap your heart makes! I renamed everybody in a table to Tanya yesterday...thank god for backups.
What is a merge?
No it's just really for reporting in terms of sql. And I'm learning python and VBA as well. But for SQL, it's reporting only
I'd argue that on an application of greater size it would be more important to be able to write custom SQL for pulling views and certain higher priority complex CRUD operations which will run better as stored procedures. EF will do a lot of cool stuff, and generally quickly when used correctly, but sometimes it takes both.
It's a Join, and you KNOW it's a Join because the columns of the able are different. In general, Joins extend columns. In general, Merges extend rows.
very much so, yes regular programming attacks a problem one record at a time sql attacks a problem all rows at once -- it's a **set-based** language
Full outer join
You may want to truncate the table then drop. 
Look up Group By logic in google. That should be the function you’re looking for
Amazing thank you 😊 
here's how i do it -- DROP TABLE table_a if it doesn't exist, then after that statement runs, it still doesn't exist if it ~does~ exist, then after that statement runs, it doesn't exist now 
the definitive list of important things for reporting -- 1. the data has to be correct
For SQL, you're going to want to focus on SELECT with JOINs, WHERE, and GROUP BY / ORDER BY clauses. With group by you're going to want to focus on aggregate functions (min, max, sum, average)
Merge is a combination of insert, update, and delete in one statment.
Get a Mac. As HR you're gonna have a lot of meetings and most windows laptops aren't equipped with good batteries. A Mac could pull you through a day. You'll also love the track pad. I wouldn't use a Mac for the life of me, because I'm a Dev and use Linux. At home I use Windows for multimedia, but neither of them will be suitable, in my opinion, for a good general purpose HR professional. I'd say a 13", with 8GB RAM.
Though, from experience, such exclude constraints are very very expensive in terms of performance.
I'd have an aneurysm if I were maintaining this table or responsible for this query. There's no guarantee that `name` is unique or that the results would be predictable.
A few months into my first Oracle job I did this when a user wanted a table copied from PD to QA but I did it in reverse by mistake. Luckily it was a fairly static table and I was able to flashback with no business impact and I've never not triple checked the environment I'm in since.
Technically it should work with this dataset but I'd go for an inner join.
Order by a.id_a
Thanks for this. I will focus on these. Once I've done these and got comfortable with them, what should I move on to? Thanks
Right, it feels like a "lets just throw this at the wall and see what sticks" situation.
The important part is to give the actual task of building the SQL statements to some pleb while you take all the credit for building the report. 
Are you using reporting software or are you just building reports from scratch using straight SQL?
They'll be built with SQL from scratch
&gt; you wouldn't write your own SQL. That's handled by the Data Mapper layer, It depends. I have a lot of query based stuff that I pulled out of Hibernate and put into straight SQL because it was much faster. 
Empires have been built on that mantra.
Agreed. This just seems like some dumb honework problem though.
Instead of INSERT INTO you would use MERGE INTO if you wanted do complicated inserts that are based on whether or not the data you want to insert already exists in the table and you just want to either update it or flat out not insert it if so. 
Okay. Sounds similar to Postgres's \`INSERT ... ON CONFLICT DO ...\`. Thanks.
I thank both you and /u/NeatHedgehog for the correction
Do you know what they're going to be quizzing you on?
Not necessarily? All SQL has to be designed accordingly, whether you want one record per transaction or infinite records. 
Not exactly. I assume it will be pretty generic and nothing ultra specific. I don't have much more info except that it will be 75 minutes. The company is tech B2B, SAAS.
Firstly this is a join. Secondly, is this a real table in your workplace without any real unique Identification between the tables?
Firstly this is a join. Secondly, is this a real table in your workplace without any real unique Identification between the tables?
I give SQL assessments for potential Business Analysts at my company. We designed our questions to test applicants understanding of 3 things: Aggregates (sum functions for example), Case Statements, and Potential Many to Many joins. When you take your assessment, pay close attention to how your joins will work for each item in the table and look for potential duplication. In addition, look up the definition of basic things like normalized data tables, keys (sort key, primary key, foreign key, etc) and other basic database fundamentals. I didn't learn much about these things for a while, but now I wouldn't hire anyone that couldn't tell me what a primary key was. Hope this helps.
Thanks a lot!
I've come across this but I don't understand how or why it works - I also need it done using the JOIN keyword and can't figure out how to reverse-engineer it because I don't understand it. SELECT BOOKS.ISBN, BOOKS.TITLE, ORDERITEMS.ORDER#, ORDERS.SHIPSTATE FROM BOOKS, ORDERITEMS, ORDERS WHERE ORDERITEMS.ORDER# = ORDERS.ORDER#(+) AND BOOKS.ISBN = ORDERITEMS.ISBN(+) ORDER BY 1, 2; &amp;#x200B;
when you do a "from" clause on more than one table, you get the product of *all* of those tables. Then the where clauses indicate what keys to join on to filter the results. This SO answer does a good job explaining it: https://stackoverflow.com/questions/1018822/inner-join-on-vs-where-clause
Could you give us a few examples? I always seem to fall short in these assessments as well. 
IDK why you're down voted when you're only one in the thread correctly identifying the purpose of a merge.
not only is there no guarantee that [name] is unique, they have dupes in table A as part of the example. Showing showing that its not unique (at-least in the one table). And its written on a post-it note? I'd say this is like freshman CS shitty question, but the fact it specifies Netezza is throwing me off. There's nothing really even Netezza specific in the question. 
I'm assuming that this is a school/homework problem. Hopefully part of the lesson involved with this problem is how necessary it is to maintain a strong data model.
Thanks, but unfortunately that's Reddit in a nutshell.
I commented on the original post. Basically, sql isnt removing the data.. its moving it to a heap with new Ids. More pages more space. Use alter table dbo.whatever rebuild To rebuild it like it was a clustered... but it's now a heap. That should trigger the page compression. You have to do this after your inserts (not all, but after many inserts) and such, it wont try and compress it like a clustered would because the pages are not linked anymore.
Aka union, or union all of you want duplicates 
But the example result also takes these duplicates into account (2, 102; 2, 103) and does not require nor assume uniqueness, so what's the problem here? Have you not ever seen a many-to-many relationship?
With merge you can create a new table called C with the desired output by matching values between table A and table B and insert the compared rows. With join you don’t have to create a new table C to get your output shown by comparing columns between table A and table B. 