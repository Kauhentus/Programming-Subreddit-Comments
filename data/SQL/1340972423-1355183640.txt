What extent of this support job is specific to SQL? i.e. Will you be supporting an application that relies heavily on an SQL database and will require potentially extensive troubleshooting through the database or will be you responsible for backups, performance issue, etc. and only deal with managing SQL servers? TLDR; What is the job description outlining your responsibilities?
Don't do ANYTHING without a backup. Seriously, back that data up.
May I ask what the company uses SQL for? I am working at a company where I do alot of db work right now, but I mostly interface with it using JDBC instead of various other things. The basics for SQL are 100% universal (thank you ISO!!!!) but some uses and implementations (DBMSs) have different strengths that are worth looking into. W3 is a semi decent resource for SQL syntax and ISO functions.
Along with that, don't do a delete unless you've done a select first with the same criteria to make sure that you are actually deleting what you want to delete.
and after you back it up and stop, then drop, drop, drop, drop it like it's hot
+1 I do this every time. 
Don't listen to him. If you mention backup up after EVERY drop your boss will assume you have not confidence in your commands. If you're not confident in them, why should he be? Next, don't confusing SQL with Oracel. SQL is short for MS SQL, which is what everyone who is serious is using. Oracel is something completely different that doesn't matter right now. Don't work about MYSQL or any of those other knockoffs, no one uses them. Lastly, focus on security. Having too many accounts increases changes the password will fall into the hands of hackers. Try to have as few accounts as possible, SQL by default includes the single account, or "SA" which is really all most companies need. With one password that covers everything, you only need to worry about protecting that one thing. Best of luck to you, -Competitive IT Guy.
Column name makes sense so A1-A4 means there are 4 columns, Column class? Is that the value of said column?
I think the important pieces of information for you are: 1. you can group parts of the where clause with parentheses. 2. You can use the conditionals "is null" and "is not null" So, you can have a where clause with: A1 = 'foo' and ( A2 is null or A3 is null or A4 is null ) Which will match on A1='foo' and any of A2, A3 or A4 as a null. Edit: also "Union All" lets you mash result sets together.
I think I see what you're doing. Something like, select trunc(sysdate()) as date_col , 'A foo' as table_stats , count(1) as total_rows , sum(case when A2='foo' and a1 is not null and a3 is not null then 1 else 0 end) as class_1_complete , sum(case when A2='foo' and a4 is not null then 1 else 0 end) as class_2_complete FROM [A] The above code should get you close to the first row in your table. Then you can union similar queries for the different tables and stats together. Add the percentages, and you should have a result that's close to your bottom table. Note that the code assumes that since A2='foo,' it is not NULL.
Do a search for ERRORLOG in the file system. Once you find it open it up and look for an error towards the bottom of the log. That should tell you what SQL Server thinks the problem is which will be a lot more valuable for troubleshooting than what you've given us here.
Posted a snippet of the log.
&gt;Directory lookup for the file "H:\SQL_DB\temp_MS_AgentSigningCertificate_database.mdf" failed with the operating system error 3(The system cannot find the path specified.) That part seems to be the root of the problem. The machine you're working on, does it have a drive labeled H? Does that mdf file exist there? You mentioned this is on a secondary DC, are you running SQL Server in a clustered environment? If you don't get help here in time, check the forums at sqlteam.com or sqlservercentral.com. Their audiences are quite a bit larger than this subreddit's.
Are you sure that you didn't get a failure on the upgrade that you ignored? This would be where you would most likely catch this issue. Check your default databases location and make sure it didn't pull in some old H:\ drive that existed when 2005 was first installed but doesn't exist anymore.
I updated the post with the 3 failures I got during installation.
Sorry that I don't have the time to walk you through this. But here is what you need to do. (This is tough to get right, I'd really recommend having an experienced DBA do it) Also, pay attention to the terms I use, they aren't arbitrary. When I say attach a DB it actually means that there is a command to attach a DB with "attach" in the name of the command. Ok, so it looks like the upgrade completed all the binaries (i.e. you are on SQL2008R2 now) but failed to upgrade the master DB. It apparently failed because of a pre-existing problem with a missing system database on the now missing H: drive. **You're absolute best bet at this point is to restore from backup, fix the original problem, and then re-upgrade the server.** Assuming that isn't an option (would you have posted if it was). You need to rebuild your master database (google this, you will find plenty of guides). Once your master database has been rebuilt you will need to re-attach all the databases on the server. This will get you online with all your databases online. Now the bad news. The user login information is stored in the master database. Because you rebuilt the master all your login information will be gone. You can get the account names (and SIDs, this will be important in a minute) that need to be recreated from the actual databases. USE &lt;database name&gt;; SELECT * FROM sysusers; You will need to recreate the logins with the same sids shown in that select statement. Google the CREATE LOGIN command, there is an option in it to specify the SID when you create the login. Now the really hard part. For windows logins you will be golden. They will map up to the SID and start working. However, SQL server logins will have had passwords stored in the master database. There is NO WAY to recover those passwords with this procedure. You will need to look them up from your documentation or set them to something new and remap them in the applications attaching to those databases. (This is why doing a restore is your best option). Good luck, you're going to need it. I've done this before and even for an experienced DBA this is tough to pull off.
SO here's what happened. I ran the repair function again (it had failed for the same reasons before), but it took this time. Everything came back green. The only thing I really changed was giving permissions to come folders based on some articles I found. However the services still will not start. This is most likely due to what you mentioned. If I was to uninstall SQL Server altogether and do a fresh install, would things go back to normal or is that not a likely hood due to the database having already been upgraded?
It is a totally legit copy. I'm guessing the H drive was there upon the initial install of 2005 but was changed sometime after that.
 SELECT '' "Blank Field 1" --Do not complete field ,'' "Blank Field 2" --Do not complete field ,Orders.orderNo "Supplier Order Number" ,"Customer PostCode" = CASE WHEN Orders.deliveryPostCode = '' OR Orders.deliveryPostCode IS NULL THEN 'N/A' ELSE Orders.deliveryPostCode END ,"Customer Organisation" = CASE WHEN dimCustomer.OrgName = '' OR dimCustomer.OrgName IS NULL OR dimCustomer.OrgName = 'Unknown' OR dimCustomer.OrgName = 'Unmapped' OR dimCustomer.OrgName = 'Unprofiled' THEN SDimension.dbo.Customers.name ELSE dimCustomer.OrgName END ,"Customer URN" = CASE --PS URN WHEN dimCustomerExt.[ExtraField1] IS NULL THEN '' ELSE dimCustomerExt.ExtraField1 END ,"Customer Order Number" = CASE WHEN Orders.customerOrderNo = '' OR Orders.customerOrderNo IS NULL THEN 'N/A' ELSE Orders.customerOrderNo END ,"Customer Invoice Date" = CASE WHEN Orders.invoiceDate between convert(datetime,'01/' + convert(varchar,datepart(mm,dateadd(mm,-1,getdate()))) + '/' + convert(varchar,datepart(yyyy,dateadd(mm,-1,getdate()))),103) and dateadd(dd,-1,convert(datetime,'01/' + convert(varchar,datepart(mm,dateadd(mm,0,getdate()))) + '/' + convert(varchar,datepart(yyyy,dateadd(mm,0,getdate()))),103)) THEN Orders.invoiceDate ELSE Orders.dateDespatched END ,Orders.invoiceNo "Customer Invoice Number" ,Orders.orderLineNo "Customer Invoice Line Number" --Order Line No ,REPLACE( CASE WHEN Orders.longDescription = '' OR Orders.longDescription IS NULL THEN 'Unknown' ELSE Orders.longDescription END ,CHAR(34),'') "Description of Service" ,'' "Blank Field 3" *,"UNSPSC" = MIN(CASE * WHEN u.UNSPSC IS NOT NULL THEN CAST ( u.UNSPSC AS INT) * WHEN Orders.longDescription like ('%del%') THEN 78102203 * WHEN dimProduct_1.Product like ('%conf%') THEN 81111809 * WHEN dimProduct_1.Product like ('%asset%') THEN 81111809 * WHEN SDimension.dbo.dimProduct_1.[Manufacturer] like ('%mindjet%') THEN 43231512 * WHEN SDimension.dbo.dimProduct_1.[Manufacturer] like ('%citrix%') THEN 43231512 * ELSE 43211600 * END ) ,'' "Blank Field 4" --Do not complete field ,'' "Blank Field 5" --Do not complete field ,'' "Blank Field 6" --Do not complete field ,'' "Blank Field 7" --Do not complete field ,sum(Orders.gbpVal) "Invoice Line Total Value ex VAT and Expenses" --Gbp value ,'' "Blank Field 8" ,'Y' "VAT Applicable?" , sum(case substring(Orders.orderNo,1,2) when 'CN' Then SDW.dbo.InvoicedExt.VATAMOUNT*-1 else SDW.dbo.InvoicedExt.VATAMOUNT end) "VAT amount charged"
 FROM SDimension.dbo.Customers INNER JOIN Orders ON (case Orders.company when 'ELT' Then 'IC' Else Orders.company End =SDimension.dbo.Customers.company and Orders.customer=SDimension.dbo.Customers.customer) INNER JOIN SDimension.dbo.dimProduct_1 ON (SDimension.dbo.dimProduct_1.supplierId=Orders.supplierId and SDimension.dbo.dimProduct_1.Product=Orders.product and case Orders.lineType when 'P' Then 'ALL' Else Case Orders.company when 'IC' Then 'ELT' Else Orders.company end end = SDimension.dbo.dimProduct_1.cstgCompany) LEFT OUTER JOIN SStage.dbo.StageLotNo ON (ltrim(rtrim(SDimension.dbo.dimProduct_1.CSTGCATEGORYCODE))=ltrim(rtrim(SStage.dbo.StageLotNo.CategoryCode))) LEFT OUTER JOIN CustomerSpecificFields CustomerSpecificFields2 ON (CustomerSpecificFields2.orderNo=Orders.orderNo and CustomerSpecificFields2.fieldName01 = 'Delivery Contact') LEFT OUTER JOIN CustomerSpecificFields CustomerSpecificFields5 ON (CustomerSpecificFields5.orderNo=Orders.orderNo and CustomerSpecificFields5.fieldName02 = 'Delivery Phone') LEFT JOIN [SDimension].[dbo].[dimCustErp] ON Orders.customer = [dimCustErp].ErpCode AND dimCustErp.Urn &lt;&gt; -2 AND dimCustErp.CompID = 6 LEFT OUTER JOIN SDimension.dbo.dimCustomerExt ON rtrim(dimCustomerExt.Urn) = dimCustErp.Urn LEFT OUTER JOIN SDW.dbo.InvoicedExt ON Orders.orderNo = InvoicedExt.orderNo AND Orders.orderLineNo = InvoicedExt.orderLineNo * LEFT OUTER JOIN ( * SELECT DISTINCT * [cstgManufacturerPartCode] "MPN" * ,ui.UNSPSC * FROM [SDimension].[dbo].[dimProduct_1] up * INNER JOIN * ( SELECT DISTINCT * [cstgManufacturerPartCode] "MPN" * ,min([cnetUnspsc] ) "UNSPSC" * FROM [SDimension].[dbo].[dimProduct_1] * WHERE * [cnetUnspsc] IS NOT NULL * AND [cnetUnspsc] &lt;&gt; '' * AND [cnetUnspsc] &lt;&gt; 'Unknown' * GROUP BY [cstgManufacturerPartCode] * ) ui * ON up.cstgManufacturerPartCode = ui.MPN * ) u ON SDimension.dbo.dimProduct_1.cstgManufacturerPartCode = u.MPN LEFT OUTER JOIN SDimension.dbo.dimCustomer ON dimCustomer.Urn = dimCustErp.Urn WHERE Orders.groupCustomer In ( 'Cust1','Cust2','Cust3','Cust4' ) AND ( ( Orders.invoiceDate between convert(datetime,'01/' + convert(varchar,datepart(mm,dateadd(mm,-1,getdate()))) + '/' + convert(varchar,datepart(yyyy,dateadd(mm,-1,getdate()))),103) and dateadd(dd,-1,convert(datetime,'01/' + convert(varchar,datepart(mm,dateadd(mm,0,getdate()))) + '/' + convert(varchar,datepart(yyyy,dateadd(mm,0,getdate()))),103)) AND (Orders.invoiceDate = Orders.dateDespatched OR Orders.dateDespatched &lt;&gt; Orders.DateEntered) ) OR ( Orders.dateDespatched between convert(datetime,'01/' + convert(varchar,datepart(mm,dateadd(mm,-1,getdate()))) + '/' + convert(varchar,datepart(yyyy,dateadd(mm,-1,getdate()))),103) and dateadd(dd,-1,convert(datetime,'01/' + convert(varchar,datepart(mm,dateadd(mm,0,getdate()))) + '/' + convert(varchar,datepart(yyyy,dateadd(mm,0,getdate()))),103)) AND Orders.invoiceDate &lt;&gt; Orders.dateDespatched AND Orders.dateDespatched = Orders.dateEntered ) ) --OR Orders.orderNo = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ1234567890' GROUP BY Orders.orderNo ,Orders.deliveryPostCode ,SDimension.dbo.Customers.customer ,dimCustomer.OrgName ,SDimension.dbo.Customers.name ,CustomerSpecificFields2.fieldValue01 ,CustomerSpecificFields5.fieldValue02 ,Orders.customerOrderNo ,Orders.invoiceDate ,Orders.invoiceNo ,Orders.orderLineNo ,Orders.dateDespatched ,SDimension.dbo.dimProduct_1.cstgManufacturerDescription ,SDimension.dbo.dimProduct_1.cstgManufacturerPartCode ,Orders.longDescription ,SDimension.dbo.dimProduct_1.cstgCategoryCode ,dimCustomerExt.ExtraField1 ,SDimension.dbo.dimProduct_1.Product ,SDimension.dbo.dimProduct_1.Manufacturer ,SDW.dbo.InvoicedExt.VATAMOUNT Order by 3,2 
I spent about 15 minutes reading over it. At first I thought it was because you had a where clause with a bunch of OUTER JOINs but the table with the where clause is the INNER JOIN. Best of luck. I hope you get your answer. I don't think I can help with this one with out actually working on the database.
Thanks for having a look through anyway!
The challenge you have is that the parameters are not 1:1. It's been a year since I used dynamic SQL (I could probably write your strings in my sleep), but you're probably going to need to nest dynamic SQL inside another SQL strings. I don't have a copy of MS SQL so I can't really mock anything up for you. You're correct that cursors are frowned upon, and there is another way to generate dynamic SQL strings, but it's a bit more abstract. declare @v nvarchar(max) set @v='' select @v=@v+ 'select trunc(sysdate()) as date_col , "'+filtercondition+'" as table_stats , count(1) as total_rows , sum(case when "'+filtercondition+'" "'+@condition1+'" then 1 else 0 end) as class_1_complete , sum(case when "'+filtercondition+'" "'+@condition2+'" then 1 else 0 end) as class_2_complete from '+tablename from metadatatable group by tablename, filtercondition The above code generates the basic SQL string for each tablename and filtercondition in your metadatatable. The challenge, again, is that you need to construct @condition1 and @condition2 separately (but is still related to the above query, since columnname and columnclass are a function of tablename and filtercriteria). In the past, I would nest another dynamic SQL string inside @v that would build @condition1 and @condition2, then pass those parameters outside @v using cmd_executesql. As for your Sharepoint thing, I'm not well versed with that stuff, but it seems easier to me to store the results on SQL, and have your sharepoint spreadsheet link to that table. 
This provided a lot of insight. Thanks for your thoughts.
There is a reason to use a real table like that: If the work table in the database grows too large and exceeds the allocated size of the files for the database, just that database will go down. If it happens to TempDB as a result of using a temp table, **all** of the databases on the server will go down. If you care about downtime, avoid using temp tables. 
publisher/subscriber is what I've mainly read about and seems like the right methodology for me. something like this: http://www.youtube.com/watch?v=9ARwLGVeUGk 
I'm running an older version of SQL; but, I have two sites which engage in merge replication for several tables. It works pretty well; though, I do make it a daily habit to verify the agent status as they are prone to stopping if the connection has issues.
yes and as tables grow or stats become outdated you get an unpredictable system which for a dba is a pain in the arse
I tried doing this using an import table (the spreadsheet), a redditor table (generated users) and a RedditGifts table (results). I wanted it to be run with as little I/O as possible, perferably no updates and no deletes and no temp- or staging-tables. This was my set up: create schema rdt; go create table rdt.imp_stats (Id int identity(1,1),CountryCode nvarchar(5) primary key clustered, Willing int, NonWilling int) --Don't forget to fill imp_stats up! create table rdt.Redditor(Id int identity(1,1) primary key clustered,RedditorCountry nvarchar(5), WillingToShipInternational bit) Create nonclustered index ix_RedditorCountry on rdt.Redditor(RedditorCountry) CREATE TABLE rdt.RedditGifts (id int identity(1,1) primary key clustered,Year int,GiverId int,RecieverId int) Create nonclustered index ix_RedditGiftGiver on rdt.RedditGifts(Year,GiverId) Create nonclustered index ix_RedditGiftReciever on rdt.RedditGifts(Year,RecieverId) --Loads Redditors from the imported data set nocount on; declare @id int, @lastId int; SELECT @id = MIN(id), @lastId = MAX(Id) FROM rdt.imp_stats declare @loopCounter int = 0 declare @Willing int = 0 declare @NonWilling int = 0; while(@id &lt;= @lastId) BEGIN SELECT @Willing = Willing, @NonWilling = NonWilling FROM rdt.imp_stats where id = @Id SET @loopCounter = 0; WHILE @loopCounter &lt; @Willing BEGIN INSERT INTO rdt.Redditor(RedditorCountry,WillingToShipInternational) SELECT CountryCode,1 FROM rdt.imp_stats where id = @Id set @loopCounter+=1; END SET @loopCounter = 0; WHILE @loopCounter &lt; @NonWilling BEGIN INSERT INTO rdt.Redditor(RedditorCountry,WillingToShipInternational) SELECT CountryCode,0 FROM rdt.imp_stats where id = @Id set @loopCounter+=1; END set @id+=1; END One approach is to split users into two sets, international and non-international. The domestic redditors would send gifts round robin achived by a CTE with row_number partioned by country and ordered by some random function; I chose hashbytes('md5',UserId+'salt') Matching those to who want to ship internationally is pretty hard to do in a set based operation since you don't want the reciever to be in the same country. One way to do it is to match givers to recievers is to sort by country in a CTE and then join on row_number so that giver row number equals the max row_number minus recievers row_number. This however makes it so that everybody in a small country will ship to the same reciever country. Even worse, the people who end up in the middle will ship domestically (if there is an odd number of people the middlest one will recieve his or her own gift. Also, all international participants will recieve gifts from the person they sent to. Even if that would be acceptable this isn't complete since 22 countries have only one person who doesn't want to ship domestically. They are completely left out. My code for this: --Domestic Shippers WITH domesticShippers as ( SELECT r.Id ,RedditorCountry ,row_number() over (partition by s.id order by HASHBYTES('md5',cast(r.Id as nvarchar)+RedditorCountry+'redditzor') asc) as rn ,s.nonWilling as lastRn FROM rdt.imp_stats as s INNER JOIN rdt.Redditor as r ON s.CountryCode = r.RedditorCountry WHERE WillingToShipInternational = 0 ) INSERT INTO rdt.RedditGifts(Year,GiverId,RecieverId) --round robin, first gets second, second gets third... SELECT '2012',giver.Id,reciever.Id FROM domesticShippers as giver INNER JOIN domesticShippers as reciever ON giver.RedditorCountry = reciever.RedditorCountry AND giver.rn = reciever.rn - 1 --... and last gets firsts. UNION ALL SELECT '2012',giver.Id,reciever.Id FROM domesticShippers as giver INNER JOIN domesticShippers as reciever ON giver.RedditorCountry = reciever.RedditorCountry WHERE reciever.rn = 1 and giver.rn = reciever.lastRn AND reciever.lastRn &gt; 1; --International shippers WITH internationalShippers as( SELECT r.Id ,RedditorCountry ,row_number() over (order by RedditorCountry asc,HASHBYTES('md5',cast(r.Id as nvarchar)+RedditorCountry+'redditzor') asc) as rn ,(select sum(s.Willing) FROM rdt.imp_stats as s) as lastRn FROM rdt.Redditor as r WHERE WillingToShipInternational = 1 ) INSERT INTO rdt.RedditGifts(Year,GiverId,RecieverId) SELECT '2012',giver.Id,reciever.Id FROM internationalShippers as giver INNER JOIN internationalShippers as reciever ON giver.rn = giver.lastrn-reciever.rn+1 order by giver.rn Anyone got any pointers?
Your users table has more columns than your insert statement specifies. (16 vs. 11)
Thanks, I'll check this out tonight.
Some more detailed information (I'll use pseudocode here and there) in original post*. Unfortunately this didn't help, as I still managed to get false id. I even modified it later to continue only after getting the same id three times in a row, with the same result in the end. That is where I stopped for now. However I might not have taken into account autocommiting, and will try manual commiting later.
Much easier to Cast your timestamp to DATE, then group by that, unless I am missing a functional requirement here you are taling the long way around by using extract. 
so im a total derpshow. Didnt think to even do that. PERFECT. your my hero man. Thanks! :)
Hmm, I'd be curious to see the index definition. This doesn't seem to be easily explainable from RCSI. I'd be curious if your incorrect value stems from the first select block or the insert. I'd be curious if your values are served out of a corrupt covering index rather than the data pages of the table itself. You found yourself a very curious problem...heh 
This is a long time late, but it so happens this error has been observed by others previously. http://sqlblogcasts.com/blogs/simons/archive/2010/11/07/upgrading-from-sql-2005-to-sql-2008-r2-fails-if-default-data-folder-doesn-t-exist.aspx Also, the MSDN blogosphere has some very good articles about troubleshooting script upgrade problems: http://blogs.msdn.com/b/karthick_pk/archive/2010/11/18/sqlserver2008-script-level-upgrade-for-database-master-failed-because-upgrade-step-sqlagent100-msdb-upgrade-sql-encountered-error-574-state-0-severity-16.aspx For example, trace flag T3601 allows you to see the first 512 characters of each batch being run by the upgrade script. The link cited in the article (http://blogs.msdn.com/b/sqlserverfaq/archive/2010/10/27/sql-server-2008-service-fails-to-start-after-applying-service-pack-1.aspx) corroborates the first. Path need to be there. Once there was an H-drive and SO SHALL IT BEEEEEE. Or registry modifications.
Have you tried to find a pattern or some relationship between the correct key and the incorrectly returned key? I doubt any such thing exists, but as kreadus005 said, this is very curious.
 That is really hard to answer. Probably because you failed to include which sql varient you are having issue with. Its like the basic kids school questions. Mary spent 22p, Jack spent 75p Howe much did Alice spend? 
It's because of scope_identity() returning wrong id I changed to OUTPUT INTO, so no. Since I have seen more than one record with incorrect id assigned, and only first one could be the result of INSERT action, I'm fairly sure it's because of SELECT statement.
I'll check indexes when I'm back at work after weekend, and post it here. About whether it's OUTPUT INTO or SELECT causing problem I think it's the latter: Since I have seen more than one record with incorrect id assigned, and only first one could be the result of INSERT action, I'm fairly sure it's because of SELECT statement. It is worth noting, that before workaround was implemented, for several (let's say 10) s having identical o_n and o_t pairs, two of them aquired correct id, while 8 got wrong id, but it was the same id for all 8 of them, and procedure was not called for all of them at the same type. This is why I think it's not random per se, erroneous yes, but not random. After workaround, I got 2 wrong ids, both the same (different than before - very old, usually when id was returned incorrect it was quite close to correct one). After the weekend I should have fair amount of data to work on.
As I'm now logging incoming data in debug table I should have fair amount of data available after the weekend. I will check then. This ended up to be not a DB error, details in OP. Investigation is ongoing.
Are there triggers on either table?
What's your patch level? http://support.microsoft.com/kb/2019779
I will have to check that, but I use neither scope_identity() nor @@IDENTITY. I use OUTPUT INTO. "Microsoft recommends that you do not use either of these functions in your queries when parallel plans are involved as they are not always reliable. Instead use the OUTPUT clause of INSERT statement to retrieve the identity value as shown in the example below."
Use varchar not nvarchar unless you need internationalisation, which is unlikely in this instance. Nvarchar uses almost twice the space of an varchar field. 
Hey, so I created the db you suggested here, just about as close as I could. [Here's the dbfile](http://tankorsmash.com/transfers/first.db) Is that pretty much what you were thinking? I'm going to try to work with this format for as long as I can, so long as it works. Also, if you know any Python, this is how I made it: style = ''' *ID3tag table * ID3Id int Primary Key * ID3Tag nvarchar(25) NOT NULL -- Might be able to get away with shorter. Also, you can ditch the 'n' if you don't expect to have non-english characters * Song table * SongId int Primary Key * SongName nvarchar(255) NOT NULL * FilePath nvarchar(255) NOT NULL -- Might consider nulls if you want a wishlist type function * TimesPlayed int NOT NULL DEFAULT 0 * Album table * AlbumId int Primary Key * AlbumName nvarchar(255) NOT NULL * CoverArtPath nvarchar(255) NULL -- Could also be a BLOB and store the image directly * Artist table * ArtistId int Primary Key * ArtistName nvarchar(255) NOT NULL And, of course, you need your linkage tables so: * SongId3Link * SongId int * ID3Id int * SongAlbum * SongId * AlbumId * SongArtist * SongId * ArtistId''' import sqlite3 as sql conn = sql.connect('first.db') #---------------------------------------------------------------------- def createTable(name, vars = {}): """create a new table, with the vars, as a columnname : datatype dict""" pairs = [] for colname, datatype in vars.items(): pairs.append('{} {}'.format(colname, datatype)) values = ', '.join(pairs) cmd = r'CREATE TABLE {} ({})'.format(name, values) print 'The command sent', `cmd` curs = conn.cursor() try: curs.execute(cmd) except sql.OperationalError as e: #except IndexError: print e #---------------------------------------------------------------------- def deleteTable(table): """Deletes table entirely""" curs = conn.cursor() cmd = r'DROP TABLE {}'.format(table) try: curs.execute(cmd) print table, 'deleted successfully' except sql.OperationalError as e: print e #createTable('TableName', {'FIRSTVAR': 'INT', 'SECONDVAR': 'TEXT'}) #deleteTable('TableName') #table name : vars(in a dict) tables = {'ID3tag': {'ID3id': 'int PRIMARY KEY', 'ID3tag': 'nvarchar(25) NOT NULL'}, 'Song': {'SONGid': 'int PRIMARY KEY', 'SONGname': 'nvarchar(255) NOT NULL', 'FILEpath': 'nvarchar(255)NOT NULL', 'PLAYcount': 'int NOT NULL DEFAULT 0',}, 'Album': {'ALBUMid': 'int PRIMARY KEY', 'ALBUMname': 'nvarchar(255) NOT NULL', 'ARTcover': 'BLOB', }, 'Artist': {'ARTISTid': 'int PRIMARY KEY', 'ARTISTname': 'nvarchar(255) NOT NULL', } } for name, variables in tables.items(): createTable(name, variables)
Added indexes and original tables definitions.
3 Don't drop ANYTHING without a backup.
That's right don't let your boss know you're conscientious enough to backup what might be a devastating loss to the company. Hmm - don't use or know about Oracle or MySQL? Really?? And Microsoft has the trademark on SQL? Finally, you're advocating the dba's AND users to use sa? Wow.... 
Wooosh.....
Like I said, I was going to have it id id3tag 1 TIT2 #title 2 AENC #audio encoding format 3 TPE3 #the performer but you're right, that doesn't save the values for each song, and that's what I was trying to figure out. The thing I came up with was an number associated with a combination of tags, so there'd be a 137! combinations. Alternatively, I was just going to store the ID3Tag object I've written in a blob, after it was converted to a string object Long story short, I really don't know.
Thanks for taking the time to explain that to me; as you can tell, I'm pretty lost. I think that is what I needed to hear. Basically a bunch of different tables, but not just containing one tag, but several united under a single category, `Album`, `Artist`, `Song`, `Comment` etc. My understanding of foreign key just means that it's a primary key from some other table right? If that's the case, could one table have both a primary and foreign key be the same value? I've been reading the article [here](http://www.deeptraining.com/litwin/dbdesign/FundamentalsOfRelationalDatabaseDesign.aspx) which has cleared what the normalization means. I just have to get my mind in the proper mindset. For example, this whole time I wasn't thinking beyond storing a song's info all in one place, album and artist data included, but now, thanks to you folks, I know that it's a better idea to split all of that up, and only make a single ID.
CAST or CONVERT (variable typing), MIN (FLOOR), not sure on MOD (it may just be MOD). You'll also want to change " to ' (or change your ODBC options to compensate). I think that's it, most of this looks like it translates. 
Also, SQL Server generally installs with something called "SQL Server Books Online" in the start menu. Unlike most application documentation Books Online is really an excellent reference.
This code here will recreate your "T2" table. However I don't really recommend it. I recommend building a "Numbers table" (Google it, every database should have one) and querying that with the ranges you want in order to create your Hour / Minute table. But nonetheless, here it is. You should be able to use this to replace your T2 table and the rest will fall into place with standard ANSI SQL. WITH Hr(Number) AS ( SELECT 6 AS Number UNION ALL SELECT Number + 1 FROM Hr WHERE Number &lt; 22 ), Mn(Number) AS ( SELECT 0 AS Number UNION ALL SELECT Number + 1 FROM Mn WHERE Number &lt; 59 ), HourMinute AS ( SELECT hr.Number AS [Hr] , mn.Number AS [Min] FROM Hr CROSS APPLY Mn ) SELECT * FROM HourMinute ORDER BY [Hr], [Min] 
Thanks for all your help. I really appreciate it.
No problem. Let me know if there's anything else you're stuck on.
I just implemented this novelty query on a production system, and now there is a "meme" field with the "lol" custom datatype appended to each table. What do? 
next time use the MS Sarcasm Serif font.
Wow. Fantastic, I knew I should have experimented with Sums, Thanks a bunch!
And this is why each developer should use their own version of the production database :)
Thanks to everyone who subscribes for being a part of the community. 
I wrote my own query for congratulations, but I somehow managed to overflow the integer. Damn floats. Go me.
All I get it this.. SQL-redditors ------------- 2000 I don't think I understand what you did.
In the from clause, separating tables or expressions with a comma is the same as writing CROSS JOIN. This gives the cartesian product and thus the amount of rows will be the number on the left times the number on the right. Do this for the numbers 2,2,2,2,5,5,5 and you will see that 2^4 * 5^3 = 2000. The "WITH" part of the select statement allows me to create common table expressions which I can use as I would a table in the FROM clause. I made it so that the one named [two] holds 2 elements and the one named [five] holds 5 elements. Does that help? (:
Try group by on your first 4 columns, then wrap the remaining ones in max()?
90, previous server was 2005 and the previous admin didn't change it when they migrated.
 DELETE address_point WHERE GEO_ID NOT IN ('7522417','358152','388937','8406999','8771612') Here is why all the records are being deleted, lets use the record where GEO_ID = '7522417' as an example. Its conditions evaluate to: * '7522417' != '7522417' =&gt; FALSE * '7522417' != '358152' =&gt; TRUE * '7522417' != '388937' =&gt; TRUE * '7522417' != '8406999' =&gt; TRUE * '7522417' != '8771612' =&gt; TRUE So delete that record where (FALSE or TRUE or TRUE or TRUE or TRUE); The 4 trues make the single false moot. You could use ANDs or the NOT IN (recommended for prettiness). edited for formatting and explanation. 
Here's a function that you should be aware of: COALESCE(). Usage: Returns the first non-null value in the list. E.g. Coalesce (null, null, null, 1, 3) Returns 1 Coalesce( 1, null, null, 3) Returns 1 Coalesce(null, null, null, null, null) returns null Coalesce (null, 3, null, 50) returns 3 
Run everything between Begin Transaction Rollback Transaction Write your query between, including updates and deletes. Ensure you return select * from related tables before the rollback to ensure your statement had the desired effect. Once you're happy run with Commit Transaction. SSMS tools pack automatically includes Begin and Rollback with all new queries.
Have you tried selecting the different types of data out into temp tables and then running traditional joins against those?
The real question is... why is the data stored like that? Why don't you simply store the data in the database on one row, instead of duplication and all the extra cruft? Is there an actual reason why you don't have one row?
I personally think that this method is a little less verbose than what you're doing SELECT '2000' as version, Count(*) as count from mytable WHERE version = 'SQL Server 2000' UNION SELECT '2005' as version, Count(*) as count from mytable WHERE version = 'SQL Server 2005' UNION SELECT '2008' as version, Count(*) as count from mytable WHERE version = 'SQL Server 2008' UNION SELECT '2008r2' as version, Count(*) as count from mytable WHERE version = 'SQL Server 2008 r2' 
Perhaps I'm not understanding the question correctly, but why don't you just use the same backup you took for building the data warehouse to also build the test instance? That way your data will be the same across the systems.
Well, if I were you, I'd create a powershell script to copy the source file using a shared read into some kind of temp directory and then have the powershell invoke the bulk insert command or trigger BCP in some way. That or have it go as a separate job step with a predefined temp file location. 
I'd use powershell. xp_cmdshell may not seem as bad, but its the devil.
If he is only copying a file, xp_cmdshell should be okay. If anything more complex needs done, then yes, use powershell. DECLARE @cmd VARCHAR(255) SET @cmd = 'copy C:\log.csv C:\temp\log.csv' EXEC master.dbo.xp_cmdshell @cmd
Ever need to do recursion in a query? For example if you have a data set where items in a single table can be the parents of other items in that table? TABLE column1 - uniqueID int primary key column2 - parentID int DATA 1,null 2,1 3,1 4,2 5,4 6,null 7,2 8,4 9,null Return that in order based on parent-child-child(n) hierarchy using simple queries ;). (indented to visualize the relationship) What we want returned: 1 2 4 5 8 7 3 6 9 There are a lot of solutions that just can't happen without getting into complex queries. This type of thing isn't that uncommon at all. 
&gt;Generally, in most of the work I have done I have built several queries (anywhere from 3 to 10) to create a data set I can work with. A lot of SQL queries I write are part of an application, a view or a report. Things that need to run fast and run often don't run well when you need a 3 to 10 step process of writing temp tables just to return an answer. I [posted a thread](http://www.reddit.com/r/SQL/comments/gun6p/the_biggest_baddest_sql_query_ive_ever_written_db2/) a long time ago about the most complicated query I've ever written. It's a view that you can select from in real time, and the amount of work going on under the hood is insane.
Our database has over 100 tables that are all related to each other in many ways. Simple won't cut it.
I would like to thank everyone who replied to this post. Obviously in my limited work I have not encountered the situations or requirements. That said, I would like to ask if the those who have encountered these types of problems and requirement what process they use to create their complex queries.
I love using [Modified Preorder Tree Traversal](http://www.sitepoint.com/hierarchical-data-database-2/) for these situations. Hierarchy can be found with one query.
I know of the optimizer have never used. I will definitely give this a shot today.
Depends on the definition of complex. Ultimately use the right tool for the right job. There's a fine line between complex code &amp; obfuscated, unmanageable, inflexible hacks. If asked on an interview whether I could handle complex queries I'd ask why they need them in the first place. 
This is a difficult question to answer. Obviously, simpler is better, but queries can only be as simple as the data relationships that house them. I work primarily in enterprise SaaS software, and we have hundreds of schemas, all of which are dynamic (varies from client to client, and client actions in the app will build out unique tables in the DB). Our base schemas has a couple thousand tables on its own. If given the choice, a simple query is what you want. But when it starts involving dozens of tables which might relate to each other in more complex ways than your standard 1:1, 1:n or even n:n, you have to get complex. 
SQL, in the right hands and as long as you stick to its strengths, is a pretty damn good language for selecting datasets. Certain kinds of changes to a SQL query, like selecting a new column or adding an outer join, can be made with little risk of side-effects, which is huge when maintaining a production system.
You need to use pivot. Google 'pl/sql pivot'.
The query took 0.43 seconds to execute, right? Forty-three hundredths of a second? Why is that too slow? How fast do you need it to execute? The index on `created_at` probably isn't as important as the indexes on the `user_id` and `customer_id` and `status_id` columns.
all those are indexed. For some reason, the benchmark tool used by the server admin says that the query is the one that causes all the problems. 
Here's one thing that might be a problem. It seems to be doing the sort on disk, and anything that has to hit the disk is going to take a speed hit. For only 26K rows, I would expect MySQL to be able to do the sort in RAM. Are the rows you're returning really big? Do you really need all the fields from all three of those tables? If you're returning big BLOB fields, that could stop MySQL from doing its sort in memory, and forcing it to write to disk.
Yeah, dig more into the benchmark stuff. I'm skeptical. Don't PM me. Email me at andy-at-petdance-dot-com and I'll look at it in the morning if you do. No promises, but it can't hurt. Good luck! 
Define broken. Is the query returning the wrong results? Will the query even run? etc. Also /r/SQLServer is available if you want msft specific help. Cheers
Which version of Oracle are you using? That is very important information in order to answer this question correctly. Either way see: [OTN: How do I convert rows to columns?](https://forums.oracle.com/forums/thread.jspa?messageID=9360005&amp;amp;tstart=0#9360005)
 They don't persist. There is 2 types of temp table in mssql. The normal per connection and global tables. The temp table is removed when you close your connection. For the normal temp table. For global temp tables the last connection to close a connection to a particular database will remove the global temp table
You need to use dynamic SQL using sp_executesql Try this: declare @i INT, @numRows INT declare @field_name NVARCHAR(100) declare @SQL nvarchar(1000) DECLARE emptyC2_cur CURSOR FOR select FIELD_NAME from definition where DBFNAME = 'CONTACTS' order by FIELD_NAME desc OPEN emptyC2_cur FETCH NEXT FROM emptyC2_cur INTO @field_name WHILE @@FETCH_STATUS = 0 BEGIN SET @SQL = N'SELECT @numRows_OUT = COUNT(distinct ' + QUOTENAME(@field_name) + ') from CONTACTS' EXEC sp_executesql @SQL, N'@numRows_OUT int OUTPUT', @numRows_OUT = @numRows print @numRows FETCH NEXT FROM emptyC2_cur INTO @field_name END CLOSE emptyC2_cur DEALLOCATE emptyC2_cur
You're doing a count of distinct values that are your value so you'll always get 1. I'm guessing you actually want the # of records associated with each field_name? If so, you'll want to do your distinct count against another column that distinctly identifies each record set. I've rewritten what you have here, and though I don't have data to test with it should be mostly, if not all, there: (If this is MSSQL, cursors are to be generally avoided) --DECLARE @workload TABLE([pk] INT IDENTITY(1,1),[field_name] NVARCHAR(100)); CREATE TABLE #workload([pk] INT IDENTITY(1,1),[field_name] NVARCHAR(100), [assoc_cnt] INT); DECLARE @field_name NVARCHAR(100) ,@row_id INT INSERT INTO #workload ( [field_name] ) SELECT [field_name] FROM [dbo].[definition] WHERE [dbfname] = 'contacts' ORDER BY [field_name] DESC; SELECT @row_id = MIN([pk]) FROM #workload; WHILE @row_id IS NOT NULL BEGIN SELECT @field_name = [field_name] FROM #workload WHERE [pk] = @row_id; UPDATE #workload SET [assoc_cnt] = (SELECT COUNT(DISTINCT recordidentifier) FROM [dbo].[contacts] WHERE [field_name] = @field_name) WHERE [pk] = @row_id; SELECT @row_id = MIN([pk]) FROM #workload WHERE [pk] &gt; @row_id; END SELECT * FROM #workload; DROP TABLE #workload;
I think that this fits more into what you describe. If so, you'd just need to pull desired data from the result set and run your alters and updates. CREATE TABLE #workload([pk] INT IDENTITY(1,1),[field_name] NVARCHAR(100), [assoc_cnt] INT); INSERT INTO #workload ( [field_name] ) SELECT [field_name] FROM [dbo].[definition] WHERE [dbfname] = 'contacts' ORDER BY [field_name] DESC; DECLARE @field_name NVARCHAR(100) ,@row_id INT ,@execstr NVARCHAR(1000) SELECT @row_id = MIN([pk]) FROM #workload; WHILE @row_id IS NOT NULL BEGIN SELECT @field_name = [field_name] FROM #workload WHERE [pk] = @row_id; SET @execstr = 'UPDATE #workload SET [assoc_cnt] = (SELECT COUNT(DISTINCT ' + @field_name + ' ) FROM [dbo].[contacts]) WHERE field_name = ''' + @field_name + ''';' EXEC(@execstr) SELECT @row_id = MIN([pk]) FROM #workload WHERE [pk] &gt; @row_id; END SELECT * FROM #workload; --DROP TABLE #workload;
Ah yes! I stand corrected, thank you. Dammit I've been wasting my life dropping those things. OP, you should heed this guy, especially if you're one of those unfortunates whose SQL is not encapsulated inside stored procedures.
Thanks for the explanation :)
Here's my explanation. I have a blog post I will publish next week on why long SQL queries are good, and how to keep them maintainable. The basic issue has to do with how we debug code. If we have a Perl subroutine (or any other high level language) there is only a minimum of enforced structure to the subroutine. Maybe you put your argument handlin at first and your return handling at the end. But maybe even that is not required. So we debug most high level languages line by line. You can think of it as scanning through a linked list for something that's wrong in the context of the whole linked list. It's slow, error prone, and becomes worse as length increases. So we like to keep those short. Debugging a few 200+ line subroutines is a mess. With SQL though we pick up on the fact that we are grouping together set operations in a highly structured form. A SQL query is highly structured in the way a subroutine in, say, Perl or Java is not. The structure is something like: output tuple structure definition primary relation join operations row filters aggregation criteria aggregation filters post-processing (ordering, limit, offset, etc) Now, the structure is such that for any given error, we probably start our debugging with an idea of what sort of error is happening, and consequently in which section of the query the error is located in. Wrong number of columns returned? We know to look at the first sction. Join projection issues? We know where to look. This means that debugging SQL is like traversing a tree to check specific likely problem nodes. So if you keep your SQL queries well structured, length can never become a problem (I have debugged 100-200 line sql queries, and length is never the problem that decay of structure is). Moreover pushing your set operations into a single query when that makes sense *greatly* simplifies app code, making it simpler and easier to work with, so you get better maintainability all around.
I don't understand why this is being downvoted? How is this not a legitimate database design question?
Thank you very much for this explanations. I would be very interested in reading your post. If you have the time I would appreciate a link to your blog.
One extremely important thing is: Run write operations in a transaction if it is on important data. This allows you to check and commit or rollback if you like. This has saved my butt more than once....
cant you just change set @numRows = (select COUNT(distinct @field_name) from CONTACTS) to set @numrows = exec('select COUNT(distinct ' + @field_name + ' ) from CONTACTS') or this if you just want to know the ones where its not null as opposed to unique values set @numRows = exec('select COUNT(*) from CONTACTS where ' + @field_name + 'IS NOT NULL')
Have you checked this: http://social.msdn.microsoft.com/Forums/en-US/sqlsecurity/thread/c3e0713c-b4e6-400e-9ba2-448cd5bf3cb8/ ?
2008 standard. It's not RTM anymore now since I installed SP3.
I found in the log that the state is 11 which has been copy-pasted into my first edit. 
Did you see the link I posted on the MSDN social site?
Yeah I'm currently reading it right now, but figure I reply to everyone anyways. 
I have turned off UAC and ensured that my user is set to administrator under control panel -&gt; user accounts -&gt; account types. The issue is still prevalent. 
Right Click &gt; run as administrator on the link for Management Studio.
I have also done that. 
Eh? Check your protocols under the Config Manager. Try accessing it using the named pipe instead of the localhost TCP/IP stack. If the ERRORLOG isn't giving you anything, the MSDN blogosphere has some very good articles about troubleshooting script upgrade problems: http://blogs.msdn.com/b/karthick_pk/archive/2010/11/18/sqlserver2008-script-level-upgrade-for-database-master-failed-because-upgrade-step-sqlagent100-msdb-upgrade-sql-encountered-error-574-state-0-severity-16.aspx For example, trace flag T3601 allows you to see the first 512 characters of each batch being run by the upgrade script.
Can you start the server up (in single user mode or whatever's easiest) and post the entire contents of the error log on pastebin or somewhere? It will have a bunch of informational messages which might be useful.
"Infrastructure error"? Is this server a member of a domain? Try removing it and re-joining.
Stop and disable the service, Open a cmd prompt Cd\program files\microsoft sql server\mssql\mssql.1 (or wherever sql server is starting up from) Sqlservr -c -m You'll see the server start up, do you see any errors? Can you copy and paste into here, cheers.
1. I believe my installation was 100% success since I used this server before the SP3 upgrade. But if you're referring to the confirmation of SP3 installation being 100% successful, I'm not sure how to confirm that. The installation GUI says it was successful and no errors occurred. 2. Unfortunately, I did not install this VM nor the original instance of SQL. So I cannot say for certain if sa or domain user was used. However, previously I was logging in with windows authentication before SP3 if that helps. 3. I have already turned off UAC and ran SSMS as admin as suggested by other people due to the error message I am getting. Unfortunately, it has not solved the problem. 
I have pasted my error log into pastebin, but here is the only error I get when doing this The SQL Server Network Interface library could not register the Service Principal Name (SPN) for the SQL Server service. Error: 0x2098, state: 15. Failure to register an SPN may cause integrated authentication to fall back to NTLM instead of Kerberos. This is an informational message. Further action is only required if Kerberos authentication is required by authentication policies.
Thats alright. It cant register with AD. What happens if You open sql server management studio, connect to object explorer and use a . for the server name and then your login details. Cheers. You could always uninstall, reinstall and reattach your user databases. All is not lost btw.
Youll have more issues if you use dbs from other servers. Try to enter using a . as the server name this uses shared memory and rules out protocol issues. 
1. Added a new user with admin running SSMS with right click &gt; Run as admin and UAC off. Problem still prevalent. 2. According to errorlog the server is in mixed mode. 3. Can you be a little more descriptive on this one? What exactly should I check? 4. I have restarted SQL Server under config manager. Login issue still here. 5. Under sql server config manager both services and agent is started. 
When using "." for server name and windows authentication I get the name error as posted in edit 4. Except it says "Cannot connect to .."
Thats strange it should let you since you are listening on shared memory. Can you pm me the pastebin link. Alternatively remove sql, reinstall, upgrade, reattach the dbs.
You havent left yourself many options tbh, you should always back up system dbs and the file system prior to an upgrade, but i guess that teaching you to suck eggs now.
PM sent. I am currently working on getting sql reinstalled, but the people responsible for that are not putting it on priority.
 dense_rank() over ( partition by DATA_ID order by DATA_GROUP ) works like a charm. Thanks.
You're a local admin on the machine right? Try this: create a local windows user and add it to the admin group. Log out and log back in as the local user. Start SQL Server in single user mode and connect using windows authentication. Single user mode makes any administrators of the machine sysadmin of the SQL instance. Windows authentication while logged in locally will bypass the need for the server to communicate with active directory to authenticate you. Once you connect using the local user reset the sa password to something new. Then you can log back off and restart the service in regular mode and connect as sa. That should at least give you a bit more to work with. 
This is my current startup Parameters: -dD:\Program Files\Microsoft SQL Server\MSSQL10.MSSQLSERVER\MSSQL\DATA\master.mdf;-eD:\Program Files\Microsoft SQL Server\MSSQL10.MSSQLSERVER\MSSQL\Log\ERRORLOG;-lD:\Program Files\Microsoft SQL Server\MSSQL10.MSSQLSERVER\MSSQL\DATA\mastlog.ldf What I did was add -m; to the beginning of the startup Parameters. Is that incorrect?
Awesome! I got in! Apparently I have to stay logged into single user mode with command prompt to do it though. Is that normal? Edit: Nevermind. Silly me forgot to start the service haha.
We are planning on MS SQL Server. No BLOB data. We're looking at high-volume usage for certain tools/applications but not for others. Replicated DB server (Single-instance) We're looking at supporting 250 users for phase 1 ... may grow to about 1000 We are also looking to sell either the entire suite of products or piecemeal applications (hence the multiple DB idea). 
i really don't know sir/miss. I'm really sorry. I will find out though. the application i'm using is really convoluted because it won't let you put in a complete sql query in a the manner i did. i'm guessing the DB is NOT MS. It runs on java, so maybe Oracle? 
i think you nailed it! i was able to get the 2 results. but both numbers are in one column. any idea how i can separate the active from the inactives on the output. anyway, thank you! 
If you want them in two columns, use buckbova's query (I cleaned it up): SELECT SUM(CASE WHEN active='Y' THEN 1 ELSE 0 END) as NumActive, COUNT(1) -SUM(CASE WHEN active='Y' THEN 1 ELSE 0 END) as NumInactive FROM am_modules
You could always run a sql agent job on Jan 1st that would reset the sequence.
I understand that you are running multiple systems .. but you have only one server ? It's depends on how you guys design the system ... poor design won't give you better result no matter which one you choose. . If you are having multiple smaller servers with one back end centralized server ( which provide master data ) .. Load will be shared and based on the physical server location/connection you can perform necessary back-end/centralized jobs without much effecting each and every system. Maintenance would be done by each site. 
It can't find the distribution database. When I get to the last step where it's supposed to actually disabled the publishers and the distribution I get this: "Database &lt;dbname&gt; cannot be opened due to inaccessible files or insufficient memory or disk space. See the SQL Server errorlog for details. Changed database context to 'master'." 
He's using Express, so he won't have Agent. But you can get around that with a Windows Scheduled Task and sqlcmd.
Upvoting this because I do extensive Oracle shit and I'm not in IT. Seems we're a rare breed.
http://www.geonames.org/ "The GeoNames geographical database covers all countries and contains over eight million placenames that are available for download free of charge."
I think that's my only choice at this time. I might just create a distributor on my machine and take that db over to their machine and restore it. Sigh.. 
We use plsql at work. Seems decent enough. 
PL/SQL is Oracle's extension of SQL. No amount of searching I've done has turned up an IDE named plsql...
I use Embarcadero RapidSQL and Oracle Developer at times. Not sure if it meets requirement #1, but it runs well on a core2duo with 2gb of RAM. I work with large ish datasets, around 30k rows. I've pulled down over 200k before.
You're going to have to be less vague. Are you generating data to insert into a table or are you selecting a subset of data from a table? How many units constitutes a population? Give examples of the assets you start with and the desired output. Help us help you.
Thanks for the reply. I think I see what you're saying here, just need further clarification. When I do a top x, wouldn't that just give me the first rows? Unless you're saying my x is a % of the population. Then do a union with the top Xs for each color. Would I randomize it by using key = @seed*RAND()+1 (where @seed is the ceiling of the random. 
 You should be able to write any data to the database and read exactly back what you inserted anything short of this is broken / failure on part of the developer. The only thing that should prevent this is either a validation layers in the gui with decent end user error handling or data constraints in the database its self. Which means it never reaches the database layer or it points to bugs in your validation. Even if there are bugs in in either layer it should not have sql injection because of a clueless / lazy developer. The above is easy to do. If it is really hard then you should dump your current set of database lib's because they are badly broken. 
I also believe that ISNULL is slightly more efficient than COALESCE, because SQL knows that it only needs to check one value for null. ISNULL(COMP_SITECORE_COMPANY_ID, 'NoCompanyGiven') AS COMP_SITECORE_COMPANY_ID COALESCE allows you to runoff through a list of values, so I only use COALESCE when I have more than one value to verify through. 
It's a strange paradox, I'd agree. But I don't work for IT. They are their own thing in this company and they run the show on the server side. Often, with an "All shields up" mentality. I'm full-on computer (IT?) based, but the people who pay me are "The Business." These people are fighting IT to get "The Business" done. I speak for them. I hack for them. I get that shit done in spite of IT -- Who would freak, if they knew how I was actually doing it. For the IT folks, it's all about securing the database. Or, backing up that network share. I understand that. But that's all they care about. Ever. Because they could do that tomorrow at a different company. And, they just might! Because I work for "The Business" I have a different mindset. For me, it's all about getting right parts to the right factories. Or, any number of other things like that. For me, it is all real and it deals with real things. Business things. Any single person in IT only has to worry about their area of expertise. I'm just not in that happy situation. I'm stuck being the one figuring out how to get Oracle data to talk with DB2 data and SAS data and SQL Server data... and the list just goes on. It has caused me to learn a ton of languages. And a ton of systems. And a ton of platforms. All while working (mostly) within Windows. All while fighting IT. It is, quite frankly, a bitch.
I've also heard that ISNULL() outperformed COALESCE() but I've actually never bothered to check. [http://sqlblog.com/blogs/adam_machanic/archive/2006/07/12/performance-isnull-vs-coalesce.aspx] The differences between how the two functions operate are pretty nice to know tho. [http://databases.aspfaq.com/database/coalesce-vs-isnull-sql.html] Like you mentioned one of the biggest reason I use COALESCE is handles multiple variables. COALESCE(@value1, @value2, Value3,'All null') ISNULL(@value1,ISNULL(@value2,ISNULL(@value3,'All null'))) Also, COALESCE() is the ANSI standard if you ever decided to run your script off MSSQL. 
Yes, I agree with your point about raw SQL having to be designated as such. [jOOQ](http://www.jooq.org) for instance does this in its Javadocs: *"**NOTE**: When inserting plain SQL into jOOQ objects, you must guarantee syntax integrity. You may also create the possibility of malicious SQL injection. Be sure to properly use bind variables and/or escape literals when concatenated into SQL clauses!"*, taken from http://www.jooq.org/javadoc/latest/org/jooq/impl/Factory.html#field(java.lang.String)
Some APIs may choose to let any SQL string pass through, by design. The most basic of those APIs is JDBC in the Java world. It has no formal means of SQL injection prevention whatsoever. Even with PreparedStatements, you can do things the wrong way. If JDBC, [DbUtils](http://commons.apache.org/dbutils/), etc. is your database access layer, it may not always be simple to prevent SQL injection caused by clueless / lazy developers. So your claim is that those APIs are "badly broken"?
[I think this may be what you're looking for.](http://msdn.microsoft.com/en-us/library/ms181765.aspx)
IT in my business wouldn't know how to query a database if it rose up and bit them. They're hardware. And phones. And networking (but just barely). And occasionally software (but they're really bad at supporting that). I've never worked at a company where IT has actively tried to keep me from doing my job, until now.
By base IDE I'm guessing SMSS and/or BIDS? And I haven't heard of the Red Gate tools - I'll have to do some research.
Yeah, that would be because I got some of the parentheses wrong. II and IST are known as Aliases. Look it up, learn to love them, they will become your best friends (EVAR!). Ignore buckbova - unfortunately his attitude is all too common, don't let it discourage you. Try this: SELECT II.ItemName AS ItemName, (CASE WHEN IST.UnitsAvailable &gt; 99 THEN '100+' ELSE CAST(IST.UnitsAvailabe AS VarChar (4)) END) UnitsAvailble FROM InventoryItem II LEFT OUTER JOIN InventoryStockTotal IST ON II.ItemCode = IST.ItemCode
Gah, I hate it when I forget a lettr! There you go :) And I had just guessed at 4 due to the 100+
*SELECT ItemName, CASE WHEN UnitsAvailable=0 THEN 'Out of stock' WHEN UnitsAvailable&gt;100 THEN '100+' ELSE UnitsAvailable END AS UnitsAvailable FROM InventoryItem,InventoryStockTotal WHERE InventoryItem.ItemCode=InventoryStockTotal.ItemCode;* Try that.
Yeah I don't use Oracle Developer much, but RapidSQL has it's own quirks. In general I don't have major complaints and it handles large sets very easily. It also has a great database search feature. Another tool that is used where I work is Aqua Data Studio but I neglected to mention it. The fact that it is also a Java product means I probably should have continued to not mention it! Good luck!
Yes, SMSS sorry. Definitely look at Redgate and let me know if you have any questions that the website doesn't answer :)
Make sure your DB supports this as it has not been around forever
I'm liking the looks of of RapidSQL XE3 - the fact that it's multi-platform is a big seller. Do you know, off hand, if it has any kind of automation capabilities? I'm beginning to suspect that Quest has that market cornered with their TDA product, which totally sucks. I'll take a look at Aqua Data Studio - not all Java apps run really slow, just some. I think I really just need an actual development laptop, not the run-of-the-mill ones that they hand everyone.
Nice alternative if pivot isn't supported in OP's engine version.
&gt; I'll take a look at Aqua Data Studio - not all Java apps run really slow, just some. I think I really just need an actual development laptop, not the run-of-the-mill ones that they hand everyone. I can tell you, without a doubt, that Aqua Data will be one of the Java apps that does run slow. If I load it up without any connections it uses about 200mb of RAM. .... What type of automation are you trying to do with RapidSQL? I'm using 7.4.2 but I can take a look in this version. I just haven't upgraded it.
Crap, PIVOT is not supported, my MSSQL Compatibility = 100, apparently is must be &lt;80. But I got SparcPlug's to work. 
Works! &lt;3
&gt;I can tell you, without a doubt, that Aqua Data will be one of the Java apps that does run slow. If I load it up without any connections it uses about 200mb of RAM. OH! So ***that's*** where all of Sun's developers went... ^I ^kid, ^I ^kid... &gt;What type of automation are you trying to do with RapidSQL? I'm using 7.4.2 but I can take a look in this version. I just haven't upgraded it. The Oracle DB that I'm using is essentially a real-time (or close to it) replication of our Billing System. As a result, there are a ton of people who need various and sundry reports - either one-offs or on a daily/weekly/monthly basis. TDA allows me to build automation scripts that, at a minimum, run selects, export those to Excel, and then email those files to whomever I choose. There's a ton of complicate stuff that it can do (such as support for executing Excel macros pre- and post- export, setting variables for use in SQL as well as filenames (dates) and emails, etc.), all of which would be nice to have, but I'll take what I can get. The issue with TDA is that it's buggy - really buggy - and they seem to keep breaking more things than they fix, and it can take forever to get an update out. And then the update won't have the private fix that they issued, so you have to wait at least another development cycle. So I'm just a bit frustrated :) Latest example: TDA doesn't recognize the SQL*Plus command for ignoring bind variables...
I was kind of afraid of that. I know that some of what TDA does could probably be replicated through PowerShell, but definitely not all of it. Either that, or a custom VB application and I just don't have time to write one or even figure out where to start. I don't know - I've gotten the 14 day trial for XE3 and I'll take a look once I get it installed, but I'm not seeing anything in the online information.
From this document: http://www.ibm.com/developerworks/data/library/techarticle/0211yip/0211yip3.html It looks like you can do: WHERE my_date &gt; current date - 1 year Sorry, I'm not otherwise familiar with DB2.
You're going to have to define "within the past year" more specifically. I'm going to assume based on your description you mean "is in the past 365 days unless one of the days in the last 365 days contains a leap year day in which case I mean the last 366 days". If I had user requirements that said "within the past year" I'd probably simply do this: where @CurrentDate between dateadd(yy, -1, @CurrentDate) and @CurrentDate Anyway, this link has a user defined function to determine whether or not a year was a leap year: [http://www.sql-server-helper.com/functions/is-leap-year.aspx](http://www.sql-server-helper.com/functions/is-leap-year.aspx) I'd do this: where case when month(dateadd(yy, -1, @CurrentDate)) &lt;=2 and fn_IsLeapYear(year(dateadd(yy, -1, @CurrentDate)) then days(CURRENT DATE) - days(my_date) &lt; 365 else days(CURRENT DATE) - days(my_date) &lt; 366 end After writing that, I can't remember for sure if case statements work inside a where. I think they do. Anyway, hope this helps. edit: ahhh db2... missed that part :P
Compatibility level 100 means [you have SQL Server 2008](http://msdn.microsoft.com/en-us/library/bb510680.aspx). PIVOT is supported in 2008. PIVOT is supported in all levels above or equal to 90. 
Yup, But it's not my tables =( ...grumble grumble legacy systems
For comparison, in Oracle it would be: WHERE some_date between months_add(sysdate, -12) and sysdate
No second trim. case when trim ( cast ( VALUE_1 as char(8) ) ) = trim( cast ( VALUE_2 as char(8) ) ) then 'true' else 'false' 
I'd recommend to use a function that returns the number of characters used in the error-message as an integer. There has to be something like str_length().
either add a trim to the section after the = or remove the trim in the part before the = or use VARCHAR instead of CHAR or forget CASTing altogether and just match one against the other if they are both the same type in their respective source tables.
Hackish but it'll work: SELECT DISTINCT code, msg from myTable a GROUP BY code,msg HAVING LEN(msg) = (SELECT(MIN(LEN(msg)) FROM myTable WHERE code=a.code)
If you arent sure you are looking at the right data, creating a test case in the app and observing how the data is populated in the database via SQL traces or just simply looking at how the data changes in the tables in question is generally a fairly thorough way to find out. In a perfect world, you'd have a well documented ERD to refer to, but this is often not available. 
When predictions of schema design match empirical results. Ultimately you're at the mercy of the designer of the schema and the data model. For all you know, NULL in a table could have a meaning other than 'undefined', depending on what hack designed the schema. :P
http://pastebin.com/skbnA2Cr This is essentially it to the barebones. The "MATCH" column returns false for every case.
I try to develop the same result set with different queries that each take a different approach. Write some query using GROUP BY across all of your aggregate fields. Then re-write a new query using aggregate CTEs, joined to the rest of your detail data. Make sure the results are the same. Write some queries that validate any assumptions you have on a data set. Use GROUP BY / HAVING COUNT(*) &gt; 1 to make sure that your data truly is unique where you think it should be. Finally, in a test environment, fudge a record or two and then re-run your queries. The behavior should change -- but does it change in the way you expect?
It depends on what you want and why you want it, but I'll often double-check against a query with another, more simplified query.
This has become my new methodology in the office when running month-end reports. Thanks!
Month end reports? Burn incense and pour a Forty on the ground for all the dead servers. My CEO reports require animal sacrifice. The phone call goes like: "I'll get you those reports right after I choke this chicken."
Wouldn't you be able to query the data mining database? The data mining model is saved as xmla IIRC, that would mean you could query for that xml-code and make reports on it.
Context?
Look at the original poster's screenshot.
Oh ok, my mind just kind of ignored it as I am Norwegian. I was expecting some big story I hadn't hear of where Google had a database of info they shouldn't have had. 
Some great responses here. General rule for me is to always have a clear idea of what I am expecting my query to do, then compare actual results to expected results. In particular: Start by noting the number of rows in table, then keeping track of the number of rows in query results as I add joins.
I'd say that big is somewhere in the neighborhood of a billion rows. Either database will serve you well without breaking a sweat, but I prefer Postgres overall because it has a generally richer feature-set.
I definitely agree. That's why I said in comparison. I usually deal with thousands max, nothing like thousands and thousands a day. Much of a difference. Thanks :)
what shoe string fixes are you talking about? just for reference.
I can't speak to your specific backup product because I've never used it. But in general most of the SQL Agent backups that I have seen use hooks provided by MS to appear to SQL Server as the same thing as a SQL Backup. So the agent backup does checkpoint, commit, and truncate the log just like a SQL Transaction Log backup would. As long as that is happening then your log growth should be controllable. This should be easy enough to test. Make a not of the amount of data in the transaction log, run your agent based backup job, take a look at the amount of data remaining in the transaction log. If the remaining data is close to 0 then then agent backup did a truncate.
There's two ways of looking at this. If a database is in full recovery, then a log backup must occur prior to being able to truncate and re-use that portion of the log. However, you can take a log backup and not have the truncation occur. Have a look at the LOG_REUSE_WAIT_DESC column in sys.databases. For example, you might have backed up the transaction log, but have an asynchronous mirroring partner that's currently offline. In this case, the log will be backed up, but won't progress until the transactions have moved to the mirror. In this case, the only thing I'm not sure about is when this part of the log is then truncated - at the next log backup, or at the next checkpoint. I suggest you look at whether your databases are having log backups, and how these are happening. Right click the database and select Properties to see the last log backup time. You can also look through the backup tables in MSDB. I think you'll find that NetVault (I've never used this tool) is handling the log backups for you. So, to confirm or deny your question: &gt; A database using the Full recovery model only truncates it's transaction logs after a log backup. Yes, it will only truncate after a log backup, but it might not be immediately, depending on other features that require that part of the transaction log. The other thing to consider is that log backups occur at the LSN level - you're backing up transactions - but the log truncation occurs at the Virtual Log File (VLF) level. If you have a 20 MB VLF, but only 2 MB of transactions, you'll still get a log backup (2 MB), but the VLF won't be truncated until you've used all 20 MB.
Apply a unique constraint, refer all bugs to legacy developer ;)
From a technical standpoint they are both very solid products and I think as long as the DB is reasonably designed either one will hold up to your technical specs. That said, and I will try to give an unbiased commentary here. You really need to look at the ownership and governance of these products. MySQL is currently under the stewardship of Oracle because of the SUN acquisition. PostgreSQL is still an independent open source product. Both of those situations and your opinion of Oracle probably need to factor into your decision. In a lot of cases you don't need to pay a lot of attention to governance at this level. But if I was writing a new application I would be looking at it really closely here. I have a strong personal opinion on the subject. So I do apologize if it tainted my comment, I don't think it did.
how hard is it to start learning one over the other? I'm used to MySQL but I haven't used it too in-depth yet. 
thanks for that point of view. I'm not very anti-Oracle but I do see what you're saying.
MySQL added them as of 5.0
They are both generally compliant to the SQL standard. Past that, each one has its own idiosyncracies that nothing but experience and googling can teach.
As far as 2. goes, I'm not sure how to approach this. I've no experience with it and Idk if I want to get into it at this point. I'd get a bit too off track ;) Thanks!
&gt; If performance of large datasets is your concern, why are you using a relational database? This is what NoSQL is for. Two misconceptions here. First, the poster says he's in the realm of thousands to possibly a million records. That's easily in the realm of what a relational database can handle. Second, he says he needs to do frequent small scale updates and occasional large updates. Again, a relational database is going to handle that better than a NoSQL solution.
Yes, absolutely. Every analyst needs at least a working understanding of relational theory and SQL. If you've mastered SQL, you could then find a MapReduce system to learn. Hadoop is gaining in popularity. 
If your intend is to become an analyst, learn SQL. Analyst's job is to deal with relational data, not flat/document/JSON data (which is the domain of NoSQL). For the foreseeable future, the domain of relational data will still be governed by SQL. NoSQL is generally use for special data applications, e.g. * a gzillion rows that can't be stored on a single computer * special data sync'ing requirements cross different servers/geographic locations * very high throughput requirement * very high update requirement Even when you need to analyze those data, you generally take a cross section of it, import it into a database, and do reports against that. 
As a data analyst whose job depends on SQL, I would say you should be fine not learning it.
That was my first thought, along with the "Set the 'truncation interval' on the agent" would lead me to believe that the FastRecover agent is taking care of the truncation.
Seconding that SQL is fun. Learn it, it's easy to start and easy to progress. Plus, it's awesome.
No. I mean yes 
Yup Yup Yup. I support Imaging Centers and Radiologists and I can't agree more with you.
Often even the other competing data systems will have access via a SQL layer. I actually learned most of my SQL knowledge building queries against an IBM DB2 system running RPG, of all things.
Close. It won't truncate until the **VLF** is full. Imagine you have a 200 MB log file containing 10 VLFs of 20 MB, and the current active VLF is #3, with no usage to speak of (a couple of minor transactions). The log backup position is at the same position of the current end of the transaction log - you've got everything backed up. Assume the transaction log backup occurs every 15 minutes. In one quick transaction, you write 5 MB of data to the transaction log, and then nothing more for 15 minutes. Once the transaction log kicks in, the 5 MB of transactions will be backed up, but there's still 15 MB left in the VLF, so the VLF won't be truncated. That's OK, because we've got 9 other VLFs that are unused. During the next 15 minutes, we have a flurry of activity, and generate 25 MB of log activity. This fills VLF #3 (which had 15 MB left), and halfway (10 MB) into VLF #4. When the log backup occurs, the 25 MB of transactions is backed up, and because we are completely out of VLF #3, it can now be truncated. If we have a slow transaction that takes an hour to complete, and creates 100 MB of transaction log activity that is equally spaced out over the hour, we will find four log backups (every 15 minutes), each containing about 25 MB of transactions. However, while this spans 5-6 VLFs (20 MB per VLF, and we would have started in the middle of one), none of them can be truncated until the transaction has committed. Extend it out to a 2 hour transaction with 210 MB of log activity, and you run out of VLFs and the transaction log must be grown. Does that help?
What type of analyst roles were you looking at? I'm a Business Analyst/ Project Manager here. My position doesnt request as much SQL knowledge; however, I've used SQL to my advantage many many times. As a BA I used it to help testing and validate workflows. I pull reports using calculations, track data, and figure out new project effectiveness before we pass the project to the Business Intelligence team. Does my current position require SQL? The answer is no. Does it help with the success of my projects? An astounding yes. I personally could have been find with learning basic SQL to pull the data I need, plop it into Excel, then create a pivot table, and have people do the analysis off that. I've done that a lot on my current project cause there was different analysis going on with the same data. Taking this route just isnt as fun as doing it all in SQL though. I recently had an interview that didnt require SQL; however, the hiring manager said it was a skill that would be tremendously valuable cause it helped with trouble shooting and testing. It greatly reduces the amount of time an issue is left open cause it would have to be sent to another team for them to get caught up then research the issue. Depending on the position you're going for you may just need basic SQL knowledge to pull and plop it into a pivot table. I would recommend learning it and doing simple select then doing joins across tables. That should give you the basic knowledge you need to actually say you have knowledge of SQL. DO NOT say you're an expert with just this knowledge cause it will come back and bite you. Beginner to intermediate should suffice once you're comfortable with the different types of joins. At my prior job I saw people claim they had intermediate to advance knowledge and then get slammed by someone more technical involved in the interview and it resulted in them not getting the job due to the fallacy on their resume. Learn as much of it as you're willing to learn. I know some engineers that ended up learning SQL but just hated it so they learned as much as they needed and then stopped there. You may end up loving it and pursuing a different career. Just try it out. 
fun? How can it be fun haha? 
Well I just figured a few people might know more than just SQL and recommend learning something else first. For example programmers all have their own opinions. 
You're probably going to need to use some VBA (Visual Basic for Applications). It's the programming language that's built-in to pretty much all MS Office applications. You could use it to create a macro in Excel that did the work for you, or a subroutine of some sort in MS Access. Either way though, it's not really something that's suited for subreddit.
Sounds like you don't even need a database for this. You could just have another sheet in excel that on a change in the first one, will update the 2nd sheet with the current count and time stamp it in the next available row. (this can all be done with vba) Just look up "VBA onchange" in google, and it will get you started. You may need to set excel to "advanced/developer" in the backstage options. It really sounds like access would be overkill for what you are trying to accomplish.
He got the job... I've been stalking him.
So, I started learning SQL yesterday. Once I got the basics down, my wife came in the room very concerned. Apparently I was cackling like mad scientist. She asked me what was wrong, and the only response I could think of was "People will pay me to do this?" and continued cackling.
SQL Fiddle is good for displaying this type of stuff http://sqlfiddle.com/ I think you need to declare and set your variables separately Declare @item = varchar(20) set @item = 'Moonbase' Also cannot see an End to your case statement
I missed the end, the variables are coming from me. This is just for a general search. Also more learning than anything. It was this or a convoluted IF Statement for each Checknumber, EmployeeID or Name
Yeah, that's much better. I shouldn't try to decipher SQL code while watching Pulp Fiction.
I would choose PostgreSQL for a couple reasons. Note that MySQL has some other tools for similar issues. However, thousands is small. Once you get into the millions range, some planning and tools are helpful. (For "thousands" chances are you can run tolerably without even using indexes. For millions you need indexes and maybe some table partitioning which PostgreSQL will do. For billions and lots of writes there are solutions around the corner for PostgreSQL that are not there for MySQL.) First, indexes are a bit more advanced in what you can index than in MySQL (in MySQL indexes are a little more advanced in how they can be used but PostgreSQL 9.2 closes that gap). In PostgreSQL, for example you can index the output of an immutable function called on attributes of a row. There are cases where this is a lifesaver when dealing with millions of rows. Secondly indexes don't have to cover a whole table, so if you have 10 million rows but only 1 million are frequently queried, you can index only those rows (yes this means you can do partial unique constraints too). if you are worried about truly big data with huge numbers of writes, there are some truly exciting projects going on right now. For example look up Postgres-XC which is a fork of PostgreSQL (probably to be released along with 9.2) which does write-extensible clustering in a way similar to Teradata. Basically to put in the terms of today, this is a transparent sharding system which enforces data consistency across the entire cluster and is also capable of intra-query parallelism. Postgres-XC will eliminate the final scalability difference between the big boys and PostgreSQL. Also if you are familiar with PHP, PostgreSQL supports PHP as a stored procedure language (I kid you not, but hey, it also supports brainfuck which is probably not an ideal choice).
It probably depends on which you know better and which you are more comfortable with. I know I am now slower on MySQL to get to market than I am with PostgreSQL, but for those who know MySQL very well, it is probably reversed.
I used to choose MySQL for rapid deployment but now I am finding I get to market faster on Postgres, and have since about 8.0. The really huge issue for me, namely difficulties with prototyping databases and adding/dropping columns, was solved in 7.3.
yes, but in PostgreSQL you can write your stored procedures in PHP or even Brainfuck. MySQL can't match *that.* ;-) (no, I am not kidding, both of those are supported via third party addons) Of course just because you can doesn't mean you should. And no I am not debugging your pl/bf stored procs for free ;-)
I sometimes deal with thousands and thousands a day. PostgreSQL rarely disappoints and when it does, it's usually me doing something stupid. Edit: stupid, for example, might be multiple large UNIONS over a 3 million row table in such a way that indexes are unusable, or a sparse index scan without an index.
Hey sorry for the late reply i had limited access for a while there. Thanks for posting back how you fixed it, but nope, never seen anything like this before ... interesting! I wonder if this is something to do with Oracle, I'm usually on SQL Server and this doesn't make much sense to me, maybe i'm missing something or maybe its something Oracle specific that I wouldn't know anyway. Can anyone else shed any light?
Well, I think this is why I have a magic circle painted onto the floor of the server room and I can make offerings to the Daemon whose services I require.
Ah, thanks for the reply. I certainly would like to know if anyone knows anything about this.
Facts are unalterable, or immutable. Immovable doesn't apply.
Yes, only for sanity purposes of round trips that the API is responsible for however. If I want to store data in my db, I want my data sanitized only for the purpose of db access. If I send it to a web page, it should be sanitized only for HTML purposes, and if sent to CSV, should be sanitized for that purpose. The approach we take in LedgerSMB is to have all data go through a single API which generates calls to stored procedures. This API utilizes lower-level API's for parameterized SQL calls, and does *some* sanitization of function names themselves. I the PHP classes we do not sanitize the function names because there is no easy way to do this in PHP (nor is there in Java). These are, however *never* passed by the user. If they are you have SQL redirection of a sort that is not ok. The lower level API is then responsible for data sanitization either by using parameterized queries (via libpq) or providing its own methods. Of course this is PostgreSQL only. Your mileage may vary.
Use multiple db's where you require provable isolution and a single db where you don't. If you are talking about a master db, you probably want a single db. Anything else is needless complication.
&gt; So we add a trigger to the table that stores the inventory movements which queues these for processing Really?
Define "works" In your first trigger the RAISERROR is _always_ going to run because it's not encased in a BEGIN / END block. If an IF statement isn't followed by a BEGIN / END block only the first statement that follows it will conditionally depend on the IF statement, Any further statements will run regardless of the outcome of the IF. Because the next statement is raising an error with a severity of 16 the transaction is getting rolled back by the database engine. So the real question is, are your join statements actually working? Is the application truly deleting from the header record before it deletes the item records?
Define "works" &gt;it prevents all the deletion no matter what It goes straight for the raiserror part. &gt;So the real question is, are your join statements actually working? Is the application truly deleting from the header record before it deletes the item records? That's probably it. The application deletes the item from the [Order Items] before my trigger runs :( Is it possible to make a 'global' trigger so I can look at the deleted items from the other table as well?
There are server level triggers but they aren't for doing things like that. Why not just create an additional trigger on the Order Items table?
How could I join or query between these triggers? There are more tables where the Order number is deleted and I would like to stop the process early on.
Your best bet would be to run a SQL Server Profiler trace to capture exactly what the application is doing in what order. Then create your triggers appropriately. Figure out which table it deletes from first and put your check / rollback / raiserror in there. Then figure out whether the application actually stops after encountering the error from the trigger or if it keeps moving on to the next tables to delete from. If it stops then you don't have to worry about any further triggers. If it ignores the error and keeps deleting then you might have to put triggers on every table. Or alternatively you could make your first trigger raise a higher severity error. For instance severity level 20 errors will immediately halt all further action for that session and disconnect the user. Granted, that's a bit extreme, but if that's what it takes to stop people from deleting stuff that they aren't supposed to, it's an option at least :)
The row contains a PK that is used in lots of other tables. And they may delete it (its often necessary) but not when there is an open order. When they do stupid stuff like this its their own problem, but I'm trying to save them (and the company) time.
I'm already pretty sure its going to require the most complicated solution...
Am I silly for asking why a foreign key isn't in place to prevent these deletions? I would think that a Foreign Key between orders and items and making sure you don't have cascading on would force an error if they tried to delete an order with items in it, and it would be a lot more reliable than a trigger, which should probably be avoided unless it's absolutely necessary (I'm sure there are a few people here who've been burned by triggers at some point or another.).
The problem isn't an issue of relational integrity. As DrTrunks discovered, the application is deleting the child records before the parent records. The problem is an issue of business rules. They don't want users deleting orders that have had items assigned to them for business reasons. At least that's the gist I got from it.
sounds like an "eggcorn" to me (look it up).
Can you modify the frontend of this database? If so maybe you could add a column to mark an order record as 'Deleted' but don't delete the actual records. Then you can easily unflag an order if it was in error.
Is there a reason you don't just make ID an IDENTITY(1,1) column and SELECT SCOPE_IDENTITY() after the insert? That should always be atomic and not require any special transactions. Also, to answer your question, no, that code won't prevent other statements from using that ID. Imagine this scenario: Nothing is blocking the table, 2 transactions kick off at the same exact time, both of them perform the select statement and get a MAX(ID)+1 of say: 5 (because repeatable read only blocks itself if something is locking the record, but since neither had started their insert yet, nothing was blocking, they both get the same ID), then they both try to insert with the same ID. Also: If you were using SQL Server 2012 you could use a SEQUENCE object, not that it helps you in SQL Server 2005, but just an FYI.
First off, how complex is your database? Are we talking 10-20 tables? Or hundreds of tables / views / procedures, etc... If it's large and complex you'll want to read over this [whitepaper](http://download.microsoft.com/download/7/C/2/7C20B070-BFF8-44B4-BD7D-1B03DF50F924/MigrateMySQLtoSQLServer2008.docx) at least. If it's small, just recreate your tables manually and use the Import/Export wizard in SSMS to import your CSVs into the tables. Technically the wizard can create the tables itself, but if you want to ensure they're using all the proper data types and have all the proper relations I recommend creating them yourself first.
This for sure. Max ID + 1 is a disaster waiting to happen. Use identity instead. 
I don't know much about the tools for MySQL, but the general gist would be: Create your SQL Server database using SQL Server Management Studio (SSMS - the standard tool for SQL Server development / administration) Export your create table statements from MySQL and run them in your SQL Server database. The syntax should be nearly identical, but you may need to make some slight alterations to get it to run. The whitepaper I linked earlier should help with any issues. In SSMS, right-click on your database and choose "Import data", change your data source to Text File / CSV, pick one of your CSV files, and then choose our destination as your database, then select which table to import the CSV into. Click on the table and set up all the appropriate mappings and click Finish / Run. You should be able to figure it out. Microsoft tools are pretty easy for anyone to use.
No matter what, at the time of the insert you need to know the exact state of the data. You'll need to do something like this: CREATE PROC foo @Val VARCHAR(50) @ID BIGINT OUTPUT AS BEGIN DECLARE @Output TABLE (ID BIGINT) INSERT INTO YourTable (ID, Val) OUTPUT inserted.ID INTO @Output SELECT (SELECT MAX(ID) +1 FROM YourTable), @Val SELECT @ID = ID FROM @Output END I would just like to go on record stating that this is terrible table design and I in no way condone it.
...yeah tell me about, this database it's a rack, perfect example of bad practices...for this problem i used an auxiliar table
I work at a data analyst, and I use SQL everyday. Here are the two books I have in my office for reference: [book 1] (http://www.amazon.com/Oracle-10g-SQL-Joan-Casteel/dp/141883629X/ref=sr_1_1?ie=UTF8&amp;qid=1345343539&amp;sr=8-1&amp;keywords=oracle+sql+10g) and [book 2] (http://www.amazon.com/SQL-All---One-For-Dummies/dp/0470929960/ref=sr_1_1?ie=UTF8&amp;qid=1345343578&amp;sr=8-1&amp;keywords=sql+for+dummies). These plus Google have saved me many times.
I try not to disappoint. And I wasn't sure you'd notice how much I care.
This will find the next Sunday based off today's date, then print the next x Sundays DECLARE @nextSunday DATE ; DECLARE @loopCount INT DECLARE @numberRequired INT; SELECT @loopCount = 0; SELECT @numberRequired=7; SELECT @nextSunday = DATEADD(week, DATEDIFF(week, 0, GETDATE()), 6); WHILE (@loopCount &lt; @numberRequired) BEGIN PRINT @nextSunday; SELECT @nextSunday = DATEADD(day, 7, @nextSunday); SELECT @loopCount = @loopCount+1; END 
In case you get monday or sunday wrongly as the first day of the week check this out. http://blog.sqlauthority.com/2007/04/22/sql-server-datefirst-and-set-datefirst-relations-and-usage/ It drove me mad in 2009 since our database language was english but our customers used a iso 8601 calendar.
W3Schools.com is very good for SQL and has a little multiple choice test at the end to test your knowledge.
[DBconvert for MSSQL and MySQL](http://dbconvert.com/convert-mssql-to-mysql-pro.php) is able to convert automatically db structure, table data and views.
If you download it "with Advanced Options", you will get Reporting Services. You'll also want to download the Management Studio. Link here to for downloads: http://www.microsoft.com/en-us/download/details.aspx?id=29062 
OP said he was trying to save disk space. Reporting services not needed to learn SQL. 
Please note that "SQL" is a language, and that there are many different tools that use SQL for doing queries. PostgreSQL, MySQL, Oracle, DB 2, SQLite and many others are all tools that use SQL to perform interactions with the database.
&gt; * What data is in the tables we are comparing? * What are the key values that constitute the relationships between them? * Why are the columns compared? * Why does the result of the comparison matter? * Are you only comparing two tables against each other at a time? * Unfortunately I can't discuss specifics as it is confidential info. But I have three physical systems, each with three smaller subsystems. The subsystems then have their own components in it, which are split into three categories. The systems' components should be identical so I'm ensuring that they are no inconsistencies. 3 categories, 3 subsystems, and 3 systems means they'll be 27 lists of components. I'm thinking each category (within a subsystem) needs to be compared with another systems' (of the same subsystem) category. *Since the components should have been the same, I created a relationship with the components (of one subsystem) of system I to the same components (of the same subsystem) with system II. I then created a relationship between system II and system III using the same mindset. This is what the picture represents. I'm unsure if this is the correct way to do this as I have only used Access for a few days now. *The columns are compared because the components should be identical with the other two systems. *The results matter because I need to ensure that there are no inconsistencies with the components. *As of now I am comparing three lists at a time. I hope this helps clarify things. I typed the OP while my battery was low so I couldn't proof read it.
And I didn't answer your question about nulls. Yes, "is null" checks for an empty record in the data. Some fields will have nulls as possible values (If your table contains names, you might have a field for Middle_Name, but you won't likely actually know the middle name of all your customers.). However, many fields should never have nulls (a generated ID field, for example). If the field should never be null, but the B table has a null, it means the b table didn't get a match in the join.
&gt; Forgive me for the dumb question but doesnt the is null command for finding empty spaces? IS NULL matches on a null value, white-space is not a null. When working with SQL it's important to keep in mind that a null represents the absence of data while a 0 or a white-space represents data with either a zero value or no text. To try and be more concise: if you perform an operation and get a 0 back, you know that the answer to your operation is 0. This is not the same as getting a NULL back as the NULL means that you do not know the answer. &gt; Also, this is for two tables correct? How could I go about doing this for 3 tables (or is it even worth the effort since doing 2 at a time would be more efficient). You can chain joins together e.g.: SELECT a.[field1] as 'A1', b.[field1] as 'B1', c.[field1] as 'C1', a.[field2] as 'A2', b.[field2] as 'B2', c.[field2] as 'C2', a.[field3] as 'A3', b.[field3] as 'B3', c.[field3] as 'C3' FROM [Table1] a LEFT JOIN [Table2] b ON ( a.[field1] = b.[field2] AND a.[field2] = b.[field2] AND a.[field3] = b.[field3] ) LEFT JOIN [Table3] c ON ( a.[field1] = c.[field2] AND a.[field2] = c.[field2] AND a.[field3] = c.[field3] ) WHERE b.[field1] IS NULL OR LTRIM(RTRIM(b.[field1])) like '' OR c.[field1] IS NULL OR LTRIM(RTRIM(c.[field1])) like '' That should give you all the fields from each table clumped together (i.e. all the field1's together) With nulls in field1 in Table2 or Table3. The "OR LTRIM(RTRIM(b.[field1])) like ''" lines will catch fields with just white-space. &gt; And how could I expand on this? I'm assuming I'd need to perform a.field4=b.field4 and a.field5=b.field5. Is there a way to have it run through table A in its entirety? Unless you have a **ton** of columns, just type it in otherwise you are going to delve into cursors and dynamic SQL; which, you probably don't want to face quite yet.
If the data is the same type and lenght you can use a checksum function on those columns and compare the results. So if you have 100 columns you don't have to write out each name if you don't are about how different they are, only that they are different. edit: I'm sorry but I only know how to do this in mssql.
When the people that request it are happy 
We have lots of storage, so I am not too worried, I'm just concerned if the log file grows unwieldy over time. Should I be setting any sort of size limits on the log files? We're currently in testing on these new servers, and so far everything is working great I just want to make sure its configured as best as possible for when we go into production
I am using full for the production dbs, simple for the test companies. 
Yes! This is an incredible resource. Thanks for this. I have been looking for something like this. 
The "v" and "w" are table aliases. Consider the two statements: SELECT SomeTableName.ColumnName1, SomeTableName.ColumnName2 FROM SomeTableName And SELECT S.ColumnName1, S.ColumnName2 FROM SomeTableName S They're both identical in terms of what they do, but the second one is using a table alias (S in this case). Here, the only benefit of using table aliases is to save writing the full name (so less typing) but aliases serve a more important function to enable you to refer to the same table more than once in a query / subquery. If I have a statement like this: SELECT SomeTableName.ColumnName1, SomeTableName.ColumnName2 FROM SomeTableName INNER JOIN SomeTableName ON SomeTableName.ColumnKey1 = SomeTableName.ColumnKey2 This won't work as the db engine has no way of telling which version of the table I'm referring to in any of my statements. But if I do the following : SELECT S1.ColumnName1, S2.ColumnName2 FROM SomeTableName S1 INNER JOIN SomeTableName S2 ON S1.ColumnKey1 = S2.ColumnKey2 Then it's apparent what is coming from what and the statement will run (well, if there were tables matching the above description). I personally think you should use table aliases in every query you write, but some disagree with that, finding the full name easier to follow in some cases. YMMV. edit: I personally try to avoid one character table aliases when I'm working although I'm not sure why. A popular convention is to use a short version of the table name, e.g. SomeTableName in the above might have an alias of STN or STN1 or what not.
in my example - us w = the table worker_edition?
Hide from who? If someone can see the SQL statement they will be able to see the table name (aliases are defined per statement, they're not something you can do globally or encapsulate somewhere else - ignoring the case of synonyms which are slightly different). In general, no table aliases are not used for security purposes that I know of. I suppose if you wanted to paste some code into a forum or something and didn't want to give the table names then aliases would help you there slightly (although you'd still be giving the column names).
Yes, although without seeing the full query I don't know what table v is supposed to be (it could be another instance of the same table or some other table - if they're following some sort of convention then it may well be a table beginning with the letter v). 
the table is not actually in he statement... this is a web application based on an oracle database, so I am guessing it is to hide the table name? but I'm just learning so maybe I'm just being ignorant.
Well the snippet you posted isn't (by itself) valid SQL - it's part of a larger statement - that code is only for one column. If this is a web application it's possible that the statement is being built dynamically and the rest of the code is somewhere else. Wherever the rest of the code is will have to contain the table name to run - aliases don't allow obfuscation of the sort you're suggesting.
First I would evaluate your need for a function at all. In Oracle using functions like this, instead of using joins, hides information from the optimizer and it usually results in a less than optimal execution plan. If you truly do need a function then this should give you a start: CREATE OR REPLACE FUNCTION a1_guest_name_exists ( p_guest_name IN a1_guests.guest_name%TYPE) RETURN NUMBER AS v_guest_id a1_guests.guest_id%TYPE; BEGIN SELECT guest_id INTO v_guest_id FROM a1_guests -- You probably want to think about case sensitivity -- as well as if you want to wrap the whole search in -- wild cards or not. WHERE guest_name like '%' || p_guest_name '%' ; RETURN v_guest_id; EXCEPTION WHEN NO_DATA_FOUND THEN -- Do you want to raise an exception or return another value? WHEN TOO_MANY_ROWS THEN -- Do you want to raise an exception or return another value? END; You'll need to consider exceptions and how you'd like to handle those. 
@edit - I personally agree with using short names. With large queries it is a real pain to look over the code and define what 'p' is. Say I'm looking at tables provider_code and provider_loc; it's easier to spot provcode rather than 'p' or 'pc' and if you *do* use 'p' you then have to figure out which provider you're looking at.
make your existing query a sub-select SELECT product, department, max(cnt) FROM ( SELECT c.product, a.department, count(a.department) cnt FROM order a, orderline b, inventory c WHERE a.orderiid=b.orderiid AND c.productiid=b.productiid AND c.product in (select product from inventory where department is null) GROUP BY a.department, c.product) GROUP by product, department something like this might work. Don't know if there is a better way to do it. Hopefully someone else will weigh in if so. 
What do you want for output if a product had the same number of orders in each department?
This looks bad ass, let me try and translate this.
Thank you, I'm getting the exact same result with the sub-select and without. ProductA Audio 2 ProductA Video 6 Ideally, someday I'l have it just tell me "Product A Video" 
I used your idea of a rank() OVER (PARTITION BY ..) I'm one step closer! How do I extrapolate just that middle row below? I tried max(rank) but I'm failing. Product Department Rank P1 Audio 3 P1 Video 4 P1 Lighting 2 
Ah yes, that is what I would have expected. Someone else posted a suggestion that uses rank which I haven't worked with much. maybe that will get you what you need.
Is [snapshot replication](http://technet.microsoft.com/en-us/sqlserver/ee848811.aspx) an option? You can schedule it and it sounds like it would fulfill your requirements.
This is a question for an assignment I have at school, so that is why it probably makes little sense. It asks for a cursor, I think that is where I had a trouble. Thanks I will try this when I can.
Yes it asks for an explicit cursor. I know, Oracle is a lot to learn and take in, especially while doing a myriad of other languages. I haven't been able to do my study at home for a while because SQL Developer will run a statement indefinitely and crash sometimes. This might sound like a silly question, but in a function can I use DBMS_OUTPUT? It seems like i should be able to, but I haven't got it to work.
That sounds close to what I'm trying to do already. There will be a limited number of phrases available mostly having to do with activities, I just want to catch all variants of that phrase (at least all of them I can think of). IE, one table would have "Eiffel Tower" as the activity, but ultimately, while "Visit The Eiffel Tower" and "See The Eiffel Tower" should both be activities the user can search for and find, I don't want an entry for "Visit The Eiffel Tower" and one for "See The Eiffel Tower" and yet another for "Go to the Eiffel Tower". The reason being, I want to be able to count how many users are going to the Eiffel Tower without duplicate DB entries, but with the users choice of phrasing in tact... does that make any more sense?
You should be able to use DBMS_OUTPUT even though in practice you probably wouldn't. The problem is most likely that you haven't enabled it. In SQL*Plus you'd enter SET SERVEROUTPUT ON; before executing your calls. Enabling it essentially depends on your client. 
So I'm at work now and I gave this a spin. I updated my first post cause I realized it had some character and alias syntax errors in it. I didn't know what to make of the where clause so I removed it, put some values in tables to match the query and ran it. I added a count field in the CTE and commented out the line that filters by rank. WITH rankings as ( SELECT RANK () OVER ( partition by inv.product order by COUNT(*) DESC) as rn --change rank to row_number() if you want more than one result in a tie. ,inv.product ,o.department ,COUNT(*) as number FROM [order] as o INNER JOIN [orderline] as ol ON o.orderiid=ol.orderiid INNER JOIN [inventory] as inv ON ol.productiid=inv.productiid GROUP BY inv.product,o.department ) SELECT product,department,number,rn FROM rankings --WHERE rn = 1 --omits any lines that aren't "the max" order by rn asc Then I get this result: product department number rn P1 Video 4 1 P1 Audio 3 2 P1 Lighting 2 3 Were I to put my rn filter back, it would only keep P1, video, 4
I ordered the records the other way around so that the max value for a product would always be 1, then I selected only rows where this value was 1.
Have you tried an SSIS package, they can backup and restored along with running custom scripts 
I haven't used Sybase extensively, and what I have is mostly via Perl. However is something like this what your after? http://manuals.sybase.com/onlinebooks/group-as/asg1250e/sqlug/@Generic__BookTextView/40957;pt=41423 I guess it would also depending on what version your running.
I think you would have to do something like this declare @setofSERIALNUMBERS varchar(1000); SET @setofSERIALNUMBERS = char(39) + '3990004184' + char(39) + ',' + char(39) + ',' + char(39) + '3990002745' + char(39) + ',' + char(39) + '3990004001' + char(39); EXECUTE('select * from tbl where serial_no in (' + @setofSERIALNUMBERS + ')'); I like using char(39) better but you may prefer to escape the single quotes with more single quotes. '''3990004184''' It may be easier to define the serial numbers in a table and then join the table instead of doing this string manipulation. I work with Sybase everyday so feel free to ask me anything. Syntax can be a bitch sometimes and I often end up in semicolon hell. 
Thanks. I'm trying to follow what that's saying but my noob-level knowledge of sql isn't making much sense of it. Here's a way simplified version of what I'm trying to do and how I interpreted the syntax on that page: DECLARE @setNIIN CHAR() select @setNIIN = "'015802575','015788805','015657449','015360983','015549530'" select 'Total Radios' = (select count() from eqlist_view where niin in (@setNIIN)), 'Total Comp Sets' = (select count() from eqlist_view where niin in ('015138459','015542356','015669895','014755277','015527849')) When I try to run the query I get the message "Database Error: Syntax error near '@setNIIN' on line 1" 
I'm no expert, but I think you may have downloaded the framework only(which you already have installed). http://www.microsoft.com/en-us/download/details.aspx?id=29062 try "ENU\x64\SQLEXPRWT_x64_ENU.exe" 
I'm getting the error "Result set not permitted in '&lt;batch statement&gt;'"
I believe the "AS" keyword only works with Column aliases in Oracle. 
Looking more into this. Neither system has the Replications component installed so I need to get them running this morning. Thanks much and I will update if you gave me an awesome answer. 
I don't know. SQL is a language and I have to install other languages on my computer. Why not SQL? /
Would this only replace the data, or would it also delete other objects like stored procs (that are only on dev)?
No, it works in T-SQL as well. :)
Yes, sorry - I meant it doesn't work with table aliases in Oracle. I know that T-SQL supports the AS statement in table aliases - I use it every day. :)
If I recall correctly, it will replace any objects defined in the publication. i.e. You can define tables and stored procedures to replicate between servers. 
Double escape the quotes. @setNIIN = " ' '015802575' ',' '015788805' ' ,' '015657449 ' ',' '015360983' ',' '015549530' ' "
Yeap...exactly what I had concluded as well. Just seeing.
With MS SQL, you can convert the column name to drop off the seconds. the syntax would be... CONVERT(VARCHAR(16), &lt;ColumnName&gt;, 120). This code would show you the difference between the two... DECLARE @time SMALLDATETIME SET @time = GETDATE() SELECT @time, CONVERT(VARCHAR(16), @time, 120) (No column name) (No column name) --------------------------------------------- 2012-08-29 15:21:00 2012-08-29 15:21
Thanks for the advice. I've been teaching myself how to operate slowly in Ubuntu, so information like this is invaluable. As far as your question, this is just an "intro" class, but we will surely start using the programming language once we have learned the basic elements of database creation in Access. They only expect us to be making tables for the next chapter, so they want a pretty presentation with not much effort (hence the need for a black box application). So for now I just need noob software; however, I personally am aiming a little higher than class requirements because, hey, I want to learn something! For what I need I think I'll experiment with the different options you provided, starting with PostgreSQL and PHP...then see where I get from there. Thanks again for responding!
The closest thing to Access on Ubuntu is Base, part of LibreOffice (the office suite included in default Ubuntu desktop installations). It has its own front end and simple database, but can also interact with separate, more complex databases. 
That was my hope. I've installed it and am going to give it a shot for comparison to the other methods. Thanks!
Wait, what? If your class is using MSAccess, you need MSAccess to get all of the benefits of the class. There are subtle differences in every database and if the instructor is using one, but you are using another... Let's re-ask your question, but change the class just a little: "I'm taking a class for scripting and the instructor is going to use PowerShell on Win7. But I use Linux. What Unix/Linux shell should I use for this class?
The instructor is using MySQL for the class, but the book and assignments require Access for the first 2 chapters (We aren't covering the working side of those chapters in class). The rest is MySQL. The instructor has also been very clear that we are allowed and encouraged to use MySQL. I personally don't think he's read the book, partially because he is teaching the class from the perspective of working in the field.
I have no idea why your instructor is mixing and matching databases for a single class. It's very likely that you are taking this class as a portion of a larger "web technology" curriculum. If this is the case, the only reason to use Access in the beginning would be because if the visuals Access provides, as well as that with Access, you are using the software directly. Access is highly visual and there is no "web front end" used to manipulate a server-side database. You use the database to manipulate the database. Simple. Simple. Simple. As others have said here, Open Office has a kind of clone of Access. The Libre Office thingy is just a nicer version of the same. 
This is so hard!!! It seems to know about the AdventureWorks database. Which, I think means that database is what you had open when you tried to "restore" a different sample database. The instructions said something very close to "create a new database, then 'Restore' it" using the restore file of the sample database. Did you skip the Create New Database part?
 Isn't MySQL considered a piriah these days after Oracle gutted/ruined it?
I would spend some time with phpmyadmin. Do this: sudo ln -s /usr/share/phpmyadmin /var/www/phpmyadmin Then visit localhost/phpmyadmin in your browser
Still tinkering with everything. Had other things comes up and had to throw this on the back burner until today. Hopefully I can report back soon. 
Uhh, no we all haven't.
"PHPMyAdmin was mentioned. I'm not a fan, I think it sucks (once again, I leave you to google the why)" As someone that has used phpmyadmin for a long time, please enlighten me as to how it sucks? I've never had any issues with it.
Woah. I just found this subreddit and saw this. I just today wrote a script to take care of a similar situation. We have a massive production database which the reporting team wanted a copy of on another host so they could hammer away at it without affecting production. We keep all of our backups on a network drive, and they wanted a weekly restore using the latest full backup. Here's the plan: 1 Drop the database on the destination host (if it exists) 2 Find the latest full backup file 3 Restore the backup file on the destination host, moving the data files to the proper drives 4 set the newly restored DB to simple mode 5 give the group access to the database This way i don't have to touch the thing and can set up an agent job to run it whenever. Also, I don't have to move the data file. You run the script on the destination host, and you'll just have to make sure the sql service logon account has rights to the backup folder. SSIS can do the job, but scripting it out was surprisingly quick. Finding the last backup was the tricky part. Here's the script: http://justpaste.it/19ga PM me if you want it emailed to you, if the link doesn't work out.
 "PHPMyAdmin was mentioned. I'm not a fan, I think it sucks (once again, **I leave you to google the why**)" ... Sigh. Whatever. http://lmgtfy.com/?q=PHPMyAdmin+sucks Basic bullet reasons: * Database administration should not happen through a web page. That's an increase in surface area (security wise) that is inappropriate in high-security situations. * When you depend on the browser for database work, you're depending on a system with no guarantee of delivery. * It is much slower than an application, and less feature rich. You have to pass all control panels through HTTP, and interpret them as HTML, as opposed to natively rendering them on the computer running the application, and communicating via TCP. * Did I mention I would never trust database management via web management, period? It's very hard to decode what a person is doing by middle manning a connection communicating binary information, but it's really trivial to interpret HTTP headers and plain english. * Just because a lot of people use something doesn't make it good or appropriate. * cough * Windows servers * cough * * You shouldn't ever run a web page on the same server as it's database. I am aware PHPMyAdmin can remote control a database, but it makes it that much easier to put everything on the same stack. I guess this is an argument against lazy sys admins more than anything else. Further more, if the web service crashes, how do you do your DBA? You can't! PHPMyAdmin isn't horrible to use, but for it's intended purpose, it's a square peg in a round hole. Congrats to never having issues with using it. Congrats on not being able to google too; there are 7,340 pages specifically with the term "phpmyadmin sucks". I'm sure there is at least one that explains it better than I have. In other questions: Have you ever used anything else? Enlighten me to what tools you've used. Ever had to DBA only using a command line? Saved my bacon time and time again when PHPMyAdmin failed because it is a browser based implementation. Honestly, if you can't use SQL from a command line, you shouldn't be doing DBA; DB's are very low level, and if you need a control panel to manage them, it's going to bite you in the ass at some point. Throw your crutch away. Also, Vim is better than emacs.
I have grown fond of the Microsoft Access Query builder interface. IMO, the visualization aspect of it, coupled with the ability to look at the underlying SQL that is generated is slick and quick way to develop database interfaces. I also have learned to work with the form (screen) design tools and the report (paper) design tools. Are their open source alternatives to these three MS Access application layer tools?
Yes and no. Individually, yes: Query building can be done by the workbench tool I believe: http://dev.mysql.com/downloads/workbench/ I suggest you not use the query builder though. You are learning. This will not help you learn. Learn you some damned SQL instead of expecting it to be built for you. Seriously. Come back to query builders when you can write it in your sleep; then you'll know what it's producing, and you'll understand the limitations (actually very severe limitations) of trying to graphically represent logic. Reporting for MySQL : http://jasperforge.org/ Once again, you should learn to write your own damn reporting so you have a clean understanding. Form building: Ewww. Ewww. Eww. http://www.php-form-generator.com/ I really suggest you not do this. You need to learn how to write your own forms, ESPECIALLY if you are learning how to write web pages. I actually suggest you not use anything I've linked here. I'm not joking. You need to understand how these things are built, otherwise you're just another tool using jackass that can't do shit when anything outside the box is needed. If you need tools to use your language, you do not understand the language; that's like needing google translate to speak French. You don't know how to speak French, you just know how to use a tool. You won't understand or know when the result the tool sends you is wrong, and you have no ability to disseminate correctness or validity. For all you know, it's correct, even if you sound like a jackass. Furthermore, you wouldn't know when words that mean different things are the same word; for example, 'light' in the sense of photons is different than 'light' in sense of weight, but they are the same word in English. If you translate to japanese, though, they are different words. The tool doesn't know what one you mean, and has no way to know. Seriously, if this is the way you're going, quit now, take some project management classes, get a business major, and learn some .NET instead, and you can be yet another tool using project management jackasses that thinks they can program. Or maybe you can be yet another 'just a web developer' that cannot work independently from the space that the industry has already created for them. Both of these can pay well, but are very unrewarding outside of financial compensation. If you're just looking for a good job, ignore everything I say about these things, and consign yourself to relearning your toolset every two years as the 'in' things change. Or you can learn to program, and create the tools you need instead of depending on others to make your world for you. Also, all these questions... They were answered by googling them. You could have googled them. You cannot survive this industry unless you take a more proactive approach to learning. Read this entirely, please: http://samizdat.mines.edu/howto/HowToBeAProgrammer.html . It should help you out on many levels. Sorry if I came out snarky. I'm an asshole.
&gt;snarky I think you have me sized up wrong. I have been developing database applications professionally since 1978. And, I was an early adopter of [DBaseII](http://en.wikipedia.org/wiki/DBase) running it on the [CP/M operating system](http://en.wikipedia.org/wiki/CP/M). And that was just the start... Of course I *can* write SQL, but I still like using a graphical interface like in MS Access for quick development.
Yep, I have been developing and administering custom database applications (using mostly Microsoft Access &amp; VBA) on salary for one client for almost 30 years. Running a series of half billion dollar projects, A/P A/R and more. ...DB's been berry berry good for me. That said, I am curious about open source options, *if* they suit me better I might want to try them out.
I've found the open source programming systems to be more effective than proprietary solutions, however, the tool sets available to them are nowhere near as complete or feature-rich. It is more likely that you'll find one tool for each thing that you want to do instead of one tool to do everything you want to do. That said, if any tool almost gets you where you want to be, you can get the source code and make it go the rest of the way yourself, or if there is a tool that you want to base your own project off of, you're free to do so. You are also much less at the mercy of bugs; in a proprietary system, you have to either wait for the authoring company / person to fix the bug, or you work around it. In open source, you can fix it yourself. It's a trade off between flexibility/control/cost and completeness/power, in my mind. I'd take flexibility and the ability to change the software above the convenience of having everything in one place, but I am horribly biased: I work in the web world and do a lot of research work, both which favor open source systems.
Also, transparency (open source) means never having to apologize. For assholes like me, the ability to say 'well, fix it yourself then' is priceless.
You could always use a date dimension. these are typically used for OLAP but can be very useful in t-sql as well. you can find a script to create and populate these tables online. A sample query to do what you want would be: Select CalendarDate From dimDate d Where DayOfWeek='Sunday' And CalendarDate Between GetDate() And DateAdd(week,3,GetDate())
- Not that Sun did a great job with Java... But they seemed to care about the community, whilst Oracle just wants consulting dollars and a basis for stupid lawsuits. 
Thank you for introducing me to Dense_rank. Just today, was I able to get the desired output.
Use lightswitch, you can actually create parameters ala @variable name, and it will apply them. Seriously VS2012 lightswitch is badass now that it uses ODATA rather (and produces a n-tier odata service and ui). The HTML beta is up on technet which use an html interface for 3-tier instead of Silverlight , but you can still make 2-tier applications. Lightswitch+SQL server (especially when html comes out) is tits compared to access. Everything but power pivot but that's another story.
I remember setting all my DB's to MAXDOP of 1 because of some strange scope bug. Have they ever fixed that? Seriously seems like a forever bug for Microsoft. 
Sorry, it was a shot in the dark. Like I said, I don't use replication. Have you found anything else that works yet?
Reinstalling SQL looks like my only choice. Thank you for your input though!
Um, doesn't Qt offer a set of functions for working with date/time values? You should be using it to read the smalldatetime string from SQL Server, and then manipulate the output to display in the format you want. Here: http://doc.qt.nokia.com/4.7-snapshot/qdatetime.html See the "fromString" and "toString" methods.
SELECT email FROM table2 WHERE email NOT IN ( SELECT email FROM table1 ); http://msdn.microsoft.com/en-us/library/ms177682.aspx
Why not: Select t2.email From t1 full outer join t2 on t1.email = t2.email where t1.email is null
See my above comments - this doesn't work because all of the emails that are missing in table one are not distinct. For example, joe@joe.joe might exist four times in table two, but only three times in table one. This makes this email address not returned by your query. 
use DISTINCT then? *edit, actually scratch that I see what you are trying to do now. See Fireman_071's comment below. :)
If you are looking for those 115 erroneous entries, dusting won't work since it will come back empty. What you could do is some elaborate unions and counts to get your list of emails. Something like: Select email from ( select t1.email, count(t1.email) from t1 Union Select t2.email, count(t2.email) From t2) Group by email Having count(email) &lt; 1 The idea is there but you may need to work it a little to get it to work. 
Same problem as techwrecker up there, unfortunately - if joe@joe.joe exists four times in table two, but only three times in table one, it will join these records together just fine, but wont give the piece of info needed - that it exists more times in one table than another. I feel like I am missing something obvious here ...
This is what I am working with right now - it will return the first record that doesnt line up, and you can kinda jimmy it by going one record at a time, but my business need requires something more scalable. The two CTEs just return the data from each table: with sendTable (rownum, email) as ( select ROW_NUMBER() over (order by m.emailaddr_ ) , m.emailaddr_ from stats.[Send] s (nolock) join Members_ m on m.MemberID_ = s.SubscriberID where s.JobID = 13659916 ) , DETable (rownum, email) as ( select ROW_NUMBER() over (order by email ) , Email from c172300.[2012-08-30 Payment not Received - wave2] (nolock) ) select s.rownum , s.email sendLogEmail , d.email deEmail from sendTable s full outer join DETable d on d.rownum = s.rownum where s.email &lt;&gt; d.email 
I think this might be what I was missing. brb EDIT: This isnt quite right(it is still returning zero results) but seems to be a step in the right direction. 
Try: ;with list1cte as ( select email, row_number() over (partition by email order by email) as newrownum from list1 ), list2cte as ( select email, row_number() over (partition by email order by email) as newrownum from list2 ) select * from list2cte a full outer join list1cte b on a.email = b.email and a.newrownum = b.newrownum where a.email is null or b.email is null
That is more concise and will definitely help get better answers! Sometimes we miss words. :) you should be able to run the application with out installing it, but it will still have extra data files with it. IE: MainApplication.exe, DatabaseFile.sdf, SomethingElse.jpg
Ok, I'm too lazy to write the psuedo sql, but how about a cte over the smaller table with a group by on email and a count Select from the second table with the same grouping and count columns and a not in where clause to the CTE on count and email address. This will give you a list of email addresses in the larger table where the count of records is not the same between tables. If you want more information to reconcile you could Union the result with a select over the CTE where email is in the dataset returned by the initial selection. Order by e-mail, add a constant as a table identifier and you've got a side by side comparison of the record counts of distinct e-mail addresses in the two tables.
Did you ever know that you're my hero? Makes perfect sense, thanks. 
SQL is not related to platform.
In this instance, they can serve the same purpose - its just preference. I am just used to the CTE syntax anymore and I find it easier to read. 
You might think it was too easy, but it lead me to the solution so thanks for the thought, haha - posting it in OP.
Glad to help!
Well, for a university assignment, we had to calculate how many days it would be till the next Thursday, given any possible date. My program would always bug out before 1753. I'm surprised, even my professor hadn't realised it!
For the first one, you could do this: select s.SubjectID, SubjectName, count(TopicID) from Subject s, TopicSubject ts where s.SubjectID = ts.SubjectID group by s.SubjectID, SubjectName This makes the assumption every Subject has at least one topic ID. Alternatively, this problem can also be solved by using a left outer join but I thought I would give you the simpler solution first. This query connects two tables on a matching field (SubjectID) and displays the subject name, and the count of topic IDs, grouped by SubjectID and SubjectName. 
Check this out: http://ss64.com/ora/syntax-fmt.html
np. Wasn't clear if you were asking to change a date from 1/1/2001 to 1/2/2001 or derive other information from the column, like which day of the week etc. 
What database are you using? You may be able to use some non-standard features to make this easier (In Oracle, LAG and LEAD come to mind). Other than that you're looking at a case statement, and possibly some really ugly subqueries to generate what you want.
use with for a recurisve function (t-sql-) http://stackoverflow.com/questions/239275/how-do-i-create-a-recursive-query-in-mssql-2005
Create a table with 4-5 columns using only SQL (W3 Schools is a good site for basic syntax) Create another ensuring one of the columns match Insert data into both tables again only using code Select data from one table, add a where clause, then add a group and a sum/count Add a having clause after grouped Write a basic select then do an inner join on your second table repeat above steps Repeat above with a left join Do all of this a few times until you don't need the reference material Then start to read about primary keys and identity columns, surrogate keys and try to employ some referential integrity This is still just basic stuff though and any well designed test will weed out the newb. Your best bet is to be honest in the face to face interview, be enthusiastic and willing to learn I would employ a eager newbie would can have a conversation with another human being rather than a jaded neckbeard who can only speak via irc. 
"ACID? Back when I started we only had AD, and we liked it just fine. And your joins are all wrong. And everything should be indented properly. That isn't ANSI compliant. whargle bargle"
Try to google SQL certification test samples, see how you feel about those. I would also *heavily* recommend reading through everything at w3school.com and doing all the demos. It's very SQL but it will help jog your memory and does a great job if (re)teaching concepts. http://www.w3schools.com/sql/default.asp 
Lots of the MSSQL jobs I've seen lately have some SSIS involved. I don't have any resources off the top of my head, but it might be worth googling and having an idea on some SSIS basics.
&gt; If this is confusing, let me know, and I will explain better A few questions: * Does the homework bonus apply to all tests for that concept, or just to the homework leading up to the test? * If a student gets a 4 but only got a 3 the prior week, then they could possibly get a 5 if they did score two 4s before that? eg. 4 4 3 4 * If a student gets a 3 but got 4s in prior weeks, eg. 4 4 4 3, what do they get?
Yeah -- I posted it here because it is really more of a SQL question -- I don't really use the wizards or design view for my queries, just SQL. Most of the RL people I know that use MS Access do not use SQL.
Since you are saying specific product .. you better Download MSSQL express version from microsoft if you don't have one. ( it's free. ) They usually comes with sample database ( with several tables with enough data to do exercises. ) If I have to study for MSSQL assessment test .. I'll do the following. And I did get the job answering following questions. (Not all in exact code, some are answered in Pseudo but my company accepted and got the job .. analyst programmer using MS-SQL. ) 1. Do simple queries, with conditions, single table, two tables equi-join 2. Use multi tables with different types of joins ( Inner/outer/left/right) 3. Use Case and Complex conditions : exists/not exists/in/not in 4. Query with sub queries, self join. 5. Group by / Having 6. Cross-tab and Pivot 7. Bulk-inserting, Temp#Table 8. Cursor Usage 9. Basic DDL ( Create, Update, Delete Tables ) 10. Indexing, partitioning, database maintenance (Knowledge) 11. Backup/Restore/Monitoring/Logging and Auditing (Knowledge) 12. Views/Stored Procedure (Knowledge) Since you are going for BI Analyst, you should have sound knowledge about SSIS related things like Data Warehousing Design, Reporting service, Schema Design, various OLAP, Cube Architecture, MDX, ETL, SSIS deployment and configuration. ( Because these are the things they are going to ask the BI guys ... not in detail but asking very wide topics to know the candidate. )
If you can work with .csv files you should be able to work with Jet Database Engine (.mdb / previously part of MDAC and known as Microsoft Access Database). If you are not unable to install anything .. you should try this too. JET is now part of Windows so no separate installation required. You can create/add/update/delete database,tables and views during the runtime. You can easily run most of the standard ANSI-SQL queries on JET DBs. You can have password protected these JET DBs (.mdf) .. so nobody else can open these files without knowing the password. etc etc. And you can migrate those data from mdbs with your main Database server using several data sync tools.
I agree with you. I'm the mod over there so I like to advertise whenever it is remotely appropriate. 
Ugh... still trying to figure out how to identify two 4s in a row. I keep coming back to having to using a custom VBA function and iterating through a recordset using a variable for previous UnitConceptID.
One question I've asked and had asked of me in interviews is the behavior of NULL values. Stuff like: Given this statement and tables, what is the output if X is NULL?
If your only SQL experience was in a class 5 years ago and you can't do basic SQL without using a query builder tool you are probably not well qualified to be a BI Analyst. Do you know anything about cubes? MDX? ETL? Dimensions and Fact tables? Star and snowflake schemas? Anyway, good luck. It might work out for you if they are not too advanced and it's just doing basic reports. At a minimum, you should be able to do inner/outer joins and a proper where clause. You might want to post a brief summary of what you know about the job description.
Select salesperson, sum(case when weekid=1 then 1 else 0 end) as week1Sales, sum(case when weekid=2 then 1 else 0 end) as week2sales From sales Group by salesperson
Have to say I'm a fan of the sum(case when...) approach more often than not. It's easy code and it runs fast.
Pivot seems ideal here, since it actually just one table with two slices of data.
[Here you go](http://help.sap.com/businessobject/product_guides/CR2011/en/xi4_cr_usergde_en.pdf). Save that £20 for some Rogaine.
Before giving any advice, can you tell us the size of your databases (both mdf/ldf) and how many databases you have in total (on each server). Your server RAM .. HDD Storage Used and Free Space(for each server).
I did do that -- but that doesn't tell me if they got two fours in a row -- it just makes it slightly easier to determine if they got 2 4s.
I would be interested in seeing your VB solution -- my VB is not what it used to be, but I can work through another's person code without too much difficulty (especially if it's only 20~30 lines of code).
Okay -- took me a second to get what you were doing, but it makes sense now. I am not just looking to see if they have two 4s -- the 4s have to be consecutive. For Example: 3 4 2 4 --&gt; with homework --&gt; 3 4 3 4 (grade is a 4 since 4 is the highest score and there are no consecutive 4s) but: 0 1 4 4 (grade is a 5 since two consecutive 4s)
Well, to capture just two consecutive grades of "4" the way I would tackle the problem in MSAccess would be to create a recordset in VBA and then step through that recordset a single record at a time and compare it to the prior record, checking for consecutive 4's. (Someone more well versed in SQL might be able to do this using SQL, but I don't know how.) If you aren't familiar with VBA working with record sets they are only tricky at first, then they become easy and versatile with experience. I learned by reading the [Allen Browne](http://allenbrowne.com/ser-29.html) stuff. Also [here](http://allenbrowne.com/func-DAO.html#DAORecordsetExample).
thanks for the advice -- I will check it out and see if it's something that I can use. I wish there was some way I could just compare dates (maybe after an ascending sort or something), but I can't think of how to it atm. 
My hunch is that there is no way to do it without stepping through the recordset, (and yes, that recordset needs to be sorted in date order). That said, creating a custom VBA function to do this would only take about 20 lines of code. 
INSERT INTO xActiveUser (serial_number,x_date,settings) SELECT serial_number ,[date] ,settings FROM xTempUser A WHERE A.[date] = ( SELECT MAX([date]) FROM xTempUser B WHERE A.serial_number = B.serial_number) 
I don't think that will work as it will return the max date, but the setting it returns can be unrelated to that row
I just slapped myself in the forehead, I don't know why I never thought of that thanks.
http://sqlzoo.net/
Thanks. That's actually what I've been using. I guess I'll just stick with it. I can't say I like the setup too much. What I don't like is how they already have the code and just have you change parts here and there. I admit I haven't gone through the whole thing yet, so hopefully that part changes the further you get into the lessons.
Https://schemaverse.com/tutorial/tutorial.php (Full disclosure, it's my site)
Am I wrong if I guess that your university lies in a city where it rains a lot?
~~Nope. You would be spot on.~~ Edit: Apparently my classmates disagree on it raining a lot here and would rather describe it as a windy city.
(Edit: assuming you're in Bergen:) I TA'd the course and its companion course for three semesters, but unfortunately I haven't used relational algebra for a while. It might help, though, to consider the query like this instead: SELECT dato, SUM(pris*antall) AS total FROM produkt p JOIN ordre o ON p.varenr = o.varenr GROUP BY dato HAVING total &gt;= 10000 That's a better representation of what you're trying to achieve. When you're writing SQL, the query optimizer in most (all?) DBMSs will translate the "FROM tableA, tableB WHERE tableA.id = tableB.id" to a proper join, but for legibility it's recommended to use the "tableA JOIN tableB on tableA.id = tableB.id". Now you might see that writing the relational query with a JOIN is probably the way to go. A note on JOIN syntax in SQL: imagine if you have a query with three joins and three where clauses: SELECT a.col1, b.col2, c.col3 FROM a,b,c WHERE a.id = b.id AND b.id = c.id AND a &gt; 10000 AND b = 24 AND c = 50 SELECT a.col1, b.col2, c.col3 FROM a JOIN b ON a.id = b.id JOIN c ON b.id = c.id WHERE a &gt; 10000 AND b = 24 AND c = 50 The second way of doing it has better readability than the first, as it clearly separates the joins and the where-clauses.
I am not in Bergen, I am in Gjøvik. The subject is new here and might be a copy of the one from Bergen. &gt;The second way of doing it has better readability than the first, as it clearly separates the joins and the where-clauses. I do not agree with this part but i suspect that is because of a lack of experience on my part. I do see how that would make it easier to translate into relational-algebra though. Thank you! 
Error Code: 1064. You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'UPDATE Database_Two.Table_Two SET `ASK_IS_PROCESSED` = '0', `ANSWER_IS_' at line 6 
I know it's overkill, but I don't have the skills to pull this off from one database. If I add columns or views I'd have to change the code that runs the system and imports the data vs. just duplicate it and copy over. Unfortunately, the formatting made no difference. I get the same error as above. 
I would normally update table 1 and set copied = 'I' (or something) then use that as a filter so things aren't missed as well. The SQL he posted looked ok, hopefully he can say what happened. 
Good stuff. Inheriting objects like Albumn_Name is one of the worst, better to keep it from creeping in. Unless you're in a shop that pays for a tool with autocomplete, it'll infuriate you every time you have to type it.
&gt; 1. If you have a question asking to delete some records, ask if there is a delete flag. This can save you the embarrassing reaction later if they ask for you to restore the records. You mean it would save the interviewer from looking like an idiot for asking to delete records, instead of remove, hide, disable, or deactivate.
Is this what people are exposed to? Trick interview questions? Any half competent interviewee would walk out of there quick as they can.
In some ways, http://sqlfiddle.com is quite similar to Code Academy (their ruby/python labs, at least). There is no lesson plan built up there yet, but you might find it to be a useful playground for working through exercises, against a variety of different backends. Full disclosure - SQL Fiddle is my site.
That looks awesome, thanks! 
That's so junior.
select top 1 ...
Thanks. For some reason I was thinking the nested select wouldn't have access to the tables I called in the parent query. I'm not a smart man. std.delivery_date Became (SELECT TOP 1 std.delivery_date FROM SALES.DBO.SHIPMENT_TRACKING_DETAILS STD WHERE std.doc_no = invoices.order_no AND std.doc_type = invoices.status ORDER BY std.delivery_date) And of course I got rid of the join referencing the STD table.
;) honestly, i didn't even read your code. 
asking the mods to weigh in
http://stackoverflow.com/questions/612231/how-can-i-select-rows-with-maxcolumn-value-distinct-by-another-column-in-sql
Thanks. Yeah, I saw that when I was googling and worked with that code for awhile. It's not similar, but thanks though.
Wow, that's a clever solution - I'd never seen adding row numbers before. As I said, I do already have a solution that works and I'm just trying to see if there's a way to make it more efficient by eliminating the sub query. This looks like it's still doing a sub query so I don't know that it's going to be any lighter... but if nothing else, I learned a new technique so thanks. And bob is, in fact, now my uncle.
I wasn't sure if you needed to order it to keep the sequence from starting over because of non contiguous cust_ids. I figured I'd throw it in just to be safe, and now that you bring it up I believe it would throw an error about ORDER BY inside of a derived table.
Thanks. By "more efficient", I was really trying to say, "The Right Way". I didn't want to be doing this with some unnecessarily complex mess of a query if there was a standard, simple and elegant technique that I should be using for this. I'm beginning to suspect that it requires a sub query.
I would definitely go your route, hands down, but as far as the logic about the sub query...Sometimes the execution plan produced is much smarter than the query the user wrote. :)
Look into using [Access pass-through queries](http://support.microsoft.com/kb/303968) and make the server do the heavy lifting of your query. /r/MSAccess
use split http://blog.logiclabz.com/sql-server/split-function-in-sql-server-to-break-comma-separated-strings-into-table.aspx
Oh shit, I hope that's not what he's asking. There is something terribly wrong with his schema if it is.
I think you're worrying about it too much. The SQL Server stack has all of those things plus more (SSAS / OLAP / Data Warehousing). You can find a job as a specialist in one of those features, or as generalist, or some mix of both. There are jobs out there for all those things. I'd recommend starting where you're comfortable, and grow from there. Pick up good TSQL syntax, learn about the internals- how does the query analyzer work? What are the various types of joins in an execution plan? What do they mean? What are the various schema objects? Understand some rough best practices in database design. Know the key differences between OTLP and OLAP, and how to properly build both. That will give you a good core understanding that will benefit you no matter what you end up doing or specializing in. From there, never stop learning. I started with SQL writing reports and building queries. My next job I was a Jr DBA. I've gone to every SQL Saturday near me, I've attended local PASS events every time I can (and make it a priority to do so). Now I'm comfortable in the complete stack. I still don't know it all, or even close to it. I still check out every lecture I can. It's a never ending job. Embrace the challenge, never stop learning and don't sweat not being an expert in everything overnight. If it was easy to do, it wouldn't be so damn cool! Good luck!
My experience has been to sort of bounce around amongst whatever needs doing. I did a lot of contracting and this is probably why. For Homework, just do a search in DICE or craigslist and make a list of different job title positions in your home town [e.g.](http://seeker.dice.com/jobsearch/servlet/JobSearch?op=300&amp;N=0&amp;Hf=0&amp;NUM_PER_PAGE=30&amp;Ntk=JobSearchRanking&amp;Ntx=mode+matchall&amp;AREA_CODES=&amp;AC_COUNTRY=1525&amp;QUICK=1&amp;ZIPCODE=&amp;RADIUS=64.37376&amp;ZC_COUNTRY=0&amp;COUNTRY=1525&amp;STAT_PROV=0&amp;METRO_AREA=33.78715899%2C-84.39164034&amp;TRAVEL=0&amp;TAXTERM=0&amp;SORTSPEC=0&amp;FRMT=0&amp;DAYSBACK=30&amp;LOCATION_OPTION=2&amp;FREE_TEXT=sql+server&amp;WHERE=Seattle+WA) So: OLTP jobs involve 'Pure' DBA jobs (altho the definition varies enormously - really large places might have a pure DBA role, but most small/medium places actually have you doing db dev work as well as classic DBA jobs. 'Database Developer' jobs 'Database Architect' jobs, 'Software Engineer with a Minor in databases' OLAP appears to me to be more specialized and fragmented. look at some of the [BI](http://seeker.dice.com/jobsearch/servlet/JobSearch?registerRemSw=0&amp;op=300&amp;caller=2&amp;LOCATION_OPTION=2&amp;AREA_CODES=&amp;ZIPCODE=98101&amp;RADIUS=64.37376&amp;COUNTRY=&amp;METRO_AREA=&amp;TRAVEL=0&amp;SORTSPEC=0&amp;FRMT=0&amp;DAYSBACK=30&amp;NUM_PER_PAGE=30&amp;N=0&amp;EXCLUDE_KEY1=&amp;EXCLUDE_KEY2=&amp;EXCLUDE_KEY3=&amp;EXCLUDE_KEY4=&amp;EXCLUDE_KEY5=&amp;EXCLUDE_KEY6=&amp;EXCLUDE_KEY7=&amp;EXCLUDE_TEXT1=&amp;EXCLUDE_TEXT2=&amp;EXCLUDE_TEXT3=&amp;EXCLUDE_TEXT4=&amp;EXCLUDE_TEXT5=&amp;EXCLUDE_TEXT6=&amp;EXCLUDE_TEXT7=&amp;NUM_PER_PAGE=30&amp;EXPANDED_NE=&amp;FREE_TEXT=sql+server+BI&amp;Ntx=mode+matchall&amp;SAVESEARCH=&amp;currentURL=&amp;divToShowHide=unRegisteredJobAlertSignUp&amp;unRegEmailAddress=&amp;divToShowHide=unRegisteredJobAlertSignUp2&amp;unRegEmailAddress=) jobs. Often what big companies do is have you specialize in some small part of the overall BI strategy - doing ETL for loads for example only. 
If you're looking to enter the job market from college I'd recommend a broad BI approach. Know TSQL. Plug in queries into Access, SSRS, and Crystal Reports. You should be able to install all of those programs on your local machine to play around with. They are all similar and at the core of all of them are basic TSQL queries. You should start feeling comfortable with all of them after a few hours of practice. Then start looking for jobs/keywords/titles like: Data Analyst, Business Analyst, System Analyst, SQL, SQL Server, Business Intelligence, Report Writer, Report Developer, Data Support... That will give you a big, broad range of jobs to apply for. When you get a call back, looks at the specific job requirements and use that a study guide to learn more about what they are looking for.
Perfect! Thanks to all for the help! :)
I feel like learning Java, C#, and Android to hedge myself in the job market. At least be able to say, I can make beyond your basic programs with them. Im already "deep" in the Android game, and C# is an option, but a cisco networking class is also....an option. I am still not sure what field to go in to......theres mobile dev, desktop dev, IT infrastructure, databases, data mining. I just need to start figuring out my niche or I am going to be a college grad with a little knowledge in a ton of things. Business intelligence definitely....has the 2 buzz words that keep me interested. But it just seems to be a broad topic about IT solutions increasing ROE and other metrics. 
All those and more... And don't confuse DBA(Admin) with DBA(Analyst). ;) I've only been in this a couple years, and I do a mixture of ETL (though no one here calls it that) and BI stuff (which is called DB Analysis here). Just be flexible, know that you don't know everything yet (or ever), and be ready to learn.
What this guy says. As MS packs in more "features" to SQL Server, the less specific you can be. My hope is that someday, things like Reporting Services, SSIS and Service Broker will become their own packages and removed from the stack. I can't help wonder if it's a case of if we can't market the base product better, can we add more to it?
The post was a [link](http://www.reddit.com/r/SQLServer/comments/10evxv/sysadmin_by_day_needing_help_with_a_dba_problem/). 
In my experience, here is what I have seen: Analyst ($75k) - Their pupose is to read reports and interpret data for business. Their most common tool is excel. These people have varying non-technical degrees and are usually fresh college grads. DBAs ($100k) - Their purpose is to maintain database software engines and hardware. Good DBAs are hard to find. Most DBAs I come across are self taught. BI Report Developer ($100k) - Their pupose is to work with BI Reporting tools and create reports. Some common tools are Microstrategy, Tableau &amp; Cognos. Microstrategy &amp; Cognos consultants are highly sought after. Data Modeler ($125k) - Their pupose is to build data models. Usually this is a senior position. Common tools are Visio. Data Scientist ($125k) - Their pupose is to do complex analysis. These people usually have advanced degrees in statistics. ETL Developer ($125k) - Their purpose is to build tools to load data and build tables to model data. Some common tools are Pentaho, Informatica and lots of scripting. Data Architect ($150k) - Usually a company will have one Data Architect that has a lot of experience. This person make high level architectual decision. Then there is the career path of consultants. Anyone with 10+ years of experience with a specific technology like Oracle, Cognos &amp; Informatica can command huge salaried. I know one older lady that bills out for $300/hr doing Oracle stuff...no idea what she does...just that she has tons of certifications. 
What is the point of the distinct categories table? You can pull a distinct list from the actual categories table whenever you want, and not have to maintain a separate table which sounds like a nightmare. Just do this, put it into a view, and join to it whenever yow want. (Not sure if you want the max check or what you would possibly want from that column) Select name, max(check) check From Categories Group By name 
Is the max(dateactive) returning more than 1 row? Not sure how grouping twice is helping you....
Youre right, that extra grouping doesn't do anything. I think Hypno11 gave me the solution almost a year ago! I think I need to set date active= to a subquery that returns max date active and then join that on inviid. 
Legrow has something that might work, and another Oracle tool that might do it for you is Keep Dense_Rank. select irp.inviid, min(irp.price) keep (dense_rank first order by irp.dateactive desc) from invrentalprice irp left join invmaster im on (irp.inviid=im.inviid) where im.maingroup ='AUDIO' group by irp.inviid ; Let us know how you work it out!
You should use normalization
Hey Friend! It works! I used your link to read up on analytic functions, and found it best to put the function into the select statement. After a little bug checking, the result matched my expectations! Such a relief, this logic problem was starting to invade my dreams!
Hi again Pythor! I've been playing with your code. I initially removed the min(irp.price) thinking I didnt care about prices, then I realized its needed for the dense_rank function! I'm going to continue tinkering, just wanted to say thanks before it got too late!
SOLVED: Mysql has a WHERE NOT EXISTS function. SELECT * FROM node N WHERE type='julio_group_post' AND NOT EXISTS ( SELECT * FROM field_data_body F WHERE F.entity_id = N.nid ) 
Lol, sorry, I did want the most recent price, the code just threw me for a loop at first. I first thought it was going to pull the lowest price, then I read more about how the function works. Thanks again. Now I just have to figure out when it's best to use dense rank vs rownum going forward.
Fully qualify your table names so you don't have to switch context.
You can use a variable for the database name, and use it in dynamic SQL.
&gt;dynamic SQL Ewww. If I'm going to build strings of SQL, I'm generally averse to doing it in SQL.
Here is the way I always do this. I assume you want to pass a variable database name in like in a stored procedure or something. Pretty much declare a string for the database name, and another string to be your SQL statement. Then construct your SQL statement as a string with the database name variable concatenated where needed. Then do EXEC @SQLString. That's kinda a high level description but you should be able to figure it out. Just message me if you need more help. 
SELECT C1 FROM &lt;table&gt; c1Table WHERE NOT EXISTS (SELECT NULL FROM &lt;table&gt; c1Table_2 WHERE c1Table.C1 = c1Table_2.C1 AND c1Table.C2 = 1) 
[UPDATE] the query just finished (after 44 mins) with the following errors: OLE DB provider "SQLNCLI10" for linked server "redacted" returned message "Protocol error in TDS stream". OLE DB provider "SQLNCLI10" for linked server "redacted" returned message "Query timeout expired". Msg 7330, Level 16, State 2, Line 127 Cannot fetch a row from OLE DB provider "SQLNCLI10" for linked server "redacted".
Hey, I think that is not a bad idea, thanks for the input. The position is for a graduate analyst and when they called me up, they asked for me to throw it some SQL buzz words (into the conversation) to check how much I know I guess... I mentioned the few JOINS, ALTER and UPDATE from the top of my head, which was enough to convince them to set me up for an interview. But what I'm trying to do till then is make an app that will show them I know more.
http://technet.microsoft.com/en-us/library/ms143251(v=sql.100).aspx It depends on the compatibility of your .rdl. Use this link to see if it address your specific issue.
Sorry I don't see anything here that is helpful, other than the backwards compatibility mode. 
 declare @dbswitch varchar(30) = 'use webdb' exec @dbswitch
A few options: - Try using OPENQUERY, not exactly based on your code but for example: SELECT lcl.column from dbo.some_local_table AS [lcl] inner join openquery('linked_server_name', 'select column from dbo.some_remote_table') AS [rmt] on lcl.column = rmt.column - Pull data from the remote server into either temp tables, table variables, or CTEs and then work from those. Index appropriately if you use temp tables or table variables - SSIS package (may be overkill based upon your needs)
Based upon the design and groupings within the report you may be able to throw everything into a rectangle and set the properties of that rectangle so that it page breaks at the end.
http://msdn.microsoft.com/en-us/library/ms165647(v=sql.90).aspx SQL Server's network access may not be enabled. 
Try messing with the Interactive Height settings int the Report properties. You might also try adding some logic in your stored procedure that numbers/groups the rows returned (adding a new column to the dataset ), then setting the page breaks on that. Perhaps use one of the SQL windowing functions like ROW_NUMBER (), RANK(), or NTILE() to assign values to the rows. For example,if you know each member of your resultset has 10 rows, make a new column that identifies the first 5 rows and second 5 rows. --&gt; NTILE(2) OVER (PARTITION BY UserName ORDER BY ItemNumber DESC). Microsoft's example of NTILE()- http://msdn.microsoft.com/en-us/library/ms175126.aspx
Select distinct object_name(id) from sys. sysindexes where rowcnt = 0 
So tiredddd
sys.sysindexes isn't guaranteed to be accurate. A better option would be to use sys.dm_db_partition_stats. No matter which way you do it, I'd avoid SELECT COUNT(*) like the plague unless you like table scans in production.
Which DB are you using? In Oracle developer, you would just right click on the column, and choose Best fit to Data. From the '...' in your screenshot, it looks like the data is there, and this is really just a display issue.
I believe the sysindexes issue is mainly to do with reading uncommitted data, but it's useful for a quick overview of how large the table is. I'm only now starting to train my fingers away from using sysindexes - I've had too many 2000 instances still hanging around until now. I absolutely agree that SELECT COUNT(*) should be avoided - production or otherwise, unless you need an exact count (which will probably be inaccurate by the time you've finished the query on a large table). It comes down to what's the most efficient way of finding out what you need to know, at the level of accuracy required. For a general overview of which tables are unused/empty/close to empty (the case in the original blog post linked), there's no need to COUNT(*) - or at least you can filter out those tables with more than 1000 rows prior to the COUNT(*).
the trouble is, award winnning is not a value in the data. He wanted us create that in the query. I don't know how to seperate the data into 'yes' and 'no'
 First question: Can we find out more about your schema or query joins? ( Book, Book-Award, Award, Book-Sale, and Sale-Header all sound like different tables...) Secondary question: Is Award Winning a stored attribute or is it calculated, and if calculated, by what method? 
okay I posted the schema in the main post. 
Schema is now in the main post. AwardWinning is when the book's ISBN appears in the awards category
 That is a strange schema. Author is defined by book isbn, so an unpublished person isn't an Author, but any Author who publishes many books appears many times, and may have a different name , city, or state? Units Sold appears twice, both as a book and sale attribute? Tell your professor he's not normal. ( I'll have another look at this when I'm not on my phone, and when I can remember what myMy SQL keywords ought to be)
I don't use that, so I can't say then, but [this page might help.](http://msdn.microsoft.com/en-us/library/dd239377.aspx)
They just wanted the average of all award winning books on one line, and the average of non award winning books on the next. Ya know, ~~to see if award winning books make more money.~~ Cause school assignments say "fuck logic!" and want you to know how, not why. **Edit: tested your code, works well (without the cast)!**
Oh, maybe I should cast it differently then, It was truncating values. 
Is it like excel where you double click the horizontal line next to the NOTES header and it autofits?
Example code ` declare @Header table ( KeyID int, StartDate datetime, Qty decimal(15, 2)) Declare @Detail table (KeyID int, LineID int, ActualDate datetime, Qty decimal(15,2)) Declare @Result table (KeyID int, StartDate datetime, Qty decimal(15, 2), ActualDate datetime, Qty_Detail decimal(15, 2)) insert into @Header values (1, GETDATE() - 5, 15), (2, GETDATE() - 7, 14), (3, GETDATE() + 5, 33); insert into @Detail values (1,1, GETDATE() - 2, 14), (2,1, GETDATE() - 6, 13), (3,1, GETDATE(), 32), (1,2, GETDATE() + 2, 1), (2,2, GETDATE() + 6, 1), (3,2, GETDATE() + 7, 1); declare @KeyID int declare @StartDate datetime declare @Qty int declare @ActualDate datetime declare @Qty_Detail decimal(15, 2) Declare C1 cursor for Select * from @Header; Declare C2 cursor for Select ActualDate , Qty from @Detail Where KeyID = @KeyID; Open C1; Fetch C1 into @KeyID, @StartDate, @QTy While @@FETCH_STATUS = 0 Begin IF CURSOR_STATUS('global','C2') not in (-1, -2) Close C2; Open C2 fetch C2 into @ActualDate, @Qty_Detail While @@FETCH_STATUS = 0 Begin insert into @Result values (@KeyID, @StartDate, @QTy, @ActualDate, @Qty_Detail) fetch C2 into @ActualDate, @Qty_Detail End Fetch C1 into @KeyID, @StartDate, @QTy End Close C2 deallocate C2; close C1; deallocate C1; Select * from @Result` 
You're doing what ever it is you're doing wrong, what are you trying to accomplish?
Yeah, that's what I ended up resorting to. I guess I'll just do that every single time I open SSMS. Thanks a bunch.
IMHO, no one should be writing new code using cursors. Look into Common Table Expressions.
Never ever never use cursors
What do you hope to accomplish?
There's two odbc wizards. The 32 bit one is located at c:\windows\SysWOW64\odbcad32.exe, try creating it there. That, or the account was entered incorrectly, should be domain\user
Yeah, the third party programme mandates the use of that particlar ODBC wizard. Thanks for the idea, though. Oh, the \\ is an escape character, so the user is set up as [Domain]\\[Sys-Account-Name], edited post. Thanks again
It seems like you are making it a bit more complicated than it needs to be. I'm not on a computer where I can run the code you posted but unless i'm missing something, it looks like everything can be accomplished with a single join instead of nested cursors. Something like this: SELECT h.keyid, h.startdate, h.qty, d.actualdate, d.qty as qty_detail from @header h inner join @detail d on h.keyid = d.keyid Like I said, I'm not on my computer to test this so it might need some tweaking, but I think it will produce the same results. While your code is valid SQL syntax, it really isn't what people typically expect to see. If possible, cursors should generally be avoided, especially nested like that where the inner cursor is repeatedly allocated and deallocated. I write SQL code daily and i never use cursors. There is almost always a better way to do it. Your code is also mostly procedural where SQL runs most efficient when you write set-based code. Just tell SQL what your after in a single statement and let the optimizer do its job of figuring out how to do it efficiently. Anyway, I assume that the code you posted was a simplified or fake example, but the code I posted should do the same thing. Hopefully you can apply it to your real code and get a much more efficient query running.
Idiomatic doesn't mean it's doable in a language. Idiomatic means it's the natural way to express something in that language. SQL derives its power from using set based logic. Idioms matter. You need to learn how other people express things and conform to that manner otherwise your code will be unreadable to many people unless they have the time to struggle through it. Think of it as you learning how to speak swahili and those who speak it fluently have to try to piece together what you're trying to say when your grammar is all mangled. Both you and the native speakers will eventually get there but it could be very exhausting.
OK - that's what I suspected, although I still don't think you need to rely on cursors and procedural code. It's tough to help you out without seeing what you are really dealing with or what you are actually trying to accomplish as an end result, though. To deal with running totals you can use a quirky update. In the code below I'm assuming that you are looking to keep a running total grouped within each keyid, starting a fresh total for each keyid group, and ordered by actualdate within each keyid group. I'm sure the other complexities of you query can be worked out as well, but without knowing more details I can't really give you any advice. Hopefully you find this useful: DECLARE @Header TABLE ( KeyID INT , StartDate DATETIME , Qty DECIMAL(15, 2) ) DECLARE @Detail TABLE ( KeyID INT , LineID INT , ActualDate DATETIME , Qty DECIMAL(15, 2) ) DECLARE @Result TABLE ( KeyID INT , StartDate DATETIME , Qty DECIMAL(15, 2) , ActualDate DATETIME , Qty_Detail DECIMAL(15, 2) PRIMARY KEY ( keyid, ActualDate ) ) DECLARE @qty_detail DECIMAL(15, 2) = 0 DECLARE @KeyId INT = 0 INSERT INTO @Header VALUES ( 1, GETDATE() - 5, 15 ), ( 2, GETDATE() - 7, 14 ), ( 3, GETDATE() + 5, 33 ); INSERT INTO @Detail VALUES ( 1, 1, GETDATE() - 2, 14 ), ( 2, 1, GETDATE() - 6, 13 ), ( 3, 1, GETDATE(), 32 ), ( 1, 2, GETDATE() + 2, 1 ), ( 2, 2, GETDATE() + 6, 1 ), ( 3, 2, GETDATE() + 7, 1 ); INSERT INTO @Result ( KeyID , StartDate , Qty , ActualDate , Qty_Detail ) SELECT h.keyid , h.startdate , h.qty , d.actualdate , d.qty AS qty_detail FROM @header h INNER JOIN @detail d ON h.keyid = d.keyid UPDATE @Result SET @qty_detail = qty_detail = CASE WHEN @Keyid = keyid THEN @qty_detail + qty_detail ELSE qty_detail END , @keyid = KeyID OUTPUT INSERTED.* 
The application - is it a web app? If so, then make sure that the application pool it's using has "Enable 32-bit Applications" set to TRUE. Also, make sure that the app works with the SQL Native Client driver. It may be, if it's a legacy app, that you need to use the SQL Server driver instead.
yeah, this is always what I do...hasn't been an issue really yet.
Your question isn't completely clear, but if you want to use tables from different databases that are on the same SQL server instance you can prefix the database name. For example. I have two databases: DB1, DB2 Select * from DB1..tableA Select * from DB2..tableB More useful can be if you want to combine info from both. Select DB1..tableA.Col1, DB2..tableB.Col2 from DB1..tableA JOIN DB2..tableB on DB1..tableA.key1 = DB2..tableB.fkey1 or with aliases (you said you are new so I wrote it out in full first) Select A.Col1, B.Col2 from DB1..tableA A JOIN DB2..tableB B on B.key1 = A.fkey1 
Simplest way: select * from table@db1 minus select * from table@db2; That will show you anything on table@db1 that doesn't exist on table@db2. If the data is being loaded via a date clause and you have a primary or unique key, you could do something like: select * From table@db1 t1 where loaddate = &lt;date&gt; and not exists (select 1 from table@db2 t2 where t1.key = t2.key and ...)
This may be because I'm inexperienced, but I have no idea what you just said.
He'll want to create a view to simulate denormalize data in the normalized database. Or use a fancier query in the set comparisons. 
Well, I know it is not solely a sql related question but there is definitely a sql aspect to it. I am concerned about what I could put in the header within sql that would be update-able by whatever process deploys it. I am not sure if there is a bind variable or something like that I could use. I don't think an additional parameter on every sproc would be legit because how would I display it in the commented header? Anyway, once I get the sql side pinned down, I can investigate the hudson side. I will take your advice and hit up stackoverflow. Initial searches there have proven fruitless. Thanks! 
I already tried to get the requirements changed, it's not a possibility
I'm fairly positive there's no way to have the columns fit the size of the data by default. You usually have to double click the little dashes in between the column headers to have them auto-fit one at a time.
Dynamically as in they generated the Pivt SQL script on the fly?
Well this was a nice challenge. The link you provided was very helpful. For my solution I needed one extra column on your table, which is QuestionNumber. If your table doesn't have this, you could add a question number with row_number() This solution meets your criteria of being dynamic and it will support up to 999 questions in it's current form. You do not need to know the number of questions in advance or modify the sql when the number of questions changes. Shameless Self-Promotion: I am a consultant, next availability Jan, 2013 In my solution I needed to pivot the table 3 times, and join them together. Code is below. -- Create a table similar to yours, but with a question number column create table #scores ( id int, name varchar(50), questionnumber int, question varchar(50), answer varchar(50), score int ) -- Populate some dummy data insert into #scores select 1,'bob',1,'Who1','ba1',5 union all select 1,'bob',2,'What2','ba2',10 union all select 1,'bob',3,'When3','ba3',15 union all select 2,'mary',1,'Who1','ma1',5 union all select 2,'mary',2,'What2','ma2',10 union all select 2,'mary',3,'When3','ma3',15 union all select 2,'mary',4,'How4','ma4',20 -- Example of what I want the sql to look like (for 3 questions) /* select scores.id,scores.name, questions.question001 as question1,answers.answer001 as answer1,scores.score001 as score1,questions.question002 as question2,answers.answer002 as answer2,scores.score002 as score2,questions.question003 as question3,answers.answer003 as answer3,scores.score003 as score3 from (select id,question,'question'+right('000' + ltrim(str(questionnumber)),3) as qn from #scores) S Pivot ( max(question) for qn in ([question001],[question002],[question003]) )as questions join (select id,'answer'+right('000' + ltrim(str(questionnumber)),3) as qn,answer from #scores) S Pivot ( max(answer) for qn in ([answer001],[answer002],[answer003]) )as answers on (answers.id = questions.id) Join (select id,name,'score' + right('000' + ltrim(str(questionnumber)),3) as qn,score from #scores) S Pivot ( max(score) for qn in ([score001],[score002],[score003]) ) as scores on (scores.id = answers.id) */ -- Make the select statement columns DECLARE @SelectCols NVARCHAR(2000) SELECT @SelectCols = STUFF(( SELECT DISTINCT TOP 100 PERCENT ',questions.question' + right('000' + ltrim(str(questionnumber)),3) + ' as question' + ltrim(str(questionnumber)) + ',answers.answer' + right('000' + ltrim(str(questionnumber)),3) + ' as answer' + ltrim(str(questionnumber)) + ',scores.score' + right('000' + ltrim(str(questionnumber)),3) + ' as score' + ltrim(str(questionnumber)) FROM #scores AS t2 ORDER BY ',questions.question' + right('000' + ltrim(str(questionnumber)),3) + ' as question' + ltrim(str(questionnumber)) + ',answers.answer' + right('000' + ltrim(str(questionnumber)),3) + ' as answer' + ltrim(str(questionnumber)) + ',scores.score' + right('000' + ltrim(str(questionnumber)),3) + ' as score' + ltrim(str(questionnumber)) FOR XML PATH('') ), 1, 1, '') -- Make the columns for the PIVOT command DECLARE @PivotCols NVARCHAR(2000) SELECT @PivotCols = STUFF(( SELECT DISTINCT TOP 100 PERCENT ',[question' + right('000' + ltrim(str(questionnumber)),3) + ']' FROM #scores AS t2 ORDER BY ',[question' + right('000' + ltrim(str(questionnumber)),3) + ']' FOR XML PATH('') ), 1, 1, '') -- Variable declaration declare @Select as varchar(max),@From as varchar(max),@Join1 as varchar(max),@Join2 as varchar(max) -- Populate the different parts of the sql statement. select @Select = 'select scores.id,scores.name, ' + @SelectCols select @From = ' from (select id,question,''question''+right(''000'' + ltrim(str(questionnumber)),3) as qn from #scores) S Pivot ( max(question) for qn in (' + @PivotCols + ') )as questions' select @Join1 = ' join (select id,''answer''+right(''000'' + ltrim(str(questionnumber)),3) as qn,answer from #scores) S Pivot ( max(answer) for qn in (' +Replace(@PivotCols,'question','answer') + ') )as answers on (answers.id = questions.id)' Select @Join2 = ' Join (select id,name,''score'' + right(''000'' + ltrim(str(questionnumber)),3) as qn,score from #scores) S Pivot ( max(score) for qn in ('+Replace(@PivotCols,'question','score') + ') ) as scores on (scores.id = answers.id)' -- execute the statement exec (@Select + @From + @Join1 + @Join2) 
Sounds like you already are a DBA, so I wouldn't talk about getting your foot in the door, you already have. In an interview I'd talk about the DBA tasks you're already doing, and try to relate those to what your prospective employers needs for their business. If you haven't studied the theory, but already know some SQL, then learning about normal forms is probably the most important thing in terms of interview questions. I started as an Oracle DBA with no skills, no degree, the DBA team lead hadn't been able to hire a good DBA so they pulled me off the (generic IT) helpdesk as a temporary fix while they found someone who was a proper DBA. The job lasted 8 years. 
1 simmular 2 yep, but a lot of fuctionality can't be displayed visual. Learn to write sql. 3 stored procudures are saved steps, just what the name says: Random example step 1 get data from table 1 Step 2 save in temp db step 3 get data from db on other server Set 4 compare 4 Microsoft reporting service server
Can you be a bit more specific? What are you going to be doing with the data that you're replicating? Is it for disaster recovery? reporting? high availability? 
Does the front-end database do any writing? If it's a read-only, i could see you easily setting up an insert trigger that connects via odbc to the front-end database that inserts the information automatically without delay. I personally would only do that if the front-end database is read-only style, otherwise you would need to figure out some kind of ID check to make sure information doesn't exist already...etc. We have a trigger that does something similar, we also have deletion triggers that copy the information to a backup table before removing it from the main table. That has saved us quite a few times.
Sorry. That part was in your title but I completely forgot after reading your actual post. It may be cheaper to just upgrade to full SQL Standard Edition licenses compared to redeveloping or hiring developers/consultants. 
I hope not! There are over 30 servers involved here. Each one probably changes only a few hundred records a day so it's not a massive amount of data.
Just doule checked - If you have one SQL Server Std edition as the publisher your satellites can merge replicate as subscriber with Express Edition. 
Effectively making them only useful as read only, correct?
Nope. Merge is two way.
In our company, we have two systems, one is internal where the Windows applications read and write to the database. The other is external where customers can view their information and requests. We have triggers on the internal database to replicates the information instantly to the external database without the need of replication process..etc. Because the external is nothing but read, we have no need to worry about duplication as the data is going one way. Could we use replication, yes but as of right now Amazon's RDS service does not support MSSQL replication, or at least that is my understanding.
That's exactly how many of our databases work. No trusts needed. 
I'll Start Old **Job Title:** SQL Developer **Experience:** &lt; 2 years **Salary:** 57K **Location:** Westchester County, NY Current: **Job Title:** SQL Developer **Salary:** 70K + bonus 
Job Title - Problem Solver with a bit of data extraction and analysis when time permits (actually it's the biggest part of my job now, but it didn't start that way) Experience - heaps but not a great deal in software development or cutting code, just fell into it and loved it Salary - 180k + house + car Location - AUD Message - Right Place, Right Time and living the dream !
You don't update primary keys, you create an orderby column which is an integer, and you instead update that.
As a rule thumb, always leave a surrogate key, like your id column. Surrogate keys are supposed to be meaningless. But if you ever find yourself with something like a duplicate row, you'll be glad you had it.
You always need a primary key, but you may not always need it to be an integer. A primary key is a clustered index, which means the data is stored on disk using that key as a reference. With out a primary key, you would have a heap table, which is slow. However, if the table is small, this may not be an issue. If you are going to be joining on any of these columns, and none of them (except sort order) may not be unique, then you need a surrogate primary key, ie: a primary int. Don't use datetime's as primary keys, because dates are finite and also large. The same thing goes with GUIDS.
Really its unnecessary to have a unique constrain on priority. Where is why. It makes switching priorities take an extra step because you have to clear one value before you can set it to a value used in a different row. You should never be referring to a row by its priority number, you refer to it by its id. So when you update a row, its by the id, not the order. Really the only thing priority is used for is sorting your list. 
Job Title: Tech Support/Jr DBA: Daily procedure processing, nightly automation monitoring, data manipulation, create stored procedures, and other IT support tasks. Experience: ~2 years Salary: 32k no beneifits Location: Greater Boston Area (MA) This is a new part of my career at the moment so I am honestly making now money. I am sticking in there so that these extra couple years following will give me some good job opportunities. 
Make a priority column (leave the PK or don't, I don't care). Order by priority. When you want to "reshuffle" just find the minimum value and subtract 1 off of it and set that as the new priority. Edit: Don't forget negative values work really well when ascending an Order.
Didn't even see the AUD, thank you.
Try again, it looks like the cut and paste made a difference between the select and order by clause in the "make select statement columns" section. I've fixed it now and it should work. Let me know.
SQL went alright. Some interesting/clever questions and the dude asking me was helpful/guiding me to the correct answer. He seemed to be looking more for a understanding of basics instead of actual ability to write a complex query from scratch and he appeared to be pleased by the end of our hour. Other aspects of the 5h marathon interview weren't handled the best by me, but I wouldn't be surprised one way or another.
Try this, works on my side: SELECT PUBLISHER, DISPLAY_NAME, COUNT(DISPLAY_NAME) as Deployment_Count FROM SOFTWARE S WHERE NOT IS_PATCH AND S.DISPLAY_NAME rlike 'Office Professional|Office Project|Office Visio' GROUP BY DISPLAY_NAME ORDER BY DISPLAY_NAME Also, if you need a SQL primer for the KBOX, try this: http://www.itninja.com/blog/view/primer-for-writing-select-statement-queries-on-the-k1000-w-custom-sql-report-example Any questions, just let me know. ^_^ John (jverbosk on ITNinja)
Knowing how to think/reach a solution is very important. We check for that during our interview process as well. We know no one will remember all syntax, etc... In any case, good luck!
you got it... ;)
I'm not getting an error in it's current state. The table gets built, it is just missing the unitcost column. The "Open PO Report" table is getting built with the follow columns... **Category, PO #, SubCategory, Mast PO, Type, Hold?, PO Date, ETA, Vendor, Whse, Item #, Description, Brand, Customer, U/M, Cases, Lbs/Kgs, Received, Back Ordered** I'm trying to add the PO2_PurchaseOrderEntryLine.UnitCost (cost) column added at the end. 
Is Open PO Report an existing table? Use [DATABASE NAME HERE] GO ALTER TABLE [Open PO Report] ADD Cost money I don't see why that wouldn't work.
If he's using a Select ... Into statement, there is no way it is an existing table.
Dude, please make a new table that has product line code and product line name. Then you can join to it to grab the product line instead of using that ridiculous case statement in every single query... Edit: also, table aliases are your friend
 SELECT CASE PO2_PurchaseOrderEntryLine.ProductLine WHEN '0001' THEN 'LOBSTER' WHEN '0002' THEN 'LOBSTER' WHEN '0003' THEN 'LOBSTER' WHEN '0004' THEN 'LOBSTER' WHEN '0005' THEN 'LOBSTER' WHEN '0006' THEN 'LOBSTER' WHEN '0007' THEN 'LOBSTER' WHEN '0008' THEN 'LOBSTER' WHEN '0009' THEN 'LOBSTER' WHEN '0010' THEN 'LOBSTER' WHEN '0011' THEN 'LOBSTER' WHEN '0014' THEN 'LOBSTER' WHEN '0015' THEN 'LOBSTER' WHEN '0103' THEN 'CRAB' WHEN '0113' THEN 'CRAB' WHEN '0105' THEN 'CRAB' WHEN '0114' THEN 'CRAB' WHEN '0120' THEN 'CRAB' WHEN '0100' THEN 'CRAB' WHEN '0101' THEN 'CRAB' WHEN '0108' THEN 'CRAB' WHEN '0110' THEN 'CRAB' WHEN '0111' THEN 'CRAB' WHEN '0118' THEN 'CRAB' WHEN '0122' THEN 'SHRIMP' WHEN '0126' THEN 'SHRIMP' WHEN '0127' THEN 'KING CRAB' WHEN '0128' THEN 'ARGENTINE CRAB' WHEN '0130' THEN 'PACKAGING' ELSE 'OTHER' END AS Category, PO1_PurchaseOrderEntryHeader.PurchaseOrderNumber AS [PO #], ci_udt_subcategory.UDF_SUBCATEGORY_DES AS SubCategory, PO1_PurchaseOrderEntryHeader.MasterRepeatingPOOrSO AS [Mast PO], PO1_PurchaseOrderEntryHeader.OrderType AS Type, PO1_PurchaseOrderEntryHeader.OnHold AS [Hold?], PO1_PurchaseOrderEntryHeader.PurchaseOrderDate AS [PO Date], PO1_PurchaseOrderEntryHeader.RequiredExpireDate AS ETA, PO1_PurchaseOrderEntryHeader.VendorName AS Vendor, PO2_PurchaseOrderEntryLine.WarehouseCode AS Whse, PO2_PurchaseOrderEntryLine.ItemNumber AS [Item #], PO2_PurchaseOrderEntryLine.Description, PO_95_UDF_PurchaseOrderLines.Pbrand AS Brand, PO_95_UDF_PurchaseOrderLines.Alloc_Cust AS Customer, PO2_PurchaseOrderEntryLine.UMConversionFactor AS [U/M], PO2_PurchaseOrderEntryLine.QtyOrdered AS Cases, PO2_PurchaseOrderEntryLine.UMConversionFactor * PO2_PurchaseOrderEntryLine.QtyOrdered AS [Lbs/Kgs], PO2_PurchaseOrderEntryLine.QtyReceived AS Received, PO2_PurchaseOrderEntryLine.QtyBckordrd AS [Back Ordered], CONVERT(money, PO2_PurchaseOrderEntryLine.UnitCost) AS Cost, CONVERT(money, PO2_PurchaseOrderEntryLine.UnitCost / PO2_PurchaseOrderEntryLine.UMConversionFactor) AS [Cost/Unit], CONVERT(money, PO2_PurchaseOrderEntryLine.OrderExtension) AS Extended , CONVERT(money, PO2_PurchaseOrderEntryLine.UnitCost) as UnitCost --&lt;this is the only change made. 
That actually executes successfully, however the last column is still **Back Ordered**. For some reason everything after that (**Cost, [Cost/Unit], Extended**) doesn't show up. 
Highlight the last line in the Select statement, Copy it, paste it just after the &gt;UDF_SUBCATEGORY_DES AS SubCategory, bit. Also delete the leading comma and put in a comma following 'UnitCost' and tell me what you see. 
If you want all titles that haven't had any sales, wouldn't it be NOT EXISTS? With EXISTS in your subquery, the exists is all books which have been sold.
What you currently have is: if your exist sub-statement comes back with any rows at all, then the where clause will be true for all rows of your main statement. This means that you will either get the full row count of you main statement or no rows at all. Without writing it fully out for you, what you want to do is either a "left join" with a "field is null" statement in the where clause, or you just a "field not in (sub-select)" in the where clause. You could also write it using a "not exists" statement. Something like "not exists (select sales c where c.titleid = b.titleid) This puts a reference from the title field of your main statement into the subselect.
 USE pubs SELECT a.title AS 'Book Title' FROM titles a LEFT JOIN sales b ON a.title_id = b.title_id WHERE NOT EXISTS (SELECT * FROM sales WHERE a.title_id = b.title_id)
Lets eliminate the table by comment out the into clause and re-runing. Does the cost column exist in the result set?
&gt;select 1 from titles a where not exists (select 'x' from sales b where a.title_id = b.title_id); Using SELECT * is not good, ever.
When using EXISTS, SELECT 1 is ~~much more efficient~~ a much better habit than using SELECT *, ~~as you're not interested in the data, just if it's actually there.~~ as using asterix's in queries is a potentially bad habit to get into. ~~http://www.sqlservercentral.com/articles/Performance+Tuning/67427/~~
That is debateable. Your source does not reference why select 1 is better. It is pretty much [equal](http://blog.sqlauthority.com/2008/02/26/sql-server-select-1-vs-select-an-interesting-observation/) in most scenarios that I have ever worked with or seen. Especially if the index has already been cached. Also, it is convention when working with MSSQL to use * in EXISTS subqueries. &gt;The select list of a subquery introduced with EXISTS, by convention, has an asterisk (*) instead of a single column name. The rules for a subquery introduced with EXISTS are the same as those for a standard select list, because a subquery introduced with EXISTS creates an existence test and returns TRUE or FALSE, instead of data. http://msdn.microsoft.com/en-us/library/ms189543%28v=sql.105%29.aspx So until you can show me why SELECT 1 is better, then I will stand with SELECT * in an EXISTS statement.
I cannot argue with the MSDN article you cite. However I will stand by the fact that it's not good to use an asterix in any query as it can lead to bad habits. The junior/entry level SQL developers I have trained up were previously using SELECT * in queries because they didn't know better (this stands for people using WMI queries in CIM also) when they only wanted data from one field. Getting them out of that habit is always the first job at hand. Edit: context.
I have no argument there. You are correct, good sir.
Job Title: senior analyst (actual: database developer) Experience: no high school or college / 3 years low level reporting and analysis (Excel, Access, Crystal Reports, and a little SQL) Salary: 56k / medical, dental, vision, 401k w/ up to 4% match, stock purchase plan, free telecom services Location: Michigan, USA
Is there typically any performance improvements from choosing one over the other in your experience? Couldn't you also just do a LEFT OUTER JOIN and no where clause at all? SELECT a.title AS [Book Title] FROM titles a LEFT OUTER JOIN sales b ON a.title_id = b.title_id 
Wouldn't 'select 1 specificIndexedField' be faster than the asterisk because of indexing?
isn't that equivalent to where b.title_id is null?
Here's a function someone wrote to do it. http://weblogs.sqlteam.com/jeffs/archive/2007/03/09/60131.aspx
Copy and paste the code into a New Query window. Execute the code to create the function. Then you can write a query using the function to adjust your addresses. SELECT ProperCase(Street) FROM Address
Bro fan. Make it a loop cause our work doesn't like us making functions. I'll show you Monday the studid work around we have to do.
Now kiss. 
Which brand of SQL are you using? Some have a built in function for this. In Oracle, INITCAP(address1) would give you what you're asking for.
Late to thread, but I'm so glad that I'm not the only one who thinks SQL writing is fun. The syntax is a little awkward, but SQL development is one of the most rewarding things about my job. There's nothing more thrilling than taking that horrible legacy database your coworkers want nothing to do with, and wrangling beautiful data out of it. I think --sniff-- [said it best](http://www.reddit.com/r/SQL/comments/xxauu/should_i_learn_sql/c5qwa6r): &gt;[SQL is fun] because you are bending data to your will.
Sad thing is everyone then converts it back (EDI) to upper case. You don't see a lot of lower cost ship labels. Hell many ERP's just throw in case insensitive collation to really throw you a bone.
use hyper-IP vpn accelerator. It's free (for veeam users too!) and basically smooths out packet loss and does some dedupe. say you get a burst of comcrap packet loss due to a retrain, rather than dropping the connection it can sorta buffer it and keep retrying. the choice of VPN underlying is equally important as some are more resilient than others to loss. I'm saying this because it is more common to see best-effort service ($189 comcrap 100/10 5 static ip) at branch office with another best-effort or simple T-1 (cbeyond) SLA service. btw, I keep asking - should microsoft be selling this? Alwayson? Cloud based replication for a monthly fee (throw in AD too). I'd buy that up in a heartbeat. 
To add, you may need to create a special exclusion list for these things
I'm currently not at my computer to test it, but something like this would have been simpler for Question 1: SELECT TOP 30 ProductName AS 'Product Name', PostalCode AS 'Postal Code', SUM(TotalSale) FROM Warehouse.dbo.orderSales WHERE ProductName IS NOT NULL and PostalCode = '44444' GROUP BY ProductName, PostalCode ORDER BY SUM(TotalSale)
[OpenEdge Architect](http://www.progress.com/en/openedge/developer-productivity/integrated-dev-environment/index.html)
 SELECT T2.PartNumber FROM TABLE2 T2 LEFT JOIN TABLE1 T1 on T2.PartNumber = T1.PartNumber WHERE T1.PartNumber IS NULL Just a quick stab, to see if I understand your post right.
thanks dude, didnt want to read /r/thewalkingdead at lunch anyways ;)
How long do your instances generally live? How many rows do you write in the that time? If the answer is anything more then a dozen or two rows you might want to consider group insert operations; not sure at what step you are inserting or how though. If it is programmatically/procedurally inserted it might be worth looking at storing output 20~ rows at a time then doing a bulk insert operation, or when an instance ends. Not sure what class is, but 'instance' sounds like it would be a good identifier, you may want to look at a clustered index for timestamp also if you find the need for it(slightly slows inserts, drastically speeds up sequential information queries). 
 Select distinct b.part_no from table2 b minus select distinct a.part_no from table1 a; This will list all the part numbers from the 2nd table, then remove any matching part numbers which exist in table 1.
rofl, tv subs need to have automatic unsubscribe as an option. I enjoy discussing it, but god it makes Mon-Tues dangerous when I miss a night of TV.
I consider myself decent with SQL and I can't see how that works...
Here's the simplest way I can think to explain it: edit: fixed link http://i.imgur.com/IPksH.png 
According to [this link](http://msdn.microsoft.com/en-us/library/ms175191.aspx#InAutoFo) ... &gt; If the witness is also disconnected from the principal server, the mirror server knows that principal database has become unavailable. In this case, the mirror server immediately initiates an automatic failover but then it goes on to say ... &gt; If the mirror server is disconnected from the witness and also from the principal server, automatic failover is not possible, regardless of the state of the principal server. So I'm pretty much as confused as you. I think this MS article has a type-o or I'm just not understanding it properly. My best guess would be if you lost both the principle and the witness at the same time there would not be an automatic fail-over because there's no majority rule in that case. If this happens, you should be able to recover the mirror manually using the following command: ALTER DATABASE [MyDB] SET PARTNER FORCE_SERVICE_ALLOW_DATA_LOSS 
A) Why do you need to log all those entries? &gt;One day that is 86~ million rows minimum, that's 7x as many rows as I have for (very) thorough logging of five thousand users in half a year. Assuming half a KB per row that is still 41GB/day. Even with the infrastructure to maintain this, eventually this table will become so unmanageable that it will have to be refactored. The old DBA/programmer adage goes "If you throw technology at optimization problems, you'll eventually run out of technology, money, or both." B) You shouldn't need to use DISTINCT on a timestamp when you have other valid identifiers. C) &gt; If the instance lasts a long time, assuming it has duplicate information about that instance that is stored in every new row. Why not just create a table for instances and just reference it as a primary key with only the additional information, would reduce storage costs dramatically and it could be much faster to search a table for instance static information without having to query the massive changes table). &gt;If you only wish to keep the most up-to-date information for each Instance, make tables for the following: * Active Instances Daily (Stored on a different storage array, along side InActive Instances) * InActive Instances (Stored on a different storage array, independent from the rest) * Active Instances Temp * Queued Instance Changes Process: * Every night during a quiet time update the 'Active Instances Temp' into the 'Active Instances Daily' table, locking the table to changes for the duration. * Upon receiving instance changes insert time order onto the 'Queued Instance Changes' table, then have a job do a update into the instanceID row of the 'Active Instances Table' from the top of the 'Queued Instance Changes' table, after x amount(adding a delete flag bit would make this easy) then delete them. Basically a cyclic update structure. * When an instance becomes InActive, push the row to the 'InActive Instances' Table and delete it from the 'Active Instances Daily' and 'Active Instances Temp' right after the first step. Then unlock the 'Active Instances Temp' table to changes and resume step two. The benefit of this style of setup are: * Data replication and safety; even a complete system failure of all primary drive arrays would set you back a maximum of a day, and could be rebuilt in a few hours instead of a few days/weeks. * Data storage requirements would be significantly reduced because changes would be queued and removed upon completion. * InActive instances rows would not be adding query time to a live database. I could be completely off base on the requirements, but it seems like there is massive room for optimization here. 
Also: SELECT PartNumber FROM TABLE2 WHERE PartNumber NOT IN (SELECT PartNumber FROM TABLE1) I prefer to use the method that you posted, however.
Yeah that's why I mentioned derived tables later on in my post - I figured it'd be good practice after he gets into joins.
There are several ways to achieve what you want which is effectively called an **anti-join**. Which returns rows from one table where no matches are found in the second table. An anti-join is essentially the opposite of a semi-join: While a semi-join returns one copy of each row in the first table for which at least one match is found, an anti-join returns one copy of each row in the first table for which no match is found. Anti-joins can be written using the NOT EXISTS, NOT IN, MINUS, LEFT OUTER JOIN with NULL check constructs. select T2.PartNumber from TABLE2 T2 where not exists (select null TABLE1 T1 where T2.PartNumber = T1.PartNumber) or... select PartNumber from TABLE2 where PartNumber not in (select PartNumber from TABLE1) or.. select PartNumber from TABLE2 minus select PartNumber from TABLE1 or.. select T2.PartNumber from TABLE2 T2 left outer join TABLE1 T1 on T2.PartNumber = T1.PartNumber where T1.PartNumber is null Query efficiency is important!!!! Example 1 and 2 : "Not exists" is generally more efficient than "not in", but "not in" can be many orders of magnitude BETTER when the subquery is known to not return nulls, is in your case. Example 3 : The minus operation requires a full table scan of both tables with a sort filter operation Example 4 : The efficiency of this isn't great as it requires a full table scan of both tables and a null filter operation. Try them yourselves and see, I expect example 2 will be your best bet.
Umbrello? 
SELECT ... ORDER BY somecolumn LIMIT 0, 10 What does the numbers mean theoretically
Yeah, without communication with either the principal and the witness, the mirror has no idea of the state of the principal. The principal may be offline, or the mirror might be the one that's dropped off the rest of the network while everything is running fine on the other side of the fault. The worst thing that could happen is for the mirror to bring itself up, and start serving requests while the principal is still operational - you'll end up with two active copies of the database, and require manual intervention to get them back in sync. So, to explain Microsoft's documentation, if the witness has detected that the principal is gone, but it can still see the mirror, then failover can occur. When the (old) principal comes back up and can contact the witness/new principal, it will realise that it's no longer the principal, and can start to receive the transaction log stream and get back in sync.
If you want to be "hard core", go with graphviz. There was also a java-based tool that was pretty good called power**something* I can't recall the exact name. However, if you're doing database modeling, I think MySQL workbench is the best tool right now. Just wish it had the ability to export to other DBs.
1st number = how many rows to skip over... 2nd number = how many rows to return
I've always looked at Clustering to be HA of the Database and Mirroring to be HA of the DATA. Subtle difference, but important. HA of the Database means it's more important that the application has the fastest access to the data and in an outage, application requests are rerouted as fast as possible to the failover solution. HA of the Data means the integrity of the data is the highest priority.
thanks
 This is correct because the second server doesn't know if it has simply lost connection to the primary and witness 
What you're asking about is called "pagination". While I'm no fan of MySQL, pagination is one thing that MySQL gets right, out of the box. It can also be done on SQL Server, but it's more complicated and takes a good deal more code.
I'm not a SQLite person, but in SQL Server the way you could do this would be to create a function that you can feed in inputs and returns a boolean. So you feed in GameId and PlayerId and inside the function you do a select statement that checks if a record exists in the table for the given gameID with a targetID equal to the playerID you passed into it. OR if no records exist in the table for that GameID at all. Then the first record you insert with validate true, and the next record you insert would validate true, and so forth. As long as your app is inserting them in order. The downside to all of that however is that means your app would have to insert 1 row at a time instead of doing batch inserts, and that's annoying.
Thanks, I got a term to google search if I need more help on it.
I understand the answer, but not the question Are you saying the tables are like.... Table1: part number, price, quantity; Table 2: Part number, price; ?
I'm a Management of Information Systems major in school, for SQL we use http://www.richardtwatson.com/dm5e/index.html. It starts you out with a information about a data manager's job, but by the third chapter your doing basic create table statements To describe my current understanding of SQL is that it is the foundation of where and how your data will be stored by. Another thing we are also doing in a co-requisite is Process Management with Bizagi using BPMN (business process modeling notation) which fun and easy to learn
Simply put, clustering is your fault tolerance (my server broke) and mirroring / log shipping are your DR (my whole data center broke). Ideally you want two sites both running clustered instances that mirror or log ship between the two.
Can you post the script?
I think you need this: SET IDENTITY_INSERT [ database_name . [ schema_name ] . ] table { ON | OFF } Check the documentation in SQL Server 2008. 
CREATE TABLE [dbo].[Address]( [CreationDate] [datetime2](7) NOT NULL, [ID] [uniqueidentifier] NOT NULL, [IsDeleted] [bit] NOT NULL, [IsPrimary] [bit] NOT NULL, [Text] [nvarchar](500) NOT NULL, [PostCode] [nvarchar](10) NOT NULL, [ImageID] [uniqueidentifier] NULL, [Latitude] [decimal](18, 8) NULL, [Longitude] [decimal](18, 8) NULL, CONSTRAINT [PK_Address] PRIMARY KEY NONCLUSTERED ( [ID] ASC )WITH (PAD_INDEX = OFF, STATISTICS_NORECOMPUTE = OFF, IGNORE_DUP_KEY = OFF, ALLOW_ROW_LOCKS = ON, ALLOW_PAGE_LOCKS = ON) ON [PRIMARY] ) ON [PRIMARY] GO ALTER TABLE [dbo].[Address] WITH NOCHECK ADD CONSTRAINT [FK_Address_Image] FOREIGN KEY([ImageID]) REFERENCES [dbo].[Image] ([ID]) GO ALTER TABLE [dbo].[Address] NOCHECK CONSTRAINT [FK_Address_Image] GO ALTER TABLE [dbo].[Address] ADD CONSTRAINT [DF_Address_CreationDate] DEFAULT (sysdatetime()) FOR [CreationDate] GO ALTER TABLE [dbo].[Address] ADD CONSTRAINT [DF_Address_IsDeleted] DEFAULT ((0)) FOR [IsDeleted] GO
Please see above
Just ignoring it and hoping SQL would set something sensible. As a datetime2(7) I thought they wouldn't clash
Yeah, I was just coming to agree with this. You can insert duplicates in to clustered indexes.
Well, remember that clustered indexes aren't about uniqueness, it's about physically ordering data, in which it would seem that it's a decent choice for your application. Technically, your primary key is determining the row's uniqueness (albeit poorly). Is there any other way of uniquely identifying a unique row aside from time? Datetime2(7) is the max time base precision. Also, I would argue that as a practice, unique indexes are iffy. If the index needs to change, the uniqueness may not need to go with it. Probably better off just having a separate unique constraint.
Yes, your clustering key must be unique. Fortunately SQL has a way to make that happen if you choose a field that is not unique. SQL will generate an internal [uniquifier](http://www.sqlskills.com/blogs/paul/post/Indexes-From-Every-Angle-What-happens-to-non-clustered-indexes-when-the-table-structure-is-changed.aspx) that makes it unique. Not optimal by any means, but should keep things running if you can't change the table structure. tl;dr Quickfix: Make your clustered index non-unique. Longer fix, change the table structure. Make the uniqueidentifier a nonclustered Primary Key. Add an integer IDENTITY field. Make the added int the cluster key. 
&gt; Even though the Part_Code 'P' does not exist the transaction will still be committed. Yes, because the `DELETE` succeeded. It just deleted 0 rows. What behavior are you looking for?
That makes sense but it still works if I misspell the name of a table. Wouldn't that mean that one of the requests couldn't be completed?
The fun part about dates, is that bulk insert does tend to take in the same date when inserting into the table. So creating a unique index on just the datetime (even with the 7 point uniqueness) it's still going to enter in the same time. I would add the ID or another identifier to make the index that's marked as unique, actually unique. Usually I just do this with an ID column, but as an int. so it would be like this: Create table &lt;table&gt; ( &lt;DateColumn&gt; datetime2(7) ,&lt;tableName&gt;Id int not null identity(1,1) ,&lt;columns&gt; ,constraint pk_&lt;table&gt;_IDDate Primary Key( &lt;DateColumn&gt; ,&lt;TableName&gt;Id ) ) on &lt;FileGroup&gt; Some liberties taken there with the psuedo code, but I hope you get the jist. 
It would fail and roll back since it caused an error
Yes, and his name was Jim Gray. 
The problem is that unique identifiers are not ordered. For example, run the following: SELECT NEWID() GO 10 You'll get 10 new unique identifier values, all randomly distributed. If this is your clustered key, then any new row inserted could be located at any random point in the table, causing more disk activity or memory usage (a random point means you're less likely to have that page in memory), and you'll experience page splitting in the middle of the indexes, leading to greater fragmentation. You can create a NEWSEQUENTIALID() default value for a unique identifier which almost eliminates this problem. The cluster key would still be 16 bytes though, which is a cost you'll have to pay for every row in every non-clustered index.
Here is an example using TRY...CATCH with error handling BEGIN TRY BEGIN TRANSACTION DELETE FROM PART WHERE Part_Code ='F' DELETE FROM PART WHERE Part_Code ='P' COMMIT TRANSACTION END TRY BEGIN CATCH ROLLBACK TRANSACTION DECLARE @ErrorMessage NVARCHAR(4000) , @ErrorSeverity INT , @ErrorState INT SELECT @ErrorMessage = 'Line ' + CONVERT(VARCHAR, ISNULL(ERROR_LINE(), 0)) + ' - ' + ISNULL(ERROR_MESSAGE(), '') , @ErrorSeverity = ISNULL(ERROR_SEVERITY(), 0) , @ErrorState = ISNULL(ERROR_STATE(), 0) RAISERROR ( @ErrorMessage , @ErrorSeverity , @ErrorState ) END CATCH 
Oh right. Sorry, I didn't know about that, I had assumed (never assume eh?) that it would be a sequential integer.
It's cool - easy to get mixed up if you haven't seen the unique identifier type before. You're thinking of an IDENTITY column, which is an integer or bigint that typically starts at 1 and automatically increments. Those are perfect for a clustered index for the reasons you said (unique, narrow, ever-increasing). 
Thanks for your reply. The primary (also unique) key for the row is the ID (uniqueidentifier). These are obviously bad for clustering on so I have a datetime2 to cluster on instead. I didn't know about the uniquifier! I suppose I could add a integer identity field to cluster on instead but that solution would lack portability. I'm going with the quick fix :P And thank you for enlightening me!
 SELECT t.uid , t.costnr , t.accnr , t.dtype FROM ( SELECT dtype , MIN(uid) AS lowest FROM yourtable GROUP BY dtype HAVING COUNT(*) &gt; 1 ) AS dupes INNER JOIN yourtable ON yourtable.dtype = dupes.dtype AND yourtable.uid &lt;&gt; dupes.lowest 
I'm not sure exactly what you want, but this following query should suffice as a basis. select UID, COSTNR, ACCNR, DTYPE from ( select t.*, row_number() over (partition by t.UID order by t.UID, t.DTYPE) as RNUM from MYTABLE t) where RNUM = 1 This is much more efficient than an aggregation approach such as using MIN, MAX. If you want say two out of the three duplicates, replace RNUM = 1 with RNUM &lt; 3
But this would select none duplicates also or not? Edited my post for an example
So DTYPE is the selection criteria, but what is your filtering requirement based on the other fields? You seem to be selecting an arbitrary row for your UID? If so, something like this... select UID from ( select t.*, row_number() over (partition by t.DTYPE order by t.DTYPE, t.UID) as RNUM from MYTABLE t) where RNUM = 1 
If you aren't worried about which UID you could use simple aggregation. select MINUID from ( select min(UID) as MINUID, DTYPE from MYTABLE group by DTYPE) 
You're welcome, hope it helped. As far as portability, you don't ever use the IDENTITY int, SQL does! To some of my larger tables I add a IDENTITY int that's literally never used in any queries. I'm never going to use it as a lookup key, I never include it in my queries. The only thing it does is provide a small unique 4 byte pointer for my non-clustered indexes. The clustering key determines the physical order on disk, which basically makes it a location pointer and extremely fast to look up on. But if you don't want to use it as a primary key, then add your primary key to the table, a GUID or whatever key you have, then create a non-clustered index on your GUID. A nonclustered index always stores the cluster key as a pointer. Now when you query on your GUID, it gets the nonclustered index record, finds the cluster key, then finds that record in the table. It's fast, very fast. All this takes place without you ever using the clustering IDENTITY int directly. If you use the non-unique datetime2 as a clustering key, then it *and* the uniquifier will be used as above. Those extra bytes add up over time because they will also be stored in every record in every nonclustered index.
"Remember, a derived table is just the result of using another SELECT statement in the FROM clause of another SELECT statement. Simply put the query in parenthesis and add a table name after the query in the parenthesis." this quote is taken from http://www.sql-server-performance.com/2002/derived-temp-tables/ i searched for quite a while before i found a link that explained it so concisely :)
You may be looking for the IN keyword. For example: SELECT [columnA], [columnB] FROM [myTable] WHERE [columnB] IN (SELECT [columnX] FROM [myOtherTable] WHERE [columnY] = 42 ) This lets you get only those rows out of [myTable] where [columnB] is equal to a value of [columnX] in a row in [myOtherTable] where [columnY] is equal to 42. Essentially, you are creating a list of values in the sub-query which is then used to filter the values in the main query. You can actually do this by hand as well for various reasons. e.g.: SELECT [columnA], [columnB] FROM [myTable] WHERE [columnB] IN (4, 8, 15, 16, 23, 42) Which will filter your results based on [columnB] being in the listed numbers.
This quick and dirty method may work: This will give you the max number of dates for any given patient/diagnosis pair Select max(count(1)) from patients group by ptid, diagnosis Then with tbl1 as (select row_number() over(partition by ptid, diagnosis order by admitdate) as rowno, ptid, diagnosis, admitdate from patients) select A.ptid, A.diagnosis, A.admidate, B.admitdate, C.admitdate, (etc... until you've gotten the maximum number of possible dates) from tbl1 A left join tbl1 B on A.ptid = B.ptid and A.diagnosis = B.diagnosis and B.rowno = 2 left join tbl1 C on A.ptid = C.ptid and A.diagnosis = C.diagnosis and C.rowno = 3 where A.rowno = 1 (Keep repeating the left joins for however many times youfound as the max in the first select statement.) I'm not sure if this will work in access, but it is a shot anyway.
I'm a bit confused by your post, but hopefully this is at least part of what you wanted. From what I understand you are saying: Write a query showing admissions for a patient\diagnosis where there has not been an admission for the same patient\diagnosis within the prior 28 days I did this is MSSQL so you'll have to adjust the systax a bit for Access. select * from #pat p1 left join #pat p2 on (p2.ptid = p1.ptid and p2.diagnosis = p1.diagnosis and p1.admitdate &gt; p2.admitdate and datediff(d,p2.admitdate,p1.admitdate) &lt;= 28 ) where p2.ptid is null -- Below creates a table and data in MSSQL similar to what you are using and was used to test the query I wrote create table #pat ( ptid int, admitdate datetime, diagnosis int) insert into #pat select 1,getdate(),1 union all select 1,getdate()-4,1 union all select 1,getdate()-5,1 union all select 1,getdate()-35,1 union all select 1,getdate()-45,1 union all select 1,getdate()-40,2 select * from #pat p1 left join #pat p2 on (p2.ptid = p1.ptid and p2.diagnosis = p1.diagnosis and p1.admitdate &gt; p2.admitdate and datediff(d,p2.admitdate,p1.admitdate) &lt;= 28 ) where p2.ptid is null 
Thanks, I might try this in two steps. I would be happy to get the left join working with the two or more dates, as many as needed, to get all dates in a single record, I can then start comparing dates. A bit more background. We count the same occurance of the same disease as 1 if it recurs within 28 days, just means we did a bad job the first time. I need to mark these cases for exclusion and only keep the first date. So I need a table with the ptid and date for these cases, which I will flag in my main table for exclusion when I run my reports on disease rates and counts. The second thing is we want to know why people are being readmitted within 28 days, what's the problem, is there a pattern? This is the easier part once I id the cases and put them in a separate table for analysis. Will let you know how the codes works out and post my code.
select to_char(datefield, 'yyyy-mm'),count(*) from subscriberlist group by to_char(datefield, 'yyyy-mm') order by to_char(datefield, 'yyyy-mm')
No you see, that won't work. I'm basically looking at two dates, begin-date and end-date. If say, 1st of june, is between those dates, the customer was a subscriber then. With what you're writing, I would be asking how many subscribers started on 'datefield' (if i'd use 'firstDayOfSubscription), and i'm not even subtracting those that quit (i.e. those who's lastDayOfSubsciption has passed). 
Try building your list of dates, then pass them into your original query. Something like this: SELECT months.datum, COUNT ( * ) activesubscribers FROM subscriberlist sub JOIN ( SELECT add_months ( '1-JAN-2012', ( LEVEL - 1 ) ) datum FROM dual CONNECT BY LEVEL &lt;= 12 ) months ON months.datum BETWEEN sub.firstDayOfSubscription AND sub.lastDayOfSubscription group by months.datum; 
Took me a minute to understand the from subquery, but I see what you did there. Interesting way to get that dates list. Couldn't you join to this result instead of using the correlated sub? I guess that wouldn't be an issue since the expected results would be limited. But, what if you (hypothetically) wanted to get the last 1000 years (given there was data for that)... Would the correlated sub-query become a performance issue?
Never worked with Oracle but in SQL Server this is what I'd do: Select Month(to_date) ,Day(to_date) ,Count(*) as 'active subs' From subscriberlist Where Day(to_date) = 1 and (to_date between sub.firstDayOfSubscription and sub.lastDayOfSubscription) Group by Month(to_date), Day(to_date) 
Here is the solution, this assumes the diagnosis are preselected: SELECT admitdate, ptid, RANK() OVER (PARTITION BY hcn ORDER BY admitdate) AS 'VisitIndex' INTO #RV FROM table as o order by ptid, admitdate Select distinct * INTO #RankedVisits from #RV order by ptid, admitdate SELECT IndexCases.admitdate AS IndexAdmit, IndexCases.ptid, IndexCases.VisitIndex, ReVisitCases.admitdate AS Revisit, DATEDIFF(d, IndexCases.admitdate, ReVisitCases.admitdate) AS ReadmitDays FROM #RankedVisits AS IndexCases LEFT OUTER JOIN #RankedVisits AS ReVisitCases ON IndexCases.ptid = ReVisitCases.ptid AND IndexCases.VisitIndex = ReVisitCases.VisitIndex - 1 ORDER BY IndexCases.ptid, IndexAdmit SELECT ptid, COUNT(ptid) AS Count FROM [#RankedVisits] GROUP BY ptid ORDER BY Count DESC
In Oracle.... select A from MYTABLE m where exists (select null from MYTABLE n where n.A = m.A and n.B = m.C and n.ROWID &lt;&gt; m.ROWID) In other RDBMSs you'll need to find a ROWID equivalent (a unique row identifier pseudo column), so that the same row isn't compared to itself in the correlation.
 SELECT DISTINCT a FROM daTable AS this INNER JOIN daTable AS that ON that.a = this.a AND that.c = this.b AND that.b &lt;&gt; this.b -- different entry 
In that case just re-factor the above query a little and use a sub query instead. Something like: SELECT months.datum , ( SELECT COUNT ( * ) FROM subscriberlist sub WHERE months.datum BETWEEN sub.firstDayOfSubscription AND sub.lastDayOfSubscription ) activesubscribers FROM ( SELECT add_months ( '1-JAN-2012', ( LEVEL - 1 ) ) datum FROM dual CONNECT BY LEVEL &lt;= 12 ) months;
 USE tempdb GO CREATE TABLE #Deli1181 ( [A] INT ,[B] NVARCHAR(10) ,[C] NVARCHAR(10) ) INSERT #Deli1181 VALUES (1, 'X', 'Z'), (1, 'X', 'Y'), (1, 'Z', 'Y'), (2, 'X', 'Z') SELECT DISTINCT [setB].A FROM #Deli1181 AS [setB] JOIN #Deli1181 AS [setC] ON [setB].B = [setC].C AND [setB].A = [setC].A GO DROP TABLE #Deli1181 GO Would probably benefit from having some more test data to be sure we're getting you what you're looking for...
You my friend are a gentleman and a scholar! This works like a charm and I'll learn a lot from this! Thank you! I'll go through your comment history tonight and upvote everything! :)
just use a date column, then the "monthly data points" can be returned via a query using the GROUP BY MONTH(date)
&gt; thinking about set based operations can be a difficult adjustment to some programmers who are used to thinking linearly. THIS.... nailed it
If you are using SQL Server, check out the STUFF function. I laughed when I first realized that STUFF is a reserved word in SSMS, but its legit.
True. Only plus I can see to mine is that the query can be edited later to still bring in the table's row information without needing to change the join from INNER back to LEFT.
Here is a complete breakdown of my existing tables in case you are interested in the larger scope so far. [Access Relationships in Progress](http://i.imgur.com/bJdlM.png) **EDIT** Also, your query worked perfectly. I am going to try the others just so I can get a handle on the syntax that can be used and try to wrap my head around the flexibility of what you can write and achieve the same goal.
Thanks, you weren't' being rude. As far as current projects, I think most if not all functions fall with the vendor to do upgrades, and any additional views, report, or queries are on "as need basis."(ie, minimal tasks). We have a medium sized, private university that in many ways is pretty antiquated and learning the basics will enable me to understand some of the business intelligence we can plan for down the road. That being said, the last couple of DBA's have been one foot in, one out. I think they know I am someone who will take my time and try to build in best practices even if it means I am learning it in a vacuum. (&lt; IE one reason I got the job.) 
Use either the ISNULL or COALESCE function. select isnull(Country,'United States) from Customer or select coalesce(Country,'United States') from Customer If the data is already in the table, you can't use INSERT to correct existing data. INSERT is only used to add new rows to a table. You need to use *UPDATE*. Also, you can't use "WHERE Country = NULL". That will cause an error. You have to use *IS NULL* instead. So, If you want to change\correct the data that is already in the table in the table, you can use something like this: update Customer set Country='United States' where Country is null Conversely, "where Country is NOT null" would update any rows were the value isn't null.
Bad idea! Ok, not neccisarily a bad idea, but would require a huuuuge amount of development time, cost, and time as cost. First and foremost you would need very stringent data rules around the entry. One piece of text in a decimal column, for example, would break your whole load process. If you have a direct link from the app to your database, this also opens up a whole range of security issues, and that would need to be addressed as well. What I think would be a simpler idea is just produce a standardised form that can be pushed over an iPhone App. This form would have all of the data validation rules built in, so as to reduce the amount of dirty records. You would then load to a secure server as a delimited text file, and load that text file into a database table, probably with more error checking in either pre-load or put to a dump table and do the data checking there. Use an unusual string of characters for the delimeter
Yes sorry! That is what I meant. 
&gt; Each visit record is duplicated in full 4 times Here's your first problem. &gt; It would work if I could group by the patient id and visit date This is why your system blew up the first time someone came back for a second visit that day. `:|` Please don't try to hack this into place. Just fix the problem correctly.
it's one visit, four billing codes, that's the problem, think there is a first() function in ms sql. yes, there should be a unique visit table.
Are you a Health Care Analyst? I'm one myself, which is why I ask. I work for a TPA.
Are you not storing the fee types for the 4 rows per visit? Either way this system sounds like it could use revamping, but here is the query that should do what you want(pending what StoneCypher said that if they come in twice the same day, it will just pick one, if you need both you will have to use dense rank) ;WITH uniqueVisit AS ( SELECT *, ROW_NUMBER() OVER (PARTITION BY patient_ID ORDER BY visitDate DESC) AS rownum FROM TBL_VisitingFees ) SELECT * FROM uniqueVisit WHERE rownum = 1 
Since you are somewhat familiar with Hyper V, might I suggest setting up a VM / Storage that has a mirror of the database setup and receives a copy of the backup files nightly(can be cleaned up on a rotation). Also worth looking into is if the ERB supplier would be willing to allow you to maintain another instance of the software connected to this VM database, should a catastrophic failure occur on the primary system it is always worth having a duplicate. These VMs could be allocated less resources than the live system, just allowing for a redundant system at fairly minimal cost.
I'm assuming from "I have a table of doctor billings" that this is some leviathan of a monolithic table. Which means a structure like: [tblBillings] ------------------- [patientID] [VisitDate] [PhysicianType] [Fee] [FeeType] You can probably get away with a SELECT DISTINCT e.g.: SELECT DISTINCT [PatientId], [VisitDate] FROM [tblBillings] This will give you one record per visit, assuming the [PatientId] and [VisitDate] are the same for all four duplications of the record. You can, of course, filter with a normal WHERE clause. If you are looking to get aggregate counts, it gets more complex. 
If you have control over this schema, you should go ahead and heed StoneCypher's advice. It needs to be normalized to eliminate duplicate data. It will be much easier to join tables together than to continually work around the problem. &gt; There is a unique patient id number, a visit date. Can you at least give us the full structure of the table so that we can see what you are up against? 
1. As has been mentioned backups are your #1 priority 24/7. This does not mean "Check to make sure the backups ran ok each morning" That is only part of it. You need to regularly attempt to RESTORE those backups otherwise you don't have shit. How long do your backups stay on disc before they go off to tape or whatever? How long do you have to be able to go back to? 2. Start planning a Disaster Recovery Plan (DR) now. It will take waaaaay longer than you think, will constantly be de-prioritized by management UNTIL an actual Disaster ;) 3. Learn about Indexes. Clustered vs. Non-Clustered is the main starting point. Learn about defragging these, updating statistics etc. as these are the second most important "maintenance tasks" after backups. 4. Join your local PASS chapter if they have one, if not get attached to PASS somehow and get their emails to tell you when they are having virtual presentations. Most of this will be over your head for awhile, don't sweat it, just make contacts to ask people your "stupid noob" questions. 5. Make good friends with whoever is in charge of your disk, number one issue for SQL Server is disk I/O as well as storage space. This person will grow to dislike you (because you will constantly be asking for more than anyone else they have to deal with) if you don't. This is slowly changing now towards memory so all infrastructure people should be bribed with treats.
This does not work. There are four rows, non-distinct, per visit. I selected distinct ptid and visit date and it's all the same visit, count of four--there is nothing distinct about the visit. Yeah, it's messed up. I was thinking group by patient id, visit date and count the first row, as Coldchaos suggests...gonnna try that now.
Depending on how many rows you are going to be putting into this table (and expected to keep live) you may want to plan to use a table partitioning scheme here as well.
Sounds like a great case of dropping 75% redundant (and probably partially defective) data. Don't just normalize. Check before you reduce. There _are_ going to be errors in that much repetition.
It is generally poor form to use a timestamp as an identifier(even as a secondary one) due to the nature of how almost every system/language handles them differently to some extent. They are extra information that can be used to refine queries/reports/etc; anything more is just asking for a world of headaches. 
I'm not sure I understand. If the PTID and Visit Date are the same for the visit, even if there are 100 records for the visit, selecting the DISTINCT for PTID and Visit date would results in one record per visit. To get a visit count by PTID, use the above as a subquery and then count(Visit Date) by PTID. Of course, this makes the assumption that the patient does't have more than one visit a day. 
Is iindustries a multi value parameter? You may want to change it to where industry in (@industries) 
This sounds tricky...You may want to try this (see link below). (create a schema (xsd file) by using the original XML file). Then try to create a WSDL file from that xsd file, which you can them use the WSDL file to configure an SSIS load (assuming you are using sql server) to transform the XML into a tabular format on sql server, then do your analysis using SQL on the resulting table. I've never done this - so I'm not even sure this is possible. I've used SSIS/xquery to work with XML..but have never ran into your situation. You may want to post something on reddit/r/xml for additional help with the first 2 steps. http://stackoverflow.com/questions/74879/any-tools-to-generate-an-xsd-schema-from-an-xml-instance-document I wish you luck! Keep us posted how you did this if you figure out a solution.
It almost sounds like you should start with a new db design and ~~migrate~~ transform the original data into the new design. That will allow you to handle any conflicts that StoneCypher is referring to. It will be nasty.
Please do. I'm curious about the result as well.
What's the datatype and value of @year?
Well our company uses SQL Server which has XML support. I could created a column of type XML and put the data in that. What I'm trying to figure out is how, without a defined schema, can I say "for each tag return a count of each possible value" without coding the names of hundreds of possible tags? Is there some advantage in this using Postgres over MS Server? Keep in mind I'd have to install Postgres on my local machine and then copy the table from our server. That alone would take hours.
I get 6 for every single row. I don't understand how to reference members with carrierID. By counting planID you get how many members are under that plan but I need to know how many members are under each carrier.
I get not a group by expression. Edit: My mistake I messed up the order by clause but I still get 6 for every row.
I will once I get near a computer. Edit: It worked just needed to change some things. Here is my final code select distinct c1.CarName as "Carrier Name", P.Plndescription as "Plan Description", count(M.PlanID) OVER (PARTITION BY c1.CarName, P.PlnDescription) as "Members in Specific Plan", count(M.PlanID) OVER (PARTITION BY c1.CarName) as "Members Serviced by Carrier" from carriers c1 inner join Plans P on P.CarrierID = c1.Carrier_ID inner join Members M on M.PlanID = P.PlanID
Excellent. SUM() was a silly mistake on my part, as COUNT() was obviously needed. Now, it doesn't look like all parts of your original question has been answered by this query. Need any additional help?
Multiply by 100...
Yeah. For instance, I had a table where each row had a date column. I was able to turn the month into a column group. Every time data was added for a new month, it would put another column on the report.
You could try it a different way and see if it helps. But I really think you're going to need an index on session and user or you're going to have a bad time.... Select s.* From SYSAUDIT s Join ( select user, session, min(ROWID) ROWID From SYSAUDIT Group By user, session ) m On m.user=s.user And m.session=s.session And m.ROWID=s.ROWID
nope, afraid not
views are not magic. views are not a panacea. repeat this to yourself every morning
If you can't index the underlying table, first see if you really need all of the columns. If you do, then look into creating an index on the view - in sql server this means you do something like Create View .... with schemabinding go Create index vw_idx (col1, col2...) on sysaudit_v. If you can't create an index, next thing to consider is whether or not you can pull information from the view into some temporary holding table periodically and index that holding table. This assumes that you're ok with looking at 1 hour old or so data. 
&gt;The problem is that performance is terrible. The underlying table has a HUGE number of records, with no index. I cannot change the underlying table. My 386 can't run win8 but i can't upgrade... SQL server is a **relational** database management system. If you can't create indexes to enhance performance you are using the wrong product...
If you're automating it, why can't you continue using trunc(sysdate-1) and automate the report to run everyday? That's what I've done with reports that pull data from the previous day.
Something like this? select case when to_char(sysdate,'DY') = 'MON' then sysdate - 3 else sysdate-1 end as previous_work_day from dual
That is what I was doing until I encountered an error on Monday since there was no results to report of Sunday. Basically I have two codes that work now I'll just say x and y I want to select the x report on Monday And the Y report on Tuesday through Friday I know I did a terrible job explaining this and truly appreciate the help 
This is the correct and sustainable method....use this.
Personally I'd design it something like this: TABLE phoneCall(idPhoneCall[PK], startTime, endTime, callRecordingLoc[file location or actual recorded data]) TABLE participant(idParticipant, idPhoneCall[FK], startTime, endTime, phoneNumber, location, staffNumber) For each person that joins the call: * The participant is tied to the primary call via idPhoneCall, allowing for as many people to join/drop as you want while only providing one recording. * A participant row would be added on each new connection, if the same person disconnected/re-connected they would be given two unique rows. * The join and end time of each participant is stored separate from the primary call. * A name or staff number can be optionally tied to the participant if possible.
Yes, but, It looks to me that in case s.LastName is null, only then will it use the substring\charindex combination to extract the last name from full name.
Whomever wrote it was concerned that LastName might be null in some instances. Could have been a bug at the time. I wonder if the orignal table design didn't have a first name and last name distinction and that was added later as an afterthought.
look into smo sir. It scripts and allows for so many options. Funny enough, i had to write something like this this past week... Almost done, but it doesn't script Data Collections or Service Broker objects correctly :(
This looks cool but, color me paranoid. This would be a perfect place to collect code to be used for exploits. I'd never put my code out there were it could be seen by people outside my organization. It does look like a great tool for learning how to format your own SQL for readability.
I'm with lumpyg on this one, but I have to ask: why? Formatting SQL isn't that hard, especially if you use an IDE or editor that has consistent (or even configurable) tab stops. I write a crap load of CASE statements in my code, some hundreds of lines long, so I use an IDE that allows me to collapse them, but I've never needed another tool to format my SQL for me.
I find it useful for inherited code, not code I wrote. 
I use [Allround Automations PL/SQL](http://www.allroundautomations.com/plsqldev.html) at work, and they have their built in [beautifier](http://www.allroundautomations.com/plsbeautifier.html). It's not the best, because I wish it had more options to set Exactly how I like my code, or maybe the option to let me paste in code exactly how I'd like it, and copy all of my context, and apply that all future code, but it doesn't. Also, I've never used a IDE that hasn't had a simple beautifier for me built in. I'd have to go with lumpyg, too. Shit's sketch.
I primarily use Toad Data Point (previously Toad for Data Analysts) - my employer has a enterprise license for the Toad for Oracle DBA Suite, so we get TDP free with it. It's got a lot of nice features, but it can also be buggy and slow (it's based on .Net, so...). I also use Toad for Oracle (built on C so it's much faster) as well - both do syntax highlighting, CASE collapsing, etc. One feature of both that I LOVE is parentheses matching - it'll showing you what your current parentheses is matched to, making it much easier to figure out where you forgot one :D I know that there are some other IDE's out there that do all these things as well, I just haven't sought them out yet.
Yeah - especially since most of the table names are like: XXXX_XXXXXXXXX_XXX or worse :D
What do you mean by this? I've used it with Oracle and had no issues. Make sure that you change the database type to "Oracle/PLSQL".
You're talking about developers here... But seriously... that could add additional logic that might complicate things on the app code side. Plus, the devs are on time constraints. What do you think will get the ax first? Free time or additional logic? I see your point, but at the same time, I don't see it happening.
...point taken. I think I'm going to go have some beer now. It's 10am, but... yeah...
I 2nd that - this is much like the method I currently use on some of my reports, though I've got more type conversion going on.
Another potential (but difficult) solution to this, would be to write a TSQL parsing script to look for the '&lt;' and '&lt;/' tags, along with parent-child relationships to determine all the possible tags and nestings. You'd probably have to cast it into a 'text datatype field. OR...just found this online... set the @r variable to be the XML that you are trying to analyze,... then modify the ('/*/*/*') section to be the # of levels you think might be nested the example is for 3 levels. ('/*/*/*/*') would be 4, etc. I have no idea what this will return, but it is worth a shot. Once you have this data, you might be able to use xquery to get what you need. ------------------------------ DECLARE @r XML SET @r = ' &lt;Reference&gt; &lt;Basic&gt; &lt;Book&gt;A1.Hj1.JU9&lt;/Book&gt; &lt;/Basic&gt; &lt;App&gt; &lt;A&gt;AK9.HL9.J0&lt;/A&gt; &lt;A&gt;A18.H.PJ69&lt;/A&gt; &lt;/App&gt; &lt;Sub&gt; &lt;B&gt;B13.H98.P9&lt;/B&gt; &lt;B&gt;B18.HO9.JIU8&lt;/B&gt; &lt;/Sub&gt; &lt;DI&gt; &lt;D&gt;D23.HYT.P6R&lt;/D&gt; &lt;/DI&gt; &lt;/Reference&gt; ' SELECT t.c.value('local-name(..)', 'varchar(max)') AS ParentNodeName, t.c.value('local-name(.)', 'varchar(max)') AS NodeName, t.c.value('text()[1]', 'varchar(max)') AS NodeText FROM @r.nodes('/*/*/*') AS t(c) 
Thank you for the response. I worked all day on this and I finally worked out an answer. DBSHRINKFILE requires the .mdf and .ndf to be in the same file group. So that does not work. What I had to do was go through ever table -&gt; Right Click -&gt; Design-&gt; F4 -&gt; Under Properties change the File or Partition Scheme Name to 'Primary' and set the Text/Image FileGroup to 'Primary' -&gt; Right click Save script. Then run the script. This moved the table, indexes, (clustered and non cluster), keys, constraints to the .mdf (Primary) file. The I used DBBC SHRINKFILE and REMOVE FILE. It took lots of manual labor but it all worked.
With respect to Coldchaos, I think he's saying he has data in different filegroups. I believe the solution can be found in [here](http://www.mssqltips.com/sqlservertip/1112/filegroups-in-sql-server-2005/). TLDR: You need to use a command to move the tables between filegroups, and then you can empty the secondary filegroup and delete it. 
Also for the second query you aren't joining the tables. That is the same as selecting * from each table in random order. If the employee table has a zip code column then it would be "where employee.zipcode =zipcode.zipcode"
Unfortunately we don't have a DBA nor have we ever. We are all software programmers. I know that having a secondary file can be of benefit but I am 99% sure that this .ndf file was created by mistake and all the tables/indexes that where attached to that file where not done so correctly or thoughtfully. Now that I have finally deleted the dang thing I will go forward with trying to figure out the correct use for it and then implement it.
I already know how to create a database and users, setting users privileges etc... by using cpanel, but that's about it. So if you could recommend a book or some good tutorials on the subject that would be very appreciated.
Your supervisor sounds like a dumbass. Anyway, /u/rbatra wrote a really good free ebook called [A Primer to SQL](http://www.reddit.com/r/learnprogramming/comments/11pdmb/my_free_ebook_a_primer_on_sql/) that shouldn't take you more than a few hours to work through. It'll teach you the a lot of what you need to know and you can build on it from there.
This is a good starting point for very brief tutorials if you are not the kind of person to sit down and read through a book in its entirety: http://www.w3schools.com/sql/default.asp I would also recommend installing SQL on your computer so you can play around with stuff on your own. If you are learning and accidentally delete all the records, no harm if it just a sandbox on your machine.
While you are using the other resources listed in this thread to learn the basics of the SQL language, try playing my game called Schemaverse. The game is a space battle game that is played entirely in a PostgreSQL database using SQL. I have had a number of people now confirm that playing has drastically improved their understanding of SQL and they had a lot of fun doing it. The game can be found here: http://schemaverse.com and there is a tutorial here: http://schemaverse.com/tutorial/tutorial.php 
marking for later reference...
Are you accidentally using wildcard '%' with '=' instead of 'LIKE' anywhere? Something similar to the following will still run, but it won't return results. select * from database.dbo.mytable where columnB = '%outputB1%'
 SELECT this.columnA FROM table1 AS this LEFT OUTER JOIN table1 AS that ON that.columnB = this.columnA WHERE that.columnB IS NULL 
If you are talking about the sets of values in the entire table, rather than comparing Column A and column B in the same row, then you would use SELECT ColumnA from Table 1 MINUS SELECT ColumnB from Table 1 This is Oracle syntax - you can find the same set operator in other systems (e.g. EXCEPT is ANSI standard)
There are a few different ways. You will need to test in your own environment. I have put the estimated subtree cost (lower is better) beside each option for my simple trial I did in MSSQL server. It was only 6 lines so it may not be a fair trial compared to the size of your data. -- Option 1 .006825 select ColumnA from Table1 where ColumnA not in (select ColumnB from Table1) --Option 2 .006848 select a.ColumnA from Table1 a left join Table1 b on b.ColumnB = a.ColumnA where b.ColumnB is null --Option 3 .01806 select ColumnA from Table1 except select ColumnB from Table1 
Fun fact: EXCEPT and INTERSECT were added to MSSQL in MSSQL 2005. ...the server I use most frequently at work uses 2000. /wrists
NOT EXISTS ( SELECT NOT IN() WHERE NOT EXISTS() POOP )
Well in my example I had such a small number of rows it really didn't matter. I redid the test with a temp table with 50,000 rows where every 25th row did not have a matching value in column B. With 50,000 rows the results are quite different. I also added in option 4 which was suggested by reginaldbuxley in another comment. In this case each option has subtree cost for 7 rows | subtree cost for 50,000 rows - execution time for 50,000 rows. Note I did not clear buffers or proccache during this test. -- option 1 .006825 | 6.55753 - 756ms select cola from #comp where cola not in (select colb from #comp) -- option 2 .006848 | 1.46884 - 43ms select a.cola from #comp a left join #comp b on b.colb = a.cola where b.colb is null -- option 3 .01806 | 2.49418 - 70ms select cola from #comp except select colb from #comp -- option 4 .006687 | 1.3985 - 62ms select cola from #comp a where not exists (select colb from #comp b where b.colb = a.cola) For those who want to try at home. declare @ColBValue varchar(5),@sql varchar(max) declare @i int select @i = 1 create table #comp ( cola varchar(5), colb varchar(5) ) while @i &lt;=50000 Begin if (@i % 25 = 0) select @ColBValue = '' else select @ColBValue = ltrim(str(@i)) select @sql = 'insert into #comp select ''' + ltrim(str(@i)) + ''',''' + ltrim(@ColBValue) + '''' exec (@sql) select @i = @i + 1 End
Good point, missed that one. upvote for you!
Thank you. I'm going to run that same test on MySQL when I get home from work for comparison.
This one isn't as important, but also make sure that the Service account you are using allows for delegation in AD. That's about everything I can think of when setti g up a new instance. 
Is it weird that SQL does those two separate lines for the SPN registration? This is a straight, fresh SQL install, and the other servers I checked look the same, but I can't recall from previous gigs if that's a usual thing. 
Certainly you can, but don't think about it in terms of an array, but a table. SQL is a relational database programming language, so put your array values in a table and query the table! You can create the table like (in SQL Server): CREATE TABLE iqValues ( val INT ); Then in your main code, once per section: ... WHERE my.IQ IN ( SELECT val FROM iqValues ORDER BY val ASC ); 
SQL defaults to set based operations, so there is always sort of a loop going on when comparing data in a table. Think of it this way. Say you write a query that returns all results in a table called employees where age is greater than 30. It will 'loop' through the table, comparing each age record with 30. when it finds the expression is true, it returns that line, and keeps looping. Maybe I am going a little simplistic here. 
I think what you're looking for is a cursor. But what will happen is that you'll get 10 outputs; one for each IQ. pseudocode incoming: declare @X varchar(4000) declare @IQ int delcare iqCurs cursor for select distinct iq from my_table open iqCurs fetch next from iqCurs into @IQ while @@fetch_status=0 begin set @x = 'select * from my_table where IQ = ' cast(@IQ as varchar(10)) fetch next from iqCurs into @IQ end deallocate iqCurs close iqCurs
When thinking in terms of sql, please try to stop thinking in terms of loops and arrays. Ideally, you would have a table with all of the relevant IQ numbers. You would then join to this table in one Select statement
So what do you want to do??? why not just use a "group by IQ" and force a new page after each IQ? 
Cursors should only be used when the SQL server is powered off.
That solved my problem. Thank you very much. 
Hi Create a new shortcut to SSMS: **Set the TARGET of the shortcut as:** %windir%\system32\runas.exe /netonly /user:DOMAIN\USERACCOUNT "C:\Program Files\Microsoft SQL Server\100\Tools\Binn\VSShell\Common7\IDE\ssms.exe" **edit:** yes this will work, always sorts me out. 
 SELECT DATENAME(weekday,datecol)+', '+ DATENAME(d,datecol)+' '+ DATENAME(month,datecol)+', '+ DATEPART(year,datecol) AS formatted_datecol FROM ... 
Thank you. I got it working. I hadn't used DATENAME before. That made things much easier. There is a typo in your solution, though. The DATEPART(year should be DATENAME(year.
Have you looked at this article? http://support.microsoft.com/kb/811889 (click expand all, otherwise it may appear useless). **Simplified explanation** &gt;If you run the SQL Server service under the LocalSystem account, the SPN is automatically registered and Kerberos authentication interacts successfully with the computer that is running SQL Server. However, if you run the SQL Server service under a domain account or under a local account, the attempt to create the SPN will fail in most cases because the domain account and the local account do not have the right to set their own SPNs. When the SPN creation is not successful, this means that no SPN is set up for the computer that is running SQL Server. If you test by using a domain administrator account as the SQL Server service account, the SPN is successfully created because the domain administrator-level credentials that you must have to create an SPN are present. &gt;Because you might not use a domain administrator account to run the SQL Server service (to prevent security risk), the computer that is running SQL Server cannot create its own SPN. Therefore, you must manually create an SPN for your computer that is running SQL Server if you want to use Kerberos authentication when you connect to a computer that is running SQL Server. This is true if you are running SQL Server under a domain user account or under a local user account. The SPN you create must be assigned to the service account of the SQL Server service on that particular computer. The SPN cannot be assigned to the computer container unless the computer that is running SQL Server starts with the local system account. There must be one and only one SPN, and it must be assigned to the appropriate container. Typically, this is the current SQL Server service account, but this is the computer account container with the local system account. " The fact that you see the records tends to make me think this isn't the case, but there's a lot of other good troubleshooting info in that article.
* Learn your set theory, it makes everything database related easier. * Be able to [answer the following](http://thomaslarock.com/2012/01/the-5-dba-interview-questions-you-have-to-ask/): &gt;What is a Database? &gt;Who is the most important user of a database? &gt;Which is faster: Inserting one million rows of data, or updating one million rows of data? &gt;If I asked you to learn how to make a query faster, what would you do? How would you approach it? &gt;How do you troubleshoot problems in your current role? * Learn about the company you are applying for, nothing rubs managers the wrong way than someone misrepresenting their company; especially ones who don't know anything about the actual position. * Talk about your ability to learn, how do you handle staying relevant with your training? * Ask a few questions back, not stupid questions like... &gt;“**How many warnings before I get fired?**” &gt; “**What would you define as sexual harassment?**” ...good questions like... &gt;"**How do you feel about flex shifts for after hour maintenance?**" &gt;"**What days do you have update meeting?**"; ...things to show you are used to working in an business environment. * Don't argue, bring up your old bosses, etc. ~~Period.~~ **PERIOD**. * Understand that you could be talking to a room of people that don't know a lick about databases. Be prepared to show laymen how you would be a good DBA. * Google some SQL questions such as [This](http://tuandao.info/html/career/SQL.pdf)
Thanks for the info, I appreciate it. I actually had the interview yesterday, not a single technical question as asked. Much more oriented around the management of the group (soft skills) more so than technical leadership. If I get to the 2nd round, which I believe I will, I'll be interviewing with members of the team. I think that one should be more technical, although I think the focus will still be on personality, management style, etc. Thanks again for replying!
This is an excellent article, and coincidentally the guy who's been working on this issue with me forwarded it to me almost simultaneously at your post time. Thanks very much. 
If you insist on doing it with a loop, here is a better way than doing it than with a cursor on a live table. -- Check to see if temp table already exists USE tempdb IF object_id('temp_tbl_iq') is not null DELETE * FROM tempdb.dbo.temp_tbl_iq ELSE CREATE TABLE tempdb.dbo.temp_tbl_iq([id] [int] NOT NULL, [iqval] [int] NOT NULL) ON [PRIMARY] GO -- Populate table INSERT INTO tempdb.dbo.temp_tbl_iq VALUES FROM (SELECT DISTINCT iq FROM database.dbo.myiqtable) GO --define cursor in temp table values DECLARE @variq int; DECLARE acurs CURSOR FOR SELECT iqval FROM tempdb.dbo.temp_tbl_iq OPEN acurs FETCH NEXT FROM acurs INTO @variq WHILE @@FETCH_STATUS = 0 BEGIN --Do your sql report query here where database.dbo.my.IQ = @variq END CLOSE acurs DEALLOCATE acurs &gt;| **Personally, I'd just pull the reports to include 'iq' and group/partition on it'.** &gt;|
Create a table of months... MONTHS [Month], [MidDate] JAN, 1/15/2012 FEB, 2/15/2012 MAR, 3/15/2012 Join the tables together with greater than/less than operators... SELECT M.Month, M.MidDate, COUNT(D.ID) ActiveMemberCount FROM Month M DATA D ON M.MidDate &gt; D.EffDate AND (M.MidDate &lt; D.TermDate OR D.TermDate IS NULL) GROUP BY M.Month ORDER BY M.MidDate This will be slow, since it's joining with a ranged comparison, rather than a foreign key, but it should work.
This would make it so the member only shows up in the month that it was initially created. I think OP wants something that will show the count for any member that was active during a current month (the month is between the EffDate and TermDate).
Correct.
Its an adapter to a flat file in my case (I know kill me now!). It just allows me to query the database using SQL syntax. Group by is great if I had a effective date each month. But what am I to group by if I'm in the middle of the effective and term date?
SQL Pretty Printer (http://www.dpriver.com/) SSMS add-in; I'm in SSMS 99.9% of my day so web-based solutions can be a hassle to use.
oh.my.god thank you
Is the flat file a one time thing? Could you not import this into a table or temp table so that you don't have to worry about the adapter?
You could try doing transaction replication on the servers, not sure how well that would work for your situation though.
I am doing a transaction replication (i forgot to put it above) but from what I was researching a few months ago this wasn't possible.. 
Good luck. If you wouldn't mind reporting back the results, I'd be quite interested, as I was planning to upgrade my distributor to 2012 next month.
Okay. I should know in about an hour. Filling out the articles right now.
something like this would work in mysql select month(effdate) as mth,count(id) as c from data where year(date) = 2012 and termdate &lt;&gt; NULL group by mth order by mth desc
Okay. I got it all set up but now I'm getting Anonymous login issue with the Log Reader Agent. "'Login failed for user 'NT AUTHORITY\ANONYMOUS LOGON'.' Now I have to figure out that issue. Last time I dealt with it it just randomly started working.. I'm letting the server reboot over night and deal with it on Monday. The key to doing it was loading against the publisher with the 2012 version. If you try to set up the publisher on the 2008r2 build it was throwing errors. 
 select to_char(first_day,'MON'), sum(case when eff_date &lt;= first_day then 1 else 0 end) - sum(case when nvl(term_date,sysdate) &lt; first_day then 1 else 0 end ) as "member count" from data cross join (select to_date('20120101','yyyymmdd') first_day from dual union all select to_date('20120201','yyyymmdd') first_day from dual union all select to_date('20120301','yyyymmdd') first_day from dual union all select to_date('20120401','yyyymmdd') first_day from dual union all select to_date('20120501','yyyymmdd') first_day from dual union all select to_date('20120601','yyyymmdd') first_day from dual union all select to_date('20120701','yyyymmdd') first_day from dual union all select to_date('20120801','yyyymmdd') first_day from dual union all select to_date('20120901','yyyymmdd') first_day from dual union all select to_date('20121001','yyyymmdd') first_day from dual union all select to_date('20121101','yyyymmdd') first_day from dual union all select to_date('20121201','yyyymmdd') first_day from dual ) months group by to_char(first_day,'MON') order by to_char(first_day,'MON') ;
wee i'm good at reading that... i only saw it querying the info schema and i was all like, no... time to tutorial this good... which was a bunch of rambling anyway. oh well.
In Oracle... I've outputted count for every month in this year and constructed an inline view for the table of DATA and the months. with qry as ( select 1 as ID, TO_DATE('05/01/2011', 'MM/DD/YYYY') as EFF_DATE, TO_DATE(null) as TERM_DATE from dual union all select 2, TO_DATE('05/01/2011', 'MM/DD/YYYY'), TO_DATE('03/01/2012', 'MM/DD/YYYY') from dual union all select 3, TO_DATE('05/01/2011', 'MM/DD/YYYY'), TO_DATE('02/01/2012', 'MM/DD/YYYY') from dual), qryDates as ( select add_months(to_date('20120101', 'YYYYMMDD'), LEVEL-1) START_DATE from dual connect by LEVEL &lt;= 12) select TO_CHAR(START_DATE, 'MON') THE_MONTH, ACT_MEMBER_COUNT from ( select START_DATE, sum(case when (A.TERM_DATE is null and TRUNC(sysdate, 'MM') = START_DATE) or (A.TERM_DATE &gt;= START_DATE) then 1 else null end) as ACT_MEMBER_COUNT from qryDates D cross join qry A group by START_DATE order by START_DATE) Output.... Note it is now November, so null TERM_DATE included in this month's results. THE_MONTH, ACT_MEMBER_COUNT "JAN",2 "FEB",2 "MAR",1 "APR", "MAY", "JUN", "JUL", "AUG", "SEP", "OCT", "NOV",1 "DEC", 
&gt; for both of the dates and find what falls between them i know you weren't using BETWEEN but your query works the same way always the earlier date first, just like the lower number first for example, this -- WHERE somecol BETWEEN 5 AND 4 will always return 0 rows
Is there a specific need to use SQLCMD? You can use BCP to export the data and the &gt;&gt; redirect operator can be added to the end to send the log output to a file. Such as this... bcp DatabaseName..TableName out ExportedData.dat &gt;&gt; YourLogFile.log You might have to play around with the command line switches to get your format right. The same &gt;&gt; redirect might work to send the console output to a file for SQLCMD.
It's what the client had requested since they use it for other things. That helps a lot, though, I think I can get them to allow BCP for this! (Which makes my life twice as easy, since it uses tab delimited fields by default.) Thank you! :D
Wow, all my formatting went out the window. Is there a better way to format this?
Formatted: SELECT AVG(( SELECT SUM(sales.quantity) FROM sales ,orders ,movies WHERE sales.orderID = orders.orderID AND movies.movieID = sales.movieID AND movies.genre = ‘DVD’ GROUP BY orders.orderID )) AS ‘Average DVD Sales per Order’ FROM sales ,orders WHERE ... 
In case anyone else is wondering, a DOS guru coworker told me how to do this. (The %s are for variables, since this is wrapped in a BAT file.) sqlcmd -S lpc:%sqlinstance% ^ -i "%filepath%\%scriptname%" ^ -s " " ^ -h -1 ^ -W ^ -r1 2&gt; "%filepath%\%outputprefix%_%date%.log" ^ 1&gt;&gt; "%filepath%\%outputprefix%_%date%.tsv" The 2&gt; redirects STDERR while 1&gt; redirects STDOUT. :) Thanks for your help, /u/MeGustaDerp!
Are you sure? Because I created a new sale for 100 VHS tapes on a new order (same customer, but that shouldn't matter) and ran the query and it came up with the same answer I had originally. Doesn't the Where clause within my nested FROM statement filter out all VHS files?
SQL is not meant to be programmatically recursive, and problem solving small bugs can take hours; If I understand the problem something like the following should be done with outer join subqueries inner joined together to form a path then union the results together to get all the results(messy and will have lots of cases to union). You can potentially use sub-queries to do this, but be aware the limit for recursive queries is usually 32 deep; if you do choose this route, flatten the table to be two columns, one key to a mapped values, then at worst case your will be doing two or three recursions. Far better would be pull the data into a directed graph data structure, where each number has a node and each directed edge would indicate a mapping to that number. A path between the numbers could be found using any traversal method you'd like, if they return null then you do not have a path between them.
This should work for you, and is the most legible: SELECT DISTINCT * --bad I know, go away FROM dbo.vNtEventLog WHERE eventId NOT IN (SELECT eventId FROM EventSetDef) ORDER BY Machine_GroupID, ApplicationName, eventid, EventMessage, agentGuid Alternative(basically equivalent): SELECT DISTINCT * FROM dbo.vNtEventLog AS a LEFT JOIN EventSetDef AS b ON a.eventId = b.eventId WHERE b.eventId IS NULL [Here](http://i.imgur.com/7Ssc4.jpg) is an image that can help you with the set theory part of what you are doing.
That part is mirroring the data so 2 -&gt; 3 means there will be a 3 -&gt; 2, so the recursive table would ideally work. I also tried the recursion method due to the way the conversions, in a way, are hierarchical, even though they do eventually loop. But what you said makes sense in the case that since it has cycles, it may not be able to evaluate all the results in a set.
The problem with the database I'm working with is; all the conversions are user defined, based on the specific item. One items conversion from 3 -&gt; 5 may be different from another item's same step. As well as the fact that one item may have 2 conversions available while another has 10 conversions. I've actually done my checks by creating the step by step levels in separate tables, and found the desired results for specific items, but at a very case by case basis. 
You mentioned how my data cycles in another post, and this idea did spark, thanks for pointing this out as well, it seems to be working now with a lvl condition. With the change below, this does work! select c.i_fk, c.u1_fk, t.u2_fk, c.MultFact * (t.u2_qty/t.u1_qty), c.lvl+1 From #cte as c join #t as t ON c.u2_fk = t.u1_fk where lvl &lt; 5
Personally I think you will be fine, there is an awful lot to learn about databases that you will be taught which doesn't require any programming logic, creating, loading, managing, monitoring are all programming free. The only thing I can see there that might require any kind of programming experience is the report writing, and that is usually all done using SQL, which is different enough from Visual Basic that anything you learnt there would not be any use.
This statement fragment is from RCTE's for SQL in DB2. I'm not sure if it is helpful or necessary here. CYCLE arrival SET cyclic_data TO ’1 ’ DEFAULT ’0 It creates a new column then when the data creates a loop it changes from '0' to '1' and then stops the loop.
CONCAT() Look it up. I'd post the whole solution, but as you said, this is homework and I don't want you to loose the learning opportunity.
So CONCAT() pushes the columns together so it looks like NY10029 versus two columns with the State and ZIP. I feel like he does want one column, but a result that looks like "NY 10029" Do I add a ' ' between the two attributes in the CONCAT function?
&gt;&gt; CONCAT() This implies that you are using SQL Server. It might be different depending on what platform you are using. But this is a good start. Just try to figure out how to concatenate strings in your flavor of SQL. Like Sage, I will refrain from giving too much more info.
I realized I can accomplish what I believe is implied by doing: cust_state ||' '|| cust_zip
This would work in MS SQL Server and you don't even need to use CONCAT. Could just do Cust_City + ', ' + Cust_State + ' ' + Cust_Zip AS FieldName. Put a RTrim() or LTrim() around it to get rid of leading or trailing spaces
You need to know oracle sql.. oracle uses some different commands than mysql or mssql, but the logic is the same. An intro to database management class has very little to do with VB...unless they are going to interact with the database using VB. If that is the case 2 hours of google will teach you everything you need to know. 
Thanks for the help, I've figured out this one, but now I'm having an issue with getting my results for another question to sort by ascending numbers Here's what I have, and it produces what I need, but not order 1, 2, 3, 4, etc. SELECT DISTINCT sales_prod_nbr, SUM(sales_last_year), SUM(sales_year_to_date) FROM sales WHERE sales_last_year&lt;200000 GROUP BY sales_prod_nbr; Can I add a SORT BY function to list the results in ascending order?
use order by after your group by clause
Im guessing CONCAT() wouldnt work in access?
Yeah the Visual Basic is supposed to be just for "logic" training. They should teach the Oracle stuff straight up from beginner level. 
Lose*
You must be using oracle.
SSIS is probably the best way to go so you get control over what is happening. If you don't want or can use that and want to use only procedures: why not just have them run in a series since the order of B1, B2 and B3 doesn't matter?
&gt; why not just have them run in a series since the order of B1, B2 and B3 doesn't matter? They're long-running procedures, and putting them in series multiplies the necessary time by the number of procedures. I'm importing nearly-live information from Oracle, and my goal is to keep the entire process under 60 minutes. I have 3 procedures now, but I could have 50 in a year as the system grows and gains acceptance.
[Same Question?](http://stackoverflow.com/questions/13392743/merge-vs-truncate-and-insert) [Some interesting comments here](https://forums.oracle.com/forums/thread.jspa?threadID=1007738) as well. I would assume that truncate/insert would be faster... no need to compare the data. More relevant with that many rows. Also need to consider if "Auto Increment" rows need to stay the same, or if any other tables are affected. For that much data, might want to consider [bulk insert](http://msdn.microsoft.com/en-us/library/ms188365.aspx).
You could create a table that has the Path and filenames of the databases that you want to connect to. Then another column with a nickname. In the form display the nickname in the drop down. That table would have to be kept up to date. I'm not sure how static all of these databases are. I would probably use a dialog box and have them pick the database through the file explorer. 
This is more of a VBA question. Post it to /r/msaccess and you will get solutions.
Here is a link about doing dynamic queries in Access. It might be what you are looking for: http://support.microsoft.com/kb/304302 It talks about getting the information from a forum which you would have to build. I haven't fully read through it but what I can gather it seems this would be what you need. 
If you go to /r/Access it seems to be a subreddit about handicap accessibility world wide. Don't think that is the subreddit he is looking for, maybe /r/Vba?
This is the pseudo-code to do it. Dim sDBPath as string Dim qd As QueryDef sDBPath = 'Get path from user however you see fit Set qd = CurrentDb.QueryDefs("MyQuery") 'one query you are always changing the SQL of qd.SQL = "SELECT table IN " &amp; sDBPath
Solved it. Apparently when trying to use "select into" a variable and it doesn't find anything, an exception is thrown (but not shown to me for some reason, probably because it is within a function?) This is the working version, if anyone was wondering: http://pastebin.com/hDFSPmTf
I know you solved it, but I wanted to assist with the exception question. The exception was throw but it was most likely caught by the code calling the function. If that code has an EXCEPTION block and had a WHEN OTHERS clause it could have been hidden from you. 
Thanks guys
I don't think Access can do two statements in one query the way SQL Mgt Studio can. You may have to do them one at a time or combine into one.
Don't know about Access, but in SQL Server, if you want to insert multiple rows of values, it would look something like this.... INSERT INTO EMP_1 VALUES ('101','News','John','G','11/8/2000','502'), ('102','Senior','David','H','7/12/1989','501'); Also in SQL Server, you can feed an insert statement with a select statement and specify column names before 'VALUES'... INSERT INTO EMP_1 (EmpNumber, LastName, FirstName, MiddleInitial, DateOfBirth, OtherNumber) VALUES SELECT EmpNumber, LastName, FirstName, MiddleInitial, DateOfBirth, OtherNumber FROM Employee; Hope this gives you some avenues to explore.
In most cases it won't matter, however, there are some rather complex statements I've seen where the ordering did make a difference. Take a look at the execution plans as they will tell the full story in your particular case.
KISS = keep it simple, stupid! Merge is perfect for finding inserts/updates/deletes, much more robust than a simple truncate and insert. It sounds like you're doing a full data wipe &amp; re-load during the execution of this process, if so I would just avoid using merge altogether as it is meant to do more than what you ultimately need to do.
Combine the two data tables into one with a union, via a Common Table Expression. Then group by, sum and select where HAVING sum &gt;= 10. Something in this ballpark. This leans towards my MSSQL experience. Probably wont' work as is, but should be close: with cte_Table as ( SELECT * FROM [Table Local] union SELECT * FROM [Table History] ) SELECT ID ,Name ,Sum(Units) as Units FROM cte_Table WHERE id in ( select distinct ID from [Table Here] where present = 'Y' and year = 2012 ) Group By ID, Name HAVING Sum(Units) &gt;= 10
Thanks for the quick reply! How does this code meet the requirement that only those people with 10+ Apple units are selected? Right now, it looks as if it would just select anybody with 10+ total units.
Your right... I think a simple addition to the where statement would fix that. with cte_Table as ( SELECT * FROM [Table Local] union SELECT * FROM [Table History] ) SELECT ID ,Name ,Sum(Units) as Units FROM cte_Table WHERE Type = 'Apple' and id in ( select distinct ID from [Table Here] where present = 'Y' and year = 2012 ) Group By ID, Name HAVING Sum(Units) &gt;= 10 
To start with, there's a huge amount of information poured into each Access file per substation via extraction from Smallworld (the company handles between 80 and 100 of these, can't remember the exact number) including voltage ratings, transformer loads, etc. Around 35 tables are populated within each Access file to hold all of this data. Beyond that, there's another "key" Access file that yields reports based on whatever data the user needs (enter the substations you want, click a button, get the data from a historical database.) So I need that key file to have a way of dumping a report it generates into any of the 80-100 files that the user selects, anywhere on the computer at any given time.
Thank you two! I'll be cross-posting in those other two subreddits and let you know if I hear anything back.
FYI /r/msaccess
Why do you need to do this? Most databases automatically handle locking fairly well.
I think I've got it... it takes a long time to execute but that's probably just because vNtEventLog has over 1 million rows... I would think there's a better way to do this, but there's not really anything to join on unless I want to return what IS excluded. So the top query is everything, the bottom query returns excluded events and then it uses EXCEPT to subtract the bottom query from the top query. I made some smaller tables to test with and this seems to work It's also only showing Errors (not warning or informational) from the System log (not application log or security log etc) SELECT distinct ev.ApplicationName ,ev.eventId ,ev.EventMessage FROM vNtEventLog as ev WHERE ev.logType = 1380569194 AND ev.eventType = 1 EXCEPT SELECT distinct ev.ApplicationName ,ev.eventId ,ev.EventMessage FROM vNtEventLog as ev INNER JOIN eventSetDef as esd ON ev.ApplicationName like esd.source AND ev.eventId like REPLACE(esd.eventId,'-1','%') AND ev.EventMessage like esd.description WHERE esd.eventSetId IN (57068342, 32226495) AND ev.logType = 1380569194 AND ev.eventType = 1
 Depending on implementation and index's then yes it does make a difference. You should also consider that the where / select may also not be acting on a field eg it could be a view or a calculated field. 
 Assuming an acid compliant database when a transaction begins do a select * from table. that should lock it :)
Supposing it is acting on a view, then is the only other thing to account for the structure of the view?
What determines rule vs cost basis? The server type? The instance?
Thanks for the pro tips. I'm just in a beginners course, but I bet I'll have use of this in the future.
 select * from ( select HE.*, sum(UN.UNITS) over (partition by UN.ID) UNIT_TOTAL from HERE HE join (select * from HISTORY union all select * from LOCAL) UN on UN.ID = HE.ID where HE.YEAR = 2012 and HE.PRESENT = 'Y' and UN.TYPE = 'Apple') where UNIT_TOTAL &gt;= 10 
Yes, generally the RDBMS platform and version and sometimes there is an optimiser setting in the RDBMS. As far as I'm aware, modern RDBMS tend to use cost based (so based on data cardinality and selectivity).
Start as you mean to go on, so get into good practises now, it pays dividends later when you are evolving production environment systems in the future. Trust me.... I've learnt the lessons! 
ahh flashbacks. i feel anxious just thinking about your problems
No, that is the operating system (ie Windows Server). You have installed Management tools for SQL Server, but it would seem you have not actually installed the SQL Server database engine itself. Now, Windows Server is not SQL Server; it is the underlying operating system - just like you run Microsoft Word, Excel, etc on top of Microsoft Windows, so too you run server products like SQL Server, Exchange Server, etc on Windows Server.
mistralol, I'm trying to perform a full table lock (and not row-share lock). I want to hold the table and prevent writes until the current transaction is complete. I was hoping to achieve this in ansi-sql instead of writing code in my application for every type of database; orace,postgresql,sql etc...
Congrats :) I would have listed the most pertinent one, but I was on my phone lol.
Nice little read, I find I use a lot of the same practices (especially the capitalization). Also I learned some things about the array_agg and unnest functions, thanks!
Do you have a reason for the commas in front? I had heard that suggestion once and liked the look of it, but it was too counter-intuitive to use.
 Yes but thats gets deep. eg view on view on view (kinda silly yes) but can also be view on table with computed fields etc... when you get to this stage it gets harder and harder for the sql optimizer to know what is fastest
Am I the only one here who *hates* that the poster mixed an implicit join with an ANSI join? FROM wine_ratings, wine_detail LEFT OUTER JOIN wine_tags ON wine_detail.id = wine_tags.wine_id WHERE wine_detail.id = wine_ratings.wine_id should be: FROM wine_ratings INNER JOIN wine_detail ON wine_ratings.wine_id = wine_detail.id LEFT OUTER JOIN wine_tags ON wine_detail.id = wine_tags.wine_id Maybe I'm missing something, but making the joins explicit and putting them where they belong makes the code so much more readable and maintainable that there should be a stigma attached to those who don't do it.
 FROM app_wine ,app_winery WHERE app_wine.winery_id = app_winery.id &amp;#3232;\_&amp;#3232;
are you installing this on a school workstation, or your personal computer? if former, talk to your school's IT. if latter, do you need enterprise sql server? if not, then you can always go with sql 2012 express (it's free). if you do need enterprise, then you can always install your sql 2005 onto a windows virtual pc: http://www.microsoft.com/windows/virtual-pc/ this option, of course, would also mean you'll need a bit of know how to setup the virtual machine, and the sql instance on it. lastly, the end-all solution is just to purchase a developer's edition: http://www.amazon.com/SQL-Server-Developer-Edition-2012/dp/B007RFXQAM/ref=sr_1_1?ie=UTF8&amp;qid=1353442975&amp;sr=8-1&amp;keywords=sql+developer
Dynamics can probably run on newer versions of SQL Server, yes. 
You can always add more conditions, but I only see two tables that you are joining, and it's a left join, not an inner, which means it will take all elements from the left and pair them with all table entries on the right. To get more accurate results you could inner join the results of the above with the original eventIds to get the information that isn't matching.
I would've missed that if you hadn't pointed it out. I honestly only know people who use one or the other, so this puzzles (and annoys) me.
Seeing explicit joins mixed with implicit joins makes me cringe. FWIW, I prefer 8 spaces for tab stops, commas to start a new line &amp; capitalize reserved words.
I'm having a tough time wrapping my head around how and where to use WITH correctly. Anyone have a good tutorial on that? 
RedGate SQL Compare / SQL Source Control are nice tools if you can afford them.
This, plus it is easier to see missing commas because they line up. It also comes in handy in certain copy/paste/edit tasks like making an insert from a create table. Makes the edit step easier for me.
I would give the trial version a shot or even just [SQL Server Express 2012] (http://www.microsoft.com/en-us/download/details.aspx?id=29062) (which is completely free) which is free and isn't a trial version. If the app gives you greif, you could try to [change the database compatibility level to 90](http://msdn.microsoft.com/en-us/library/bb933794.aspx#SSMSProcedure) which would allow any SQL syntax not available after 2005 to run on 2012. However, I would be surprised if you did actually have issues with running 2012.
Quick point on your terminology that may cause confusion. * "SQL" is a language to work with databases. * "Microsoft SQL Server" is a database program that uses SQL as its language. You're talking about installing Microsoft SQL Server, not SQL. /r/sql is about the language, unless specified with [MS SQL] tag.
So if you have a car dealership, you have cars. Their natural ID is probably their VIN, though I'm not a car guy. And you know, maybe you have a table of the staff involved in selling a car. There's the salesperson, there's the two or three people who make your modifications (you wanted those door bumpers and a better stereo,) there's the person who did the delivery to the lot, et cetera. Because if they're tracked, then you can find out who planted the bomb, you see. Or alternately, you could go by car type - maybe you're a Toyota dealership, so you have one table that lists the base car types (Prius, Celica, whatever) and then another that correlates the actual physical cars on the lot to the known types. And that way you can run a query to see what Camries are in stock.
VINs are supposed to be unique, but genuine duplication mistakes exist in the world where a manufacturer has been given an overlapping range. Some sort of AssetID or unique ID is best. Several points for a better model : 1. Maintenance and repairs generally include parts, labour and other costs (like sundries, admin fees). Servicing is different from repairs for example you car needs to have oil and spark plugs changed yearly or per 12000 miles etc. and tyres are sometimes part of a maintenance contract. Repair / servicing date is required. 2. Vehicles can have factory fitted options (during manufacturing, e.g. sun roof, stereo system, climate control, pearlescent paint etc.) or dealer fitted options (whilst at dealership, e.g. mud flaps, car mats, child seats etc.). Some are standard (so free), some optional and optional ones can be free or have a cost. 3. Each invoices can have many invoice items and can be either sales ledger invoice (to customer) or purchase ledger (to suppliers) - sales and purchase ledgers should balance when you include profit and admin fees. (You buy from a supplier, so you send a PI and you bill customer so you send a SI to match). An invoice has to store tax point date as this can vary from invoice date. Each invoice item should store the Tax Rate (as some items are not subject to tax e.g. child seats). There should be a payments table linked to invoices, so that it can track when invoices have been paid (fully or partially). 4. There should be dates on the ownership table. 5. Vehicle data is a separate table from a vehicle asset data. i.e. Vehicle data is just info about the manufacturer's vehicle product (Manufacturer, Model, Basic Price, M.R.P., insurance group, road tax group, commercial vehicle), asset is an actual instance of that vehicle - one that's been made. Vehicle asset would have things pertaining to the asset like mileage, colour, VIN, options, accessories etc. 6. Create a supplier table that both parts, vehicle asset, invoices, can link to. An example of a correlated query using your current model... **Show customers' vehicles who had an invoice raised in the past 30 days (Oracle date syntax)** select c.*, v.* from CAR_OWNERSHIP o join VEHICLE v on v.VEH_ID = o.VEH_ID join CUSTOMER c on c.CUS_ID = o.CUS_ID where exists (select null from INVOICE i where i.INV_ID = o.INV_ID and i.INV_DATE &gt;= sysdate - 30) 
Thank you, this was helpful. 
hey ziptime. ~~Im trying to get your query to run in access but am getting "Syntax Error in FROM Clause" I changed the sysdate to to Date().~~ With some help over at Stackover Flow, I was able to get it to run. In Access you need to specific the join and have to use brackets with more than one join. TIL. Thanks greatly for your help. select * from (CAR_OWNERSHIP O left outer join CUSTOMER C on C.CUS_ID = O.Cus_ID) left outer join VEHICLE V on v.VEH_ID = O.VEH_ID where exists (select null from INVOICE I where I.INV_ID = O.INV_ID and I.INV_DATE &gt;= date() - 365);
commas go at the beginning of lines in other languages as well http://bofh.org.uk/2008/04/16/a-quick-javascript-formatting-tip http://ajaxian.com/archives/is-there-something-to-the-crazy-comma-first-style but this convention does seem to be most useful in SQL -- http://www.sitepoint.com/forums/showthread.php?755704-Getting-an-error-message-on-an-inner-join
I've tried it both ways, someone I worked with a couple of years ago was using the leading comma convention and I tried it for a couple of months, ended up going back to the comma at the end because I much preferred it. I did have a reason for doing it other than 'I just preferred it', but I honestly can't remember it now lol.
Access doesn't do joins. You have to specify which type of join you want. I thought the left outer was the default join method?
Make the Flushing predicate part of the join 'on' clause.
Your only pulling back rows where the enrollment and student student id are equal, you could probably add in an or e.studentid is null (bracket that and the id=id part), or join student and move that equals section to the join on clause
I see that you fixed it in your solution, but I wouldn't mix the old and new join syntax like this: &gt; FROM enrollment e, student s RIGHT OUTER JOIN zipcode z ON (z.zip = s.zip) WHERE e.student_id = s.student_id It makes it more difficult to follow.
Yes, but you wanted a correlated example and my query was about showing customer vehicles invoiced, not their amounts.
upvote for converting to left outer join -- right outer joins are awful
I don't know what your professor may mean by a "divide" but I think this will work. Of course, this is typed into Reddit now without an actual database to reference, so please test - refinements may be needed! Essentially, what this does is use a subquery to find all the properties which are missing at least one of the user's utility requirements. Then, in the outer select it lists the details of all the *other* properties, ie those who have all the user's utility requirements. The trick to the inner subquery is the right outer join. select * from Property where PropertyID not in (select distinct PropertyID from Property_Utility right outer join User_Utility on Property_Utility.Utility = User_Utility.Utility where Property_Utility is NULL) 
Where are the one-to-many relationships ?
Can you replicate the data to another RMDB?
You could likely do it as a UNION SELECT query. INSERT INTO EMP_1 SELECT '101','News','John','G','11/8/2000','502' UNION SELECT '102','Senior','David','H','7/12/1989','501';
upvote for the general strategy of writing your FROM clause starting with the most restricted table however, in this case OP wants to "return all records in the Transactions table and only matching TASK_TYPE from the Movement_Task table where available (left outer joins)" i did not interpret this to mean there was going to be a WHERE condition on task type, rather, it should return all transactions with or without task type, and the "only" part was referring to only returning this one additional column to the columns of the transactions table the sample spreadsheet data seems to confirm this as well, although it does not show any example where the left outer join is required
Correct, not all Transactions will have a matching record in the movements and task tables 
You're requirement isn't explicit enough to give an exact solution but you should be able to work with this: WITH days AS ( -- Replace the YEAR variable with your start year SELECT TRUNC(TO_DATE('01/01/' || :YEAR, 'MM/DD/YYYY')) + LEVEL - 1 AS dt FROM dual CONNECT BY LEVEL &lt;= 365 + 1-- Loop for how ever many years times 365 ) -- Here you can apply formatting to get the desired output SELECT TRUNC(dt,'MONTH') AS mnth , dt AS dy FROM days ORDER BY 2 ;
Alternatively create 'new' tables, with just the info you need. Might speed it up somewhat if there are less fields? Suppose depends how big the tables are, and if they are being used etc.
 CREATE TABLE `continents` ( `continent` varchar(255) ); INSERT INTO `continents` VALUES ('N. America'), ('S. America'), ('Africa'), ('Asia'), ('Europe'), ('Antartica'), ('Australia'); SELECT DISTINCT `c1`.`continent` `cont_1`, `c2`.`continent` `cont_2`, `c3`.`continent` `cont_3` FROM `continents` `c1` JOIN `continents` `c2` JOIN `continents` `c3` WHERE `c1`.`continent` != `c2`.`continent` AND `c1`.`continent` != `c3`.`continent` AND `c2`.`continent` != `c3`.`continent` This will produce a set of 210 permutations. These permutations will have duplication in them, of the unordered sort: compare 'Asia','Africa','Europe' with 'Africa','Asia','Europe' for instance. It does so by doing outer joins between a table of the 7 continents two times. An outer join is a join where every row in one table is aligned with every row in the table being joined to. This is done twice to produce three columns (the original table + the two join tables). It is only contrivance that the tables being joined in our case happen to be the same table. We then take that set, and we use a where clause to remove duplication from within the rows. I am not sure how to remove, between the rows, the duplicate unordered permutations using SQL. That is the part that SQL may not be so good at, but should I think of a method that doesn't involve stepping through procedural logic or functional logic, I will post it here.
Sorry about the delay, had a few things come up at work, I created a temp table and tested this it out, the following is even smaller and does work. --Table allCont has all the continent names in it select a.name, b.name, c.name from allCont as a left outer join allCont as b on a.pk1 != b.pk1 left outer join allCont as c on a.pk1 != c.pk1 and b.pk1 != c.pk1 Note: I assigned each continent a key so that the joins could be done on integers instead of string. This isn't required, just habit.
Just an aside: &gt;SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED Unless you know exactly what you are doing, NEVER type this. For your specific case it probably isn't a big deal. But I've seen people do some REALLY stupid things with that particular command.
Well, when you intend on ad-hoc read-only querying, it seems like an obvious choice, doesnt it? You'd never want this in production code where data integrity is key, but if you are simply doing reporting or basic troubleshooting (and dont want to type nolock after every table ), is there a downside? 
Look into grouping. You basically let similar items "roll up". You can group by almost any column you want. http://www.sqlteam.com/forums/topic.asp?TOPIC_ID=6600 
downvote for sending an access noob to a sql server thread (sql syntax varies significantly) i think he wants conditional totals -- SELECT COUNT(IIF(EnterTM BETWEEN #8:00# AND #8:59# ,1,NULL)) AS count_8_9 , COUNT(IIF(EnterTM BETWEEN #9:00# AND #9:59# ,1,NULL)) AS count_9_10 , ... , COUNT(IIF(EnterTM BETWEEN #22:00# AND #22:59# ,1,NULL)) AS count_22_23 FROM [Prescription_Timing_9/12]
It will be very messy and convoluted, but you could do something with scripting. I was thinking that you could build some dynamic SQL (huge SQL strings) to create a table based off all of the distinct assignment names, then do some IF statements to fill that table, like if name is X then insert it into Y column. Having said that, I'm sure there's a better way to present the data you're looking at. I would try and look for another common identifier (or create one) to separate your data, dates are always helpful in this area. 
Yes, the odds are what you're doing is going to be easier to handle in your script rather than in the DB itself (and if you're just spitting out data from the DB and pivoting, you could also avoid the scripting if you want to just dump it in Excel and use a pivot table there). Having said that, pivoting data is a common enough question that it probably wouldn't hurt to blow a couple hours trying to piece through it and see if you can get it done in SQL (even if it's only for a trivial non-dynamic case, doing so will at least begin to illustrate to you how you might go about accomplishing the larger task). Search StackOverflow (or just generically Google) for "pivot MySQL query" and you'll find a wealth of examples (newer versions of MySQL may even have a pivot function? I'm not sure). TL;DR - You're potentially looking at a long list of CASE statements, potentially dynamically generated to account for your dynamic data set.
Here you go big dawg. I took the liberty of adding fictional Exam2 and HW2 to expand the example. I also chose to do left joins because maybe someone has missing assignments.. its all made up anyways. select s.first ,s.last ,e1.score as "Exam 1" ,hw1.score as "HW 1" ,e2.score as "Exam 2" ,hw2.score as "HW 2" from scores s left join scores e1 on e1.first = s.first and e1.last = s.last and e1.assignment = 'Exam 1' left join scores hw1 on hw1.first = s.first and hw1.last = s.last and hw1.assignment = 'HW 1' left join scores e2 on e2.first = s.first and e2.last = s.last and e2.assignment = 'Exam 2' left join scores hw2 on hw2.first = s.first and hw2.last = s.last and hw2.assignment = 'HW 2';
Just some food for thought... This solution (as originally written) doesn't produce the output he's looking for as is (it needs a group by). Additionally, it's less than the optimal approach for pivoting data in this manner. The primary difference between the left join approach you are taking here and the SUM(CASE()) approach I take in the dynamically generated SQL in [my other comment](http://www.reddit.com/r/SQL/comments/13wejq/sql_multicolumn_display/c77yq7e) is that your approach will require a full table scan on the table plus an additional (hopefully indexed) join on the same table N times (once for each assignment) whereas my result will only require a single full table scan and nothing else.
I didn't read his requirement that he wanted to be able to dynamically include new assignments. As far as performance, considering the lack of info on keys or anything else... this is the query cozzbp deserves, but not the one he needs right now. So, criticize me, because I can take it. Because I'm not your hero. I'm a silent guardian. A watchful protector. A Dark Knight.
Haha, I certainly didn't mean to criticize harshly or cut your solution up (maybe it is exactly what he deserves, especially if there is a lack of keys because the performance of those left joins is going to be brutal on any reasonable set of data without a proper index), I just wanted to point out the differences between the two different proposed methods of pivoting the data. Thanks for the laugh, btw. ;)
Wow, I don't even want to attempt that haha. I just ended up doing it in PHP. Probably just as ugly, but at least I know PHP rather well.
&gt; The aggregate function column must be after the grouped column. say again please? if you mean what i think you mean, that's incorrect
It would help if we knew what the database structure was like. 
To restate what BinaryApe is asking you for: Can you please give us the table structure? DESC whateverTheHellMyTableIsCalled; Some sample data and a mock-up of your desired output wouldn't hurt, either. EDIT - Updated syntax for Oracle (brain was stuck on MySQL).
Outside of not giving us any information about your data model or a mock-up of your desired output, I should also point out that your question itself is flawed (or at least incomplete). You ask for "the Tool name that has been rented the most", but that's not sufficient. What if there is a tie? Do you want a single row of output regardless (i.e. ONE tool) or do you want N rows of all the tools that are tied for first? If the latter, how do you want them ordered?
Here's a start. Order/Limit it as you see fit. SELECT TOOL_ID, TOOL_NAME, COUNT(*) rentalCt FROM RENTALLINE GROUP BY TOOL_ID, TOOL_NAME ORDER BY rentalCt; If you want rankings or other analytics take a look at [Oracle's Analytical Functions](http://www.orafaq.com/node/55). Incidentally, is there any reason you are storing the Tool's name in the rental table when you clearly have tool data normalized elsewhere (or at least I hope you do based on having a Tool ID here as well)?
You mean ctrl+k c and ctrl+k u? =)
HUG!!
Toolbar? Toolbar!!! Command line SQL 4 LYFE!!!!!!
Little tips like this are exactly why I'm subscribed here. I don't often get them, though, so thanks for retroactively making me look like I know what I'm doing. 
I develop against SQL Server daily with Python, but I use SSMS to write and debug my SQL. In fact, I try to keep the business logic in the database, and rely on executive stored procedures to pull the data back out.
Well that's embarrassing. I see you have a solution below which is terrific; if not I'd try and work this out. I think my logic was on track so hopefully the problem was semantic.
Just make sure there are no GO statements in the comment block!
This concept works well with any block comment scheme. Since seeing this, I'm not adding an extra # to the end of block comments in Power Shell. Example: &lt;# commented text extra hash-&gt;# #&gt; I bet that this would work with the appropriate comment characters in any language.
Your approach should work fine... Try "Where IsNumeric(MyField) = 1" instead
I actually got pretty lucky finding my first SQL job. I didn't have an certifications or any of the mumbo jumbo. I just showed a strong enthusiasm for SQL and databases. I had taken SQL classes but still wasn't at a professional level. I asked r/SQL for advice on some good database books to read and just read them cover to cover. 
If you're applying for entry level jobs and have decent experience doing personal stuff and have learned a lot on your own then you'll have your pick of jobs. SQL is a very neglected subject in school and amongst younger folks who don't have the experience to understand its usefulness. 
Thanks people. I do have experience with university work however that's on software like oracle SQL *plus and a great knowledge on theory. But what's MS SQL server? Is it just the same as oracle SQL *plus? 
SQL Server (MSSQL) and Oracle are two competing products. SQL Server is from Microsoft (MS) and Oracle Database is from the Oracle corporation (started by Larry Ellison about two decades ago). SQL Server is a MS product, so it only runs on Windows OS's. Oracle, on the other hand, is very common on Unix\Linux since thats where its roots lay, but also has a good presence in Windows environments. Each has its own flavor of the SQL language (PL\SQL -&gt; Oracle ; T-SQL -&gt; SQL Server). Each is a little bit different from the other, but the basic syntax is the same. As far as user interfaces go, SQL Server is very GUI driven while Oracle (being rooted in UNIX) is mostly command line driven. SQL*Plus is Oracle's primary command line utility. It allows the DBA to connect to an Oracle database and do whatever via scripting. IMHO, Oracle is much more complicated than SQL Server. SQL Server seems to be better documented than Oracle and there is a larger online community around SQL Server than Oracle. I'm kinda just spouting out what comes to mind as I type. I can go on. What else do you want to know?
One way to find yourself in a position to use, and improve, your SQL knowledge every day is as an analyst of some type. Every operations has data packed away in an Oracle warehouse or a SQL Server back-end to an application. In every large corporation I've worked in, analysts do a vast majority of reporting (which means writing the SQL). Technology (IT) takes too long to accomplish just a single adhoc request and would actually rather have analysts doing the SQL writing so they can concentrate on more technical issues. I'd suggest looking for an analyst position that mentions SQL as a required skill (thus you know they have actual databases, not just a mess of Access databases). Once there, you can reverse engineer any current queries and then start building your own. After a while, you will find analyst and senior analyst jobs thrown at you since you know how to actually write SQL and not hack together an Access database with 20 queries chained together for one report. EDIT - forgot to mention, I have no formal SQL education/training. All self-taught using above methods. Just keep looking at code from those better than you and keep improving your existing code. It has turned in to a very lucrative and flexible career for me. And don't worry too much about SQL Server vs Oracle. You can easily be fluent in both, they just handle dates a little differently and have some different functions. Nothing you can't solve via a Google search in 20 seconds. Just stick to ANSI SQL and you will find moving from one RDBMS to the other is easy (even DB2).
I have the most SQL experience out of the guys in my group (took a semester in college), so I get to do the database work. I'm supposed to get a pay raise here pretty soon to reflect that.
I don't think normalizing is going to help you. However, if you are doing a bulk insert into a table, you could potentially speed up the bulk insert by removing any indexes on the target table. After the bulk insert is complete, you could add the indexes back on. But, really, 20 minutes is nothing in the database world.
Okay, that's at least decent news that I shouldn't worry about 20 minutes then, especially as it only needs to happen once in a while. I'll have to look into removing the indexes, thanks for that.
Nah, [check it out](http://msdn.microsoft.com/en-us/library/ms191178\(v=sql.105\).aspx) Gotta escape the parens with slashes in the url. I've been on reddit for a while now and I only recently ran into the issue myself.
It breaks up the normalization a bit, but use a combination of referential fields in a couple of tables, i.e. table activity tracks action_id(pk), player_id, game_id, action_code, and result while you have table actions that tracks action_code(pk), game_type, action_description, action_result. This way is removes the game from impacting the schema as you're just defining each type of action and results as you see them. You could get really fun and add a prev_action_id and winning_move fields to the activity table and track a games progress all the way through, etc. Edit: I guess the biggest thing I'm doing differently from your initial thought is removing the concept of a dedicated table per game; still works though, just have a players table and a games table in addition to the ones I listed earlier. 
signed up for it. Hopefully it'll give me the kick i need to finish the Python course i'm doing at the minute.
I get a not a single group group function error.
What am I missing here? 
hi, i assume that 1 carecenter = n beds and 1 patient can only occupy 1 bed so /* rewriting of your query */ select p.firstname ,p.lastname ,c.carecentername from patient_t p,carecenter_t c, bed_t b where 1=1 and b.bednumber=p.bednumber and c.carecentername like 'Emergency Center'; /* rewriting with count */ select c.carecentername ,count(distinct p.first_name) /* but the best will be to count the p.patient_id or something like this because if 2 people have the same name it's false */ from patient_t p, bed_t b,carecenter_t c where 1=1 and p.bednumber = b.bednumber and b.carecenterid = c.carecenterid /* need this from MeGustaDerp */ and c.carecentername like 'Emergency Center'; group by c.carecentername ; edit:grammar 
I'M SIGNED UP!
I took the original class too. It was well-worth it. I recommend it for anyone looking to learn more about databases.
So if I get what you are saying its basically to have a table that will contain all actions that happen in any game? So this so I will have : * A Table for all of the games(instead of one for each game) with name and other game info * A table for players * A table for actions (their descriptions and names and what not) * A table for for each individual action that that takes place in any game Does that sound about right? 
Could you take a look at my reply to NatecUDF below? Is that basically what you are saying with EAV? Also could you tell me more about NoSQL? It sounds like that might be what I'm looking for. Is that the same as an object oriented DB?
CHAR(34) will give you a double-quote. I believe you can also escape it but I find using the char function to be a little more legible in my SQL.
very poor quality article example: "Unique [index] only allows one column within the table to have the value" example: LIMIT takes two integers, the first is "the row to start at" (wrong) describes many of mysql's peccadillos (e.g. backticks) but ascribes them to "SQL" 
select * from tbl where patindex(char(34) ,person_data) &gt; 0 
GROUP BY is ~not~ required when the SELECT clause contains only aggregate functions
Why didn't I think of the char function? Thanks! I also added a + '%' to get my results as it was returning those with only one double-quote in the field.
Use a recursive table to get all the ref numbers into one field, then do comparisons on that new field.
Your previous query was doing a cartesian join on carecentre, which I assume is incorrect. I also prefer ANSI join syntax and table aliases.... select p.firstname, p.lastname, c.carecentername from patient_t p join bed_t b on b.bednumber = p.bednumber join carecenter_t c on c.carecenterid = b.carecenterid where c.carecentername = 'Emergency Center' A patient could have been to the Emergency Center more than once, even staying in the same or different bed, so you may want distinct. Count distinct patients who have visited the Emergency Center (remove distinct if you want total patient bed visits there): select count(distinct p.patientid) from patient_t p join bed_t b on b.bednumber = p.bednumber join carecenter_t c on c.carecenterid = b.carecenterid where c.carecentername = 'Emergency Center' 
I have a reference to DB2 for I SQL syntax but I don't think that will be as helpful footer an Oracle DB. I'll try to post the first half of working code later tonight. In the mean time Google "RCTE" or recursive common table expressions for lots if examples.
&gt; Glad it worked for you! Sometimes in the heat of battle all you see is smoke Nice. I like that. I'm going to have to remember that phrase.
Wow, this is awesome. My wife and I are both heavy data users, maybe I can convince her to take this with me.
Great source, thank you! I've exhausted my options at Community Colleges, and was looking for a "next" course, even if half is review, I have a feeling it'll be taught from much more of an academic standpoint, as opposed to "real world" applications. So that itself may be helpful for understanding certain concepts better. Looking forward to it, since I take these on my own (not for degree credit) to further my knowledge on SQL.
Thank you for posting this. Signed up for it and waiting for it to begin. ^_^
Do you think it would be worth doing this course to gain a good knowledge of using SQL to query databases?
I think so. It helped me as I was starting a job using SQL frequently. There is good coverage on joins too, which tend to be confusing to people. I'm not master, but I'm much more comfortable with it now. Just be aware that a good portion of the class will cover stuff like XML and database theory which aren't necessary for understanding SQL, but can be helpful.
When I do that I get the error "Incorrect syntax near the keyword 'AND'. "
It might be a bit more than you're used to working with, but I can't recommend Bonfire enough. It's a great PHP framework that's build on CodeIgniter that makes building out a backend almost painless. http://www.cibonfire.com
 WHEN #Client_Diagnosis."Birth_Date" BETWEEN ((GETDATE()-2190) AND (GETDATE()-4379)) THEN '6-11' ???
Sounds good. I work in data analysis and really need to get more comfortable working in the back end of the database, so sounds like this course will be helpful in lots of ways.
Still getting "Incorrect syntax near the keyword 'AND'"
Meh, SSMS is pretty handy if you write lots of queries. 
To each his own. But if you tell me you don't capitalize your keywords, I'm not touching your code. That shit bugs me.
I would say your logic is wrong. &lt; and &gt; are mixed up. SELECT #Client_Diagnosis."Client_ID", CASE WHEN #Client_Diagnosis."Birth_Date" &gt;= GETDATE()-2189 THEN '0-5' WHEN #Client_Diagnosis."Birth_Date" &gt;= GETDATE()-4379 THEN '6-11' WHEN #Client_Diagnosis."Birth_Date" &gt;= GETDATE()-6569 THEN '12-17' ELSE 'Older than 18' END AS 'Age_Group', #Client_Diagnosis."Diagnosis" FROM #Client_Diagnosis The earlier cases will shortcut the evaluation, so you don't need BETWEEN. Also, I removed what appeared to be an extraneous underscore.
try this SELECT #Client_Diagnosis."Client_ID", CASE WHEN #Client_Diagnosis."Birth_Date" &lt; (GETDATE()-2189) THEN '0-5' WHEN #Client_Diagnosis."Birth_Date" BETWEEN (GETDATE()-2190) AND (GETDATE()-4379) THEN '6-11' WHEN #Client_Diagnosis."Birth_Date" BETWEEN (GETDATE()-4380) AND (GETDATE()-6569) THEN '12-17' ELSE 'Older than 18' END AS 'Age_Group', #Client_Diagnosis."Diagnosis" FROM #Client_Diagnosis 
I believe so yes. It's the syntax that keeps tripping me up. 
add having count(a.email) = 1 between the group and order by statements. If that doesn't work try adding distinct. also won't make much of a difference but will make the code more legible, move the last two on statements into a where clause. Hope that helps. 
that's perfect. The next thing I have to figure out is to plug that into an update statement so I get the unique emails back. Thank you.
Generally &lt;&gt; is used when it a binary comparable data type(ex integer, boolean) and != is used when it is a complex type where additional conditions are applied(ex string, xml). This is only convention for clarity though, I don't believe there is an actual difference.
I'd go with DELETE FROM TABLE_FINAL WHERE WhateverYourKeyIs in (select WhateverYourKeyIs FROM TABLE_LOAD) This work for ya? *Edit to add a space.
upvote for keyboard ergonomics analysis standard sql makes your sql more portable (if that matters)
why not just update the record?
My intention was to try and eliminate any user error. This process requires copying and pasting data from an excel spreadsheet into a SQL editor. If there is any overlap with the copied INSERT statements, I would want to fix it. That being said, this was the path I went down, but any other ideas are absolutely welcome. It's Friday afternoon and my brain is almost done.
I'll leave the discussion of the sanity of what I'm doing for another time :) The UPDATE makes sense, but how would I then only INSERT the rows that were not part of the UPDATE process?
Tried that. DB2 didn't like the syntax.
That's my mistake. I should have included that from the beginning. Unfortunately, I can't seem to edit the title now.
I'll give that a try! Thankfully I'm done for the weekend, so this is Monday's problem :)
This! Null is in fact VERY important, and a very valid value! 
Stalker
no, city would not be a foreign key in any of those tables, primarily because you don't appear to have a cities table to reference as for your other questions, what have you tried?
http://pastebin.com/HB8pAGn8 paste bin for it in a neater form (at least I think) . Think it looks correct? 
&gt; A) select SNO from SPJ Where JNO = 'J2' Order by SNO ASC; this returns supplier numbers for parts supplied to SAnalyst project, so i'm not sure which question this is trying to answer you originally posted six unnumbered questions, then provided eight attempts, a) through h), so if i try to match them up, your first question is matched to attempt c) and c) looks like it's the right query one nice thing about this homework assignment, they've given you actual data, so you must actually run your queries, to confirm that they produce the desired results
Even I am beginner. I will try to answer 1) Get Part numbers for parts supplied by a supplier in LA Select spj.pno, parts.pname From Supplier su Inner join parts on parts.city=su.city Inner join spj on spj.sno=su.sno Where su.city='LA' I could try more.
I'm a bit confused to be honest :) The reason I say that is if I run my query I get the expected results. Would you be able to provide test data that meets the conditions above? Also, please run the following query to get your Oracle version: SELECT * FROM v$version; 
The query above pulls any record that has a reference number with RE/RW status on table 1 and a non-RE/RW status on table 2. *I want to pull any id from table1 where all reference numbers with a RE/RW status for that id have a non RE/RW status on table2. *So if you have 3 reference numbers with a RE/RW status on table1 but only one of those reference numbers has a non-RE/RW status on table, I do not want to pull you. If I have 4 reference numbers with with RE/RW status and all 4 of those have non RE/RW statuses on table2, I want to be pulled by the query. Does that make sense? I've included another example: [Example 2](http://i.imgur.com/NQITA.png) My version is 11.2.0.2 from the query above.
A quick Google search brings up http://www.youtube.com/watch?v=UeJKioNqe5w - I couldn't comment on the quality though as I learned SQL prior to the advent of the web, let alone YouTube. There is a link on the subreddit sidebar to the right for some SQL learning materials which help.
Knowing how to hand write them is pretty much essential if you're going to claim to be "knowledgeable" on a resume. You should know simple queries, how to use JOINs, using nested queries: select id, name from employees where department in (select department_name from departments where department_num &gt; 110) order by name; Since you're using MAMP, look for a "Learning MySQL" book and follow along its tutorials from the command line. 
as the guy there said, for 5) you need GROUP BY, as well as SUM() are there any solutions that you still need help with? reason i ask is, the data set is small enough that you can easily verify the correctness of your solutions by testing them 
Or inner joins for that matter 
Actually on testing again, they are the same. I was messing with different scenarios, must have done one with one set of options and one with the other. 
perhaps the reason you got no replies in 2 days is because this is obviously a homework assignment and you don't seem to have made an attempt give us something to critique, because we aren't going to do it for you
This is a good question. Our "users" are an internal group only. This app is not accessible to the outside world. These are our data entry people. If someone wants to fuck us they have to be sitting in the office with the other data entry people, and even if they manage to delete everything we have a backup from 10 minutes ago. xcombelle is correct - although since I posted this I have found out that I only have to escape brackets [] and underscore in the incoming data since those are the only wildcards that are contained in our lookup list. That should not give me too much of a performance hit when the stored proc runs against the user input. I will know more tomorrow when I can test it. 
I'll be looking at advertising data sets and helping advertisers optimize their campaigns. 
Thanks man, your examples sound easy peasy, and i'm just one week into learning SQL!
Seems to be more on the advanced end, but thanks for the heads up, I do want utilize best practices as i'm progressing.
&gt; This app is not accessible to the outside world. These are our data entry people. You two clearly haven't met, so please let me introduce you to [little Bobby Tables](http://xkcd.com/327/). Whether someone is intentionally attacking your database, ignorantly supplying bad data not knowing the impact, or are unluckily fat-fingering the wrong string of keys is completely irrelevant. If you're not worried about sanitizing input data because you trust your users you are doing it wrong. There are plenty of good reasons to sanitize your input (and it's as easy as using bind variables in most cases) and very few good reasons not to (as in so few good reasons that I'm quite positive you haven't happened upon such a reason yet). TL;DR - Sanitize user input if you're planning on sending it to your DB for processing. No, you don't have a good reason not to.
Don't be so paranoid about that ... I have a few clients asking these security concerns ... etc etc. I told them that their network is private and their users can connect to their server directly ... all of them know SA password and Server Password .. I know these are bad practice but that's not my company, I'm not their boss. I refused to do any modification on my part. I know their data are secure into certain extent and I can't provide any security while everyone can jump on the server anytime they want. It's sound like they are asking me to make a secure gate while they have no wall/fence. 
I shudder at thinking we're at an age when departments are no longer doing production work with duct-taped together Excel and MS Access databases to giving a slew of people direct access to an SQL database. Proper practice, IMHO, is *someone* at the company should be developing a frontend to this SQL database that will allow you to get those queries you want without having to install SQL Management Studio on your desktop (costly) and giving a user direct access to an SQL database (query or not). I think I understand where they and you are coming from but from my experience and standpoint it's bad business practice. SQL access should be left to the DBAs and developers which to me seems like you would be doing neither.
I'm fairly disappointed that nobody has mentioned **PowerPivot** (link: http://www.microsoft.com/en-us/bi/powerpivot.aspx); it's a free plugin for Excel that basically acts as an intermediary between the database and you. Great features that are designed for number crunching in marketing, easily powers out data analysis on millions of rows and integrates beautifully with the charts/etc. - A little SQL knowledge won't hurt, but this is definitely better suited for the type of job you are describing. If you are interested there is a $10 book that starts at the basics and works its way up called **DAX Formulas for PowerPivot: The Excel Pro's Guide to Mastering DAX** - Link: http://www.amazon.com/gp/product/1615470158/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=1615470158&amp;linkCode=as2&amp;tag=sq0f-20
Thanks - I'll check that out! It does look fairly similar to what I've used before, although it'll be tough to compare against the free version, which seems to lack some big features that I use regularly.
Is the stored proc pulling data from the excel workbook, and creating a table on the server? Or are you simply using the workbook as a catalyst to call the stored proc?
Yup. Stop using so many points of data manipulation. Let's go over this: &gt; to execute a stored procedure on SQL Server 2005 which creates a recordset of about 10 columns by 40K rows. So, we're using a (point 1) domain specific language (VBA) interpreted from inside a document (Excel, point 2) to manage calling a procedure over ADODB (point 3) on an external system that creates a temporary recordset/table (point 4), and populates around half a million pieces (point 5) of data, then takes that and shoves it off back to VBA (point 6) to be populated in Excel (point 7). Vs... &gt; if I create a table which is 10 columns by 40K rows and link it to the same stored procedure using OLEDB, it only take about 5 seconds to refresh. I am assuming you are using OLEDB to pull the *static data* from another Excel workbook or something. That way, you are skipping points 3 through 6. Of course it's going to be loads faster. There is no real way to get around this. You can move over to real programming environments (Visual Studio Express, for example) to get a little better performance, but you'll still be going through the process of creating your data set and porting it, which will still take time. Hell, most of Visual Studio knows how to interact with Office documents, so you should be able to write a program that points at an Excel spreadsheet, points at a database, and then does the manipulations needed directly to the Excel spreadsheet without ever having to launch Excel or interpret the Office environment. You could take the data generation out of the database and put it directly into the environment you are working with, as working with processes in memory/on chip is much faster than shoving it from databases through .NET architectures. Also, reading from files has always been faster than reading from databases; relational systems have a bit of overhead related to organization of the data. I don't suggest this though; usually there is good reason data is in a DB. You could stop using Excel altogether and write a Visual Studio application that just outputs CSV's with your end data set after all is computed. That would save you time. Or you could accept that using static data will always be faster than creating it, passing it, and wrapping that communication session up. Also, hate to be that guy, but that guy must say: Get off of Micro$oft dependent platforms. It will hurt in the short term but will save you time, money, and computons down the line/long term. 
Also, stop using a document (Excel) as a programming environment. It is bad idea for maintainability and inter-polarity and your personal sanity (IMO).
The latter. I'm pulling data off the server into the workbook using the SP.
Thank you for the response. I understand and agree with most of your points. However, since I'm so new to SQL and VBA, I'm mainly trying to understand why certain things work the way they do rather than build new MS-independent processes. This procedure via ADODB isn't creating any more tables on the server than the Excel table using OLEDB is. Both methods are executing the same procedure, yet one takes so much more time than the other. If I exec the same SP directly from SQL Mgmt Studio, it takes 2 seconds flat. Are recordsets in VBA inherently unwieldy or is there another [solvable] reason for the lag?
Ah, I misunderstood you. Future posts for help should be very specific; keep in mind we cannot surmise your intentions or reasons. Recordsets, in general, are inherently unwieldy, mostly due to the fact that they are static data with dynamic addressing (recordset.next() anyone?). Addressing data of a database type sort can be done just as well with an array of arrays. I do not think you understand what you are dealing with though, if you were to use OLEDB over ADODB on the same objects and expect equal performance. Please go to http://stackoverflow.com/questions/3766433/what-is-difference-in-adodb-and-oledb and read that. Also, read the wiki articles on both. ADODB is a way to talk to M$ databases. It returns a set of data that is very specific to it's use, which must be interpreted by the environment/programming, which just happens to be the OLEDB API. VBA compiles to interpreted code (P-code), meaning that it is not rendered down to binary/executable, but rather, is rendered down half way, and interpreted dynamically (translated to machine instruction) when it is called instead of before. So already we have to do two dynamic interpretations- this will be slow. OLEDB is a way to talk to any datasource in general, whether an Excel spreadsheet, Oracle DB2, M$ SQL, etc. Note that the two environments we are working with, data wise, are Excel and M$ SQL, and we are using an interface that understands both those formats in both a read and write fashion. It is part of the COM libraries used directly by both Excel and M$SQL. By using ADODB, you are running an additional layer of interpretation that is pushed to OLEDB to be processed. By using OLEDB, you are skipping ADODB. My guess is that the ADODB system you have set up has some process where it must iterate over all the data (half a million data points) before it is handed out to OLEDB. OLEDB is probably requesting and providing the data to be presented in a format that doesn't require that iteration. If you are new to VBA, I suggest you stop now anyways. VBA is VB6, extended to interact with the M$ Office environment. VB6 is archaic and difficult, at best. I really suggest you move to Visual Studio. Visual Basic in Visual Studio understands what a MSSQL db is, understands what a M$ Excel document is, and can do all the things you are looking to do, probably a fair bit faster. None of this is M$ independent, it's just using modern tools. SQL is not your bottle neck. I guarantee it. It is the processing and/or transport of the dataset.
This is very helpful, thanks so much! I'll read through the stackexchange article as I wasn't aware of the major difference b/w ADO and OLE, and will be looking into Visual Studio as well.