figured u could do it either way~ i had not in at first, but figured not exists works too~
Think of the subquery as a separate step in the middle of wherever command you place it. Get in the habit of running it independently first so you know what results it generates. Seeing that data will make it easier to place the subquery in the context of the overall query. It will also make you more confident in using subqueries.
yep, I did skim through his channel yesterday. I'll be taking in a deeper look today.
&gt;SQL Queries for Mere Mortals. I just downloaded it. I'll use to go over more advanced topics that I am not too familiar with. Thank You!
&gt;t should look like "JOIN dim\_site\_asset dsa USING (asset\_id)". Thank you for catching that I omitted dsa. That one completely got by me. I tried to validate it after correcting that and it still gives me the same error
Maybe check [this](https://www.postgresql.org/message-id/16133.1201484717@sss.pgh.pa.us) post out....it might have to do with how you are doing your JOINs (with the USING).
ah, i missed the fact that you're using USING first of all, that error message doesn't make sense -- there's no way a single table can have the same column name twice secondly, please don't use USING -- spell out which columns to use in the joins
So I should be using ON?
Also, I agree it doesnt make any sense I've been stumped for two days and I couldn't find any help that made sense online
How many user requests and transactions are you expecting each day? As an idea calculating live with 100k+ daily requests over 12 million users and 750 million+ transactions does not even fall into the areas to optimise.
I don't see why you would need to do it differently. Once the cron pattern is confirmed schedule the recurances. If you look at very large scale systems the tickets will be pre generated then just allocated. (Can be a pain to change after publishing and in some cases this is not allowed). At lower scale just add the ticket at the point of selling it. This will scale easily to 10k a minute.
yup, i would try it and see if your error goes away i mean, it's not as bad as using NATURAL JOIN, but i still prefer to be in control of which columns are being joined
&gt;n, it's not as bad as using NATURAL JOIN, but i still prefer to be in control of which columns are being joined Im not entirely sure how to use ON yet, but I'll figure it out.
change this -- FROM dim_asset_group_asset daga JOIN dim_asset da USING (asset_id) JOIN dim_site_asset USING (asset_id) to this -- FROM dim_asset_group_asset daga INNER JOIN dim_asset da ON da.asset_id = daga.asset_id INNER JOIN dim_site_asset dsa ON dsa.asset_id = da.asset_id i added table alias `dsa` to follow the pattern i added INNER just because
Does this work? on ci.PGM\_ID = 'CIS' and ci.PGM\_ID = cp.PGM\_ID and ci.PLAN\_ID = cp.PLAN\_ID AND (ci.IMEI\_VAL is NULL AND ci.ESN\_VAL is null)
i think what you're missing is that a donor can make 3 donations in 2009, 3 donations in 2011, and 4 donations in 2014, and therefore still qualify the count
No I discovered that quickly enough. What I can't wrap my head around is how to grab the people who have donated in consecutive years and make it repeatable for the next reporting period(likely 3 or 5 years down the road).
 SELECT c.donor_ID , c.Contribution_ID , c.Donation_Year FROM ( SELECT donor_ID FROM dbo.Contributions WHERE Donation_Year BETWEEN 2009 AND 2018 GROUP BY donor_ID HAVING COUNT(DISTINCT Donation_Year) &gt;= 10 ) AS cnt INNER JOIN dbo.Contributions AS c ON c.donor_ID = cnt.donor_ID ORDER BY c.Donor_ID , c.Contribution_ID
As someone who probably is familiar with a variety of database systems, what do you think of MS SQL Server vs MySQL? I'm running into frustrations with MS SQL when trying to get this created and I don't recall such frustrations with MySQL. I went with MS as we're a Windows environment and I figured it'd be easier to integrate into the existing SQL server we have. But the minor issues are very frustrating. As an example, I'm experimenting a bit and trying to add a column to a table that's NOT NULL. But the server keeps telling me that that is not allowed. Yet I was able to do it no problem with the MySQL server I was playing with a few months ago. Any thoughts as to which one to use/focus on? Esp as a beginner.
To make it more flexible for all year ranges, try the following: ...WHERE Donation_Year BETWEEN &amp;Start_Yr AND &amp;End_Yr GROUP BY donor_ID HAVING COUNT(DISTINCT Donation_Year) &gt;= (&amp;End_Yr - &amp;Start_Yr) + 1...
I think you would be better off using CASE statements for your Segment column counts and not creating such an elaborate derived table. You can aggregate within CASE statements with something like SUM(CASE WHEN Segment = 1 THEN Segment\_Column END) as S1\_Count.
So is the requirement: **Report Donor If:** 1. Made a donation &gt;= 1 time per year 2. Have made donations consecutive yearly donations If so, I think this will work for you. BTW, Based off of the # in front of the table name, I assume you are using SQL Server / Transact SQL, so that is the flavor the below is written in. I haven't tested this, but it *should* work, if not, it certainly should point you in the correct direction. &amp;#x200B; `/********************************` `* CREATE DonationCount Temp Table` `*********************************/` &amp;#x200B; `CREATE TABLE #DonationCount(` donor\_id INTEGER, /\*assuming this is a numerical primary key in the Contributions table\*/ donation\_count INTEGER `);` &amp;#x200B; `CREATE INDEX idx_donor_id ON #DonationCount(donor_id); /*The index may or may not speed up the script, I am not really sure what your data looks like, or how much of it you have*/` &amp;#x200B; &amp;#x200B; &amp;#x200B; `/********************************` `* Populate DonationCount Temp Table` `*********************************/` &amp;#x200B; `INSERT INTO #Count(donor_id, donation_counts)` SELECT DISTINCT C.donor\_id COUNT(DISTINCT C.donation\_year) FROM Contributions C; &amp;#x200B; &amp;#x200B; &amp;#x200B; `/********************************` `* Declare &amp; Set Year Vars` `*********************************/` `DECLARE @START_YEAR INTEGER` `DECLARE @END_YEAR INTEGER` &amp;#x200B; `SET @START_YEAR = 2009` `SET @END_YEAR = 2018` &amp;#x200B; `/********************************` `* Generate results set` `*********************************/` &amp;#x200B; `SELECT` C.donor\_id, C.contribution\_id, C.donation\_year `FROM Contributions C` INNER JOIN #DonationCount DC ON DC.donor\_id = C.donor\_id `WHERE DC.donation_count = (@END_YEAR - @START_YEAR);l`
I think the 762 or 767 have a chance of being the most applicable for you. The 762 is pretty general, I'd say anyone who works with SQL Server would benefit from 761 / 762.
If you just see JOIN, then that implies INNER JOIN in SQL. Yeah, outer joins are difficult to understand at first. I began using LEFT OUTER JOIN more once I understood them better.
Have you tried /r/relationshipadvice ? :p
You can try something like this: select c.Donor_ID, count(distinct c.Donation_Year) as DistinctDontationYearCnt from Contributions as C where c.Donation_Year IN ('2009','2010','2011','2012','2013','2014','2015','2016','2017','2018') having count(distinct c.Donation_Year)&gt;=10 That will at least give you the donors that have 10 or more distinct years. If you want additional donor info in the results, you can use the above in a subquery: select * from Contributions as C0 where exists( select c.Donor_ID, count(distinct c.Donation_Year) as DistinctDontationYearCnt from Contributions as C having count(distinct c.Donation_Year)&gt;=10 and C0.Donor_ID = C.Donor_ID )
If I am correct you need a stored procedure. I'm assuming SQL Server / T-SQL? &amp;#x200B; Okay first you need to link to the target server with a stored procedure: &gt;USE master; &gt; &gt;EXEC sp\_addlinkedserver &gt; &gt;@server='DBServer', &gt; &gt;@srvproduct='', &gt; &gt;@provider='SQLNCLI', &gt; &gt;@datasrc'localhost\\SQLExpress'; Querie the tables from the other server using *fully-qualified object names*. &gt;linked\_server.database.schema.object &amp;#x200B; &amp;#x200B; I hope this helps! I guess from there you could make some views? I am sort of understanding what you're asking but I do know that in order to link another server you use that stored procedure (change the settings to fit your server) and then you can talk to the linked server by Querying the target servers fully qualified object names. Think of it like typing exact URL to a target location remotely. You gotta point directly to it.
I'd be in a very select group, usually people hate me for shooting holes into their "architecture" ;)
That is awesome. Simple queries like this I can wrap my mind around - I’ve begun using sql at work to simply and streamline a lot of reporting and data gathering. Our ERP will show the SQL on each screen - but there are 15-20 joins in the SQL info so it can look really intimidating. Is a right outer join a thing? Would that show you all persons without a pen?
And by the way - I really appreciate you two for taking the time to help me. You are a good teacher and make material extremely absorbable.
Right and left outer joins just depend on the order of the tables in the FROM clause. This is essentially the same query and should give you the same results: SELECT Person.name as PersonName, Pet.name as PetName FROM Pet RIGHT OUTER JOIN Person ON Person.id = Pets.ownerid Do you see the difference? One tip is to try not to mix left and right joins in the same query. I think most people in this sub prefer left joins. There's been threads about it before.
Oh, Dad...
The problem is that NOT EXISTS is not an equality like you have it written. The equality only works with NOT IN. NOT EXISTS uses the correlation in the subquery.
ah shit -- yep -- you're right do NOT IN :P
Seems like a lot of complaints about the differences with the way different rdbms's arent standardized.
You have to alter the column to *allow* NULL first, since you defined it with NOT NULL.
Right that makes sense, what's the syntax for allowing null?
I found [this answer on Stack Overflow](https://stackoverflow.com/a/51662660/1324345) But what kind of busted backwater is Netezza that it doesn't even have a `REVERSE()` function?
ALTER TABLE \[table name\] ALTER COLUMN \[column name\] \[data type\] NULL;
Your subquery needs to be in the select part of your statement, not the where clause.
Ive fixed that but I still dont get any kind of output that helps me.
no, it doesn't
 SELECT customers.name , quantity.quantity FROM customers LEFT OUTER JOIN ( SELECT orders.customerID , COUNT(*) AS quantity FROM orders NATURAL JOIN ordercontents GROUP BY orders.customerID ) AS quantity ON quantity.customerID = customers.customerID ORDER BY customers.name
https://xkcd.com/927/
I dont think it really matters which version of pgAdmin you download...
Thanks, I still dont really understand it very well but I'll look over it until it sinks in. Thanks for your trouble.
sounds like you need a database server...
For anyone else who seens this and has never heard of the website and thinks it might just be a weird spam link, it's not. It's a real article just with a dodgy looking link.
[SQL Server Developer Edition](https://www.microsoft.com/en-us/sql-server/sql-server-downloads) is free.
Or just MSSQL Express
I can't tell without seeing the file in question. Try to make an [MCVE](https://stackoverflow.com/help/mcve) and post it.
30 someodd years married...if I need relationship advice, the Mrs will give it to me! LOL
or SQL LocalDB or Access (only half /s)
Or Excel or Notepad.exe
This intrigues me, and I have it running now... u/jacbryques solution was also able to help. I'm learning SQL on the job as a need was identified. I'll let you know how it goes. :)
Um, No...? As long as it returns a value that's Legal it's fine. Subqueries are used everywhere. Also, they were instructed to fill in the Blanks. The WHERE clause was incorrect anyway, regardless of your misinformation.
This seems too complicated for my needs, and I want something that is 100% local, so nothing connected to the internet or the cloud
&gt;some connections stays open on the database and it's preventing us from doing some automated tasks An open but idle connection should not interfere with _anything_. I have to ask - what are your automated tasks doing, and what makes them so fragile? Your bi-weekly tasks should be built such that they can coexist with other users. How will you prevent connections from being established mid-process? If you really must cut everyone off, do it via firewall rule and don't even let them reach the instance.
It is local.
Careful about limiting yourself. It may be too complicated now but as you grow in your knowledge, the full installation will become necessary. SQL Dev Edition is free and will not take DBA skillset to install or even manage. It will patiently wait for you to catch up to what it can offer. :)
What I need is really simple, and I don't intend to go much past that for now. Just the installation of sql server was painful and complicated. It took me about an hour to try to install, and I couldn't even open it because I couldn't find the exe file.
Your question is absolutely legitimate and I must say that I worded my intro poorly. The tasks are not run on the server directly but trough the application (we schedule tasks trough the application). At one point, we are running a task that checks if there are still connections from the application (doesn't care about other connections) and it can't run if there are idle connections from the application. I should have worded that and I apologize. I'll edit my post to reflect that. So my problem is that there is not supposed to have connections (even idle) at a given time, every other week (we prevent starting the application and force quit). However, during the day, some connections sometimes stay up (because of crashes and such), so I would like to kill them before the application automatically runs the tasks. I would have to kill the connections directed at a certain DB with a certain name.
Why are you using `NATURAL JOIN` when a proper `INNER JOIN` will be perfectly fine and leave no ambiguity?
Do you have any triggers on the table?
What are you actually trying to accomplish? Is there a particular reason you aren't using something like LucidChart or Draw.io (or pen and paper for that matter) and then building the SQL out yourself? Do you already have a SQL isntance? (It isn't clear why you need this)
Everything was flagged when I did that.
&gt; am I getting distinct values of A, or am I getting distinct combinations of A, B, and C? the latter i would be interested in seeing any data that you have that contradicts this
No idea how to tell.
This article should help: [https://docs.microsoft.com/en-us/sql/relational-databases/system-stored-procedures/sp-helptrigger-transact-sql?view=sql-server-2017](https://docs.microsoft.com/en-us/sql/relational-databases/system-stored-procedures/sp-helptrigger-transact-sql?view=sql-server-2017)
Download Microsoft's SQL Server Managment Studio, choose the one that comes Sql Server developer edition. This is exactly what you are asking for
This is the query I'm using. SELECT DISTINCT DueDate, PortfolioID, PaymentCycleDay, proxyGroupName, AddressType, AddressID, PrimaryAccount FROM MorningMoneyMovementStage WHERE (AddressName Not Like'%auto debit%') AND (AddressType &lt;&gt; 'Check') OR (PayingEntityID in('802426', '802623', '802552', '802546') and AddressType = 'Check') It's returning a duplicate row. Note: there are many more columns in the table than are being selected. Can you see any problems with what I've written?
I'm just trying to create an easily searchable table(Can search with sql commands), with multiple values in some columns. I have created some sql code in notepad++, and all I need is a tool to actually see the tables I have created.
Cool! trigger_name trigger_owner isupdate isdelete isinsert isafter isinsteadof trigger_schema tr_CrmSync_TW80514 dbo 1 1 1 1 0 dbo twSBInsertTW80514 dbo 0 0 1 1 0 dbo twSBUpdateTW80514 dbo 1 0 0 1 0 dbo twSBDeleteTW80514 dbo 0 1 0 1 0 dbo TW80514_ChangeTracking dbo 1 1 1 1 0 dbo The results make sense here. Update the table, update CRM, update change tracking. Still can't see what's changing 13,000 records.
&gt; Can you see any problems with what I've written? well no, of course not, not without seeing your data if you think there is a duplicate row, there isn't please compare **every column** in those two rows and you'll see i'm right
Well that's the issue. Of course every row will be unique if you examine every row in a table because there will be a Primary Key. The issue I'm seeing is that the above select statement is returning duplicate values. I'm not sure how sensitive this data is so I'm not comfortable posting the raw data to this forum.
Excel. Ctrl+F. You're asking for a simple version of a complicated solution when there is a far simpler way.
i tried to clarify with an edit to my previous comment but i was too slow you don't have to actually show me your data here's an idea... change this -- SELECT DISTINCT DueDate, PortfolioID, etcetera with this -- SELECT DueDate, PortfolioID, etcetera, COUNT(*) as howmany and then add these two clauses at the end -- GROUP BY DueDate, PortfolioID, etcetera ORDER BY howmany DESC and tell me if you get a count higher than 1
You get distinct row combinations not distinct values. You won't get duplicate rows but if they're different combinations you'll see them. even if you want Distinct. There is a DISTINCT ON A good Example is: Select DISTINCT A,B,C FROM Table will return A | B | C B | A | B B | B | B &amp;#x200B; So your Columns have duplicates, but the Duplicate Row combinations will be removed. Like another BBB or BAB.
I want a more advanced way to search than just ctrl + f, and I need to pay to use excel. With libreoffice as an alternative I'm not aware of a way to store multiple values into one cell
ssis is the answer
I still think that this application and its scheduled tasks are far too fragile if all users _must_ be locked out while they're running. You still have the problem of users "wandering into" the database mid-process. How will you control for that?
If I had to guess, your query is triggering other updates/inserts which are also triggering other updates/inserts OR one of those triggers on that table is updating a bunch of records in other tables. You'd have to look at the triggers on those tables to see what they're doing. If you're using MS SQL Server and have SSMS, open the database, go to the table in question, then look under triggers. You can see the definition of a trigger by right-clicking it and clicking Modify.
Log shipping doesn't do a great job of outputting a good error message. Go into your SQL Agent Jobs for both the COPY and RESTORE job. They will have a CMD.EXE line that you can run. Copy, paste it into a CMD.EXE window on the same server where the Agent job runs and you'll get a verbose output that will help you diagnose the problem. More than likely you have a log chain that's been broken due to the following: 1. You have another LOG backup job, either through a SQL Agent job or maintenance plan that is running in parallel with the Log shipping backup job. Since these aren't COPY ONLY backups by default they will truncate the active TLOG for the database and require a full LSN chain to be able to restore to the log shipping target. 2. You *HAD* another LOG backup job, either through a SQL Agent job or maintenance plan and the LSN's aren't lining up because you haven't full prepared the log shipping target database. Find these TLOGs and restore them and try the Log Shipping restore again. Post the output of your CMD.EXE RESTORE if you're still having trouble and I can help further.
looks good, going to try to replicate this over the weekend; will PM if I run into any problems.
This is Excel being colossally stupid in its attempt to be "helpful". * In SSMS, save the results of the query to CSV file * Open Excel * _Import_ the CSV file (or open it as CSV) * In the wizard, tell it that the column with leading 0s is `Text`, not `General` * Finish Or come up with some automation that can tell Excel that the column is text instead of leaving it to its own devices.
You can run sp_helptext [trigger Name] to see what code the trigger is running (if you do not have access to the object explorer)
It really sounds like you want a solution that doesn't exist.
Can you just preformat the column as text and then paste?
Putting an apostrophe at the start of the field is excels way of knowing to keep the preceding zero
Hey just an update, this worked perfectly I actually didn't think to just join like that. Thanks so much that solves a huge headache of mine
You may think they are the same, but I guarantee there's a difference, even if you can't spot it. Maybe one is `example` and the other is `example ​`. Note the trailing space. Maybe 1 is `example` and the other is `exa​mple`. Note the zero width space between the 'a' and the 'm'. Get the record ID's of the ones you think are duplicates, and try to find which column it thinks is unique.
^This
What do you mean by pasting/importing? From memory, if you copy and paste, Excel will mess up the formatting, but, if you import the table from SQL (data - get data - from database) it won't.
You can also pad in excel-text(a1,”00000”) To pad 5 up to 5 zeros. For a zipcode.
It exists with the sql language, I just need a tool to visualize the table. http://sqlfiddle.com/ works almost exactly as I need, I just need something for offline use
No problem, glad I could help! I've been doing SQL work for a couple of DECADES now, so I know a thing or two. Honestly, if you need that type of join data frequently, you should put it in a View and use that instead of the base table.
Microsoft Access? Or any database software and a client suitable for it? &gt; I don't need whole server and database systems that most sql visualizers seem to come with. All visualizers come with a server, how else would they work? SqlFiddle has database servers behind it. You can't visualize or fiddle with SQL without having a real database for it to operate on The best you're going to do here in terms of "minimal install" is some kind of file-based database like MS Access/Libreoffice Base, or SQLite
Why aren't you using Libreoffice Base? You can create tables and then run SQL against them
MS Access, LibreOffice Base, or an SQLite editor, would all fit I'd have thought?
You know what it may be due to a bug in the software I'm using as I am not querying the DB directly. I'm querying a staging table, but not directly. I'm using software that interacts with the staging table.
You know what it may be due to a bug in the software I'm using as I am not querying the DB directly. I'm querying a staging table, but not directly. I'm using software that interacts with the staging table.
select the column in excel, format it as text, then paste your results. works every time.
looks like you got some cascading going on.
[https://rextester.com](https://rextester.com)
Lots of good answers here. I do this kinda thing on the regular. When I open up a clean spreadsheet to dump my data into, I make ALL the columns TEXT and then go back and adjust to other formats as necessary. That works for my particular flavor of data but it may not for yours. It is just seems a hellova lot easier than trying to format each column prior to pasting the data in. All Text formatted, dump data, reformat as necessary.
Copy the query results, go into Excel, click the arrow under the Paste button, use text import wizard. Follow the instructions, you'll eventually get a a screen where it'll ask you what format the columns are in. Make sure anything that has leading zeros is imported as text.
Your business logic doesn't account for 30 year olds, so I randomly allocated them to the name sorting cohort. &amp;#x200B; ORDER BY CASE WHEN AGE &lt;= 30 THEN NAME ELSE (AGE \* -1) END ASC, CASE WHEN AGE &lt;= 30 THEN AGE ELSE NR\_OF\_CHILDREN END DESC
I never knew that was an option. I just tried it, however, and the output from the sql queries are kind of hard to read, is it possible to view it in table form?
I'm not massively familiar with Base (I just suggest it because you mention Libreoffice) but I'd assume that like MS Access, you can do something to prettify results - is there no option to populate some kind of form?
**DO NOT EVER USE NATURAL JOIN IN REAL-LIFE WORK!** Thought I'd drop that nugget instead of answering since it looks like you already have some correct answers. Natural join has no place in real-world applications. Pass your quiz and forget you ever heard about it. 1) Natural joins are hard to read. You have to know the joined tables' structures to really understand what the join is doing. 2) Nature joins can, and given Murphy's law, eventually *will *, behave unpredictably. What happens when someone adds a new column to one of the tables that just happens to match the name and datatype in the other table, but is actually a different concept? What happens when one of your columns is renamed? Chaos!
You shouldn’t be putting multiple values in a cell in SQL table either. That is a violation of normalization.
As far as I can see, there doesn't seem to be any options beside showing it in the output field https://i.imgur.com/E4qNZRL.png
How else do you suggest I build the kind of dataset I want? If I want to list the genres of something, it often has more than one genre
That’s not one cell. That’s one column. You can have multiple values in a column but you should not on a cell.
As other people have suggested, install Sql server express. It installs Sql server locally on your machine. Then you simply create a database and the tables you want under it. It is free, and you can run it locally without a need to connect to network.
Say I wanted to create something like this https://i.imgur.com/f7K7CzI.png
Create a table Movie, then create three columns, and insert rows. All can be achieved within SQL Server management studio. And heck you don’t even need to write Sql for this.
Select your first row ordered by whatever column you use to determine what the "first row" is, remembering that you may not get the same exact result unless you're specifying an `ORDER BY` clause, then `UNION` those results to your `TOP 15` random query: SELECT TOP 1 [WebURL] FROM table WHERE [SiteUrl] = "https://helloword.com" ORDER BY [Column] UNION ALL SELECT TOP 15 [WebURL] FROM table WHERE [SiteUrl] = "https://helloword.com" ORDER BY NEWID()
In Excel, "Format Cells" -&gt; Text, then past the values. If you leave the cell formatting as either General or Numeric, the leading zeros will be removed.
friggin zip codes
The only problem is that I get an error using ORDER BY where the items must appear in the select list if the statement contains UNION.
It doesn't matter what you're querying. distinct will always only return distinct values.
Oh, dur. Sorry, you should be able to CTEs: with a as (SELECT TOP 1 [WebURL] FROM table WHERE [SiteUrl] = "https://helloword.com" ORDER BY [Column] ), b as ( SELECT TOP 15 [WebURL] FROM table WHERE [SiteUrl] = "https://helloword.com" ORDER BY NEWID() ) SELECT * FROM a UNION ALL SELECT * FROM b
You could throw it in excel and hit the data to columns option. Again I assume Libreoffice calc has something similar
You are beautiful. &amp;#x200B; Freakin' worked like a charm.
How do I do that?
Copy it from the output, open excel, paste it, then click Data (on the ribbon) and the option that’s called something like “Data to Columns”
Thank you, I managed to do it. A bit more convoluted than I would have hoped, but this might do
Bring it in with power query - you can set the data types properly which usually preserves the zeroes. If not you can use something like Pad.Start to fix it. Added bonus is that the table is then connected and can be quickly refreshed as needed.
I would split this table into three tables honestly. `units`, `squads` and `squad_battles`. Units holding the key identifier for each character/unit, squads being the aggregate that shows what units are in each squad and then squad_battles as the squad with its measurables. This allows you to pull stats on individual units easily as well - in case you have a unit/class/member that may be overpowered. (not sure if you're using these metrics for balancing or not). Other measurables that you may want to consider are total damage done, total damage taken by party members and time of each battle from start to finish - which is going to allude more to synergy in party characters than just total battles completed (a fluke battle may end in defeat for an otherwise highly synergized party).
I see, you don't actually want database visualizaiton (which most people would assume means mapping relationships between tables). You just want to have a database that has output. LibreOffice Base will work as well as SQLite (https://sqlitebrowser.org/ is my preferred browser for the latter). SQLite only needs a simple install and then all of your databases are stored locally; no listener involved.
Sadly, it's the application I have to deal with. We're not the ones making it. I'm not even sure having people connected to the DB is such a big deal while it runs the tasks, but the application DEMANDS IT, so I have to deal with that (for connections originated from the application only). It doesn't even look for other connections from other sources. Users can't wander into the database from the application during the tasks, the application is locked and informs anybody who tries that the application is off. Other sources are no concern, because the application doesn't care. So it's where I'm at.
Thank you, this sounds like what I'm wanting, I'll try it out later
You mean export the table from SQL using the wizard, and this can cause all sorts of issues depending on the structure. SSMS Tools has an export grid results to Excel which works really well for large sets of data and can break them into separate files. Had to do this recently on a few tables that were dozens of columns, and millions of rows, so that we could send them on a thumb drive to a university where students are doing projects based on our data.
If you know the first 6 many characters will always match, you can use ON a.col1 = left(b.col1,6) That'll work in SQL server at least. Also, don't expect good performance on this. Using functions in the join or where clause results in bad performance.
 SELECT a.Col1 , b.Col2 FROM tableA AS a INNER JOIN TableB AS b ON b.Col2 LIKE a.Col1 || '%'
Which rides see the most and least traffic and during which periods Which booth items are purchased the most and least and during which periods Which booths are most and least active and correlation with rides
Time vs staffing at that time
Check foreign keys in sys.foreign_keys. If you have On Delete Cascade.
- Wait times per ride at various dates/times of day - Customer "flow" per ride (# of customers per ride * (loading time + ride duration + unloading time)) - Total Park attendance across date/time (sliced hourly, perhaps) - Souvenir shop daily sales totals, with detail breakdown of inventory sold by category (T-shirts, hats, toys, knicknacks, posters, etc...) - Average vs expected downtime per ride, total and variance %
Okay I’ll try it again and see if I have any luck. Thank you for the response.
That won't work for the example given because Abc123 Ghi234 is NOT like Abc123EFG Ghi234567 due to the EFG in tableB.col2. For this specific example, it would have to be b.col2 LIKE LEFT(a.col1,1,6) || '%' AND b.col2 LIKE '%' || RIGHT(a.col1,6) || '%' But even that has the assumption that they always want a match of first six and last six characters in a.col1 being somewhere in b.col2, and *in that specific order* as well.
I recommend dbeaver insteaf of pgadmin, great tool
You've got multiple portions of a value (a.col1) that you are wanting to match up. The following needs to be addressed: - Is it always two portions of that value? Or could there be more? - Will those portions always be matched in THAT specific order? Or, for example, would "Ghi234567 Abc123Xyz" also be a match? - Is each portion always matched to six characters, or can that vary? I'm sure that there are several more questions I could come up with, but those came to mind immediately. The LIKE operator works on a single match criteria (i.e. - 'Abc123') at a time. It can have stuff - before it (matching to '%' || {value}), - after it (matching to {value} || '%'), - or embedded within it (matching to '%' || {value} || '%') Note: '%' represents Zero or more wildcard characters present. You can also use '_' instead for a single wildcard character in a given spot. Other wildcard options may be available depending upon your DBMS.
If you have time of rides, you may want to get rides/hour for each employee. Assuming a line, some employees may be more efficient at loading and unloading.
employees, rides, ride sales, ride shifts, booths, booth sales, booth shifts, restaurants, restaurant sales, and restaurant shifts. Over each quarterly period, what's the ratio of aggregate ride sales to employees working, and aggregate restaurant sales to employees working? Would it make sense to convert some employees to part time status? Full time? Cut some shifts? Expand rides? Expand restaurants? Expand booths? Cut booths? Populate the data however you like, and then come up with a big scary number that is the total permitting, leasing, wear and tear and insurance cost each quarter, which at the end of the year, represents how much this park costs to run. Optimize for profit.
For each ride, graph the average weight time by time of day (break it up into 30 minute increments). It will help visualize the "peak" hours of each ride and determine how much staff you need to dedicate to the ride depending on the time of day.
match up your SELECT columns with your GROUP BY columns first error -- `NM.FIRST_NAME ||' '||NM.MIDDLE_NAMES||' '||NM.LAST_NAME AS OWNER_NAME` is in the SELECT list but not in the GROUP BY list there are other anomalies
Yeah, it's Excel's "Oh, it looks like a number? I better format it like Double! Who cares if it's a 20 digit identifier that gets translated into scientific notation and truncated?" It really highlights that Excel is not meant for manipulation of arbitrary table data. You can cheat by right-padding the cell with some whitespace. In Excel's logic, `,1234567890,` is a number, but `,1234567890 ,` is text. You can also cheat by using an apostrophe as the escape character, so `,'1234567890,` is also text, but a lot of CSV parsers don't like that, so it'd have to be `,"'1234567890",`.
I just use a custom format of 00000.
read my solution more closely please `Abc123EFG` **is** LIKE `Abc123%`
Like 75% of business questions are "How much did that cost" and "How much revenue did that generate?" There are a lot of things that aren't revenue generating, your info booths, your janitors, whatever, everyone needs to know that. Saying a ride brought in $300k isn't useful on it's own; is that good? No one knows. You spent $100k running it, $150k on maintenance, and $75k on staffing. Is that ride good now? No one knows. The ride is just there to get 80k people/day to walk by the gift shop which has an astonishing 83% gross margin. A lot of business things lose money, because they are really just there to enable the lucrative part of the business. The cost is usually more important. 15% are "Is there going to be an issue?", do we have enough food for how much we normally sell, do we have enough staff, who reports to who, what division. This is mostly going back to the first paragraph. How much does it cost to fully staff the ticket sales for 3 hours? How many people come through in those first 3 hours? 5% are "Who is fucking up?" Who was late to their shift? How long was the ride down? We sold 50 advertising engagements, we only fulfilled 48 of them, we need to figure out a way to squeeze in another 2 mentions that Tesla actually is 30% cheaper than similar luxury cars over a 5 year life. 5% are the fun queries, where you are actually trying to analyze the data. Your first question falls into that category. It's mostly about costs and revenues, and of course, deal fulfillment. Tesla.
Sorry, got the a.col1 and b.col2 transposed in my brain.
To expand upon this, if the first Left(N) is ALWAYS the same, perhaps setting up a computed column in both tables which contain the first left(N) and joining on those would help with the SARGability.
remindmebot reminds you to come back and look at a post or comment. very useful.
Without having my SQL open I’d recommend trying order by case when age =&lt; 30 then name asc, age desc else age, nr_of_children end I can’t see your original code while commenting so may have got the asc/desc wrong but you get the idea!
Dude stop using netezza if it doesn't even have a built-in reverse() function
CTEs are almost always more readable than subqueries, and the optimization difference is negligible. You may save a miniscule amount of time by using subqueries instead of CTEs.
Just to add to my comment, looks like you may not have fully grasped case when statements yet, you shouldn’t need more than one at any point, it should be structured as case when ... then ... when ... then ... else ... end
CASE WHEN (All that stuff) IS NOT NULL THEN (All that Stuff) ELSE 0 AS whateverYouWant
Look into ISNULL. Select ISNULL(MYFIELD,0) Will return 0 when the field is NULL.
[SQL Server 2017 added a nifty STRING_AGG() function](https://stackoverflow.com/questions/194852/how-to-concatenate-text-from-multiple-rows-into-a-single-text-string-in-sql-serv/42778050#42778050) SELECT STRING_AGG(Filedname, ', ') AS FieldNames FROM ##KeyTable;
Didn't get the normal confirmation screen for the gold, let me know if there's an issue but thanks a lot. I figured there was a simple way, but a few Google's started to annoy me.
Thanks; sorry, meant to add that the gold was unnecessary. Just wanted to be helpful. :)
Why is this a group by? Are you trying to select distinct?
He has a SUM(field) in the select list. That's an Aggregate function that requires a Group By
Ah, I missed that. Thanks.
ForXML is what I've used before with Stuff() but I use so rarely the syntax annoys me.
I meant MySQL Cookbook. I am a massive beginner myself but I downloaded it as it was recommended to me many months ago when I thought of learning SQL. Where'd you find the mere mortal one?
Stuff works with way fewer lines but can also use cross apply if your in an old version.
Pull down order I'd, productcategory, profit, profit again. Turn on the aggregates. Group by order ID, productcategory, sum profit, where profit criteria &lt;0
&gt;I'd have no way of knowing which employee made the sale (as it's counting them all together) It doesn't seem like you'd ever have any way to tell which result belongs to which employee (remember, you can not rely on the order of results unless you explicitly provide an order). You would need to include an identifier for the employee along with it in order to tell them apart. In this case, you can simply add the employee identifier to the column list (`select count(*), employeeID`) Also, *technically* this query would miss any sales that happen during the last second of the day. This may not matter in your case (known work hours that don't overlap with this time), but I'd recommend preferring something like `and cast(AppDate as Date) = '04/01/2019'` it's a good habit to get into - one day that second will matter - and IMO it looks cleaner. If you want results for rows that don't exist, you'll need some way of keeping track of which results you care about. (since this information can't be found in the table you're using). There's many ways you could do this - you could store the ids in a table variable, temp table, etc. But assuming you already have an employee table somewhere, I would just include that in the query. Assuming that table were at POS..employees, it would look something like this: ``` select e.employeeID, isnull(count(m.*), 0) from POS..employees as e left join POS..MCApplications as m on m.employeeID = e.employeeID where e.employeeID in ('EMP000140','EMP000610','EMP000504','EMP000672') and Status in ('Pending', 'Approved') and cast(AppDate as Date) = '04/01/2019' group by e.employeeID ```
I dont think he even needs order id? as long as profit &lt; 0 in hgis where statement it will look at profit on the order level because where doesnt gho across lines. I would add in a count of order id though so you can know if the losses are happening across only a few orders or many. select productcategory ,count(distinct order id) as orders ,sum(profit) as losses from orders where profit &lt; 0 group by productcategory order by losses asc
This is the succinct and correct answer.
Try this: select employeeID ,sum(case when sale_date is not null then 1 else 0 end) as 'record_count' from POS..MCApplications where Status in ('Pending', 'Approved') and AppDate between '2019-04-01' and '2019-04-30' and employeeID in ('EMP000140','EMP000610','EMP000504','EMP000672') group by employeeID; If you have some kind of transaction ID for each application, substitute that in for "sale_date" within the sum equation. Let me know if it works.
Yep my bad
If you're using a SQL Server version older than 2017, you could use the XML features to do this. ``` select FieldName from ##KeyTable for xml path('') ```
If you're using a SQL Server version older than 2017, you could use the XML features to do this. ``` select ', ' + FieldName from ##KeyTable for xml path('') ``` This would return a single row `, 1, 2, 3, 4, 5` We'd then use stuff to take out those first two characters. ``` select stuff(( select ', ' + FieldName from ##KeyTable for xml path('') ), 0, 2, '') ```
Yes, I do this frequently.
Already had a winner here. https://www.reddit.com/r/SQL/comments/bn6yh5/ms_sql_interesting_dynamic_sql_fun_platinum_for/
I am pretty sure that the same is possible to be done with FOR JSON (never done it though tbh)
Just as an aside, you can use coalesce() as a shortcut for “case when null then...else...” SELECT coalesce(x.key1, y.key1) this returns the first non-null item inside the parentheses.
Thanks, yeah its just for the quiz, although I dont remember anyone mentioning natural joins are no good.
&gt;Im just curious, at the beginning of the query you stated, quantity.quantity as if it had been made into an alias somewhere. I just wanted to know how you did that?
This is a god damn great point. Outstanding.
it changed my life when I learned about coalesce lol.
I’d use COALESCE(count(*), 0), so if there are no records, it backs off to zero, rather than not returning anything. Alternatively, if you don’t want to associate these counts with individual users, you can wait to coalesce until you sum across all the relevant users. In either case, same tool. Usually, you wouldn’t have to COALESCE on a COUNT, but when you GROUP BY and get no groups, you do.
you should be grouping by every item in your select that isn’t an aggregate.
Oh, I'm well aware of it, and generally more fond of ISNULL() but here I just had my head up my ass.
&gt; as if it had been made into an alias somewhere yes, there are two aliases, one for the table and one for the column look carefully, you will find them both it's unusual to give both the same alias name, but it works
Not sure what you mean by that
To clarify, I did try myself and purposely cut off my working to not mislead anyone. I’m sure everyone’s been in a situation where they really don’t understand a question in their homework, sorry if I’ve offended you. What I did was: SELECT P.property_id, primary_purpose, locality, COALESCE(area::text, 'N/A') AS "Area" FROM Localities L JOIN (Properties P LEFT OUTER JOIN Areas A) WHERE P.property_id NOT IN ( SELECT S.property_id FROM Sales S)
To clarify, I did try myself for at least an hour and purposely cut off my working to not mislead anyone. I’m sure everyone’s been in a situation where they really don’t understand a question in their homework and I was just looking for guidance from a fresh perspective to build upon what I’ve done , sorry if I’ve offended you in any way. What I did was: SELECT property_id, primary_purpose, locality, COALESCE(area::text, 'N/A') AS "Area" FROM (Properties P NATURAL JOIN Areas) NATURAL JOIN Localities WHERE NOT EXISTS (SELECT * FROM Sales S WHERE S.property_id = P.property_id) GROUP BY P.property_id, primary_purpose, locality, area
This code is very similar to the one I wrote, and has resulted in the same issue of the null values in the table not being displayed, any thoughts on how to resolve the issue?
String_agg or stuff
Don’t I have to create a table to use string_agg?
&gt; I have shown this in my ER Diagram. What i am not sure about is, should the Application entity which contains a StudentId foreign key have direct relations to other entity's associated with the student such as the Address, Qualification or EmploymentHistory to name a few? No, for two reasons: 1. If you can already collect these from the student, you can retrieve this information via joins 2. There will be nothing to stop someone inserting the a qualification/address onto a row that belongs to another student. Here's an example http://sqlfiddle.com/#!17/f9b70b/16
Use a left join for joining properties to sales. Then add where clause to filter for NULL's in the Sales table. (Properties records that do not have a corresponding sales record)
Nope.. sample code from Microsoft: SELECT STRING_AGG ( ISNULL(FirstName,'N/A'), ',') AS csv FROM Person.Person;
Please, for the love of all that you hold dear, stop using `NATURAL JOIN`
Thank you so much I'm still a beginner &amp; I have been stressing over this massively. That example explained it really well &amp; it has just clicked in my head &amp; I understand. Thanks again.
&gt; Use a right join for joining properties to sales **NOO**OOOoooo... that would suggest you're looking for sales with or without their matching properties, i.e. sales for properties that don't exist
Would it not be easier to use a case in the query? If purchase\_price &lt; 0 then show 'N/A' else show the purchase price.
Reverse.. properties that have no sales. That's your whole.point right ? So what ID's in properties do not exist in the sales table.
ahh sorry i’m aware it’s not the best way but this is the only way i know
ah i see, thanks string_agg worked great! do you have any idea on how to replace a missing value with an empty string i.e. does not produce a blank space.
Not BigQuery specific, but this is the book on hierarchical querying in SQL: https://www.amazon.com/Hierarchies-Smarties-Kaufmann-Management-Systems/dp/0123877334
Maybe? I’ll definitely try it thanks, I was under the impression that a case statement and coalesce would yield the same results?
the problem statement actually gives the biggest hint -- "properties that have never been sold" in order to test whether `purchase_price &lt; 0` you would have to have a sales row, because that's where the purchase_price column is, and why would there be a sales row for a property that was never sold?
or LIST_AGG if you're using Oracle SQL
You could use just a regular JOIN on whichever keys/columns you need. For example: SELECT a.col1, a.col2, b.col3, b.col4 FROM Table1 a JOIN Table2 b ON a.col1 = b.col1 AND a.col2 &gt;= b.col2 WHERE a.date &gt;= '01-JAN-2001';
Your GROUP BY clause must essentially match your SELECT field list, except for any fields which use/contain an aggregate function, such as COUNT or SUM. For the example above, your GROUP BY clause should be: contacts.contact_id, contacts.vendor_id, contacts.group_id, contacts.contact, contacts.title, contacts.phone, contacts.email, vendors.vendor, contact_settings.note, contact_settings.will_call, contact_settings.my_contact
Here's my solution, I think this should suffice. I can't try it out though so it might require debugging lol. [Spoiler Alert!](https://imgur.com/a/h3fYCpy)
TblStudents TblApplication TblCourse TblRelationship TblRelationshipType All tables have an minimum a PK identity and a TableNumber (that will be the same for every row but never appear in any other table) The relationship table has a pk, tableNumber and relationshipTypeId. The RelationshipType table has the following Student to application Application to course Student to course Now you can have one or many on either side.
What does Rest_shift look like? Does it have timestamps? Does it have a column for start time and a column for end time? Does it have employee names and their shift times? You didn’t explain it very well
Below should work with some tweaking. Also might want to add a Static column defining "Day_Shift" AS "Shift" or something Select COUNT(*) From Rest_Sales Where CAST(SaleTime as time) &gt;= '11:00:00' AND CAST(SaleTime as time) &lt;= '16:00:00' UNION (Same shit but different Times)
ah sorry. yes it has a column for start time and end time just like you showed
can just do a case when instead of a union
 select rest_shift, count(*) from sales s inner join rest_shift r on s.sale_time between r.start_time and r.end_time group by 1 sale_time, start_time, and end_time should all be the same datatype (so timestamps or datetimes).
STRING_AGG is SQL 2017 or greater. Classically, people have used this XML technique: https://blog.sqlauthority.com/2009/11/25/sql-server-comma-separated-values-csv-from-table-column/
rest\_shift is a table name, not a column name.
edited. sorry, meant shift_name, or whatever the column is called
I think this might work, give it a shot and let me know. ``select p.property\_id, p.primary\_purpose, l.locatlity, coalesce(a.area::text, 'N/A') as "Area" from Properties p inner join Areas a on a.area = p.area inner join Localities l on l.locality\_id = p.locality\_id where p.property\_id not in ( select s.property\_id from Sales s ) group by p.property\_id, p.primary\_purpose, l.locality, a.area`` &amp;#x200B; &amp;#x200B; [https://imgur.com/a/8scZQhC](https://imgur.com/a/8scZQhC)
Use SQLite. It's the simplest SQL implementation you will find. It saves the data in a file, and there are implementations to that. Simple is your priority, though, and this is it. https://sqlitebrowser.org/ https://www.sqlite.org/index.html
What errors do you get? What’s the DDL used to create the table?
Does all of the current non-null data stored as vharcar look like an integer? If not some data cleansing might be needed first. Some suggestions here https://stackoverflow.com/questions/5136013/how-to-change-column-datatype-in-sql-database-without-losing-data
thank you! i will give it a go
TYPE is not necessary. ALTER TABLE [TableName] ALTER COLUMN [ColumnName] INT;
Wow tysm for typing that up! I gave it a go and unfortunately it output the same 3 rows :( The null values still elude me...
Just use a left join for the areas table, it literally says in the question that area information might not be there in the properties table
did it and it worked!!! thank you for the suggestion
I think you're right. I frequently use "ORDER BY [Date] DESC" to see the most recent log entries at work.
I think they mean to sort by Seniority descending. You're absolutely right that timestamps ascend as they go from the past to now.
The first three sentences do indeed declare that your interpretation is correct, and that the listed-"correct"-answer is, in-fact, non-correct. &gt; 1: ... produce a report that shows each employee's name and the number of years each employee has worked for the company. &gt; 2: Assume continuous employment. &gt; 3: ... employees should be sorted such that the person with the most seniority is first. Sentence-2 is critical, because without it, it would be absolutely non-correct to sort based on `hiredate`, and instead you must sort on the computed-attribute aliased as `Seniority`. And honestly I'm not sure why that wasn't done... but... whatever.
Can't recall SS syntax exactly, but this is a classic: @list = ''; select @list = @list + ',' + Fieldname from ##KeyTable; select @list;
Thank you. I didnt know you could do that.
I use the term "relation" over "table" because by saying "relation" I make explicit reference to the terms used in the *theory* behind SQL; the relational-theory. A relation is-not a table. Strictly speaking no SQL-product is truly relational, but I prefer to use the terminology set forth by the theory, and not the SQL-standard or SQL-products. I think that understanding the theory on which SQL is loosely predicated can enhance your usage of SQL. Reversely, if all you know about relational-theory stems from your understanding of SQL, well, I'll see you at the finish line.
not only CAN you, but you pretty much MUST do it that way or you'll get the error message "Field not in GROUP BY clause" (or something similar).
This right here OP. Was literally chiming out the exact query in my head as I scrolled down to your post. Good job!
Which site has this problem?
are talking about the N/A assignment 'area' was missing. and i solved that by doing the NVL in the select statement
Sometimes I put USE [fakeDB] GO at the top so it will try to execute against a nonexistent database when I am scared of what im writing
Oh, I see, living life in the fast lane with that F5 button.
Ok, so the question states: &gt;"...the report should be sorted such that he person with the most seniority is first." Seniority is a computed value: NVL(ROUND(SYSDATE - HIREDATE)/365,1),0) "Seniority" However, the order by clause is on HIREDATE. Therefore you need to order it in ASC order so that the person with the earliest hire date (and as such the largest number calculated by the "Seniority" calculation, is listed first. Looks like C is the correct answer.
&gt; And honestly I'm not sure why that wasn't done... but... whatever. Column 3 is rounded to years, If you sort by it, two people with 1 year seniority would be sorted essentially randomly. Sorting by hiredate ensures that the one hired in January comes before the one hired in October.
SQL Prompt has functionality like this. It also stops and warns you when you're running updates/deletes without a where clause. You could put a BEGIN TRAN/ROLLBACK around your adhoc queries, so if you accidentally EXEC something it won't commit anything. Just be careful with this is your queries take time to complete or you're running them on tables that are accessed frequently (don't want to cause any locking). It's a bit more work, but you could add DDL triggers to your tables to stop any adhoc schema changes.
A Windows or Linux VM
@AbstractSqlEngineer, I've never seen relationships modeled in this way before. How does this model fair with large datasets (i.e. millions of rows in the relationship table)? Also, what are the advantages of this model, vs. have relational tables between each entity table (i.e. tblStudent, tblStudentApplication, tblApplication, etc...), aside from having fewer tables in the database?
https://www.sequelpro.com/
It would help to know specifically what error you are getting.
Which is the option of the requirements in this case. Person with most seniority would be the one hired first, meaning earliest HireDate, here ORDER BY ASC.
Exactly. The "correct" answer from the screenshot is definitely wrong.
Proper cluster, proper physical, I'm up to 162 million in under 100ms on more complex items. Advatages... let's say I add a tblRelationshipFamily, and my relationshipType table now has a relationshipFamilyid. I'll triple my records in relationship Type, and create 3 new Families.. 1 for each set of 3 types (since they are triplicate) one family would be direct, one descendant, and one ancestor. Now I dont have to run a recursive cte everytime my code runs. Just everytime new relationships are added In a specific area. And I don't have to use so many joins. Select student From student Where in (parent from relationship where type = student2Application and child in (child from relationship where type = student2application and parent in... I can just run around and find relationships . Even better. Create function Children returns table (id) @type, @parent Select child from relationship where type=@type and (parent =@parent or @parent is null) Then I have code like.... Select Id From students Where in ( select Id from child(schoolToStudent,@schoolid) And in (select Id from child(scholarshipToStudent, @scholarshipId)
I used to declare a safety bit variable. All the statements that wrote to the database would have a “where safety = 1” but read statements would still go if you had safety of 0.
Could use DDL Triggers that run on scheme modification and do a selective KILL of actively running queries?
No need to go killing SPIDS. DDL triggers (like DML triggers) allow you stop basically stop the operation adn return an error message. See the examples on the MSDN page on DDL triggers: CREATE TRIGGER safety ON DATABASE FOR DROP_TABLE, ALTER_TABLE AS PRINT 'You must disable Trigger "safety" to drop or alter tables!' ROLLBACK; [https://docs.microsoft.com/en-us/sql/relational-databases/triggers/ddl-triggers?view=sql-server-2017](https://docs.microsoft.com/en-us/sql/relational-databases/triggers/ddl-triggers?view=sql-server-2017)
Navicat is the best one I've seen on a Mac, but if you ask me, it's not that great. I much prefer: your editor of choosing and a terminal window set to run your script when you make a change.
Regarding an 'Offline' server/database, see the below link on connecting to LOCALHOST (use windnows authentication menthod):
I am getting this error message on PgAdmin 4 using Postgres &amp;#x200B; **ERROR: column "credits" cannot be cast automatically to type integer**
Yup pretty much all non-null data stored as varchar is numbers and can be converted to integer. It is the null values that is preventing me from doing the conversion. This is my error message - ERROR: column "credits" cannot be cast automatically to type integer
Error message: &amp;#x200B; ERROR: column "credits" cannot be cast automatically to type integer
Datagrid by jetbrains. Best I’ve seen on any platform.
I was thinking in regards to OP's original question, wanting to stop execution of any queries, for whatever reason that is. Maybe I misread.
Ah I see. Yeah you could probably do it by literally killing the SPID. IMO it's cleaner and probably easier to gracefully stop the operation and return some type of warning/error message. At the end of the day the result is more or less the same i guess.
You can download sample databases. Look up AdventureWorks you can find a backup file and recover the database and use that.
I personally start off *any* query I write that is updating, deleting, altering, etc with: BEGIN TRAN --COMMIT TRAN --ROLLBACK TRAN So then when I do hit F5 by accident, I can highlight ROLLBACK TRAN and run it to undo. However, that does not work if you highlight part of your SQL and run it by accident, which I've done that too... Alternatively (and this is my practice), backup and restore the db to a new place where it does not harm to accidentally run bad queries.
You can download a lot of databases. North wind is one. They're all free.
My guess is that you have data in one of the fields that is not recognized as an integer for some reason. NULL fields shouldn't trip that, but a VARCHAR in one of the fields definitely would
Download and run the installer for the (free) Developer edition here: [https://www.microsoft.com/en-us/sql-server/sql-server-downloads](https://www.microsoft.com/en-us/sql-server/sql-server-downloads) On the first screen where it asks what kind of installation you want to do, click "Download Media". That'll let you download an ISO of the installation files that you can use on your home computer.
\&gt; Column 3 is rounded to years Dunno about the code, but the assignment called for rounded-to-years to the first-decimal-place, which allows some margin; \~1/5. So in your example of January vs. October, that would still sort correctly.
I've looked over the data by filtering the CSV file that I exported and it has all numbers and blanks. All are whole numbers with the exception of 2.5. Would that be causing an issue? &amp;#x200B; Here is the full error message: &amp;#x200B; ERROR: column "credits" cannot be cast automatically to type integer HINT: You might need to specify "USING credits::integer". SQL state: 42804
groklearning
Having a weird glitch when ROUND(FieldName, 2) where the FieldName is originally a DECIMAL(18, 6). Getting nonsensical values like 1.0 when the original value is much higher. Not sure if it's because of pushing into an NVARCHAR column, or if it's coming through on the FULL OUTER JOIN. Still functionally showing a discrepancy between the two columns, but it's weird to see.
You need to add within_group (order by..) to order by date https://docs.microsoft.com/en-us/sql/t-sql/functions/string-agg-transact-sql?view=sql-server-2017. You also need to use either case to get rid of the spaces or coalesce(concat(house_number::text, ' '), ''), street...
Ok, so in another comment you say you think you may know why it’s happening (there is a non-integer value in this column). Have you done any testing to validate your hypothesis?
C should be the correct answer, and the answer in the "Explanation" box matches C not A.
r/ProgrammerHumor
Stop doing dev in production, problem solved.
You seem to really be struggling with your class; you've posted asking for help with 5 problems in two days. I think you need to sit down and have a conversation with your instructor, not ask the internet to help you with every question in your assignments.
I wish I could but this course is actually taken purely online and I cant see an instructor and don’t know who my classmates are i.e. I’m completely in charge of my own learning. It’s really frustrating that I can’t pick up everything as quickly as I want to. I came to the internet for help with the questions I’m struggling to understand and to be proactive with my learning, as I’m sure you would do for any other subject, is that so wrong? I’m not forcing anyone to help me and definitely not asking to be told that I’m struggling (as I already obviously know that).
&gt; purely online and I cant see an instructor You can't get a phone call? A chat? Email? Literally no way to contact the instructor 1:1?
Yes. We are unable to contact the directors of the course.
That sounds like a terrible course and I would demand a refund.
What new ETL tool are you moving to?
I don't think so. It needs a sum group-by for the daily total, then you can use avg(sum) window over country, but then need a distinct on country to get rid of duplicate rows. This happens since windows operate on rows after group by but there are 2 rounds of grouping required. Might as well stick with subquery so its more obvious whats going on. Example: http://www.sqlfiddle.com/#!17/c7cbd/36/0
It *is* a programming language. It's just a domain specific, declarative programming language. However, most people who say this mean that it's not an imperative programming language, and they think that imperative programming languages are the only "real" languages.
2.5 is not a whole number, you need to use the DECIMAL format instead of INTEGER if you want to store it there. You could cast it but it will truncate the decimal point.
Well most SQL is turing complete as deployed. There are SQL standards (e.g. SQL 92) that are not turing complete. There are enough extensions to most common SQL implementations though that there are ways of being turing complete, and the standard is now fully turing complete (since SQL 99). Turing complete for our purposes can just mean it can run another (turing complete) programing language, axiomatically [the turing machine](https://en.wikipedia.org/wiki/Turing_machine) which is generally regarded as the simplest programing system. A turing machine is representation of a tape and symbols that can replicate a class of logic systems. Any turing complete system can run any other one (maybe not efficiently). Though really technically a turing machine is infinite, but for turing complete purposes that is not considered. Generally what prevents SQL from being seen as turing complete is that classic SQL couldn't loop, and reprocess data. Once you add triggers or CTEs it becomes fairly easy to build turing complete systems. The underlying thing people are going for when they say SQL isn't turing complete. Is that relational algebra is not turing complete, and SQL is a relational algebra system plus some stuff. Of course SQL isn't a quite a full relational system for complex reasons but can represent one, and yeah, it gets complex and I'm glossing over some stuff; hopefully not in a way that causes too many issues.
When people say SQL is not Turing complete, they are referencing the ISO standard SQL, and there are a few types of computational operations that they thought you couldn't do in SQL. Eventually it turns out that more clever engineers wrestled the problem and came up with ways to perform all of these operations, so in practice SQL is a Turing complete language, but overall that's a red herring to begin with. &amp;#x200B; As mentioned by others, SQL \*is\* a programming language, but it has a relatively specific use case and wouldn't make sense for general purpose software writing which is the most common case where someone tries to claim it isn't. It is all built around manipulating data sets, and it does that at scale infinitely better than any "standard" programming language. You wouldn't try to cook up something like an email client in SQL. It could probably be done, in some fashion, but it isn't an efficient and practical tool for that job. Likewise you find that if you are trying to manipulate a data set with few million object in it in java it will normally take significantly longer to run than if you pushed that operation to a database engine. &amp;#x200B; Also, SQL can do loops, it just depends on which brand of SQL you are using.
With the addition of recursive CTEs, SQL is now [Turing complete](https://optimalbi.com/blog/2014/09/11/t-sql-common-table-expressions-are-turing-complete/). That, of course, doesn't mean it's the best tool for anything but a narrow domain of use cases. You could, theoretically, write something like a recursive descent parser in SQL, but you most likely wouldn't enjoy the experience.
Thanks man!
Like...you just know, man
Like how I'm supposed to know if a girl likes me, cause I don't pick up on that either
You are welcome !
nice job you did there!
Thank you!
T-SQL is Turing complete, so it is a programming language. The end.
Thank you! I will check it out!
Thank you for the response.
Thank you! I will check it out. I do like W3 site
she will, as soon as you can demonstrate your understanding of PK-FK relationships
It’s a book, it’s pretty cheap, if you don’t want to wait though Wise owl tutorials on YouTube does both basic and advanced courses :)
FK is the primary key of the table I'm referencing, right? So, then how do I know when I need to reference a table?
when it's necessary for data integrity consider this example -- you have an `orders` table in which you record all the information about orders placed with your company for some of your products the customer_id in the orders table is a FK to the customers table would you record an order for a customer_id that doesn't exist? does that make any sense at all? no therefore, you use the PK-FK relationship to automatically ensure the integrity of the customer_id value in the orders table is Todd not a valid customer? then i won't take any order from Todd
Nice work! I'm currently learning SQL and PL/SQL, this will come in handy! Thanks.
&gt;LIKE ‘a_%_%’ (find any values that start with “a” and are at least 3 characters in length) The middle % is redundant. Also a useful LIKE pattern is: LIKE '%[^0-9]%' Which matches all strings that don't have a digit.
#
DB Compare was on Code Plex from 2013 - 2017. DB Compare compares the database schema of two SQL Server databases: [https://github.com/DataJuggler/SharedRepo](https://github.com/DataJuggler/SharedRepo) &amp;#x200B; There is an also an install version located here for those people without Visual Studio 2017 or 2019 (although anyone can download Visual Studio community edition for free). &amp;#x200B; [https://github.com/DataJuggler/SharedRepo/tree/master/DBCompare/Install](https://github.com/DataJuggler/SharedRepo/tree/master/DBCompare/Install) &amp;#x200B; DB Compare also includes the ability to export your database schema to Xml, and perform a comparison against a SQL Server database located on a remote virtual machine. &amp;#x200B; Let me know if you find it useful. If you don't, say thank you and throw it away because I have had all the rejection I can handle for one weekend trying to give something away for free that maybe one person might find useful. &amp;#x200B; Peace
Upvoted. Good job.
Thank you! &lt;3
One way is partition pruning. Imagine you’ve got a table with audit data, a timestamp and an action column at least, probably some others. Imagine this table is so big that a full table scan takes hours to run. Some columns are indexed but not all. Specifically the timestamp column is not! This table is now virtually unusable for anything except queries with a where clause on an indexed column. Partitioning this table by day/week/month means when a query is run that has a where clause on the date field, the database can immediately forget about all the data in all partitions except the relevant ones. This drastically reduces the amount of data the database needs to read to get the same result as if it had to do a FTS.
Unified auditing requires relinking the binaries, so you can’t do it in SQL Developer. I don’t know about doing it in RDS. Have you looked at the documentation? e.g. https://docs.aws.amazon.com/SchemaConversionTool/latest/userguide/CHAP_Source.Oracle.ToRDSOracle.html &gt; ‘Unsupported Oracle features such as Data Guard, Unified Auditing, and Real Application Clusters that Amazon RDS ‘
Thank you so much!
Forcing yourself to use the one tool you're badass at is a mistake. Worrying about it being considered coding or not is silly. Just be a badass at that and learn other things too. It's rare for a company to go with ONE language anyway so worrying over one of the many being considered special is not really worth your time.
Perhaps he means using Bind variables? Like for example: Select col1, col2, col3, ... From Table1 Where col1 = &amp;Bind1 and col2 = &amp;Bind2; Running this in SQL Developer would cause you to be prompted to enter values for the &amp;Bind1 and &amp;Bind2 variables, and then those values would be used as criteria in the query. (Note: I always forget the syntax...it might be :Bind1 and :Bind2 or %Bind1 and %Bind2. Regardless, you get the idea)
probably a trigger on your user table
Imagine a company with many different business units. Each "division" of the company contains a handful of distinct Business Units. The Divisions generally are self-contained when it comes to reporting/queries/etc. Partitioning the tables such that only Business Units related to a specific Division are in a given unit, and the queries for that Division run much faster, as they can exclude the other partitions--and thus a LOT of extraneous data--much more quickly.
Thanks!
Nice, I will add that ;)
You're welcome!
Thank you!
Not really, I disagree - partitioning itself will not improve your query performance.. It depends on your filegroups policy - if all partitions are created inside the same filegroup, well.. It doesn't make sense at all. If you distribute your filegroups into different disks and then partition your table to use these filegroups - yup, this would work. On the other hand think about the huge table with 2 yrs of data, where only one year is used every day - you can use partitions to move all data with dates &gt; year ago in a second using alter table switch. [https://www.youtube.com/watch?time\_continue=211&amp;v=PXvgsaBiVOo](https://www.youtube.com/watch?time_continue=211&amp;v=PXvgsaBiVOo)
 Aaa &gt;screen (rather than receiving from an in
&gt;It depends on your filegroups policy - if all partitions are created inside the same filegroup, well.. It doesn't make sense at all. Won't it just need to examine that particular partition and forget all other partitions and thus reduce the amount of data to be read?
 -- i did this in vertica, you can do this in mysql as well. --creating the table DROP TABLE IF EXISTS table1; CREATE LOCAL TEMP TABLE table1 ON COMMIT PRESERVE ROWS AS ( SELECT current_date - 5 as date_column , 0 as number_column UNION ALL SELECT current_date - 4 ,10 UNION ALL SELECT current_date - 3 ,0 UNION ALL SELECT current_date - 2 ,10 UNION ALL SELECT current_date - 1 ,10 ); --checking how it looks SELECT date_column,number_column FROM table1 ORDER BY date_column; --required summation --assume that the day "WILL BE UNIQUE" SELECT a.date_column,SUM(b.number_column) as cummulative_sum FROM table1 a JOIN table1 b ON (a.date_column &gt;= b.date_column) GROUP BY a.date_column ORDER BY a.date_column;
Structured Query Language Its a language.
For those who don't want to spend 8 minutes watching a video... &gt; DB Compare is an open source C# project that compares the database schema of two SQL Server databases. &gt; &gt; The code is available here: &gt; &gt; https://github.com/DataJuggler/SharedRepo/tree/master/DBCompare &gt; &gt; DB Compare also includes the ability to export you database schema to Xml, and perform a comparison against a SQL Server database located on a virtual machine. &gt; &gt; DB Compare was actually a sample project of DataTier.Net, that creates all stored procedure driven data tiers for C# / SQL developers.
You mean you need a running sum? SELECT [Date] , SUM([Number]) OVER (ORDER BY [Date]) AS RunningSum FROM @EG_Table
&gt;SELECT \[Date\] , SUM(\[Number\]) OVER (ORDER BY \[Date\]) AS RunningSum FROM @EG\_Table Holy \*\*\*\*! Thank you!
I should have posted the YouTube description here. Marketing is not my strong suit as I trouble giving away free code. The way the create post works on Reddit, I wish you could add a video link add a post a little message with it. If you select a Text post and add a video, the video shows up as a link. &amp;#x200B; Thanks SQL Server DBA
Looks like this is already the best answer but just wanted to add in case you didn't know, you can insert multiple rows in one statement: DECLARE @EG_TABLE TABLE([DATE] DATE, [NUMBER] INT); INSERT INTO @EG_TABLE (DATE, NUMBER) VALUES ('2019-01-01', 0), ('2019-02-01', 10), ('2019-03-01', 0), ('2019-04-01', 10), ('2019-05-01', 10);
Oh I did know that, just wanted to make the example clearer, but thank you :)
* There's no table aliased `c` in the select * There's no table aliased `c` in the on clause * The query aliased `p` doesn't return the column `id` * The query aliased `p` doesn't return the column `yyyy_mm_dd` * In the select, `p,` indicates a column `p`. Maybe you mean `p.*`?
I fixed up the aliases. I forgot to change some after copy / pasting.
Still `t2` doesn't return `id` or `yyyy_mm_dd`, but references them in the `ON` clause. In general, a missing ) error doesn't necessarily mean that a ) is missing, but could indicate any syntax error before that point, like a `CASE` without an `END`. It's going to be impossible to find the error with abbreviated code like this. The error is very likely in the code you're leaving out.
Thanks everyone for the ideas. It looks like it can only be done manually. I was hoping there was an automated way of doing it. I tried putting a ' in front of the data, but when I paste it into excel it keeps the 0 but also the ' in front it, so I would still have to clean up the column manually. I think I may have to attempt to write a script to do it.
The query given is the full query. I've added the additional fields into the subselect like so; SELECT t1.*, t2.programme FROM table1 t1 LEFT JOIN (SELECT programme, yyyy_mm_dd, id FROM table2 WHERE yyyy_mm_dd = (MAKEDATE(YEAR(CURDATE()), 1) + INTERVAL QUARTER(CURDATE())-1 QUARTER - INTERVAL 1 DAY) ) t2 ON t1.id = t2.id AND t1.yyyy_mm_dd = t2.yyyy_mm_dd I've only changed table ./ column names, for the post here. This is the error that I get now: \&gt;Error while compiling statement: FAILED: ParseException line 4:76 mismatched input 'QUARTER' expecting ) near 'INTERVAL' in expression specification
What version of Hive do you have? Hive didn't have the `QUARTER` function before 1.3.
Ah. I'm on 1.1.0. So I guess the question is how can I get the last day of the previous quarter in HQL? As that will replace my `MAKEDATE(YEAR(CURDATE()), 1) + INTERVAL QUARTER(CURDATE())-1 QUARTER - INTERVAL 1 DAY`
Usually Group By's need to be done when there is an aggregate function in your Select. Meaning it adds,counts,finds a MAX etc. Only when there are more than 1 column(s) called. a single: *select MAX(id) from Table* would **not need one.** In your case i believe there is no way around it &amp;#x200B; Null values can be manipulated with the ISNULL() function ISNULL(&lt;Column&gt;,&lt;String Replacement IF null&gt;) Ex. ISNULL(person.date,'') Will print with No Space &amp;#x200B; But I agree with what someone else said here. You need to talk with someone. A lot of these are basics you're not grasping. You're starting to get into more in depth Queries and you are not ready it seems.
What's the Data Make-up/Type of the Column? The Column may have been made with Decimal(10,2) or Money. In which case you'll have to Edit the Column. You can't give it more places than Defined &amp;#x200B; You might be able to Convert to Decimal(10,6) in your Query CONVERT(DECIMAL(10,6),s.allhours)
I capitalize keywords in SQL, but nothing else. I also quote identifiers, so that I have to use the exact capitalization of the table names and fields.
http://poorsql.com/ and related add-ins to the rescue. I always post this in these sorts of threads because it has saved me many hours of both formatting to meet others' requirements and translating stranger-code into my preferred reading format.
Most coders have a certain approach they like for capitalization/indenting/etc. It's natural when you stare at code all day that you'd want to rearrange it to make the most sense or be the most pleasing. My degree is in philosophy/english lit, so I understand what you mean about a compulsion to fussiness with words. But we have to keep in mind that this shit does not actually matter at all -- SQL doesn't care about your caps. By all means do it however you like, but in the absence of team standards, you really shouldn't rewrite other people's code on this basis.
As a safe choice without intermediary casting to decimal you could try the format function `format(value,'G6')` It's performance can sometimes be questionable and it will mark any view or function invoking it as non-deterministic., so an alternative that doesn't require some intermediary casting could be: `ltrim(str(value,17,6))` See: https://docs.microsoft.com/en-us/dotnet/standard/base-types/standard-numeric-format-strings for more detail about format (internally this invokes the CLR).
everything in lowercase, fight me.
[ScaiPlatform](https://scaidata.com/product/scaiplatform) from ScaiData. It is free on AWS, Google Cloud and Azure.
Try [ScaiPlatform](https://scaidata.com/product/scaiplatform), it is free on AWS, Azure and Google Cloud.
Just tried this with a column in my database with data type DECIMAL(10,2). `select top 5` `ihar_amt1,` `cast(ihar_amt1 as decimal(10, 6))` `from invhist_hdr` `where ihar_amt1 is not null` &amp;#x200B; My results: &amp;#x200B; `ihar_amt1 sixplaces` `-------- ---------` `47.66 47.660000` `-298.93 298.930000` `456.00 456.000000` `366.12 366.120000` `518.10 518.100000` &amp;#x200B; `(5 rows affected)`
You can use [ScaiPlatform](https://scaidata.com/product/scaiplatform). It is free on [AWS](https://aws.amazon.com/marketplace/pp/B07NPPSPJ1?ref=_ptnr_web_pricing), [Google Cloud](https://console.cloud.google.com/marketplace/details/scaidata/scai-platform-2?utm_source=website_pricing&amp;utm_medium=web) or [Azure](https://azuremarketplace.microsoft.com/en-us/marketplace/apps/scaidata.scai_platform?tab=Overview).
&gt; and change everything so that it's capitalized for the love of $Deity, **NOO**OOoooo.... capitalize the SQL keywords only, please SELECT THIS_LOOKS , LIKE_SHIT , BECAUSE FROM EVERYTHING INNER JOIN IS_CAPITALIZED ON IS_CAPITALIZED.OBSESSION = EVERYTHING.IS_INSANE SELECT this_looks , much_better , and_easier , to_read , because FROM only_keywords INNER JOIN are_capitalized ON are_capitalized.sanity = only_keywords.is_better
\&gt;I also quote identifiers, so that I have to use the exact capitalization of the table names and fields. &amp;#x200B; but only in queries right? Not when you're creating tables?
I don't understand why there would be a distinction. If you use the same rules for naming in either case, you aren't going to have issues, even if the database's naming rules are different. That said, I would personally always use `snake_case` table names.
the difference is if someone else has to go back in and work on it after you. quoting table names and columns when creating them is a PITA for other people to go back and work on your stuff.
 WHERE type = 'thing' AND state &gt; 30 OR type = 'certain thing' AND state &gt; 60
Good advice. I learned this technique from Vern Rabe, and find it useful: -- abort if accidentally execute the entire script SET NOEXEC ON; /**** SET NOEXEC OFF; ****/ Put this at the top of your script and the NOEXEC setting will only compile your query. You have to explicitly highlight the SET NOEXEC OFF comment to run your script.
2 different select statements using a UNION? That might work.
Don't you have to put parentheses around that to make it work?
Not if you use one consistent scheme for naming, eg. `lower_snake_case`, `UPPER_SNAKE_CASE`, `noseparators`. If you don't quote when creating the table, you are at the mercy of the database in terms of how identifiers are normalized. For example, Firebird (at least before version 2.0), creates table names in uppercase by default. Postgres will lowercase identifiers if they are not quoted. In either case, if you quote the identifiers when creating the table, and when using the table, you can ensure consistency.
Some people just want to see the world burn
Write it lazy and use SQL toolbelt to format. If I am on a client site with just the basics I will write it about 90% toolbelt standard.
My formatting was broken. Just fixed it.
Yes
Imo this is ideal. Select types where type &lt;&gt; [that one type] AND time &gt;30min Union All SELECT type where type = [that one] and time &gt; 60 minutes. This is better than an or statement because it will perform faster.
but if you don't quote them then you can use the same key inputs on multiple databases and just let them translate it how they're programmed to, at that point you don't have to know if they're capitalized or mixed case or all lower, etc.
That could work... I was thinking something along the lines of an "IF" statement. I'm trying to get it to work for one case, but the "full" version of this script will have a few different rules since there are a few different "things" with specific needs out of a few hundred "things"
Sorry, my headline is incorrect. #2 is what I meant.
If OP wants just one query, and if it would be accomplished by an OR in the WHERE... then you'd not want a UNION ALL or you would get duplicates. Not sure but a CASE might be the most efficient execution plan.
Where upper(cola) like ‘%BIRTHDAY%’ MSSql is not case sensitive by default. I know the line above would work in mssql, you may need to look up a similar function for your flavor.
I don't understand what you are asking? Are you trying to find birthday in all the possible ways it can be capitalized? You should just do: where lower(colA) like '%birthday%' this wil automatically change the column to be all lowercase, removing all veriations, then just do the LIKE against that.
 WHERE type = 'thing' AND ((certainType != 'certain thing' AND state &gt; 30) OR (certainType = 'certain thing' AND state &gt; 60)) Updated for you. Edit: Made it more explicit to the scenario.
Yep.
no, because ANDS take precedence over ORs
This is me, I prefer everything lower case. The prevalent convention that keywords should be capitalized feels like a hold-over from when syntax highlighting wasn't available. No other language YELLS THEIR KEYWORDS and everyone is okay with that so why not with SQL too? I could see some value in it if one were writing lots of inline SQL in a string in a code file, but I rarely find myself doing that.
So you're saying A AND B OR B AND C will be interpreted as (A AND B) OR (B AND C) by default? I try to avoid OR like the plague and make sure to use parans whenever I do, but I did not know that.
&gt; So you're saying A AND B OR B AND C will be interpreted as (A AND B) OR (B AND C) by default? yes, exactly
I honestly don't care. I got syntax highlighting for marking keywords, so, I for my personal coding style really do not care. The addons I got installed will capitalize keywords if I let them, so most of the time, I end up with capitalized key words, but I in no ways put in effort to have that. I'm way more anal about "commas in front god damn it" and indenting.
I usually call it a mild case of OCD when I talk about my coding style, I never heard "My degree is in philosophy/english lit, so I understand" ;) I'll take that as a sign that my mind is less broken than I assumed, thank you kind sire.
you do make a good point. Syntax highlighting however, everything in lower caps, but in color, ahhh no, I am not starting a flamewar over codeing styles. Bad svtr, dont even get near starting shit like that... bad....
You will not get duplicates because the sets are mutually exclusive. In one select query, he is pulling &lt;&gt;X. In the other he is pulling =X. Duplicates are not possible.
Oops, brain fart. You're correct. I was thinking that distinctivity could be based a layer above, dunno why.
Happens to the best of us
Does it mean it only benefits queries that are 1. Full table scan AND 2. Not indexed columns AND 3. Concerning only one or very few partitions?
I tend to do keywords in full lowercase. I find indentation matters a *lot* more than case from a readability standpoint.
Not your answer but I've seen this multiple ways and wondering why this is database1.transactions "t" and database2.customers "c" Is the point of the "t" and the "c" to basically give that column/database a variable? what's the use of it vs just writing that you join database1 with database2?
Aliases. Depending on where you are, there are naming standards. It can help if you can shorthand table names as long as you know the schema.
Those are the table aliases. So instead of saying, for example, `SELECT database2.customers.customer` every time I want to reference the customer column in the customers table, I can just say `SELECT c.customer` because adding the c at the end when I declare the table tells the system that the customers table is now just called "c." Saves a lot of typing and is probably easier to follow.
From the Netezza docs: `IBM® Netezza® SQL does not support ILIKE (not case-sensitive search) SQL operators. However, you can always use UPPER() or LOWER() to do searches that are not case-sensitive. For example:` WHERE UPPER(first\_name) LIKE 'PAT%' &amp;#x200B; [https://www.ibm.com/support/knowledgecenter/en/SSULQD\_7.1.0/com.ibm.nz.dbu.doc/r\_dbuser\_functions\_expressions\_pattern\_match.html](https://www.ibm.com/support/knowledgecenter/en/SSULQD_7.1.0/com.ibm.nz.dbu.doc/r_dbuser_functions_expressions_pattern_match.html)
Thanks so much!
What does your execution plan say, and what indexes do you have on `database2.customers` currently? But regardless of that, anytime you use a linked server you're throwing any chance at good performance right out the window unless you know how to play the game.
fight? you mean love. edit: i may even go as far as pasting a previously written query in notepad++, ctrl+a, right click, and lowercase. xd
Linked DBs are usually slow and difficult to maintain. My company has a blanket ban on DB linking. Our preferred method is an aggregation layer. We keep the DB completely separate, then use a third system to pull in the required datasets and merge
Alinroc is right, your linked server reference is probably going to be your main choke point. There are ways to minimize it, but if you need days from multiple servers, you might want to stage the data on one server so you can report on it locally. That's one if the main reasons data warehouses exist. If your cross server querying is going to be minimal and it is not worth doing, then here is a way you can make it run faster. Do a select statement to load all of the data for the customers you are going to be referencing account numbers for, into a temp table. It would probably be helpful to index to customers cable by customer ID to make this run faster. The second part of your query would be selecting from that temp table for the account IDs that you want.
 Can you explain your third system? Just don't think I have heard of this configuration
Sounds like an ETL process that loads the data into an independent server.
I'm not sure how SQL Server's engine treat join order, but have you tried joining the customer table to the transaction table (instead of the other way around in your example)? If the customer field isn't indexed/partitioned, it likely is trying to do the table join before filtering to 'customer 1': SELECT c.customer, c.accountnumber, t.amount FROM database2.customers c INNER JOIN database1.transactions t ON t.customerid = c.id WHERE c.customer = 'customer 1'
You may want to consider incorporating OPENQUERY to execute your remote queries. When you utilize a 4 part qualified reference to your remote table you essentially asking your local sql server to do some work against something it doesn’t know anything about. I don’t know enough about internals to explain it more technically. But it’s like bringing a whole buttload of data all the way across the network so the local server can pick through the data to get the few things it actually needs. OPENQUERY can help in this regard because you are asking the remote server to do all the heavy lifting for you. It might be the better suited place to do it since it has all the juicy statistics, indexes, and query plans to help your query perform better. It’s basically like saying... “hey remote server... you pick through all this data for me and only give me back what I need.” That’s my dumb guy explanation of remote queries. Joining a local table with a remote table directly is almost always going to give you some performance problems. In some cases, you may be better off just dumping the entire remote table to a local temp table first and joining it later in your query.
The other guy is right. If the data doesn’t change much, then we use an ETL to populate a third database with the content of SystemA and SystemB If we need on-demand access, then we do the join in the application layer at runtime. A user will interact with a website running on ServerC. ServerC will request data from SystemA and SystemB. ServerC can then combine the data sets and present them to the user
cough repl the table cough XD
I love this. I’ll definitely be adopting it and I’ll share with you the decrease in runtime. Thanks a lot for this fantastic solution!
I have a certain style that I use. Basically I capitalize Keywords, or at least InitCap them. I try to have a clean "gutter" column in the 7th character position (the blank space after SELECT) throughout the whole query if possible. I absolutely HATE the "one selected column or expression to a line" thing because queries are long enough vertically as it is without that. But I generally will NOT reformat a query that someone else coded unless I'm having difficulty understanding what it is trying to do.
Came here to say this. You the real MVP.
Bruh, how you gonna come in here and not talk about OPENQUERY?
I used to be, then I used to not be, then I was again. I'll probably change my mind again.
If a girl likes you she will send you a text/DM that says "smash my shit famdizzle".
Sorry bud, but that is in no way a SHORT survey. And consider what you're gonna have to do to parse all the free text answers (assuming you get any). I strongly recommend refocusing it if you want feedback.
You only have to out parenthesis around it if you don't want to get kicked off the team.
[conditional join?](https://stackoverflow.com/questions/10279116/conditional-join-different-tables)
no. the stack overflow is conditional join condition (when if a, use condition a, else use conidtion b). i want join different table (if a, use table a to join, else use table b to join. but the conidtion is the same)
No. If the condition you have in mind is based on the current row, you could LEFT JOIN both tables, and have a condition in the SELECT. Or if not, just have a conditional block with two separate queries, one with each table.
yes. separate query is a method. but my query is 300 lines (ok. it is very bad practice). Copy and paste and just changing 1 table name will create very hard maintenance problem (it is hard to maintain now).
Oof. Yeah might have to do separate ones then combine, not familar with another way.
Have a look into dynamic sql: [https://docs.microsoft.com/en-us/sql/relational-databases/system-stored-procedures/sp-executesql-transact-sql?view=sql-server-2017](https://docs.microsoft.com/en-us/sql/relational-databases/system-stored-procedures/sp-executesql-transact-sql?view=sql-server-2017) [https://www.mssqltips.com/sqlservertip/1160/execute-dynamic-sql-commands-in-sql-server/](https://www.mssqltips.com/sqlservertip/1160/execute-dynamic-sql-commands-in-sql-server/)
I would just use set operators to combine the results of both queries &amp;#x200B; select a.\* from a left join b using(join\_key) where whatever = 'something' union all select a.\* from a left join c using (join\_key) where whatever = 'somethingelse'
Look at table partitions. You can write this query using a partition selector
Yeah, this looks like a union to me, using the "some condition" to limit b and c down to only the rows from them you want and adding both results together.
You mention a 300 line query, so this might be too cumbersome, but it works. Select a.*, case when &lt;condition&gt; then b.columnB else c.columnB end as columnB from a left join table_b on a.columnA=b.columnA left join table_c on a.columnA=c.columnA Good luck
Incoming "But Excel and PowerPoint are Turing complete"
PIVOT
my sincere advice is not to try to do this with SQL, but rather in your front-end programming language using the results from the simple query
definitely a UNION situation
So SQL Server 2012 already has a built in transaction log. Here is the Microsoft link: https://docs.microsoft.com/en-us/sql/relational-databases/logs/the-transaction-log-sql-server?view=sql-server-2017 Though you could certainly use AFTER or INSTEAD OF triggers for auditing.
Unfortunately, in my line of work, we don't do anything to the results after we get them out of the database - it's used solely for data storage, so if I don't do anything with SQL, then nothing else will happen
Might be easier to answer the question if we know *why* you want that sine in this case, there is no evident reason why 'Bob' is parent 1 while 'Robert' is parent 2 other than by random chance.
There isn't any significance as to whom is one and who is two, it's just to make things more obvious for when I was explaining it
&gt; solely for data storage then store it the way the simple query returns it
I mean, you could join the tables on the event id, select only the columns you want, and limit it to a single row, but I don’t know that you could guarantee you’re getting “parent 1”, whatever you want that to mean.
Use a union.
is there something that prevents you from simply doing 2 joins? select * from a left join table_b on &lt;some condition&gt; AND a.columnA = table_b.columnB left join table_c on NOT (&lt;some condition&gt;) on a.columnA = table_c .columnB
This should get you started. [https://www.mssqltips.com/sqlservertip/4055/create-a-simple-sql-server-trigger-to-build-an-audit-trail/](https://www.mssqltips.com/sqlservertip/4055/create-a-simple-sql-server-trigger-to-build-an-audit-trail/)
I thought about doing that, but if I have multiple people with the relation 'parent' then how am I meant to add multiple people if they all have the same data? Like if I tell it to add people that have parent as a relation, it's just going to bring back the first person twice :(
no, in order for the query to be efficient, you should not perform a function on your column you'll need to set boundaries for a date range like this -- WHERE dai_.period &gt;= 201810 AND dai_.period &lt; 201906 now all you gotta do is come up with some formulas to calculate the boundary values using `getdate()`
 where dai_.period between dateadd(month, -7, getdate()) and getdate()
Above answer is spot on but does not answer your question I would put your where clause in your select and remove from where. See what results your returning on that column. Couple be a number of things.
the question was, and i quote, "Datediff?" the answer is no
So the idea is to add this query into Excel and be able to refresh the table to pull back new data without actually modifying the query. The above would require me to change text within the query, right? Apologies for not making that clear.
I get no data back with the above?
you can find the query (-ies) that use that view, generate the execution plan and see what is taking the time.
I think u/frandroid87 has the same response I do. This is definitely a situation to LEFT JOIN both tables and then CASE logic in your SELECT. If your response to the right answer is "we use bad practice" then it's time to address adjusting your practices. Normalization/standards exist for a reason - to keep everything downstream as efficient as possible. And it sounds like you need some adjustments to table structure. I'm not trying to be rude - just honest (and hopefully helpful?). As painful as it is - it would be much better to take some time now and sit down and correct the bad practices than to continue working around them - because every time you work around it everything just gets harder in the future. Give your future self a break and take some time to do it correctly right now. Good luck. I'm sure everyone here would be happy to help you get updated if you need anyone help or have any questions.
&gt; The above would require me to change text within the query, right? no once you figure it out, it can run completely unaltered as long as you like give me a hint, is `dai_.period` a period number, or is it an actual date?
It is a period number (date?). Current period is 201905.
So in oracle, my full query was returning stuff from the first day of this month in addition to last month's data. When i did a ***select distinct(transaction\_date)*** against the table. I had to change the statement from "less than day 1 of the current month" to "less than or equal to day 1 of current month minus 1". Did I do something wrong when adding the statement? This is what I changed it to: &amp;#x200B; WHERE transaction\_date &gt;= ADD\_MONTHS(LAST\_DAY(CURRENT\_DATE)+1,-2) AND transaction\_date &lt;= ADD\_MONTHS(LAST\_DAY(CURRENT\_DATE)+1,-1)***-1***
so i guessed correctly in the WHERE conditions i posted above, right? i don't know SQL Server functions terribly well, but i'll have a look in **da manual** and see if i can find out how to calculate those two boundary values starting with only `getdate()` you should do likewise, you might beat me to it ;o)
Then convert dai_.period to a date and see what happens
okay, try this -- WHERE dai_.period BETWEEN YEAR(GETDATE() - INTERVAL 6 MONTH) * 100 + MONTH(GETDATE() - INTERVAL 6 MONTH) AND YEAR(GETDATE()) * 100 + MONTH(GETDATE())
So a date diff would not work at all? Or will just not be SARGable?
LISTAGG
Yes, correct in the WHERE. I'll dig into it. I'm a complete noob - am googling how to learn SQL. :/
heh i think you missed the fact that in the ranges i posted, it was `&gt;=` for the beginning of the range, and `&lt;` (note, not `&lt;=') for the end of the range
the latter
what if there are rows in the table for dates in 201905 beyond today?
So instead of just no Perhaps it should have been Datediff is not your problem, and it's inefficient. Try using &gt; &lt; values and maybe link a SAREability article. Our aim on this sub is to educate!
please check the rest of this thread, mister high-and-mighty who pissed in your cornflakes this morning? and where's your solution? oh yeah, "put your where clause in your select"
Given the provided constraints, I'd go with PIVOT: -- first two to emulate the real tables create table #Event ( EventId int, EventName varchar(80) ) create table #Attendees ( EventId int, AttendeeType varchar(20), AttendeeName varchar(80) ) insert into #Event (EventId, EventName) values (1001, 'Birthday party - ice rink'), (1002, 'Birthday party - swimming pool') insert into #Attendees (EventId, AttendeeType, AttendeeName) values (1001, 'Parent', 'Bob'), (1001, 'Family Friend', 'Mary'), (1001, 'Parent', 'Robert'), (1002, 'Parent', 'Gary'), (1002, 'Family Friend', 'Charlie') -- #table to stage the query data create table #Working ( EventId int, EventName varchar(80), AttendeeType varchar(20), AttendeeName varchar(80), PivotColName varchar(30) ) insert into #Working (EventId, EventName, AttendeeType, AttendeeName, PivotColName) select #Event.EventId, #Event.EventName, AttendeeType, AttendeeName, AttendeeType + ' ' + convert(varchar(2), (ROW_NUMBER() over (partition by #Event.EventId, AttendeeType order by AttendeeName))) from #Event inner join #Attendees on #Event.EventId = #Attendees.EventId Select * From ( Select EventName, PivotColName, AttendeeName From #Working) base Pivot (Max(AttendeeName) For PivotColName in ([Parent 1], [Parent 2], [Parent 3], [Family Friend 1], [Family Friend 2], [Family Friend 3]) )pvt If you want a more dynamic output that automatically compensates for greater/fewer family members or family friends, or even new attendee types, then you can use this: declare @PivotCols varchar(500) ;with AttendeeTypes as ( select distinct PivotColName from #Working ) select @PivotCols = STUFF(( SELECT ', ' + quotename(PivotColName) from AttendeeTypes FOR XML PATH('') ), 1, 1, '') exec (' Select EventName, ' + @PivotCols + ' From ( Select EventName, PivotColName, AttendeeName From #Working) base Pivot (Max(AttendeeName) For PivotColName in (' + @PivotCols + ') )pvt ')
The execution plans of the queries utilizing the view may be thrown off by whatever changes were made to the view. If the performance of the straight SELECT statement for the view alone doesn't seem to be significantly different, you'll have to do as already suggested and examine the execution plans of whatever calls it.
Can you select your conditional data in a CTE or temptable and join that to your main query?
Always nope out when it gets personal. Have a good one friend.
By making them columns, you *are* assigning a significance. You are ordering them. You can add a column to order them and then pivot on it if you really want to widen the data but you shouldn't store it like that because it makes it a nightmare to query.
Try updating statistics on the underlying tables.
 when ( on.way (out) ) { let.not ( you.hit ( door ) on.ass ) }
Strongly recommend using Change Tracking to keep the auditing solution as lightweight as possible. If only a few tables, triggers can be sufficient. When you begin tracking many, you’re putting a lot of undue load on the DB to achieve built-in functionality. https://www.sqlshack.com/creating-a-sql-server-audit-using-sql-server-change-tracking/
check event viewer in windows
Sorry I haven't really used event viewer before. Where and what should I be looking for inside of it?
Windows Logs &gt; Application, see if theres an error at the time you heard the ding
I do not get an error but from about 15 minutes ago in the details it gave me this SqlCeip started pid: 8300 instance: SQLEXPRESS CPEFlag: True
yea something is goofy i'd try reinstalling or something
Shoot alright. Do I need to reinstall all of SQL or just the SSMS?
Use count, not pivot.
how should I do it then? it's just easier to keep all the data to one row
So a relational database is what you want for a production system, SQL is still probably most popular for interacting with that although nosql alternatives have become more popular. That said, if you have a fixed dataset (or at least one not directly exposed to a production environment) then you don't need a relational database. But whether your data starts in a db or not, for more than rudimentary data analysis SQL is not optimal. R is popular.
A bit old school? Sorry, I'm laughing and dying on the inside. &gt; Lets say you work with close to a billion rows of data and it grows every month. You have queries you have written against that data and run them every month. Let's say we have 100+ queries run against the data set. The number of queries grows every month as new ideas are developed. You are describing an OLAP. &gt;Ok so that seems old school to me. It seems like there has to be a better way to tackle this. The words "rules engine" comes to mind, but it looks like it is a term that is not used as much the last decade. I would like to be able to have 1000s of things looked for in my dataset, but writing queries to get at the data seems antiquated. As soon as you can find a tool to replace a qualified analytics developer, at a price that costs less than my salary, let me know. You're talking about machine learning. As someone else mentioned, R is more efficient for processing large datasets, but I wholly disagree that SQL is not optimal for rudimentary, or even advanced analytics. SQL tends to be the backbone for any form of analytics, especially rudimentary analysis, and then it supplies R, SAS, SPSS, or other tools. What you're essentially describing, without the queries, is machine learning and AI that can spot these things for you.
I assume two professions for the below: SELECT doctor, lawyer FROM (SELECT p.profession, count(*) number, FROM #profession p INNER JOIN #players p2 ON p.id=p2.profession_id GROUP BY p.profession) AS sumbyprofession PIVOT (sum(number) FOR profession in (doctor, lawyer)) AS pivoted
Download MS SQL Server, install it on your computer, login to the localhost and then create a database, then import the file. Done.
Thank you I will try that right now! I can't wait till your fro is full grown as well.
Do an Initial query with this - load it into a temp table Then select from the original table and null where \[Name\] not in (select distinct\[Name\]from temp table) Union all Select from Temp Table &amp;#x200B; Also typically Order By requires that the values you are ordering by be in the query.
move your conditions into the "order by sum.." part, e.g, order by sum( case when Division='Midwest' and [DOS Year]=@RunYear and [DOS Num]=@RunMonth and [Type]='Sales' then [total sales] else 0 end ) desc, ...
If you're using a recent version (2017-onwards) I believe you'll only need to reinstall SSMS. I think Microsoft decoupled SQL and SSMS in 2017, however I could be wrong.
the key words are "I don't know exactly what I am looking for" and overwhelming vagueness of "this" as a qualifier. Is it data exploration? Data mining? Pattern recognition? Clustering? Visual query building? &gt; What software is being used to tackle this kind of issue? Youtube, Google and Kindle - read, learn and come up with ideas that formulate issues or re-shape the status quo of addressing problems.
Generally speaking, it *isn't* easier to keep all the data in one row. Wide data is much more difficult to query in SQL. Someone provided you with a good example of how to do something dynamic with T-SQL which is going to be about the only way to do this unless you know which parents you can pivot on. That said, if you can provide an example of something that is made easier by making the data wide, that might be helpful. (E.g. what question is easier answered by wide data than tall data). I'm not seeing anything immediately evident by looking at it but often times examples like this aren't easy to derive such use cases out of as a 3rd party observer. I'd probably store the data exactly as you have it otherwise. If you did add a row_number() to the event table, you could also number the events and then use that key to pivot. So if there are 10 combinations of parents and 10 combinations of family friends, you'd have 1-10 of each to pivot on.
Where Month(table.datecolumn) between Month(getdate()) -6 and month(getdate())
No, you need to configure clustering at the OS level first.
Try the commands you have in the .bat file interactively in a CMD window, no doubt there is a path error or similar. You can see the error without the window closing, fix them then amend your .bat file accordingly
 I was receiving a syntax error on the 7. I tried a subquery and I believe it's giving me what I need. where (DAI\_.PERIOD in ( SELECT Period FROM dbo.Accola\_TypeTime\_Date WHERE (Tran\_Date BETWEEN GETDATE() - 182 AND GETDATE()) GROUP BY Period ) ) Where 182 days is 6 months.
&gt; The server that I need to do this only has a SQL 2016 Express instance so I am going to have to use a batch file to call the .sql files. Why is that? Is the batch file the actual goal of the project? If not, you can [download SSMS and use that](https://docs.microsoft.com/en-us/sql/ssms/download-sql-server-management-studio-ssms?view=sql-server-2017). SQL Server Express typically installs to the SQLExpress instance. Other than that it's the same as connecting to any other SQL Server installation, so .\SQLEXPRESS as the host\instance should connect. If your goal is to write the batch file, then I think you've got a lot more learning to do than just how to run sqlcmd.exe, though that's a place to start.
Don't waste your time with a batch file. Use PowerShell, and specifically the `dbatools` module. I can expand upon this later.
Awesome, you rock!
Looks like an aliasing issue: SELECT TRN_STO_FK AS Store, SUM(ExtendedBasePrice) AS Sales1, SUM(ExtendedBasePrice) AS Sales2 We're Summing up ExtendedBasePrice twice from the subquery in your example code. I think you want to alias ExtendedBasePrice as Sales1 and Sales2 appropraitely in the subquery, then sum(Sales1) as Sales1 and sum(Sales2) as Sales2 in the main query. &gt; Additionally, I cannot seem to understand why I need "a" after the close brackets and before GROUP BY to make this work. SQL wants you to name that subquery (via aliasing) so it knows how to reference it; similar to the "No Column was specified for column X" issue when creating views or CTEs.
nah I mean it's easier to work with once it's out of sql; in my job, we need to be able to see one 'event' per line - and it gets confusing when you're trying to examine ages of people when there's multiple lines for the same 'event'
it's difficult to explain, but we need to have the data more readily available for clients, so we can use excel
Thank you so much! I've got it working now.
Hint 1: Use `union`
Try unpivot.
I need this to be able to be executed by a scheduled task on the server is why I am trying the batch file route. I can run all of my scripts successfully in SSMS, just having an issue constructing the batch file correctly to call a sequence of .sql files.
Awww 2012. If/when you get on SQL Server 2016 or later lookup Temporal Tables as they can greatly simplify this for you. Its version tracking handled by the server, no need to roll your own. Very cool stuff. Some of the other comments already listed solutions that may work on 2012 - Change Data Capture / Change Data Tracking.
if that works, that's the main objective
Often, in complex views, over time they seem to become less performative. This can be due to the underlying tables growing in size, not indexed optimally and a variety of other reasons. As a part of your Transformation process, you could convert that select statement back into a table. This can bring a significant performance improvement, though you may increase your ETL process. If the data isn’t refreshed frequently throughout the day, this can really increase performance, especially if your then passing that data off to a BI solution.
Have not done it myself but there are ODBC drivers for mySQL/Mac that Excel can use. Personally, on Postgres/Linux I most frequently Copy to CSV for one off exports.
Does the professional you're querying against have activity in that period? You won't get any data back if there is none that matches your criteria.
I am not sure but are you declaring @AttyNo as a single character? (ie char(1))? And is that what you want?
Ah if you're doing analysis in Excel, export it as tall and then do the pivot in Excel. It's pretty good at doing that dynamically.
Yes, one way you can either exclude any meals that have ingredients beyond the given list - some ppl struggle to express that because of the double negative. The second method I find easier to reason about - count # of ingredients in a meal and count of ingredients matching the given list. Include only the results where counts match.
not in SQL 2016. in sql 2017 you can setup up a clusterless availability group(s) for HA.
Exactly, I was trying to work out the first one but had no luck assembling the query. Do you know what it looks like? Otherwise I'll break out some paper and try your second option in the mean time. Thanks for the help
Thanks for the reply. Jr Sys admin still learning. I got more information from our senior and DBA. We are using sql 2017 and are attempting read-scale availability group. Not HA availability group. Any chance someone has any info on getting that setup to work with AWS and EC2?
Put a sqlcmd line in for each of the scripts in order. There are other flags to set which server, user, password, etc which you may want to set to get the job done. Open a command window first, then call the batch file so you can see the messages. Or put pause at the end.
Agreed that importing excel files to a table is probably the most straightforward and probably what you’re looking for. If you want to get fancy you can also check out the OPENROWSET function to directly work with data in Excel using SSMS.
I might be missing something, but could you use SSMS to save the .sql scripts as Stored Procedures and schedule them with the SQL Job Agent?
This is called Relational Division. Should help you with searching
SQL Express doesn't support Agent. Otherwise I would have built the scripts into a Job or into an SSIS package.
Ah that would do it. Thanks.
I take this approach
Yeah, double check what the actual datatype and length of that field is. You likely would want char(4) or appropriate length to be sure it is not truncating.
thanks all for the good approach and solution provided &amp;#x200B; i thought i describe problem not good. i use simple data, but the real table is 100K rows table 1 - prmykey, field1, field2, field3 - it is a 10k row table 2 - prmykey, fieldA\_forieign\_key\_to\_table1, fieldB, field C - it is 100k row &amp;#x200B; i need a store proc which take parameter (c\_value) - by use c\_value, i just take small amount of table2 from 100k row. if c\_value is not null, use a subquery like this i will do "select a.\* from table1 a left join (select \* from table2 where fieldC = c\_value) b on a.prmykey = b.fieldA\_foreign\_key\_table1 if c\_value is null, use while table2 i will do "select a.\* from table1 a left join table2 b on a.prmykey = b.fieldA\_foreign\_key\_table1 &amp;#x200B; as it is only a small part of the query. i will use table2 in many places and all the other part need to cater c\_value like this.
Check for nulls in your professional column. and isnull(j3.professional,'') = @AttyNo
Are you using any 3rd party extensions that may be crashing?
Whole post reeks of upper management poking their nose into the weeds, and not understanding why they need to pay people to manage queries... because using code is so old school. Like, aren't there tools that do that for you so you don't have to code? Yes, there are. Amateurs use them. And, PS, those tools were all written in code. So any one who presupposes that "coding" is old school is very clearly not someone with any formal education or technical proficiency. It's like saying geometry is antiquated and asking if there is a better way to calculate pi than using an equation. The premise of the question itself is staggering.
Regular expression.
Not sure why it works, or doesn't work, but I typically use syntax such as: `DECLARE @StartPeriod int = '201811'` Or such as: DECLARE @StartPeriod int SET @StartPeriod = '2018111' I'm not sure what you're getting when you SELECT @StartPeriod and then try to use @StartPeriod in a WHERE clause without setting the value, e.g.: declare @start int = '201811' declare @StartPeriod set @STartPeriod = (Select @start) But as someone else mentioned, you need to define the length of the char field you're using, such as: `declare @attyno char(4) = '1234'` or else you may run into issues.
We've got some linked tables at work that some of our people us in access as a front end. I just flat out don't understand how they want you to do things. I'd much rather just work in SMSS. Access is not for me.
 select m.meal from meals m join meal_to_ingredient mti on m.meal_id = mti.meal_id join ( select meal_id ,count(*) as ingredient_count from meal_to_ingredient group by meal_id having ingredient_count = 3 ) filtered_meals on filtered_meals.meal_id = mti.meal_id join ingredients i on i.ingredient_id = mti.ingredient_id and i.ingredient in ('flank steak','octopus','carrot')
I want your recipe!
 * Preheat oven to 350 * Arrange carrot, live octopus and flanks steak in an equilateral triangle on a circular sheet of wax paper. * If the octopus begins to eat the flank steak, realize that predation is a part of life and eat the octopus quickly and without remorse. * If the octopus eats the carrot, realize that all animals may have souls. Share the carrot with the octopus, feed the flank steak to a pet, and release the octopus into the closest saltwater habitat. * Discard wax paper.
When should I get in the oven? I'm in the UK so we use C not F. Collation error. I'm now on fire. Or cold.
Don't worry about the oven. Just keep preheating. Always preheating.
I have tried it this way. My problem with this is, that all columns get saved/logged while I only want the specific column which got changed with its name and old/new value. Hope you understand what I mean.
I didnt mention it in my main post but what I want to archive is to create reports from this for some users so they can see the edits. Maybe even with e-mail notification when some specific column changed. Would this be possible with trans logs?
Somethink like that, thanks
This answer may or may not go over too terribly well with the community, but I think a fun way to accomplish this would be to use Talend Open Studio. It's good to have ETL experience and Talend is incredibly powerful, I tend to use to for just about anything data related, including loading tables with flat files. Importing through SQL server is easy sure, but if you want to make any changes or anything like that, Talend is the way to go. In case you're interested https://www.talend.com/products/data-integration/data-integration-open-studio/
Looks like you're set on the technical part, however big part of developing data warehouses are understanding the business you're developing it for. In my 7 years as BI dev I have yet to encounter a problem that was not described in "The Data Warehouse Toolkit" by Ralph Kimball, I would strongly recommend reading it so you would get some real life examples and how the problems were approached/solved.
If you have a column named ID and it's the first column in a table, it is normally unigue (and probably auto-generated). That is a primary key. You look as though you have 2 tables with primary keys. The rows might, correlate, so the ID's possibly relate to each other but the best course of action would be to look at the DDL for the tables to see if a foreign key exists. Normally you would have a second column seperate from ID called Table\_2\_ID which is the ID column from Table\_2 within Table\_1.
Thanks for the reply. The Ids are the same for table 1 as table2. I have set the table 1 primary key to be ID would i set the id in table2 to a foreign key?
What is the relationship between the table1 and table2 rows. Sounds like a one to one relationship. You would ideally want a primary key for table 2 and add an additional column as the foreign key. However nothing to stop you defining the ID column as a foreign key that is referenced from table 1. Be sure that's what you want though...
Correct it would be a one to one relationship. Just trying to work out how to add the data to both these tables. Sorry if my information is all over the place. Thanks for the help.
WTF are you talking about? Do you have a question?
From what I've read SQL Server 2012 doesn't really support reporting from the transaction log very well unfortunately. It was really intended for data replication, and the backup/recovery process. I think in SQL Server 2017 they came out with SQL Audit, which I believe may be more of what you're looking for. Here's a link that explains it's functions: https://docs.microsoft.com/en-us/sql/relational-databases/security/auditing/sql-server-audit-database-engine?view=sql-server-2017 For the email notification, I would use triggers on the base table. This link gives a step by step process for that: https://sweetcode.io/using-triggers-and-email-alerts-in-microsoft-sql-server/ Hope this helps some.
&gt; are you declaring @AttyNo as a single character? Yep, that's the problem. MS SQL assumes that a variable declared as `char` is `char(1)`. You have to declare a length.
select * from header join item join [values1-5] join footer FOR XML AUTO, ROOT('root')
you should probably look how to [Insert Into](https://www.w3schools.com/sql/sql_insert.asp) values for each of the tables
I think OP is just frustrated because they know SQL DDL and can't figure out how to create a table in Access's graphical designer, in part because they're working off poorly-written instructions.
The data type is "char" and the CHARACTER\_MAXIMUM\_LENGTH IS 6
So if there is any NULL in that column, regardless of the person I'm querying, it will cause no data to return?
Well...that fixed it. char(4)
I believe that's what I was doing. Truth be told I'm self-taught out of necessity and know just enough to get by. I changed the declare to char(4) and was able to return the results I was looking for.
Here is what I have learned about data warehousing. You have to be the biggest A-hole on the face of the planet! Super nerd troll greater than any other ever.
Video isn't very useful for learning any type of computer language. You want animated visuals: https://sqlbolt.com/
I always account for nulls if a column allows them, they can cause unexpected results.
Best practice would be to match the datatype, yes. A char datatype will always contain the number of characters specified as the "max". For instance, if you declare a char(6) but set it to "ABCD" it will actually be "ABCD " with two trailing spaces. If you use varchar it will adjust to the length of the data entered so it would be "ABCD".
this is actually wrong (or, rather, incomplete)
&gt; Do you know what it looks like? What have you got so far?
You just need an IF statement in your sproc. Assuming MSSQL: IF @c_value IS NOT NULL BEGIN select a.* FROM table1 a LEFT JOIN( SELECT * FROM table2 WHERE fieldC = c_value ) b ON a.prmykey = b.fieldA_foreign_key_table1 END IF @c_value IS NULL BEGIN SELECT a.* FROM table1 a LEFT JOIN table2 b ON a.prmykey = b.fieldA_foreign_key_table1 END
If id in table one is your primary key, and id is primary key in table 2, you would need to set identity insert to on in table 2 to allow you to insert that same key value from table 1.
Thank you. Here's a [video](https://www.youtube.com/watch?v=OtSUxy206Gw) if anyone needs a clear how to.
How does Talend compare to SSIS for ETL work? Thanks!
Thank you for the explanation. That makes sense now!
Assuming MSSQL, use square brackets `SELECT [case#] FROM ...`
 Amazon Athena ? let's hope they follow SQL standards, which is doublequotes SELECT "case#" FROM ...
If your looking for a somewhat crash course introductory lesson I found this video very helpful. [https://www.youtube.com/watch?v=9Pzj7Aj25lw](https://www.youtube.com/watch?v=9Pzj7Aj25lw)
 SELECT DISTINCT meals.name FROM meals WHERE meals.name NOT IN ( SELECT meals.name FROM mti JOIN meals ON mti.meal_id = meals.id JOIN ingredients ON mti.ingredient_id = ingredients.id WHERE ingredients.name NOT IN ( &lt;Ingredient List Provided&gt; ) ); Is what I have so far. I think it works but it may not be organized the best. I started learning SQL recently. Thanks for the advice
Uh oh. How so?
Ok, I think I see what you're saying. I had read OP's requirement as meals with *three* specific ingredients (totally not the requirement though.) My solution only works for that case. Looks like u/HalfHeart1848 has the correct solution that accepts arbitrary ingredient lists here: https://old.reddit.com/r/SQL/comments/bor30k/mysql_get_meals_which_have_only_these_ingredients/enmuk0g/
I think that second implementation is fine. The one with a weekday column. Index on biz Id and /or weekday and you'll be all set. What about holidays though? Does that matter here? May be something to consider. I would use a number for weekday vs a string.
That's it! Athena accepts the "case#" . thank you!
You can watch videos and code alongside in whatever IDE is appropriate, it's what any decent tutorial assumes you're doing. Websites that attempt to replace the IDE are usually junk, in my experience. Plus you should really be learning the IDE anyway.
This [playlist](https://www.youtube.com/playlist?list=PL08903FB7ACA1C2FB) helped me a lot
Do you mean like if a business is normally open on Monday, but closed a specific Monday that's a holiday? For something like that, we were planning on just having the business manually change the hours until it became a problem. &amp;#x200B; I went with a string because I thought it might help future me or someone else from making something that mixes up when it starts, and assumed the trade off wouldn't be that big. Thinking about it though, it does make more sense as a number so I can order them easier. Plus with Rails I'd be able to reference \`0\` as \`:sunday\` anyway, so I don't think I'd have any issues really. &amp;#x200B; Thanks!
Will try but didn’t think you will learn my listening haha. Thanks
I have learn SQL in the toilet with the SOLOLEARN app. Few minutes per day and now I can write my own codes
this works. the counts version, imo, is simpler: select m.name from meals m join meal_ingredient mi on mi.meal_id = m.meal_id join ingredient i on i.ingredient_id = mi.ingredient_id group by m.name having count( case when i.ingredient_name in (..) then 1 end) &gt; 0 and count( case when i.ingredient_name not in (..) then 1 end) = 0
&gt; Which basically means that a business that doesn't even need this functionality now has 5-7 records. Why does a business that doesn't need this functionality have *any* rows in the table of operating hours? What does that even mean?
I meant the ability to have multiple sets of hours in a day. Some restaurants, for example, close to the public in an afternoon, so wouldn’t be able to have a single start and end time for a day. (Also the hours that are stored are consumer facing, not necessarily hours that people will be there like what “operating hours” might imply. I just worded my question badly.)
Alter table xxxx alter column credits type int using credits::int This is for PostgreSQL, btw
If your RDBMS supports it. Use a CTE. That said, this nor the original still won’t be a performant way of doing things given high volumes of data but it’s a one to one replacement
&gt;`GETDATE()-1` Don't do integer math on dates. What unit are you subtracting one of here? Days? Weeks? Hours? Seconds? Use `dateadd()` and make your intention clear. Why do you need this time to "display" with the date? Is this for doing filtering of something else later? If this is for some kind of reporting output, do that formatting as close to the user as possible. Keep dates and times as their types when you're dealing with them in SQL Server. As soon as you transform them to another type (string), you'll cause more work for yourself.
 Something like Select table.Hsp_account_id, Min(table.change_date) from Table where Coalesce(table.change_date,'2099-12-31') = ( Select Min(coalesce(table.change_date,2099-12-31')) from Table t2 where table.Hsp_account_id = t2.Hsp_account_id)
Why can't you use a temp table? Create one batch that contains two queries and run them as a single batch. Select table.Hsp_account_id, Min(table.change_date) into #temptable from Table where table.change_type_C = 87 Group by hsp_account_id;select whatever from #temptable left join othertable Or, put the whole thing into a stored procedure and call that instead of a plain query.
Apparently tableau will only do one at a time...
`DECLARE @Now DATE` `SET @now =GETDATE()-1` `SELECT CONCAT(DATEADD(week,-52,@now), ' 11:59:59'` &amp;#x200B; I suspect you're pulling this code from somewhere else and aren't sure what is going on here. I agree with alinroc, you should be using a DATEADD to line 2 rather than just subtracting 1 from GETDATE().
Thanks for the advice. I'll change that to DATEADD(day, -1, @now) Just a little confused with how to add (week, -52, @now) to the same Select statement. This is for reporting, so I'll try and not convert it to a string like you suggested. Thanks again.
Well my interest with SQL has only just begun so you are right; I'm not sure what's going on with a lot of things. I did come up with this myself, however (as incorrect as it is) Is there another forum you would suggest for posting more beginner related questions? I feel quite silly in this subreddit. Thanks
No you're fine. You have to learn sometime. I only said that because you had a DATEADD in there already so I would have thought you would have used that to subtract the day too if you were the one writing this from scratch. I see this a lot when I'm maintaining code that is several years old and has gone through few different analysts along the way. There are so many different ways to do things and people have their common go-tos. I used to use the getdate()-1 trick to subtract a day all of the time but it isn't the best for suportability. It is too ambiguous.
For what it costs, it should do better. Go the stored procedure route.
From the sidebar: **Learning SQL** A common question is how to learn SQL. [Please view the Wiki ](https://www.reddit.com/r/SQL/wiki/index)for online resources.
Thanks!! I should have been a bit more clear though. I'm really asking for a preferred software for databases to practice at home. This subreddit had a great thread on it a few weeks ago but I forgot to save it.
Thanks for sharing. Wrote one myself but never got as far as sharing it with others (my bad). Have loads of tips tricks that I never have time to properly document/test. Just seem to get stuck adding to a never ending list of lists. Pm me and I'll send them on.
I'm not familiar with that package, but it looks like a query builder and the actual query is obfuscated. If we saw the raw SQL, we ***may*** be able to help here. Have you tried asking this in actual python subs?
&gt;I'm trying to migrate all of this over into an Access DB so I can just use Excel as the Front End. This doesn't sound like a great idea.... Access will let you do a few things in a kind of faux-database way, but really the only good reason to use Access is if you want to have a code-light way of creating a really basic set of forms for a front end. However, having said that.... Imgur is blocked at work so I might be misunderstanding your data. But I'd try to avoid doing this via grouping at all, which you might be able to achieve by adding a range to your lookup table, and joining on the range. e.g., Terminal Code|Tariff_Amount|Payment_from|Payment_to :--|:--|:--|:-- x|1.20|1.20|2.39 x|2.40|2.40|3.59 x|3.60|3.60|999999 Then your join condition might go something like FROM RingGo INNER JOIN Tariffs ON RingGo.ParkingZone = Tariffs.[Terminal Code] AND RingGo.SessionCost/100 &gt;= Tariffs.Payment_from AND RingGo.SessionCost/100 &lt;= Tariffs.Payment_to Then you can simply select tariff_amount from the tariff table with no grouping or aggregating at all, which will save you a lot of headaches further down the line. Of course when setting this up you'll need to make sure that your tariff table has ranges that are mutually exclusive, and exhaustive (i.e., there's no combination of zone/tariff that doesn't have a match in the tariff table, and no combination that can have more than one). This is why I've got 999999 as the tariff_to for the highest tariff. You'll also need to consider what to do about tariffs below the lowest hourly rate.
Thank you, correct, this is somewhere between frustration and shared chagrin at myself.
I know this is possible in Crystal and if it's not in tableau I am going to lose my mind. With that said, Sub reports; can you create them? If so, use your 'temp table' as the filter report and pass its output to the 'sub report' which would be your main report. inb4 RIP.
I don't think you can.. exactly... do that in tableau. I COULD generate all results and then filter it for my report, but it would create several millions new lines of data that i would have to then filter in every report, which would be pretty cumbersome. I'm a Sequel journeyman and a tableau novice, so, it's possible there is a way I don't know about. Just hoping for a SQL answer.
SoloLearn is also great as an intro to other languages. I picked it up for C#!
If that's truly the case it's sad an antiquated reporting solution(s) is more capable in what one would deem the most commonable of ways.
Data Analyst or Business Intelligence Analyst Might not be as heavy on math as you would like day to day. But could segue to a data science type position after you get more experience.
My understanding is that LEFT OUTER JOIN and RIGHT OUTER JOIN are functionally equivalent if you switch the A and B tables. Not entirely positive if that's still the case with a 3 table join. Have to think about that.
The answer is correct just want to clarify the &amp;#x200B; On table1 = table2 part &amp;#x200B; does table1 and table 2 requires to be L and R respectively?
Oh, I don't think the order of the table.fields matters in that regard, as long as they are qualified with the proper alias.
You may not use the advanced math on day one, but being able to recognize and work through the logic statements will give you a leg up in a data analyst or BI roll. Then down the line you’ll find opportunities to use additional math skills.
I don't think this is true. Run the following using Initial SQL (NOT custom SQL) SELECT t.sp_account_id, MIN(t.change_date) INTO #TempMinGroup FROM Table t WHERE table.change_type_C = 87 GROUP BY hsp_account_id ; SELECT * INTO #TempTableFinal FROM OtherTable ot LEFT JOIN #TempMinGroup g ON g.hsp_account_id = ot.hsp_account_id ; Then, run this using custom SQL SELECT * FROM #TempTableFinal Using this method I've never run into any issue with defining complex queries.
No. These are the exact same: from TABLE1 as t1 left join TABLE2 as t2 on t1.key = t2.key from TABLE1 as t1 left join TABLE2 as t2 on t2.key = t1.key
Thanks everyone for the reply. Cheers!
Other than sql, what language do you use to write reports in ssrs? I don’t use SSRS
You can check out Strata Scratch platform as they have datasets preloaded with the questions you can practice with. They source their questions from technical interviews from companies so I found it helpful and relevant to working on a job.
You can't point Tableau at a #table as far as I know.
Tableau is a piece of shit.
&gt;I am writing a report for Tableau, and i can not use temporary tables. You can write a stored procedure that uses #tables and dump the results into a table, then point to Tableau. This would be the best practice. If you insist on using custom sql in Tableau, why can't you simply use a sub query?
Couple of things: 1. Your GROUP BY may not behave as you expect, since you're not performing any AGGREGATES on DATAAREAID, SALESID, SALESNAME, ITEMID and LINEAMOUNT. 2. You're not using any of the values that you SELECT in your subselect B, so you don't really need to SELECT anything more than what you're using for your JOIN to B 3. It looks like the ambiguous column is IN your subselect, you reference it by table alias in the first part of your main query, but then don't do the same in the SELECT portion of your subselect (although you did use the alias in your GROUP BY) I've reformatted the query to make it easier for me to read, and added the aliases to your subselect columns (and changed the aliases in the main SELECT to not be the same as the table names, I find aliasing a table/view to be the same as its actual name makes it harder to follow and remember if that instance in a query is the \_real\_ reference or the alias reference. &amp;#x200B; `SELECT` `ST.DATAAREAID AS COMPANY` `,ST.SALESID` `,ST.SALESNAME` `,ST.ITEMID` `,SL.CUSTACCOUNT` `,SL.QTYORDERED` `,SL.SALESQTY` `,SL.LINEAMOUNT` `FROM dbo.SALESLINE AS SL` `INNER JOIN dbo.SALESTABLE AS ST` `ON SL.SALESID = ST.SALESID` `AND SL.DATAAREAID = ST.DATAAREAID` `INNER JOIN (` `SELECT` `STABLE.DATAAREAID AS COMPANY` `,STABLE.SALESID` `,STABLE.SALESNAME` `,STABLE.ITEMID` `,LINE.CUSTACCOUNT` `,LINE.QTYORDERED` `,LINE.SALESQTY` `,LINEAMOUNT` `FROM dbo.SALESLINE AS LINE` `INNER JOIN dbo.SALESTABLE AS STABLE` `ON LINE.SALESID = STABLE.SALESID` `AND LINE.DATAAREAID = STABLE.DATAAREAID` `GROUP BY` `LINE.CUSTACCOUNT` `,LINE.QTYORDERED` `,LINE.ITEMID` `HAVING COUNT(*) &gt; 1` `) AS B` `ON B.CUSTACCOUNT = SL.CUSTACCOUNT` `AND B.QTYORDERED = SL.QTYORDERED` `AND B.ITEMID = SL.ITEMID;`
Unfortunately it's difficult to get the tools in this place so have to stick with what we have. On the plus side it's been a decent learning experience. This works, cheers mate.
Good question, but I think this was originally linked: https://www.quickcode.co/learning-guide/learn-sql-guide
Different people learn in different ways.
Can you paste in your full error message and let me know what version of SQL &amp; SQL Server you're using, please?
The custacct in your subselect exists in both Line and STable tables and SQL Server can't distinguish which one you want to select. Use your alias to identify which table you want custacct to be sourced from.
Thanks for the insight. I was reading through the book you recommended and realised datawarehousing is focused around working closely with the business and the technical work only accommodates that. This deviates a bit from what I would like to focus on in a job. For example, at my current work, we have two project teams within database department. One works closely with the business to add or change business logics in SQL. The other is my team with much less business interactions because it's about improving database scalability by refactoring SQL codes. I wonder if there is a clear defined job title for a SQL development role where your primary focus is to optimise database solutions rather than define one by talking to the business? My 20yr+ exp coworker also avoids doing any business requirement analysis and couldn't care less about in's and out's of how the business operates but he's extremely knowledgeable about the in's and out's of SQL server.
I also got a python parser that does this. Was going to share on Github but this seems much superior
That may get tedious but that's your call. Where I previously worked, we had a holidays table that we could left join to and based on a match we would state the day closed. My situation may be different as it was a global trading day calendar and I don't want to overly complicate your design. (We had a dates table so our implementation and needs were different to determine when a global stock exchange was open vs closed vs half day.) I was advising on holidays just so you're aware that it will come up and if you can solve for it ahead of time vs getting caught off guard.
Yes. I mostly use SQL for transformations and cleaning. If I'm moving somewhere in the same environment, it does the loading too. You can even use it across database servers with database links.
&gt; FETCH FIRST 10 ROWS OFFSET 20 figure out a size, but still, an easy approach
I use SQL as an end to end solution (master data management). Like a giant function. You load data in it's original format via file or table, it gets transformed into things commonly understood as an ODS, changes to data are recorded in history database as well as moved all the way to output database (reports, warehouse,analysis, output) while stopping at a denormalized structure. Our C# middle tier has been reduced to just a middle tier. All logic is in SQL. Anything other than retrieval is data driven (data driven calc engine, dynamic report generation, content decisions, etc). No ssis, just pure SQL data driven movement. 3 years ago I would have said this was insane... now I can spin up 64+ databases and 20k tables views procedures functions etc... in about 24 hours of dev time... from an empty instance and no scripts.
don't recommend database links... they're prone to misuse and performance issues.
Thanks, that was my next question was how it does the loading but a link makes sense. Do you manually push the queries or do you know of any software that does it on a timed basis (like every morning or something) I'm relatively new to this but trying to understand movement vs basic statements
Gotta do it the hard way. Find the business user and ask.
Yes you can, using the method above. Granted you aren’t pointing to the temp table in the database per se but if you generate it in tableau using initial sql then you can.
I am 99% positive you need to make it a ##table, and that a #table won't work.
The thing is I can’t really ask anyone except the data architect but this person is very busy and I don’t wanna bother them with questions
Bummer. Seems like you got setup for failure.
This is a very convoluted way to achieve concatenating two names... Can't you just use an Excel function and be done? Anyways, with an SSIS package you would need an "Excel Source" pointed to your input file. Then you can use one of the transformation components (probably Derived Column Transform) to concatenate the two names and output it as a new column. Now you will have 3 columns coming out of the transformation. Now you connect the output from the Transformation to an Excel Destination which points to the output file. You don't need a "table" anywhere, it just passes the data around in the package. Again though, you shouldn't need to go through any of this, a simple Excel formula should give you everything you need.
The concatenation of the names was just a kind of first pancake to see how these packages work. Hopefully if I can make this work and understand the package I would be able to produce reports from my 800k row excel without having to do each process manually. But is the dtsx package the way to go, in terms of, putting data from a csv, doing something to it in sql server and then exporting it as a csv? is an SSIS just the import part of the dtsx etc. Thanks so much for your reply
This is the direction I have been naturally moving in since day one. I have a natural hatred for anything GUI based, find them incredibly frustrating, and very slow/inefficient to modify compared to working in pure code.
This is a classic contradiction. On one hand, you have been given a fool's errand and a non-expert user should never develop a data dictionary. On the other hand, you are the perfect person to put together a comprehensive data dictionary, because in order to do so you must learn everything, which will then make you an expert user. It will also serve as a sort of validation against what an expert user might say without validation. Very slow and tedious process. You will have to 'bug' someone with a lot of questions or you won't succeed.
try bothering the other business users, analysts, Excel report pushers, people who have been there a long time, ANYONE. Don't be scared. Schedule 30 mins on their calendar a week if they are "that busy". If they refuse to help tell your boss ASAP instead of weeks down the line and you have nothing to show for it.
You're talking about two different things. SQL is the language used inside of a database to Create, Insert, Update, or Delete data. It's also used to define the structure of the data: create tables, indexes, etc. An application like SSIS uses SQL to query the database, which can they be used to extract/export data to another location. Python can certainly be used to create an ETL application/program. &amp;#x200B; It's going to depend on the intent and use of the data.
You're going to have to go to the source of information and that means bugging experts. Don't rely on your assumptions, even if something seems obvious, there's no telling what happened between design and production and what that table actually s does now.
You made an SSMS extension like this also? That's crazy! How long ago did you build it? Don't feel bad. It's hard to put yourself out there like this. Though, it is a great feeling to share your work with everyone and receive praise, it can be pretty stressful and anxiety driven for many reasons. I also have zero time for most anything nowadays, but thinking in terms of quick wins is what keeps me going. I primarily built this because it was the most annoying thing to me ever so I was driven to fix that and thought this is useful for literally anyone using SSMS I must share:)
That's cool, I think I recall seeing something out in the wild using python to parse this...maybe that was you!
Oh, this is for SQL Server Management Studio only, not SSRS. I don't use SSRS either. This extension is for outputting SET STATISTICS IO, TIME ON into a beautiful layout directly inside of SSMS:)
Should post in the sqlserver sub Reddit, you'll probably get more his there. This sub covers Structured Query Language.
I'd separate concerns - Focus first on the import - my preference is to do any sort of transformation inside SQL itself - then run an export.
Sounds a lot like the situation I was in! I'm entry level as well and the architect is my lead/mentor. Always very busy. I had to make data definitions for a bunch of queries we were writing but had never done something like that before. The other responses are right though, you have to just put your balls out there and bug someone about it. Otherwise you will have a finished(wrong most likely) product that will need to be reworked. Good luck and cheers!
Yeah I’ve been doing “View Dependencies” which helps, just not sure of all the tables their purpose yet
You are right!
Thx!
Try it - it works with # (at least with SQL server)
I just did, and it didn't work. Steps: 1. In SSMS, `select 1 as t into #test` 2. Open Tableau, connect to that SQL server 3. Using custom SQL, `select * from #test` It doesn't work. It does work if I use an ##table.
You're going to need to get access to the front end app and a super user, preferably one from each functional area that uses the app.
They use Access for the front-end, which is also something I need to figure out: Which tables are used in the front-end?
`SELECT,`?
If a data dictionary is needed, it should be made during creation of the table(s). I’m a bit shocked that they would have you attempt such a thing. As others have said, the business users are who you’re probably going to need to ask, but if you can find out who created a table and/or get access to their code you can do it that way. To be honest, that’s a shitty thing for them to have you attempt. It makes me question their knowledge and background in OOP and SQL.
That’s isn’t what I said to do. Create the temp table within tableau via initial SQL, then you can access that table in custom SQL. That is good to know however that you can directly connect to a global temp table created in SMSS.
What is "initial SQL" -- not sure I understood. I have always used global temps for this purpose, so I'm assuming initial SQL is some kind of staging which sucks data up off the database and stores it within Tableau so that you can use custom SQL to point to it? How would you refresh in that example?
Everyone is too busy for one question over and over. Show some initiative and make some guesses and then highlight those where you guessed and ask someone to look over it after you’ve taken a first swing. People should be willing to answer if you queue things up, and have obviously put some work in to get started. Also, for SQL Server, use to column description field once you have a confirmed definition. A good modeling tool will pull those in. You’ll thank me later!
I had the same questions a couple of months ago. how can I practice sql from home? so I installed the free oracle database but there is a learning curve on installing and enabling the sample schema. just search google on the how to. I'm using sql developer for writing queries along with excel and power bi for visualization at work. I'm new to power bi tho.
I only made an io start parser. Minimal effort, given time constraints.
If you can get the distinct years and customer IDs, I think this could be performed with a LAG() OVER(PARTITION BY).
Have a error msg parser too linked to put to straight back into a database.(feedback loop completed)
Crude but important to me.
Haven't learned python yet believe it's very popular. Technology I work on is quite outdated. Same principles apply though.
Ah! Yeah, I think that may have solved it. I had to do a CTE since you can't use HAVING or WHERE on window functions, but it is looking good. Here's what I ended up with: WITH secondYear as ( SELECT customer_id, (LAG(year(purchase_date), 1, 0) OVER ( PARTITION BY customer_id)) - (FIRST_VALUE(year(purchase_date)) OVER ( PARTITION BY customer_id)) as 'yearDiff', FIRST_VALUE(year(dd_date)) OVER ( PARTITION BY dd_donor_id)) as 'nextYear' FROM transaction_table) SELECT DISTINCT(customer_id), yearDiff, nextYear FROM secondYear WHERE yearDiff = 1 Thanks a ton for getting me on the right track.
Cool! No problem.
Oh good lord you need to find a new job if they using Access.
OSS / NFP / NGO specifically? like, if you're just bored and want to spend your day tuning queries, I can ask at work... we've got enough stuff to keep you busy for the rest of your life ;) issue we face is that anyone we talk to that can spell SQL seems to feel *quite*confident with their salary requirements.
&gt; OSS / NFP / NGO specifically? Correct. I'm specifically looking for public work so I can add that section to my resume :)
I would advise caution, but that's my usual advice when asked about professional services aimed at people just beginning their career. There are a few warning signs, their blog hasn't been updated in two months, their github has been inactive for three months, which all ties into the same period as the ban. They registered their domain privately, which is common for personal websites, but is a bit odd for a businesses, since businesses want to show a clear chain of custody over their website. &amp;#x200B; It's kind of a cliche at this point, but the best place to see real world problems is stack overflow, and they will usually have workable answers below them.
have something to show on your resume, or be able to show the code?
Mainly resume, as I understand that doing work for pro bono can still lead to proprietary code. I have enough utility apps and such that I can show if code is needed. I was able to search and find some open source business applications/suites that may meet my needs. Particularly Odoo and OpenEMR. I'll begin digging into those I suppose!
It's a small but growing company. Only one person, the data architect, knows everything inside and out. One other analyst who has been with the company longer also knows all the processes so I could ask him as well.
It's a pretty small company though.
Why are you mentioning elasticsearch in the post title? Is that where the data is currently stored? If so you should know that it already has pretty strong fuzzy search capabilities https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-fuzzy-query.html
I heard it has strong capabilities. That is the only reason I mentioned it.
Validate your 70-762 Exam learning and preparation with our most updated 70-762 dumps. Dumpsgator has experienced IT experts who gather and approve a huge range of Microsoft [70-762 Braindumps](https://www.dumpsgator.com/download-70-762-braindumps-questions.html) for MCSA-SQL 2016 Database Development Certification seekers. Practicing our 100% updated 70-762 Practice Tests is a guaranteed way towards your success in Microsoft 70-762 Exam. Get the huge discount.
How are you going to link the vendor to anything?
I guess put VEND\_CODE in Sales too then?
Is it syntax wrong? Then you should say the db engine you are using. Otherwise you have to say what you want to achive, to see how is it wrong/good
My Bad, im on MY SQL workbench 8.0. And yeah it was a syntax error. Im trying to write a command to create a view to list the students who achieved grade 7 in a subject
Table is called students but in the join you reference student
fucking thank you lmao
What parts of spreadsheet UX do you want to have? If you’re just wanting to visually check out the data from time to time, maybe R’s or Python’s tabular data handling would give you enough programmatic flexibility to do basically arbitrary analysis (with SQL-like syntax even) but still let you look in on the data really easily, and they’d be plenty fast at that data volume. If the UX elements you want are more on the “click and manipulate” side of things, not sure. I’ve heard of some BI tools with those capabilities but can’t speak to their quality.
as an aside, you have this (slightly reformatted) -- SELECT surname , firstName , subjectCode FROM students INNER JOIN grades ON grades.studentID = students.studentID WHERE grades.grade = 7 i would change it to this -- SELECT surname , firstName , subjectCode FROM students INNER JOIN grades ON grades.studentID = students.studentID AND grades.grade = 7 notice the difference? it doesn't really matter because the database optimizer is pretty smart, and will execute it the most efficient way but with a WHERE clause, you are saying "join all the rows from grades to students, then throw away the ones that aren't grade 7", whereas with the ON clause you are saying "join only the rows from grade 7 to students" in other queries, though, more complex ones, it may well make a difference, so it's a good idea always to consider putting restrictive conditions into the join when that's actually what you want
Holy shit thanks man. Didn’t even think of that.
you can use excel as a frontend an a database as the back end.
First an inner query: top two, order desc Now an outer query of that: top one, order asc.
Thank you for the suggestion!
Spaces in SQL object names make me cry
first year student my man, my bad
It means they ain't teaching right, but I have seen teachers do the same :(
I was looking into them as well but won't be using them now. Off topic from OP, but what databases can I use at home to practice SQL?
SSIS (aka .dtsx packages) are like workflows for data manipulation. "ETL" (Extract, Transform, Load) is typically what it is used for. So yes, generally, if you want to have an automated process that loads data from a CSV into a SQL Server table, does some manipulation, and writes out a CSV then an SSIS package is where you would do that. That being said, there are always multiple ways to get from A to B and it really depends on what you are trying to accomplish. Excel already has a lot of functionality that might let you accomplish what you want without jumping around to SQL Server. For example, you can load data into the Excel Data Model and that opens up all kinds of data manipulation options through Power Pivot &amp; DAX. You can find out more information on that [here](https://support.office.com/en-ie/article/Import-and-analyze-data-ccd3c4a6-272f-4c97-afbb-d3f27407fcde#ID0EAABAAA=Data_models)
The data entry part of a spreadsheet. The resistance for me is that when using raw SQL, I have to keep track of which ids correspond to which name/enum/etc for relationships.
What is the question?
Oh sorry, yes this is a question, my bad
Select table c field Join table a and table b. Join table b and table c. Where table c field = ?
Join the three tables on their respective joining keys and use a where condition.
I want select 2 columns from table A and B
From both tables? Four columns total?
Do you already have a DB to access? If so, I'd suggest putting at least an hour or two into Heidi SQL. Very user friendly interface and that allows manual data entry and many other features without needing to "know" queries that well. If you're talking about setting up a DB yourself...that's a big step that you may find is more risk than reward from what you've explained.
That or the product table.
1 column from table A and 1 column from table b
It’s gonna be hard to beat spreadsheets for data entry, but if you’re set on using sql, you could set up an “ingest” table that you insert only raw values into, and then write sql to handle the mapping from raw values to ids, and then insert the mapped values into your target table(s). Is the data (and your goal) shareable here?
I think some of the other comments are really overcomplicating this. You could do it as simply as joining with the given ingredients to make sure all ingredients in the recipe are in the list of given ingredients, and then left joining back to your join table where Ingredient_ID is not in the given ingredients and ensuring the no ingredient join is null. Real simple and quick example (sorry it's ugly, time crunch - I'll try to circle back): select m.mealname ,i.ingredname from #meals m join #jointab j on m.mealid = j.mealid join #ingred i on j.ingredid = i.ingredid left join #jointab noIng on m.mealid = noIng.mealid and noIng.ingredid not in (1,2,3) where i.ingredid in (1,2,3) and noIng.joinid is null here's the temp table data I used so people can double check for mistakes... I make plenty. create table #meals( mealid int ,mealname nvarchar(25) ) create table #ingred( ingredid int ,ingredname nvarchar(25) ) create table #jointab( joinid int ,mealid int ,ingredid int ) insert into #meals values (1, 'spghet') ,(2, 'spghetAndMeatball') ,(3, 'chickenParm') insert into #ingred values (1, 'noodle') ,(2, 'redSauce') ,(3, 'meatball') ,(4, 'chicken') ,(5, 'parm') insert into #jointab values (1, 1, 1) ,(2, 1, 2) ,(3, 2, 1) ,(4, 2, 2) ,(5, 2, 3) ,(6, 3, 1) ,(7, 3, 2) ,(8, 3, 4) ,(9, 3, 5)
MySQL is very common and free to install. Ms SQL also had a free version.
Thank you! So just install MySQL and find random datasets online to play with?
You could use MS Access as your front-end with good UX forms and connect to a Postgres database via an ODBC driver. This would allow you to maintain specific data integrity, with good logical data entry forms and still give you the ability to use Postgres on the backend. The only real hurdle would be making sure you cast data types from MS Access to the equivalent postgres data types.
Hey, A bit late, but thank you very much for your reply. I tried your suggestion and it worked like charm! Tnx!
First of all, thanks for your help. This was not homework, this is an independent project of mine. I am studying SQL on my own. I'll post more code next time if I need help. I usually come to reddit when I am completely stumped, however. In this case, none of my drafted queries made any sense. I understand the sentiment though and will try to do that in the future. You can see my other comment here for my attempt at solving it. About your solution, I am having trouble fully digesting it but I think that it makes sense. I still have trouble parsing the order of operations in SQL. Thanks for your time.
Of course! Always glad to help - and only suggest the homework part because a lot of people on this sub see something that looks like homework and if there's no code they will downvote and ignore it and people are left without getting help. Sounds like a fun project. &amp;#x200B; Quick parse of it - you join the INGREDIENTS and MEALS with your JOIN table to give you all the MEALS that use the given INGREDIENTS. In my test data this leaves you with all 3 MEALS - Spaghetti, Spaghetti and Meatballs, and Chicken Parmesan. You have to find a way to exclude Chicken Parmesan because you don't have any chicken or parmesan. Every answer you've already been given works - and maybe semantically makes more sense, but it won't to any optimizer running your code (and isn't a set-based answer to your problem, which SQL loves). You just need to exclude any MEAL that uses an INGREDIENT not in your given list - so LEFT JOIN back to your join table and look for any MEALs using INGREDIENTs not in given list (the "noIng.ingredid not in (1,2,3)"). Chicken Parmesan will be the only MEAL that has an INGREDIENT\_ID not in (1,2,3) - so it's the only MEAL that will show up in that *SET* of MEALs. When you add the "and noIng.joinid is null" to the WHERE clause - it removes the *SET* of MEALs in your JOIN table aliased "noIng" from your SELECT - so Chicken Parmesan is removed from the RESULT SET. You're left with only recipes that use the given INGREDIENTS and have no INGREDIENTS that aren't in the given list.
Nope not shareable. In this method, how can I assure proper relationships? I have a few 1:m and m:m relationships.
you really should only cross join weeks and items, not the whole transaction set. Simply left join to transactions after that.
This is actually wrong, despite the upvotes. Downvoting until the duplication issue is addressed.
Look into exists condition.
&gt; .. one column .. The requirements do not specify how many rows to return.
How do you become Sr.?
how about: 1. Swapping the subselect for a cte, 2. Using a Group By Id instead of Where Rownum = 1 3. Switching from an Outer Apply to a Left Join 4. Using Coalesce e.g. with cte_ot as ( select ID, 1 as Flag from Schema.OtherTable ot group by ot.ID, 1 ) select Field1 ,Coalesce(ot.Flag, 0) as Flag from Schema.Table t left join cte_ot ot on t.id = ot.id;
Are you saying that the query only returns 23 results or that only 23 of the dates correctly cast? I don't think the cast statement should impact the number of results returned. Also, which database are you working on? I would search "Cast dates in different formats in \[db name\]"
BTW, you will likely need a case statement to handle some of the non-dates like "Upon Receipt"
You'll simplify your logic a LOT if you add the "isClosed" column and put an index on it vs creating tables to store completed orders. Otherwise you end up with two sets of schemas, both "theoretically" kept in sync as you copy data from one to the other. If you are using MS SQL Server, you can possibly go further. Table partitioning can help if you are on Enterprise Edition. Partition on the isClosed column, with all "0" values in one partition, and the "1" values in a different partition. When you write any queries against the table, if you can use the isClosed column in the WHERE clause, it will speed things up because it will only have to look in one of the partitions.
Assuming sql server here, when I run into this I manually clean up the data, then create a new column for the "fixed" data. If you do select * from table where try_cast(datefield) is null you can figure out what data SQL is unable to convert to date as it stands. Run some updates to clean up the common offenders, then in the new "fixed" column set it to try_cast(datefield) and you should be good to go.
I'm assuming this query is something the users want/need, so tell them to fix the data in that column because it's impossible to report out of as-is. Give them an Excel sheet with the primary key and the date column in it, have them type in correctly formatted dates, then re-import it into the table.
I'll have to rework the `where` condition if I do that right?
I mean, without more details from OP how would you solve this? I can add a DISTINCT to the select but feel free to suggest a different solution.
This is an ok workaround, but the scenario I am in is against a large quantity of data, so the APPLY would be way more efficient. Thank you for this suggestions because this is what I am going to go with until I can figure out the APPLY. It is greatly appreciated.
Try using isdate() first to make sure it’s a valid date time
Use regex to find matching date patterns, insert a standard yyyy-mm-dd format into another column, and essentially clean up data as best as you can. Otherwise your final results are going to be inaccurate.
You've got a query, and you're saying it's slow. But you don't tell us much about it: how much data do you have? What's are the cardinaltiie like? What is your physical structure? What indexes do you have? Sample data? Other columns? When I have a performance problem with a query, I look at the query plan. Then, I think about all of the above information and try to figure out how I make the query better. You've given us absolutely nothing to go on. Posting the table definitions, the indexes, the data would help. The query plan will tell you exactly what indexes you are using. Seems like you really want an index on `maltran` with `(trans_type, trans_date)` columns to satisfy your `WHERE` clause. What's in the `#Weeks` table? Is it just a a list of week numbers?
Great answer. At this point I just downvote any posts that don't mention their SQL variant. If they can't be bothered to read the side bar I can't be bothered to answer them.
There’s SAS university which is a free download, if you’re interested in learning it. Wouldn’t you rather learn python or R instead though?
Oracle moves in mysterious ways, I wouldn't assume APPLY will be more efficient. I don't have access to a DB to experiment myself, but I think the issue in your original code is combining 'WHERE t.ID =ot.ID AND ROWNUM =' in the WHERE &gt; WHERE t.ID =ot.ID &gt; &gt;AND rownum = 1 Shortcircuit evaluation does not work here - Imagine ROWNUM was calculated before the 't.ID =ot.ID' So in the first row of the entire table (i.e. ROWNUM = 1), t.ID =ot.ID will probably be false, so there will be no rows returned for that subselect. What you would need to do then, is another subquery: SELECT Flag FROM ( select 1 as "Flag" from Schema.OtherTable ot WHERE t.ID =ot.ID ) ex2 WHERE rownum = 1
Oh, so this problem is impossible to solve in ANSI SQL?
Sorry, db2 variant
Figured it would be easier for me to learn SAS syntax since I was more familiar with SQL
There's about 2.5 million rows and you're right, those indexes would make this query faster. And the `#Weeks` table is just one column of the numbers 1 through 52. I guess I was more curious if there was a better method than the `cross join` that we're using. It takes about 1:30 to run and we'd rarely need run it so I'm not super worried about performance. Just thought I'd use it as a learning experience.
You could write the expression in the excel file. In the first empty cell type update CLIENT set DOB = "DOBCellName" where clientId = "ClientIdCellName" then copy that all the way down the excel sheet and copy that into ssms
Makes sense, thanks for the input
PROC SQL is similar enough to Oracle SQL for you to pick it up in no time. Why SAS though? If it's the tool of choice in your workplace then fair enough but it's rapidly getting replaced by the likes of R &amp; Python now. I've a background is Oracle &amp; SAS but I'm now using open source tools like postgres and Python. You can use Python to connect to any database and execute SQL to your heart's content. Once you master that, Python has numerous libraries for data wrangling.
I was using rownum the same way I'd use TOP in T Sql. In my real query, I'm having to go to a surgery payment table where rows from the main query could have no or many entries in the payment table. Essentially all I'm wanting is a 1 if there are any or 0 of there's not. The CTE above is going to basically get a distinct list of ids (the real table doesn't have a primary key, so it is 6 fields). In TSQL, the apply would correlate to the inner query and only execute for rows returned but the outer query. That's the reason for preferring to use apply.
For the type of data work the banks is currently utilizing, building and automating datasets with SAS seems to be the approach we have other applications that we use for profitability and analysis (what my whole career is built on) i am just trying to Branch out and become more of a data Wrangler/Data scientist, and less of a Management reporting/ Account specialist... I'll need to look more into python I didn't realize it was being written in open source apps like you mention above.
This data shouldn't be stored in the first place, it should be calculated when you query for it. At least in an OLTP system. Reformatted for better readability. Use EmployeeCustomTabFields Update EmployeeCustomTabFields.CustCurrentYearsWithTheFirm = (DATEDIFF(qq, EM.HireDate, getdate())/4) INNER JOIN EmployeeCustomTabFields ON EM.Employee = EmployeeCustomTabFields.Employee * Lose `Use EmployeeCustomTabFields`, as that's used to switch database contexts and `EmployeeCustomTabFields` appears to be a table, not a database. * Since `EmployeeCustomTabFields` is a database (at least I'm guessing based on the context), you haven't yet specified the tables you're updating. * `EM` appears to be an alias for a table, but you haven't said what table it is you're aliasing. * If you need fractional years, you need to divide by `4.0`. Otherwise you'll get an integer result. * An `UPDATE` with a `JOIN` takes the form of: UPDATE U set field=value FROM TableToBeUpdated U JOIN OtherTable O on U.JoinField = O.JoinField WHERE OPTIONAL WHERE CLAUSE &gt;I just want to make sure I am doing this sql expression correctly Running your query inside a transaction which you then roll back will teach you a lot about whether it's valid or not.
 &gt; Just thought I'd use it as a learning experience. What is it that you'd like to learn? I think it would be a great opportunity to learn how to read an execution plan and troubleshoot performance. But to help with that, you must provide a lot more information. &gt; I was more curious if there was a better method than the cross join that we're using. Depends on what result you'd like: what is "correct"? You give this problem description, but maybe there's more to it: &gt; We need a weekly quantity of items transacted. &gt; What we are having trouble with is assuming a value of 0 if there were no transactions in the week. I guess I'd start by writing a query that gave the number of transactions per week, per item: SELECT DATEPART(ww, m.trans_date) WeekNumber, m.item, SUM(m.qty) Quantity FROM maltran_mst GROUP BY DATEPART(ww, m.trans_date), m.item Seems like you'd also only be looking at the last week year-over-year given the clause you wrote for `m.trans_date` in the query you provided. I guess that limiter should be added, but it seems like it's going to make a bit of ambiguous output: is week 33 last year, or this year? The query I wrote does have the problem of not producing a row for any week where a particular item was sold, so I'd proceed by trying to do an outer join with reference tables that get me those values. You've said that #Weeks is 1..52, so that's good. Do you have a reference table that gives your distinct item numbers? If so, outer-join against that, too. If I assume you have an Items table which distinctly lists item numbers, we can make some progress: SELECT W.Week_Num, I.Item, ISNULL(WeekOrderTotals.Quantity, 0) AS Quantity FROM ( SELECT DATEPART(ww, m.trans_date) WeekNumber, m.item, SUM(m.qty) Quantity FROM maltran_mst GROUP BY DATEPART(ww, m.trans_date), m.item ) AS WeekOrderTotals RIGHT JOIN Items I on WeekOrderTotals.item = i.item RIGHT JOIN #Weeks W on WeekOrderTotals.WeekNumber = W.week_num Maybe that's what you're after, and maybe it's faster ...
No, I don't believe so. Besides, there are many standards, from SQL-86 to SQL:2016. To make it even more complicated, every vendor implements a different set of features, most adding non-standard features like try_cast and isdate.
&gt; I was using rownum the same way I'd use TOP in T Sql It doesn't work the same way, unfortunately. &gt;In TSQL, the apply would correlate to the inner query and only execute for rows returned but the outer query. That's the reason for preferring to use apply. Honestly, CTEs in SQL Server and Oracle don't work like that (they do in postgreSQL) - any where or join where they are referenced will be 'hoisted', so it shouldn't be any less efficient than APPLY.
It is, it’s only really used in financial services and pharmaceuticals though and it’s super expensive to buy, so it’s less accessible to medium sized companies
Bummer. Thank you for your detailed responses.
That's why you don't store calculated values... Store their date of birth and calculate their age in a view
Yeah banking IT guy here so.. SAS is the go to data mining/manipulation/wrangling tool for all of the industry from what I can't tell.. like I mentioned I'm having a hard time moving into a SAS role only knowing SQl on Orcle GL and Profit Apps... Needed a few certs under my belt so that I can hopefully land something
Work hard, find a boss who will help you learn then become so valuable they have to promote you?
u/alinroc is correct OP. The Structure is not close to an Update. The 4.0 **IS** needed. I ran it through a Select locally on my DB at work, as : select p.hmy, convert(decimal(4,2),datediff(qq,p.dtlastmodified,getdate())/4.0) from property p join tenant on p.hmy = t.hproperty and got below as my Years since last modification: 2.25 3.00 3.00 3.00 3.00 1.50 3.00 3.00 3.00 2.00 1.50 2.25 3.00 3.00 3.00 for my DateDiff (which I assume is what you want). So his Update should work. Just follow his Structure
Edited. I mean Op’s requirements were pretty vague. There are different optimal solutions based on what the tables and data actually look like. You could just add DISTINCT to deal with the duplication issue on the original query.
Use the Concatenate Function in Excel. Start on the First blank Cell under the next available Column next to the First Row with Data. My Example is A1 and B1. So you type the below in C1 &amp;#x200B; =Concatenate("update &lt;table&gt; set &lt;column&gt; = ",A1," where ID = ",B1) The Drag the little Square down to the bottom of the last Cell where the data cuts off. It should Dynamically change based on the other columns. Then Save as Comma Delimited and Open with .txt &amp;#x200B; Copy Paste into SSMS
No problem. If the performance is disappointing, it's probably best to talk to a DBA if you have access to one, and look at the execution plan, indexes and partitioning.
A correct end result would be a week number, an item, and a quantity transacted. Once we have that result we do the variability analysis for the past year on each item. The execution plan says 44% of the query is spent on a `Hash Match` which I'm fairly confident is the `sum(case when w.week_num = datepart(ww, m.trans_date) then m.qty else 0 end)`, but to be honest I could definitely study up on how to read these exec plan xml files. So we can easily get a set of the transacted quantities grouped by week. And we have the weeks table. Let's say an item sold 5 in week 1, 0 in week 2 and 6 in week 3. The records produced by a pretty straightforward query would get us: week_num|item|qty :-:|:-:|:-- 1|item|5 3|item|6 So if we join the #weeks table we'll have 50 records where not only is the quantity null but the item is as well. I can't tell if I'm just dumb or there really is no way to get around needing something like an item_week table so there's a week_num|item record for each item. Which is pretty much what the `cross join` is doing.
Thanks! Thats sort of what i ended up doing.
Thanks!
Never use not in. Use a count sub query and check the result is 0, or any other thing. Not in returns true if the element is not in the list, or if any element in the list is null.
Based on the last .jpg, it seems that maybe there is data in *employees* table that is not shown? And if that data contains rows with entries for buildings 1w and 2e, that would effectively weed out ALL the rows from *buildings*. Otherwise I'm not seeing any obvious issue in the SQL off the top of my head. Do you HAVE to use a NOT IN clause? Could you instead do a NOT EXISTS subselect with a correlated subquery?
&gt; Never use not in. If what you're trying to do doesn't care about NULL it's a perfectly valid method.
No matter what platform this is on, you need to refactor this. To catch and correct data as it is entered. My suggestion would be to create a date dimension with an int pk using YYYYMMDD format. What makes this tricky is the (MM[/-]DD) vs (DD[/-]MM) complication. Are you able to verify that one or the other exists? Can you implement controls on the input?
&gt; which I'm fairly confident is the It could be the GROUP BY over that, yes. But it could also be your join condition. Or something else, if a view or computed index is involved. &gt; but to be honest I could definitely study up on how to read these exec plan xml files. It's probably going to be easier to look at graphical output in SQL Server Management Studio. &gt; needing something like an item_week table That would help, yes. You need to know the set of (item, week) tuples you want to have because they're not always going to be in the transaction table. I suggested getting them with two outer joins: one outer join to the `#Weeks` table you said you have, and another outer join to an `Items` table that I imagine you must have.
 Something to do with the null in the set returned to the not in. Nulls are funky in SQL. I don't know what SQL dialect it is using, but the following works. For future, reference is something funky happens, and it everything looks correct, verify that you handled the NULL case. Also, different databases handle NULL differently. select * from buildings where building_name not in ( select coalesce(building,'cat') from employees )
Ternary logic, you probably have Null somewhere in the building name. Either add WHERE building IS NOT NULL inthe subquery, or use Not exists.
Yup, MySQL has a few dbs you can check out, here is one that is structured as a db of employees: https://dev.mysql.com/doc/employee/en/
It *should* work. It must be that the tutorial site would prefer you use "not exists" instead, which does.
Check out this tool to troubleshoot SQL and Kerberos/SPN issues [link](https://blogs.msdn.microsoft.com/farukcelik/2013/05/21/new-tool-microsoft-kerberos-configuration-manager-for-sql-server-is-ready-to-resolve-your-kerberosconnectivity-issues/)
in what case would it be ok to duplicate records in normal circumstances? Would you truly think that being asked 'select all columns from table a' a response "select a.* from a cross join a b" should be accepted? It is asinine to require every customary requirement to be spelled out.
You do need to add DISTINCT (which wasn't there initially) to the first one (and _that's why_ it is not the best option to begin with) but you shouldn't need to add it to the next one (and that's why it's the right thing to do).
Dang, he's a turd for having you use "Microsoft Sequel Server", no matter now it's pronounced!
What if you just put "SQ"?
Do you want to be a sql administrator or do you want to build reports and focus on queries?
The Job was for a Supply chain company. Sql is used for queries pull databases and feed it to the software the company use to build scenarios based on said info.
So the latter
Correct... sorry if I’m vague It’s been a tough day.
&gt;strings that don't have a digit Ok, , just made the changes.. thanks man!
&gt; normal circumstances &gt; customary Define, please. &gt; Would you truly think that being asked 'select all columns from table a' a response "select a.* from a cross join a b" should be accepted? Wait, what.
what DBMS?
Query 1 runs forever, never returns results, Query 2 returns results in under a half second. I'm baffled
There’s an index for the name column on the product table?
nope, index is on sku no less
You’ll be okay man. Another opportunity will arise. Good on you for learning from an experience and taking steps to improve yourself.
Thanks man ... I really appreciate your message.
Could you clarify what you mean by “proper”? As in, “I want to model the data appropriately”, or “this combination of values should never/always happen”, etc.
oh yeah, good point. microsoft sql server 2016, SP2
This can occur if you have an spn configured to your old box on that account. Try: setspn -L domain\accountname This will list the spn's on that account. If you see the old decommisioned server name on that list, you will want to setspn -D to remove those, restart your sql service and try again.
You could also check out Vertabelo Academy. They also often run deals, and they have a pretty comprehensive series of SQL courses that let you learn by doing - they pose the problem and give you a coding interface in the browser. So far, I’ve been pretty impressed.
Okay, so after you've had some time, what questions gave you trouble, if you are interested I'll give you feedback on what they were looking for and where to read up on it and any important lessons I wish I knew before I started SQL.
I highly recommend the official SQL Certs from Microsoft or Oracle, depending on which is more popular in your area.
They were things like give me the count of clients who have buy this product or give me the total count of clients from number of sales very simple things one thing that influenced my failure was the interface of the sql program I was using I started with sql on mac and I looked different totally throwing me off.
UGH, I'd probably fail that interview because I can't type fast and I'm a BI dev doing equal parts SQL and ETL. The two I deal with are mssql and oracle SQL developer: I'd recommend trying both and playing with the unix one that is command line only (find your hotkey and data discovery &lt;finding the map&gt; shortcuts) then if someone uses something else practice it before the interview.
Man even tho is the same syntax the environment throw me off.
Not an answer, but have you taken a look at the query plan? It may give more insight.
Quick caveat. I have certs (MCSA) but I'll be the first to tell you they are pretty much worthless for getting a job. What got me my current job is my skills and experience and my employer didn't give a flying f whether I had a cert or not. I got my certs for *myself* so I could prove to myself I had the skills to get a good job. They also gave me the chance to learn about things I hadn't come across in the 'real' world. My advice is to just write code. Fish swim, birds fly, programmers program. Download whatever DB you're using and just make stuff. When you get stuck, use google. Watch Pluralsight videos. Just make stuff.
Quick caveat. I have certs (MCSA) but I'll be the first to tell you they are pretty much worthless for getting a job. What got me my current job is my skills and experience and my employer didn't give a flying f whether I had a cert or not. I got my certs for *myself* so I could prove to myself I had the skills to get a good job. They also gave me the chance to learn about things I hadn't come across in the 'real' world. My advice is to just write code. Fish swim, birds fly, programmers program. Download whatever DB you're using and just make stuff. When you get stuck, use google. Watch Pluralsight videos. Just make stuff.
Quick caveat. I have certs (MCSA) but I'll be the first to tell you they are pretty much worthless for getting a job. What got me my current job is my skills and experience and my employer didn't give a flying f whether I had a cert or not. I got my certs for *myself* so I could prove to myself I had the skills to get a good job. They also gave me the chance to learn about things I hadn't come across in the 'real' world. My advice is to just write code. Fish swim, birds fly, programmers program. Download whatever DB you're using and just make stuff. When you get stuck, use google. Watch Pluralsight videos. Just make stuff.
Quick caveat. I have certs (MCSA) but I'll be the first to tell you they are pretty much worthless for getting a job. What got me my current job is my skills and experience and my employer didn't give a flying f whether I had a cert or not. I got my certs for *myself* so I could prove to myself I had the skills to get a good job. They also gave me the chance to learn about things I hadn't come across in the 'real' world. My advice is to just write code. Fish swim, birds fly, programmers program. Download whatever DB you're using and just make stuff. When you get stuck, use google. Watch Pluralsight videos. Just make stuff.
Quick caveat. I have certs (MCSA) but I'll be the first to tell you they are pretty much worthless for getting a job. What got me my current job is my skills and experience and my employer didn't give a flying f whether I had a cert or not. I got my certs for *myself* so I could prove to myself I had the skills to get a good job. They also gave me the chance to learn about things I hadn't come across in the 'real' world. My advice is to just write code. Fish swim, birds fly, programmers program. Download whatever DB you're using and just make stuff. When you get stuck, use google. Watch Pluralsight videos. Just make stuff.
Quick caveat. I have certs (MCSA) but I'll be the first to tell you they are pretty much worthless for getting a job. What got me my current job is my skills and experience and my employer didn't give a flying f whether I had a cert or not. I got my certs for myself, so I could prove to myself I had the skills to get a good job. They also gave me the chance to learn about things I hadn't come across in the 'real' world. My advice is to just write code. Fish swim, birds fly, programmers program. Download whatever DB you're using and just make stuff. When you get stuck, use google. Watch Pluralsight videos. Just make stuff.
Quick caveat. I have certs (MCSA) but I'll be the first to tell you they are pretty much worthless for getting a job. What got me my current job is my skills and experience and my employer didn't give a flying f whether I had a cert or not. I got my certs for myself, so I could prove to myself I had the skills to get a good job. They also gave me the chance to learn about things I hadn't come across in the 'real' world. My advice is to just write code. Fish swim, birds fly, programmers program. Download whatever DB you're using and just make stuff. When you get stuck, use google. Watch Pluralsight videos. Just make stuff.
Quick caveat. I have certs (MCSA) but I'll be the first to tell you they are pretty much worthless for getting a job. What got me my current job is my skills and experience and my employer didn't give a flying f whether I had a cert or not. I got my certs for myself, so I could prove to myself I had the skills to get a good job. They also gave me the chance to learn about things I hadn't come across in the 'real' world. My advice is to just write code. Fish swim, birds fly, programmers program. Download whatever DB you're using and just make stuff. When you get stuck, use google. Watch Pluralsight videos. Just make stuff.
Quick caveat. I have certs (MCSA) but I'll be the first to tell you they are pretty much worthless for getting a job. What got me my current job is my skills and experience and my employer didn't give a flying f whether I had a cert or not. I got my certs for myself, so I could prove to myself I had the skills to get a good job. They also gave me the chance to learn about things I hadn't come across in the 'real' world. My advice is to just write code. Fish swim, birds fly, programmers program. Download whatever DB you're using and just make stuff. When you get stuck, use google. Watch Pluralsight videos. Just make stuff.
I have certs (MCSA), it's nice to have them and all but I'll be the first to tell you they are pretty much worthless for getting a job. What got me my current job is my skills and experience and my employer didn't give a flip whether I had certs or not. Most employers are like this. I got my certs for myself, so I could prove to myself I had the skills to get a good job. They also gave me the chance to learn about things I hadn't come across in the 'real' world., but what overwhelmingly gets you a job is experience. My advice is to just write code. Fish swim, birds fly, programmers program. Download whatever DB you're using and just make stuff. When you get stuck, use google. Watch Pluralsight videos. Bottom line, just make stuff and learn while you're doing it.
Did you run an explain plan (execution plan)? Check for differences
Did you run an explain plan (execution plan)? Check for differences
Smells like an expensive lookup on the exec plan to get the second field in the select statement. If it is then add this as an included field in the index being used and then re run the exec plan to see if the lookup is removed. If none of this is the case then check fragmentation of used indexes. I even see disk fragmentation cause issues like this in the past also. But it all starts with what the difference is with exec plans for the 2 queries.
Smells like an expensive lookup on the exec plan to get the second field in the select statement. If it is then add this as an included field in the index being used and then re run the exec plan to see if the lookup is removed. If none of this is the case then check fragmentation of used indexes. I even see disk fragmentation cause issues like this in the past also. But it all starts with what the difference is with exec plans for the 2 queries.
I have a solution, it’s called Unix
Thanks I'm going to do this
From my experience SSIS works really fast with SQL Server databases and loads in real time where Talend will load in batches, but where a lot of logic and transformations are done in the query when working with SSIS, Talend is a bit more programmatic since logic like that is done in the tMap using Java. I like that SSIS does the mapping almost automatically, though Talend has an auto map, it still feels like there's an extra step. But Talend really shines with the components, most of the time if you can think of something you might need to do then Talend has a component for it.
From my experience SSIS works really fast with SQL Server databases and loads in real time where Talend will load in batches, but where a lot of logic and transformations are done in the query when working with SSIS, Talend is a bit more programmatic since logic like that is done in the tMap using Java. I like that SSIS does the mapping almost automatically, though Talend has an auto map, it still feels like there's an extra step. But Talend really shines with the components, most of the time if you can think of something you might need to do then Talend has a component for it.
From my experience SSIS works really fast with SQL Server databases and loads in real time where Talend will load in batches, but where a lot of logic and transformations are done in the query when working with SSIS, Talend is a bit more programmatic since logic like that is done in the tMap using Java. I like that SSIS does the mapping almost automatically, though Talend has an auto map, it still feels like there's an extra step. But Talend really shines with the components, most of the time if you can think of something you might need to do then Talend has a component for it.
Do you use SQL in your current job at all? If not, see if that's an option, since you've got at least basic knowledge. If you do, work on doing more/learning more.
why not a join?
Have a look at Airtable
Sams teach yourself sql in 24hours book was (and continues to be) a good resource for me when learning, though I had the luxury of access to a production e commerce database at the time. You learn the most by doing in my experience. When gearing up for my last series of interviews I used hackerrank’s sql track and that was helpful to gain confidence.
This is how I got started too.
for syntax, just refer to **da manual** it's what i do still, even after 30+ years working with SQL
Look up stuff about relational database design and normalization. Referring to temporary tables, those are intended to be used in a particular session, such as a stored procedure, to be a work area to work towards tour end result. If you want data to persist, which is the norm, then you obviously dont want to store it in a temp table.
Can you double and triple check the dayatype of the author_id column? What is the data type of the category column? You may need to examine the execution plan to see if it gives you any clues as to what is going on. If the query erros, try the estimated execution plan. Casting the column to int if ANY of the data in that column isn't valid ints will definately throw an error at you - regardless of your search argument.
I want to model the data appropriately
With regards to database structuring, I would look up entity relationship diagrams first if you haven't heard of them, they will help visualize the database, how the tables are related, and establish cardinality. I think it's probably best to really spend some time to think about how you want the database to be set up because it's not necessarily too easy to change things later. You'll also want to establish if this system is an OLTP system or a DW, which will help determine the basic schema and how the tables might need to be normalized. The normalization will address the first question. The second question it's a use case thing. Just for one example, there are query limitations when it comes to filtering aggregate functions. To get around this you can use a CTE (temporary table) to hold the aggregate query information, and in the same batch, running another query that selects the data set from the first. There are obviously plenty of instances though that you might use temporary tables though. I think the Murach's books are pretty good. There is a reference book for the 98-364 exam (Microsoft Database Fundamentals) that would cover most of this. Though you might just find all that you need online. Keywords would be like: ERD, Normalization, Cardinality, Temporary tables. Hope this helps some.
The bible for data warehouse design is the Kimball book "The Data Warehousing Toolkit"
Mostly cause I suck at SQL, any suggestions would be welcome
SQL Antipatterns: Avoiding the Pitfalls of Database Programming [https://www.goodreads.com/book/show/7959038-sql-antipatterns](https://www.goodreads.com/book/show/7959038-sql-antipatterns)
smaller tables isn't just about speed (less RAM should take less time to fetch), but the ability for simultaneous updates. Should there be a reason for some users to be blocked from updating a setting for example, because someone else is updating another setting. The speed benefits of flat tables generally come from poor database design, query design or extremely specific use-case. Yes locality is a thing, but given that you should only ever be returning a subset from database, it should cost less (through use of indexes, constraints, bound queries) than coordinating updates to a mono-table.
ah, okay, this did it! I forgot that estimated query plan still shows the anticipated converts, so I didn't bother. it was actually the category column causing the casting because data quality somewhere put an ID in the category column \*as well as\* the ID column - so I was assuming the integer overflow was this too-big author ID, but didn't realize that same too-big ID was also in CATEGORY\_C. CATEGORY\_C is a varchar column even though it's supposed to only have INTs, basically as a legacy definition. When I'm only searching on one author, it does the author search first, which means we never hit this bad data value for CATEGORY\_C. When I search for multiples, it decided to do the implicit convert first, which is what caused the bug. Lesson learned - unless SQL finally decides to get smarter about errors and tells you \*which column\* caused the overflow, I need to look at the estimated plan instead of making an assumption based on which column I know that value to be in :)
Use sql server export wizard and bring it into sql and update it that way
Add the non fqdn to the spn first. Also check the services on the instance make sure they are enabled. Doesn’t make sense as you don’t need Kerberos to connect to sql you can use ntlm
Took a quick glance. This seems to be the answer I've been looking for. Thanks!
That seems completely pointless to me...
yeah, it's called thoughtspot and all these other tools trying to get in on the NLP analytics train
Not really unless it was REALLY specific. If its faster to type than trying to speak it out, its useless.
yeah, you are right that there are many such natural language tools. But, I think, they can compose only simple queries as they are not good at handling ambiguties generated by the inherent natural language. SQL, on the other hand is unambiguous.
Do you mean that no one would ever compose SQL queries on phones or tablets? I see tools such as [Domo](https://www.domo.com/) that provides BI tools and visualizations right from phones/tablets. I wonder what the use cases are?
Download SQL Developer for macOS. It’s java based so it’s multi-platform. https://www.oracle.com/technetwork/developer-tools/sql-developer/downloads/index.html
Maybe but usually if its that important it must be a pretty complex query and if it could do it there would be not much use for writing SQL in general. If its simple I could just write it on my device quickly.
I mean if there was an autocomplete feature, I would have no issue typing out queries. I visualize things in my head first, so typing them out is the hard part, autocomplete solves that.
I use a tablet at work, but it has a keyboard so I use that. The only time I remember it's a tablet is when someone tries to point at something on the screen and the cursor jumps to where they are pointing. I can't imagine trying to dictate SQL being much fun. Entity names with underscores or no spaces mixed with abbreviations, acronyms and spelling mistakes would make this a nightmare for the databases I use. Perhaps with a fresh database setup with touch or tts in mind it might work but for any existing system it's probably more of a handicap.
&gt; However, typing SQL queries on phones or tablets is painful. JFC..... get a real computer to work on.
why...? why are you writing queries on a phone?? I don’t see a large need for this. I can’t even connect to my company’s database from a mobile device. most older companies are going to have on-prem servers that require vpn access to connect to. Unless your company is fully cloud I don’t think you’d be able to work from a mobile device. and if it’s cloud you’d be working in the web interface to send queries.
Glad it helped you. I've using it too and it's such a good tool.
Displaying dashboards on a tablet or phone certainly has its use. But creating those dashboards on a phone seems like torture.
Ugh... Sorry for the poor formatting
Not at a computer where I can test this, but can't you just have the code give it a generic name like Trans\_Snap\_New and then use sp\_rename to rename it with your variable at the very end?
You could check for the existence of the table (i.e. table\_52019, table for May 2019): IF NOT EXISTS (SELECT 1 FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = 'youSchema' AND TABLE_NAME = 'table_52019') BEGIN --EXEC proc to create table END In the proc, you can combine the Month datepart with the Year datepart, convert to NVARCHAR and then prefix it with the rest of your desired table name (i.e. table\_). Following this you would insert into your newly created table, if the table exists you insert the desired data into it, if it doesn't you create the table and then insert into it: --Main Proc BEGIN DECLARE @table_name NVARCHAR(20) SELECT @table_name = 'table_' + CAST(DATEPART(MONTH, GETDATE()) AS NVARCHAR) + CAST(DATEPART(YEAR, GETDATE()) AS NVARCHAR) IF NOT EXISTS (SELECT 1 FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = 'yourSchema' AND TABLE_NAME = @table_name) BEGIN BEGIN TRY --EXEC proc to create table END TRY BEGIN CATCH -- ERROR HANDLING END CATCH END ELSE BEGIN TRUNCATE TABLE @table_name END INSERT @table_name SELECT data FROM youTable WHERE dateRangeLogic END
Not really (when it comes to phone and/or voice interfaces) but using Stored Procedures could make working with SQL via the aforementioned interfaces a bit easier?
Are you mad or what?? Why anyone want to write code in phone or tab.
The only certification programs worth anything are the ones offered by the platform vendors themselves.
You probably have to work with dynamic SQL since a table name in a variable will not work most of the time. Select \* into @variable from &lt;table&gt; will not work since a variable can also be a table. It will try to insert all the data into that variable. &amp;#x200B; More important question: Why do you want to do this? There might be better ways to get to your goal.
We're on prem and I've connecting on my phone through VPN only once in 3 years because I didn't have my laptop and it was a weekend emergency. Otherwise, I see it as pointless. This is why we have laptops and netbooks, etc...
I assume this a question of interest for some development project you’re working on. Most DBAs will have their laptop with them or the person in Need of an emergency would have to wait till he/she does. I think it would be a waste of dev hours. I hate my phone for typing. I wouldn’t use it even in extreme emergencies since I have people/interns to do that. But that’s my opinion. Good luck OP.
I don't know SQL Server, but many rdbmses have table partitioning. The name of the partition can be whatever you want (within limits of your rdbms).
I’m a DBA and while checking certain areas for database exceptions or issues from my phone would be beneficial, it’s mostly handled on my end by using [spotlight cloud](https://www.spotlightcloud.io) and PowerBI (where I have reports that look at exceptions, issues, etc). 90% of the time I can tell what the issue is (and make decisions) from my phone. I have my laptop with me for the other 10%.
I can barely get Alexa to turn the lights on and off. I can’t imagine trying to dictate code to it.
I write queries on a tablet all the time. I use a keyboard that links to the tablet. Who is going to speak to a NLP engine while in an airport to write queries. I don’t see any value in this.
&gt;More important question: Why do you want to do this? There might be better ways to get to your goal. I've come across dynamic SQL before but SQL, in general, is not my main language. I'm competent enough to do what I need to do, but the 'fancier' stuff is something I don't have much experience with. &amp;#x200B; Anyway, I need to create monthly subsets of the data in order to complete reporting. Those snap shots are also used again for quarterly reporting. &amp;#x200B; Technically I don't need to use them and could run my queries against the main table except it would probably take 4x as long to run. &amp;#x200B; I know there's a way to do this so I've been dieing on this hill trying to figure it out, lol. I'd even be fine with declaring the suffix of the table as an option and somehow using it.
That is a clever option and one I haven't thought of... I'll have to test it out when I'm back in the office. Thanks for the suggestion.
Awesome. In the past, I think I've come across something similar when searching for an answer online. &amp;#x200B; I'll have to play with it when I'm back in the office but thanks!
SQLServer allows you to index based on where conditions. You might want to investigate putting those on for your date ranges. You might be surprised at the performance gains.
Will definitely look into it, thanks for the info.
You could look into partitioning and indexing to improve performance. It should be possible to run the queries over the entire table without much performance loss. With partitioning you can split tables based on a date column into separate files, an index can be used to further optimize the table. This could work especially well if you only need a few columns of the table. Search for "covering index" for more info.
That doesn't create a local Oracle SQL Server. I managed to do that with docker, finally. I have an SQL server running on docker and I can connect to that using SQL developer. Took me a while
Why do you need new tables? Could you get what you need from views?
I’ve never heard of anyone needing or ever had a need to install Oracle SQL *Server* on a local machine. What’re you trying to do exactly?
I'm working on a college project and this is somehow one of the requirements. I also don't have a server available to host that database :D
Building new tables is not really the right way to do this. You will have problems as you are creating two sources of truth. You need to build a date dimension and then also partition the table indexes. You can use int keys for the date (YYYYMMDD). Then join to the date dimension. Select tt.* from trans tt Joind d_date dt On dt.d_date_key = tt.trans_date_key Where MonthYr = [MMYYYY]
If you absolutely have to, you could create a view for each month, but honestly that seems like it is a problem you should get out ahead of. If its Oracle you can actually name the partitions and access them directly. I'm pretty sure SQL server 2016+ supports this too.
I would use this after a CDC process. There will be some write latency to the index IIRC.
What class is using Oracle for the RDBMS? So you’ve been trying to install Express Edition?
Still great knowledge and the fundamentals are accurate, however, it's accurate for row-based databases. Some of the book is outdated now that we have column-store cloud DBs like Redshift, BigQuery, Snowflake, etc.
I work in this industry. The point of Domo or related BI tools is for a SQL user to create the dashboards on a computer. Executives and other data consumers can then view the data on their phone/tablet, if they're traveling etc.
Remove the extra characters and then look into Levenshtein distance for the name matching afterwards https://en.wikipedia.org/wiki/Levenshtein_distance You'd need to create a function which applies the levenshtein algorithm in access. But really don't use Access. Instead use Postgres (which is free) and has levenshtein as well as some other fuzzy match algorithms built in. https://www.postgresql.org/docs/9.1/fuzzystrmatch.html
Well, the subsets are just "select *'s" from the main transaction table so reconciliation shouldn't be a problem if it's ever needed.
First, you need to identify all the functional dependencies. Normalization through BCNF is based on functional dependencies.
Table.Column
Orders.OrderId is letting you know that the OrderId column comes from the Orders table. Customer.CustomerName means CustomerName comes from the customer table. Etc.
Or depending on the database engine: database.schema.table.column
Thanks for responding. I removed all the special characters, spaces, and items that I noticed were put in(LLC, Co,Company,etc) as well as the trailing number if it had one. After that I think I only had 500 lines that needed to be manually changed due to misspellings. Was not expecting it to be that easy.
This. Orders and Customer are just the table names, but if they have identically names columns in each, you need a way to distinguish which column(s) you mean. So you need to qualify the reference using table.column notation to avoid getting an error.
If you are creating a table, I'm assuming you are getting the data from a table that is already there. What I mean is that in the future if you add a column to the table, you will have to go add it to all of the other tables or whatever you report in will spit out an error saying it cant find a column if you try to query an old table. Famous last words "this table will never change". If you need static tables (to not update, say you have a return 3 months later) I would create an archive database (on the same server). There is always a problem when you have two copies of the same time period/data that don't disagree. Also please DO NOT select * in your query. It is a nightmare to maintain. List the columns out. If that doesnt make any sense let me know.
Second or named instance will be running in a different part than the default instance. Look on the SQL log for the instance the app needs to connect to, find the port that it is using and open the firewall for that port.
Only one app on SQL....yes named instance (I was told by app dev) but thru dont know the 2nd instance....they assume is a single ... How would one start finding 2nd instance....logs u said?
Unblock 1434 port, too
I usually set up the firewall to allow sqlsvr.exe instead of looking for ports. Not sure if that's better or worse...
1433 is the port for the default instance which is also known as the "unnamed instance'. You have installed a named instance (named SQLEXPRESS) and it will use a pretty other than 1433. I'm guessing that your installation instructions say something like Use 'computername\SQLEXPRESS' as the name of the server to connect to. Where computername is the actual name of your server. Having SQL applications work ok with the fire wall is off and then stop working when you turn the firewall on is a giveaway that you haven't got the port number right. Many people do not realize that port 1433 is for the unnamed instance and that other instances need other ports. (You should keep 1433 in the exception list of the firewall file as well, SQL uses it for more than just the default instance.) If you look in the SQL log file for the SQLEXPRESS instance, you will see a port number. SQL picks a port number during installation of named instance and uses that port when it starts up. This port number is written to the SQL log for troubleshooting purposes. The SQL log file is just a text file without a txt extension, you can use notepad to look at it. I don't remember exactly where that file is located on the hard drive and Location varies slightly by SQL Server version. Sorry for formatting, I'm on a phone.
What does the .. notation mean?
And if you're dealing with a Linked Server in SQL Server, `server.database.schema.table.column` At which point, if you're not using aliases you're just a masochist. Or a sadist. Depending upon whether you have to read the code in the future, or your foisting it upon someone else.
In SQL Server, two dots between an identifier means you're taking the default for the current user for the identifier that would go between those two dots normally. Usually you see this to skip over specifying the default schema, which is `dbo` unless your user has a different default. If it's possible to do this in other places, I've never seen it used that way; I honestly don't know if it even is possible. `database.schema.table` is the same as `database..table` if `schema` is the default schema for the current user.
Tcp 1433 is the default port. You have to check if you're using a different port on a named instance. Open sql server configuration manager, then open.. SQL Server Network Configuration -&gt; Protocols for (instance name) -&gt; TCP/IP Tab over to IP addresses and look at what is under "IPAll" because that's the port you're using. If it's in dynamic ports, cut and paste it under "TCP Port" instead so that it doesnt change on you. For a named instance you will also need to unblock TCP 1434 and UDP 1434 for the browser service. So in total your firewall rules should allow: TCP 1433 TCP 1434 UDP 1434 TCP (whatever you found under IPAll)
Yep, that's needed for the browser service. TCP AND UDP. Good call.
Knew it was easy but didn't understand some queries I was seeing. Thanks!
It's easy, but I dislike using that notation because it can cause breakage if you start using schemas for security within your database (which is a totally OK thing to do). Accepting a default like this is making an assumption that things will always be the way they are today, and that can hurt you in the future. It costs nothing to specify `.dbo.` instead of `..`, so be explicit about the schema when using 3-part naming.
I hear where you're coming from and know what you mean. I only keep these tables on a rolling 12 month basis. Older ones get deleted. There are some other restraints which is why I'm on the path I'm on. The most cumbersome is that the data environment is run by a third party with Citrix as the only means of access. There are some other restrictions in the environment that essentially preclude automating these processes.
Enables TCP/UDP 1434....SAME ISSUE Noticed a line in logs: Server is listening on ['any' &lt;ipv4&gt; 60447] - opened TCP on this port *same issue Looked in logs....shows SQL server network interface library could not registered the service principal name (spn) [MSSQLSvc/ServerName.Domain.local:SQLEXPRESS] for the SQL service. Windows return code: 0x2098, state 15
Why are you adding in the dollar sign and casting as a decimal?
They want values related to money to have a dollar value and to two decimal places.
Who is they? They really want your sub-query to have a dollar sign concatenated to the decimal? For what reason?
Sorry, should have been more clear. This is for a assignment, so it is a requirement monetary values to have dollar signs and to two decimal places.
Haha yeah my boss likes to drag tables in with the GUI instead of aliasing so sometimes I get code that looks like: select some-long-server-name.somedatabase1.bba.sales.id1, some-long-server-name.somedatabase1.bbb.sales.id2, some-long-server-name.somedatabase1.bba.sales.fact1, some-long-server-name.somedatabase1.bbb.sales.fact2 from some-long-server-name.somedatabase1.bba.sales inner join some-long-server-name.somedatabase1.bba.sales on some-long-server-name.somedatabase1.bba.sales.id1 = some-long-server-name.somedatabase1.bbb.sales.id1 I don't know how he makes heads or tails of it. I get dizzy just looking at it
He makes heads or tails of it by only looking at the GUI designer. Personally, I can't stand GUI query designers. I understand better in code. Which is odd, because I'll often draw our relationships on a whiteboard to figure out the code.
Un-blocked port 1434UDP/TCP...No resolution.
I dont see any reason to add the currency sign to your where clause. Take that out and see what happens. Put the currency In your results that are returned...but not the where clause
Do a quick google search on the most popular sql platform should give u an idea of where to start. The complete sql boot camp by Jose is done in Postgres, where as the MySQL bootcamp is done on mysql. My self is currently in this training program for Microsoft sql server, I heard it is the most popular one for a developer role.
I completed the Ultimate course by Colt Steele. I highly suggest it. I even help out in the Q&amp;A forums there, so know that both Colt and his TA Ian are very invested in the course.
Yes. It's likely the currency symbol is causing a mismatch against the WHERE clause criteria.
/u/Rdmsco11, this is almost certainly the answer. Doing unnecessary formatting in your `WHERE` clause is a good way to get bad results.
Look at firewall logs then on server. Will say what it is blocking. C:Windows:system32:firewall I think
That seemed to have fixed it! Thanks
data manipulation never happens in a where clause ;-)
I took this course as well. A lot of the SQL can be applied elsewhere. Most of the differences will be with the functions of the server you're using and some naming of the data types.
If you absolutely need to format the monetary value, you should be doing it in the SELECT section, not the WHERE clause
Why does it need to be a subquery? A subquery of what? Is the first query defining your temp table? I’m not really clear on how your code relates to your question but it’s a common problem to have to join to a “middle” table in order to get a relationship. You can just do a three-way join, or you can use WHERE EXISTS.
See my other comment too.
Spn's are relevent to kerberos authentication. Test connecting to your database from the local server and also from a remote location (like with ssms from your desktop). If both connections succeed, you're pretty much fine for the spn. If you can do this without bumping heads with a company standard, the easy resolution to spn issues is to change your sql service to run as local system. (But I don't think that's your problem since you stated disabling the firewall fixes the issue) Is the application piece sitting on the server with the database, or is your database server dedicated to only the database? Some rare applications like to do things on other ports or create folder directories. It sounds like the app has some requirement that isn't known. I would ask your app dev for any other ports they will need based on app function. If you don't get a helpful answer, a netstat from the app server while they recreate the issue should show you a port with either time-wait or syn-sent. Of course, the next step is deciding if those ports should actually be opened, since they aren't likely to be actual sql ports, and are probably something like rdp, unc ports, or etc which should not be required for a dedicated database server connection.
You can install and run it on your local computer if you wanted. They have made the install in two pieces though. One is the database engine and the other install is the GUI so you can interact visually with the database. In a lot of prod instances, best practice dictates no GUI on the local server which is partially why they were separated.
[SQL Server Express](https://www.microsoft.com/en-us/sql-server/sql-server-editions-express) on your home PC take a few moments to evaluate a front end program, my advice is to choose one which uses SQL code (rather than a drag-and-drop or point-and-click graphical interface) my tool of choice is HeidiSQL, but i've never used it with SQL Server
Alright so running the following command: .\kerberosConfigMgr.exe -q -l Shows me that both Service Account/Required SPN associations are in the "Misplaced" status. I guess I have to remove them and re-add them?
Go to YouTube and watch a beginner's video to SQL server 2017 it will answer all your questions
I took /u/kevine323's advice and I went and ran the Kerberos Configuration tool. Found out that the two Service Account/Required_SPN associations are coming up as "misplaced". Instance_Name Service_Account Required_SPN Status MSSQLSERVER domain\ADSERVICEACCOUNTNAME MSSQLSvc/hostname.fqdn.domain.com Misplaced MSSQLSERVER domain\ADSERVICEACCOUNTNAME MSSQLSvc/hostname.fqdn.domain.com:1433 Misplaced Looks like I have to delete the SPNs and re-add them? What account should I do this from? An account that has Domain Admin access? When I run the "setspn" command to add, should I also run that from an elevated privileged command prompt?
Yes, delete and re-add, then reboot. If you’re able, run the GUI as an admin (preferably a domain admin). The GUI will give you the option to create a file that will fix the SPN or fix it for you automatically. Whether the GUI runs the SPN commands or you choose to manually, you’ll likely need domain admin to fix it.
Just remember the cardinal rule: 1. Always SELECT * FROM TABLE first before doing anything else.
I ran the "Fix All" option from the GUI. Everything came back green and fixed. I gave the server a reboot and was able to successfully connect remotely from the same data center. Other data centers are still having issues, but that has more to do with AD convergence more than anything. I am going to wait an hour or so for AD to replicate across all sites and give it another shot. Thanks for your help on this. This issue has been a thorn in my side since we migrated servers and it's good to have it resolved.
Sounds good!
Thank you for that! I sometimes get the tables backward when I'm doing OUTER JOIN....IS NULL. The diagram makes that out pretty clear.
&gt;SELECT YEAR FROM TABLE WHERE NOT EXISTS ( SELECT YEAR &gt; &gt; FROM TABLE InnerTable WHERE TABLE.YEAR = InnerTable.YEAR AND VAL &gt;= 7 ) GROUP BY YEAR &amp;#x200B; Basically the trick is to use the logical negation of whatever predicate you have and check for nothing not being correct instead of everything being correct.
I think you have two primary issues. &amp;#x200B; 1. The main issue is likely that you are doing = dateadd instead of =&gt; dateadd. On Sunday, you are saying grab all records where the Work\_Date is 6 days from today. You are not saying grab all records greater or equal to from 6 days ago. 2. The second issue is you should handle Monday in your case when clause, if Monday comes around, you're just going to look for active employees. &amp;#x200B; Side thoughts: 1. I don't think you need to cast your getdate as a date, get rid of the extra cpu cycles. 2. [Date tables are amazing](https://www.mssqltips.com/sqlservertip/4054/creating-a-date-dimension-or-calendar-table-in-sql-server/), I highly recommend to use one if you can and this make will things more efficient and easier to code.
Wouldn't self join have the same table twice? So, Select A.column1, B.column3 from tableA as A join tableA as B ON etc.
I think you're trying to use "case" as a statement. case is a value expression: case when &lt;condition&gt; then &lt;value&gt; ... end an example of the syntactically correct use would be CASE WHEN DATENAME(DW, GETDATE()) = 'Friday' then 3 end &gt; 2
Say what now? Writing queries on your phone? Why? I don't even like texting people on my phone and have a wireless keyboard that lets me switch to multiple devices. I could see a use for using voice recognition to write frameworks of queries, but it would require a very robust type of AI to make it useful, and overall wouldn't be that practical.
Okay so yes, I believe this is possible. I'm commenting so I can find this post when I'm at my computer and I'll update this as soon as I can.
Not necessarily, nor is that the intention. Consider a table of employees. This table has 3 columns: id, name, manager_id If I asked you for a list of employees grouped by their manager you would join the table to itself. For example t1.id = t2.manager_id (assuming t1 is the manager and t2 is the employees).
I think we're talking about the same thing. Wouldn't you'd do the t1.id = t2.manager_id in the ON part of the JOIN? Wouldn't the actual JOIN connect the same table but with a different alias, then in the ON clause differentiate between the two sets of the same table?
Okay I'm going to try and do this from mobile so formatting will be non existent. With a table called Table_A having column 'Field A' that contains your Name|Country|Age values I used this to select the Country value. SUBSTRING(A.`FIELD A`,(LOCATE('|',A.`FIELD A`)+1,LOCATE('|',SUBSTRING(A.`FIELD A `,(LOCATE('|',A.`FIELD A`)+1)))-1) AS 'Country' This will work where all three values, Name|Country|Age, exist. I would build a case statement that counts how many |'s exist to apply what I wrote above or a similar line that is only looking for everything to the right of the first |.
 SELECT A.`FIELD A` ,CASE ROUND((LENGTH(A.`FIELD A`) - LENGTH( REPLACE ( A.`FIELD A`, '|', ""))) / LENGTH('|')) WHEN 2 THEN SUBSTRING(A.`FIELD A`,(LOCATE('|',A.`FIELD A`)+1),LOCATE('|',SUBSTRING(A.`FIELD A`,(LOCATE('|',A.`FIELD A`)+1)))-1) WHEN 1 THEN SUBSTRING(A.`FIELD A`,(LOCATE('|',A.`FIELD A`)+1)) END AS 'Country' FROM `table_a` A
That works for me to account for either 1 or 2 |'s.
There's a free learning tool called SAS University Edition that you can use to help learn SAS: [https://www.sas.com/en\_us/software/university-edition.html](https://www.sas.com/en_us/software/university-edition.html) . There are also some free e-learning courses, the intro SAS Programming 1: Essentials course touches on using SQL in SAS: [https://www.sas.com/en\_us/training/offers/free-training.html](https://www.sas.com/en_us/training/offers/free-training.html)
Yes and isn't that what the example showed?
Thanks, Your comments were helpful! Thank you for quick reply as well.
this seems to be an XY problem (what is that you were trying to do before you settled on your current solution?) &gt; this would have to go into a few different case statemtents and i wanted tp be able to change it in one place instead of lots. one way to add a calculated value for all your result set records is to cross join: select a.some_value * cj.parameter from mytable a cross join (select &lt;expression&gt; as parameter) cj Another thing: when @now between @lasthour and @lasthourlimit then shouldnt be a part of your case expression - since all of your variables are pre-calculated, the result of this comparison should be known by the time you need to run the query
"where" clause filters your base dataset (the result of the "from" clause, including all your joins). If you need to filter the grouped up records, you can use "having" or wrap your query into another one
You should add union, intersect, except and both Applys.
How do you know 456 goes with 123 and not 124?
What's your lunch/mid-day routine? Are you working on one single stream of work or bouncing between multiple tasks/projects/items in a queue?
You've gotten some good solutions with the case statement comments, but I'd add that regular expressions can make this type of string splitting much more compact and can handle trickier strings. I don't know what platform you're on but look for documentation on functions like "regexp", e.g. regexp_replace() in Oracle, Postgres and others. Learning the basics of regular expressions is totally worth it because it pays off across not only SQL but pretty much every programming language and tons of applications.
Yes. My solution is to be super productive in the morning, then space out notifications of my work throughout the day so it seems like I'm going at a steady pace.
Usually bouncing around to different tasks. Usually have a few i spread my time out between them.
And scroll reddit all afternoon? 😂
Eat more green leaf veggies at noon. Avoid highly caloric meals based on high glycolic carbs. Do more water in the morning. Use this window to schedule meetings and catch up on alternatively work that is not coding.
https://blog.jooq.org/2016/07/05/say-no-to-venn-diagrams-when-explaining-joins/
All good ideas! Thanks. I am doing keto and i added in an afternoon coffee to see if that helps.
Something semi-major came up and I've not been able to work on this until today. &amp;#x200B; I tried your suggestion and it's not working. I believe it's not working because the sale\_date data isn't there if the record doesn't exist (no sale, no record). &amp;#x200B; I changed the dates for when there are sales records and the one sales guy who didn't make a sale isn't listed (so it shows 3 of 4)..this works fine for getting the data, but the problem is that I need it to show an integer. &amp;#x200B; Thanks for your help though
This is me too. Get in, get my coffee and try to get as much deep work done as I can. It’s a little tougher on Mondays when everyone wants to catch up after the weekend.
&gt;This may work, but I haven't been able to figure out how to get the count in there.
Thank you. I will try the cross join when I get to work. I had to add in that part of the case statement because at 5 minutes past the hour it was still within the acceptable time window. But instead of @now being within 10 it was within 11 so lasthour calculates to be 11 and lasthourlimit 11:15. That part of the case statement accounted for that. Thanks again for the help, I will try it out
Depends on your platform. If you're using SQL Server, Microsoft has at least two sample databases available (Adventureworks &amp; WideWorld Importers), and Stack Overflow makes periodic copies of their database available for download and use. You can also query the latter directly at https://data.stackexchange.com/
Get Adventureworks from here https://docs.microsoft.com/en-us/sql/samples/adventureworks-install-configure?view=sql-server-2017#oltp-downloads It is Microsoft's own test database and should give you everything you need. The OLTP variant is available, as is the data warehouse model.
 [https://docs.microsoft.com/en-us/sql/samples/adventureworks-install-configure?view=sql-server-2017](https://docs.microsoft.com/en-us/sql/samples/adventureworks-install-configure?view=sql-server-2017) These are the Microsoft databases used by most people for either training or re-creating issues, etc. Free to download and use. Get the right one for the version of SQL your using. Since you only want to borrow it, you'll have to figure out how to send it back when your done.
Maybe get away from the office for half an hour after lunch? Everyone seems to have their own optimal regime. After 4:30PM it gradually becomes quieter in my open seated office space and all of a sudden I’m flying. Also due to fewer people hitting the server which speeds things up
If you're using SQL Server and don't want to spend money, use SSDT's Compare Data tooling. https://docs.microsoft.com/en-us/sql/ssdt/how-to-compare-and-synchronize-the-data-of-two-databases?view=sql-server-2017 How much data are we talking about here? A thousand rows or a million?
If I eat like crap at lunch I’m done for the rest of the afternoon. Sometimes I close my door and have a mini dance party all by myself and that wakes me up. I’ll diffuse peppermint oil. I also have a giant bean bag chair in my office so if all else fails a quick power nap helps but I’m lucky my boss doesn’t care if we nap. He says if we need one that badly our code would probably be crap anyways so better to just rest for a bit.
Yeah, Take a break. Switch to a different task IE organizational, write ups, or stretch the legs. Break for a minimum of 15 mins. Leave the coding zone. Then come back after you went outside or something like that.
Great advice
A) The scenario is not clear enough to offer a solution. Does their bulk import overwrite existing data, or is all data retained and versioned? Same question for the changes coming from users. Why can't the user group just re-import their previous import if they have a good version of it sitting around in excel. B) If this scenario is a real-world possiblity in any system you are actually involved with, you need to help prevent this at a very high priority. It's too cheap and easy to just keep all your old data around nowadays for something like this to actually happen.
Working from home is great for this. I'm hyper productive in the morning, I can sometimes close tasks estimated for 2 days in 4 hours if everything is right and I'm in the zone. Then I feel exhausted and just binge watch Netflix or yell at other developers in pull request reviews, both equally entertaining.
Go out for a walk around the parking lot.
Yeah, I also have been fortunate that I can do a little WFH in the AM so I'll work from like 6-8:30 then come in to work at maybe 9:15 and then by noon I'm already ~5.5/8 hours. So if I do a 15 minute lunch, I'm done by 3 so I can drive home w/o traffic and go on a walk or something when I'm mentally spent
Go to the Gym! Get a good workout in and when you get back your mind will be refreshed and you'll be able to get back into the zone, if you want.
I wish i could work from home. My office is so noisy. Maybe some noise canceling headphones.
This is the life! Hire me haha
You are way to productive and you are making the rest of us look like slackers. Slow down speedy. :)
I scroll reddit like 10 minutes out of every hour. It kind of lets me brain idle, while not losing all the temp data I am holding in my head. ;)
Drop table if reddit &gt; 10 mins
Haha! Lots of google news in there too
Isn't there some kind of intra office sport you can play at 2-3 Ping pong or foosball. I have worked in a lot of offices and there is always some thing squirreled away somewhere like any good action movie from the 90's our hero fighting for money in a a back alley.
Well yeah. But don’t TELL anyone!
This hits too close to home man. Super productive in the morning, sloth like in the afternoon. When I'm focused I can just crank through tasks but as soon as I get sidetracked even the littlest bit, usually after lunch, I'm doomed for the rest of the day. Just need to make sure all of my commits are done before stepping away.
Every office job I’ve had I crash around 2-3pm. Not just a coding thing. Sometimes it’s so bad that my eyes won’t stay open. I’ve tried drinking caffeine, cutting caffeine, avoiding carbs before noon, carb-loading around 1, nothing seems to help. My office is lax about breaks so I usually go for a walk during that time to try wake up again and give my eyes a break from looking at a screen. Or I go buy a snack (which is now becoming a separate problem). I just wish afternoon naps/siestas were widely accepted in North America. Sometimes I really just need to to doze for 20 minutes and then I’ll be fine, but I can’t do that at my desk.
Our off is pretty lame. No one talks to each other
I get up, go downstairs, and buy a candy at the store in the lobby. Generally a short walk gets me feeling ready to do more.
This one worked.
You should definitely start something. Take survey of which games people like to play. Start the intranet office league.
Start work 3 hours later. Then you lose steam at 530 and just go home, everyone is gone already anyways.
No, it selects from table A and joins to table B
Looks like you need group on number, and state.
Nvm
I tend to be helping others in morning and do my best coding after lunch or late at night when I should be sleeping. Try doing team stuff earlier and code after.
 ✅This Video shows you how to Backup and Restore Databases in SQL Server, as well as create a Maintenance Plan for Automated Database Backups. ✅If you're new to SQL Server, you had SQL Server responsibilities thrown into your job duties, or you simply want to learn more about SQL Server, then this video will be great for you.
Just plain old SSMS. If I'm writing in powershell (dbatools) then I use Atom with the powershell packages installed.
Well, it has its downsides too. A 3 year old yelling "DADDY COME PLAY WITH ME" right into your mic as you have a conference call and forgot to close the door is fun. So noise cancelling headphones are a blessing as well. I recently bought Sony's WH-1000XM3 and they are amazing, Bose's QuickComfort 35 II very close second. Working from home is not for everyone by the way, be careful what you wish for. A lot of people complain about not having enough social interactions. I'm very much introverted so it's a blessing for me as I've become much more social privately so it's a perfect match for me, but for extroverts it becomes unbearable to work 100% remotely. Some people also really need others watching over their shoulder to actually do some fucking work.
Maybe I'm reading it wrong, but doesn't it say: SELECT a.column_1, b.column_1 ...?
I'm stuck using an outdated enterprise application for our main DB at work that is simply a fixed text box about 50% the size of an Internet Explorer window. I hate it, but it's what I learned on, and honestly I think it was the best way to learn because I didn't have intellisense giving me a syntax crutch. For my SQL server, I primarily use SSMS, but I'll use Azure data studio or VS code sometimes. If i'm on that server i'm usually doing DBA or Dev things, so SSMS is the best option there.
HeidiSQL
that HAVING condition should be a WHERE condition, but otherwise the query looks okay what exactly is the trouble?
[Relevant.](https://www.youtube.com/watch?v=co_DNpTMKXk)
I think the error is on line 3 where it says INNER JOIN tableB as b Instead it should say INNER JOIN tableA as b
I wake up at 11, get to work at 12.30. Work well til 16.00, crash, go home, do home stuff. Start working at 23, work until 03.00, sleep, repeat. I'm super messed up. Don't do this.
Coffee or tea and fresh air
Don't feel like you need to power through it. We solve problems for a living. We are not paid by the keystroke. Sometimes you aren't going to be in the mood to relentlessly solve one thing after another. So don't. I suspect I have spent less than half the time I'm in work actually working in my career but I'm still pretty good value. And you do get those big absorbing problems where you will be so focused you forget lunch and stay late just because you feel you have to solve it. But that can't be every day and shouldn't.
I beg your pardon? ^^^^1000% ^^^^accurate
It barely matters. At data analyst level you're not going to be expected to do much in the way of things like developing sprocs or other development-type stuff, which is where the differences start to become important.
I'm a night owl, so the same timing wouldn't apply, but I sometimes feel like I've fatigued a certain portion of my brain. Sometimes it helps if I change the kind of work I'm doing (e.g. switch from something that demands a lot of focus to something that's repetitive), sometimes I'll start to "wind down", usually ending with just looking at the code I intend to work on the next day or reading documentation. Occasionally, I'll think "but what if I tried this" or "maybe I can fix this" or "I'll just make a placeholder and fill in the logic tomorrow" and piggy back on small doses of motivation to continue.
All great ideas. Thanks for sharing!!
... and I think you're entirely correct!
@pooerh Same here.