In my younger years I did a lot of web development and used WAMP. It is really easy to setup, and you can get a lot of valuable experience from the MySQL database. However, I felt I worked on 'real life' Dbs once I started working with SQL Server. SQL Server brought a lot more aspects of programming into the language itself, and I felt I was able to grow a lot more, work towards certifications, and have a lot more real world application. I echo the words of another redditor: Install SQL Server Express and install the AdventureWorks database. Many books and websites will teach you to query and become acquainted with a working database via this database. I think this link should have what you need. https://msftdbprodsamples.codeplex.com/releases/view/105902
Create a view defined with this query: select id, name, sales, rank() over (order by sales desc) as sales_ranking; Then query your view: select id,name, sales,sales_ranking where sales_ranking in (10,110); Or do it all in a single query (no view): select id, name, sales_ranking from (select id, name, sales, rank() over (order by sales desc) as sales_ranking) where sales_ranking in (10,110); Stop thinking about a trigger. Now. It's not the right way to do this and you will see performance problems each time you update your data. While technically you **can** write your trigger to only fire on certain conditions (date/time in this case), it would be a very bad idea in your case - if it only fires weekly, **your ranking data is invalid and doesn't match the rest of the table for the whole week**.
This is exactly what I want to be doing. I want to get an install of sql server on my spare box and interrogate it from my laptop like I would at work. I'm looking into certifications at the moment but would like to kick off the learning on my own. 
I could use MySQL but with my work using sql server I was thinking about installing that. Also; halifax in UK, halifax Canada or halifax the bank? 
All good choices. Halifax, Canada. And for SQL Server, you could use [SQL Server Express, perhaps?](https://en.wikipedia.org/wiki/SQL_Server_Express) If you have Visual Studio, you might already have it.
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**SQL Server Express**](https://en.wikipedia.org/wiki/SQL%20Server%20Express): [](#sfw) --- &gt; &gt;__Microsoft SQL Server Express__ is a version of [Microsoft](https://en.wikipedia.org/wiki/Microsoft)'s [SQL Server](https://en.wikipedia.org/wiki/Microsoft_SQL_Server) [relational database management system](https://en.wikipedia.org/wiki/Relational_database_management_system) that is free to download, distribute and use. It comprises a database specifically targeted for [embedded](https://en.wikipedia.org/wiki/Embedded_system) and smaller-scale applications. The product traces its roots to the [Microsoft Database Engine (MSDE)](https://en.wikipedia.org/wiki/MSDE) product, which was shipped with SQL Server 2000. The 'Express' branding has been used since the release of SQL Server 2005. &gt; --- ^Interesting: [^Microsoft ^Visual ^Studio ^Express](https://en.wikipedia.org/wiki/Microsoft_Visual_Studio_Express) ^| [^MSDE](https://en.wikipedia.org/wiki/MSDE) ^| [^Microsoft ^Small ^Business ^Financials](https://en.wikipedia.org/wiki/Microsoft_Small_Business_Financials) ^| [^Persistor.NET](https://en.wikipedia.org/wiki/Persistor.NET) ^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cpz0rkd) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cpz0rkd)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
I will have a look into it when I get home. Also bot thinks sql server express is nsfw lol
There is no SQL Server 2010.
Likely it's Access 2010.
To confirm 2008 R2
One important thing to note as you take the plunge into coding SQL (whatever flavour you choose), is that Access is nothing close to standard on how it does anything (joining multiple tables, I'm looking at you). Take all of the SQL text that Access builds for you from the GUI with a grain of salt knowing that there will be some things you simply can't copy paste into SQLServer or Oracle and have it run without issue. There is an excellent free resource right now at Code School called [Try SQL](https://www.codeschool.com/courses/try-sql/) that will get you started with understanding databases and retrieving data from them using standard SQL syntax. There is no environment setup required to take the course as they all you to do everything in browser. Another resource I would recommend is [SQLFiddle](http://sqlfiddle.com/) which allows you to test your sql code in browser and share it with others if you need troubleshooting, etc. Good luck!
SQL Server 2012 or 2014 Express Edition will be fine for your needs (and free!). It's unlikely that you'll hit on a feature that's new to one of those versions and in use in your current work environment.
I have quite a good relationship with one of the SQL Devs at work. He always goes on about using T-SQL (I'm yet to figure out what this is, i thought sql was sql) rather than letting access build it for me. This is where the real knowledge gap lies. I use the access version of sql everyday (by letting access write it). Am I able to use T-SQL (or another SQL that would work with Oracle/SQLServer) in Access? It'd be good practice. I was planning on using Access at home to write the SQL but could pick up another program if theres something better? I saw a post at the top of this Sub going on about a text edited similar to notepad++, might have to check that out.
Access has two ways of dealing with SQL. Linked tables and Pass through queries. If you have to maintain an Access database, the pass-through queries portion let's you write TSQL against a SQL Server. Otherwise the tool of choice is Management Studio which is also available as a separate download from MS. 
As /u/Obbers already gave some good suggestions for tools, let me just say this about SQL in general. SQL is a standardized language, however there are small variations in the implementation if that standard across different platforms. Two big company implementations are as follows: Microsoft SQL Server uses T-SQL Oracle uses PL-SQL The two languages (ignoring the way their engines work) are nearly identical with some small variations on syntax. In the past, these two languages had some large differences that have been resolved in recent years. For example, my company is a SQLServer shop so I write T-SQL queries all day. I can still come here and glance at the MySql, Oracle, etc. threads though because I can still easily read code for those and understand (hopefully) the question being asked. If you learn one implementation of SQL that is close to the standard, then transitioning platforms is pretty easy language wise. That said, the optimizers and engines do work differently between all of the RDBMS out there, but the languages are fairly similar.
Ah, yes. I was just trying to work my way through [this](http://support.microsoft.com/en-us/kb/262499) link, but unfortunately the example seems so over my head that I can't figure out how to apply that to my case...
You don't set the sqlcmd variable at the end of the query, meaning you are running the same query twice Should be: set @sqlCmd = 'update ' + @tablename + ' set value2 = value 1 - ' + @median execute sp_executesql @sqlcmd
OK, good to know. So for an ~8 digit decimal number, I should just use numeric? For a temporary table, go for something [like this](https://smehrozalam.wordpress.com/2009/10/14/t-sql-using-result-of-a-dynamic-sql-query-in-a-variable-or-table/)? And then in place of @median, use 'select * from @temptable'? Thank you for your help!
This was my first solo presentation ever (prep, post and everything), so be nice! This part of a weekly class we are using to train our Software Engineers at [Emergency Reporting](http://www.EmergencyReporting.com). Please keep discussions on Reddit please!
The last bit looks a little wonky to me, in MSSQL if you're joining a table you need to alias it. select * from playlist where playlist.playlistid not in (select distinct playlist.playlistID from playlist inner join playlisttrack ON playlist.playlistID = playlisttrack.playlistid inner join track ON playlisttrack.trackid = track.trackid inner join album ON album.albumid = track.albumid inner join artist ON album.artistid = artist.artistid where artist.name = 'Black Sabbath' OR artist.name = 'Chico Buarque') The way you were doing it, it looked like you were trying to return all rows related to any playlistid containing the two artists, which isn't really necessary.
&gt; If you've arrived at this point it's likely some bad decisions about how your data is structured have already been made. Bingo
SQL Server Express will do everything your likely to need. When doing the install, pay attention to the prompt and make sure you install the Adventureworks database. All the technet and MSDN articles use it for examples
&gt; The 2nd to last line is where I am lost. In plain english, I want it to say "select any from table searchT, where the data in any given column is is like the text written in the box " That's not how databases work. The reason they don't work that way--is no sane person would do that. You'll have to query each column, you can get a list of columns: select * from INFORMATION_SCHEMA.COLUMNS where TABLE_NAME='tablenName' This is not something you really should ever do--and your teacher should fail you if you tried.
&gt; the worst case for utilizing EXISTS for this situation is O(nÂ²) It shouldn't be slower than SELECT DISTINCT as the optimizer can always choose to do that instead, and as ziptime pointed out it's a sort, which is n log n.
Cool it should work with Qt Firebird driver http://doc.qt.io/qt-5/sql-driver.html#qibase
This is difficult to do without anything to test against, but I think you want something like this. select film From FÃ¶restÃ¤llning inner join Salong on salong = salongid where Stad = 'stockholm' group by film having count(*)&gt;2
Update Reddit Set TotalUsers = 10000 Where subreddit = SQL
So I had to re-format this thing to make it me-readable: SELECT COUNT(*) AS antalfilmer FROM ( SELECT Film FROM ( SELECT DISTINCT Film, namn FROM ( SELECT Film, salong FROM FÃ¶restÃ¤llning ) AS X, ( SELECT Stad, namn, salongid FROM Salong ) AS Y WHERE salong = salongid AND Stad = 'stockholm' ) AS FilmerPerSalong GROUP BY Film HAVING COUNT(*) &gt; 2 ) AS MinstTre; That done, the only aliases used in this query are on subqueries. To remove all aliases, you would have to remove the subqueries. Based on the aggregate functions being done, you cant really get rid of the subqueries. The only thing I could think to do is use a series of temp tables to perform the subqueries.
True, so then just count the results.
The views are usually to pull together code tables (e.g. StatusName or ProductType) or FR tables (Product or CustomerName). I just want to do that once. Sometimes there's a bit of logic in those views. If I want to use them elsewhere then I'd rather use the view already built rather than having to rewrite it all again. These aggregate views are then used for reports or integrations into other applications.
Some years ago I was really interesting in using Firebird. It looked the most impressive Open Source DB at the time. Unfortunately work got too busy and I didn't give me the time to learn a new DB. Good to see someone using it. I know it has come forward in leaps and bounds. IMHO I have never understood why it never got the same amount of press as MySQL, Postgresql and Sqlite. 
Chicken - Maybe my wording is what is making everyone criticize the design. but maybe not. Basically, my Access Table looks like this across the columns: COUNTRY CONTINENT POPULATION ECONOMY etc. A B C D E F G H I J I don't really see what is so screwy about that design. Frankly, I don't know of an alternative. With that said, I see what you and tdavis mean about putting everything in an array. I was reading a bit about them the other day so I'll dig into that. Also, I tried putting all my column names in that 5th line of code I posted above - actually works until you reach the word limit.
OK, the columns I gave are very simple and just a tidy example. Really, I have about 40 columns give or take - and most of their cells are filled with long open ended text. Across 25 countries this makes for a lot of text.... So our client wants to use a search function to find a particular cell value. So say the client remembers that "Dr. Obiga" is the head physician at a TB clinic, but can't remember what country. The client types "Obiga" into the search box and hits search, and by golly, the row that Dr. Obiga is mentioned in is brought up and the country is Nigeria! At that point I could give a crap what the client wants to do with the row they have searched. Listing all columns exhaustively can work.... Private Sub test1_Click() Dim strsearch As String Dim strtext As String strtext = Me.test1.Value strsearch = "SELECT * from searchT where ((Country like ""*" &amp; strtext &amp; "*"")or(Report_Type like ""*" &amp; strtext &amp; "*"") or (nextvar like ""*" &amp; strtext &amp; "*"")or (nextvar like ""*" &amp; strtext &amp; "*"")or (nextvar like ""*" &amp; strtext &amp; "*"")...........)" Me.RecordSource = strsearch End Sub BUT a VBA line can only have so much text, and I run out of room if I try to enter all columns the lines above. What I could do is just shorten all the column names to 1 char, and *maybe* I could fit them all. Thanks! 
 SELECT COUNT(1) FROM SubredditUser SU INNER JOIN Subreddit SR ON SR.SubredditId = SU.SubredditId INNER JOIN User U ON U.UserId = SU.UserId WHERE SR.name = 'SQL' AND U.IsActive = 1 Subscribing to a sub isn't something you track active or not on. It's just going to be a link table that you either have a record in or not. I doubt anything on the server side cares that you were once subbed to something but aren't anymore. You're also not going to store the subreddit name with every subscription. Strings as PKs? What is this amateur hour?
You can learn this on your own. It sounds like you should look into SSIS and maybe the SQL Server import/export wizard. You won't need a cert to do conversions. If you ever want to be a DBA and they are willing to pay for it then take them up on the offer. It certainly won't hurt your career. You will also learn about much more than data conversion. https://www.microsoft.com/learning/en-us/find-partner.aspx https://www.microsoft.com/learning/en-us/certification-overview.aspx
&gt; So how do you learn it? Just by doing it? Sort of - prior to being the integration specialist I was the senior data / reporting analyst, and had many roles in the organization dealing with the database before that as well. It's all a matter of scope though - our project was 300 or so tables going into a system with 450 or so tables, so it was a fairly major project with a lot of different facets. If you're talking about smaller databases, it'll be much easier. On the target end we had design documents for the database and business processes. &gt; Also, how do I know I have reached a basic understanding of SQL? A good indicator would be when you can read through those stored procs and know what's going on. 
What core_dumpd is referring to with domain knowledge is the specific company knowledge about the various systems you will be converting. Some examples of this are: - Within the product system a widget is the lowest level of product and that multiple widgets make up a thingamabob - Customers are billed in the billing system at the thingamabob level, but its called a watchamacallit - The order entry system has both thigamabob and widget in the same column, which they naturally called watchamacallit, and the only differentiator between the two is that widgets have a cost but not list price You get this information by talking to people, hence why you cant learn it in a university course. Experience at this type of work helps you make these connections and acquire domain knowledge faster, but it still always comes down to meeting with business teams to understand what they are doing with the systems they have.
There's no way we'd be searching for SR.name = 'SQL'. Needs an autonumber PK at minimum.
I use SSIS to do this, but I mainly use it as a framework. To make the SSIS packages reusable, I offload almost all of the processing to C#. Compiled libraries and a properly managed GAC go a long way in making your SSIS packages truly portable. Remember, SQL is more about understanding the engine and the data that it is about understanding the syntax and the patterns. Set-up test conversions and work through them, there is no substitute for experience. 
Good video. Thanks Aaron!
Great news!
Thanks for JOINing us :)
INT doesn't allow decimals. Neither does BIGINT. I believe for 8 digits or larger you'd want BIGINT but I could be mistaken. Depending on how precise you need your calculations, I'd recommend using REAL data type. I've seen weird things happen when using FLOAT. The only time I ever used FLOAT was in lieu of GEOGRAPHIC data type, to store lat/long data.
Memory/caching could be an issue here. So think of the ETL as an object used to move dirt. By default, every ETL uses a good sized pickup truck. If you have basic driveway with a big pile of dirt and three teenage boys with shovels, this is optimal. It takes a standard amount of time to back up the driveway, wait for the kids to shovel the dirt, then haul it out. Pretty optimal. Similarly, if your source data is half a dozen columns, mostly INTs or DATEs with a couple low-width CHARs, holding a couple hundred thousand records, the default settings are pretty optimal. If your dirt is at the end of a narrow and winding bike path and it's a bunch of little mole hills, a pickup and shovels is a bad idea. Not only is it overkill, it's actually INefficient. Similarly, using the default memory settings to grab a couple dozen lookup tables with two or three fields and a couple dozen records per table will actually slow your processing .. youre spending unnecessary time creating in-memory tools to move a "relatively" big set of data .. when it's not big data. Using the dirt analogy, what you want is to give each of those teenage boys a wheelbarrow and a trowel. Similarly, for small source objects, you want lots of individual independent requests, each with a very small cache limit. A big pile of dirt, like a house sized pile of dirt, needs more than a pickup. It needs a dumptruck and a front end bucket loader. Similarly, big source objects .. big ass tables with several million records and several dozen fields including NVARCHAR(255) and XML data types, these need a giant cache or memory allocation. In fact, you *might* even find it more efficient to put them all in the same task and share a big memory allocation. Do a simple query plan on the tasks performed and identify small, midsized and large source objects, and build separate tasks for these, each with custom designed memory settings. *** Other tip is to look at indexing. If you're pushing data into a big ass table that some smart DBA indexed because it gets read all the time during production usage, for chrissake kill those indexes. *THEN* obtain source data, and then *REBUILD* your indexes. If you can, and it's fruitful, maybe create temporary indexes on source data as well. You can kill the index when you're done. *** Do you have nested queries in source object querying? Is this more efficient than creating a temp or staging table? *** Are you modifying data types during your destination insertion? If so, do you need to do this during the ETL? Can it be done via stored procedure instead of a transformation within the ETL? *** These are things to consider when trying to prove ETL performance 
INSERT JOIN pun here.
I think you are trying to get the details for the account with the most payments? So SELECT TOP 1 accountID FROM Payments GROUP BY accountID ORDER BY Count(accountID) DESC
I'm trying to get the details for the account with the most payments in a specific year. I have never used top (I'm still a novice :)). When I did that I got error 1064 related to using TOP. Are there parenthesis involved at all?
I just read online that MySQL doesn't support TOP? 
lol sorry thought it was MS-SQL. :( Try LIMIT
SELECT accountID FROM payment LIMIT 1 GROUP BY accountID ORDER BY Count(accountID) DESC; I tried that and got syntax errors for the group by. Online said the formatting should be like that. 
That worked lol. I'm going to run through it some more to make sure it returned the correct number (and to make sure I understand the logic). Thank you :)
Cool glad I could help (kinda) good luck! Try it with a Subquery also both work, subquery is more flexible and more complex.
Yeah! That's what I was trying to do originally. I'll try to get it that way too. Thank you again
Ok. Thanks!
https://soundcloud.com/threewayhandshakee/citric-acid-provides-tartness
I live and breathe this stuff every day. This is the minimal amount of seriousness I can apply to the situation.
How many columns (values) does the acctmanager table have?
nvm I got it, but thank you quick question to rename a column ALTER TABLE demo RENAME Column contact to contact person; old column name is contact. I want to rename it to contact person am I entering it right? thank you
You shouldn't have two-word column names, but if you must you'll have to do something like: ALTER TABLE demo RENAME COLUMN contact to [contact person]
Use an underscore between contact and person, can't have a space without quotation marks
Thanks for the response, I'll crack on with them!
So I take it the visits are linked to locations by a location ID? I think you need something like select l.location, lvisit.latestvisit from location l inner join (select locationID, max(visitdate) latestvisit from visit group by locationID) as lvisit On l.locationID = lvisit.locationID Sorry for formatting, I'm on my phone. Basically find the latest date for each location first using a group by then join to locations to get the actual name. In fact you don't need to use the subquery at all, you could do it in one step but I like subqueries and can't be arsed to rewrite it. 
This too http://www.ifa.hawaii.edu/faculty/chambers/seminar/sql_intro.pdf
Looking at the Android app store there are about 6 different versions. What version are you looking into? It appears they all use the same logo style but different authors. I work with SQL server daily and want to look further into and possibly make it my career. SQL in 10 minutes, Edition 3 SQL in 10 minutes, Edition 4 Sams teach yourself SQL in 24 hours: Edition 4 Sams teach yourself SQL in 24 hours: Edition 5
If their Membertype is NULL, it would be pretty straightforward (but otherwise it's hard to tell what you need without more info on your tables. Give the below a shot: SELECT * FROM member WHERE membertype IS NULL
At a guess left outer join your member table to your team table where your team table member id field is null (give me everything from this table that doesn't have a linked record in that table) Hope this helps. 
Use escape character? X like '%#%sample%' escape '#'
With tsql (sql server) should be along the line of select sum(case when column is not null then 1 else 0 end) / sum(case when column is null then 1 else 0 end) from... With other dialects something similar could be possible. Probably something orribile would happen if no null is detected (division by zero), perhaps is better if you count null and not null occurrence and do the calculation outside of the db where is easier to check for division by zero. 
ahhhh, excellent, thanks. I was trying to escape the char's but didn't know to add the "escape '\' at the end. Nice solution.
Normalize, normalize, normalize. Seriously, if it's not in 3NF, there had better be a good reason for that.
When you do EXEC(@sql) it has no context of the @x variable declared outside of the EXEC.
MySQL's federated engine will let you create something like a linkserver, but only to other mysql databases. You cannot create a federated database to a SQL Server database from MySQL.
I figured that was the case so I tried adding DECLARE @x xml to the beginning of the @sql statement. It still wouldn't pass the result back to the @x variable in the main statement. After more googling, I found another example that I just got working. Thanks!
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Junction table**](https://en.wikipedia.org/wiki/Junction%20table): [](#sfw) --- &gt;In [database management systems](https://en.wikipedia.org/wiki/Database_management_systems) following the [relational model](https://en.wikipedia.org/wiki/Relational_model), a __junction table__ is a database table that contains common fields from two or more other database tables within the same database. It is on the many side of a one-to-many relationship with each of the other tables. Junction tables are known under many names, among them __cross-reference table__, __bridge table__, __join table__, __map table__, __intersection table__, __linking table__, __many-to-many resolver__, __link table__, __pairing table__, __transition table__, __crosswalk__, [associative entity](https://en.wikipedia.org/wiki/Associative_entity) or __association table__. &gt; --- ^Interesting: [^Many-to-many ^\(data ^model)](https://en.wikipedia.org/wiki/Many-to-many_\(data_model\)) ^| [^Bloor ^Street](https://en.wikipedia.org/wiki/Bloor_Street) ^| [^Wisconsin ^Highway ^44](https://en.wikipedia.org/wiki/Wisconsin_Highway_44) ^| [^Route ^26 ^\(Uruguay)](https://en.wikipedia.org/wiki/Route_26_\(Uruguay\)) ^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cq385l6) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cq385l6)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
I am gonna go with "link", thanks dude! I am just trying to figure out what to name my tables (dimAccount, factAccount, linkAccount) I love it!
Also table constraints and keys.
Kimball considers those "bridge tables." Looking specifically at Ch 6 in his data warehouse 2nd ed lifecycle toolkit.
Manage - Check constraints for valid values and/or functional dependencies can limit data corruption until proper normalized structures are in place. For higher cardinality normalization, a foreign key to a transitional normalized structure may be necessary. Correct - Migrate to appropriately normalized structures AND confirm corrections to existing data. If you have redundant data, there's a good chance there's data integrity issues. Maybe some records still have old/invalid values. Maybe some records have inconsistent functionally dependent attributes (e.g. Company Abbrev = IBM, company name = Microsoft). Getting the corrections will help you migrate the data correctly AND inform you of any design requirements for the normalized structure(s). Reduce - Implement appropriately normalized designs from the start. Generally speaking, 3NF for heavy transaction systems (OLTP = On Line Transaction Processing) and 2NF for heavy query systems (DW = Data Warehouse). There are exceptions to every rule, but that's a good start.
n-to-m combinations table is what I usually call them or short as bridge table. The table's name can be either a combination of both terms or something that would provide an understandable term of it (e.g. table 'users' and 'groups' would be 'user_group_links' or whatever else you might want to call them)
I wasn't going to humor it. :) I hoped we were in fact out of the dark ages where we shouldn't be littering our domain with sFirst_Name and iDaysOld with hwndPosition meta data.
We name them like link_tableA_tableB Remember, the important part is consistency in whatever naming convention you decide on.
Naming convention I've seen most places is to just jam the two table names together. InsuranceProvider_Account, Account_InsuranceProvider, take your pick.
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Database normalization**](https://en.wikipedia.org/wiki/Database%20normalization): [](#sfw) --- &gt; &gt;__Database normalization__ is the process of organizing the [attributes](https://en.wikipedia.org/wiki/Attribute_(computer_science\)) and [tables](https://en.wikipedia.org/wiki/Table_(database\)) of a [relational database](https://en.wikipedia.org/wiki/Relational_database) to minimize [data redundancy](https://en.wikipedia.org/wiki/Data_redundancy). &gt;Normalization involves refactoring a table into smaller (and less redundant) tables but without losing information; defining [foreign keys](https://en.wikipedia.org/wiki/Foreign_Key) in the old table referencing the [primary keys](https://en.wikipedia.org/wiki/Primary_Key) of the new ones. The objective is to isolate data so that additions, deletions, and modifications of an attribute can be made in just one table and then propagated through the rest of the database using the defined foreign keys. &gt;[Edgar F. Codd](https://en.wikipedia.org/wiki/Edgar_F._Codd), the inventor of the [relational model](https://en.wikipedia.org/wiki/Relational_model) (RM), introduced the concept of normalization and what we now know as the [First normal form](https://en.wikipedia.org/wiki/First_normal_form) (1NF) in 1970. Codd went on to define the [Second normal form](https://en.wikipedia.org/wiki/Second_normal_form) (2NF) and [Third normal form](https://en.wikipedia.org/wiki/Third_normal_form) (3NF) in 1971, and Codd and [Raymond F. Boyce](https://en.wikipedia.org/wiki/Raymond_F._Boyce) defined the Boyce-Codd Normal Form ([BCNF](https://en.wikipedia.org/wiki/Boyce%E2%80%93Codd_normal_form)) in 1974. Informally, a relational database table is often described as "normalized" if it is in the Third Normal Form. Most 3NF tables are free of insertion, update, and deletion anomalies. &gt;==== &gt;[**Image**](https://i.imgur.com/uujiSOK.png) [^(i)](https://commons.wikimedia.org/wiki/File:Insertion_anomaly.svg) --- ^Interesting: [^Sixth ^normal ^form](https://en.wikipedia.org/wiki/Sixth_normal_form) ^| [^Second ^normal ^form](https://en.wikipedia.org/wiki/Second_normal_form) ^| [^Third ^normal ^form](https://en.wikipedia.org/wiki/Third_normal_form) ^| [^Fifth ^normal ^form](https://en.wikipedia.org/wiki/Fifth_normal_form) ^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cq3h47y) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cq3h47y)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
Depends on your use case, a lot of places I've seen you just keep the data archived in a 3NF database, and when parts are needed, you pull them out and denormalize them before running the analysis.
A table that links 2 _dimensions_ in a Kimball dwh would be a fact (could be a factless fact table too), although it's a unusual to have only 2 dimensions . In inmon (data Mart area) it could be a lot of different things - map, bridge, association (kind of a generic term), etc. If, on the other hand, you have a 'normal' db, then you have a junction/association/map/link table. So you can have those in the dw/stage area too. Why are you calling your tables 'dimensions' - is it to follow a naming convention or are you designing a dwh?
&gt; 5) Prefixes on variables and schema objects. In the early days, the compilers were close to the hardware and needed all the help they could get. In Fortran, all variable names that started with I through N were of type INTEGER and the rest were floating point numbers. In BASIC, variables that started with a $ were strings. &gt; &gt; This is the year 2000; the â60s are over! Things like "str_firstname" or "tblPayroll" are redundant. The syntax of SQL takes care of the allowable datatypes for you and the compiler. &gt; &gt; What you are doing is trying to expose the physical storage choices in your logical data model all over again. And you are making your code hard to read. Try to read "Paris in the Spring," "nounParis prepIn artThe nounSpring" and see if the prefixes make it easier to understand; now imagine that was a 20 word sentence with subclauses. -- http://web.archive.org/web/20020611032650/http://intelligententerprise.com/001205/celko1_1.shtml
I believe so, but I'm not completely sure. At this time I can't use VB or C#. Is there anyway to speed up the SQL?
See if your validation tool has a batch API, as that approach looks like it's more geared toward real time singleton validation.
Hi there, I am a end user anduse MySQL workbench (not sure what database engine is). As for the table given by you there is one small change for simplicity assume that I have one column, that is date and it can have only two values i.e it will have date or it will be Null in case the date is Null give me sum of all Nulls and In case there is a date then sum of all rows having date ** each entry with date or NULL should be counted as 1 unit** and finally I need Sum of All NULL divided by Sum of all dates, For you given example the answer should be --- Sum of all NULL = 2 and Sum of all entries with dates= 4 (based on the fact that if there is date then count will be 1 for that row or entry) answer should be 2/4 
ahh nice, so it does get logged, but more efficiently. Nice link, thanks
You can use openquery to force the query to run on the remote db for example something like this: SELECT * FROM LocalTable1 t1 JOIN OPENQUERY(RemoteDB, 'SELECT * FROM Table2' ) t2 ON t1.id = t2.id It's probably good to read up on [this](http://thomaslarock.com/2013/05/top-3-performance-killers-for-linked-server-queries/) article as well as it goes a bit more in depth about performance. 
Well the problem is the data all needs to be in one place so one server can do the joining. One option is copying the entire table locally first and then joining, but I've also done things like create a stored proc on the remote server and pass a delimited string of what I'm trying to match as a parameter, and then return the results. I've also created alwayson replica databases on the requesting server so there are no linked server queries. SP calls are fast. Straight joins in a query are just usually not. 
O_o dont know whether to try it... or.... o_O
It doesnt do anything other than repeat the string 'Luftballons ,' 99 times
DOES IT EXPLODE AT THE END
Select REPLICATE('Luftballons, ', 99) + 'POP!' 
Self solved: ,DENSE_RANK() OVER(PARTITION BY name ORDER BY COUNT(DISTINCT transaction) DESC) 
The biggest gotcha is using openquery rather than selecting directly from the linked server. If you need one record from the remote table that has 100 million records in the table... 1. If you select directly from the linked server, **ALL 100 million records will come down** to your sql server then the where et al will be done. &gt; select * from &lt;the linked server&gt; 2. If you use openquery, the query is sent to the remote server. The remote server will only send back the result set. &gt; select * from openquery(...) 
Any 3NF one will be in 1NF and 2NF (by definition).
Bravo!
Low level certifications don't really have much point but if you are absolutely required to - pick the flavor of SQL your job uses and go for that. The MTA if you're using SQL Server etc.
This almost worked, I think I can tweak it to get exactly what I'm looking for. Thanks a ton!
Thank you very much good sir.
be careful with openrowset. It can create contention while its executing the query on the remote server such that you cant even refresh object explorer in SSMS. 
I was compelled to CTRL+e to test. Everything looked right, but I needed to be sure. Well done, sir. 
FROM is going to be your left table, basically. Since I avoid right joins altogether the FROM table is going to be my base table - what I build off of. If a client gives me a list of items in their scope and want data for those records I'll make it my 'FROM' table and left join off of it.
*flinches*
I think Oracle lets you jump right into their SQL certification. Theirs looks more attainable. Teradata and SAS require you to pass another exam prior to doing the SQL cert. 
That varies by city. If you contact a recruiter at a technical placement agency, they will know. 
Due to the changed requirements of last time I did this, my nearest-at-hand was used for identifying the in-string non-ASCII characters, and liaising with business areas for approval to modify their data. (Extensively audited IT environments... Blergh) The below used table S_INVOICE and column X_MLSTN_LG_DESC, but change to your liking. (Alternatively, turn the final SELECT into a delete statement. /* drop table ##JGTally drop table ##JGOuter */ /* ========================================================================= Create and populate the Tally table on the fly to cater for running against columns up to 4,000 characters in length 500 is acceptable for most system fields, but this script is intended for occasions when script used by Tier 2 becomes difficult to find the offending character Change table &amp; field names as appropriate to the occasion. ========================================================================= */ SELECT TOP 500 IDENTITY(INT,1,1) AS N INTO ##JGTally FROM INFORMATION_SCHEMA.COLUMNS with (nolock) /* ===== Add a PK Constraint on TempTable to maximise performance */ ALTER TABLE ##JGTally ADD CONSTRAINT PK_Tally_N PRIMARY KEY CLUSTERED (N) WITH FILLFACTOR = 100 --========================================================================= -- Use Tally table against table to find any 'Tab' characters -- or other nonstandard characters --========================================================================= SELECT ROW_ID /*Any relevant fields from source table*/ , N AS [Char Position Within Field (As int)] , SUBSTRING(X_MLSTN_LG_DESC,N,1) as [Character] , ASCII(SUBSTRING(X_MLSTN_LG_DESC,N,1)) AS [ASCII Value] INTO ##JGOuter FROM S_INVOICE with (nolock) CROSS JOIN ##JGTally with (nolock) WHERE X_MLSTN_LG_DESC is not null /*check for only Tab character*/ -- AND ASCII(SUBSTRING(X_MLSTN_LG_DESC,N,1))= 9 /*check for ANY non-printable, special Word characters characters*/ AND (ASCII(SUBSTRING(X_MLSTN_LG_DESC,N,1))&lt; 32 -- Anything less than ASCII 32 is non-printable. OR ASCII(SUBSTRING(X_MLSTN_LG_DESC,N,1)) &gt; 127) -- Anything over 127 is most likely Latin-1 encoded, but 128-160 Block is Windows-Only select * from ##JGOuter with (nolock) order by ROW_ID, 2 asc select ROW_ID , X_MLSTN_LG_DESC from S_INVOICE with (nolock) where ROW_ID in (select ROW_ID from ##JGOuter) order by 1, 2 asc 
I would if I had the program. Maybe I can ask for it but i will probably be told I don't need it.
As Pork_Taco said, it's likely MS Access will do the job, however if it were me, I'd strongly advise implementing a more robust reporting framework as it will provide more benefit in the long run. I'm a reporting coordinator for a Service Management tool (Incidents/Service Requests etc) and can't stress enough how important it is to ensure that your data is organised in a way that allows you to easily pull together data historically. I'd honestly be pushing your employer to work out some way of implementing a more mature reporting solution (It'll make you look like a rockstar!) I've included some open source solutions below: [Open Source Solution 1: Pentaho] (http://community.pentaho.com/projects/reporting/) [Open Source Solution 2: Jasper] (http://community.jaspersoft.com/project/jasperreports-library)
create a user function that'll scan for a non-ascii symbol, maybe? set @t = N'ãã¡ã' -- 'asasdfa'" while unicode(@t) &lt; 128 begin set @t = substring(@t,2,2000) end; select case when isnull( @t,'') ='' then 'ASCII' else 'NOT ASCII' end 
Not null = a constraint. Primary Key = Unique index and unique constraint. Identity = property of a column
Most likely your mail server configuration, check the settings.
I designed a betting website, but I didn't have a background in gaming / gambling. If your SQL is sharp you should have no problems. However you may need to know about asynchronous message queues, as a lot of gambling websites use them to prevent last minute bets being paid out due to volume related transactional timing issues. They act like a timing buffer.
Solid tip. Thank you
I seem to recall being able to use a parameter in the definition of a query. So you could create a query that includes all columns of the table you want want to search, as well as the like "*" &amp; [@Parameter] &amp; "*" criteria for each column, and then use VBA to populate the value of the query parameter and execute the query and get back the result. 
Thanks for the reply! i'll give that a shot!
This. The best complement you can have for SQL is a solid understanding Set Theory. All major programming languages and their related tools generally handle the required intermediate steps when connecting to a data source; online tutorials to make up the difference where they don't. 
Ok! This is helpful, watching videos about it now. Are you familiar with any location on the world wide interwebs wherein someone can practice SQL with a database? 
THIS! Thank you!!! 
You're most welcome. ð 
TBH, for an awful lot of software development work (language being immaterial) because of the pervasiveness and quality of modern ORMs (Object Relational Mapping), if you are using SQL, often you are "doing it wrong". I'm not saying that SQL is never used, just that it should not be the prefered solution. Personally I'd advise you to look at which problem domains most benefit from SQL and then work your way towards app/platform/software ... Edit: Downvoted? Seriously? /r/SQL you're the Indiana of subreddits.
It *only* affects dbmail all other methods send CCs correctly.
&gt; I'm not a SQLLite expert, but your questions are generic sql, so I'll come in. I very much appreciate that. Any and all advice is helpful. Also, &gt; Good luck with your class Just wanted to clarify that I'm doing this at work. This semester was the first time I've interacted with SQLite and, well, it was only for a week. So there's that :/ &gt; SQL databases are excellent at this. Make the tables as deep as you need them to be So I should know that it's a given fact that **Relations** are a primary feature of SQL, however . . . &gt; Combined tables can be more efficient for reporting, but please don't use them for operational stores like you have here. . . . in certain practices . . . &gt; The problem with flat tables is you don't get valuation on inserts and deletes, so some of your children from question 1 may become orphans or twins . . . this becomes a problem when (un)linking data? Is it an issue with too many cross-references/complexity? I'll read into 3NF. &gt; Primary Keys help keep a database from holding garbage and let's other tables reference information. Only skip them when you absolutely need to. So I should always plan on using a Key. The exception would be tables of non-vital data that does not need sorting or will be dropped? &gt; It's not used very often as most people find it easier to add columns to a table rather than creating a 1-to-1 join Hmm, ok. I'll keep looking around to see similar examples. I still want to know about performance differences between searching a long table as opposed to shorter tables with cross-relationships. I'm guessing it really depends on the implementation within a program's code because it may be shortcutting outside of the SQL database. And, after all, I'd have to give an explicit example which would be too difficult to bother over a written submission. Thank you for your comment and guidance!
With PHP's PDO... PHP and SQL in general go hand in hand, as it has drivers for a [dozen different databases](http://php.net/manual/en/pdo.drivers.php).
here's a post with my comment from few weeks ago about learning SQL http://www.reddit.com/r/SQL/comments/304qvb/need_help_in_practicing_sql/ 
How good is python for pairing with SQL?
Thank you so much! I can't express enough gratitude for your response. I'll use this as a part of my speech! 
Stack Exchange provides [a website](http://data.stackexchange.com/stackoverflow/query/new) where you can run just about any query against their databases online. The people at Brent Ozar Unlimited [created a post](http://www.brentozar.com/archive/2014/01/how-to-query-the-stackexchange-databases/) on how you can use this site, and also how you can import the SE data dumps to your local machine.
Agreed. The from table is your baseline table, it's the table to which all other tables join.
&gt;Each row in the table records an instance of the id and the day it took place. In other words "Day, SUM(ID) group by Day" would return the number of instances per day I think you'd want Day, Count(ID) group by Day as for the main question, somebody already answered
I don't know if he wrote it, he may have pulled it from a slide in someone else's talk. I'd expect to see "bring bacon, say 'you're brilliant'" in that last column if he'd written it :-) 
If you can't read all you have is CUD, and all you can do is chew on it.
Yes, checked HIVE's documentation and sub queries do need an alias. It's a weird requirement as Oracle, SQL Server, DB2 etc don't have this syntactic requirement.
It's not unheard of. I usually write it like this myself to handle multiple values when needed. INSERT INTO Person (Name, Age) SELECT v.Name, v.Age FROM ( VALUES ('Tom', 25), ('Bob', 23) ) AS v (Name, Age) LEFT OUTER JOIN Person AS p ON p.Name = v.Name AND p.Age = v.Age WHERE p.Name IS NULL AND p.Age IS NULL;
I use INSERT INTO ... SELECT more often than INSERT INTO ... VALUES().... Like your friend says, at least in SQL Server, it's easy to write a SELECT statement that selects literals to make sure you've got it typed out correctly, and then you can just append the INSERT to the front of it. As far as the WHERE NOT EXISTS part, I'd say that when I am inserting a literal row, I'll typically put an IF NOT EXISTS at the beginning. When inserting based on a select from another table, I will sometimes use the WHERE NOT EXISTS. It's just personal preference. Everyone will have their own style.
It is required from MSSQL when it is in the FROM clause I think. Great solution though.
Sane databases use implicit transactions so there is no need to delete. You just rollback.
Yeah, as I was writing my response, I thought the same thing, but we're just trying to explain what the terms mean, not suggest that that is the way to do it in a production system ;)
Using a LOJ this way is an anti-pattern. Use WHERE NOT EXISTS. 
Your best complements will be to learn skills and software packages to report on the data and to do extract/transform/load work. If you use MySQL, learn JasperServer, Birt, or Kettle. If you use MS SQL, learn SSRS or SSIS. If you use Oracle, I feel for you son.
I worked in the gaming industry for five years. Some specifics would be things like Average Daily Theoretical Win: This is the amount of money the casino expects, on average, to win from a customer based on the amount of money the customer bets and the hold percentage of the game played. You may be calculating and querying for this if you are going to have to provide reports. Trip Frequency: How often the customer comes in, and how long on average their trips are. You may also be tasked to query for this. This could involve some unique SQL where you look for gaps in consecutive days of gaming activity by customer. Coin In: The amount bet on a slot machine. If you show up with a $20 bill, you typically win some and lose some. If you end up with no money at the end of the day, your Coin In would have been more than $20...because you rei-nput the money you won. Hold Percentage: This is the amount that a game is supposed to, on average, earn. Each game will have a defined hold percentage, but what actually happens could vary. You may be asked to provide insights into actual hold percentage vs. the assumed hold percentage. Response rates: Smaller and regional casinos like to look at the rates of responses for promotions and mailers that they sent out. Again, something you may have to query with the SQL databases. If they have a hotel, then possibly REVPAR and occupancy rate. It would depend on if they are trying to make money from the hotel, or if the hotel just exists as a convenience for the gambling. There are also a lot of things that are similar to other industries such as accounting information, accounting payable, etc. 
The cardinality of the column is poor (two distinct values) but the stats for frequency are favourable if selecting for IsLocked = 0. If selecting IsLocked = 1, the selectivity is poor, and any index range scan will have worse performance than a non indexed filter check as a hash or equivalent will have to be performed for the index, which will invariably be slower than a check against the column. My advice, don't bother, in DB terms, the number of rows involved are tiny anyway. 
right...thanks for the correction. 
Thanks for your response. Can I ask what your thoughts are if the View and sprocs are selecting for IsLocked = 0? Would that change your answer at all? 
You may see a notional advantage in performance, yes, but with your row numbers it'll be marginal.
Awesome. Thanks so much for your input. Very much appreciated.
That's great. I still would have liked it to be the default though but this will come in handy.
One of the explanations could be that when you're selecting one column per table and the query is using covering indexes, the tables themselves are not accessed at all. When you start adding other columns, the engine needs to access the tables themselves (or the query plan changes). Try comparing the execution plans for both queries to look for clues of the performance hit.
&gt; If it was as bad as you claim, how come the internet isn't littered with blogs telling every one it's the very first thing they should change on a new SQL Server install? Well, people use mysql and it has even worse defaults. Doesn't make it good.
Honestly, I'd much prefer to work with SQL Server over Oracle. It's just this little default of SQL Server that I don't like. 
SQL Server will handle a multi-TB database just fine, assuming you have a good schema, indexes, maintenance plans, hardware, etc. You'll probably be looking at Enterprise Edition to get the best performance (unlimited RAM, table partitioning, etc). "With lots of photos" - ehhhh, that makes me uneasy. Databases aren't meant to be large-scale file servers. Can SQL Server store it? Yes, in a few ways. *Should* you do it? That's debatable.
In my experience, this is usually always the explanation. OP - I'm not sure about SSMS Express, but in full SSMS, your results tab may also give you a green suggestion for a covering index to speed up your query. Remember that you don't always need to make a new index, but could add more columns to an existing index so that it covers your query.
This is all very good advice. Configuring Kerberos to work with 2+ hop queries is a nightmare, even moreso if you cross domains/forests. Avoiding joins directly over a linked server is probably the most important here....if you can bring a subset of the data over to a temporary table and then join against that, or do scheduled updates to keep your local server in sync with the remote one, you can avoid most of the pitfalls.
I migrated many databases from Sybase to SQL Server and did it mostly by hand using BCP, DDL extract, and even a crude parser for stored procedure syntax. This was back in the day when the code base for each platform was similar. These days the SQL Server Migration Assistant makes things a bit easier (http://blogs.msdn.com/b/ssma/). In your case, it sounds as if you need a content management system, like Stellant or Sharepoint. Without knowing more about the requirements of your photo storing application it's hard to say what platform is the right home for your data. But SQL Server can handle multi-TB of data, yes. 
Yup, that was it. Thanks.
You could use a cursor(better yet: sp_MSforeachdb) to loop through all the database and run the following against them. *** IF OBJECT_ID('tempdb..#tmp') IS NOT NULL DROP TABLE #tmp CREATE TABLE #tmp ( Username NVARCHAR(255), UserSID VARBINARY(255) ) INSERT INTO #tmp EXEC sp_change_users_login 'Report' DECLARE @sql NVARCHAR(MAX) = '' SELECT @sql = @sql + 'DROP USER ' + username + '; ' FROM #tmp EXEC(@sql) *** **EXEC sp_change_users_login 'Report'** provides all orphaned users for a given database. 
Link doesn't work.
You're gonna need to do a count on `customer` and group by `customer, date`. Any customer that has a count of 1 here means that all events were grouped into a single record, meaning they are all identical. So throw in a where clause to grab only records where the count is one, and that's your list of customers who have identical date times. I don't want to say this smells like homework, but this smells like homework, and I probably gave out a little too much to begin with.
Were you linked to another sql server? If so, I seem to remember that ss is smart enough to know it is talking with another ss and will send the entire command with the predicate et al to the remote ss. This article ([https://thedailydba.wordpress.com/2013/10/08/distributed-queries-vs-openquery/](https://thedailydba.wordpress.com/2013/10/08/distributed-queries-vs-openquery/)) discusses distributed querys vs openquery.
The obvious is now so apparent. Thanks for the quick responses..I'll pay it forward.
They look to me like they do almost exactly the same.
Did you try the built in stored procedure sp_who2? Usually that would suffice.
That might point them to what user is being blocked, eventually, but that won't get to the root of the issue which is what process is running at the time of the blocking.
I will work on this. Only thing I would want to modify is the select. Seems like you have to list each column? I have a table with about 100 columns that I would rather not write out, anyway to dynamically do the select?
I've written functions to generate these queries in this past, based on the data dictionary tables. So all dynamic. Worked a treat.
Could you provide a sample?
Something like this, maybe? select a.*, b.* from (select a.* from db_a.a except select a.* from db_b.a) a full outer join (select a.* from db_b.a except select a.* from db_a.a) b on b.id = a.id 
If it's always at the same time, it's possible a SQL Job is kicked off that runs against any of the highly read tables. If it's safe to do so, you can run table reads with nolock (probably not prefered/desired) or maybe indexing the table. Here's the query I made to help diagnose any potential hangups (Requires 2012 or above): create table #who2( SPID INT, Status VARCHAR(MAX), LOGIN VARCHAR(MAX), HostName VARCHAR(MAX), BlkBy VARCHAR(MAX), DBName VARCHAR(MAX), Command VARCHAR(MAX), CPUTime INT, DiskIO INT, LastBatch VARCHAR(MAX), ProgramName VARCHAR(MAX), SPID_1 INT, REQUESTID INT ) INSERT INTO #who2 EXEC sp_who2 select spid, blkby, dbname, login, status, text from #who2 t1 join sys.dm_exec_connections on spid = session_id cross apply sys.dm_exec_sql_text(most_recent_sql_handle) where blkby != ' .' or exists (select * from #who2 t2 where t1.spid = try_convert(int, t2.blkby)) order by blkby drop table #who2
Sys.dm_exec_requests will give you the blocking session IDs. Sys.dm_exrc_sessions will give you their login names, hosts, and applications. Find the lead blocker. Pass its sql_handle value to sys.dm_exec_sql_text which will give you the query the lead blocker is running. Sys.dm_os_waiting_tasks will give you the wait type for the lead blocker which can help you identify root cause. You can also pass the plan_handle to sys.dm_exec_query_plan to get the execution plan for the lead blocker. You don't need to spend money on a fancy tool, sql server is instrumented within an inch of its life and will tell you everything you need to know.
Save yourself a huge headache and look up MERGE... WHEN NOT MATCHED. It was created for exactly this situation, and it goes back to 2008 RTM if I'm not mistaken.
I think there is a version of SQL Server Management Studio for Mac. The express version is free. Also the Adventureworks and Northwind schemas are free. With those and some searching around you should be able to turn up some tutorials that will teach you to use the more common functions. I'm fairly new to it too, but feel free to hit me up for goodies. 
Here's what I do: select * from ( ( select 'tab1 row', a.* from table1 a minus select 'tab1 row', a.* from table2 a ) union ( select 'tab2 row', a.* from table2 a minus select 'tab2 row', a.* from table1 a ) ) 'tab1 row' is returned when a row does not exist in table 2, and vice versa. Order to suit. Your request is a bit more normalized, and I think the pivot example by Trollfailbot may get you closer, but if you just want to identify differences in data you can do the above. Check out dynamic SQL if you want to do cool things on the fly - it can generate pivot statements based on schema that isn't known ahead of time.
That's how I wanted to do in the first place, but my senior devs told me not to do it this way. Main reason is that you cannot see the row result you'll be inserting. Hope it makes sense. It didn't click for me at first.
Select * from a Except Select * from b Union all Select * from b Except Select * from a Got that from the book refactoring sql applications
My solution will identify differences in records in the fastest approach possible, but won't tell you the individual column deltas, for that you'll need an approach like /u/Trollfailbot suggests, however this would be **VERY, VERY** server intensive with a big O notation of at least O( m*n^2 ), that's not good at all, so if you have one million rows in a table, with 10 columns you could be looking at a maximum of 10 trillion comparisons. If you are going to take /u/Trollfailbot's approach, filter down using my query first. Note, my approach can be changed to filter in different ways, by altering the having clause... **Identify matches...** having count(SRC1) = count(SRC2) **Records unique to table 1...** having count(SRC1) = 1 and count(SRC2) = 0 **Records unique to table 2...** having count(SRC1) = 0 and count(SRC2) = 1 **Records which differ...** having count(SRC1) &lt;&gt; count(SRC2) As for code for dynamically generating my query approach, I couldn't find it (it's probably on another external drive), I don't have an MS SQL instance, but the concept is very easy here's some pseudocode for the table column list part and all you'd do is write a function to build the queries from that for both tables: create function TableColumnList(@Schema varchar, @TableName varchar) returns VARCHAR(4000) as declare cursor curCols is select COLUMN_NAME from INFORMATION_SCHEMA.COLUMNS where TABLE_SCHEMA = @Schema and TABLE_NAME = @TableName order by ORDINAL_POSITION; vResult VARCHAR2(4000); begin --open cursor --loop -- fetch from cursor -- if cursor record found, -- append it with a comma to vResult (first one, no comma) --end loop; --close cursor; --return vResult; end; 
Have a look at the tablediff utility: https://msdn.microsoft.com/en-us/library/ms162843.aspx 
I guess I can see that if you just want to run the select to test it. This seems like such a basic insert that it wouldn't be necessary for me at this point. Anymore, I'm not writing singular insert statements unless it's some type of setting table. Any data like this would come from an external data file or code that would be bulk inserted and checks would be made to avoid duplicates in different ways.
Umm, isn't this simply ordering it by post# within a respective category and then by category? So if you can do the MySQL @rownum trick per category then union and sort the results that should do it, imo.
Like this....? EDIT : So I give an example that fulfils the criteria and it gets down voted? Someone doesn't like me! create table TEST ( ID int(10) unsigned not null, CAT varchar(1) not null, primary key (ID) ) ; insert into TEST (ID,CAT) values (1,'A'); insert into TEST (ID,CAT) values (2,'A'); insert into TEST (ID,CAT) values (3,'A'); insert into TEST (ID,CAT) values (4,'A'); insert into TEST (ID,CAT) values (5,'B'); insert into TEST (ID,CAT) values (6,'B'); insert into TEST (ID,CAT) values (7,'B'); insert into TEST (ID,CAT) values (8,'B'); insert into TEST (ID,CAT) values (9,'B'); insert into TEST (ID,CAT) values (10,'B'); insert into TEST (ID,CAT) values (11,'B'); insert into TEST (ID,CAT) values (12,'C'); insert into TEST (ID,CAT) values (13,'C'); insert into TEST (ID,CAT) values (14,'C'); insert into TEST (ID,CAT) values (15,'C'); insert into TEST (ID,CAT) values (16,'D'); insert into TEST (ID,CAT) values (17,'D'); select t.* from ( select ( case CAT when @curcat then @currow := @currow + 1 else @currow := 1 and @curcat := CAT end ) as RANK, p.* from TEST p inner join (select @currow := 0, @curcat := '') r order by p.CAT) t order by t.RANK, t.CAT; | RANK | ID | CAT | |------|----|-----| | 0 | 1 | A | | 0 | 11 | B | | 0 | 12 | C | | 0 | 16 | D | | 1 | 2 | A | | 1 | 10 | B | | 1 | 13 | C | | 1 | 17 | D | | 2 | 3 | A | | 2 | 9 | B | | 2 | 14 | C | | 3 | 4 | A | | 3 | 8 | B | | 3 | 15 | C | | 4 | 7 | B | | 5 | 6 | B | | 6 | 5 | B |
The the cost of living or for the pay being offered? Pay for this seems low but not sure what it cost to live out there. My company pays Interns more than this job is offering and its based in a very cheap part of the country. But eh, I know nothing of the UK and what these sort of jobs usually pay out there.
I'd use more targeted tools tbh. If SQL Fiddle is considered a tool, you are dearly lacking in the tool department (I like SQL Fiddle, for sharing some codesnippet on the internet and there such, but I would hardly consider it a productive tool). What I'm saying is, If you are working on MSSQL, SQL Managment Studio blows any of those out of the water. On Mysql, SQL Workbench, on Oracle, that what was it called thingy, on Postgree I actually dont know of any, but if all else fails, Toad for anything. Bulking a CSV into the RDBMS is something any of the above will do, and syntax highlighting is so basic that notepad++ will do a decent enough job at it. As far as code formatting is concerned, call me an elitist ass, but those that don't even format their code, should not be allowed to write it in the first place imho
I always check the select statement if I'm inserting data from other tables/views/whatever to make sure I get what I expect (usually row count. I never use it for inserting a single row of literal values. Does it really buy you anything? I mean, you could easily accidentally do this: INSERT INTO Person (Age, Name) SELECT 'Tom' AS 'Name', 25 AS 'Age' WHERE NOT EXISTS ( SELECT * FROM Person WHERE Name = 'Tom' AND Age = 25 ) GO You select statement will look fine when you run it, because you'll still get: Name | Age ---|--- Tom | 25 But (if the insert is successful, i.e. the datatypes parse ok), you'll have a record in the person table with: Name | Age ---|--- 25| Tom tl;dr I don't see that it gives any real advantage over a straight INSERT INTO [tablename]([columns]) values ([literal values]). Unless you're checking your spelling or something. But that's just, like, my opinion, man.
where lol like 2
Thanks guys. I think I got it working this way Select Case When lol like 1 then 'LOL' When lol like 2 then 'ROFL' End as 'HAR' Where (Case When lol like 1 then 'LOL' When lol like 2 then 'ROFL' End as 'HAR') Like 'LOL' So, this isn't the best example. But this will help me quite a bit on select statements with quite a few Case statements and I ultimately want to throw down a Where based on one of the Case returns.
nice
Seems about fair for the experience most of the uk is about 20-26k for 1-2 years experience. So with a London weighting as cost of living around there is higher than the rest of the country looks pretty good for a relatively new dba. There is a salary jump when you hit the 3 years xp. 
I agree wholeheartedly with your sentiments on people who do not format their code. These tools are not for everyday use, if I am sitting at my desk, connected to my DB, working on a script I have written then there is no need to use an online tool. This is not always the case. 
You could always TOP 100 PERCENT after the SELECT, and that would allow for the ORDER BY in the sub query ...
Right now it works but not copying everything. There's a few tables like this and some I have triggers on update and delete too. 
You don't need a subquery in mssql case.
why ssis over a stored proc?
&gt; Why would you use a stored procedure instead of an SSIS package? TouchÃ©
Does it specifically have to be MS SQL, or do you just want to learn basic structures and queries? If you just want to get started, you can easily install MySQL on your MacBook. It'll do everything you need it to.
A simplistic approach would be to do X-1 left joins for components and a join (with coalesce) to whatever you need @ the last level.
http://stackoverflow.com/questions/18426812/recursive-query-for-bill-of-materials http://stackoverflow.com/questions/13877568/generating-bom-with-hierarchial-query BOM issues are almost an art-form within themselves. 
this is pure gold \*bookmarked\* i can't count how many times i've run into idiots with closed minds who say that every table has to have an identity or auto_increment PK 
for an example of this, see [Categories and Subcategories](http://sqllessons.com/categories.html)
Which DB?!! (i don't know how many times I have to ask this on posts), it makes a huge difference as some RDBMS platforms support more advanced SQL constructs. Assuming SQL Server or similar : You'll need a recursive CTE to tree walk the dependencies, with a column to indicate the tree node level, then you pivot that resultant data based on the tree node level. Pivot columns have to be static, so you'll probably have to restrict to a certain number of levels.
I'd hate to run across them, that's for sure. I guess i've been lucky enough to have people receptive to the "values can be unique keys themselves" talk. I shudder at the thought of someone thinking your example is proper.
you make good points... but the closed minds i encountered were seasoned seniors set in their ("id") ways p.s. seems there are closed minds here too, judging by the downvotes i'm getting
I guess i've just been lucky. I don't know that I could work where there was that level of stubbornness. 
stubbornness is part of it, arrogance another 
What about Data Warehousing? Kimball states all natural keys should be substituted for surrogate keys? http://www.kimballgroup.com/1998/05/surrogate-keys/
Can you provide an example of a table in the wrong format and then what it should look like in comparison? I am one of those whom was told to put and id into every table but I want to do things the correct way. I am a visual person so seeing it helps solidify it.
TL;DR: Use composite primary keys, because an autogen primary reflects nothing meaningful about the data it represents.
&gt; Can you provide an example of a table in the wrong format . . . Scroll up. 
Ah if everything would be so clear-cut... Performance is always an issue once you move past trivial-size databases and varchar(2000) columns albeit could make a perfect logical key are never ever a good option for a primary/foreign key. Mutability is a huge operational issue (and no, on cascade update does very very little to alleviate mutability across different real types of environments that aren't event linked by referential integrity: staging/transactional/historical environments, extracts/reconciliations, key captures in reporting or long(er)-term external references (daemon processes, pre-spooled services, etc). Mutability of the key structure itself is a completely different matter altogether (i.e. a 2-column key becoming a 3-column key), the tendency of mixing subtypes in a single table is another matter, and so on and so forth. But yes - /u/lSecretlyLoveYou is right, CS students should be introduced to the distinction of business/logical key vs a synthetic one and the need to define and have the former enforced/verified (unique constraints, alternative keys, etc.) Truth to be told though - 9 times out of 10 an application built around a DB with synthetic key thrown on every table by 'default' would work just fine and with less of 'over-engineering', 'too many restrictions', 'too much SME involvement required'. And that's what most stakeholders would want and so the pattern becomes set. My .02.
&gt;Scroll up. May I point out that people can sort their messages based off of different criteria, so telling someone to scroll up/scroll down is pretty meaningless?
yes, you'd want surrogate keys on top of surrogate keys, since you're capturing time-variant denormalized aggregate dimension points instead of the original objects.
Well that is its chief effect, is it not? Not *what it is*, exactly, but someone who gives that answer likely "gets it," I would say. 
Probably: /r/coding
Thank you
yeah, but that doesn't give the results in most recent order -- edited to add: it's also **invalid sql** in every system except mysql OP, have a look at this -- [Getting just the maximum for the group is simple, getting the full row which is belonging to the maximum is the interesting step.](http://jan.kneschke.de/projects/mysql/groupwise-max/)
for help with coding sql, post in /r/sql
How would I go about removing my ID column from table A? Would I just use some hybrid of columns to create a key on? Table A: id | capture_id | channel | average :--|:--:|:--:|--: 1 | 101 | 1 | -32.456 2 | 101 | 2 | -37.654 3 | 101 | 3 | -78.354 ... | ... | ... | ... 10 | 102 | 1 | -29-234 11 | 102 | 2 | -45-678 12 | 102 | 3 | -76.275 ... | ... | ... | ... Table B: capture_id | site_id | type_id | dtg :--|:--:|:--:|--: 101 | 263 | 10 | 2015-01-05 14:18:53 102 | 174 | 7 | 2015-01-05 14:25:23 ... | ... | ... | ...
SELECT Cust, MAX(score) FROM your_table GROUP BY Cust ORDER BY Date desc 
It is stored as date. So if start date is 2/17/2015 then the value for the February column called FTEFeb needs to be 0.43 (FTE Calculation = Days in the month - (startdate -1) / Days in the month) 
Use a case statement, it's standard SQL so good coding practice. "If" is proprietary to MySQL. Days in month are given by DAY(LAST_DAY(startdate)), the rest is easy enough for you.
&gt; "If" is proprietary to MySQL. The syntax, maybe, but I'd be surprised if PL/SQL, PL/pgSQL, T-SQL, and the like didn't have conditionals. But yeah, this seems a good solution, pick out the month, the number of days (don't forget leap years), and do the math.
They do, yes, but they are programming languages not SQL.
What type of job are you starting on Monday? Like, what's the title?
Whilst they are Turing complete extensions of SQL; T-SQL, PL/SQL and PL/pgSQL are proprietary procedural language extensions of SQL but not SQL (as in ANSI). I too am a database designer and developer and would always recommend solutions that can be achieved in SQL over SQL procedure language extensions (faster, no context switching, p-compiling, transferable to other RDBMS platforms). Hence my suggestion for CASE.
What does this have to do with /r/sql ?
Make sure the columns are of the correct type (number).
No prob. It's definitely a life saver, especially when you have clients who have no idea on how to properly format a data file. Man I sounds like these guys: http://www.cc.com/video-clips/00ftvy/futurama-going-robot
Senior DBA/Production Engineer hehehe
This is what I use.
Logshipping is also an option, although I haven't used that one in like 10 years. Most use replication or mirroring.
 SELECT IIF(LEN(diagnosis_code_id)&gt;3, LEFT(diagnosis_code_id, 3) + '.' + SUBSTRING(diagnosis_code_id,4, LEN(diagnosis_code_id)-3),diagnosis_code_id) FROM patient_diagnosis WHERE diagnosis_code_id NOT LIKE '%.%' This worked for me. I added the IIF because I had a case where a record had a field length of less than 3 characters. Edit: /u/arcanis888 's solution is better
 update patient_diagnosis set diagnosis_code_id = LEFT(diagnosis_code_id ,3) + '.' + RIGHT(column_name,LEN(diagnosis_code_id )-3) WHERE diagnosis_code_id NOT LIKE '%.%'
Assuming it's just the A.ACT_DEP_TM you want the latest time for, then it would be something like this: SELECT A.RTE_ID AS DEALER , A.CNTR_NO AS CONTAINER_NO , MAX(A.ACT_DEP_TM) AS CLOSE_TIME , TIME(B.TM_STMP) AS GATE_OUT_TIME , DATE(B.TM_STMP) AS GATE_OUT_DATE , A.SCH_DEP_DT AS SCH_DEP_DATE FROM CNTR A LEFT JOIN CNTR_HISTORY B ON A.CNTR_NO = B.CNTR_NO AND A.ACT_DEP_DT = DATE(B.TM_STMP) WHERE A.CARR_CD = 'H222' AND B.MLSTN_ID_NO = 'DEPARTURE' GROUP BY A.RTE_ID , A.CNTR_NO , TIME(B.TM_STMP) , DATE(B.TM_STMP) , A.SCH_DEP_DT ORDER BY B.TM_STMP
use the STUFF function 
Thanks! I worked it out after going through some smaller examples with less tables. As for the address table, it was required to be separate. Thanks again for the help!
this. You might want to check for evaluation/precursors for good measure (E/V codes) too, probably: stuff( diagnosis_code_id, case when left( diagnosis_code_id,1) in ('E','V') then 5 else 4 end, 0, '.') 
I think you'll first need to convert the timestamp to utc, which i don't think t-sql has built-in. One you have that, convert it to datetime then use datediff to count the seconds between 1970-01-01 and your datetime to get the unix timestamp.
Put in your value and execute this: SELECT DISTINCT 'SELECT ''' + OBJECT_NAME(c.object_id) + ''' AS TableName, * FROM ' + OBJECT_NAME(c.object_id) + ' WHERE zone_num = &lt;Value&gt;;' FROM sys.columns AS c WHERE c.name = 'zone_num' Copy the results into a new query window and run them. 
Generate a statement from the data dictionary that looks like "Generic output for 1 table" (below) for each table in question. Concatenate the statements and drop the last "UNION ALL". Then wrap it in a select: "SELECT * FROM (&lt;statement&gt;) WHERE ZONE_NUM = &lt;value&gt;" The end result is you'll get a query that returns the table names that contain &lt;value&gt; in ZONE_NUM. Generic output for 1 table SELECT '&lt;table name&gt;' TAB, ZONE_NUM FROM &lt;table name&gt; GROUP BY ZONE_NUM UNION ALL Example for 2 tables: SELECT * FROM ( SELECT 'TAB1' TAB, ZONE_NUM FROM TAB1 GROUP BY ZONE_NUM UNION ALL SELECT 'TAB2' TAB, ZONE_NUM FROM TAB2 GROUP BY ZONE_NUM ) WHERE ZONE_NUM = 12;
Yea that's what I figured. I wasn't sure if there was some cool function that would cut down on all the joins.
Someone a bit more knowledgeable than me may come along and teach us both something today. Wouldn't be the first time on this sub haha.
Yea can't really change the tables unfortunately, its coming from our data warehouse. Our organization is structured kinda weird and the data was structured that way because of it.
before people waste their time giving you solutions that won't work for you because of your platform, please see the sidebar where you are instructed to **please identify your platform**
Write your own functions to do this. I'll show you the steps, but you'll have to write NSTimeStampToDate yourself. Write a function to convert the non standard time stamp to a valid date. create function NSTimeStampToDate(@NSTimeStamp varchar2) returns datetime... ... begin .... -- Convert YYYYmmddhhmmssTZ (non standard) to -- yyyy-mm-ddThh:mi:ss.mmmTZ (standard ISO8601 with time zone) -- Which is a string in date conversion style 127. Do this by slicing and adding the correct separators then.... set @ISO8601Str = ...; select @StandardDate = CONVERT(DATETIME, @ISO8601Str, 127); return @StandardDate end create function UnixTimestamp(@ctimestamp datetime) returns int as begin declare @return int set @ctimestamp = dateadd(hh, datediff(hh, getdate(), getutcdate()),@ctimestamp); select @return = datediff(second,{d '1970-01-01'}, @ctimestamp); return @return end create function NSTimeStampToUnix(@NSTimeStamp varchar2) returns integer as begin return UnixTimestamp(NSTimeStampToDate(@NSTimeStamp)); end Then simply use your NSTimeStampToUnix function. Note : I've kept the functions separate in case you want to use them for other purposes.
I also work with ICD9 codes (and ICD10 soon??) in my DB. Are you sure a blanket update to insert after the 3rd char is fully correct? Just a quick query of my data shows 700+ codes where the decimal is in the 5th position (e.g. E924.8) Pretty much any E codes. If those aren't in your system, then you should be fine. edit: [WHOOOOPS](http://www.reddit.com/r/SQL/comments/32lr3s/ms_sql_beginner_is_it_possible_to_insert_a/cqcp1h8)
If you're in Oracle, [SYS_CONNECT_BY_PATH](http://docs.oracle.com/cd/B19306_01/server.102/b14200/functions164.htm#i1038266) might give you some alternatives to multiple joins. 
That function isn't a single line TVF, so it'll be worse for the performance. This will at least be a little better. CREATE FUNCTION dbo.GetName(@ID int) RETURNS TABLE BEGIN SELECT Name FROM Employee WHERE ID = @ID END http://stackoverflow.com/questions/4447346/testing-performance-of-scalar-vs-table-valued-functions-in-sql-server Both tables have 50,000 rows each: /* SET STATISTICS TIME ON SET STATISTICS IO ON DBCC FREEPROCCACHE DBCC DROPCLEANBUFFERS */ SELECT [ID], [CTE].[dbo].[GetNameML] ([CEOID]) AS [CEOID], [CTE].[dbo].[GetNameML] (DirectorID) AS DirectorID, [CTE].[dbo].[GetNameML] (PrimaryManagerID) AS PrimaryManagerID, [CTE].[dbo].[GetNameML] (SupervisorID) AS SupervisorID, --[dbo].[GetNameML]([CEOID]) AS [CEOID], --[dbo].[GetNameML]([DirectorID]) AS [DirectorID], --[dbo].[GetNameML]([PrimaryManagerID]) AS [PrimaryManagerID], --[dbo].[GetNameML]([SupervisorID]) AS [SupervisorID] '' FROM [dbo].[Hierarchy] OPTION (MAXDOP 1) &gt; SQL Server parse and compile time: &gt; CPU time = 0 ms, elapsed time = 1 ms. &gt; &gt; (50000 row(s) affected) &gt; Table 'Hierarchy'. Scan count 1, logical reads 182, physical reads 0, read-ahead reads 7, lob logical reads 0, lob physical reads 0, lob read-ahead reads 0. &gt; &gt; SQL Server Execution Times: &gt; CPU time = 6224 ms, elapsed time = 6654 ms. &gt; **VS** /* SET STATISTICS TIME ON SET STATISTICS IO ON DBCC FREEPROCCACHE DBCC DROPCLEANBUFFERS */ SELECT [ID], (SELECT [Name] FROM [CTE].[dbo].[GetName] ([CEOID])) AS [CEOID], (SELECT [Name] FROM [CTE].[dbo].[GetName] (DirectorID)) AS DirectorID, (SELECT [Name] FROM [CTE].[dbo].[GetName] (PrimaryManagerID)) AS PrimaryManagerID, (SELECT [Name] FROM [CTE].[dbo].[GetName] (SupervisorID)) AS SupervisorID, --[dbo].[GetName]([CEOID]) AS [CEOID], --[dbo].[GetName]([DirectorID]) AS [DirectorID], --[dbo].[GetName]([PrimaryManagerID]) AS [PrimaryManagerID], --[dbo].[GetName]([SupervisorID]) AS [SupervisorID] '' FROM [dbo].[Hierarchy] OPTION (MAXDOP 1) &gt; SQL Server parse and compile time: &gt; CPU time = 15 ms, elapsed time = 15 ms. &gt; &gt; (50000 row(s) affected) &gt; Table 'Worktable'. Scan count 0, logical reads 0, physical reads 0, read-ahead reads 0, lob logical reads 0, lob physical reads 0, lob read-ahead reads 0. &gt; Table 'Employee'. Scan count 4, logical reads 1724, physical reads 4, read-ahead reads 429, lob logical reads 0, lob physical reads 0, lob read-ahead reads 0. &gt; Table 'Hierarchy'. Scan count 1, logical reads 182, physical reads 0, read-ahead reads 0, lob logical reads 0, lob physical reads 0, lob read-ahead reads 0. &gt; &gt; SQL Server Execution Times: &gt; CPU time = 218 ms, elapsed time = 629 ms. &gt; 
I'm not sure there's a "better" way to do it - multiple joins is more text, but it is straightforward to write, will be easy to maintain, and should perform admirably. Out of academic interest, here's a query that uses fewer joins, but my hunch is that it'll be slower and harder to read/maintain than the query with more joins. SELECT e.NAME, MAX( CASE WHEN h.CEOID = e2.ID THEN e2.NAME END ) CEO_NAME, MAX( CASE WHEN h.DirectorID = e2.ID THEN e2.NAME END ) DIRECTOR_NAME, MAX( CASE WHEN h.PrimaryManagerID = e2.ID THEN e2.NAME END ) PRIMARY_MANAGER_NAME, MAX( CASE WHEN h.SupervisorID = e2.ID THEN e2.NAME END ) SUPERVISOR_NAME FROM EMPLOYEE e JOIN HIERARCHY h ON h.id = e.id JOIN EMPLOYEE e2 ON e2.id in ( h.ID, h.CEOID, h.DirectorID, h.PrimaryManagerID, h.SupervisorID ) GROUP BY e.NAME
 I think they could also get away with using a FULL OUTER JOIN as well.
Medical records? All of my nope. Not touching it with a ten-foot pole.
Yeah, no problem. I have a physical copy of his book. So..
Aliasing a table is generally required if you are doing operations with multiple tables in the same query, it prevents scoping and column ambiguity issues, for example where different tables have the same columns names. When qualifying columns, this alias can be short (which helps developers by making queries less verbose) or implicitly can be the table name if that table doesn't appear multiple times. In your example the correlated part (select AVG(salary) ... ) section knows the scope of its own department column (its the table being selected from), so strictly doesn't need an alias - although most developers from my experience would use one here - but the employees department column from the outer query is outside of the correlated part, so needs an aliased column. When a correlation occurs, the outer query effectively pushes value(s) down into the correlated section (in this case it is pushing the department column value). The correlated part is then performing an operation on that pushed down value (average for that pushed down value - i.e. average salary for the department), and the outer query also performs some operation based on that correlation result. In your case compares each employee's salary to the average for their department.
Add backgroundd and intent? The #temp table seems unnecessary, and your delete statement doesnt make much sense.
Yep, I totally agree with the natural join only being academic. I think they are a terrible idea, and have the potential to do a lot of damage (updates / deletes / inserts based on them), especially with the potential of some schema evolution having taken place after they were authored. Joins should always be explicitly defined in queries. IMHO natural joins should be banned.
Your solution is extremely convoluted, the whole operation can be done as a single correlated delete. So unless I've misunderstood your table relationships, this should achieve the same result : delete from dbo.cardaccesslevels as cal where exists (select null from dbo.cards ca join dbo.accesslevels al on al.accesslevelid = cal.accesslevelid join dbo.sites s on s.siteid = al.siteid where s.sitename = 'Main' and ca.cardnumber = 1296 and ca.cardid = cal.cardid) If you want to see the rows it'll delete first, simply replace delete from with select cal.* from Oh and use table aliases more!
Ah, I see your dilemma. I would do something like this. The idea here is to access / change the data the least number of times possible and to do it as a set, rather than procedurally. If you truly want to learn, try explaining what this code is doing. This seems like a better option. Never trust what someone writes on the internet without testing it. Delete from cardaccesslevels where EXISTS ( Select * FROM dbo.cards a INNER JOIN dbo.cardaccesslevels b ON a.cardid = b.cardid INNER JOIN dbo.accesslevels c ON b.accesslevelid = c.accesslevelid INNER JOIN dbo.sites d ON d.siteid = c.siteid WHERE d.sitename = 'Main' AND a.cardnumber = 1296 AND b.cardid = cardaccesslevels.cardid AND b.accesslevelid = cardaccesslevels.accesslevelid ) 
Are you sure you're defining the parameter correctly? What happens if you run it manually? EXEC msdb.dbo.sp_send_dbmail @profile_name = 'MailProfile', @recipients = 'test@yourdomainhere', @copy_recipients = 'test@yourdomainhere', @body = 'Test of sp_send_dbmail.', @subject = 'Test of sp_send_dbmail.' ;
Thanks a lot, this was very helpful. I think I get it now, the mechanics of "pushing a value down to the correlated section" were throwing me off the most. So if I understand correctly, MySQL is going through the employees table line by line comparing the salary of each line to the result of the subquery, in which "Bob.department" is substituted with the department of the line that the outer query is currently looking at.
Yep. The important part of the exists is to make sure you reference objects outside of it. If you don't, exists() will return true, and thus delete all your records. If you notice in my example, I also aliased all the objects in the subquery, so I didnt get names confused.
I did notice that you used aliases, I am not familiar with them and need to look more into how properly utilize them.
Can you post what you ran and the actual error that returned - note I did have a typo earlier that I corrected.
There are no guarantees for evaluation order of AND predicates in your where clause. The optimizer will try to find the most efficient way to execute the query, using available information. However, you can try to make the optimizer approach it as a filter on a filtered result set : select a.* from ( select * from dbo.V_REDDITEXAMPLE e where e.DATE between '2012-02-20' and '2013-02-20') a where a.NATIONID = 'GUID_HERE'
I ran the script directly as you provided and the error is as seen below. I did ensure I had the updated script that you corrected as well. Msg 102, Level 15, State 1, Line 1 Incorrect syntax near 'cal'.
I think it's a quirk with SQL Server syntax, delete table aliases have to be defined using **AS**. Change first line to delete from cardaccesslevels as cal 
powershell
When you click modify, it opens the procedure in what is called an ALTER Statement. So, all you need to do is make your change and click execute (or press F5). That will commit the changes to the procedure since it is in an ALTER, instead of creating a new procedure or executing the procedure.
This did work. Would you mind explaining how this is different than the previous syntax as to why this worked but the other didn't.
 --create table vars &amp; insert data declare @cards table(cardid int,cardnumber int) declare @cardaccesslevels table(cardid int,accesslevelid int) declare @accesslevels table(siteid int,accesslevelid int) declare @sites table(siteid int,sitename varchar(50)) insert into @cards values(1,1),(2,1296) insert into @cardaccesslevels values(1,1),(2,1) insert into @accesslevels values(1,1) insert into @sites values(1, 'Main') --check vals select * FROM @cards c INNER JOIN @cardaccesslevels cal ON c.cardid = cal.cardid INNER JOIN @accesslevels al ON cal.accesslevelid = al.accesslevelid INNER JOIN @sites s ON al.siteid = s.siteid --delete with output delete cal OUTPUT deleted.* FROM @cards c INNER JOIN @cardaccesslevels cal ON c.cardid = cal.cardid INNER JOIN @accesslevels al ON cal.accesslevelid = al.accesslevelid INNER JOIN @sites s ON al.siteid = s.siteid WHERE s.sitename = 'Main' AND c.cardnumber = 1296 --check vals select * FROM @cards c INNER JOIN @cardaccesslevels cal ON c.cardid = cal.cardid INNER JOIN @accesslevels al ON cal.accesslevelid = al.accesslevelid INNER JOIN @sites s ON al.siteid = s.siteid 
It will work, but EXISTS is a better approach as it is much more efficient. EXISTS is only used to test if a subquery returns results, and short circuits as soon as it does. JOIN is used to extend a result set by combining it with additional fields from another table to which there is a relation.
The "else 0" part of your statement is what is causing the error. The first part of your case statement is always going to the sum of Field1. If the sum of Field1 is 100 then it will calculate 100/1 = 100 If the sum of Field1 is 0 then it will calculate 0/0 and cause an error. What are you trying to achieve? It looks like some sort of average?
I'm trying to calculate an average. Field one is a dollar value. I want to add up all the values from field1 and then divide by how many there were. 
R. 
html
DBA Dev here: I have written a ton of Perl and Python programs that easily interface with SQL Server, Oracle and SQLite
MDX? I assume you are talking about SQL Server, but... case when 'sql server' then 'C#' when 'postgres' then 'javascript, python, perl' when 'oracle' then 'java' when 'mysql' then 'brainfuck' end Python's a weird choice. I mean Python's good, but it's more startups and open source type shops that use it. If you're gonna use a db like SQL Server and Windows, then go w C#
On this query I would say the performance of it is not detrimental, as all of the scripts that have been provided execute in 00:00:00. However, I would like to know if there is a performance difference between the two methods. Seems with SQL there are tons of ways of doing essentially the same function.
There are a lot of ways to do the same thing. You can measure performance using the execution plan in SSMS. My advice is use whatever method seems most natural to you, your code will be easier to maintain and it will be easier to optimize in the future.
It was my understanding the exists is better in a situation where you don't want to use the resources to count, but to just know there is a result. If this is true then exists would be the better route if the number of records does not matter, correct? Although, in the case of my query there is no need for a sub-query so perhaps it is just adding more work.
That's correct, but in this case you still need to identify what records you want not just that they exist. As I said above I don't think the exists does anything in this case. You can't short circuit this operation because you need to know all of the results. 
Will do. I just left the house, but I'll post later tonight.
"list the branch numbers for branches that have no salespersons working at the branch with ANY orders from customers located in the state in which the branch is located" What?!!! Do you mean?... List the branch numbers that have no salespersons but have any orders from customers located in the branch's state.
SolarWinds has a PDF of a 12-step tuning plan; Thomas LaRock does a SQL Saturday talk based on it. Execution plans are in there, but not until step 6 - don't get sucked into the execution plan right away. Before reading the plan is: * Check rowcounts * Examine filters * Know your selectivity * Analyze the query columns * Review keys, constraints &amp; indexes There's lots of work you can (and should) do before diving into the plan. Not so say that it isn't important - it absolutely is! But reading execution plans is part art, part science, part "I'll know it's bad when I see it."
You'll love this story... I spent the first three years of college pursuing Computer Science. I stopped because I didn't want to put the effort into maths past Calc II and switched to business. Eventually I graduate with a dual major in both Finance and Economics. My first couple of jobs are at large companies in finance where I use access databases to do some reporting. My third job is at another large (5 billion+ annual revenue) company in accounting. We have all of our accounting data on an Oracle server and it occurs to me that my job would be much easier if I used that to pull my data instead of going onesy-twosy through the GUI. After about 2.5 years of THAT I move to another slightly more technical job involving Netezza (no Oracle any longer) and python along with a smattering of unix environment stuff. I'm still in finance but I'm doing analysis of VERY large data sets (I think our largest table is something around 50 billion rows currently). After about 1.5 years doing that I am approached about an Oracle developer position which is where I currently am. So that is where I am now. I do data loads, build new tables, optimize queries, build reports etc etc. Also... we get a lot of our DBAs from the Database Analyst roles so take a look at those.
Database Administration is a weird niche of IT because you rarely see degrees dedicated to it as you see for software programming, networking, or web design, but it's just as important as these. And I'd venture to say most people who start going to school for IT don't plan on being a DBA. For me I went to school for Network Administration though I found out quickly that software development was more my forte. I finished my degree and started at a business in the late 90's working help desk. Quickly after getting there they decided to start working on an Intranet Site, so I took it on and started learning Cold Fusion (not my choice) and MS SQL (version 6.5 or 7, can't remember) for the site. After about 6 months I had a nice site created with a full content manager with everything saving back to MS SQL. We had a DBA, but he was more a DBA by title since at that time we had very few database servers. So when he left I asked to be moved to the DBA position since I enjoyed databases and it was my ticket off the Help Desk. I got the gig, and I've been a DBA ever since. This was round 1999. 
Why? It's a mental decision, Oracle 12c supports JSON natively and comprehensively. &gt;standards based SQL Ha ha ha. There is no such thing, every RDMBS platform has their own flavour of SQL, with slight variations, none are fully standard. Oracle works and works well, it's robust, scalable and well supported, it's why it is the world's most popular database.
I graduated from college almost 30 years ago with a bachelors degree in information systems (a melding of Business and IT) the Program I went to was among the first in the nation offering such a degree. At the time I was working in the micrographics industry (filming documents for Microfilm and Microfiche) . The camera I was using was computer controlled and would store a doc # and other info into a database. I thought that was pretty cool. At the Compnay I was working for, I created a database for other clients using PC-File (an old DOS flat file database). When I moved to another company, I was in charge of their Computer aided retrieval system and it was based on the PICK operating system. I kind of fell into Databases because I could see how businesses needed them to store data in the early days of PCs. I know do Business Intelligence systems and am rarely lacking for work. Look for SQL developer jobs, that is where you'll want to concentrate.
Although you could address this with a left join, I think the problem/question is about an existence check first and foremost. Why not try doing just that?
Naming should tell you pretty much EVERYTHING you need to know about the table.field, etc I have no problem with Customer.FirstName, Customer.LastName, Inventory.SKU, Inventory.OnHand, Inventory.OnOrder, PO.PONumber, etc. Same thing goes with program variables. I'd rather not have to refer to a document to tell me what a table, field or variable means. It is like having a conversation in your code and it becomes self commented.
That's a good way to look at it, thanks.
From what I am currently reading.... &gt;the SELECT clause is processed after the FROM, WHERE, GROUP BY, and HAVING clauses. This means that aliases assigned to expressions in the SELECT clause do not exist as far as clauses that are processed before the SELECT clause are concerned. &gt;A very typical mistake made by programmers who are not familiar with the correct logical processing order of query clauses is to refer to expression aliases in clauses that are processed prior to the SELECT clause. 
Oh - right. You can't use column aliases in a WHERE or GROUP BY clause. I've never found that to be a problem, though. And if you really have a need to use that, you could always use a common table expression.
Any tips on how not to be the guy causing massive slow downs?
Sure - as I pointed out, the ordering isn't really for processing. It's there for human readability.
Learn in a test environment. It sounds stupid silly but no one cares if you blow up development or a test database. Cut your teeth there and work with your dbas to get the hang of it. You will begin to notice the kinds of things you can and can't do. Also, there are enough tools for mainstream DBMSes that no one person should be able to slow everyone else down. I run many large oracle and sql databases and I have evreything set up so people can do whatever stupid stuff they want but it will just eventually fail and will absolutely not big everyone else down. I still have to figure out the case where 15 senior analysts go brain dead and all do something stupid at once in the same database but that's a different story.
Not going to do your homework for you, but here's how you should break the problem up. Start by returning all the Last Names that have more than one first name associated to them. Then use that Last Name list as a filter to only return owners with those last names.
I've been a DBA for almost 20 years. I currently manage a DB Operations group. I didn't choose the DBA life, the DBA life chose me. That is to say I was put into a DBA role while in the military and I never had the sense to stop. Most of my career I've been a production engineer and on-call for Sybase. I tend to tell folks that the hours are ridiculous and and you will be on-call until you retire. Where I work it tends to be high pressure but really that has been one of the things that has kept me there. There is also a lot of recognition, and the pay is flipping fabulous. You are the guy maintaining and protecting a companies customer data. Most folks I know who are happy and successful in this role lean toward being workaholics and a bit OCD. Like myself, most of the Sybase ASE and IQ folks I meet are old and I don't see a lot of new blood in the way that Oracle and SQL Server seem to churn out baby DBAs. I think there is an opportunity there. 
But think of all the rows of data that could have potentially affected by your change? Sure you can revert the code but think of the cleanup of data that you might have to do if you don't have an accurate record of what was changed and when? I have dealt with this first hand where a "simple change" went a couple days without anyone noticing and it ended up manipulating over 300k rows. Even if I changed the sproc back, I still had to to go back to all the affected rows and "undo" the simple change.
I am graduated with a degree in Computer Science, I had maybe 1 or 2 database related classes. I applied to a large company for what is basically there default entry level IT job, and they saw I had database classes and WALAH I am a DBA. I dont find the job very challenging or really fulfilling, its not exactly creative. But it does pay a shit ton, and I work from home and chill, on call every 10 weeks. Live in the west and work east coast hours...done at 1:30 each day. Its a good life. 
For example, https://www.apexsql.com/sql_tools_refactor.aspx
Only pull back the data you need, use indexes (eg, don't use a where clause with a leading wild card) consider whether your select is going to cause blocking so can you use nolock, don't do sorts on massive rowsetsunless you need to, make sure you're not accidentally making a Cartesian join, don't kill the buffer cache ... These are some of the things I've seen Really just understand that how you write your query has a massive impact on how the DBMS will run it, if you give it an inefficient query it won't stop you! Ask the DBA for help, they can show you how to view a query plan. Run in test first! 
"PostgreSQL supports most of the major features of SQL:2003. Out of 164 mandatory features required for full Core conformance, PostgreSQL conforms to at least 150." Not fully compliant, also has SQL variance that isn't in the standards.
A weak entity is one that can only exist when owned by another one. For example: a ROOM can only exist in a BUILDING. On the other hand, a WHEEL might be considered as a strong entity because it also can exist without being attached to a CAR. In your case an EMPLOYEE can't exist without a JOB and a DEPENDENT can't exist without an EMPLOYEE, but I'll leave the rest for you to figure out so that I'm not doing your homework for you.
If you *have* to use an unindexed view, a simple way to force execution plan is to store intermediate results in a #temp table. I've done it plenty of times to speed up queries by 10x. Usually have to use it when I have two beefy NOT EXISTS correlated subqueries in the WHERE clause.
&gt; "What you are doing is trying to expose the physical storage choices in your logical data model all over again. And you are making your code hard to read. Try to read "Paris in the Spring," "nounParis prepIn artThe nounSpring" and see if the prefixes make it easier to understand; now imagine that was a 20 word sentence with subclauses." &gt; -- Joe Celko
This may not be free after beta but for now it is https://www.jetbrains.com/dbe/
This is what I am trying to get right. What standards/packages should I a I'm to use to get maximum coverage of plsql and max portability. Any advice? 
Have you played with LINQ? They changed the order to what you proposed. var People = from P in db.Person select P
What about views? I think in some cases its easier to prefix the view with v. But I am against prefixing tables with tbl but a view is hard to name because it doesnt represent a single entity. What if I had a view for Customer and I did on it with Customer.orderId, Customer.addressId how would I name that?
I'm a librarian. Database design is something we learn about in theory in our masters' degree, but we don't really get into the nuts and bolts of it in school. When I was looking for work I knew I wanted to be on the techy side of the profession so I brushed up on it a bit, and ended up in a job where I do a variety of IT stuff - I built our website (but not app dev - we have someone else for that), I work with databases (but I don't built or manage them really - app dev guy does that too), I do some support/troubleshooting stuff too. Pretty much all of it I learned on the job.
Walah = voila
Perfect description -- thank you. I know why it's wrong, I just couldn't explain for it. Thanks!! (and yes, I was trying to split it by year/month)
* Is it invalid? No. I'm not aware of any SQL engine that rejects a table creation because one of its columns has the same name as the table. * Or bad practice? Yes. I don't see this being a big concern for the SQL engine, but more for the code written against the table. If the code has variables named after the table/column it may confuse maintainers unless the variable is properly distinguished (i.e. adding "tbl" or "col" or similar to the variable name). * Or completely fine - Maybe. While it might not be best practice, it is "completely fine" if the business determines that the cost of "fixing it" outweighs the value/benefit of doing so.
"Standard" developer path: Mainframe/COBOL --&gt; UNIX/C --&gt; PCs/C. While on PCs, I got into dBase and it was like a light came on. All was right with the world. Something about working from a data-perspective (as opposed to a code-perspective, for lack of a better term) just clicked with me. From there, it was Clipper, Informix, FoxPro. Both Informix and FoxPro introduced me to SQL, and I used that knowledge to worm my way into a SQL Server dev role for which I was actually underqualified. Once there, I glommed onto all info I could regarding the inner-workings of SQL Server and used that to become a DBA. RE your situation: From my own experience (small sample size!) it does seem that most DBA positions are more "senior"... so you might have to try starting out in a more traditional SQL developer role and watch for a DBA opening. Or look for a small company who can't afford a senior DBA but recognizes the need for one anyway (and, honestly, that would be a great spot because you'd very likely be able to have your hands on "everything"... server config, networking, etc.) RE "waste of time": Yeah, who can tell? I think we're starting to accelerate towards a situation where computers/robots do almost all jobs. But short- to mid-term? I don't go a week without a headhunter pinging me to see if I'm interested in a DBA opening they have. I'd just go for what you like and are good at but keep as many other avenues open as possible.
I'm an applications analyst. I was a user 20 years ago and became one of the superusers in my work group because I could understand and parse out the querying language our database used--which was the only way of getting data out of the database. I really enjoyed the puzzles and elegant solutions presented by querying languages and picked it up well enough to get a job doing it. From here, you will probably start out as an application analyst. That is where I have ended up (so far) but I don't have a computer science degree. I work with a fresh college graduate who basically has the same job as me in a different department. Every organization with more than 200 employees 500 customers has at least one application analyst. All universities have a teams of database support people. As for it being a "waste of time": Data is the "Plastics" of your generation. The ability to collect, mine, secure, manipulate, and interpret data is changing healthcare, commerce, public safety, education, um--everything. With every step up in the power of computing increases the ability to manage data. Currently, the field appears to me to be something of a mess as different companies compete to develop ways of making analytic tools more user-friendly. I have yet to see a tool succeed at allowing people who don't understand querying languages to query databases without a team of application specialists supporting them. Far from being a waste of time, database work is probably going to be just as dynamic and interesting over the next several decades as any other field. However, keep your edge sharp. There is one development I fear: someone will finally crack the "simple database" puzzle and develop a way of making querying easy for the non-tech. Computer intelligence may make tools like wolframalpha really truly intelligent enough that most of the lower-level application support staff will be eliminated like workers in a buggy-whip factory. So maintain your tech chops and stay ahead of the curve on the design and development stuff--maybe you will be the one that creates the smart-analytic tool that puts the rest of us out of business. Then you can track down your networking-weenie friends still configuring their routers or whatever, and buy-and-sell them. Enjoy. It really is enjoyable.
When aggregating, read each attribute as a list after the words "for each". If desired, you can add "unique combination of" after "each"; however, I find that confuses people more even though it is technically more accurate. 1. For each user, url, and session the number of unique sessions is. 1. For each yea and month the number of unique sessions is. You could also look at it as a hash array with each "group by" field as a key and each aggregate function result as a property. e.g. USER, URL, and SESSION are keys and COUNT( DISTINCT SESSION) is a property. 1. RESULT[USER][URL][SESSION].session_count = x 1. RESULT[YEAR][MONTH].session_count = x
Hey, There are tons of different places to learn about SQL. http://www.sqlcourse.com/ In terms of conversion of data from one product to another, check out boomi. http://www.boomi.com/ 
To add on top of this, most software companies will offer HUGE discounts towards end of quarter and even larger discounts towards end of year / quarter. Start talks in the beginning, string them along, then at the end, make them bite the bullet and low ball em. 
also, you cannot "fix" this by using COUNT(DISTINCT ...) instead of COUNT() in the existing query for client 100002, for example, the query will still return 108 rows, while the COUNT(DISTINCT ...) functions would require extensive processing to come up with 12 and 9 it would get you the right answers, but not very high marks -- in school work or in the real world
For real, OP is so close! You are already there! Instead of doing the 2 counts at the top of what you have [(http://i.imgur.com/yx8xIcg.png)](http://i.imgur.com/yx8xIcg.png), use a subquery. Take the 2 individual queries showing the correct results, and do an inner join on those queries. Good luck.
http://sqlschool.modeanalytics.com/ You are welcome..
Yes, I love using LINQ, thats why returning to SQL seems so illogical :-)
how would you join 2 queries like that?
Normally, you have a query like this: select [column] from [table] You can use what's called a subquery, like below: select [column] from (select [column] from [table]) as [subquery] Now you run what's in the parenthesis, the subquery, first, which will produce a result set. Then you are building a query against that result set. You can chain these together, like below: select [subquery1].[ID], [subquery1].[column], [subquery2].[column] from (select [ID], [column] from [table1]) as [subquery1] inner join (select [ID], [column] from [table2]) as [subquery2] on [subquery1].[ID] = [subquery2].[ID] Just gotta replace those subqueries with the queries you have that already produce the data you want, then you can join those results after you get them independently. 
http://www.pluralsight.com/ is about the best I've used, though it's not free.
it's undergrad course, towards end of current semester (almost finals). class is called database administration. 1 pre-req in US
The [APEX SQL Complete](https://www.apexsql.com/sql_tools_complete.aspx) would match the auto complete features of SQL Prompt I would think. I use it and it does pretty well. Although it does crash SSMS from time to time.
Look up kudvenkat on youtube, he pretty much covers the how to on everything with sql.
I think this might be the ticket. Plan to try it on mysql this week. https://www.simple-talk.com/sql/t-sql-programming/consuming-json-strings-in-sql-server/
Absolutely the best book is **SQL For Smarties** by Joe Celko. You might want to put ".pdf" at the end of your Google search. :)
Added to my queue, thanks!
Try the following resource here for free. You've got both course and exercises online http://www.studybyyourself.com/seminar/sql/course/?lang=eng.
Do a distinct employee count by day. For the jimbo part, yoy can either use an aggregate to setermine if jimbo worked that day or use a having clause to include only days that jimbo worked. * Flag for jimbo worked "Case when count( case when employee.name like 'Jimbo' then 1 end ) &gt; 1 then 'Y' else 'N' end" * For the having clause (after group by), "having count( case when employee.name like 'Jimbo' then 1 end ) &gt; 1"
Try the following resource here for free. You've got both course and exercises online http://www.studybyyourself.com/seminar/sql/course/?lang=eng.
 --Create a reference table create table missing ( size decimal(3,1) not null primary key ); --Put reference sizes into the table --including possible decimal like 14.5, etc insert into missing(size) values (1), (2), (3), (4), (6), (7), (8), (9), (10), (15), (16), (17), (18), (19), (20); --Find out what you don't have select size from missing where size not in (select screensize from Laptop where screensize between 15 and 20)
&gt; Is there a way to check and see if Jimo worked on a given day, and if so, count how many distinct employees worked that day? Get yourself a date table, it's a table with dates. Then you can join against that table, find any occurrences of Jimbo being 0.
 Select name from professor as p inner join [course section] as c on c.profid = p.profid where term = 'winter 2015' group by name There. Now I think I can breathe.
unfortunately I am not allowed to create or edit tables, only read from them 
I think i get the gist of what you are outlining, but i'm not quite sure how it would be set up in this situation. How would you set it up in this situation?
A nice resource for beginners here. http://www.studybyyourself.com/seminar/sql/course/?lang=eng. You can refresh you knowledge smoothly and quickly at the same time.
Thank you!
 SELECT e.CalendarDate, COUNT (DISTINCT e.active) as Staffed FROM Employees e WHERE e.CalendarDate &gt; DATEADD(d,-@intFlag, getdate()) GROUP BY e.CalendarDate HAVING COUNT( CASE WHEN e.Name like 'Jimbo' THEN 1 END ) &gt; 0
Thanks for sharing!
[Link about Quorum](https://msdn.microsoft.com/en-us/library/ms189902%28v=sql.110%29.aspx) A domain controller should not have anything besides required features &amp; roles *unless absolutely necessary*. It is the heartbeat of most enterprise environments and shouldn't be used for anything but. It also adds complications when updates/etc need to occur. In an ideal situation any partner or witness should be a separate server/vm. 
So with that said is it critical to have Quorum sit on a dedicated server?
What sql is that? it's select is case-sensitive :o
Quorum is a relationship/grouping, it's not instance by itself. For example, a Full Quorum consists of: * Witness Server * Principle Partner Server * Mirror Partner Server These are expected to be independent hosts for each. That said, yes, you separate them. As a general rule you should attempt to granulate as many primary systems from each-other as possible unless it is prohibitive to do so (ex hardware/licensing costs); VMs have made this aspect slightly more effective as well. 
Ahh ok I got you. I'm still learning and was thinking in my head that the witness is Quorum as when you set it up they refer to it as that. I would just hate to use a license and resources on the witness if it can sit on another server.
This is really good
They probably wouldn't even know unless you forgot to drop it at the end of your query or they checked the transaction logs haha.
The Import Wizard does not work for me.
What's your database? What format is the data in? 
Is the server hosted on the machine you are querying from or a server? I found it to be super easy when I realize that the txt file had to be on the server and not on the client. Edit: This was SQL Server, not sure what flavor you are using
What os? Can you just `cat queryfile | psql` or mysql if you swing that way.
I am not so tech savvy so let me know if I am not giving you enough info. The data is space separated and I am using Microsoft SQL Server Management Studio (2014). I am trying to query from a txt file that is on my desktop.
Try the following web site. You've got both course and exercises online http://www.studybyyourself.com/seminar/sql/course/?lang=eng.
Oh, the data is in a text file, not the query. I take it importing the data into a database isn't an option? I don't know if csvkit has a windows build.
Given the software that you have available, the import wizard is probably your easiest option. If you have Excel available and are more comfortable with that, it might be easier for you to get the text file into Excel first, making sure that the data is organized in different columns as you like, then save the Excel file and use the Wizard to import the Excel file. We understand the jargon, so if you post questions, we should be able to help you get through it.
thank you, that worked perfectly. I've never seen a table abbreviated/turned into a variable like that. Very cool stuff. I've been able to play around with the format a bit to actually consolidate a few other queries into a single more robust one because of it. You da real MVP.
Is the text file arranged in rows and columns? Depending on what you want to query, can you simply use ctrl + F? If you need to actually use SQL, yes you'll have to import it somehow. Either through a wizard or using an open source db like sqlite3 that makes importing easier. 
You can learn SQL quickly since it is high level language. Higher than PHP, Java or whatever since you do not design you own module (classes, methods, etc). By working hard one week is feasible. Mastering SQL can lead you to new horizon. Try this nice tuto. http://www.studybyyourself.com/seminar/sql/course/?lang=eng.
&gt; The data is space separated Are you 100% there are no spaces in the data points, as well?
You should explain more what "not working" means. Like what error message are you getting.
Make sure that the file isn't set as read-only or that the account running SSRS has the correct permissions to that directory. If that doesn't work you might want to just make a Batch/PowerShell module to delete them. 
tl;dr Too many people have unwarranted privileges. Just last week, I ran across three blog articles written by Rails devs, each of whom had accidentally dropped a production database. Their solution was to build a rake task that prevented them from dropping a database in the "production" environment. (They also described a workaround, for when Rails devs *really* need to drop a production database.) In my little corner of hell, developers rarely have *any* access to production. And I think that's a good thing.
Instead of an inequality, use a greater than symbol in your WHERE. 
We're going to have a simple "database" consisting of houses, cats, and a relationship that indicates which cats belong to which houses (cats can only belong to 1 house). 1. Houses natural join Cats - Answers "Which houses have cats?" The database will return a list of cats that belong to houses and, for each cat, the house to which it belongs. That means any houses with more than one cat will be listed more than once in the database result set. 1. Houses left join Cats - Answers "What is the population of cats in each house?" Some houses may have no cats and we want to include them in our database results. 1. Cats right join Houses - Answers the same type of question as left join with the tables reversed. This allows us to join tables in whatever order makes the most sense rather than forcing a specific order when a "left join" type operation is required. 1. Houses full outer join Cats - Answers questions that require a full accounting of all Houses AND all Cats AND their relationships. In this case, perhaps we want to identify all the houses without cats so we can recommend they adopt one of the cats without a home.
This query will return all duplicate records that have a higher value for GL_ID than the "original". If you want to pick a different duplicate to preserve, change the "AND o2.GL_ID &lt; o.GL_ID" accordingly. SELECT * FROM iep_goal_objectives o WHERE EXISTS ( SELECT NULL FROM iep_goal_objects o2 WHERE CAST( o2.OBJECTIVE as VARCHAR(max)) = CAST( o.OBJECTIVE as varchar(max)) AND o2.GL_ID &lt; o.GL_ID )
Why create a trigger? Recreate the foreign key(s) with "ON DELETE SET NULL" configured and the database will do that work for you.
&gt; if a sales order has no customer LOL at your relational integrity 
yeah, i have a few pictures that i use, i understand what each are representing. I'm just having a hard time implementing them. I like your example for Primary and Foreign keys. but what if I want to compare something that does not involve just the IDs. For example, Get a list of all the employees with a project in the year 2015. Assuming that there is a year attribute in the project entity. Would I still use a join for this? or is it something completely different?
It does recreate itself, that's the problem. I don't want it to use data cache at all. It hasn't used it across 2 years and 2 machines until yesterday. After changing none of the VS settings. I tested the solution on a co-workers' machine, and it doesn't cache the data, so it seems to be a problem with my local instance of VS.
Why is no-one else horrified that 13% of restores are because of system failures? Aren't we nowadays meant to be able to build systems that are resilient to that?
http://docs.oracle.com/cd/B19306_01/appdev.102/b14251/adfns_triggers.htm
You could still use a join for that if you'd like and add a where statement. SELECT e.employeename, p.projectname, p.projectdate FROM employee e INNER JOIN project p on e.employeeid = p.employeeid WHERE p.projectyear = 2015 (assuming that the year is separate from the p.projectdate column) that should give you the data set you're looking for in your example.
LOL... I read the headline as "in this article we'll tell you **which** people are to blame for 9 in 10 restore reqs". 
This'll do what the professor is looking for. Create or Replace Trigger Employee_Trigger before Delete on Employee REFERENCING NEW AS NEW OLD AS OLD FOR EACH ROW Declare Begin UPDATE INSURANCE i SET i.EMP = NULL WHERE i.EMP = :OLD.EMP; UPDATE DAMAGES d SET d.EMP = NULL WHERE d.EMP = :OLD.EMP; END;
Yeah, no its true. I was just trying to give a simple example.
Download BIDS Helper. It has a clear cache option. https://bidshelper.codeplex.com/wikipage?title=Delete%20Dataset%20Cache%20Files
wow, very clean. very functional. thank you
As someone that works as a solo SQL Data Analyst, you should learn a little bit of DBA work, at least stuff like deadlocks and permissions.
You're hitting the same table ten times and counting each label? What about doing count(*), label GROUP BY label If that won't work, what if you did a SUM(CASE WHEN label='value' then 1 else 0 end) as ValueName, SUM(CASE WHEN label='value2' then 1 else 0 end) as Value2Name,
You may want to consider caching on the web layer side. Meanwhile, are you using indexes and foreign keys? Indexes can really help performance on GROUP BY queries. Creating a view for the subqueries and properly indexing that may increase performance (it may reduce it too; depending on engine and/or setup of view). If there's a way to get rid of any joins, pursue that solution. Also, what is your SQL engine? (MySQL, MSSQL, PSQL, etc)
FYI using a foreign key does nothing for performance FKs are about relational integrity (ensuring that incorrect data cannot be inserted)
um, no better figure out inner/outer joins first also, pro tip: never use natural joins
Ok, is it inner joins that returns the matches from two tables regarding the condition? And outer join returns the table whether or not there is a match in the row? So like right outer join will return everything in the right table and only the matches in the left table (if there is any) and vise versa for the left outer join?
that's more or less correct, yes one thing you might want to keep in mind in future is that there's no reason ever to write a right outer join -- use the equivalent left outer join instead
I get pretty a little confused when joining more than 2 tables. A lot of the tables I'm trying to join only seem like they have one column in common and I'm just joining to get through them to the next. 
Ok thanks for the help!
A nested parent child relationship like a bill of materials. Coming up with a way to enter one parent part and get all raw materials for it and any sub components. 
Look up CTE You define the first statement and then reference it in your second statement With a1 As Select id From table a Where clause = x or clause = y select a1.id, desc from a1 inner join table1 on table1.id = a1.id Am on mobile so not sure syntax is right, but quick google search will confirm
Late to the party but i'm relatively fresh out of uni compared to some of the other responses (not that is matters). Post uni started in a junior support role for a development shop. SQL skills weren't required but helped me in the job. After a few years and a few different roles i moved in a BI (right place right time). Have been cranking away for the last 3 years now. You can become a bit of swiss army knife doing BI, part analyst, part programmer, part dba, part report designer. Everyday can bring a new challenge and there is no shortage of work, at least in major cities around the world. Its a growing industry and the people with either techie or business skills can find their niche. Me personally i'm still enjoying it. I don't get to write SQL everyday but when I do it tends to be my favourite part. Looking for work you'll need to focus on big business. At the end of the day experience leads to opportunity. A career in IT means the learning phase is never over
Here here. I only use sql prompt once a week on average but still worth it
WITH allows you to dump in to a temp table without first creating that table. It's really quite useful.
That should do what you are looking for, although it would be better to utilize a windowed function because it will let you see all the information; including Primary Key, which you will need to clean up the rows anyway. Additionally allows you to order them to pick the the most/least recent. *** SELECT * ROM ( SELECT rn = ROW_NUMBER() OVER (PARTITION BY name, ModelRID, description, Price, DateEffective, DateExpiration --Change this for the ordering ORDER BY DateExpiration DESC) ,* FROM prcoptpres ) WHERE rn &gt; 1 *** Obviously you'd want to replace * with the correct column names, but that is basically the quick of it.
Common Table Expressions looks like what Im after. Is there any side affects (performance wise) both implementing this and a full Text Index?
Try the CTE first. If that doesn't work, then look into Full Text Indexes. I would expect that one of two things is happening: 1. Your search for text in a large field is hitting every record on the table, in which case the CTE willl fix your problem 2. Your search for text in a large field is being applied as the last filter (the query plan generator is usually pretty good at this), and the CTE won't help you at all. In this case, you would need to look into options for improving the large field search. One option is the Full Text Index. 
MS SQL has a conversion wizard, I would use that to pull it into a MSSQL database and then dump a file from there you can use in VS. http://blogs.msdn.com/b/ssma/
I understand that sometimes people become "developers" when originally, they were hired to do something else. That is what I did in my previous position, creating ACCESS databases for my section. In my current group, our issue isn't related to training or budget. They were hired as devs many years ago. And they get sent out to training while the rest of us stay behind. (And they come back whining that the training is useless.) It is part of the organizational tradition to keep the incumbent on the payroll. It is all about office politics. Each project is an item on a project manager's resume. It doesn't really matter if the designated developer is interested in making the project itself successful as the 20-year incumbent developer is just collecting a paycheck until retirement. The developers have shown time after time the willingness to blame the users (i.e. bad data, bad procedures, out of scope) because the tactic works. Even previous managers are on the hitlist. In one case, a project was claimed to be successful and working but the users kept saying it doesn't work. It was obvious that it didn't work. The developers kept claiming that the users were asking for "new things". Back and forth, back and forth it went. Then finally the CIO had enough and asked the developers for a copy of the requirements to see if what the users were asking were really new. The devs said they don't have the requirements. They claimed that a project manager that left kept the requirements. That manager left many months earlier so what were these developers doing all that time? And how would they know that the requirements are "new" if they themselves don't have a copy of it? Rather than fire the developers, they were told to keep working, collect the requirements, and given a new deadline to deploy the app for the third time. If the project made the deadline then "by definition", the project is always successful until the users complain and the cycle starts all over again. That's why the app was deployed three times already. In-house, we call this "job security". Applications have been deliberately deployed knowing ahead of time that there were problems with it (e.g. records are erroneously overwritten, data fields not populated at all for simple things like addresses, duplicate records, addresses linked to the wrong person record). We know these problems exist because it showed up in the test database. The users don't see the problems in the test environment because they are prevented from testing properly. This is also why database restore are requested by our devs. The devs keep asking for these restores in order to delete any indication that the app does not work properly. Sometimes, the app is not tested at all by the users. Most of the time, it is only after deployment are the users given the opportunity to finally test the app correctly. People responsible for reports have to sometimes rewrite queries to compensate for these known issues, to cover up the limitations of the developers. Data entry procedures were even requested to be changed as well to cover up their limitation (e.g. users were told to not include phone number extensions when recording phone numbers).
Nope. With a CTE the optimizer gets to choose whether to use spooling or not.
CTEs are **not** temp tables. While CTEs *are* useful, it's also easy to abuse them and kill performance. In SQL Server, if a CTE is referenced multiple times in a query, that CTE is evaluated "fresh" each time. So if you're using it to simplify subqueries, it may be better to insert into an **actual** temp table.
Thanks for the heads up man!
 SELECT id , checkindate FROM daTable AS t WHERE 1 = ( SELECT COUNT(*) FROM daTable WHERE id = t.id AND checkindate &lt; t.checkindate ) will not return ids with only one checkindate
I only mentioned this in response to your comment because the article starts off "In Rails", and I wanted to state that I agreed with your method.
A view is just a select statement. I find it useful because i do a lot of work with excel vba so rather than write a select statement which can be manipulated and used to find more information than the user has access to i just allow them to access data from the view. If they knew enough about the database we had, ive tried to minimise their knowledge of it, they probably could anyway so not much i can do there. Another reason i use views is because if im writing a particularly long query it is soo much faster to select data, input conditional variables and join tables than writing a query is. 
I've got a similar story to /u/hodoshow, except I have a BA in History. Got a job doing customer support for a software company whose software relied heavily on Oracle. Learned a little bit about basic queries to help troubleshoot client issues. Got to know people in the data conversion team and basically was offered to hop over and help with the new branch of software that utilized MSSQL (after I took a week long SQL class). Other than that I learned as I went, because most of what I do now is all manual data conversion; nearly every data source we convert from requires extensive changes from every other data so there's no set process. It's challenging because it's essentially 4-5 different jobs rolled into one but it certainly keeps me busy AND lets me really utilize and research new ways to do SQL. Been doing it for almost 2 years now, I've caught up to/surpassed the other few people on the team in SQL-fu and it's awesome. Pay is shite though because of my lack of formal SQL training/knowledge and being pulled up from below.
I'll get you started, you'll want to use partitioning with an order by, last three can be solved with this, and the rest is what you already had. *** ;WITH bhn AS (SELECT rn = ROW_NUMBER() OVER (PARTITION BY GroupID, Hex ORDER BY DocDate DESC), ID, GroupID, Hex, DocDate, Keeper WHERE Hex IS NOT NULL) *** -- Update Keeper to One UPDATE table1 SET keeper = 1 WHERE (Hex IS NULL) OR (ID IN (SELECT ID FROM bhn WHERE rn = 1)) *** -- Update Keeper to One and set others to null UPDATE table1 SET keeper = CASE WHEN (Hex IS NULL) OR (ID IN (SELECT ID FROM bhn WHERE rn = 1)) THEN 1 ELSE NULL END *** *Edit - Note that you'll probably want to update the older keeper to NULL again if a newer row is available. Added another UPDATE to reflect that.* 
Most databases are written in C or C++ , as they need high performance and most db projects were started in the 70s or 80s when C was the best choice. There are a few Java databases too such as Derby. PostgreSQL lets you write functions in Python, Perl, Java, JavaScript, R and a bunch of other languages. You can write regular functions or aggregate functions, not sure about window functions. You can view PostgreSQL's source code for its window functions and check things out. 
Thanks, I'd be curious to see what some actually goes into a window function in c or c++. Do you know where I can find the source code? tried looking online couldnt find anything. 
interesting, never used postgres. At work, we use oracle, sql server and composite. Composite is a cisco data virtualization. Do you know where I could find the source code for sql server functions?
What you have isnt going to work like you want. Do you have the sql to query it. I think its possible but its going to be really messy. The easiest way i see to do this is to list the hours for each day. So you would have a table with 3 columns, project, date, and hours. Then you would have entries like ProjectA, 1-1-2015,5 ProjectA, 1-2-2015,5 ... ProjectA, 1-20-2015,5 Then you could query it like Select project, sum(hours) From table Where date between 1-1-2015 and 1-5-2015 Group by project This has the benefit of having multiple projects with hours on the same day and the sql is easy to read. Downside is you have enter each day of the project in the db.
Thanks for the info. Here is the solution I had in mind (remember I know nothing about this). I was thinking of using a date difference function to get the duration of each project, and dividing the total remaining work hours by that number. So in my example above, it would be 100 / (4-20-15 - 4-1-15) = 5 This would give me the average work hours needed per day of the project. Then in the query I would need to have a function that determines how many days overlap between the START and END dates in the Projects table and the Start and End dates used in the query. So, if my project had a START DATE of 4-1-15 and an END DATE of 4-20-15. And my query was for work hours between 3-25-15 and 4-5-15. There would be 5 days overlapping between the date ranges. So, 5 days multiplied by the average work hours per day (5) would give me the correct information. Does this sound possible? Is there a way using date difference functions or IF THEN statements to determine the overlap between two date ranges? Thanks again!
Here's another method that should work for #3: update table1 set Keeper = 1 where DocDate &gt; ISNULL((select MAX(t.DocDate) from table1 t where GroupID = t.GroupID and Hex = t.Hex and ID &lt;&gt; t.ID), '9999-12-31') 
You can use Transact-SQL to write basic scripts using sql language. In this case you could set a start and end date parameter and then run your various calculations in SQL. You can save and reuse the code as a stored procedure.
Dream Tech Labs â Best oracle Training in Jalandhar, Amritsar, Punjab, India. Course duration â 6 weeks / Months. For more Detail, please contact - +91-181-7102415 / +91-183-5017174.
Hopefully you put indexes on table.phone, table.cellphone, table.workphone and keytable.contactkey, otherwise it'll be O(n*m) order execution (500k * 500k) rows, if you haven't you've been a bit stupid. With indexes it'll be more in the order of 500k times 4. 
&gt; PostgreSQL lets you write functions in Python, Perl, Java, JavaScript, R and a bunch of other languages SQL Server lets you write functions in any language supported by the .NET CLR. That doesn't mean that you *should* though; last I knew, there was a performance hit for running CLR functions vs. native SQL. But when you gotta, you gotta.
Actually, you are correct here. It's looks like OP will be stuck having to use some serious wizardry to solve his problem.
You might be able to find an already made free package out there.
The update was only intended to solve the third scenario and is based on the assumption that you can run multiple updates. You can just add an OR clause for Hex IS NULL to handle both scenarios. 
Can you elaborate on why? I have indexes on most every column the query touches (i.e. phone, cellphone, workphone, prefphone, contactid, bazookapk, leadpk).
**Transpose** command Does **EXACTLY** what you want. Use it on column A. In fact What you want to with Column A is the exact definition of the command. 
Yes, this is my exact issue, the aggregate function. However, there are 2 more columns. One counts up 1,2,3,4 etc. The other is user_id. So it is unique of each quality. What I mean is first_name, last_name, school, hometown, will all be 1. When they are repeated, they get assigned to 2 and so on. There is no repeat for that. I think that could be my key to fixing this.
Joe Celko's Analytics and OLAP in SQL http://www.amazon.com/Celkos-Analytics-Kaufmann-Management-Systems/dp/0123695120
As far as I know, a view is only of the current data, not a snapshot. If you need an 'as at' then I'd setup a query as a stored procedure that inserts the selected data into a seperate table, then schedule the SP to run every day at midnight. You'll then have a snapshot of data in a table ready to be queried.
Are you talking about Oracle SQL Developer and a Microsoft SQL Server Database? If you're talking about Oracle SQL Developer and _any_ third party database, OSD is intended only to view objects in third party databases. What is your presentation on? What do you need to do with PHP and why do you need SQL Developer for it?
My presentation has to show the audience how I can connect a database to PHP. How can I manipulate a database by using PHP etc. The only software I have used to manage databases is SQL Developer. But I might try it with PDO.
This shouldn't be a problem, just set multiple references in the table CREATE. [MySQL Foreign Key Documentation](http://dev.mysql.com/doc/refman/5.5/en/innodb-foreign-key-constraints.html)
Fantastic, thank you so much!
My manager understood your query but it took him a bit. I don't understand it. We're regrouping tomorrow on our approach but I updated the post with an edit with where our heads are at.
I'd go with MS-SQL server. Take your time setting it up so it works well for you as you already have a solution, but not automation..... I believe that there is a migration utility to move MS-Access into SQL server. You can run your reports on a schedule and email them to your users without touching things. If you want to give a web page for ad-hoc queries, you can also write a web page/form to do manual reports to the user
Dude, who pissed in your Wheaties this morning? My comment was directed at the common concept of Big Data which many take to mean Billions of rows or more. Not at your usage. Seems to me you're the one being unprofessional. Furthermore, if you've really got that kind of setup, and you're here asking for help, perhaps those records would be better held by someone else, eh?
I agree with halifaxdatageek...Shiny is a great tool. There's another one called plot.ly that is good for user-facing applications. Here's some info on integration of the two: https://github.com/chriddyp/plotly-shiny
Have you played around with MS Access pass-through queries? It might be a gentle transition from some of your existing processes to true TSQL. 
&gt;One of the IT firms that I havenât worked with enough to trust suggested that they put in a SharePoint server, that that would be the next step up from MS Access. Iâve only touched SharePoint years ago and didnât really know that it could even handle this type of function. When I had taken a look at it, it was simply a company web portal and sort of a document management system. I recently saw a few posts in /r/sysadmin about SharePoint being a complete nightmare. That "IT firm" suggested a SharePoint setup because *it will make them a metric crap-ton of money*. SharePoint is **not** a "step up from Access"; they're completely different products. Can you use it for publishing reports? Yes, but those reports have to come from somewhere. Is it mostly a web portal and document management platform? Eh....that's an over-simplified explanation but it's not invalid either. If you just need pure reporting and aren't too picky about the UI, there's SSRS which comes with SQL Server starting with Standard Edition. The next step from MS Access is SQL Server, and you probably already have some level of licensing for it if you've got an EA with MS. If you don't, there's SQL Server Express Edition which, while limited, is free and will let you get started. However, if you need end-user apps to connect to your database, there's nothing that's "canned" - ideally, you'll build your own (preferably web-based, these days, ASP.NET MVC). But you can also use MS Access as a front-end, with your SQL Server database linked to it. &gt;In 95% of my use cases, I need to pull data in from various sources (ODBC), convert it using a lot of SQL, and then spit out to an Excel sheet. Various sources? Conversion? Output to Excel? That's an ETL job, which is perfect for SSIS. Which *also* comes with SQL Server. I would not give MS Access to users, pointed at SQL Server, and tell them to build queries graphically. That's how good SQL Servers get killed. Find out what queries they need, and build stored procedures for them to run.
Others have said it but I will reiterate--you are doing the right thing with SQL Server. With few exceptions my work history has been moving projects from Access to SQLS. Access is good for a quick and dirty front end, but mastering SQLS will get you a lot farther. After you get those tables up there, work on moving the joins to the server side using views and moving the automation up there using stored procedures. It'll make everything run that much faster and you will be glad you did.
It sounds like SSIS for gathering data from other sources via ODBC connections, SQL Server to store the data, and SSRS to run reports sourcing from SQL Server. SharePoint is good for deploying the SSRS reports that users can access and schedule . Although, it seems like overkill to get SharePoint just for your BI reports though. SQL Server Data Tools is a pretty good stack and I think it's free?
Since you are doing are complete select from BAZOOKAlead(a) sql will need to retrieve all the data. At this point a index doesn't really matter cause he need to selects everything anyway. Having a primary key does give you a automatic order since you transformed your heap into a cluster. &gt;(https://msdn.microsoft.com/en-us/library/hh213609.aspx) The join on BAZOOKAPK(b) only uses b.CONTACTID, so having a join on this column makes it easier for sql to join them. Having a index on A.EMAIL A.PHONE A.CELLPHONE A.WORKPHONE A.PREFPHONE could help as well, if they have the same order type he could go for a merge join. &gt;( http://www.madeirasql.com/loop-hash-and-merge-join-types/) However even if the join is perfect he still needs to go find the b.BAZOOKAPK. If you only need very little data from one table it could help to make the index covering(this does mean more storage is needed). Meaning he doesn't need to go back to the table to retrieve the extra data. Since b.BAZOOKAPK inst needed for any functional way in the query. But only as a presentation it doesn't need to be in the index for ordering purpose, this means includeding it is enough(if b.BAZOOKAPK is the primary key of the table it is automatically included in every non clustered index). &gt; (https://www.simple-talk.com/sql/learn-sql-server/using-covering-indexes-to-improve-query-performance/) I hope this helps. 
This is an important question. How many end users op? I work at a small company that heavily relies on sql but we do not have a dedicated dba. I run the default settings for ms sql on a decent rig and have never had system wide performance issues. You can get away with alot if your user base is small.
read up on creating and manipulate XML in T-SQL: https://msdn.microsoft.com/en-us/library/ms189885.aspx https://msdn.microsoft.com/en-us/library/ms186918.aspx http://www.w3schools.com/XPath/xpath_syntax.asp
&gt; Iâm nervous about losing some of the visual tools that make rapid development in Access so convenient. Open up Sql Server Management Studio (SSMS), hit 'New Query', then in the top left choose the database you want. In the menu, Query -&gt; Design Query in Editor. &gt; So my question is, what is the next step up from MS Access? Trust me on this, go to SQL. In 6 months, you won't look back. &gt; In 95% of my use cases, I need to pull data in from various sources (ODBC), convert it using a lot of SQL, and then spit out to an Excel sheet. Like anything, the devil is in the details. What *will* work is using SSIS for the scheduling and task flow. First, it will run a stored procedure you can write on sql server to prime whatever table you want (like if you need to remove old data before the process, or mark inactive). Second, it will connect to your ODBC sources and give a graphical 'column to column' gui so you can map the data from the source to the destination. Third it will call another stored procedure you can write to do all the data transformations you need. Finally, it will export from sql server to excel. Also, 2 other things. Excel makes it really easy and reliable to connect to a SQL server. You can have it a spreadsheet that will just pull all the current data when you open it. The other thing is for including input, SQL Server Reporting Services (SSRS) is included with a SQL Server license, and is made for actual reporting. It handles all the display in the browser, you can specify parameters for users to point in, and can export the results to Excel. The exact best course of action is going to vary on exactly what you are doing, but what I outlined will reliably work in pretty much all cases.
DB2 is still around. Not sure why people think RDBM is going away ever. Oh, and Oracle sucks.
DB2 is still around. Not sure why people think RDBMs are going away ever.
Yes, the number of rows to randomize is quite large. As suggested I used TABLESPACE along with NEWID()
I was planning on taking a class on PL/SQL next year... why does Oracle suck?
And the holy war continues...
&gt; Eh it is probably personal preference. Yes your comment was an opinion and an ill informed one. &amp;nbsp; &gt;I prefer MS-SQL but that is just me, I am sure there are reasons people use Oracle. I prefer Microsoft's default toolset to Oracle's for one. In the past MS's default toolset was always stronger than Oracle's, but this hasn't been the case for many years now. Oracle's [**SQL Developer**](http://www.oracle.com/technetwork/developer-tools/sql-developer/downloads/index.html) is excellent for development and management of Oracle Databases (as good as anything MS have to offer), and [**Data Modeler**](http://www.oracle.com/technetwork/developer-tools/datamodeler/overview/index.html) is regarded by many to be the best free multi-platform DB design tool there is. &amp;nbsp; &gt;There are some things PL-SQL can do that MS can't like grouping stored procs. So there are some arguments to be made that it is "more powerful" but at the same time more complex. PL/SQL is far more advanced than T-SQL, [**here's**](http://psoug.org/reference/sqlserver.html) some comparisons. Yet it is an intuitive and easy language to learn. &amp;nbsp; &gt;Learning and using T-SQL is a breeze imo. Learning PL/SQL is also easy, I've worked with many Junior developers / ex MS SQL Server developers who have picked things up in a very short period of time. It's a comprehensive, well designed, feature rich, well thought-out 3G language. &amp;nbsp; &gt; I prefer Microsoft's transaction handling model as well, tho there are those who disagree. I do. I've worked with both and I prefer Oracle's - ability to have autonomous transactions, rollback on exceptions (which is technically correct), save points, implicit transactions (no need to explicitly begin a transaction) etc. &amp;nbsp; &gt; Finally, I would say for the money you can do more for cheaper with SQL 2012 than you can with Oracle. Yes, for more than 5 users, MS SQL Server is probably cheaper (less than 5 Oracle is free), but SQL Server is nowhere near as scalable. &amp;nbsp; &gt;I also would say the MS-SQL community, documentation, etc. is much stronger than for Oracle. I strongly disagree, Oracle has a huge online documentation library, many whitepapers, conference videos, has a large community with many tech forums, which are equal to MS SQL Server.
Wow, calm down man. Calling me uninformed is a mighty giant conclusion you jumped to. I've used Oracle and I don't like it, it was an opinion and so is yours. You seem to have a big stake in "proving" me wrong however. Why is that? I even allowed that it was largely a personal preference and you still took "offense" somehow. If you can't even admit that VisualStudio &amp; SSMS &gt; ANYTHING Oracle has ever put out then I am not going to take you seriously. Never mind the tools for SSAS. You can argue PL-SQL is more complex but takes more effort to learn or the opposite, it can't be both. Also, you mention "5 users" for Oracle is free but somehow don't know that SQLExpress exists? You are far from an expert, sound like a fanboi and/or a shill. Try not to take this sort of thing so seriously.
Citation? Obviously large segments of the world disagree with you. (ya know, places like the NASDAQ)
* Some context : I was just trying to remove the opinion aspect for /u/BigBullshitBananas , I've worked on both platforms for many years and I've lost count of the number of times I've heard SQL Server lovers say the same things, hence why I addressed your comments. * I agree, Visual Studio is amazing, probably the best IDE there is, but SQL Developer is as good a tool as SSMS. I think Oracle realised they were being left behind on tools. * PL/SQL is not complex, I find it easier than T-SQL, you can do more advanced things with it than T-SQL, but you don't have to. * SQL Server is definitely cheaper, yes. * No offence intended to you fella and I'm calm, honest, just because I disagree doesn't make me angry. 
Thanks, I'll take a look.
What columns does the table have? How do you know what posts are related?
I've been a freelance consultant for 8 years. SQL related stuff is about 50% of my business. I do conversions, migrations, integrations, reporting, and design/programming. I love it but it can get pretty stressful at times with no one but yourself to rely on. I generally work 10-12 hours per day weekdays and about 2 Saturdays a month. Of that, I'll spend half the time on technical work and half on administrative/sales. If you can get a steady client base (including subcontracting) and are disciplined you can make a lot of money. I currently charge $150-$200/hour. 
Of course SQL is here to stay, there's so much infrastructure built on top of it. I didn't even watch the video, so maybe there are more compelling points too.
pv1Tot = SUM(SUM(CASE t.Value WHEN 1 THEN t.Amount ELSE 0.00 END)) OVER(PARTITION BY t.Key)
My SQL service is running under domain admin (yeah, I'm that guy). Last I checked that's the security context for xp_cmdshell, isn't it?
Got it thank you!
 SELECT SUM( IIF(PerceentOfClassesClosed = "Closed", 1.0, 0.0) ) / COUNT(1) leave off the WHERE clause.
Like this? SELECT SUM( IIF(PerceentOfClassesClosed = "Closed", 1.0, 0.0) ) / COUNT(1) FROM (SELECT IIF((Class.Enrollment / Class.Capacity) = 1, "Closed", "Open") AS PerceentOfClassesClosed FROM Class) EDIT: It worked! Thank you so much!
Yes, you would dictate the remote server in the script. You could even send the SQL query through PowerShell commandlets to the remote server.
Yep! You could even cut out the subquery if you wanted: SELECT CAST(SUM(Enrollment/Capacity) as decimal)/COUNT(1) FROM Class 
&gt; FROM Class, (SELECT IIF((Class.Enrollment / Class.Capacity) = 1, "Closed", "Open") AS PercentOfClassesClosed FROM Class) AS [%$##@_Alias] You've got a table and subquery in your FROM clause, but no join condition. This is creating a [Cartesian Product](http://en.wikipedia.org/wiki/Cartesian_product), where every row in the Class table is joined to every row in the subquery. You need to find a way to join the two (probably Class_ID or something like that) or get rid of the subquery. Starting from the shortened query I posted above, you can just add the "`GROUP BY Department`" and it'll work just fine. SELECT Department, CAST(SUM(Enrollment/Capacity) as decimal)/COUNT(1) FROM Class GROUP BY Department
yes, usually you have an IDE (in the case of mysql, MySQL Workbench is a popular choice) that connects to the server. The server can be remote (on a WAN or LAN) or can be local (on the machine with the IDE). I personally have a few AWS environments that have MySQL set up on them that I interact with using MySQL Workbench or HeidiSQL from my home PC.
it looks frusterating 
If you haven't tried [0xDBE](https://www.jetbrains.com/dbe/), you should. It's like terminal multiplexing... for database connections.
Several options. A CTE, a temp table, or a table variable, depending on the number of rows in the initial recordset. 
That looks like it will be incredibly useful. Thanks!
Glad to help!
That's the first I've heard of that with a CTE, but a little google-fu did confirm that, TIL. I found a comment from Joe Celko that indicates its a flaw in the the SQL Server engine and that Oracle and DB2 don't have that issue. I have used CTEs to great effect to replace both table variables and temp tables, so like everything else, it depends. I normally decide based on how large the data set is, and how I want to organize the script. If this is a one off, the CTE would be faster to write IMO. I didn't even think of using a view, but that's also an option.
As /u/carpii mentioned, a view is probably the best way to go. If you find that the select statement is the bottleneck (and that it's that way due to the complicated logic rather the amount of data) then you can do a select once &amp; drop it into a temp table. Then select from the temp table where the data exists alone without the logic that parses it.
This has nothing to do with SQL. You posted HTML. 
Yes. There are a lot of good ways to do it, but the lightest, simplest &amp; most universal is with [HeidiSQL](http://www.heidisql.com/). Just pop in your credentials (Host, name &amp; password). Then you have access to the entire database. You can see the raw data see your views (which are predefined Select queries) launch stored procedures (predetermined actionable commands) or do anything else that you need from any computer.
The agencies are offering contracts. This is substantially different from consulting in the purest sense. I've just stopped contracting after 23 years. You will work a regular day job for the period of the contract, after which you may or may not be offered extensions/renewals. During the term of the contract you will be paid the daily rate stipulated. From this you must fund all company and personal taxes, expenses, training etc. At first, you may wish to work under an umbrella company - this removes some of the stress of running your own company - for a fee. I thing Parasol is one such company - there are others. Work for me has been pretty consistent overall, though of late I have found the market to be a bit more volatile. To be fair, I work in large-scale data warehousing which is pretty niche - your situation may be more stable. The best rate I achieved for SQL-based application design and development was Â£500/day + expenses. The modal (most popular) rate for me was Â£400/day. There has been no real increase in rates since around 1998 for me. HTH
Which (if any) exams have you taken, and if you have done some, do they help with getting you new contracts or is experience more important?
If you're using the `address_id` as a filter for your unioned tables, then you're thinking about this backwards. You want your "base table" to be the unioned queries and use your first `address_id` table to filter that query. SELECT address_id, col1, col2 FROM ( SELECT address_id, col1, col2 FROM tbl_a UNION -- or UNION ALL if you don't care about possible duplicates -- it'll run slightly faster SELECT address_id, col1, col2 FROM tbl_b ) AS base_table WHERE EXISTS ( SELECT * FROM tbl_c WHERE tbl_c.col = @val -- however you want to filter this AND tbl_c.address_id = tbl_b.address_id ) If your first select query (what I call `tbl_c` in the above example) has an appropriate index, it will be faster than writing to a temp table. This also has the advantage over creating a view because you're not creating a database object that you possibly not need or don't have permission to create (unless you do need to do this often, then it's a consideration). 
Depends on the RDBMS. Greenplum (PostgreSQL on Big Data Steroids) does not execute the CTE multiple times.
I was once a MCSE but don't have any current certifications. I stay current by reading a lot. If you're just starting out I think certs are helpful to get your foot in the door. 
Sounds awesome. Any chance you could blog your experience? I would love to read up on the steps you took.
&gt; I think Orders, Order_Details and Inventory are the required databases for this query. you nailed it, good job -- except they're called tables, not databases
It's a type of `char(1)` and then read about `CONSTRAINT`s (on mobile so I don't have a reference handy). Edit: are you accounting for `NULL`s?
I'm a microsoft SQL Developer and I use C# and SSIS for most of my work. I can do very complex things with these tools and I'm just wondering if someone can explain what pl/sql can do that sql server(t-sql) cannot do? I work for a large company and our corporate teams use oracle to house our data and we use SQL Server to consume the data and generate reports, build SSAS cubes etc... The reason we don't touch oracle is because its way too expensive and probably not neccessary for my team. If someone could explain what pl/sql can do that you cant do in t-sql would love to hear it. 
Thank you for this! 
My thoughts, I'll use a select, which you can turn into a delete Join inventory and orderdetail on product select * from inventory where product_ref in (select product_ref from order_detail where Last Last_Sale &lt; DATE_SUB(now(), INTERVAL 6 MONTH) )
Problem is you don't have a last sold date so you will need to make a few assumptions, like would I ever re-order a product with out selling completely out of stock.
This actually varies by database. Reddit is not a manual. You should look to see whether your database supports boolean fields, instead.
Something like this, although if a cd has never sold then do you delete or not? If, not this should do : Select * -- change 'select *' to 'delete' From Inventory Where product_ref in ( select Product_Ref from Orders Join Order_Details on Orders.Order_ID = Order_Details.Order_ID Group by product_ref Having max(Order_Date) &lt; dateadd(mm, -5, getdate()) ) Sqlfiddle to play with it http://sqlfiddle.com/#!6/fb440/1
mysql workbench has free versions
Download postgres. Or sqlite.
OP, Google: soft delete.
I might be wrong, but I can't think of a compelling reason to use UNION ALL to perform multiple selects on a single table. I could make something up, but it would be a rare situation and not useful here. Your professor is probably just trying to get you to perform 4 unions to show you understand basic syntax, but it's stupid. Make him a bunch of table reads if that's what he wants. Your below statement sounds inefficient though. IIF() isn't the greatest in SQL - it works now but I don't bother with it. The criteria in your post is not clear but I'm interpreting your use of "class" as Class.capacity. Try something like: SELECT Class.Teacher, CASE WHEN Class.Capacity &gt;= 20 THEN CASE WHEN Class.Enrollment &gt;= .9 THEN 100 ELSE 50 END WHEN Class.Capacity &lt; 20 THEN CASE WHEN Class.Enrollment &gt;= .9 THEN 25 ELSE 1 END END AS Bonus etc. Might be some syntax errors, I'm tipsy. Also assumes your columns are NOT NULL because it doesn't deal with ISNULL. I doubt your professor will produce a faster query using UNION ALL. Enjoy!
Mods are sucking and this sub is going to shit because it's only full of basic questions that can be googled. 
We pretty much store what you do, but have a scheduled process which clears out old items (older than a certain criteria), that way the data doesn't get too huge. If you want to keep all the data, but keep the table size manageable, dump old data to a file or a data warehouse.
Try breaking it down into its constituent parts :- SELECT COUNT(*) FROM suggestion_votes WHERE s_id = suggestions.id AND vote = 1 what does that return?
It returns 0 or 1 depending on if the user has voted on Yes
How much of that data is truly actionable today? Are errors that came up 3 years ago **really** relevant to today's code? You need to purge out old data. Set a time limit and on a scheduled basis, purge out anything that's older than that. You may also want to look into a more sophisticated tracking system like https://raygun.io/ That said, in a properly set up database, "way too large" isn't really a thing, unless you've got bad indexes, a bad schema, aren't taking advantage of all the features of your DBMS, or your server is undersized.
Thanks, worked like a charm. I didn't realise that you couldn't use an alias that way.
&gt; Then have a monthly/weekly job insert events older than a three(?) months to that table and on success delete from the primary under the same condition. If you're using Enterprise Edition, you don't even have to do it in steps like that - just partition the table, and move the partition from the "live" table to the "archive" table. It'll take about a second to move even large amounts of data.
This avoids the sub-selects. I think this would work... SELECT suggestions.*, sum(if(vote = 1, 1, 0)) as Y, sum(IF(vote = 0, 1, 0)) as N, SUM(IF(vote = 1, 1, IF(vote = 0, -1, 0)) as balance FROM suggestions JOIN suggestion_votes ON (s_id = suggestions.id) 
Oh really? I thought ms sql in-memory engine kinda comes attached at the hip to the delayed durability/less logging (at least I remember append-only was one of the use cases they showcased). Thanks for the tip - I'll try to read up on it more.
Nope, in-memory OLTP is Expensive Edition, but Delayed Durability is good all the way down to Express.
I echo alinroc's statement that, in the regular course of business you shouldn't ever find yourself in a position where you should use cursor. Of all of the loop constructs, cursor is the most expensive (possibly debatable if it's fast-forward, but I digress). RBAR is bad mmkay? (row by agonizing row) If the issue is in the trigger, check into that further. Do not flippantly disable the trigger, but first understand WHY it is acting this way. May I suggest you begin wrapping your updates in a transaction.
I suppose a reasonably efficient query might involve a single scan through both tables by using just a tiny bit of analytic functions: SELECT x1.NAME, x1.sum_pop FROM (SELECT x.NAME, x.sum_pop, MAX(x.sum_pop) OVER() AS max_sum_pop FROM (SELECT p.NAME, Sum(p.percentage* c.population) AS sum_pop FROM professions p INNER JOIN city c ON p.city = c.NAME GROUP BY p.NAME) x) x1 WHERE x1.sum_pop = x1.max_sum_pop; The OVER() keyword below is the indicator of an analytic function. [THIS](http://www.oracle.com/technetwork/issue-archive/2013/13-mar/o23sql-1906475.html) might be a good place to start if you want to learn more about them.
hmm. let me go letter-by-letter. A: log writes are mostly a single-record, separate from the main transaction (you wouldn't want to lose the log record if your transaction rolls back, would you?). So - no A. C: there aren't (or shouldn't) be any triggers on logging tables, the data integrity in the flat record where your app log/tracing statements have a bunch of hardcoded strings/codes/ids is more about source code control than data integrity, and hopefully, we're not going to fall apart without our brilliant sql engine generating sequential numbers. So, no C. I: app logging is write-mostly kinda function. What isolation we need? Just the ability to do concurrent inserts/appends. What about reading, however infrequent? A flat record is either there or not, so we should be read-consistent already. So, no I (other than concurrent appends). D: ah, the transaction log. We don't have transactions (see A) and rolling back a log insert is silly talk. Most things that will destroy your logging framework - disk down, server down, network down - are dramatic enough and having "transactional" logging does nothing to detect/record those. Same goes for running out of space: neither the sql engine nor the file-based logging framework are going to be writing anything. So, throwing a raid 10 (or even 5) should cover most of problems of the D. You are welcome to hold on to your opinion though.
A: Granted, but if there is a transaction rollbacked back on a log write, it probably failed a consistency check and shouldn't be inserted. MongoDB offers no benefit for this operation either. C: Triggers/etc no, but detecting injection attempts, yes. Also worth noting is how poorly hardened the [default installation of MongoDB](https://www.owasp.org/index.php/Testing_for_NoSQL_injection) is against a JavaScript attacks via a log entry(doubly so as the catch all will generally attempt to log all the information that caused the problem). I: This is my primary sticking point, if you are running multiple applications that write to the same logger or are operating a load balanced environment(*edit - even multiple threads from the same application*), this can/will cause problems. D: Not true at all, most RDBMS allow you to define a checkpoint constraints or you can force them manually. So unless you have a catastrophic failure of multiple systems(see: server room power bump) you could checkpoint to another drive manually and still be consistent (with a little work). As a side note, your logging server should probably be on a dynamically expandable SAN drive which automatically allocates more space when a drive gets low and sends out an alert to the appropriate administrator that it occurred. *** In summary, not saying MongoDB can't be a logging server, you just generally have to be more careful or utilize redundancy if you do; alternatively write the risks into your recovery policies and call it a day. 
das not it mane. pepper your resume.
 SELECT CustomerID , VisitDate FROM daTable GROUP BY CustomerID , VisitDate HAVING COUNT(*) &gt; 1 
I feel your pain..
I'd have used Joins. But the tables are in two different servers.
That's not an ideal case in my situation unfortunately. I have a lot of servers and a lot of tables.
Ummm....Please tell me you didn't just run that on prod without seeing what happened on Dev first...
Then might I recommend utilizing a staging server that pulls the related information and can be re-populated and queried when required?
Ha! I like testing on prod. I feel like a rebel without a cause.
Why not try running it with a top 10 clause first?
We're doing the same. Some of our data sources are just horrible, but the contractor doing the work has an unending supply of optimism to counter my skepticism. 
Thanks guys I was able to derive what I wanted from what was mentioned by /u/r3pr0b8 and /u/standgeblasen. Using just 'Group By' on both fields seems to have worked.
No Dev environment? Wow. Just wow.
&gt;I want to learn SQL. It will help me later down my future. What are some good books, guides, websites, etc. I will be taking a few classes in school for SQL. However I want to learn more then they can offer. SQL, in and of itself, is a generic language. Understanding the basics of CRUD (Create [INSERT], Retrieve [SELECT], Update [UPDATE], and Delete [DELETE]) will set you on the right path. Where things get interesting are the 'customization' that various RDBMS vendors have chosen to implement - Microsoft calls theirs T-SQL or Transact SQL; Oracle's is PL/SQL or Programming Language/SQL and there are others - for example, Postgres is very similar in syntax to Oracle's PL/SQL, but not exactly the same. &gt;Edit: also R is something I would like to learn. SQL can build the dataset for R, and SQL - through the aforementioned T-SQL or PL/SQL 'flavors' - has some analytic functionality, but that's about where the relationship usually ends. Do yourself a favor and don't think that SQL = R.
I'll have to give that a run, thanks for the input, that can be run in a batch file? sorry for my ignorance, looks more like t-sql to me but I'm probably wrong.
dynamic query builder == microsoft access if you don't know why 1. that's not a sarcastic answer, and 2. it's also a non-trivial answer... then perhaps you might suggest some more insight as to what you're working on 
There was a story in /r/talesfromtechsupport where someone was trying to do analysis using a database designed by geneticists. At least one of the tables had 450+ columns.
&gt; There was a reason the DBA's denied it ...And that reason is not what you think it is.
Your current edit process is pretty terrible. **SQL Server tables are not spreadsheets and should not be treated as such**. You really ought to have a client application that updates your data as needed, or perform your updates through SQL queries. But editing direct in a grid view in SSMS? No. Just..no. What happens when user A updates a record, then user B attempts to update the same record? What you're basically doing is giving all users unfettered access to completely hose your data. No input validation beyond the data types and any `CONSTRAINT`s on the fields. Not to mention the ability to write and execute a train wreck of a query, bringing your server to a crawl. Give users a client application (web-based or otherwise) to update their data. That application gets its own ID to access the database with, and that ID only has access to do the minimum it needs. Control access to the application via AD. 1. How nested are we talking about? They can be a mess to unwind and figure out where your data **really** comes from (not to mention your performance problems). Andy Yun has a [stored proc to help untangle them](https://sqlbek.wordpress.com/2015/03/03/debuting-sp_helpexpandview/) but the best way is to avoid using them in the first place, if you can. Like any tool, they aren't inherently bad - it's the specific usage that will make or break you. 2. A clustered index on a GUID? Lots of page splits and unfilled pages. See http://sqlmag.com/database-performance-tuning/clustered-indexes-based-upon-guids
Heh yea... I'm still not sure how to create empty line breaks. We have trying to work through giving them access through a web page but there are like 100 tables that need to be constantly touched by our support staff. We are also in the process of moving over to a domain model! :) Thank you for the info. I'm not sure how nested the views are but I will find out today. Thanks again for all your feedback!!
&gt; We have trying to work through giving them access through a web page but there are like 100 tables that need to be constantly touched by our support staff. Stop thinking about it at the table level. Group things logically, by business function or task. If you're having to constantly update every field on 100 tables, there's something really weird with your system, or your business processes.
as having a table or a column named "group" or "order" is any better. And boy, do developers like to use those in their ORMs... 
Awesome, thanks!!
to clarify: I know this. This is a godawful system we PAY for that uses this. 
We lynch anyone in our organization that uses spaces in table/view/stored proc names. This one guy made a table that found it's way into production and I cringe every time I run across it; Monthly Sales Data. Why of why couldn't he have named it MonthlySalesData or Monthly_Sales_Data, we just enclose it in brackets [Monthly Sales Data] but I still rage when I see it.
What's listed above is the table structure. 
I worked on a project where someone had named a table 'dictionary' I thought the dba was going to have an aneurysm 
I hate this function as if you miss a comma you accidentally name column1 as column2 instead of choosing them both. 
Look at doing sub selects. select blah ,(select blah from foo where blahId= table.blahId and BirthOrder=1) as SomeName from table
will do thank you!
Original Post was in SQL Server. http://redd.it/34bjia 
 SELECT t1.AdultFemaleName , t2_1.ChildName AS 1st_child , t2_2.ChildName AS 2nd_child FROM t1 LEFT OUTER JOIN t2 AS t2_1 ON t2_1.FK = t1.PK AND t2_1.ChildBirthOrderNumber = 1 LEFT OUTER JOIN t2 AS t2_2 ON t2_2.FK = t1.PK AND t2_2.ChildBirthOrderNumber = 2
That's what I would do too, like: SELECT AdultFemaleName, (SELECT ChildName FROM t2 WHERE FK = t1.PK AND ChildBirthOrderNumber = 1) AS oldest_child, (SELECT ChildName FROM t2 WHERE FK = t1.PK AND ChildBirthOrderNumber = 2) AS second_child, FROM t1 ORDER BY t1.AdultFemaleName
you can use them but you will have to delimit them using brackets []
Someone decided to name a column PRIMARY in one our tables. All I can do is shake my head and cry.
please explain the exact format of the 3rd column -- is it comma-separated with no spaces, or pipe-separated with spaces?
Also the same IDs in column one will show up multiple times and we want to capture every time a ID in column 1 matches any ID in column 3.
what a horrible table design -- no offence maybe this is what you're looking for? UPDATE daTable AS T SET IsaType = 'X' WHERE EXISTS ( SELECT Type FROM daTable WHERE Type = T.Type ) 
Honestly, I don't see the need for PowerBI anymore. SSRS comes free with a license of SQL Server, and you don't need Visual Studio to design most types of reports anymore. Report Builder 3.0 is free, and works well in the cases where features co-exist with BI. Was there a specific reason you wanted to use BI over Reporting Services? 
Shouldn't it be SELECT IDS
Updated my example a bit http://imgur.com/cfvbZXz
It sounds like you need a lot more than just a database. To me, it looks like a fairly simple web application with a database backend and some reporting functions. The hard part of the web app will be making sure it is secure and that people can only see the patient records that are relevant to the user, not just everything. Especially if you are putting this on the internet and not just your internal network.
I've written/updated a lot of queries to pull credit card data. Nothing fancy, really. The DB is pretty big, so you want to be sure you can write an efficient query, but there aren't any secret "financial industry" tricks to that. Know what's indexed, know how the joins work, write decent where clauses. Grouping and summing is pretty important, so we can add up the transactions for each card, or sum up the spend by month, or by bucket of spend. But all of that is done with basic aggregate functions. If you knew SQL at a decent level, there's nothing in any of our canned queries that you wouldn't understand. 
i think you should test it on that example and find out ;o)
You might be the first person I've ever heard "like" cognos. I've little experience with it myself, but people I know who've used it describe it as over complicated, slow, and difficult to manage. 
Here's a primer on deadlocking in SQL Server: https://www.simple-talk.com/sql/database-administration/handling-deadlocks-in-sql-server/ About halfway down, Jonathan gets into a discussion about the common types of deadlock scenarios: "This section assumes knowledge of basic locking mechanisms inside SQL Server and examines how to resolve the most common types of deadlock, namely the bookmark lookup deadlock, the serializable range scan deadlock, the cascading constraint deadlock, the intra-query parallelism deadlock and the accessing objects in different orders deadlock." I suspect you are seeing intra-query parallelism, but maybe not. Once you get your deadlock graph you should be able to better determine what is happening under the hood. HTH
Perfect, thanks a lot i just put 1 and 2 together and realized that the sqlcmd gave me the ability to run .sql scripts. That works a lot better. Thanks for the assistance, just need to tweak it a little to my enviroment and tasking.
Ask around for a data model or ERD (entity relationship diagram). It'll look something like [this](http://i.imgur.com/z1mLI4F.png). There are tools, depending on your RDBMS, that can build one for you, but in my experience if the database is poorly managed enough that one isn't readily available, it isn't properly created with PKs and FKs that would allow one to be generated automatically.
&gt; I don't like the idea of "loose-data" living out there, giving the users abilities to infer incorrect assumptions and query the server on a whim. This is one of those things that will always happen, unfortunately. We can preach the "one version of the truth" to everyone that wants to hear, but if an exec comes to someone who needs something that may not live in the RDBMS, these users will simply build it themselves. I have struggled with it for the last 20 years of my career.
Could you go into a bit more detail about the differences between PostgreSQL and MySQL? How are they related to SQL and NoSQL?
Further to memorising the most important tables, its really just all about practice, the more time you spend using the DB (in the back-end of course), the more familiar you get with it, there aren't really any other shortcuts to this unfortunately. Most DBAs start off in a similar situation to you, they inherit a large DB, or DBs, that others have created, you just need to keep plugging away and eventually it sinks in. If you keep coming across the same tables, they are probably worth remembering, if you keep coming across the same stored procedures, have a look at the tables they are using, You may never understand the full system, they can be huge, but the important bits will eventually fit in to place.
I asked for a list of all the tables in my database and all the fields in them. The list was given to me with 10 of the most useful tables. Ive slowly been going through the others and adding comments to tables describing how they might be useful for analysis. When ive found something worth analysing ive gone to my application and found out how to get the data in the field.
This part of a weekly class we are using to train our Software Engineers at [Emergency Reporting](http://www.EmergencyReporting.com). Please keep discussions on Reddit and I welcome any feedback!
that's very helpful. they only see the front end interface, and never see how the data where and how the information is stored in the system or what's seen behind the scenes. So I'm pretty much going to be the only person in the company that will have intimate knowledge of the database structure if I do this correctly. Not even the IT team is looking at that, they pretty much only maintain the network and deal with technical issues.
Thanks.. I was able to use Microsoft SQL Server express to pull a list of all the tables.. and that's actually helped me somewhat understand the relationships of some tables to other, but I only see snapshots whereas I want to see the bigger picture :I
I've been using Visual Studio, how do these compare it terms of establishing a web app that makes calls to an SQL database? I've experimented in labs with Northwind, using MVC as well. They go together pretty seamlessly but all of it was done with Entity Framework. Which threw together my app without me doing much. Now i'm sitting here with a relatively functional webpage that does queries and lets me edit the database with the 3 default options (create,edit,delete). Yet, I found it puzzling to work out how to add more options and pages that will let me do even more to it. The codes all there but I'm still left wondering how to utilize it to my will. ie adding onto it with more ID's and tables, What keys to past to it, normalization of the DB, beginner stuff. Edit: All of these data models are similar to what I use in Visual Studio, what an amazing IDE... I recommend Visual Studio then. To the OP definitely get your hands on Northwind and start passing select statements to it, as well as going to www.w3schools.com/sql (they use northwind model as well I believe.)
A quick google tells me R's sprintf is a wrapper for the C version, so you should be able to escape the percent signs by typing them twice. AND ( Date LIKE '%%01/07/2014%%') Is it just me, or does everyone else get really nervous about implicit string to date conversion? I'd wrap that in a str_to_date just to be safe.
You won't regret explicitly returning `NULL` in your `ELSE` case. Explicitly defining behavior is preferable over database-specific behavior which may change in a future release.
Good to know.. Same idea though regardless.
Another thing to consider is to turn on snapshot isolation (mvcc) . You can turn it on for just this transaction if you don't want to turn it on by default for the whole db. 
Thanks! Thats a good idea!
An even *better* way may be to have a date table which you `JOIN` to and defines which dates are in which quarters. Your execution plans will thank you. http://www.brentozar.com/archive/2014/12/simply-must-date-table-video/ - He specifically calls out fiscal quarters around 1:25 in the intro. Bonus question: why are you casting strings as `int`s (`cast('1' as int)`) when you can just return an `int` explicitly (`WHEN month(a.date) in ('1','2','3') THEN 1`)? What I'm getting at (as a fellow programmer) in my previous post is this: Let's say you've got some C code that compiles and works the way you want it to, but the behavior you're utilizing isn't defined by the C standard. A future compiler (or even a different compiler that's a contemporary of your current compiler) may handle the condition differently, resulting in unexpected behavior of your software. 
Absolutely. It's a perfect interview question to separate the basics. I used to get tripped up by the join stuff because I used to work in a database that had a really neat syntax for it using "\*=","=","=\*","\*=\*" for the different joins. So I could never remember which was which as I never had to type it out longhand. 
Explicitly defining the default case lets future developers know what happens when an unspecified value is encountered. 
I do loves me some emacs. My ".emacs" file is older than people I work with. Why wouldn't you get to use emacs? SSIS seems to me to have been created for people who are afraid of writing code. Maybe it's great when it's scaled up to huge bundles of processes and vast etl workflows. But it's so unbelievably fragile that I'll never willingly use it. Change a column or sheet name in an excel workbook? BANG. Add an additional header row in your import file? BANG. Bad data? BANG. Fuck that. I can write a truly simple perl script that will import workbooks (or anything else) into a database with adaptive processing of import files, massively flexible error handling and reporting, that will kick out an email, ftp files in or out or do whatever else I need if need be. SSIS is an amazing step backwards in every way I've seen in the name of that goofy workflow crap that looks good in a management presentation. Sorry. I have work to do. 
I would prefer them to find out about it by having the job fail when it encounters something it isn't explicitly defined to do, such as most languages I'm familiar with developing in. SQL is it's own weird little world.
Would I need an emacs editor? I don't have admin rights to my local machine so I can only use what's on the approved list.
&gt;SQL Server, Crystal Reports and SSRS Ssshheeeiiittt, who are you, me? A good portion of my job is building stored procedures/view and crafting Crystal and SSRS reports around them. &gt;AS400... We had a party a few years ago when we retired our AS400. Good times. First, it depends what version of SQL server you are working on. MERGE and IIF are VERY useful but you are SOL if you are working on any T-SQL server lower than 2008. DATEPART and DATEDIFF is useful as well. T-SQL has great CURSOR functions that let you build something similar to a For Each function in VBA. I'd start here: [Built-in Functions (Transact-SQL)](https://msdn.microsoft.com/en-us/library/ms174318.aspx) I've download and watched a bunch of PluralSight, TrainSignal, UDemy, etc SQL videos and while the speakers can be long winded at times, watching and absorbing the information is helpful. 
what's an optimization fence?
Can't you ask the support for the program you pay for how to get the information you want? You could look around in the files and probably work something out. But can you trust your conclusions? There's a ton of files in that zip that might be relevant too. If you want to use the data for a thesis you might as well make sure it's correct. The only way to do that is to use the program the way it's intended. Digging around in the program's internal files isn't a supported use case.
You will still need to set it to simple to truncate the existing log. ALTER DATABASE YourDatabase SET RECOVERY SIMPLE; GO DBCC SHRINKFILE (YourDatabase_Log, NewSizeInMB); GO ALTER DATABASE YourDatabase SET RECOVERY FULL; GO NewSizeInMB should be set to what you want the initial size of the new log to be. Some use 50. Obviously you would want to take a [FULL COPY-ONLY](http://www.mssqltips.com/sqlservertip/1772/copy-only-backup-for-sql-server-2005-and-sql-server-2008/) backup before doing this; *also highly recommend a regular FULL backup after*. Additionally make sure to do this in a low use period, otherwise you could cause serious performance problems. 
&gt; I will be looking into a better way to deploy and **monitor SQL backups** I find a more reliable method is to check the msdb.dbo.backupsets because you can pull more information. Here is my baseline query(only gives info for FULL and DIFFERENTIAL, only returns the number of days since last backup occurred) for finding last backup time and type, I've expanded this extensively to give me much more information, but this one is a good start. SELECT abset.ServerName, abset.DatabaseName, abset.LastFullBackupDate, abset.LastDiffBackupDate, abset.LastCompletedBackup, abset.LastBackupType, CASE WHEN LastCompletedBackup IS NULL THEN NULL ELSE DATEDIFF(DD, abset.LastCompletedBackup, GETDATE()) END AS BackupDeltaInDays FROM (SELECT @@SERVERNAME AS ServerName, sd.name AS DatabaseName, lfb.LastFullBackupDate, ldb.LastDiffBackupDate, CASE WHEN ISNULL(lfb.LastFullBackupDate, '1900-01-02') &gt;= ISNULL(ldb.LastDiffBackupDate, '1900-01-01') THEN LastFullBackupDate ELSE LastDiffBackupDate END AS LastCompletedBackup, CASE WHEN (lfb.LastFullBackupDate IS NULL) AND (ldb.LastDiffBackupDate IS NULL) THEN 'NEVER' WHEN ((lfb.LastFullBackupDate IS NOT NULL) AND (ldb.LastDiffBackupDate IS NULL)) OR ((lfb.LastFullBackupDate IS NOT NULL) AND (ldb.LastDiffBackupDate IS NOT NULL) AND (ldb.LastDiffBackupDate &lt; lfb.LastFullBackupDate)) THEN 'FULL' WHEN (lfb.LastFullBackupDate IS NOT NULL) AND (ldb.LastDiffBackupDate IS NOT NULL) AND (ldb.LastDiffBackupDate &gt; lfb.LastFullBackupDate) THEN 'DIFF' -- NOTE: This should never occur, will most likely indicate corrupt backup set or alternate backup methods are being utilized. ELSE 'NA/ERR' END AS LastBackupType FROM sys.sysdatabases AS sd LEFT JOIN (SELECT bf.database_name, bf.type AS BackupType, MAX(bf.backup_finish_date) AS LastFullBackupDate FROM msdb.dbo.backupset AS bf WHERE bf.type = 'D' AND bf.database_name IN (SELECT name FROM sys.sysdatabases) GROUP BY bf.database_name, bf.type) AS lfb ON lfb.database_name = sd.name LEFT JOIN (SELECT bd.database_name, bd.type AS BackupType, MAX(bd.backup_finish_date) AS LastDiffBackupDate FROM msdb.dbo.backupset AS bd WHERE bd.type = 'I' AND bd.database_name IN (SELECT name FROM sys.sysdatabases) GROUP BY bd.database_name, bd.type) AS ldb ON lfb.database_name = ldb.database_name WHERE sd.name &lt;&gt; 'tempdb') AS abset 
A "fence" that shields off the optimiser from doing certain optimisations. This can be useful in cases when you *know* that the optimiser will make a wrong decision and another plan is more optimal. E.g. if you *know* that a full table scan will be faster than hitting the index first, you might want to apply a trick to keep the optimiser from choosing the index.
That has confused the crap out of me on more than one occasion!
Not an answer to you question but I often have to write queries in MySQL, oracle, and PostgreSQL myself. I have a big whiteboard across from my desk where I write the few key differences in the syntax in each option in a big matrix and it helps me just look up and and translate what I want to do in the syntax I know versus the syntax I'm working with
I agree with you and I don't. For this particular case, there should never ever be a null, and in the event that there is the insert into will fail because the table will not accept a null. For any possible legitimate date, all cases are covered by the statement and adding "ELSE null" or "ELSE anything" is unnecessary code, which by definition is only inserted into something by an incompetent developer. I was more curious about the broader concept, which shows I have considered the ramifications. I really don't care whether my future replacement thinks I'm an idiot or not.
Maybe "unnecessary" but at a minimum, comment. Not doing so could cost someone hours of troubleshooting down the road and I would secretly hate you for wasting my time.
&gt; Wise Owl SQL Server Procedures and Programming, and/or Wise Owl SSRS. BAM! This is exactly what I was looking for. Thank you good sir. 
Alright I will book mark this, thanks. Yeah I'm going to be hopping from server to server so I'll be on anything from 2005 to the latest and greatest. Thanks a bunch!
Damn you, that my goto line. 
Group By CountyField?
One useful tool is to generate an [explain plan](http://stackoverflow.com/questions/7359702/how-do-i-obtain-a-query-execution-plan) for a query. This will essentially show the output from the optimizer, and is a representation of how the engine intends to go about executing the query.
This sounds... Bdaas. 
I would say that it's already here to a certain degree. 
I agree, location based services are one example. Larger companies bring in millions of data points everyday. Supply an API and bam. Big data as a service.
Yes. It already is. 
Look up the company YouData they had a stellar initial traction run to the point they had to shut their service down while they upgraded. They make money by storing ads on their servers and showing them to you based on a 400+ question questionnaire that you fill out showing your interests and clothing sizes ect. so your ads are actually targeted at you. 
I don't think that works since you are grouping by the geometry of that layer, rather than names
I'm also in the financial industry, and used Access long enough to develop a strong dislike for it. I did Excel vlookups, VBA and Access all day every day. Copy, paste, make Excel pivot table. I dreamed of the day when I would use real enterprise tools and say goodbye to Access forever. I finally found my exit ticket when we started using [TOAD]( http://en.m.wikipedia.org/wiki/Toad_%28software%29) to query some Oracle databases that would not allow a link to Access. As an Access user I found the TOAD learning curve pretty easy. The licensed version of TOAD can link multiple sources. So you can join an Access table, an Excel worksheet and a text file for example. I'm at the same company and now in a better job writing T-SQL code every day. 
highlight in excel, then click "merge cells"
Probably not. Once you start dealing with petabytes of data, "the cloud" starts getting very far away. How far away? Even if companies started shelling out for gigabit fibre connections, 1 petabyte would take over 100 days to transfer between servers. **In a best case scenario.**
I meant from the business side, getting all your data from your servers to the cloud takes a fuckton of time :P
I found the exact opposite - we recently dumped all our Cognos OLAP tools for MS solutions. It blew my mind when I found out that the cube build process for Cognos could only use a maximum of 2 cores, and that it had no built in scheduling system (was strung together by batch files and Windows Task Scheduler).
You can put an unencrypted file on a server available for SFTP pickup or you can encrypt the file and use plain FTP. GPG, the free alternative to PGP, works well for this. Files can be encrypted at the command line with relative ease.....
Likely a cost cutting measure. each physical machine costs money. Each server OS and SQL-Server install requires a license + per user. Pushing data to each location clogs the internet gateway. Stopping all this data allows the company to use a smaller pipe in some cases, gain internet speed without upgrading. Consolidating saves money
Executing cmd batches from SQL Server gives me the screaming willies. You should be doing ETL with SSIS.
I don't really know. it's not like you can browse the standard. I actually want the lowest common denominator more than the standard
Encrypting the file doesn't protect the credentials you use to connect to the FTP server. FTP needs to die. It is a horrible and insecure protocol.
It's *one* approach, for sure. I'd suggest wrapping everything up in an SSIS package if you can, and schedule that to run via Agent. There are several SFTP-capable libraries you can utilize from within script tasks, both free and paid. Please don't say "unfortunately" about SFTP. Plain FTP is a terrible, messy, insecure protocol which needs to die. It should have died over a decade ago.
Yup personally speaking if we're talking about a smaller company that isn't using a job scheduler then either setup a cron job or a Windows Scheduled task to run a python script that using ftplib and pyodbc. Agree 110% with FTP. It still amazes me that people use it, especially in the corporate world (cough. BloombergWTF. cough)
We have a network share we use. We spool the file with sqlplus and drop it on the share. Later the as400 comes along and pulls it then loads it into our accounting system. If it were for a user off our network we would drop it into a directory accessible with sftp.
What RDBMS are you using? As /u/alinroc suggested, SSIS is perfect, but only if you're using Microsoft SQL server. A scheduled task also isn't bad. It's not not a SQL-specific place to store tasks.
I'm fairly certain that there isn't a procedural language in the SQL standard. Hence it's not possible to write standards compliant pl/sql. Besides, all the databases implement the standard differently so even if you managed to write standards compliant sql that works on oracle there is no guarantee it will work on another database. My suggestion is to first decide what database to move to so you know what it can do. Then leverage the heck out of its unique features once you move. By using oracle but staying within the standard you are paying a premium but don't use the premium features.
Can't tell if a troll or a really small bot script.
The challenge is that every day I write Oracle code, I am digging the whole deeper for getting to Aurora. So I'm being told to write something that works on both with as little rewrite as possible. Originally I thought this was a SQL standards topic, but now I see it as a lowest common denominator topic. If I were a lot smarter, I'd probably figure out how to write the code once and then have an interpreter convert it to platform specific SQL. 
Lowest common denominator, that's a very good way to put it. But you're right, it is a good idea to have some idea about what it is part of the standard and what is not. Then the question is whether Aurora implements the standard. :) For example, pl/sql is Oracle's invention. Mysql/Aurora have their own language for implementing stored procedures. I don't even think the standard says anything how stored procs should be implemented, just that it's a good idea to support them. But when it comes to implementing a language for it everybody is free to choose however they see fit. One thing you could do before you implement stuff the way you usually do is to search on how to do the same thing in mysql/aurora and see if there is a difference. Maybe there are ways that work in both environments. Sometimes there is. Sometimes there isn't. Good luck on your journey. :)
I asked the same thing in the /r/SQLServer group a few weeks ago. On github I kept finding peoples half finished (what I assume to be) college projects. My goal was to fix peoples SQL so it ran quicker, etc. I came across maybe one project that was interesting. I have a feeling freshmeat/sourceforge would have been easier to find these projects on than github.
True.
The poster has been updated with feedback from the previous versions. Thanks everyone who made suggestions.
One method using the wizard would be to script out the import to an SSIS and add the additional validation and deletion steps.
Thanks, I'll look into the SSIS method, but would that mean the Excel file name would need to be the same every time?
Thank you. I was not up to speed that DTSX had grown up. This is my next learning curve.
Hey, for more beginners, would it be redundant to include a self-join example on here as well? Just a thought. Awesome diagrams btw, very usesful for people like me who think more visually.
SSIS has a lot of flexibility. You could set the filename as a variable. If the filename is consistent (YourData_May_2015.xlsx), you could make the filename an expression which automatically changes based on the current date. Probably the best way is to use a For Each File Loop which loads any *.xlsx files from a specific folder. True temp tables (#TempExcelFile) are quite tricky to work with in SSIS, but it's possible. Permanent temp tables (dbo.TempExcelFile) are easier but can cause issues if multiple copies of the package run at the same time. If you want to work with #TempTables make sure to set your OLEDB connection to RetainSameConnection=true. Also, you will want to create a permanent (dbo.TempTable) version of your table when building your SSIS package. Then when everything is built, you go back into the properties and change the OpenRowset to #TempTable and ValidateExternalMetadata to false. Also, any delete or update steps which use the #TempTable have to use the same connection manager.
Sounds cool. I will check it out :)
Need not be the same. Let me know if you need any help with the SSIS package
I've always thought using Venn diagrams for representing joins is mathematically incorrect and the wrong approach, because the set circles for the tables sometimes cannot represent the full table. [Here's a good article](http://datamonkey.pro/blog/why_venn_diagram_is_a_bad_choice/) explaining some of the other reasons. I feel a much better and more accurate approach is to use [colour segments like in this diagram](http://m.imgur.com/2mlaF1M).
I've always landed on Venns being "good enough" to get the message across to beginners and worry about explaining the effective difference between join operations and set operations, as well as the fact that not all joins have to be equijoins, later on. Now I no longer have to accept that. Coloured segments are the new black!
Which DB?!!!!!!!
Look at ROLLUP or WITH ROLLUP depending on DBMS
&gt; about 600 tables, You poor bastard...
:/
You could use the LEFT operator and compare the leftmost n characters of the values to the first set of values. Or use the LIKE operator and the ? or * wildcards as needed.
Cast your datediffs to varchar(sufficient).
So... do you need help with SQL or Excel?
Then replace 'No-Activity' with NULL or some integer (-1)
 SELECT * FROM yourTable AS y LEFT JOIN (SELECT event_nbr FROM yourTable WHERE canceled = 1) AS s ON y.event_nbr = s.event_nbr WHERE s.event_nbr IS NULL 
and now compliance is 40%....argh
good idea- thanks
&gt; Also, this is a read only DB, so using temp vars/tables is impossible That's not true, at least with MS SQL Server. Even if you have read-only access to the database, you can use table variables and temp tables. But in this case, a temp table/table variable isn't needed as /u/Coldchaos points out.
Thanks. But this is for Informix, and not SQL Server.
I am getting my Masters in Cyber Security and I'm taking a Database Engineering and Analytic course right now. A lot of the sources listed are very good and will give you the general understanding of SQL. Once you understand the basics you'll want to learn GROUP BY functions (CUBE, GROUPING SETS, ROLLUP) and that is when SQL starts to get fun and advanced. Word of advice, make sure you familiarize yourself with RStudio (I like this better than R) for database analysis. This is an extremely helpful tool that'll benefit your knowledge of SQL. You can run sqldf (SQL data frames) commands, other hypothesis tests, ggplots, aggregate functions, and a lot more. I personally like RStudio better than MatLab but the programs are similar. It is important to remember that a well structured database is extremely beneficial for a company or organization, but how you analyse the data is where the real potential lies for advancing an organizations business intelligence. Last note: you should also look into NoSQL (non-relational databases) as well. This database style uses the BASE theory and are "schema less" and "eliminate" the use of JOINS. An organization will usually have both relational and non-relational databases and use them for different purposes. 
Set goes between update and join
So simple, thank you very much.
Also, your LEFT OUTER JOINs are actually performing INNER JOINs in this case. The filter on tables b and c requires the records exist (have been joined) in order to evaluate. Syntax wise it doesn't matter; the database will perform an INNER JOIN regardless of how it's written. It's just a pet peeve of mine.
NULLs?
&gt; It seems clunky to put all that in one table. clunkiness is way down the list of charcteristics for choosing proper table design one table is the answer &gt; I can't really use Radius/Length/Angle as primary keys because they are non-consecutive decimals. yes you can, the datatype of a column does not restrict its usage as a PK column
Not sure if appropriate for this subreddit but I'll post it since it's a funny store. We got a call from a customer about a problem they experienced and a lot of our analysis is done through SQL. As it so happens we needed to copy their database to our ftp server since restoring locally would have too big of an impact on production performance. So we send their system administrator the ftp details so he could copy it to there outside of working hours so we could restore and research the problem here. 2 days later we've received a package from them in the mail containing something that looked a lot like [this](http://images.bit-tech.net/content_images/2010/04/the-facts-4k-advanced-format-hard-disks/2.jpg). I don't think anything can ever trump that. 
&gt; I get 260,000 rows when I am expecting a thousand at most Another way of thinking about your statement is that it is a cross-join of 4 tables records where x=1234 in each table and from the Cartesian product you take only a.y =1 records. Try addressing it this way - chop off from that Cartesian product more things that you don't need.
Question, why use a derived table when you can do the left join with two predicates, the initial join and the cancelled = 1, then the same where clause? SELECT * FROM yourTable AS y LEFT JOIN yourTable AS s ON y.event_nbr = s.event_nbr canceled = 1 WHERE s.event_nbr IS NULL
guys, tell me please, the following service belongs to BDaaS model aihitdata.com ? thanks
I try not to give you the entire solution, but some info and a nudge... good job figuring it out.
Agreed on all points. Normalization care very much about repeating numeric datatypes. It's repeating strings that hurts you. The reason you might want configuration type separate is if you occasionally plan changing the weight on configuration "13" and have it affect all associated radii.
&gt; Are they duplicates? Can you use "group by" or "select distinct"? Or delete duplicates in Excel? a.x is the main key I need to match up against the other tables; it ends up with a one-to-many relationship on the other tables. To explain further- a.x is a key that is associated to a phone number. when people dial that phone number there are conditions that affect where the call is routed, so the output looks like: phone number 1 -&gt;rule 1-&gt;rule 2-&gt;transfer phone number 1 phone number 1 -&gt;rule 1 -&gt;transfer phone number 2 ... So I'm expecting a.x (the phone number key) to match up against multiple rows, but not as many as my actual output produces. My version of Sybase does not support the newer style of joins syntax; it's = for inner, = * for right, * = for left. I'm trying to get an inner to work, but it ends up being a Cartesian join instead. I did end up using Excel, but to me that's really an inelegant solution to what should be a straightforward SQL query. I'm just trying to figure out what I could have done differently. 
Why did it become a Cartesian join though? That's where I'm stuck in terms of syntax.
nice
If you have SQL Server installed, you can download AdventureWorks database. That is Microsofts teaching database and most of their examples involve the AdventureWorks tables. You could use BCP to export them to CSV files and play with them that way.
Oh great, I didn't know about this. Thanks!
What do you mean with "does nothing"? Was it created successfully? Just to make sure: Have you called the procedure after it was created? Your command only creates it, but does not excute it.
In my last job, I was primarily an SSRS/SSIS/SSAS developer, but I also used PowerBI for doing ad hoc analysis. I always thought of PowerBI (specifically, Power Pivot) as a way to do more in Excel. The biggest benefit is that it lets you set up a whole data model so you can answer questions at multiple levels without rewriting a SQL query to return data at a different grain. For example, I was once asked to look into departmental turnover numbers. Instead of writing a SQL query to get me aggregates at the department level, then rewriting it to get me aggregates at the manager level, and then again to get them at the job role level, I just created a localized data mart by writing a few queries. Using basic pivot table functionality, I could get those numbers just by dragging and dropping fields. I showed this to the executive who made the request, and he never asked me for it again, since he could always get the up-to-date numbers from this one file. Another advantage is that you get cube-like functionality to handle things like relative time functions (e.g. moving six-month average) with decent performance. It's definitely a power user tool. Lots of users will never create their own Power Pivot data models or use OLAP functions; when I observed Power Pivot users on the business side, they were typically just using it as a way to operate on more than 1 million rows. If you have SharePoint (and the right version of it), the integration lets you publish Power Pivot data models, which is a nice middle ground between self service and IT control. If you don't have the time or resources to set up an SSAS cube, or if you don't have a BI tool with decent metadata management, PowerBI can be a decent substitute.
Update: after reading this: http://www.sqlservercentral.com/Forums/Topic1440476-391-1.aspx I'm not so sure that SSIS is the way to go. Any comment?
another source for data http://lemire.me/blog/archives/2012/03/27/publicly-available-large-data-sets-for-database-research/
hm? i dont understand, could you give me an example or a more detalied answer.
Need a little more info. Are you using PowerPivot? Is this for a pivottable? Is there VBA code behind the scenes? There's quite a few ways to skin this cat...
just construct the right boundary date as you need it (and 8 quarters prior is 2 years prior date): eomonth( datefromparts( datepart( yy, getdate()) - case datepart( mm, getdate()) /3 when 0 then 1 else 0 end, case datepart( mm, getdate()) /3 when 0 then 12 when 1 then 3 when 2 then 6 when 3 then 9 end, 1 )) 
There are a few ways to do this but if you need to use a left join I would left join your matches table to stadium. If you did a regular join you're ONLY going to get stadiums that have had matches. If you left join you will connect the matches to the stadiums AND you will return stadiums that do not have a record in the match table. Then you just use your where statement to filter out anything but NULL match values. Then you'll have your stadium list that does not contain matches. Without seeing your tables or columns I can really only take a stab at how this would look for you but here it is: select * from stadiumTable s left join matchesTable m on s.uniqueID = m.stadiumID where matchID = null
if you can provide a data sample of table names, columns names, and like maybe 2 rows per table I can write the entire thing for you in several styles. hope this works for you
Try this: SELECT City, Count(SSN) as numberoffans FROM (Persons as P &amp;nbsp;JOIN favorites as F &amp;nbsp;&amp;nbsp;ON F.SSN = S.SSN) WHERE F.Artist = 'Pitbull' GROUP BY City ORDER BY numberoffans DESC;
If you get bored with AdventureWorks, there's also [Chinook](http://nugetmusthaves.com/Package/ChinookDatabase.SqlScripts) :)
&gt; For example, Get all the cities that Pitbull is listened in, and then sort them by number of people listening by each city. Is this to create some sort of bombing run? But seriously, an INNER JOIN, a COUNT(id) AS number_of_fans, and a GROUP BY author should do what you're looking for. Edit: Oh yeah, and an ORDER BY number_of_fans DESC.
[The secret cheatsheet, handed down from dev to dev over millenia.](http://www.codeproject.com/KB/database/Visual_SQL_Joins/Visual_SQL_JOINS_orig.jpg)
I am studying for 70-461 Querying cert, and that's what they recommend in the training book. :)
It's to count how many cities Pitbull has name dropped in his songs. ;) 
Because it makes it so damn easy to type longer queries. The one here is pretty short, but when you're looking at selecting multiple fields or columns from multiple tables, you get pretty damn tired of typing production.whatever over and over again. Popping it in as an alias just makes it quicker.
Yea, intellisense doesn't always give you what you want though, have to be careful with it :P.
So this is either a synchronization or messaging scenario? That's odd, but I can support that. What is the point of logging them to a table? Does that fire a business process or is it just for record keeping? If you say 'warehouse', it makes me thing these are scanned bills of lading or something like that. SSIS would not be the first tool I went for, but It can totally do that. This is going off-topic for the post, confirm or deny my assumption above and I will PM you my thoughts.
not sure i follow, sorry. Anyways, would the following work? (CAST( FLOOR( CAST(mytable.mycolumn AS FLOAT ) ) AS DATETIME) BETWEEN dateadd(qq, datediff(qq, 0, getdate()) -8, 0) AND DATEADD(qq, DATEDIFF(qq,0,getdate()), -1) ) 
The table is both our audit trail of every file to come into the business process(es) **and** a control point for future processes. My.file.Txt comes in, it becomes file84765957 and is logged as belonging to user12, for business partner 73, destined for appliction92, on this date, via this entry method. In other words, once file84765957 gets picked up by appliction92 on another server, downstream processes can access its original owner and other source details asynchronously. Hope that's not too vague. 
My boss is pretty damn strict about me getting my work done. Lax with most other stuff though :P
Yea, he didn't even want me using power architect to get a visualization of table interactions... 
| "take most recent" Translation: you are filtering/reducing your dataset. Some solution(s): conditions or subquery in where, inner join, group by with min/max | Does there exist [...] If yes, mark qrygroup6 = 'n' Translation: you are fetching and adding information to your dataset that is not otherwise present. Some solution(s): Subquery in select (calculated field) or outer join
a very rough example might go something like UPDATE sunset_training.dbo.ocrd SET sunset_training.dbo.ocrd.qrygroup6 = 'N' WHERE ocrd.cardcode IN (select x.cardcode from (select cardcode, max(docdate) as MostRecent from oinv group by cardcode having max(docdate) &gt; getdate()-180) as x inner join oinv on x.cardcode = oinv.cardcode where Datediff(day,oinv.docdate ,x.mostrecent) &gt; 0 and Datediff(day,oinv.docdate ,x.mostrecent) &lt;= 365) That's the gist of it, you'll have to fix aliases and syntax. The subquery will return a duplicate cardcode for each invoice within the 365 day period but that doesn't matter for what you're doing.
no trigger?? how can that be? oh, you wrote a function for that column GTFO with your "magic" 
Thank you so much, the only thing I did was put city instead of SSN in COUNT(SSN) --&gt; COUNT(City) And it works excellent. Virtual beer for you man :)
Good luck with that, I passed it last month and it was one of the toughest exams I've ever had to do.
These have few columns but a heck of a lot of rows, like 4.5 million per table.
No, &gt; where m.matchID = null Is the proper context. MatchID is NULL beacuse there is no JOIN for those records. Personally I write it where m.StatiumID IS NULL I find that cleaner since that's the column I'm joining on. If it's NULL (no JOIN) I know it's not in the table. You can not join a NULL value.
It will not work how you want. There are options though: X BETWEEN Time_Window_Start AND ISNULL(Time_WIndow_Stop, X) or X &gt;= Time_Window_Start AND ( X&lt;= Time_Window_End OR Time_Window_End IS NULL) I prefer the first one, myself. 
We have a system that does column-level access across its thousands of tables. That's the default security method. Last I heard, it was something like 30K security "controls" across the whole DB.
Awesome. so i could do: BETWEEN ISNULL(time_window_start, 00:00:00) AND ISNULL(time_window_end, 23:59:59) That would accomplish what I need. Thanks! 
Thanks! I can absolutely make this work. Much appreciated.
The joke for years is that we give Scott Adams ideas for his Dilbert comic strip.
the trouble with applying functions to your datetime columns is that the optimizer then cannot utilize an index what you're asking for is today's date be not between start and stop columns another way of saying this is that you want start to be greater than today's date, OR stop to be less than today's date you mentioned the time_window columns can have nulls... would both be null at the same time? if so, you want that job or not? also, can one of start or stop be null while the other one isn't? what would that imply about the job? 
well that makes a lot of sense right there. I think I'd still need the function though because I'm comparing current time to the times that may be present in time_window_start and time_window_end. Though I guess the time windows columns could be varchar instead of datetime.
For this purpose, it wouldn't make sense for one column to be populated without the other. So they'd either both have values or neither. If it's null then i am evaluating over the entire day, not just in the time window.
These guys cost, but they do offer a free trial for 10 days and the course is only a couple of hours long. http://www.pluralsight.com/courses/introduction-to-sql
I don't think you need a group by?? SELECT [NAME], [COLOR], [PLACE] WHERE [PLACE] = 'basket1' I may have missed something in your question though...
If he truly wants the first color for basket1 to be pulled.. then you're absolutely right. He's over-thinking it. I thought he just wanted the first color and location to be pulled (regardless of whether it was basket1).
Is the primary key being automatically assigned by the database? (such as an identity field) Post up your schema and the text file.
This is not an SQL problem. This is an Excel problem. The queries you present should work in SQL just fine, but I dunno how one would get them to show properly in Excel. SQL just does the data. Excel controls the flow. There is probably some sort of if/then/else logic you can plug into the cells in excel. http://www.reddit.com/r/excel They can help you.
What version of SQL and Excel? Also, do you happen to have permission to import the excel sheet into your SQL instance? It could be in a separate database.
Does anyone out there still use the old join syntax? I haven't seen it in the wild outside of the "don't do this" articles. *Edit: I guess I've just been lucky.*
I honestly didn't even know you could do a join in a where clause ... Then again, I've taught myself SQL so there are bound to be gaps.
Thank youuuu so much ... Such a simple solution lol
I think you're looking for [INSERT ... ON DUPLICATE KEY UPDATE](https://dev.mysql.com/doc/refman/5.7/en/insert-on-duplicate.html) for your first query. You should be able to do something like: INSERT INTO Words_Table (word, lemma, category, occurrenceCount) VALUES('{$input[0]}', '{$input[1]}', '{$input[2]}', 1) ON DUPLICATE KEY UPDATE occurrenceCount = occurrenceCount + 1; You'll need to have a unique key on (word, lemma, category) for it to work properly.
Interesting! That definitely shortens the query significantly. I'm using PDO, so I imagine I could do something like... $wordId = $connection-&gt;lastInsertId(); $connection-&gt;query("INSERT INTO Linking_Table (url_id, mot_id) VALUES($URLId, $wordId)"); Right after, right? Or would there be an even simpler way to do this?
My task is to keep the "SKILL_DESC" column exactly how it is, every single value showing. My problem is I need to get rid of the "NOT CERT" values along with their C_ID. When i do a WHERE clause for only 'CERTIFIED' values it get's rid of my "OKAY" and "UNKNOWN" values which I want to keep, just show them as null. Please help! I have spent way to much time on this :(
The other day I learned about "natural joins" and raged a little inside. They're where you don't specify which columns are equal in the two tables, the database just looks at the list of columns for each table and joins on the first two with the same name. **NO. NO. NO.**
where C_ID &lt;&gt; 'NOT CERT'
if you MUST have those values return as NULL then you could do: select blabla from blablabla where case when C_ID &lt;&gt; 'CERTIFIED' then NULL else 'Certified' end as 'Cert_Stat' and C_ID &lt;&gt; 'NOT CERT' 
Ahh that looks great, I was not familiar with the CASE command. Although when I try to use it I get an error with the "ILLEGAL USE OF KEYWORD AS". I've tried several ways. Am I doing something wrong? https://imgur.com/qgL9QBD Also thank you so much for helping out!
I see it all the time in my current job. I always use ansi standard. I've had more than one person that thought the old way was the only way to do them.
Yeah, it can be problematic and I've decided to never use one. An example would be: SELECT first_name, last_name FROM employee NATURAL JOIN leave_days; 
Yes. NULLs would indicate that I'd be looking at the whole day. I'm having some trouble with the results I'm getting. I'll elaborate tomorrow. 
Haha. I came here for help. Granted, I haven't been to class in months and I just started this tonight. 
We are getting closer!!! https://imgur.com/o1ZXey2 Lol that got rid of my "UNKNOWN" value and also completely nulled my "CERT_STAT"s where I only want the "NOT CERT"'s nulled. Thank you for helping by the way!!
Damn, good luck bro, I started on this Tuesday and have been cramming to get it done. lol
So you just want 'NOT CERT' to be NULLED? Change the CASE statement. CASE WHEN c.CERT_STAT = 'NOT CERT' THEN NULL ELSE 'CERTIFIED' END as CERT_STAT Edit: My condition was on c.C_ID not c.CERT_STAT on accident whoops! See earlier reply as well.
What unknown value are you talking about? Also if you want to keep the 'NOT CERT' values (but as nulls) you need to remove the WHERE clause that filters everything incorrectly as well. See first reply. It should have been `WHERE c.CERT_STAT &lt;&gt; 'NOT CERT'` anyways, but you do want to have those rows, just with NOT CERT as NULL. Edit: Ohh I see what you mean. Instead of the `FULL OUTER JOIN`s do `LEFT JOIN` in place of `FULL OUTER JOIN`s SELECT s.SKILL_DESC, cs.C_ID, CASE WHEN c.CERT_STAT = 'NOT CERT' THEN NULL WHEN c.CERT_STAT IS NULL THEN NULL ELSE 'CERTIFIED' END as CERT_STAT FROM SKILLS s LEFT JOIN ConsultantSkill cs ON s.SKILL_ID = cs.SKILL_ID LEFT JOIN Consultant c ON cs.C_ID = c.C_ID Edit 2: Fixed a couple things
Check the original post, you see it at the bottom as "UNKNOWN -------------- -----------" Why does it need to be there? I do not know. Why it is required? Is beyond me.
The script (lets call it your install_script) is separate to your program, and its only purpose is to run once to setup the database. Of course the install_script can be complex and do lots of things, but at a very simple level it creates the tables in SQL CREATE TABLE CORE_PEOPLE ( id NUMBER, name VARCHAR2(200), key VARCHAR2(200), value VARCHAR2(200), UPDATE_DATE DATE ); In your main program you would have a connect database function somewhere which points to this SQL database and then you can start populating it with data from your main program. 
I learned it on the job. It never occurred to me that comparing keys in the WHERE clause and doing a JOIN in the FROM were the same thing. Makes sense though.
&gt; JOIN leave_days ON employee.employee_id = leave.employee_id Even though it's just syntactic sugar, I think it's better form to use INNER JOIN instead of JOIN. And LEFT JOIN rather than LEFT OUTER JOIN. I think it strikes the right balance between terseness and verbosity and is more consistent.
I see it more commonly preferred to put the commas in front of the fields instead of at the end. It makes it easier to comment one or more out: SELECT employee_id , first_name , last_name , start_date FROM employee WHERE employee_type_id = 5; It's not vital for a small example like this, but it can help when you end up with more complex expressions you're trying to debug.
I've read his books and seen him talk and he's good. Definitely knows his stuff.
what you want is GROUP BY with COUNT(DISTINCT...)
Yeah I understand. I apologize for being confusing. So I have two tables: the first is written by a scheduling application and it holds the status of jobs it's running (I'll call this TableJ?); the second is a reference table that we use as a way to know which jobs we're interested in reporting on. (The second table I had referred to previously as TableA so I'll continue with that nomenclature.) I admit that this may not be the right way to go about this and I am certainly seeing its limitations here. So the way this is working now, if you want to have a report showing the status of a job, you make an entry in tableA with various info - job name, jobID, etc. One of the fields there is time_window_start, time_window_end. My intention here is that I have a job that I am only interested in its status between 12am and 1am. After that I don't care. So I would make an entry in the time_window_ fields. If I left them as NULL I wouldn't have a time window which would denote that I want to know the status for the entire day. Now I got some help using NULLIF() above, but I can't put that into production because what I am realizing is that if I am outside the time window for this job, I can't know if the job failed to run inside the time window. This is partly due to the query and partly due to how the scheduler runs. Each time this particular job runs, the row in the job table (TableJ) is overwritten because it is running the same instance. Again, sorry if this is confusing. I can provide more detail if you want, but I may just not have a way to report this.
I don't think this is the best way to get your result.. but it does work.... http://sqlfiddle.com/#!4/eb2cb/3 
I appreciate your help. I don't know if I can get the effect that I want to get out of it. Thank you though.
A Cartesian join will produce ... Total Number of Classes (for a student) = Total Number of Teachers * Total Number of Subjects. So this is probably the most efficient way : select STUDENT_ID, count(*) from CLASSES group by STUDENT_ID having count(*) = ((select count(*) from TEACHERS) * (select count(*) from SUBJECTS)) | STUDENT_ID | COUNT(*) | |-----------|----------| | 10002 | 6 | | 10001 | 6 |
Some of the older devs in my company do this. They even rebel against my use of ANSI-92. This is something I simply will not budge on, and if I'm given something to debug I will change that syntax in 99% of the cases. Leads to some awkward lunch-room encounters. 
I find this causes problems when I don't have my portrait monitor and am writing huge queries with lots of subqueries. I'll have to constantly scroll up and down the page to reference things. Still cleaner, but I think there's a time and a place for both. 
Maybe a combination of CDC and BIML packages? The CDC could be setup to tell you what happened in for instance the last day then you use the BIML packages to transfer over the CDC data. You will then have to write a SQL process on the other side to process through the CDC actions. Replication or log shipping may be a easier maintenance solution. 
Oh hell yes. Fucking christ. I am surrounded by people who do this. Thankfully I don't work WITH them, but I often see their code and it makes me cry.
that said, there is some handy shorthand, at least in PostgreSQL that is sort of like this but more specific and not random. USING(column_name) where two tables have the same column by the same name, you can specify this clause instead of ON(a.column_name = b.column_name) It also supports multiple columns: SELECT t1.uid, t1.eeid, t1.eename, t1.managername, t2.transaction_date, t2.transactionamount FROM t1 INNER JOIN t2 USING (uid,eeid) 
You can use decode for this: select s.skill_desc ,decode(c.cert_stat,'NOT CERT',NULL,CS.C_ID) as "C_ID" ,decode(c.cert_stat,'NOT CERT',NULL,C.CERT_STAT) as "CERT_STAT" FROM .... and just do your joins as shown in image.
One of the best writers on SQL ever.
Sounds like you are a bit confused. Imagine it like this. When you create the database as thats_how_you_cook says above its like creating a new folder in an old school file cabinet in this case the folder is called CORE_PEOPLE. It also defines the fields in the table for each record (Think fields on a paper form). So now you run the installer and the table is created in the SQL_LITE Server which you have to have already installed and configured. The table is there forever in theory unless you wipe it out. Your app just starts, connects to the DB and table and can find, insert, delete, records using SQL commands to update the table data. When you close the app the changes you made are in the table/database for next time you start it. As long as you execute the commands the table will update you dont need to "Save" the data per say.
Under 'Use Column Aliases', you didn't update your select statement to include the aliases: &gt;SELECT first_name, last_name, dept as "Department" Is this a mistake or by design?
How is it easier? Comma in front, can't comment out first line, comma at end, can't comment last line.
 select BACHNUMB, VENDORID from dbo.POP30300 where VENDORID in ('1000000015', '1000200696') group by BACHNUMB, VENDORID having count(VENDORID) &gt; 1 This is what I've tried so far. :/
Its not obvious what you're trying here but a couple of pointers: GROUP BY generally has to have all of the columns in the SELECT clause, except the aggregates like COUNT. HAVING clause comes after GROUP BY. you might want to use a CTE or a subselect.
Yea, just figured I would share - I don't use often / in "institutionalized" work, but its handy shorthand for adhoc work.
I also do something similar in my WHERE clauses too when I am testing out a lot of where logic or have specific test cases to run. I just throw a 1=1 at the top so don't have to comment out the WHERE clause when I comment out all my clauses. SELECT employee_id , first_name , last_name , start_date FROM employee WHERE 1=1 AND employee_type_id = 5 AND start_date &gt; getdate(); -- For testing purposes So in this case, I could comment out both where clauses and run the query without having to remove my WHERE. Of course, it isn't hard to comment out the WHERE either but I have found that when I am writing overly complex T-SQL, this helps in the debugging and unit testing phase.
I'm trying to select the column of batch IDs and also get a column of anywhere these two different vendor IDs share the same batch ID
Hey, I love Postgres, and don't get to use it nearly as much as I would like. Anything I can learn makes me happy :)
 With MyFullList as ( select BACHNUMB, VENDORID from dbo.POP30300 where VENDORID in ('1000000015', '1000200696') group by BACHNUMB, VENDORID ) select MyFullList.vendorid, COUNT(MyFullList.vendorid) from MyFullList where COUNT(MyFullList.vendorid) &gt; 1 I think this is what I would try
I may be misunderstanding what you're trying to achieve, but if not, I think this is kinda what you need: SELECT a.BachNumb, a.VendorID, b.VendorID FROM dbo.POP30300 a JOIN dbo.POP30300 b ON a.BachNumb = b.BachNumb AND a.VendorID &lt;&gt; b.VendorID WHERE a.VendorID IN ('1000000015', '1000200696') AND b.VendorID IN ('1000000015', '1000200696')
If I replace the left join with right join it excludes the hours where count = 0. Is there a way to do it without excluding those records?
try adding this to your existing statement where a.hour &lt;= datepart(hour,getdate()) 
Answer this first: Why are you using SQLCMD to do this? That being said, I believe you can utilize the **-b** and **-m-1** to better define the error reporting and output. **-b** returning 0 will be mean that it was successful. [SQLCMD Reference](https://msdn.microsoft.com/en-us/library/ms162773.aspx).
 where (a.Hour &gt; datepart(hour,getdate() and a.date = getdate()) or (a.date &lt; getdate())
Awesome, that's much better than the except statement I was using. Thanks for the help.
Why not give them CREATE/ALTER permissions instead? Is there a reason it's being done this way? On the surface this sounds like it's an unnecessary layer of complexity.
Thanks for the reply. I will try this out when I get back in front of the machine!
any foreign keys and/or triggers on the table?
I guess I'm still a little confused. Here's the code I wrote recently: import sqlite3 conn = sqlite3.connect("C:/code/py3/SQLitePractice/test2.db") cursor = conn.cursor() cursor.execute('''CREATE TABLE IF NOT EXISTS contacts (name text, id_num text)''') list = [('Taylor', '001'), ('Tory', '002'), ('Joe', '003'), ('Steve', '004')] cursor.executemany("INSERT INTO contacts VALUES (?,?)", list) conn.commit() for row in cursor.execute("SELECT rowid, * FROM contacts"): print (row) When I run this program it creates a database. But every subsequent time I run it, it keeps adding to that database. My programming friend said I was doing it all wrong. That I need too "write a script." Where do I write this script? Does it go in this file or in a new file that this file accesses? EDIT: Ok, after re reading your post, I'm thinking that I should make the SQL file *outside* of the python file. Then use the Python file to make changes to the SQL file. Does that sound right?
I like your CTE style lookup table, my instinct was a CASE statement.
I'm not sitting in front of my machine right now, but that looks solid! Thank-you very much.
Sweet. Also see http://www.joeconway.com/plr/ 
It's only $250 to open a case
Actually, I have an MSDN license and get 3 or 4 support incidents with my yearly renewal, however I've found that the amount of time I have to invest opening &amp; resolving a case far eclipses any actual cost involved. It can become a full time job of it's own, and on multiple occasions I have had the end result of 'we don't support that'. There's also [this](https://msdn.microsoft.com/en-ca/subscriptions/aa974230.aspx), which is a complete crock. Sure someone will answer your post, it'll be done by Microsoft Contingent Staff (i.e., non-Microsoft posters) who just funnel people to Connect or uservoice. Hence, the 'community has responded' and there's no need for a Microsoft Engineer to reply to fulfill the actual priority support - or give you a useful answer (like maybe 'we don't support that, so don't waste your time on a support incident'). Anyways, the cost I was referring to wasn't the support incident cost, rather the ridiculous license and SA fees we pay for the myriad of SQL Server installs we have (Prod/HA/Dev (MSDN)/BI). The fact that it feels unsupported after that is just salt in the wound. 
I noted that Dev was MSDN, mainly because it's where our support incidents come from. And yes, HA can be included when you have SA so long as that server remains 'passive', but then there's the snapshots for ETL loads and read-only replicas for reporting, offloading backups, etc. I guess I tend to group that into 'HA' as well, and those scenarios certainly aren't 'freebies'. At the end of the day, it's a very expensive product.
Data guy here. Machine learning, predictive modeling, general stats. Currently using R, SAS, Python, and several flavors of SQL including SQL Server, MySQL, and others. Your doubts are well founded. Microsoft has been talking a good game recently, and has even surprised me a few times with their efforts support multiple OS's (though let's be honest, their offerings for OS X are shit compared to what they are on Windows), but Microsft has done absolutely nothing to support R integration into SQL Server outside of the self contained Azure stacks. Sure there is the RODBC package, but right now binaries only exist for Windows (Microsoft has free drivers for SQL Server connections on Windows plus built in ODBC support). On a Unix based system? Blah. It's hell. I've been fighting with it for the past two weeks and burned dozens of man hours trying to get it connected with both open source and commercial drivers, but have yet to be successful. RJDBC is an option but the performance hit is massive. It would be incredibly easy for Microsoft to open source both a Unix driver and an R package that would allow Unix machines to connect to SQL Server, but they don't (and I can't imagine they will). So for that reason alone, I view Microsoft's constant drone about R integration as a load of crap. Microsoft is only supporting R inside of self contained Microsoft products. If Microsoft wanted to introduce SQL Server to more users in the data world, they would follow the lead of other industry players and open source the tools needed to connect to their databases easily and reliably. I guess I could always run my analysis on a Windows machine, (I use a VM for SAS) but then I'd lose a lot of my R parallelization code, all of the work it do in terminal (which has been integrated with Python so beautifully in things like Jupyter), and I'd have a pain in the ass time of working with the loads of data sitting on our Linux servers. So Tl;dr version is: Microsoft will only support your use THEIR R. No thanks. One more reason to continue our push to switch all current projects off of SQL Server onto other platforms and to not even consider it on future projects. 
http://lmgtfy.com/?q=Oracle+create+new+database
Are you looking to create a second database or just connect to an already existing second database? 
I was looking to create one and switch between them, but apparently we don't need to do anything with the first database so I resolved the problem by just deleting everything within this one and adding the tables for the new one. 
More information is needed. Linux or Windows? For Linux, set your env variables where TESTDB is the DB name: export ORACLE_SID=TESTDB export ORACLE_HOME=/u01/app/oracle/product/12.1.0/db_home export PATH=$PATH:$ORACLE_HOME/bin For Windows via command prompt: set ORACLE_SID=TESTDB then sqlplus and login To change your database, change the ORACLE_SID to something different. 
We do backups from the master servers with no problems
Thank you! It sounds terrifying, but a little bit exciting. Can the DBA career pathway lead to a Data Analyticsy/Data Sciency end goal, or will I just be falling deeper into the Data Management rabbit hole? I got a taste of normalising tables with Access and Surrogate Keys and all that good stuff. Any advice to get my head around the overwhelming size of SSMS?
Yeah, I got SSMS because it looked like the only sane way to communicate with SQL Server. I'm a 'Jump in at the deep end, drown, repeat' kinda guy. Procedures seem to work like Functions or Methods. I think I'll try and make a mini, mock database to emulate (read: brown nose) the company that were offering the job and just learn as I go. Thanks for all your help!
I don't think you're ready to be a DBA, even a Junior one (sorry!). Access is ok for end-users just pulling out numbers (similar to how office workers use Excel), but its considered nothing more than a novelty toy in the world of developers and enterprise databases. I'd really just focus on being a developer with SQL Server first, there is probably a LOT you don't yet understand about T-SQL, let alone having to act as DBA. I think you'd need at least 2 or 3 years of working with SQL before you could realistically move into a DBA role.
I'm curious, could you elaborate more on Visual Studio and how it's the "go to builder" for end users (sorry for jacking your post OP)? Short back story: I'm currently an Analyst, but I'm always looking to improve and find a way to help either automate a process or streamline the process. Our current problem is that out of a group of 100 people, only about 10% knows how to use SQL. What ends up happening is that ALL the technical resources are then forced to take on all the weight of every client, from basic data retrieval to some very complex queries. The other people only know how to run queries, but they can't even alter basic ones, such as inputting new data, even after we've commented it into the script (and in one instance, created input parameters). Ideally, we teach everyone SQL. Realistically...based on the people we've hired, I don't think that's going to happen. Ever. Do you think it'd be possible to build out an external tool for the end user? I've been sitting here looking at how we can make it easier for the end user to sift through tables, and generate information they want. My two ideas were 1) somehow requesting the company to get access (lol) or 2) using query designer in MS Server 2012 to teach them how tables are linked, and help generate their queries. I'd like to improve on my current skills too, so if you could push me in the right direction as to Visual Studio and MS SQL to create an external tool...that would be fucking amazing.
Do you mean I didn't apply the column aliases to all columns (e.g. first_name as "First Name")? Or that I didn't include the table aliases in the SELECT part (e.first_name)? 
You are asking for an "end user query tool" and, no, Visual Studio does not fit that need. VS is the go-to development environment for building well defined user interfaces, imho. For end-user ad-hoc querying I don't have any clear favorite. Sorry.
Do you know of any free ones that work with MS SQL Server? I need to find a way to get through the directors skulls that no, teaching everyone SQL is not the solution; getting an end user query tool is the optimal solution. 
Table aliases in the select. e.first_name
Hm, are you sure you've installed postgres and not postgres-xc? http://postgres-xc.sourceforge.net/docs/1_1/creating-cluster.html
I installed postgres-xc, but I've since removed it after following these instructions. http://askubuntu.com/questions/249531/postgres-xc-wont-uninstall-fails-on-stopping-co-ordinators For some reason right now the `postgres` command doesn't work (there's no binary for it in /usr/bin/), and when I try using `Postgres`, there's a message that says `postgres` is associated with postgres-xc.
I don't know the major differences between the versions - maybe I should look that up - but it's mainly because the HR lady told me that I needed to know SSRS. I can't afford the licensing to get the BI edition of any of the new ones, and I heard that the "newest" edition I can get SSRS for Express is 2008, so I went for 2008. Just had a bit of a Google around and noticed that I may be able to get all the things I need from SQL Server 2014, so...wish me luck!
I'm very new to the game, so my opinion might not count for much, at this point, but I believe you're looking for a data mart? From my research, a data mart seems to be an organised data warehouse with a pretty front-end where stakeholders can just ask the database questions and get answers, without having to worry about SQL.
&gt; I'm probably going to be shooting myself in the foot, but I'll make some kind of pet project to display what I can do and hope to learn the rest on the job This is a great idea, assuming you choose something suitably complex. If you're serious about becoming a DBA, I'd really recommend hanging out on the forums at http://www.sql-server-performance.com. I don't use SQL Server these days (MySQL) but I learnt maybe 40% of what I know from trawling this site's articles and forums (ended up being a forum mod I was that keen!) Other than simply working with SQL, the rest of it came from reading great technical books.... anything by Kalen Delaney is always good bet. For me it was "Inside Microsoft SQL Server 2000" which is kinda outdated now, but still very much worth a read, if you happen to see it cheap Her site is at http://sqlserverinternals.com/ and it seems she has a newer book out. I haven't read it but Id certainly start there if I was getting back into SQL Server There are also the Microsoft SQL Certifications you can do. Frankly these are very much a waste of time IMO (unless your employer wants them as a trophy). If you can find a site showing past papers, it can sometimes be useful to review them though, just to gauge your knowledge level
At the moment I'm fiddling with the AdventureWorks database. It seems complex enough, it has some stored procedures and other things for me to pick up on...I'll go through some tutes. Truthfully, though, I'm not serious about becoming a DBA. I want to do something closer to the business, but I'm willing to learn what I need to, to get there. Having said that, I don't expect to be as effective business facing if I'm triple nesting queries!
Also, they did ask for an MTA cert but for the kind of money they're offering, I'm not doing that. Even if they say yes to my limited SSRS exp, I'm going to be doing some negotiating on salary, so this is more of a foot in the water. Those are some really wonderful resources, though, I'll be looking through those throughout the day and I hope someone else finds them useful!
Quick question: How can I get something similar to the Relationships view from Access? I saw the Database design menu and I can make relationships, but I'm sure the sample database has its own relationships? Can I see the structure of the existing database in some neat way? I think it's called the Schema?
Skip right to 2014 if you can.
The following would happen: 2014-01-23 15:20:29.000 - Group 1 2014-01-23 15:21:20.000 - Group 1 2014-01-23 15:22:10.000 - Group 2 (because it is greater than 60s off the first instance). This sort of edge case is really causing problems when doing the JOIN T1.RowID -1 method. 
Unfortunately, I am. This is just to clean up data in an archive going back a couple years. Currently the data is being captured correctly but they are trying to get a script to go back and fix old stuff. Also unfortunately, this has to be done as a SQL script because they are only willing to run a script in production, otherwise, I would just drop out to LINQ and write a small program to clean it up.
If you select back in your table using the time column you can select the lowest time from there. I added some extra timestamps to get a better test result, tijd means times. Pardon my dutch. with tijd as( select cast('2014-01-23 15:20:29.000' as datetime) tijd union all select cast('2014-01-23 15:20:29.000' as datetime) tijd union all select cast('2014-01-23 15:20:29.000' as datetime) tijd union all select cast('2014-01-23 15:20:38.000' as datetime) tijd union all select cast('2014-01-23 15:20:38.000' as datetime) tijd union all select cast('2014-01-23 15:22:15.000' as datetime) tijd union all select cast('2014-01-23 15:22:15.000' as datetime) tijd union all select cast('2014-01-23 15:24:15.000' as datetime) tijd union all select cast('2014-01-23 17:30:15.000' as datetime) tijd union all select cast('2014-01-23 17:30:27.000' as datetime) tijd ) select tijd, dense_rank() over ( order by ( select min(t2.tijd) from tijd t2 where t1.tijd between t2.tijd and dateadd(ss,60,t2.tijd) ) ) from tijd t1 
Look at the windowed analytic function [**LAG**](http://www.postgresql.org/docs/8.4/static/functions-window.html).
Forgive me for being dense: But, the queries behind the scenes or working in/with the Report Builder specifically? (Or both)
That did it. Thank you very much.
am I right in saying you want to use the excel sheet as a data source? Would you not write a quick import dts (2005) to import the excel sheet as a table to use in your query?
why not just create a new column on the table that computes a pivot value based on the time to group by? you can drop bullshit seconds and crap with trimming this out (because above method totally drops feasibility when youhave like 10000 values you need to group over)... and just setting all second values to 0? this may add a little overhead and slow down the result, but fuck, beats manyually doing the above for each second that you cant group... 
Both really. The company already has a very large database with millions of entries however we have only just started to explore the possibilities of using reporting services to extract this data and produce usable data. 
What OS are you on?
That's expected. Without getting too far into it, the data returned is every combination of related data in all of the joined tables. Since there is another LLC2 row relating to the LLC record, the 26 related LLC4 rows are returned for the new record as well.
I have 3 other similar scripts. That was the only one that had the typo. Those also have the LEFT JOIN you suggested. I guess my question is that with the data presented is it expected to have the rows returned like that? 
"Is Null" not to be confused with "ISNull"
And you've done the purge directly through dkpg?
I used `apt-get remove --purge`
Yep. I don't understand why right joins are even taught. They only add confusion to beginners.
I tried finding the book I used but none of the covers I saw online looked familiar and it has been many years since I used it. Sorry I can't help :( If you have any specific questions on SSRS you can't find an answer to online feel free to PM me and I can help.
So how would I emulate a right join? For example, I used a right join to get information from Table 2 on matching fields that weren't NULL, so that the end table would only show the rows that had the data from Table 2 i wanted. I mean I guess you can use WHERE clauses to get that same information, though... idk. I'm confused now.
If that were the case, normally I'd just start with table 2 and left join to table 1.
the difference between left and right join is which table you write first. `a left join b` is the same thing as `b right join a`. the only reason they both exist is sometimes it's hard to get the second thing to be the thing you're working with efficiently, so being able to write either order can be important.
what you're asking for might be an `inner join` and might be a `right join`. it's hard to tell from the question. a `full outer join` will include everything unmatched from both sides. a `left` or a `right join` will include only unmatched things from one side. an `inner join` will include nothing unmatched. the way to tell whether you want a `right join` or an `inner join` is what you want to happen to things that aren't matched. in both cases, things unmatched in table 1 will be discarded, like you asked for. with a `right join`, things present in table 2 which are unmatched will be kept. with an `inner join`, they won't. so, suppose you're talking `owners foo join pets on owners.id = pets.ownerid`. a `right join` will include pets that don't have owners, such as the fish in the office. an `inner join` won't. both are "correct" in some sense; it depends on what you want in the answer. (the thing that's "always" being thrown away in that case is owners who don't have pets. in neither scenario will they be included.) if everything in table2 is always present in table1, both queries will have the same result.
SQLite also doesn't implement `outer join`. Or column types. Or `alter table`. Or a whole bunch of other important stuff.
Nice, I have always found the Khan Academy way of teaching to be quite effective. Looking forward to giving this one a try.
A full join is equivalent to a left join unioned (UNION not UNION ALL) with a right join.
really a shame that you're using mysql syntax instead of **sql** syntax
right so when there are a bunch of CTEs, derived columns, aliases and so on, this is going to 'magically' figure out all of the source columns??!! Obviously me and every other db dev has been doing it wrong all these years...
And? Only your first point is relevant here. 
Thanks for the response, please read my reply to /u/utexaspunk for more info on what I am trying to do. With that said, I have absolutely no idea what "Foreign Keys" and "Non-Clustered Indexes" are. Can you give me some details/links to read up on? Thanks.
Well, I'm assuming you'd have more than what was listed in the Item table, like the name of the item, perhaps the name of the image for the item, etc., assuming there are some attributes which are common to all instances of a particular item. Otherwise, yeah, I guess you could get rid of it. By join I mean you're going to query this doing something like SELECT P.Name, i.Description, inv.ItemLevel, inv.ItemDurability FROM Player p INNER JOIN Inventory inv ON inv.PlayerId = p.PlayerId INNER JOIN Item i ON i.ItemId = inv.ItemID Something like that, although within a game your UI is probably already going to have the player info, so your app would just be making a call to query from Inventory where PlayerID = *PlayerId*. Either way, an index on PlayerID in Inventory will make that query be negligible as far as the time it takes to fetch the records. People regularly do queries of tables with millions of rows that return in milliseconds.
The specifics/syntax may vary depending on the DB you're using (google "index mysql" or "foreign key postgres" or whatever) http://en.wikipedia.org/wiki/Foreign_key http://en.wikipedia.org/wiki/Database_index 
Currently working with mySQL via phpMyAdmin.
Nothing is wrong with it per se, however if it has specific functions/syntax, then the course won't be usable by people using other DBMS's. 
Is there a reason you are looking to log these? This seems like a fair amount of overhead for something that sounds like a process / permission problem. Most of these 'solutions' only look at the transactions that occur, when utilizing the FULL recovery model you can: DBCC LOG(your_database_name, output) For output you can set one of the following: * 0: Default - Operation / Context / Transaction ID. * 1: Same as default, but also includes flags / record lengths * 2: Same as #1, but also includes object name / index name / page ID / slot ID * 3: Full dump * 4: Same as #3, also includes a hex dump 
True but it takes less than 5 minutes for a person with an internet connection to find the equivalent function in another DBMS (in most cases).
I guess my point was "SQLite not implementing a thing doesn't really say much."
Yeah--but it does speak to how commonly a feature actually gets used, at least in transactional systems.
okay, let's try it in this specific case go to the link, don't go any further into the site, just look at that home page, and tell me where the biggest sql error is, and what the equivalent function in standard sql is 
&gt; DBCC LOG(your_database_name, output) Thanks, I tried all the options, but i don't really understand what I see there. I want to see the queries that have been run. And the reason is because I want to be able to know what other users working on SQL did in the database. I cannot restrict their permissions to do it.
Well I only know MySQL so I'm of no use...heh But I'd assume it's either the LIKE operator or more likely, the COUNT(*)?
Why not utilize something like [SQL Fiddle](http://sqlfiddle.com/)? 
nope
winner
The easiest way I can see, use an Insert Trigger. There are other ways, but they may or may not work depending on the format of the data. CREATE TRIGGER triggerName ON Devices FOR INSERT AS INSERT INTO Input (Name, Channel, Bundle, IsManuallyDisabled, DeviceId, IsAutoDisabled, CNTXDSPObjectID) SELECT name, 1, 0, 0, DeviceId, 0, 0 FROM inserted Edit - If you want to do it without a trigger, performed after the insert transaction has been committed: INSERT INTO Input (Name, Channel, Bundle, IsManuallyDisabled, DeviceId, IsAutoDisabled, CNTXDSPObjectID) SELECT d.name, 1, 0, 0, d.DeviceId, 0, 0 FROM Devices AS d LEFT JOIN Input AS i ON d.DeviceId = i.DeviceId WHERE i.DeviceId IS NULL Edit Note: You also shouldn't need to duplicate the name in Input since the DeviceId ties to that information in the other table already. Edit2 - The LEFT JOIN method also assumes you don't want to remove devices from Input that are still in Devices.
worst spam ever... reported
"the date of last entered for a column" can't be done, sorry, unless there is a "table recording the date of last entered for every column"
what you meant to say was status='new' OR status='closed' which can be simplified to status IN ('new','closed') except that this is ~not~ what OP is looking for
you most likely have a date-to-string conversion function for your platform, such as the datepart for MSSQL. You'd want to get year and month out of your date and group by these.
 SELECT fk_sales_order AS id_sales_order FROM sales_order_item GROUP BY fk_sales_order HAVING COUNT( CASE WHEN status IN ('new','closed') THEN 'humpty' ELSE NULL END ) = 2 AND COUNT( CASE WHEN status IN ('new','closed') THEN 'dumpty' ELSE NULL END ) = 0 
Thats what i was tryjng to clarify. I'm aware sql is a standard separate from any particular vendor. 
Im aware sql is a vendor independent standard. I was actually looking to clarify, as I know several peiple who consider ms sql the only "correct" sql. I could have worded that better. 
Not sure why a simple inner join and Where statement wouldn't work. SELECT so.id_sales_order, so_item.id_sales_order_item, so_item.status FROM sales_order as so &amp;nbsp;INNER JOIN sales_order_item as so_item &amp;nbsp;&amp;nbsp;on so.id_sales_order=so_item.fk_sales_order WHERE so_item.status IN (new,close) GROUP BY so.id_sales_order;
&gt; SELECT total_worker_time/execution_count AS [Avg CPU Time], &gt; SUBSTRING(st.text, (qs.statement_start_offset/2)+1, &gt; ((CASE qs.statement_end_offset &gt; WHEN -1 THEN DATALENGTH(st.text) &gt; ELSE qs.statement_end_offset &gt; END - qs.statement_start_offset)/2) + 1) AS statement_text &gt; FROM sys.dm_exec_query_stats AS qs &gt; CROSS APPLY sys.dm_exec_sql_text(qs.sql_handle) AS st &gt; ORDER BY total_worker_time/execution_count DESC; Coldchaos, I appreciate your attempt to help, but I really don't understand why are you going into the whole "why we need this". It has NOTHING to do with the question I asked (because I asked a technical solution) and with employees, . and as you can imagine every company is different, works differently and have different needs. Besides, a log has nothing to do with trust neither. Logs are everywhere and a basic necessity. Explaining you what and why would take a lot of time and is completely irrelevant, please let's try to focus on the query you provided instead.. It gives me a syntax error on line 8. Here: CROSS APPLY sys.dm_exec_sql_text(qs.sql_handle) AS st Thanks again!
I tried this but it returned 2790 rows that all look like this. yyyy_mm cnt 2014-11 1 What I would like to see is basically this October 2014 2745 November 2014 2752 ETC ETC ETC 
So like this? use system SELECT CONVERT(CHAR(7),closedate,126) AS yyyy_mm , COUNT(*) AS cnt FROM hstsupportincident WHERE closedate BETWEEN '2014-11-01' AND '2014-11-30' AND fkmoduleid IN (58,72,71) GROUP BY yyyy_mm When I did that it returns this Msg 207, Level 16, State 1, Line 7 Invalid column name 'yyyy_mm'. And if I put single quotes around it like this Group By 'yyyy_mm' It returns this Msg 164, Level 15, State 1, Line 2 Each GROUP BY expression must contain at least one column that is not an outer reference. 
u/emilyarden quit spamming this subreddit. Find a more viable way to market Mode because right now all you're going to do is reap negative sentiment for your business.
Why would you store both the name of the device and DeviceId in the Input table? I say, get rid of the (presumed) identity column on the Devices table and make the Name column your primary key. Then all you need to insert into your Input table is just the name. You can eliminate the DeviceId column altogether. P.S. Don't forget your foreign key constraints either!
Start here to get the basics down. The short exercises at the end of each chapter are helpful. http://www.amazon.com/Microsoft-Server-Fundamentals-Developer-Reference/dp/0735658145 Chapter 1-5 and 8 are the bare essentials. Prioritize those. A DBA should be familiar with 90% of what is in this book, but you need to start at these chapters first if you don't already know it. Most of your T-SQL will probably be from [dynamic management views](https://msdn.microsoft.com/en-us/library/ms188754.aspx), but you'll have a develop a script library of these to troubleshoot and diagnose issues with SQL Server. Aside from SQL, you should probably read this (and bookmark the site) http://www.brentozar.com/archive/2011/06/ozar-hierarchy-database-needs/ Also, come by /r/SQLServer
I started this last night and while I'm vaguely familiar with SQL I'm having to use it more and more with a recent SAP project. I see the comments about which version of SQL this is teaching, as long as I can get the basics I can google the rest when I have problems and learn from that route as well...along with several books. It was a good intro and that was it, a little frustrating when you didn't add something like the ; at the end and it just says error with no other help. 
What RDBMS are you using? Is there any chance that once you get the data cleaned up, you can ALTER` the column to be a numeric type so this doesn't happen again?
You could just pull it to a string, then go through character by character. If it's of a consistent format of letters/numbers you could use a regex. Are you using T-SQL, or something else like PHP, Perl, or Python?
Perfect, thank you very much. I'm curious as to what the 126 does?
&gt; Are you using T-SQL, or something else like PHP, Perl, or Python? I'm using T-SQL
Im not sure what you are trying, but i do hope your not trying to achieve a dynamic column name. If you just want to check if decution exists in table 2 try something like this below: SELECT t1.IDNumber ,t1.LastName ,coalesce(t2.deduction ,'N') as 401K ,coalesce(t3.deduction ,'N') as Insurance ,coalesce(t4.deduction ,'N') as VLTD FROM Table1 t1 left join table2 t2 on t2.IDNumber = t1.IDNumber and t2.deduction = '401K' left join table2 t3 on t3.IDNumber = t1.IDNumber and t3.deduction = 'Insurance' left join table2 t4 on t4.IDNumber = t1.IDNumber and t4.deduction = 'VLTD' A bit ugly tough, you could create a more clean situation with a pivot statement or if needed dynamic sql.
I think [this](http://stackoverflow.com/a/20972979) might do it?
126 is a style code... compare to other style codes, and to your requirements https://msdn.microsoft.com/en-us/library/ms187928.aspx
ahhh exactly what I was looking for. Thank you. I don't know why I couldn't find this on stack exchange. I looked for 20 damn minutes haha.
Ah, so your report is only going to be run on certain days of the month? 
correct. it will only generate data for the last two days of the month. 
This seems the most scalable. Imagining if there weren't just 3 options for deduction type but there actually 68 different types, you'd be best served here instead of join after join. Additionally, a pivot might work as well for this. 
Oh! That's simple. SELECT DISTINCT item WHERE salesperson = 'Jim' DISTINCT is the special sauce here - it strips out duplicates. Very handy.
This is the actual answer to OP's question. EDIT: unless op is using a different dbms. MySQL has GROUP_CONCAT for example. 
*MSSQL solution SELECT UniqueID ,STUFF((SELECT ', ' + CAST([Values] AS varchar) FROM TableA a1 WHERE a1.UniqueID = a2.UniqueID FOR XML PATH('')),1,2,'') [Values] FROM TableA a2 GROUP BY UniqueID
What database are you using?
I love all the folks just throwing out answers, assuming OP is using SQL Server, or MySQL.
My bad on the MSSQL assumption. I've labeled it as such. I threw it out there because I had very similar code already written. If anyone has a MySQL solution, I'd love to see it (regardless of the OP's platform).
My solution would probably be to use PHP :P But I'm sure you could use SQL/PSM (the MySQL version of stored procs)
Alright. So in T-SQL, you'd want to SELECT all the salespeople INTO an array, then for each salesperson, run the SELECT statement INTO a string, and print the string to the screen (or a file, or whatever). You could also do it in PHP, Python, Perl, etc.
Sorry, still building the sample database and writing further info. I needed to save the comment to make sure the formatting was readable.
Hey, this works! But exists and not exists are tricky to understand. Need to read more on this. Thanks. 
What is this query is actually doing? 
as /u/Darth_Vaporizer already mentioned, mysql has the GROUP_CONCAT function SELECT Salesman , GROUP_CONCAT(Items) AS all_items FROM AllItemsSoldJanToMay WHERE Salesman IN ('Tom','Dick','Harry') GROUP BY Salesman 
Hi laligood Thanks for the comments. I'm kinda old school, so new to with (even if it is old in computer years), but I'll check it out. Also, I talk about column aliases. I agree.
Let me see what's going on - thanks for the heads up.
The Group_Concat answers are great for your first proposed solution. If it turns out that doesn't work for your needs, and you would like something more like your second option, read through the [MSDN](https://technet.microsoft.com/en-us/library/ms177410%28v=sql.105%29.aspx) on pivoting. 
Agne! You know i can't help you but thanks for showing me this subreddit exists, mate. :)
haha, np
presumably by "between" you mean the new value is not actually equal to 1 or 5 (the min and max) SELECT n.newvalue FROM ( SELECT 1 AS newvalue UNION ALL SELECT 3 UNION ALL SELECT 4 UNION ALL SELECT 5 ) AS n CROSS JOIN ( SELECT MIN(key) AS minkey , MAX(key) AS maxkey FROM testkeys ) AS d WHERE n.newvalue &gt; d.minkey AND n.newvalue &lt; d.maxkey 
I mean a number 'x' can not come between any two or more occurences of a different number 'y'. I need a query to find if this condition is satisfied or not, and what vlaues violated the condition.
begin transaction; insert 1 insert 2 update 1 delete 1 update 2 delete 2 ... commit transaction; http://www.postgresql.org/docs/8.3/static/tutorial-transactions.html Probably something like this: http://docs.sequelizejs.com/en/1.7.0/docs/transactions/ (but I do perl development with postgresql, not js, so that might not quite be what you are looking for). 
Is this Transact SQL? Also, without knowing the database it's hard to tell what you could do differently. Maybe those master tables need some cleanup work. Maybe your indexes need work generally. I think looking at a query plan would be painful with something this complicated but something might jump out at you. Whenever I have to build something this complicated I chunk it up in temp tables. This will allow you to debug performance piecemeal. Take the inner most query, throw it in a temp table and see how it looks. Did it run quickly? Can you see ways to add aggregation at this level that you have in one of the outer queries? Also, with this smaller data set (assuming 20000 rows is not all the data available) that includes most of the elements you need to aggregate you can swap out all those outer queries and just reference the temp table repeatedly. I've seen queries hang on function calls so it would be a useful test to run without them. No doubt you need get those function results somewhere but at least you would know if it was a performance bottleneck and go from there. I like to use window functions and correlated subqueries to build aggregations at different levels. So you could have a total by item then a total by category x and finally a grand total for all rows all in the same result set. Sometimes it comes in handy. Generally there is always a better way to do something. Taking the time to rewrite queries is a great learning tool. But you have to get a job done so you cut whatever corners you can to make it "good enough". Good luck!
Updated above
Once we get around to installing BI, I was told that the data meant for it would be copied over to a different purpose-made database every so often (once a night?) As it is right now, the system is already heavily taxed by day to day affairs such as production launches and the sheer simple load on the server from many people using it at once. I'm not super-worried about the upcoming BI, as I've been told we could prepare and send out the data in any fashion we deemed fit. My expectation is that once the true BI is active, then the report I'm working on will become obsolete: it'll get replaced by hopefully an easier to use and/or more powerful version on the BI side.
The way that you have presented the data there is not a good definitive way to join a MonitorID = 2 set of records to its potentially existing related set of MonitorID = 1 records. Sure, you could look at StartTime as they would occur later, but you would need logic to determine whether the later MonitorID = 1 event was the really tied to that MonitorID = 2 event or a later one (which gets really messy). Can you give more realistic data for the Comment column? Is it really random junk, or is it useful data for filtering and joining?
Then you need to know the min/max timestamp for a Key and then find all the rows that have the wrong key. Try this ( http://sqlfiddle.com/#!6/9778c/9 ): WITH minKey as (select keys, min(times) tm from test group by keys) ,maxKey as (select keys, max(times) tm from test group by keys) Select t.keys ,t.times From test t where exists (select 1 from minKey nk where nk.keys &gt; t.keys and nk.tm &lt; t.times) or exists (select 1 from maxKey xk where xk.keys &lt; t.keys and xk.tm &gt; t.times)
Thanks, I'll check it tomorrow.
Some specific pointers: * I see that you have to go through the assemblies a lot. Some of these are via function calls (e.g. dbo.GetAssemblyInstructionValueByLineItemID) and others are via scalar sub-query (e.g. HasSpecialOrderOption). I suspect that all of these calls are super fast for a single product, but slow for a lot of products because it has to go through and compute the same assembly information tons of times. Instead, look at the recursive capability of common table expressions - you can compute the results for one record and then combine that output with another line item (input). Alternatively, if assemblies are relatively static (i.e. don't change multiple times a day every day), just create a materialized view of the product that includes the assembly-based columns. * For function calls, each call might be super fast, but even 10 ms per call can add up very quickly. See if you can query the answer instead - a view that can be used by the function and by your query is ideal. Otherwise, the duplication may be worth it for performance and you'll just need to comment the function and report query so that maintainers know updates need to occur in both places. * For scalar subqueries, sometimes its better to compute the result for every record and join it in. For example, vehicle description - can you calculate the AID for all line items and then join it in via the main query? * Why oh why isn't the product codes table outer joined in?
The alert/upsell, you could pass this back on the client side, and use a cookie with a random expire time. Is the 1-page questionnaire a one-time thing, or does it matter how often it is entered? If one-time you could put it in a table, otherwise a cookie could do the same thing and you just push it onto the web layer. 
&gt;I see that you have to go through the assemblies a lot. Some of these are via function calls [...] Each product has its own specific assembly; it's part of its unique work order. It's generated only once, when we launch the item into production. I do feel that it's one of the prime suspects into what's bogging the query down, since it's got to run outside the main query's scope to grab all those extra bits. The killer is that each work order has multiple assemblies - one for each major component of the product, so we're looking at between 10 and 20 assemblies per item. Fortunately in most cases I only need to grab the root one and I'm good to go. &gt;For scalar subqueries, sometimes its better to compute the result for every record and join it in. For example, vehicle description - can you calculate the AID for all line items and then join it in via the main query? Yes, I could do that. The AID I'm looking for has a one-to-one relationship with my line item. In other words, I think what you're saying is to do the whole VehicleDesc column processing in one go table-style, rather than function-calling it once per result row. Is that correct? &gt;Why oh why isn't the product codes table outer joined in? Truthfully, I hadn't noticed that part at all! It's only now that I look at it again, that I realize the product codes thing feature-crept on me as I went along. In other queries, I only had to refer to that once or twice, and thought little of it. If not for the query speed, at least joining on the table will simplify the execution plan by a mile... I'll refactor that morsel later today. Good catch!
Something I was curious about, and I didn't find any 'definite' info on guides about it: is a WITH clause functionally the same as what I'm doing now? In other words: Is this... SELECT things FROM (SELECT things FROM aTable) AS MyInnerQuery The same as this: WITH MyInnerQuery (things) AS (SELECT things FROM aTable) SELECT things FROM MyInnerQuery **NOTE TO SELF:** Materialized view is known as 'indexed view' in Microsoft SQL. I'll take a look at that avenue later.
Thats what happens with analytics - they keep wanting more! Once you have an El Cheapo in place, you'll find new uses for it too.
looking hard for relevance to SQL... trying hard to resist urge to report you for spamming
I love this team needs is a man first.
Dunno found it quite interesting. http://www.gurufoo.com/index.xhtml?cat=DB
Yeah, that's what I figured. I'm still new to SQL so I was hoping I was missing something. Currently I have it grouped by name PER HOUR. This is CLOSE but still missing some when we get multiple drops in 1 hour. I'll check out those techniques now. Thanks a lot for the help.
Thanks! The site is semi-curated, in that our staff actively posts informative articles that we find (and some create their own on topics that are under-represented). There are several SQL articles, but it is one of the categories that can always use more!
I do not see a lot of right joins in use. I think people tend to organize their queries so only an inner join or a left join is needed. Because of that, I would have tried: SELECT * FROM A INNER JOIN B ON B.id = A.id LEFT JOIN C ON C.id = A.id WHERE B.id = X
My thoughts exactly when these take place and at least some hope.
Assuming you are using a version of MS SQL that supports GROUPING SETS, is this close to what you are looking for? SELECT Brand ,Product ,COUNT(*) [Total Orders] ,COUNT(CASE WHEN [Source] = 'SEO' THEN 1 END) SEO ,COUNT(CASE WHEN [Source] = 'Direct' THEN 1 END) [Direct Nav] ,COUNT(CASE WHEN [Source] IN ('Search','Content') THEN 1 END) [SEM/CPC] FROM Table GROUP BY GROUPING SETS ((Brand,Product) ,(Brand) ,()) ORDER BY CASE WHEN Brand IS NULL THEN 1 ELSE 0 END ,Brand ,CASE WHEN Product IS NULL THEN 1 ELSE 0 END ,Product
Thank you I would highly recommend you get a HD alternate angle on retirement.
I don't understand.
Tenho uma prima que Ã© um problema.
 WITH cte AS ( SELECT ID ,[Key] ,LEAD([Key],1) OVER (ORDER BY id) AS [Next] FROM TableD ) SELECT * FROM cte WHERE [Key] &gt; [Next]
Powershell.
Awesome thanks! I'll try this first thing in the morning. 
 Powershell can help take query automation to the next level. It gives you access to SQL with the power of .net. 
&gt; having count(distinct(zip) ) &gt; 4 After group by.
You cant add a WHERE clause, because WHERE filters the result set before the GROUP BY is applied. So one row will never have count(distinct(ZIP)) &gt; 4, since for that row it will always = 1. Easiest way (not always the most performant though) is to add a HAVING clause, which is a way of filtering the result set after the GROUP BY select STATE as STATE, count(distinct(ZIP)) as ZIPCODES from ZIPCODE group by STATE HAVING count(distinct(ZIP)) &gt; 4 order by count(distinct(ZIP)) DESC; 
Original [article](http://www.javacodegeeks.com/2015/05/how-sql-group-by-should-have-been-designed-like-neo4js-implicit-group-by.html) which this one is attempting to rebut.
The code is portable to ETLs and linked servers and scalable for future security upgrades and data growth. I always call out dbname and schema, personally. It only takes a second.
To add to above its good to call out explicitly as some of features in MS SQL like Indexed view requires you to use the two part names.
you can start with SSRS which is Sql Server Reporting Services this comes as a standard with MS SQL and gives you an option to install when you are installing MS SQL, if its 2005 or later. You can also connect Access to SSRS. Refer to this link its a good starting guide http://www.codeproject.com/Articles/10377/Integrating-Microsoft-Access-DB-SQL-Reporting-Serv
It can't hurt but it can also be time consuming. To make the process quicker use table aliases. &gt;SELECT S.Sales, P.Product, P.Price FROM Sales AS S INNER JOIN Product AS P on S.UID = P.UID
Got to thinking about this, is there a performance difference by chance? 
When you don't have more than one column with the same name, it's not strictly needed, but in my opinion having the schema or table names before the column name makes the code so much easier to read. For my job, I've written statements that exceed 100 lines of code and join numerous tables. If I have to come back to it a few months later, it will be that much easier to remind myself how the query works. Do the future /u/Ihavenowand a favor and be verbose with your schema and table names.
CRM is simply a generic description of a type of application/service: Customer Relationship Management. To answer your question, you need to know *which* CRM you will be wanting to integrate with SQL Server. Keeping in mind, that some CRM's run on SQL Server natively. The big names in the CRM world are Salesforce, SAP, Oracle, and Microsoft Dynamics. There are hundreds of smaller companies making CRM software/products for nearly every industry. Also, "Integrating with SQL Server" is not really the point of an integration. Transferring data between Database systems (such as Microsoft SQL Server and MySQL, Oracle, PervasiveSQL, etc.) is easy and there are tools that come with SQL Server to help that (SSIS!). What you want probably is to integrate the "applications" that use the SQL Server for data storage. A good example is integrating Salesforce (CRM) with an Accounting package (ERP). Yes, SQL Server is the man-in-the-middle that stores the data and makes it all happen, but the logic and requirements for the integration come from the application, not just the database server.
I'm messing with this now and it's a terrific idea, but it seems like I can only have one sql table open at a time, how can I join two different tables together using my current connection? The pivot table only lets me select columns from the one table I selected
I assume you've joined the tables together already using a query; if you actually click what you have, under the data tab, you'll see connections; click connections. Select what you just created, go to properties. After that, there's a definition tab. If you go to "command type", you can select SQL. Command text, just copy and paste your SQL text, and you'll have the data. To this day...I never understood why I can't just connect, not select a table OR a view, and just post my SQL code. 
that is crazy, as a beginner it definitely threw me off, but I appreciate that tip for sure. Thanks!
not sure how it works on a mac (apparently you need wine, lol) but i've used and loved [heidisql](http://www.heidisql.com/) for years
&gt; apparently you need wine, lol I always need wine, haha. I actually didn't know WINE existed until I started digging into this question. I'll look into heidisql for sure. Thanks!
I struggle with the group by function a lot when I know it's really straight forward. Are there any recommended sites for learning it properly as at the moment it's just trial and error.
There is no performance difference there might be gain in performance if you fully qualify it as it does not need to do an internal look up but its going to be very very less I would say negligible.
Oh wow I really like the design of this. I'm definitely going to check this one out. Thanks!
if you select a lot of data it could lock it up against users and apps trying to change it at the same time
Actually, if you users have been created in the DB after you've migrated to SQL Server 2005 or later, their 'default_schema' will be set to 'dbo', circumventing the alleged name resolution performance issue.
From a dba's pov i would be concerned with them seeing things they shouldn't. Also, the ability to do tons of crazy joins all with select * could impact performance.
Use (NOLOCK) on SQL Server. 
Oh probably. But nothing you couldn't fix. The above mentioned "you could lock up the server" is true, but if that happens, just stop running the query :P
The way I see it is: It's specificity overkill, but as a database guy, specificity overkill is my stock in trade :P I always use them.
You might want to have a look at dB visualizer.
I only know mysql, with that you can give permission to specific tables only, but you can't specify specific rows.
I'll reiterate what sandgraal says. I've seen some filthy queries that completely slow down the operation of the age because they consume: -cpu -memory - I/O speeds The data they is returned from a query has to be stored in a temporary location (mostly the tempdb, a system database) and therefore there is a bulk copy-paste operation of sorts going on. The SQL query execution plan denotes how large the data is. I'd look up SQL query execution plans and how they work. This is also highly dependent on the data itself, which means that if you're touching a table on a database that isn't being accessed by the normal users (like application users) since it lives on an entirely separate database then it likely won't hinder your queries. All of this applies to tables with large tables of many million rows. If you're taking hundreds or thousands then it may not be an issue. Hope this helps 
or if its scanning the whole table anyways
 No. It is possible to how read-only locks on the database when others are attempting to update it. Which could in theory cause some problems.
Dynamics runs on SQL Server already. So you shouldn't need any intermediate step. I am not that familiar with R, but in general when approaching any system integration: 1. Determine the data format/specification that R needs to accept data 2. Develop an Extraction process (using an ETL most likely, such as SSIS or Pentaho, Redgate tools, etc.) to extract the data from the SQL Server database that MS Dynamics is using. A CSV is probably a good first step. 3. Utilize whatever import/data access methods R has to read the extracted data (that CSV file from #2 above, for example)
I have seen poorly written queries lock up a SQL server many many many times. If you do not know what you are doing you can bring a SQL server to its knees. You have to make sure your queries use indexes properly and no locks and intelligent joins. You also have to make sure you are not pulling back a butt load of unneeded data. If you know what you are doing, any data in SQL is your bitch to do with as you please, but if you dont know your way around the park...... she will make you pay.
I don't know if people are looking for SQL Certifications, but it might be an excellent goal to set for yourself to learn more. I didn't get Java certified, but the one week I spent studying forced me to learn generics a little better. Hope this helps.
If the money is the only thing determining whether you'll do it, I'd say yes. If you want to do something different, the cert + your current education makes you a great looking candidate for an analysis job. And sometimes you need better ways to get/present information than Excel, SQL/SSRS does that. How useful will vary from company to company, but just based on the doors it will open I'd say go for it. 
Take a look at the order of operations too. The FROM line is run first. I tend to write that right away so that intellisense picks up my laziness. 
There are a lot of comments about performance, but you will have to be very careful about what data you're using and how you're using it. Depending on how your BI system is set up, innocent looking queries could paint the wrong picture. Different tables related to different business processes will likely have different grain or definitions which can skew your metrics or provide false data and degrade performance. You should probably consult with your data team for some training or documentation and ping them when you have questions about it (it's best to ask someone than to act on incorrect data).
This video could help: https://youtu.be/IEYrNg3MpZg
I'm a Senior BI Developer and Data Modeler for a major telecom. I have over ten years experience with SQL, in progressively more responsible roles. I've also been a hiring manager for previous employers. I don't have any certs and I've never seen a hiring manager take them seriously. I certainly never did. I've worked with guys in the past who had them and we both earned the same salary. I don't know if any of my current coworkers have them. I'm not trying to insult the merits of a good education but employers don't care about them nearly as much as experience and abilities. 
OPs request is reasonable within a view. I would ask if they have a reporting clone. Data can go stale on a reporting clone, but at least locks are less of an issue
This is neat, but one thing to keep in mind is this significantly breaks code portability if you need to move the logic to another RDBMS. If you know you won't ever need to run it on another rdbms, then you should be fine.
If you're using MSSQL, you can use the CONTAINS function. You would order the logic flow of your tests by using parentheses: WHERE ( CONTAINS(word, "a") OR CONTAINS(word, "p") ) AND ( CONTAINS(word, "l") OR CONTAINS(word, "x") ) AND ( CONTAINS(word, "e") OR CONTAINS(word, "c") ) If you can't use CONTAINS, then you can substitute the LIKE function as such: (word LIKE '%a%' OR word LIKE '%p%') The % is a wildcard, so you're basically using it to search for any string that contains the letter a or p. CONTAINS is preferable since it should be computationally quicker, it can take advantage of an index if one exists. The LIKE statement in this usage can't. Other versions of SQL may or may not support CONTAINS.
Whoa, this is perfect. &gt; If you're using MSSQL So I don't know much about setting up SQL, but I have learned a bit about to ask it for stuff. So I am aware of SQL servers BUT I am assuming I can run one locally and query that? What should I Google to get that going? Also where does one find a dictionary DB? Tons of links in the /r/SQL wiki, anything in particular I should look into in order to load a DB into a SQL (program?) and then using the advice you gave me?
I think you want the LIKE operator, it's standard SQL. % is the wildcard here. SELECT words.word FROM dictionary.words WHERE word (LIKE '%a%' OR LIKE '%p%') AND (LIKE '%l%' OR LIKE '%x%') AND (LIKE '%e%' OR LIKE '%c%') ORDER BY LENGTH(words.word) DESC; Somethin' like that. If you're on SQL Server instead of MySQL/Oracle/PostgreSQL, it's LEN instead of LENGTH. That'll match axe, apple, and people, but not ape, because ape doesn't contain an l or an x. Am I interpreting your requirements right?
The dominant RDBMSes (Relational Database Management Systems) are Oracle, SQL Server, and MySQL. You use SQL as a programming language to issue commands to the database, which could be to do things like create tables, alter the structure of tables that already exist, insert data into tables, or update already existing data in tables. Or likely the most common use, select and return all the data that matches your query. [Here's a good intro to SQL tutorial.](http://sql.learncodethehardway.org/book/)
I feel like SQL just needs a TOP N WITHIN GROUP or LIMIT N WITHIN GROUP expression. I feel like it's confusing semantics to overload DISTINCT like this, because it's really not what's going on. Personally, I would never do this: Select s.username ,s.email ,m.message From ( Select u.username ,u.email ,Max(m.message_time) As last_message_time From users u Join logins l On l.username = u.username Join messages m On m.username = u.username Group By u.username, u.email ) s Join messages m on m.username = s.username and m.message_time = s.last_message_time I'd write it like this: with CTE as ( Select u.username ,u.email ,m.message ,row_number() over (partition by u.username, u.email order by m.message_time desc) rnum From users u Join logins l On l.username = u.username Join messages m On m.username = u.username ) Select username ,email ,message from CTE where rnum = 1 Which I expect is much better.
Shoot. I have never had to setup a DB before I just used SQL to pull info from one. So in a typical situation like mine, would the process look similar to this: * Create the table using SQL commands. * Look through and understand how the data is layed out in a DB(?) file. * Match the DB(?) file's layout with my SQL table's layout * Some how load it all.
I'm not sure I'll be able to help you, but /r/learnSQL might be able to?
actually, what created this mess is the FK that was never declared
I'm not going to do your work for you, you won't learn anything, but I'll help you. The first part is something like this... create or replace function EmployeesSalaryTotal(pDkod DOLGOZO.DKOD%TYPE) return DOLGOZO.FIZETES%TYPE is vResult DOLGOZO.FIZETES%TYPE; begin select sum(FIZETES) into vResult from DOLGOZO where FONOKE = pDkod; return vResult; -- This could be null if someone has no employees, so may want to return nvl(vResult, 0) end; Even rows can be found by using ROWNUM and mod. You have to order first and then select from that, you can't use ROWNUM with order by in the same block as the ordering affects the ROWNUM. select * from ( select * from DOLGOZO order by DNEV) d where mod(d.ROWNUM, 2) = 0 To find how many employees each person has, you'll have to use count(*) and group on FANOKE select FANOKE, count(*) from DOLGOZO group by FANOKE You'll have to join this back to the even number row query, on DKOD = FANOKE, you'll have to use an outer join as not everyone has employees. When you update, make sure your update statements have a where clause, so that you are only updating based on DKOD.
This is great help, thanks ever so much. I posted it on stackoverflow too, no answers, and from my school group too, no answers. but finally, somebody with some info I can possibly use to figure this out!
Allow me to disagree. You will hit this issue whenever you want to map a nontrivial relational schema to a new object model. 
I've noticed application developers seem to create unnecessary cursors
10 Common Mistakes Java Developers Make when Writing SQL, the Revised List: 1. Used Java 2. Used Java 3. Used Java 4. Used Java 5. Used Java 6. Used Java 7. Used Java 8. Used Java 9. Used Java 10. Used Java
"Locking up the server" can mean minutes or hours of lost productivity for the people who are making money for the company with that system. It isn't something to be flippant about. Writing good queries is **everyone's** responsibility, and if you have access to the database and *aren't* doing that, your access should be taken away until you can get some training on it.
Oh yeah, there are definitely consequences, but nothing to lose sleep over. I write good queries, but occasionally I mess up too, it happens. I've done some awful things to databases, but that's just a consequence of working with them long enough often enough. And technically, good queries shouldn't fail when run against a locked-up database :P
You have my condolences. I was giving OP the benefit of the doubt, because they were nice enough to ask beforehand, and folks like that rarely (in my experience) screw things up too badly. But fuck the people who did that shit time and time again to you.
Haha scarred
LOL
OH god, I just read that list and #10 should be repeated for #1,2 &amp;3! I supported a retail chain that used a java based POS with a MSSQL backend. The software was OK but SLOW AS FUCKING MOLASSES IN A CANADIAN WINTER. After some troubleshooting I found each and every record was inserted one at a time, and when it came to reports it would also query 1 record at a time. I wrote a report in SSRS that analyzed annual sales and it took ~ 30-90 seconds to run depending on the categories/parameters, but well over 30 minutes to run in the app.
Well we are kind of a small branch of a transport companie so we have very little support from the main house, and since each costumer has its own pricing table adjusted to its needs we tend to have alot of different files from diferent eras. Some of them are scans of A4 sheets lol.
Thanks - I'm not so great with PL/SQL so any help is appreciated.
http://stackoverflow.com/questions/3701233/oracle-drop-table-constraints-without-dropping-tables
I think you've got 2 culprits here: * False assumption that First + Last name is unique. Each instance of this will produce "N" squared - "N" extra records - i.e. if there are 3 people with the same first/last name, it will produce 6 extra records (9 total, up to 3 of which may be legitimate). * Fan trap due to some people having multiple addresses on on either contact table. A fan trap is essentially a cartesian join that occurs due to resolving 2 many to one relationships to the same one record. For example, if one table tells you that Jon has 3 cats and another table tells you Jon has 4 dogs, if you join the two on Jon you'll end up with 12 records, one for each pairing of cat and dog that Jon has. I recommend using "union all" to add the records from the 2nd contact list to the first contact list and then doing aggregation to identify "duplicates". Below will get you started. By looking at the sorted results, I think you'll find things trickier than first impression. For example, the address might be reported as "Rd" on one contact and "Road" on another or a person's name might be "Richard" in one place and "Rick" in another. Good luck! SELECT COUNT( CONTACT_A ) A_CNT, COUNT( CONTACT_B ) B_CNT, FIRSTNAME, LASTNAME, CITY FROM ( SELECT 1 CONTACT_A, NULL CONTACT_B, a.FIRSTNAME, a.LASTNAME, b.CITY FROM CONTACT a, ADDRESS b WHERE a.contactID = b.entityID UNION ALL SELECT NULL CONTACT_A, 1 CONTACT_B c.FIRSTNAME, c.LASTNAME, c.CITY FROM CONTACT2 c ) GROUP BY FIRSTNAME, LASTNAME, CITY ORDER BY FIRSTNAME, LASTNAME, CITY;
If you want to drop the table and constraints drop table MY_TABLE cascade constraints / Info about which tables reference your table.... select OWNER, CONSTRAINT_NAME, CONSTRAINT_TYPE, TABLE_NAME, R_OWNER, R_CONSTRAINT_NAME from ALL_CONSTRAINTS where CONSTRAINT_TYPE='R' and R_CONSTRAINT_NAME in (select CONSTRAINT_NAME from ALL_CONSTRAINTS where CONSTRAINT_TYPE IN ('P', 'U') and TABLE_NAME = 'MY_TABLE') Script for generating code to disable constraints... select 'alter table '||A.OWNER||'.'||A.TABLE_NAME|| ' disable constraint '||A.CONSTRAINT_NAME||';' from ALL_CONSTRAINTS A join ALL_CONSTRAINTS B on B.OWNER = A.R_OWNER and B.TABLE_NAME = 'MY_TABLE' and B.CONSTRAINT_NAME = A.R_CONSTRAINT_NAME where A.CONSTRAINT_TYPE = 'R' 
The question seems based on a misunderstanding. This would be akin to asking which programming language a text editor uses. Each database engine uses its own SQL dialect and communication protocol. Cross platform database management tools simply connect via the appropriate protocol and send text commands to the engine. See http://en.wikipedia.org/wiki/SQL#Procedural_extensions for a list of common dialects.
#####&amp;#009; ######&amp;#009; ####&amp;#009; Section 19. [**Procedural extensions**](https://en.wikipedia.org/wiki/SQL#Procedural_extensions) of article [**SQL**](https://en.wikipedia.org/wiki/SQL): [](#sfw) --- &gt; &gt;SQL is designed for a specific purpose: to query [data](https://en.wikipedia.org/wiki/Data) contained in a [relational database](https://en.wikipedia.org/wiki/Relational_database). SQL is a [set](https://en.wikipedia.org/wiki/Set_(computer_science\))-based, [declarative](https://en.wikipedia.org/wiki/Declarative_programming) query language, not an [imperative language](https://en.wikipedia.org/wiki/Imperative_programming) like [C](https://en.wikipedia.org/wiki/C_(programming_language\)) or [BASIC](https://en.wikipedia.org/wiki/BASIC_programming_language). However, extensions to Standard SQL add [procedural programming language](https://en.wikipedia.org/wiki/Procedural_programming_language) functionality, such as control-of-flow constructs. These include: &gt;In addition to the standard SQL/PSM extensions and proprietary SQL extensions, procedural and [object-oriented](https://en.wikipedia.org/wiki/Object-oriented_programming_language) programmability is available on many SQL platforms via DBMS integration with other languages. The SQL standard defines [SQL/JRT](https://en.wikipedia.org/wiki/SQL/JRT) extensions (SQL Routines and Types for the Java Programming Language) to support [Java](https://en.wikipedia.org/wiki/Java_(programming_language\)) code in SQL databases. [SQL Server 2005](https://en.wikipedia.org/wiki/SQL_Server_2005) uses the [SQLCLR](https://en.wikipedia.org/wiki/SQLCLR) (SQL Server Common Language Runtime) to host managed [.NET](https://en.wikipedia.org/wiki/Microsoft_.NET) assemblies in the database, while prior versions of SQL Server were restricted to unmanaged extended stored procedures primarily written in C. PostgreSQL lets users write functions in a wide variety of languagesâincluding [Perl](https://en.wikipedia.org/wiki/Perl), [Python](https://en.wikipedia.org/wiki/Python_(programming_language\)), [Tcl](https://en.wikipedia.org/wiki/Tcl), and C. &gt; --- ^Interesting: [^SQL ^Server ^Compact](https://en.wikipedia.org/wiki/SQL_Server_Compact) ^| [^Microsoft ^SQL ^Server](https://en.wikipedia.org/wiki/Microsoft_SQL_Server) ^| [^Data ^definition ^language](https://en.wikipedia.org/wiki/Data_definition_language) ^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+crdr9pn) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+crdr9pn)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
Thanks so how would I structure this query then? I am still getting the order mixed up I feel. Been staring at this for two hours now 
What was the deleted reply? Was my link at least in the ballpark?
?
sorry I just meant 'rows' not 'column rows'. edited
So it would be unique rows: * 1- 4 as individually, left alone * 5-10 summed up into a single line item * 11 -&gt; 20 individually 
FYI, this statement at the high level is not an inner join, this is a cross-join of 2 tables (the subquery in the where clause is independent of the main statement).
Writing to temp tables isn't a great practice to get into. Temp tables aren't indexed and if you are dealing with datasets with millions of rows these stored procedures can be very costly and slow. One of my biggest achievements was removing all the temp tables from a one of our largest stored procedures and replacing them with views. That stored procedure that created 15 temp tables and then join them at the end took 35 minutes to process 10k rows of data. After removing them and replacing them with views and a single CTE, it could process 1 million claims in 9 minutes and outputted the results in an archive table for reporting. The impact of the speed increase caused the company to hire 8 people and fire 3 because the added reporting showed multiple mistakes on one department over 3 years time. We caught up on 3 years worth of unprocessed claims. Too many people on /r/sql support techniques that work with small databases because they don't know how to transform data correctly and use methods like writing to temp tables. These practices do not work in big datasets. 