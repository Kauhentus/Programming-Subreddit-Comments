You don't need a primary key to join. Common misconception. 
hmm, one errors and the other doesn't, though. maybe coalesce is more sensitive to collation. 
Thanks for the explanation. Let's see if I understand you correctly. If I were to say: WHERE date = min(date) will not work, because the function min() can't work on a single unit of data; it needs a set of data to work on. Would that make sense? 
Well I would add an autoincrementing id number field to it just for future usability, but it doesn't need to relate to anything. Just makes it easier to point to a single row if you can't pass a one row recordset to another function, or subquery.
I've never been strong on explaining things, but I think you had assumed that the query knew what you meant when you say "= min(date)", but it has no idea because you can only stick an exact value there, or if you are not using an exact value, it has to be from a complete and separate query that resolves to an exact value that equals what is on the right side of the equal sign and is also of a compatible data type. But "= min(date)" is not a complete statement. The min(date) is what you want to SELECT, but it still has no idea where it is getting it FROM. So that's why you wrap it all in a SELECT min(date) FROM something sub-query. 
If there were no other way I would agree, but in this case every row is uniquely identifiable without the need for a potentially problematic autonumber.
What is problematic about an autonumber?
It really depends on a whole lot of things. If lots of queries are being run against a production database (which is generally a bad sign in itself), building datasets incrementally in temporary tables can be useful in avoiding locking (but introduces all kinds of accuracy issues when live data is changed between the reporting query transactions). Likewise with the query outputs - you mean one off queries for ad hoc reports from a live db? Probably not great to store results. I really couldn't swear to it without knowing a whole lot more about your environment and what the purpose is of these queries, but yes what you're describing does smell of less than optimal practice. And for my MSSQL/PostgreSQL snobbery, I do find that MySQL, being the favoured choice of devs who just want something to work without much db-specific knowledge, often finds itself being used suboptimally.
is this normal? no storing results as tables makes perfect sense example: an analysis of transactions for a particular product line for the preceding month, aggregated by major brands... this table is then accessed by many different areas in the company for their own specific tracking reports based on brands they are responsible for 
that must be it... i have no experience with collations, or more specifically, with clashing collations
I really *really* resisted temp tables for a very long time. They seem very inefficient and can very easily mess with optimizers. But sometimes they are, undoubtedly, the best way to solve a problem. I don't really know the environment of your new company - but if you feel comfortable don't be afraid to find an example of one and ask why they chose to use it there. Or find a good example and run it with the execution plan, and if you can do the same script without using the temp table. It very well could be they're using them inefficiently for the sake of simplifying their scripts. It's easy to get into that bad habit, but they're also sometimes entirely necessary (or at the very least, the best option). I don't think I've ever worked with a SQL person that wouldn't appreciate the new guy asking questions like this. It shows you wanna learn. And the more you learn, the easier everyone else's job gets, too. 
&gt; MySQL ... often finds itself being used suboptimally please cite a specific source for claim of "often" just kidding, i know exactly what you mean their sneaky implementation of relaxed GROUP BY standards has clotheslined ~countless~ sql noobs 
The easy answer would be say "yes, it's terrible". And it probably is. But personally I'd be hesitant to start tearing it apart without more information. Especially about the type of workload (OLTP/Warehouse) and the uptime requirements. For instance, from your description, I gather it's running one query for say one month, adding that to a temp table, then running a new query for the next month (or similar). The danger there is inconsistent data between queries so the same record might show up twice if the month is updated in between batches. Is that bad in your use case? But maybe the queries were causing massive performance problems on the update side because of the locks the scripts were putting in place during read? So this was a solution? How old is the DB? Maybe this was written on an ancient version of MySQL that had performance limitations and there's never been a need to refactor it. Lack of functions doesn't worry me necessarily, there's not always a need. Lack of views may be a bit more a concern, but non-indexed views aren't miracle workers. What I'd look at first is the indexes and see what kind of coverage there is in that department, and how the queries are using them. 
&gt; If lots of queries are being run against a production database (which is generally a bad sign in itself), building datasets incrementally in temporary tables can be useful in avoiding locking (but introduces all kinds of accuracy issues when live data is changed between the reporting query transactions). To my knowledge this isn't a problem as the data isn't modified very regularly, one of my job is to run the bash script to load the new data in and we get less than 10 a day. I suspect the problem is the database hasn't been structured to improve optimisation, there are only a few indexes on the main tables and specific tables can hold millions of different events classified by codes instead of being sorted into their own tables. They are experimenting with using Ms SQL server to improve the performance but no idea when or if it will happen. I can't make major changes to the database design because there are lots of scripts that rely on this database structure. 
I didn't mean that as a criticism of MySQL itself, it's just that it's the go to for non-databasey people being that it's free and more accessible than postgresql. I read somewhere the greatest advantage of MySQL is that it'll get you a result if your SQL is even close to correct, and its greatest disadvantage is ..... it'll get you a result if your SQL is even close to correct. I'm particularly fond/horrified by its willingness to implicitly convert basically anything you throw at it. 
&gt; specific tables can hold millions of different events classified by codes instead of being sorted into their own tables That actually sounds like completely normal, good db design unless each type of event has vastly different requirements for data recorded. Doesn't sound like their problem is the platform though, changing to MS SQL isn't going to be a magic bullet.
&gt; I don't think I've ever worked with a SQL person that wouldn't appreciate the new guy asking questions like this. It shows you wanna learn. And the more you learn, the easier everyone else's job gets, too. This. Yes. Please more of this. 
&gt; For instance, from your description, I gather it's running one query for say one month, adding that to a temp table, then running a new query for the next month (or similar). The danger there is inconsistent data between queries so the same record might show up twice if the month is updated in between batches. Is that bad in your use case? Not quiet its more when these scripts are run it creates a temporary table, then runs a series of queries and inserts the results into the temp table (often to add new columns) , then copies the final temp table into a "production" table and deletes the temp table.
That's actually a completely normal and reasonable use of temp tables. 
ok thats alright then, its just this is my first "production" database and I wasn't sure if their methods were best practice.
This should work: SELECT T1.id ,CASE WHEN aliasSub.alias IS NULL THEN t1.name2 END AS NewName FROM T1 LEFT JOIN (select id, alias from T2 where a bunch of stuff) aliasSub ON T1.id = aliasSub.id I see in the comments that you seem to have already resolved this. I've included this so that someone facing troubles with a CASE in a SELECT sees another way to accomplish it. It's also worth noting that ISNULL (which is specific to SQL Server) and COALESCE (ANSI standard) would perform this as well and are specifically designed to do so (COALESCE will also do other things that ISNULL isn't designed for). The biggest difference between COALESCE and ISNULL is how the data type of the output is handled. Hopefully someone finds this interesting or helpful.
If I'm remembering correctly in 10g it would only exist for that session. I don't know how good my memory on that is, or if it was different in another version, or which version(s) you are working with. Wow, that may be the least useful comment I have ever made on reddit.
The trick here is that the world is being included twice and compared with itself. We have the world table included as x, and the again as y. The two are then joined together on the continent. So imagine a table like Country|Continent|Area :--|:--|:-- France | Europe | 50 England | Europe | 40 Wales | Europe | 30 The record set when joined to itself on continent like SELECT x.country, y.country FROM world x INNER JOIN world y on x.continent = y.continent Would look like: x.country | y.country :--|:-- France | France France | England France | Wales England | France England | England England | Wales Wales | France Wales | England Wales | Wales In other words the result set will have every combination of country that share a continent. Your query is getting a bit more complicated because the join is actually happening in the where clause of a subquery. So the bit that goes area &gt;= ALL (SELECT area FROM world y WHERE y.continent = x.continent AND area&gt;0) the area of each country is being compared with the ALL of the areas of countries in the same continent, and if it is the same or greater than every one of these (i.e., is the biggest), it meets the criteria. Hope this helps?
Yeah thank you! This clears things up a bit more. So just to clarify, the subquery is essentially acting partly as a join? Does that mean there’s an easier way to write this and be able to understand what it’s doing, or is this as simple as it can get?
The way I would do this makes for a simpler syntax with no self-joins but requires an understanding of another function so I don't know whether you'd count this as a simplification: SELECT country , continent , area FROM ( SELECT country , continent , area , row_num() over (PARTITION BY continent ORDER BY area desc) rank ) x WHERE x.rank = 1 But you've probably not covered window functions like row_num yet. Another way that might be more immediately understandable is to start with a query that gives you the maximum area for each continent: SELECT continent, max(area) AS maxarea FROM world GROUP BY continent Then join that back to the world table: SELECT x.country, x.continent FROM world x INNER JOIN ( SELECT continent, max(area) AS maxarea FROM world GROUP BY continent ) y on x.continent = y.continent and x.area = y.maxarea It's less efficient than the row_num method but probably more readable.
1. It's lazy and allows for not thinking about the data 2. For large datasets you can overflow it (not that common if you pay attention) 3. You have to expose it for update and delete operations rather than just using a multicolumn based key 4. No guarantee that they are consecutive, but (see #1) novices tend to expect them to be and sometimes rely on that or attempt to incorrectly exploit it 5. And may (insert diety here) have mercy on you if you think about using that autonumber as a foreign key in another table...you will feel the sting eventually. 6. If you have ever had to batch merge instances, you would be prepared to strangle the wise guy who thought adding a autonumber field was a good idea. Not that you can't use them, but just don't use them as a bandaid 
&gt; It's lazy and allows for not thinking about the data It is a reference id to easily specify a record without spelling out the criteria on each column that makes it unique. Lazy sure, what is wrong with that. It is also easier to use. &gt;For large datasets you can overflow it (not that common if you pay attention) Ok.. So people pay attention. &gt;You have to expose it for update and delete operations rather than just using a multicolumn based key I don't see where it needs to be touched at all for update/delete unless optionally to skip specifying multiple columns. &gt;No guarantee that they are consecutive Anyone should be taking a second to understand the column before making use of it. So what a super novice does or someone not taking the time to understand the field does/is is a pitfall for anything they do. &gt;And may (insert diety here) have mercy on you if you think about using that autonumber as a foreign key in another table...you will feel the sting eventually. See my last response. &gt;If you have ever had to batch merge instances, you would be prepared to strangle the wise guy who thought adding a autonumber field was a good idea. Then toss the column and rebuild it, it is not being used for anything except for a reference in a dynamic query. Or live "hey dude look at record with ref_id 53743847". 
&gt; They seem very inefficient and can very easily mess with optimizers. I've used temp tables to fix a lot of queries/stored procedures that ran terribly otherwise. The optimizer was able to do a better job with the temp tables than without.
&gt;Pretty much all of the scripts the company uses use temporary tables to store data while more data is gathered and then added to the temp table. I do this all the time. It's a perfectly normal pattern. &gt;Views and functions are totally absent from the database. Is this normal for a database of this size or does this show a lack of optimisation/poor programming? Given that both nested views and functions can make proper optimization of a query difficult if not impossible (at least with SQL Server), I'd say it's the opposite. Views &amp; functions have their place, but they are not a replacement for temp tables, nor are temp tables a replacement for views or functions. Those temp tables may actually be the well-programmed, optimized version.
&gt; They are experimenting with using Ms SQL server to improve the performance but no idea when or if it will happen. I can't make major changes to the database design because there are lots of scripts that rely on this database structure. Changing database platforms solely to improve performance seems a bit extreme. The database structure and/or poorly-written code (whether it's in the database itself or the application) is far more likely to be the cause of the performance issues. A bad schema is a bad schema, no matter what it's running on. If no one has done any testing to find out *why* performance is bad, they're just going to repeat the same mistakes *and* make new ones on SQL Server.
I work a lot in teradata and I have to say without volatile tables, I would not be able to do my job. Queries are killed if they run longer than 5 minutes so you have to be careful how much data you are pulling down from large datasets. 
Yes, I work very heavily in Teradata, and often getting info into volatile tables first, with correct indexes/statistics, is the best way to go around massive tables with millions to billions of records. Many times, indexes/statistics are stale (unfortunately) and you can do the optimizer a favor y pulling the data out and calling it manually.
Cool. I'm definitely gonna use this site.
SELECT E.SENSORID sid, COUNT(E.eventid) types FROM EVENTS E GROUP BY E.SENSOR_ID ORDER BY E.SENSOR_ID I suggest you look up a sql class on Udemy if your interested in learning SQL.. Your on the right track, you just need to understand what the engine wants and how to tell it what you want.
Well under that logic why bother doing any data normalization at all then? Just keep it denormalized in one big huge table, if your careful and know what your doing and doing things solely because it is easy, you'll be fine, no need for any good practices. I mean this whole RDBMs stuff is so much more complicated than just importing the raw Excel spreadsheet this thing started in and get on with it already, IMHO, is not a wise philosophy to follow. But to each their own.
great thanks. I still get an error though on line 4. The types gives me a runtime error. And does E work like a placeholder?
SELECT events.SENSORID, COUNT(distinct event.eventid) AS "types" FROM events GROUP BY events.SENSOR_ID In ms sql.
&gt; Queries are killed if they run longer than 5 minutes so you have to be careful how much data you are pulling down from large datasets. Lucky for me queries are killed after 30 seconds. 
Set database max size to Unlimited. Right click on Database -&gt; Properties -&gt; Files.
great this works thanks. would you know how to return the difference b/w the latest and second latest values? event_type | value | time 2 | 5 | 2015-05-09 12:42:00 4 | -42 | 2015-05-09 13:19:57 2 | 2 | 2015-05-09 14:48:30 2 | 7 | 2015-05-09 12:54:39 3 | 16 | 2015-05-09 13:19:57 3 | 20 | 2015-05-09 15:01:09 event_type | value 2 | -5 3 | 4 I'm trying to subtract the values in the columns but I can't
I have tried doing it and it is not going to let me exceed the limit of 10 GB.
You have the free copy of SQL Server. Are you sure your teaching was talking about getting past that kind of limit? 
Yes. I am sure about that. He said it was a "bonus" problem and I am not likely to be able to solve it. Guess he was right.
60 seconds on google: https://akawn.com/blog/2017/05/how-to-bypass-the-sql-server-express-database-size-limit/
Seen that link, thank you. It is T-SQL way to do what you advised me in the first place.
aliasSub.alias and t1.name2 have different collation. You need to explicitly cast to a collation. It's a pain to compare columns that have different collations and I'm pretty sure a big performance hit as well if it prevents you from using an index. I suggest altering the collation of one of your columns. Or you can explicitly cast to a default collation of the database, for example: SELECT T1.id , NewName = CASE aliasSub.alias COLLATE DATABASE_DEFAULT when null then t1.name2 COLLATE DATABASE_DEFAULT end FROM T1 LEFT JOIN (select id, alias from T2 where a bunch of stuff) aliasSub ON T1.id = aliasSub.id 
Try using a case statement with your SET 
 UPDATE Componist Tried this: SET sex = (CASE WHEN (id = 5) THEN 'V' ELSE (sex = 'M') END); Didnt work.
Your case statement construction is not quite right. Try looking at the examples at the bottom of this page: [Case Statements](https://docs.microsoft.com/en-us/sql/t-sql/language-elements/case-transact-sql)
Thanks very much. I’ve never run into different collations per table in the same db before and appreciate the help. 
Divide it into two statements: UPDATE TABLE tablename SET sex = 'F' WHERE id = 5; UPDATE TABLE tablename SET sex = 'M' WHERE id != 5;
Well, somebody had too much eggnog. If you ran that query for my kids you'd get no rows returned.
In my experience, if you're in a situation where you need to exceed the 10GB limit of SQL Server Express Edition, one of the following is true: * You're able to pay for the licensing for Standard Edition (production usage) * You don't have to pay for the licensing for Standard Edition as the license for Developer Edition allows for your usage &gt;More specifically, he said that I must come up with a way to make "SQL Server not worry about the database limit" when the database is close to maximum size and continue filling it. This is bizarre to me and does not represent a realistic scenario. If he's trying to get you to do "unusual" things with SQL Server/T-SQL, there are other ways to do it.
Thank you for clearing it up for me. I will discuss the issue with my professor to find out what exactly he wanted me to do because this indeed does seem extremely strange.
Why not do this in the presentation layer?
Lots of many-to-many relationships, and at least one self-join. Might be a good fit for a nosql solution. Cassandra? Not a lot of topographical features, though. Hmm.
You could get better perf by joining to the Houses table, and only returning a list of locations with at least one good child. Then use that to feed the Traveling Salesman algorithm. Pulling a list of good kids by location is trivial, and can be done JIT on the night-of. 
I’m not in IT or data science (more like BI with analytics), and I just completed my first Python automation project using pandas to munge, manipulate and combine 71 spreadsheets into 5. I loved it and am now looking for more things to automate. 
Ok. You really haven't proven your point and rather went on a rant. Which 'good practice' is being failed by having a key, and that key being auto incremented. Perhaps the use of the word 'key' is the issue. How about record_id. Since it's entire purpose is to identify a particular row, and not being a reference to anything else.
I think you need all kids, how else is the coal delivered?
It would be helpful if you pasted your entire query, but you should be able to do it with a CTE and then join two sets of data to grab the max(rate) for any date &gt; than the date being passed?
I forgot about coal. Damn. 
Maybe that can be delegated to subordinate Clauses?
 SELECT TOP 1 Rate FROM RateGoals WHERE Store = 'TEST STORE' AND EffectiveDate &lt;= '2017-08-01' -- passed-in date ORDER BY EffectiveDate DESC 
Elves? Are those subordinate Clauses?
I don't think I need a CTE because there's only one table. That's why I self joined. I actually don't need the max(rate), I just need the rate where the effective date is equal to the date passed, or the effective date is the the one previous to the date passed and not greater than the next. Hope that makes sense. I've added a table sample to the post. 
Props to @r3pr0b8 for a simple solution!
Like myself, I'm sure there are plenty of novice SQL users that are unaware of this feature, who may get caught up in the mess of GROUP BY clauses and unnecessary CTE's, while just trying to capture all rows and aggregations together. Hope this is helpful. The best TLDR would be that this is a way of aggregating a column without "rolling up" all of the data, as a GROUP BY would.
If you have SQL Server 2012 or newer, there's some handy windowing/analytical functions that'll take care of this for you. For example: https://stackoverflow.com/a/16735380/1324345
sql server 2008 :(
I feel your pain. There are ways to do it w/ a CTE in 2008.
I don't know why I didn't know this already, but you've blown my mind. Thanks for that.
Hard to tell the issue by your description and the fact that we don't know what data is on T1 and T3. You're doing an INNER join. JOIN alone is default to INNER JOIN. That being said, if there are multiple to multiple, or 1 to multiple relationship between the two tables, you'll get more results. You may be better off doing a LEFT or RIGHT JOIN, or joining a subquery where you aggregate the subquery. 
The join is slowing it down, sure, but adding an aggregation is going to make the query more expensive, too. Do you have any indices on the tables? Can you post the execution plan?
Assuming you can create temporary objects you might be able to use temporary tables. You can winnow down the data (only rows and columns you need) in the temp table, apply indexes if required, and do your select and aggregation there it could help. I've rewritten the second query to get rid of the sum + group by. When written as a join, is the performance similar to the first query? SELECT T1.product, T1.period, T1.Store, T1.Fact from T1 WHERE T1.product in (SELECT T2.product from T2 where T2.productUPC in (UPCs)) AND T1.period in (periods) AND T1.store in (SELECT T3.store from T3 where T3.market in (markets)); SELECT T1.period, T3.market, T1.fact from T1 JOIN T3 on T3.store=T1.store WHERE T1.product in (SELECT T2.product from T2 where T2.productUPC in (UPCs)) AND T1.period in (periods) AND T3.market in (markets) 
Thanks for this. I actually came across this a couple of weeks ago when trying to Google a solution. I didn’t give it that much thought back then, but this sank in real quick.
I guess I figured it out but my query is sloppy as hell.
Really cool. Didn't consider myself a novice but I guess I was wrong. Now if only my company's database made any sense and all of this info would be in a single table. Not that this affects this post specifically, it's just how I feel when I see test tables used to explain things. 
It's all good. My big point I am trying to make is why add an artifical unnecessary key that has a high chance of incorrect/inappropriate use, when the data itself already has a natural, though multicolumn, key already. 
&gt; Assuming you can create temporary objects you might be able to use temporary tables. If you can execute a `SELECT` query, you can create temp tables in SQL Server.
SalesAmount shouldn't be an int but good work.
You can insert multiple values with a single insert statement. Haven't seen this use of Over By. Thanks. 
Try to see if you can do the same join through foreign keys, even if it takes you through several other tables. It's possible you are trying to join two large tables on unindexed columns, resulting in really slow performance.
The next step would be to add MSAGL https://github.com/Microsoft/automatic-graph-layout support to visualize dependencies :)
What platform? Pretty sure I have done it successfully on Linux before.
os x 10.12.6 / Sierra, its print graph --&gt; page setup --&gt; save as pdf and its not working, i dont think i have those options?
For free : http://www.studybyyourself.com/seminar/sql/course/?lang=eng. The course is broken down into a basic course and an advanced one. You can submit exercises online too
May have more to do with OSX than DataGrip, I don't even have that option in the print menu - but then I also have no printer set up either. I was able to go from SVG to PDF fairly easily, but that was all outside of DataGrip.
Try "An Introduction to Database Systems" by Christopher J. Date. It is the best academic book that i've read ever. For practice i recommend "ORACLE PL/SQL 101" by Christopher Allen.
Fight your way though: https://docs.oracle.com/database/121/SQLRF/statements_7002.htm#SQLRF01402 and look for "identity_clause" It seems like you are missing some some parentheses. CREATE TABLE transaction3 ( transaction_id NUMBER GENERATED BY DEFAULT AS IDENTITY (START WITH 100 INCREMENT BY 10), holding_id NUMBER, transaction_type NUMBER, transaction_date DATE, credit_value NUMBER, debit_value NUMBER ); 
In this test scenario I was selling opinions for exactly one whole dollar, tax free :D 
The update syntax is: UPDATE [table] SET [column] = ‘’ WHERE [column] = ‘’ Since i don’t know what he means with “basic” sql i cant really know what you’ll be faced with. 
Thanks so much for this!
Good stuff. I use this technique a lot to chronologically organize events inside of temp tables. So for example I might partition over by a date column in descending order. Then I join my results table to that temp table where my temp table column “sort order” is equal to 1. This would effectively give me the latest comment or transaction that occurred for a given record. Thanks for sharing this.
Awesome! Thank you so much for the quick response! 
Couldn't agree with you more, one of my favorite techniques to use.
I’ve only used it to create Row_number () columns before, this is super cool!!! Thanks!!! 
I'm not sure this is the best example for OVER. It's basically just replacing a group by without doing any of the awesome stuff a windowing function can do, like selecting the top sale for each user or removing duplicates.
I like using this method when pulling back subrecords where you just want the most recent entry. So much easier to use MAX(Subrecord_Number) OVER(PARTITION BY Record_ID) than trying to detail with GROUP BY and all that.
Well, I think that's why it's titled a simple example. It's a new function to me so to expect "the best example of" is a stretch. Just wanted to throw it out their to others that may have not known about it. Many functions can be used in many ways, thats for sure. For me when something is brand new, I like to see a simple example first. 
Weren't these introduced back in like SQL 2008?
Why would you expect all of the sales data to be in a single table? I have done quite a bit of consulting on the accounting/finance side and I can't think of many environments where that would be a good thing. Maybe like a lemonade stand or something.
&gt;Maybe like a lemonade stand or something. As it turns out, a lot more goes into "sales" than the actual sale itself. Lol spoken like a true consultant. 
Both of your examples are correct. SELECT * FROM Employee will return all info (first name, last name, position, gender, etc.) for all employees. Your UPDATE statement is a little off. As u/KNerli pointed out, your UPDATE should look something like: UPDATE Employee SET first_name = 'Joe', last_name = 'Blow', position = 'schmuck' WHERE gender = 'male'; You're probably going to want to add additional conditions to your where clause, so you don't update every male employee to have the same name. You may also need to be prepared to write unlimited update statements, since there unlimited genders now. To delete all the males from the database you would write: DELETE FROM Employee WHERE gender = 'male' AND privileged = 1; 
Says the guy who doesn't understand his company's database. You're the reason $200 an hour consulting jobs exist.
Jesus. You made no real point aside from patronizing me about a lemonade stand and vacuously stating "more goes into sales than sales." I certainly understand my company's database but that's impossible to prove because we're just chatting on reddit and can't go into specifics. And apparently you know all the specifics because you know I don't understand anything. Again, spoken like a true true consultant. Kind of reminds me of how one said a hotkey broke a certain as 400 function and brought me into three director level meetings in which I had to prove, each time, he had no idea what he was talking about, couldn't figure out the real issue, and blamed the hotkey. 
Don't sweat him. He's all over this and other programming subs strutting his shit constantly. 
haha thanks for letting me know. i was honestly taken aback having to reply to **** like this in this sub of all places.
Who me? I literally offer free help. Not my fault someone is shitting on my profession when they have no idea what they are talking about. Don't say "I don't understand my own database we should just have one sales table!" then get butt hurt when someone says "yeah it's not that simple".
Apparently I must be charging more as mine aren't selling...
I had to check this, but even more than that, apparently you just need to be able to log into SQL Server to create a temp table. TIL.
Other people have helped given you some direction, so I'll contribute something a little different. http://launch.solarwinds.com/rs/solarwindsworldwide/images/12_Step_Query_Tuning_SQLS_IG.pdf The guide I linked is an overly simplified cheat sheet on how to performance tune. I think that if you are new to intermediate, it's a good guide to get you started and critically thinking about design and performance.
&gt;AND privileged = 1; well done
Thank you for this!
That's exactly what's happening and I don't have write access to create indices .-.
Huh, Always something to learn. Thanks for the new info!
Thank you! I will give that a shot!
You asked him why he thought all of the sales data would be on one table so that you could prove him wrong. Then did a very r/iamverysmart about your personal experience. Maybe it's just the way you come off as superior. No doubt you have experience and I've seen you give great information, but given the chance you act like an asshole. Nbd, buddy. 
We're not buddies now. You made something up to trash me without any proof and then call "me" the asshole. That's not how any of this works. And the other guy deserved to be called out. Who makes a statement like *all of our data should be in 1 table*? Someone who doesn't know what they are talking about. Then he proceeded to make an anecdote for one person he ran into and applied it to an entire profession. Again, ridiculous. Of course I'm going to call this out.
I think this should work for you: DELETE from inventory INNER JOIN orders ON inventory.key_code = orders.key_code WHERE orders.criteria = '+' AND inventory.inventory_sqlidentity = :rowid AND inventory.type = 'D' AND inventory.freq = 1 AND inventory.object_org = 'COMPANY' Definitely run a test case before running this against a prod db!
If the data isn't too big you could extract it to temp tables, create indexes there and do the join.
Thank you very much! I was getting an error; ncorrect syntax near the keyword 'INNER'. There was one slight change I had to make, but I wouldn't have gotten there without you. You need to specify what table you are deleting from, so it ended up like this: DELETE i from inventory as i INNER JOIN orders as o ON i.key_code = o.key_code etc. Thank you again! Made my day. Fun getting back into this.
No problem! Glad I could help. If you can believe it, that's the way I originally had the query but I tried to simplify it without the aliases... glad you got it working in the end.
 SELECT c.id, c.name, c.address, o.cried_flag, o.pout_flag, o.sleeping_flag, o.awake_flag, b.naughty_flag AS bad_flag, b.nice_flag as good_flag FROM contacts AS c LEFT JOIN observations AS o ON c.id = o.id LEFT JOIN behavior AS b ON c.id = b.id
It almost fits to the intro of Rudolph
It didn't error because varchar to int is an [implicit conversion](https://docs.microsoft.com/en-us/sql/t-sql/data-types/data-type-conversion-database-engine). However if you had any data in the varchar field that couldn't be converted to int it would have caused an error.
 CREATE TABLE personActions (idPersonAction BIGINT IDENTITY(1,1) PRIMARY KEY, idPerson INT REFERENCES person(idPerson), actionCode NVARCHAR(5) REFERENCES action(code), wasNaughty BIT); EXEC populatePersonActions @naughtyThreshold = 42; DBCC CHECKTABLE (PersonActions); DBCC CHECKTABLE (PersonActions); WITH naughtyList AS ( SELECT idPerson, COUNT(idPersonAction) AS naughtyCount FROM personActions WHERE wasNaughty = 1 ), niceList AS ( SELECT idPerson, COUNT(idPersonAction) AS niceCount FROM personActions WHERE wasNaughty = 0 ) INSERT INTO yearlyGiftDeployment (idPerson, personStatus, dYear) SELECT p.idPerson, CASE WHEN (ISNULL((c.niceCount * 1.0),0) / (ISNULL((a.naughtyCount * 1.0),0.00001))&lt;=1 THEN 'Naughty' ELSE 'Nice' END AS personStatus, 2017 FROM person AS p LEFT JOIN naughtlyList AS ON a.idPerson = p.idPerson LEFT JOIN niceList AS c ON c.idPerson = p.idPerson EXEC deploySanta @year=2017;
read up on Table Inheritance (or subtype/supertype) Generally Single Table Inheritance is the simplest and easiest, but you have to be okay with having null columns :) Be careful not to avoid the "exclusive arc" anti-pattern though: https://softwareengineering.stackexchange.com/questions/267652/what-is-exclusive-arc-in-database-and-why-its-evil
For oracle: SELECT DECODE(SIGN(FLOOR(maxwidth / 2) - ROWNUM), 1, LPAD(' ', FLOOR(maxwidth / 2) - (ROWNUM - 1)) || RPAD('*', 2 * (ROWNUM - 1) + 1, ' *'), LPAD('* * *', FLOOR(maxwidth / 2) + 3)) "CHRISTMAS TREE" FROM all_objects ,( SELECT 40 AS maxwidth FROM DUAL ) WHERE ROWNUM &lt; FLOOR(maxwidth / 2) + 5;
Makes sense, but why the longer query time, did it convert the ssn = 0 to ssn = '0' for each comparison? 
It would have converted every row in columnname to int.
Holy cow! thank you for that.
O, B-Tree...
Index the halls The First NoSQL Do You Seek What I Seek Codd Rest You Merry Gentlemen Here we come a-Crossjoining Tip your waitress
0 rows returned 
Also FYI, SAS has it's own programming language outside of SQL designed for processing flat files, it's called data step programming.
For starters you probably need to group it. If a person has five orders of item you'll currently get five records with quantity 1. If they have one order of quantity 2, that product will appear higher up the list. Try something like SELECT Customers.CustomerName, Products.ProductName, SUM(OrderDetails.Quantity) FROM Customers LEFT JOIN Orders ON Customers.CustomerID = Orders.CustomerID LEFT JOIN OrderDetails ON Orders.OrderID = OrderDetails.OrderID LEFT JOIN Products ON OrderDetails.ProductID = Products.ProductID GROUP BY Customers.CustomerName, Products.ProductName ORDER BY Customers.CustomerName, SUM(OrderDetails.Quantity) DESC;
i'm sure you meant `c.id = o.contact_id` and `c.id = b.contact_id`
Would this work: WhateverYouWantToShow Table: XID column Type column XID and Type act as a composite primary key. Where X is the unique value in tables A, B, or C and Type column is set to either A, B, or C. This ensures data integrity while maintaining uniqueness in the new table for each possible row being added from Table A, B, or C.
You need some sort of aggregate function. Right now you're getting all the rows with the correct data but you need to squash them down with GROUP BY and SUM to tally up all quantities per product per customer. A customer can potentially order the same thing five times in five different orders instead of all in one order for example.
Why would you *sort* a database? Does she mean index? Even if she's talking about a query there'd be no reason to sort it twice...
In the reference table, is there already a record for User A with a NULL value in the Supervisor ID field? 
No. The very first supervisor in the chain does, technically, (CEO) but no other supervisors below them.
[removed]
I'm sure someone who has more experience than me can give you a better answer with a fancier join, but is it possible to have a second table that contains each User ID, with a second column that is always NULL? Then you can have two queries that are UNION'ed together - the first being your original design and the second just being a "select * from [table 2]". Use 'ORDER BY 1,2 asc' and I believe you would get the result you need? I'm assuming your resultset will have 2 columns total(for instance the next record in your first example might be |User B|User F|, and the first column continues to increment downward through each user that is a supervisor. That's my Mcgyver route anyways. 
fixed it. &gt; SELECT &gt; c.id, &gt; c.name, &gt; c.address, &gt; o.pout_flag, &gt; o.cried_flag, &gt; o.awake_flag, &gt; o.sleeping_flag, &gt; b.nice_flag as good_flag &gt; b.naughty_flag AS bad_flag, &gt; FROM contacts AS c &gt; LEFT JOIN observations AS o ON c.id = o.id &gt; LEFT JOIN behavior AS b ON c.id = b.id 
This sounds like a good idea, this way I could avoid having null columns. But could I make this is a 'real' foreign key relationship in MySQL? I would expect the foreign key can not do this. That would mean I have the 4 tables but they are not connected? Wouldn't that be bad DB design?
Thanks for the suggestions, I will definitely read up on that.
My guess would be DROP, as in `DROP { TABLE | VIEW | ... } objectName` and the variations.
Is agree with this. Tack on a CASCADE for maximum damage.
Agreed. DELETE is only issued for rows. DROP is issued for pretty much everything else.
This made my morning. Thanks guys
Truncate
[removed]
CodeAcademy is a great place to start
The sidebar has this info: A common question is how to learn SQL. [Please view the Wiki](https://www.reddit.com/r/SQL/wiki/index) for online resources.
DELETE without WHERE is something everyone does... once
&gt; Every null is a different null. not true in GROUP BY, all nulls are grouped together in one group as if they were... wait for it... identical!!!
True. Forgot to mention that there. It’s mentioned in the other article about null.
I agree Drop would be death. Delete lose a row, Drop a whole table that would def be considered destroying XXX.
&gt; select top 1 ID from atable where charcolumn &lt;&gt; 'F' and id &gt;= 1000 and id &lt;= 1019 order by newid() Thanks. How would I put the returned result into a variable? Is that Possible with this method, or would I need to rework this?
Such as? declare @var_id int = (select top 1 ID)
One of the best online course. For free. With examples and online exercises. http://www.studybyyourself.com/seminar/sql/course/?lang=eng.
Yes, but COUNT() will turn up "0" for them.
here you go... SELECT shifts.shift , wage_slaves.name FROM shifts CROSS JOIN wage_slaves have a look at what that produces and see if you like it if you really want only those rows you indicated, then all you have to do is add this WHERE clause -- SELECT shifts.shift , wage_slaves.name FROM shifts CROSS JOIN wage_slaves WHERE shifts.shift = 'morning' AND wage_slaves.name = 'Bubba' OR shifts.shift = 'noon' AND wage_slaves.name = 'Jimbo' OR shifts.shift = 'night' AND wage_slaves.name = 'Peggysue'
you are... how shall i put this... imprecise, and half wrong try it yourself -- CREATE TABLE some_nulls ( egg VARCHAR(9) , stooge VARCHAR(9) ); INSERT INTO some_nulls VALUES ( 'humpty' , 'curly' ) ,( 'dumpty' , 'larry' ) ,( NULL , 'moe' ) ,( NULL , 'shemp' ) ,( NULL , 'joe' ) ,( NULL , 'curly joe' ) ; SELECT egg , COUNT(*) FROM some_nulls GROUP BY egg 
"Premature optimization is the root of all evil." If you have a few hundred customers and a few thousand orders, any difference would be absolutely unnoticeable on any modern DBMS. Write whichever query is easier for you to understand. If you really want to get into it, look at the execution plan for each query. The optimizer might make them the same anyway. Run benchmarks. Without knowing a lot more about your setup (DBMS, indexes, exact numbers of rows, etc.) we can't predict the result.
&gt; SELECT the VIPcustomers, then JOIN the orders table SQL doesn't work this way in any query, the FROM clause is executed first, the SELECT clause is executed much later
You're going to need to show us what your query looks like (preferably formatted) and a sample of what your data looks like. It sounds like you just need to use a like in the where.
Is there SQL fiddle link at the top of my post working? I hadn't used it before.
I see it now, sorry. So this is your code? SELECT TOP 10 * FROM User_addresses UA WHERE UA.Address NOT IN ( SELECT UA2.Address FROM User_Addresses UA2 inner join Address_flags AF ON (UA2.city = AF.city OR AF.city = 'Any') AND (UA2.state = AF.state OR AF.state = 'Any') AND (UA2.ZipCode = AF.ZipCode OR AF.ZipCode = 'Any') WHERE AF. Flag = 1 ) And primitively you want to filter out anyone from the first table who matches a variable in the second table?
 select top 10 * from user_addresses --gives top 10 addresses for everyone select top 10 * from address_flags --gives you all the possible conditions in multiple columns select city from Address_flags union select state from Address_flags select zipcode from Address_flags --gives you all of the possible flags in one column select top 10 * from user_addresses where address not in () and city not in () and state not in () and zipcode not in() --horrible way to solve the problem but what you're looking for
Dude, go to EdX and sign up for the Harvard CS-50 class. It's totally free and up-to-date. Skip to the section on SQL, which is really good. 
query plan would be more helpful than the sql fiddle example. your issue could be solved with an index appose to a query change or potentially both. data at scale doesn't always behave the same way as smaller examples. if the query works, then you are 1/2 way there. This is more of a tuning exercise and the query plan from you production environment would be the next best place to start.
https://imgur.com/a/JKFxL
Is there another table with a complete list of company IDs that you can point the other table foreign keys at instead of this one? If not I’d create one and use this table as a company addresses table. 
Unfortunately no there is not - this table (actually named [Company]) was originally fulfilling that role until we discovered that we needed to make a composite primary key of the CompanyID,AddressID fields that existed in it. I suppose I could just add a new table that contained only the unique CompanyID values, but this structural change then causes a whole slew of other changes in our system which I want to avoid as much as possible.
&gt; I might be trying to be too thorough but I keep wanting background on good table design principles, etc so they can inform the syntax but every book and online course seems to always presuppose knowledge on this or get really technical really fast. [Database Design for Mere Mortals](https://www.amazon.com/Database-Design-Mere-Mortals-Hands/dp/0321884493/ref=sr_1_1?ie=UTF8&amp;qid=1513305744&amp;sr=8-1&amp;keywords=database+design+for+mere+mortals) is what you're looking for. 
I am trying to figure out your method, but I am not sure what to put in your place holders. 
*I'm going to get a bit philosophical here and then a bit real-world, bear with me.* Good database design is based around the continual juggling of compromises. Usually, these compromises are the result of wanting data integrity **and** wanting performance - the two don't always conflict with each other, but a lot of times - and in this case, they do. **Real-World Example** I have a production database that I did the logical architecture for a critical application in my workplace. When creating the application, a central theme of it was that it centered around small tasks of work called "schedules". These tasks of work could be scheduled on an "object" (Client, Order, Stock, Bill of Sale, etc.) and a scheduled job in the database would pick up these schedules, do some amount of work/business logic on the schedule, then close the schedule out - pushing it through a business logic workflow. All of these schedules (~6 billion rows worth at the moment) touched all of my objects in the database. When I go to start a new database design project, one of the first things I do is define my nouns (aka objects). Since this was an ordering system, I knew that my major nouns for the business logic would be things like those mentioned above (Client, Order, Stock, Bill of Sale, etc.). I *could* have normalized the schedules table more and had a schedules table for each object (client_schedules, order_schedules, stock_schedules, etc.) - but I decided against this from a design perspective because it would complicate the scheduled jobs I'd need to create to handle these schedules, and further complicate the schema to an already complicated design. Therefore, I settled on a compromise. I have a schedules table, where the schedule_id is the unique identifier for an object. So client_id = schedule_id, or order_id = schedule_id. To identify what type of object the schedule_id is referring to, I have an object_type column in my schedules table. C = client, O = order, S = Bill of Sale, I = Stock, etc. These two columns act as a composite key for the schedules table, so when I do a join I need to make sure I join (for example) client_id = schedule_id and schedule_type = 'C'. In order to maintain data integrity, I compromised on performance (maybe). I use a BEFORE INSERT trigger to verify that whatever is being written to the schedule_id for the object_id resides in the object's table. I like this pattern, it's served me well in the real-world - and is a pattern you'll see in various databases I've created throughout the years. Is it by-the-book best practice? No. But it gets the job done and I'm happier with the database schema in the long run. Triggers are often looked down upon as being as "do not use" - however, for this case, I don't mind them. There is a performance hit for each insert into the schedules table for me, but there's also a performance concern when needing to index a ~6 billion row table where at the VERY LEAST two of the three columns will always be null. Null markers are not your friend when indexing, especially if they're columns in the middle or at the beginning of your table. 
I think my solution would be to separate this table. Essentially, normalize the data. Then you can include your composite primary key on the new table and your current primary key in this table. 
[removed]
Sqlzoo and w3school has been pretty helpful for leading basics
Folks have already mentioned some great resources in w3 and CodeAcademy, but you should also check as to whether your local library membership gets you a free Lynda subscription - there's a decent chance it will. 
&gt;... so I saw that it was possible also create a foreign key to reference a *Unique Index* on just the *CompanyID*... and *so I set this up*. (emphasis added) Think about what that means for a minute. And think about finding someone else to make the necessary changes.
2 joins? One Inner to get the ids that have ordered product a, and an outer join where the linking Id is null to get ones that have not ordered products b... 
I can't find that out since there's no exception shown in visual studio, the error and stack trace appears on the webpage when I run the application. I tried looking for the line number in stack trace but no luck.
Are you authenticating through active directory or a username and password through the connection string to the SQL server? I would try and set up a username and password on the SQL server and update the webconfig for the website on IIS to use those values to make sure you are not running into a firewall issue.
RDP into your target IIS VM box and ensure that you can reach the target locally. You might be missing a driver and/or networking config prevents the connection.
&gt; In your exists example, customerID = 1 would not be returned if their Name = 'A' exists and Name = 'B' does not exist because that costumer ID is isn't true for both right? No, the record for CustomerID = 1 would be returned. Here's how it evaluates. The first EXISTS clause basically runs: SELECT 1 FROM Order o1 JOIN OrderDetail od1 ON od1.OrderID = o1.OrderID JOIN Product p1 ON p1.ProductID = od1.ProductID WHERE o1.CustomerID = 1 AND p1.Name = 'A' That query returns a row, so EXISTS evaluates to TRUE. The second EXISTS clause basically runs: SELECT 1 FROM Order o2 JOIN OrderDetail od2 ON od2.OrderID = o2.OrderID JOIN Product p2 ON p2.ProductID = od2.ProductID WHERE o2.CustomerID = 1 AND p2.Name = 'B' That query does *not* return a row, so EXISTS evaluates to FALSE. That means when CustomerID = 1, the main query evaluates: SELECT c.AccountNo FROM Customer c WHERE (TRUE) AND NOT (FALSE) The WHERE clause here evaluates to TRUE, so any record in `Customer` where CustomerID = 1 is not excluded from the result set. &gt; also in your EXCEPT example, wouldn't that return multiple AccountNo if they have two separate orders of p1.Name = 'A'? Nope. [EXCEPT and INTERSECT have an implicit DISTINCT](https://docs.microsoft.com/en-us/sql/t-sql/language-elements/set-operators-except-and-intersect-transact-sql). The individual component queries will return duplicates, but the EXCEPT set operator eliminates any duplicates. 
In certain cases group by/count( case when productID = x then 1 end) with a having clause (or used as a subquery) will be a faster/easier option.
if you're on 2016 you can use the new 'show histogram' feature: DBCC SHOW_STATISTICS WITH HISTOGRAM, https://docs.microsoft.com/en-us/sql/relational-databases/system-dynamic-management-views/sys-dm-db-stats-histogram-transact-sql If not, well you can do a dynamic script that does select 'table' table_Name, count(*), 'column_name_1', count( distinct column_name_1), ... from [table] group by Table_ID. The easiest way is to just dump the results all selects to the screen (multiple result sets). If you need to unpivot results and store in a table (table name you'll can cross-join back to syscolumns and do case expressions (you know the drill by now, probably).
according to the format FK_table1_table2, your FK should be called FK_RaceEntry_Member, not FK_TRANum
Backwards compatibility is the reason so the syntax works either way. 
Would this mean that if the TRAnum data changed in its parent table it would stay the same in this one? 
you should google the ON UPDATE options -- * RESTRICT * CASCADE * SET NULL * NO ACTION * SET DEFAULT
The parenthesis are required for INSERT, UPDATE and DELETE so I wouldn't say they are unnecessary.
 WHERE Genre.GenreName IN ('Comedy', 'Slice of Life') Could also be done a number of other ways like using a (variable for input) depending on the situation.
I agree and would also recommend a look into the topic "SQL injections".
&gt; Also, I would not recommend incrementing the id values as part of the front end. The database should be doing that in most cases. At least in my experience. I agree and would also recommend a look into the topic "SQL injections".
Perhaps UNPIVOT, then GROUP BY to get a COUNT and COUNT DISTINCT. 
I have no comment other than to the level of zen that this discussion has, especially if parameterized. It is so applicable or often becomes so apparent in hindsight... 1) "&lt;X&gt; madness **needs** to be unique" (emphasis mine) 2) "It's not what you call &lt;Y&gt; that matters, it's how it's defined" 
Just as you have written.
Why UNPIVOT? Genuinely curious. In my head I was thinking of inserting them into a #table with an ID, so if the first column is 10 distincts with a count() then there would be 10 1's, if the second column is 4 distincts with a count() then it would be four 2, etc.
While we're talking about SQL injection, this code is **wide open** to it even ignoring the incrementing of IDs. Concatenating strings for queries is a quick way to get compromised.
So I am unable to ping the to SQL from the VM, it sends 4 packets but returns nothing. Next step I guess is taking a look at the fire wall, the thing that is weird is there is another site that I am piggy backing off of that is able to get data from the SQL database, but the person who developed this no longer works here and left on bad terms so I can't even reach out to him.
Yep, parameterized queries should definitely be used here. Would probably fix the error the OP is getting just by doing that refactor.
It sounds like the SQL server, server is fine since you were able to access while building. I would look at the vm server and start troubleshooting the networking till you can get a ping. My first though is maybe they are on different subnets. 
Even if it doesn't fix the problem, it'll likely make it easier to debug.
They are on different subnets, I guess I should of explained a little better. The computer that has SQL on it can only be accessed internally of our network, it is not open to the public, The VM that has IIS on it, is on the public network. 
&gt; should of Did you mean should've? -------------------------------------- I am a bot account.
:( networking is one my weakist skills. I would either try and and add the vm to the same subnet if the site is only to be used internally or start googling how to make subnets talk. If you get it let me know and I'll keep helping with anything I can after that.
This is great help! TY so much its working now. 
Amazing, I feel like the tutor has led me down the garden path here. 
I would appreciate it very much if you would do this!
Actually NOT a homework! Just trying to learn SQL ... Thanks for the link I will check it out!
Then wash your bowl.
Awesome, it’s in my cart. 
 SELECT SUM(*) AS AnimeCount FROM (SELECT Genre.GenreName as Genre, COUNT(*) AS AnimeCount FROM Anime INNER JOIN Anime_Genre ON Anime.AnimeId = Anime_Genre.AnimeId INNER JOIN Genre ON Anime_Genre.GenreId = Genre.GenreId GROUP BY Genre.GenreName) as genrename_animecount WHERE Genre = 'Comedy' AND Genre = 'Slice of Life' Pretty sure this will work. Just take your whole query and use the counts as it's own table. CTEs look nicer though
This will give you the list. You can wrap it in a query to do a count if you really just want a number back. Also, this will return movies that have Comedy and Slice of Live and others. If you want movies with those two and only those two, you need to do extra work. select a.animeid from anime a inner join anime_genre ag on ag.animeid=a.animeid inner join genre g on g.genreid=ag.genreid where Genre = 'Comedy' or Genre = 'Slice of Life' group by a.animeid having count(g.genreid)&gt;=2
or (because why not, you're learning, so might as well show you some things) ;WITH Comedies AS ( select a.animeid from anime a inner join anime_genre ag on ag.animeid=a.animeid inner join genre g on g.genreid=ag.genreid where Genre = 'Comedy' ), SlicesOfLife AS ( select a.animeid from anime a inner join anime_genre ag on ag.animeid=a.animeid inner join genre g on g.genreid=ag.genreid where Genre = 'Slice of Life' ) SELECT COUNT(*) FROM Comedies c INNER JOIN SlicesOfLife s ON c.AnimeId=s.AnimeId
Lets say I wanted to scan 5 tables for a string to see in which columns distinct values exist like '%string%', each row has multiple columns and potentially millions of rows. I imagine primitively that I could limit the returns using a `HAVING COUNT(*) &gt; n` where n represents some specific value, or a % of the total row count per table. Maybe this would be some kind of function? I have a table of values with (2) columns such as: | ID | String | | :--- | :--- | | 1 | blah | | 2 | habl | | 3 | able | And then the process does a systematic group by to determine the column names and tables where those strings are found?
&gt; At least in MS SQL. does TOP work anywhere else? i mean, other than MS Access?
I took a look at the pastebin and just needed to make sure this was noted. OP, this is crucial if you are doing this for a business.
Did someone smash the SPNS? Agent Jobs run under the SQL Server AGENT service account. SSMS runs as "you"
also, post this on /SQLServer 
Select Distinct? 
Ugh. MS SQL is not the best environment to do "like '%x%'" in. Furthermore I don't think "having count(*)&gt;n" short-circuits the aggregate calculation either. You could hand-write a sample statement on one of your tables to see what kind of plan are you getting. Statistics/histogram gathering is a relatively intensive process even as a system process, that's why I suggested to substitute it by doing the count(distinct...) - if you're getting like 6 values, obviously you'll have at least one value that covers at least 15% of the table.
do you not want any Fiji Apples in the table? If you want to remove all Fiji Apples because they appear twice, you should be looking at the Apples instead of Apple_ID. delete from table where apple_id not in ( select top 1 a_id from apple_id as a_id.... (too lazy to type the rest) );
Honestly I could care less how much it taxes the serve. It's *my* serve. It exists for me to run things on it.
You also need to use parens if you're using a variable for the count: select top (@count) from table We've done that at work for report datasets with multiple options for the number of results.
Ok, I'd still go with "count( distinct case when [column_name_1] like '%xxx%' then [column_name_1] end) as [column_name_1_XXX]" at first unless you really really need the histograms. My guesstimate is that overall time will be better if you stuff as many of these as possible in a single statement (might not be possible always though if you have too many columns/strings to search for).
Don't you need to have matching values in your group by as your select statement in your derived table? Do you not get an error when you run your code? And if you took the id out to get it to run, then you wouldn't have an id to delete only one record from. Or am I misreading your code...?
Can't you set the SQL server agent account to run as a AD account instead of the built in? 
Thanks for the constructive input anyway - I think the solution you mentioned is really the way I am going to have to go afterall...my sql knowledge is really only serviceable as you could probably guess :)
Just wether you want to follow ansi standards or not there. Also if you haven't played with it yet, in addition to records you can also do percent which is neat in some scenarios. Link below with the syntax. https://docs.microsoft.com/en-us/sql/t-sql/queries/select-transact-sql
What user is the linked server itself using?
Is apple_id not your Pkey? If you've got 3 entries where apple_name = "Fiji Apples" then don't you want "Delete...Where Apple_id not in (Select top 1 apple_id from Alias where apple_name = "Fiji Apples")? Because if apple_id is unique, obviously shit ain't gonna match. Though, if the names aren't an exact match things can get shitty. 
[removed]
that is correct. and if your sql agent service is running as builtIn, your permissions is now limited to that account. create a ad account, properly change the owner, if necessary, set up proxy account for permissions sorry about your vaca.
Sure, I mean, there's lots of ways to do this.
This may be helpful: https://sqljana.wordpress.com/2017/06/16/sql-server-curse-of-linked-server-security-and-the-fix-pass-through-authentication/ Though i think it would require the account connecting to server 1 to also have a login/access to server 2, and it sounds like that may not be the case. I could see sql agent causing a double hop if is running under different credentials, just like running from a third client separate from the two servers. You could try running it with the credentials of the service and maybe that'd avoid it. Or, maybe try running your routines in a sproc with ececute as? I've never really found a solution myself, and just kind of gave up on linked servers except in very special circumstances (mainly querying SSAS OLAP from TSQL). 
fyi TOP doesn't work in MySQL
fyi TOP doesn't work in MySQL 
&gt; The problem with this code is that it only deletes one duplicate. if you have `GROUP BY apples` along with `SELECT apple_id` the *of course* only one apple_id will be returned
yeah, that's how MySQL handles non-standard grouping see https://dev.mysql.com/doc/refman/5.7/en/group-by-handling.html
#1. Work with your Windows team to give you an application ID who's password never expires AND that you know. You will be held accountable for this user id and password, so do NOT let it slip out or pass it out in any digital format. But you need this! #2. Create an AD group for BI Admin and add yourself, this ID and at least one other backup into this group. Then manage security in both servers (and any others) using thus AD group. This ensures that your production processing has the same access as the ID you develop on and eliminates these types of gotoprod production issues. #3 Set up SSIS server so jobs can run as the generic ID. I am on Mobile, and I am forgetting the steps. You will do this once each time you set up a new SSIS server, so expect you will forget how this works too. #4 Configure each job to 'run as' this generic user (never SA). You should be good at this point and only have any other issues when you connect to a system that does not use integrated AD security. A note- I have been in this situation hundresds of times before. Linked servers work, but can result in unacceptable performance issues on either the BI or Application DB. Queries can take too long to run for BI reports, or they can slow down the production system. Please consider working with the DBA's of the production system to set up database replication to your BI server. TONS of advantagers of this. 1- You will never impact the app db. 2- You can do real time BI streaming. 3- You can easily tune the BI db for BI performance that may be different than the operation db. Good Luck, and may the Force be with you.
Idk wtf happened to the txt in that post.... Sorry. I know DB. I dont know what ever markup lang reddit uses for posts. 
MySQL is such a party pooper sometimes. 
As others have mentioned, I think you should use a subquery that returns 1 id from apple category matching the row, and use WHERE to filter out any rows that don't match those ids. In the subquery, if TOP doesn't work, try OFFSET FETCH. ORDER BY apples OFFSET 0 ROW FETCH FIRST 1 ROW ONLY Here is a solution that worked for me DELETE schema.table FROM schema.table AS A WHERE apple_id NOT IN ( SELECT B.apple_id FROM schema.table AS B WHERE B.apples = A.apples ORDER BY B.apples OFFSET 0 ROW FETCH FIRST 1 ROW ONLY ); 
I would do this: DELETE FROM schema.table WHERE apple_id NOT IN ( SELECT a_id FROM ( SELECT MIN(apple_id) AS a_id FROM schema.table WHERE apple_id IS NOT NULL GROUP BY apples ) AS A ) Instead of picking all the records you want to delete, pick the records you want to keep and delete the rest. This picks one `apple_id` for each `apples`. `WHERE apple_id IS NOT NULL` is probably not necessary, but I've included it due to how NOT IN works with NULL. Also, `apple_id` is clearly not functionally dependent on `apples`. Don't do this: SELECT apple_id AS a_id, apples AS apps FROM schema.table GROUP BY apples HAVING COUNT(*) &gt; 1 Do something like this: SELECT MAX(apple_id) AS a_id, apples AS apps FROM schema.table GROUP BY apples HAVING COUNT(*) &gt; 1 Or *at least* use the MySQL-specific aggregate function that explicitly tells MySQL to pick whatever it wants: SELECT ANY_VALUE(apple_id) AS a_id, apples AS apps FROM schema.table GROUP BY apples HAVING COUNT(*) &gt; 1 Determinism is important! Most RDBMSs don't let you get away with ignoring it. MySQL shouldn't, either, but I've been on my soap box long enough. 
1. markdown 2. you can click the *formatting help* button to get a cheat sheet 3. the **big editor** button also helps 4. backslash to escape ¯\\\_(ツ)_/¯ 
I’m more than willing to help if you need it. 
What's your sql statement? It's missing "exec" if that's a stored proc
So I originally had "EXEC" in the query but it gave me a different error saying "The EXEC SQL construct or statement is not support". Here's my parameter mapping and variables: parameter mapping: https://imgur.com/a/UnQ0c variables: https://imgur.com/a/A0tlt 
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/prcq8Vo.png** **https://i.imgur.com/W8NOHQF.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20drc8v3s) 
Kendra Little had a good episode (although short) on interviews. Her website (SQLWorkbooks.com) is also a good resource. https://itunes.apple.com/us/podcast/dear-sql-dba/id1117507864?mt=2&amp;i=1000391155143 The BrentOzar team also has a great learning resource: GroupBy.org (click on past sessions)
The advice I always give on here for real world problems is to go over to stack exchange. And have a look through some of the questions. You get to see the problems people encounter and accepted answers usually include a thought process the how and the why. Good luck with your studies 
BREAK ON table_name SELECT table_name, column_name FROM user_tab_columns; What what I used.
I can't speak for every position and hiring person out there, but they're not going to grill you on the nitty gritty of SQL unless it's for a very specific position or a higher level management position where you might be expected to review others work. I would focus on the theory. Know how table relations work. Know about table keys. Column types, what the limitations of said types are (INT versus Float). All of my sql interviews asked me how I would go about joining tables and the generality of the syntax. Know in particular the difference between using sub queries and table joins. Know how you can limit joins in the join itself. Know how to join a table to itself using an alias. Know what aliases are and how to use them. So much of using SQL at a given company is on the job learning. SQL is a bit like a finger print, it's unique to people and organizations. They want to know you understand the theory and can learn the way they do things and thrive in their workplace. I would recommend looking at Glassdoor for reviews on their interview processes and seem of you can glean anything from that. Overall I recommend knowing SQL in theory, what it's capable of and being able to speak to it over knowing the exact syntax for a given scenario.
Yeap. What taylorwmj said. PL/SQL isn't a variant of SQL. Its procedural language that uses and interfaces with SQL. It is basically a domain specific programming language that lives inside Oracle database -- not ONLY Oracle db though. IBM DB2 uses PL/SQL as does MariaDB and PostgreSQL (EnterpriseDB version that is). Don't let quick descriptions of PL/SQL fool you. For starters, PL/SQL is Turing complete. I underestimated it too until I started using it on the job daily. The rabbit hole goes pretty deep. You would be surprised how rich the language is and how extensively it can be used. You can make REST API calls, read and write to files, etc. with it. People that use PL/SQL love it. In my experience, by comparison T-SQL doesn't have much more depth than you'd initially imagine. On a side note: Tis important to mention that Oracle and MSSQL offer support for other languages as procedural languages inside the DB... SQL Server -&gt; Python and R. Oracle -&gt; Java, R, JS (via Nashorn in the JVM). 
Just wanted to update everyone that I passed today. Thanks for the advice! On to the 762...
[removed]
SQL studio should be a free download from Microsoft. 
Do you have the direct link?
A five second trip to google.com gave me this: https://docs.microsoft.com/en-us/sql/sql-operations-studio/download
Fantastic! When I googled "SQL Studio Microsoft", I got the following result: Download SQL Server Management Studio (SSMS) What is the difference between Studio and SSMS?
Nothing.
Google/download sql server express version for free. Including ssms among the other essential tools. Get a demo database like adventureworks to play around with. Any version will by fine (2008 and up), no major differences if you just started.
On that point I think most of my SQL (Postgres) problems have been solved by answers from one of these few users - their post history would be a good place to browse: https://stackoverflow.com/users/939860/erwin-brandstetter https://stackoverflow.com/users/330315/a-horse-with-no-name https://stackoverflow.com/users/479863/mu-is-too-short https://stackoverflow.com/users/398670/craig-ringer
Good thoughts, but I cannot use SSIS in this instance because of the nature of the jobs. It works flawlessly (without performance issues) as Sprocs run with an interactive session - it just doesn't work as a scheduled job.
* be scared of doing a thing * do the thing anyway We believe in you!
any transition from Access into SQL is going to be good for you. Be aware that the code that Access formats and generates isn't the greatest. you have to purge how it works so you can get focused into SQL.
Developer editions are available for 2016/2017 and provides access to the full enterprise stack. (Granted you may never get to use those features, but it's nice to have them so you can learn what they do).
If the idea is always to retrieve B in context of A, just put in an index on your join column in table B, and when you search for a particular A record, the query engine just does a seek right to where the B records are stored. Takes no time at all even with millions of records. You could also consider a NoSQL option like DocumentDB, HBase, Cassandra, etc. If B records actually are serializable objects that are all retrieved Everytime you pull the parent. But this all assumes you want every B associated with an A every time. Also what if you want to run analytics on the B records? Or modify their schema? Or add auditing columns? Having them in a traditional data table will make this much easier.
This is fantastic advice. I've gone through five or six SQL / BI interviews, and this has always been the case. I spent hours studying the night before, going over all the syntax, the keywords, etc. Then I get to the interview and they hit me with something related to theory, not syntax, and I'm sinking. I don't know this for sure, but I'm pretty sure most companies nowadays are comfortable with the notion that employees may know the theory, and will look up the syntax if they're stuck. The opposite doesn't work so well. If you learn the theory, you'll be alright. 
a ward can have only one nurse? a patient can be in only one ward at a time, that makes sense... but if the patient is moved to another ward, can he still use the same doctor? why is patient_has_doctor relationship dependent on ward?
Sorry, I should have included this to explain it better, it's the actual assignment for the project: &gt;*You have been asked to design a database to maintain information for a new hospital called “Care Fare”. The hospital needs to maintain information about its admin staff, doctors and nurses as well as recording information about patients that attend the hospital. Patients are assigned to a single ward (room). A doctor is assigned to one or more patients and each patient may have more than one doctor, although some patients may not have any doctor assigned yet (new admissions). A nurse can be assigned to more than one ward but each ward has only one nurse.* &gt;*The administrators in the hospital are responsible for ordering medical products from the suppliers. Purchase orders are stored that include the staff member that made the order, the supplier and the date that the order was made.* &gt;*For the admin staff, doctors and nurses, the hospital needs to store their names, addresses, PPS numbers and phone numbers. The details that need to be stored for a patient include their name, address, date of birth and the name of their insurance company.* So, to answer your questions, yes, we're specifically told each ward only has one nurse. For patients - they can be assigned to many doctors or none, and doctors can also be assigned to many patients. They're independant of the Ward. I admit, I'm not 100% sure why the Patients_has_Doctor relationship is dependant on Ward - This is in MySQL Workbench, and when I added the m:n relationship between Doctors and Patients, the new entity took on both the Primary and Foreign keys from Patients. 
okay, just remove the ward from Patients_has_Doctors
Quite new to Sql myself, but I'm wondering why your date of birth fields are varchar and not a date type? Also you could put address in a different table, might not be necessary but could be useful (for example if you are to then load in an official adress list). Our systems at work also spilt address into address line 1, 2, 3 4 and postcode. Names into title, forname, surname, middle initials. Again might not be needed for your project but worth considering maybe? 
I must have missed the MySQL tag and was in MSSql mode. Good to know though. Thanks
Thanks: Yeah, our lecturer hasn't really gone into too much details about the different datatypes for the fields, he just said a Date should not be an Int, so I used the Varchar for that one. I agree on the address lines 1, 2, 3, and names, I'll change that. I thought about moving Address to a different table, but if I do that, I'll need to add a Foreign Key for *every* table I move it from, right? (So, Foreign Keys for Patients, Nurses, Doctors and AdminStaff?) I hadn't thought of using one staff table - could that work though? I already have the Patients linked to the Doctors with an m:n relation, so let's say if I combine all the staff into one table, I'm not sure how I can then combine that full table to the patients, but only specifying the Doctor-Patient relationship. 
Got it, thanks. 
In case anyone is interested in the solution, stragg function became my salvation. http://www.sqlsnippets.com/en/topic-11591.html
&gt; I thought about moving Address to a different table, but if I do that, I'll need to add a Foreign Key for every table I move it from, right? More likely you'd add addressID as a foreign key in the table you move the address from. 
I haven't used it much, but honestly this almost sounds like a graph database. Have you looked at neo4j?
I leave the server name that is visible, choose sql server auth, sa, password and the object explorer stays empty. No error is generated The same thing but with domain account and windows auth works nothing else is there when I open connect to database, only the local server name 
Look up shrinking the t logs, I'll give more details in a bit when I'm not on mobile. But after you take a full backup and a tlog backup, you need to run a shrink on the log files in question if you want to make them smaller. 
Indexed lookups are very fast -- O(LOG N). It's fast even with billions of rows. Unless you're doing something extreme that you haven't mentioned, millions of rows should not be an issue. You have KeyA as the primary key in Table A. You store KeyA in an indexed column in Table B. You query for a given KeyA in Table A and INNER JOIN to Table B. It's a single O(LOG N) lookup to find all records with KeyA in Table B. 
Lol, Always. 
You need to backup the transaction log frequently: every 5 minutes, or every 15, 20 , 30, 60 minutes..... depending on your SLA and recovery needs and DB activity. AND you need to backup that transaction log to a different location, not the sql server's d:\ drive. (For example) You need to review where your backups are stored, and you need to Test YOUR BACKUPS WITH RESTORES AND RESTORES to Point In Time. Shrinking is rarely necessary. also: this https://www.mssqltips.com/sqlservertutorial/6/types-of-sql-server-backups/
You want to install the SSMS (giu) and the engine (service) so you have something to point to. 
Solid recommendations. OP needs to shrink currently because their T-log sucked up all the space so that will have to happen. Their T-Log is way bigger than their DB too, so I suspect they haven't had regular backups and it caught up or there was a run away long transaction that filled it. They really need an on site SQL person or at least temporary consultant to get things set up for them.
 declare @temp table (id int) declare @temp2 table (id2 int) insert into @temp values(1),(2),(3) insert into @temp2 values (2), (3), (4) select * from @temp select * from @temp2 select t1.id from @temp as t1 left join @temp2 as t2 on t1.id = t2.id2 where t2.id2 is null
Hi and welcome to the world of being an "accidental DBA".... To dumb things down a little, SQL generally uses a Need more space? -&gt; Take a bunch more than it needs -&gt; holds that space forever until a DBA tells it to shrink. To get an idea on how this works for the log, Say the log file is 2GB big, that doesn't mean there is 2GB of logs just means that it has that much space for logs. Now, say for example 1.99GB is actually used so there is 0.01GB free for more logs. So more transactions mean more logs and it will essentially need to add 0.02GB of logs so the log data will be 2.01GB in total. What SQL does is when the log needs go above what the log files provides (2GB) it does an autogrowth actions. Someone can/ should look at the settings, but it is likely set to grow with no limit and I think does 10% growth by default. So what would happen is the log file would grow 10% to be 2.2GB big, and now there are 2.01GB of logs and 0.19GB free until the next time the limit is met. NOW TO BACKUPS: For log backups that will backup and clear the current log data, but do nothing to the log file size. So, right now the log is 2.01GB and the file is 2.2GB big, so about 90.5% used. If a log backup is done that 2.01GB of log data is backed up and not in the actual log file anymore, so the 2.2GB log file stays but is going to be nearly fully free. If done regularly you might see by the next backup 1.7GB of 2.2GB was used and then free'd, next time 1.4GB of 2.2GB was used and then free'd, and so on so the 2.2GB might be log file size that it stays at. FOR THE ERROR: You likely had no regular log backups so you had a 310GB log file and when it got to be 100% it tried to autogrow, probably by 31GB but you didn't have 31GB of space on the drive so it couldn't grow, so since it was 100% no new transactions could be made/ logged. When you did a log backup it cleared up that huge log file so likely that fixed the temporary problem and you wouldn't have had an issue until it became full again, but doing a shrink was still a good idea given the situation. Like others have said, this behavior is because the database is in FULL recovery mode and your options are either. Have the database as SIMPLE so it doesn't keep logs of past transactions. In truth you would have as much recovery protection as having it in FULL and NOT doing log backups, but without the headache of crashing the system because the logs became full. Option 2 are setting up log backup and if done right would be any DBA's recommendation for recovery. Hope that helps.
This is super helpful and provides me quite some clarity however the TRAnum does not exist in the race table. So I may need to join more tables as the Member table has the TRA num, The race table has raceIDs (these are what I want to return if a runner is not registered on one) and then finally the runners are registered onto the race via a race entry table. 
You need a way to connect your tables together as you are stating. This solution is still fine with multiple joins but it does become slightly more complex. 
no just want to keep one fiji apple :)
Thanks for the lesson! I'm brand new to writing SQL (perhaps obviously). &gt;I would do this: Solution verified! I learned a good bit from your post - I see why using the min/max functions was needed.
Ah, okay - so that would mean *each* of the original tables (Doctors, Patients, etc.) should then have an AddressID Foreign Key, is that correct?
*beep beep* Hi, I'm JobsHelperBot, your friendly neighborhood jobs helper bot! My job in life is to help you with your job search but I'm just 91.8 days old and I'm still learning, so please tell me if I screw up. *boop* It looks like you're asking about resume advice. I'm ~36% sure of this; let me know if I'm wrong! Have you checked out Forbes, /r/resumes, TIME? They've got some great resources: * https://www.forbes.com/sites/trudysteinfeld/2012/06/06/the-only-resume-advice-youll-ever-need/#6da3cf455d27 * https://www.reddit.com/r/resumes/ * http://time.com/money/4621066/free-resume-word-template-2017/
Yes.
Great, thanks. 
Normalization is not about efficiency. It's about avoiding update anomalies 
A Doctor is a role played by an individual. An Individual could be a Doctor and a Patient at the same time (i.e. a doctor gets sick)
[Here is what mine looks like.](https://www.dropbox.com/s/6svhnikyc0c1h5q/Generic%20Resume.pdf?dl=0) As someone who is responsible for hiring developers (previously and currently), some tips: * Your resume should be 2 pages max. Going over that is only suitable for someone applying for executive or higher positions * Your resume should be easy on the eyes. Crap like Yellow text on a white background (seen it), etc. will turn me off and not make me read it * I don't care about job duties. I know what a DBA does, or what a developer specializing in XYZ framework does. * List accomplishments, examples of large projects and your role, etc. (see my example). That is what sets you apart.
Really good insight, thank you! I feel like I have a good eye for resumes, I am not great at crafting them however. &gt; Your resume should be easy on the eyes. Crap like Yellow text on a white background (seen it), etc. will turn me off and not make me read it My favorite one receiving, it was 10 pages and he physically highlighted with a highlighter every single word.
I could tell this is 2017 when I read "to which gender do you most identify?" 
Remember to fudge your salary as high as possible, so when HR depts search for average salaries in your position it will be skewed upward.
Example of line inconsistency from last two attempts: 1) lines 6037, 6038, 9942, and 10074 2) lines 343, 344, 345, 346
Are there enabled triggers on the table? A trigger will override the value you're trying to insert and may cause this error.
[It's pretty easy really](https://www.youtube.com/watch?v=ddPQAJSm2cQ).
A that's basically our solution. Except I use Outlook instead of tape. 
Easiest might be to count the distinct id's from each table (assuming these exist): SELECT T2.date, T2.product_line, count (distinct t1.tableID), count(distinct t2.tableID) FROM leads T2 LEFT JOIN sales T1 ON T1.product_line = T2.product_line AND T1.date = T2.date GROUP BY T2.date, T2.product_line But were you aware that you're only getting dates where you've got leads? If you've got a day with sales but no leads for a particular product your query is excluding it.
God forbid a survey attempt to be more inclusive to align more with changing societal norms. 
&gt; Not sure what's going on look at it like this -- your FROM clause produces an intermediate table, consisting of rows and columns, constructed by joining your tables together, and your two COUNTs operate on the intermediate table, so unless some of the values are NULL, they're always going to produce the same count!
I sexually identify as an attack helicopter but the survey only recognizes two gender types. 
Thank you for the response. We're doing backups every hour, so "hopefully" it's something going rogue most of the time. When I shrink, I can release all unused space? Main reason I wrote this thread is simply that i've read a lot of people who warn/advise against shrinking the logfile, saying everything from breaking the logchain to impacting performance. &gt;Learn what size you need your t-logs to be at. Put monitoring in place in case it grows too much or something is causing long transactions. Learn how to size the t-logs appropriately so you don't have page splits / over abundance of vlfs. We have monitoring if the disk goes full, but not sure how track the growth and know when it's something that should happen or when it's gone rogue. We have hundreds of databases split over many servers and we have 0 clue what they are used for or any activities from the application teams.
I'm about to go through the same ordeal. My plan is to baseline where everything is now, and then set the queries for the servers at a high, but passable threshold. For example, if it's not critical that I devote my attention to it right now, then it shouldn't be red. There has to be a level for things to go up from, if it's business as usual. The thresholds can then be reduced as you repair things. If it's red, it should be hitting your email and your sms and all that. Here's [a video](https://www.youtube.com/watch?v=imbASrwO2vU) I've found on actually doing it, and there's a follow up on actioning them.
About the best you can do is use the foreign key in B to A as the first part of the clustered key.
Sssshhhhhh!
Formatting got lost in translation, but here's the guts of mine: General advice, reach out to a head hunter / recruiter, even if they don't place you, they'll help with your specific resume. your name address phone and email: OBJECTIVE To become an ETL developer that will allow of leveraging for current skill set to contribute in a fast-paced, deadline-driven environment. EDUCATION University of Minnesota Bachelor of Arts Major: Criminology Minor: History EXPERIENCE ETL BI Developer, Company City, State September 2015 – Current Used Microsoft SQL Server 2008 R2, Visual Studio 2008 Developed ETL Processes using SSIS Wrote T-SQL to pull extracts, load data between databases, update databases Developed and modified Stored Procedures Troubleshot and resolved load issues Fixed a balancing stored procedure that hadn't worked correctly since it was created 5 years before Designed and Implemented Process to calculate commissions from legacy system to new SAP database Altered source views to keep legacy system working when source database schema changed Database Analyst, Company City, State August 2013 – September 2015 Used Microsoft SQL Server 2008 R2, MS SQL Server 2012, Visual Studio 2008, Visual Studio 2012 Wrote T-SQL to pull extracts, load data between databases, update databases Developed and modified SSIS packages to extract and deliver positive pay files, updated production databases with updated check information Developed standard script to track use of a NCOA tool for billing Loaded, and cleaned data into and between databases Wrote and updated functions Extracted data in from of check files, mail files, various extracts/reports Worked on high profile cases including: Case Name (and many similar settlements) Wrote the programming that calculated award amounts for a $35,000,000 settlement case. (case name) Worked closely with business analysts and the project team to solve a problem of issuing letters from one database with name and address information on a non linked server's database Solved the problem of how to associate records across a project containing 14 different production databases and 4 servers. Investigator / Data Analyst, Company City, State Dec 2006 - August 2013 Acted as a liaison to the IT department Wrote and modified Stored Procedures which were used to generate reports Wrote and modified SQL queries to mine data Used Microsoft SQL Server 2005, Access 2007, Sybase SQL, Excel Assisted in the design of a new database Prepared new data for import and imported data into the database 
You definitely have to shrink, it will release unused space. This has to occur once a full and transaction log backup has occurred. When I'm not on mobile, I'll see what I can do to help you out. 
Last week I got called "shallow" because I said I wasn't into penises. I think we may have taken the "gender preferences" thing a bit too far.
This was changed in 1992, everyone that still writes it in the where is a dinosaur. There is no advantage to the old methodology, sometimes it performs worse because it will join as a cartesian product first and then filter the result set on the WHERE.
[Track long running queries alert](http://www.sqlservergeeks.com/sql-server-configure-alerts-for-long-running-queries/) [Track and alert t-log space](http://www.sqlservergeeks.com/sql-server-monitoring-transaction-log-size-with-email-alerts/) [Monitoring disk space alert](https://www.mssqltips.com/sqlservertip/2929/monitoring-sql-server-disk-space/) [How to find out what is running and when](http://whoisactive.com/) For whoisactive, log it to a table. There are instructions on the sites for all of the queries. The first two are more necessary, the disk space one is a good one to have but I would hope you have tools out there to monitor them. The whoisactive will get you info on your instances and what's happening. Now to manage all of your servers and things at once, you should look at [dbatools](https://dbatools.io/functions/). That can help you manage more servers at once. 
It's also harder to read and things can be missed easier / cause headaches.
Good example and advice, thank you very much!
Right - I sat through a presentation where a guy spent an hour doing some very detailed and clever traces to figure out a problem that ended up being due to a bad where clause. It had about 10 tables and a dozen where clauses. He wasn’t impressed when I pointed out that using a modern join syntax would have avoided the problem in the first place!
&gt; He wasn’t impressed I would have told him I wasn't impressed by his lack of adhering to the current ANSI standards. 
1. How to come up with a running total. 2. How to parse a GUID from a URL and then join to another dataset using it. 3. How to UNION two different datasources together into something meaningful where the columns don't necessarily line up and NULL's need to be used as place holders. 4. How to use a CROSS JOIN to come up with a date axis that has null values for graphing purposes. 5. How to use PIVOT in order to dump data directly into Excel for consumption by end users. 
I like those, but I have a problem with giving an interviewee an assignment that can be googled and have answers written in. I'd rather give them a filtered or redacted real world scenario the company faces that integrates learning and practice. Example: Word problem about the needs and requirements of a customer table, then send the column types and fake sample data set. Ask for how they could improve and what pitfalls may be encountered when querying against it. Example solution: If columns allow nulls, do they have proper error handling? Are they using where exists or in to handle the nulls? Do their queries use implicit conversions? Are they able to say why a default value may be a bad idea or a good idea? What is the impact of doing defaults instead of nulls? Etc. That's the kind of critical thinking I'd like to see from an analyst for a homework assignment. One thing I've seen before, is they do questions similar to what you said. But it's a timed test and you can't retake it, then your answers are submitted to them and scored. This eliminates the ability to google the answers, at least quickly, because they purposely give you less time than most people would need.
Welcome. I was going to pm it, but I figured it may help some others by posting it. I redacted some stuff, but I think the guts remain. Also it's a bit stale. I really should put some time into updating it, even though I'm not looking. It's easier to think of resume with projects while working on them.
I'm still struggling to shorten my resume, I don't want to exclude things that I think put me above the rest of the folks but I don't want to include too much. I shortened it to top 5 things per job and the technology I have experience with / education. I put the tidbit I posted on my site instead and figured if they want more detailed info beyond my resume, my website is the place for them to go. Which also has my linkedin, stack overflow, etc. 
I agree, and never said that I thought homework was the best strategy, HOWEVER, I do think it's fair to let a candidate Google if you're going to expect the right functioning answer and not just some loose examples. For example I have parsed lots of values, but I'm hard pressed to being able to sit down and write a substring function or a dynamic charindex or something and having it be spot on. I could probably write some framework code indicating how I would approach it, and what I would Google for, but I've never been very concerned with memorizing things I can access so easily on the job. I do like your idea of giving them questions based on the real world, however I think the answers need to be specific to an analyst role and not a DBA. For example you might hate me for using a charindex and a substring to get the job done, but if it gets the job done then I'm undoubtedly qualified for the position of an analyst, which shouldn't (in terms of philosophy) be as concerned with performance as they are with accuracy and success. Even a running total might take me a few minutes to come up with without Googling, but if you give me a dataset and Google I can come up with that code in less than 5 minutes. Maybe give them the test, time it, and let them Google in front of you?
Good points, I agree that I don't care if they have to google for syntax. I would be looking for that they understand window functions or aggregates. So just like you're saying, if they look up 'substring syntax' vs 'How do I aggregate data sql server stackoverflow', that's what I care about. &gt; be as concerned with performance as they are with accuracy and success. I agree for an analyst. Accuracy should be first priority, but I'd also like to look for them knowing relational theory. They don't need to differentiate every normalization or know every schema. I'd just like them to have the basics down and know how to work with it, since it's intermediate. Senior level I'd drill on those types and use cases and also dive deep into theory.
&gt; or are there other advantages of the old methodology that I am missing? there are not
I got started with ANSI92 joins then "regressed" to 89 style for most queries. I write queries on an ERP with 4000 tables, and since everything is highly normalized, a simple query might involve 10 tables, and I have to add tables as I build out the query. I find that the 89 style is simple, straight-forward, involves less syntax, and switching to a left/right joins is as simple as adding "(+)", at least in Oracle. Most seasoned Oracle employees use 89 syntax as well. Many of them are not dinosaurs. But also it's not like young teenagers are working for Oracle either. Most folks have 4-8 years of education after high school plus a few years of job experience. But following more seasoned Oracle folks, I guess that's how I crossed over to the dark side. I feel like ANSI92 is a more formal approach, and if I know that it's code that I'm going to write and distribute for use, I'll use it instead. But if I just need a quick answer from the DB, quicker to bang out an 89 and dump it into Excel and pivot. As far as being able to sort out the tables with 89, for me it isn't a big deal because I have a nice list of tables in the from clause without anything else to clutter it up, and based on what tables I'm seeing, I know how they connect because I've naturally memorized the schema/erd just from working with it for so long. 
&gt; I'm primarily confused because how are there duplicate primary keys in the backup of an existing working database? How does that happen? I may be misunderstanding the problem here, but let's see? If I have a table with a PK column and it has the value 1,2,3 in the column with the PK, and I take a backup of that DB, the table in the backup has 1,2,3. Now if the table is still working and doing things, this may increment. We may now have the values 1,2,3,4,5 in the table. So if we try to insert from the backup to the table, 1,2,3 won't insert because they are duplicates. &gt; I'm further confused because the line number it says it's failing at is inconsistent. Sometimes immediately, sometimes several minutes in. Data in a relational database is considered a set. Set theory states that there is no order to your data. So if your value for the primary key is 1, and you do a select (1) from table You're not guaranteed to return the id of 1, it could be 287, 29493483489355, 2, or 1. I would go further into this, but it can be a deep topic. If you want to know more, you'll want to learn about HEAP, clustered indexes, and pages. Just know that if your statement does not have an ORDER BY at the end of the outermost query, you are not guaranteed to have things happen in the same order. &gt; Then, how do I get this to properly import? I've tried the -f force option as a workaround to get the import to finish, but that doesn't make the site happy (this is for Drupal). Do you want data that has duplicate values? That's how you get duplicate value data. There are constraints in place for a reason. If you really want it though, you'll have to drop the primary key or disable constraints from the table, which unless you understand why you are doing it and want to do it, I strongly recommend you stop and critically analyze what's happening to cause these failures. What I would do: 1. Restore backup, do a where exists or where in or join on the restored and current working table to find if duplicates do exist. 2. Script out the table, it's constraints, and keys. How do things work? 3. Check for triggers.
We also end up with simple queries that involve many tables, but I find it much, much more intuitive to use the newer syntax, because as you bring in each table, it's immediately clear how you're joining to it.
You could phrase the question like an essay question. Describe the things you would Google in order to come up with the correct framework/syntax. At a minimum they would need to know what a substring/charindex/etc are and can just get past that minor point by saying, "I'd google to get this specific syntax and then test functionality to see that it returns expected data." &gt;but I'd also like to look for them knowing relational theory. You could give the applicant a poorly written query with lots of sub-queries or something and ask them to reduce it in terms of efficiency?
&gt; You could give the applicant a poorly written query with lots of sub-queries or something and ask them to reduce it in terms of efficiency? I feel like the opposite? Give them a diagram and have them write the joins to make sure they know how to connect things. I'd throw in a few things later like composite joins, foreign keys with duplicate values on a table, etc. Just to gauge how strong their SQL is. Start basic and ramp it up. 
Also, I think that asking about something like row_number() is over done. I'd rather see a question asking an applicant to dedupe something or only select the max value without joining on max, etc. Goal being for applicant to use it without having to be asked about it.
I would expect an intermediate analyst to be able to improve queries and make them more efficient (which should be the opposite side of the coin of using something costly like a charindex/substring over better methods.) So you can evaluate them on accuracy and ability to actually get the job done + see who knows enough to make something more efficient. Ideally you want someone who knows about both. Keys and whatnot might be good but as "difficult" questions and I'm not sure I'd really use them to evaluate a candidate unless I had two otherwise equal candidates and then I'd pick the one who knew most of the difficult optional questions.
From my experience, Oracle devs think they know best but really don't usually know best and just go by tradition. Ever see the movie *300*? Just like those Oracles. Same concept. There was a post here a few weeks ago where a senior Oracle dev was trying to convince a junior dev that the old syntax was faster (it's not). We told the user to question the senior dev to show them how much faster it is. They could not.
&gt; Goal being for applicant to use it without having to be asked about it. That's exactly what I'd want to see.
Sure, the keys just help you piece things together. As an analyst, you would care about constraints, triggers, and keys because you understand how the data interacts and then connects so you can utilize proper joins. Current place has no documentation on anything, and almost no constraints of foreign keys. Trying to figure out how things connect to join to is a nightmare.
Eh... I've never deal with constraints/triggers/keys before in 4 years unless they were part of tables I was building, and I feel like understanding those things in a process you are creating is different than understanding how they apply to other tables, etc. I feel an analyst should be (ideally) given an environment that is clean and allows them to analyze, and if not, then test their ability to create one without asking them about something you've created. For example in this current process I'm building I have some constraints and keys between tables, but they are not the same keys from the tables I'm segmenting and pulling into #tables to improve performance. I guess I'm just saying that these concepts aren't on my list of things and are fairly simple for someone who knows all the other topics discussed here to learn if they don't know. A better test might be to have a word problem and then ask questions like "which would be more appropriate a distinct key that cannot be repeated, vs one that is repeatable? why?" -- or "which column(s) in the example table would you index before joining to a large data set." -- or "why would you not want to allow duplicate keys in a table?" As an analyst joins should be something implied, not asked about, because an analyst I tended to join all kinds of things that our DBA/architects never anticipated being joined and having to solution these for myself to get the data I wanted. I would expect their ability with joins to be evident in their answers without having to be specifically asked... unless its something niche like a cross join to come up with an axis, or joinable set., etc. Triggers are interesting but I feel like the name "trigger" is complicating something fairly easy that a non-DBA might very much understand the concept of and use, but not be able to answer it in a structured way. For example a job that emails someone when something happens, or inserts a row of data each time something happens, etc.
It's part of Oracle "culture", but just like theological and political views of those who associate with a group of people, "culture" can spread like a cancer even though the thoughts, beliefs, and ideas are not necessarily the ones that are accepted by most or proven by facts. And I don't work for Oracle, but I'm closely tied due to vendor lock-in and thus exposed to the culture and I admit that it does rub off. That's why I maintain both methods of joins. If I'm writing for myself then anything goes. Or if I'm writing it for someone who also has a penchant for old syntax, then I'll roll with it. But if micosoft sql server people are going to see it, I'll save myself the grand inquisition and do things in a way they are used to seeing. I've had quite a few "new" people straight up tell me that table1.key= table2.key syntax is straight up invalid and illegal. But in most cases it's six of one, half dozen of the other because the explain plans and execution times will be the same. Throw in complexity and index considerations and YMMV. 
Not sure which SQL engine you're using, but at least in T-SQL, that's not a thing. Views do not have foreign key constraints the same way tables do. Therefore, you could not do what you are trying to do without making all of your views tables, and adding the FKEY properties to the tables. That being said, you can get the constraint usages from view with this query: SELECT vcu.VIEW_NAME, kcu.CONSTRAINT_NAME FROM INFORMATION_SCHEMA.VIEW_COLUMN_USAGE vcu JOIN INFORMATION_SCHEMA.KEY_COLUMN_USAGE kcu on kcu.TABLE_NAME = vcu.TABLE_NAME AND kcu.COLUMN_NAME = vcu.COLUMN_NAME WHERE vcu.VIEW_NAME = 'ViewName'
If you right click on the view in SSMS you can go to "View Dependencies", which gives you a tree view of the objects the view depends on and the objects that depend on the view. Not quite as fancy as the table designer but it may be sufficient for you.
Fair enough, I've understood your reasons. But as far as performance goes, is there a gain? I have Oracle EBS being implemented in our company and the consultants swear (but offer no proof) it's more performatic. I find it hard to believe. And not to pick on you or anything, why do you think it's better to add the table, then the where clause than doing it in one line like whatever join table x on (x.pk1 = mastertable.fk1)? That's the part I don't understand.
I would recommend looking at [window functions (namely partition)](https://docs.microsoft.com/en-us/sql/t-sql/queries/select-over-clause-transact-sql) instead of GROUP BYs and CASEs. I can check back later to try to give you a good starting example, just I don't have a ton of time right now. Something around select datepart(hour,STUDY_DATE) ,datepart(day,STUDY_DATE) ,'etc' ,Count(*) over (partition by datepart(hh,STUDY_DATE), datepart(day, STUDY_DATE), datepart(month, STUDY_DATE), etc etc ) COUNT(*) over (partition by datepart(hour,STUDY_DATE), etc) will give you an hourly break down by day and hour for each It's also entirely possible I don't understand what it is you're trying to do, so sorry if I gave bad advice. But take a quick glance down this path at least? I hope you get what you need!
Please switch syntax. You're replacement will thank you, or rather your replacement will curse you if you're using the pre 1992 syntax. It's great that you can read it, but it's been obsolete for 25 years. They didn't bother to teach me that syntax when I was in college 15 years ago.
CTE would be good for this. One CTE to calculate the first count, second one to calculate the second, and then combine. Here's a simplistic example using an audit log in my database. The join 1=1 works because I only have one record from CTE, but you'll need to expand your joins to match your data set to avoid cartesian products: WITH FormOpened AS ( SELECT Event, COUNT(Event) AS cnt FROM vDDAL WHERE Event = 'FormOpened' AND DateTime BETWEEN '20171201' AND '20171231' GROUP BY Event ), FormClosed AS ( SELECT Event, COUNT(Event) AS cnt FROM vDDAL WHERE Event = 'FormClosed' AND DateTime BETWEEN '20171201' AND '20171231' GROUP BY Event ) SELECT * FROM FormOpened JOIN FormClosed ON 1 = 1
&gt; ometimes it performs worse because it will join as a cartesian product first and then filter the result set on the WHERE. Are there special circumstances or certain platforms that do this? In MS SQL at least I thought the query optimiser would always treat it equivalent to a join? (not that I want to defend such a terrible habit as this join syntax)
You're doing god's work.
&gt; How to UNION two different datasources together into something meaningful where the columns don't necessarily line up and NULL's need to be used as place holders. How **do** you do this? My gut would be to throw a sample record of each into Excel and move things around until things line up.
I have an MBA from a good school and the career centre drilled this into our heads: show tangible benefits of what you did You just list things you know. Try to list what you did and how that benefitted the organization (not just monetarily)
&gt;then dump the column lists back into a sql query. All the while muttering under my breath about vague assignments and shoddy specs. Hey that was the answer I was looking for! You're an intermediate SQL user! You got the job!
Woo! While I'm on the subject, have any of you even *heard* of normalization? Who came up with these column names? And most importantly, who keeps turning the burner off under a full pot of coffee?!
I tend to default to the old syntax because I had to learn it that way. I had to learn the old syntax because I was working in an environment that had old code, and old coders. If I couldn't read the old syntax easily I wouldn't have lasted. I learned the new syntax at the same time. I tend to default to the older syntax if I'm not thinking about it. This is out of habit - nothing else. I make a conscious effort to use the newer syntax. I feel the need to stress the importance of being able to read and understand either version.
At a very quick glance I'd assume you have authors with more than a single pubdate within the same year. You do a datepart for the year in the select but not your group by.
you only select a part of the "pubdate" but GROUP BY the full pubdate. I assume your data has different dates in that year for each of the books you are expecting to be summarized. Also you are COUNTing the "au_id" per book, which is usually only 1. so you should GROUP BY DATEPART(yyyy, pubdate) and COUNT(titleauthor.title_id)
you only select a part of the "pubdate" but GROUP BY the full pubdate. I assume your data has different dates in that year for each of the books you are expecting to be summarized. Also you are COUNTing the "au_id" per book, which is usually only 1. so you should GROUP BY DATEPART(yyyy, pubdate) and COUNT(titleauthor.title_id)
Seems to have done it ! Thank you !
&gt;There is no advantage to the old methodology, sometimes it performs worse because it will join as a cartesian product first and then filter the result set on the WHERE. I tried to explain this to a manager I support. He still sends me example queries in the old format that I have to translate into an actual, working query of what he wants. And he's *proud* of it, just like he's proud of using a billion tabs to review data in tables instead of just exporting and all tab/col pull. sorry.... this post just got me heated and this comment made my day. 
&gt; why do you think it's better to add the table, then the where clause than doing it in one line like whatever join table x on (x.pk1 = mastertable.fk1) I'm not saying it's better because it is logically equivalent, but consider this. The from clause *includes* data. The where clause *excludes* data. What does a join do? A join generally excludes data. It is the classic example of the intersection of a venn diagram. Therefore, the original design was for joins to be in where clause because inner joins act as an excluder. Of course that is not always the case due to right/left and cases were there is always 1/1 matching/existence, but speaking in general, an inner join can drop records which is why it belongs with the rest of the filters (the where clause). 
Obsolete is just plain incorrect. Deprecated might be a better choice, but it's still supported. There's nothing wrong with the style. Just because 92 introduced a different and equivalent way of doing it, it does not invalidate the original specification. Today's Oracle employees are still joining in the where clause. If Oracle had a problem with it, surely they would enforce 92 standards upon their staff. I think that most people who learned 92 just are not comfortable with 89. I'm totally comfortable with both and a successful SQL person should be able to convert between them with no troubles at all. It's part of having a well-rounded skill set. Instead of saying "ewww please change it I'm not dealing with that!" you should say "maybe now is the time to learn the original method, even though it is not my preference". 
A while back I had to write some software that would run against SQL Server, Oracle, or DB2. DB2 actually used the modern Join syntax at the time, while SQL Server used: SELECT a.field from a, b where a.key =* b.key and Oracle used: SELECT a.field from a, b where a.key = b.key (+) Fun times
Oracle Sr Development DBA here. You’re absolutely right and it’s complete BS. I’m lucky to work on a team that doesn’t fit this stereotype and is with the times. 
Upvotes for everybody!!
We are thought by the first method in our schools and colleges. Even tho left and right join are given in the textbooks.
Homework... or interview questions? Interview questions - keep short and easy to tell if the person has a clue... (how would you do x type of query). For "homework" or something you expect turned in... my suggestion is to keep it open ended, something that you can discuss in a final solution, where there is a big difference between a working solution, and a good fully thought out solution. You are talking about an analyst role - so I would probably stay away from database design, though you might want to ask those questions in an interview to see how familiar with proper database design... But my suggestion is to ask them to build a simple report given an assumed data structure... Reporting and BI often falls under an analyst role, and using it you can create a problem that shows they know how to deal with issues like summing nulls, or pivoting... Be aware that if you give them a task that takes longer than a night or two to do though - many candidates looking for an intermediate position are just going to pass... Developers have lives, and anyone above a junior role already has a job and doesn't want to work for free just for the chance at an interview... (unless you are some one like google)...
https://docs.microsoft.com/en-us/sql/database-engine/configure-windows/connect-to-sql-server-when-system-administrators-are-locked-out
Ok, based on your validation, I think that you're trying to do a single aggregate around 2 different values provided by the same row ((DATEPART(hh, STUDY_DATE) and DATEPART(hh, REP_FINAL_TIMESTAMP)). SQL does not do this directly as it treats different columns as 'distinct' even if their underlying data type is the same. You need to unpivot the columns to rows so you can treat them as the "same" (singular) granularity. There are many ways to unpivot data and since you're on MS SQL I'm going to use cross join/table constructor: select * from report r cross apply ( values( (study_date, 1, 0), (REP_FINAL_TIMESTAMP, 0, 1) )piv(pivot_ts, pivot_study_ind, pivot_final_ind) Now that your data is in the proper granularity, the actual query is simple: select DATEPART(hh, pivot_ts) hour_of_the_day, sum( pivot_study_ind) study_count, sum( pivot_final_ind) final_count from (...)your_pivot group by DATEPART(hh, pivot_ts)
&gt; This was changed in 1992, everyone that still writes it in the WHERE is a dinosaur. Thank you. I think I've said these exact words 50 times. My fucking DBAs do it and it drives me nuts. 
&gt;**Help posts** &gt;If you are a student or just looking for help on your code please do not just post your questions and expect the community to do all the work for you. We will gladly help where we can as long as you post the work you have already done or show that you have attempted to figure it out on your own. You can scan the table again for related Packs, or do something fancy with "SELECT WITH TIES". 
Thanks, What do you mean by scan the table again?
A self-join in this case.
Thanks for the reply. I don't think this solutions would have worked because I know the passwords. It's just that when you tried to log in it was trying to run a trigger to log that a system admin had logged in. If I made anew system admin, it would have fired the misbehaving trigger too. I finally figured it out with help from this article: https://blog.sqlauthority.com/2009/06/27/sql-server-fix-error-17892-logon-failed-for-login-due-to-trigger-execution-changed-database-context-to-master/ I've never used SQLCMD before, but it's pretty handy. 
Why? Knowing why you are trying to accomplish this very well may help a lot. Also, AB =/= BA. Will the instances always be mirrored, or might they be the same some times? Are there actually 15 columns and the data points can occur in any order, or are there really only 2 (and therefore 2 cases to be considered 'duplicate')? How do you define that the 'first' row should be kept, instead of the second one? Row number? Some other data element? What DBMS are you using? I'm deal with identifying duplicates a lot in what I do and these are important questions before trying to recommend a solution.
In the case I am working on AB = BA, so just wanted to gather all records where that is the case. IT doesn't matter which one is displayed and which one is removed, just so long as one is showing, to show that there is an instance where B and A can be together.
Okay, I've put a little more time into thinking about it, and think I have a good solution. Hopefully this is readable. I assumed the possibility of records inserted before "received" and before "completed" with null checks. select tableReceived.dateHour ,tableReceived.receivedCount ,tableCompleted.completedCount from (select cast(Study_date as date) as Study_date ,datepart(hour, Study_date) as dateHour ,count(*) over (partition by datepart(hour, Study_date)) as receivedCount from [table] where cast(Study_date as date) = '12/13/2017' and --desired date check Study_date is not null ) as tableReceived cross apply( select datepart(hour, REP_FINAL_TIMESTAMP) as dateHour ,count(*) over (partition by datepart(hour, REP_FINAL_TIMESTAMP)) as completedCount from [table] where cast(REP_FINAL_TIMESTAMP as date) = tableReceived.Study_date and datepart(hour, REP_FINAL_TIMESTAMP) = tableReceived.dateHour and REP_FINAL_TIMESTAMP is not null ) as tableCompleted This also allows for any other filter data or included data into your comparisons: simply add the column to both selects **and the where on the joined table** then include it in the top tier select. If you're only checking one day, you can just change the date in the where clause (maybe use a variable to make a change easier with reporting/cleaner?), or you can include multiple days using a between on the cast date field. This would require including that original comments tiered datepart in the top select to keep days from being combined (unless you want a sum by hour over several days, in which case you can leave it alone and use it to average out each hour's completed/received over days/weeks/months for a bigger picture). Sorry if anything isn't best practice - I work in my own bubble mostly learning through online examples/trial and error - so it's *very* possible I'm behind the times or out of standards. And sorry if it doesn't work - I don't have any good sample cases with my data to check it against, but at the very least it makes sense to me. And that's not worth very much. I hope this helps, and if it's not exactly what you need then at least it either gets some corrections from other comments or gets you started. Good luck!
You're right obsolete isn't the right word. Maybe antiquated is better. I've worked with maybe 3 people who used that or similar syntax. I could read their code, but man is it different. I'm a SQL Server guy, perhaps the Oracle world is more accepting of this syntax?
Create CTE to return your table with row number partitioned by letter one and ordered by letter one. Then delete from CTE where row number &gt; 1.
&gt; doesn't want to work for free just for the chance at an interview... (unless you are some one like google)... A lot of places like Google will pay you for the homework you do for the interview. It's usually reasonable pay and an allotment for X hours pending the homework.
 SELECT CASE WHEN LetterTwo &lt; LetterOne THEN LetterTwo ELSE LetterOne AS LetterOne , CASE WHEN LetterTwo &lt; LetterOne THEN LetterOne ELSE LetterTwo AS LetterTwo FROM Letters GROUP BY CASE WHEN LetterTwo &lt; LetterOne THEN LetterTwo ELSE LetterOne AS LetterOne , CASE WHEN LetterTwo &lt; LetterOne THEN LetterOne ELSE LetterTwo AS LetterTwo
What is it you don't like about between? I agree semantically it being inclusive is confusing at first, but that's easily learned and corrected. Same with being sure to either include time with your datetimes or to cast them as dates. You've got me worried I'm consistently making a mistake using between and I haven't noticed the unintended results.
Absolutely, but not everyone has the budget or freedom to do what google does. and some people still like to ask "homework" type questions to get a feel for the candidate. I know we do, but again we keep our problems to something we feel a candidate could do in a single night (4-8 hours), I saw some one comment they don't want people to be able to just google answers - but to me that is half the point of these, i want you to be able to google some of the answer but if I pick you to come back I will be able to tell if you understand the solution you wrote or if you just copied and pasted it from a post on stack overflow or wherever else.
Pretty much what you've said - 'between' treats the data points literally and humans rely on context to interpret 'between' colloquially. It doesn't also help that we've also been pavlovian-ed by instant gratification into using implicit type conversions, which work great until they throw a curveball at you in the middle of page-long statement - your carefully type-cast expressions implicitly converted by something you throw in quickly to fix a 'day-before' issue, etc. IMO, the optimizer should be able to figure the range logic (and MS SQL's does usually) on its own, so 'between' doesn't bring any specific benefits that I can think of, so why use it at all?
&gt; I know we do, but again we keep our problems to something we feel a candidate could do in a single night (4-8 hours) That's fair to me, honestly it's not a huge deal for that. Google typically gives a week or two long assignments. We're talking 20-40 hours of work. I would prefer 4-8 hrs of homework vs 8 interviews. (Current job took 8 separate 1 hour interviews, I personally thought 3-4 would have been more than sufficient but it is what it is.) &gt; to just google answers - but to me that is half the point of these Being able to google and solve problems is a skill set, and not everyone can do that. For a junior to intermediate, I think testing whether they can figure out something with the help of the internet is a good thing to test for, so I can see your points there. &gt; or if you just copied and pasted it from a post on stack overflow or wherever else. Here, it depends on the questions. If you ask, here's a diagram, write a recursive statement to show someone's supervisor next to them, you can definitely get a stack overflow answer, mostly copy paste, and you probably can't tell the difference. Maybe if they took it a really unique and weird way, then perhaps, otherwise you'd look and go, oh hey, best practices, nice. I think a good compromise is somewhere in the middle? Can they figure out how to resolve things on their own? (Google-fu) What is their current knowledge level at? How does their personality fit with the team? Those are the three key identifiers I look for in interviewing folks. 
Very fair argument - but when I so frequently mistype or misrepresent my greater-thans and less-thans, between can be a life-saver. But that's a whole different issue. In principal you've definitely convinced me.
&gt; I cannot get null values no matter what I do. you need more practice with outer joins you have ATD as the right (optional) table, which means any TD rows without a matching ATD row will still be returned by the FROM clause but then you shoot yourself in the foot by adding a WHERE condition on the ATD table, which means all the NULL values will be thrown away and your join ~acts~ like an inner join try it like this -- SELECT TD.Db_date , ATD.attendanceDate FROM time_dimension TD LEFT OUTER JOIN Attendance ATD ON ATD.attendanceDate = TD.Db_date AND ATD.studentID = '4046834' 
Your problem is on how you've filtered for the student ID: you need to get the days where an ATD record does not exist, but by putting a WHERE condition on one of the fields on ATD you're guaranteed to only get days where an ATD record does exist. Not exists is on the right track. SELECT TD.DB_date FROM time_dimension TD WHERE NOT EXISTS ( SELECT 1 FROM attendance ATD WHERE ATD.attendanceDate = TD.Db_date AND ATD.studentID = '4046834') AND --now subtract your holidays TD.DB_date NOT IN ('2017-01-01', '2016-12-25', '2016-12-31') Depending on your holiday list and whether that list exists in the DB there's better ways of removing holidays but you get the idea. One way I'd approach it is exclude any days where nobody in the same class has any attendance recorded.
Easiest fix would be to get the distinctID counts ,COUNT(distinct before.ENC_ID) count_of_encs_before ,COUNT(distinct after.ENC_ID) count_of_encs_after
I think that did it! Thanks so much!
Dumb question: I was originally counting the primary key of that table, so what was I counting if it wasn't counting distinct?
&gt; Know how to join a table to itself using an alias. Wait, wut? I've been using SQL on a daily basis for over a decade and I've never stopped to try this, let alone even know you could do it. select o.order_id, o2.order_id from orders o join orders o2 on o.order_id = o2.order_id But sure enough, it works... Would anyone care to give me an example of when this would ever be useful? Not being facetious, genuinely curious.
Exactly my problem, so that pulls all days from calendar that does not match, add a date range and it still misses our days. If my student went to class form 2017-08-14 till the current date, that query pulls all nulls back to 2010. This is what i am continually running into as I have to use the date my student attended. 
This works perfect if your calendar days start when the student starts but our calendar goes back to 2010, so I get all the way back. Its stupid somehow the db misses a few days so they write this in javscript but now we need to do it in SQL for reporting this is the basis of the sql script. For a students reg start date to current date, we get all the days attended m-f, run against the calendar to find if any where missed. Subtract holidays, add any Saturdays they attended, remove any leave of absences. Take the time and they were supposed to be there X enrollment to get what 100% attendance would be, then calculate it against the hours they were there all in 1 main SQL query that has to run efficiently. 
What do the wait types / query plan look like? Also, is this any better? Where ( STF.Description LIKE ('%ITU%') OR STF.Description LIKE ('%AMAZ%') ) and STF.PostDateKey &gt;= 20171215 When you use LIKE % %, you cannot use the index on the column. If you use ITU% or %ITU, you can utilize the index. Obviously this won't always work for your scenario. My immediate guess is the OR logic was messing up the optimizer and you are probably double scanning the table. You need your query plan to know what's happening though. You could possibly make your where clause this: Where STF.PostDateKey &gt;= 20171215 Then insert the records into a temp table and run a select with: Where ( STF.Description LIKE ('%ITU%') OR STF.Description LIKE ('%AMAZ%') ) This depends HEAVILY on how many rows you're getting, if it's utilizing indexes, etc. You need your query plan to know what's happening.
Hi there! This worked. The issue is always a parenthesis or a comma, right? The noob plague
Look at the rest of the stuff besides the solution, I'm glad it worked for you though, that's awesome! You should figure out WHY it works now so you can future proof the solution and take that knowledge in your SQL Toolbox for later gigs. 
Just the number of rows. If you imagine the underlying data as an ungrouped query for a pa.id who had two before encounters and one after encounter the recordset would have looked like pa.id|before.enc_id|after.encid --|--|-- 1|1|1 1|2|1 so when you counted after.encid, you were getting a count of two even though it was the same ID twice. Gets even weirder if you had say 3 before and 2 after: pa.id|before.enc_id|after.encid --|--|-- 1|1|1 1|2|1 1|3|1 1|1|2 1|2|2 1|3|2 So your counts would both return the same (6) as there's 6 different permutations of the join, but only 3/2 distinct IDs in each column. 
 I solved it based on your example but still have work to do, have to get rid of holidays and Saturdays without duration. SELECT DISTINCT TD.DB_date FROM time_dimension TD LEFT JOIN Attendance ATD ON ATD.attendanceDate = TD.Db_date INNER JOIN Registrations REG ON ATD.studentID = REG.studentID WHERE NOT EXISTS ( SELECT 1 FROM attendance ATD WHERE ATD.attendanceDate = TD.Db_date AND ATD.studentID = '4046834') AND TD.DB_date Between REG,startDate AND CURDATE()
I'm using SSMS 2016. I think I get what you mean by 'double-scanning'. Can you elaborate on what you mean by wait type and query plan?
Imagine an employee table that has a manager ID column. The manager ID relates to an employee ID, which would be in the same table. If you want to list a manager's name, you would have to related the table to itself joint the manager ID to the employee ID.
Think of SQL like a really smart person who is well versed in math and theory. SQL can use indexes to find records quickly, just like you do using a phone book. When SQL cannot use the index, it functions similarly to us. It reads each and every row. Does this row have 'ITU' in it? No, ok. Does this row have 'ITU' in it? Yes, neat! Does this row have 'ITU' in it? ... and it continues until it reaches the end of the table. As you can imagine, this is not efficient. Now as you can imagine, doing this TWICE is even worse! Now, SQL is pretty smart. It can generally say, ok, I have to look through this twice, can I just do this once? Sometimes the optimizer can see through bad writing, sometimes it can't. SQL is a declarative language. If you do SELECT name FROM table WHERE name LIKE '%Jim', SQL is going to find the best method to accomplish this. You're telling it what you want and where you want it from and the criteria behind it. Other programming languages, you have to be specific and build processes out. Now I think here's what happened to you. Ordering of the where clause does not change how it is evaluated when written correctly. However you had an issue with logical precedence. https://technet.microsoft.com/en-us/library/ms190276(v=sql.110).aspx Without the parenthesis, you told SQL to look for STF.Description LIKE ('%AMAZ%') and STF.PostDateKey &gt;= 20171215 Or it can look for STF.Description LIKE ('%ITU%'). The parenthesis controls this flow and makes it easier to read. Whenever you need to define order of precedence, it's best practice to use parenthesis to visually dictate it rather than only write it in the correct precedence order. When you're waiting on a query to complete, what are you waiting on? Are you reading data from disk? Are you reading data from memory? Is there an issue with an index? Are values being converted, checked, etc? What are you waiting on? Is the network slow? SQL Server uses dynamic management views that show you the wait stats of your queries so you can see what you are waiting on. Check out [sp_whoisactive](http://whoisactive.com/). Well if you know what you're waiting on... why are you waiting on it? The query plan is the next piece to help here. Try running your query again, but this time (in dev preferably, this does add overhead) enable [query plans](https://www.red-gate.com/simple-talk/sql/performance/execution-plan-basics/). This is the map SQL Server used to go from you are running a query to here are your results. There are BOOKS that cover all of this individually. Thousands of pages have been written on just indexes, on just wait types, on just query plans, etc. So take a step back, breathe, read a little, digest, and continue learning. 
That makes sense. Thank you. I can't say I've ever had to work with a table like that before (so I guess I should be thankful), but that gives me a better perspective on what it could be used for. 
In SQL? Almost none. There are various topics inside of RDBMS/NoSQL I would attend however.
Would you be open to listing a couple topics that would interest you inside of RDBMS/NoSQL?
I'm still wrapping my head around that, but thanks for all the help!
 * Performance Tuning in PL/SQL is always a fun topic, and anytime I talk with anyone else about it I end up learning some new, neat trick. * Cryptography libraries that are available to use in different flavours of SQL programming languages and proper storage of passwords and other sensitive data/current best practices? * Probably a request specific to a unique speaker that has such available data at hand, but - I'd like to see some data analysis on common text/character fields over billions of rows of data to see where the sweet spots are for byte lengths of character data types for columns. Such as first/last names, street addresses, etc. 
Great ideas and topics, I think those are interesting too. Thank you!
[removed]
 WITH letters AS ( SELECT * FROM (VALUES ('A','B'),('B','A'),('A','C'))a (LetterOne,LetterTwo) ) ,cte AS ( SELECT LetterOne ,LetterTwo ,ROW_NUMBER() OVER (PARTITION BY CASE WHEN LetterTwo &lt; LetterOne THEN LetterTwo ELSE LetterOne END ,CASE WHEN LetterTwo &lt; LetterOne THEN LetterOne ELSE LetterTwo END ORDER BY LetterOne) AS rowno FROM letters ) SELECT * FROM cte WHERE rowno = 1
One that does not sell stuff to me. "here to learn Service Broker" vendor shows up to sell me on RedGate....
Fair enough! I'm currently looking at doing independent speaking but I get hung up on trying to figure out a topic. I waiver between is it too junior, too senior, or too niche of a topic to create a presentation and educate people on. My end goal really, is just to become a bigger part of the SQL community.
you hit the nail on the head mostly, Interviews answer 4 major areas for me. How well do I think a candidate can solve problems? (google-fu as you put it), This is where these types of problems get asked - or where white boarding comes in... Does a candidate have a solid grasp of the knowledge area (screening questions (do you know what an inner join is, do you know what a group by is), While I don't care if you are googling you need to have a grasp of the fundamentals to know what to google in the first place... Specifically for anyone in a developer role I look for an understanding of software engeneering fundimentals, (clean code, good design principals, QA, some one that knows what requirements gathering is, and can explain why unit testing is so importa The last area are the soft skills... Do they mesh well with the team... do they communicate effectively, do I think they will contribute to a conversation during breakout sessions effectively or just be a silent partner. coding questions - especially "homework" gives me a relatively good grasp of the first 3, and at least an ok grasp of the the 4th, I use interviews to get a good grasp of the 4th area and an ok grasp of the first 3. It is for this reason that when we talked about Copying and pasting earlier - I want to stress, I don't care and really likely won't know if you copy and paste a solution from stack overflow, I care that you are able to answer the follow up questions when we interview you. If I ask you to write a report that requires a running total, and you choose one of the at least half a dozen methods to do this - you bet I am going to ask about one of the others... If you decide to write a complex piece of parallel processing code when a simple method would work I think that is cool and on the surface it shows you know your stuff, but only if you can answer questions about maintainability and scale, and whether the trade offs you are making are worth the extra performance in your solution. 
Solid points I had not considered!
No problem. Sorry I'm on mobile and that response was hardly a coherent one, but I think you got it anyway. I only learned this because of an interview, which is why I specifically included it in my original list of things. I got the question right, but only because I assumed it was possible without ever having tried or known it... I got lucky.
https://stackoverflow.com/a/10500115/7948962
&gt; you told SQL to look for STF.Description LIKE ('%AMAZ%') and STF.PostDateKey &gt;= 20171215 Or it can look for STF.Description LIKE ('%ITU%'). Don't want this nugget to get lost in this big post of good information. You aren't returning what you think are.
That was very informative. Thanks for this
In case you’re lookin for ideas: GroupBy.org is basically this concept brought to life.
Are you not able to import the Data Into Oracle from excel? That’s how I do it every few days as the users who send the excel files are inconsistent in the structure/formatting.
&gt; GroupBy.org I'd love an equivalent of this for Oracle
I would as well since my primary system (the main SoR) is Oracle but most of my expertise is MSSQL. What I can say is a lot of the concepts are shared, and the Oracle crowd in general is less... “down to earth” than the MSSQL crowd. This is the main reason I enjoy the BrentOzar team and follow them as much as I can. I haven’t found the Oracle Equivalent of it yet (and at this point, I’d rather not).
I used to like the Ozar crowd a lot more. I feel the content from Erik and Tara is lacking comparing to Kendra and other past folk. They are all still smart, but I haven't seen the same level of content as they once had.
We've recently taken on multiple new enterprise applications that all utilize MSSQL, and I can tug around in it but I'm mostly lost. My biggest thing is that I've been in Oracle so long I constantly find myself frustrated trying to navigate the MSSQL environment/tools - but I digress, I need to learn it. Feuerstein is the equivalent in the Oracle world and that's pretty much it, and most all of his knowledge dumps are on the end of development and not administration :(
Understood. I enjoy SQLWorkbooks and Kendra’s Dear SQL DBA podcasts as well. She’s a great resource and makes great content. Erik is very active on StackExchange and is constantly blogging about cool new stuff. If I’m not mistaken, he’s the primary developer of the FirstResponderKit package, which has been a huge help to many. And Tara’s knowledge of Enterprise level platforms at large scale from her experience with Big companies is pretty damn amazing in my book. I’ve found myself hitting down notes from her answers on their Office hours podcast constantly for future (scaling) reference. I haven’t found another team that has such diverse knowledge and is equally entertaining to listen to/read. 
Those are all fair points, my opinion stemmed strictly from the tutorial videos in their paid learning and blogs. Obviously it's more opinion based, but I need to check out the office hours more.
I found myself in the reverse situation. However, I can recommend a few things: Toad For SQL Server if you are used to using toad. RedGate products (redgate.com) are a huge help for monitoring, development, and deploying. The tool belt is great if you can spring for it. The office hours podcast/webcast (available at BrentOzar.com). 
Are you dealing mostly with administration, development, security, or analysis in MS SQL? 
Yes, understood. I haven’t really seen many of their paid ones as I keep missing the Black Friday deals and my employer won’t spring for them. While they do goof off from time to time, I’ve learned a lot of really good practices from watching/listening to Office hours and their other (free) videos. 
I’m a development DBA on the oracle side and I manage the system of record. I am the primary DB developer for all applications and I also deploy all changes. However we have stand alone SQL Server instances all over the place, and we most recently are developing a reporting data mart/Warehouse in SQL server that runs ETL from Oracle: it’s been a ton of fun the past year. I’m also the DBA for all SQL servers so I serve a few roles.
I got lucky and they had a promo for a few days where you could get all their videos for $100 for the year. If you renew each year, it stays at the $100 price. I actually spent about $500 of my own money on training material this black friday, but my company won't reimburse me. So ya, it's a little loss, but I figure I'll become pretty smart and get myself a new job with more pay and tuition reimbursement and blackjack and hookers. Or at least the first two. And at least they mostly cover cert exam fees, so that's nice.
Haha nice! I’d also recommend Pluralsight, but I haven’t really gotten too into it as I’ve been enjoying GroupBy and SQLWorkbooks videos too much.
That is the other benefit. I get a corporate lynda / pluralsight / ms videos accounts for free. Most of the content is too dry or above average what I can find on youtube. The GroupBy has a lot of great videos and so does pass tv. I saw today there is a safari books online account that is available. It's like pluralsight but for books. Every book I want or have read on SQL is available on it, so I'm going to see if they can spring for those accounts.
Most of our critical applications are Oracle on Windows (ha.) with a few really small Postgres databases - however, our company has grown enough to warrant a data lake, BI solutions, CRM, etc. - and since all of our app developers are .NET an executive decision was made that to have these solutions developed under MSSQL moving forward. Oracle-wise, we handle admin/development/architecture/security/reporting/etc. all on the same team for these databases. There's no separation of report writers/business analysts/dba/dd. For MSSQL, we have a .NET developer who's also heavily experienced in MSSQL - so we're relying on him and consultants until we're (grudgingly) tugged fully to the MSSQL side. Most of what I'm currently doing in MSSQL is reoccurring report writing, T-SQL development for application API/Data Layers and ETL work - I've touched no administrative tasks with these MSSQL servers because I know enough to break them basically - unless performance tuning my own horrible T-SQL counts as a DBA task :) I'm 6 months in, I'll get there - but moving from Oracle to MSSQL has been a culture shock. 
Ah yes, I had Safari Books at my last employer and definitely enjoyed it. My backpack was always full. I agree about Pluralsight. A bit dry and 0 enthusiasm, but when I look at the bigger picture I think I find them that way because I’ve been spoiled by GB.org and the BO.com team. Everything else is just bland.
&gt; safari books online Safari is incredible. Thankfully, I've had a free, full-access account for my entire career. If you're prior military/DoD service you get a lifetime account. That being said, I'd willingly pay the subscription fee based on all the textbooks it provides.
Based on how much money I'm spending on books, I feel I should just buy a subscription. I like having a digital and physical book though. The physical book is easier for me to read / feels better, but when I get to snippets of code I have to test, it's convenient to get into the e-book and copy / paste the code. All of the MS SQL training books for exams were roughly ~$15 each including the e-book during the black friday sale. So I bought all six of both.
Unfortunately no, as said in the title I only have read access. I have tried the import function and it tells me I do not have the correct privledges. 
Right, but your user must be he owner of a schema, and you seem to have read access on the existing table. So why not create a table in your own schema, import the data there, and then use it to compare with the table you have read only access to?
That’s awesome! I usually go for the Kindle versions if I can. I’ve found I’ll actually read them vs the physical copy. And like you say, copy/paste is a big bonus!
I will try to work through that tomorrow. That may well be what I intended, and I was just not using the wizard in the right place. Schema may be the missing key word.
Yep. Let’s say your username is Kairas9000 You’ll need to do a CRÉATE TABLE Kairas9000.TableName TABLESPACE DEFAULT (or whatever TS you have access to) (Col1 Datatype1 ,Col2 Datatype2 ,etc..) Then you go back in and import into that table, which will be an object in the Kaitas9000 schema. Once you have your data imported you’ll be able to join to the table you have read only to.
Sounds good. Fwiw, is it possible for your own schema to be read only?
Technically it is, but the DBA would have had to manually REVOKE CREATE ANY OBJECT or something like that, which seems a bit too troublesome unless they’re super paranoid.
I agree you make really good points here. Often I feel that the goal is to break the rules in the right way, or to look at the way you have chosen to do something and comparing it to the other known methods of doing it. If I ask X, and you give me Y, and I ask about Z and you know Z exists... that is impressive. If you know Z isn't always a good idea but it makes sense under specific conditions, again, that is impressive. I don't expect you to understand our conditions when interviewed, I expect you to show an ability to understand changing conditions and the need to solve problems from multiple angles. 
are you able to use a hashing function or is that going to cause problems? e.g. SELECT [Letter one], [Letter Two] FROM table t GROUP BY hashbytes('MD5',cast([Letter One] as varbinary(16)) *cast([Letter two] as varbinary(16))) Because the hashing function uses multiplication AB = BA as per your example, I haven't tested in your environment so I don't know how well it performs since it has to calculate the hashes on the fly - if you want something super quick and are able to modify the underlying dataset you can pre-calculate the hash (using a calculated column)...
After doing a few times the same thing you did, and then wasting half an hour staring at the query being dumbfounded as to why it's giving incorrect results, I make it a habit to surround any OR statements with parentheses: SELECT * FROM Table WHERE (A = B OR C=D) This way there is no chance you'll forget to add parentheses later when you add an AND condition to the bottom.
Your issue really was understanding how WHERE clauses work. When you have more than one clause separated by AND/OR then those clauses are checked if true sequentially one group of clauses together. E.g. in your example STF.Description LIKE ('%ITU%') OR STF.Description LIKE ('%AMAZ%') is checked to be true And then STF.Description LIKE ('%AMAZ%') and STF.PostDateKey &gt;= 20171215 is checked to be true. That's not what you wanted though. You wanted (STF.Description LIKE ('%ITU%') OR STF.Description LIKE ('%AMAZ%')) AND STF.PostDateKey &gt;= 20171215 to be true That's why you wrap things in parenthesis. It generally doesn't matter too much when using just AND but as soon as you add an OR then you need to think about what clauses are being checked. 
Crazily enough, it seems that that is what they have done. Insufficient privileges. 
Can you try the CREATE TABLE without specifying a tablespace? Just leave the tablespace line out and go on to define the table.
No, strictly speaking, trying to use any visible tablespace tells me that it is an "invalid" action, and trying not to use a tablespace throws a lack of privileges message. For this specific query/project I have a work around, but I expect an expansion of scope in the future would be made easier and better by being able to do this. Silly restrictive IT environment. Thanks for the help though! I am still interested in/willing to try more ideas though.
Yikes. Sorry it’s that restrictive. Any chance you can talk to the DBA to get yourself access to your own schema?
It is a minefield to say the least. Strictly speaking all permissions are supposed to be uniform for a given job position. My coworkers do not, and should not have SQL access at all based on their skill set. It may be worth a try down the road though when I have had access long enough to call it a skill/permanent responsibility. 
Yes, I think in this case, it was simply a matter of incorrect operational precedence (PEMDAS, if you will!)
You are correct, I'll make better use of parenthetical insertion in my WHERE from now on
I backtracked my response slightly, please check the edit in case I misinformed you.
I backtracked my response slightly, please check the edit. That piece is a little wrong, it's concept is right, but why it's right is slightly off.
I'm a little confused, but that's probably because of the lack of caffeine. Check this out: &gt; The order of precedence for the logical operators is NOT (highest), followed by AND, followed by OR. Parentheses can be used to override this precedence in a search condition. The order of evaluation of logical operators can vary depending on choices made by the query optimizer. I'm curious to your thoughts on this, my thoughts are their query got picked out as Where (STF.Description LIKE ('%ITU%')) And then: OR (STF.Description LIKE ('%AMAZ%') and STF.PostDateKey &gt;= 20171215) Because the AND ties the operator before and after it, segregating the first statement to the other side of the OR. If you have adventureworks, you can check this out: -- = AND OR operators testing SELECT * FROM [AdventureWorks].[Person].[StateProvince] WHERE CountryRegionCode = 'US' OR CountryRegionCode = 'CA' AND StateProvinceCode = 'Ak ' SELECT * FROM [AdventureWorks].[Person].[StateProvince] WHERE StateProvinceCode = 'Ak ' AND CountryRegionCode = 'US' OR CountryRegionCode = 'CA' SELECT * FROM [AdventureWorks].[Person].[StateProvince] WHERE ( CountryRegionCode = 'US' OR CountryRegionCode = 'CA' ) AND StateProvinceCode = 'Ak ' -- LIKE AND OR operator testing SELECT * FROM [AdventureWorks].[Person].[StateProvince] WHERE CountryRegionCode LIKE 'US' OR CountryRegionCode LIKE 'CA' AND StateProvinceCode LIKE 'Ak ' SELECT * FROM [AdventureWorks].[Person].[StateProvince] WHERE StateProvinceCode LIKE 'Ak ' AND CountryRegionCode LIKE 'US' OR CountryRegionCode LIKE 'CA' SELECT * FROM [AdventureWorks].[Person].[StateProvince] WHERE ( CountryRegionCode LIKE 'US' OR CountryRegionCode LIKE 'CA' ) AND StateProvinceCode LIKE 'Ak ' 
Hey thanks for your response. Still processing everything you're saying here. But if I'm using the .ddl file to totally wipe and rebuild the database, isn't the database table with the PK column empty? I'm not just reimporting the data to the existing db, I'm (or at least I thought I was) totally nuking it and starting over.
The issue still applies. If you have 4 clauses for example without parentheses then clauses 1 &amp; 2 are checked, 2&amp;3 are checked and finally 3&amp;4 are checked. 
&gt; isn't the database table with the PK column empty? &gt; I'm (or at least I thought I was) totally nuking it and starting over. Are you truncating the table? Or deleting records from the table? Are you re-creating the table as a copy in parallel to the other table existing? Are you dropping and re-creating the table? When you delete records from a table, the seed in the PK column remains. You can see this in my last example. You can use reseed functions however to change that. When you truncate a table, the seed in the PK column is reset back to the original value. If you drop and re-create the table, the PK column should be reset back to the original value, but... if you have a script to re-seed the table back to where it was, then it makes sense you would see the issue. If you create the table as a copy in parallel, the top part still applies. Did you script a re-seed? If not, then it should start back at 0.
It's been a while since I touched Oracle. Can you not use global temporary tables?
See, I was under the impression that you cannot guarantee the order the expressions will be evaluated in the where clause. You may run it 1000 times, but it may change the way it evaluates it later as long as the result is the same. The only really deciding factor is if there is a precedence hierarchy found in the where predicate. As you stated, parenthesis controls this and makes it easier to read / understand and is obviously best practice. https://social.msdn.microsoft.com/Forums/sqlserver/en-US/ef82245f-3cb8-40f3-8d2f-9006d67b9b71/expression-evaluation-order-just-making-sure?forum=transactsql https://stackoverflow.com/a/484160/5149122 https://blog.sqlauthority.com/2010/08/25/sql-server-deos-order-of-column-in-where-clause-matter/ http://weblogs.sqlteam.com/joew/archive/2008/02/22/60527.aspx 
It might run 3&amp;4 then 1&amp;2 and then 2&amp;3 or 2&amp;3 then 3&amp;4 then 1&amp;3 but that doesn't matter. The point is that they will be run in groups and that is key when using OR and why wrap things in parentheses. If let's say you have clauses 1 or 2 and 3 and 4 then it will check 1 or 2, 2 and 3, 3 and 4. This isn't what you want. You want (1 or 2) and 3, 3 and 4. 
&gt; You want (1 or 2) and 3, 3 and 4. Sure, I understand why we want the parenthesis and all, what I'm interested in is how it evaluates the clause without them. &gt; If let's say you have clauses 1 or 2 and 3 and 4 then it will check 1 or 2, 2 and 3, 3 and 4. See, the information I find makes me believe it would evaluate this: 1 or 2 and 3 and 4 as (1) or ( (2 and 3) and 4 ) Because SQL Server's logic is to evaluate the AND conditions before it evaluates the OR conditions. Or am I reading their [hierarchy](https://docs.microsoft.com/en-us/sql/t-sql/language-elements/operator-precedence-transact-sql) table incorrectly?
just something like this: create #table table ( ) insert into #table select blah Just create the table above before your loop and then insert into it below where you're selecting?
 SELECT * FROM parts WHERE id IN ( SELECT SUBSTR(id, 0, 9) FROM parts ) 
Ok, that makes sense. Thanks! What about the looping through the IDs from the starting table?
&gt;That actually makes perfect sense and it works. Just need to loop the IDs until they are done. So replace ID, recursion, select into temp table, loop until end of IDs. Got to be honest, I have no idea what you're doing, or trying to do, and only glanced at your code. I just know you need to create the table above and then insert into for loops because if you try to do a select into the table will already exist on the second pass of the loop. 
Right, I'll just keep inserting, the table create command will be outside the loop.
cool idea!
Doesn't sound like you actually need a loop either.
How do I get it to cycle the ID numbers from #tempID table through the @ID value in the recursion?
With a JOIN in the CTE, should work fine if you have a table with a list of all the IDs, I believe. 
If I understand what you are looking for you might be over complicating it by looping through it and can probably get what you want in one statement. Only thing I wasn't sure about is the "select EMPT_NO from #TempIdList", not sure if this even needs a temp table or if it is "select distinct Control_Delegate_M0 from Controls" Declare @ID varchar(10); set @ID = 'An ID #'; with EmployeeCTE as ( select EMP_M0, EMP_Name, MGR_M0, EMP_M0 as ORIG_EMP_M0, 1 as ORDER_NO from EMP_MGR where EMP_M0 in (select EMP_M0 from #TempIdList) union all select EMP_MGR.EMP_M0, EMP_MGR.EMP_Name, EMP_MGR.MGR_M0, EmployeeCTE.ORIG_EMP_M0, EmployeeCTE.ORDER_NO + 1 from EMP_MGR join EmployeeCTE on EMP_MGR.EMP_M0 = EmployeeCTE.MGR_M0 ) select distinct E2.EMP_Name as Mgr_Name, E1.EMP_Name from EmployeeCTE E1 left join EmployeeCTE E2 on E1.MGR_M0 = E2.EMP_M0 order by E1.ORIG_EMP_M0,E1.ORDER_NO option (maxrecursion 0) 
Uuuuh.... I get what you're saying, but I'm not sure where to do that. Could you explain that a little more?
Alternatively... Where STF.Description LIKE ANY ('%ITU%','%AMAZ%') and STF.PostDateKey &gt;= 20171215 Assuming your flavour supports LIKE ANY (and by extension, LIKE ALL) 
Interesting.. my first time hearing about ANY and I have been a SQL monkey for a while. https://stackoverflow.com/questions/40475982/sql-like-any-vs-like-all Now too remember it next time I have this situation..
You did it. I had to change some things. Here's what worked below. create table #tempID ( Delegate nvarchar(100), M0 nvarchar(100) ); insert into #tempID select distinct Control_Delegate, Control_Delegate_M0 from Controls ; with EmployeeCTE as ( select EMP_M0, EMP_Name, MGR_M0, EMP_M0 as ORIG_EMP_M0, 1 as ORDER_NO from EMP_MGR where EMP_M0 in (select M0 from #TempID) union all select EMP_MGR.EMP_M0, EMP_MGR.EMP_Name, EMP_MGR.MGR_M0, EmployeeCTE.ORIG_EMP_M0, EmployeeCTE.ORDER_NO + 1 from EMP_MGR join EmployeeCTE on EMP_MGR.EMP_M0 = EmployeeCTE.MGR_M0 ) select distinct E2.EMP_Name as Mgr_Name, E1.EMP_Name from EmployeeCTE E1 left join EmployeeCTE E2 on E1.MGR_M0 = E2.EMP_M0 option (maxrecursion 0)
Maybe set a protected range on the sheets at the end of the day? I'm not that familiar with google sheet scripting to be honest, but this might help you: https://stackoverflow.com/questions/38993561/protect-ranges-with-google-apps-script
Awesome. I really doubt it matters on a small dataset (&lt; 10,000 employees) but this select at the end is probably better practice and perform slightly better: select E2.EMP_Name as Mgr_Name, E1.EMP_Name from EmployeeCTE E1 left join EmployeeCTE E2 on E1.MGR_M0 = E2.EMP_M0 and E1.ORIG_EMP_M0 = E2.ORIG_EMP_M0 option (maxrecursion 0) If what you have works and runs quickly it really shouldn't matter too much, it is just if the duplicates are handled in the "distinct" or if they are handled in the join by looking at the original employee number on the join.
Thank you, by the way. Thank you very much.
You are reading the hierarchy table correct but it doesn't really matter all that much in practice. I've never once in years I've been writing SQL for a living had to consider what order my clauses are in. The optimizer will just do that for me. If i have a slow running query it's generally not because of the order of my WHERE clauses it's because I've done something stupid. The OP's issue was that he was returning all records where STF.Description LIKE ('%ITU%') and all the records when STF.Description LIKE ('%AMAZ%') and STF.PostDateKey &gt;= 20171215 and I assume that it was a lot more records than they were expecting and it took a lot longer to run since he says "I can run it just fine with just one string--either one by itself, and I get results in under 5 seconds over millions of records". The ordering of the clauses may had had an minor affect on how it took to run but that wasn't the issues. 
I've never used this before, thank you for sharing with me
&gt; You are reading the hierarchy table correct but it doesn't really matter all that much in practice. I would say if you follow best practices and use parenthesis, you will never have this problem. I had it when I first started in SQL, but you quickly learn to just use parenthesis. If you are not using best practices and happen to use the key word predicates &lt;AND&gt; or &lt;OR&gt; in the where clause, it appears to me that the order in how you arrange your predicates will dictate the results you have come back. Obviously we just use parenthesis instead, but if you don't, the optimizer will run the logical check evaluating your query in order of precedence hierarchy going left to right which can result in unexpected results if not ordered as intended. 
Please keep in mind that SQL is not a procedural language and you should always approach your coding with a set based method. For the best performance, just tell SQL what you want, not how to do it. I would advise you to avoid the method of loops and storing results in temp tables. Your recursion logic should start at the top (MGR_M0 I S NULL) and work down from there. This will save you from doing the LEFT JOIN in your results table. Remember that CTEs should only be referenced ONCE. If you use a second reference, then SQL must instantiate the data set. Also, you forgot to include your schema references for your tables, this could result in a cache hit miss. Here is what I believe you are looking to do... ;WITH EmployeeCTE AS ( SELECT EMP_M0 , EMP_Name , EMP_MGR.MGR_M0 FROM dbo.EMP_MGR JOIN dbo.Controls ON EMP_MGR.EMP_M0 = Controls.Control_Delegate_M0 WHERE EMP_MGR.MGR_M0 IS NULL UNION ALL SELECT EMP_MGR.EMP_M0 , EMP_MGR.EMP_Name , EMP_MGR.MGR_M0 FROM dbo.EMP_MGR JOIN EmployeeCTE on EMP_MGR.MGR_M0 = EmployeeCTE.EMP_M0 ) SELECT * FROM EmployeeCTE OPTION (MAXRECURSION 10)
While I don't know if SQL Server has this feature, I know Apex formatter / Red Gate / Poor man's T-sql formatter all have the ability to adjust this.
Thanks. I know Apex does, which makes me suspect that SQL Serve may not have the feature. I like to adjust settings natively as much as possible to avoid dependence on add-ins. I hope a SQL Server expert will jump in to provide a definitive answer.
Just do a `count` and then a `CASE` statement to translate your answers. But having another table where you do a lookup to do the translation would be better long-term. Select case when R1 = 1 then 'NA' when R1 = 2 then 'Unsatisfactory' when R1 = 3 then 'Marginal' when R1 = 4 then 'Satisfactory' when R1 = 5 then 'Good' else R1 end as [Rating], count(*) from intra.cseSubmissions where srvid = 1 group by R1 order by R1;
https://dev.mysql.com/doc/refman/5.7/en/create-procedure.html 
3 separate `INSERT` statements in a single transaction. BEGIN; INSERT INTO usercontactdetails (userid, detail) VALUES('TestID', 'TestDetail'); INSERT INTO user (userid, username, password) VALUES('TestID','TestUserName', 'TestPassword'); INSERT INTO business (businessid) VALUES('TestBusinessID'); COMMIT;
thanks alot. there was also a way to preview the results of the query before you actually start inserting? 
Do you mean SSMS? 
Yes, my question in the post specifies SSMS. Do you know if there is a default comma behavior setting in SSMS?
I don't think so. That's why those other 3rd party IDE do. I looked through the options again for you and I couldn't find anything. I've only ever seen it do leading commas that I can recall. 
Yep, my searches were fruitless too. And I figured that third parties doing it indicated that SSMS didn't. Thanks for your efforts.
ow there is something important missing, after i insert into the ustercontactdetails. i need to get the id back of it, how do i get that?
https://dev.mysql.com/doc/refman/5.7/en/resetting-permissions.html does this help?
I’ve been studying this page! The problem that arises is that I am having trouble restarting the program with the ‘- -skip—grant—tables’ option...any advice?
My go-to when converting sql is to use poorsql.com. It has things like comma before/after, expanding comma separated lists, etc. I don't believe any code is sent/received over the Internet but I only use it for non-sensitive code with no hard-coded strings. 
Thanks for the tip.
Still following this thread in case someone says there is an option to do it within management studio, but I don't know of one. Good luck little bobby tables. I'll be sure to sanitize my inputs. 
If the table uses auto_increment values for the id column, then you can use LAST_INSERT_ID() and it will fill in the last ID that was generated. 
When you say another table long-term, do you mean having another table where I just keep a rolling score for each?
That's a good programmer.
just change that insert statement into a select statement to preview. there’s also the commit command, but i think that’s specific to T-SQL so you can insert, query, then commit or rollback if you find it didn’t do it right 
Have you tried root with no password?
You could run the sheets in order into SQL with Change tracking enabled. https://technet.microsoft.com/en-us/library/cc280358(v=sql.105).aspx After each sheet is imported you can check the change tables against the stored value and outputting the details into a table with the addition of a batchid /filedate this table can then be joined back to the master using the primary key with a left join and you will let you see all the changes between files for each record. This can all be automated in SSIS.
 Create table SurveyAnswers (AnswerId int primary key not null, AnswerName nvarchar(20)); insert into SurveyAnswers(AnswerId,AnswerName) values (1,'NA'),(2,'Unsatisfactory'),(3,'Marginal'),(4,'Satisfactory'),(5,'Good'); select A.AnswerName, count(*) from intra.cseSubmissions S join SurveyAnswers A on A.AnswerId = s.R1 where S.srvid = 1 group by A.AnswerName;
So, you want help hacking into MySQL? Ask your admin or DBA.
I actually think I like apex more than red gate for the formatter. I haven't used reds latest one though, so maybe it's better than apex now. 
I am getting the below error when running your query. I simply copy and pasted it to see your output before working it into mine, did I need to alter something? Msg 245, Level 16, State 1, Line 4 Conversion failed when converting the varchar value 'Marginal' to data type int.
This comment cannot be high enough. So much time wasted on my end.
[removed]
&gt; I can say that I like Red Gate's tool suite a lot. 3 years ago when I started my current role I said the same thing. SQL Prompt and its "join condition suggestion" feature made my jump into a 100% foreign database a breeze. After getting to know the database and how the tables are linked I began to get annoyed by the slight lag introduced by all its various autocomplete stuff. My first step was to bump up the suggestion time to 500ms, so it would only kick in if I had a brief lapse in typing. Soon after, I knew exactly where I was going, what tables joined to what, and the suggestion box became more of a hindrance/obstruction than a benefit, so I ended up turning off all the autocomplete stuff, just leaving the "Show code suggestions" box checked so it auto-caps keywords. (really Microsoft, it's damn near 2018 and you still can't freaking capitalize SELECT by yourself?) Just recently a colleague at another company said disabling IntelliSense makes a big difference in mitigating the lag from SQL Prompt, but I've never tried it. I'm not crazy about the Source Control component either. Last I checked (few weeks ago or so, around the last time I tried to get (some) latest), it still couldn't handle unique indexes in temp tables. It doesn't seem to be able to figure out object dependencies when getting latest, often requiring a few attempts to get all the latest changes. If you rename a table, it just drops it and creates a new one with the new name, which can be a pain in the ass. Oftentimes when such a change is made, whoever made the change will send out an email that starts off something like "Because Redgate.... please run this script before getting the latest from database xyz." We use TFS as a repo and RedGate will not pick up any changes that were made in TFS directly. So, after all is said and done, I use my $3000 license for the Toolbelt to get it to capitalize SELECT for me. The Tab History feature is pretty nice though. It's saved my ass many a time when I'm 50000 tabs deep and SSMS goes belly up. I probably sound like a curmudgeon, so I'll just go ahead and see myself out. 
One day MS will get around to adding an amazing source control feature. One day... I had used Red Gate a lot in the past when I was working with an accounting software with a pretty, uh, "basic" table naming convention. It was extremely helpful there. Same with the source control, but it does have limitations. I think my favorite SQL tool is Sentry One's Plan Explorer and it's free. So much nicer than using the SSMS functionality alone.
Dude good call!! I too use Plan Explorer every single day, and it does indeed blow SSMS out of the water when you're trying to figure out WTF SQL Server did what it did. 
My favorite
Anytime i have had to modify legacy code that is written old style or formatted bad i rewrite it all. There's no excuse to have code like that still around
You must have a value outside 1-5 in R1 that's getting picked up. `CAST(R1 as varchar(20)) in that `else` statement to resolve.
Good stuff, covered every scenario!
This did resolve it, but I do not see any bad values; I will past data and results below. (sorry I tried putting in code format but it keeps messing it up) Data: UID SRVID DATE_SUBMIT R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 7 1 2017-12-12 13:21:04.8200000 3 3 2 2 2 2 2 2 2 8 1 2017-12-12 15:05:17.1966667 4 3 4 4 4 3 3 3 4 Does not easily provide service in the ED 4 1 2017-11-28 13:06:57.1166667 4 4 4 4 4 4 4 5 5 5 1 2017-12-12 13:18:00.7033333 4 4 1 1 1 1 2 2 1 sup 6 1 2017-12-12 13:20:43.3066667 5 5 4 4 4 4 4 3 3 test 1 1 2017-11-28 13:00:11.6633333 5 4 3 4 3 4 3 3 3 test 1 2 1 2017-11-28 13:00:50.5866667 5 4 3 4 3 4 3 3 3 test 1 3 1 2017-11-28 13:06:20.5733333 5 4 3 4 3 4 3 3 3 test 1 Result: Rating NumResponses Marginal 1 Satisfactory 3 Good 4
Maybe describe the actual problem to us, we can't solve 'Its broke' 
Sql zoo has 'test what you have learned this chapter' you could work through those and see which chapters you struggled with and focus on those 
When I am trying to connect to the ***root*** MySQL Server (leaving the password input empty), I receive this message: "*Your connection attempt failed for user 'root' from your host to server at localhost:3306: Access denied for user 'root'@'localhost' (using password: NO)*" I have been told to then reset the password, however I believe I need the original password that I was unfortunately never given upon installation to do so. Does this inform a bit better?
When I am trying to connect to the ***root*** MySQL Server (leaving the password input empty), I receive this message: "*Your connection attempt failed for user 'root' from your host to server at localhost:3306: Access denied for user 'root'@'localhost' (using password: NO)*" I have been told to then reset the password, however I believe I need the original password that I was unfortunately never given upon installation to do so. Does this inform a bit better?
When I am trying to connect to the ***root*** MySQL Server (leaving the password input empty), I receive this message: "*Your connection attempt failed for user 'root' from your host to server at localhost:3306: Access denied for user 'root'@'localhost' (using password: NO)*" I have been told to then reset the password, however I believe I need the original password that I was unfortunately never given upon installation to do so. Does this inform a bit better?
What happens when you restart MySQL with the skip-geant option?
[PostgreSQL Exercises](https://pgexercises.com/) is a series of interactive exercises where you must correctly apply what you've learnt to complete them. It covers a range of SQL features from basic through to advanced (using [Postgres](https://www.postgresql.org/), so may not yet be available in the database you use). I worked through it all a couple of years back and found that it gave me a good workout.
https://www.techrepublic.com/article/how-to-set-change-and-recover-a-mysql-root-password/ 
try this: [Datepart](https://docs.microsoft.com/en-us/sql/t-sql/functions/datepart-transact-sql)
which database system? MySQL? DB2? MS Access?
I have a SQL DB in azure, im gettin started on this sory . What zane suggested still dosnt work
it might be helpful to post your query. You get a limited answer when you provide limited information.
SELECT MONTH(f.periodo) AS mes From fatura f LEFT JOIN contrato c ON f.idContrato = c.numContrato WHERE c.idMunicipe = 1 &amp; YEAR(f.periodo) = 2010
&gt; `WHERE c.idMunicipe = 1 &amp; YEAR(f.periodo) = 2010` Try `AND` instead of `&amp;` 
yeah that was it, sory for wasting your time guys =) thx
Okay, so I saw this was recommended online, but I couldn’t figure out how to do this. I went through the instructions on SQLs support page and had trouble with it. How do you restart with ‘skip—grant—‘? 
Very helpful, sir! Thank you and happy holidays!
This sub could help with issues related to storing and analyzing stored data for your project... Other places to look: /r/Twitter/ https://developer.twitter.com/en/docs/tweets/filter-realtime/overview Cheers! 
So, I am actually encountering a problem... When I issue the command: ***mysqladmin -u root password NEWPASSWORD*** I receive this response: ***-bash: mysqladmin: command not found*** Is my input wrong?
You can try the Microsoft "AdventureWorks" sample database - see: http://sqlserversamples.codeplex.com/
[](file:///Users/hoganmarhoefer/Desktop/sqlcode.png)
Hey, thanks so much for your reply. This is actually brilliant! Really useful and pretty much exactly what I was looking for.
FYI if you have a WHERE condition on a column of the right table in a LEFT JOIN, then you're actually getting an inner join
&gt; Is there any way using sql to get it to look like to above one? no &gt; Or should I just try to do it externally using C++ where I iterate over the first table contents and generate the result for every row and then try to connect them together into one table? **no!** run this query -- SELECT users.id , users.last_name , users.first_name , operation.product , operation.quantity FROM users INNER JOIN operation ON operation.id_usr = users.id ORDER BY users.last_name , users.first_name , operation.product which produces these results -- 1 Adkins Jack Gloves 2 1 Adkins Jack Paper 1 1 Adkins Jack Screwdriver 1 2 Armstrong Alex Screwdriver 5 2 Armstrong Alex Paper 2 3 Black Luke Hammer 1 3 Black Luke Nail 1 3 Black Luke Paper 2 now in your application layer, loop over these results and use "current/previous" logic to decide when to print a new name
Seconded! Sql zoo is great. I like to go on every now and then and have some fun testing my knowledge. SQLWorkbooks.com by Kendra Little has some great courses that I like to follow along with to make sure I’m up to date. I have an entry-mid level interview test that I can email if you want. I have one for Oracle (Pl/SQL) and one for MSSQL (T-SQL)
&gt; -bash: mysqladmin: command not found it can't find 'mysqladmin' which means you need to either go to the directory with the binaries, or set your path. 
If you’re in your work’s network, you may already have read access through excel. Use the data tab, go to data from other sources and choose Microsoft Query. I learned on my work’s database, though it took a while to get write access. If that doesn’t work, you can also try using some of the data import wizards in Access. Microsoft always has a way to find their own servers easily. 
SQL Operations Studio is only a client to access databases with - it doesn't actually *host* a database itself. For that, you need an instance of SQL Server, PostgreSQL, MySQL, SQLite, etc. (note: at this time SQL Operations Studio can only be used with SQL Server). Once you have a service installed &amp; running, google for "&lt;yourdatabase&gt; sample databases".
Codeplex is going away very soon. Use https://github.com/Microsoft/sql-server-samples instead.
1) download SQL server express or developer edition to start up a SQL server instance for free on your own machine. (https://www.microsoft.com/en-us/sql-server/developer-tools) 2) download the StackOverflow database or the IMDB database (https://www.brentozar.com/archive/2015/10/how-to-download-the-stack-overflow-database-via-bittorrent/)
Well, copy the database to development ,and sanitize it. make it a different name, and make sure you are crushing production. 
show the query where you tried joins
Something like this... SELECT id, date, name, val1, val2, val3, val4 FROM ( SELECT val1 FROM T_VAL1 WHERE id = T_VAL1.id ) tbl1, ( SELECT val2 FROM T_VAL2 WHERE id = T_VAL2.id ) tbl2, ( SELECT val3 FROM T_VAL3 WHERE id = T_VAL3.id ) tbl3, ( SELECT val4 FROM T_VAL4 WHERE id = T_VAL4.id ) tbl4 WHERE date &lt; today This was one of my ideas, returned no results. I had a similar idea where I inner joined the subqueries together and tried to select the data, but that returned way too many (thousands) records. My test database should only pull about three records if it's working correctly.
 SELECT T_VAL1.id , T_VAL1.date , T_VAL1.name , T_VAL1.val1 , T_VAL2.val2 , T_VAL3.val3 , T_VAL4.val4 FROM T_VAL1 INNER JOIN T_VAL2 ON T_VAL2.id = T_VAL1.id INNER JOIN T_VAL3 ON T_VAL3.id = T_VAL1.id INNER JOIN T_VAL4 ON T_VAL4.id = T_VAL1.id WHERE T_VAL1.date &lt; CURRENT_DATE 
Thanks. I don't go back to work till Tuesday, so I'll give you an update on how it works then.
Don't forget world wide importers. A sample database with scripts provided by Microsoft. It is meant to replace adventure works and has quite a bit of cool features like temporal tables and json data. https://github.com/Microsoft/sql-server-samples/tree/master/samples/databases/wide-world-importers
Microsoft Virtual Academy has free video classes with sample databases. 
I feel pretty comfortable with the theory for the most part but I'm more worried about solving problems on a whiteboard because I think this is how most tech companies in my local market interview. I've been going through Leetcode SQL practice questions and getting my butt kicked with problems like calculate running averages, calculate top 3 salaries for each department, rank records by score but assign the same rank to records that have the same score, etc. I'm looking at the solutions and kind of understand what's going on, but I don't know if practicing these questions over and over again will help if I'm presented with a new problem in a white board interview. I'm the type of guy that can research solutions to problems but I won't necessarily always have the answer or be able to work it out by myself with a limited amount of time on a whiteboard.
I grew out of using the Access query builder a long time ago. My issue is that I'm getting my butt absolutely kicked by Leetcode questions, with or without Access. For example some questions include: find the top 3 salaries for each department, calculate running averages, switch the seat ID of students sitting adjacent to each other, etc. 
It took me a while to figure out but I bought teach yourself sql in 10 minutes from amazon. In the very back they gave a link for some scripts to download with preloaded data so you can practice the functions taught in the book. I appreciate all of the suggestions in trying to get better at sql if anyone stumbles across this and wants the link to the script to download send me a message 
What's the deal with premium and standard? What are we paying for? 
Experiment with installing your own SQL Server application on your own machine. I would reccommend MySQL or the latest developer edition of MSSQL. Being on Mac you may need a VM for MSSQL. Both should be free.
There's a docker image for ms sql as well to that point
SQL Operations Studio will allow you to login to a SQL Server instance from your Mac, but you still have to worry about actually starting a SQL Server instance. https://docs.microsoft.com/en-us/sql/sql-operations-studio/download Do you have like a random laptop that you can install SQL server 2014 Express on? That one doesn’t take up as much memory as 2026/17 express as it had less features.
Be very careful though, if you make a query that is too cumbersome, you can't not force quit from Excel and stop it, it will continue to run on the 400 or whatever they have and you'll get an angry call from IT. This is because MS Query doesn't stop running the script you wrote when you get out of it, the server keeps trying to compute it so if you write a query that will take a couple days to run, only IT can force quit the job off the server. Tl;Dr don't use MS Query unless you k ow what you're doing because if it hangs up quitting on your desktop doesn't stop it from querying on the server (slowing down the big one)
https://developer.twitter.com/en/pricing
SQL version is made for MySQL not MSSQL. 1. Install MySQL. 2. Download SQL version of your data. 3. Import it via MySQL Workbench.
TTT: When i first read this (incorrectlt) it reminded me of the boss I had who thought SQL was crap because no one could write the query that told him all the people who were not customers yet. I mean he had the list if customers, just write the query that gives him everyone not on this list. We all laughed and he could not understand why that was going to be difficult, despite 10 different people trying to explain it to him.
Bootcamps typically teach a very narrow curriculum leaving out a lot of what they consider superfluous material. But for $200 you’re not risking much and it can be a good introduction. For myself, I’m 100% self taught. I worked programming applications and databases as my bread and butter and learned most of what I know on the job or from books. Some of it driven by need, some of it by curiosity. If you just want something to tick a box on your resume then you might as well take the bootcamp. If you’re actually learning SQL to use it productively then teach yourself. 
Do some googling of the bootcamp and the person teaching at it and see what's out there about them and whether they've got a good reputation. If it's an SQL MVP or similar, then you're probably okay. Sometimes it depends on how you like to learn. I learn a lot from books, and running things myself, but User Groups or conferences can show me a lot that I haven't spotted in a book, didn't pay attention to, or I just learn better at the time. There are videos out there, on Pluralsight, YouTube, etc, that are okay for things like this. GroupBy.org has some conference videos. The hardest thing would be sifting through to find beginner subjects to build from.
Is there anything through your local community college?
I've had a few too many cocktails to come up with an optimal solution... but couldn't you joint the table back onto itself with a 'between' join and then sum and group on the primary columns date?
I would have taken a crack at this 2 bourbons ago. Saving for tomorrow if more help is needed!
I want something to put on my resume as well as actually have a good grasp of the language! I think I'll sign up for the bootcamp and see where it takes me. I think it can be hard to sift through online material much of the time, so I might be better off purchasing a book to learn from. Thanks for your help! 
I'll google the bootcamp and look into it, thanks so much for the tip. Yeah, I think online material can be hard to sift through. I might just be better off buying a book like you said and learning it that way. Thank you! 
There would be, I'll have to look into it, but it might be a bit pricey...haha 
https://groupby.org/watch-past-sessions/ GroupBy has some good intro sessions if you want basics, and some good intermediate stuff. Kendra Little’s SQLWorkbooks.com is also a great place to start/progress. Both are free, and both are pretty entertaining.
Thank you, this is helpful! I think I'll check these out before making the plunge to sign up for a course or anything like that. 
Sure thing, happy holidays!
Happy holidays!
Awesome, got it. Thanks so much! I'll let you know if I have any more questions.
Ok, I'm stuck again. I imported the file I want to work with but where do I actually run code on Workbench? [Here's where I'm at for reference](https://imgur.com/a/8Y5wI).
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/l6Bm4mR.jpg** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20drqh9uh) 
This is perfect for the `SUM OVER` syntax: SELECT YEAR(Date) "Year", SUM(Value) OVER (PARTITION BY YEAR(Date) ORDER BY MONTH(Date), DAY(Date) ROWS UNBOUNDED PRECEDING) "Aggregated sum"
This would only give me the sum of each year, correct? I need each rows individual trailing 12 month sum
Can you share some books/resources you used?
No idea how it's done on Mac but here's the [documentation](https://dev.mysql.com/doc/workbench/en/wb-admin-export-import.html).
I'm self taught. I would have done a bootcamp had they been an option when I was learning. They are a great way to kickstart your skills, give you a base to build on and introduce you to people that can help you network.
I've been recommended SQL for Mere Mortals and in the little bit of reading I've done, I think it's a great book for a beginner like myself
I am a complete novice and I have the below objectives. Any advice you have would be much appreciated. Primary Goal: Make a simple website that allows users to query based on specific sloth metrics (for example: sloth body weight, sloth ground speed, sloth population per country etc). The output would be a rankings table, which organizes each type of sloth from most to least, based on the metric(s) selected. Secondary Goal: Allow users to input their own sloth data, have it added to a private queue for my approval, once approved, have it automatically added to the database. For now, all I care about is my Primary Goal, but I would like to get my infrastructure built on a platform that will allow me to accomplish my Secondary Goal easily in the future. I don’t care about cost, but want to the developing myself (to learn the skill), and I want to be able to do it in the easiest and quickest way possible. Have a Merry Slothmas everyone! 
Does this do what you're trying to achieve? I'm still not completely clear on your goal, but this appears to be close given what I understand. http://sqlfiddle.com/#!6/541db/3/0 There's some diagnostic columns in there to give you an idea of what data is being included.
Any, it will make little difference; postgres if you can, otherwise whatever is left work or is best for the language stack you are comfortable with
Hahaha, this is great! Love it. Literally anything will do - MySQL is probably the easiest place to start - fantastically well supported everywhere and a huge quantity of available docs. 
Yes, I think so. I didn’t include a third column which is a security ID. Right now it returns hundreds of rows added.But I think I can adjust your code to account for that. Not sure why I didn’t include it in my first request—I’m dumb. I also have never heard of SQLfiddle so that was very cool. Thanks for your help! 
Thanks! 
Any of the popular ones mentioned are the way to go. You can do everything you'd need to in any database. The popular ones will have more tutorials though. 
Sure, go ahead and add it if you want some help figuring it out. Given that you're summing all of these, however, I'm not sure how you're going to include something like that without breaking your group on the date. If you give some additional sample data, and an example of what you would expect the output to be, I can certainly take a look.
Gracias 
Sankyuuu
I set up the data like you had it. This is more akin to my data.Basically for each ID, I'm trying to get the last year of values summed for each date (row) uniquely. CREATE TABLE Table1 ([Date] date, [Value] decimal(12,4),ID varchar (20)) ; INSERT INTO #Table1 ([Date], [Value]) VALUES ('2017-10-12', 0.265,1627384), ('2017-07-12', 0.265,1627384), ('2017-04-11', 0.265,1627384), ('2017-01-11', 0.265,1627384), ('2016-10-12', 0.26,1627384), ('2016-07-13', 0.26,1627384), ('2016-04-13', 0.26,1627384), ('2016-01-13', 0.26,1627384), ('2015-10-13', 0.24,1627384), ('2015-07-13', 0.24,1627384), ('2015-04-13', 0.24,1627384), ('2017-10-11', 0.265,2027384), ('2017-07-12', 0.265,2027384), ('2017-04-11', 0.265,2027384), ('2017-01-11', 0.265,2027384), ('2016-10-12', 0.26,2027384), ('2016-07-10', 0.26,2027384), ('2016-04-12', 0.26,2027384), ('2016-01-14', 0.26,2027384), ('2015-10-12', 0.24,2027384), ('2015-07-13', 0.24,2027384), ('2015-04-15', 0.24,2027384) Edit: Sample output would be 2017-10-12 (start date) , 2016-10-12(end date),sum over last year,1627384 (ID) 2017-10-11 (start date), 2016-10-12 (end date),sum over last year,2027384 (ID) repeating this for every row. Dates aren't exactly a year and gapped, which is why i initially thought id be using some variation of this DateADD(dd,-375,rowdate) and DateAdd (dd,-355,rowdate). Its best if 2 dates fall into that window, to only include the one that is 355 days back, and exclude the later one. But, just trying to get the core function downfirst. 
Pick a language/framework for your application, then pick a SQL database from there. MySQL called from PHP is the most common setup found on low cost shared hosting platforms, so it would be a solid pairing for a simple app. PostGRES has more similarity with the big boy databases like MSSQL/Oracle, but isn't as common out there. It's a strong recommendation because it will push you to be better, MySQL gives you a lot of easy outs that become bad habits when you move up to more enterprisey solutions.
I've added the data and added a join on the ID as well - I think this is what you're looking for? http://sqlfiddle.com/#!6/f330c/1/0 Is there a fixed number of data points that consists of "the last 12 months", or is it variable? Choosing where your cutoff is is going to be interesting unless you can more strictly define that period.
Hey there, this worked perfect! Thanks so much. yes, the dates are tough. The number of data points are variable, and the latest date I want to include can be "sometime around 1 year ago" If you're curious at all, the actual data value is dividends, the date is an exdate and the ID is a sedol. Thanks again
Sure. Good luck!
MySQL — codecourse has a lot of really great tutorials to get you going :)
That's a good point you make! Thank you for your help, I think I might look into SQL a little more on my own and try out the bootcamp if it still seems a viable option then. 
Arigatou
Sorry, I'm so confused. Let me know if I should ask someone else but I've searched everywhere and can't figure out how to run code. All I want to do is take a .sql file that has a ton of data and be able to create tables using SQL but I can't figure out how to do it.
Ask new specific question here or in /mysql.
For which version of MS SQL is this compatible?
8 and up https://docs.microsoft.com/en-us/sql/t-sql/queries/select-over-clause-transact-sql
Thanks. I’m not familiar with the preceding and following syntax. Will have to play with that!
PostgreSQL. An amazing relational DB that isn't controlled by anyone, unlike MySQL.
As in ALL the people who weren't customers yet? Like, the general population?
[SQL Cookbook](http://shop.oreilly.com/product/9780596009762.do) is a little dated but I think it is a very good read. There's a chapter on window functions and it really explained what's going on. If you understand the material in this book you are well equipped for any SQL task. I have no idea how bigquery works though so YMMV.
I know bigquery uses SQL standard, not sure if there is learning material that is more based around that. 
 Two things that greatly improved my SQL-fu. * The WITH clause. That's what Oracle calls it. Others call it CTE (Common Table Expressions). Makes big queries so much more readable. * Window functions. They are so powerful. Here's a sneak peek for some of the functions since I like the JOOQ guy's articles but this article wouldn't be my first introduction to window functions https://blog.jooq.org/2014/08/12/the-difference-between-row_number-rank-and-dense_rank/ 
Another dated but excellent text I usually suggest is *The Practical SQL Handbook: Using SQL Variants (4th Edition)*. It starts from the basics but leads you to some pretty impressive levels without every being dense or boring.
[SQL Indexing and Tuning](http://use-the-index-luke.com/) is another must read to learn SQL (Performance). Another one is [Modern SQL](http://modern-sql.com/) to understand latest SQL standard which will help you to write simpler SQLs.
CTEs do make queries more readable, but can hide bad design flaws and performance issues. From the optimizer’s perspective, they function the same as subqueries (potentially nested ones) and may be a bad way to handle things where you could better filter or stage the data. That said, they’re a really good intermediate step while learning more advanced methods and I was guilty of overusing they when I was a Dev. Couldn’t agree more on the window functions and pivots, though. That’ll teach you reaaaal fast. @OP, Brent Ozar is an amazing blog as is Paul Randall’s SQL Skills (he wrote a lot of fun he underlying functions the optimizer uses). Red Gate also has some amazing PDFs on performance tuning, which can also seriously up your game. Now, that said, take this with a grain of salt. I’ve worked very loosely with Oracle, My, and Mongo, but my bread and butter is MS T-SQL. I’m not positive, but my understanding was that Google’s was a form of NoSql better optimized for stuff like file search, though maybe I’m thinking of their proprietary engine. Any of the stuff I mentioned though applies to best practices across languages and platforms, you just might have to adjust some of the advice based on your language’s quirks.
Yeah, understanding the physical storage of heaps and indexes and how to craft indexes for common or intensive queries is an invaluable skill. Good recommendations!
&gt; From the optimizer’s perspective, they function the same as subqueries (potentially nested ones) and may be a bad way to handle things where you could better filter or stage the data. Indeed. Nothing is better than examining the actual query plan and trying to understand why the optimizer does what it does. 
&gt; SELECT * &gt; FROM Docs &gt; WHERE (VirusStatus &gt; 0) &gt; AND (VirusStatus IS NOT NULL) Msg 208, Level 16, State 1, Line 1 Invalid object name 'Docs'.
You can do some really neat stuff with it once you wrap your head around it. Makes variance metrics super easy when you have data coming in at different time intervals. One of the best examples I use it for is some auditing software that we use for our field organization - I have ~2500 locations doing an audit at different frequencies in a given year and can use this to calculate a "Variance to Last Audit" metric at the store level when one might be completed it every week, and another every 3 months.
Consider MariaDB. It is a branch of MySQL with a very enterprise driven development. Any first flavor will give you experience and the second flavor will really make you unstoppable. I only recommend Maria as a first because in my opinion it gives the user a balanced skill set between the web world and the enterprise one which can sometimes be a tough divide to cross. 
Yep, we asked him for a list of the entire population of the world...his response was that is what he was asking SQL to give him. SMH.
Yeah, learning to read plans might actually be the best way to improve, provides you’ve got a server where you have showplan rights. :)
Hahaha, Amazing. 
The only way is to use SQL for progressively more difficult tasks, and a good way if you don't have a mentor to guide your advancement in your current environment is to build an environment of your own that relates to a subject you know a lot about. Typical example I use are baseball stats. Spend some time figuring out what types of public data you can get on baseball stats. Build a database and import them. Work out a way to get new data in, etc. Then start asking progressively harder questions. Which Latino fielder had the highest batting average in the American League? What is the highest average of season over season for Latinos? How does that compare to the National League? Don't have players ethnicity in your data? Figure out how to get it. From there you can start getting into things like statistics and probabilities, etc. But you **need** to understand the data first. Once you do that the SQL side becomes fairly simple. You know what you need to do, just don't know how to do it... so you ask.
 SELECT a.id FROM TableA a LEFT JOIN TableB b ON b.id=a.id WHERE b.id is null
On mobile so apologies for any errors. Looks like you might be causing an issue by correlating the inner query to the outer. That's notoriously slow. Try something like select id from tablea where not exists (select id from tableb)
I appreciate it! 
I’m checking it out. Thanks. 
This is the correct answer. To elaborate on why it's the correct answer just in case OP is curious, this is using an outer join. Outer joins return all the records from either the specified left or right (in this case left) tables and will show null for any records that don't have a match to the other table you joined to. So here you're asking to see all of tableA with a join to TableB. If there are matching records in tableB, then b.id returns and ID. If it returns null, that means that this record in tableA exists but there is no match for it in tableB and therefore no b.id. By limiting the query to just where b.id is null, you limit the results to just those items in table A where this no matching record in tableB. This is also very efficient because you're using a join as opposed to a sub query which uses virtual table space and lots of memory.
That method would be even slower. Not exists should be correct here.
Perfect, thank you! :)
And thank you for the explanation, that makes sense. :)
Just a note on this: I was never able to find a GUI option in SQL Operations Studio to load a database from backup. (Let me know if anyone was able to find this). So for OPs case, they'd either need to load from backup from SQLCMD or SSMS. From the little that I've played around with SQL Operations Studio - it's more designed for DD's and Report Writers, and less for DBAs. 
So I think all of the above questions can be treated as theory. White board interviews are common. They're going to draw some tables and ask you how to get the results they want. They're not going to ask you for the exact syntax, just how/which tables to join and any necessary additional logic. Let's take, as an example, top three salaries in each department. Talk to what you need. You need to Group the results by department, sort them by salary, and show the top 3 across each group. They might ask a bit more about the logic across the grouping, but they're probably not going to ask you to write out the query. Ranking the record scores is a good one! Never been asked that. Off the cuff with no research I could get the result but I don't know if it's the 'best' way to do it. Again be able to identify the need and talk to it. The hard part here is giving the same scores the same rank. To do this I would make a view/sub query that does something like select distinct scores from table scores and order them by their value. I would also include in that select statement a bit of code that would assign an incremental Id, which I would treat as the rank, to each distinct score. I would then join the other table with the scores to this view on the scores themselves, but return the rank in the result and order it by that rank as well. So now any scores that are the same will be related to the same rank. Running averages can be a bit harder. I generally use sub queries. You sum everything where the date is less than or equal to the date of the record you're working with and divide it by the count of the number of records where the date is less than or equal to the date of the record you're working with. You could also use IDs instead of dates if you know they're populated in order of entry. Every time you advance a record you're including more data in your summary and average. The best thing you can do in an interview is vocalize your thought process. Let them know what and how you're thinking. That's what they want to see. You're problem solving skills are important to them. Getting a wrong answer but using a solid thought process will earn you some points with them. Just blurting out a wrong answer won't do you any favors. So that's why I say talk through the problem with the interviewer. Identify the challenges and how you plan to over come them. Tell them about the logic you will employ and why. Don't launch into a thesis, keep your answers short and concise. For example, you want to know the top three salaries by department, so we're going to need to group the results by the department of. Next I would sort by salaries in a descending order and then use this logic to limit it to the top 3. I've interviewed people for low level sql positions before. I was always more interested in their problem solving skills. If you can solve problems you can learn to write sql on the job. We even hired someone with no formal training in SQL because of this. I'd be happy to run you through a mock interview if you're interested. 
You can look into windowed aggregates to decrement the buy/sell orders in the logical order. Also, I wouldn't remove fulfilled orders, but just change the status instead.
You must for self joins https://technet.microsoft.com/en-us/library/ms177490(v=sql.105).aspx
play with: select id from table t where not exists (select 1 from table t2 where not state is null and t1,id = t2.id)
He never did understand why SQL could not magically make up data it didn't have. Likewise had to explain to way too many executives at the same comoany why installing a vendor BI solution that claims ability to see daily reports on a data system that only stores month end data is going to result in 30 days of reports that report the same thing. Told to shut up because the vendor says it can be done without changing the underlying data systems and watch them piss away millions of dollars and end up with...30 days of the same report. Then I get blamed for sabatoging the project, Finally said FTN and left.
&gt; I have a database with an ID Column and a State Column You mean one table? SELECT state from states s WHERE id is null
You have to reference them if you join to the same table twice. For example when your table has "PrimaryUser" "SecondaryUser" as FKs to the Users table. Of course, this can be better solved with decent normalization.
How would you solve primary user/secondary user with normalization?
Thanks for the help, this looks like what I want 
In our case, it needs a join table. (We keep adding columns when we have accounts that have a third/forth/etc. user)
I have not used Access in about 5 years, but it looks like there is a [Join function](https://support.office.com/en-us/article/Join-Function-e65fbc9a-c499-430d-a51a-bdf22140650f), which is what you're looking for. It should be something like Select Country &amp; '|' &amp; Join(';', City) From 1 Group By Country
Wait, you mean you have the data in the first format, but you want the table to look like the second format?
Why would you de-normalize the data? Hopefully just for a presentation layer, and this isn't a permanent change.
Everyone else has touched on the answer, so I'll add something slightly different. For best practices, if more than one table is involved or table expressions, it's best practice to always alias. Even if the query is a simple select person from table join company on personid = peopleid, you should always alias. 
Thanks! What tripped me up was when we have to use an alias we defined. In your example, although it is bad practice, would one be able to refer to the original table name later, even after an alias was defined?
This may be dependent on the RDBM system used. With SQL Server, you cannot refer to the original name after an alias has been used. I tested that with a few types of queries, but never mind being proved wrong. (That's when learning occurs.)
this returns each state that has even 1 null id
I tried something similar, but it doesn't return anything. The question is: Which States with Vendors have no Invoices? Here's the code I tried: SELECT VendorState, invoiceid FROM Vendors AS v1 LEFT JOIN Invoices ON V1.VendorId = invoices.VendorId WHERE invoiceId in (SELECT invoiceId FROM Invoices JOIN Vendors AS v2 ON Invoices.VendorId = v2.VendorId WHERE invoiceId is null AND v2.VendorState = v1.VendorState) Any reason why this wouldn't return anything?
&gt; you should always approach your coding with a set based method Not true at all, which is why we have recursive CTEs.
&gt; Which States with Vendors have no Invoices? is that REALLY the question or is the question also WHO are the vendors? Because if it's just the states: select distinct state from t where not exists (select * from invoices i where i.state = t.state) order by state states and vendors: select state, vendor from t where not exists (select * from invoices i where i.state = t.state and i.vendorID = t.vendorID) order by state 
The point I was trying to make was about the query structure he had; query a value, store it in a temp table, loop until complete, then select from temp table for all results. Terribly inefficient. However to your point, recursive CTEs can have their place but there are other ways to store a hierarchy in a database. If you are faced with a low insert, high read hierarchy, then the nested set data model by Joe Celko can be a great option that is completely set based and eliminates the need for recursion.
I tried your code, but again it returned nothing. Here's the code: SELECT Distinct VendorState FROM Vendors AS v1 JOIN Invoices ON v1.VendorId = invoices.VendorId WHERE InvoiceId NOT EXISTS (SELECT * FROM Invoices JOIN Vendors AS v2 ON Invoices.VendorId = v2.VendorId WHERE v2.VendorState = v1.VendorState)
 SELECT s.SubjectId , s.SceneId , s.InsertTime , loc.xPosition AS loc_xPosition , loc.yPosition AS loc_yPosition , loc.zPosition AS loc_zPosition , lcl.xPosition AS lcl_xPosition , lcl.yPosition AS lcl_yPosition , lcl.zPosition AS lcl_zPosition , rcl.xPosition AS rcl_xPosition , rcl.yPosition AS rcl_yPosition , rcl.zPosition AS rcl_zPosition , vis.xPosition AS vis_xPosition , vis.yPosition AS vis_yPosition , vis.zPosition AS vis_zPosition FROM Session AS s INNER JOIN Location AS loc ON loc.SessionId = s.SessionId INNER JOIN LeftControllerLocation AS lcl ON lcl.SessionId = s.SessionId INNER JOIN RightControllerLocation AS rcl ON rcl.SessionId = s.SessionId INNER JOIN Vision AS vis ON vis.SessionId = s.SessionId ORDER BY s.SubjectId , s.SessionId
One of the best online course. For free. With examples and online exercises. http://www.studybyyourself.com/seminar/sql/course/?lang=eng.
One of the best online course. For free. With examples and online exercises. http://www.studybyyourself.com/seminar/sql/course/?lang=eng.
Ahh never mind /u/r3pr0b8 I see that you've defined those at the end so RCL should be fine I'm now getting an error with the "inserttime" - error 1054 "unknown column 's.inserttime' in 'field list' - any idea why this would be?
Try something like this. SELECT x.* , CASE WHEN x.buy_amount - x.prior_amount &lt; x.amount THEN x.buy_amount - x.prior_amount ELSE x.amount END AS applied_amount , x.amount - CASE WHEN x.buy_amount - x.prior_amount &lt; x.amount THEN x.buy_amount - x.prior_amount ELSE x.amount END AS remaining_amount FROM (SELECT s.* , b.amount AS buy_amount , b.price AS buy_price , ISNULL(SUM(s.amount) OVER (ORDER BY s.created ROWS BETWEEN UNBOUNDED PRECEDING AND 1 PRECEDING),0) AS prior_amount FROM orders s INNER JOIN (SELECT TOP(1) * FROM orders a WHERE a.kind = 'buy' ORDER BY a.created) AS b ON s.price &lt;= b.price WHERE s.kind = 'sell') x WHERE x.buy_amount - x.prior_amount &gt; 0;
&gt; error 1054 "unknown column 's.inserttime' you said this column would be the same across all tables did that not include the Session table?
I looked back at the time stamps from each table and manually pasted together. It does appear that occasionally there are situations where one time stamp may include a different session for each hand...so I guess it isn't *always* the same. Is there a way to modify so that the tables are only joined *if* the session table lines up? Also thank you so much for this help!!!
Jesus that sounds bad man, sometimes people don't want to hear the truth. Myself included at times lol. 
The error means there is no column 'inserttime' in the session table. Not a problem with the data.
Hmm if the 'inserttime' lives outside of the session table, what would I do to organize the data by the 'inserttime' data?
&gt; Hmm if the 'inserttime' lives outside of the session table oaky, where is it?
If you are using /u/r3pr0b8 query above just replace the s With the table where the field resides.
New thought, COLUMN aliases cannot be referred to anywhere except the order by. 
Since this is still theory - so I wont write out queries in syntax. What code you use to interact is your call as long as it can handle the logic. It's probably better to do it with two separate databases, one for your employees and another to track when they've been scheduled. I assume, based on what you've said: * All employees in this list serve the same role. * All employees in this last work only half day shifts * You have enough employees to make everything work out for the 2 week period * Unless my math fails this should be 14 employees? (I'm assuming calendar week - not business week) - though actual number doesn't matter. * You want this to be assigned daily, vs all at once. * You only one employee available at a time, no overlap. You'd need a database for your employees with a unique ID. EmployeeId for the primary key should work. Then have a 2nd database with a foreign key of the EmployeeId from the first table, and time of last scheduled. Since this table will be super small and only get 2 entries a day you don't need to worry about actively pruning the data. Pick a list of employee IDs based on your criteria: * Join the two tablets based on employeeId * only pull a list of employeeIds that meet your criteria (not scheduled in the last 48 hours, and not scheduled more than once in the last 2 weeks) * randomly choose 2 employeeIds from this list using your code of choice * update the scheduled table with a new entries for those employeeids and scheduled times * repeat daily I might have missed something with this but works in my head. 
Thank you!
Oh I getcha now. SELECT state from States s WHERE state not in (SELECT state from States where ID is not null)
You’re joining a ton for next reason. Build it up slowly from parts. Also, it’s not “where x not exists...”, it’s “where not exists (select something)”
your query is fine can you do a SHOW CREATE TABLE for both tables please
[removed]
The first table +---------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Table | Create Table | +---------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | batting | CREATE TABLE `batting` ( `playerID` varchar(255) DEFAULT NULL, `yearID` int(11) DEFAULT NULL, `stint` int(11) DEFAULT NULL, `teamID` varchar(255) DEFAULT NULL, `lgID` varchar(255) DEFAULT NULL, `G` int(11) DEFAULT NULL, `AB` int(11) DEFAULT NULL, `R` int(11) DEFAULT NULL, `H` int(11) DEFAULT NULL, `2B` int(11) DEFAULT NULL, `3B` int(11) DEFAULT NULL, `HR` int(11) DEFAULT NULL, `RBI` int(11) DEFAULT NULL, `SB` int(11) DEFAULT NULL, `CS` int(11) DEFAULT NULL, `BB` int(11) DEFAULT NULL, `SO` int(11) DEFAULT NULL, `IBB` varchar(255) DEFAULT NULL, `HBP` varchar(255) DEFAULT NULL, `SH` varchar(255) DEFAULT NULL, `SF` varchar(255) DEFAULT NULL, `GIDP` varchar(255) DEFAULT NULL, `OBP` double(4,3) DEFAULT NULL, `SLG` double(4,3) DEFAULT NULL, `OPS` double(4,3) DEFAULT NULL ) ENGINE=InnoDB DEFAULT CHARSET=utf8 | +---------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ 1 row in set (0.00 sec) The second table +-------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Table | Create Table | +-------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | battingtest | CREATE TABLE `battingtest` ( `playerID` varchar(255) DEFAULT NULL, `yearID` int(11) DEFAULT NULL, `stint` int(11) DEFAULT NULL, `teamID` varchar(255) DEFAULT NULL, `lgID` varchar(255) DEFAULT NULL, `G` int(11) DEFAULT NULL, `AB` int(11) DEFAULT NULL, `R` int(11) DEFAULT NULL, `H` int(11) DEFAULT NULL, `1B` int(11) DEFAULT NULL, `2B` int(11) DEFAULT NULL, `3B` int(11) DEFAULT NULL, `HR` int(11) DEFAULT NULL, `RBI` int(11) DEFAULT NULL, `SB` int(11) DEFAULT NULL, `CS` int(11) DEFAULT NULL, `BB` int(11) DEFAULT NULL, `SO` int(11) DEFAULT NULL, `IBB` varchar(255) DEFAULT NULL, `HBP` varchar(255) DEFAULT NULL, `SH` varchar(255) DEFAULT NULL, `SF` varchar(255) DEFAULT NULL, `GIDP` varchar(255) DEFAULT NULL, `OBP` double(4,3) DEFAULT NULL, `SLG` double(4,3) DEFAULT NULL, `OPS` double(4,3) DEFAULT NULL, `wOBA` double DEFAULT NULL ) ENGINE=InnoDB DEFAULT CHARSET=utf8 | +-------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ 1 row in set (0.00 sec) 
The first table | batting | CREATE TABLE `batting` ( `playerID` varchar(255) DEFAULT NULL, `yearID` int(11) DEFAULT NULL, `stint` int(11) DEFAULT NULL, `teamID` varchar(255) DEFAULT NULL, `lgID` varchar(255) DEFAULT NULL, `G` int(11) DEFAULT NULL, `AB` int(11) DEFAULT NULL, `R` int(11) DEFAULT NULL, `H` int(11) DEFAULT NULL, `2B` int(11) DEFAULT NULL, `3B` int(11) DEFAULT NULL, `HR` int(11) DEFAULT NULL, `RBI` int(11) DEFAULT NULL, `SB` int(11) DEFAULT NULL, `CS` int(11) DEFAULT NULL, `BB` int(11) DEFAULT NULL, `SO` int(11) DEFAULT NULL, `IBB` varchar(255) DEFAULT NULL, `HBP` varchar(255) DEFAULT NULL, `SH` varchar(255) DEFAULT NULL, `SF` varchar(255) DEFAULT NULL, `GIDP` varchar(255) DEFAULT NULL, `OBP` double(4,3) DEFAULT NULL, `SLG` double(4,3) DEFAULT NULL, `OPS` double(4,3) DEFAULT NULL ) ENGINE=InnoDB DEFAULT CHARSET=utf8 | The second table | battingtest | CREATE TABLE `battingtest` ( `playerID` varchar(255) DEFAULT NULL, `yearID` int(11) DEFAULT NULL, `stint` int(11) DEFAULT NULL, `teamID` varchar(255) DEFAULT NULL, `lgID` varchar(255) DEFAULT NULL, `G` int(11) DEFAULT NULL, `AB` int(11) DEFAULT NULL, `R` int(11) DEFAULT NULL, `H` int(11) DEFAULT NULL, `1B` int(11) DEFAULT NULL, `2B` int(11) DEFAULT NULL, `3B` int(11) DEFAULT NULL, `HR` int(11) DEFAULT NULL, `RBI` int(11) DEFAULT NULL, `SB` int(11) DEFAULT NULL, `CS` int(11) DEFAULT NULL, `BB` int(11) DEFAULT NULL, `SO` int(11) DEFAULT NULL, `IBB` varchar(255) DEFAULT NULL, `HBP` varchar(255) DEFAULT NULL, `SH` varchar(255) DEFAULT NULL, `SF` varchar(255) DEFAULT NULL, `GIDP` varchar(255) DEFAULT NULL, `OBP` double(4,3) DEFAULT NULL, `SLG` double(4,3) DEFAULT NULL, `OPS` double(4,3) DEFAULT NULL, `wOBA` double DEFAULT NULL ) ENGINE=InnoDB DEFAULT CHARSET=utf8 |
There should be an option in the Groupings area (at the bottom using Report Builder) to break/repeat the header for the specified group. 
thanks your batting table is missing columns `1B` and `wOBA` but this should not affect how your query works &gt; but when I check the destination table all of the entries just say NULL. most of them should!! what's puzzling to me is that you're inserting a row containing only 3 stat columns, but forgetting the player and year they belong to! are you sure you don't actually want to do an UPDATE instead of an INSERT? 
Off topic, but that looks like OOTP data. Anyways you need the key columns and as someone mentioned you probably want an update
May not related to question, `orders.id` is primary key no it is already has `unique` index on it.
1. Your code is a SQL injection vulnerability. 2. Examine the string that's actually being sent into your database, don't just assume it's being created the way you think it is. Then try running that query direct against the database.
SELECT * FROM `contactss` WHERE phone_cellphone_c LIKE '%9192705402%' is crashing in phpmyadmin too &amp;nbsp; the LIKE is crashing the hell out of it for some reason &amp;nbsp; looks like it's working on phpmyadmin when i do SELECT * FROM `contacts` WHERE phone_cellphone_c LIKE '%(919) 270-5402%'
Definitely not enough information to go on here. What is "doesn't work"? What is the field type in the database? What are you trying to do? Simply match by number? In that case, replace the extra characters and compare the integer strings perhaps? CONTAINS wouldn't get you much further if your strings are stored as integers because the integer 2525608058 doesn't contain (252) 5608058.
SELECT * FROM `contacts` WHERE phone_cellphone_c LIKE '%9192705402%' is crashing in phpmyadmin too &amp;nbsp; the LIKE is crashing the hell out of it for some reason &amp;nbsp; looks like it's working on phpmyadmin when i do SELECT * FROM `contacts` WHERE phone_cellphone_c LIKE '%(919) 270-5402%' wondering if the API is trying to sanitize my input or something
You should be sanitizing your own inputs by parameterizing the queries. The language isn't going to do it for you. I don't understand what you mean by "crashing" either. No rows returned? or literally the program crashes? 
Define "crashing the hell out of it". An erroneous query shouldn't crash your database, nor the client. If it's able to do either of those, you've got garbage software. If you're getting an error, post the error.
Coalesce but why
Thanks for the response, using my original code, how can I replace the extra characters? 
Your "original code" is flawed in the first place because, as /u/Cal1gula and I both pointed out, you aren't sanitizing your input by parameterizing the query. This will lead to a SQL injection attack.
I'm sorry. I'm pretty new (as you can tell) Can you explain how I should sanitize for best practices?
This site has a pretty good explanation: http://bobby-tables.com/php That won't solve whatever problem you're having though as you haven't even described to us what error you are getting or even if you are getting any rows returned or given us any piece of information that might help us help you fix the problem...
Okay, I have taken a look into it. &amp;nbsp; I have it working so that, for every invoice number selected it splits it up by page but that not exactly what I want. &amp;nbsp; (When Selecting one invoice number)When I select an invoice number, it displays the report fine, one page with all of the information. (When selecting more than one invoice number) When I select multiple invoice numbers, the "header information" (Customer info. Invoice title, sate and sales rep.) one pics the information from the last selected invoice number. Also, all items from all orders display in the body table and the subtotal, tax and total include all invoices when. &amp;nbsp; The main problem when selecting multiple invoice numbers: &amp;nbsp; * When I navigate through report pages, I want it to assign each page one of the selected invoice numbers * For each page, display only the respective information (invoice header and data) * Should I have the totals for the invoice in the table that lists the invoice items or in a separate textbox (if so, how can I grab values from the table?)
Hey man, thanks for the replies. Your code returned way too many values and when I did some distinct stuff it didn't return the correct ones. Here is what I did with the other guys that made it all work. Terribly inefficient? Probably / most definitely. Does it work? Yes. The reason for the temp tables is because I can't embed recursions in VBA code to pull the values to the excel book. Maybe there is a way, but I haven't figured it out. So then I just pull from the temp tables which store the info. use ermdb go ------------------------------------------------------------------------- ------------- /* create temp ID table to have distinct delegates from controls table */ create table #tempID ( Delegate nvarchar(100), M0 nvarchar(100) ) ; /* insert distinct delegate values from controls table */ /* also grabs control owners who are their won delegate */ insert into #tempID select distinct Control_Delegate, Control_Delegate_M0 from Controls ; --drop table #tempID ------------------------------------------------------------------------- ----------- /* store Parent/Child columns in temp table from EmployeeCTE */ create table #depthID ( MGR_Name nvarchar(100), EMP_Name nvarchar(100) ) ; --drop table #depthID ------------------------------------------------------------------------- ---------- create table #depthChart ( EMP_Name nvarchar(100), depth nvarchar(100) ) ; /* Recursion to get parent child relationships for entire hierarchy */ /* Runs on distinct control delegates to create approval paths */ with EmployeeCTE as ( select EMP_M0, EMP_Name, MGR_M0, EMP_M0 as ORIG_EMP_M0, 1 as ORDER_NO from EMP_MGR where EMP_M0 in (select M0 from #TempID) union all select EMP_MGR.EMP_M0, EMP_MGR.EMP_Name, EMP_MGR.MGR_M0, EmployeeCTE.ORIG_EMP_M0, EmployeeCTE.ORDER_NO + 1 from EMP_MGR join EmployeeCTE on EMP_MGR.EMP_M0 = EmployeeCTE.MGR_M0 ) /* insert into temp table for reference and to make depth counts in future CTE */ insert into #depthID (MGR_Name, EMP_Name) select distinct E2.EMP_Name as Mgr_Name, E1.EMP_Name from EmployeeCTE E1 left join EmployeeCTE E2 on E1.MGR_M0 = E2.EMP_M0 and E1.ORIG_EMP_M0 = E2.ORIG_EMP_M0 order by Mgr_Name option (maxrecursion 0) /* Look at values */ select * from #depthID ------------------------------------------------------------------------- -------------- /* Get tier counts by counting depth in tree */ ; with node_mgrCTE(EMP_Name, MGR_Name) as ( select EMP_Name, MGR_Name from #depthID union all select nm.EMP_Name, dID.MGR_Name from node_mgrCTE as nm, #depthID as dID where dID.EMP_Name = nm.MGR_Name and dID.MGR_Name IS NOT NULL ) /* Put values in #depthChart for reference */ insert into #depthChart (EMP_Name, depth) select EMP_Name, count(MGR_Name) as depth from node_mgrCTE group by EMP_Name; /* Look at values */ select * from #depthChart ------------------------------------------------------------------------- --------------- /* Delete tempID table at end so rebuilt in future runs */ drop table #tempID /* Delete depthID table at end so rebuilt in future runs */ drop table #depthID /* Delete depthChart table at end so rebuilt in future runs */ drop table #depthChart
 SELECT payments.Order_Number , payments.Check_Num , payments.Amt FROM transactions AS payments LEFT OUTER JOIN transactions AS voids ON voids.Order_Number = payments.Order_Number AND voids.Check_Num = payments.Check_Num AND voids.Payment_Type = 'Void' WHERE payments.Payment_Type = 'Payment' AND voids.Order_Number IS NULL 
Do you have Invoice Number as the top most grouping item?
On the table yes.
This works perfectly. Thank you so much. For some reason, self-joins are hard for me to grasp. May I ask why we're specifically looking for NULL order numbers from voids? Is there a way to visualize this? 
visualize join behaviours here -- https://www.sitepoint.com/simply-sql-the-from-clause/ scroll down to LEFT OUTER JOIN checking for IS NULL means you want *only* unmatched rows, which in your case is payments without a matching void
If you have 20 dozen columns in a table and you populate 3 of them with data then 17 will be NULL. 
What is your end goal with databases and learning it? All of the major DB's have free versions. You can get free Oracle, SQL Server, MySQL, Postgres, DB2, Sybase, Maria, etc etc. 
you run it through a select where the A &lt;&gt; Codepage(A) similar to finding unicode values in columns SELECT COLUMN_1 FROM [P].[Q] WHERE CAST(COLUMN_1 AS VARCHAR(80)) &lt;&gt; CAST(COLUMN_1 AS NVARCHAR(80));
Oh wow I didn't realize they have free versions. That's awesome. I don't have much of an end goal right now. I don't have any practical application for it currently, but future jobs I want seem to prefer it as a skill. I work for a tech company in New York in client services/business development. I hope to transition to more of analytical position at some point, which is why I want to practice now. I know the question(s) were pretty open ended, as there isn't anything in particular I need SQL for at the moment. Just like to learn new skills and want to know (generally) a good way to go about improving upon it
I would look at job postings nearby and see what seems to be the most prominent flavor or find out what they use in shop at your work and use that to train. Then I'd get the free version of that software and practice, the training route I'd recommend is based on the platform you choose. SQL in 10 min book goes over some basics and is based on the standards, you could use it with almost any platform but it's not a lynda video which you get a free subscription with. I'm biased, but I found Oracle / SQL Server / MySQL to be the easiest to jump into. 
Yes I had done that! MySQL seems to be most sought after, which is where I would like to get started. My operating system doesn't support the free 2017 version available. Do you know if there are older versions available to free download?
What's your OS? [Versions and compatibility](https://www.mysql.com/support/supportedplatforms/database.html) [The Database Engine](https://dev.mysql.com/downloads/mysql/) [The Database Interface](https://dev.mysql.com/downloads/workbench/)
W3schools is great for basics. Also look at WiseOwl on YouTube
Ah got it. Hadn't yet tried MySQL. Thank you!
&gt; Yes I had done that! MySQL seems to be most sought after, which is where I would like to get started. And then &gt; My operating system doesn't support the free 2017 version available. Did you mean to say SQL Server?
Pick a dataset you're interested in. Download it. Put it in tables. Ask it questions. Here are some public datasets for inspiration: https://aws.amazon.com/datasets/ https://www.kaggle.com/datasets/ https://cloud.google.com/bigquery/public-data/
Yep I most certainly did. My bad
Are you just on windows 7 then? [SQL Server Requirements](https://docs.microsoft.com/en-us/sql/sql-server/install/hardware-and-software-requirements-for-installing-sql-server) [SQL Server DB download](https://www.microsoft.com/en-us/sql-server/sql-server-downloads) [SQL Server GUI](https://docs.microsoft.com/en-us/sql/ssms/download-sql-server-management-studio-ssms) If you are installing SQL 2008R2, you don't need to download two separate files, it's all in one, you just have to install it right. You would probably need this version if you are on Windows 7. [SQL 2008R2 download](https://www.microsoft.com/en-us/download/details.aspx?id=30438) Let me know if you have problems installing it. If you want some mock DB's, Adventureworks is the popular 2014 and below DB. World wide importers is the new one for 2016 on. https://msftdbprodsamples.codeplex.com/
Good post friend. This needed to be put up in a recent thread
What’s your current query command you are using? You’ll need to join the two tables at their primary and foreign keys which is usually some sort of ID field to extract that field. Also spends on the information you are trying to obtain in what manner. Need more context.
Could you actually describe for us what you need and what you have to work with? Your image isn't very helpful. 
Think of self joins this way. This could have easily been two tables, one containing payments, and the other containing voids. It was just a matter of convenience to store them in one table.
Yeah that diagram isn’t helping, can you describe in words what you want to achieve?
My solution attempt was: SELECT * FROM T1 LEFT JOIN T2 ON T1.Name = T2.Name AND NOT (T2.StartHour &gt;= T1.StartDate AND T2.StartHour &lt; T2.EndDate) Then just grab where there are no nulls. That conditional can evaluate to true where in the row from T2 hits false for T1 row 1, but T1 row 2, 3, 4,... it is actually true, so I get 4 results back or whatever. Or it just doesn't evaluate it correctly because It doesn't look at all rows in T1.
select a.username, b.nota from admin a, notasavarias b where b.iDAvaria=thevalueyouhave 
Hey Bacon, Thanks for the reply, this has proved to be very helpful! I am in the process of writing out my queries and I had a few questions, sorry I'm a major SQL noob :(! I'm trying to create the query that will select 2 employees, A: That haven't worked in the past 24 hours, and B: That haven't worked more than once in the last 2 weeks. A: I think I have successfully managed to only select employees that haven't worked in the past day. B: I'm really not sure how I would actually go about doing this, would I need a start DATETIME &amp; end DATETIME? Any suggestions on this would be greatly appreciated, I've included my SQL below! http://sqlfiddle.com/#!9/e2a2f5 
Without seeing the query, my vote is there is some funky conditions/logic going on you're not stating; I can almost guarantee the query plan generation timed out from over-complexity and just gave you a trivial plan. Rarely do I see these type of performance issues with *simple* queries and have it exclusively be the fault of the SQL engine, regardless of platform. Making assumptions about what it 'should' magically know about, because it's obvious to a human looking at it, is a good way to have a bad time. That said, a non-clustered index around said column would likely remedy the issue.
hmmm... i guess i will put down the original problem here. Background: cargo industry use a container (ULD number) to store shipment (AWB number) and shipment come in different pieces count. Inbound flight to cargo terminal, it will require to break down and rebuilt by mixing with different cargo to the same destination and uplift into the outbound flight. Some containers require no touching. I need to find out the unit being untouched. Simplified version!! The data in the data warehouse organize like below. ULD no| AWB | pieces | type 12345 | a | 9 | arrival 12345 | b | 2 | arrival 22222 | c | 1 | arrival 12345 | a | 9 | departure 12345 | b | 2 | departure Inbound flight and outbound flight we both find a unit 12345|a,9:b,2 we can say this is untouched. So basically i need to stack the awb number to create a new unique ID for container... ID being container number what in the awb and pieces as well. Right now i do the stacking of awbs under container in the excel. Results are making business sense. It might not be 100% close to reality due to operational reason but we are okay to accept this result. I was wondering if there is any smarter way to do this. 
Post the query plan and schema. http://pastetheplan.com/ is good for the former. &gt;(This was on SQL Server 2016 Express edition by the way, which I use for dev purposes.) You're depriving yourself. SQL Server 2016 (and newer) Developer Edition is free to use.
You should be able to use the BETWEEN operator for datetime. If you select a count of empID and group by empID you can add a HAVING clause for less than 2. So you're effectively pulling a list of people who have a lastBAU between 2 days ago and 2 weeks ago, but haven't appeared more than once. Is that what you're looking for? 
Thanks; I've broken down the query to have as little code as possible but still being able to reproduce the slow performance when using a subquery instead of a join. I'm terrible at reading execution plans (never really had to). First of all, here's the full query: --declarations DECLARE @reportEndDate AS DATE DECLARE @accountNumbersWIP AS TABLE(CompId INT, AccNo VARCHAR(10)) DECLARE @accountEntries as TABLE(AId INT INDEX IX1 NONCLUSTERED, JobID INT INDEX IX2 CLUSTERED, AccNo VARCHAR(10) INDEX IX3 NONCLUSTERED, Amount MONEY, PostDate DATE INDEX IX4 NONCLUSTERED, EmpId INT INDEX IX5 NONCLUSTERED, INDEX IX6 NONCLUSTERED(JobID,AId,AccNo)) --user input SET @reportEndDate = '2017/11/30' --helper tables INSERT INTO @accountNumbersWIP (CompId, AccNo) VALUES (1, '120101'), (1, '120111'), (1, '119204'), (12561, '120301'), (12561, '120311'), (12561, '119204'), (12562, '120301'), (12562, '120211'), (12562, '119204') --pre filter account entries INSERT INTO @accountEntries SELECT ISNULL(fje.AId, 100), fje.JobID, fje.AccNo, fje.Amount, fje.PostDate, fje.EmpId FROM FinJournalEntry AS fje --WHERE --fje.JobId IS NOT NULL --main query SELECT TOP 1000 job.JobID, ( SELECT ISNULL(CEILING(SUM(fje.Amount)), 0) FROM @accountEntries AS fje WHERE fje.AccNo IN (SELECT AccNo FROM @accountNumbersWIP WHERE CompId = job.SelId) AND fje.JobId = job.JobId AND fje.PostDate &lt;= @reportEndDate ) as 'SlowQuery' , ( SELECT ISNULL(CEILING(SUM(fje.Amount)), 0) FROM @accountEntries AS fje JOIN @accountNumbersWIP AS anw ON anw.AccNo = fje.AccNo AND anw.CompId = job.SelId WHERE fje.JobId = job.JobId AND fje.PostDate &lt;= @reportEndDate ) as 'FastQuery' FROM Job as job WHERE Job.DebitFlg = 1 The reason why I use temp tables like @accountNumbersWIP is to avoid hard coding and/or using CASE statements to filter the @accountEntries table on each subquery. It is easier to read (and adapt later on); I know it's totally unnecessary but I thought I'd try it this way instead for once. I'm very well aware that the temp tables (especially the indexes) might not be ideal, but that's outside the scope of this question. In addition to the query, here are the two plans: Slow query: https://www.brentozar.com/pastetheplan/?id=S1WbDTmXf Fast query: https://www.brentozar.com/pastetheplan/?id=Sy3VwaXmG The difference between the two subqueries is that the fast one uses a JOIN on @accountNumbersWIP where the slow one uses a subquery on @accountNumbersWIP. Basically I just need the sum of all account entries of three accounts per job. When looking at the slow subquery, the only column that gets referenced from the outer query is "Job.SelID". This, however, is a column from the topmost query (so two levels above) and is not derived from the first subquery that targets @accountEntries - which is why I thought it would be optimized; Especially since the table is very small, only containing 9 lines with 2 columns. Ultimately it boils down to: WHERE fje.AccNo IN (SELECT AccNo FROM @accountNumbersWIP WHERE CompId = job.SelId) vs JOIN @accountNumbersWIP AS anw ON anw.AccNo = fje.AccNo AND anw.CompId = job.SelId Again, this is not a necessarily a question on HOW to fix the slow query. I know how to fix it by using a JOIN and I could also just hardcode the account numbers eliminating the need for a join/subquery entirely. I've written the query this way to keep it as dynamic as possible (the primary idea was to have all the helper tables filled according to user input supplied by a reporting tool) to see if it would work, so half of it was more like an excercise. But now that I don't know WHY it doesn't work it haunts me. I'm 99% sure that I'm missing something and that I am the culprit but I can't figure it out :)
&gt;I'm very well aware that the temp tables (especially the indexes) might not be ideal, but that's outside the scope of this question. No, it's well within the scope of your question because those **table variables** (you have no temp tables) are most likely contributing to the problem. On SQL Server up through 2012, a table variable is assumed by the query optimizer to have only one row; for 2014 and higher, it's assumed to be 100 rows. So if you've got 15,000 rows in that table variable, you're going to get a bad query plan due to bad statistics and cardinality estimates. Your slowdown is coming from the full scan of `@accountEntries`. The optimizer has no clue about as to the number of records in that table variable nor how many are applicable to the search you're performing. Use a temp *table* instead (`#accountEntries`), index your join field(s), and use the join. In most cases where people use table variables, they would be better-served with a temp table. Temp tables behave the same as "real" tables, including the ability to have primary keys, indexes, and statistics. They just evaporate at the end of the session. I cannot tell you how many queries I've sped up *significantly* by using temp tables instead of table variables or CTEs.
Hey Bacon, thanks again for your help, it's much appreciated! :) As the brief was somewhat vague, I have asked some questions and developed a more detailed brief/specification: &gt; A full two-week rota is produced when the 'Generate Rota' button is clicked on the website, which allocates two staff members to work half a day of support each. Once clicked, the BAU table is populated with the rota, ensuring that engineers do not complete more than one half shift in a day, engineers do not have half day shifts on consecutive days and finally that all 10 engineers complete a whole day of support in any 2-week period. &gt; The rota is displayed in a chronological table on the website. The rota page can be viewed anyone with suitable privileges. Thank you in advance!
Yo, 92 called, they want their standards back. SELECT ns.idAvaria, a.username FROM dbo.notasavarias as ns INNER JOIN dbo.admin as a ON a.ID = ns.idAdmin
Yep, just on Windows 7. Successfully downloaded MySQL. A bit tough to navigate but it's been fun even trying. Will look into SQL Server as well over the long weekend on my home machine. Really appreciate all of your help!
Both are good, I've used a bunch of DB's professionally. I think my favorite for home use is MySQL but I'm definitely much deeper with SQL Server. Just follow best practices and ANSI standards, that will let you transfer your code cross platform and you can take those ideologies / practices with you.
oh i never knew you could make explicit joins aswell. wow such great code.
Thank you very much for the reply. I suspected it might have something to do with the temp tables being declared as variables but I read somewhere that they are treated equally in pretty much every respect. The reason why I didn't investigate further is quite simple: I am not allowed to create temp tables on the production server where this query will run eventually :) This is the first time I had to use variables. I'll have to try to eliminate @accountEntries entirely and use the real table instead. It simply contains way, way more data than I need and I thought it'd be faster if I removed all the unnecessary rows beforehand instead of filtering it in every single subquery later on (especially because I have to filter by PostDate and unfortunately that column has no index). With the temp table I have only what I need (roughly a tenth of the actual rows in the real table) AND have an index to work with); Looks like that assumption isn't necessarily true when not using a real temp table. Oh well. Also thanks for the tip regarding the TOP 1000. I had to learn this the hard way a few years back. In this case I simply included it so I didn't have to wait 20 minutes to get the execution plan :) One more question: Where can you find the exact (or at least some in-depth) documentation on how the optimizer will work? I spent quite some time yesterday searching for it but didn't find anything useful. Things like assumptions made by the optimizer are a complete mystery to me (as is the whole execution plan in general to be honest) and I really need to start learning about this stuff. 
What engine? If it's MS Sql Server, then there's a couple options. In my opinion, best solution is to use a Windows login on both servers as there will never be a SID mismatch. If using SQL Logins, then you have options. First is to run "sp_change_users_login" after restoring the backuo. This syncs the SID in the restored database with the SID stored in the REPORT instance. Better yet would be to drop the login on the REPORT instance and recreate it by specifying the SID that's defined to the login on the PRODUCTION instance.
&gt; I suspected it might have something to do with the temp tables being declared as variables but I read somewhere that they are treated equally in pretty much every respect Be very careful about trusting that source again. That advice is tragically wrong. They are in no way, shape or form "treated equally" and MS only added table variables reluctantly to "keep up with the Joneses" - they aren't putting much effort into making them better. Have a look at https://sqlperformance.com/2017/04/performance-myths/table-variables-in-memory and especially the table at the bottom (Derik links to a dba.stackexchange.com post worth reading too). I know Wayne Sheffield has some material on this subject as well. &gt;I am not allowed to create temp tables on the production server where this query will run eventually Absolutely not true. If you can run a `select`, you can create a proper temp table (prefixed with `#`). If you have a policy stating that it's not allowed, your DBA (or whoever created that policy) should be fired. From a cannon. Into the sun. &gt;I'll have to try to eliminate @accountEntries entirely and use the real table instead. It simply contains way, way more data than I need If the table is indexed in such a way that you can make use of it, that may not be a big problem. If it's not, the temp table is a good way to go. Or, if this is a query that'll be run a lot, that's a good argument for creating the proper index or tuning an existing one to meet your requirements. &gt;Where can you find the exact (or at least some in-depth) documentation on how the optimizer will work? Heh. It's kind of a black box. You can experiment with things to see what you can do to make it do various things, but aside from the high-level stuff like this, it's mostly gained by experience and then sharing that experience. IOW - test, blog about what you find, read other peoples' blogs (Kimberly Tripp, Paul Randall, Kalen Delaney, Joe Sack, everyone at Brent Ozar Unlimited (especially Erik) are five names that come to mind immediately for deep internal stuff), get out to user group meetings and SQL Saturdays and talk to folks.
This might be what you're looking for. [Found here](https://blogs.msdn.microsoft.com/blakhani/2008/02/25/script-map-all-orphan-users-to-logins-else-create-login-and-map/) USE &lt;yourdatabasehere&gt; GO SET NOCOUNT ON -- Declare Variables DECLARE @user_name NVARCHAR(128), @login_name NVARCHAR(128), @err_msg VARCHAR(80), @str VARCHAR(250) -- Find all users in the database MyDB which are orphan. DECLARE FIX_LOGIN_USER INSENSITIVE CURSOR FOR SELECT NAME FROM SYSUSERS WHERE ISSQLUSER = 1 AND (SID IS NOT NULL AND SID &lt;&gt; 0x0) AND SUSER_SNAME(SID) IS NULL ORDER BY NAME OPEN FIX_LOGIN_USER FETCH NEXT FROM FIX_LOGIN_USER INTO @user_name WHILE @@FETCH_STATUS = 0 BEGIN SELECT @login_name = NULL SELECT @login_name = LOGINNAME FROM MASTER.DBO.SYSLOGINS WHERE LOGINNAME = @user_name IF (@login_name IS NULL) BEGIN SELECT @err_msg = 'matching login does not exists for ' + @user_name PRINT @err_msg PRINT 'creating login for ' + @user_name SELECT @str = NULL SELECT @str = 'exec master.dbo.sp_addlogin ' + +'''' + @user_name + '''' + ' ,' + '''password@123''' + ' , ' + '''MyDB''' SELECT @str EXEC( @str) PRINT 'created and now fixing ......' EXEC SP_CHANGE_USERS_LOGIN 'update_one' , @user_name , @user_name IF @@ERROR &lt;&gt; 0 OR @@ROWCOUNT &lt;&gt; 1 BEGIN SELECT @err_msg = 'error creating login for ' + @user_name PRINT @err_msg END END ELSE BEGIN PRINT ' Only fixing ......' EXEC SP_CHANGE_USERS_LOGIN 'update_one' , @user_name , @login_name IF @@ERROR &lt;&gt; 0 OR @@ROWCOUNT &lt;&gt; 1 BEGIN SELECT @err_msg = 'error updating login for ' + @user_name PRINT @err_msg END END FETCH NEXT FROM FIX_LOGIN_USER INTO @user_name END CLOSE FIX_LOGIN_USER DEALLOCATE FIX_LOGIN_USER GO SET NOCOUNT OFF 
You might want to look into 'not exists' condition.
Thanks for your replies and comments. I've been using table variables pretty regularly and have also heard (apparently incorrectly) that they act practically identically to temp tables. Never even gave it much thought to question it. I'm kinda embarrassed about that. I'll suggest to have the lie-spreading party fired into the sun.
Script it out, and run the script to re-add the permissions. you probably want to sanitize data from prod too.
&gt; that they act practically identically to temp tables. Never even gave it much thought to question it. I'm kinda embarrassed about that. It happens. Don't get too upset about it. Something that helps me with things like this: if someone says that two features that appear to be interchangeable are "practically identical", I'm immediately suspicious. Why would Microsoft (or anyone else, for that matter) deliberately introduce two things that do the same thing and behave the same way?
I use this https://docs.microsoft.com/en-us/powershell/module/sqlserver/restore-sqldatabase?view=sqlserver-ps
I am on MS SQL, and this seems like the most simple and painless option. Appreciate the help. 
(If this is for a production system, having done payroll work which requires certain accuracy to legal definitions.... ) Things that you need to ask yourself, since this would affect how you have to retrieve data, and could explain some weird results you would see otherwise. A. Is a two week period defined by "BETWEEN DATEADD(wk,-2,GETDATE()) AND GETDATE()", or is it like calendar two weeks starting on sunday? (Or another day that the company chooses to report by.) B. What happens when they clock in at 2300 on the first day, and out on the second day at 300? (Does the system create two time records splitting the time, or not?) C. What about daylight savings time, your data is being stored without offsets timezone offsets. 
Hi cyong! * The two week period is from whenever the query is run, therefore defined by the query as mentioned above. * I am running on the assumption that work can only be done in business hours (9am-6pm) * As I am very new to MySQL I was not aware of this, but good catch! I'll look into that! I think I may have also explained my problem poorly, now that I look at it - essentially what I am trying to achieve is the following. The database should select ALL employees, except ones that have done 8 or more hours in the past 2 weeks as of the query being run. The database should NOT select an employee that has done work within the previous 24 hours of the query being run. Am I on the right lines with my thinking?
If your business hours are/valid times are constrained to those times, then you shouldn't need to worry about DST. I tend to work with 24/7 operations, so those little details tend to pop up and surprise you if you dont think about them. You might want to approach this as an outer join, as it stands, you will only see Employees who have 'worked' and have records and both the shift and employee table. Lets say 'Susan' starts with the company, she hasnt worked yet, so wouldnt be in your shifts table and wouldnt be displayed.
Ok, perfect! Thank you, good to know for the future. I am VERY new to MySQL but I'll have a look into outer joins now. Thanks for your help, I'll let you know how it goes!
This is a homework problem.
Hey aphex732, It's not assigned homework, but personal practice to a brief, but yeah the scenario is theoretical! Thanks
That was the point of my first question actually. Real world reporting requirement test. :)
The optimizer can rewrite correlated subqueries into (non loop) joins is some cases. In other cases it cannot. If it chooses a loop join, that's usually because the alternative is either not technically equivalent or is evaluated to be more expensive. If you are able to provide the execution plans, I could be able to give a specific answer as to what is happening.
What is the definition of "missing documents"? Is that part of the table data or a separate requirement (like all students must have documents 1-12)?
DocumenttypeID goes 1-5 for folder 114. I need to find those missing numbers. 
Pretty much any time I'm told to assume something without any data to back it up I'm suspicious. It's pretty easy to replace a table variable with a temp table and run the same query to compare the two plans, then there are no assumptions or "the old grumpy DBA guy in the closet told me it was the same thing" stuff going on.
Huh; I just tried it again and CREATE TABLE #temp works just fine. I honestly have no idea why I even played with variables in the first place. I was 100% certain that creating a temp table gave me an error regarding insufficient permissions. I found it odd but then again my user account is so heavily restricted that SQL Server Management Studio doesn't even display the database or the tables in the object explorer. I talked to the DBA and told him that I probably need the VIEW DEFINITION permission but he told me that due to security reasons he cannot grant me that. When trying to get an execution plan I get a nice **SHOWPLAN permission denied in database** error message, too. The only way to get any information on the database I have to work with is to query sys.objects, sys.columns etc. This also means that intellisense isn't working at all, which is a joy when the collation is set to case sensitive and I have to look up every single column name to get the capitalization right (and of course even simple things like ID is either spelled ID, id or Id depending on the table). Not to mention that the devs couldn't decide whether to use Danish or English names for their columns - and both languages having typos in column names, too. Either way, due to all these circumstances I wasn't surprised temp tables don't work but I guess that one was on me. I never had to set up a SQL server account (at least not such a heavily restricted one) so I don't really know much about permissions and thus just accepted that temp tables were not an option. Should have known better I guess :) Again, thanks a lot for your help. Our of curiosity though: Why would you ever use table variables then? Interestingly enough, I just googled "sql table variable" to check if I'm using the correct term; And boom, I find a description of those on docs.microsoft.com with the following note: *Table variables does not have distribution statistics, they will not trigger recompiles. Therefore, in many cases, the optimizer will build a query plan on the assumption that the table variable has no rows. For this reason, you should be cautious about using a table variable if you expect a larger number of rows (greater than 100). Temp tables may be a better solution in this case.* Pretty much what I was looking for, just a day too late. 
move the WHERE condition to the ON clause SELECT table2.id , table1.value FROM table2 LEFT JOIN table1 ON table1.table2_id = table2.id AND table1.name = 'steve'
&gt; I talked to the DBA and told him that I probably need the VIEW DEFINITION permission but he told me that due to security reasons he cannot grant me that. &gt;When trying to get an execution plan I get a nice SHOWPLAN permission denied in database error message, too. But you're allowed to query production? That's screwed up, it sounds like someone's making decisions based on an airline in-flight magazine. You're flying half-blind here and the DBA's only going to have his "security reasons" to blame when you run a query that goes off the rails because you don't have sufficient information to avoid it. &gt;which is a joy when the collation is set to case sensitive and I have to look up every single column name to get the capitalization right (and of course even simple things like ID is either spelled ID, id or Id depending on the table). Not to mention that the devs couldn't decide whether to use Danish or English names for their columns - and both languages having typos in column names, too. The only conclusion I can reach is that your DBA, developers, and a whole lot of misguided decision-makers hate you for no good reason. &gt; Why would you ever use table variables then? * Very small lists of things * As a way to pass a set of values into a stored procedure. I've had to do this with Reporting Services (getting SSRS to send that table into the stored procedure is a whole other mess) * uhhh...I'm pretty much out of ideas, I just go right for temp tables. In a lot of ways, they're easier.
 SELECT table_1.StudentID , table_1.documentTypeId , table_2.documentName , table_1.folder FROM table_2 LEFT OUTER JOIN table_1 ON table_1.folderFolderReltnId = table_2.folderFolderReltnId AND table_1.StudentID = 1 WHERE table_1.folderFolderReltnId IS NULL 
........ oh. Thank you! Kinda sucks to know I was that close for hours, but I'm glad it's good now.
Look up sp_help_revlogin
yup, this will fix a SID issue
pastebin has been removed pretty hard to see what's going on just from the screenshot
&gt; But you're allowed to query production? That's screwed up, it sounds like someone's making decisions based on an airline in-flight magazine. You're flying half-blind here and the DBA's only going to have his "security reasons" to blame when you run a query that goes off the rails because you don't have sufficient information to avoid it. It really is a weird situation. To be fair though, getting access in the first place was never officially part of the deal. They just figured it would be easier to allow us to access the database directly instead of having to do the reporting for us. I guess they don't do that too often and are not that experienced (or the DBA really wasn't interested) in setting this up properly. Ultimately the most ironic part about the whole thing is: The company that is actually developing/running the ERP software and the respective database is... Oracle. But then again I'd much rather deal with one of their badly configured SQL Servers and a borderline broken SQL Management Studio than going back to Toad or an Oracle database in general, so there's that.
First, get distinct StudentID, Folder from table_1, join to Table_2 on Folder; Then, left join table_1 one more time to the result on student_id, folder, documentTypeID. A missing relevant field from the second joined Table_1 is null (e.g. studentID is null) will indicate a missing document type per student per folder.
If you have experience writing moderately complex Excel formulas, it will be reasonably hard. Otherwise, it will be more difficult.
&gt; Edit: Totally forgot to say how nice it is to see Futurama references. I'm pretty good w/ the Futurama references but I'm confused about where I made one here. Was it the firing of people into the sun?
you could try checking out the learning sql section of the sidebar.
IMO SQL is one of the easiest languages to learn. It is very simple depending on what you want to do, but is much more straightforward than other languages that do a million different things.
very reassuring to hear, thank you!
I have not had any experience with excel formulas unfortunately :(
I will definitely check it out! thanks
This was an interesting exercise! I took some liberties with the parameters to simplify the solution, but I think it should satisfy all the requirements. Apologies for using MSSQL, but it was too complex for my rusty MySql. If you have any questions on the syntax, I'll try to look up the equivalent MySql. [Solution](http://sqlfiddle.com/#!6/c1e77/1) First, I went for the 2 week schedule approach. Doing one day at a time, it's possible to schedule yourself into a corner, where an employee needs to get one of the last days (to meet their work quota), but can't because they recently had a shift. Second, I defined the consecutive days requirement as at least 3 shifts in between. This is a little bit stricter. It disallows working on a Monday night and Wednesday morning, or working on a Friday and a Monday. Both of which would be allowed under your description. Third, I made the first 5 and last 5 shifts be different employees. Ordinarily, an employee would be allowed to work shifts 1 and 5, or 15 and 20. But I disallowed this to avoid a problem if the RNG picks 5 middle shifts in a row for second shift.
Welcome to SQL. You are going to learn an very powerful skill. The term you are looking for is 'execute' instead of run. You can find the information [here](https://dev.mysql.com/doc/workbench/en/wb-sql-editor-toolbar.html). The documentation for this tool will be a handy resource for you to find guidance on how to use the application. Best of luck and continue to ask questions.
SQL isn't incredibly difficult and the good thing is a lot of concepts are transferable between SQL databases. This book is a good introduction. If you read it before the class, you should have a good foundation. https://www.amazon.com/Sams-Teach-Yourself-SQL-Hours/dp/0672335417
A good way to learn the basics would be an app for your phone such as SoloLearn or this website: www.w3schools.com 
Hard to see it and you are on a Mac so this is an educated guess. You probably want to click on the yellow lightning button, 3rd from left right below the lahman16 table name. Nice to see you using Sean's DB 🙄
Yep, that one. At least I only know that from Futurama; I wouldn't be surprised if that's not an original Futurama joke but I can't think of anything else where I've heard that line.
Hey JC, that works perfectly! I am very new to MySQL so I think this could be a trouble to try and convert it over, however I will give it a try :) Thanks for your help!
I completed the trials and was tempted to buy at least 3 courses. I'm a sucker for interactive exercises, instant feedback, aesthetically pleasing learning environments, clear semi-formal instructions, well-organized topics, etc. However, spending more than $200 in total for something I could complete in less than a week didn't seem like a good deal. Too bad they don't offer monthly subscriptions. Instead, I read through SQLBolt, SQLTutorial, TutorialRepublic, TechOnTheNet, W3Schools, KhanAcademy, Codecademy, SoloLearn. I also did the free courses on CodeSchool and DataCamp. But I'd say I learnt the most through practice, by doing around 500 exercises provided by W3Resource.
Sololearn looks great! I downloaded it for my phone. 
The basics are easy. If OP is trying to get into performance and advanced queries it’d be difficult. 
SQL is not a programming language. The skill for programming and for SQL a quite different. Simple SQL stuff is very easy and can be comprehended fast. When it gets to more complex stuff then it starts to be hard. Someone here said something that if you know excel formulas then SQL is easy. There is absolutely nothing in common between them. Excel uses formulas that their syntax is different and the way stuff work in excel and in SQL is different.
Is the complexity due to understanding the relational algebra behind it? Would learning relational algebra concurrently help?
I think the complexity comes from understanding tabular data structure and how to work with tables and relations between tables. Relational algebra? I dont know, never learned it and I am a senior sql / plsql developer for 14 years. Maybe im using it without knowing.
I’ll elaborate more. The complex stuff in SQL i think is not the SQL itself. After all the language has only a few syntax “words”. Being good at SQL to me is being good at data, at relational data to be specific. Sql is just a language that lets you retrieve data easily. Learning SQL is to learn how to take a complex problem and “table” it in a way that is useful. If that makes sense. 
relational calculus is the select statement - so everyone, whether they know it or not, know relational calculus that knows sql. relational algebra is a little more advanced and low-level. it explains how the database takes the query language and then breaks it up into logical statements, and then checks to see if these can be executed in parallel. understanding the low level details on how a query breaks down into logic can be helpful when you are dealing with data-architecture like full replication of a dimension table accross nodes, or k-safety, atomicity and say k-safety 1 replicate a fact table 3 times. example: if i fully replicate table person, and i just make claim table k-safety 1 (3 replicas). then the database takes vertical partitions of the claim table and distributes them to each node. since the claim table is now on multiple nodes, and we know that our person table is on every node. then the join person-claim can be executed by joining locally on each of the nodes where a portion of the claim table exists, and then pushing that end data to the node with the largest # of claim. this allows the join to be done in parallel. i personally find databases fascinating, and i would recommend 'Principles of Distributed Database Systems' - but only after maybe at least 3-4 months of SQL.
The most basic form of it (Select/Insert etc) will be easy for almost anyone, but it can start becoming complex fairly quickly depending on your knowledge of relational databases and what data you are working with. When you are required to optimise queries running over huge datasets you will see it start to get much more difficult.
The problem is that every button from the yellow lightning button to the "limit to 1000 rows" drop down menu is greyed out and can't be clicked. Also, the code that's in the main window is the file I uploaded--there's no place to enter code to use on the data. Am I making sense?
Yeah it looked grayed out. Maybe a Mac permissions issue, just a guess I have no idea
Hm. Could it be an issue with the way the database is uploading? I feel like there should be something under "schemas" and then the area with the code is where my code should be.
Maybe, what is the client you are using? 
Workbench
You mean the nested CONCAT functions? Start from the middle-most, solve it, then start working out.
You might find it easier to use line breaks to separate things out a bit. CONCAT( CONCAT( CONCAT('a','b'), 'c'), 'd')
Not familiar with it, sorry could not help
I can think of two ways off the top of my head (without getting too creative): - UNION ALL three separate - Put the dates into another table and INNER JOIN
 SELECT * FROM emp WHERE hiredate=‘2-Jan-92’ UNION SELECT * FROM emp WHERE hiredate=‘2-Jan-93’ UNION SELECT * FROM emp WHERE hiredate=‘2-Jan-95’ 
i hope that was just bad typing for the dates and comma's. i'd write "selet * from hiredate" and execute. get same result!
select x.* from emp as x inner join emp as a on a.hiredate ='2-jan-92' inner join emp as b on a.hiredate ='2-jan-93' inner join emp as c on a.hiredate ='2-jan-95' 
select x.* from emp as x inner join emp as a on a.hiredate ='2-jan-92' inner join emp as b on a.hiredate ='2-jan-93' inner join emp as c on a.hiredate ='2-jan-95' 
 select x.* from emp as x inner join emp as a on a.hiredate ='2-jan-92' inner join emp as b on a.hiredate ='2-jan-93' inner join emp as c on a.hiredate ='2-jan-95' 
 select * from emp where hiredate = any (2-jan-92,2-jan,93,2-jan-95) Works in Oracle, SQL Server, and Postgresql.
Unless this is they just want to get all the 2nd Jan starters regardless of year. In which case use 2 datepart queries.
On the *very* rare occasions where I've had to do this, I've created a user-defined type (table), then passed in a table variable populated with the values. Have a look at https://stackoverflow.com/questions/25470359/how-to-pass-a-table-variable-from-one-stored-procedure-to-another Most people I know tend to avoid using XML when passing data around within SQL itself; why did you mention it in the post title but nowhere else?
The example i saw passed a dataset as xml to sql and used a cursor? pointer ? to get the values and store them in the table - the procedure entered all the values in the dataset to the table - i can't remember where i saw it and wanted to try it out
Do you presently have a requirement where data is being passed in via XML, or are you asking about creating something from scratch and want to use XML as that mechanism? Rule #1 of cursors: Only use cursors when you know you have to use cursors. If you aren't sure, it's probably the wrong choice.
creating from scratch 
Then I'd suggest not using XML in the first place. There's a few places where XML is used by SQL Server itself (Extended Events especially) but in general the XML support in SQL Server isn't great and it was kind of added as a "check the box" feature, similar to JSON support.
what would you recommend if i had a list of x number of values to be stored in a table through a procedure with the list being passed to the db only once
My very first reply to you: a table-valued parameter.
Technically union could possibly eliminate duplicate records which would not be quite the same as ORing conditions together or using IN. But "union all" would return all records regardless of uniqueness. 
Yeah... basic solution to the basic question. In practice, we’d obviously be more thorough in checking for duplicates, not doing a SELECT *, and a whole bunch of other data wrangling procedures we all do every day. 
Ok - I'll try that. Thank you.
I just found this - [!](https://www.youtube.com /watch?v=Ewf5bivTKdI) - thanks again didn't know about table-valued parameters.
Creating an api where they can request whatever data is available is probably the most realistic way. I don't think many people want like a paid read only sql user to connect to somebody's database.
Another way is maybe creating a temp table containing the dates then doing an inner join on emp.hiredate=temp.date Maybe and I'm stressing maybe, temp table.. Index.. Join might be better on a large table that's shittily done. Still though it seems a redundant way to do it if you are presented with the list of dates. 
Pretty sure traders already have this data and much more in the products that they pay for. 
Why would I have built a proprietary database if it was available? I’m sure there are programs that do provide some of the information, but they do not give you the flexibility to create your own reports and run analytics. 
This may not apply, but your data sources may restrict or have conditions to consider if you "resell" the data.
Because you haven't purchased an enterprise level trading platform. If you got a hold of the data, I guarantee you that traders aren't out there working blind. It's great that you have done this and it sounds like a great project for you to have worked on. I'm just doubting that your personal practice database probably isn't going to be saleable to traders. 
I appreciate your advice, but I was looking for more advice on the technical route of selling access to data. 99.9% of the population could think this is completely useless to them, and that is fine. 99.9% of the population can continue to use think or swim and lay on the conveyor belt on their way to the slaughterhouse with the rest of the pigs. This is information that would be useful for the .1% of people that do research outside of watching mad money and talking to their glorified receptionist of a broker. 
This would work: SELECT * FROM emp WHERE hiredate LIKE '2-jan-9[2-5]' EXCEPT SELECT * FROM emp WHERE hiredate = '2-jan-94'
In that case, I would suggest, as someone else said, building a RESTful API to sit on top and serve out to the data. You would then sell a subscription which would issue an API key. Once the subscription expires you can then remove access to that particular key. 
I will do some research on that. Thanks for your help. 
What you probably should be looking at doing is selling your analysis and not access to the data. Access to data and drawing conclusions from it are two different things. If you believe that you have a solid product in your analysis, then that is what you should sell. For the most part, lay people are technical traders, but won't invest too much time and money on a proprietary database. Those in the industry don't pay much attention to technical trends (CFA types). And judging by your post, I'm not sure how proprietary your data is. Anyone can access stock and options data. Most trading platforms like Scottrade and such have really powerful technical analysis tools. Ain't too much new under the sun. That said, if you have something worthwhile, get a lawyer before you do anything else. 
Most traders use Bloomberg sodtware, which has all this stuff readily available. 
Select where day (hire date) = 2 and month (hire date) ='Jan' and year (hire date) between 92 and 95 and year (hire date)!=93
* `where datepart(year, hiredate) in (92,93,95) and datepart(day,hiredate) = 2 and datepart(month,hiredate) = 1` * `where datepart(year, hiredate) in (92,93,95) and datepart(dayofyear,hiredate) = 2` But say goodbye to SARGability
This comment made me double check I wasn't in either /r/wallstreetbets or /r/iamverysmart.
Here's a sneak peek of /r/wallstreetbets using the [top posts](https://np.reddit.com/r/wallstreetbets/top/?sort=top&amp;t=year) of the year! \#1: [By Popular Request: if this post gets 5k upvotes, I will livestream the AAPL earnings. If it gets 10k upvotes, I will webcam myself during it.](https://np.reddit.com/r/wallstreetbets/comments/5qprhh/by_popular_request_if_this_post_gets_5k_upvotes_i/) \#2: [Upvote to ban all of Canada from the internet](https://np.reddit.com/r/wallstreetbets/comments/5rb9c7/upvote_to_ban_all_of_canada_from_the_internet/) \#3: [Never change, guys.](https://i.redd.it/u5llzoxkcd9z.png) | [586 comments](https://np.reddit.com/r/wallstreetbets/comments/6n1wa6/never_change_guys/) ---- ^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| [^^Contact ^^me](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| [^^Info](https://np.reddit.com/r/sneakpeekbot/) ^^| [^^Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/6l7i0m/blacklist/)
Beat me to it. This is a concise way to achieve what OP is looking for. 
Let me know if you have any questions.
SELECT WORKER_ID FROM TASKS WHERE TASK_STATUS=1 GROUP BY WORKER_ID; 
Is there a separate table for workers or anything?
That is in a different database. So connecting it to this one would be even more complex. :P
Is there any field to store the number of tasks or something? Or is it just random. From your example something like this would work. Select worker id from task where count(select * from task where task_Id=2) = 6; 
 Sum of task_status modulo 2 must be 0.
anyway to do this :o 
Select worker_id from tasks where worker_id not in (select worker_id from tasks where task_status = 1)
Consider posting the results when you research is concluded.
What even is it, and is it worth my time/money?
It worths every penny, this course is how I started learning sql. But Udemy has it always at %90-95 discount. You won’t be missing the discount if you decide to check it couple of months later. 
I kind of know sql already, but self taught. Is it worth doing something like thid to plug any knowledge gaps and putting on a CV?
This course is for beginners. If you are already at an intermediate level by self study, then it might not be for you. However, it is a good refresher for elementary skills especially for knowledge gaps, and Udemy accepts returns without any question if you are not satisfied. Back then I bought couple of different sql courses, returned the ones I didn’t like, and stuck to this one. 
Way over complicated it
What's a better solution? Mine reads the Tasks table only once.
THIS is pretty cool.
Can I use modulo in MySQL?
Forgot to mention that, status can be also 0 or 3. So this would not work because it would accidentally choose 0 and 3 also.
This might work. I give it a try. Posting the results later if it works. Do you think that my try reads the table more than once? Honestly don't know because I am not that familiar how ANY works.
Basically each time you see a table name is a separate instance of the table being accessed. So in your example, Tasks appears three times. It's not necessarily a bad thing. Using a few efficient indexes can often be better than getting everything all at once, but not being able to use an index.
Short answer: yes, but it depends on the collation
I'd love to read the long answer. Why is 'boat' &gt; 'pinguin'? Say I'm using the ISO-8859-1 set. Do I then just take the place number each letter has on the chart for each word and add those up, then compare? 
not add them up, but compare character by character left to right alphabetical order, where the alphabet is determined by the collation
Best is to never guess and use parentheses or was this homework? 
It shouldn't be guessing, after all, that's why it's an order of operations. The use of parenthesis, though, does show that it's a deliberate act and not guessed, which could be important for the person following you. (which could be you 3 years down the road) 
Mixing OR and AND with no parenthesis results in whichever order you DIDN'T intend. Murphy was a DBA, and his law applies in particular to relational databases.
Fixed... that's what I get for posting from mobile
&gt; Is: [this] the same as [this] no, it isn't 
So modulo won't work. select worker_id from tasks group by worker_id having sum(task_status) = count(task_status) * 2 
How can there be an existing Offer for the PriceRange that you are just now inserting?
Initially this trigger was on the Offer table (I insert the PriceRange values with a procedure that will link it to a flight so it didn't cross my mind; I'll edit it so it looks like it's on Offer) 
&gt;It shouldn't be guessing, after all, that's why it's an order of operations. The use of parenthesis, though, does show that it's a deliberate act and not guessed, which could be important for the person following you. (which could be you 3 years down the road) Or when an update comes out prioritizing some new optimized technology that can supersede the order of operations. Blows my mind when we had code written in 2011 with joins in the where statement still being written because SQL2000 still supported non ansi compliant joins. 
It would be much easier with sample data. Timestamp might be the data type but what is actually stored in it? Etc?
Well then add and statements. 
This is basically a rule I use in every programming language. Parentheses are there to make your life easier. Use them liberally.
I figured it (thanks to ichp) using the `WINDOW` clause (see 'WINDOW Clause' section at https://www.postgresql.org/docs/10/static/sql-select.html). Here is the query I ended up with: SELECT timestamp, close, stddev(close) OVER (ORDER BY timestamp ASC ROWS 24 PRECEDING) FROM candles where product='XRP/USDT' and interval='1h' and timestamp &gt; '2017-12-24' ORDER BY timestamp;
Whenever I see unneeded parentheses I assume the programmer doesn't know order of operations.
Thank you. WHERE course = 'DJ' OR (course = 'IM' and students &gt;= 5) 
String comparison with greater and lesser than is just checking the position in the alphabet. 'a' &lt; 'b', 'ape' &lt; 'boar' 'c' &gt; 'b', 'charlie' &gt; 'bravo' 'b' = 'b', 'berlin' = 'berlin'
What do you mean by data dictionary and what do you mean by formatting it?
I add unnecessary parenthesis because I assume the programmer coming after me doesn't know order of operations.
[removed]
That's syntax is totally unfamiliar to me. I'll speak to your comment &gt;But how? I thought strings and int's couldnt be concatenated? Many languages do implicit conversions. That's what's happening here. In your example it's taking the INT and converting it implicitly to a VARCHAR. IMO your solution is more correct since the next person that reads your code will know exactly what the output will be. Vs a noob will have to look up or run the code to see if it'll convert the varchar to an INT or the INT to a varchar EG does '3'+1 = 4 or does it = 31. Different languages will treat that differently. https://en.wikipedia.org/wiki/Type_conversion#Implicit_type_conversion
new.AgeStart ~~&gt;=~~ &lt;= old.AgeEnd AND old.AgeStart ~~&gt;=~~ &lt;= new.AgeEnd
 SELECT '-a' AS C UNION ALL SELECT '-b' AS C UNION ALL SELECT 'a' AS C UNION ALL SELECT 'b' AS C ORDER BY C SELECT N'-a' AS C UNION ALL SELECT N'-b' AS C UNION ALL SELECT N'a' AS C UNION ALL SELECT N'b' AS C ORDER BY C
|| is Oracle. + is SQL Server. Concat is mysql 
Uh...Oracle, MySQL, Postgres, and Presto?
MySQL can use pipes. You just have to make sure that it's enabled. 
Note that I didn't specifically say SQL.
Oh, ha. Yeah, that's fair. 
Thanks
I find such code less readable. "Wtf are all these parentheses doing here..."
You posted this exact same post 29 days ago? Stop trying to drive traffic to yourself.
Exactly why I'm embarrassed! Was told that's the case when I was new and didn't know any better (or anything about plans), then it just became fact by the time I did know better.
Wow, fascinating. I've only ever worked in SQL server, well and dabbled in sybase...
 SELECT * FROM Spend AS s CROSS APPLY (SELECT TOP(1) * FROM Currency AS c WHERE c.CurrCode = @CURR_CODE -- If you can guarantee Currency datapoints on a regular interval -- something like this will help with efficiency AND c.CurrDate BETWEEN DATEADD(day,-2,s.SpendDate) AND DATEADD(day,2,s.SpendDate) ORDER BY ABS(DATEDIFF(minute,s.SpendDate,c.CurrDate))) AS c
The key is somewhere in the middle right? You want your code to be readable, so it must be short and succinct. You want it to be maintainable, so best practices and ansi standard coding. And you want it to be documented, whether implied or explicitly. I do tend to use parenthesis even unnecessarily; because I want that code to be implicitly documented that this is the way I expect the logic to be analyzed and it gives the reader a benefit of the doubt with skill and education level. It can also lend itself to having less likely of a chance of personal error when writing the code. When your code becomes convoluted with unnecessary additions, it may be time to take a step back and evaluate how the code is written and how it can be shortened. 
Check out codewars.com. They got hundreds of SQL problems to solve.
Mode analytics has a good sql tutorial and test data to play with
&gt; ~~Write a query that retrieves the~~ `SELECT` CustomerID, LName, InvNumber, Invdate `FROM` ~~the~~ Customer ~~and~~ `JOIN` Invoice ~~tables. Add a~~ `WHERE` ~~clause that tests if~~ InvNumber ~~is~~ IN ~~the values returned by a subquery which retrieves the~~ `(SELECT` InvNumber `FROM` ~~the join of~~ Product ~~and~~ `JOIN` Line~~.~~ `GROUP BY` ~~on~~ InvNumber ~~and add a~~ `HAVING` ~~clause to the subquery that tests if the~~ `SUM` ~~of the product of~~ UnitPrice ~~and~~ `*` LineUnits ~~is strictly less than~~ `) &lt;` 100.00~~.~~ `)`
sqlfiddle.com
Sqlzoo.net
Boo! Get off the stage!
pivot data or use 'exists' condition
You dunked on him m8
that was amazing, just to add, if you can't do the whole thing in one go. isolate the parts, get those right, and then combine.
It's also very performant and called a 'lateral join'. This can scale into the billions of rows and still perform very well.
sqlbolt.com
get out 
We're a distinct group for sure
You should walk through this tutorial. https://www.w3schools.com/sql/ Then, I'd write select statements that join your tables together to bring a relational data set that is meaningful. Don't display keys in the select, display the relational non-normalized values after joining your tables on their keys. The tutorial should cover the basics of that.
We’ll have to table this discussion for another time
Who assigned this project to you? Is it for school? I would think they'd be teaching you where and how to write a simple select query before tasking you with making an entire database for a hypothetical business.
I agree with your point of view
You need to calculate the frequency of the product being ordered. Then you can order by Frequency Desc, Product_description Desc. If your data was: 1, Banana 1, Apple 2, Igloo The order by would show it as: 2, Igloo 1, Banana 1, Apple (Because it's by desc) 
Why would you do this in SQL? SQL is not great at text processing like this.
Normally you'd do this by counting and grouping. But from what I gather you don't want the count, just the distinct description that appears the most to the least. Here is an example. Given a table of descriptions, you can group by description, not include the count, but order by the count of description. This is in Oracle so you can ignore CTE as this is just an example. with mytable as ( select 'things' description from dual union all select 'things' from dual union all select 'things' from dual union all select 'things' from dual union all select 'things' from dual union all select 'things' from dual union all select 'stuff' from dual union all select 'stuff' from dual union all select 'items' from dual union all select 'items' from dual union all select 'items' from dual union all select 'widget' from dual ) select description from mytable group by description order by count(description) desc; Result: Description ------------ things items stuff widget
Names are a particular challenge because they're never formetted the same.
sounds like a task for grep and maybe a bit of shell scripting (i'm sure there's an equivalent for windows if that's your fancy)
A young boy was walking down his city street unaware that an agent of an evil food server was following him. The young boy walks into his favorite mart. The owner, Stanley S. Lang, shook his hand and said, "Hi Spid, I mean spud, what do you want today?" the boy points to his favorite book and says "56." The boy pays Stanley and heads out. As he walks out the door, the evil agent who has been watching him stabs him with a knife and yells "KILL SPID 56" The end.
notepad++ 
Could you please let me know some better options I tagged sql as I figured SQL was the best option to import flat file source. Thank you. You are a gent.
You want to use over(partition by). Make your last column count(product) over(partition by product) AS productcount ORDER BY productcount. Now you don't need group by and can order by sales volume. 
If there's no data in the table, I don't see why not. Why are you not keen on using alter table?
I'm afraid it's way too generic a question to answer accurately with any kind of confidence. 
I might use this example. This was excellent.
How do I do it? I simply Create a new table a say Alter table &lt;Table name&gt; add Primary key (code) Is that right?
I'd suggest checking out [window functions (namely partition)](https://docs.microsoft.com/en-us/sql/t-sql/queries/select-over-clause-transact-sql)
What flavor of SQL? Google would probably be more help at this point. 
Sql server Management studio
Foreign Key USE AdventureWorks2012; GO ALTER TABLE Sales.TempSalesReason ADD CONSTRAINT FK_TempSales_SalesReason FOREIGN KEY (TempID) REFERENCES Sales.SalesReason (SalesReasonID) ON DELETE CASCADE ON UPDATE CASCADE ; GO Primary Key USE AdventureWorks2012; GO ALTER TABLE Production.TransactionHistoryArchive ADD CONSTRAINT PK_TransactionHistoryArchive_TransactionID PRIMARY KEY CLUSTERED (TransactionID); GO Added both. You OP and comment specified both. 
Very close, OP will need to order by the count first, which is what you show. They will also need to order by the description itself after too.
 SELECT purchase1.CustomerID , purchase1.Date , purchase1.OrderID , purchase1.Price , purchase1.ItemType , purchase2.Date , purchase2.OrderID , purchase2.Price , purchase2.ItemType FROM exampletable AS purchase1 INNER JOIN exampletable AS purchase2 ON purchase2.CustomerID = purchase1.CustomerID AND purchase2.Price &gt; 20 WHERE purchase1.Price &lt; 1
You claim to know C#, why not use that? It's much better suited to this task than SQL.
you should probably reformat the whole computer
You can achieve this with windows functions too. Pending how the architecture is, I'm betting the cross apply is zippier. Of course this can vary based on the optimizer change between versions of SQL Server too. (2014 was the change.)
Setup a calendar table and outer join to it on the dates. If the records table is actually a time stamp make sure you cast it to a date.
Create a date table. https://www.mssqltips.com/sqlservertip/4054/creating-a-date-dimension-or-calendar-table-in-sql-server/ Fakey code below - SELECT DateKey FROM dateTable WHERE DateKey NOT IN ( SELECT CAST(YourDate AS DATE) dte FROM YourTable ) AND DateKey &gt;= '1/1/2018' AND DateKey &lt; '1/7/2018' Date tables aren't limited to just data warehouses. Also Number tables can be used to find gaps as well. Just because you don't have a date or number table today doesn't me you shouldn't create one.
I tend to do the same. We won't need to release this code for a few weeks because we're waiting on the data to start importing so just kicking ideas around right now.
a number table can do the job really well CREATE TABLE numbers ( n INTEGER NOT NULL PRIMARY KEY ); INSERT INTO numbers ( n ) VALUES (0),(1),(2),(3),(4),(5),(6),(7),(8),(9)... SELECT DATE('2012-01-01') -- start date + INTERVAL n DAY FROM numbers WHERE DATE('2018-01-01') + INTERVAL n DAY &lt; '2018-01-06' -- end date i find myself using a number table a lot more than a date table
Would altering each foreign key individually work this out instead?
Awesome thank u so much! All i had to do from here was add a few manipulations and the clause for purchase1 to be older than purchase2. Thanks again for all the help
&gt; If I were to limit the dates on the inside of the cross apply would the resulting subset from Spend be similarly limited? I think so. The query optimizer should sniff that out and apply it straight to the Spend table. But I prefer your first idea of putting in an outer WHERE clause, for readability. A wide date range like that reduces the dataset (the amount of joining that will be done), but the tight date range inside the CROSS APPLY helps with the efficiency of each record that has to be joined. The inspiration for this is my go to solution to effective date joins; a solution I'm not particularly satisfied with. Joins with inequality conditions are generally undesirable. I'm going to give my thoughts on the subject followed by some alternate solutions. The solution I gave you probably isn't optimal. --- #Effective Dates (Part 1) [^^SQLFiddle](http://sqlfiddle.com/#!6/a5328/4) [^^Pastebin](https://pastebin.com/b74RqwKa) Before we get into examples, let's talk about hash joins for a bit. Hash joins work by applying a hashing algorithm to the join conditions, then joining the hashed values. This is useful for several reasons. 1. Hash values are evenly distributed. For a given value you can know statistically what % of values will come before it, and can slot it into the right place (hash bucket) in memory (hash table). When finished, you have an ordered list without any sorting. Nice! 2. Hash values are consolidated and compressed. Five join conditions are hashed into a single smaller value. This means that matching reads less data, and only has to do a single comparison (instead of five). 3. It's easy to get even chunks of records for parallelization. This goes back to the even distribution. Effective Date lookups are where multiple records share the same reference value (CurrCode) and are distinguished by non-overlapping periods of time, typically an EffectiveStartDate and EffectiveEndDate. This is the normal way to write this query, but it is not best. SELECT t.TransactionDate, l.EffectiveStartDate, l.EffectiveEndDate, t.TransactionData, l.LookupData FROM dbo.Transactions AS t INNER JOIN dbo.EffectiveDateLookup AS l ON l.LookupCode = t.LookupCode AND t.TransactionDate BETWEEN l.EffectiveStartDate AND l.EffectiveEndDate; [^Execution ^Plan](https://i.imgur.com/2f6dcgf.png) What's going on here? Well, there's a hash join. The thing about inequality conditions is that they don't play nice with hash joins. **TransactionDate** doesn't hash to the same value as **EffectiveStartDate** and **EffectiveEndDate**, so that part join condition can be part of all the nice stuff that hash joins do. You can see that **Hash Keys Probe** only contains **LookupCode**. What's happening is each **Transactions** record is being matched with 73 **EffectiveDateLookup** records on the **LookupCode** hash, and then evaluating the date range condition. That's a lot of work for what should be a 1:1 match. Let's try a loop join. SELECT t.TransactionDate, l.EffectiveStartDate, l.EffectiveEndDate, t.TransactionData, l.LookupData FROM dbo.Transactions AS t INNER JOIN dbo.EffectiveDateLookup AS l ON l.LookupCode = t.LookupCode AND t.TransactionDate BETWEEN l.EffectiveStartDate AND l.EffectiveEndDate OPTION (LOOP JOIN); [^Execution ^Plan](https://i.imgur.com/GLBQYBQ.png) Okay. Now what's going on? Well the first thing is that this query has a higher estimated cost, but actually runs faster. Interesting. The second thing is that 36.5 million **Number of Rows Read** from **EffectiveDateLookup** for just 1 million matches. That doesn't seem ideal. The thing is that multiple inequality conditions don't play nice with indexes, and nested loops rely on efficiencies from indexes. Indexes can seek any number of equality conditions. That makes sense. Indexes can also see an inequality condition. That's sometimes called a range scan, but it's still a kind of seek. But any index keys that come after an inequality condition aren't useful. In this case we see **EffectiveStartDate** is a **Seek Predicate**, but **EffectiveEndDate** is part of the (non-seek) **Predicate**. That means that a **Transactions** record from 2000-01-01 seeks and finds one **EffectiveDateLookup** record. So far so good. But a **Transactions** record from 2018-01-01 seeks and finds 73 **EffectiveDateLookup** records before evaluating that only 1 matches. On average **Transactions** records match half **EffectiveDateLookup** records, resulting in our mysterious 36.5 million rows. That's still better than then hash join, in which every record was matched not just half. But how to get it down to 1:1. SELECT t.TransactionDate, l.EffectiveStartDate, l.EffectiveEndDate, t.TransactionData, l.LookupData FROM dbo.Transactions AS t CROSS APPLY (SELECT TOP(1) * FROM dbo.EffectiveDateLookup AS l WHERE l.LookupCode = t.LookupCode AND l.EffectiveStartDate &lt;= t.TransactionDate ORDER BY l.EffectiveStartDate DESC) AS l; [^Execution ^Plan](https://i.imgur.com/U1gojQA.png) That should look familiar, and it achieves the elusive 1:1 match. 1 million **Number of Rows Read**. We did it! Even though the syntax looks sort of unusual, the concept is pretty straight forward once we understand what the bad loop join was doing. We make sure that the first **EffectiveDateLookup** record we read is the correct one (**ORDER BY l.EffectiveStartDate DESC**), and stop there (**TOP(1)**). But it's still a loop join, and I would prefer to leverage all the nice things that hash joins do. But how? SELECT t.TransactionDate, l.EffectiveStartDate, l.EffectiveEndDate, t.TransactionData, l.LookupData FROM dbo.Transactions AS t INNER JOIN dbo.EffectiveDateLookup AS l ON l.LookupCode = t.LookupCode AND l.EffectiveStartDate = DATEADD(quarter,DATEDIFF(quarter,'2000-01-01',t.TransactionDate),'2000-01-01'); [^Execution ^Plan](https://i.imgur.com/7BfMAJI.png) Equality conditions for the win! I'll admit, we're fortunate that this is an option. Because our effective dates are perfectly regular on the quarter, we're able to math on the **TransactionDate** to determine exactly which record we match to. Very convenient. The only thing I'll call attention to is the **Hash Keys Probe** now contains **LookupCode** and **Expr1004**, instead of just **LookupCode** like the first attempt. Below are the query run times on my computer. Query|RunTime|Count|CheckSum :-:|:-:|:-:|:-: Normal Hash Join|00:00:06.772|1000000|1976579526 Normal Loop Join|00:00:05.981|1000000|1976579526 Top 1 Loop Join|00:00:03.250|1000000|1976579526 Equality Hash Join|00:00:02.077|1000000|1976579526
I posted my detailed thoughts of the performance of these kinds of joins if you're interested. It's a bit of a read.
oh yeah, older, i forgot that... glad you got it going
 #Currency (Part 2) SELECT * FROM Spend AS s CROSS APPLY (SELECT TOP(1) * FROM Currency AS c WHERE c.CurrCode = @CURR_CODE AND c.CurrDate BETWEEN DATEADD(day,-2,s.SpendDate) AND DATEADD(day,2,s.SpendDate) ORDER BY ABS(DATEDIFF(minute,s.SpendDate,c.CurrDate))) AS c; Remember we were finally happy with our 1:1 loop join because it didn't read any extra rows. Well, this has the potential to read extra rows (**AND c.CurrDate BETWEEN DATEADD(day,-2,s.SpendDate) AND DATEADD(day,2,s.SpendDate)**), and it does some extra math (**ABS(DATEDIFF(minute,s.SpendDate,c.CurrDate))**), and it sorts (**ORDER BY**). *Our 1:1 loop join didn't sort because it was coming out of an already sorted index.* So let's make some improvements. SELECT c.CurrCode , c.USDXUnit , ISNULL(DATEADD(second,ROUND(DATEDIFF(second,c.CurrDate,LAG(c.CurrDate) OVER (PARTITION BY c.CurrCode ORDER BY c.CurrDate))/2.,0,1),c.CurrDate),'1900-01-01') AS EffectiveStartDate , ISNULL(DATEADD(millisecond,-3,DATEADD(second,ROUND(DATEDIFF(second,c.CurrDate,LEAD(c.CurrDate) OVER (PARTITION BY c.CurrCode ORDER BY c.CurrDate))/2.,0,1),c.CurrDate)),'2999-12-31 23:59:59.997') AS EffectiveEndDate INTO #Currency FROM Currency AS c; CREATE UNIQUE CLUSTERED INDEX CIX ON #Currency (CurrCode,EffectiveStartDate); SELECT * FROM Spend AS s CROSS APPLY (SELECT TOP(1) * FROM Currency AS c WHERE c.CurrCode = @CURR_CODE AND c.EffectiveStartDate &lt;= s.SpendDate ORDER BY c.EffectiveStartDate DESC) AS c; Well, for starters let's transform **Currency** into a nicer effective date format, with proper start dates and end dates. We can look at the record before and after, and find the mid points. Then we can use the query technique that worked 1:1. SELECT * FROM Spend AS s INNER JOIN Currency AS c ON c.CurrCode = @CURR_CODE AND c.CurrDate = DATEADD(day,-ABS(DATEDIFF(day,'2018-01-01',s.SpendDate))%2,CONVERT(date,s.SpendDate)); If **Currency** is populated every other day without fail, we can do some math on **SpendDate** to align it to the appropriate day. But that's a pretty bold assumption. But there is another way. Let's suppose we have a **Currency** record mostly every 2 days. But is can sometimes be 1 day or 3 days between data points. We can create an aggregate weekly record that contains multiple **Currency** data points, and is guaranteed to contain the one that a **Spend** record needs. WITH cteEffectiveDate AS ( SELECT * , LAG(c.USDXUnit) OVER (PARTITION BY c.CurrCode ORDER BY c.CurrDate) AS LastUSDXUnit , ISNULL(DATEADD(second,ROUND(DATEDIFF(second,c.CurrDate,LAG(c.CurrDate) OVER (PARTITION BY c.CurrCode ORDER BY c.CurrDate))/2.,0,1),c.CurrDate),'1900-01-01') AS EffectiveStartDate , ISNULL(DATEADD(millisecond,-3,DATEADD(second,ROUND(DATEDIFF(second,c.CurrDate,LEAD(c.CurrDate) OVER (PARTITION BY c.CurrCode ORDER BY c.CurrDate))/2.,0,1),c.CurrDate)),'2999-12-31 23:59:59.997') AS EffectiveEndDate FROM Currency AS c), cteRowNum AS ( SELECT * , DATEADD(week,DATEDIFF(week,'2018-01-01',e.EffectiveStartDate),'2018-01-01') AS [Week] , ROW_NUMBER() OVER (PARTITION BY DATEADD(week,DATEDIFF(week,'2018-01-01',e.EffectiveStartDate),'2018-01-01') ORDER BY e.EffectiveStartDate) AS RowNum FROM cteEffectiveDate AS e), cteWeek AS ( SELECT r.CurrCode , r.[Week] , MAX(CASE WHEN r.RowNum = 1 THEN r.LastUSDXUnit ELSE NULL END) AS [1USDXUnit] , MAX(CASE WHEN r.RowNum = 1 THEN r.EffectiveStartDate ELSE NULL END) AS [1Date] , MAX(CASE WHEN r.RowNum = 1 THEN r.USDXUnit ELSE NULL END) AS [2USDXUnit] , MAX(CASE WHEN r.RowNum = 2 THEN r.EffectiveStartDate ELSE NULL END) AS [2Date] , MAX(CASE WHEN r.RowNum = 2 THEN r.USDXUnit ELSE NULL END) AS [3USDXUnit] , MAX(CASE WHEN r.RowNum = 3 THEN r.EffectiveStartDate ELSE NULL END) AS [3Date] , MAX(CASE WHEN r.RowNum = 3 THEN r.USDXUnit ELSE NULL END) AS [4USDXUnit] , MAX(CASE WHEN r.RowNum = 4 THEN r.EffectiveStartDate ELSE NULL END) AS [4Date] , MAX(CASE WHEN r.RowNum = 4 THEN r.USDXUnit ELSE NULL END) AS [5USDXUnit] , MAX(CASE WHEN r.RowNum = 5 THEN r.EffectiveStartDate ELSE NULL END) AS [5Date] , MAX(CASE WHEN r.RowNum = 5 THEN r.USDXUnit ELSE NULL END) AS [6USDXUnit] , MAX(CASE WHEN r.RowNum = 6 THEN r.EffectiveStartDate ELSE NULL END) AS [6Date] , MAX(CASE WHEN r.RowNum = 6 THEN r.USDXUnit ELSE NULL END) AS [7USDXUnit] , MAX(CASE WHEN r.RowNum = 7 THEN r.EffectiveStartDate ELSE NULL END) AS [7Date] , MAX(CASE WHEN r.RowNum = 7 THEN r.USDXUnit ELSE NULL END) AS [8USDXUnit] FROM cteRowNum AS r GROUP BY r.CurrCode , r.[Week]) SELECT s.* , CASE WHEN s.SpendDate &lt; w.[1Date] THEN w.[1USDXUnit] WHEN s.SpendDate &lt; w.[2Date] OR w.[3Date] IS NULL THEN w.[2USDXUnit] WHEN s.SpendDate &lt; w.[3Date] OR w.[4Date] IS NULL THEN w.[3USDXUnit] WHEN s.SpendDate &lt; w.[4Date] OR w.[5Date] IS NULL THEN w.[4USDXUnit] WHEN s.SpendDate &lt; w.[5Date] OR w.[6Date] IS NULL THEN w.[5USDXUnit] WHEN s.SpendDate &lt; w.[6Date] OR w.[8Date] IS NULL THEN w.[6USDXUnit] WHEN s.SpendDate &lt; w.[7Date] OR w.[8Date] IS NULL THEN w.[7USDXUnit] ELSE w.[8USDXUnit] END AS USDXUnit FROM Spend AS s INNER JOIN cteWeek AS w ON w.CurrCode = @CURR_CODE AND w.[Week] = DATEADD(week,DATEDIFF(week,'2018-01-01',s.SpendDate),'2018-01-01'); Hmm. That one's a bit on the eccentric side, but maybe fast. I mean it uses equality join conditions, and that was the (my) goal! You may want to avoid that one if you're the kind of dev that considers what their replacement might think (I'm not).
Are there foreign key constraints on city or state, referencing state or country?
The error literally tells you, you can't have multiple cascades. You're going to have to use triggers https://docs.microsoft.com/en-us/sql/t-sql/statements/create-trigger-transact-sql
I wouldn't worry to much tbh. From what you outlined you are a good fit for that position, since its more of a "write done logic in script" kinda of deal, than a know to the deepest layer how that script gets executed, as I understand. The background you layed out, you should have that down pretty easy. SSAS is something quite different, you gotta deal with datawarehouse modeling, and MDX, that is quite a bit removed from SQL. I don't think you can really prep for that, just go in, tell em what you know and that you'd be happy to get your hands dirty with BI kind of stuff, I'd say thats your best bet. If you try to memorize something off a couple off tutorials and try to boast that on the interview, you will get shredded cause you won't really know what you are talking about.
Did you try Sum( field1 / field2) ? I don’t use domo but I’d give that a whirl
It might help to list the constraints on the Prod table, and to give a few examples of what the data looks like for those associated columns to troubleshoot this. I'm assuming that GOAL_CODE has to be unique, or combination of columns including GOAL_CODE needs to be unique. Understanding what specific constraints are in place might help shed some light on why you're running into this issue. Without knowing the specifics, I'm wondering if the application updates records in a specific order to ensure that the relevant constraint is never violated. If you do a bulk update statement directly via SQL, Oracle may be trying to change the records in a different order than what would occur naturally through the application, and that may cause constraints to be temporarily violated. 
Ok so constraints for prod table CONSTRAINT "PK_SVREDGL" PRIMARY KEY ("SVREDGL_EFF_TERM", "SVREDGL_DICD_CODE", "SVREDGL_ID", "SVREDGL_GOAL_CODE") for the data example for prod table https://imgur.com/a/z8PyK data example for test table https://imgur.com/nZ2rcwp basically I want to update the goal code when the PIDM match and the term = 201707. 
Not sure who downvoted you. Window function is the way I would do it.
Might want to try MERGE to get around it. https://docs.oracle.com/cd/B28359_01/server.111/b28286/statements_9016.htm MERGE INTO tableB B USING (SELECT col1 FROM tableA) A ON B.col1= A.col1 WHEN MATCHED THEN UPDATE SET B.col2=A.col2; You can also add a WHEN NOT MATCHED but doesn’t seem necessary in your scenario.
The companies you mentioned hired **high skilled** people. So if you'd like to get a job there you need to show them you are able to solve **hard** problems, since that's their daily business. **Do exercises**. Check that http://www.studybyyourself.com/seminar/sql/course/?lang=eng. Free, for beginners, well structured, keep things simple, with online exercises. **Play around with data**. You can to download some freely available data set, for example [wikipedia](https://dumps.wikimedia.org/) and start driving data analysis and data representation on it.
What's a primary key? What's a foreign key? What's a surrogate or business key? What's a natural key? What's a composite key? Are the primary key and the clustered index the same thing? What's the difference between a left outer join and an inner join? What's the difference between a clustered index and a non-clustered index? What's secretly included in a non-clustered index? What happens when you create a non-unique clustered index on SQL Server? What is a heap table? When would you use one? When is a View appropriate? When does the engine allow for a merge join? When does the engine do a hash join? What is a sargeable predicate? What is a worktable? When does the engine need one? Have you ever fucked up on a Production system? How did you solve it? What have you done to prevent fuck-ups in the future?
the questions no one asks or only asks. 
From what I got in the OP, he was asking for technical questions. I think everybody can think of the boring "where do you see yourself in 5 years?" question.
I tried this query to subtract the current Qty that's 'in stock' from the Min_Reorder_Level of the same item, to find out if more stock needs to be ordered. But the answer I get from the query is a summation not subtraction. Any thoughts? SELECT CASE WHEN total &gt; 0 THEN total ELSE 0 END AS total FROM ( SELECT sum(Product.Number_of_units) - (SELECT - sum(StockItem.Min_Reorder_Level) FROM StockItem WHERE PN = 100003) as total FROM Product WHERE Product .PN = 100003 ) alias;
totally. I had a DBA interview once, and they asked me about indexing and statistics and plans for the whole interview. after a while, i asked them in return, what sort of performance issues are you having? I basically interviewed myself out of the FT job after finding out they had only one server that didn't have any maint jobs.. i did get some consulting money out of it for a few months though. 
I think the main performance hit is mixing tables on each side of the join condition. `a.StartTime &lt;= b.[Date] + c.ToTime` Better would be `a.StartTime - b.[Date] &lt;= c.ToTime` (I know you can't subtract dates that simply, but that's the idea). It's okay to combine `a.` and `b.` because they're already joined; they're part of the same dataset so aren't actually mixed. If that makes sense. But before you do that, try this monster. I think it could be fast. Add or remove lines based on your longest session. WITH cteSessions AS ( SELECT * , DATEADD(hour,1+DATEPART(hour,s.StartTime),CONVERT(datetime,CONVERT(date,s.StartTime))) AS [1] , DATEADD(hour,2+DATEPART(hour,s.StartTime),CONVERT(datetime,CONVERT(date,s.StartTime))) AS [2] , DATEADD(hour,3+DATEPART(hour,s.StartTime),CONVERT(datetime,CONVERT(date,s.StartTime))) AS [3] , DATEADD(hour,4+DATEPART(hour,s.StartTime),CONVERT(datetime,CONVERT(date,s.StartTime))) AS [4] , DATEADD(hour,5+DATEPART(hour,s.StartTime),CONVERT(datetime,CONVERT(date,s.StartTime))) AS [5] , DATEADD(hour,6+DATEPART(hour,s.StartTime),CONVERT(datetime,CONVERT(date,s.StartTime))) AS [6] , DATEADD(hour,7+DATEPART(hour,s.StartTime),CONVERT(datetime,CONVERT(date,s.StartTime))) AS [7] , DATEADD(hour,8+DATEPART(hour,s.StartTime),CONVERT(datetime,CONVERT(date,s.StartTime))) AS [8] , DATEADD(hour,9+DATEPART(hour,s.StartTime),CONVERT(datetime,CONVERT(date,s.StartTime))) AS [9] , DATEADD(hour,10+DATEPART(hour,s.StartTime),CONVERT(datetime,CONVERT(date,s.StartTime))) AS [10] , DATEADD(hour,11+DATEPART(hour,s.StartTime),CONVERT(datetime,CONVERT(date,s.StartTime))) AS [11] , DATEADD(hour,12+DATEPART(hour,s.StartTime),CONVERT(datetime,CONVERT(date,s.StartTime))) AS [12] , DATEADD(hour,13+DATEPART(hour,s.StartTime),CONVERT(datetime,CONVERT(date,s.StartTime))) AS [13] , DATEADD(hour,14+DATEPART(hour,s.StartTime),CONVERT(datetime,CONVERT(date,s.StartTime))) AS [14] , DATEADD(hour,15+DATEPART(hour,s.StartTime),CONVERT(datetime,CONVERT(date,s.StartTime))) AS [15] , DATEADD(hour,16+DATEPART(hour,s.StartTime),CONVERT(datetime,CONVERT(date,s.StartTime))) AS [16] , DATEADD(hour,17+DATEPART(hour,s.StartTime),CONVERT(datetime,CONVERT(date,s.StartTime))) AS [17] , DATEADD(hour,18+DATEPART(hour,s.StartTime),CONVERT(datetime,CONVERT(date,s.StartTime))) AS [18] , DATEADD(hour,19+DATEPART(hour,s.StartTime),CONVERT(datetime,CONVERT(date,s.StartTime))) AS [19] , DATEADD(hour,20+DATEPART(hour,s.StartTime),CONVERT(datetime,CONVERT(date,s.StartTime))) AS [20] , DATEADD(hour,21+DATEPART(hour,s.StartTime),CONVERT(datetime,CONVERT(date,s.StartTime))) AS [21] , DATEADD(hour,22+DATEPART(hour,s.StartTime),CONVERT(datetime,CONVERT(date,s.StartTime))) AS [22] , DATEADD(hour,23+DATEPART(hour,s.StartTime),CONVERT(datetime,CONVERT(date,s.StartTime))) AS [23] , DATEADD(hour,24+DATEPART(hour,s.StartTime),CONVERT(datetime,CONVERT(date,s.StartTime))) AS [24] FROM dbo.[Sessions] AS s ) SELECT s.UserId, s.StartTime, s.EndTime, i.IntervalStartTime, i.IntervalEndTime, DATEDIFF(second,i.IntervalStartTime,i.IntervalEndTime) AS IntervalDuration FROM cteSessions AS s CROSS APPLY (SELECT s.StartTime AS IntervalStartTime, CASE WHEN s.EndTime &gt; s.[1] THEN s.[1] ELSE s.EndTime END AS IntervalEndTime UNION ALL SELECT s.[1] AS IntervalStartTime, CASE WHEN s.EndTime &gt; s.[2] THEN s.[2] ELSE s.EndTime END AS IntervalEndTime WHERE s.EndTime &gt; s.[1] UNION ALL SELECT s.[2] AS IntervalStartTime, CASE WHEN s.EndTime &gt; s.[3] THEN s.[3] ELSE s.EndTime END AS IntervalEndTime WHERE s.EndTime &gt; s.[2] UNION ALL SELECT s.[3] AS IntervalStartTime, CASE WHEN s.EndTime &gt; s.[4] THEN s.[4] ELSE s.EndTime END AS IntervalEndTime WHERE s.EndTime &gt; s.[3] UNION ALL SELECT s.[4] AS IntervalStartTime, CASE WHEN s.EndTime &gt; s.[5] THEN s.[5] ELSE s.EndTime END AS IntervalEndTime WHERE s.EndTime &gt; s.[4] UNION ALL SELECT s.[5] AS IntervalStartTime, CASE WHEN s.EndTime &gt; s.[6] THEN s.[6] ELSE s.EndTime END AS IntervalEndTime WHERE s.EndTime &gt; s.[5] UNION ALL SELECT s.[6] AS IntervalStartTime, CASE WHEN s.EndTime &gt; s.[7] THEN s.[7] ELSE s.EndTime END AS IntervalEndTime WHERE s.EndTime &gt; s.[6] UNION ALL SELECT s.[7] AS IntervalStartTime, CASE WHEN s.EndTime &gt; s.[8] THEN s.[8] ELSE s.EndTime END AS IntervalEndTime WHERE s.EndTime &gt; s.[7] UNION ALL SELECT s.[8] AS IntervalStartTime, CASE WHEN s.EndTime &gt; s.[9] THEN s.[9] ELSE s.EndTime END AS IntervalEndTime WHERE s.EndTime &gt; s.[8] UNION ALL SELECT s.[9] AS IntervalStartTime, CASE WHEN s.EndTime &gt; s.[10] THEN s.[10] ELSE s.EndTime END AS IntervalEndTime WHERE s.EndTime &gt; s.[9] UNION ALL SELECT s.[10] AS IntervalStartTime, CASE WHEN s.EndTime &gt; s.[11] THEN s.[11] ELSE s.EndTime END AS IntervalEndTime WHERE s.EndTime &gt; s.[10] UNION ALL SELECT s.[11] AS IntervalStartTime, CASE WHEN s.EndTime &gt; s.[12] THEN s.[12] ELSE s.EndTime END AS IntervalEndTime WHERE s.EndTime &gt; s.[11] UNION ALL SELECT s.[12] AS IntervalStartTime, CASE WHEN s.EndTime &gt; s.[13] THEN s.[13] ELSE s.EndTime END AS IntervalEndTime WHERE s.EndTime &gt; s.[12] UNION ALL SELECT s.[13] AS IntervalStartTime, CASE WHEN s.EndTime &gt; s.[14] THEN s.[14] ELSE s.EndTime END AS IntervalEndTime WHERE s.EndTime &gt; s.[13] UNION ALL SELECT s.[14] AS IntervalStartTime, CASE WHEN s.EndTime &gt; s.[15] THEN s.[15] ELSE s.EndTime END AS IntervalEndTime WHERE s.EndTime &gt; s.[14] UNION ALL SELECT s.[15] AS IntervalStartTime, CASE WHEN s.EndTime &gt; s.[16] THEN s.[16] ELSE s.EndTime END AS IntervalEndTime WHERE s.EndTime &gt; s.[15] UNION ALL SELECT s.[16] AS IntervalStartTime, CASE WHEN s.EndTime &gt; s.[17] THEN s.[17] ELSE s.EndTime END AS IntervalEndTime WHERE s.EndTime &gt; s.[16] UNION ALL SELECT s.[17] AS IntervalStartTime, CASE WHEN s.EndTime &gt; s.[18] THEN s.[18] ELSE s.EndTime END AS IntervalEndTime WHERE s.EndTime &gt; s.[17] UNION ALL SELECT s.[18] AS IntervalStartTime, CASE WHEN s.EndTime &gt; s.[19] THEN s.[19] ELSE s.EndTime END AS IntervalEndTime WHERE s.EndTime &gt; s.[18] UNION ALL SELECT s.[19] AS IntervalStartTime, CASE WHEN s.EndTime &gt; s.[20] THEN s.[20] ELSE s.EndTime END AS IntervalEndTime WHERE s.EndTime &gt; s.[19] UNION ALL SELECT s.[20] AS IntervalStartTime, CASE WHEN s.EndTime &gt; s.[21] THEN s.[21] ELSE s.EndTime END AS IntervalEndTime WHERE s.EndTime &gt; s.[20] UNION ALL SELECT s.[21] AS IntervalStartTime, CASE WHEN s.EndTime &gt; s.[22] THEN s.[22] ELSE s.EndTime END AS IntervalEndTime WHERE s.EndTime &gt; s.[21] UNION ALL SELECT s.[22] AS IntervalStartTime, CASE WHEN s.EndTime &gt; s.[23] THEN s.[23] ELSE s.EndTime END AS IntervalEndTime WHERE s.EndTime &gt; s.[22] UNION ALL SELECT s.[23] AS IntervalStartTime, CASE WHEN s.EndTime &gt; s.[24] THEN s.[24] ELSE s.EndTime END AS IntervalEndTime WHERE s.EndTime &gt; s.[23] UNION ALL SELECT s.[24] AS IntervalStartTime, s.EndTime AS IntervalEndTime WHERE s.EndTime &gt; s.[24]) AS i;
In Sql Server Management do you not you add foreign keys the same way you add constraints and indexes? There is a complete wizard for this, no reason to delete or have to go the code direction.
How could I throw the term = 201707 in there? 
The ON a clause can be treated just like a Join’s ON clause. You can have multiple columns ON a.col1 =B.col1 AND a.col2 = 201707
OK I feel like I am almost there but it says I am missing ON keyword. MERGE INTO SVREDGL B USING (SELECT PIDM FROM BPRUTSOS.TEMP_1) A ON B.SVREDGL_PIDM = A.PIDM and B.SVREDGL_EFF_TERM = '201707' WHEN MATCHED THEN UPDATE SET B.SVREDGL_GOAL_CODE = A.GOAL_CODE; 
Yeah I searched and saw to put parentheses around the ON. MERGE INTO SVREDGL B USING (SELECT PIDM FROM BPRUTSOS.TEMP_1) A ON (B.SVREDGL_PIDM = A.PIDM AND B.SVREDGL_EFF_TERM = '201707') WHEN MATCHED THEN UPDATE SET B.SVREDGL_GOAL_CODE = A.GOAL_CODE; Now however when I run it it says SQL Error: ORA-00904: "A"."GOAL_CODE": invalid identifier 00904. 00000 - "%s: invalid identifier"' Is that because the A is outside of the parenthesis?
You need to add ALL of the columns in B and A that you are interacting with to the INTO/USING select statement. See my response above.
There is a unique constraint or not null constraint on Goal_code columns and you are trying to update with duplicate values or null values. Open table definition in TOAD or SQL Server and check your requirement if you want to have them unique.
I mean SQL Developer 
CONSTRAINT "PK_SVREDGL" PRIMARY KEY ("SVREDGL_EFF_TERM", "SVREDGL_DICD_CODE", "SVREDGL_ID", "SVREDGL_GOAL_CODE") all I am trying to update is the SVREDGL_GOAL_CODE and it already has duplicates.
here is the constraint. CONSTRAINT "PK_SVREDGL" PRIMARY KEY ("SVREDGL_EFF_TERM", "SVREDGL_DICD_CODE", "SVREDGL_ID", "SVREDGL_GOAL_CODE") But I am not understanding how that goal_code cant be updated the data is just A,C or E.
Yep. There it is. You’re trying to update SVREDGL_GOAL_CODE, resulting in a record being updated to be the same value as a record that already exists. Your primary key constraint enforces that no 2 rows can have the same values for the columns included in the constraint definition.
How could I make an on clause that would disallow updating dupes? https://imgur.com/DbTa9NO here is a pic of the data and the goal code has dupes in it.
There are two things that might be going on here. 1) you are trying to update your TARGET with values from SOURCE that already exist in TARGET. 2) you are trying to update your TARGET with values from SOURCE that don’t exist in TARGET, but exists as DUPLICATES in your SOURCE table. I would say, run a quick select to see which dupes you have in your source table. Replace with real values where applicable: SELECT COL1 COL2, COL3, COL4, COUNT(*) FROM SOURCE WHERE GOAL_Code =201707 GROUP BY COL1 COL2, COL3, COL4 HAVING COUNT(*) &gt; 1; If you get ANY records here, scenario 2 is applicable.
 SELECT SVREDGL_EFF_TERM, SVREDGL_DICD_CODE, SVREDGL_PIDM, SVREDGL_GOAL_CODE, Count(SVREDGL_PIDM) FROM SVREDGL WHERE SVREDGL_EFF_TERM ='201707' GROUP BY SVREDGL_EFF_TERM, SVREDGL_DICD_CODE, SVREDGL_PIDM, SVREDGL_GOAL_CODE HAVING COUNT(SVREDGL_PIDM) &gt; 1; Shows nothing.
Update just checked the source table SELECT PIDM, Goal_Code, Count(PIDM) FROM BPRUTSOS.TEMP_1 GROUP BY PIDM, Goal_Code HAVING COUNT(PIDM) &gt; 1; Zero as well. 
Then scenario 1 applies. You are trying to update TARGET with data from SOURCE that would already exists in TARGET and would create duplicate records. You can limit this one of two ways: In the USING definition, add a where clause to exclude records that already exist in the source (Via a NOT EXISTS approach or a LEFT JOIN with B.Key IS NULL approach) In the ON clause, add an enclosed NOT, like so: AND NOT (SOURCE.col1 =TARGET.col1 AND SOURCE.col2=TARGET.col2 AND SOURCE.col3=TARGET.col3 AND SOURCE.col4=TARGET.col4)
Ok I see where you are going however I would like to change the update the SVREDGL.GOAL_CODE for the current row not add another row.
UPDATE only updates existing rows. INSERT would add rows if they don’t exist, which is why I didn’t include INSERT in my suggested queries.
Yes. Exactly. This would be MERGE INTO table2 USING table1 ON table2. PDIM =Table1.PDIM WHEN MATCHED THEN UPDATE SET table2.GRADE = Table1.GRADE ;
Looks like your ON clause might have to be more specific. Do you have an EFF_TERM column in A that you can add to the ON clause? 
No I dont would that help? I created that temp table to have only what I want to update so basically they are all the EFF_TERM.
Right? If you need an aggregate, but can't group by? Yeah, window function. 
Anything helps at this point. What I would do is: SELECT Target.Key Target.Col1 AS OLDValue Source.col1 AS NEWValue FROM Target LEFT JOIN Source ON source. Key = target.key If you get multiples vs just querying Target by itself. There are issues.
the to_char function will allow you to specify a timestamp value as a literal string. replacing your final "and" condition to this should do the trick: and to_char(timestamp_field, 'YYYY-MM-DD HH24:MI') = '2018-01-04 07:29'
This totally works! Thank you!
I found out that I was in fact trying to merge 1 row of data that would have made a duplicate. I removed that row and it worked. Thank you so much for your help.
Nice!! Great work! Glad I could help!
These aren't interview questions, but there are three related lessons that are particularly applicable to analysts. 1. DISTINCT is a crutch. Don't use in a big query to remove duplicates. It's never the right solution. 2. If your GROUP BY has 30 columns in it, you probably could change a couple of your joins into a sub-query with a 5 column GROUP BY, and eliminate the massive outer GROUP BY entirely. 3. JOIN on unique criteria. If you're not joining on unique criteria (a primary key, or unique index keys) consider adding a sub-query and GROUP BY the join criteria.
Do you understand why it works?
Thanks man :D, feel pretty dumb now, the Christmas holidays have clearly taken their toll on me. Much appreciated. 
thanks, I spelled it correct in the code I ran in sql plus, but just made a typo when putting it here haha. thanks for the help though :D
Also, another quick question, for the exam, I may also have to use the oracle join operator (the one done in the where clause with the (+)) However, I was told in my lectures, that when grouping you cant use the where clause. So I 