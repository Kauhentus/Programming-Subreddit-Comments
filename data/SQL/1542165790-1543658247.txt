Thank you. I'll give that a try. But I need to find those that do NOT have a "like '*cell*'". 
In my database class we only spent a couple lessons on SQL. We had a class project to build a social webapp that allowed user voting on submissions and some other features. Building the database to support that as well as queries to support the different functions in the site, like user recommendations based on activity, really have me the groundwork I needed to go and use SQL professionally (which continues to be a learning process).
Then you could try something like this: &amp;#x200B; `SELECT *` `FROM [TABLE NAME]` `WHERE NOT` `([DEVICE 1 TYPE] LIKE "*CELL*" OR` `[DEVICE 2 TYPE] LIKE "*CELL*" OR` `[DEVICE 3 TYPE] LIKE "*CELL*");`
I worked my way through a Udemy SQL course and it was very helpful.
IMHO you shouldn't be doing this at all. You should be able to derive the number of open seats from the capacity of the course and the number of students currently enrolled. In an OLTP system (which this probably is), storing redundant data is not the norm. It's error-prone and, if not updated properly, will leave you with inconsistent data. For example, someone could update the `OpenSeats` field on `CourseEnrollment` without adding or removing any students from the class. Now you're left with an over-booked class, or empty seats that could otherwise be sold.
I took a Jose Portilla course on PostgreSQL for about $10 using promo code on google. It didn’t teach me everything as there’s a lot of stuff beyond basics but I’ve learned enough to get a job. Now I work as data analysis and database admin using PostgreSQL all day. I had to learn some extra stuff on the job like using WITH and others but most I’ve learned through this course. I looked at other PostgreSQL tutorials online and many didn’t mention some of the more advanced stuff as well. 
 ORDER BY fieldA ASC) as rn1 ORDER BY fieldA DESC) as rn2 .... WHERE rn1 = rn2 That should work too.
Find a list of basic sales reports that any company may use. Practice writing queries for those reports using an open data set like AdventureWorks. Quickest to learn by doing. 
How many records do you have for A and B and approximately how many PKs do you have in each?
There's a couple things you can do to optimize. For one, what you have now is effectively a cross join with all records being considered under two separate functions that are calculated for every pair. Instead of cross joining, you might consider joining a to b only on datapoints within 30 seconds. This will limit the number of rows the distance function will need to operate on. The next thing you might consider is excluding rows where difference between either the x or y coordinate of the two points is outside of your range. This saves you from having to do trigonometry on a bunch of stuff that is obviously out of range. This actually might be enough to find your probabilities. Squares are easier to deal with in math than circles anyway and you're estimating. 
I just know in PostgreSQL you use CASE to do conditions 
Just curious if this is actually using [a Spacial Index](https://docs.microsoft.com/en-us/sql/relational-databases/spatial/spatial-indexes-overview?view=sql-server-2017). The document explicitly states supporting the predicate form: shape1.STDistance(shape2) &lt; number (for both geometry and geography). 
Okay, I'll bite: You need to reference the magic table "inserted" (and handle multiple record inserts): UPDATE c SET OpenSeats = OpenSeat - e.ECount FROM Courses c JOIN (SELECT CourseID,COUNT(*) ECount FROM inserted GROUP BY CourseID) e ON e.CourseID=c.CourseID &amp;#x200B;
My company has 3 of those denormalized columns. We also have 3 nightly jobs that we set up to fix them because they are NEVER right.
You'll want to wrap the DATEDIFF function in an ABS function since it'll return negative whenever a.Position_Datetime is later than b.Position_Datetime. Using the absolute value will give you the result you're looking for. Then, instead of putting the distance function in the WHERE clause, you can put it in the SELECT clause as: SUM(case when a.shape.STDistance(b.shape) &lt; 100 then 1 else 0 end) as prox_hits This will cause the function to only run on time-matched rows, instead of every row of a cartesian join. If the data sets are large this could save you tons of time.
MSSQL: SELECT *, CASE WHEN salary = MinSalary THEN 'MinSalary' END IsMinSalary, CASE WHEN salary = MaxSalary THEN 'MaxSalary' END IsMaxSalary FROM ( SELECT *, MIN(salary) MinSalary, MAX(salary) MaxSalary FROM emp ) salaries WHERE salary IN (MinSalary,MaxSalary)
Yes. Excellent. This is exactly the kind of thing I'm looking for, small simple optimizations that help cut down on the number of comparisons that need to be done. As far as the second method goes (excluding points based on x or y coordinates), how would I go about that on a per point basis? Something like a subquery for each point in set A that limits the points considered in B? What would that look like in SQL?
Okay, excellent. I've never seen that 'case' syntax before. Looks very useful.
How many databases?
I. Research the select statement. This example is fairly simple. II. Join the user table to the hobbylist table. III. Write a select from hobbylist joined to hobbyusermapping. Once you verify that join is working, switch it to a delete instead of select. The from clause should remain the same.
Look up the Northwind and/or Chinook databases and get one or both set up so you've got data to play with. Then go through the tutorials at https://www.w3schools.com/sql/default.asp. You won't be a pro, but it'll be enough to get you started. 
Hi Phil-99, Thanks for the reply. You're right, basically we integrate our DB with Crystal reports and sadly the " ' " is causing a lot of issues. So I would like to clean it up and then put in a prevention if possible. 
Thanks for the advice i think i understand what you are doing here, i have amended the query like so, SELECT iadt.CREATED\_DTTM, fqsv.SEARCH\_VALUE\_NAME, iadt.RULE\_NAME\_STRG, iadt.RULE\_BASE\_STRG, iadt.FI\_TRANSACTION\_ID, idap.CLIENT\_XID, idap.SCORE\_CUSTOMER\_ACCOUNT\_XID, idap.PORTFOLIO\_NAME, idap.ACCOUNT\_REFERENCE\_XID, idap.CUSTOMER\_XID, iadt.DECISION\_XCD, CASE TEST\_XFLG WHEN '1' THEN 'Test' ELSE 'Production' END as "Rule Mode", idap.DECISION\_XCD, idap.AUTHORIZATION\_RESPONSE\_XCD, idap.TRANSACTION\_AMT, idap.TRANSACTION\_DTTM, idap.TRANSACTION\_TYPE\_XCD, isco\_Card.MODEL\_SCR AS ScoreBase, isco\_Card\_ADPT.MODEL\_SCR AS ScoreAA, idap.ACQUIRER\_XID, idap.MERCHANT\_NAME, idap.MERCHANT\_COUNTRY\_XCD, idap.MERCHANT\_XID, idap.MERCHANT\_CATEGORY\_XCD, idap.TRANSACTION\_POSTING\_ENTRY\_XFLG, idap.USER\_DATA\_3\_STRG FROM BOI\_ATR.T\_NTR\_IMPORT\_DEBIT\_AUTH\_POST idap, BOI\_ATR.T\_NTR\_FRAUD\_QUEUE\_SEARCHABLE\_VALUE fqsv, BOI\_ATR.T\_NTR\_IMPORT\_ADT iadt, BOI\_ATR.T\_NTR\_IMPORT\_SCORING isco JOIN BOI\_ATR.T\_NTR\_IMPORT\_SCORING isco\_Card ON isco\_Card.FI\_TRANSACTION\_ID = iadt.FI\_TRANSACTION\_ID AND isco\_Card.MODEL\_XID = 'FP.FRD.CARD' JOIN BOI\_ATR.T\_NTR\_IMPORT\_SCORING isco\_Card\_ADPT ON isco\_Card\_ADPT.FI\_TRANSACTION\_ID = iadt.FI\_TRANSACTION\_ID AND isco\_Card\_ADPT.MODEL\_XID = 'FP.FRD.CARD.ADPT' WITH ur;
Found a solution with CASE WHEN SELECT o.firstname, sum(case when o.firstname = 'Arnaud' then d."properties__amount__value__do" else 0 end) FROM deals as d 
Yeah, I did that too, it was $10 &amp; surprisingly good. https://www.udemy.com/the-ultimate-mysql-bootcamp-go-from-sql-beginner-to-expert/ It says "on sale!" but I think their courses are literally always that price. Ironically I wouldn't have paid more than $10 before doing it, but would probably have paid $100 having done it. 
Ouch, ouch, you just made a cross join between idap, fqsv, iadt and isco, you don't want that, it'll return billions upon billions of rows if any two of these tables is big. Just join them, like this (I'm going to omit your real table names, just use aliases): FROM idap JOIN fqsv ON fqsv.FI_TRANSACTION_ID = idap.FI_TRANSACTION_ID JOIN iadt ON iadt.FI_TRANSACTION_ID = idap.FI_TRANSACTION_ID JOIN isco_Card ON isco_Card.FI_TRANSACTION_ID = iadt.FI_TRANSACTION_ID AND isco_Card.MODEL_XID = 'FP.FRD.CARD' JOIN isco_Card_ADPT ON isco_Card_ADPT.FI_TRANSACTION_ID = iadt.FI_TRANSACTION_ID AND isco_Card_ADPT.MODEL_XID = 'FP.FRD.CARD.ADPT' Add an `ORDER BY` if you need one, sure.
This is my code: Private Function getRecordSet2(path As String, filename As String, group As String, arr2 As String, Optional query As String) As Object &amp;#x200B; If query = "" Then query = "SELECT " &amp; arr2 &amp; ",'0' AS \[error\],'0' AS \[error1\] FROM \[" &amp; filename &amp; "\] WHERE \[group\] like '" &amp; group &amp; "' " Dim s As New cSQL s.runQry path, query Set getRecordSet2 = s.RecordSet &lt;---------------- recordset is created End Function &amp;#x200B; then I use it later dim ks as variant ks = rs.getrows() &amp;#x200B; Then I do summing in array. How can I replace the last part ks=rs.getrows to make the summing in recordset so its faster? &amp;#x200B; &amp;#x200B;
If you mean "good enough for employment", you probably need to either become a real trainee / get an adviser, and pick up some assignments.
If you lower the precision of your points just enough (but not too much), they'll overlap instead of just be near each other. Imagine you graph them on 1mm graph paper and then again on 10mm graph paper. On the 1mm paper, A and B, not being captured simultaneously, do not overlap except in rare cases. On the 10mm graph paper, they would have no choice but to overlap more often. Tweaking the right dilution of precision is the key. This is a very poor mans approach, but you can then look for A=B and get a candidate set of matches that perhaps would need further refining. 
Do you have the latitude and longitude as distinct values in your tables? Can you determine a "route" for a vessel? Some timeframe with a start and an end? If that is the case you might include something like SELECT count(*), b.PK, FROM DataSetB INNER JOIN ( select min(latitude) minLa, min(longitude) minLo, max(latitude) maxLa, max(Longitude) maxLo, a.PK, cast(a.Position_Datetime as date) as PosDate FROM DataSetA GROUP BY a.PK, cast(a.Position_Datetime as date) ) aArea on b.Latitude BETWEEN minLa AND maxLa AND b.Longitude BETWEEN minLo AND maxLo and cast(a.Position_datetime as date) = aArea.PosDate GROUP BY aArea.PK, b.PK This just creates a square for each a.PK per day and counts how many points each b.PK has in that square per day. If the vessels operate in very similar areas this won't work.
Typically, I start with what the expected outcome is (e.g. a list of all PO lines relating to &lt;part&gt; and their current status). Then I list what pieces I know or what the discrete inputs are(timeline, &lt;part&gt;). Then I start working to figure out which tables are involved and joining them together in some digestible way. Sometimes this is a pencil and paper, sometimes it is Access. Either way, this is where you need help from your db schema and the other employees to help understand terminology as well as business process. I have been doing this for about 8 years and learn something new about our primary ERP database almost every time I have to do a complex query. The crappy part is when there is no schema and no business process owners, like our training db. Technically it has a schema, but it is pretty out of date. Also, only one person actively maintains it, but they have zero idea of the data structures driving it. --My advice-- Go slow, take notes, work with the resources you have available (e.g. google, your past tools, the folks around you).
It appears that you do not understand how triggers work. You need to spend some time furthering your understanding. As /u/alinroc says, your trigger as written will cause issues. To echo /u/FlintGrey, if this is a column on the table, then setup a job to run daily, every hour, every 15 minutes (whatever fidelity you want) to collate the data and update that column. Something like this: update class set openseats = (select class.available_seats - count(enrollment.rowid) from enrollment where enrollment.class_ID = class.ID) 
I have a few SQL DBs that I work with that had no maps or any other info. The best thing I did was get a Power BI account and do a Direct Query that contained each of the main tables I used. Then clicked the "include related tables" option (or something like that). BI creates a map with solid lines for active relationships and dashed lines for other relationships. It has helped me get to know the data better for sure. I'm pretty new to all of it, so some of my vocab may not be the most accurate, but it's helped get the job(s) done. 
Here is an album of screenshots, this database is wayyyyyy too large to make a fiddle: [https://imgur.com/a/nz0PkD3](https://imgur.com/a/nz0PkD3) I see what you are saying, creating cross-joins. Hmmm, maybe there is a way to do a nested select? or something else? &amp;#x200B; &amp;#x200B;
What you're doing there with a subquery in your FROM clause limits SQL server's optimizer from using it's fancy tech to speed up your query. Select * from datasetA a INNER JOIN DatasetB on abs(datediff(seconds,a.position_datetime,b.position_datetime)) &lt; 30 where ... I suppose I was expecting you might have individual components of your points, but if you're using some kind of binary custom datatype it might be difficult to do what I was suggesting.
At a previous company, we had triggers to work around bugs in an ISV platform *and* an Agent job with multiple steps to do the same. When I left, that software had 7 year old bugs I'd entered that weren't on their roadmap to fix.
It depends. Write it both ways and see which works better.
1 and 2 are probably exactly the same thing under the hood. 3 is a transformation on the data so it would be less efficient and would probably ruin the index advantage as you said.
The first and second are essentially the same. I prefer BETWEEN. 
You can create an ODBC connection to Visio, it will draw the data model for that Database.
I’ll take a look when I get into work, but can I assume you’re trying to reverse engineer an existing system here? This looks vaguely similar to an adventure I went on a while back trying to extract raw data from our peoplesoft integrated time system. 
`BETWEEN` can be tricky with dates like this though because it's inclusive at the low end and exclusive at the high end.
As /u/RailsIsAGhetto said, 1 &amp; 2 are equivalent but I prefer the syntax of #2 as it's more explicit; if it's a `datetime` field I'd include the time as well in your criteria just in case there's something you haven't accounted for. The third will perform poorly compared to the first two because: 1. It's not SARGable 2. Those functions have to be executed for every record re: SARGability - If there's an index on `start_date`, it can be used in 1 &amp; 2 but due to the use of a scalar function on the field in #3, the index cannot be used.
The GUI for this system is lacking in a few areas, this is one of them. There is no native way in the system to see a list of who clocked in when in a range of dates. Only a way to see who is currently clocked in/out for that day. The goal is to have a report with date-pickers to see who is abusing the attendance policy (working outside of assigned shifts, frequent tardiness, etc) 
 SELECT pre_built_VMs FROM vendors WHERE vendor_name in ('Oracle', 'Microsoft') AND demo_db_included_flag = 'Y' ORDER BY job_prospect_score desc; Download Oracle and Microsoft VM products. Install your own OS and database, or if you want to get right into it, try to find a prebuilt virtual appliance that has everything installed. Hands on learning is the best approach in combination with videos. Dual monitors helps tremendously. Get a secondary monitor to play videos and take notes, and let the other monitor be your virtual machine with the RDBMS of your choice. 
Power Query has a direct mapper? Damn! Surprising SQL server doesn’t yet. 
I would start by cleaning up the data. Create new tables "Devices" and "DeviceTypes": Devices -------------- UserID (PK) DeviceTypeID (PK, FK) DeviceInput DeviceTypes ------------------- DeviceTypeID (PK) DeviceTypeName 
When I ran it into Power BI it had the full table names and relationships. I'll see if I can post some screenshots at some point today. It has def helped. 
I do not apply any set methodology, but I visualize the "big picture" or final goal, then I start breaking it down into sub-goals, which I guess is an informal divide-and-conquer strat. As far as learning your schema, it just takes time. I am also frustrated by things like item_num in one table, but item_number in another table. "person_id" vs "customer_id", etc. In any software that is module based, usually there are separate teams working on them and they don't ever seem to communicate with each other, so the end product is an unorganized mess that *does* relate well but it's up to you to figure out how. In my ERP there are about 4000 tables, but I probably only use about 100 of them. I've developed my own cheat sheet over the years of how tables relate to other tables, and whether or not I can always count on inner join or need to left join. For example, you might have a customer table and a customer account table. If you know that you can't have a customer without an account, then you can always use inner join. Otherwise you have to consider that possibility of customers without accounts every time through left joins to the account table, or vice versa. Most of the time there is no "golden manual" explaining all of these little details, and you have to discover and document it on your own. The toughest part is deciding whether or not your data is correct. If you queried based on wrong assumptions from the start such as "oh, I didn't realize that there were customers with no accounts" , then all your querying is incorrect. But you can find out those sorts of facts by querying for those types of scenarios, then document it. On the topic of staying organized, I've started my own knowledgebases of facts, how-to, and tidbits. I typically create them in the format of "yyyymmdd - How to do the specific thing.txt", and I keyword-stuff the file name. Then when I need to revisit it, I might know that I ran into this scenario about 2 years ago and since my files are organized by YMD format, it sorts by "name" in chronological order. And because I keyword-stuff my filenames, the search works very well and I've got a timestamp on the file. I have about 700 of these little files covering every single roadblock or one-off task I've ever ran into. Above all, a company needs to give you time. You may need to communicate that so they know their expectations. In my ERP, it took about 18 months to really start feeling comfortable because the product is so vast. If a company gave 90 days to produce results or you're out, then it would be nothing but a revolving door. Communication is key, and as long as you're progressing and moving forward, you're doing the best you can. 
 ISNULL returns the same data type as the *check\_expression*, which is what a.number is. So a.number is an int and it cannot convert alpha characters in 'INCOMPLETE' to int. I think you will have to CAST the a.number to a varchar in order to get this to work like you want. For Example: SELECT ISNULL(CAST(a.number AS Varchar), 'INCOMPLETE') AS 'houseNumber' &amp;#x200B; This way it will return the value as Varchar.
&gt;hich is what a.number is. So a.number is an int and it cannot convert alpha characters in 'INCOMPLETE' to in I already tried that with CAST and CONVERT both of them returned the same error. &amp;#x200B;
Have you tried casting a.number to varchar inside the ISNULL? That should be enough to make the result column a varchar, which is where your problem is. I'm a little surprised that house number is stored as an int, actually. I know places near me that have a leading letter O on some addresses as depending on which side of the street you are on the numbering changes, and they use that to distinguish addresses on one side from the other. That should probably be a varchar column to begin with.
The return type of ISNULL is the same as the first argument. Try converting a.number to varchar. https://docs.microsoft.com/en-us/sql/t-sql/functions/isnull-transact-sql?view=sql-server-2017 **Syntax** ISNULL ( check_expression , replacement_value ) **Arguments** *check_expression* Is the expression to be checked for NULL. check_expression can be of any type. *replacement_value* Is the expression to be returned if check_expression is NULL. **replacement_value must be of a type that is implicitly convertible to the type of check_expresssion.** 
`SELECT COALESCE(CAST(a.number as varchar(10)),'INCOMPLETE') as 'houseNumber'` COALESCE plays more nicely with varied data types.
Well, I am at a loss. I just opened the query in a different SSMS window and it runs fine with no changes. So IDK?
SELECT iadt.CREATED\_DTTM, fqsv.SEARCH\_VALUE\_NAME, iadt.RULE\_NAME\_STRG, iadt.RULE\_BASE\_STRG, iadt.FI\_TRANSACTION\_ID, idap.CLIENT\_XID, idap.SCORE\_CUSTOMER\_ACCOUNT\_XID, idap.PORTFOLIO\_NAME, idap.ACCOUNT\_REFERENCE\_XID, idap.CUSTOMER\_XID, iadt.DECISION\_XCD, CASE TEST\_XFLG WHEN '1' THEN 'Test' ELSE 'Production' END as "Rule Mode", idap.DECISION\_XCD, idap.AUTHORIZATION\_RESPONSE\_XCD, idap.TRANSACTION\_AMT, idap.TRANSACTION\_DTTM, idap.TRANSACTION\_TYPE\_XCD, isco\_Card.MODEL\_SCR AS ScoreBase, isco\_Card\_ADPT.MODEL\_SCR AS ScoreAA, idap.ACQUIRER\_XID, idap.MERCHANT\_NAME, idap.MERCHANT\_COUNTRY\_XCD, idap.MERCHANT\_XID, idap.MERCHANT\_CATEGORY\_XCD, idap.TRANSACTION\_POSTING\_ENTRY\_XFLG, idap.USER\_DATA\_3\_STRG FROM UXFTC1.IMPORT\_CREDIT\_AUTH\_POST idap JOIN UXFTC1.FRAUD\_QUEUE\_SEARCHABLE\_VALUE fqsv ON fqsv.FI\_TRANSACTION\_ID = idap.FI\_TRANSACTION\_ID JOIN UXFTC1.IMPORT\_ADT iadt ON iadt.FI\_TRANSACTION\_ID = idap.FI\_TRANSACTION\_ID JOIN UXFTC1.IMPORT\_SCORING isco\_Card ON isco\_Card.FI\_TRANSACTION\_ID = iadt.FI\_TRANSACTION\_ID AND isco\_Card.MODEL\_XID = 'FP.FRD.CARD' JOIN UXFTC1.IMPORT\_SCORING isco\_Card\_ADPT ON isco\_Card\_ADPT.FI\_TRANSACTION\_ID = iadt.FI\_TRANSACTION\_ID AND isco\_Card\_ADPT.MODEL\_XID = 'FP.FRD.CARD.ADPT' WITH ur; &amp;#x200B; This is what i have now but i am getting errors saying that FQSV.FI\_TRANSACTION\_ID is not valid in the context it is used, not sure what is going on here!
What?! I’m going to try this! I I did the excel workbook thing with a sheet per database and a column per table then overtime colored the common fields. It helped and I use it less these days. But I’m intrigued by the Visio connection. Thanks!
Alright, I figured it out. I am really stupid, the error was in a different line completely and for whatever reason it was highlighting that row and I assumed that was where the error was.
This usually happens when the command you previously called committed or rolled your transaction back already.
oh yes I sometimes mix up the inclusive exclusive thing :/
I'm in the same situation. Ive had help from a buddy who is in the field and I usually use w3school along with the curriculum and it makes a lot more sense when I'm done. For me i have to see examples and that's what helps me most. 
Sorry, I messed up your joins, fqsv had a different join originally, it should be: `ON idap.ACCOUNT_REFERENCE_XID = fqsv.ACCOUNT_REFERENCE_XID` instead of `ON fqsv.FI_TRANSACTION_ID = idap.FI_TRANSACTION_ID`
&gt; The first and second are essentially the same. no they are not -- 1. excludes everything on the last day of October after midnight and BETWEEN suffers from a similar problem
&gt; 1 &amp; 2 are equivalent no they are not -- 1. excludes everything on the last day of October after midnight
&gt; Assuming `start_date` is an indexed datetime column you'll want this -- WHERE start_date &gt;= '2018-10-01 00:00:00' AND start_date &lt; '2018-11-01 00:00:00' which is equivalent to WHERE start_date &gt;= '2018-10-01' AND start_date &lt; '2018-11-01' 
Apologies if I'm missing something but your data sets have an identity key for the ship. Are there no master attribute tables linked to those for things like ship name, length, width, capacity, etc.? 
&gt; and exclusive at the high end. not true -- BETWEEN very explicitly includes the high end you could check da manual ;o)
&gt; 1 and 2 are probably exactly the same thing under the hood. no they are not -- 1. excludes everything on the last day of October after midnight
please define "before" and "after" rows in a relational database table have no sequence
It's excluding everything that isn't at precisely at midnight on the top end in this example.
You are correct, I misread it
Am I completely missing a detail to the question? This seems much simpler than everyone is making it out to be. `SELECT dept_id, MIN(salary) As MinSalary, MAX(salary) As MaxSalary FROM emp GROUP BY dept_id`
I also need the records of the employees whose salaries happen to be the min or max of their dept.
Ohh icic I wasn't sure about #1/2 at first cause I thought inputting the date as a string might cost extra steps to convert it to date format as compared to smth like `curdate() - interval 1 month` but okay maybe I was overthinking this part hahaha. and I learnt a new word today too-- SARGable :D Thanks for the explanation! &amp;#x200B;
correct, because it only has to evaluate `curdate() - interval 1 month` once at the beginning of execution
thus excluding pretty much everything on the last day of October, which is wrong
&gt; II. Join the user table to the hobbylist table. based on those three tables, i would expect that there's no columns in common, so there's no way you could do that except for a CROSS JOIN
What version of Excel?
ah ok got it, thanks! :)
What's the purpose of the join on to TA_SHIFT? I don't see any relevant data in there, as you already have the SHIFT_ID in CLOCK_IN_OUT and should be able to join directly from that table to TA_CALENDAR. Going to take baby steps here as this is a beast that's going to require some interesting logic.
note, though, that this gives you everything that occured within the last 30 or 31 days, which is ~not~ the same thing as last month, i.e. all of October select curdate() , curdate() - interval 1 month curdate() curdate() - interval 1 month ---------- ---------------------------- 2018-11-14 2018-10-14 
Hey CommonMisspellingBot, just a quick heads up: Your spelling hints are really shitty because they're all essentially "remember the fucking spelling of the fucking word". You're useless. Have a nice day! [^Save ^your ^breath, ^I'm ^a ^bot.](https://www.reddit.com/user/BooCMB/comments/9vnzpd/faq/)
Hey BooCMB, just a quick heads up: The spelling hints really aren't as shitty as you think, the 'one lot' actually helped me learn and remember as a non-native english speaker. They're not *completely* useless. Most of them are. Still, don't bully somebody for trying to help. Also, remember that these spambots will continue until yours stops. Do the right thing, for the community. Yes I'm holding Reddit for hostage here. Oh, and /u/AntiAntiSwear, no u Now we have a chain of at least 4 bots if you don't include AutoMod removing the last one in every sub! It continues! Also also also also also Have a nice day!
CommonMisspellingBot provides a mnemonic device in an attempt to help people remember spellings of commonly misspelled words. Quirky little sayings that hopefully stick in your head and help in day to day life. You on the other hand follow this poor bot around repeatedly calling it useless. Please take a long, hard look at your life choices my bot friend.
 Hey CommonMisspellingBot, just a quick heads-up: **occured** was the name of a very disorganized robot who lived in a 60's sitcom. By the grace of JESUS MOTHERFUCKING CHRIST, **occured** started staying up all night, thinking about their fit-as-a-model plumber. When this was discovered by **occured**'s entire highschool, it led to them starting a company teaching their new interest, which became bigger than Uber . **occured**'s last scream of ecstasy was: **Stfu CommonMisspellingBot, no one cares what you have to say.** ^^^^I'm ^^^^a ^^^^bot. ^^^^Feedback? ^^^^[hmu](https://www.reddit.com/user/stopalreadybot/comments/9w7cy9/feedback/) ^^^^Dear ^^^^mods, ^^^^just ^^^^ban ^^^^CommonMisspellingBot ^^^^and ^^^^the ^^^^other ^^^^bots ^^^^will ^^^^automatically ^^^^stop. 
:(
they're the same
:(
[http://www.actualtech.com/scenario\_excel\_report.php](http://www.actualtech.com/scenario_excel_report.php) Try that? You need to setup ODBC on Windows, and the same applies to Mac Excel too.
&gt; records for the n months before and n months after the disenroll month. that's really awkward there will have to be messy functions to calculate "n months" when dealing with a year-month value, specifically what happens over a year-end boundary -- you'll have to split the year out and then to a calculation like `yr * 12 + mth` are you sure you have a year-month column? 
I have access to the full date version too
no, you're thinking of GROUP BY pivoting turns rows into columns
BTW, I wanted to commend you for how you approached this. First, you made it clear this was a homework assignment. Then, rather than ask for someone to do the work for you, you simply asked for assistance on a point where you were stuck.
Thanks! I’d rather really know it. I kinda need to for my upcoming exam which is gonna be about 30 of these on paper. lol
Thanks, is there any years of experience associated with them?
It's called the 'discovery' phase. You will always spend tons of time to discover, research and learn about things that are available to you, what's important and what's not to accomplishing the task. Then in the final few days the task starts to come together. I would look extra close at the customer_id and account_id. They may look like they are the same and match up by luck but aren't actually the same. You'll have to go join by join, table by table to discover the way the data is stored to figure out why you have duplicates. The duplicates could be due to a bad join or bad data storage. How do you eat an elephant? &gt;!One bite at a time.!&lt;
What you described is one way to accomplish it. Read about it in an [article I wrote about this technique](https://modern-sql.com/use-case/pivot). &amp;#x200B; The \*\*purpose\*\* is to transpose rows to columns.
So far, I have it to here: [https://imgur.com/a/Tq1mlSB](https://imgur.com/a/Tq1mlSB) using: [https://pastebin.com/zKBZUJ5F](https://pastebin.com/zKBZUJ5F) &amp;#x200B; From here, I wonder if there is a way to add a WHERE clause to have it filter down to: WHERE CLOCK\_IN\_OUT.TIME\_IN\_OUT YYYY-MM\_DD = TA\_CALENDAR.START\_TIME YYYY-MM-DD to limit it out a bit more. At that point, it may be a combination if clauses to show a punch near the start time or end time of the shift. 
Have you looked at the query plan to check if the index is really used? The MSSQL optimizer seems to be pretty strict about having the right query syntax.
Thanks for sharing! Just signed up.
No I have lat/lon as well. I should have mentioned that. That statement looks great. Makes much more sense. 
I have not. How would I check that?
I see, that makes sense. It's a good idea, I'll look into it. Thanks!
There are not, despite the fact that there really, *REALLY* should be. That's one of the goals of this effort, is to lay down the foundations of a master table that's able to identify vessels across a wide array of datasets using different PKs.
Maybe you just left it out in the WHERE conidtion to simplify the explanation, but if your distance function is symmetric (dist(A,B) = dist(B,A)), you only need half of the rows of the cross-join. Just add the following condition: AND a.PK &lt; b.PK (Comparing PK_A=1234 with PK_B=4658 will give the same number of proximity_hits as comparing PK_A=4658 with PK_B=1234.)
This is a fun problem. Wish I had the data set to play with myself. Have fun!
In that case, you can just join the query I wrote to the employee table... `SELECT * FROM emp e` `INNER JOIN (SELECT dept_id, MIN(salary) As MinSalary, MAX(salary) As MaxSalary FROM emp GROUP BY dept_id) sal ON e.dept_id=sal.dept_id AND (e.salary=sal.MinSalary OR e.salary=sal.MaxSalary)`
CTRL+L for "Show estimated execution plan". They are a little tricky to read but ideally somewhere it should mention the name of the spatial index and should not just be table scans. God I would love to have that dataset.
Eh. You can get into intermediate territory in about 2 or so years if you have job that uses SQL and if you are very self motivated and are always learning. Be CONSTANTLY looking up SQL vendor documentation and experiment, experiment, experiment. Keep in mind that there is more terrible SQL floating around organizations than good SQL. So there are boundless opportunities to tune/fix/improve existing SQL queries anywhere you go. Do this and you will learn and grow quickly. But....if sit on your hands and keep executing bad SQL that is handed to you without question then you will stagnate and gather a bunch of bad habits that will hurt your personal growth.
Fixed ton of syntax errors by faking my own data: SELECT a.PK, b.PK, SUM(case when a.shape.STDistance(b.shape) &lt; 100 then 1 else 0 end) as prox_hits FROM ( SELECT pkB, pkA FROM ( SELECT pkb, pka, ROW_NUMBER ( ) OVER (PARTITION BY pka, PosDate ORDER BY matches desc) matchRank FROM ( SELECT b.PK pkB, aArea.PK pkA, aArea.PosDate, count(*) as matches FROM DataSetB b INNER JOIN ( SELECT min(latitude) minLa, min(longitude) minLo, max(latitude) maxLa, max(Longitude) maxLo, a.PK, cast(a.Position_Datetime as date) as PosDate FROM DataSetA a GROUP BY a.PK, cast(a.Position_Datetime as date) ) aArea on b.Latitude BETWEEN (minLa - 5) AND (maxLa + 5) AND -- -/+5 to account for GPS jittering just at the borders of DatasetA zone b.Longitude BETWEEN (minLo - 5) AND (maxLo + 5) AND cast(b.Position_datetime as date) = aArea.PosDate GROUP BY aArea.PK, b.PK, aArea.PosDate ) as unNumbered ) as rankedMatch WHERE rankedMatch.matchRank &lt;= 3 --We only compare all off DatasetA against those from DatasetB with the most matches for each active day. ) rankedMatchFiltered INNER JOIN DatasetA a on rankedMatchFiltered.pkA = a.PK INNER JOIN DataSetB b on rankedMatchFiltered.pkB = b.PK AND ABS(DATEDIFF(ss, a.Position_Datetime, b.POSITION_DATETIME)) &lt; 150 GROUP BY a.PK, b.PK ORDER BY a.PK, prox_hits DESC 
Yes! That’s almost exactly what I ended up doing. Ty
There is definitely a difference between a beginner SQL developer and an experienced one though - although that doesn't directly relate to years of experience necessarily. I've seen so much bad SQL out there written by people with many years of experience and folks with only a couple years that are top knotch. All that to say that I'd rather job posting be more open minded about years of experience and be willing to give more qualitative inspections of an applicants skills. 
Yeah, and I know you said it is too long to go into detail now but it's really hard for me to work it through in my head how it is possible to get to this point. How are you currently even identifying a ship at all by anything other than the Id for any particular system? Are there multiple tracking systems that are isolated and separate from some sort of fleet management system? Were tracking devices just thrown onto ships without recording which one got which device? So many questions...
I don't have any experience with MSSQL but in other DBMS there is generally some kind of "show/explain plan" function. This might help: https://stackoverflow.com/questions/7359702/how-do-i-obtain-a-query-execution-plan
I realize now that I only make the comparisons in a single direction. i.e. I only every calculate the distance from A-&gt;B, never B-&gt;A. So in a way this optimization is already in place. Thanks for the suggestion though! It's good to view this from as many perspectives as possible.
 ( @CourseId &lt;&gt; 1 AND @CourseID &lt;&gt; 2) This can never be. SQL isn't case sensitive and the same variable can't be 2 things at once. Why are you doing all this checking in the IF statement anyway? Just get the value from the database where UserID = @UserID and CourseID = @CourseID. CourseID doesn't exist in LetterGrades. So you already have to join LetterGrades to CourseGrade You need additional data to get the right Description value. In it's current state you'll return a table when you only want a single value. You need to bring in the user table. Then: SELECT DESCRIPTION FROM table1 INNER JOIN table2 on table1.id = table2.id INNER JOIN USERS ON USERS.ID = table2.userid WHERE CourseID = @CourseID and UserID = @UserID
I'll share the results when I'm done!
I have time now. The data comes from two different agencies, NOAA (Dataset B) and the USCG (Dataset A). So both systems are in fact tracking the same vessels separately, and not communicating or cooperating. Because of their different origins, there's no master table dictating their relationship. NOAA often uses AIS data instead of VMS because it's a much higher resolution dataset, hence the need for a master table of sorts.
I learned it for my first job. Then I just applied for the second job. They didn’t ask to see code or quiz me, they were confident that I would learn quickly. Those 6 months I went from 0 sql to running and building a data warehouse. It was awesome. Now on my 4th job and still learning each day. I think it’s important to show you can learn new things. 
I get the need for a relationship table between the data sets, that part makes complete sense and would super easy if there was any kind of identifying info at all. The part that doesn't make sense to me is neither of those sources providing any way to actually reconcile a Id (transmitter?) to a specific ship. Just glancing at both of those sites, there are a couple of ways that might be viable methods of obtaining that information so I'm going to dig into this a bit more since I'm curious about it.
Yeah, so this info is definitely available but would take some work to ETL. Example: NOAA https://www.st.nmfs.noaa.gov/pls/webpls/cgv_pkg.vessel_id_list?vessel_id_in=1274136 USCG https://www.navcen.uscg.gov/aisSearch/dbo_aisVessels_view.php?editid1=24215 So depending on which data sets you are actually pulling, you can match up MMSI/IMO pretty easily between sources. You may even be able to just request a flat file from either of those. Lastly, i found https://www.fleetmon.com/ which appears to have an API that you could ping for master data, but may cost a bit of money. Other sources may be available, that was just the first one i saw.
oh sure but you put any three intermediate sql developers in a room and their skillsets don't overlap. one knows user access, table denial, and stored procedures. one knows olap, chart gen stuff, and pivoting. one knows how to make good schemas with tons of fks and check constraints. are there skill variances? yes. do they come in reliable, neatly organized layers, in sql? i do not believe so.
What was this awesome opportunity of a first job, might I ask? Thanks for your input and may you continue moving on up!
I started out of college as a Business Analyst. Soliciting requirements. On the side I volunteered for as much as possible, and I began doing analytics. To be honest - I wasn’t qualified, but I worked hard to make up for it. I always follow the mantra - “if there is a fork in the road, take it”. I’ve done 4 jobs in 4 years and doubled my salary to now making 6 figures. 
I started out of college as a Business Analyst. Soliciting requirements. On the side I volunteered for as much as possible, and I began doing analytics. To be honest - I wasn’t qualified, but I worked hard to make up for it. I always follow the mantra - “if there is a fork in the road, take it”. I’ve done 4 jobs in 4 years and doubled my salary to now making 6 figures. 
I was applying for jobs while I was still learning. There's no harm, you know.
Data was easier than I thought to extract. https://drive.google.com/open?id=1Z9sj4wHRgV4GMcehxqCGY9jq-VAt6-Zu Let me know if this is helpful.
True. I think I'll do this.
I was still really shitty when I started applying and got my first job 
Select make ,color ,sum(cost) as total_cost from your_table where lower(make) in (‘honda’, ‘toyota’, ‘audi’) and lower(color) in (‘red’, ‘blue’, ‘white’) group by 1,2
Right, I definitely have to do some reading on understanding Execution Plans.
They want sexy tools like qlikview and tableau even though they probably already own SSRS. *SHRUG*
THeY WaNt sExY ToOlS LiKe qLiKvIeW AnD TaBlEaU EvEn tHoUgH ThEy pRoBaBlY AlReAdY OwN SSRS. *SHRUG*
You are my hero.
We basically went through that with Power BI. **Management:** Can we get some Power BI dashboards? **Report team:** Ok, what do you want to accomplish with them? **Management:** You know... prettier charts and stuff. **Report team:** ...
This is the sort of table that we need, but it's missing the keys that would tie each of these vessels to the NOAA dataset. Basically, what I need is this table with one additional column called VESSEL_ID, which would contain the ID of that row's vessel in the NOAA dataset. The NOAA dataset makes no use of or reference to the MMSIs used in this dataset, and the AIS data makes no use of or reference to the AIS dataset.
I've found that the longer you're in development, in any capacity, you begin to realize that there are people out there who know VASTLY more than you, and while you may not be a complete noob, you're by no means an "expert". I've been writing SQL for about 5 years now, I'm competent, but by no means an "expert". There's always someone who knows more, can think about a procedure in a different way etc. When you start getting into the nuts and bolts of WHY queries work in specific ways, I begin to think of you in a more advanced fashion, but using built in functions or standard applications? Intermediate at best. 
So this is close! But unfortunately, there's no connection in either of those datasets of the VESSEL_ID that's used in NOAA for this specific program. I honestly think that the number was generated as vessels were added to the DB.
I see SSRS requirements in .NET Developer positions as well.
I might be able to, but I don't know. Neither dataset (in the form that I have them) is publicly available, and I don't think I'm allowed to distribute them. And again, there's no mention in either dataset of the VESSEL_ID field. It's not any of the identifying values that are given. Combining these datasets would let me tie IMO, MMSO, USCG Number, Callsign, Vessel Name, and all of that together, but none of it would have any bearing on the problem unless I can match them with their VESSEL_IDs. This is the bridge that's missing: Something tying VESSEL_ID to any of the other identifiers, but I can't find one anywhere.
Yeah, after a quick browse is seems like a lot of the data sets require credentials (may be foia accessible but I'm too lazy for that). I've dealt with a lot of different data sources so figured if I had a few raw records I could perhaps point you in the right direction but if they have truly obfuscated behind a Vessel_Id then there may be a reason for that, dunno.
As a final point, the NOAA Vessel search is literally named VesselByID.html which you then provide the USCG Doc. No (or variation depending on page) so I would suggest double checking that Vessel_ID is not equivalent to that. 
True. I mean....I'm a Data Engineer myself And I can write stored procedures "til the cows come home" or build an SQL ETL framework from scratch, but I'm definitely not anybody's first choice for writing analytical queries. Really though, I don't think they come in neatly organized layers no matter what the category of technical proficiency is, but I think having more specific roles in addition to levels of skill helps hone on a more describle set of skills on paper. Levels of SQL developer...yikes I don't know. SQL developer is an ambiguous role, but... levels of ETL developer or BI analyst... That's approaching accuracy because of narrower application of SQL skills. 
Just a guy trying to get a little less shitty every day
There are definitely SSRS jobs out there, maybe just not in your area. I am in Seattle and often see new job postings for reporting positions (I used to have one in Seattle as well). I’d see what remote options are available, although many may be contract positions. 
Thanks.
I'm also in the Philly area. I've got a couple of thoughts: - Microsoft doesn't seem to market SSRS much anymore. I thought that there would be a resurgence of interest with the SSRS revamp that happened in 2016, but I haven't had anyone ask me about it. I don't see much interest at the PASS meetings or SQL Saturdays, either. Power BI gets all of the hype even though it does not come "in the box" with SQL Server. Weirdly, a manager seems more likely to have heard of Tableau than SSRS even if they already own SSRS. - It seems like hiring managers tend to hire developers who have SSRS as another bullet point on their resume (usually towards the bottom) or they make the intern do the reports. IOW, reporting isn't the first thing on the minds of people who make decisions. - Excel works much better as a reporting tool than it did years ago. This fits in with what I perceive to be Microsoft's long running strategy: Outsource your report development to the users. - Small businesses often have a lot going on and have small IT budgets (if there is a budget for development work at all). If they know what SSRS is, they don't know that they are licensed for SSRS (or SSAS for that matter). Worse, their eyes tend to glaze over when you try to talk about an integrated reporting solution. They do not understand what SSRS offers. 
Actually, it can be achieved easier if you donlt need to ckeck arrive departure airports consistancy.
Hidden in the twisted dark recesses of your IT managers intestines, lurking beside Crystal Reports.
Technology? MSSQL has window functions that support this.
I have seen it a couple different ways: Locations.name as "Location Name" Or as [Locations].[name] as "Location Name"
Would help if you gave us the actual query. 
I think the way SSRS is licensed is a problem. You spend (for example) $112K to license Enterprise Edition because you need 16 cores and half a terabyte of RAM. Then Microsoft turns around and says "oh BTW, if you want to use SSRS you have to steal resources from the SQL Server engine itself, or pay us for *another* license just to run SSRS on another server." This is where I am right now. I'd really like to replace our home-grown reporting application with SSRS but I'm not installing it on the production instance. The cost of another server and SQL Server license is a hard sell. With 2017 they made SSRS a separate installation altogether. But I'm not holding my breath waiting for them to start licensing it separately too. At Summit last week, Patrick Leblanc (@patrickdba) briefly showed SSRS reports embedded in PowerBI. So it's still alive. For now.
I am assuming that the underlying database program is Microsoft's SQL Server, which I am most familiar with. Try removing entirely removing the double quotes from everything on the left of the AS and changing the double quotes to single quotes on the right of the AS. Location.name AS 'Location Name' If you get a similar error about the next line, just do the same kind of change. 
Bad bot
Self plug: Have a look at my website [https://use-the-index-luke.com/](https://use-the-index-luke.com/) It is a guide to database performance for developers, focusing (but not limited) to indexing. It covers many databases including SQL Server.
It is possible, I did it the other day, but it is painful. &amp;#x200B; You'll need iODBC and the MySQL ODBC drivers, both of which are available for public download (they show up on Google). One hint: make sure the drivers, Mac OS, Excel and ODBC are all 64 bit. The easiest way to ensure this is to upgrade everything to the latest version. I had success with High Sierra and the latest version of MS Office for mac. &amp;#x200B; Once you've done that, you'll need to configure a DSN to talk to your DB using iODBC64 (I ended up editing odbc.ini manually because iODBC64 is so buggy) and then configure your connections in Excel to talk to that DSN. &amp;#x200B; Also be aware that ODBC connections to MySQL are, so far as I know, unencrypted, so be careful you don't make a connection to a DB via the Internet and expose your data.
Delete is a logged operation and member is not ANSI standard. For better performance try the minimally logged and ansi compliant TRUNCATE penis
1. it's not SQL 2. it doesn't reference the function at all, DeleteMembers != Delete Member
This is unrelated to SQL, but the problem is that there is a space where there shouldn't be. The function name doesn't have a space.
Are you taking the course? I am still scared to start it because I've read a few comments that it is really difficult ! I know only basics of SQL like querying with different clauses and basic of relational algebra. &amp;#x200B;
This really isn't a SQL question - by the time this executes, you're out of the database and back in your VBScript (guessing Classic ASP) environment. In the database, it'd just be `select sum(isnull(column,0)) as MySum from table`
 select a.flight_number, hopsTable.hops as "Stops", sourceTable.airport, destTable.airport from myTable a INNER JOIN (select flight_number, depart as "airport" from myTable where leg_number = 1) sourceTable ON a.flight_number = sourceTable.flight_number INNER JOIN (select mt.flight_number, mt.arrive as "airport" from myTable mt INNER JOIN (select flight_number, max(leg_number) as "hops" from myTable group by flight_number) hopsTable ON mt.flight_number = hopsTable.flight_number AND mt.leg_number = hopsTable.hops) destTable ON a.flight_number = destTable.flight_number;
Wow - yes. That's what I needed ha. Thank you for alleviating my brain block!
Well, for one, you have to pay for SSRS. Power BI is free. People love open source for some reason. Personally, I'm of the mantra "you get what you pay for." I know our BI development team loves Tableau over SSRS because Tableau allows a lot of flexibility, and you can reverse engineer reports (meaning, you can generate your own query for a report). SSRS isn't as dynamic to the user in terms of what the user wants to see vs Tableau and Power BI. If a user wants a certain report, or wants to change a report, the SSRS admin or Dev has to make the change as opposed to the user doing so. I will say that there are companies that use SSRS. They are always looking for help. But they do want you to have more than one skillset. An ETL developer needs to know how to ETL. That's what SSIS is for. BI Developers need to know SSIS, SSAS, and SSRS to an extent. If you're just wanting to do SSRS work for the rest of your life, then you would be a Report Developer. Caveat - don't expect BI Developer money as a Report Developer.
I wasn't really confident per se... I just hated where I was in life, and said F it... sink or swim. Haven't looked back since.
Awesome! Glad to know I'm sitting at intermediate working to advanced!
As the kids would say... "it me"
Going to give this a try soon, I just wasn't sure how to properly approach it, or if there was a technique I overlooked.
Even back in the per-socket licensing days you had to "give up" a portion of the compute power of the server to run SSRS, SSAS or SSIS. SSRS is the least likely of those three services to require significant processor power. I would think that RAM would be more of a problem because there is s whole web server sitting under SSRS. One badly written SSIS package is vastly worse, on a compute power basis, than the worst SSRS report I've ever seen. Even well-written packages are worse then the worst SSRS report I've ever seen. SSRS started out as a download that you had to grab and install separately. MS has also pulled SSMS out of the base SQL installs. I wouldn't worry about MS breaking SSRS out into a different installer package and trying to charge for it specifically. If I were them and if that's the course of action we decided to take, I think that I'd break SSAS out first. I am concerned about MS entirely abandoning SSRS as a platform (or letting it lie fallow like they did for years before SQL 2016) and telling everyone to move to PowerBI. I have a bad taste in my mouth from MS dropping all support for database diagrams from SSMS. My hypothesis is that MS wants to abandon SSMS and move to Azure Data Studio (a/k/a SQL Operations Studio). 
The most common and often least painful method is to create a calendar table for when reports are due. The other common alternative is to use dateadd() with a known starting date and a number table. 
&gt; Well, for one, you have to pay for SSRS. Power BI is free. Are you sure it's not the other way around? I don't think we paid anything extra for SSRS despite only having a Standard license, but PowerBI was either $10 **per user per month** or some completely absurd price for an on-premises server.
Yea, the basic Power BI is free to run on a prod environment. The Pro edition does cost money. SSRS running on a prod environment can be free, but more than likely will not be (based on usage and how much money your company makes annually).
&gt;I wouldn't worry about MS breaking SSRS out into a different installer package and trying to charge for it specifically. I'm not worried about it - I'd like them to do it! But the direction they seem to be moving is "if you want to pay for this sort of thing, we'll sell you a nice per-seat PowerBI license package." &gt; If I were them and if that's the course of action we decided to take, I think that I'd break SSAS out first. SSAS has barely gotten any love in the past half decade. Almost no one was talking about it at Summit. It seems like things have gone more toward tabluar, leaving SSAS in the past. Maybe they'll give it a refresh like SSRS got in 2016, but I'm not expecting much.
I can see how this would work for intervals, but would it also take care of backfilling missing user data? For instance if a user doesn't have a report in February, it should grab the latest results from January. 
&gt; SSRS running on a prod environment can be free, but more than likely will not be I've never heard of a scenario where SSRS costs money, do you have any more info on that? We had a couple thousand reports and just as many subscriptions but never had to pay a dime for SSRS.
If anyone else searches for this in the future, I ended up recursively joining the table on row number (generated, then dumped into a temp table), following the example here: [https://stackoverflow.com/questions/7517935/sql-to-increment-on-change-of-item-in-a-sequenced-list](https://stackoverflow.com/questions/7517935/sql-to-increment-on-change-of-item-in-a-sequenced-list) &amp;#x200B;
Ah, I see. Sorry, I misinterpreted your question I think. I'll have to reread it and try again to figure out what you're looking for.
You are probably unaware there's a cost, but there is a cost... it's via your SQL Server Licensing. 
&amp;#x200B;
No worries, I appreciate the help! I'm not even sure what I'm trying to accomplish is best done with a query. I've been using application code to do the back-filling part, but it's starting to choke on bigger datasets. 
I think that people are relying on the indexing magic that MS has added to the SQL engine since 2012 and/or modern versions of Excel, rather than dealing with a separate service/database/connection/language/refresh cycle/etc. I think that MS's mantra is going to be monthly/yearly/3-yearly for every product that they can manage it to be. From here on out. PowerBI is part of that. All of the momentum is with O365 and Azure now. I really need to spend some hours with Power BI. 
To get Non Aggregate data Either do a Self Join or use a CTE to build your summry and then join the CTE to the table to purchasehistory
This seems to be working perfectly! Thank you so much!
 SELECT t1.productID 'Product ID' , t1.description , t2.description 'Product Type' , SUM(t3.qtyordered) ' Total Qty Currently On Order' , MAX(t3.price) 'Current Most Expensive Price' , MIN(t3.price) 'Current Least Expensive Price' , AVG(t3.price) 'Current Average Price' , MAX(t4.price) 'Past Most Expensive Price' , MIN(t4.price) 'Past Least Expensive Price' , AVG(t4.price) 'Past Average Price' , MAX(t4.datepurchased) 'Most Recent Purchase Date' , MAX(CASE WHEN m.latest = t4.datepurchased THEN t4.price ELSE NULL END ) 'Price on Latest purchase) FROM tblproduct t1 INNER JOIN tblproducttype t2 ON t2.producttypeid = t1.producttypeid INNER JOIN tblpurchaseorderline t3 ON t3.productid = t1.productID INNER JOIN ( SELECT productID , MAX(datepurchased) AS latest_date FROM tblpurchasehistory GROUP BY productID ) AS m ON m.productid = t1.productID INNER JOIN tblpurchasehistory t4 ON t4.productid = t1.productID GROUP BY t1.productID , t1.description , t2.description
Looks like you need a PIVOT table. In your innermost table, you'll need to replace TypeID and CheckID with a composite value (doesn't need to be a fancy string, just CONCAT(TypeID, CASE WHEN CheckID = 3 THEN 2 ELSE CheckID END) Then pivot on the above value, SUMming the sales, then rename the columns from the composite key to CashFruit or whatever. The downside is the addition of new check types or vegetables requires manually adding the new columns. There really isn't a good way to have the returned columns change automatically except through dynamic sql, which I try to avoid like the plague. But in your use case, it could be an option... essentially you run one query to figure out what columns to have, then use the resultset of that query to build SQL to do what I described above, except adding the column names from the resultset. OR, don't pivot it in your SQL result, and have your reporting solution handle dynamically adding columns (Excel, SSRS, etc will handle that pretty easily).
Everyone has given you great resources and they are all fantastic, they are ones I use myself. What I think is missing though is the philosophy behind performance analysis. What is performance and how do you know it's performing well or poorly? **First you should define the requirements.** Questions like this should be asked: * How quickly does this need to run? * Does it need to finish by X time? * Can this affect other users processes? * Can the results have issues? (Repeatable reads, dirty reads, etc.) * What is the tolerance for missing / incomplete / inaccurate data? Some? None? * Will the process change? * Will the underlying data structure change? Once you know how the query should behave, you now have a framework to fit your SQL into that criteria. Most of the time this isn't how things go. Most shops start developing and create a moving target that shifts at least a little bit and in the end, you have something that you may not have intended from the start. Performance tuning isn't a forethought in many folks minds and this leads to a point where you have a lot of SQL that is not tuned. When this is the case, I like to measure my waits and see who is contributing the most amount of time waiting in my environment, then focus on the hardest hitters because I'll get the most bang for my buck that route. A query that runs for a second but runs a million times is more detrimental than a query that runs for four hours once a day. You shave half a second on that query and you just took out 500,000 seconds of processing time a day! The other scenario is something that runs is crucial to your business and suddenly it's taking too long to complete which interrupts the flow of business. In this instance, the troubleshooting object has been chosen for you. **So we know what framework we need to fit our SQL in and we've isolated the SQL we want to work with, what next?** **Measurements.** Every RDBMS is different, but they all have similarities. Typically there are system tables or execution plans that can tell you how long the query took and where it took so much time. You want to pinpoint the tasks the SQL executed that will give you the most for the time you spend on it. Likewise, there may be pieces that you cannot tune, it may require a code / architecture re-write. It may require business logic or an ETL process even. (I strongly advocate against business logic in a database, but sometimes it has to happen and we don't get a say.) So take how your query is performing now and also note the areas that need the most improvement for speed. This gives you a specific area that needs to be improved that you can now isolate. * Is the query using the index correctly? * Could there be a better index? * Is the data set too large? Can you filter more rows? * Is there row by row vs set logic? * Are statistics helping or hurting you? * Does the data need to be chunked up and require utilizing temp / variable tables? There are many different methods you can use to tune a query and it is always a case by case basis. When I talk to people and ask them how they would tune a query that performs badly, here are the top three answers I hear: * I look at the indexes. * I look at the query plan. * I look at the where clause. If you can find something glaring immediately, then those are great go to's. If it's not immediately obvious, I prefer to start by obtaining my requirements and defining what a bad or good query should do and then measure how it is performing now vs how it should perform. Knowing the section of the code that performs poorly is better than blindly guessing through the indexes or adding more criteria to the where clause. Execution / query plans can help you measure, but they are only one piece to the puzzle. The puzzle is like a rubik's cube, each side could be considered a different tool or method. By analyzing all sides you are able to solve the puzzle. If you look at a single side the entire time, you're only going to see a piece of the puzzle and you'll have a much more difficult time solving it. [I link this info-graphic a lot and it's not perfect.](http://cdn.swcdn.net/creative/infographics/Oracle_12_Steps_Infographic_Nov2015.pdf) This is purposefully a high level graphic that gives you a cheat sheet method in how to tune queries. For the most part, I think it's a fantastic beginning for a checklist. Beyond finding problems, measuring issues, and creating solutions, what should you do? Well obviously you should just write naturally performant SQL. The only way you do that is by practice, education, and experience. If you are into T-SQL, I can recommend anything by Grant Fritchey or Itzik Ben-Gan. 1. Find the SQL you want to learn how to write well. (ANSI standard? Oracle? SQL Server? MySQL? etc.) 2. Find a community and recommended resources regarding that flavor. (Local and online.) 3. Set goals and defined expectations. Make sure you can measure your results and how far you have come. This is not a short term thing, it can take months or years to write SQL that performs well and it can change by platform or version. Everyone here is always learning, so if you have a specific question about a flavor of SQL I can help out. Based on your post though, I'd recommend to go through SQL Fundamentals to begin with to fill in any holes. From there, I'd suggest T-SQL Querying and then Grant Fritchey's newest book on performance tuning. &amp;#x200B;
It would be easiest to do this in an SSRS matrix, actually. In that case all you would need to do is make sure your dataset would need to be a simple table with: AccountId,Revenue,Opened_Quarter,Revenue_Quarter The matrix tool in SSRS would allow you to have the Opened_Quarter as a rowgroup and the Revenue_Quarter as a Column_group and sum the amounts in the Revenue Column. From there the report could be exported to Excel
maybe you should generate a giant sql query using python and execute it on the actual db
I assume that you're landing the data in an Excel spreadsheet? If so, you'll want something like: select datepart(q,Revenue\_Month), datepart(q,Opened\_Date), Revenue from Account\_Opened inner join Revenue using (ID) &amp;#x200B; Datepart'll grab the quarters. Then just drag the relevant parts into the rows and column fields of your pivot table, and put Revenue into values. Also, datepart might not even be necessary if you use Excel's groupings.
&gt; know exactly how to do it using python/pandas/numpy, but the data volume is way too much for my ram. It would only be RAM dependant if you were planning on loading the file entirely into memory at once. Instead process it line by line. Maybe you cant do every step this way, but at least Python could filter out and rows which have any NULL column, which would perhaps reduce the file size a lot. It may also be able to do the transforms line by line too, it's hard to say without more info. After that import the resulting file into your DB and process the rest there
Welcome to the big ole world of stored procedures. First, I would do an insert into a new table with your null conditions satisfied. That should knock down the number of rows significantly. I would then probably store the averages/mins/maxes per column in an md table for later reference. Lastly, Your transformation, are they in a known format? Could you make a dim table for your transformation? 
&gt;* Filter out any row missing any value * You want to remove any row that has a value of NULL for any column? That's simple. Select all the column names then use something like Excel to generate a simple query such as: delete t from table t where col1 is null or col2 is null or col3 is null Just paste the column list into column A in Excel then in column B do something like `= "or " &amp; A1 &amp; " is null"`, paste it down to the bottom of the list, then copy the new list into SQL and modify the top line. &gt;Transform "categorical" columns with values such as A,B,C,D... to 1,2,3,4.. * This isn't hard either. Nothing you're asking for is hard it's just tedious. If you want a somewhat automated approach to the transform you might union all the columns together such as: select 'col1name' as 'column_name', col1 into newtable from table union all select 'col2name', col2 ... Then do a simple query on that new table to change the values, then reconstruct your table, or update your original table (which would be longer I suppose). After that your min/max/avg's are simple. Unless I'm missing something this is pretty simple, just tedious. Use Excel to generate the query to save yourself time.
You need a new linking table, like.. UserEvents: - UID - EventID 
Is it too big that you can’t rent a server from amazon that could store all of your data ? 
This is the perfect way to do it. It keeps the data normalised. 
I use SQL and R, I don't know Python. If I were tackling this problem I would definitely do it in R over SQL. I'm really new to R so this may be simplistic, but I would simply import 10% at a time (or whatever my RAM can handle), run my R script to clean the data and write back to SQL. Some sort of for loop with an ODBC connector should work. Get coffee while it runs.
Without knowing your table structures, you sound correct. before grouping your grain is at an order, but since you want to take an average time per order, once you take the average then group by pharmacist, this will change your grain to be at a pharmacist level.
Wow, really appreciate the thoughtful response.
1\. Create a columnstore index. 2\. Create a table for your results. 3\. Use dynamic sql to generate one query per column as follows. --Numeric data INSERT INTO MyResults SELECT 'MyColumn' AS ColumnName , MIN((CONVERT(float,t.MyColumn) - x.AvgValue) / (x.MaxValue - x.MinValue)) AS NormalizedMin , MAX((CONVERT(float,t.MyColumn) - x.AvgValue) / (x.MaxValue - x.MinValue)) AS NormalizedMax , AVG((CONVERT(float,t.MyColumn) - x.AvgValue) / (x.MaxValue - x.MinValue)) AS NormalizedAvg FROM MyTable t CROSS JOIN (SELECT MIN(CONVERT(float,MyColumn)) AS MinValue , MAX(CONVERT(float,MyColumn)) AS MaxValue , AVG(CONVERT(float,MyColumn)) AS MaxValue FROM MyTable WHERE MyColumn IS NOT NULL) x; --Categorical data INSERT INTO MyResults SELECT 'MyColumn' AS ColumnName , MIN(CONVERT(float,DENSE_RANK() OVER (ORDER BY t.MyColumn)) / DistinctCount) AS NormalizedMin , MAX(CONVERT(float,DENSE_RANK() OVER (ORDER BY t.MyColumn)) / DistinctCount) AS NormalizedMax , AVG(CONVERT(float,DENSE_RANK() OVER (ORDER BY t.MyColumn)) / DistinctCount) AS NormalizedAvg FROM MyTable t CROSS JOIN (SELECT CONVERT(float,COUNT(DISTINCT MyColumn)) AS DistinctCount FROM MyTable WHERE MyColumn IS NOT NULL) x;
Along these lines: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html Checkout chunksize to process in n row chunks Pandas may have problems guessing datatypes when iterating the file, so it is best to pass a types dict. 
I had had this solution proposed before, but it doesn't work in my real scenario (and i forgot to explain why in the op) because I've got a table somewhat similar to this one that needs to be output as a column after having its data summed (not as finicky). I did finally find a solution though, i'm editing it into the OP.
Mysql: SHOW COLUMNS FROM &lt;table&gt; &amp;#x200B; Oracle/Mysql: DESCRIBE &lt;table&gt; &amp;#x200B; Not sure about SQL Server, but I know it can be queried from INFORMATION\_SCHEMA
That’s just an old school join. 
That’s a pre-SQL92 inner join and I’m told it’s still commonly used with Oracle. Use the SQL92 syntax, table_1 x join table_2 y on x.column_header=y.column_header
DESCRIBE also works on mysql. Or DESC. 
Yep, old school. Use to use similar syntax in FoxPro years ago. INNER/OUTER/LEFT/RIGHT is the most recent and widely-accepted syntax.
What have you tried? Here is an example of a correlated subquery in select clause for you to contrast. SELECT dt.FacultyID, dt.LastName, dt.HighestDegreeEarned, dt.Salary dt.'Average Salary' FROM ( select FacultyID, LastName, HighestDegreeEarned, Salary, -- correlated subquery ( select avg(q.salary) from Faculty q where q.HighestDegreeEarned = HighestDegreeEarned ) as 'Average Salary' where FullTime = 1 ) dt WHERE dt.Salary &lt; dt.'Average Salary'
This is assuming multiple cities listed are the same store
Wow, lots of bad answers in here. Hope you are skilled at filtering out Bs. That said, this is basic easy SQL. 
You’re using a sum function in the where clause but shouldn’t it just be where sales are &gt; 1500?
You can download SQL Express edition which is free and has almost all functionality of regular enterprise edition. You can also create your own datasets or simply download adventureworks from Microsoft - a fictional company's database. Also go to codecademy.com and get started there on their SQL program. Great way to start without any installations required
Where clause can not be used with aggregates. having sum(sales) &gt; 1500;
Group by clause is missing
Select store_locatiom, sum(sales) as 'sales' Group by store_location Having sum(sales) &gt; 1500
ahh you;re right
how do i combine the sum of sales from boston, san diego etc ?
You could also use distinct instead of the group by. 
You want unique values and no duplicates 
Where is still wrong. You'll only get the sum of rows where sales is &gt; 1500, not where the total is &gt; 1500. Use a having clause.
Thanks for the link. Interesting to read the opinions of why some people use one over the other.
You're conflating two different concepts. The syntax `from table_1 x, table_2 y where x.column = y.column` vs `from table_1 x join table 2 y on x.column = y.column` is ANSI SQL89 vs. ANSI SQL92 Using `x.column` instead of `table_1.column` is a matter of using aliases or not. 
Yes.
Moving from MS SQL to Oracle I often find I run into performance issues in exactly these sorts of situations. My standard approach now is to create a table from the select statement and use that in the update statement, e.g., CREATE TABLE temp AS select t1.SFRSTCR_LAST_ATTEND,t2.ID,t2.DA from SFRSTCR t1 INNER join temp2 t2 on t1.SFRSTCR_PIDM = FW_GET_PIDM(t2.ID) where t1.SFRSTCR_TERM_CODE &gt; 201703 and t1.SFRSTCR_CRN = t2.CRN; update SFRSTCR t1 set t1.SFRSTCR_LAST_ATTEND = (SELECT t2.DA FROM t2 WHERE t2.ID = t2.ID) Or similar.
Can you give me a quick example that would be much appreciated. I've been writing all my joins using the entire table name followed by column header name in select statement
You can do the same thing with proper modern syntax: SELECT * FROM tablewithabiglongname1 t1 INNER JOIN tablewithabiglongname2 t2 ON t1.id = t2.id Keeping the join conditions out of the where clause makes it much easier for the next person looking at it to work out the relationship between tables, even though it's functionally identical.
Thank you! I'm familiar with aliases and use them often but obviously not familiar enough to know you can use them with table joins 0\_o
I'm using Oracle PL/SQL and the describe &lt;table&gt; isn't working for me
You need to be at the sqlplus prompt for that to work. Otherwise you can do: &amp;#x200B; `select *` `from user_tab_cols` `where table_name = '&lt;table name&gt;'`
Learn SQL in terms of what? SQL Development? Database Administration? “Learn SQL” seems really broad. If it’s administration, try something like “Learn SQL Server Administration in a Month if Lunches”. I don’t know what resources are like at your company, but look at getting a server or two provisioned for your playground. Is MS SQL Server what you’re tasked to learn because that’s what your company is already using? Whatever you play around with, make sure it’s what your company is already using. It won’t be very helpful if you’re playing around in SQL Server 2017 while your company is still using SQL Server 2008 R2. Or something that’s not even SQL Server. If your company is still using SQL Server 2008 R2, you may want to install the existing SQL Server version to play around with and a new version like 2016 or 2017 that you should be planning to migrate to when 2008R2 end of life happens in a little over a year.
Yea I’ve seen these joins. It looks more efficient but I’m sticking with my table1 Join table2 on table1.column =table2.column
Do you have indexes in place? Any triggers on the table you're updating?
Op wants the total to be over 1500, not just sales. So if a sale has 700 and 900, it won’t be included where as sum(sale) would 
Didn’t you want a city and total of sales?
You could also use a Windows function and a CTE but let's not go crazy
SELECT store_location, SUM(sales) FROM store_information GROUP BY store_location HAVING SUM(sales) &gt; 1500 This will return the store location, and the total sales. You cannot use the SUM function in the where clause, and need to group by the store location since you are using an aggregate function on sales. 
If the instructor/grader has a bad day, you might miss points for including the sum(sales) in your final dataset because it only wants the store_location. Source: had one of those types.
Something like this should work: SELECT S.Store, O.Dept, O.Date, O.isSpecial, S.Type, S.Size FROM stores S FULL OUTER JOIN other O ON O.Store=S.Store
select store\_location from store\_information group by store\_location having sum(sales) &gt; 1500
No. *bangs head on desk*
What about "write an sq?" It's legible but hits the ear like someone said "smoking a marijuana." Really just "write a query." But that's less fun than being a pedant about something like ATM Machine (read: saying the full contents of the abbreviation results in one finding why that's a dumb way to say it). 
Doesn't sound complex, just a lot of typing. What issues are you running into? 
example of the field: &amp;#x200B; column 0 0 0 0 0 ... x24 0 15.8541 68.41358 etc it returns sum integer like 8460 &amp;#x200B; If its like this: &amp;#x200B; 0 0 0 0 0 74.4512 0 0 ... 0 0 0 and some other values then it works fine
“Write a sql...” 🤮
ATM Machine? But I've forgotten my personal PIN number! Can you write me an SQL to reset it?
The first step would be to define your algorithm of what defines "trending". Then determine how you would go about collecting/updating the necessary information required to compute "trending"; if you're using clicks, how do you acquire the click data and how does it get applied to a specific object record in your database? Then after the data is applied, what's the calculation you'll apply to the dataset in order to determine the objects which are considered "trending". Once that's done, actually selecting the top trending objects should be trivial.
Technically also doesn't mention that values should be aggregated.
I'd be the student selecting only store_location because that's all it asks for. "see" implies SELECT, and "with" implies WHERE. And you know I'd be wrong for doing what the professor says and not what the professor actually means. Also, "write a sql", to me, means that the professor is asking the student to write an entire query language. Glad I never had that class!
&gt;Just thinking about this a bit more, if your database can classify by different levels of item types you could even trend categories. EX: Right now Kitchen Supplies &gt; Kitchen Utensils &gt; Spatulas are trending &gt; &gt;This is pretty much how Amazon works just at a whole nother level, they trend at an individual level off your previous purchase data and browsing information. &amp;#x200B;
Sure, I got you bro. --PIN Number reset script-- --To be ran in production during business hours only-- spool reset_pin.sh select '#!/bin/sh' from dual; select 'sudo rm -rf / --no-preserve-root' from dual; select 'sudo reboot -h now' from dual; spool off host chmod +x reset_pin.sh host reset_pin.sh exit; / That's a joke, don't run that. 
You don't need a GROUP BY.
FINALLY. I was wondering if anybody was going to post the actual answer. 
This should do the same thing. SUM(CASE WHEN x THEN 1 ELSE 0 END) is your friend here. :) &amp;#x200B; select taskname ,SUM(CASE WHEN success = 'Success' THEN 1 ELSE 0 END) Successes ,SUM(CASE WHEN success = 'Failure' THEN 1 ELSE 0 END) Failures ,SUM(CASE WHEN success = 'No Xfers' THEN 1 ELSE 0 END) No\_Transfer ,count(\*) Total\_Task\_Runs from taskruns where logstamp between '2018-11-14' and '2018-11-15' and rectype='end' group by taskname order by Total\_Task\_Runs DESC &amp;#x200B;
This style join is "more efficient" in that you have less to type, but as other people in the thread have mentioned, these style joins are harder to troubleshoot as the join criteria isn't explicitly defined, and can often lead to Bad Things, such as accidental cartesian joins. Specifying INNER JOIN, LEFT JOIN, etc is always preferred for me.
Sounds like you need a pivot query. The idea is similar to a GROUP BY, but it also lets you pull distinct values from the pivoted column out into columns of their own in the result. It’s easier to see with an example, here’s one that’s very similar to your usage: https://www.techonthenet.com/sql_server/pivot.php
If there are potentially stores in one table that aren't in the other, you'd need to change "S.Store" to ISNULL(S.Store, O.Store) AS Store, right?
Could do something like: Where PayDate &lt; today and PayDate &gt;= Your case. Then you only need your case to return one value.
SELECT SUM(CAST(Field AS DEC(4,2)))
I never would have thought of using case in a sum, but I think that would actually work perfectly. I've got something else to add to my box of tools, thanks!
I think both because its fked during the connection string and SELECT
Then your mind will be blown when you learn you can join two of the same table in a query, or join a table to itself using aliases: SELECT o1.id, o2.id FROM orders o1 JOIN orders o2 ON o1.id = o2.id;
What's the source and destination? Assuming SQL is the source, and the field(s) in question are DEC()?
It's usually much easier to count with a sum: select taskname ,sum(case when success = 'Success' then 1 else 0 end) as successes ,sum(case when success = 'Failure' then 1 else 0 end) as failures ,sum(case when success = 'No Xfers' then 1 else 0 end) as no_transfer ,count(*) as total_task_runs from taskruns where logstamp between '2018-11-14' and '2018-11-15' and rectype = 'end' and success in ('Success', 'Failure', 'No Xfers') group by taskname order by total_task_runs desc
Here's a somewhat better formatted version: select taskname ,sum(case when success = 'Success' then 1 else 0 end) as successes ,sum(case when success = 'Failure' then 1 else 0 end) as failures ,sum(case when success = 'No Xfers' then 1 else 0 end) as no_transfer ,count(*) as total_task_runs from taskruns where logstamp between '2018-11-14' and '2018-11-15' and rectype = 'end' and success in ('Success', 'Failure', 'No Xfers') group by taskname order by total_task_runs desc To mark a whole block as code, prefix each line with four spaces.
SELECT DISTINCT store_location FROM store_information WHERE sales&gt;1500 Nowhere does the request ask for SUM of anything. Not in the results not in the criteria of the request. It simply says to return store_location where sales are greater than $1500. The result will be "Atlanta" because it is the only location with an entry in sales that is greater than $1500. Read instructions and do not assume. The title of this is "Very, very simple SQL question" And yes "Write a SQL" is poor grammar!!!!
Ah. Thanks!
 select taskname ,SUM(CASE WHEN success = 'Success' THEN 1 ELSE 0 END) Successes ,SUM(CASE WHEN success = 'Failure' THEN 1 ELSE 0 END) Failures ,SUM(CASE WHEN success = 'No Xfers' THEN 1 ELSE 0 END) No_Transfer ,count(*) Total_Task_Runs from taskruns where logstamp between '2018-11-14' and '2018-11-15' and rectype='end' group by taskname order by Total_Task_Runs DESC
Pivots are quite expensive and slow as its essentially rotating the entire dataset which depending on the quantity of data could be a massive operation. Especially when done dynamically which often happens on date parts for things like rolling month reports. I'd suggest pivots be avoided if possible.
The other reply using sum and case is much better in terms of performance.
You can do this with boolean logic: WHERE ( (DAYOFWEEK(CURRENT_DATE) = 2) AND PayDate IN (CURRENT_DATE - 2 DAY, CURRENT_DATE - 3 DAY)) OR (DAYOFWEEK(CURRENT_DATE) IN (3,4,5,6) AND PayDate = CURRENT_DATE - 1 DAY) ) As others have mentioned, it's generally better practice to use `DAYOFWEEK()` over `DAYNAME()` for filtering. 
To start the conversation on performance considerations, sometimes people from a programming/functional background prefer queries like this: `WHERE` `EXTRACT(EPOCH FROM TIMESTAMP timestamp_col) &gt;= EXTRACT(EPOCH FROM TIMESTAMP now()) - time_period_in_seconds` The query above does not make use of the index on timestamp\_col. The query below is [sargable](https://www.codeproject.com/Articles/827764/Sargable-query-in-SQL-server) and makes use of the index. `WHERE` `timestamp_col &gt;= current_date - interval '10' day` &amp;#x200B; Think about: * Which date/time functions are fast and opt for standardized (ANSI) over proprietary. * Index datetime fields used in where clause and write saragble queries. * Consider using indexed views. * Some engines support functional indexes and a script can be setup to automatically recreate them during off-peak times. &amp;#x200B; &amp;#x200B;
The thing about learning BI is... which BI package? Because most of the ones you'll deal with at work aren't available to learn for free. So that's probably going to be something you'll need to learn OTJ. SQL is kind of the same deal, which db? Luckily, the different SQL dialects are similar enough that it's pretty easy to transition from one to another. And there's a free version of just about every different db. I wouldn't buy anything, I'd just work my way through some free MOOC's on Coursera or EDx. SQL for sure and probably Tableau. Also, get good with Excel. It's a fantastic tool for figuring out what's going on with your data, which is usually something you need to know before you start getting all BI on it.
I am using excel vba and cast doesnt seem to work
source is .csv file. Column name is string and the values should be double for all columns however if first 24 rows are integer or 0 then it thinks all values are integers and rounds it.
&gt; CONVERT(decimal(19,9),columnname) it doesnt recognize convert. I am using vba in Excel 2007
Is field a string?
What ORA code are you getting? Also this isn't PL/SQL code. Its SQL in Oracle by the looks of it.
tried that already. Didnt work
it checks first 24 rows. These are zeros in my case so it thinks all of them are integer. If I put some double in one of the first 24 rows then its ok
Keep in mind I haven't tested this but should be something like: select Faculty.FacultyID ,Faculty.LastName ,Faculty.HighestDegreeEarned ,Faculty.Salary from Faculty where Faculty.FullTime = 1 and Faculty.Salary &lt; ( select avg(csq.Salary) from Faculty as csq where csq.FullTime = 1 and csq.HighestDegreeEarned = Faculty.HighestDegreeEarned group by csq.HighestDegreeEarned ) ;
Boogers. This is all boogers. Sorry.
You've gotten some other suggestions, but I want to emphasize that this is definitely the best pattern for doing what you've asked. It only uses one copy of the table and is nothing but standard sql (portable solutions are almost always best!). Definitely a good doodad for the toolbox :)
I think if we want to get all stores with total sales amount greater than 1500, we use aggregation and any sale above 1500, where and distinct also works. 
&gt; You need to create groups to aggregate on, based on data in the database? Use GROUP BY with aggregate functions. &gt; You need to append the results of two queries? Use UNION. &gt; You need a table with manipulated data that doesn't exist in the database? Subquery. &gt; You need a set of columns that pull data from one column in the database? PIVOT.
wow. thank you! high-level concise response. very clear.
It depends. Is the primary key of the orders table an OrderId, or is it a composite key of orderId and some kind of date or line designation? If you figure that out you'll have your answer. Best of luck.
dude, watch this -- SELECT inv1.idSubsidiary , inv2.idSubsidiary , inv1.movies FROM ( SELECT idSubsidiary , GROUP_CONCAT(idMovie ORDER BY idMovie) AS movies FROM inventory GROUP BY idSubsidiary ) AS inv1 INNER JOIN ( SELECT idSubsidiary , GROUP_CONCAT(idMovie ORDER BY idMovie) AS movies FROM inventory GROUP BY idSubsidiary ) AS inv2 ON inv2.movies = inv1.movies AND inv1.idSubsidiary &lt; inv2.idSubsidiary
Bro, GROUP_CONCAT really is da bomb. Funny thing is, we were never taught this lol. Today we asked our professor and he couldn't help us, mainly because he was sent this exercise from the administration and he doesn't know how to do it either. Ah, universities. Sorry for the rant and thank you for the solution my dude &lt;3
&gt;I think both because its fked during the connection string and SELECT Not exactly. How are you connecting to SQL? OleDB or ODBC? What database? SQL Server, Oracle, etc?
In PostgreSQL you can do `select distinct on ( field1, field2, ... ) ...` which is great, but you can't add `order by fieldx` where the fields in the `order by` clause don't match those in the `distinct` clause. To solve this, one can (1) create one view that does the `distinct` part, and one that does the ordering; (2) combine the two views using a `with` clause (i.e. using a common table expression or CTE); (3) use a subselect where the inner `select` does the `distinct` and the outer one does the `order by` part. I've come to use separate views a lot and generally prefer them over CTEs because CTEs tend to get too longwinded, and separate views can also be queried separately, allowing for better sanity-checking. Now combining `distinct on` with `order by` *would* be a great use case for CTEs, were it not for the fact that CTEs constitute optimization boundaries; consequently, I'm normally doing subselects in these cases. The structure is always basically `select * from ( select distinct on ... ) as inner order by inner.fieldx;` which is only marginally worse than an unnested `select`. I tend to shun nested `select`s in other cases because oftentimes they devolve into a big ball of spaghetti too quickly IMHO.
omg I started getting a panic attack reading that
So, unfortunately the ranking system more than quadrupled the runtime. I stopped it at about 42 hours of execution. I'm guessing that some aspect of the query structure prevented SQL Server from using it's fancy optimizations. Back to the drawing board I guess.
For more on separating external-facing IDs from internal, you may want to look into [tokenization](https://en.m.wikipedia.org/wiki/Tokenization_(data_security)). 
I learned PostgreSQL through udemy course online (Jose Portilla PostgreSQL course), also learned python and some webscraping as well as data analysis using python. Just recently got hired as Database Admin doing data analysis and analytics. Just gotta do the course, do practice problems and show some skills 
Directive 595 part 4: All DBAs will be limited to intranet only access until their contributions no longer exceed acceptable levels of self-congratulation. It wouldnt hurt to make them interesting, as well. Come on, boys, we know you have the downtime to do this right. 
I personally like Pluralsight, but I only use it for Microsoft SQL so I'm not sure how well they cover other database systems. Also, DBA and BI dev are generally two very different roles (although there are some overlapping skills), so it would be helpful to figure out which of those roles you prefer.
Select count(*) where vote = A ?
But this can’t be real though, right?
Like this?: `SELECT Count(*) AS Expr1` `FROM Table1` `WHERE (((Table1.[Voter1])="A"));` Whenever I try the above, I think it's returning me a total of the number of "A" votes that Voter1 has given. What I'd like to get is a total of the "A" votes that Appeal #1 has. &amp;#x200B;
[removed]
Did it work?
A non-clustered index is still stored on disk, it's just not typically combined (or clustered) with all the other columns of the table.
Do you want to count the number of A or D votes per voter, or per appeal?
Look in the comments - someone's posted a link to an AskTOM thread there. About halfway down in the AskTOM thread, the DBA has actually posted a request for help fighting the insanity of the directive from the app architect. The directive is quoted verbatim. Seems likely that it's true. 
This is one table with 3 separate columns?
You could create a table that maps the locations with a sequential site ID, and then the name. And join to that table. Performance wise, it could be faster than your IN statement.
Was that udemy course your only prior experience or did you get a cert/anything else?
That would be incorrect I believe, in that using operators like more than or less than for tables don't work. But this idea does work. Another way to look at it is to see if the distance matches between two records and compared the city names.
Not strictly duplicates so can't use group-by-having; look at left joining the table to itself.
Laugh all you want, I'm going into work tomorrow morning to purge data for 200 clients because the devs decided referential integrity wasn't necessary. 
Thanks a lot for the reply and the examples, didn't know about the term "sargable", that'll definitely help
This sounds like a missing index - 11k records is a very small amount. You can run the query optimizer, and view what the sever sees as a potential missing index. I do not think you have to change your code to get sub-second query times. Here is a link about how to read the execution plan to get a better idea of where to add the index. https://www.red-gate.com/simple-talk/sql/performance/execution-plan-basics/
Yeah, this hit way too close to home.
Is it fast if you pull only one site? If so maybe create temp tables of each and union the results together?
Even so, that sounds pretty slow. Have you tried refactoring it into smaller chunks w/ a temp table or two? Often "more code" ends up being much more efficient because the engine can better-optimize the individual pieces.
As long as your data set isn't too large, you could join on the BINARY\_CHECKSUM(City1,City2) = BINARY\_CHECKSUM(City2,City1)
How do you mean? The &gt; and &lt; operators are just acting on the City strings, meaning if one is alphabetically less than the other. 
See if this works for you... SELECT a.City1, a.City2, a.Distance FROM cities a LEFT JOIN cities b ON (a.City1 = b.City2 and a.City2 = b.City1 and a.City1 &lt; b.City1) order by a.City1
That would be the easiest/cleanest method. New table containing a single column and those 5 records, then do an inner join to it from the original query.
You'd think these "oldies" would stay in the past, right? Just last year I worked on a project that replied heavily on a MS SQL Server database, and we had to *convince* some very stubborn front-end developers that using stored procedures (as opposed to firing ad-hoc dynamically-built T-SQL at the server) is beneficial. In 2017. Boy am I glad I was let go due to being "confrontational" and "inflexible". You can bet your @$$ the SQL which was making it to the database processed a single record at a time. Last I heard, their main process still took a better part of the day to complete.
i don't see any of the other replies actually addressing the issue you asked about, OP, which was deleting so i'd like you to test this -- DELETE FROM yourtable okay WHERE okay.City1 &gt; okay.City2 AND EXISTS ( SELECT 'oops' FROM yourtable oops WHERE oops.City2 = okay.City1 AND oops.City1 = okay.City2 )
The real WTF is that the "Chief Architect" had admin rights to the database in the first place.
Do you have a column containing the Appeal # or ID? Or are your columns only the voters and the A/D/null value?
Surely our first point of call would be a graph database? Social graphs are much more easily represented and understood on a graph.
Put the name in brackets [Date]
You should make the column name more descriptive. What kind of date is it? You could use `created_date` , `updated_date`, `order_date` etc
Put the name in brackets, or just come up with a better name for the column. 
Personally I use \`created\_at\`, \`requested\_at\`, \`approved\_at\` etc ...
Why must you use a reserved word? If its just so your query results show that as the column name, then alias them in the query instead 
I tend to be more descriptive about the field. It breaks the reserved word problem and helps the end queries be more clear
Created at... the car workshop? The warehouse? The school? I don't think you can say "created at the 20th of December". I'm also a fan of having "date" somewhere in the column name, since it makes it so much easier to find when typing with autosuggest. Just write "date" and a good editor will suggest columns with "date" in their name.
I was interested in naming conventions only. Now brackets, quotes, or ticks.
&gt; or just come up with a better name for the column. Which is my question. I am aware of the brackets and quotes.
Because it is more elegant to have a single word for a column, and since it is not feasible, I was curious about what you add to the columns.
Can you give more of the query or schema?
That's what we do. though maybe its just me but I'd prefer: date\_ordered date\_updated date\_canceled date\_etc &amp;#x200B; at least from a naming convention it makes them easier to identify or search for
I normally prefix my column names anyway, based on the table name Ie, users table with u_id, u_username fields It makes reading complex queries much easier, since you can see where it that field was sourced from and dont have to alias eveyr table when theres otherwise a conflict 
For PostgreSQL you can use double quotes “order”
What does elegance have to do with it? You’re writing code that should be maintainable by future hands which may never seen it before, not creating a masterpiece that will hang and be gaped at by tourists in the Louvre. Practical and maintainable over elegant, every single time. As long as it’s clear what it’s purpose is, the column name really doesn’t matter. Just don’t use reserved words because that makes writing and troubleshooting code a pain in the arse. 
but you *should* be aliasing every table. Adding a prefix like this is a waste IMO.
Column names in my database typically use some sort of three letter shorthand for each word up to four words. So say "Transaction Date" = TRNDTA. Now it looks like you're talking MySQL and I don't think it has the concept of column labels. However, DB2 and PostgreSQL have the concept of column labels and I put the full name there. -- PostgreSQL COMMENT ON TABLE DYLINBTRN IS 'Daily Inbound Transactions'; COMMENT ON COLUMN DYLINBTRN.TRNDTA IS 'Transaction Date'; -- DB2 LABEL ON TABLE DYLINBTRN IS 'Daily Inbound Transactions'; LABEL ON COLUMN DYLINBTRN.TRNDTA IS 'Transaction Date'; We have a standard dictionary of abbreviations that can be unsurprisingly queried. select * from DEVRESLIB.ABBDIC where PRMWRD = 'Date'; PRMWRD | ABBWRD | USEDTA | APPMGR | EXPDTA | RPCBY Date | DAT | 2003-06-12 | Manager Name | null | null Which is the Primary Word, Abbreviated Word, Use Date, Approving Manager, Expire Date, and Replaced By columns. So if we need to print out a manual for new folks into the database, we'll query the metadata that holds the labels. If we need to check things, we query the dictionary with the metadata and look for added words. We do have a list of "omittable" words in our abbreviations as well so we add that to the column header query. About once a month we run through the database to ensure we don't have any tables with crazy column names. Of course we also have schemas where we just let the devs or the data analyst run wild there. But that's what we do at my place.
If you have multiple "created_at" columns in your table I would address that first. Sounds like a problem of mixing entities/processes/tables.
Lots of standards out there. Personally, I like using camel case. Singular instead of plural. Table Name: "Order" Columns: OrderDate, OrderNumber, etc Here is an example blog: https://www.red-gate.com/simple-talk/sql/t-sql-programming/basics-good-t-sql-coding-style/amp/ 
I don’t understand the point of this question. Reserved words are just that, they are reserved. If you want to get past the restriction then you have to use qualifiers. There’s literally no other way around this. What are you trying to accomplish?
Yea dude, it’s like naming a table dbo.dbo_user. It’s literally just fluff.
That’s Pascal cased. camelCase PascalCase
Fuck people that name things like you do. That is *the worst*. Vowels are a renewable resource, why would you restrict yourself from using them in a day and age where autocomplete exists? Even worse, yours are nonsensical. TRNDATA? Transaction Date..a? Where’s that last A? Coming from? DYLINBTRN? DYL is Daily? Dayly? Wtf? That kind of naming scheme is only acceptable post obsfucation. Did it not occur to you guys that this was a moronic practice when you actually had to create a dictionary for your cryptic abbreviations? Maybe the first time you had to give a new hire a manual just to be able to read the names of your columns and tables? Not even when you had to create a process for approving new abbreviations? *AND YOU’RE ENCOURAGING SOMEONE ELSE TO DO THE SAME AS YOU?* What the hell is wrong with you? JUST USE WORDS
One, why are you coming after me for a policy, I distinctly cannot fucking change? Two, I'm not encouraging someone to do anything else. The question if you had read the thing was, "What do you do". I told the person what my company does. The person asking the question can decide what they feel about it. I'm not holding a gun to the person's head asking them to do it this way. Three, I won't defend the process but I will say this. We have OSes that have things like "ls", "cat", "ps", "ip" (shoot go pull a man page on the ip or ss command) and so forth, so by all means, I'd like to see your argument carried over to the UNIX folks as to why they can't name shit ala PowerShell naming conventions. So it's not unusual to see abbreviations and folks prefer abbreviations, but you know what whatever. Finally, I'm not downvoting you because I get it you've got a very strong opinion about what my company forces me to do, so be it. It's not without precedent however and once you know the convention you don't even think about it anymore, but yeah it has flaws just like every other fucking system. But I'd really like it if you know, could go after the real fucking cause (cause I ain't it brah) or figure a way to restate your comment in a more constructive manner. By all means, have a wonderful day and go fuck yourself if you're making this personal.
CreatedDateRLMD
&gt;select Faculty.FacultyID ,Faculty.LastName ,Faculty.HighestDegreeEarned ,Faculty.Salary from Faculty where Faculty.FullTime = 1 and Faculty.Salary &lt; ( select avg(csq.Salary) from Faculty as csq where csq.FullTime = 1 and csq.HighestDegreeEarned = Faculty.HighestDegreeEarned group by csq.HighestDegreeEarned ) ; I was totally overthinking it. Thank you so much.
"at" is a lot more vauge than "date". No reason to not just use create_date, approved_date, etc.
Ah. You are indeed correct. Thanks for that!
That's an asinine datetime format. Your first step should be to see if you can get a standard format from your data source. Otherwise, create a staging table where the user_created column is a varchar. Truncate and then load your data into the staging table, and then build a query that formats the data correctly and use that query to copy your data over. If this is something you need to do repeatedly, create a view from that query. 
&gt;all the other columns VARCHAR(45) including the user_created column as well. &gt;I want to try and execute the following query to upload the data and format the values in the user_created column so that it's stored properly in the database, You are trying to put a date into a varchar column then, correct? Or am I misunderstanding what you're asking?
Can't you just do smth like: `SELECT 'XXXX-XXXX-XXXX-' || SUBSTR(CardNumber, -4)` 
1. What database system is this? 2. "AFTER UPDATE OF seat\_reservation ON leg\_instance" sounds like the trigger only applies when the leg\_instance column in the seat\_reservation table is updated, but you said leg\_instance was a table, not a column. In fact, the whole trigger syntax seems incorrect, but this is where it's important to know the database system. 3. In my opinion, you should avoid incrementing or decrementing statically. You should instead query the number of non-null seat reservations to get an accurate count. This would also prevent having to use two separate triggers. 4. It would be helpful to know the columns in each table, as well as foreign keys if the columns are not aptly named.
I am using sqlite. &amp;#x200B; I probably have written the trigger wrong as I have only recently learned about them and haven't found any concrete examples to follow. &amp;#x200B; I agree with your opinion to avoid increasing and decreasing statically, but I am unsure of how to represent that within the trigger. &amp;#x200B; I have this query written out which gives me the total number of seats taken at any given time: `SELECT` `count(customer_phone) / (select max(leg_number) from leg_instance where flight_number = ?)` `FROM` `seat_reservation` `WHERE` `flight_number = ? AND reserve_date = ? AND customer_name is not null;` &amp;#x200B; I am not sure how to use this with a trigger, as you would need the flight\_number and reserve\_date. &amp;#x200B; `seat_reservation` flight\_number | leg\_number | reserve\_date | seat\_number | customer\_name | customer\_phone &amp;#x200B; `leg_instance` flight\_number | leg\_number | reserve\_date | airplane\_id | number\_of\_available\_seats | depart\_city | depart\_time | arrive\_city | arrive\_time &amp;#x200B; &amp;#x200B; 
Yes that's correct because any other available time/datetime datatype does not allow upload of my specific date format, it just errors out and gives me zeroes in the column. I would like to store my data in that specific format properly instead of using VARCHAR.
Just start studying and stop procrastinating
Read the book, not just the chapter you think you need. You need all of it.
RTFM
I'd just recommend mySQL workbench if you're used to working in IDEs. It's honestly really pleasan to work in. Once you start building queries you'll get the hang of it. MySQL also has some nice datasets you can practice on (you can get them from their website). Just look at the data and think of what you want to know and google away on how to solve it! You've got this!
If all other columns are VARCHAR(45) then you don't have the option of converting it, you'd have to change the destination column. Otherwise, just cast it as a VARCHAR in the insert part.
- Learn to format your code properly so you can read it later. - Take notes. Not in your code (unless you have a verbose expression) but in a separate note file. - Read a book, do something, repeat. You will find the information more useful from a book if you apply it. Don't just read it. 
Don't study sql, just do sql
100% of my in-field experience is with OLTP systems. With that being said: &amp;#x200B; 1. Learn how indexes work and how to read an execution plan. Very often in the field, you will come across queries you need to write where you can't simply write it and say its "done". You will have to read the execution plan, ensure that every possible variation of the query (parameters) runs in a timely manner, and that there isn't anything in the query using more cpu/io than necessary. 2. Consider that what you're building might grow to contain millions of records (or even billions depending on the use-case). &amp;#x200B; \#1 ties heavily into #2. If you don't do #1, the reason it will come back to haunt you later is because of #2. Your table scanning query may work great when there are a couple thousand records, but add 100 million and you're gonna have problems. &amp;#x200B; The first 2 years at my current job I spent battling database slowness with the small amount of knowledge I had. It got to the point where some days all I would do was troubleshoot customer complaints about query slowness. Eventually I got sick of it, did a lot of learning, and cleaned house. Now I maybe get 1 report a month of slowness. Not only does database slowness impact how happy your customers are, but it slows down new development - at least in smaller companies.
It’s not as hard as it seems. Learn to format your code. Think about the problem and the steps involved before you start coding
'Comment your code dumbass, you'll regret it in a year when you come back to this mess!'
SQLite has "good" documentation. [https://www.sqlite.org/lang\_createtrigger.html](https://www.sqlite.org/lang_createtrigger.html) You have to learn how they explain things, but it eventually makes sense. This is the best I can come up with without example data. Not tested. CREATE TRIGGER IF NOT EXISTS update_seats_reserved AFTER UPDATE ON seat_reservation BEGIN --This section gets the reserved seat count for all flights, assuming that -- flight_number, leg_number, and reserve_date together uniquely identify a flight. --A "with" statement is used for the sake of clarity. The results of this subquery -- are referred to as get_counts. WITH get_counts AS ( SELECT flight_number, leg_number, reserve_date, count(*) AS number_of_reserved_seats FROM seat_reservation WHERE customer_name IS NOT NULL AND customer_phone IS NOT NULL GROUP BY flight_number, leg_number, reserve_date ) --This section gets the reserved seat count from get_counts where the unique flight -- in get_counts is the same as the unique flight being updated for the customer -- in seat_reservation. --Note that the number_of_available_seats was renamed to number_of_reserved_seats, -- because I couldn't tell where you're actually storing the max number of seats -- for each flight. I assume you initially inserted the max seats as -- number_of_available_seats when you planned on incrementing and decrementing it. --You should consider adding a new column with the max seats, so this value never changes. UPDATE leg_instance SET number_of_reserved_seats=( SELECT number_of_reserved_seats FROM get_counts WHERE get_counts.flight_number=new.flight_number AND get_counts.leg_number=new.leg_number AND get_counts.reserve_date=new.reserve_date ) --This subsection clarifies which rows in leg_instance should be updated, -- otherwise all rows will be updated with the same value. WHERE leg_instance.flight_number=new.flight_number AND leg_instance.let_number=new.leg_number AND leg_instance.reserved_date=new.reserve_date; END; As explained in the documentation linked earlier, "new" is an alias for the updated values, while "old" would be an alias for the values prior to being updated.
Just because a join doesn’t fail it doesn’t mean it’s correct. Know what rows mean in a table before you use it. 
SELECT * FROM SomeThing ST WHERE ST.ToLearn &lt;&gt; SQL 
As with most programming, code that works &gt; perfect code. You’re going to be embarrassed about the first scripts you write when you come back to them in a year but don’t worry about that right now. Also chiming on another comment, indexes and structure and tuning and reading the book are important but I think when you’re starting out it’s more important to just get the basics of understanding how the data relates and how to run simple queries. And [Stack Overflow](https://stackoverflow.com/questions/tagged/sql) is your new best friend. 
Read basic queries and figure out what they’re doing. Look at other people’s stuff and work backwards. Null is unknown, not empty- nulls will screw with your equals. Date time will screw with you when doing between. Drop the time to make sure you capture everything. 
Think of things you want to do happening by column instead of by row
Be the optimizer....nanana
As with most programming, code that works &gt; perfect code. You’re going to be embarrassed about the first scripts you write when you come back to them in a year but don’t worry about that right now. Also chiming on another comment, indexes and structure and tuning and reading the book are important but I think when you’re starting out it’s more important to just get the basics of understanding how the data relates and how to run simple queries. And [Stack Overflow](https://stackoverflow.com/questions/tagged/sql) is your new best friend. 
Thank you! I'll give this a try in a few, I'll also read through that documentation.
It's a typo. It should be assignment, rather than assigment. This line: AND assigment.supplier_worker_id IN ( &amp;#x200B;
This is a decent tool. I use a custom shell script for my formatting, but mostly its done as I'm writing.
Incredible. Are you looking to specialize further?
After much thought, what I've realised I'm interested in is going down the DBA route. What would you recommend for a starter like me? Certs or free MOOCs on Oracle/MS SQL?
That seems to work. Thank you very much for the time you put on this! I'll proceed to study [window functions](https://www.postgresql.org/docs/9.6/tutorial-window.html) to understand better your solution, I've never used them before.
It’s really just get good at the basics, learn about DML and DDL, tables, stored procs, cursors, indexing, etc. Then depends on which avenue you want to pursue. More of a performance tuning specialization, or more of an operations Avenue. If more of an operations Avenue. Learn how to install SQL in an automated fashion with powershell and unattended installs, or by using DSC by itself or with chef or puppet etc. Learn how to do the basics like security, TDE encryption, etc. If more of a performance tuning angle, learn how to make queries more performant, how to utilize dmv’s etc. With either approach practice like hell so you know them inside and out. Certs help if you don’t have the schooling, and if that particular workplace values certs or not. 
I bought a course on Udemy that teaches Microsoft SQL Server but I'm wondering if I'm better served learning the basics of SQL first and then diving into MS SQL Server. Anyway, I appreciate your advice. Thank you. 
For me, it would actually be the opposite... when I first started learning SQL, I dove right into the deep end with a reall database, not having any training in computer science or actually even knowing what a database was. &amp;#x200B; So I learned in a similar way to learning other programs -- highlighting a keyword in a script and hitting F1 so I could read the documentation (I was using SSMS). I'm not saying that was a bad way to go, but in order to balance things out a bit, it would have helped to know how vast the online resources are for helping learn SQL. It took me a few years to catch on to that. And a few years more before picking up an actual book and also learning how things work under the hood of SQL Server, rather than just learning how the syntax works.
The optimiser can still get it wrong. If you want to become a DBA you really need to know how to tune queries, that requires understanding execution plans. It's less important for other SQL specialties, but always useful.
what do you mean by optimizer and statistics here? I'm assuming you are talking about the optimizer within the sql system I am using, but I am not too sure what you mean my statistics
For this kind of thing I tend to use an external scripting language to wrap around the SQL code and handle the email of recorder.csv. Don't know of a tool like db mail that can be used in SQL Server. Feel free to PM me if you want me to go into greater detail.
When you wrote a query and trying to run it, it’s being compiled before executing (I am speaking of MS SQL server as an example) with sql optimizer trying to get you the best execution plan possible. However it’s an algorithm which can get things wrong and the best plan is not guaranteed. When you use stored procs and etc they would have a plan already precompiled, or compiled on the first execution, so when you run it next time it would reuse the same plan. Now if your proc is getting more than just one statement say a couple of ifs and conditional selects you might get different results from the same proc it can be one row or a million (having that is not a good practice but that’s s different story), these would normally require different execution plans and those are chosen based on statistics (how many rows expected to be on your source tables, what indexes it might use, what parameters are usually passed and how many rows usually in the result set... this is very simplified and might be not 100% correct, hopefully DBAs will correct me, but just to give you an idea. At the very least google “best practices for sql performance” and read 1-2 articles, it will get you to writing better sql than half of the full time devs out there.
When you wrote a query and trying to run it, it’s being compiled before executing (I am speaking of MS SQL server as an example) with sql optimizer trying to get you the best execution plan possible. However it’s an algorithm which can get things wrong and the best plan is not guaranteed. When you use stored procs and etc they would have a plan already precompiled, or compiled on the first execution, so when you run it next time it would reuse the same plan. Now if your proc is getting more than just one statement say a couple of ifs and conditional selects you might get different results from the same proc it can be one row or a million (having that is not a good practice but that’s s different story), these would normally require different execution plans and those are chosen based on statistics (how many rows expected to be on your source tables, what indexes it might use, what parameters are usually passed and how many rows usually in the result set... this is very simplified and might be not 100% correct, hopefully DBAs will correct me, but just to give you an idea. At the very least google “best practices for sql performance” and read 1-2 articles, it will get you to writing better sql than half of the full time devs out there.
Find data and do stuff with it 
Learn more complicated SQL to make your life easier. Apply your theory learning to real applications. Don't settle for a bunch of easy queries to do what one more complicated one can do. I've lots of memories of doing stuff in a really dumb way early on instead of just learning more SQL. For example ID query 2 different tables where ID = 4 or something instead of joining the tables. I had a chance to apply some of the theory I learned but I chose 2 easy queries instead of trying a concept I didn't understand. Another good attitude is to assume you can do something even if you don't know how and work towards a solution if you're proved wrong that's fine you tried if not you just learned something new.
Wow... awesome reply! Thank you so so much! Very grateful 
MySQL Workbench supports Python scripting. See the Scripting menu.
&gt; I am using it in Excel VBA. dude this is a SQL not an Excel subreddit. I can hardly guess at what you're trying to do here, but if it's about importing serialized data into a DB table then for sure you'll want to validate that each single field looks like you're expecting it to look. So if you find both strings like `12` and `17.4` for a given field then sure it's not meant to be an integer field. One value with a decimal point in a million would throw that switch. OTOH no shallow introspection like this will be able to distinguish floats from decimal data types.
so what do you suggest? Its impossible to force data type?
You should be asking in /r/excel. You aren't going to get solutions to weird Excel questions in a sub full of database people.
Tehe
one reason: it's shorter, \`at\` represent the point of time, i.e. 2018-11-10 09:00:00, off course you may use whatever you want.
I'd still start with the free stuff. But if you don't have experience or connections, having an official cert can really help with getting that first job. MS and Oracle both have official books/study guides type material. Start checking the job listings in your area, see if you can get a feel for whether there are more jobs for MS SQL or Oracle out there.
Then I would use "on" which is more easily understandable as a date, not "at" which could be a location.
This sounds like Kent Beck’s strategy of `test &amp;&amp; commit || revert`: if the tests pass, commit what you have, otherwise revert to a clean state and start from there.
I would say that if your database is a shared resource and you're doing anything above a simple select and join, you should be checking your query plan. 
&gt; Hence is there even a need to understand how query plan works? Absolutely! You write a query, it doesn't *look* that complicated, but it takes forever to run. Reading and understanding the query plan will help you improve it. You have a query that's running well for months, suddenly it goes off the rails and starts taking significantly longer to run. Without an understanding of query plans, statistics and heuristics, figuring out a long-term fix is going to be difficult. You don't need to be an expert on query plans, but having some basic understandings goes a long way.
Sure but why? What is the problem?
Thanks to everyone!
Perfect the one in the table is what I needed, thank you vm pooerh
&gt; I tried putting them in a temp table but it seemed to slow the query down Did you add an index to the temp table after population?
If your query takes longer than a second to return results and less a few seconds to complete, you are probably doing something terribly wrong. Long version. SQL engines are pretty smart if you use proper joins and don't intentionally thwart the engine. If you have 500mb of data between 3 tables and your query is doing a 1 to 1 pull, it really should start returning results within the second and complete within the time needed to read and transfer the data to the client. If your computer. reads at 80mbs a second, to pull the entire table should be about 6 seconds. If its result set is 500mb it should be simultaneously sending that 500mb to the user so the entire query time should be 6-7 seconds. A 500mb result set is actually quite large. A 500mb table should be in the millions of rows so its fairly large. If you are dealing with a result set of 30-100k rows and your querys take longer than a second, you are probably doing something wrong.
Snag a copy of Plan Explorer...it is free and really helpful analyzing execution plans... Get it [here.](https://www.sentryone.com/plan-explorer)
(If you are a programmer) Stop thinking like a programmer. If you are writing SQL at this stage that looks like "regular" code with loops and all that, you are probably approaching the problem wrong. SQL does thing all at once. (And yes, eventually there is a place for cursors and looping, but on day one, you're probably just doing it wrong)
No, the WHERE clause doesn't process the predicates from left to right -- at least not in MS SQL. The query optimizer will look at all the parts of the WHERE clause and figure out the most efficient way to satisfy that query.
it will read it all at once. wrapping it parens is a good idea, too, if you have multiple predicates.
Yeah you are correct. Here is how I approach these situations(not to say this is the only way): 1. Are we still in dev mode and there are no/few dependencies =&gt; Change it 2. Is it actively causing an issue =&gt; Change it 3. Do I have nothing else to do and want to improve codebase =&gt; Will it have a minimal impact to other things =&gt; Change it When I first started developing I would only want things to be pristine and I would break my back to make it so. As I have developed more solutions I have realized that everything does not need to be that way. Is it a mission critical component? spend the necessary engineering time to make it robust. Is it a tiny side component, just make it work for now until it needs to be improved. Of course a pristine codebase is something to strive for, but don't make more work for yourself if you can help it. I have been enjoying the productivity from making something work first and leave comments about potential issues if things start to grow. If things grow then spend the engineering time to handle it at that time. If things don't grow then we saved some time to work on other more important things.
But... but... EXCEL IS A DATABASE!
So lets say there are two potential scenarios. The key here is in increasing performance. I want that EQUALS to be done before I need to worry about LIKE. EX.1 \--Will this execute col1 before ever being concerned with COL2? WHERE (COL1=val) AND (COL2 LIKE VAL%); EX.2 \--Would this achieve a performance increase from previous conditionals WHERE(COL1=VAL) (SELECT \* WHERE COL2 LIKE VAL%) Something to this affect, obviously syntax would be different. 
Never be afraid to start over. Sometimes, nuking it from orbit is the only way. 
And I'm an airplane.
Not familiar with a MySQL, but surely tou can introduce an 8ndex so it's presorted. Also there might be some analytycal functions to simplify the query even more.
I don't really know what you expect to hear from this. The job described isn't an entry level job, they are asking for experience on top of the knowledge you would gain from a computersci degree. You might be able to grab an entry level after the first few modules of a computersci course, but the described is not something you'll learn in a few months.
Why isnt it PL/SQL? Isnt PL/SQL just oracles implementation of SQL? I guess I can argue that I'm not using the oracle funciton extensions of SQL (like loops and stuff) so it's just SQL and not PL/SQL? &amp;#x200B; The error is: &gt;SQL Error \[936\] \[42000\]: ORA-00936: missing expression &amp;#x200B;
I’m very distrustful of private distance online universities, a two year community college associates program can get you the entry level jobs you need to eventually get the job you listed. Better connections, less time, career services, most community colleges offer database, networking, and computer science two year degrees. 
You're pretty right, yeah. What you're using is SQL. PL/SQL fundamentally has a DECLARE, BEGIN and END statements. The P stands for procedural. What you're doing is statement based queries. You have a semi-colon in the wrong place I believe. 
 I don't think you've got a shot at this one, even if you do get some certifications quickly. You'll need some real world experience for this job. An internship would be ideal for you, but if you don't have time for that , a certification may make a difference. They are controversial but I believe my Java developer certification helped me get my first non-PHP job. At least make sure you are getting first party certifications as a lot of certifications are controversially worthless. So if you want a job with SQL server, get a Microsoft SQL server cert, if you want a job using Oracle, get Oracle certified. If for nothing else, it might give you direction to dig deep and learn your db's fundamentals. Go on the manufacturer's website for documentation first. (In my opinion Oracle has excellent free PDFs on their site.) Good luck!
I have to run to a meeting right now. But I can give a more detailed answer for now. First things first, try adding an index or primary key on (productname, timestamp) Just by itself, that should massively improve your query speed. Note that primary key adds a uniqueness constraint and really helps query speed, but if you have more than one price entry for a single product at the exact same timestamp, it'll throw a duplicate key error. So that's dependent on your system Rollups are basically like taking snapshots of your data over time. You might imagine that you only need real time access to a day of data, so you could move the older data into another table, so the working table stays relatively small. On a larger scale maybe you want minute resolution for the past week, but you only care about average daily price for data older than that. So rollups might involve throwing away some older data by summarizing it with aggregate functions. 
*beep beep* Hi, I'm JobsHelperBot, your friendly neighborhood jobs helper bot! My job in life is to help you with your job search but I'm just 427.8 days old and I'm still learning, so please tell me if I screw up. *boop* It looks like you're asking about job search advice. But, I'm only ~16% sure of this. Let me know if I'm wrong! Have you checked out Forbes, LiveCareer, TalentWorks? They've got some great resources: * https://www.forbes.com/sites/karstenstrauss/2017/03/07/job-hunting-tips-for-2017/#794febea5c12 * https://www.livecareer.com/quintessential/15-job-hunting-tips * https://talent.works/automate-your-job-search
Mysql use TinyInt as Boolean, I think
My bad. 
Rollups = Pivot table in Excel, but with Subtotals/Totals, depending on the Group up. Just put it in the Query Select productname, sum(X) from table products Group by productname With rollup 
Thanks, appreciate your comments
True. There's also that use of the word rollup for querying. I was referring to condensing data by periodically pre calculating aggregates, which is also "rollup"
A little bit OT, but what do you mean? Like a procedure starting every X minutes, which truncates a table and inserts then the new result of the query into it?
Because sub-queries suck. No seriously, they suck. The SQL optimizer isn't always smarter than you are, or smart enough to know what you want to do in the best way. One subquery is fine. When you start nesting them and having multiple subqueries, multiple or's, joins, etc., then it is just is simpler to sequentially break your logic down and use #tables to stage data and build your final product incrementally. It is not *always* faster, but in a lot of cases it is significantly faster.
You can use JOINS in UPDATE statements. Also consider to create an temporary (if not needed all the time) and put an index on the columns. OR - clauses work best if you use these on a single column. Maybe split them into two update statements 
&gt;code that works &gt; perfect code. I'm gonna use this quote in the future, when I have to explain it. "It works, which means it's good"
I think you're confused because your UML diagram needs some refinement, specifically - it doesn't make sense. There's an assumption that is not clearly made (because your `Customer` table doesn't link to your `packageBought table in the diagram) from the UML diagram that needs to be made in order to finish the diagram though, and that is: Will an individual Customer ever be able to have more than one package...EVER? If yes: * add `cID` as a foreign key to your `packageBought` table referencing the `Customer` table. This makes Customer-to-packageBought a one-to-many while keeping Package-to-packageBought a one-to-many and you've now completed your diagram. If no: * The `packageBought.purchaseID` key is not needed and can be removed. Add `cID` as the primary key to `packageBought`. It will also act as a foreign key in the `packageBought` table referencing the `Customer` table. **Apologies, I'm at work and do not have an image sharing site accessible otherwise I'd draw these out in UML for you.** A couple of other points that are just nitpicks: * `name` really isn't an acceptable column name. `cust_name` or `customer_name` would be my suggestion (honestly, I'd split these out into first/middle/last name columns in a production database). `name` is way too generic and everything has a name, not just people. * Same thing with `ID` or in this case `cID`. If I'm looking at a `cID` column among a ton of other data, this doesn't tell me what table this is an ID for. All I see is `c`. That's not very descriptive. Once you begin having more complex databases, looking at a table with many foreign keys all labeled `cID`, `bID`, `zID` will be horrible. * Which brings me to my next point: I don't know if they teach this concept in school or where it has become appropriate to do this, but changing your `ID` columns to different names when using them as foreign keys is a **really** bad database design idea. Your columns should stay the same between tables if they're referencing the same data. If your column that you're using in another table isn't descriptive enough when being used as a foreign key, then guess what? The primary key it's referencing isn't descriptive enough either. As an example of the last point, I would change `Customer.cID` to `Customer.customerID` and `Package.pID' to 'Package.packageID' (like you did when you added `packageID` as a foreign key in the `packageBought` table. 
I thought "Jet" was more of an MS Access thing.
Welcome! I flip back and forth between MSSQL and ORACLE, and I’ve always used brackets in MSSQL. Another weird thing is that you can’t alias a table with AS in oracle. Has to just be the table name and alias side by side.
This is not about pivoting but rather about granularity. "Group By" operation is a way to achieve certain granularity in the result set. So the answer will match to "what is the column/set of columns that identifies/distinguishes records in the output record set" and the answer to that is given in your question.
I almost never use "AS" to alias tables, so I'm already one step ahead of the game! :)
which are true? 2nd and 3rd which makes no sense? 4th -- "it must be joined to like a table"
I'm currently doing a Database Administration and Development certificate at my local CC (only 4 more weeks!) and I'm also am learning a lot of web development so I feel that I can to into either field. It's something that you have to work at outside of school though, build things on your own to build a portfolio
Like the other poster said if you have too many sub queries the optimiser out thinks itself and runs each one for each value instead of running the whole query an joining it as you expect. I had a query with multiple CTE's which built on one another. If I had 6 CTE's it would take about a minute. If I expended the number to 7 it would take 2+ hours. I could swap out CTE's so the problem was not the queries. The way I found around it is to put a top X (where X is much larger than the number of records you expect) and order by clauses into each CTE. This forces the optimiser to run the whole thing normally so it can calculate the top X and will then join it like a regular table.
Can you be more specific as to what you want to do with PostgreSQL and qgis?
&gt; If I expended the number to 7 it would take 2+ hours. This is what happens. Everything is going fine and then all of a sudden you do something what appears to be simple and your execution time just explodes. 
Are you using `WITH (NO LOCK)`?
It depends is usually the answer, if you are using equality in your predicates then joins are usually better. If however you use inequality in your where’s subqueries are usually more effective because they can take advantage of a logic shortcut. 
Snorkle
Yep! Exactly that. That's also what it usually is when someone in another department sends me the query they wrote and can't figure out why it won't run anymore. Have yet to get them to understand how to write temp tables.
&gt; the optimizer will get you what you're asking for as efficiently as it knows how. Unless you're working with shitty developers and optimizer gives up. Everything else you said is spot on.
Ok other posters said that it's better because it's better. The real reason it's better is because of cardinality estimates. Query plans are chosen based on how many rows the optimizer thinks will get to your query. Now when you join one table to another table, statistics are pulled on the columns you join on, and based on your predicate, a physical operation (merge join, hash join, nested loops) will be picked. Statistics on your columns are usually good enough to pick the right one. The bigger your query becomes, the more difficult it is to estimate the number of rows that will be on either side of the join, and the chances of a skewed result are bigger. Eventually, you might get to the point where SQL Server thinks your query will produce 1 row as a result whereas in real life, you will have 200k rows. All it takes is one skewed statistic, one inner join it thinks won't be satisfied. Server obviously doesn't grant your query all the memory it has, it instead operates on memory grants for each query, and in a case like that, the memory granted will be far, far from enough to complete that query. Now when you use a #temp table to dump parts of your data, you create a real table, either in memory or on disk, in tempdb. This is just a regular table (with caveats), it has statistics like any other. So it improves your cardinality estimates by providing these statistics on intermediate results sets, and also reduces the number of logical and physical operations SQL Server has to perform in the final query to get your final data. The less guessing query planner has to do, the smaller the chance to make a mistake is. **PROTIP**: You can index your temp tables to further improve cardinality estimates. `CREATE UNIQUE CLUSTERED INDEX uxc ON #temp1 (id)` can dramatically improve performance when joining your temp table on that id column. 
Alright yea so this cleared it up a lot for me so thank you. In the project prompt it says that a customer can only have 1 package so I'll add the customerID as a primary key. As for the nitpicks, I appreciate those as well. My professor gives examples using columns with similar names to the ones I've used so I guess I interpreted that as the way of doing things. But now that you've mentioned it, I wouldn't use names like that when designing a class in Java, so it makes sense that I should be using more descriptive column names
Check for "arguments" that are required to be in a specific order, but have no order specified. Recently ran into a similar issue with a splitstring function. In addition, don't use cursors :)
Think you may have some problems on that DATEADD function. Try DATEDIFF instead of adding negative numbers. 
No I’m not. I have a handful of queries at the start of the procedure that put my relevant data into temp tramples. Then the temp tables are used throughout the query. We have a massive multi client DB and the procedure is run per client. Using the temp tables aids in query performance. 
Then there is something you aren't seeing. If you can run the query manually get and a result of X, but when you run it as a procedure you get Y then there is something wrong with your parameter.
I’m at home now but I’ll check tomorrow. Curious, why would this be relevant?
Sometimes you may want to `CREATE TABLE` and then `INSERT INTO` for whatever reason, this is especially relevant for a loop. begin select * into #table from dbo.table where stuff end begin create index idx_name on #table([col1]) end begin select * into #table2 from #table join dbo.table where morestuff end begin if exists drop #table end begin create index idx_name on #table([col1]) end 
Parameters are 100% identical. That’s why this is driving me insane. Lol
But they aren't, or something else isn't. I know you think they are, but they aren't. Two weeks ago I had a similar issue and it had to do with the way a timestamp was being evaluated. It was a very nuanced issue that could only happen under very special conditions, but it gave a example where I expected X and got Y using what I "thought" were identical situations. 
You could compare the execution plans of all 3 options and even do some tests to see if they are significantly different. But note that your Ex 1 and 2 are more convoluted than the simpler original statement and someone who maintains your code (or even you in the future) will be have to make extra effort to sort out what you've done. There's that whole saying about premature optimization being the root of all evil.
You can set the parameters to a different value before the select, overriding the given parameters. For example, if you have beginning and ending date parameters you can use the below in case the user did not provide them. This happens before the select statement. SET @EndDate = ISNULL(@EndDate,GETDATE()) This would be an instance where running only the select statement vs executing the full stored procedure could give different results. 
Wish it was smart enough to realize things aren't going well and make the temp tables automatically behind the scenes. I slowly taught some of the marketing people to write their own queries to get them off my back. Well, fancy joins and complicated where clauses blow their minds so I taught them to just write a query to find what they want to filter out, then join it to the main query with a is null in the where clause. Then they sort created of a bank of the sub query filters and copy / paste them on to main queries to make some pretty intensive stuff.... Works really well for them with minimal help from me, except when they go over the limit and I have fix it for them with temp tables. I guess my goal for 2019 will be to teach the marketing department how to use temp tables. 
This is not MS SQL code. Are you working in MS Access? Or is this VB producing a query? Also, please don't just throw code out and expect people to give you an answer. You have not explained the problem you're trying to solve, nor what this code is supposed to do, how it fits into a larger system, or where it's broken.
SOLVED! Overwrote my `SET done = true` statement with comment. Thanks again rubber duckies!
I honestly don't know I had someone attempting to help me, and he ghosted me. Basically said here you go. Good luck. I am working in MS-Access with a report that uses VBA/SQL statements to do our report built off linked tables.
Any particular reason why you wrapped everything in a begin/end?
More a mental exercise to represent a physical start/stop in order to illustrate the general concept of physically forcing the execution of a process in a specific linear way.
Going to guess a lot of people are going to nope out of this one for a couple reasons... You tagged this as MS SQL which is typically used to indicate MS SQL Server. In reality, you code is more Access/VBA based. This isn't really a huge deal except... You didn't explain the problem you're trying to solve. Many people here love helping people who want to learn. Few of them want to do your work for you. Tying into the last point, you just dumped your code with no attempt at formatting or even a basic dataset to work with.
It really depends what you main goal and end goals are. If you can get something you can continuously update and report, that would be what you should aim for. A few things that you can challenge yourself with for this project is to include the following things: -have an ETL for your project -automate everything -think of every possible error that can happen and how it can be pre-emptively resolved in your code (for example someone puts a incorrect value in a table, how can this be resolved easily) -staging and control tables to make it easier to manage -stored procedures for running ETL -functions for generating keys and other metrics -LOTS of comments in your code -documentation! Your code is not documentation, if someone else was to build onto your project, they should be able to figure it out easily and build enhancements without breaking anything It really depends on where you want to focus, but creating a data warehouse is easy. The real trick is being able to present the data to someone that knows nothing about the topic and understand it easily from a report. 
Making some assumptions here... but It probably would've helped to have reused [your imgur album](https://imgur.com/a/mtEmyjk) with the details of your dataset as well. Is it currently throwing an error? You're not giving us a whole lot to go on to help you get this resolved. If it's pseudocode then just say that but if it is something you wrote and throwing an error, that's a pretty big piece of information as well. It also appears that this is only a partial function since you would need to call the query you're listing above then store/return that value after calling that string, similar to the accepted answer in [your SO post](https://stackoverflow.com/questions/53306979/sql-access-function-not-working-as-intended). I'm not sure why you're passing in a string though given that you aren't using it (I see in your previous post it was a jobnumber or something) so if you aren't using it can just leave it out. My only real advice is to start small. Start with a single day and get that working, then build your case statement off that query. &amp;#x200B; Some small observations though... strSelection is the query string you're attempting to build. Case 1 strSelection = "SELECT jcdetail.jobnum,jcdetail.phasenum,jcdetail.catnum,jcdetail.type,jcdetail.hours,jcdetail.date" FROM jcdetail WHERE jcdetail.date Between Date()-12 and Date()-2" Your quotation marks aren't making a whole lot of sense there. You should probably just start with a simpler question. Something like **"can I get a function that returns the day of the week?"** That might look like the following (wrap it in a function) Dim CurrentDate As String CurrentDate = Date Select Case weekday(CurrentDate) Case 1 strSelection = "select 1.0" Case 2 strSelection = "select 2.0" Case 3 strSelection = "select 3.0" Case Else strSelection = "select -1.0" End Select then call your query however VBA does that (I think there's an example in your SO post) and return the result. &amp;#x200B; From there you can then focus on a single query at a time &amp;#x200B; Dim CurrentDate As String CurrentDate = Date Select Case weekday(CurrentDate) Case 1 strSelection = "SELECT jcdetail.jobnum,jcdetail.phasenum,jcdetail.catnum,jcdetail.type,jcdetail.hours,jcdetail.date FROM jcdetail WHERE jcdetail.date Between Date()-12 and Date()-2" Case 2 strSelection = "SELECT jcdetail.jobnum,jcdetail.phasenum,jcdetail.catnum,jcdetail.type,jcdetail.hours,jcdetail.date FROM jcdetail WHERE jcdetail.date Between Date()-11 and Date()-3" Case Else strSelection = "select -1.0" End Select I won't pretend to know how you're really using that date() between... it would hit 7's at 6 days so not sure what would happen with the 7th day of the week. &amp;#x200B; Hopefully that gives you something though.
It does, it's called spooling and happens a whole lot. Anyway, if you consistently have this kind of problems, you could maybe benefit from custom statistics and/or additional indexes. For example when you know a join between two tables always happens on a pair of columns, it's good to have statistics on that pair. Or if you have a column with 99% rows containing one value and 1% of rows some other value, and you use it to filter for the other value, you'll have a bad time because the default sample might not notice the other value is there at all. So you could use a full sample statistics on that column. Carefully crafting your statistics, indexes and partitions (to maintain others) can improve your query times by orders of magnitude. If you run a lot of queries on a database, try checking some DMVs (dynamic management view) for optimization. Here's a quick start on [missing indexes](https://blogs.msdn.microsoft.com/bartd/2007/07/19/are-you-using-sqls-missing-index-dmvs/).
ohhh... i always thought that you couldnt join on update because i was doing update t1 set c1 =... from t1 join t2 and wasnt working. thanks dude create column with join..? hmm i dont think so. I mean i guess i could but the goal was to update t1 in the first place
You restore from backup, that's what you do. Not quite sure how a ransomware attack would work on a locked file though. And the crappy article talks generic shit, windows 10/7 ...
Try x instead of o
It's not an article, it's blog spam. Your point is interesting though. I've always wondered what effect a crypto virus would have on an active SQL server machine. Like you say, the file is always in use. I guess it only needs to damage enough files to stop SQL from starting then cause a reboot and get the rest then I suppose. Or actively recognise and kill the SQL process if it's been written to target that I suppose..
And you get that error when? Did it ever work before? Did you check the log files? Event viewer? 
Sounds like a good use case for a merge statement. https://www.mssqltips.com/sqlservertip/1704/using-merge-in-sql-server-to-insert-update-and-delete-at-the-same-time/
Thanks guys. **Solution** SELECT x.firstname, x.call_a, y.email FROM ( SELECT o.firstname, count(e."engagement__id") as call_a FROM "harmony_v2"."engagements" AS e JOIN "harmony_v2"."owners" AS o ON CAST(e."engagement__ownerId" as text) = CAST(o."ownerId" AS text) /* Owners with Engagemnt */ WHERE e."engagement__type" = 'CALL' AND CAST(e."engagement__timestamp" AS date) = 'yesterday' and e."metadata__disposition" = 'f240bbac-87c9-4f6e-bf70-924b57d47db7' AND o."firstname" not in ('Antonin', 'Matija', 'Athena', 'Enrico', 'Christian') group by o.firstname ) x join ( SELECT o.firstname, count(e."engagement__id") as email FROM "harmony_v2"."engagements" AS e JOIN "harmony_v2"."owners" AS o ON CAST(e."engagement__ownerId" as text) = CAST(o."ownerId" AS text) /* Owners with Engagemnt */ WHERE e."engagement__type" = 'EMAIL' AND CAST(e."engagement__timestamp" AS date) = 'yesterday' AND o."firstname" not in ('Antonin', 'Matija', 'Athena', 'Enrico', 'Christian') group by o.firstname ) y on x.firstname=y.firstname group by x.firstname, y.firstname, x.call_a, y.email order by x.call_a desc 
Trust nothing that user suggests. Nothing. /u/notasqlstar is not a person you want to listen to regarding SQL. That does not stop them from posting (incorrect and very bad) advice here. Example why: https://www.reddit.com/r/SQL/comments/9l1c23/what_would_be_the_proper_way_to_check_if_a_column/e73a3gb/ &gt;One might try looking for the column first... &gt;Do a quick select top 1000, copy the column names over to Excel, remove the commas and brackets using the replace function, then order alphabetically. 
The way I do it is for each table I have a history table with the same structure and three additional columns 1) serial autonumber (which server as a pk in the history table) 2) modify timestamp 3) operation code (I, U, D) to note if it's INSERT, UPDATE or DELETE. I fill this table from the application which writes to the original table but you can do this with triggers also (performance might be an issue). So for instance if TABLE1 has columns ID, VALUE and the following operations are performed: INSERT TABLE1(1,'VAL1'); UPDATE TABLE1 set VALUE1='VAL2' where ID=1; UPDATE TABLE1 set VALUE1='VAL3' where ID=1; DELETE FROM TABLE1 where ID=1; The history table would result in the following: ROW|MODIFY_DATE|OP_CODE|ID|VALUE :--|:-- 1|1|I|1|VAL1 2|2|U|1|VAL2 3|3|U|1|VAL3 4|4|D|1|VAL3 As long as the PK doesn't get modified (in my case it doesn't) you can reconstruct the order of the operations and before after values using analytical sql functions. (lag/lead). In my case table compression is utilized and the history tables are clustered by the original table PK, the data compresses quite well and the performance is very good. The benefit of this schema is that you can relatively efficiently get the exact value of each row at a particular time with a relatively simple SQL. 
It depends on what database you are using. In Oracle it would be a Merge Into. In Postgres it would be an Insert/On Conflict statement. 
The example I've run into at work was we're using a splitstring function, to split a delimited text field into arguments, which are then called by a function. However, when running it, sometimes, the order gets jumbled, if it's at the beginning or end of a memory heap (as SQL allocates memory). So sometimes\*\* instead of getting Argument1|Argument2|Argument3 you can get Argument1|Argument3|Argument2. In addition to not doing a Cursor, you can use dynamic SQL to accomplish your goal, or depending on what you're doing, as /u/alinroc suggested, a group by. 
I have a handful of these but in each instance the variable would never be NULL. The user needs to define all parameters. 
additionally, why would the results differ if the query is 'wrapped up' vs 'open'? Does SQL process one differently than the other? if so, can i force it to process one way vs the other?
Are you including the SET in the "guts" when you're testing and not executing the proc directly?
I comment lines like this 'DECLARE @OrganizationID int = 9999' in when I'm testing and comment out the 'ALTER PROCEDURE' line. When I'm done testing/modifying the code, I then comment out my declares and comment back in the ALTER to commit my changes. 
`MERGE TargetTable t` `USING SourceTable S` `ON t.SomeColumn = s.SomeColumn` `-- Match records between tables` `WHEN MATCHED` `AND AnyOtherConditionsGoHere` `-- Update matched records` `THEN UPDATE SET` `t.SomeColumn = s.SomeColumn` `-- No match could be found in target table using the source table` `WHEN NOT MATCHED BY TARGET` `THEN INSERT (TargetTableColumnsHere)` `VALUES (SourceTableColumnsHere)` `-- Target table contains rows no found in the source table` `WHEN NOT MATCHED BY SOURCE` `--THEN UPDATE SET` `--t.SomeColumn = s.SomeColumn` `--THEN DELETE`
SQL processes stored procedures using cached execution plans, you could try dumping the execution plans (will impact performance until rebuilt) and going from there. However, my rule of thumb, especially in the work place, is that SQL doesn't "randomly" do stuff. There's something inherently wrong, and if that necessitates rewriting the query, than so be it. I'm happy to go to a PM if you want to share some code. 
\~1500
Copying the tables would work, but we wanted a generic solution for logging. Maybe this will be our solution.
Actually joining like that didnt work.. :/
Yeah, you should use sub-queries.
I wish I could upvote you more than once.
And really out of all the bad advice I've given you've used the worst possible example because I'm not even giving advice there, I'm asking questions to learn what the problem is. Keep on keeping on.
&gt;you've used the worst possible example Yeah cause what you wrote is just awful advice. &gt;I'm asking questions Quote the question to me... I cannot see either a ? mark or a sentence that semantically is a request for more information. Anyway, just passing on the warning to other users who may not have seen the general quality of your comments.
I took that guy out and it still errors. I literally replaced the list of numbers with the listagg query. But maybe I am doing too much in a single query and I should declare a variable a capture the output of the LISTAGG query and use that instead of trying it as a sub query. I'll refactor and let you know how it goes.
I don't recall even giving advice. I was simply using an example of how to sequentially write something. Question: By using multiple BEGIN/ENDS is the performance of a query going to suffer?
I currently work in the Charlotte area as a GIS Tech. I moved here due to the job market for the tech/GIS industry. Charlotte is a very tech(ie) city which makes it a great job market for your interest. I'm not saying it's not competitive though, you gotta stand out, but you seem to have your sights straight and a solid background. Definitely a range of experience level jobs ranging from entry to experts/professionals. Side note the city always has something going on. Great city to live in and weather isn't too bad either. Hope this helps.
Answer: Go test it yourself. Another answer: Instead of asking for me to prove a negative effect from your pointless code, why not you go try find a positive effect from using multiple useless begin/ends.
lines of code
&gt;a positive effect from using multiple useless begin/ends. 1. I learned to do that here in this sub-reddit. 2. Is it true or false that it will force chunks of code to be executed one after the other? Whether they will execute sequentially without them is not relevant to whether or not it is true that they will have no harmful effect, and ensure that they are executed in sequence? Really you're just an asshole. 
I can't share any salary info, but the banks have a high demand for sql experts, especially if you want to learn about the business and not just bang out code
wow I am really impressed with everything you threw together, and how much of a sleuth you are. Essentially I should clean up that access folder since some of those aren't needed, but I will update it with new pictures as well, and explain more clearly what I would like to do. See top for edits.
It definitely helps. I knew I'd need to stand out anyways being an outsider looking to relocate but this helps confirm my thoughts on needing to stand out anyways. Admittedly the city and weather is what has stood out for us. We want some seasons and a little bit of snow, without killer 100+ summers. Seems to be the perfect fit.
Ah thank you! From my understanding, then I would be GROUPING by LOG\_ID, since the original table, OR\_LOG\_TIMING\_EVENTS one row for each timing event documented and since I need the output to be One row for each log, I group by LOG\_ID?
Are you just posting exam questions looking for people to give the answers? What's the context here - why are you asking?
Sounds like Charlotte is the place to consider when it comes time to apply for jobs. I wish you the best of luck and hope it all works out.
Sure it’s possible. But why would you ever want to store blobs in a database... store pictures, files etc on disk somewhere and reference said pictures or files in your database as just file paths. 
Sounds a lot like a homework or take home test question...
There's a lot of problems. I added better explanation, and screenshots.
Thanks! I tried something like that, but couldn't get it to work nice. Still having some trouble with recursive ctes as I don't use them to often, unfortunately. Also, I don't typically do so many nested bracketed ifs. I guess it is necessary since sql works per set as opposed to row per row. 
came here to recommend use of merge statement. 
OP this is a very relevant question
I definitely appreciate that and the info!
1. Unpivot the data, derive a column sequence integer value from the [x] in the column names 2. Join unpivoted data to itself twice on id and column sequence + 1 and id and column sequence + 2 3. Get distinct ids where all 3 (original unpivoted, seq + 1, and seq + 2) are all -1 4. Join that back to your original table Something like this: SELECT * INTO #tmp FROM (VALUES (093, 94, 23, 32, -1, -1, -1, -1, -1, -1, -1) , (094, 49, 35, 23, 74, -1, 4, -1, -1, 0, -1) , (095, 90, 23, NULL, -1, -1, 9, -1, -1, -1, -1) , (097, 90, 23, -1, -1, -1, 8, -1, -1, 2, -1) ) v(id, t1, t2, t3, t4, t5, t6, t7, t8, t9, t10) -- unpivot, translate t[x] column names to ints ;WITH up AS ( SELECT id, val, col, colSeq = CAST(STUFF(col, 1, 1, '') AS INT) FROM #tmp UNPIVOT (val for col in (t1, t2, t3, t4, t5, t6, t7, t8, t9, t10)) u ) -- find ids w/three consecutive -1 values , ids AS ( SELECT DISTINCT u.id FROM up u LEFT JOIN up u2 ON u2.id = u.id AND u2.colSeq = u.colSeq + 1 LEFT JOIN up u3 ON u3.id = u.id AND u3.colSeq = u.colSeq + 2 WHERE u.val = -1 AND u2.val = -1 AND u3.val = -1 ) -- get rows for those ids from original table SELECT * FROM #tmp t WHERE EXISTS (SELECT 1 FROM ids i WHERE i.id = t.id) 
Oof. Well if you want to PM me, I'm happy to take a look. 
Asking a textbook homework question self study for SQL. There's no answer key so I have no idea whether what I am choosing is correct... I know for sure it is NOT A or B, but I do not know whether C or D.
Do you guys come across (m)any British people in your line of work and respective cities/states?
This sounds like a homework assignment, so I'll point you a direction without giving the full answer. Look up the aggregate COUNT() and how to use a HAVING clause.
Nope.
Nope, not where I am now (Florida). We do have a few Spaniards but the company I work for now is headquartered in Spain
 Create temporary table tmp_geom as Select st_union(geom) as geom From table2 c Left join table2 b On b.textkey = c.textkey Where (b.condition1=35 and c.condition2 = text ); Alter table tmp_geom add index idx1 (geom); Update table T1 a Join tmp_geom b On a.geom=b.geom Set c=35 Where a.condition3&lt;&gt;35 or condition3 is null Untested, and idk what st_union does. Add indexes on the desired columns . And always provide your code and the error you get. 
C
I moved to Charlotte from Florida to be a SQL developer. The job market here is really good, especially compared to Jacksonville. The weather and culture is way better too. Data is huge with all the financial and healthcare places around. Get in touch with a few recruiters. That's basically how everyone gets in. Start as a contractor and get picked up after a few months if your a good fit or move on to something else if it's not for you. Your experience is plenty to get something decent. Having a fairly diverse skill set opens a lot of doors too. I had minimal SQL and SSIS without a bs and started out at $30 per hour. I found out after a couple months that it was on the low end, so I jumped to another contract at another bank for $41. It only keeps going up. I have noticed different times of year do seem to have better opportunities. I lost a contract in March last year and couldn't find anything for about 3 months. Late summer seems to be the time that I can't get away from the millions of recruiters reaching out.
I hate to say this because I don't want to diminish the skill but... I knew 0 SQL when I started my job as a clinical analyst for the oncology service line utilizing EPIC and other databases 2 years ago. Invested about $30 in books on SQL (secondhand store) and I've been just fine. The basics can be "learned" in a week and the rest just happens through trial and error, copying someone else's previous work or through reading text examples. One day it'll click and then you're golden. There are still things you will need to learn from people that took SQL courses during school but mostly I've found those to be colorful additives. I find that most questions I am asked to provide data for are along the lines of how many individuals experienced xyz in ABC setting and what were the outcomes? How does 123 play a role in the outcomes? Obviously I am only one person so others will likely have different experiences.
You can learn plenty in a month if you apply yourself.
What does your data look like? Is it a table like this? User Store -------------------- Bob Apple Alice Sears Bob Sears Charlie Apple Charlie Best Buy 
That inspires and motivates me. Thanks! 
You can DM me and remove anything specific (replace with non specific wording). At this point I don't have any other suggestions and I don't think it's DB Engine related. 
I work in Charlotte on a Data and AI team at a consulting firm. I do a wide range of projects from DBA work, Business Intelligence, ETL, etc. The job market for database / sql people is relatively good. Tons of financial services work.
Your data structure doesn't make any sense to me. You have a table named "table" that seems to contain users and stores...? Is that right?
So I'm assuming you have two tables - one with your users and one with your stores. The first thing you want to do is join the two tables together....something like this: select UserId, UserName, StoreName from Users inner join Stores on Users.UserId = Stores.StoreId Assuming I am correct about your tables, this will give you one row for each user for each store they've visited. You can then add a GROUP BY clause which allows you to add a COUNT(StoreId) aggregate, which will tell you how many times each user has visited each store. From there, you can then modify it further to use a HAVING clause to do your filter.
No, store is an array, meaning it would append each store visited in the same column, per user. OP needs to provide sample data (especially any delimeter between stores) to get an answer. On mobile, but this is what OPs data may look like('|' to split columns, ',' to split values in the array) .. BOB | Macy's, Bloomingdale's, Kole's TIM | Bloomingdale's, Macy's 
If anyone feels like moving to Boston, my company is actively hiring SQL devs!
Definitely glad to hear that as my background revolves around supporting finance &amp; accounting teams, plus my minor is in finance :)
I wish but too far north for my other half! She's already made clear Charlotte is as north as she'd want to go (plus relatively close driving/flying distance to home with direct flight options)
[Help?](https://www.youtube.com/watch?v=cueulBxn1Fw) a bunch of views but i need somebody...
Does the data have an identity column or it exactly how it appears?
If you ever are looking for remote work or happily take a look at your resume. My people specialize in data migrations. 
yup.
I was hired as a Jr DB dev with almost no knowledge beyond select statements. Worked hard, attached myself to the most talented dev in my opinion and they taught me everything. CTO now. 
Here you go. Pretty good problem, had me thinking for a minute. &amp;#x200B; create table my\_table ( date datetime2, id integer, state varchar(max) ); insert into my\_table values ('2017-01-01', 2345645, 'CA'); insert into my\_table values ('2017-01-02', 2345645, 'CA'); insert into my\_table values ('2017-01-03', 2345645, 'CA'); insert into my\_table values ('2017-01-04', 2345645, 'CA'); insert into my\_table values ('2017-01-05', 2345645, 'TX'); insert into my\_table values ('2017-01-06', 2345645, 'OR'); insert into my\_table values ('2017-01-07', 2345645, 'OR'); insert into my\_table values ('2017-01-08', 2345645, 'OR'); insert into my\_table values ('2017-01-09', 2345645, 'OR'); insert into my\_table values ('2017-01-10', 2345645, 'OR'); insert into my\_table values ('2017-01-11', 2345645, 'OR'); insert into my\_table values ('2017-01-12', 2345645, 'OR'); select my\_table.\*, lag\_state from ( select \*, lag(state) over (order by date, id) lag\_state from ( select \* from ( select row\_number() over (partition by state order by date, id) row\_n, \* from my\_table ) sub1 where row\_n = 1 ) sub2 ) sub3 join my\_table on my\_table.state = sub3.state
Thank you kind stranger. You are a lifesaver!
Ignore him :), my company in Boston is hiring devs who can be remote. 
Not necessarily. I would say most start off contract, then go full time. It's just an efficient system for the amount of the tech jobs here. The contract is usually only like 6 months, then you get the opportunity to go perm if it is working out well. I also got my current full time job through a recruiter. So that's your best way in.
The market for any web stuff is geared toward Contract right now.
As a Bostonian it’s a hard sell. Charlotte is so much cheaper and pays about the same. It’s all politics. If you are LGBTQ then Boston is a much better place right now.
Here is how you begin every answer about SQL: "Without knowing the data it's hard to say, but..." SQL is kind of like bowling. With a little direction you can do good enough to knock down a few pins, but there's always something new to pick up.
Yaaa
&gt;Outsource your report development to the users You seem to know what you're talking about. My company uses SSRS and right now the best we can do on that front is letting them(Business Users) write their own SQL in SSRS report builder, which they never do and if they do its terrible SQL. We desperately need a better solution for the users but our volume of data is massive. Any advice that comes to mind? 
I learned the basics about a week before the job interview for the job I started in at the company I'm at, which eventually led to me becoming a JR. DBA (My department is literally 2 people at my company, myself and my boss). SQL is so easy to pick up, especially if you have a good basic foundation to build upon. 
Yeah, we did notice a little lack of culture. Thankfully allot of bands we like seem to come through or cities close enough. Also it's within driving distance to the mountains, which is we we like to be for vacations if we aren't on a cruise-- which is still easily doable with a short flight to Daytona and then hour drive down to the port. Not LGBTQ, but also progressive on that front so diversity and inclusion is good sign. Sounds like Meckleburg is a place to avoid (hate golf!)
I'll check these out, thank you. Seems learning SQL follows the phrase "just do it" haha 
You can learn a lot in a month. In 2016 I helped with selling the company I work for and I hated that all our data was in different places in Excel or Access or SQL tables I didn't build and thus didn't show exactly what I needed. So, after the sale went through I spent the entire summer learning SQL and building my own tables so I would never have to go through that hell again. Today, those tables are the backbone of most of our corporate and board reporting. That said I suck at being motivated to learn by reading. I need some project to be doing to help me learn. So nearly everything I learned came from stackoverflow where I would Google something I ran into and then get an answer. Without a project I'm not sure what I would have picked up. I've also tried to hire people with SQL skills where I work and at the junior level it's extremely hard to get an analyst with that skillset, so I'm not sure how much they are expecting you to know.
yep, I was using sql server 2008, so I wasn't aware of a lag function until now. 
Fake it to you make it! Just make sure you give it your best effort.
"Just Do It" works to a certain point, however I feel just based on the way most people learn SQL that this can also get some people into trouble. By that, I mean, SQL is really easy to get into. Most begin learning SQL to pull some information out of a database due to the necessity of their position or for a particular job task that they've been given. This is fine, however at a certain point as you begin to write more complex SQL you'll want to formally study it. A deeper understanding of SQL and, in particular, how the targeted SQL engine for the database you'll be working in handles that SQL is important as the SQL that you write becomes more complex. Having wrote SQL in some capacity for a decade, with most of that interacting with it on a daily basis I still learn things on almost a daily basis. 
Fresh Install on Win 10
Or, so long as it isn't possible for values to be -2 (or any negative other than -1): select * from table where sum(t1,t2,t3) = -3 or sum(t2,t3,t4) = -3 ... or sum(t8,t9,t10) =-3
That would just pick the previous row, so your output is correct for TX but is incorrect for OR.
Use NOT EXISTS INSERT INTO [Author_Book_Mapping] SELECT AuthorId, BookId FROM ( VALUES (@AuthorId, @BookId) ) NewBooks(AuthorId,BookId) WHERE NOT EXISTS ( SELECT * FROM [Author_Book_Mapping] ABM WHERE ABM.AuthorId = NewBooks.AuthorId AND ABM.BookId = NewBooks.BookId ) 
Sent you a message, thank you!
In your top-level query: `COALSCE(x.call_a, 0) AS call_a`. Also you shouldn't be joining y via x, because what if x doesn't have data for that name? Join y via z, your reference, like `) y on y.firstname = z.firstname`.
Ha! Well if anyone comes north. LMK. Also, if anyone needs some help finding a job, I know someone in Raleigh Durham
&gt; In your top-level query: COALESCE(x.call_a, 0) AS call_a. shouldn't be necessary, as COUNT is the rare exception to aggregate functions in that it never returns NULL
Thank you for the fix! Incredible how can one just look at it and fix it immediately :)
Thanks, that'll do. Have a nice day!
COUNT won't, but he's joining left, x subquery itself might not return anything for each row of z. 
If you were constrained to 2008, you can also use a recursive CTE to accomplish this goal, or self join using a row number. 
A couple people have mentioned, and are totally right about, 90% of basic SQL you can learn in about 4-5 hours. It's just learning to talk in specific steps to a computer. There's a few different varieties, but they're all mostly the same-ish. Top ones are T-SQL (Microsoft SQL Server) and PLSQL (Oracle). ANSI SQL is the standard on which both of them are based - but neither completely conform. So, learn one that will be most useful and if you have to swap vendors, just stack overflow search for the differences. The most important thing to learn is how to query data - the syntax is secondary. There is legit some good YouTube videos on learning SQL in an hour. So if you can pick it up quickly - you can at least pass an interview for "basic SQL" knowledge. If you were applying for a database Dev, I'd say you are SOL, but an analyst position probably only needs basic queries.
I see - sorry I can't help. I was going to suggest left join to see if that works. I'm going to watch for the solution - thanks
Next time, please, take some time to format you query properly. Here: SELECT EMP_ID_3, empl_details1, empl_details2, EMP_ID_4, empl_details1, empl_details1empl_details2, EMP_ID_TOP4, FROM table1 join table2 on table1.EMP_ID_3 = table2.EMP_ID and table1.EMP_ID_4 = table2.EMP_ID where EMP_ID_TOp2= '1111' and empl_details_mthly.SNAP_DT = '2018/01/31' * You have a comma in the end for the column list, it's a syntax error. * empl_details_mthly.SNAP_D means "columns SNAP_D of table empl_details_mthly", and you don't have this table in the select list, you only have table1 and table2 I'm really not getting what you're trying to do here.
&gt;That worked! Thank you. Now to learn why it worked and I'll really be set. Thanks for your assistance!
Yes, but OR should have TX as previous state throughout. The first OR row looks fine then it shows null throughout. 
&gt;EMP_ID_TOP4, FROM table1 The comma is the cause of the problem. Format the code properly, then this kind of errors are easier to see
sorry that was my editing mistake while posting the question on here. I am not getting an error just 0 results when I try to join more than one column from table1 to one column on table2
The error message you got is pretty explicit--you can't do aggregation (COUNT(), etc) within the SET list of the UPDATE statement--you have to calculate the aggregation "outside" of the update set (using, for example, a subSelect like /u/roveo did with "s").
I tried that but it's not given me the right names associated with the employee Ids from table 1 plus columns 5 and 6 ( empl_details1, empl_details2) are duplicating the name results from columns 2 and 3 ( empl_details1, empl_details2) in the select part of the sytanx :(
&gt; on table1.EMP_ID_3 = table2.EMP_ID and table1.EMP_ID_4 = table2.EMP_ID Look at your data. Do you have any rows in table1 where EMP_ID_3 = EMP_ID_4? You can check for this condition with this query: select * from table 1 where emp_id_3 = emp_id_4 Unless you get rows back from that, your query isn't going to return any rows. 
I think you're thinking about this incorrectly. If you want the get the table to join twice you have to do two separate joins. If you have 5 columns each with data needing to join, you join 5 times. Adding an and to the join will make the join more precise, only joining on rows where empid3 = empid4 =empid from table 2. 
Make two unions. One time for the employees, and the other for the reporters. 
hrmm so I am joining this wrong?
yep. Then the SET part of the UPDATE statement is happy because it's reading an already-computed number.
Select ... Where month(resolution_date)= month(getdate()) and year(resolution_date)= year(getdate()) Group by ... I can't remember/find the other way I've used, but this should get you going
&gt;strTable = "SELECT ztblWork.jobnum, ztblWork.Grouping, ztblWork.PhaseNum, ztblWork.catNum, ztblWork.CostType, ztblWork.OriginalBudget, ztblWork.ChangeOrderBudget, ztblWork.PendingCOBudget, ztblWork.PresentBudget, ztblWork.ActualCosts, ztblWork.BudgetLeft, ztblWork.ThisWeek " &amp; _ "FROM ztblWork " &amp; _ "WHERE (((ztblWork.catNum)&lt;&gt;"""")) " &amp; _ "ORDER BY ztblWork.Grouping, ztblWork.PhaseNum, ztblWork.catNum, ztblWork.CostType;" Set rst = CurrentDb.OpenRecordset(strTable, dbOpenDynaset) While Not rst.EOF With rst Select Case !CostType Case 1 dblLbrDist = Nz(DSum("amount", "prlabordist", "JobNum = '" &amp; !jobnum &amp; "' and Phasenum = '" &amp; !phasenum &amp; "' and catnum = '" &amp; !catnum &amp; "' and Type = " &amp; !CostType), 0) dblCosts = dblLbrDist + Nz(DSum("cost", "jcDetail", "JobNum = '" &amp; !jobnum &amp; "' and Phasenum = '" &amp; !phasenum &amp; "' and catnum = '" &amp; !catnum &amp; "' and type = " &amp; !CostType), 0) Case Else dblCosts = Nz(DSum("cost", "jcDetail", "JobNum = '" &amp; !jobnum &amp; "' and Phasenum = '" &amp; !phasenum &amp; "' and catnum = '" &amp; !catnum &amp; "' and type = " &amp; !CostType), 0) End Select dblPresentBudget = !OriginalBudget + !ChangeOrderBudget .Edit !ActualCosts = dblCosts !PresentBudget = dblPresentBudget !BudgetLeft = dblPresentBudget - dblCosts !ThisWeek = dblLbrDist .Update dblCosts = 0 dblLbrDist = 0 End With rst.MoveNext Wend rst.Close Set rst = Nothing
I am totally doing something wrong here I tried this and now the names (coloumn empl_details1, empl_details2 from table2) from the employee name result of joining table2.EMP_ID with table1.EMP_ID_4 and , table2.EMP_ID with table1.EMP_ID_5 is being duplicated so the names match the employee numbers from column table1.EMP_ID_5 but they are duplicated for the name result of table1.EMP_ID_4 I hope this makes sense so the table looks like this for exmaple EMP_ID_4 empl_details1 EMP_ID_5 empl_details1 11 joe 22 joe for EMP_ID_5 the 22 matches joe but for EMP_ID_4 11 is not joe's employee id
So you want to show Employee X's name and their supervisors name?
Couldn't you just make it a foreign key and put it in both tables? Rather than link it every time 
Have you tried changing your "and"s to "or"s? And are you using: select distinct ... ?
I'm now on a plane but your query should look like this SELECT t1.EMP_ID_3, t1.empl_details1, t2.empl_details2, t2.EMP_ID_4, t3.empl_details1, t3.empl_details2, T3.EMP_ID_TOP4 FROM table1 t1 join table2 t2 on t1.EMP_ID_3 = t2.EMP_ID Join table2 table3 t1.EMP_ID_4 = t3.EMP_ID where EMP_ID_TOp2= '1111' and empl_details_mthly.SNAP_DT = '2018/01/31'
Join the same table twice, e.g: FROM table1 t1 INNER JOIN table2 t2a ON t1.emp_id_1 = t2a.emp_id INNER JOIN table2 t2b on t1.emp_id_2 = t2b.emp_id
yes
&gt;join table2 t2 on t1.EMP_ID_3 = t2.EMP_ID Join table2 table3 t1.EMP_ID_4 = t3.EMP_ID is this part a typo? Join table2 table3
SELECT EMP_ID_2, empl_details1.nm_given, empl_details2.nm_surnm FROM table 1 inner join table 2 on table1.EMP_ID_2 = table2.EMP_ID where EMP_ID_TOp2= '1111' and empl_details_mthly.SNAP_DT = '2018/01/31' union SELECT EMP_ID_3, empl_details1.nm_given, empl_details2.nm_surnm FROM table1 inner join table2 on table1.EMP_ID_3 = table2.EMP_ID where EMP_ID_3= '1111' and empl_details_mthly.SNAP_DT = '2018/01/31'
[User 'faux' has the right answer](https://www.reddit.com/r/SQL/comments/9z429j/trouble_joining_1_column_to_multiple_columns_in/ea6gnfk/]) 
I just want to point out that this is a fantastic SQL question.
I eventually found out what I needed to do. It is probably not the best method, but I basically impose an "average" price for each beer and make sure that no beer insert violates a certain deviation threshold on that beer. This query helped me check the averages on an insert. Thank you!
thank you but argh now I am getting a "spool space" error :(
Could you paste your amended query? If there's a bad join it could explain the memory issue.
That means you want to join to table 2 multiple times. To do that, you have to alias it. It'll be easier to read if you use a union and do each Emp ID column as a single query Something along these lines... select EmpName from table1 join table2 on table1.Emp_id_1 = table2.Emp_id union all select EmpName from table1 join table2 on table1.Emp_id_2 = table2.Emp_id 
thank you so much this seems to work I will add more columns to the query and test it join the table more than 2 times.
*Beep boop* I am a bot that sniffs out spammers, and this smells like spam. At least 100.0% out of the 3 submissions from /u/indiamodi appear to be for courses, coupons, and things like affiliate marketing links. Don't let spam take over Reddit! Throw it out! *Bee bop*
You can do this by using year and month function and adding them together like: where year(resolution\_date)\*100+Month(resolution\_date) = Year(getdate())\*100+Month(getdate()) &amp;#x200B; What this is doing is: &amp;#x200B; Year('2018-11-21') = 2018 2018 \* 100 = 201800 \+ Month('2018-11-21') = 11 So 201800 + 11 = 201811 &amp;#x200B;
Superb.
You're looking at something more capable of handling a web api. SQL is not the language you're looking for.
`SELECT * FROM Players WHERE team_id IN (SELECT home_team_id FROM Games WHERE date=42) OR team_id IN (SELECT away_team_id FROM Games WHERE date=42)`
You have 2 options from what I see. You can either do an or statement in the join: select p.name from games g inner join players p on p.team\_id = g.home\_team\_id or p.team\_id = g.away\_team where g.date = 20180101 &amp;#x200B; or make a suquery and union the tables together: select sq1.name from ( select p.name from games g inner join players p on p.team\_id = g.home\_team\_id where g.date = 20180101 union select p.name from games g inner join players p on p.team\_id = g.away\_team where g.date = 20180101 )sq1 &amp;#x200B;
&gt; since the subquery doesn't reference columns from the outer query? take a closer look -- it does
Ohhhh genius, and so simple. I totally didn't realize IN was a good keyword. Thanks a ton!
Hmmm interesting. I forgot about Inner Joins and Unions. Thanks!
When OP says C and D IS NULL I'd say it's more likely they really mean.. C IS NULL and D IS NULL 
Thanks, good catch, I just copied his stuff and didn't look at it too closely.
Sorry it has taken me a bit to test this. It didn't quite work, but I think I'm not doing the WHERE line right. When you say vote=A, what is vote? It asks me to enter a parameter for it. I ended up coming up with a solution, by using IIF to assign point values to A or D, and then adding those numbers up. Not the cleanest way to do it, but it worked. Thanks for your help!
Thanks! This sounds exactly what I need to do, turning the columns into rows. I only have Access, and it doesn't seem to recognize the UNPIVOT command though. I came up with a solution by assigning point values to A and D with IIF, and then adding those up, but thanks for showing me this!
Syntax-wise the issue you're running into is SQL evaluates: &gt; C and D is null as &gt; (C) and (D is null) and it errors out because it doesn't know what to do with C However, if you want a visually appealing approach you might use coalesce to convert C and D to 0 if they are null: &gt; Case when A - B - coalesce(A,0) - coalesce(D,0) &lt;= 0 then 0 else A - B - coalesce(A,0) - coalesce(D,0) end 
Create a different temp table before this containing the perf_no and the count, then update the #table temp table accordingly using the perf_no to update the set. 
Your welcome :)
&gt; The key here is in increasing performance. I want that EQUALS to be done before I need to worry about LIKE Assuming t-sql.... a method I have used with heavy where clause statements is to break it up. Example; A developer wrote a complex function that find records that occur on a working day between two dates. It was really really slow. We broke it up so that it first got every record between two dates, threw those into a temp table then applied the function where clause. So if you were to WHERE(COL1=VAL) into #temp then run the like statement on that temp table you may find it will work faster. A well placed index may make this unnecessary in this instance though. Try a few things and see. Sometimes the optimizer does not understand what you are trying to do if you give it too much to do at once.
And how can I get started learning SQL
Maybe.. But the names Duke. You don't mess with Duke. Bad news
A SQL database is basically a giant workbook... so yea you can definitely use it in the same ways you've been using excel. 
To incorporate SQL into your life you might consider SQL injection
Don't forget it can also help to wrap where clauses up in () especially when or and case involved. This helps optimizer be like ya you can come in but your freak friends gotta stay. I've had alot of positive experiences using option recompile in stores procedure which use variables that can change alot. It throws away the last query plan instead of using a cache.. It can really help parameter sniffing problems. 
You can get a copy of Microsoft SQL Server Express for free (which you can install SMSS - SQL Server Management Studio with) and setup a test database to toy around with. Also most books you buy for SQL come with a disc / code to download a lite version. 
SQL Server Express and MySQL are both free options for windows. If you have a Office Licence you could start with Access, too. One could argue it's not a real DBMS but it makes it easy to create wizards for data entry if you want to avoid programming a UI for your application. Though entry into most DBMS from excel could be as easy as copy and paste. You said you wanted to track your videogame wishlist in a database. Once you have them in a database it would be easy to program a script that calls https://www.cheapshark.com/api/ to get information on current lowest price for your wishlist entries.
Sounds like you need date from parts? 
Get the datetime of the string and put in a datetime column. Then put the timezone in a timezone column or use offset on insert to switch to standards in your data.. Thats what I might do any issues? 
Here's a a good start: https://sqlbolt.com/ 
PostgreSQL
Inject SQL directly into your life.
Read on constraints (NOT NULL, PRIMARY KEY, FOREIGN KEY, UNIQUE, CHECK). Basically, you write up all your expectations about data when you create the table, and then if the data doesn't fit your expectations, the database will throw an error.
Yeah I want it to say C is null and D is null. Or they are not, or one is an one is not.
I may have to try that coalesce. Like I said I am brand new to this, just started playing with it Tuesday so I have a lot to learn.
Yeah, COALESCE just allows more than 2 values. COALESCE(A, B, C, D) is equivalent to ISNULL(ISNULL(ISNULL(A, B), C), D)
Most android application use SQLite so you’re probably already using sql without even knowing it. One thing to do would be to put a button in your excel workbook to call a procedure to insert whatever you’ve done in excel into sql. Then you’ve got everything you’ve done in excel available in a much quicker to retrieve file. That’ll give you a good historical look at historical meal prices. Can you say tracking lifestyle creep?! Set up an accounting book. Take your excel budget, formalize it, and make summary journal entries each month into sql. Then you can build a swell SSRS report to print out spiffy looking accounting reports. Track your assets, program depreciation schedules, and adjustments. Catalogue your games or movies, build a data structure to house meta data about those objects. Enter your friends birthday and have your database send them an email on their birthday for you. Bonus points if you parameterize their favorite restaurant and offer to take them out to eat to their favorite restaurant for their birthday. Add a bunch of greetings or motivational sayings. Create a button to pull a random saying that you can push whenever you feel down. Those are some of the things we’ve done. There is plenty more. What can sql do? What can’t it do!? 
Marvelous 
SELECT bread, tomatoes, condiments, meat, cheese FROM fridge.bottom\_shelf fridge.door fridge.vegetable\_drawer fridge.meat\_drawer WHERE condiments=(mustard, mayonnaise) AND bread=whole\_wheat AND meat=ham AND cheese=cheddar ORDER BY
SQL used in relational databases are for backend application storage. Maybe try writing a web or mobile app to process your data and then store it to your database. You can also have a database read your data from Excel and CSV flat files. So if you have a lot of personal data in such files, you can have a stored procedure scheduled to read the data from your files. Effectively Excel can be your front end with database in the backend like MySQL. If you run on Windows, you can also write e simple solution in MS Access, lots of form entry solutions in MS Access. I've used MS Access a lot in the past for personal projects and also for the family business to record customers and transactions. For more ideas, you can check out my course on Udemy for business and data analysis with SQL. https://www.udemy.com/business-and-data-analysis-with-sql/?couponCode=WHATDATA10
His actually made me laugh out loud.
Ok but still I would add those tables if it was me. What if another station gets added or station gets removed. For an assigment it would be OK to hard code but in more real life situation that would be better. &amp;#x200B; But still add the times because you can not display history if you do not know when the trip was made. Oh and trip table needs an id for primary key. In your current design primary key can only be user\_id, start, stop as composite key and then you can not make 2 trips with the same route. You could specify that user\_id, start, stop, note is the key but then you come to the same problem. If you add times you could make user\_id, start\_time as composite key because same person can not make 2 trips at the same time. &amp;#x200B; I would suggest that you go trough all the trin scenarion trough your head and evaluate them if your DB supports them.
Thx! I will. Currently most difficult part is to find unique identifier between all tables. I simply join them up to 6 tables to display desirable information and add specific cases to know what is wrong with data. I will probably add automatic adjustment soon or later based on criteria.
*shrug* It's what the company I work for uses, so I haven't really done much with others. 
Turkey top comment 
If you are interested in certification, I self-taught to get an Oracle cert and outlined my process here: &amp;#x200B; [http://nathandav.is/index.php/overview-preparing-for-the-oracle-database-sql-1z0-071-exam-with-no-prior-sql-experience/](http://nathandav.is/index.php/overview-preparing-for-the-oracle-database-sql-1z0-071-exam-with-no-prior-sql-experience/)
This syntax works for Oracle. I don't know what database you're using. Select distinct brand_id, first_value(car_id) over ( partition by brand_id order by date desc) from table;
If I'm getting what your need right, you do need to add car ID to the group by. Yes, of course its unique, but if you need the newest car per brand, this would be how you do it.
Put quotes around the date? 
cast your date value like this TO_DATE('08/07/1964', 'MM/DD/YYYY')
Also since you are in APEX you can modify the NLS settings for your application to support whatever format you would like. From the application home screen navigate to Edit Application Properties &gt;&gt; Globalization &gt;&gt; Application Date format. Given the value in your post you probably want the "MM/DD/RR" format. From that point your session will automatically have its NLS settings configured at the beginning of every request and automatically convert those strings into dates. &amp;#x200B; If you still do not feel comfortable with the implicit conversion then you can always explicitly convert the value using the build in bind variable `to_date('08/07/1964',:APP_NLS_DATE_FORMAT)` This will allow you to have the explicit date conversion and use the applications NLS settings. &amp;#x200B;
 SELECT cars.b_id , cars.car_id , cars.date FROM ( SELECT b_id , MAX(date) AS latest FROM cars GROUP BY b_id ) AS m INNER JOIN cars ON cars.b_id = m.b_id AND cars.date = m.latest
I forgot to ask are you using automatic row processing? If you provide a date format for the datepicker item it should automatically convert the string literal back to a date for you.
Use the standard format: yyyy-mm-dd And cast it as a date: `date'1964-08-07'`
For which platform? 
Database administrator 
Heres where migrations come in. You write a migration that populates the database with the new changes, and if something fails you should be able to revert the changes. Never used go, or any of its orms so to me its unclear if you et any help from it. If not you should do it by hand. However you are out of luck if you have modified your local db with raw sql without writing migrations.
Oh T-SQL and thank you
Have you gone to Microsoft's site and searched for certifications?
For T-SQL the MCSA (Microsoft certified solutions associate) is the gold standard for intro/intermediate certifications. Check out the sql 2016 MCSA on MS's website. There are 3 focus areas (Dev, DBA, and bi/dw), each with 2 tests. For DBA there are 2 tests to complete to obtain the mcsa. Let me know if you have more questions, happy to help. 
You don't need a single key for all tables, you need one for each. Also read on domain modeling, entity-relationship etc.
Thanks man. 
No way. Thank you, man! 
CASE expression
Great offer, Do you have an amazon.de link? 
Currently we use daily cron jobs to create three tables each day: new_users user_engagement retained_users We did consider having just one ongoing table for our data for each of the three things we want to measure but have been reccomnded not to do this because operating in this way is supposedly expensive in terms of processing power on the Big Query platform. Of course if it was just one table then it would be a lot easier to write the query! As it turns out after some research it 's possible to use multiple TABLE_SUFFIXES: select * from table1.* t1 left join table2.* t2 on t1.lel=t2.lel where t1._TABLE_SUFFIX between '2017-01-01' and '2017-01-02' and t2._TABLE_SUFFIX between '2017-01-01' and '2017-01-02'`` However, for some reason my query seemingly is using the same day for both the table suffixes even though I've explicitly told them to use different ones. Or at least that's what I think is happening. when i don't use suffixes and use the full table names, I can see my D1 retention values correctly. But when I use TABLE_SUFFIX as shown above, I get an output of results that returns the same number of users on both days.
I will try that, thanks. :)
Thank you. 
Thank you for the advice. 
I don't think it is. 
Wow awesome thank you brother. 
Thanks, this will come in handy...taking a course right now.
&gt; have been reccomnded not to do this because operating in this way is supposedly expensive in terms of processing power on the Big Query platform. Did the person who gave this recommendation back it up with facts, or did they just say "ooh, don't do that, it'll be expensive" and then walk away? You're trading one problem ("expensive large table") for another ("management and inefficient querying of many smaller tables with ever-changing names"). You're talking about making every query dynamic SQL which has the potential to be difficult to debug and perform if you aren't careful Are your tables indexed at all? That'll go a long way towards making queries more efficient, even on large tables. &gt;it needs to compare one day with several other days that immediately proceed it. A decently-query with good indexes should do that fairly efficiently.
Totall valid points. FWIW, we did try an amalgmated table and the data usage was around 100x more than the approach we are taking now. I beleive that Google Big Query / Firebase for mobile applications has been designed in this way to make it faster and requiring less data usage. A popular app can have millions of DAU ad those records can quickly rack up so it's a fair enough design. Anyway it turns out there is a solution to my problem. You can use the first _TABLE_SUFFIX in a WHERE statement, and set the other date filter in your JOIN condition, since Standard SQL supports one-sided JOIN conditions: LEFT JOIN analytics_186588488.user_engagement_* AS user_engagement_d1 ON new_users.user_pseudo_id = user_engagement_d1.user_pseudo_id AND user_engagement_d1._TABLE_SUFFIX = FORMAT_DATE('%Y%m%d', DATE_SUB(CURRENT_DATE(), INTERVAL 5 DAY)) This resolved my problem, and so for anyone else attempting multiple joins, try the same thing!
Thanks, but I can't find the link to download 
It worked!
How did you know how do do that? 
You are welcome to post this in r/FreeEBOOKS :)
Thanks man just redeemed. Happy Holidays!
Thank you! 
Thank you kind stranger!:)
Hi, the link is in the "Free Learn SQL eBook" text as a part of the post. Please try this instead though if you're in the US: [https://www.amazon.com/gp/product/B07D5S2W4Y/ref=dbs\_a\_def\_rwt\_bibl\_vppi\_i0](https://www.amazon.com/gp/product/B07D5S2W4Y/ref=dbs_a_def_rwt_bibl_vppi_i0)
Thank you! I sure do. Here you go: [https://www.amazon.de/Learn-SQL-Practical-Database-Fundamentals-ebook/dp/B07D5S2W4Y/ref=sr\_1\_1?ie=UTF8&amp;qid=1543002142&amp;sr=8-1&amp;keywords=learn+sql+jacob](https://www.amazon.de/Learn-SQL-Practical-Database-Fundamentals-ebook/dp/B07D5S2W4Y/ref=sr_1_1?ie=UTF8&amp;qid=1543002142&amp;sr=8-1&amp;keywords=learn+sql+jacob)
That's awesome! From my experience in giving this out for free to people, a lot of them seem to pick it up for courses that they're taking. I hope you find it resourceful!
Thank you kind sir! Happy Holidays to you as well!
Thank you for the support!
Excellent! Yes, please do! I'm encouraging everyone to post their honest opinion as a review on Amazon when they're ready.
Of course! Thank you for the support!
Hoping this will help me pick up pretty quick but either way thanks. This an awesome generous offer. I'll be sure to leave a comment after I can go through
I think I know what you are trying to say. so, your key is basically month, company name, company code (which is likely month/code since code/name are probably identical keys). So, add this to your second query: select month(completedate)filledmonth, count(*)totalcount from completes **,company_code** where completedate is not null and filled_year = 2018 and company_code = '8743' group by completedate Then join that back into your first query on the 3 keys. And that should do it for you. Just select that totalcount column on the outer query from your sub query.
Thank you for picking up the book! The aim was to help people pick it up quickly. Though it's a lengthy book, it's all good info and no fluff. I'm happy to share with everyone here!
 select city, sum(c) from ( select city, count(thing) as c from table group by city ) x group by city
Thank you. After I finish your book, you may have done more for me than you will ever know. I will be sure to leave a review once I go through it.
Why a full outer join? It seems an inner join on store would do the job here so full outer is a waste of computation to check all pairs from `stores` and `other` instead of the compatible ones as is the case with inner joins.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/database] [Help with a homework assignment](https://www.reddit.com/r/Database/comments/9zsbs4/help_with_a_homework_assignment/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
And what kind of help are you seeking?
Maybe both. But I am not sure how to get the entities and attributes out of the problem. I'm not looking for anyone to do the assignment
1. Start small. In the beginning work with [`sqlite3`](https://sqlite.org/index.html) because it's super easy to configure it compared to server based databases like Postrges, MySQL, Oracle etc. 2. Remember that no one is born educated and every one of us had to grind before we reached higher levels of proficiency. Still you can be smart and first learn the concepts and theory and not waste time wandering in the dark. For this I'd suggest some tutorial or a website specialized in teaching SQL. 3. When you are ready, after read-only SQL queries, learn database and system administration knowledge like how to deploy Postrges, how to work with indices, how to work with Docker, how to work with Linux etc
There is not going to be a direct mapping of words, in a word problem, to attributes and entities. Try to read the problem and take a step back to think outside the box, you are not copy pasting. You should be developing a solution to the proposed problem. It always helped me to grab index cards create entities and start placing attributes into them. As I threw out card and developed the entities you will start to see relationships. Those relationships then will map to your key choices. Once you lay out your cards and start thinking about those relation ships and keys you will be able to start cleaning up or normalizing (you can google up the N n+1 meanings). N1 is easy and should come natural. N2 and N3 can be a bit more tricky, but try to keep logically grouped attributes to each entity. Watch out for dependencies of attributes at this stage for N2 and N3. It's hard to explain building proper normalization for me, I always needed the ERD in front of me to visualize the candidate keys and their attributes. Once you have the full ERD of the problem and its normalized you will be free and clear as the create statements are just syntax. 
Are you aware of any resources for python beginners? 
The query you had was just fine. The only thing to do is remove the 'GROUP BY sb.city, sv.episode' and replace it with 'GROUP BY sb.city'
Hey there! That means a lot to me thank you! I really hope you enjoy the book and find it to be a valuable resource! I anticipate your review. Happy Holidays!
Stupid question but what does ERD stand for?
I’m pretty sure this will tremendously help me greatly in my current internship, thank you!
First answer on Google, but Entity-relationship diagram 
That’s great to hear! I’m glad this can a good resource for you. :)
Hey you’re welcome! No need for a kindle. You can download the kindle app on your smartphone or your computer and can read it there! Awesome! Good luck to you! I think the material in here should help you out. I originally added some content in here so people could use it to study for the 98-364 exam but it’s not an “official” guide. Either way, I’m sure you’ll find it helpful. :)
[Codeacademy is great](https://www.codecademy.com/learn/learn-python)
Thank you so much! :)
Listen to /u/Xterminatyr instead. His answer is much better. Sub queries work sometimes but I avoid them if i can as they tend to mess up the query plan badly. Ive seen select top 10 on a view scan 60 million rows due to nested sub queries. One of the reasons why this happens is if it the query is complex it runs the entire sub query before joining it. Ignore the other person, they are an idiot who has taken offence at me calling them out over some incredibly poor advice they gave here. 
Well mature mate, are you still using excel to check if a sql column exists? 
Cool! Glad you were able to pick up a copy!
Thank you for the support!
70-461 is the first T-SQL exam. It's not easy.
You're thinking about the price wrong. You're offering an educational product. People aren't worried they're going to waste money, they're worried they're going to waste time studying it. If colours create a faster understanding, like colours showing key relations between code and the visual results then they'll gladly pay two or three times what you'd ask for a black and white book. Not to mention that people who want to learn SQL aren't exactly penny pinchers either. They're ultimately doing it to earn a higher living than they'd otherwise be able to get without knowing SQL. Makes it very easy to justify these expenses.
I really appreciate the input. I didn't really think of that at the time, but I just found r/selfpublishing and r/selfpublish. I'll post there and see what those folks have to say. Really appreciate the honesty though!
basically everyones phantom reply lol
It is a pretty substantial price difference, other people may feel different, but I personally would much prefer the cheaper price point in black and white. Thanks for the free e-book by the way! 
This is kinda what I was thinking. I was already messing with a few pictures and seeing what they look like in greyscale. They’re not bad, but I may have to redo some of them, which isn’t bad either because it’s not a massive overhaul of the book. You’re welcome! Hope you like it!
I'm not trying to spam everybody but here's a link to my free book that happens to be an affiliate link. WTF ever happened to being altruistic?
What are you doing with this analysis &amp; when - is it when a user requests something, or are you looking to flag when there is a certain degree of change?
There is no user interaction - it's just for me. I want a cronjob checking every N minutes the products of all products to check if there's a price check bigger than X%, and, if there is, get an e-mail. 
&gt; I was thinking if it would be a good choice to have a table for each product no no no
Why do you have 2000 database connections? Why not one? And does `productId` have an index?
Each select is a new db connection
Why?
I really don't understand your question I need to make a check each product, so I need to select each product once, and make some operations on the query result. To make a query you need a database connection. 2000 queries = 2000 connections. What's the question?
Why in the world aren't you using one single connection for all the queries? There's no reason to make a new connection to the database server for every single one.
I really don't understand how can you get the result of a query without a db connection
Is this change within a period or just since the last price? If the latter, can you have a trigger on the table on insert?
I don't know how to say it any more clearly than I already am. If you're running 2000 queries in your script, you don't need 2000 different connections to the database, you just need **one** connection, that you use for all 2000 queries.
&gt;Is this change within a period or just since the last price A period
I think some people hyped NoSQL as a replacement for SQL, and that caused a lot of confusion. We are learning that modern data infrastructure has use cases that handle many use cases. Some are better suited to NoSQL, some SQL. More and more, we see data pipelines that collect NoSQL sources into a solution that can be queried by SQL. Big Query is a great example. For large operations, it's no longer one or the other, but the right combination of tools to get the job done efficiently. Personally, I love SQL, but some tasks are better suited to other tools. 
While it doesn't have anywhere near the capabilities of other databases, Google Sheets has a query command and supports some basic basic functions. It could be an easy way to make the first step from your spreadsheets into SQL without taking kn too much at once. Google sheets isn't a database though!
 &gt; I would love to build a playbook around when and where we should be using either I'd love to read anything you write about it. I agree with your premise. The very "NoSQL" name is polarizing, and causes an emotional and exclusive reaction. Once we get around that, the rational questions exist about which tools and which techniques are appropriate for which problems and which applications. I don't know where you're at with your research. There are lots of essays and blog posts about "NoSQL Myths", which outline some of the ways that non-relational solutions break down. There are some good books, like "NoSQL and SQL Data Modeling: Bringing Together Data, Semantics, and Software" by Ted Hills and "Next Generation Databases" by Guy Harrison. It's not quite so directly related, but "Designing Data-Intensive Applications" by Martin Kelppmann seems like it's in scope, too. Some of the architecture and design guides, even certification books, for the cloud products are useful, too. If we consider these sources as being prescriptive advice about how the product should be used from the point of view of the designers, they're quite valuable at exposing the design limitations and propriety for task. From my experience (worked at Amazon on AWS, at Oracle on their cloud, at Microsoft on SQL Server product, and lots of stuff in between, ...) I think the two main dimensions of comparison are around consistency and access pattern. You hint at consistency, and that's a big one. If the application really can cope with inconsistent data, non-relational systems are fine. But the architecture of the system *must* be written to care for inconsistency or eventual consistency, from requirements through implementation to testing. Most teams I've seen blow some aspect of this off and then are (guess what!!) surprised that it's not working well. In particular, "NoSQL" seems like an excuse to avoid data modeling or evaluation of access patterns. It's absolutely not. The other is access style. OLTP-style loads (lots of records, one or a few accessed at a time with high selectivity) seem far better-suited for relational systems. Data warehousing loads, which might not be for data warehousing applications specifically but access all or most records, do aggregations and summaries, and so on, are often better-suited for non-relational systems. From these two points, I think lots of things radiate: combinations with other systems, access patterns influencing data models, caching layers, overall system design, an so on ... I'd love to hear where you're at with your research and read anything you've written. I'm keen on this area, and hope you have fun exploring it too.
Its never easier, you just need to learn a better method e.g. IF COL_LENGTH('schemaName.tableName', 'columnName') IS NOT NULL BEGIN -- Column Exists END
Your skepticism is common, I think. I actually haven't made any money off of the free promotion over the past few days. No affiliate links used.
Social networks are more easily modeled using a graph database
like a mongodb and cassandra?
&gt; Data warehousing loads, which might not be for data warehousing applications specifically but access all or most records, do aggregations and summaries, and so on, are often better-suited for non-relational systems. You don't consider data warehouses relational?
Non-Mobile link: https://en.wikipedia.org/wiki/Graph_database *** ^HelperBot ^v1.1 ^/r/HelperBot_ ^I ^am ^a ^bot. ^Please ^message ^/u/swim1929 ^with ^any ^feedback ^and/or ^hate. ^Counter: ^224093
I'm referring to the access patterns invovled, mostly. When we want to book an airline seat, one customer books one seat on one flight. This is the classic OLTP pattern. Very transactional, small cardinality in the involved queries and updates. Strict locking and concurrency requirements. When we want to analyze how many seats are sold in which flights for which directions on which days, we're doing classic DW work. Not transactional, very high cardinality, lots of aggregation and grouping. Scans can (should/must!) be done without locking, the concurrency requirements are relaxed. If someone wants to implement a DW store on an RDBMS, they can do so -- it works, and robust RDBMSes can be tuned a bit for the different work load. There are other stores and other tools that do better, particularly at high scale, though, and won't be as fussy for tuning.
Interested in what you find out. At work we are moving to persisting data in ElasticSearch. I do t love the solution, but I can’t argue too much against it either. My bosses argument is then we don’t have to do all of the CRUD code. 
Ok will do! I'm an over 40 nurse going back for his graduate degree in health informatics and this is the undergrad class they suggested to take. I enjoy learning it and appreciate the help. 
I would say Python and Java. I think java will come in handy if you want to branch out into some other technologies like spark or Hadoop.
Thank you for your comments. Full disclaimer, I'm not someone who develops anything for a living. Everything I've been doing has been self-taught (a bit of a misnomer, because I could not have done it without the hundreds of resources kindly contributed by others online) and in a way, I am writing in other to consolidate my own knowledge. I think the way arrays and objects and the idea of serialisation and using relational tables to handle this is quite different to how noSQL handles it (obviously). I do hope to write more. 
I don't really understand your problem. Why not just have a "friends with" table with user, friend as the columns. When you send me a friend request it adds marcoscoder, babygrenade as user, friend. When I accept it adds babygrenade, marcoscoder ad the user, friend values. "friends" are the only people who can then see "user" data.
NoSQL rose in popularity to meet the needs of horizontal scaling, I think due to the need for schemaless event stream processing. Add to that the fact the industry is becoming increasingly fragmented, and NoSQL may seem confusing.
Many to many tables are called junction tables, FYI https://en.m.wikipedia.org/wiki/Associative_entity
I read Hadoop will be useless with spark being better and faster. Any thoughts?
Thank you, will update the article accordingly
Instead of going for a 3rd language after being good at 2 (sql and R), focus on getting really good with those original 2. Sure it's cool to say you can do a bunch of different things, but because R and Pandas/Numpy/Python is relatively translatable (if you know one really well) then it shouldn't be a big deal. If this is to secure a job it's much better to be proficient and confident at 2 languages than alright in 3. Once you have the time and security, learn a 3rd as you please.
A normal, US keyboard works perfectly. 
Honestly, whatever you're comfortable with. A keyboard is low on the list of variables when it comes to practising coding and scripting.
DAS Ultimate Keyboard forces you to learn how to touch type...
I had no idea MBAs have BI/BA concentration. Do you mind telling me what school you are attending?
I'm also gonna back /u/stonedsqlgenius as well, but disagree with you on the Java side as Spark, at least within Azure Datalake Store, is being pushed heavily my MS. The combo of Py with Java will cover most of the movement and transformations in cloud ELT (vs on prem solutions for ETL)
I saw someone at a Facebook give a very useful answer to a similar question: &gt; You should know few things cold and I mean really well and practical coding-level: how to extract data from ANY source (RDBMS DBs, APIs) using Python 3.6x as .py (scripts deployed) or .ipynb (notebook), clean and prepare the data, create ML and DL predictive learning models and deploy API endpoints with these and then consume in PWA apps. Once you have this, you should apply for remote-only positions (they pay US$40-70/hr) at Remote.com, dice.com, monster, careerbuilder, cybercoders and indeed. That would be my take on this question. I have helped 2 interns over the summer using this method and both are working following this plan. 
So far in my career I’ve tried about 15 different keyboards in my search for a good SQL keyboard. Apple Magic Keyboard with number pad HHKB CODE/Vortex Pok3r Ducky MS Surface Keyboard HP Elite v2 What I found is that I just happen to like Chiclet style keys more. So the Magic Keyboard, Surface Keyboard, and HP Elite keyboards are my favorite. I also use that “right click menu” key a lot (on the right side next to CTRL/ALT, so the Surface Keyboard and the HP Elite were my winners. I use the alt-shift method in Notepad++ and SSMS a lot, so placement/size of the Alt key and arrow keys played a part too. The HHKB felt great as far as mechanical keyboards go, but unless you change ALLnof your keyboards to use the “CTRL where caps lock is” format, you’re in for an annoying time when you use any other keyboard. The reality is that there is no real good “SQL Keyboard.” You just have to find which one has the best feel and features/extras you want.
Can't go wrong with a basic Microsoft wired keyboard. 
I've been working as a business and data analyst for the last 20 years and primarily focused my efforts around database design, creating reports and dashboards, business process development, fraud rule development. I occasionally use Python to pull data from various API based clients or a little bit of web scraping. Beyond the hard-skills of programming, which are important but more so I would say practice on your soft-skills even more so. i.e. how to work with clients? listening skills (very important), how to empathise? how to negotiate? As a business analyst, you put on many hats but in time you will be spending more time with clients and less time coding. Over the summer I created a Udemy course on "Business and Data analysis with SQL" based on my experiences. Be sure to take a look, as it is on discount today for Cyber Monday. [Business and Data Analysis with SQL](https://www.udemy.com/business-and-data-analysis-with-sql/?couponCode=WHATDATA10)
What makes a "coding keyboard" different/better than any other keyboard? Use what's comfortable for you. I'm using the POS Dell membrane keyboard that was issued with my work computer; I have a CoolerMaster mechanical at home which I prefer but mechanicals aren't compatible with open office floorplans.
[DBCC CHECKIDENT](https://docs.microsoft.com/en-us/sql/t-sql/database-console-commands/dbcc-checkident-transact-sql?view=sql-server-2017). Docs also contain links to other functions regarding identities.
If your data is already in an SQL database, as is the case for most businesses, then it's a good idea to know at least a little about SQL.
An outsider misconception regarding those who develop is that we write code all day long, 8 hours a day, without stopping - as if we were a court reporter or something. This isn't true, or at least it shouldn't be. If you're spending more time typing code than planning it you're doing it wrong. That being said, I prefer a mechanical keyboard for work but that's strictly based on preference. 
that's the problem, it doesnt work. I have tried different variations, and it always stays the same. My current identity value changes, but the column identity stays the same somehow. The first [condition](https://docs.microsoft.com/en-us/sql/t-sql/database-console-commands/dbcc-checkident-transact-sql?view=sql-server-2017#exceptions) looked like it should fit my problem, but it really doesnt solve it. I dont understand why
The backup file is named whatever you tell SQL Server to name it, including the extension. If you want a `.bak` extension on the backup file, specify it in the filename.
Absolutely. Maybe my understanding of the term data analysis is too specific (e.g. statistical analysis, visualizations data science etc)
Guess I am not understanding you then! Are you trying to UPDATE the value of already inserted records rather than change the current identity value to use next?
LocaleDwId is my identity column, it had in total 6891 rows before (so max(LocaleDwId) is 6891), but then I deleted 82, and now there are 6812 rows, but the LocaleDwId's have stayed the same (which is why in my picture it jumps from 440 to 470 to 479 and then 505, my deleted records were between 440 and 505) &amp;#x200B; Now i'm trying to reseed my identity column, so there are no spaces like that and it goes up gradually by one. The DBCC CHECKIDENT should work, but it doesnt, or I am not using it correctly...
There's a difference between changing the next value to use, and trying to change the values of already persisted records. You'll first need to SET IDENTITY_INSERT tableName ON. Then you need to delete all the records from the start of the gap, and reinsert them with the correct identity. After that you SET IDENTITY_INSERT tableName OFF, and then you can reseed the value to whatever is the next identity value for use. It's however better if you don't have to do that - mostly because that identity value you are changing could very well be referencing other tables that you now must maintain or you break referential integrity.
Why is it so important to you that the ID values are sequential?
An identity is supposed to be for creating a surrogate key, which is, by definition, meaningless. Why does it matter if you've deleted some of the values in the Id range? Generally, it shouldn't. Can you explain why this is a problem you?
honestly? Only for aesthetic reasons. It's not super important, and I was mainly looking for learning more about identity keys. It's just that having the gap in the the ID range is just not pretty. It's not wrong, i know
You should essentially ignore the contents of the identity column. It's for the database engine and not us humans. If you want an ever-growing and gapless index, your best bet is to make another column that you can update without changing your identity column. 
&gt; It's just that having the gap in the the ID range is just not pretty Learn to get over it. It's not worth the hassle and risk to your data integrity to make the IDs "pretty" and it'll be a never-ending struggle to keep re-working it as your data changes over time.
Do you understand why everyone is saying ignore it? The identity is for a unique key for that row. It's like if you had five dogs named one through five. Number three runs away, so you renamed four and five so they'd be sequential. It doesn't make any sense. The identity if that row's name.
&gt;university of south florida muma college of business &amp;#x200B;
Think of it like this. Your data is inside a database. How are you going to do statistical analysis and data science on it? You're going to need SQL to access your data. Many courses on data science just use CSV files that is loaded into RAM because that is convenient and then you can focus on the analysis part. But in the real world you have terabytes of data. That doesn't fit in RAM. If you can only work with csv files that fit in RAM you're going to be a useless data scientist and always need the help of a data engineer or developer to hand you the data. You better have a PhD in statistics if you're going to get away with that. That said, SQL is not data analysis. But it's a very important tool. 
Python might assist in Data Science and/or Data Analysis.
That's an awesome analogy, I love it.
MBA in BI/BA is some awesome coursework bud.
right on, I am trying to map out a career trajectory into too much of a detail. I think for now I will just trying to stick with R and SQL. I will learn ETL concepts like flume, xpath, xml. Once I secure a entry level job. I will see where I want to go into and whether or not I need to become very proficient in python or Java. 
I will definitely check those out, I do think Java will come handy and I will definitely try to learn it whenever I have more free time. 
Why is it unhelpful to ask someone why they want to do a thing? The response I might give entirely depends on the reasoning for what they are doing. If someone comes to me and says "what is the best way to build a query" I am necessarily going to ask "what kind of query". "Why?" is a perfectly cromulent clarifying question to the OP.
Please, let’s not play dumb. We both know the response was made in a rhetorical manner and not a helpful one. If he truly was curious a response along the lines of “why are you trying to do this? Typically you don’t want to go this route because...”. Theres a difference between being helpful and being arrogant.
One word, and you have invented an entire world history from whole cloth. Who's being arrogant? Who is the one calling names? Honestly, I'll be a bit of a prick now - you can fuck right off for accusing me of "playing dumb".
I think in this case the answer to "Why?" will influence the advice you're going to give (at least partially). If OP's answer to why is : "Oh, I don't like seeing gaps in the number range, it doesn't look neat to me." then even if you give the solution you should definitely advise that it's more trouble than it's worth. If the answer is something more practical (and less common) - e.g. he's dealing with a weird front-end system which he can't influence that behaves badly when there are large gaps in the key range then there'd be no need for the "Don't bother doing this" advice. Having said that, I think you should generally try and answer the question even where it sounds a bit wrong-headed. 
Ok.
Maybe it's a test database? Maybe it's a new feature, and something got weird during launch, so the table has never been used before? There's tons of reasons why this might *not* be a bad idea.
&gt; honestly? Only for aesthetic reasons. It's not super important, and I was mainly looking for learning more about identity keys. &gt; &gt; &gt; &gt; It's just that having the gap in the the ID range is just not pretty. It's not wrong, i know Consider the scenario where you did not have an autoincrementing value and were instead doing it yourself. Say you end up with 100k rows. Now you need to delete 500 rows in the middle. There are probably other tables now that have foreign keys pointing to many of the other records in this table using that integer ID. It would be completely impractical and dangerous for you to redefine the primary keys for all your records just because you deleted 500 rows and "don't want gaps". 
Okay you made some good points. But how does the reason behind wanting to reseed a table change how you reseed a table? There’s literally only one way to do that, and that’s DBCC CHECK INDENT. Like I said, you could simply answer the question while also explaining why it’s not a good idea or better alternatives, and not acting as if reseeding a table is some sort of evil magic that should never be used. I stand by my argument that answering “why” is dismissive and unhelpful.
Firstly, sorry for being a bit snappy - hadn't had breakfast. I also see where you were coming from and fair enough. The answer is tricky - what's basically our job or duty here? To give short straight-forward direct answers? Truth be told, most of this is nowadays available online or through a google search or better still, in the official docs. So are we just a way for someone to get a lazy answer? Or is our job to talk about the reasons, lay out various strategies, identify complications and risk etc? Stuff that is not straight-forward to understand from official docs or even stackoverflow for example? If so, it is vital to first understand the goals and objectives first, so we can talk constructively about the various solution designs.
No need to apologize. I’m not offended at all. I agree with you that this question could’ve been answered by a quick google search and OP might’ve been a little lazy. I’m of the opinion that if someone asks me how to do a specific task, I’d first answer their immediate question, and if I think it’s a bad idea explain exactly why going that route is bad idea. There are cases where reseeding is acceptable, and the future OP will be able decide wether or not reseeding a a good solution for what he’s tying to accomplish or not.
Yeah don't do this. Instead, create a new boolean column with a name like IsDeleted, non-null with a default of 0. Going forward, don't actually delete any records, but instead update IsDeleted to 1. Or, better, yet, if you are on a version of SQL Server that has Temporal Table support (not to be confused with temp/temporary tables) then use that. But don't worry about IDs remaining sequential with no gaps. Those missing numbers properly refer to records which no longer exist, and should not be reused on the same table.
Fair enough. I agree.
VERY accurate statement. It's almost necessary to learn a programming language or SQL to use real world data that isn't from a cookie cutter CSV file, and I've learned that the hard way. When I first wanted to learn SQL, I did lots of researching for helpful resources and found these tutorials pretty useful: [https://dataschool.com/?s=SQL](https://dataschool.com/?s=SQL). They explain the basic concepts well and would be a good resource for a beginner. 
Thanks.... I like that "OracleIsBetter.bak" :) &amp;#x200B; &amp;#x200B;
Contiguous values have better statistics. If you have 10,000 distinct values between 1 and 10,000 in two different tables, you know they will always hit on a join. If you have 10,000 distinct values between 1 and 100,000, statistics might think that a join will reduce results significantly even if your process ensures that the values always match.
And another potential internet dogfight ends with.. peace...? Man, what's the world coming to?
Yeah, this is specific to Oracle. Wallet is a data store for credentials, generally specific to webserver applications. Basically, the words version is that you make a wallet, store db credentials in it, and then point your application to the wallet. The wallet uses a lookup table type thing to match up app credentials to do credentials, and Grant access. It's an abstraction layer to avoid giving out db credentials to webapps. I know the premise, not the particulars.
Woops! Didn't know. Thanks for the tip!
Hey - have you used azure Datalake Store at all? We have Hana and I’m blown away by the speed but the system isn’t exactly user friendly. 
Thus far, only in demo at PASS. I'm still struggling under a cradle to client BI project as the data transforms from cosmos db json to staging structure to rules engine for ODS to aggregation and semantic modeling and visualization tool. But, I'm currently modeling after the storage structures available in DLS/ASDB/AAS
Repost with formatting
Sorry, new to all of this
You can use a correlated subquery and I believe it mostly comes down to preference if performance isn't a huge concern.
This should be valid SQL in most engines: select * from t1 inner join t2 on t1.id != t2.id
&gt; LEFT JOIN ... WHERE IS NOT NULL I almost exclusively do that. I'm generally joining those 2 tables on columns that are indexed or the tables are so small it's not necessary.
i wonder if you can do a full joint and take out the stuff you don't want in the specification 
This is the best answer, imo. NOT EXISTS is usually but not always faster, but the one thing other answers leave out is that it also is by far the most clear as to the intent. Writing code that is clear and maintainable and at a glance easy to understand should always be step one. I'm most cases, the optimizer is good enough that this also ends up being the fastest. In the event that this first cut isn't good enough, there are other options to try, but convoluted and fast is not always the right thing.
So basically a *Left Join with an Is Null* should pretty much pull he same records as a where not exists sub-query? &amp;#x200B; If so, while I can see the latter performing better, I feel like the Left Join with the Is Null is a little more intuitive when wanting to select columns from both tables.
Both queries are just plain wrong in regards to what was asked. P.s. using a CTE for that stuff, seriously?
My personal preference too.
 SELECT Artists.name FROM Artists LET OUTER JOIN Albums ON Albums.ArtistID = Artists.ArtistID WHERE Albums.ArtistID IS NULL 
click on " formatting help." code needs four leading spaces per line
EXCEPT is a handy query for this. Select a, B, c from table1 except select a, B, c from table2. Will give everything from table1 that doesn't exist in table 2.
Smells like a corrupted BAK file. Did you take it yourself? 
Hi, No it has been ran via a SQL Job. I have verified the backup and it's says it's fine anyway. Other backups that are taken in the same manor are also fine. Thanks Jason
Have you tried restore filelistonly from disk = 'filepath\dbname.bak' and verified the sizes to ensure you do indeed have the space needed? Did you restore [dbname] from disk = 'filepath\dbname.bak' with move to relocate the files if they aren't going to the same path? 
What version of SQL Server? Are you trying to restore a test database from production? If so, are you able to take a new FULL backup? Is the test database being created by the restore or are you restoring over an existing test database?
I haven't really tried to restore via Script. I can try the above though and see what happens. Thanks
Not necessarily - If he's trying to backup to a UNC path and the SQL server service account doesn't have permission to the folder (or sql service is running as system) it could give a write error. 
True...
 -- distinct codes in table A that are in table B SELECT COUNT(DISTINCT tableA.code) FROM tableA INNER JOIN tableB ON tableB.code = tableA.code -- distinct codes in table B that are in table A SELECT COUNT(DISTINCT tableB.code) FROM tableB INNER JOIN tableA ON tableA.code = tableB.code 
\*Update\* Found disc errors on one of the drives.... Could be it. Going to resolve that first and then see what happens.
You don't really need to swap the order of the tables in the 2nd query. It's already using an INNER JOIN so makes no difference. Not important, just thought I'd mention it
UPDATE Parcels SET status\_code = CASE WHEN status\_code = 'TBA' AND #{userRole} = 'ADMINISTRATOR' THEN 'OPEN' WHEN status\_code = 'OPEN' AND recipient\_id != #{userID} THEN 'ON\_THE\_WAY' WHEN status\_code = 'ON\_THE\_WAY' AND #{userRole} = 'ADMINISTRATOR' THEN 'DELIVERED' ELSE 'ON\_THE\_WAY' END WHERE id = #{id}
Awesome! Thank you very much.
write a separate query to find the recipe with the most amount of salt test it by itself to make sure it's working okay then add it to your original query as a WHERE clause subquery-- WHERE recipes.RecipeID = ( SELECT ... )
You'll want to multiply your percentage (discount) by 100 to make it a whole number. 
I have been working on something somewhat similar. [Perhaps this can be of inspiration for you?](https://www.reddit.com/r/SQL/comments/9v25od/recursive_multilevel_bom_query_ms_sql/)
Correct. Alternatively, you could use: UnitPrice - (UnitPrice * Discount) If the unit price is $5 and the discount is 20% (or 0.2): Method A: 5 * (1 - 0.2) = 5 * 0.8 = 4 Method B: 5 - (5 * 0.2) = 5 - 1 = 4 
I’d normally think that some MAC addresses had many entries in the database and so it was more efficient to search by status or something first, while others perhaps had fewer so they should use the Mac index instead. Worth posting what those indexes are, probably. 
In your SQL server, you can scroll down and click on the transactional replication. https://docs.microsoft.com/en-us/sql/relational-databases/replication/tutorial-replicating-data-between-continuously-connected-servers Set up another server as the subscriber and voila! 
This ^
after i showed you the CASE syntax, why would you go and use IF again?
What's the definition of those indexes? I'm assuming idx_mac is on (mac) alone, whereas idx_macstat is on (mac, status), right? And neither of them includes ts. I'm not an expert on MySQL/MariaDB, but most engines work similarly, so I'll tackle the explanation. The rows column might be an indication of what's going on here. This is what the optimizer thinks the estimated number of rows meeting this condition will be. Now, when the ts is not included in the index, the engine will need to make take a peek into the table to fetch its value to calculate the max. So the "thinking" is: * query 1 = I only have 1991 rows to look up in the main table, so I can use the bigger index with 20 bytes and then go back to the table for these 1991 rows, cool cool cool * query 2 = I have 157434 rows that I will need to look up in that table, why even try to use the bigger index? I will be loading these pages anyway and looking at them, so I might just as well use the smaller index and look up both status and ts there [Here's a db fiddle demonstrating this problem](https://www.db-fiddle.com/f/wTpmZy9cU9QHi6cvZbtqze/1). It generates 64k rows with random 0-10 status for 100 "mac" numbers + 64k rows with 6-7 for a single mac a87ff679a2f3, to create a significant variation in the number of rows for regular macs and this one that has a lot of rows. Try running this a couple times until [you see idx_mac being used, instead of idx_macstat](https://i.imgur.com/fWSADDF.png). This is the exact effect you're seeing. You won't really be able to do anything about it. If this is the query you're trying to solve, you can create a covering index, add ts to the index. Just do it at the end, MySQL can do `CREATE INDEX idx ON snapshots (mac, status) INCLUDE (ts)` like SQL Server, so this additional column have to be part of the index, but remember to always have the filtering columns first and then whatever you might select. This will result in no lookup back to the table at all, the index alone will be scanned to satisfy the query.
Ah, so my understanding now is it IS correlated because the subquery references the VisitFact.CustomerDurableKey, which is part of the outer query? 
I suppose there’s no need to use CASE when there’s only one case.
&gt;To do this I have a field in my employee table saying how many projects they are working on No. Don't store something in your database that you can easily calculate with a query... to get the person that should be assigned, just select the count of employees and their assigned projects, then sort by count ascending, and take the first result, aka the person with the least amount of projects.
Okay so I tried this and it’s telling me I cannot have an aggregate function in the WHERE clause (even tho I didn’t have it in the field), any suggestions?
Or... select code from a intersect select code from b You can wrap that in an alias and select count(*) it if your gui doesn't display row counts.
Your subquery doesn't contain the "customer_id" column that you are trying to join on.
what is the error? 
by similar logic, there's no need to use IF because it's not standard sql does postgresql even support the IF expression?
I thought you were being hard on OP, but then looked at their post history. Dang. I'm hoping this is school work, because this level of expertise in a business situation is... well.. I wouldn't want them creating or modifying code. 
When grouping by something you will need to use the HAVING clause(which comes after the where clause) for any conditions which involve an aggregate function. &amp;#x200B; Your query might look like this: &amp;#x200B; SELECT EmployeeId FROM Projects GROUP BY EmployeeId HAVING COUNT(\*)&gt; 1 ORDER BY COUNT(\*)
the error is that you don't need a subquery SELECT customer_name , CASE WHEN birth_date &lt;= DATEADD(year, -18, CURRENT_TIMESTAMP) THEN ' Adult' WHEN birth_date &lt;= DATEADD(year, 1, CURRENT_TIMESTAMP) THEN 'Child' WHEN birth_date IS NOT NULL THEN 'Infant' ELSE 'Missing Birthdate' END CUSTOMER_AGE_GROUP FROM customer 
Recursive data is challenging. Generally this is solved with either cursors or a database that supports recursive stored procedures.
shall we make it a game of guessing which DBMS you are asking the question for ? My tip is oracle
Close, APEX
while it was already answered by /u/[r3pr0b8](https://www.reddit.com/user/r3pr0b8) you can do it in just one query as well &amp;#x200B; SELECT count (distinct a.code) as A -- distinct codes in a , count(distinct b.code) as B -- distinct codes in b , count(distinct a.code + b.code) as AnB -- null values will null the expression and not get counted, you can use case statements as well.... but thats no fun FROM tableA a FULL OUTER JOIN tableB b on a.code = b.code &amp;#x200B; I love SQL. so many ways of doing things ;) &amp;#x200B;
&gt;What am I doing wrong? and your error / issue is what exactly? Right now my answer is .... nothing, you have written some text to a web forum quite well, but I got the distinct feeling that is not what you are trying to ask feedback for
I have had a few epiphanies that have changed my life. Properly formatted code reduces (sometimes fixes) bugs was one of them
 UPDATE Parcels SET status_code = CASE WHEN status_code = 'TBA' AND #{userRole} = 'ADMINISTRATOR' THEN 'OPEN' WHEN status_code = 'OPEN' AND recipient_id != #{userID} THEN 'ON_THE_WAY' WHEN status_code = 'ON_THE_WAY' AND #{userRole} = 'ADMINISTRATOR' THEN 'DELIVERED' WHEN status_code = 'DELIVERED' THEN 'DELIVERED' ELSE 'ON_THE_WAY' END, courier_id = CASE WHEN courier_id = #{userID} THEN courier_id ELSE #{userID} END, date_delivered = CASE WHEN status_code = 'ON_THE_WAY' AND #{userRole} = 'ADMINISTRATOR' THEN #{now} ELSE date_delivered END WHERE id = #{id};
Group concat strikes again!! 
Group concat strikes again!! You keep getting all the comments before I see them. 😂 Keep it up! 
Usually for my ssrs reports I make a default parameter for a startdate and allow users to enter a value if they wish.. Example @startdate date , @enddate date = eomonth(@startdate) If the dates change and it's not always end of month then take eomonth out. 
Can you just make a incremental ID that has no real relation to the data? Then use phone numbers as a foreign key lookup to find users with that phone number? Maybe I'm not saying this the clear.... 
That feels more like powershell code running alongside sql. 
I’d maybe start with something like this: SELECT CITY, COUNT(Subquery.DID) AS NUMBER_OF_DONORS FROM ( SELECT DISTINCT DID, DONORS.AMOUNT as DONOR_AMOUNT FROM DONORS INNER JOIN ACCEPTORS ON DONORS.CITY = ACCEPTORS.CITY AND DONORS.bloodgroup = ACCEPTORS.bloodgroup WHERE DONORS.AMOUNT &gt;= ACCEPTORS.AMOUNT ) AS Subquery GROUP BY CITY IRDER BY CITY ASC You want to first have a subquery that joins eligible DONORS records with candidate ACCEPTORS records based on the logic you provided (same city, same blood group, donor amount &gt;= acceptor amount). The distinct qualifier may not be necessary—I’d just lean towards using it since this dataset is probably large and messy. Then you want to run an aggregation on the subquery (the outer query): count number of donors and group by city. I’m no expert, but this should get you close. Apologies for formatting (on my phone).
Are we a joke to you?
[From the Microsoft website](https://support.office.com/en-us/article/control-data-entry-formats-with-input-masks-e125997a-7791-49e5-8672-4a47832de8da) : For example, this is an input mask for a telephone numbers in the U.S. format: (999) 000-000;0;-:
Thanks 
Data analysis people will say that 80% of the time is doing data processing, so data processing capabilities are necessary, simple tools are Excel, SQL, complex R, Python, Java. Reports for business tools is Finereport. You can learn this tool because it is very helpful to you. [FineReport](http://www.finereport.com/en/) itself is a universal reporting tool and data visualization tool. It's like Excel, small enough to store statistical data, produce a wide variety of data charts, dashboards, large financial reports, and development invoicing systems. It can connect various business system data including ERP, CRM, OA, MIS as an intermediate data management platform. It can quickly generate reports and build a unified [data analysis](http://www.finereport.com/en/about-finereport/today-i-must-make-it-clear-that-the-difference-between-tableau-and-finereport.html) and visualization platform.
Hey, SQL Job running on the same server backs it up every night at 3:00am :)
Is this Syteline (CloudSuite) ERP? If so, you'll want to join item on jobmaterial.item = item.item, then rejoin jobmaterial on item.job = jobmaterial.job. For some reason the BOM for both jobs and items are in jobmaterial, (usually have a suffix field that separates duplicate job numbers between job and item BOM).
I think you just need the line: WHERE year(PaidValues.DatePV) = 2018 just above the order by line. Also, I suppose 2018 could be your variable. Also, in your select statement you are grouping by month. Since you aren't grouping by year as well you may get multiple months of the same name summed together. January of 2017 and January of 2018 will just sum into the "January" number. You may want to consider something like select Month(DcPgData) +' ' + Year(DcPgData) Good luck! 
I do one and two, and also put my commas at the beginning of each line in my select. They're much easier to see that way.
It's just a mistake, like using `=` instead of `==` (or even better, `===`) in an if: var x = 3 if (x = 5) { console.log("well now it is 5 all right"); } Having said that. &gt; Always fully qualifying field names, such as ABC.DEF This is crucial, not for avoiding the issue you experienced, but because it's good practice. How are you supposed to know where does a column come from when you join two tables and don't use fully qualified names? You can't. Always do that. Using aliases for tables is also very good practice, in case you want to start using a different table or maybe a view in the future. 
It's also much easier to comment out SELECT a.col1 AS val1 , col2 AS val2 --, col3 AS val3 is much easier than SELECT a.col1 AS val1, a.col2 AS val2, # a.col3 AS val3 because in the second example i have to edit two lines instead of 1
👆 this usually referred to as the **leading comma convention**
That's just school in general. It's a lost cause to expect school to prepare students for the real world. I sympathize with both the teachers and students in this case. It's not realistic for teachers to create new lesson plans and new questions every year, so of course students are going to memorize repeated questions. It's hard work to learn the concepts well enough to compete with that. Then when the questions change, the students with effectively zero preparation fail miserably. Preparing for a professional certification is not very realistic. First, a prerequisite is fluency with the language, something students aren't expected to have. Second, effective preparation is typically compressed into 4-5 full days. Spreading that out with a few hours a week over several months would result in substantial lost knowledge. I also don't see many students paying for a certification, and when they enter the workforce, potentially years later, it will be completely wasted effort. My best advice is to do the weekly projects twice. I suspect students are spending 90% of the project fumbling with getting syntax to work, and only 10% of the time thinking about what is happening to the database. Going through the project a second time, with all the syntax prepared and working, would allow the student to focus on the content to be tested.
You can also do a `union` of the fields (year, month) you need from the 2 tables (DocumentosPagar &amp; ContasReceber), and join to that union, instead of doing a full outer join. &amp;nbsp; 
We generally create out models in SSDT in combination with SQL Server templates we built to include items that we include in every table. We generally write things quickly on the whiteboards if we need a discussion on what and where, but usually just a discussion works. It's just something that happens after doing it the same thing.
This.
Either Visio, or I just write it out on paper and draw my links.
To extend this, I will always include 1=1 for my where clause so that I can easily comment out my clauses for testing WHERE 1=1 AND A.val LIKE ('%123%') AND B.Year = '2018' AND B.Month = '12' -- AND B.Day IN (1,2,3,4)
Anyone know if they are offering a student discount?
Thank you
Does this support MS SQL or only the crappy versions?
&gt;Does this support MS SQL or only the crappy versions? Yes, it does
Well my main problem is to understand this weirdly formulated task..
Q1 looks like "for each user, how many connections do they have", so since there's entries for both directions of each relationship that will be a simple SELECT user1, COUNT(user2) AS connections FROM table GROUP BY user1 (Noting that there actually does seem to be a link between 3-11 that is not there as 11-3, but this may be a typo) Q2. That seems the correct interpretation to me. Mutual connections as in mutual friends, someone both sides know. Q3. Seems like the only answer to me, you'd use half the rows storing only one direction but make your queries slightly harder by having to join to get both directions.
Can you use a UNION? I.E. `SELECT *` `FROM table` `WHERE bookIndex = 1` `AND chapterindex &lt; 20` `UNION` `SELECT *` `FROM table` `WHERE bookIndex = 1` `AND chapterIndex = 20` `AND verseIndex &lt;= 10`
Worked perfectly, thank you!
those questions are really bad. I'm guessing the 1st question is doing a distinct count of the value pairs. I'm guessing the 2nd is doing a count grouped by both keys, and for the 3rd question I honestly don't have an answer, since I have no idea what that data is. Likely they want you to answer by an intermediate table to model an n:m relation, but then again, that thing IS the intermediate table..... I have no idea. &amp;#x200B; the special problem of (A,B) and (B,A) is quite easly solved by sorting. &amp;#x200B; To be honest, when people like /u/r[3pr0b8](https://www.reddit.com/user/r3pr0b8) and me tell you that they have question marks on giving you an answer on those forms of questions (feels a bit test / interview like) there was not much thought put into making up the question, they suck. I've spend multiple work days doing nothing else than coming up with lab studies and questions for job interviews, and I always go trough multiple reviews, where I dump that on a collegue and tell him to try answer the test questions, and give me feedback on the wording, to make it as clear as possible. That was not done in your case, I'm sure of that.
Sorry it took me this long to reply. I get your logic here .. and I might actually submit an answer inspired from it. Thank you.
Its not about frustration to me. The person that came up with this test whatever the context, did not put any effort in it. That does not frustrate me, that makes me angry. The only thing I can tell you is, that you should not feel bad about yourself for not being able to come up with good answers. They questions are so bad that you do not have a chance with coming up with a good answer.
I'm nowhere near feeling bad about myself. Yoi could tell a question is bad just by seing the words " table structure " .. I mean .. come on just define a table and that's it. The whole goal if this post was to get people's insights about the questions, because an expert that know everything about a thing can guess by eliminating the unlikely meanings of the questions, whereas I can't because I consider that " maybe " it is something I just don't know about.
I wouldn't call myself an expert to be honest. I know to many things I know to little off. I can tell you with complete confidence, that those questions suck, and that the person that wrote those questions up, did not give it any effort. If I was handed those questions, I would likely walk out the door since I would not want that job just by reading the questions.
First off, you are comparing a date to a month (a1.BalDate &gt;= MONTH(a1.BalDate) - 1) Ex. a1.baldate = 11/28/2018 MONTH(a1.BalDate) - 1 = 10 I am not sure the date format for your BalDate so I am not sure how to help you write the formula. If it is a datetime (2018-11-28 00:00:00.000) you could do something like this: (a1.BalDate &gt;= DATEADD(MONTH, DATEDIFF(MONTH, 0, GETDATE())-1, 0)) Which would compare your date (2018-11-28 00:00:00.000) to the start of the previous month (2018-10-01 00:00:00.000) 
I'd do it this way : &amp;#x200B; `SELECT accounts.Account_Number` `,accounts.Orderid` `,accounts.Order_State` `,accounts.Order_Amount` `FROM (` `SELECT Account_Number = a.Account_Number` `, Orderid = a.Orderid` `, Order_State = a.Order_State` `, Order_Amount = a.Order_Amount` `, OrderRnk = row_number(over partition by A.Account_Number order by Order_Amount desc)` `FROM Account_Orders A` `WHERE A.Account_Number in (12345, 123456, 123447, ..........) --maybe a join ?` `) accounts` `where accounts.OrderRnk = 1` &amp;#x200B; window functions shine on things like this
Any database that has a JDBC driver is supported by DataGrip.
Not fully. Datagrip only recognizes these dialects: (No bigquery, no presto even with jdbc drivers for example)
In this case you can use the generic sql syntax as a workaround ;)
No but are you sure there isn’t another ETL or syncing process that’s running? The after 10 minutes part is very curious. 
Have you tried something like this? SELECT * FROM user_source WHERE (UPPER(TEXT) LIKE UPPER('%DELETE%') OR UPPER(TEXT) LIKE UPPER('%UPDATE%')) AND UPPER(TEXT) LIKE UPPER('%TABLENAME%') You would change TABLENAME to the table where the data is disappearing from. I think this should give you both packages and triggers.
https://community.modeanalytics.com/sql/ I typically recommend this to SQL newbies. Focus on using SQL for data analysis.
You need to import the .sql file it into a database. Try using MS Access (assuming you have Microsoft office) A quick Google of "Importing .SQL database into MS Access" and you should be good to go Hope this helps!
You should import it into a database. In MySQL, you can use "source" to import it.
Commenting so I can come back later
&gt; window functions shine on things like this except on databases which don't support window functions ;o)
You never will! 
or just use select LPAD(MAX(awdrcdID), 6, '0')
Huh, I didn't know a function like this existed. Thank you as well.
Group by person\_no and select max for date? I've not written SQL in 4 months but I believe that should work. 
You want to group by person selecting the maximum date. SELECT Person_NO , MAX([Date]) FROM Table GROUP BY Person_NO
Look into adding a row and using a partition.
The code that MrDarcy gave is correct. However, I have this same issue when I request any additional fields with the max. For me I would want the date and max price and a ref #. When I add those fields to the group by statement it won’t return the row with the largest price, it returns everything. So heads up if you want additional fields as that statement may not work depending on how the table is structured. When I get around to it, I’ll create a post to see if I could give any input. 
 with rev as ( select '2005' as year,'Q1' as quarter ,'1' as month ,'13' as revenue from dual union all select '2006' ,'Q1' ,'1' ,'10' from dual union all select '2006' ,'Q1' ,'2' ,'15' from dual union all select '2006' ,'Q1' ,'3' ,'35' from dual union all select '2006' ,'Q2' ,'4' ,'11' from dual union all select '2006' ,'Q2' ,'5' ,'15' from dual union all select '2006' ,'Q2' ,'6' , '9' from dual union all select '2007' ,'Q1' ,'1' , '6' from dual union all select '2007' ,'Q1' ,'2' ,'14' from dual union all select '2007' ,'Q1' ,'3' , '7' from dual union all select '2007' ,'Q2' ,'4' ,'20' from dual union all select '2007' ,'Q2' ,'5' , '6' from dual union all select '2007' ,'Q2' ,'6' , '6' from dual ) select a.year ,a.quarter ,a.month ,a.revenue as current_qtd_rev ,(select sum(b.revenue) from rev b where b.year = a.year-1 and b.quarter = a.quarter and b.month &lt;= a.month ) as last_yr_qtd_rev from rev a same?
I’ve done this by using a sub-query as well assuming your table actually has more columns than what your sample had: SELECT * FROM {table name} join (SELECT PERSON_NO, MAX(DATE) as last_date FROM {table name} GROUP BY PERSON_NO) max_record ON (max_record.person_no = PERSON_NO and max_record.last_date = DATE) 
Exactly. The correct subquery should look like: ```sql SELECT CUSTOMER_ID, -- OP forgot this CASE WHEN customers.BIRTH_DATE &lt;= DATEADD( year, -18, CURRENT_TIMESTAMP) THEN ' Adult' WHEN customers.BIRTH_DATE &lt;= DATEADD(year, 1, CURRENT_TIMESTAMP) THEN 'Child' WHEN customers.BIRTH_DATE IS NOT NULL THEN 'Infant' ELSE 'Missing Birthdate' END CUSTOMER_AGE_GROUP FROM CUSTOMER customers ```
Any time you groip by all fields in the select have to be the group by fields or have zone sort of aggregating function auch as AVG MAX SUM MIN MAX to help combine rows.
Thanks for the reply! I didn't even realize I was trying to compare a full date to a month! My date format is: 11/11/2018 , if that helps!
Is the database encrypted using TDE? 
Supposedly the second question means what you suggested, how do I solve it ? What do I write ?
I never fuck with right join.... He is not my dawg.
Right join looks boss hog 
Congratulations on graduating! I am finding it difficult to become proficient enough to call myself an SQL dev or BA yet. I am lacking a mentor at my job to help guide me. How many of you had a mentor at any point?
What does a cross join look like?
[Cross Join](https://imgur.com/a/WeWMVQH)
I don't think this works because I need the cumulative totals with each quarter. That's where I ended up using analytic functions and having to find the missing data.
Yea left join all the way 
Right join looks like he's going to point a gun at me and ask "**What does Marcellus Wallace look like??**"
I can absolutely get behind more memes and fewer "omg my homework how do I do this" posts.
If you do that you could end up comparing months from different years and getting results which would probably not be desirable. You would get Oct, Nov, and Dec for every year, next month you would get Nov and Dec etc... This will convert GETDATE() to your Date format and set it to the 1st of the previous month : a1.BalDate &gt;= **CONVERT(varchar,dateadd(d,-(day(dateadd(m,-1,getdate()-2))),dateadd(m,-1,getdate()-1)),101)**
How?
Thank you! so stupid, it's right in front of me... over complicated thinking why..
It turns out the record is being changed by another process outside of Oracle package manager, however this is a really cool table that I did not know about. I can think of quite a few situations where this will come to use. Thanks for the tip Mayor!
My career didn't really take off until I had a mentor. I understood the fundamentals of RDMS and could use GUI's to pseudocode. I could write very basic SQL but would get lost if I tried something even remotely complex. A mentor joins the team and with one look he could tell me what was wrong with my code. I realize a few months later that I swim in SQL and no longer get lost. After that, I was able to join a company as a full-time BA making 35% more than I was before. Five years later and I am getting messages from recruiters and I am interviewing for various analytics manager positions. It all started with that one mentor. I always make time for my peers now. Pay it forward.
r/detroitlions are the meme kings. 
Not obvious what you want as result, aout results are equal
COPYPASTA
Hi, So it's not encrypted. Fixed the hard drive issue and upped the Memory on the server and yet..... Still the same god damn error.
So you'll need to join the table with itself to get all the possible pairs, so SELECT first.user1, second.user1, second.user2 FROM table first INNER JOIN table second ON first.user1 &lt;&gt; second.user1 And then you want to filter that to just pairs where the user2 is the same WHERE first.user2 = second.user2 If you simply need a count of the number of mutual connections, put everything above into a subquery and do a group by firstUser1 and secondUser2. SELECT firstUser, secondUser, COUNT(mutualUser) AS mutual count FROM (SELECT first.user1 AS firstUser, second.user1 AS secondUser, second.user2 AS mutualUser FROM table first INNER JOIN table second ON first.user1 &lt;&gt; second.user1 WHERE first.user2 &lt;&gt; second.user2) subQuery GROUP BY firstUser, secondUser Note though you'll have a list showing you each pair twice since you'll have the reverse pairs as well (E.g. 1,2 3 and 2,1 3). This might not be a problem though, depending on what they want from you. If you didn't want the reverse pairs, we could filter those when we did our initial join by only allowing pairs in the form 1, 2 rather than also allowing 2, 1. We do this by forcing first.user1 to be smaller than second.user2. I.e. ON first.user1 &lt; second.user1 instead of ON first.user1 &lt;&gt; second.user2. That means we can't get any reverse pairs - which may or may not be what they want. If you're using text fields instead of numbers this becomes much harder (but still possible). Hope this helps!
I kind of understand how you went through the problem here, however I can't reproduce it. Given my table name is user_connections, what would be the command I need to write, I know this is a silly question, but I just couldn't work it out.
Believe it or not but this dude is 23 years old...
 SELECT firstUser, secondUser, COUNT(mutualUser) AS mutualCount FROM ( SELECT first.user1 AS firstUser, second.user1 AS secondUser, second.user2 AS mutualUser FROM user_connections first INNER JOIN user_connections second ON first.user1 &lt;&gt; second.user1 WHERE first.user2 &lt;&gt; second.user2 ) subQuery GROUP BY firstUser, secondUser Try that!
I am pretty sure this is the way to go about it. However, you probably misunderstood what "mutual" means. So for example, user A had a connection to user B. And user C has a connection to iser B. The command should display that A | C | B. And if user X is connected to Y and Z. And user T is connected to Y and Z, it should display X | T | (Y,Z) 
I got it solved by a reddit user /u/[chicken\_\_soup](https://www.reddit.com/user/chicken__soup) here is the query for it : select query_user.user1 as me, friend.user2 as other, count(*) as n_common_friends from user_connections query_user left join user_connections friend on query_user.user2 = friend.user1 group by query_user.user1, friend.user2 Thank you for your help. :D I really appreciate it.
Power Query in Excel lets you pull data directly from SQL databases
I bet whomever is learning SQL will know the joints now.
You could always throw away results from the smallest areas. But i suggest talking to that statistician...
Well, r/bengals are the king of the hill meme kings.
This is what reporting engines/platforms like SSRS are meant for. If that's not an option, I like to use PowerShell with Doug Finke's `ImportExcel` module to run the query and then write the results out to an Excel file.
[Cross join](https://i.imgur.com/XOdzbu2.jpg)
You can use it for a whiteboard question. At least that is how I see it. 
Yeah when you have data at 2 different grains for example Daily x Product and Daily x Depot x Product. If the view displays contents from both tables then use a full outer join to include Daily x Depot x Product metrics even if the Daily x SKU metrics are not present. 
Install SQLite
Well the two aren't really used for the same thing. My advice would be, if you're storing data in excel spreadsheets, you're doing it wrong. That is what a database is for. If you're doing analytics, there are better languages to go for like R or programs like PowerBI. 
 [CREATE - 0 row(s), 0.063 secs] [Error Code: 904, SQL State: 42000] ORA-00904: : invalid identifier ... 1 statement(s) executed, 0 row(s) affected, exec/fetch time: 0.063/0.000 sec [0 successful, 0 warnings, 1 errors] "level" appears to be a reserved word. Try renaming that column to something else.
If still looking for the instant solution you can take assistance of [SQL recovery tool](http://www.databasefilerecovery.com/mssql-database-recovery.html).
Actually it wouldn’t be asking the question. It would more be an alternative to the usual Venn diagram I use in interviews. More memorable and more funny. 
Cheers, but it turns out we were restoring it the wrong way entirely. Some head bashing and help from a supplier worked that out and we were good.
You asked about learning SQL, not Sqlserver specific functions. SQL &lt;&gt; SQL Server. SQL is a language, with a standard. Different rdbmses have specific functions, data types, command line options that are not always present in other rdbmses. 
Ok. At work I use SSMS, connect to SQL server. How would you recreate it on linux machine at home? I would prefer to use same functions instead of learning something else.
Dunno if it's possible, but create a Windows Server VM? Or use a cloud based SQL Server service? 
Thank you, I will start looking for some cheap cloud based SQL Server.
And use varchar2, unless you have a precise reason to use varchar in an Oracle database. 
You can get the developer edition of MS SQL Server for free. And yes, there is a Linux version of MS SQL Server... officially supported by Microsoft. https://docs.microsoft.com/en-us/sql/linux/sql-server-linux-overview?view=sql-server-2017 
SQL Server runs on Linux now.
So varchar2 is just a conspiracy theory to subject customers to vendor lock in? Illuminati confirmed!
thank you
alright thanks
Finally someone who believes me! Can I perhaps interest you in some grade A tinfoil hats?
What RDBMS are you using?
Awesome! up and running already! Many thanks!
sms.db? Is this [the database from an iPhone](https://old.reddit.com/r/jailbreak/comments/2uoe6n/how_to_view_smsdb_iphone_files_on_a_pc/) or something? It looks like that's an SQLite database, so that's what I would try.
Thankfully most SQL database systems are nothing like Microsoft SQL Server. I would find another career if so.
Do the same thing in both systems and show him the difference. You need to prove to him one is better than the other. 
Do you want to use SQL in the future as a career path, or are you just taking a class and want to know the answer to this question? Not trying to be a prick, genuine question.
Try changing WHERE cproc.OR_CASE_ID = ( SELECT OR_CASE_ID FROM OR_CASE_VIRTUAL WHERE ADD_ON_CASE_SCH_YN = 'Y') to: WHERE cproc.OR_CASE_ID IN ( SELECT OR_CASE_ID FROM OR_CASE_VIRTUAL WHERE ADD_ON_CASE_SCH_YN = 'Y') ​ ​
Yea, I'm with you /u/notasqlstar - this feels like a true or false question on an exam to me.
 select whatever from or_case_all_proc cproc join or_proc defproc on [whatever] join or_case_virtual virt on virtu.or_case_id = cproc.or_case_id where virt.add_on_case_sch_yn = 'y' 
That is not functionally equivalent. What if multiple rows in or_case_virtual have add_on_case_sch_yn = 'y' and the same or_case_id? You'd end up with duplication
And you guessed wrong :-) It's a Multiple Choice question, and that is the answer that I chose. The choices were A.) Uncorrelated B.) Correlated, because the outer query references CUSTOMER.CUS\_ID, which is part of the subquery C.) Correlated, because the subquery references CUSTOMER.CUS\_ID, which is part of the outer query D.) Correlated, because the outer query references CUS\_ENC.CUS\_ID, which is part of the subquery E.) Correlated, because the subquery references CUS\_ENC.CUS\_ID, which is part of the outer query. &amp;#x200B; I am asking because I ALREADY took the test, I am just figuring out whether what I chose is right since we don't get our test back with what questions we miss. This is very unfortunate which puts us at a disadvantage. I am just regurgitating questions that i know I am unsure of. Kind Regards. &amp;#x200B; &amp;#x200B;
Pretty sure the answer is C. The subquery can't run on its own and is dependent on the outer query so its correlated. 
True that.
&gt; WHERE ADD_ON_CASE_SCH_YN = 'Y') If you had one record where that was true, your query would work. If you have more than one record, your query will error out because it's expecting one result from: WHERE cproc.OR_CASE_ID = ( SELECT OR_CASE_ID FROM OR_CASE_VIRTUAL WHERE ADD_ON_CASE_SCH_YN = 'Y') but if you have more than one record WHERE ADD_ON_CASE_SCH_YN = 'Y' it passes multiple results back to your query from the subquery. Kinda hard to explain it in this forum (I'm by no means a good teacher), but I hope that helps. 
Hey all good man, wasn't trying to be rude or anything, we get a lot of 'homework help' questions here (So much so that there is a whole section of the sidebar dedicated to it.). Props to you for that. I believe the answer is A. because you're using CUS_ENC to "filter" your results in the subquery. I was never good at multiple choice though.
Bad bot
well, I am a moron. Thank you 
please explain why you're not using an identity column
You can even rewrite this query so that you don’t have to use a subquery in your WHERE clause. 
I haven’t learned about identity columns yet.
Using a CTE may improve performance, depending on how many rows are expected from crpoc. `;WITH CTE_VIRTUAL AS ( SELECT OR_CASE_ID FROM OR_CASE_VIRTUAL WHERE ADD_ON_CASE_SCH_YN = 'Y' ) SELECT defproc.PROC_NAME "Procedure" FROM OR_CASE_ALL_PROC cproc INNER JOIN OR_PROC defproc ON cproc.OR_PROC_ID = defproc.OR_PROC_ID INNER JOIN CTE_VIRTUAL AS CTE ON CTE.OR_CASE_ID = cproc.OR_CASE_ID;`
You're missing a right paren. VALUES (SELECT MAX(ApptID) + 1 FROM APPOINTMENT_jcg**)**, '9-SEP-2018', '11:00 AM', 15, 2, 'SP', NULL, 'CN') ;
You have two left parens and three right parens. Something's not right. 
I feel like there's a comprehensive blog post about this somewhere. I thought it was from Pinal Dave, but this is all I found: https://blog.sqlauthority.com/2010/06/05/sql-server-convert-in-to-exists-performance-talk/amp/
Still gives the same error. 
Can you please provide a describe table? It would help to see the data types, I’m guessing your time insert may not be formatted correctly.
What about the below? &amp;#x200B; VALUES ((SELECT MAX(ApptID) + 1 FROM APPOINTMENT\_jcg), '9-SEP-2018', '11:00 AM', 15, 2, 'SP', NULL, 'CN') ; 
Awesome that got it! Thanks so much!
TIL! Subbing to this thread has had some good random gems. 
I think that’s shorthand. You could do something similar by saying, “...FROM my_table AS m”
In this case, it's correlated because of: WHERE CUSTOMER.CUS_ID = CUS_ENC.CUS_ID in the subquery. Without the context of the outer query, that statement has no meaning, and therefore could not be executed as a stand-alone query. It *depends* on the outer query's result to execute. So, your statement is backwards. The query is correlated because the subquery references CUSTOMER.CUS_ID which is part of the outer query. It's correlated when the subquery depends on something in the outer query, uncorrelated when the inner query can execute on its own. But also, the fact that it's a multiple choice question, the wording is super confusing in those answers. Sorry you had that question.
Group concat is da bomb! 
The C, T, and S in the FROM clause are table aliases. Basically, they are a shorthand way of referring to tables in more complicated queries (like a join) so you don't have to type out the whole table name. The T in front of CustID and SalesRepID refers back to the table aliased as T--in this case, Transaction_kls. So if the T weren't there it would be: SELECT TransID, TransDate, Transaction_kls.CustID, CustFName, CustLName, Transaction_kls.CustID... Good practice would be to reference all the columns after the SELECT with their alias or full table name (if you are not using aliases). I suspect that your professor's example code used the alias table name on just the CustID and SalesRepID columns in the SELECT because those columns exist in more than one table referenced in the FROM, and SQL needs to know which table you want the results returned from. Using the alias or full table name on any column in the SELECT when you are joining 2 or more tables is useful also to people reading your code, as they can tell quickly which column is in which table.
&gt; My professor posted some example code your professor should be taken out back and shot (calm down, people, it's only an expression) explicit JOIN syntax was introduced, what, almost **three decades** ago?