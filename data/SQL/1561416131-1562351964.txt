i cannot test it because it's a mock dbase for a quiz. It's just MSSQL I guess
Query store is fucking amazing. Getting that data used to be *hard*. You left out another fave of mine: online index rebuilds.
Use Google's libphone to format them in a standard format like E164
well that's not what your title says are you familiar with ROW_NUMBER()?
Use it throughout a script with one definition at the top, makes it easy to change one place instead of many.
Thank you
You don't. Well, you shouldn't. A relational database typically stores date or date-time as a numeric representation of a date since a certain time in history. The formatting is done in the client, as it should be. If you have a web application and you want the date displayed a certain way, then you do that in the application code. If an analysts pulls the data into a spreadsheet, they should be able to format the date however they like for their needs. Additionally, if you convert the date to a different data type (which is what you would need to do to accomplish what you're asking), then the end users are going to have more difficulty using the data. If you convert it to a character type so you can have the hyphen (-) separators, the data will sort all wrong on that column. Hopefully, that just saved you some work.
thank you, you definitely did save me a lot of time!! would this also apply to changing the format to MM/DD/YYYY?
Save yourself the eventual heartache and **use Postgres**. It also has Binary JSON, and it's **not** a one-trick pony with a broken leg like MongoDB is. PG is the best open source database around.
You need to understand that a graph *is* a relational model. Before graph syntax, this exact same problem was solved using complex table structures and recursive traversal queries. Social media projects are usually good for this kinda thing. Learn how to apply graph theory to social media needs, and build it all with regular tables. You’ll have nodes and edges and probably a good amount of recursion. Then rebuild it as a graph DB. You’ll discover that it’s all the same thing, just different syntax
Yes. But I'm thinking it could be simpler than that.
Interesting. I guess I assumed that there was a boundary between where recursive traversal would blow up and where graph syntax would take over. Thanks for weighing in. I will definitely keep this all in mind.
Null values in the IN result hit me in the past also. This is why I usually change to an EXISTS query if possible.
I got a plurasight subscription as part of my job. I’ve taken a few of the SQL courses, and yes, they’re very solid.
Another good reason: the database engine recompiles an execution plan for every new query it sees. Declaring variables lets you avoid that recompile cost for frequently executed queries where the only thing that changes is a value in the WHERE clause. It’s also helpful for having a deterministic value for the current date. Imagine calling your db’s now()/getdate() function over and over in your script and having each call be off by a few milliseconds. Instead, one call to`DECLARE @now DATETIME = GETDATE()` at the top of your query let’s you have a single value for @now for the life of the query.
Behind the scenes, your database is just storing those dates as a number. When you see the different formats, that’s the human readable string version of that date. Since formatting your date turns it into a string, you really shouldn’t format your dates until you are all the way at the presentation layer (maybe with JavaScript on a web page, or code in your web framework). If you are strictly dealing with SQL and the data is being exported to a file (so SQL is your presentation layer), then look up your database’s date format functions. For example, SQLServer uses `FORMAT()` for turning dates into pretty strings.
Not familiar with EXCEPT. Is that an actual keyword? Maybe try a MINUS instead?
nope wont work :/ this is dealing with advanced queries
Doesn't seem that advanced. SELECT fname, lname FROM student WHERE sid IN (SELECT sid FROM registration WHERE crn in (SELECT crn FROM course WHERE dept = 'CIS') AND sid NOT IN (SELECT sid FROM registration WHERE crn IN (SELECT crn FROM course WHERE dept = 'HIS') ); ought to work. Or take your original SQL, remove the first ';' and swap in MINUS for EXCEPT.
 DECLARE @CIS int, @HIS int; SELECT @CIS = crn FROM course WHERE dep = 'CIS' SELECT @HIS = crn FROM course WHERE dep = 'HIS' SELECT fname,lname FROM student WHERE sid IN ( select sid from registration where crn = @CIS ) AND sid NOT IN ( select sid from registration where crn = @HIS) Why not use a not in?
My understanding is that MySQL doesn't support either EXCEPT or MINUS.
Use IN/NOT in when you have static or hard coded values. All other cases should use EXISTS, Exists performs better and also handles NULL values. https://www.mssqltips.com/sqlservertip/6013/sql-server-in-vs-exists/ http://www.dba-oracle.com/t_exists_clause_vs_in_clause.htm Also, as /u/Alinroc says, upgrade from 2005z SqlserverUpdates.com
okay, it seems like the issue is that i'm not limiting the amount of data being brought in by the joins.. so i'm trying to filter down my joins further, but i'm not sure if i'm doing it right. My joins are below from ActiveProjects LEFT OUTER JOIN LedgerAR b on ActiveProjects.WBS1 = b.wbs1 and left(b.Period,4) = '2018' LEFT OUTER JOIN LedgerMisc c on ActiveProjects.WBS1 = c.wbs1 and left(c.Period,4) = '2018' group by ActiveProjects.WBS1 or should i do: from ActiveProjects LEFT OUTER JOIN LedgerAR b on ActiveProjects.WBS1 = b.wbs1 and left(b.Period,4) = '2018' LEFT OUTER JOIN LedgerMisc c on ActiveProjects.WBS1 = c.wbs1 and left(c.Period,4) = '2018' group by ActiveProjects.WBS1 or from ActiveProjects LEFT OUTER JOIN LedgerAR b on ActiveProjects.WBS1 = b.wbs1 LEFT OUTER JOIN LedgerMisc c on ActiveProjects.WBS1 = c.wbs1 where left(b.Period,4) = '2018' and left(c.Period,4) = '2018' group by ActiveProjects.WBS1 or from ActiveProjects LEFT OUTER JOIN LedgerAR b on ActiveProjects.WBS1 = b.wbs1 LEFT OUTER JOIN LedgerMisc c on ActiveProjects.WBS1 = c.wbs1 where left(b.Period,4) = '2018' or left(c.Period,4) = '2018' group by ActiveProjects.WBS1 It's important that I limit the joins to 2018 results so that my aggregated sums are getting multiplied by arbitrary amounts. What's bothering me right now is that if I only join B, the aggregate amounts for all projects are accurate, but once I add that second Join, that's when my select/sums get multiplied by arbitrary amounts... Please help if you can :(
Not super familiar with MySQL, so idk if this will run. The logic should be there though. You left out the join to the products table, you didn't filter for cities with &gt; 100 customers, and using top 5 will only return 5 results rather than 5 per city. `select * from (` `select` `*, row_number() over(partition by customers.city order by sales desc) as rank` `from (` `select` `categories.category_name` `, customers.city` `, sum(order_items.product_price * order_items.quantity) as sales` `from order_items` `inner join products` `on order_items.product_id = products.product_id` `inner join orders` `on order_items.order_id = orders.order_id` `inner join customers` `on orders.customer_id = customers.customer_id` `left join categories` `on products.category_id = categories.category_id` `inner join (` `select city` `from customers` `group by city` `having count(distinct customer_id) &gt; 100` `) cities` `on customers.city = cities.city` `group by` `categories.category_name` `, customers.city` `) a` `) b where rank &lt;= 5`
thanks so much it worked!
hmm. interesting techniques/ I'll dig in line by line and see if i understand what you did
I work at a School District and this is my most common use for DECLARE, just using it to replace GETDATE() in my queries saved an ungodly amount of time. I also used it to create a Dynamic endyear for school years by using DATEADD for year adjustments when ever the month was greater than 8 so I always know what the end year is live in the queries. Combined that with moving everything to stored procedures so they are pre-saved and I brought down the 60ish application queries we had from 2-5 minutes per query to my longest now being 10 seconds and most being less than one.
Nothing super special. Another user mentioned row_number, which I included. I also filtered for cities with more than 100 customers using having + inner join.
It depends on the client. Most allow at least the developer to control the format (.NET, Java, etc.). Excel and other spreadsheet apps allow the end user to pick the format they want. What client tools are you using or supporting?
I use DECLARE for setting values for audit columns - last updated by and updated at - when I'm doing multiple table updates. We have to select before/after results on case a rollback is needed, so every table gets 4 statements: Declare @now datetime = getdate() Declare @me varchar(30) = 'my work order' Select 'before', \* from table Update table Set values1 = 'new value', value2 = @Now, value3 = @Me Select 'updated rows', @@rowcount Select 'after updating', \* from table where updatedby = @me and updatedat = @now Many of my scripts are reusable as well so when I need to run a script for a new work order I can just change the variable name and not fuss about the rest of the script. Another use is to declare a temporary table (instead of \#tables) - Declare @data table (colname datatype, col2 datatype, col2 datatype) Then I can join onto it like I would with any other table: Select \* from table t join @data d on d.column = t.column Useful for inserting/updating/deleting lots of user provided data when you don't have a query to select that data from.
Not sure if this will run properly, I'm still learning. First, we need to create some time stamps in the first temporary table. Then we put all the start and end session events together into one column of "events", still tagged as starts or ends. Then we order that table of events and look at the previous event's count of users. If the present event is a start session event, we increment the count of active users, if the current event is a session end we decrement. The stamp events do not change the count of active users but do copy down the current active users. We then extract the stamp times and active user counts with a "WHERE kind='Stamp'". WITH Stamps(time, kind) as (SELECT CAST('2019-01-01 00:00:00.' as datetime) as time, 'Stamp' as kind FROM t UNION ALL SELECT DISTINCT DATEADD(minute,1,Stamps.time) as time, 'Stamp' as kind WHERE Stamps.time in (SELECT time FROM Stamps) and Stamps.time &lt; CAST('2019-06-25' as datetime)) Events(time,kind) as (SELECT sessionstart as time, 'start' as kind FROM t UNION ALL SELECT sessionend as time, 'end' as kind FROM t UNION ALL SELECT * FROM Stamps) Counter(time,kind,activeUsers) as (SELECT CAST('2019-01-01 00:00:00.' as datetime) as time, 'Stamp' as kind, 0 as activeUsers FROM t UNIONALL SELECT A.time,A.kind,B.activeUsers+1 as activeUsers FROM (SELECT *, ROW_NUMBER() OVER(ORDER BY time ASC) as eventNo FROM Events) as A, (SELECT *, ROW_NUMBER() OVER(ORDER BY time ASC) as eventNo FROM Events) as B WHERE B.eventNo+1= A.eventNo AND A.kind = 'start' UNIONALL SELECT A.time,A.kind,B.activeUsers-1 as activeUsers FROM (SELECT *, ROW_NUMBER() OVER(ORDER BY time ASC) as eventNo FROM Events) as A, (SELECT *, ROW_NUMBER() OVER(ORDER BY time ASC) as eventNo FROM Events) as B WHERE B.eventNo+1= A.eventNo AND A.kind = 'end' UNIONALL SELECT A.time,A.kind,B.activeUsers+0 as activeUsers FROM (SELECT *, ROW_NUMBER() OVER(ORDER BY time ASC) as eventNo FROM Events) as A, (SELECT *, ROW_NUMBER() OVER(ORDER BY time ASC) as eventNo FROM Events) as B WHERE B.eventNo+1= A.eventNo AND A.kind = 'Stamp') SELECT time, activeUsers FROM Counter WHERE kind='Stamp'
why did you not just grab order_items.subtotal instead you did - sum(order_items.product_price * order_items.quantity) - to get sales dollars?
I made the assumption that any given order could include multiple products, and that the subtotal would include all the products in that order. I guess that's not really a safe assumption though, and if each row represents one product groom the order the subtotal would probably equal quantity * price.
This. Always this.
Remember to use SYSDATETIME instead of GETDATE for all your DATETIME2 goodness.
What a great thread! You guys have beautifully outlined the benefits. So as a first time user of declare: once you declare and select a DATE variable, say @last month, how do you use it within the query (i.e. select balance @last month) to get a value? I’m literally stumped here.
So if @lastmonth was a date type, something like SELECT * FROM table WHERE date &gt;= @lastmonth
As much as I wish it wasn't possible, it will be in the near future. SQL Server 2019 has a polybase connector that can connect to mongodb. You can set up a table in your SQL db that links to a collection in mongodb, all while using tsql to query it.
I have a question and I hope you don’t take this as an insult, but what’s your development background that you’re working in SQL but don’t already see the utility in setting a variable? Are you a student, or is SQL your first ever exposure to programming at all?
SELECT * FROM ( SELECT *, ROW_NUMBER() OVER (PARTITION BY CITY ORDER BY subtotal DESC) as rank FROM ( SELECT order_items.city as city, cat_info.category_name as category_name, sum(order_items.subtotal) as subtotal FROM ( SELECT customers.city as city, order_items.product_id as product_id sum(order_items.subtotal) as subtotal FROM order_items JOIN orders ON orders.order_id=order_items.order_id JOIN customers on customers.customer_id = orders.customer_id WHERE customers.city in (SELECT CITY FROM (SELECT city,COUNT(DISTINCT customer_id) as cust_count FROM customers GROUP BY 1 HAVING cust_count &gt;100)) GROUP BY 1,2 ORDER BY subtotal ) order_items LEFT JOIN ( SELECT products.product_id as product_id, categories.category_name as category_name FROM products LEFT JOIN categories on products.category_id = categories.category_id GROUP BY 1,2 ) cat_info ON order_items.product_id = cat_info.product_id GROUP BY 1,2 ) a ) b WHERE rank &lt;=5
&gt;You have to configure Mongo to do sharding. [MongoDB Atlas](https://www.mongodb.com/cloud/atlas/faq) gives you automatic sharding. FAQ page says: "Scalability: One-click, automated sharding for scale out, and zero-downtime scale up to larger instance types. You can provision TBs of database storage, all on high performance SSDs with dedicated I/O bandwidth." &gt;The same " common headline features " are available in many RDBMSes. Some RDBMSes might give you read replicas and automatic failover, but that's it. And these are not their headline features. Distributed database technologies like [Vitess](https://vitess.io/) built as extensions on top of RDBMS do not count, as they're not part of the original RDBMS offering and cannot be clubbed with traditional RDBMSes. I could be missing something, so provide reference links for further discussion. &gt; AWS, Azure, Oracle and GCP offer this for the most popular relational databases as well. Cloud providers add a ton of stuff on top of RDBMS software (sometimes a complete redesign with only API compatibility for MySQL, PostgreSQL, etc. to enable lift-and-shift workloads) to make them scale and work in cloud-native environments. And being cloud-native is not a binary choice, it's measured by the level of integration you have with cloud environments in terms of observability, tolerance to myriad fault modes in dynamic cloud environments, etc. - something which traditional RDBMSes were not built for.
Hey OP, can you give an example of what you would *like* your code to do? pseudo code is fine, having a hard time visualizing your problem. If i understand correctly, you want to perfrom aggregation functions on the the result of LAG and SUM window functions. If that's the case, it may just take one extra subquery. Calculcate your LAG and SUMs, and then do a SELECT *, AGGREGATE_FUNCTION(lag*sum) FROM etc... am i in the ballpark?
I'm not sure on the specifics, but you cen define your Lag and Sum in CTE, and then do the calculation in a second step.
This is a regexp question. https://en.wikipedia.org/wiki/Regular_expression#POSIX_basic_and_extended tells you everything you need to know about {5} and $. If you don't get that you should read up on regular expressions. Also if anynumberhere is really a numeric input it will be implicitly converted into a varchar2.
ROW_NUMBER() is actually the simplest approach to do the classic "top x per y" problem you should see how it had to be done before ROW_NUMBER() was invented!!!
You should therefore specifiy that MongoDB Atlas has sharding enabled. The Community Edition which can be installed onto your own servers requires configuration. &amp;#x200B; Look at Oracle's Price list to see the features that are available at extra cost. Also look into features that are not extra cost. They can be found easily. In 2001 Oracle came out with RAC, in 2002 Advanced Queueing. These are HA and replication functionalities. &amp;#x200B; PostgreSQL features can also be found, some of these features are now making their way into NoSQL and NewSQL databases. PostgreSQL 9 had hot standby in 2010. The Wikipedia page for Postgres makes interesting reading in particular the Release History section. RDBMSes like DB2 on the mainframe have had cluster and failover abilities for years, if you think that "that's it" in terms of functionality in 2019, then you don't know the industry. &amp;#x200B; Traditional RDBMSes have been able to maintain high availability long before anyone dreamed of cloud based services. I personally worked on some in the last century. Your article gives the wrong impression about the state of the database market, there is no historical knowledge.
&gt; Looks like there will be a lot of repetitive columns such as Customer Name, Address, etc. You'll have far fewer headaches if you do this the 'right way' rather than adding redundant/repeated data to various tables. This would, in essence, mean inserting a row into a 'customer' table for each new customer. Each new customer would be assigned a unique primary key (typically an auto-incrementing integer). From there, each of your other tables (newsletters, orders, jobs, etc) simply needs to have a 'customer_id' column, linking the row in that table back to your your customers table. Your homework: google 'database normalisation' and do a little reading :)
The thing is, I'm quite familiar with normalization and know how to do it the right way. In fact, I've already built a DB exactly as you described. However, I am not that good of a programmer, and it's way harder to make an App this way (models having to communicate with each other dynamically), so I'm trying to see if I can give it a pass this other way, lol.
Can you use a view for the app?
I do use views to show data. But I am not sure what you mean exactly.
Try the Stairway series on SQLServerCentral: https://www.sqlservercentral.com/stairways
He means a sql view instead of direct table access
This sounds like it could grow into a full blown mobile ERP. Normalize it now and you won't regret it. I mean, right off the bat it sounds like altering one customer's address would turn into a whole ordeal.
If you're just worried about a newsletter, could you add a field called "Gets\_Newsletter" with a boolean? Then, when someone wants to send a newsletter, a `SELECT Email FROM clients WHERE Gets_Newsletter = 1;` would work. This would be very inefficient over time, especially if this becomes bigger. At some point, though, an off-the-shelf ERP might be a better option.
Can't because customers in newsletter and standard customers are kind of different customers... That's why it made sense for me to be thinking about this route of having different tables with seemingly repetitive columns, we have three types of customers...
Can a customer be part of 2 or more? If so, you'll want to have a 'customers' table with whatever information you have for everyone, then tables for the three different types. Once you start repeating data, you have to find a way to normalize it. If John Smith is in two different "types" and his email address or phone number changes, you have to check every "type" and make the change -- and only if it's definitely the same John Smith.
We have some pretty custom requirements too. Do you happen to know an ERP that is designed for customization? Sort of like an ERP framework is maybe what we need.
For me it's *Functions and Procedures* &amp;#x200B; Being able to dynamically place Data into a Function/Proc and return what you want is easier than writing the Script from Scratch every time. Something where Declaring Variables is necessary within the Creation of them. Something very complex can be done multiple Times in a DB easily. Taking the Painstaking process of writing things from Scratch. If you're familiar with Classes and Methods its a very cool concept when SQL has OOP Qualities (to me at least).
Ooh got it! What if I have two declared variables @currentmonth and @lastmonth and the where clause is already written as the following with two fields current balance and payment. SELECT * Current Balance Case when Payment is equal to @lastmonth From table WHERE date = @currentmonth
Someone recommended this and its helped : https://hackernoon.com/top-5-sql-and-database-courses-to-learn-online-48424533ac61
I mean, you could do it, but why? It's just dividing the last element in the series by the first with extra steps. So, we got n_1, n_2, n_3, ... n_N. Let r_x=(n_{x+1})/(n_x). You want PI_{i=1}^{N-1} r_i. But n_1*(PI_{i=1}^{N-1} r_i) = n_N. Therefore the value you want is simply (n_N)/(n_1). I suppose if you had a column indicating days you held the security and wanted a product of only those days, then you'd need to get fancier. Here we have a data set with closing prices and a column indicating whether we were in the position at yesterday's closing price holding until today's closing price with 1s and 0s. This query returns the total return for the investment strategy (the PositionHeld values) between date Earliest and date Latest: WITH LogRat(dayRet,positionHeld) as (SELECT LOG(Value/(LAG(Value,1) OVER (ORDER BY PriceDate))) as dayRet, positionHeld FROM SecuritiesTable WHERE PriceDate &gt; Earliest AND PriceDate &lt; Latest) SELECT EXP(SUM(dayRet)) FROM LogRat WHERE positionHeld = 1 This uses a CTE as per u/Zikato 's suggestion.
Ah! I see now. I'll give that a go. And...thank you for the point to SQLFiddle. Nice utility.
Try this: ...setting the initial size of the SQL Server Transaction Log file to 20-30% of the database data file size and the auto-growth to a large amount, above 1024MB, based on your database growth plan can be considered as a good starting point for the normal workload. source: https://www.sqlshack.com/sql-server-transaction-log-administration-best-practices/
&gt;What is the "best practice" way to control the growth of these transaction logs? Keep the database in `FULL` recovery model and take transaction log backups on a schedule. Depending on your RPO requirements and transaction volume, this could be anywhere from once a minute to once every few hours. Then a FULL backup on a slower schedule (daily or weekly). Ola Hallengren's Maintenance Solution can install the Agent jobs to do all this, you just have to schedule them.
He means to normalize the tables, but create a view that will match the current table. That way the interface for reading would look the same (but you would have to solve modifications).
Since everyone warned you about the normalization, I'll answer the second part of your question. When will performance become and issue? When you'll need fast data access. For that you need indexes. But as the number of columns grows the number of combination of columns you need to request also grows. Soon you will need 10+ indexes and while they make reads fast, the make modifications slow (and take some disk space as well). Also the table is bigger and you'll notice even when not requesting all the column. It will be stored on more data pages and without optimistic concurrency you will have more locks and deadlocks as well.
No doubt. We also have SQL 2014 on site. I have moved several application DBs over to it. One particular application stopped working because of a data type conversion issue in some views that caused the app to no longer work in 2014 until I corrected the views. The application that I had trouble with in this post is the core of our business so we have to be extra careful to thoroughly test it before moving it over. There are about 1,500 stored procedures with several different writing styles so the chance of there being issues is fairly high. We are currently looking at an upgrade to the application.
use a common table expression (CTE). so something like: ``` with people as (select id, name from employees union all select id, name from customers) select id, name from people order by id limit 50; ``` pg will create a temporary in-memory table called `"people"` which is the union of `"employee"` and `"customer"` which you can query, group, order, limit, etc.
Thanks for this. That was the issue and all is fixed. Also, I learned something today. I had no idea a null would cause an issue with IN. I would have never written a subquery without something in the WHERE clause. Is it more or less efficient to leave out a where clause?
Is it seconds since a reference time, or a duration? Where is it going (IOW, are you OK with losing the ability to do any kind of time math against the result)? What RDBMS? What have you tried? I bet you can find working code on Stack Overflow.
MySQL doesn't support INTERSECT. You should be able to fake it, though. Here's an example I found (haven't really looked into whether it's correct): [https://www.tutorialspoint.com/How-can-we-simulate-the-MySQL-INTERSECT-query-returning-multiple-expressions](https://www.tutorialspoint.com/How-can-we-simulate-the-MySQL-INTERSECT-query-returning-multiple-expressions)
According to [this StackOverflow post](https://stackoverflow.com/questions/2302873/sql-syntax-error-with-intersect), MySQL does not support the INTERSECT operator - the accepted answer to that post shows a workaround
Lol yes to the dev server question :) thanks! I’ll check that podcast out!
Thank you all for the feedback! I look forward to learning everything I can from it! I might see if I can get a subscription from my boss, as it would be much cheaper than an Oracle training class.
&gt; includes the categories that don't have any items this requires that you have a `categories` table which includes all categories, so that you can then use it as the left table in a LEFT OUTER JOIN you're never going to get this by "selecting the distinct categories from the items table"
&gt;However, I am not that good of a programmer, Get good.
&gt; How do I convert a column that is currently formatted as an integer (its time in seconds) to hh:mm:ss? piece of cake SELECT SEC_TO_TIME(yourcolumn) AS yourtime
OP wanted "first 50 rows worth of data from each table" this gives only 50 overall
I tried doing it like this SELECT Repairshop.rid FROM RepairShop WHERE RepairShop.rid = Customer.rid\_FK AND EXISTS (SELECT \* FROM Customer WHERE Customer.rid\_FK = RepairShop.rid); &amp;#x200B; but I keep getting another error involving unknown column in Customer.rid\_FK
I’m not sure I’m understanding your question, but you should be able to use a CASE statement in your SELECT to handle the records with missing data values. In those cases return the string or numeric value you want.
So you're saying he should move to MongoDB
I'm no expert but I don't think there's anything wrong with leaving out the WHERE clause if it isn't needed. In your situation the reason the WHERE clause solved the issue was because it filtered out the NULL value that was causing problems. If you had written a WHERE clause that left it in you would have had the same problem.
your references are very incorrect.
so I found some data completementary to the exercise . I tried your query on a mock d-base and I got this error message: Msg 164, Level 15, State 1, Line 25 Each GROUP BY expression must contain at least one column that is not an outer reference. Msg 4104, Level 16, State 1, Line 3 The multi-part identifier "customers.customer_city" could not be bound. when I fix things, i get more errors and errors
This is going to sound strange but: [Programming ASP .NET](http://shop.oreilly.com/product/9780596001711.do) You’re building an ASP.net application But what it really use is a UI interface to an SQL database. Do you have to design tables, triggers, stored procedures. Write join statements.. Everything you would have to do in the real world you have to Accomplish in this book.
No worries! I'm an analyst who's new to SQL, I have a database that I created 6 months ago but it isn't advanced. I was searching around the internet trying to troubleshoot a problem and came across variables/DECLARE. It seemed like a critical part of SQL and all the articles I found on the topic were really advanced (far ahead of where I'm at). And I wasn't even sure why someone would want to use it if I've survived without it this long.
Thanks so much for describing this so clearly
The only client tools used/supported are Excel/PowerBI, I just connect to the database using PowerQuery and put it in pivot tables
It seems that every company has their own set of job titles. Look into Risk Management, several companies I’ve worked for have them and it sounds like what you’re looking for.
&gt; each individual table in one combined search result. Is there a way to use the join function in a way that gets me information from a variety of different tables while also incorporating the sum function? Or is this not possible/requires a different group of functions? select * from table1 limit 50 union all select * from table2 limit 50 etc.
This seems to be a reasonable approach. Thank you.
? How so
&gt; At what point does performance become an issue? At very least, as soon as you forget what you did, or another person has to take care of it. Once you've got normalized tables, you re-use them in other projects over and over. Best to do the work now.
Wow that actually motivated me.
For example, at the last join, the aggregate you call cat_info appears to be invalid and not recognised , I get an error. The select section at the very top retrieves from the 2nd and 3rd aggregates right?
Hmm, could be missing an AS (?) Cat_info is just an alias I gave that subquery
Normalization and programming have little to do with each other, you can properly normalize and structure your db on a piece of paper
Are you seeing duplicate rows for projects?
Yeah and I did. But I still need an app.
OK... without seeing your tables and data, it looks as though you gather every account number you'll want in the CTE (ActiveProjects). Because that's a list of every account number you'll sum against, I don't think you'll need the left joins any more. Try INNER JOINs (...tentatively. I'm dangerously low on coffee and have not yet switched to bourbon).
And my apologies if that works. I'm the one that pointed us toward left joins yesterday.
 SELECT @A = (SELECT @B), @B = (SELECT @A)
 UPDATE yourtable SET A = B , B = A standard SQL uses initial values before the update (MySQL uses current value, so since A is set to B=10 first, then B is set to the current value of A which is 10)
[https://blog.sqlauthority.com/2008/10/20/sql-server-transaction-and-local-variables-swap-variables-update-all-at-once-concept/](https://blog.sqlauthority.com/2008/10/20/sql-server-transaction-and-local-variables-swap-variables-update-all-at-once-concept/)
maybe Postgresql is different, but as far as i know LIMIT is part of the ORDER BY clause, and a UNION query can have only one ORDER BY clause (at the end) i think you can work around this, though, by using subqueries
You do A = A + B B = A - B A = A - B
OK... let me plod through some questions: in your various ledger tables, whats the difference between 'Account' and 'WBS1'?
this only works for numerical values
Someone downvoted you, but this is the correct answer.
Account is the ledger account we post the transaction to. WBS1 is the project number Thanks, so to elaborate, the key differences between the three "where" statements in my previous reply is how I designated the year 2018 I used "and/or" on the bottom 2, and on the first where statement, I made the 2018 part of the join...
So somewhat counter-intuitively, a denormalized database will often perform *faster*. Joins are expensive, and over-normalizing a database is absolutely a thing. Finding the right balance between normalization and performance is a non-trivial part of a data architect's responsibilities, and it's more of an art than a science - it's something that comes over time with experience. Don't think that 4th normal form is a law (or even a guideline); in the business world it's really more of a concept you have to understand to comprehend all your options, and you can use it as a starting point then back it off where it makes sense.
I did try putting in the AS. It didn't take it, still error
Arithmetically
 UPDATE foo SET (a, b) = (b, a); ?
&gt; (models having to communicate with each other dynamically) But why? There’s no easy, correct way to do what you’re doing. You can’t have organized data and not take the time to continue designing it explicitly and right. The correct way of doing this would be to build a Data Abstraction Layer out of SPs, that way your application doesn’t care where it’s pulling the information from - only that the same information is pulled each time. Then when something changes you only need to change the SP and not the app. So for example you’d have a GetCustomer SP that pulled all the relevant Customer information for a customer. Calling this from the app gives you separation of concerns. If a field, is added to the Customers table then you add it to the SP and the application does nothing with that data until it’s absolutely ready to.
i DID forget a comma between "order_items.product_id AS product_id" and "SUM(order_items.subtotal) AS subtotal" which would def throw an error.
Ah, yes, but that's exactly the problem. I don't know how to solve these modifications, otherwise I would have use the normalized db from the start. I actually de-normalized it to make it simpler to build the CRUD app.
There is a super easy answer, which other's have pointed out, and then an actual math problem that can have you arriving at the result. WITH numbers as ( SELECT 5 as A, 10 as B ) Super easy: Select B as A, A as B from numbers Math Brain Version: ABS(A-B) as A, B-A as B from numbers
Assuming you're using a database that supports window functions... WITH data(id, num, color) AS (SELECT 1, 2, 'green' UNION ALL SELECT 2, 8, 'yellow' UNION ALL SELECT 3, 50, 'blue' UNION ALL SELECT 4, 57, 'red') SELECT *, lag(color, 1) OVER (ORDER BY id) AS prev_color FROM data ORDER BY id;
I think we need to do the aggregation in the CTEs and then join those single rows, like this: With ActiveProjects AS ( Select distinct wbs1 from LedgerAR where left(period,4) = '2018' and amount &lt;&gt; 0 and left(wbs1,1) not in ('z', 's', 'u', 'i', 'c', 'p', 'a', 'e', 'f', 'g', 'h', 'j', 'l', 'm', 'n', 'r', 't', 'v', 't') AND len(wbs1) = 6 and left(account,1) = '4' Union Select distinct wbs1 from LedgerMisc where left(Period,4) = '2018' and amount &lt;&gt; 0 AND left(wbs1,1) not in ('z', 's', 'u', 'i', 'c', 'p', 'a', 'e', 'f', 'g', 'h', 'j', 'l', 'm', 'n', 'r', 't', 'v', 't') AND (Account between '50000' and '59900' or Account between '60300' and '69999' or Account in ('50201', '60201', '50000', '60000', '50200', '60200')) Union Select distinct wbs1 from LD where year(transdate) = '2018' and regamt &lt;&gt; 0 and left(wbs1,1) not in ('z', 's', 'u', 'i', 'c', 'p', 'a', 'e', 'f', 'g', 'h', 'j', 'l', 'm', 'n', 'r', 't', 'v', 't') ), ARTotals as ( Select a.wbs1 [Project], sum(case when b.Account in ('40500', '40100', '43000') then coalesce(b.Amount,0) else 0 end) [Fixed Fee Billings], sum(case when b.Account in ('43100') then coalesce(b.Amount,0) else 0 end) [Hourly Billings], sum(case when b.Account in ('40300') then coalesce(b.Amount,0) else 0 end) [Reimbursable Consultant Fees], sum(case when b.Account in ('43200') then coalesce(b.Amount,0) else 0 end) [Reimbursable Consultant Expenses], sum(case when b.Account in ('40302', '42000') then coalesce(b.Amount,0) else 0 end) [Ennead &amp; Consultant Markup], sum(case when b.Account in ('40500', '40100', '43000', '43100', '40300', '43200', '40302', '42000') then coalesce(b.Amount,0) else 0 end) [Gross Fees Total] from ActiveProjects a left outer join LedgerAR b on b.wbs1 = a.wbs1 where left(b.Period,4) = '2018' group by a.wbs1 ), MiscTotals as ( select a.wbs1 [Project], sum(case when c.Account in ('50000') then coalesce(c.Amount,0) else 0 end) [Reimbursable Consultant Fees], sum(case when c.Account in ('60000') then coalesce(c.Amount,0) else 0 end) [Non Reimbursable Conultant Fees], sum(case when c.Account in ('50200') then coalesce(c.Amount,0) else 0 end) [Consultant Expenses 5x], sum(case when c.Account in ('60200') then coalesce(c.Amount,0) else 0 end) [Consultant Expenses 6x], sum(case when c.Account between '50001' and '59900' or c.Account between '60300' and '69999' or c.Account in ('50201', '60201') then coalesce(c.Amount,0) else 0 end) [DirectCosts], sum(case when c.Account between '50000' and '59900' or c.Account between '60300' and '69999' or c.Account in ('50201', '60201', '50000', '60000', '50200', '60200') then coalesce(c.Amount,0) else 0 end) [TotalExpenses] from ActiveProjects a left outer join LedgerMisc c on c.wbs1 = a.wbs1 where left(c.Period,4) = '2018' group by a.wbs1 ) select a.Project, b.[Fixed Fee Billings], b.[Hourly Billings], b.[Reimbursable Consultant Fees], b.[Reimbursable Consultant Expenses], b.[Ennead &amp; Consultant Markup], b.[Gross Fees Total], c.[Reimbursable Consultant Fees], c.[Non Reimbursable Conultant Fees], c.[Consultant Expenses 5x], c.[Consultant Expenses 6x], c.[DirectCosts], c.[TotalExpenses] from ActiveProjects A left outer join ArTotals b on b.wbs1 = a.wbs1 left outer join MiscTotals c on c.wbs1 = a.wbs1 Let me know what happens... my cofffee is truly depleted by now.
Just brainstorming here, but I'm thinking since it's coming from an SP I can just make a #temp table that stores a combo of every customer, salesperson, and category, then do a right outer join of the whole thing on the condition that the category code and customerID both don't match the original join? Pretty sloppy, but might be the only way at this point
Unfortunately I have to use Proc SQL for now. I don’t have data steps set up to run within VBA connected to the SAS server :/ but thanks for taking the time!
Upvote and move on, good Sir or Ma'am. Thank you. https://www.reddit.com/wiki/reddiquette/
This is why I SQL...
Is 'Proc SQL' an actual product (That apparently lacks window functions?) and not a strange way of saying 'stored procedure'?
Using SAS Enterprise Guide. It is essentially a stored procedure using SQL code instead of SAS code.
I'm not sure if MS SQL will let you use LIMIT in a sub-query but from a quick Google it seems that many other flavors of SQL will not so you would need to use #tables, or possibly a CTE... though I imagine if it works in a CTE it would work in a subquery.
Rather simple regexp. Match any 5 digit number. It seems it is done weirdly but maybe regexp in Oracle has diverted a bit. I don't know since I'm rusty. But I'd do it in *nix/perl like '^[0-9]{5}$' and that would only be if you're not selecting from dual or if anynumberhere was something like, "238784378\n23874\n238783478\n" so that only 23874 was found but I know that dual doesn't accept multiline input, e.g. ^ beginning of line and $ end of line.
Never heard of SAS either. Whoosh me, I guess.
Hands down the best resource is 70-461 Training Kit from Itzek Ben Gan. Look it up on Amazon. It will walk you though everything step by step and provide the best code to follow and practice with. As a DBA I wasn't strong on programming or querying / reporting and wow did this get me ready and I passed my 70-461 certification so there's that. GL.
... and the question is asking about arithmetic values
Assuming a simple `delete from tbl1 where row_ts &lt; 'whenever'` isn't good enough... How many records are newer than that time? Copy them to a temp table, delete everything from the original, copy the saved values back?
AlReady fixed tbaf
 DECLARE @NUMSECS INT = 2227801; SELECT cast(@NUMSECS/3600 as varchar) + ':' + right('0' + cast(@NUMSECS / 60 - ((@NUMSECS/3600)*60) as varchar(2)),2) + ':' + right('0' + cast(@NUMSECS%60 as varchar(2)),2)
I always place them in blocks of records. Say 15k or so at a time. It takes a while but doesn't blow out the tempdb logs, et al and crash the server.
Normalizing on paper is like reading a cookbook to get fed. Sooner or later, the DDL has to happen. But even that's being disingenuous. A normalized database is MUCH easier to code and index against. Rows feel more like objects... I'd much rather think of one customer with multiple ship-tos (separate table) with multiple contact people (separate table) with multiple phone numbers (separate table) than try to query what I need out of one table with a spaghetti predicate. WHERE FaxNumber IS NOT NULL
The fastest way is to clone the table. Truncate the original version, then insert records from the clone. That will be far quicker than delete. If this was Oracle I would suggest using the nologging option when doing the insert which improves performance further (it doesn't write to the dB logs) No idea if there is an equivalent in Postgres though!
I'm not sure if SSRS can use sprocs or not, but even if it can I wouldn't advise it. What you should be doing are creating sprocs that run queries against tables, and then dump the results into new tables where you can point your SSRS reports at. Now you might be asking, why use a sproc and create a new table just for a report, when you could use a view? Because a lot of logic can get complex, and pointing the report to a view can result in a very poor user experience as it takes a lot of time to get the data back. So the solution is to make a sproc, dump the results out into a table, and schedule to have it run at whatever interval you need it to in order to refresh the data. In simple terms say you have a query that is embedded in an SSRS report such as: select stuff from table join (subquery) join (subquery) where stuff At first this runs in a few seconds, and life is good. But over time as your data in those tables grow, and as your report becomes more popular so that you can have multiple concurrent users hitting it... the performance suffers, and now it takes 5, or even 10 minutes to run the report, and sometimes it just times out. This can become particularly important the more filters and parameters you have at the top of the SSRS report. Anywho, all you need to do is: select stuff into newtable from table join (subquery) join (subquery) where stuff And then in SSRS change the entire query to: select * from newtable See? Now when a user runs the report, there is no logic to perform, it just reads the table. The purpose of the sproc here is to automate the process of dumping the data into the new table so that you don't have to manually do it yourself.
 U mean substrings?
This, using a CTAS for the rows you want to keep. Careful of FKs. &amp;#x200B; OR &amp;#x200B; Use pg\_dump and export only the rows you want, then re-import.
Yeah, I'm looking through my code, and I realized I don't use TONS of variable declarations, really. Most of them are parameters passed to the stored procedures, and really, most of them are used as *constants* (like the @now examples given here). In an ad hoc query I'd use it like this: DECLARE @keyval int = '123'; SELECT [Cols...] FROM Table1 t WHERE t.Key = @KeyVal; SELECT [Cols...] FROM Table2 t WHERE t.Key = @KeyVal; SELECT [Cols...] FROM Table3 t WHERE t.Key = @KeyVal; This way I can look at all the related data for a Key and there's no potential to mistype for one table, and I can quickly change the value at the top of the query if I need to re-run for a new Key
Also, how could I forget, you use DECLARE when defining a table variable, which is very useful.
be sure and post back here and let us know how that went
i think SQL Server will allow TOP in subqueries don't have it handy to test it, though
substrings regarding what? Unless (rusty, remember?) you're working with CLOBs I don't thing you can work with multi-line matches in Oracle.
LOL, you beat me to my edit. MS SQL doesn't use LIMIT at all, I just tried to test it and realized it only uses TOP. I thought it did use LIMIT, but I have always used TOP instead. However, you absolutely can use TOP with a UNION ALL, or TOP in a subquery. From my Googling in MySQL it looks like you can't use LIMIT in a subquery, or a union, so you would need to use #tables.
That's not valid syntax, and it looks like your comparing payment (which I'm guessing is a number) with @lastmonth (a date) so I'm not quite sure what you're looking for there? SELECT
U can group by one or multiple columns. The columns you dont group by have to be aggregated (min, max, sum, etc)
alllrighty i think i see what the issue could be. product_id has type VARCHAR in products table, but has type INT in order_items. This will cause the join to fail most likely. Fix for that is to replace "order_items.product_id" with "CAST(order_items.product_id as VARCHAR) as product_id" in my first subquery
A little bit of magic, and a wee bit of sorcery. But seriously, GROUP BY will "collapse" your data set based on whatever type of function you're using. Say you have one million records of data such as: | Client | DateRange | City | State | Zip | SalesAmount | | :--- | :--- | :--- | :--- | :--- | :--- | | ABC | 2019-01-01 | New York | NY | 12345 | 12.66 | | ABC | 2019-02-01 | New York | NY | 12345 | 64.10 | | ABC | 2019-03-01 | New York | NY | 12345 | 72.23 | | ABC | 2019-04-01 | New York | NY | 12345 | 8.64 | | ABC | 2019-05-01 | New York | NY | 12345 | 26.14 | | ABC | 2019-06-01 | New York | NY | 12345 | 7.91 | Now from here you may want to `SELECT SUM(SalesAmount)` which would sum all the rows in the table, which would give you your total sales. But say you wanted to do it by client, so you would have to `SELECT Client, SUM(SalesAmount) FROM Table GROUP BY Client`. Maybe you want to get this data by month, by date, by hour, by city, by state, by zip code, etc. That is why you need to GROUP BY the same columns you are SELECTING.
I can guarantee you that I will be going back to this comment throughout my SQL development as I get more experience. Thanks so much for your time.
Thanks for the info. &amp;#x200B; A vendor installed the SQL databases originally and there is a daily backup of all SQL databases, I'm not sure of the exact process they implemented until I dig in deeper, but they drop on the local server. &amp;#x200B; We recently implemented Solarwinds backup for the hyperv server, local and off site BMR. I'll also be using this to implement disaster recovery plan (offline VMs on a backup server). It is supposed to be app aware of databases and such since we have active directory, etc. on other servers. &amp;#x200B; I'll see how it does with the SQL databases in a disaster scenario. I'm trying to shore up any lose ends, SQL being one of them. I work with a lot of SMBs that have misc. databases for this and that.
That's a good interview question. Very simple, but you probably know what you're talking about if you can answer it.
Why does MySQL have to be the way that it is? I mean, clearly it's designed by and for people who hate SQL, maybe they should just not?
SELECT location, sale_dt, customer_id, SUM(sale_price) FROM sales GROUP BY location, sale_dt, customer_id Or am I missing something?
I don’t fully understand your requirements, but hopefully this will get you close: SELECT Date, Location, Customer, SUM(Price) FROM Sales GROUP BY Date, Location, Customer WHERE ....
Your example doesn't make sense, if you're not using something like SUM you'd just use SELECT DISTINCT instead of a group by. MySQL will actually let you configure it to allow you to group by one thing only. The danger there of course is what happens when you have different values in one of your non-aggregated , non-grouped columns. If you ran SELECT name, date, age FROM persons GROUP BY name What would you expect to happen if you had two people with the same name, with a different date and age? In the only variant of SQL that would allow this, you'd end up with one row for these two people, with the date and age being arbitrarily selected from those two records. There are times writing SQL when you know that there aren't going to be two values (e.g., if there was a person_id, you could logically group on that safe in the knowledge that there couldn't possibly be one person ID with two different names), but to allow that syntax means allowing you to write statements that are actually logically ambiguous. For my two cents, allowing that ambiguity in the language for the cost of shaving a little bit of typing/copypasting off is not a good idea and MySQL should be ashamed of itself for allowing that kind of carryon.
So I guess my concern is that will this realise that customer ID should be bucketed according to location ID? As there are duplicate IDs across different locations
So I guess my concern is that will this realise that customer ID should be bucketed according to location ID? As there are duplicate IDs across different locations
It looks like you want to split your 2nd and 3rd column, then join them based on the first 1st column and the sequence of the element in the split columns. You need to treat each of your splits as separate tables and include those in a subquery that uses a RANK() function in order to get the sequence. You'll then join the subqueries on column 1 and their rank. It'll be a messy query.
This is just a really brief and hypothetical example dataset to demonstrate the question
I think I'm starting to understand it a bit more clearly , thanks
We’re grouping by Location and Customer, so they’ll be treated separately. For example: 08/06/19 | Location A | Customer 2 | £265.00 08/06/19 | Location C | Customer 2 | £322.00 Note that both customers are “2”, but are separate records because we factor in Location.
No, because you're also grouping by location. There's a bucket for every unique combination of location, id and date.
My answer still stands, if you grouped by name, what would you expect the DB to return for age and date if there are two records with the same name?
You need to keep using sql to query things and try grouping it. Some won’t work and some will and you’ll get an idea of how it works. It’s basically grouping by the same types but bit more than that
Finally I understand why the ungrouped columns have to be aggregated.
Easy to get an overflow error for legitimate values
Is that so you don’t end up with multiple rows for a given value that’s Group by’d?
Yes, it will result in a separate 'bucket' per combination of customer_id, location, and sale_dt.
SSRS is a reporting add on to visual studio. Stored procedures are created using SSMS, not SSRS. You can create stored procedures to aggregate data and return the desired report result set, then call these stored procedures from within SSRS reports. You can set up parameters and pass them through to the stored procedure.
I might be under-thinking this. What is wrong with: SELECT count(*) AS [total row count], count(distinct columnB) AS [Count of unique values in column B], count(distinct columnC) AS [Count of unique records in column C] FROM Zas3nfkb
Thank you for the headache. xD I named the table 'stringSplit' and used your original table's data. Hopefully you're using SQL's native string\_split; if not, the function/sproc you're using should function similarly. select s.col1, c2.[value] as col2, null as col3 into #t from stringSplit s cross apply string_split(col2, ',') as c2 declare @col2_min int, @col2_max int, @col3_min int, @col3_max int; set @col2_min = (select min([value]) as col2_min from stringSplit s cross apply string_split(col2, ',')) set @col2_max = (select max([value]) as col2_max from stringSplit s cross apply string_split(col2, ',')) set @col3_min = (select min([value]) as col3_min from stringSplit s cross apply string_split(col3, ',')) set @col3_max = (select max([value]) as col3_max from stringSplit s cross apply string_split(col3, ',')) while (@col3_min &lt;= @col3_max) begin update #t set col3 = @col3_min where col2 = @col2_min ; if (@col2_min &lt;&gt; @col2_max) begin set @col2_min += 1 end set @col3_min += 1 end select * from #t drop table #t
"A vendor installed the databases" ... on root .... both the data and log share the same directory ... why am I not surprised.
Yea I don't think there is one. You can use a limited set of features in the like statement. https://docs.microsoft.com/en-us/sql/t-sql/language-elements/like-transact-sql?view=sql-server-2017
Seriously?? ugh... Thank you very much.
You may have to use a variance of [PATINDEX](https://docs.microsoft.com/en-us/sql/t-sql/functions/patindex-transact-sql?view=sql-server-2017) (or [CHARINDEX](https://docs.microsoft.com/en-us/sql/t-sql/functions/charindex-transact-sql?view=sql-server-2017)) plus [SUBSTRING](https://docs.microsoft.com/en-us/sql/t-sql/functions/substring-transact-sql?view=sql-server-2017). I want to say there are regex functions and CLR out there which can be leveraged (or you can default to the app tier :P).
Use a CLR function. It's pretty easy to do: ``` using System; using System.Collections; using System.Collections.Generic; using System.Data; using System.Diagnostics; using System.Data.SqlClient; using System.Data.SqlTypes; using System.Text; using System.Text.RegularExpressions; using Microsoft.SqlServer.Server; /// &lt;summary&gt; /// compile using: /// C:\Windows\Microsoft.NET\Framework\v2.0.50727\csc.exe /t:library UserDefinedFunctions.cs /// https://docs.microsoft.com/en-us/sql/relational-databases/clr-integration-database-objects-user-defined-functions/clr-scalar-valued-functions /// CREATE ASSEMBLY CommonFunctions from 'C:\Program Files\Microsoft SQL Server\CLR\UserDefinedFunctions.dll' WITH PERMISSION_SET = SAFE; /// &lt;/summary&gt; public partial class UserDefinedFunctions { private const RegexOptions Xms = RegexOptions.IgnorePatternWhitespace | RegexOptions.Multiline | RegexOptions.Singleline; private const RegexOptions Xmsi = Xms | RegexOptions.IgnoreCase; // CREATE FUNCTION RegexReplace(@input NVARCHAR(4000), @pattern NVARCHAR(4000), @replacement NVARCHAR(4000)) RETURNS NVARCHAR(4000) AS // EXTERNAL NAME CommonFunctions.UserDefinedFunctions.RegexReplace // GO [Microsoft.SqlServer.Server.SqlFunction()] public static SqlString RegexReplace(SqlString input, SqlString pattern, SqlString replacement) { if (input.IsNull || pattern.IsNull || replacement.IsNull) return SqlString.Null; return Regex.Replace(input.Value, pattern.Value, replacement.Value, Xms); } } ```
you're my hero i'm going to study what you've written here to make sure i understand it thank you! i'll keep you posted
It depends. A) Check your WRITELOG wait stats are see what they tell you B) Check your VLF count C) Check your read/write ms for the log file(s) If they all check out; size shouldn't matter. Curious: What's the allocated to unallocated space in your bigger log files?
&gt;regex I appreciate the code snippet and thanks for the samples in your repo. C# doesn't scare me off, but the fact that explicit paths are used is a problem - the operation must be carried out in multiple environments where nothing is consistent. Are there environment variables laying around or any way to deal with environment variations automatically? If you have a code snippet, a hyperlink for a response would be great!
FROM Zas3nfkb WITH NOLOCK ??? /s Also I am having difficulty with the original request. So A is all rows in the table, B is a count of distinct **values** in column B, C is a count of distinct **values** in column C? So for your table: Table: A B C D OR Q1 FO PO AC B1 FO ST LE FO GR DB B1 ES A = 4, B = 2 C = 1?
The path is unnecessary except for the first install. Do that on a test server. From there, generate the assembly bytecode SQL from that first import, and then just execute that bytecode script on all your servers. No dlls/paths necessary after that. ``` CREATE ASSEMBLY [RegexFunctions] FROM 0x123456789etcetcetcblahblah WITH PERMISSION_SET = SAFE ``` Read the directions in install.sql.
The path is unnecessary except for the first install. Do that on a test server. From there, generate the assembly bytecode SQL from that first import, and then just execute that bytecode script on all your servers. No dlls/paths necessary after that. ``` CREATE ASSEMBLY [RegexFunctions] FROM 0x123456789etcetcetcblahblah WITH PERMISSION_SET = SAFE ``` Read the directions in install.sql.
Think of groups as buckets, everything in the bucket has to be aggregated (sum,count,avg,etc). Select company, office, sum(employees) From table Group by company, office If you only group by company, then asking for a list of offices makes no sense since all the employees are in the company bucket and not also separated into office buckets. There are some databases that will let you skip some columns in your group by. I don't trust them. Programmers usually don't like when actions are hidden from us. They can be correct and helpful 99% of the time, but when they go wrong, thenyou can't find it. And sometimes if you find it, then it can't be fixed until the the third party company pushes an update.
This worked! I swear I tried this first before I started googling it but I think i was just over complicating it. Thank you!
It did work! I think I was over complicating it! Thank you!
You can think of it as Group By finds values in a column that are the same and groups those values together, then an aggregation function can performed on any of the columns. The default is to collapse the group and return the unique values from that column. If you Group By multiple columns it will collapse to the unique value combinations of each column. So if you did a GROUP BY State you would get 50 rows returned each being one of the 50 states. If you did GROUP BY State, City you would get a row for every unique city state combination. I also wrote an article a while back that visualizes with gifs the group by and aggregation process which might be useful for you: [https://dataschool.com/how-to-teach-people-sql/how-sql-aggregations-work/](https://dataschool.com/how-to-teach-people-sql/how-sql-aggregations-work/)
Just as a protip, I find even after 25 years of writing SQL that it always helps to think in terms of "doing this thing on paper." That is, imagine each row in your tables is a piece of paper, and somebody slides questions to you through a mail slot and your goal is to send an answer back as efficiently as possible. Say you had daily average temperature data for 100 major cities over the past 12 months. 36,500 pieces of paper, each representing the average temperature for a single city for a single day. if someone asked you to tell them what was the monthly average daily temperature in all of the cities, you'd * divvy the rows up into 12 piles, one per month - that is, you GROUP BY month * look up the daily temperatures on each sheet of paper, sum them up, and divide by the number of papers to get the average daily temperature for that month * write that down * repeat until you had each month's daily average temperature and then send the answer back. Now if I asked you to give me the monthly average daily temperature for each city, could you do that with your existing 12 piles? No, you'd have to divide your data into 1200 piles, one for each month in each city. You should hopefully notice that the number of piles corresponds to the number of rows you need to return - which is the number of unique combinations of your GROUP BY predicates. For months, that's 12. For cities, that's 100. And for months and cities, that's 12 * 100 or 1200. Anyway, thinking in terms of "paper pushing" really helps me square things like data modeling, set theory, windowing and partitioning, complex joins ... It even extends to things like Hadoop and Spark, Pandas and Scikit-learn, and OLAP and Gremlin queries.
Whew. This is far beyond my ability level. Column 2 and 3 have no relationship. It's the same as then being text. In fact, some values are yext. This is an interesting formula to try. I am not using the built in string split but I made my own... I tried to use a built in one but it wasn't recognized. This could be an error on my own part. I'm not very experienced in sql
&gt;In 2001 Oracle came out with RAC, in 2002 Advanced Queueing. These are HA and replication functionalities. &gt; &gt;RDBMSes like DB2 on the mainframe have had cluster and failover abilities for years Oracle RAC and IBM DB2 pureScale fall in the **shared-disk** category whereas NoSQL/NewSQL data stores fall in the **shared-nothing** category. There's certainly a lot of debate on *shared-nothing vs shared-disk* data systems. But the NoSQL/NewSQL shared-nothing category is a growing market, as is evident from the numerous successful products and companies serving it. Also, deploying Oracle RAC, IBM DB2 in the public cloud is not straightforward. [AWS](https://aws.amazon.com/articles/oracle-rac-on-amazon-ec2/) and [Azure](https://docs.microsoft.com/en-us/azure/virtual-machines/workloads/oracle/oracle-overview#high-availability-and-disaster-recovery-options) themselves recommend third-party vendors like [FlashGrid](https://www.flashgrid.io/) (which have their own sort-of shoehorned solutions) for running Oracle RAC in public clouds. Deploying on public cloud (or commodity hardware) vs custom data appliances designed for vendor-specific clusterware is another discussion. &gt;if you think that "that's it" in terms of functionality in 2019, then you don't know the industry. In 2019, yes. Check out this [graphic](https://www.nuodb.com/sites/default/files/graphics/conceptual-diagrams/database-decision-tree_text_FINAL.jpg) from NuoDB. It's not 100% accurate (as it's marketing material) but is nonetheless helpful. &gt; PostgreSQL 9 had hot standby in 2010. NoSQL had already taken off by 2010. And RDBMSes &amp; NoSQL/NewSQL do not compete on the hot standby feature. Hot standby is a must-have for any DB solution. &gt; Traditional RDBMSes have been able to maintain high availability long before anyone dreamed of cloud based services. Of course, they typically did this on custom appliances. Shared-nothing NoSQL/NewSQL databases run on commodity hardware. These databases provide first-class support for multi-region deployments while maintaining high availability. That is not to say that Oracle and IBM DB offerings have lost their charm and the DB-Engines popularity rankings (linked in the article) is evidence. &gt;Your article gives the wrong impression about the state of the database market, there is no historical knowledge. The article is intended to be a summary of the current shared-nothing NoSQL and NewSQL market.
Exactly. If you group on something it will only return one row for each value in that column. Anything that you don’t want grouped on each distinct value must be aggregated in some way.
This is a really good point that I forgot to mention, and I think it really helps express the concept best. Whatever you group on, will be distinct.
It is possible to group by one column. You just need to specify how to aggregate the columns you are not including in the group by. Ex. SELECT name, max(date), max(age) FROM persons GROUP BY name This will give you one row for each name and the max date and max age for each name.
1. update &lt;MySQL table name&gt; m inner join &lt;salesforce table name&gt; s on m.name = s.name set m.account = s.account This is with assuming name is the unique id 2. update &lt;MySQL table name&gt; m inner join &lt;salesforce table name&gt; s on m.id = s.id set m.account = s.account This is with assuming id column is the unique id
Look at using the GROUP BY clause.
You need a sum or count function in your query for group by to work.
SQL is a very important skill to have, it transfers across jobs and languages. SQL is also very powerful, compared to some ORM like Eloquent. I recommend sql skills for any manager or other person dabbling with excel or similar tools. If you really, REALLY had to use an orm, sqlalchemy is the most powerful one ive used. I think its the only orm i never had to drop down to raw sql. That said vanilla sql is a valuable skill to have and it is the first thing i ask about in job interviews.
You're arguing on Reddit. Fix the article.
So you’re correct: a) payment or balance is a $ number b) @currentmonth is the last day of the most recent completed month (I.e. May 31) and so @lastmonth would be April 30 and c) you pointed out a great flaw, in that I’m incorrectly equating balance $ to date by using the variable the way I did above. So overall my issue is that I have declared two dates (like the ones mentioned above), and have selected 15 fields from a table and my where clause is set to @currentmonth because most of the 15 fields are as of that date. However, a few fields like balance, interest, fees (all $ values) are ones I need as of the date @lastmonth. My thought process was that first I declare the dates I need as variables and then I’m stuck in how to select a $ field to return a $ value as of the @lastmonth date. I hope that makes sense.
&gt; the two statements in the FROM alone get the correct numbers alone i think you're wrong... you've got an implied cross join going on in each subquery, so those counts will be wildly inflated -- when re-written as inner joins, you can see the join conditions are missing i've re-written your query using explicit JOIN syntax (and fixed the TEMP(T) stuff which i think is wrong) note your use of cross joins for the two counts is correct, as the subqueries produce only one row each SELECT S.LName FROM student S INNER JOIN class C ON C.CNumber = S.CNO CROSS JOIN ( SELECT COUNT(*) AS cnt FROM student S INNER JOIN class C ON ???? WHERE S.GENDER='M') AS T CROSS JOIN ( SELECT COUNT(*) AS cnt FROM student S INNER JOIN class C ON ???? WHERE S.GENDER='F') AS T1 WHERE T1.cnt &gt; T.cnt
I haven't noticed a performance difference based on size myself. The actual size of the LDF is generally larger than the data inside of it. Do they really need to be that big? If your backup plans are good its likely you are only using a small fraction of the reserved space for those large LDF files. &amp;#x200B; I did this at my job recently, our log file for our main db was 150gb+ but we run a log backup hourly so it would never go above 5% usage. I did a full/transaction backup and then immediately shrunk it to 50gb after. There was no noticeable performance change, however the backup and restore process runs faster due to the decreased log file size. &amp;#x200B; If you do find that you want to decrease the log file sizes, make sure you some wiggle-room. If the log file has to auto-grow, it will have a very large performance impact while growing.
its a duration that someone was doing something. Big Query - Yeah but I dont get it! haha
the rules of group by are as follows: the columns in the select can only be: the grouped by columns aggregate functions you cannot bypass this rule. you cannot add a coumnd that is grouped and then another column that is not grouped. when you think about it it makes sense. think about it that on grouping a single row represents many rows. so if you group by name and you have many persons named Dan they will be represented by one single Dan. In that single Dan what date should the DB put? there are many.... it has to choose one, which one? this is why you aggregate the date for example max(date) / min(date) ... etc...
One way of skinning the cat would be SELECT sum(case when date = @lastmonth then balance else null end), Sum(case when date =@currentmonth then payment else null end) From table Would give you total balances as at last month and total payment at current month. Not sure I completely understand your data, if this doesn't help please post a sample few rows of the table you're working with.
Maybe this video might help. 127 likes and 2 dislikes, I think it working for majority of the folks. https://youtu.be/rWW-j04cZQI
Your site looks great! I am learning SQL now for a job and this’ll help immensely.
Just an update, but it's not going well :/ Honestly, what's happening kind of defies my whole understanding of how right joins work. I've looked through a ton of examples, and all of them indicate that this should work, yet I just get back the same 15 rows for my test customer/salesperson without the additional ones where all lines are null except customerno, salespersonno, and category1. Here's my from statement. I've confirmed that #categories has one of each category for every possible combination of salesperson and customer, including the one I'm using as a sample. #shd is just a couple joined tables that has all the sales orders broken into line items, and most of the other joins after that are just for reference information. I've commented out the "where" from this statement for testing to make things simpler and easier to understand. FROM #shd LEFT JOIN AR_Customer c ON c.CustomerNo = #shd.CustomerNo AND c.ARDivisionNo = #shd.ARDivisionNo LEFT JOIN AR_CustomerContact cc ON c.contactCode = cc.ContactCode AND c.customerNo = cc.CustomerNo AND c.ARDivisionNo = cc.ARDivisionNo AND cc.ContactCode IN ( '01', '1' ) LEFT JOIN AR_SalesPerson s ON s.SalespersonNo = c.SalespersonNo AND s.SalespersonDivisionNo = c.ARDivisionNo LEFT JOIN CI_Item i ON i.ItemCode = #shd.ItemCode right outer join #categories on #categories.CustomerNo = c.CustomerNo and #categories.SalespersonNo = s.SalespersonNo
I think it's nuts that there are so many adapters replacing SQL with something "better".
I'm likely way off base, but I'm pretty sure you can import it as string data?
I see this pattern all too often. It goes something like: 1. Developers choose MySQL because reasons. 2. Once the project is far into production they run into countless issues. 3. As the resident database expert (or closest thing to one), I get asked how I can fix such-and-such issue. I look into it and think, well if we had Postgres we could take advantage of such-and-such feature or we wouldn't have to scan the same table repeatedly because we could use a CTE in this query, or if we had Oracle we could use a materialized view that automatically updates incrementally instead of this hacky solution that keeps breaking or so on and so forth... 4. Developer generalize this problem to "**SQL** can't scale" or "**SQL** can't handle our application" - we should look into maybe a NoSQL solution such as Mongo or even a GraphDB. 5. Me: [https://i.imgur.com/iWKad22.jpg](https://i.imgur.com/iWKad22.jpg)
So the table has 432872638 records of which I need to delete 293964985 records. I am unable to clone or duplicate the table as this table is filling our data drive on the production database server: /dev/sdb1 ext4 197G 176G 12G 94% /var/lib/pgsql Do I have any options here on deleting the above set based on a timestamp without time zone column?
Well written introduction to SQL.
Thanks for the website! I have been dabbling and this will help a lot.
This is an awesome link! I'm just now learning SQL and realizing it's importance in finance.
I would load the json into python pandas and create a dataframe, then use an ORM like sqlalchemy if it supports your database
MySQL (which you error message reveals is your platform) doesn not support UPDATE FROM syntax could you please explain why you need a RIGHT JOIN? then i'll try to re-write your statement in MySQL syntax
Thank you! Basically, both tables are to have the same columns, rows, and values. However, I wanted to select and isolate the rows where the KeyAccountCode differed between the two. So, generally the flow of data goes from customerdatasql -&gt; customerdatasf. However, there currently isn't something to overwrite data in customerdatasql with data in customerdatasf if an employee went into customerdatasf and changed the KeyAccountCode.
OOOF!! could you please elaborate on this -- "#categories has one of each category for every possible combination of salesperson and customer" in order to narrow things down, i would remove "other joins after that are just for reference information" figure out the categories situation first
i'm sorry but that paragraph doesn't make sense to me -- it might make sense, but not to me let me try to translate your statement based only on syntax UPDATE customerdatasql INNER JOIN customerdatasf ON customerdatasf.CustomerNo = customerdatasql.CustomerNo SET customerdatasql.FreightLevel = customerdatasf.FreightLevel , customerdatasql.FreightAmount = customerdatasf.FreightAmount , customerdatasql.FreeBOShipping = customerdatasf.FreeBOShipping , customerdatasql.KeyAccountCode = customerdatasf.KeyAccountCode WHERE customerdatasf.KeyAccountCode &lt;&gt; customerdatasql.KeyAccountCode
It’s something more extensible rather than better.
After doing some more research after you stated that MySQL cannot do UPDATE FROM, I actually restructured the query and I got ALMOST the same as you! Thank you very much for your assistance!! and I apologize for my paragraph not making sense. To be concise, I am just trying to overwrite incorrect values in the customerdatasql table with the correct values from the customerdatasf table.
We all know its Pronounced Sea-Quail
(Transactional Replication)[https://docs.microsoft.com/en-us/sql/relational-databases/replication/transactional/transactional-replication?view=sql-server-2017] should work fairly well for that. We actually have that same set up but in reverse, taking data from a 2016 server and moving it to a 2008 R2 server (at least until we can kill the 2008 server) along with several others.
For real though does anyone really say "S-Q-L?" At a previous company I worked for, we had some contractor come in work on some project and when I said "Sequel" he was surprised that anyone my age (I was like 24 at the time, so pretty young) still called it that, and that everyone really pronounces it "S-Q-L" now. Since then I've literally never met another human being who calls pronounces the initials.
In my 16+ years of experience as a developer with a focus on databases/sometimes-DBA, I have never once seen a database dump in JSON format. So I would say it's very weird that anyone is making you handle this format. It sounds like you are able to convert this into CSV, which would be much easier to load into a database. Most databases have a LOAD or COPY command or similar that would do the job. If you are using SQLLite, here is some documentation: [https://sqlite.org/cli.html#csv\_import](https://sqlite.org/cli.html#csv_import)
Are test cases supported within SSMS? Right now I haven't quite got VS figured out unfortunately, although I'd like to test cases in the future. Kind of a moot point for now though, as I can't really share any of the data in question (although I appreciate the effort) &gt; could you please elaborate on this -- "#categories has one of each category for every possible combination of salesperson and customer" The easiest way to explain this is to say that the list that I need to add categories to exists in a report where each page is one customer and groups of pages are organized by salesperson. So, I need to have a row where all values are null except salesperson, customer, and category for each category not represented in the salesperson's sales for that customer. (If I do achieve this, it will produce the desired behavior in the report with no further changes.) To achieve that, I'm trying to right join the whole query to a temp table that has cross-joined the every customer/salesperson combo with the list of categories like so: SELECT * INTO #categories FROM ( SELECT CustomerNo, SalesPersonNo, Category1 FROM ar_customer c CROSS JOIN ( SELECT DISTINCT category1 FROM CI_Item WHERE category1 NOT IN ('UNCATEGRZD') ) i ) AS "CategoriesCrossed" Which results in, for example: CustomerNo SalesPersonNo Category1 0002008 JL TREAD 0002008 JL SAMPLE 0002008 JL ACCESSORY 0002008 JL ROLLFLOOR ... for every combo of customers, salespeople, and categories. If it's still confusing, I can elaborate more later. I currently working on other parts of the report while tackling this problem in a copy of the SP, but I'm going to be looking at it again tomorrow. It may help to know that this aspect of the report isn't the most logical way to do reporting, but I'm under pressure from pretty high up to do it this way and I've already had the conversation. The argument is that they want the categories that have no sales for the customer in question listed in the vertical column with the items that do have sales that "the salespeople can see what categories of item we're not selling to that customer," although it's not that long a list and it's not exactly confusing to note that anything not listed isn't being sold to the customer ¯\_(ツ)_/¯
You dropped this \ *** ^^&amp;#32;To&amp;#32;prevent&amp;#32;anymore&amp;#32;lost&amp;#32;limbs&amp;#32;throughout&amp;#32;Reddit,&amp;#32;correctly&amp;#32;escape&amp;#32;the&amp;#32;arms&amp;#32;and&amp;#32;shoulders&amp;#32;by&amp;#32;typing&amp;#32;the&amp;#32;shrug&amp;#32;as&amp;#32;`¯\\\_(ツ)_/¯`&amp;#32;or&amp;#32;`¯\\\_(ツ)\_/¯` [^^Click&amp;#32;here&amp;#32;to&amp;#32;see&amp;#32;why&amp;#32;this&amp;#32;is&amp;#32;necessary](https://np.reddit.com/r/OutOfTheLoop/comments/3fbrg3/is_there_a_reason_why_the_arm_is_always_missing/ctn5gbf/)
Why three? Why these three? Why not 5? Why not one? These are very very arbitrary levels of aggregation with no reason to use them. If you have a list of events, you only need the rest if you have exactly that kind of report. 3 tables that almost nobody needs
Redshift is not a relational database engine.
*Every development analyst.
Never heard anyone pronounce it anything but S-Q-L here in Poland, but we use our own letter pronunciation (no "i" in letter Q).
dude, what about a [numbers table](https://www.red-gate.com/simple-talk/sql/database-administration/creative-solutions-by-using-a-number-table/)
 SELECT DISTINCT category1 FROM CI_Item WHERE category1 NOT IN ('UNCATEGRZD') i'm sorry, but this only gives you categories which at least one item has it will ~not~ give you categories that don't have any items
Sorry for the confusion, for reference CI_Item is just a table that has every item that sell listed in it with some basic info about the item, so pulling the distinct categories does list all our categories that currently exist. #shd (short for salesorder header / detail) is the table that contains our actual sales orders, which needs to be padded out to include rows for categories that don't exist as sales orders for any grouping of customers/salespeople. I may have bitten off more than I can chew in trying to summarize this particular issue, there's a ton going on in this query / report. This evening I might come back and try to disambiguate some of it
I hear both often, I say "sequel" because it's easier unless I'm saying MySQL in which case I say "My S-Q-L" for some reason...
What langauge do you speak?
I read that the Polish alphabet doesn't have a Q, do you actually say an english Q or do you substitute another letter or sound?
English, but deal with foreign contacts in Mexico, India, and China regularly with mixed results from each region--no real consistency for each region that I've noticed.
I'd imagine a K / Kw sound, pronounced similar to the beginning of the English word "Cool", but my Polish is second hand and I'm not a native speaker... just live / socialize with native speakers.
So mostly S-Q-L or sequel from them? I'm assuming you communicate with them in English? I'd really like to know what they would say while speaking in their native languages.
Tried it and I got blanks as a result, but no errors, so I think we may be close. The one referencing “last_principal” uses your solution. Here’s the whole query below: in general, it’s a financial statement that requires current principal balance (easy to do since the where clause is set to current month) and last principal (needs to pull balance but as of @lastmonth) Declare @currentmonth DATE Declare @lastmonth DATE Select @currentmonth = eomonth(getdate(), -1) Select @lastmonth = eomonth(getdate(), -2) Select vdp.participation_pool_name as Buyer, convert(varchar,dateadd(d,-(day(getdate()-1)),getdate()),106) as Run_Date, vlm.effective_date as Effective_Date, vlm.AKUSA_Account_Number as Loan_Number, case when vlm.Effective_Date = @lastmonth then vlm.balance*0.90 else 0 end as Last_Principal, &lt;returns blanks&gt; CASE WHEN vlm.loan_close_date IS NOT NULL AND vlm.balance = 0 THEN 0 WHEN vlm.due_date &lt;GETDATE() THEN DATEDIFF(d,vlm.due_date, GETDATE()) ELSE 0 END AS Days_Late, &lt;this subtracts dates correctly&gt; round(vlm.balance*0.9000, 2, 1) as Curr_Principal From dbo.view_fact_Loan_Monthly_Snapshot vlm Where vlm.Effective_Date = DATEADD(MONTH, DATEDIFF(MONTH, -1, GETDATE())-1, -1) — the above clause also works if I use vlm.Effective_Date = @currentmonth
Honestly, I think it depends on their English proficiency... the Q sound isn't the same / doesn't exist in many languages so non-native English speakers have a hard time pronouncing it... so they may use "sequel" because it's easier to pronounce. But this is all just anecdotal from me working with other IT professionals in these countries / languages, specifically for database / software development and testing.
&gt; so pulling the distinct categories does list all our categories that currently exist. sorry to keep hammering away at this, but your original post said you wanted all categories
&gt; so pulling the distinct categories does list all our categories that currently exist sorry to keep hammering at this point, but your original post said you wanted the report to include "categories that don't have any items"
It's great though! I'm not sure there exists anything but anecdotal evidence for this question, so your answers are definitely helpful.
The reason why the previous years are coming up blank is your where clause is excluding those records. But now I understand the problem, I'd suggest a completely different approach: I assume there is only one record per loan number per day? If so, I'd tackle it as a self join, e.g., SELECT curr. Balance, prev. Balance FROM view_fact_Loan_Monthly_Snapshot curr LEFT JOIN VIEW_FACT_LOAN_MONTHLY_SNAPSHOT prev ON Curr.AKUSA_Account_Number = prev. AKUSA_Account_Number AND prev.effective_date = @previousmonth WHERE curr.effective_date = @currentmonth This way you've got the table in there twice, with the version aliased as curr returning the data as at @currentmonth and the one called prev returning the data as at @lastmonth
Had a Chinese (Shanghai) coworker that would say 'S-Q-L'. He was cool I miss that guy :)
Were you actually working in Shanghai, or was it just that he was from Shanghai? What language were you two communicating in?
Can it all be overhauled? Trash the application all together? Also did you buy the third party app without the source code for it?
Well no, they didn't provide source code with their program. The company still exists, but we need a fix before they can patch the bug. And to say it's "integrated" with our main application is a complete understatement. It's so highly customized that the buttons the users click are identical to the ones provided by the application. It's not seamless, but if you don't know what you're looking for it would appear so. The back end resides literally in the same database as the main application. When we remove deprecated parts, we delete tables from the main application database. Not to mention reporting, ETL, web services, etc. All these other pieces are tied into the business in one way or another. So there is no trashing or overhauling or anything of that possibility it in the near future. Most likely we'd be looking at replacing the main application that runs our entire business. Which is a multi-year project, if it ever has a chance of happening. Let's put it this way, the only way it would be possible to consider replacing it is if bugs cause us to lose funding. Which would be an extremely painful process, but probably necessary.
That sounds like it was put together really quickly to meet deadlines back in ‘02 and now you’re paying for it. Literally &amp; figuratively. You now only have two options. 1. Ask for a raise 2. Shit on your bosses desk everyday in the middle of a meeting as a show of dominance. 3. Both 1 &amp; 2.
Not quite, because cool AFAIK has a bit of "e" after "k". In Polish it is just "k" and "u" with nothing subtle in-between. Our "q" is pronounced like "Ku" in Ku Klux Klan (sorry couldn't find any alternative example).
That is correct, but like many others we borrow some words or even letters (like X) from other languages and come up with similar pronouncation but better fit to what we are used to. Q is seldom used, but known by most.
Try this: `select * into Projects_Backup from Backup`
Legend, cheers.
It's a UAT which will be dropped after a quick fix has been resolved.
Note: is it also possible to write such a query? Select * from tableA having (distinct colA); Thanks!
 SELECT t.* FROM ( SELECT colA FROM tableA GROUP BY colA HAVING COUNT(*) = 1 ) AS x INNER JOIN tableA AS t ON t.colA = x.colA
&gt; And that developer decided to use integers for monthly income. but... why... ever... We're currently working on moving a large project in house that was developed in partnership with another company, but it was all done in PowerBuilder which none of us use, and I'm pretty sure the software generated the DB. The whole thing is a bit of a nightmare. None of the tables have foreign keys, and there are almost no constraints on important fields, except where they're a major pain. One of the ID fields is a single digit, so we can only have 10 entries in that table and we're already pressed up against it with hints we'll need more soon.
This is old but I think it can't hurt, and will help you sharpen your skills. I got certified back in 09, and have been working off and on with SQL since then (and before I took it as well). Looking to get a more recent SQL cert soon.... figure it's not insanely expensive, and forces you to sharpen up on some topics you may not be fresh on otherwise. At the end, you get to plop it on your resume, and combined with experience it certainly doesn't hurt.
Oh good catch. I meant RDS: [https://aws.amazon.com/rds/](https://aws.amazon.com/rds/)
Great to hear!
Yeah I was really surprised by how many fields it touches. Definitely a good technical skill to know
Does :’[%0]’ or ‘:[%0]’ work? Is it giving you an error message?
Not sure what you're asking. Do you just mean: Select ColA From TableA; That will totally get you all values of ColA, dupes and everything.
I don't think it's far beyond your ability at all. SQL's native string\_split() will treat the items as text and without relationship. You're the one who defines the relationship(s). You're probably on a version of SQL Server (or CU) which doesn't support string\_split natively (compatibility level = 130). In a nutshell: taking the minimum and maximum of both split columns and incrementing them while they are &lt;= (and setting max=max). Note: The native 130 string\_split should function identically to the sproc version &lt; 130
Select all rows and columns from tableA, where colA is distinct Im not sure how to write this...
Ok, so still not following. If you've got TableA as follows: ColA ColB 1 abc 1 cde 1 def 1 efg 2 abc 2 bcd 2 cde 3 abc Then SELECT Distinct ColA from TableA will result in: ColA 1 2 3 So if you then want all rows where ColA is distinct, you're essentially getting the ENTIRE table! It's not different than SELECT * FROM TableA;
Would storing values as cents rather than dollars help? That way the rounding errors would be on the order of cents rather than dollars. Would still mean a few changes but perhaps a manageable amount? Obviously a really bad fix but maybe enough to get you through the audit?
Ever heard of [fixed-point arithmetic](https://en.wikipedia.org/wiki/Fixed-point_arithmetic)? Usually uses scaled integers for money. A careful database designer would probably have used a decimal(p,s) data type for money, even 20 years ago. In 1991, I was using at least one dbms that represented money using scaled integers, and a couple that used decimal(p,s).
Still giving me an error message.
&gt; over-normalizing a database is absolutely a thing this reminds me of something I came across at work the other day. I was helping a coworker with a task in a database I'm not *that* familiar with. I came across a table that had three columns, PK column and two FK columns. I stared at it for a second before going about my business because what I was trying to accomplish had nothing to do with this table. I could not imagine a scenario where having a table like this makes sense.
What is the error?
This going to be one heckin' heck of a learning experience for you.
Isn't there a way in SQL Server to just generate numbers dynamically? Most of my experience is in Oracle and Postgres - in the former case I'd do something like "SELECT LEVEL FROM DUAL CONNECT BY LEVEL &lt; 1000" for numbers 1 to 1000, and in the latter I'd just call generate\_series(1,1000). It's strange to me that they are recommending inserting potentially billions of rows just for generating numbers.
Given this table, what are you expecting as the returned set? If you want to pull in both columns, where A is distinct, which row are you expecting it to return? ColA | ColB ---|--- 1 | a 1 | b 1 | c 2 | a 2 | b 3 | a
1). [SAP AG][LIBODBCHDB32 DLL][HDBODBC32] Syntax error or access violation;257 sql syntax error: incorrect syntax near "ADD_YEARS": line 1 col 1 '' (ECM2) (at pos 1)
Squill*
Does it need to be SELECT add years? Or something else that goes before it?
The problem isn't that the value was an integer, per se, and an integer would work perfectly fine... the problem is that they are entering in annual income, then dividing by 12 to get the monthly, then passing the monthly value. If they were just passing the annual value and letting /u/Cal1gula derive the monthly from that then there wouldn't be a problem. On the surface I see no problem with having annual income stored as an integer, and from a UX/UI perspective you would probably want your front end to work with integers so that the end user couldn't input an annual income of €100,000.0019867.... is that coming in as a varchar, is the user allowed to enter symbols... are they restricted to only entering two decimals? Why do you want decimals (cents) for annual income? Even still if you do take an annual income as an integer, divide by twelve, and pass that value to the database as an integer the maximum variance you can have is approximately $6. Kind of makes you think what or how you would fail an audit, or how % discounts based on income would really vary much over a maximum variance of $6/year. I suppose the issue might be from cases where a person makes enough to qualify for X%, but because of the variance it shows that they technically only qualify for Y%. My bet is that this would, at most, impact less than 1% of all accounts. Is it poorly designed? Yes, but not because of the integer, because it's doing the logic at the application level instead of in the database --&gt; which is actually even more apparent when you consider the fact that they can't change it because it's a legacy application. So basically these poor fucks might: 1. Possibly lose seven figures. 2. Possibly have to rewrite everything from the ground up. 3. Possibly lose their funding entirely. All because someone thought it was a good idea to do the logic in the application instead of just passing the data raw to the database.
The problem isn't that the value was an integer, per se, and an integer would work perfectly fine... the problem is that they are entering in annual income, then dividing by 12 to get the monthly, then passing the monthly value. If they were just passing the annual value and letting /u/Cal1gula derive the monthly from that then there wouldn't be a problem. On the surface I see no problem with having annual income stored as an integer, and from a UX/UI perspective you would probably want your front end to work with integers so that the end user couldn't input an annual income of €100,000.0019867.... is that coming in as a varchar, is the user allowed to enter symbols... are they restricted to only entering two decimals? Why do you want decimals (cents) for annual income? Even still if you do take an annual income as an integer, divide by twelve, and pass that value to the database as an integer the maximum variance you can have is approximately $6. Kind of makes you think what or how you would fail an audit, or how % discounts based on income would really vary much over a maximum variance of $6/year. I suppose the issue might be from cases where a person makes enough to qualify for X%, but because of the variance it shows that they technically only qualify for Y%. My bet is that this would, at most, impact less than 1% of all accounts. Is it poorly designed? Yes, but not because of the integer, because it's doing the logic at the application level instead of in the database --&gt; which is actually even more apparent when you consider the fact that they can't change it because it's a legacy application. So basically these poor fucks might: 1. Possibly lose seven figures. 2. Possibly have to rewrite everything from the ground up. 3. Possibly lose their funding entirely. All because someone thought it was a good idea to do the logic in the application instead of just passing the data raw to the database.
I'm struggling to convert the dynamic query to a non-dynamic one. Can you show me what the non-dynamic version would look like?
What would the non-dynamic version of the SQL look like?
Can you give an example? Sorry, I'm new to SQL.
[https://support.microsoft.com/en-us/help/308656/how-to-open-a-sql-server-database-by-using-the-sql-server-net-data-pro](https://support.microsoft.com/en-us/help/308656/how-to-open-a-sql-server-database-by-using-the-sql-server-net-data-pro)
This is the right answer!
The stored procedure, the C# code, or both?
The stored procedure. I was able to make sense of the C# code :)
I'm not suggesting you need only three, but if you're an analyst at a tech startup, these three are likely to very useful. There are definitely some caveats, for example this table structure particularly make sense in cloud data warehouses like Redshift and BigQuery. I chose to leave that complexity out of the article title though.
What's a development analyst? I've never heard this phrase before, I'm interested!
Yeah fair point, a numbers table can be useful too. As I mentioned in another comment, these 3 are just a subset of the useful tables for analytics. That said, I wanted to focus on slightly more user focused tables. &amp;#x200B; I'm so glad BigQuery makes it easier to generate a numbers table! `select number from unnest(generate_array(1,10)) as number`
Ok, I can go into a bit more details now. So everybody has some sort of "Events" dataset. Be it website sessions, user interactions, offer e-mails, call center interactions, or all together combined. My first gripe is in using an array or CSV of actions per user. That is incredibly hard to query, especially by 3rd party visualization tools. You might as well not have that field at all, since it just makes things slow unless you store it in columnar store fashion. So you should either have 1 row per individual event, or summarize the interaction without encoding details like that. Why summarize per user or per day or both, or other aggregation levels? Only if the requested reports are fine with that. The trade-off is that it takes time and space to compute these tables, and in a lot of cases it's not worth it. It's easier to have 1 detailed data set and offer it to users as one thing, versus many things to learn. Once the data user learns the structure of this dataset, it will be much easier to query than when you have to explain to him/her "Yeah, for this you need the other table.". Sure, having one big table has its drawbacks, like making reports on high levels of aggregation slower than when you have pre-aggregated tables. Caching helps, and proper data infrastructure makes things fast. "Proper" meaning whatever fits the use case well. A lot of cases warrant the use of ELT rather than ETL. If someone builds a report 1 time per week, it surely doesn't make sense to compute it 5 times per day.
i think OP meant distinct = unique = occurs only once so `3 a` only see my earlier reply
This is nice.. Little sniplet of info
Thanks for your reply. This file is coming from the PM of a client of mine, who I haven't found to be particularly helpful in general. I ended up converting the file to a csv and then importing it into SQLite yesterday before I read your message. I'm still left with the question of what to do with a .sql file, though. If I have a file like this and want to query it locally, what is the best solution? As a side note, this csv ended up with null values that clearly should not have been null: this is ecommerce data, and only one item in each order would have an order number or shipping data. I think this was the result of the intermediate csv step, because, browsing the json as a text file, it looks like the hierarchy it contains preserves this information. But I'm definitely not familiar with these filetypes, so maybe that's wrong. Anyway, I fixed this within the csv, so hopefully everything's alright, and now I have real data that I need to analyze to use for practice. What I haven't seen yet is how to save this information to a database file on my computer within SQLite, but I'm optimistic I'll be able to figure that out on my own.
 CREATE PROCEDURE [dbo].[setDescription] @product int, @descript nvarchar(255) AS BEGIN UPDATE dbo.products SET Description = @descript WHERE ProdID = @product END
So do it in a non-production environment where you can restore easily from backup. These things often get forgotten and never dropped. On SQL Server, I'd think about using a database snapshot to revert the whole thing after the testing is complete.
I saw HAVING being used. That I believe is correct. You may use it to get records where the count is greater than 1 for a particular column, telling you that the value is not singular / distinct
Depends on who I'm speaking to, I alternate between both, been working with sql now for 20 years, I find fellow developers are more responsive to "sequel" but none developers seem to appreciate SQL more, its a strange one
Sequel. It’s faster and easier to say, and that’s how the SQL Server community refers to it. I suppose if I did more MySQL I would feel differently since they tend toward Ess-Queue-El. And the Postgres folks just punt.
See here: [https://ocelot.ca/blog/blog/2013/09/23/how-to-pronounce-sql/](https://ocelot.ca/blog/blog/2013/09/23/how-to-pronounce-sql/)
It's alright, it's hard to contextualize. The CI_Item table is a lookup table that contains some information about every item we sell, including a column containing the categorization of that item. But the report I'm writing is a sales report aggregated by customer, salesperson, and item categorization. When I say the items don't exist, I mean that no items with that that categorization exist as sales for the salesperson and customer being reported on, so they end up omitted from the list. So as a simplified example, a report might look like this: --- Salesperson: John Smith Customer: Bill's Supplies, LLC Item Sales -------------------------------- Rail --- Long rail A6 4 Medium rail B12 6 Short rail A9 2 Lumber --- 2x2x15 8 2x4x8 25 But the request is for it to look more like this: --- Salesperson: John Smith Customer: Bill's Supplies, LLC Item Sales -------------------------------- Rail --- Long rail A6 4 Medium rail B12 6 Short rail A9 2 Boxes --- Lumber --- 2x2x15 8 2x4x8 25 Iron ---- Doors ---
I started with S Q L but quickly gave up because just saying sequel is so much easier. 2 years in the industry
I think that's for his second request, but the original request in the OP, and where he states &gt; all **rows** ***and*** **columns** from tableA, where colA is distinct Still has me confused.
I think what you're looking for is [Unpivot](https://docs.microsoft.com/en-us/sql/t-sql/queries/from-using-pivot-and-unpivot?view=sql-server-2017).
You need to wrap the ) in single quotes to signify you want a string inserted
Well yes I assume. The full sentence I'm running is: SELECT ADD_YEARS ([%0], 4) FROM DUMMY Works fine when I do: SELECT ADD_YEARS ('1.1.2012', 4) FROM DUMMY so I must be getting something wrong with the [%0] variable
&gt; Postgres folks just punt lol
It may be because you have it in the first select statement twice.
Is it is because you've listed q.course_number twice in your select?
you select se.course\_no and t.source\_no which end up as columns in q1. &amp;#x200B; So you need to alias one of them to something else (other than course\_no), then when you select q1.course\_no it is unambiguous.
Oops. I was trying to create a new column there. I removed it but that wasn't the reason.
This was it. Thank you!
For this particular query, the `JOIN` predicates force `se.course_no` and `t.course_no` to be identical anyway. There is no point in having both columns in the join result. No information is added.
&gt; and that’s how the SQL Server community refers to it as if that mattered
i started with " S Q L" too, and steadfastly stuck with it 32 years in the industry
i was expecting nonsense but this article is rather enlightened
&gt; When I say the items don't exist, I mean that no items with that that categorization exist as sales for the salesperson and customer being reported on, so they end up omitted from the list. AHA!!! so that means salesperson/customer combo has to be your left table, with the items as the right table, in a LEFT OUTER JOIN
did you see my earlier reply
There is no standard format for a .sql file unfortunately, but typically the extension implies that the files contains a series of SQL statements. It could be anything. Oftentimes it's a whole database dump but not necessarily. If it is a dump, it is almost always specific to one specific RDBMS. For example a MySQL .sql dump file is not compatible with SQL Server.
In a T-SQL procedure it's pretty much automatic with static SQL, and actually takes more work to do it wrong (i.e. dynamic sql with inputs concatenated). :) See /u/redneckrockuhtree 's comment: [https://www.reddit.com/r/SQL/comments/c31w5b/is\_this\_code\_vulnerable\_to\_sql\_injection/es5vuc6/](https://www.reddit.com/r/SQL/comments/c31w5b/is_this_code_vulnerable_to_sql_injection/es5vuc6/)
SQL dev for 4 years. I prefer “squell”
depends who i'm talking to if it's a Microsoft person, i say "squeal" if it's a Postresql person, i say "squirrel" if it's an Oracle person, i say "squellison" if it's a MySQL person, i say "no, *your* SQL" if it's an Access person, i say "just use Design View, bruh"
Nice, more snippets!
MTA 98-364? It’s a Microsoft Certification that demonstrates you have a fundamental knowledge of relational databases pertaining to SQL Server and fundamental knowledge of how to query tables. It’s nothing incredibly fancy BUT you can now say you have a Microsoft Certification that you can proudly put on your resume/LinkedIn.
 CREATE PROCEDURE [dbo].[setDescription] @product int, @descript nvarchar(255) AS BEGIN UPDATE dbo.products SET [Description] = @descript WHERE ProdID = @product END GO
perfect, thank you for this help &amp;#x200B; would you say that this is my easiest option &amp;#x200B; the boss in question has no sql knowledge and won't be checking to see if my "certification" was easy to get or hard to get
i have a genuine dislike of the pivot/unpivot syntax (and extra functionality) plus it's mssql so there's always something more fun: select * from (values (1,2,3)) t1(a,b,c) cross apply (values (a),(b),(c)) t2(d)
Is this an ad-hoc maintenance task you're doing once, or part of regular application logic? How much data are we talking about?
2)
It's an on-going process that syncs the data. It involves 12 tables, each with around 3 million rows. &amp;#x200B; I'd like to avoid adding the timestamp column if possible. &amp;#x200B; My best idea is to run the UPDATE, then something like: DELETE FROM table WHERE id=1 AND time\_updated!=\*\*time\_of\_update\*\*
Oh I see! Thanks for the clarification. Also I will ask my usual question - which database are you using?
Yeah I think it’s the perfect blend of valid credentials. If they’re not technical then all they will see is “Microsoft Certification”. That will carry way more weight than any other cert you get online from a 3rd party source. When I took it, I believe I studied for about a week at nights and passed with a 98%. It’s pretty darn easy. There’s some brain dumps out there that pretty much are word for word what you’ll see on the exam.
MariaDB
The only certifications that "look good" are the ones from the platform vendors themselves (Microsoft &amp; Oracle). But...they're not going to give you a decision for a month? Keep looking around and if someone makes you a good offer in the meantime, take it. Don't put everything on this one job.
&gt;There’s some brain dumps out there that pretty much are word for word what you’ll see on the exam. All this does is prove that you managed to memorize the answers to the test. It says literally nothing about your ability to do the job. Congratulations, you've now completely devalued the certification.
Are you wanting it to return zero for each type of currency or just one row with a zero balance if there is no balance for any currency? If it's just one row, what do you want returned for c.code?
all
His question was to gain a certification that will look good to a future boss who is non technical. I provided that. Cert is not devalued in any way, I simply gave a method of making sure to pass for this instance.
In the last bit, try wrapping '(select Balance from @temp) + x.Balance' in an ISNULL or COALESCE statement to replace it with 0 if it's NULL, and changing the join from an INNER JOIN to a RIGHT OUTER JOIN.
Thanks. I"m not a programmer and wasn't sure what to look for specifically.
Do you have an idea on the % new / deleted rows? At a certain point it's going to be more efficient to truncate the tables and re-import them from scratch. &amp;#x200B; If there are very few rows that are being removed / added and most of them are being updated, it'll probably be more efficient to go with a simple "shouldDelete" int column and just set this to 1 for all rows, change it to 0 for new / updated rows, then delete all rows where "shouldDelete = 1".
No need to have a GO at the end?
The GO just separates statements in a batch. If you're just doing the stored procedure by itself, you don't need one.
https://en.wikipedia.org/wiki/Goodhart%27s_law
Yes! Take on a project. The best way I’ve found to get an understanding of how it all works is to start with creating a website where you have to create containers and tables and SP’s etc. if your a data nut, it can be really fun.
It says he can get paid more.
Which parts are you having trouble with?
yeah can i like see your query
you have been very helpful thanks
According to the exam(sorry it is a long list): Apply general functions and conditional expressions in a SELECT statement Build a SELECT statement to retrieve data from an Oracle Database table Create a simple table Create, maintain and use sequences Define subqueries Describe data types that are available for columns Describe how schema objects work Describe the different types of joins and their features Describe the features of multitable INSERTs Describe the purpose of DML Describe the types of problems subqueries can solve Differentiate system privileges from object privileges Explain the relationship between a database and SQL Explain the theoretical and physical aspects of a relational database Grant privileges on tables and on a user Identify the connection between an ERD and a database using SQL SELECT statements Insert rows into a table Merge rows in a table Update rows in a table Use DDL to manage tables and their relationships Use SELECT statements to access data from more than one table using equijoins and nonequijoins Use character, number, and date and analytical (PERCENTILE\_CONT, STDDEV, LAG, LEAD) functions in SELECT statements Use self joins Use the EXISTS and NOT EXISTS operators Use the TO\_CHAR, TO\_NUMBER, and TO\_DATE conversion functions Use the data dictionary views to research data on objects Use various types of functions available in SQL View data that generally does not meet a join condition by using outer joins
Ah, the relationship table. Weirdly this is usually the cleanest way to build a many-to-many relationship between two tables, especially when both tables have a ton of columns. What may have thrown you off was the PK - some organizations have a database rule that mandates every table have an identity column set as the primary key. You'll see arguments for and against this practice, but in my experience it can make things like incremental ETLs a lot easier to design when you can rely on every table having an automatically indexed identity column.
And when the employer discovers that he faked his way through the exam to get that extra money but doesn't actually have the skills, what happens?
this seems to cover pretty much everything. out of curiosity, and since you've got 55% on your latest one, which areas were you ABLE to respond correctly?
What you’re looking for is INNER JOIN syntax. https://www.w3schools.com/sql/sql_join.asp
 select ISNULL((select Balance from @temp) + x.Balance,0) as Balance,c.Code from v x RIGHT OUTER join Currency c on c.Codeid = currencyid The RIGHT OUTER JOIN brings in all rows from that table regardless of if they match the join criteria which will generate a row for every entry in currency. The ISNULL is then used to replace the resulting NULL value with a 0. Alternately you can set up your select statement as follows depending on your needs: select ISNULL((select Balance from @temp)) + ISNULL(x.Balance,0) as Balance,c.Code
We’re talking introductory level here...if he’s truly interested in this position he will acquire the skill. All he’s trying to do is take that initial first step to get this position and grow his career? This isn’t a certification to save lives or something earth shattering. It’s to prove an INTRODUCTORY knowledge of SQL so that he can go from there. Deep breath my friend.
Thanks !
Thanks !
Control Transactions Create and maintain indexes including invisible indexes and multiple indexes on the same columns Delete rows from a table Describe the purpose of DML statements And im assuming that they use multiple questions for the same topic where I got some but not all correct.
Does "On t.varA = x.varA and t.varb = x.varB" not work?
Awesome, as noted previously I appreciate your effort with these.
Thank you. I think, now that I am constrained for time, this will be THE series. Creativity through constraints and speed via the snippet manager. I really want to get to a point where I can do live vids, take other people's data, and push it into the system / answer questions and all that... by pushing buttons instead of writing code... like my actual job using this system.
This is a great initiative. Looking forward to go through them as soon as I get off from work today.
Does your database support the MERGE INTO sql statement? That can do all of the above in one operation.
I would probably go with something like this: SELECT a.[customer], a.[store], a.[date] INTO [table2] FROM [table1] AS a INNER JOIN (SELECT [customer], MIN(date) AS [firstdate] FROM TABLE1 GROUP BY customer) AS b ON a.[customer] = b.[customer] AND a.[date] = b.[firstdate] Though if table1.date was just a date and not a datetime and it was possible for customers to visit more than one store on the same day I might re-examine things a bit to avoid duplicates.
You can choose whether or not to apply the windows password policy and password expirations to a sql login on a case by case basis. My recommendation if possible is to so set up logins based on Windows Authentication for different security groups in AD. You set up access per group, and the AD admins handle moving people into and out of the groups for you. You rarely have to worry about adding or removing users or dealing with passwords.
First, MicrosoftDB isn't a thing. I'm assuming you mean Microsoft's Access DB. Sqlite3 is incompatible with Access DB. So focusing away from that, you need to be able to access Access DB. Luckily, if you're developing on Windows using Visual Studio for your C++ IDE, there is a lot of built in functionality for it as long as you are using at least one windows toolkit. I highly recommend looking up in CodeProject or Google how to connect to a Microsoft Access database in C++. You should see references to OBDC or ACCDB etc and those are correct dependent on what you need.
Thank you so much works perfectly.
Microsoft SQL Server Express (64-bit) is what my database is on. I've literally never heard of Microsoft Access Database lol (Keep in mind this is the first time i even touch a database).
Oh! I wrongly assumed you were on desktop and not on a server. Sqlite3 still is incompatible, though, and since Microsoft is okay at keeping consistent, you're still looking at using OBDC. Etc.; i.e., you can only use Microsoft specific queries and libraries to support Microsoft SQL Server.
cool, man. looking forward to digging in!
&gt;Thanks, i figured there was a good reason why i couldn't use SQLite3. &gt; &gt;I'll definitely check out OBDC. Thanks for the suggestion, hope it works
Glad to hear that! Feedback would be greatly appreciated. Especially when I get the retrospective video uploaded.
Thank you, and I hope you enjoy it.
Thanks for the feedback. Do you happen to know what the default security parameters are for a SQL DB?
I came across this reference: [https://www.codeproject.com/Questions/652979/Cplusplus-connect-to-a-MS-SQL-DB](https://www.codeproject.com/Questions/652979/Cplusplus-connect-to-a-MS-SQL-DB) which seems to corroborate my use of ancient OBDC. I am not sure how available OBDC is for you, though, in a server setting, so play it by ear. There are multiple methods detailed in that link, but there are probably many more available. You'll need to choose which works best for you and your setup. A quick Google search can turn up even more results. Overall, this isn't going to be something easy - I'd even expect it to take me 3 days to get everything in order, so don't stress it - especially if this is your first time working with a database. The big things to really focus on are, in order: * Get a live connection to the database - simply have it return TRUE that it has connected. Don't do any other coding until you get this. * Once you get a connection to the database, then pull down the whole table, since that will probably be easier than to pull down a certain row out of a specific database within it. This will also expose the internal referencing structure of the database that is exposed to you. * With that internal structure, you should then be able to start creating queries, etc., and sending them through and testing them. Dependent on what is available via the API you choose, some options will be easier than others (raw SQL string commands vs createRow(...) fxn's).
I'm already connected and able to retrieve information, my problem is when it comes to sending information to the database. I followed this [https://www.youtube.com/watch?v=1g\_Xng\_uH2w&amp;t=149s&amp;ab\_channel=CarloCappello](https://www.youtube.com/watch?v=1g_Xng_uH2w&amp;t=149s&amp;ab_channel=CarloCappello) to get the SELECT but as you might imagine someone who has no idea what he is doing, taking this and switching it around to send data is not simple. &amp;#x200B; I am making progress though, slowly. I had 5 days to get my code to recieve and send info to the database. following the tutorial, i got the recieving done in a day. but i have until tomorrow to figure out the sending. Cherry on top is that now VS doesn't even want to compile anymore lol. LNK1257 came out of nowhere to fuck me in the ass. I guess that's the life of a programmer
Hah! If you followed that video, he's utilizing OBDC! I was curious how you were able to connect, etc., without it. As one reference to one of the functions he is using: [https://docs.microsoft.com/en-us/sql/odbc/reference/syntax/sqlsetconnectattr-function?view=sql-server-2017](https://docs.microsoft.com/en-us/sql/odbc/reference/syntax/sqlsetconnectattr-function?view=sql-server-2017) You can see it is under the OBDC 3.0 protocol. :p LNK1257 and all linker errors are usually easy to resolve, I have faith in you. You should be able to do everything this video is saying to do. At what point are you having problems with? Bonus if you can just timestamp the video and tell me what error you're getting.
No problems with the video, i was able to follow to the end. But he doesn't send information to the database in it, he just retrieves. I have to send. Not only do i have to send values but i have to send variables
Check out the code example on this page: [https://docs.microsoft.com/en-us/sql/odbc/reference/syntax/sqlputdata-function?view=sql-server-2017](https://docs.microsoft.com/en-us/sql/odbc/reference/syntax/sqlputdata-function?view=sql-server-2017) and [https://docs.microsoft.com/en-us/sql/odbc/reference/syntax/sqlsetpos-function?view=sql-server-2017](https://docs.microsoft.com/en-us/sql/odbc/reference/syntax/sqlsetpos-function?view=sql-server-2017) &amp;#x200B; You can use the raw SQLExecDirect (...) functions to execute your INSERT command, and those code examples give you a good idea of how to handle the execution. The API is very well documented as you can see. SQL just stores the values of the variables, not the variables themselves, so pass by reference, etc., isn't going to work here. What are you trying to do with "sending variables"?
Hey btw you should know Sequel Pro is still under development, download nightly builds from GitHub!! I was stuck on the lastest (2016) release till a week ago!
I believe the default is to enforce the password policy and not enforce password expiration.
Just that, storing the values of my variables, it's just that i can't hardcode them. A device sends data, my program just interprets it and stores it into a database
This is my experience. People who live it, and work with it, say, 'sequel,' but managers, and others (who don't know what they're talking about) will often say S-Q-L. It's generally a good indicator of whether or not you actually use it, or just talk about it.
But he didn't fake it. He memorized the information-- he knows it
Massive thank you mate!
No, he memorized the answers to the test. It doesn't mean he _understands_ the material.
Prove it.
I can't believe I didn't know there was a name for this.
I actually just got a new job because I talked about this in the interview. I work in analytics, and they were talking to me about all the KPI's they've built, and watch, and I was just like, "look, that's great, but as soon as you make a KPI and expect people to meet it, then it stops being a good KPI." Room went totally silent.
Ha ha, take my upvote!
It's also incredibly relevant to something I'm looking at right now: some time ago some bright spark made the case that there was an issue with a business process, and provided analysis on the average time it took for staff to update a thing. Management got excited, made dashboards around this measure, and it improved markedly. Predictably, staff are now routinely clicking edit/save without making any changes in order to hit KPI so we can't even use the original measure to understand whether the problem still exists. It's nice to know there's an actual name for this.
This is why I try to change jobs every 2-4 years. I come in, try to revamp things, organically look at metrics, etc., and then bounce on to the next place. As soon as you create them, and they become a target, human ingenuity invalidates them as a metric in the first place. A clever business would never divulge the metrics to the people that they are measuring.
Microsoft has some courses that you take at your own pace and you get a certificate at the end of the course.
Nice... THanks and keep up the good work
Okay, then we're golden - you're saving the values of the variables, not the variables themselves, which made me feel like it was some sort of memory address pointer stuff being passed around. Let me know how the rest goes!
How to open and run it ? I am not too familiar with GitHub
Sadly no. It seems MySQL does not support MERGE INTO.
If you go on the GitHub link above, there's a link down below for "How to Install", you can download it straight from that page :) it will be downloaded as a desktop application.
Rows are updated or new rows added 99% of the time. Rows are only deleted if the data was incorrect - which happens from time to time. The "shouldDelete" int column appeals to me - much more efficient than dealing with timestamps. Thank you for sharing this idea - I appreciate it!
Looks like the link is for MacOS, I'm on Windows
thanks for sharing! I subscribed to your yt channel.
Excellent content, for me the music playing throughout the instructions is what keeps me from subbing.
Even if I told you that, I learned blender to make my title sequence, and I take cuts from 1920-40s jazz, run them through my mpc, and create the music live? 100% free range organic grass fed series.
No, thank you!
Thank you! I hope you find what you are looking for.
No apologies needed! That's not a pain at all. We are currently working on getting a working version for Windows. We will be releasing it as soon as we can :) This is currently a beta release.
Oh ok. Thank you very much
quick question- is this a test i would take in real life at a building or is it online
You can do either. I took it in a testing center. Just made me mentally more prepared and focused knowing I was walking into a test center
Looks reasonable at a quick glance. Work out one of the monthly averages by hand, see if you get the same number? Maybe there's just not much variation month to month?
Like a week? SQL is easy. Learning good relational database design is more important.
Yes, there are multiple VisitDetails rows for each VisitID row. For example in the VisitDetails table I have: VisitID| VisitDetailID| Charge ---|---|---- 1| 1| 68 1| 2| 77 2| 1| 82 2| 2| 75 If I do a query that outputs all the Charges that occurred in January and copy/paste those 962 rows into an average calculator then I get the same answer. I'm just scratching my head over the part that says 'Charge PER Visit'. That makes me feel like I need to be doing something with VisitID.
About 1.5 week for the basics and 2 weeks for more advanced
You probably need to sum up each visits charges and use those totals when calculating the per-visit average.
Hmmm okay. Any idea how I go about this? I assume a nested query but I don't have much experience with that. Sorry, pretty new to SQL.
Start with a query that gets the sum of the charges for each visit, date not relevant. Use that as a sub query, averaging the sums by date.
A week to get the basics. A couple weeks to get advanced. A couple months to realize I wasn’t advanced. A year to become proficient. A couple years to be advanced again.
Wow, what a moment of clarity. After seeing it written out it kinda just clicks, I just couldn't visualize it in my head. I guess that must come with experience. Much appreciated.
You were on the right track. Charge per visit. Start there. THEN average those. I commend you on realizing something wasn’t right and second guessing your answer. When that happens sit back and think what it really means. What IS a charge per VISIT? Obviously it is he sum of charges for that visit.
Very cool! I saw your other play list linked in Reddit and bookmarked it, not knowing if I'll ever get into it. 90 minutes is much more doable for an intro. So MDM. All I know about MDM is the "golden record" concept and vendors like Informatica. Does your video series cover concepts like what informatica sells to businesses? What is different between data modeling and MDM? These questions come from a data analyst with about two years experience. Mostly writing ad hoc reports, stored procedures, validation, and small SSIS packages. I work to support one department as compared to supporting at an enterprise level.
It's fairly easy to pick up and you really don't have to develop your skill much after six months to make it in a simpler role. Though, there are some crazy skilled people out there that when you see their code it looks like magic.
Let me share a video with you, that will be useful if you're trying to learn how to approach interviews. [https://youtu.be/n6gM265zG68](https://youtu.be/n6gM265zG68)
You realize that's how most people get through school right? Some part of that information sticks, get a job, Google your ass off until you get experience, get a more challenging job, repeat until you're at the top.
Data modeling and master data managment are the same thing.... except that the common perception of data modeling is modeling the data for the needs of the business or the right now need, and not the needs of the data management system. Master data managment is a collection of theories without public facing designs... and that is what informatica, sap, redpoint is selling... an interface with their design. Do you want to pay them for their product and be at the will of their developers, or learn how their product functions? So that is what this series is covering. The design. Each episode is 10 mins, 10 episodes a sprint. A sprint every 2 weeks (I hope). This is a hands on series. I recommend you build this at home. Our tables will look the same (columns), our system will build most of the objects we need (tables, views, triggers, databases, files, procedures, indexes UI, IX, CC, CI, etc). We will use patterns to manage and create any data for any need. Multi tenant system, self cleaning tables, security, timers, temporal abilities for every record in every table. The question is: is master data management... master (data-managment), or (master-data) management... or D. All of the above? I wont be mentioning those companies, and I suspect when I am closer to episode 50 or so... if you were following along, you'll know exactly how those companies do what they do.
Same for me. I'm still learning years on, but I'd say that after a week I was confident enough that I haven't come up against a problem I could solve/Google myself
&gt;select \* from (values (1,2,3)) t1(a,b,c) cross apply (values (a),(b),(c)) t2(d) \+1 for CROSS APPLY, so much more versatile than UNPIVOT. SELECT crossaplied.name , crossaplied.result FROM sometable CROSS APPLY ( VALUES ('Result1', Result1) , ('Result2', Result2) , ('Result3', Result3) ) crossaplied (name, result)
Select left(field,2) from table where field between 41 and 49
I realize how unclear my initial post was. The actual values are unknown in the real case, so I cannot hardcode it to specific values. I would somehow need to identify if a complete range even exists.
It looks like you are joining the "Artwork" table to the "Artwork" table. You will need to include the "Purchase" table in your FROM or JOIN section.
Update "Purchase" instead of "Artwork" in Line-3. And after that let me know what error you are facing.
This is one of those odd one that actually depends on your backend. I'm assuming MySQL, so you should be able to do this by doing your aggregating in a subquery UPDATE p SET p.Total = t.TotalPrice FROM purchase p INNER JOIN (SELECT a.PurchaseID, sum(a.Price) TotalPrice FROM Artwork a GROUP BY a.PurchaseID) t on t.purchaseID = t.purchaseID WHERE a.PurchaseID = 'D4758';
Thank you for your reply. I have inserted this code but I still receive the same syntax error message as below: &amp;#x200B; \#1064 - You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'FROM purchase p INNER JOIN (SELECT a.PurchaseID, sum(a.Price) TotalPrice' at line 3
 files_path=pathlib.Path().glob("*.csv") for fname in files_path: filename_ext = os.path.basename(fname) l_filename, file_extension = os.path.splitext(filename_ext) with open(fname,'r') as loop_files: csvreader = csv.reader(loop_files,delimiter='|') fields=next(csvreader) f_str=",".join(fields) # find the length of the columns and add that many place hollders # place_holder = ', '.join(["'"+'%s'+"'"] * len(fields)) # sql formatting begins here # sql_format = "INSERT INTO " + "\" +database_name+"."+l_filename + "`" + "(" + f_str + ")"` sql_format_values = " VALUES ("+ place_holder +")" sql_format= "'"+sql_format+sql_format_values + "'" # loop through the each file and insert into table # for row in csvreader: print(sql_format, row) cur.execute("USE " + database_name +";") cur.execute(sql_format,row) mysql_conn.commit()
Possibly: UPDATE p FROM purchase p INNER JOIN (SELECT a.PurchaseID, sum(a.Price) TotalPrice FROM Artwork a GROUP BY a.PurchaseID) t on t.purchaseID = t.purchaseID SET p.Total = t.TotalPrice WHERE a.PurchaseID = 'D4758';
&gt;UPDATE p FROM purchase p INNER JOIN (SELECT a.PurchaseID, sum(a.Price) TotalPrice FROM Artwork a GROUP BY a.PurchaseID) t on t.purchaseID = t.purchaseID SET p.Total = t.TotalPrice WHERE a.PurchaseID = 'D4758'; That gives the same error but for a different line. &amp;#x200B; \#1064 - You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'FROM purchase p INNER JOIN (SELECT a.PurchaseID, sum(a.Price) TotalPrice' at line 2
How long does the sequence have to be unbroken before it counts as a complete range? Is 98, 99, 00, 01, 02 a range of 5, or two ranges of 2 and 3?
I just noticed another error in what I gave you but I don't think it's actually the critical one: UPDATE p FROM purchase p INNER JOIN (SELECT a.PurchaseID, sum(a.Price) TotalPrice FROM Artwork a GROUP BY a.PurchaseID) t on t.purchaseID = t.purchaseID SET p.Total = t.TotalPrice WHERE p.PurchaseID = 'D4758'; Well, another option which should work across all types of SQL but is less performant is: UPDATE purchase SET TotalPrice = (SELECT sum(price) FROM artwork WHERE purchase.purchaseID = artwork.purchaseID) WHERE purchase.purchaseID = 'D4758';
That’s not valid SQL, but you can have IF... ELSE statements in the SELECT clause with comparators, such as SELECT (IF table1.col1 &gt;= table2.col2 THEN 1 ELSE 0 ENDIF ) as zero_or_one FROM table1, table2 WHERE .... etc
For your first question, let's say it has to be 9 unbroken for 9 digits. 98, 99, 00, 01, 02, for my case, that would count as a single, unbroken range of 5. The other digits in the number is irrelevant, I'm only interested in the last two digits.
from the original post, it looks like this range would qualify 2041 3442 5243 8844 345 4446 8947 and now i'm wondering what the heck kind of use case this is for
Thank You!!! It works perfectly!!!! I have been trying to fix this for a while now so thank you!!!
guys! MySQL does not support UPDATE FROM syntax!!
MySQL does not support UPDATE FROM syntax
First of all, please sort out your formatting. It is really hard to read. For example format like this: files_path=pathlib.Path().glob("*.csv") for fname in files_path: filename_ext = os.path.basename(fname) l_filename, file_extension = os.path.splitext(filename_ext) with open(fname,'r') as loop_files: csvreader = csv.reader(loop_files,delimiter='|') fields=next(csvreader) f_str=",".join(fields) # find the length of the columns and add that many place hollders # place_holder = ', '.join(["'"+'%s'+"'"] * len(fields)) # sql formatting begins here # sql_format = "INSERT INTO " + "\" +database_name+"."+l_filename + "`" + "(" + f_str + ")"` sql_format_values = " VALUES ("+ place_holder +")" sql_format= "'"+sql_format+sql_format_values + "'" # loop through the each file and insert into table # for row in csvreader: print(sql_format, row) cur.execute("USE " + database_name +";") cur.execute(sql_format,row) mysql_conn.commit() I think this is your problem: sql_format = "INSERT INTO " + "\" +database_name+"."+l_filename + "`" + "(" + f_str + ")" I think this is what you actually want: sql_format = "INSERT INTO " + database_name + "." + l_filename + "(" + f_str + ")"
You're very correct. That range would qualify. I'm trying to identify non-legitimate SMS traffic in which the destination phone numbers are being submitted to in a range(but not the whole numbers though, only the last 2 digits), which often means that the traffic is non-legitimate. \+44123456**41** \+44123456**42** \+44123456**43** \+44123456**44** \+44123456**45**
ok cool no there was no IF ELSE in there juts pure comparison operators
I'm not dumb it's MySQL that's dumb.
I reformatted as per your suggestion, when I print before cursor execution i get "INSERT INTO db.esc_uc(Report_Month,Academic_UC,Community_UC,Overall) VALUES ('%s', '%s', '%s', '%s')" ['Feb-2016', '25%', '10%', '11%'] While hitting cur.execute (sql\_format,row), it's throwing the error pymysql.err.ProgrammingError: 1064 **so I assume while cursor execute will converted to** INSERT INTO db.esc_uc(Report_Month,Academic_UC,Community_UC,Overall) VALUES('Feb-2016', '25%', '10%', '11%') In that case after each insert **;** will be the problem I guess.. Is assumption is correct.
That's a bit clearer, but raises another question: I thought you meant sequences out of the whole table ever? But it sounds like you're looking for maybe something like consecutive messages that form a range? If so, what's the ordering to use? Although it might look like your table already has an order, it doesn't *really*. There's no guarantee on the order of records you get from a query unless you specify one. So are you looking for consecutive messages forming a range of 9 or more? If so, what's the field to sort on, something like message_time?
I think a week to learn the basics is fair, but you can spend years learning the intricacies of an implementation. The syntax is easy but implementing it can be hard and bending 3rd party databases to your will is not easy
What makes you think it’s inefficient? Remember, if it’s not fixed don’t break it. 90% of good is better than 100% bad. Let’s say there’s 20 steps and each step you can save 50% of the time. It gets done in half the time, right? Sounds great, correct? Let’s just say it takes a total of 10 minutes to get done. Will saving 5 minutes make a difference? Unless there’s a specific business reason to get it done don’t mess with it. That doesn’t mean you can’t learn what specifically it’s doing and come up with what you believe is better. You may need to have a conceptual plan. That’s good business practice. But doing something just for the sake of doing it isn’t always the best.
If you mentioned your DB, I missed it. On Oracle, I'd take a look into the LEAD and LAG functions, which I think can help you out. I'd explain more, but I'm on my way out the door. If you're on a different DBMS, let us know and we can try to find something similar.
I agree, but the whole thing has been programmed in the last 8 years or so and i was specifically asked to think about alternatives/improvements since it has become spaghetti (i am currently untangling it and analyzing what it all does) So first of all i just want to know if these kind of ETL processes are best done within SQL Management Studio...wouldn't it be possible to have a 'real-time' connection between the databases so any change in database A would be transformed an loaded into B as soon as it happens, so you have a connection between the databases that is not prone to inconsistencies due to the time between changes in application A, scheduled tasks, and propagated changes in application B (and/or C and/or D) &amp;#x200B; Thanks for your quick response btw, appreciated.
Wrong. That **is** valid (standard) SQL. However it requires the database product to properly support the `boolean` data type.
Yes that is valid standard SQL. However not all databases fully support boolean expressions like that. Online example: [https://rextester.com/LVN56569](https://rextester.com/LVN56569)
What's the end goal? Why are you reconfiguring the processes?
Are you able to upload the full .py file and I will see if I can get it working?
Sounds like you’re on the right path as far as documenting things first. Lay it all out then find out what tools are the best for your solution. SSIS may work just fine for you. If you haven’t looked at it yet go check this out. [SSIS How to Create an ETL Package](https://docs.microsoft.com/en-us/sql/integration-services/ssis-how-to-create-an-etl-package?view=sql-server-2017) Your budget may not allow some of the larger solutions (Oracle, IBM) I’ve had done some that I just use a combination of stored views, procedures and triggers.
I'm not sure if I understand the difference in your first question, but the table will have a large number of phone numbers, and I need to be able to if any of the numbers together form a sequence of 9 or more. The table is not ordered already, that's just how I put it as an example. There are no timestamps in the table, all there is the receiving number, and how many messages has been submitted to that number. So in the below example, some of the numbers together form a sequence of 31-36 based on the last two digits. Number Message count \+44123456**31 24** \+44123456**69 99** \+44123456**35 136** \+44123456**32 15** \+44123456**36 33** \+44123456**33** **215** \+44123456**34 2** \+44123456**53 11**
yes i was already looking into SSIS, i will dive into that then, thanks!
Below is my python file code, In this I am doing the following process 1. Directory has multiple csv files 2. In MySQL db I have tables named similar to csv file in the directory 3. I am loading csv files into Mysql db &amp;#x200B; Python version is 3.6 MySQL version is 5.7 import csv import json import os import glob import sqlite3 import datetime import configparser import pymysql import pathlib def mysql_getConn(db_name): config = configparser.ConfigParser() config.read("./00._local_settings.ini") ms_hostnm = config.get("configuration","hostname") ms_usernm = config.get("configuration","username") ms_passwd = config.get("configuration","password") if db_name == '': conn = pymysql.connect(host=ms_hostnm, user=ms_usernm, password=ms_passwd, use_unicode=True, charset="utf8"); else: conn = pymysql.connect(host=ms_hostnm, user=ms_usernm, password=ms_passwd, db=db_name, use_unicode=True, charset="utf8"); return conn class dotdict(dict): __getattr__ = dict.get __setattr__ = dict.__setitem__ __delattr__ = dict.__delitem__ def get_results(db_cursor): desc = [d[0] for d in db_cursor.description] results = [dotdict(dict(zip(desc, res))) for res in db_cursor.fetchall()] return results def esc_load_tables(database_name): mysql_conn = mysql_getConn('db') cur = mysql_conn.cursor(pymysql.cursors.DictCursor) cur._defer_warnings=True files_path=pathlib.Path().glob("*.csv") for fname in files_path: filename_ext = os.path.basename(fname) l_filename, file_extension = os.path.splitext(filename_ext) with open(fname,'r') as loop_files: csvreader = csv.reader(loop_files,delimiter='|') fields=next(csvreader) f_str=",".join(fields) # find the length of the columns and add that many place hollders # place_holder = ','.join(['%s'] * len(fields)) # sql formatting begins here # sql_format = "INSERT INTO " + database_name+"."+l_filename + "(" + f_str + ")" sql_format_values = " VALUES ("+ place_holder +")" sql_format= '"' + sql_format+sql_format_values +';"' # loop through the each file and insert into table # for row in csvreader: cur.execute("USE " + database_name +";") t = row print(sql_format,(t)) cur.execute(sql_format,(t)) mysql_conn.commit() Thanks for your valuable support.
Amazon Redshift, which should be an RDBMS. I think :)
The processes are outdated and although they work, i was asked to analyze and rewrite into better code/processes. The person who wrote the code doesn't work here anymore and we will possibly change our applications. So basically i need to untangle the spaghetti and make it more comprehensible.
Excellent. Redshift has LEAD() and LAG() functions. I seem to remember they don't work in where clauses, so I'd try a subquery: Select * from ( select phone, right(phone, 2) seq, lead(right(phone,2),1) over (order by phone) as lead_seq, lag(right(phone,2),1) over (order by phone) as lag_seq friom tablename ) foo where (seq=lag_seq+1 or seq=lead_seq-1) Now... That is ordering by phone number, and you might want to tweak that to order by the right(phone,2) instead. Also, this doesn't account for the 99-00 case. Hopefully if you need that you can figure it out. Finally, I don't have access to a Redshift DB right now, so there might be minor syntax issues (in particular I'm not sure if the subquery needs a name or not). Good luck.
You're awesome. I think this should be working, however, seems like the system cannot decide which of the possible operators should be preferred, and I think an explicit cast necessary somehow.
Wait..... so do you or don't you want to consider the first digits when determining a range? You said you didn't, but if you've got more than a few hundred phone numbers then you're going to have every possible combination of numbers from 00 to 99, so *everything* would be in a range? I don't understand what the point is in doing this on two digits is, just do it on the whole thing. Treat them like numbers, line them up sequentially, and find the right size blocks. I'm just doodling here and I'm sure you can improve upon this: I've done this as a series of CTEs to explain my logic but I don't think it needs to be this many separate steps --get them as distinct numbers, easier for the sorting WITH distinct_numbers as ( SELECT DISTINCT regexp_replace( number, '^[0-9]') number FROM yourtable), --for each number, check whether it's a new group. Return a 1 if it is, 0 if it isn't. sequence_numbers as ( SELECT number, case when lag(number, 1) over (order by number asc) = number - 1 then 0 else 1 end NewGroup FROM distinct_numbers ), --give each sequence a number: we've assigned 1 to where sequence starts, --so if we get a running sum of that across the same order we're essentially counting our sequences. group_numbers as ( SELECT number, sum(NewGroup) over (order by number asc) sequence_number FROM sequence_numbers), --Now for each phone number, get the total number of calls in the same sequence numbers_sequence_size as ( SELECT number, count(number) over (partition by sequence_number) sequence_size FROM group_numbers ) --Then you can get your numbers and join them back to your original table to see which ones are in a sequence, --note you can now specify the sequence size in your where clause SELECT * FROM yourtable t INNER JOIN numbers_sequence_size nss ON regexp_replace( yourtable.number, '^[0-9]') = nss.regexp_replace( yourtable.number, '^[0-9]') WHERE nss.sequence_size &gt;= 9
the process SHOULD be simple. 1. Load source data into stage tables 2. sprocs convert staged data, load into final tables generally speaking, I prefer EITHER a sproc for each source (if there are only a small number of destination tables), or a sproc for each destination table. that should be it. then you can use SQL Agent, or SSIS, or whatever you want, for controlling the execution of them... once the stage tables are loaded, try to run as many sprocs as possible in parallel, without negatively impacting performance, which just depends on the size/resources of your server. the pattern is simple, direct, and can be very fast/efficient.
Have a look here at how to recreate the root user: http://www.helpfromfriend.com/featured/how-to-recreate-root-account-in-mysql/
Oh interesting. I've seen an alternative used by Jefferey Way of [Laracasts](https://laracasts.com/) (forget the name) but Sequel Pro works great for me. Good call on pulling latest builds
I found the site that lays out their courses kind of confusing.
I’d like to hear an answer to this as well. However, it’s worth downloading some sample databases and practicing querying them yourself. At the very least you could take a document to an interview with some basic and then more complex queries on it to demonstrate experience.
Busy at work now, hope you got it working. If not, and you're having trouble with casts, it depends on the type you're using for the phone number. Ideally, it's a string type so that the right() works correctly. If redshift isn't happy doing math on the seq fields, you might have to explicitly "convert(int, lag_seq)" for that part.
My personal go-to for importing potentially suspect data is to stuff things into varchar columns in a temp/staging table and clean up the data there after importing it. It's a lot easier to manipulate the data once it's in a table than trying to sort out the fiddly bits during import.
When you go into the SSIS route, realize it does some things great and others not so good (SCD Wizard, I'm looking at you buddy!). We went the route of a hybrid approach. SSIS to load the source data into stage tables since it can handle the various connectors. Once everything is local we use SPs to cleanse and move the data into the destination tables.
somebody is mad at themselves for not looking up the syntax in da manual
u/aarontbarratt probably already answered the question. &amp;#x200B; I'll just add, please use f-strings. It makes the syntax so much easier to read. [https://www.python.org/dev/peps/pep-0498/#how-to-denote-f-strings](https://www.python.org/dev/peps/pep-0498/#how-to-denote-f-strings)
If you’re not sure just shoot it a second time
I don't understand how someone can be a "data scientist" and not have some level of SQL under their belts. I know they exist because I work with many of them, and they struggle as a result.
I would have for sure thought a float or decimal would have worked. What is the error you are receiving on the import?
Depending on the database you could do something like this: select time'00:00:00' + cast(NNN as interval second(8))
Because the people that hire them have no idea what they’re hiring for or the technical competency required for it. Buzzword chasing.
"SQL can't handle 100gb big data!" - said everyone who doesn't know a db from a hole in the ground.
the same people that think NoSQL means "No SQL", and talk about how elastic is going to take down big data with innovative warehousing ETL blackbox solutions.
That's what I thought as well. Unfortunately I really can't edit the CSV as it is well over 2mil records. &amp;#x200B; TITLE: Microsoft SQL Server Management Studio \------------------------------ &amp;#x200B; Error inserting data into table. (Microsoft.SqlServer.Import.Wizard) &amp;#x200B; \------------------------------ ADDITIONAL INFORMATION: &amp;#x200B; Error inserting data into table. (Microsoft.SqlServer.Prose.Import) &amp;#x200B; \------------------------------ &amp;#x200B; The given value of type String from the data source cannot be converted to type float of the specified target column. ([System.Data](https://System.Data)) &amp;#x200B; \------------------------------ &amp;#x200B; Failed to convert parameter value from a String to a Double. ([System.Data](https://System.Data)) &amp;#x200B; \------------------------------ &amp;#x200B; Input string was not in a correct format. (mscorlib)
&gt; wouldn't it be possible to have a 'real-time' connection between the databases so any change in database A would be transformed an loaded into B as soon as it happens This would be a lot less efficient than doing it on a scheduled basis, and "real time" isn't even a word that has meaning here. Theoretically you could set something up on the first database so that anytime any transaction is made, it triggers a process to to the ETL... but why?
I'd go with something like this, assuming that you've already built a new table in your database to hold the data that gets returned. INSERT INTO [myschema].[mytable] EXECUTE [otherdatabasename].[dbo].[SSRS_CurrentGW]
\*laughs in set based operations\*
recursive CTE, hierarchical, PL/SQL XML, table parameters, etc.... The sky is the limit. You can never be “advanced”, just more and more knowledgeable.
Are the tables ready in your database or do they actually need to be created?
Rule number 2. Double tap
Should pop open the proc, see what it’s doing, just to take a look. Take a look at select into statements, incorporate the select into using the stored proc or alter the stored proc.
Here's the update, I figured i could use SQLExecDirect to bypass my problem, and i sort of can. string arr\[10\]; stringstream ssin(outputData); while (ssin.good() &amp;&amp; i &lt; 10) { ssin &gt;&gt; arr\[i\]; \++i; } char SQLInsert\[\] = { sprintf((char\*)SQLStmt, "INSERT INTO Ard\_Data\_PIP(v1) VALUES ('arr\[0\]');", arr\[0\]), sprintf((char\*)SQLStmt, "INSERT INTO Ard\_Data\_PIP(v2) VALUES ('arr\[1\]');", arr\[1\]), sprintf((char\*)SQLStmt, "INSERT INTO Ard\_Data\_PIP(v3) VALUES ('arr\[2\]');", arr\[2\]), sprintf((char\*)SQLStmt, "INSERT INTO Ard\_Data\_PIP(v4) VALUES ('arr\[3\]');", arr\[3\]), sprintf((char\*)SQLStmt, "INSERT INTO Ard\_Data\_PIP(v5) VALUES ('arr\[4\]');", arr\[4\]), sprintf((char\*)SQLStmt, "INSERT INTO Ard\_Data\_PIP(v6) VALUES ('arr\[5\]');", arr\[5\]), sprintf((char\*)SQLStmt, "INSERT INTO Ard\_Data\_PIP(v7) VALUES ('arr\[6\]');", arr\[6\]) }; if (SQL\_SUCCESS != SQLExecDirect(SQLStatementHandle, (SQLCHAR\*)SQLInsert, SQL\_NTS)) { showSQLError(SQL\_HANDLE\_STMT, SQLStatementHandle); } &amp;#x200B; but as you can see, im using a char array to send strings so naturally i dont get the right values. besides these errors, i think this should work out
Does this work? Select * From opendatasource('microsoft.ace.oledb.12.0','Data Source=C:\Server\directory;Extended Properties="TEXT;HDR=YES;IMEX=1"'...nameoffile#csv The file has to be on the server that SQL runs on, not locally (unless its local) HDR is first row are colimns.
I became "comfortable" with basic SQL functionality and commands within a week. Within that same week I was making my own databases and using them to automate parts of my report writing job. To be fair, I am currently living, breathing, and sleeping the stuff. I bought several textbooks on the subject and cleanly read through them all. Then I listened to podcasts on my commute and kept having them play in the background even if I was lost. Problem is SQL gets massively complicated very fast. The textbooks I'm reading show ER flowcharts representing databases that consist of hundreds or thousands of tables, several catalogs and dozens of schema. Most real life applicable SQL projects are going to require a little more than the basics. One typically sized hospital for example might require a small team of DBA's just to manage the database after several months of setting it up. M
If the tables exists........insert into [your table] exec procedure and params
;WITH cte_temp AS (the select statement from the stored proc) SELECT * INTO tablename FROM cte_temp ; This is probably not the best way, but it's how I'd do it.
OPENROWSET
 INSERT INTO db.schema.yourtable EXEC db2.schema2.thestoredproc param1value, param2value2...
If you think it’s a bug then report it through the proper channels so it can be fixed. Since you’re using beta software you should be expecting to see a few things that aren’t fully intentional.
Yes I think I did but there has been zero feedback. Here is where I posted it: [https://feedback.azure.com/forums/908035-sql-server/suggestions/38018374-string-aggregation-with-user-defined-functions-bro](https://feedback.azure.com/forums/908035-sql-server/suggestions/38018374-string-aggregation-with-user-defined-functions-bro) If you know of a better place to log this bug(or feature?) with Microsoft please let me know. Thanks.
What values are you expecting for the ID field in between ID 1 and 2?
&gt; I am not entirely sure how to approach this problem completely re-think it find a solution that does not involve the auto increment
&gt; a vending machine &amp; connect four wut
101 102 103 This is a problem for a DBA position, I'm a recent college graduate and have not come across anything like this before.
Seems a little odd. As another commenter said, I'd take a step back and evaluate what's trying to be accomplished and find another way to tackle this. I dont know the reasoning behind this but doing it this way seems to lead to a complete mess
Is there any funky data which you can see? Try casting each individual 'money' column as a float and see which column errors. There is most likely a funky string causing the conversion failure. Hopefully you can leverage the [TRY\_CAST()](https://docs.microsoft.com/en-us/sql/t-sql/functions/try-cast-transact-sql?view=sql-server-2017) function to convert any failures NULL and hand fix or something similar.
So is there not a way to insert a row in between rows in an int identity(1,1) column?
Start with support.microsoft.com
I mean I guess you could but then when you meet your 101st row you'll have problems with what I'm understanding.
Ummm...so im an accidental dba and a data scientist?? ILL TAKE IT! i thought data science was an advanced field...this article is bogus lol
Are you sure they wanted an ER diagram and not a state transition diagram?
&gt; This is a problem for a DBA position the correct answer is **do it another way without the auto increment nonsense** rows in a relational database don't have a sequence -- use some other column value
Maybe I misunderstood the problem, but a simple Trigger would do that. CREATE TRIGGER insertLines ON some_Table AFTER INSERT AS INSERT INTO some_Table ( number ) SELECT number*100 ) FROM INSERTED GO (quick example, without validation, etc)
Could I message you?
Go ahead
FACE THE LED!
To expand upon this; you *may need to look into* a truncate or create a MERGE statement in order to eliminate duplicates.
Thanks, sent!
ER diagram would be like Customer/Invoice/Item. Not sure what you mean.
Most people would put most stocks in one table. You might have a second table for customers as an example, then a third for trades that shows customer I'd, stock Id, a buy or sell flag, a timestamps, and a market value or whatever.
On the right track! I don't think using char arrays is bad - happens everywhere. I don't think you're using sprintf correctly, though. I don't see %s or %d etc anywhere..
First off. It's not a table it's a result. That matters. Second off, what is the advantage of moving it over doing an a linked server or an openrow query? You're adding a bunch of steps by running it, returning it then trying to transfer it. Just have your database use the others stored procedure via a linked server to do an update query
Lol.. another fluff a article for those who can't
Ohhh buddy. You're in for a bad time. Real time is cool and all but in terms of efficiency, real time is not efficient at all. Regardless of what you use it basically involves having a hook constantly looking for changes then updating live. The question really becomes if the business even needs real time data or if you're simply trying to fix a problem that doesn't exist. Why do you need real time and for the massive effort what is accomplished?
I know right. New guy never used sql before wants to change everything - obviously going to be everybody's favorite in the office
No, I'm mad I read the manual and still got it wrong.
It's not entirely clear to me what you are trying to do, but note that for an identity column in SQL Server (I assume this is SQL Server - you didn't state which database), the incremented value is not guaranteed to be gap-free. So if you're trying to determine "every other row" by looking at, say, only the even numbers, that's not a correct way to solve the problem. Note I could be off base with this remark because again I don't totally understand the problem.
Which podcasts would you recommend?
Are you still looking for help with this? Your error doesn't quite make sense to me. If you still need help maybe quote the error message verbatim, please.
Should just do a business analytics course on edx (they have ones devoted purely to either), or just pick out a project from one of the courses. These two are totally different tho. Sqls a query language, tableau a data vis tool. Obviously go hand in hand, but tableau def a lot less commonly used and is honestly one of those buzz word resume builders. At the same time, tableau is super well respected and likely a basic requirement for data science jobs. Still, id guess there are 100 times as many pure sql jobs than ones even involving tableau. Just think, if I’m automating a business process (hands down the #1 most common business function), why would I need a pretty scatter plot. Most problems are not that complex. Really most problems are just getting files/data/input to process automatically and their upstream processes running automatically, whether it’s a fast food restaurant or even a tech company
Tables have no order, so what do you mean insert "between"?
Hmm just a thought, not sure if this would work, but could you alter the clustered index to be indexed on the autoincrementing key and the next column that would logically put the record in that place? In other words the record would go between the two rows because it would be ordered on that second column in such a way that it deserves to be there. I don't know if that makes sense.
I don't know what error you're getting but those strings need to be enclosed in single quotes
Litterally just came back to say I fixed it. The quotes were giving me errors earlier so I thought they were the problem and removed them (as I'm not confident clearly) then added them again and everything is wrapping up nicely. Clearly to tired. I need a damn rubber duck.
[https://www.postgresql.org/docs/current/sql-syntax-lexical.html#SQL-SYNTAX-CONSTANTS](https://www.postgresql.org/docs/current/sql-syntax-lexical.html#SQL-SYNTAX-CONSTANTS)
Ooooh spacing matters, thank you.
Use a window function to generate your own row Id’s based on whatever unique key. your row Id’s would end up being reliably sequenced. then update the DB with that. then perform your inserts
*too Sorry couldn’t help myself. Hope you find the solution!
Your first attempt might have been foiled by the name O'Neill. If that originally had an apostrophe then it was probably breaking the string, and needed to be replaced by a double-apostrophe.
You only need to write insert into customer T Values a single time. That will reduce it to a single query instead of many.
Why the picture of a screen with a cell phone?
I go to a school for computer science (it's age 14-19 so not uni/college) and they had us draw ER diagrams for over half a year before we even got to write SQL.
&gt;This is actually the first time i work with these kind of processes, but i feel this configuration is inefficient and i would like to reprogram it so it works more like real-time ETL (instead of scheduled jobs) Is there a business need for real time? I would not classify a scheduled job as inefficient configuration really, but reporting/data flow cadence needs to be changed. Inefficient configuration would be more like bad SQL- cursors (software developers struggle to think in set based SQL development), excessive cte chains, non-sargrable queries. Figuring out what the SQL actually *does*, not what management *thinks* it does is your first step. if the SQL is bad it may need to be scrapped or refactored. Find out the correct cadence of refresh. Real time may sap too much resources and an angry DBA may appear, when business doesn't require that cadence.
Probably not a good idea to put names, phone numbers, and addresses on the internet.
I see, maybe I'm overthinking things a bit
are these inserts just a sample? Or have you just uploaded your clients adresses to Reddit?
Sample :) Random as hell lol.
It definitely did! I cleaned it all up and looks awesome now, I did leave out the apostrophe tho, so I will know for next time now! Thank you!
I’ve never heard of them. Why not just take a class through edx or coursera from a top 30 university? Makes life for Recruiters easier too. Not to say university education can be inefficient but a sql focused uni course should be good
boolean expressions are generally fine, most modern DBMS support them. i.e. &lt; some TRUE expression &gt; AND &lt; expr. also always evaluated&gt; &lt; some TRUE expression &gt; OR &lt; expr. never evaluated &gt; &lt; some FALSE expression&gt; OR &lt; expr. also evaluated&gt; The last two compound expressions demonstrate boolean short-circuiting, where the 2nd &lt;expression&gt; is never reached b/c the 1st &lt;expression&gt; is TRUE. In code: SELECT , ( ( 1 = 1 ) and ( 1 = 1 ))::boolean AS _true_and /** 2nd expr always evaluated regardless **/ , ( ( 1 = 1 ) and ( 1 &lt;&gt; 1 ))::boolean AS _false_and /** 2nd expr always evaluated regardless **/ , ( ( 1 = 1 ) OR ( 1 &lt;&gt; 1 ))::boolean AS _true_or /** 2nd expr short-circuited b/c 1st expr is TRUE **/ , ( ( 1 &lt;&gt; 1 ) OR ( 1 = 1 ))::boolean AS _true_or /** 2nd expr is evaluated b/c 1st expr is FALSE **/ , ( ( 1 &lt;&gt; 1 ) OR ( 1 &lt;&gt; 1 ))::boolean AS _false_or /** 2nd expr is evaluated b/c 1st expr is FALSE ** / ; Boolean short-circuiting is also another way to tackle sets using OR blocks in the predicate where clause select user.birthdate , user.height , family.role , family.birthdate , family.height FROM user JOIN family ON user.family_id = family.family_id WHERE ANY ( select from family f2 where f2.family_id = family.family_id -- this code might run faster and f2.birthdate &gt;= '2001-06-30' -- this code might run faster ) -- end of ANY block OR -- when ^^ ANY f2.birthdate &gt; '2001-06-30' ^^ -- then below code will never run -- as it's been "short-circuited". -- moreover expr in the ( OR block ) might run more slowly b/c of the join ( family.role &lt;&gt; 'PARENT' AND birthdate IN ( SELECT MAX( birthdate ) FROM family f2 where f2.family_id = family.family_id -- correlated subquery ) ) -- end of OR block
It's interesting. I've had positions where I wasn't allowed to touch the database at all. A lot of times it seems that the people who work most closely with the data stores end up being gatekeepers in certain ways. I understand enough SQL to write queries, but you probably wouldn't want me building a production database that had a ton of complexity. I've always wanted to improve my SQL skills, but I've been stymied when I try to get the access that allows me to learn more interesting techniques.
Moreover, most DBMS support this: select ( 1, 2, 3 ) = ( 1, 2, 3 ) as composite_expr_eq , ( 1, 2, 3 ) = ( 1, 2, 4 ) as composite_expr_ne ; And if you're on Postgres, here's something else to try: select ( ta ).id as ta_id , ( tas.field_1, tas.field_2 ) = ( ta.field_1, ta.field_2 ) as bool_diffs , ( tas )::text as tas_row2txt , ( tas ).field_1 -- PG is object-relational ... FROM table_A ta JOIN table_A_snapshot tas -- some archive of table_A ON ta.id = tas.id -- a NATURAL JOIN could've done this WHERE ( ta ) &lt;&gt; ( tas ) -- changes to table_A since the snapshot was taken ... ; Most installs of PG are limited to 32 columns in an index, but an entire tuple is a composite object so... CREATE INDEX foo on bar (( (bar)::bar )) ; -- or -- CREATE INDEX foo on bar (( bar.* )) ; ... are valid &amp; afford optimizing expressions like \[ WHERE ( bar\_a ) &lt;&gt; ( bar\_b\_snapshot ) \]
Neither of these two resources are worth the $ imo. I think a better investment would be a udemy course or the "T-SQL Fundamentals" book by Ben Gan
A few suggestions. Instead of inserting into the table at the same time as it's being created, create the table first so you aren't locking. `MULTISET` and `NO LOG` will also reduce some extra processing. CREATE VOLATILE MULTISET TABLE BASE2, NO LOG( INSTANCE_ID BIGINT, OFFER_TYPE VARCHAR(100), SESSION_ID BIGINT, FIRST_IND INTEGER ) PRIMARY INDEX(INSTANCE_ID,SESSION_ID) ON COMMIT PRESERVE ROWS; Followed by INSERT INTO BASE2 SELECT etc... Next, as was suggested, remove the cast in the WHERE. You can always make another temp table to use for date variables. Other than that, how many rows are in these tables? If you're only using one criteria in your join, you could be getting tons more records returned. Typically I start getting spool errors because I forgot to put a date qualifier in my JOIN. If you're using SQL Assistant, push F6 to see the execution plan.
Just use the free learning at W3Schools.
I found a cert through Coursera from UC Davis, will this really hold clout with recruiters?
https://www.reddit.com/r/SQL/comments/9utzl4/anybody_take_the_ikm_sql_assessment_for_job/?utm_source=share&amp;utm_medium=ios_app
Basically all you need is the count function, group by and a way to truncate your datetime/timestamp columns to the date. Each database has a different way of doing that last part, and you didn't mention with database, but it should be easy in any case.
(Not OP). I can see that this is straightforward if you’re only looking at one timestamp column. But is it as simple with two columns? my thought is, since the opened time and closed time are on the same row, then if the closed time was in a different month than the open time it would make things a little trickier. Like if an entry was Opened in May and Closed in June. I was thinking you might want to build a CTE where you count the Opens per month and a second CTE where you count the Closes per month and then join the two CTEs. Or is there a simpler way that I’m not thinking of?
&gt; my thought is, since the submitted time and closed time are on the same row, then if the closed time was in a different month than the submitted time it would make things a little trickier. Like if an entry was Submitted in May and Closed in June. yep, this is the scenario i was facing.
unpivot your dataset any way you like it so the date is a single column, then calculate counts
 SELECT Mon, COUNT(*) FROM (SELECT MONTH(DatSub) as Mon FROM Tabl WHERE DatSub IS NOT NULL UNION ALL SELECT MONTH(DatEnd) as Mon FROM Tabl WHERE DatEnd IS NOT NULL) as Stuff GROUP BY Mon; I'm pretty sure that's all you need. I don't have anything convenient to test it on right now.
Generally, no one cares about tech based certificates. They will ask you sql questions directly. Look up data analyst interview questions at companies like amazon, Facebook, or any Fortune 500 company for an idea.
just want to say you are beautiful
What if OP wants to have Submitted and Closed as two separate counts? Would this modification work: SELECT Mon, COUNT(case when metric_id = 0 then 1 end) as count_submissions, COUNT(case when metric_id = 1 then 1 end) as count_closed FROM (SELECT 0 as metric_id, MONTH(DatSub) as Mon FROM Tabl WHERE DatSub IS NOT NULL UNION ALL SELECT 1 as metric_id, MONTH(DatEnd) as Mon FROM Tabl WHERE DatEnd IS NOT NULL) as Stuff GROUP BY Mon;
On mobile so please bear with me.. Select Sum( Case When 'your condition' Then 'if its true' 1 Else 'if its not' 0 End ) So each time your given matrix is detected on the month and day grid.. You sum up the times it was true.. Secondly you can add another sum case combo to sum up the second condition type you might want to get checked..
Hey, If you're looking to overhaul the whole process take a look at [Azure Data Factories](https://azure.microsoft.com/en-gb/services/data-factory/?&amp;OCID=AID719823_SEM_hZn4jqwj&amp;lnkd=Google_Azure_Brand&amp;dclid=CjgKEAjwu-HoBRCPg_2j7LfC4xkSJABTohMRshX0MVUfwI5Z4YKyJdJPZ0L9_ZARc2QrEnAFcFeVCPD_BwE)
Totally agree. I purchased a udemy course for $11 and its has been the best “investment”. I went from no knowledge to building large complex queries and reports in weeks. Of course Incorporating it into my daily work helped a ton with retention and skill building
If the the primary\_worker is always going to be 45, 46, and 47 and you're always wanting seven days ago; the sproc would not need any parameters passed. With that said, the CE may work better if: A) You are able to pass the DATEAD() into the sproc as a literal B) Declaring the DATEADD() as a variable within the sproc I'm confused about the order\_dt and cont\_dt &gt;= 1900-01-01. Your predicates in the left joins are only going back seven days. Are there nulls in those columns?
Thanks for the reply. Can you give me some insight as to why: - I would not need any parameters passed here? - declaring the DATEADD() as a variable is best (feel free to point me in the direction of something to read that might clarify all of this) To your question about the order_dt and cont_dt, yes, and the other tables are a bit messy, so this was the best fix.
You don't need any parameters passed because your query is self sufficient. A sproc can be wrapped around your query and executed without parameters. Also, regarding declaring the a variable it was due to trying to force a literal which would keep sargability, but, I apologize, as I confused DATEADD() with DATEDIFF(). Both of the LEFTs should be sargable presuming there is an index on the respective date fields. If you have the ability, double check the execution plan for the query to ensure you're seeking on the date fields.
I can’t speak to the value of these courses, but if others are saying they’re not value, I can recommend Codeacademy.com. Also, there are countless YouTube videos.
Are you saying you want to break down totals by year? You could just add year to the group by (and select list). Your table is confusing since it doesn't show the Wins column; it looks like you're doing something there with Win % but it's not shown.
Good catch. I guess one thing to point out is, I haven't been generating #temp_tables for every single year. Rather I am creating #temp_tables for 5 year subsets (2010-2015, 2015-2020, etc).
You might want to massage all the columns into another temp table, or you can use CTE's. create table #summary as select Year, Player, CASE WHEN Rating &gt;= 2 and Rating &lt; 5 THEN 1 WHEN Rating &gt;= 5 and Rating &lt; 7 THEN 2... END as rating_category, CASE WHEN round(Wins, 1) BETWEEN 0 AND 25 THEN '0-25%' ELSE '25-100%' END as wins_category, FROM (select * from #temp_table1 union select * from temp_table2 ...) all_years Then the groupings are easier to write: select year, wins_category, SUM(CASE WHEN rating_category = 1 THEN 1 ELSE 0 END) AS Rating1, SUM(CASE WHEN rating_category = 2 THEN 1 else 0 END) AS Rating2, SUM(CASE WHEN rating_category = 3 THEN 1 ELSE 0 END) AS Rating3 from #summary group by 1,2;
You only need parameters when your where clause needs to change the values it compares. Where ID in (@parm1, @parm2, @parm3) There are actually a lot more use cases, but that's when you get into the more programming side. You just want to store a SQL query because your DBA is anal.
"where clause needs to change the values it compares" This is where the block is for me. Could you give me an example of where I would need to change the value in the query up above? Here is my take, if they wanted to see a report on a specific person and needed to tell the system that, I would need to create a WHERE clause that made that an option AND create a parameter towards that end?
Date ranges are the most common parameter I've seen. If you have a stored procedure that lists some sort of transactions, you probably want flexibility on the date range of those transactions. Often times you can get away with using a "last week" filter or something that looks at the system date, but assigning a start date/end date parameter gives you more flexibility.
I'm not entirely sure whether you're asking *how* to use sproc parameters, or *which* parameters to use. But to try and answer both: the most obvious variables in a report like this would be the person and the date range. So you might declare your sproc like so: CREATE PROCEDURE dbo.MyReport @primary_worker_id int, @start_date datetime = null, @end_date datetime = null And you'd incorporate into your query like so: where primary_worker = @primary_worker_id
What udemy course did you use? I am interested. Do you write stored procedures at work? Thanks!
If a constituent is a customer then you would declare a parameter (e.g @customerno int), and include Where customer_no = @customerno If a constituent is a primary worker and you may have one or more primary workers passed into the stored procedure at a time, you could declare a varchar parameter (@primaryworkers varchar(max)) that takes in a list separated by a delimiter. Before your query, you could then split the parameter based on your delimiter and select the values into a table variable that stores each individual value E.g. Declare @primaryworker table ( primaryworker int ) Insert into @primaryworker (primaryworker) Select string_split(@primaryworkers, ',') You could then use a sub query in your where clause to get all the primary workers that were passed in Where primary_worker in (select primaryworker from @primaryworker)
Here's a very simple example: say you wanted to create a stored procedure that returned an employee's first name, last name, and hire date, based on their ID. The employee_id would be the variable that you need to pass into the procedure as a parameter. It might look something like this: CREATE PROCEDURE get_employee_info_sp ( @employee_id INT ) AS BEGIN SELECT First_Name, Last_Name, Hire_Date FROM Employee WHERE Employee_Id = @employee_id END You would then call the SP like so: EXEC get_employee_info_sp @employee_id = 27 Which would return the info for the employee with an ID of 27.
I'll answer as someone who is transitioning out of finance and over to the IT side. &amp;#x200B; The in-house IT folks won't always have the expertise needed to troubleshoot, implement, and customize the ERP or database. &amp;#x200B; A company also doesn't want to be in the position of having some recurring problem they can't solve, and the ERP vendor says "sorry, you didn't buy support-fuck off" &amp;#x200B; Sometimes these issues are very serious, like if activity performed in a sub ledger isn't properly posting to the general ledger; a company can't function like that, we NEED it fixed, and again our in-house people may not be able to fix/investigate in a timely manner.
I'm not really understanding what you want, or going to dive too far into your data, but it sounds like you want a loop with dynamic SQL.
As others have mentioned, in this case the likely target would be on the primary_worker column, and probably the date range. That adds flexibility, if the date range is pretty common, then you can make that a default configuration, which parameters can change as needed. A couple of other comments, if this wasn't intentional just to make the request faster; Try to get in the habit of qualifying the output columns based on the join names, even if the engine allows it, it can be confusing to figure out at a glance in 3 months where the heck this particular column comes from. It will save you time later when diagnosing, or adding to it. Also pay attention to scalars when writing your filters or in row results directly. Depending on the magic of the system you can very easily turn a query that could use parallelism or more efficient plans (beyond sargability concerns) without realizing it. So the points where you do the dateadd, may not impact the query as forcing serialized execution, But it may. To avoid that, if you can help it throw scalars into a set variable external to the query and use it. The comparison to 1900-01-01 portion for will force an implicit conversion too, depending on many things that could blow up on you. Again if you are dealing with NULL dates, you want to change those out to constructed date or datetime entities, or, properly check NULL. Pay attention to ANSI NULLS settings, that will effect what is detected from a null column value. You would need to use something like where order_st IS NOT NULL for example, if you run into that situation down the line, something like order_dt &lt;&gt; NULL wouldn't work as you may expect with ansi nulls on.
OP, this is correct. If you need more parameters, you just add a comma between them.
FYI, stored procedures don't NEED to have parameters, you can certainly make one without any. Basically a parameter is just a variable you are passing into the stored procedure. If you were thinking you might make 2 stored procedures, one to get info over the last 7 days and another to get info over the last 14 days, for example, you could instead make one procedure and pass in the number of days as a parameter. This allows you to reuse code, helps eliminate bugs, and is a much better practice.
You could avoid ctes and all that logic inside the select by generating a table that has something like this... Create table #measure ( measureName nvarchar(20), measurelookup int, measureresult bit); Or better yet, make it a permanent table and create and interface to add and change it. Then you could just do joins for the values and that would produce your result per measure, you would also have the luxury of grouping it by year as well, then producing the table from that with player, year, and measure would roll up per year per player and you could put together whatever you want. You might need to alter this, I’m on a phone so I can’t view your original question while writing this but you get the idea.
Check out Strata Scratch site. They did a great job on this: [https://www.youtube.com/watch?v=n6gM265zG68&amp;feature=youtu.be](https://www.youtube.com/watch?v=n6gM265zG68&amp;feature=youtu.be) I'd also recommend HackerRank and DataCamp once you are comfortable writing queries and need to focus on building indexes and understanding statistics.
Without clicking the link, am I safe to assume [Betteridge's law applies](https://en.wikipedia.org/wiki/Betteridge%27s_law_of_headlines).
**Betteridge's law of headlines** Betteridge's law of headlines is an adage that states: "Any headline that ends in a question mark can be answered by the word no." It is named after Ian Betteridge, a British technology journalist who wrote about it in 2009, although the principle is much older. As with similar "laws" (e.g., Murphy's law), it is intended to be humorous rather than the literal truth.The maxim has been cited by other names since as early as 1991, when a published compilation of Murphy's Law variants called it "Davis's law", a name that also crops up online, without any explanation of who Davis was. It has also been referred to as the "journalistic principle", and in 2007 was referred to in commentary as "an old truism among journalists". *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/SQL/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
When you are writing the report, if the WHERE criteria is going to be something that the report viewer is going to want to change, make it a parameter. They might usually only want the numbers for the last 7 days, but what if they decide they want to look at two weeks? A month? If you turn it into a parameter you can default it to 7 days, and if they want to get the data for a different amount they can change that setting. Is the report only for Bob’s numbers? What about when Joe gets hired and they need to look at his numbers too? Does it matter if the customer was a commercial entity or a person? You can set up a parameter that will filter by either or include everything, whatever the report viewer needs. Parameters are how the user gets to choose what data they see. If you don’t know what parameters are needed, you should ask whoever is requesting the report. “Are you only ever going to want the last seven days or do you want to be able to pull further back? Do you only want X days from today or do you want to be able to pick a timeframe? Do you always want everyone’s numbers or do you want to be able to filter between Bob, Joe, and Susan?”
Hi, I'm not an expert but I did a little setting up for databases. I use MySQL for databases and MySQL Workbench as a GUI to manage databases.
It should be specified in the course what tools you should use throughout. Can you share a weblink to your course?
You can use SQL live oracle cloud database platform to perform your SQL queries and and save all your scripts on Oracle cloud.
you can simply open your browser and create an account on SQL live site....
Oracle Cloud: the Yahoo! of cloud based RDBMS providers.
It is very practical. especially for beginners.
OK I'll find out more about it from Bing and see whether my MySpace friends can recommend a good place to start.
watch a youtube video on sql queries
haha and don't forget to share it on Orkut.
You guys are right..., :) Before sql_format = "INSERT INTO " + database_name + "." + l_filename + "(" + f_str + ")" I changed this to f'INSERT INTO {l_filename} ({f_str}) VALUES ({place_holder})' And it worked.., Thanks for your time.
DataCamp is a good website for learning if you want to try something other than coursera
Microsoft SQL uses SSMS (SQL Server management studio) Download SQL Server Express and SSMS and that should be all you need.
https://www.brentozar.com/archive/2018/06/new-stack-overflow-public-database-available-2018-06/amp/ /u/BrentOzar’s articles on the StackOverflow database are what made me feel comfortable with it. https://www.brentozar.com/archive/2015/10/how-to-download-the-stack-overflow-database-via-bittorrent/amp/
You can try to set up a ODBC Data Source for the computers that will use that excel file. Then use that source in excel to read your data.
This is how you set up database connections with PyCharm. https://www.jetbrains.com/help/pycharm/connecting-to-a-database.html There’s a database toolbar you have to show. Also, go grab Postgres or create a new SQLite DB file and connect to it. If you’re a Pythonista grab the EDB PG installer because it has an option to install Pl/Python as an extension during the install. (That means you can write Postgres functions with Python inside the database, which is awesome). https://www.enterprisedb.com/downloads/postgres-postgresql-downloads
Do you know what type of database you'll be using (e.g. Postgres, MySQL, SQL Server, etc.). You're looking for a "desktop SQL client", so you can just Google "desktop SQL client for [your type of database]". [SeekWell](https://seekwell.io/) supports most common database types and is super fast to write code in. Check it out and let me know what you think!
Read on the website "Artificial Intelligence" -&gt; ALT+F4.
Each DBMS uses different syntax. I think you may have missed something in the course
I do this very thing with my ERP. My ERP runs on Oracle and Linux, and one thing about Oracle is that Oracle is really good at pretending that Microsoft doesn't exist. Therefore each computer that needs to run the Excel report has to have its own ODBC driver and TNS setup, and that's not natively available in Windows. That makes it a pain to run because not everyone can just open Excel and refresh the data, as ODBC is both the middleman and the gatekeeper. Once you get ODBC working, you can easily save the password in the ODBC data connection properties. Be very careful with that. Sometimes you might be using a schema that serves as the root user for the ERP, or at least has some ability for DML or DDL, which is dangerous. I created a read-only user for that purpose that can explicitly select from a list of key tables and nothing more, and you'd want to do the same. And depending on where you save the password, sometimes the password is saved in plain text and can be read in properties of the connection, so that has security implications that you'll have to be the judge of. If your ERP is using MS SQL as a back end, I believe drivers are natively contained in Windows 7 and Windows 10, so forget everything I said about Oracle and TNS. But they would still need an ODBC connection with whatever DSN name that you used at the time you created the report. It's the same idea as described above but it takes less steps because your drivers already exist. I also want to present another option that plays very nice with Excel. Use MS Access as another middleman. With Access, you can create a passthrough query that is your ANSI-compliant SQL, as complex as you want to make it. But Excel cannot "see" those so they cannot be used directly as a data source. But what you can do is create another non-passthrough, or "regular" query that simply "selects *" from the passthrough query, then create an Excel data connection using the "regular" query object. In that configuration your request goes something like this: Excel --&gt; Excel data connection --&gt; MDB/ACCDB file --&gt; Access Object (query) --&gt; Access passthrough query --&gt; ODBC DSN --&gt; ODBC driver --&gt; Database call As far as automatically refreshing the workbook when it opens, simply add some VBA code to the workbook open event for all data connections and pivot tables, if applicable. Private Sub Workbook_Open() ActiveWorkbook.Connections("CONNECTION_NAME").Refresh Sheets("SHEET_NAME").PivotTables("PIVOT_TABLE_NAME").PivotCache.Refresh end sub
Awesome, glad I could help!
Get AdvevtureWorks. It's a free download. It's the sample db that Microsoft used to do all their demos &amp; instruction with. There are still copies readily available online with some light Google action. It has many tables with decent table sizes.
fake it till you make it. but in all honesty, we are developing models that rank both search results (in your code repository) and autocomplete results. It's certainly narrow AI, but AI none the less.
Are you referring to the fact that trailing spaces are considered insignificant for almost everything in SQL, so "hello" is equal to "hello ", for example. This is all part of the ANSI standard for SQL, so products have to behave like this. See [https://dba.stackexchange.com/questions/10510/behavior-of-varchar-with-spaces-at-the-end](https://dba.stackexchange.com/questions/10510/behavior-of-varchar-with-spaces-at-the-end) for a good discussion on this.
Isn't Hadoop Ambari a Web UI? What is the SQL Language? Oracle? T-SQL? MySQL?
All I know is apparently in the environment we are using SQL in, there are issues when joining the tables and we think it’s due to spaces being padded after field values. I was basically asked to create a table and prove this. But I don’t know how I would prove that
Yes, using zeppelin in Ambari which uses spark sql
If you're looking to find the ones with Spaces (this isnt Spark SQL but, I assume you can adjust): &amp;#x200B; SELECT * FROM table where CHARINDEX(' ', &lt;Column&gt;) &gt;0
So I can create two tables, join them, and then run that query on it to see if spaces were padded?
I apologize but I am an amateur. I’m trying to learn though. We all have to start somewhere
Yes, it's columns specific so I would use the ones their having the most problems with from that Table. Like "LastName", "FirstName" etc from a Person Table.
Make a read only user account for the tables that are needed, then use that user's login credentials inside your database connection.
I use “The Ultimate Oracle SQL Course” from Standout Dev because I use Oracle SQL Developer. I write stored procedures for my BI tools but anything that will be put into production has to be sent to, vetted and tested by our Developers and they will place into production
i second this suggestion. i've been doing self study with AW both for SQL and PowerBI. &amp;#x200B; Here's a download link to both OLTP and DW versions: &amp;#x200B; [https://github.com/Microsoft/sql-server-samples/releases/tag/adventureworks](https://github.com/Microsoft/sql-server-samples/releases/tag/adventureworks)
I'd probably do something silly like length(field)-length(trim(field) regards
Take the inner subquery and dump it into a #table, then run your query against that. See if it still timesout.
There’s a lot to unpack here. First let’s start with trying joins again. In your main query, when you get to your from, ask yourself how each of these tables relates to each other. For example you have a product table, does the product table have any foreign keys or is it itself a foreign key on any of the other tables you are getting information from? And vice versa for the other tables? It appears you’re joining all the tables using pr_xref_id tables pin column. You could join each table using that column maybe? Looking at your IN statement it looks like the sane kind of issue. Some people say IN statements aren’t the best to use, optimally speaking, and they’re right, but for now let’s keep the IN, but think about maybe creating two IN statements: one for pr_xref_id and one for mkt_cd. In the WHERE clause in the IN statement you say where d.pin = c.pin and c.pin = d.pin. These are redundant, you can remove one of them. Start with that, let me know if you are having trouble with any of the concepts. I’m on mobile so it’s hard to type all the nuance out.
&gt; When I've tried using JOINs and ON statements, my sybase connection timesout. It's probably because I'm not optimizing it. No, that's not it. &amp;nbsp; things to ask yourself while writing a query: what's the granularity of the desired output? what are granularities of the initial data sets? How are the datasets related? This will allow you to write your joins/unions/group bys.
Unrelated to your request, my gut is telling me you might want to parenthetically phrase the "[a.pin](https://a.pin) = [c.pin](https://c.pin) and [a.pin=d.pin](https://a.pin=d.pin)" part of the where clause so: where (a.pin = c.pin and a.pin = d.pin and a.pin = [e.pin](https://e.pin)) - just to make the query feel cleaner. &amp;#x200B; Related to your request: as said below by, [nebacahnezzer](https://www.reddit.com/user/nebacahnezzer/), I would try to write two separate IN statements for a.pr\_xref\_id, c.mkt\_cd IN... I can't imagine SQL liking that syntax very much. &amp;#x200B; Are you getting errors, or are you getting unexpected data returned?
You need parentheses around the two columns. and (a.pr_xref_id, c.mkt_cd) in (select a.pr_xref_id, c.mkt_cd from product c join interest_level d on d.pin =c.pin join pr_xref a on a.pin = c.pin WHERE c.mkt_sector_cd = 'OPTION' and d.interest_level =1 and a.pr_xref_cd = 'CBT') The above is standard SQL, but not all DBMS products actually support that. I don't know if Sybase is one of them.
And in the rare case you **do** have the required talent in-house, the contract is there in case the product is not behaving as it *should*.
Not sure if you've gotten this straightened out, but I thought that I would reply anyway... A stored procedure does not have to have any parameters. You could write an SP named sp\_GetCustomer1 that simply executes this sql statement. "SELECT \* FROM Customer WHERE CustomerID = 1". Every time you run that SP you get the results for CustomerID 1 because that is what is hard coded in to the SQL statement. Where stored procedures become more powerful is in their ability to accept parameters. Wouldn't it be more useful for the query above to work for any customer? Instead you could write it as "SELECT \* FROM Customer WHERE CustomerId = @CustomerID" Now you can pass the customer ID or better your the customer name and dynamically return the data for different customers. Example of that... "SELECT \* FROM Customer WHERE CustomerName = @CustomerName" Now your SQL code is reusable. This becomes much more important as your data model gets more complicated. If you have to join multiple tables and filter on many different criteria would you prefer to write that query once and reuse it, or have to write that query from scratch every time. Particularly if the query becomes large. You don't want to write a 100 line query from scratch every time. There are other considerations such as database source control, the ability to use a cached query plan, or the fact that you should never use SELECT \* in a stored procedure, but I think those are outside the scope of your question. One thing I did want to point out is that the SQL community is one of the best tech communities I have ever found. There are tons of people out there who actively support other SQL server professionals and offer free advice on platforms such as slack, twitter (#SQLHelp), sqlservercentral.com , etc. Hopefully you find an answer to your question.
 SELECT * FROM ( SELECT player_id , transaction_year , july_salary , sep_salary , ROW_NUMBER() OVER (PARTITION BY player_id ORDER BY transaction_year ASC) AS rn FROM #temp ) AS x WHERE rn = 1
How can I do this making another temp table rather than an AS table?
just do the inner part as a temp table, then, select * from your temp table where rn=1...right?
&gt; How can I do this making another temp table rather than an AS table why would you want to? this keeps things simple and avoids the overhead of creating another temp table why do you have `#temp` anyway? how do you create it?
I’ve never quite understood this partition thing.
Data.gov has cool, real data sets collected by the US government.
Yes you can. Look for "analyst" jobs. Shoot for 60k/year minimum, unless there is something wildly attractive that you can leverage in the future.
think about it as a group by but just for that function
Yes.
There are a number of databasing, analyst, or science jobs that you are pretty qualified for just by knowing SQL well. Data scientist, data analyist, data visuslization or DBA, (database administrator/manager) Data scientists and visualizers usually know quite a bit of tableau as well. Though, tableau is much easier to learn than SQL in my opinion. I would not accept less than 60k for any of the jobs listed in any location. If you work in a huge tech center like Seattle or San Francisco for example. I would ask for 85-170k depending on experience and portfolio.
Entry level analyst jobs are a great way to break into more lucrative work. I processed claims for an insurance company for years, then looked into entry level IT positions and grew my career form there. I have to say being knowledgeable about what the companies business processes were made transitioning to a more technical role and relating backend database structure much easier.
Thank you for the reply! I did use Tableau for a couple years as well, so I’m pretty familiar. My company, as a part of cost savings, downgraded to PowerBI recently, which is much less capable tool. But it is nice that I’ve had to build data sets and do analysis with more than one tool. And it just so happens that I live in Seattle, so I know there will be opportunity.
Would you recommend 85k for entry level data analysts in SF?
Data scientists are expected to know quite a bit about statistics and how to use statistical modeling. At least in California. No way a guy who just knows SQL can be a data scientist.
I agree with you; however based on my own experience are lots of places right now asking for 'data scientists' but all they want is someone to make pretty charts in tableau... You can usually tell by the salary offered, around 65k for a 'data scientist' vs 150k - 200k for a 'real' Data Scientist.
I would kill for an entry for a level role.
&gt;Shoot for 60k/year minimum Everyone on the internet is assumed American until proven otherwise.
i would kill for someone in IT to actually care about knowing what the business does and how it works... since, ya know, the software is supposed to enable the business folk... instead of doing what the IT folk think it should.
That job title inflation is so real, and so harmful. Not only does it de-prestige a really specialised and highly educated role, it's further confusing those poor management types who already struggle trying to understand what a good data team looks like.
&gt; downgraded to PowerBI recently, which is much less capable tool whoa son thems are fighting words 'round here
Haha Not looking for any trouble, so I’ll just leave it at that.
Well, figure out whatever that translates into your local currency, and if jobs don't pay that much then figure out what it takes to get an H1 visa and go that route.
Sigh. You aren't wrong.
Haha I’m sure that’s a real problem out there. I feel good about my position because I am very familiar with how the company operates on all levels, from product, sourcing, financials to marketing. I really don’t feel confident not knowing, so I ask more questions than I think most would believe is necessary.
I've found Tableau to be easier to get pretty visuals out of the box, but the Tableau Prep add-on is nowhere close to what you get with Power BI out of the box. If your data sources are anything other than SQL databases, or if your measures require complicated multi-table models that can't be flattened before importing the data, Power BI is streets ahead.
Hi, I'm a data scientist. I have three years experience working in Excel, and can make charts in Tableau.
I can agree with that. One of my biggest frustrations was with Tableau’s shitty “blend” feature that was pretty useless. So PowerBI definitely wins there.
They call maintenance workers "engineers" at a lot of hotels around me. It makes job searching just a little more anoying for me.
You better hold me back!!
That looks muuuuch better.
Start with simple selects, move on to table joins. If you have a test database you can work on insert, update, delete, and truncate statements. Then on to creating tables, and learning about keys and indexes. Stackoverflow is a great site to find answers. Good luck and have fun.
\*does math\* \*cries in antipodean\*
I can't tell if you're joking or literally quoting a job candidate.
&gt; antipodean Probably want to learn English if you're going to come to Amerika, too.
/r/businessintelligence
Udemy.com There’s a course called something like 70-406 that seems very good.
A lot of companies like to use the title, "data engineer," or something similar. I have a friend who talks about how he's an, "engineer" at parties, and I never say anything, but yeah, he didn't go to engineering school, and definitely is not an engineer.
Tableau is a piece of shit.
Check this https://www.youtube.com/watch?v=n6gM265zG68&amp;feature=youtu.be. It is kind a what you re looking for. Quality exercises, for beginners, well structured, with data sets preloaded.
"We tried to do Agile, realized its ridiculously tedious and encourages micromanagement, so now we do just do 'Sprints' by themselves and call it Agile"
Yes. Or...sort of. You can get a job in a number of positions that are SQL primarily. However, for a career there’s always something specific to go along with it. Data Warehousing, ETL development, Database Administration, BI Development - these are all heavy SQL development jobs. SQL will get you a job, but you have to pick up other skills. Also, a scripting/automation language is something you should learn well. Python, PowerShell, Bash, R etc.. Python in particular is popular. Works well with SQL, is an easy general purpose programming language, is good for data manipulation, and has a ton on data analysis and data science tools out there. Python and SQL make an awesome pairing. I prefer to do my real data work in SQL, and use functional Python code as the “glue” to automate between processes.
Production support. But might as well learn the code and make more money for less work. Imo
Download ApexSQL Complete. This plugin helps a lot
Wagile
Fr-agile
I see, thanks for pointing this out. There's no necessity for realtime but i thought it might be used more nowadays and that it would be relatively easy to implement. Given all reactions, i think i'll stick to the Management Studio configuration with scheduled jobs, since i definitely don't want to complicate things.
The best way of viewing, searching, a database is using a database server. There are several database servers that are free, e.g. [Microsoft SQL Server](https://www.microsoft.com/en-us/sql-server/sql-server-downloads), [MySql](https://www.mysql.com/downloads/), [PostgreSQL](https://www.postgresql.org/download/) The .SQL file might be specific for a certain server so check where you have downloaded it for what server they recommend/use. &amp;#x200B; For Microsoft SQL Server there is SQL Server Management Studio for viewing/searching the database. MySql has Workbench for the same and I guessing that PostgreSQL also have some application for viewing/searching. &amp;#x200B; There are a lot of possibilities but there is a learning curve!
 Wet good job. I’m impressed
Jesus. That's so accurate, it hurts.
Just the note, the called SP can't have Insert Exec, because nesting that isn't supported. Also you need to define the schema of the resultset beforehand.
1/ density vector from statistics of that column 😁
Jumping in on this, I just purchased a course by Jose Portilla (sp?) that’s on sale for $11. Loving it so far as he’s also taught a few other courses on Udemy!
Hey matt - Sorry about that, please drop me a note at mike@seekwell.io
Can’t wait until the day they make us stop this form of ‘agile’. Waste more time having meetings about meetings about meetings. Just let me work
Can you post the data without the servicerUser?
Seriously. — “15 min” scrum 5x / week — 1 hour sprint planning — 30 min Retrospective — 30 min Sprint grooming That’s almost 4 hours of a 40 hour work week spent planning how to do work that I’ve been doing for years
Depends on how you're using this code (production versus ad-hoc; single query versus stored proc, etc.), but the quickest solution would be to separate this table into multiple tables based on "Action", then join back onto itself to perform the datediff(). &amp;#x200B; You could separate the table into multiple methods, again depending on how you're using this code: Subqueries, CTEs, temp tables, etc. &amp;#x200B; For ad-hoc, my preference is CTEs because of the added readability. So, for example, assuming the table is named "dbo.ThisTable", the solution could look something like this: &amp;#x200B; &gt;`with cte_Released as (` &gt; &gt;`select &lt;columns&gt; from dbo.ThisTable where Action="Released"` &gt; &gt;`)` &gt; &gt;`, cte_Resubmitted as (` &gt; &gt;`select &lt;columns&gt; from dbo.ThisTable where Action="Resubmitted"` &gt; &gt;`)` &gt; &gt;`select &lt;columns&gt;, datediff(days, cte_Released.CreatedDate, cte_Resubmitted.CreatedDate) as DateDifference` &gt; &gt;`from cte_Released` &gt; &gt;`left join cte_Resubmitted on &lt;table's prime key&gt;`
I’m in but I’m a little advanced. How do you wanna stay in contact ?
I’m keen. I’ve done a few courses but honestly haven’t practiced enough for it to sink in. Any thoughts on how to buddy?
This picture and all these comments... you are my spirit animals!
we do 2 1 hour every sprint, sometimes 3
It might be interesting to grab some data and have everyone create a database and then create different queries and reports. That way everyone can look and see how others came up with their information. Then have a discussion on which method worked best and what you can use from each.
Ok I’ve got that far, now how do I tell SQL which ones to pair together for the date diff? In this scenario there are 3 releases and 3 resubmits. Thanks!!!
Hi. I have added more information in the post above. Please read it and let me know if you are in.
&gt;Hey. I am not so advanced to do that. I am a novice in this as of now.
Haha, I hear you. I am only a beginner at SQL. It still might be a good start. Just do what you can do and learn from everyone else
I have edited the post above a bit to provide more information as to how I plan to learn SQL. Please let me know if you are in.
Have you attempted this? Or just trying to get someone to so your homework..
Not familiar with Postgres but I see a few issues: * There's nothing aliased as 'p', but you are selecting p.gender, p.dob and the where statement all refers to 'p' * That's a lot of tables, but you don't have any join statements? you'll need to tell the query how those tables relate. I'm not sure if Postgres has comma seperated UNIONs or the UNION keyword but given the name of the tables I'm not sure a UNION is the intended result * Unless in a subquery you can't have multiple FROM statements * Normally you aren't allowed to filter on calculated columns from the query, so 'rn = 1' might not work unless you wrap the whole query in a CTE first
[How about $15/hr](https://www.indeed.com/jobs?q=data%20scientist&amp;vjk=eeaaa4fe58ff0551)
&gt;Postgres has comma seper How do you suggest I go about it? My whole goal is to display all of those columns but I keep having the subject Id repeat countless times. So i'd like it only display one row per subject Id. ): &amp;#x200B; I did fix the 'p' issue though still displayed the same error.
That gets a lot more complicated. Without pulling the dataset and just off the top of my head, I'd say might get by with some combination of a value window function like lag() in one of your CTEs and a between (or preferabbly x &gt;= y, x &lt;= z) in your join. &amp;#x200B; value windows functions link (look for lag() function): [https://www.sqlshack.com/use-window-functions-sql-server/](https://www.sqlshack.com/use-window-functions-sql-server/) &amp;#x200B; My first attempt would be to take any release date record and use the lag() function to create a new column that contain the next release chronologically sorted release date value. So you'd have something like this: &amp;#x200B; **RealeaseRecordColumns | ReleaseDate | NextReleaseDate** &lt;Columns&gt; | '2019-03-20 10:20:00' | '2019-04-09 11:50:00' &lt;Columns&gt; | '2019-04-09 11:50:00' | '2019-05-07 11:20:00' &amp;#x200B; Then from resubmitted, you could join where resubmitted.CreatedDate &gt;= released.ReleasedDate and resubmitted.CreatedDate &lt; released.NextReleaseDate
I like where you’re going with this. I’ll be back shortly after I try it.
I used to work in a team that spent an entire day (8h) on planning each sprint (2-3 weeks). We were doing it by the book - writing down tasks on a whiteboard and discussing the cost in Story Points etc. The whole team (5 people) participated. Later on 2 people did the same thing in 30 minutes each sprint without writing down task details and the productivity and estimation accuracy was the same. &amp;#x200B; SCRUM is a waste. Try to change my mind.
&gt; no two columns in b or c should exist that reference the same column in A. this is literally the opposite of what you want `B` and `C` should each have a column named something like `A_id` which is a **foreign key** that reference's `A`'s primary key perhaps you could rephrase your questions in light of this
based on ui look, this seems to be ms sql. Did you try the usual "cross apply (select top 1 ... order by ...)?
&gt; you don't have any join statements? the tables are all mentioned in the FROM clause the join conditions are all mentioned in the WHERE clause this is deprecated but still perfectly valid SQL
&gt; How do you suggest I go about it? you're okay for now but i suggest you rewrite the joins using explicit JOIN syntax
&gt; but also they must have one common autoincremented field Postgres has sequences, so having a common synthetic key across 2 tables shouldnt be an issue.
I have been studying sql for three weeks &amp;#x200B; Please contact me &amp;#x200B; I am currently working on sqlbolt dot com. I'm not sure if you are ahead or behind me in terms of knowledge &amp;#x200B; I still don't understand why "WHERE YEAR % 2 = 0" shows only movies released on even years. SQL Bolt lession9 &amp;#x200B; SELECT title, year FROM movies WHERE year % 2 = 0;
not going to dig into your code, but you could do the following: for both your buckets and holdings, create running sums of "value" and for each record have running total of value. You get the "range" for each record this way ("from value - to value"). Join on overlapping ranges, and calculate the overlap to figure out what's applied where.
Thanks for the response. I've done basically this in my code. However running into an issue where the AmountApplied is displaying the TotalAmount of the bucket rather than the remainder applied if a bucket is filled with a leftover holding.
The ROW\_NUMBER statement has to go in the select part of the query, not the FROM. IN other words move ROW\_NUMBER ... rn to before FROM. &amp;#x200B; Then your query will fail because you can't filter on a window function and your where statement says 'WHERE rn = 1'. You'll need to wrap a derived table around the query and put the rn=1 filter outside it to make that work.
Actually you might want that row number in its own derived table, so something like : ... as outp, (select ROW\_NUMBER ... FROM ...as ad) t
I'd be interested in hearing more about this. I usually don't reach for apply outside of using functions, but I could see this working better.
what data is being \*defined\* by truncate/delete? it is to laugh truncate/delete \*manipulates\* data by deleting it, without changing the definition of the table
what's the issue? there are 9 scenarios of the overlap, just code them one-by-one accordingly or code your overlaps and create a map of overlap type to a relevant format.
the **execution plan** here's a sample article -- https://developer.rackspace.com/blog/understanding-a-sql-server-execution-plan/
Check out Grant Frichey's book on Execution Plans, it's free from RedGate's website
If I understand correctly, you might find either CASE statements or the proper use AND/OR operators in a WHERE clause helpful. &amp;#x200B; CASE Statements: [https://www.w3schools.com/sql/sql\_case.asp](https://www.w3schools.com/sql/sql_case.asp) AND/OR in WHERE Clause: [https://www.w3schools.com/sql/sql\_and\_or.asp](https://www.w3schools.com/sql/sql_and_or.asp)
Your query is doing a modulus calculation and saying "where the YEAR column's value... divided by 2... has a remainder of zero." In essence, "is YEAR cleanly divisible by two and therefore an even number?" \[dividend\] % \[divisor\] = \[remainder\] 2000 % 2 = 0 TRUE 2001 % 2 = 0 FALSE 2002 % 2 = 0 FALSE 2003 % 3 = 0 FALSE 2004 % 3 = 0 TRUE A good example of when you might use this is to grab a sample set from a table. If you wanted to grab every 15th row in a table that has an incrementing ID column you could do: SELECT \* from table1 WHERE [table1.ID](https://table1.ID) % 15 = 0
If you do get a small study group together, I'd be willing to sit in and help answer questions when you're stuck. Shoot me a PM.
You can probably do this with a "case when statement" in your select statement. A "Case when" will allow you to parse the data into it's own column. Example: SELECT CASE WHEN (person\_age &gt; 55 and person\_age &lt; 81 and person\_gender = female) THEN 1 -- a 1 in this column will indicate true ELSE 0 END AS Females&gt;55, -- so this will make a new column called "Female&gt;55" but call it whatever you want CASE WHEN (person\_age &gt; 50 and person\_age &lt; 81 and person\_gender = male) THEN 1 ELSE 0 END AS Males&gt;50 FROM Table \-- The case statements act as their own columns here. New columns that were not there before. You can make the 1's -- and 0's strings or whatever variable you want. \-- OR you can put it all in one column: SELECT CASE WHEN (person\_age &gt; 55 and person\_age &lt; 81 and person\_gender = female) THEN 1 -- a 1 in this column will indicate true for the female population WHEN (person\_age &gt; 50 and person\_age &lt; 81 and person\_gender = male) THEN 2 -- a 2 in this column will indicate true for the male population ELSE 0 END AS Geriatric\_Population FROM Table &amp;#x200B; Hope this helps!
Ok thanks
Hey all! Thanks so much to this community for the comments and help - I sort of combined some of everyone's posts here and made it work! Here's what I did in case you all want to see. &amp;#x200B; ,released AS (SELECT reltime.InvoiceID, reltime.createddate, ROW_NUMBER() OVER(PARTITION BY InvoicePull.InvoiceID ORDER BY reltime.CreatedDate ASC) AS Ranked FROM InvoicePull LEFT JOIN ecde.INVCE.InvoiceChronology_current reltime WITH (NOLOCK) ON reltime.InvoiceID = InvoicePull.InvoiceID WHERE 1=1 AND reltime.Action = 'Released') ,resubmitted AS (SELECT reltime.InvoiceID, reltime.createddate, ROW_NUMBER() OVER(PARTITION BY InvoicePull.InvoiceID ORDER BY reltime.CreatedDate ASC) AS Ranked FROM InvoicePull LEFT JOIN ecde.INVCE.InvoiceChronology_current reltime WITH (NOLOCK) ON reltime.InvoiceID = InvoicePull.InvoiceID WHERE 1=1 AND reltime.Action = 'Resubmitted') SELECT InvoicePull.*, AVG(DATEDIFF(d,released.CreatedDate,resubmitted.CreatedDate)) AS ResponseDays FROM InvoicePull LEFT JOIN released ON released.InvoiceID = InvoicePull.InvoiceID LEFT JOIN resubmitted ON resubmitted.Ranked = released.Ranked AND resubmitted.InvoiceID = released.InvoiceID
/u/username_dsf /u/ichp /u/noesqL
That depends on the actual DBMS you are using. Every query optimizer works differently and has different options on how to optimize a statement (different index types, different index access methods, different physical table options, different join types, parallelism and so on). I doubt you will find a DBMS independent book that helps you with the way a specific optimizer works.
Thanks.:)
It's a CASE *expression*, not a "statement".
In standard SQL can apply a filter on e.g. an aggregate function: e.g.: select count(*) filter (where gender = 'female' and age between 56 and 79) as female_count, count(*) filter (where gender = 'male' and age between 51 and 79) as male_count from ...; Not all DBMS support that however. If that is the case for the DBMS you are using, you need a CASE expression to achieve the same thing. The equivalent of the above using a CASE expression would be: select count(case when gender = 'female' and age between 56 and 79 then 'x' end) end as female_count, count(case when gender = 'male' and age between 51 and 79 then 'x' end) end as male_count from ....; All aggregate functions (count(), avg(), sum(), ...) ignore NULL values, and the CASE expression returns a non-null value for those rows that should be counted and a NULL value for all others (because there is no ELSE branch)
Sounds like people ITT are doing it wrong, blame management. Daily 15 min standups. Avg about 7 minutes for a team of 8. What did you do yesterday and will do today... you dont have to go into detail. You shouldn't. You are talking to your team, not your manager. Dont chat about how to fix it until we are done. just say.. I need your help offline. Or... there is a non sprint goal fire i have to put out, I'll create cards accordingly. End of stand up. You're probably in an open office and chat throughout the day, this morning meet is to just let your team know where you will be messing stuff up in case they have issues with messing it up themselves. Same crap as yesterday is an acceptable answer. Retrospect (oxt friday from planning) 30 mins every 2 weeks (end of sprint). Usually us just saying, things went awesome. Nobody came and asked us to do anything non sprint related because my 'manager' blocked them / is the go to. +30 minutes for a demo, if we have one. Keep it simple: the code is irrelevant, the concept and use are the important part. Its Friday, let's waste the day. Planning (monday after retro) 1 hour easy. More if stakeholders are there. 20 mins: That stuff we prevented you from being bothered with... that's gotta be done now. Score it. 30 mins: what can we do this sprint. 10 mins: what, of what we can do, should go into the goal as... WILL BE DONE no questions asked. Its Monday, let's chat instead of work. Garfield the hell out of this day. How many points should you get per sprint? Doesnt matter, sometimes a 5 is a 2 and a 1 is an 8. Do what you can, meet the goal, grab more. Your team is setting the goal, your manager is trying to keep you on target. Target meaning this is what upper wants, let's show them we are moving in that direction using scores and cards.
Ah okay, its strange that everyone else labels it as DDL, thank you.
https://launch.solarwinds.com/rs/solarwindsworldwide/images/12_Step_Query_Tuning_SQLS_IG.pdf
 select stuff from people where (gender = 'M' and age between 50 and 80) or (gender = 'F' and age between 55 and 80)
DELETE is DML in every implementation that I've worked with, which makes sense: it's just changing the data in a table, not the table object itself. You're right TRUNCATE is considered DDL in most every implementation, which is kinda weird since it's so similar to DELETE. Now, DML statements happen within transactions, i.e., a DML statement is not "permanent" until you run COMMIT. Before running COMMIT, the statement can be undone with ROLLBACK. In most implementations TRUNCATE does *not* run within a transaction, and specifically it usually can't be rolled back. As far as I know, this is why it get's bucketed with other DDL. SQL server is not one of these implementations. In SQL server, TRUNCATE can be rolled back during a transaction. So in this case, it does make sense for them to clump TRUNCATE in with the other DML statements.
I absolutely agree that they're all different. I read a good one by Jonathan Lewis for Oracle, about performance in general but it did get into the guts of the query optimzier, but that was at least 10 years ago...
That was very insightful thank you.
Just started ultimate mySQL boot camp on udemy if anyone is taking that
Hey. I have a study group going. Let me ping you.
Which database? I can guess by your use of brackets but it's best when clearly stated. The date math functions are different per database. That said it sounds like, for starters, you basically need to use something like the LAG window function to get the previous date, then just add 42 days to it.
MS SQL 2016, but instead of adding new columns using LAG and LEAD I actually want to add new rows with new Appointment\_date. At the moment I'm looking at an UNION ALL with a cross join to add new calculated rows.
How the fuck does agile encourage micro management. It basically abolishes your manager when done right.
Trying to understand what you're going for here... If you could build your sample data into temp tables on [sqlfiddle.com](https://sqlfiddle.com) that would help a lot. You're wanting to count the number of accounts within a company and represent that as a percentage of the total, grouped by type?
The execution plan is always the place to start for optimization of a particular query/proc. There's so much information in there if you know how to read it properly. Of course, sometimes the best optimization technique is a design/architecture rethink.
Every single day in Scrum I get to announce to my group(and my manager) exactly what I worked on the day prior and what I’ll be working on that day. It breaks every task I do into a piece and a timeframe that my manager can review and plan out in sprint planning. Now my Manager can keep tabs on every single part of a project I’m working on. Sure, maybe If implemented correctly it wouldn’t encourage micromanagement. But the vast majority of teams don’t implement agile correctly and use “their own version” just like the post describes.
I don't want to count the number of accounts, but rather sum up the total amounts for each type. For a particular company, each date will have multiple accounts with a different amounts. Each account is associated with a category (Type). I want to sum the amounts for each Type, then divide it by the total across all accounts (for that company and date) to get an overall percentage. I would be left with the overall percentage of each type (Small, Medium, Big), for each date within the preselected range
How are you deriving the value of 49 for the prediction?
Are there any indexes in the large table? It may help to do a temp table or cte of an indexed column to narrow the result set before doing the rest of the needed functions, then looping through for each company. Using an indexed column should speed up the "narrowing down" part. Just a though.
Row over summing? On mobile at the moment but let me throw together psuedo code to show what I mean when back at pc.
You could put all your queries in temp tables or common table expressions and then join on them with one final query.
Definite push in the right direction Thank you!!
Thanks a TON for your effort
You're all right on this - thank you
49? I think you mean 42? &amp;#x200B; The 42 days are the avarage days between appointments for all projects. It is a set value.
And you want that to update as time goes on?
The column 'running\_total' is the amount of total days that passed since the start (sum of days\_between). It needs to update as long as days passed are &lt; 450.
How involved are the queries? Because if they are fairly simple then INNER JOIN *query 2* as Q2 on Q2.MS_07 = Q1.MS_07 INNER JOIN *query 3* as Q3 on Q3.MS_07 = Q1.MS_07 INNER JOIN *query 4* as Q4 on Q4.MS_07 = Q1.MS_07 INNER JOIN *query 5* as Q5 on Q5.MS_07 = Q1.MS_07 Should work just fine If they are more complex then as /u/Herdnerfer said you can use Temp Tables or CTE's
Hi Daakuryu, the queries are faily simple... here is an extract of the first two: /\*\*\*\*\*\* Find Resources with Strategic Suppliers \*\*\*\*\*\*/ Select \[MS\_07\_ReWorked\], Count(\*) AS 'Resources\_With\_Strategic\_Suppliers' From \[dbo\].\[GDW\_EXTRACTED\_WIP2\] Where \[Strategic\_NonStrategic\] = 'Strategic' Group By \[MS\_07\_ReWorked\]; &amp;#x200B; /\*\*\*\*\*\* Find Resources with NON-Strategic Suppliers \*\*\*\*\*\*/ Select \[MS\_07\_ReWorked\], Count(\*) AS 'Resources\_With\_NON\_Strategic\_Suppliers' From \[dbo\].\[GDW\_EXTRACTED\_WIP2\] Where \[Strategic\_NonStrategic\] = 'Non-Strategic' Group By \[MS\_07\_ReWorked\]; &amp;#x200B; So with your solution, how would I incorporate it into the queries I have above? Thanks again
Should be pretty straight forward to do with a CTE or #table, and you can just UNION your new rows in. Where are you getting lost?
The fact that I'm quiet new to SQL for reportbuilding :) you got any good examples?
Something like this: with model as ( Select unique_id , appointment_date , days_between , SUM(days_between) OVER(ORDER BY unique_id, appointment_date) as Running_total from [records] ) select * from model union select logic_here from model Add a couple more queries to the CTE to calculate 49, or whatever it is. Should be pretty straight forward.
All you need to do is add the [Strategic_NonStrategic] column to your GROUP BY list. No separate queries needed for these two.
This way you select the 4 rows two times using Model, but what select logic do I use to transform the rows to new appointment dates until running_total &lt; 450?
Oh, I see, so for each ID you want to dynamically insert n number of rows based on the total, so it might be 2 rows for this ID, 10 rows for the other ID, etc., and each ID would increment by the average?
I think the reason for him doing it this way may be that he wants all 5 query results horizontal though So MS_07, RWSS ,RWNSS, Etc
I tried working it out with row over summing with the results pivoted and got the format you need, but the problem is that based on the two tables you provided there is nothing matching each line in the amount table to a specific type. If you join the tables on account-id then for the 67,456 for Name C then it will match to both a medium and a small type. This results in it saying that both those values represent 100% of the total for that day.
You can do that one in one shot to be honest Select [MS_07_ReWorked], ,SUM(CASE WHEN [Strategic_NonStrategic] = 'Strategic' THEN 1 ELSE 0 END) AS 'Resources_With_Strategic_Suppliers' ,SUM(CASE WHEN [Strategic_NonStrategic] = 'Non-Strategic' THEN 1 ELSE 0 END) AS 'Resources_With_NON_Strategic_Suppliers' GROUP BY [MS_07_ReWorked];
For example, lets work with (1) ID and talk through it. In your original data you return these results: | ID | Appt_Dt | Days_between | Running_total | | :--- | :--- | :--- | :--- | | 123 | 2019-07-02 | 0 | 0 | | 123 | 2019-07-02+42 | 42 | 42 | | 123 | 2019-07-02+42+42 | 84 | 84 | | 123 | 2019-07-02+42+42+42 | 106 | 106 | | 123 | 2019-07-02+42 | 42 | 42 | So here you want to keep adding future appointment dates until the running total is ~450. However, lets say you have a row of data like this: | ID | Appt_Dt | Days_between | Running_total | | :--- | :--- | :--- | :--- | | 789 | 2019-07-02 | 42 | 410 | So above for ID 123, you want to add as many rows as you can up until your 450 thresh hold, but here for 789 you want to only add 1 or 2 more rows. Is that approximately what you're after?
Here's as far as I got with the code based on your examples. tblAmount is the first one, tblDescription is the second. Here's my code: Select RepDate, \[Small\] Small, \[Medium\] Medium, \[Big\] Big from ( select RepDate, type, cast((1.0 \* amount) / (select sum(amount) amt from tblAmount amt where amt.repdate = a.repdate) as Decimal(18,2)) PercentOfTotal from tblAmount A full join tblDescription D on A.Account\_ID = D.Account\_ID ) x pivot ( sum(PercentOfTotal) for Type in ( \[Small\],\[Medium\],\[Big\] ) ) pvt order by RepDate;
Well, where to start, there is no manager in a dev team. There’s just the team and the things standing in there way to do their job. There is a scrum master who’s job it is to remove those impediments and organise retrospectives, and talk to other teams to resolve dependencies. But ultimately in scrum the team is autonomous.
This. Simple but perfect.
PostgreSQL has PGAdmin. Another is Oracle, which does have a free version.
Basically! If we look at the data provided, I would expect the Total table to look like this: |Unique\_id|Appointment\_date|Days\_between|Running\_total| |:-|:-|:-|:-| |P0115220|2018-11-19|0|0| |P0115220|2019-01-17|59|59| |P0115220|2019-03-21|63|122| |P0115220|2019-06-27|98|229| |P0115220|2019-06-27+42|42|271| |P0115220|2019-06-27+42+42|42|313| |P0115220|2019-06-27+42+42+42|42|355| |P0115220|2019-06-27+42+42+42+42|42|397| |P0115220|2019-06-27+42+42+42+42+42|42|439|
What about for when the other queries become slightly more complicated? Such as the one below which has to utilize distinct and multiple lookups? &amp;#x200B; Select \[MS\_07\_ReWorked\], Count (Distinct \[Vendor\_Consolidated\]) AS 'Under\_Utilized\_Suppliers' From \[dbo\].\[GDW\_EXTRACTED\_WIP2\] Where \[TailClassification\] = 'Non-Strategic Under-Utilized Supplier (less than 7 FTE)' OR \[TailClassification\] = 'Strategic Under-Utilized Supplier (less than 7 FTE)' Group By \[MS\_07\_ReWorked\]
If you have any complaints about the process then you are doing it wrong. This told to me by our 'certified' scumm master'. This process won't go away either, there are too many people invested in keeping it going. It's been a massive waste of resource and time but everyone looks busy running from task to task (standup, planning, scrubbing, etc). I hope to change fields soon and won't miss the pain at all.
That I would do as a CTE and join it to the simpler main one.
Right so, you can do this with a loop, but I'm trying to think if there would be a better way to do this with set logic. Something like a cross join or apply. You're basically going to take the rows of data you have, and then calculate all the days that are + 42 from the last row (in this case 6-27), then you're going to throw out all the ones that are greater than a #, which you'll need to either hard code (bad) or calculate in a section of the CTE. So you'll need some kind of date table. You may want to resubmit your post with this additional level of detail. I won't be able to spend much time working on code to help you. I'm almost positive you can do this without a loop.
Take a look at "SQL Tuning" by Dan Tow
Select * from MEAT_STATE Where MEAT not in (Select * from MEAT_STATE Where STATE = 'Cooked')
What ERP do you use?
Most likely much more. Check glassdoor.com.
I don't know. You need to know a lot more than just SQL to be a DBA. It wouldn't take terribly long to get the hang of it but it's not just SQL stuff. Also you can totally get by with just shell for DB stuff on *nix anyway.
Embed a row\_number subquery and then select the top row per partition. Something like this - &amp;#x200B; select meat, meat\_state, time from ( select meat, meat\_state, time, row\_number() over(partition by meat order by time desc) as \[Row\] from meat\_state ) a where 1=1 and \[Row\] = 1
SQL Express is my preference as well.
WITH Cooked AS (SELECT Meat FROM MEAT\_STATE WHERE STATE = 'Cooked' ) SELECT \* FROM MEAT\_STATE AS MS LEFT JOIN Cooked AS C ON MS.MEAT = C.MEAT WHERE C.MEAT IS NULL
25 minutes for a 26 mil row table? 26 mil is small and a full table scan should run much faster than that. You should check your execution plan and see where it’s slow.
I might have some time tomorrow to help with the code development, but let's start primitively. You need a dates table. Google what this is. It's easy to do. So now you're going to all the dates from the min() to the max() in your source table, for all ID's, and then select a list of days that is the total range from start to finish, for all ID's. Next you're going to cross join to this with all your distinct ID's, which is going to give you a list of all combinations of ID's and dates. Next you're going to limit this to only dates that are within X number of days of the first date. You will callculate days_between and running_total last. Focus on getting the dates and ID's together and that part will be simple.
I’m not sure what DataGrip has to do with this question. Seems like it’s just a SQL statement you’re after, but you also didn’t specify the database you’re using, which is probably the most import thing to know. The general idea here is if you want date and time separate, then you should cast the column to a date, and to a time separately. `SELECT CAST(T.DT AS DATE) AS DATE_PART, CAST(T.DT AS TIME) AS TIME_PART FROM TBL AS T`
just because I dislike NOT IN.. SELECT * FROM MEAT_STATE ms WHERE not EXISTS ( SELECT 1 FROM MEAT_STATE ms1 WHERE ms.MEAT = ms1.MEAT and MS1.STATE = 'Cooked' )
Which DBMS product are you using? Date and timestamp function are highly DBMS vendor specific, the SQL client you are using has nothing to do with that. And why are you storing a date or timestamp in a VARCHAR ("text") column? That is a really, really bad idea to begin with.
Things I look for (most of which require an execution plan, although sometimes an estimated plan is good enough if the whole problem is that it tanks the server when you try to run the query): * Sargability issues where a function is being applied to a column to compare it with a constant predicate, when I could instead compare the column to the inverse function on the constant. In the wild, happens most frequently when people manipulate date/datetime columns to match their parameters, instead of having the parameters match the columns. Whenever I make a query 100+ times faster, this is usually what went wrong. * Implicit casting. Sometimes this means you chose the wrong column, often it just means you need to explicitly cast the more general column to the more specific form. Occasionally the fix is to materialize the subset you want ahead of time in the more specific data type. * Sorting earlier than it needs to. You just kind of need to get an intuition for this one - when you see a sort far on the right side, stop and think "Does it really need to be ordered right now?" * Bad row estimates. Again, no general fix, just get to googling. Here's a fun trick, though: if the row estimate is too LOW, look for a predicate that's filtering out no or few rows - you can grab the rows described by that predicate, materialize and index them in a temp table, and then anti-join to that temp table instead of using that predicate. * In general, when it needs to hold a LOT in memory, or there's any sort of "repetitive" element, you'd be amazed at the time savings you can get materializing and indexing stuff in temp tables before your query. If you have a staging database, look in to running it in simple recovery and using minimal logging. * Whenever a GROUP BY has functions in it, I tend to look for ways to handle that grouping ahead of time so the GROUP BY itself is "simpler". This is especially important when people group by pretty/formatted columns - if you want to group by a product name + ID and it's over million of rows, you're probably better served grouping by the ID and doing all that work, and then at the very end taking your aggregated query and joining it to the same table to fetch the stuff you need to make it pretty. That's pretty much my on-hand toolbox of stuff I try first. Beyond that, I just kinda look for operations that are a high %age of the work and aren't index scans and seeks on the right side of the query and google until I find something that looks right.
Why do you dislike not in?
I'm interested!!
Understand executution plans and add \*\*only the right\*\* indexes. &amp;#x200B; https://use-the-index-luke.com/
Solved it always check the params I was passing through a different value to the one I was testing in the SQL Management studio.
https://stackoverflow.com/questions/173041/not-in-vs-not-exists is a decent starting point.
I’ve used a lot of Brent Ozar’s resources. His First Responder toolkit is great, and he’s got a few videos on YouTube about tuning queries. Once you’ve made the changes (in a test environment of course...), SQL Query Stress is a great tool to see how it does under load. Gotta start with that Execution Plan though
Depending on which RDBMS you are working on and which version of that platform, you may have to use different methods. On the newer SQL Server versions, the following is the simplest way I am aware of: &amp;#x200B; SELECT FORMAT (GETDATE(), 'HH:MM')
What IDE are you using? SSMS?
SSMS
There are a few both free and paid - I don't use any of them so I cannot recommend one specifically. Here are a couple of lists of free SSMS add-ons, tools you are looking for are like APEXSQL Complete, but maybe you find something else more useful or even something you haven't considered [https://www.sqlshack.com/top-free-add-ins-for-sql-server-management-studio-ssms/](https://www.sqlshack.com/top-free-add-ins-for-sql-server-management-studio-ssms/) [https://www.sqlservercentral.com/articles/sql-server-management-studio-add-ins](https://www.sqlservercentral.com/articles/sql-server-management-studio-add-ins)
Additionally, if you to View-&gt;Templates inside of SSMS, there are some templates you might find useful
Thanks very much
audio is terrible, cannot hear what is being said
Will do. I’m sure there’s some improvement that can be done on the method to my madness. I’ve got my end, just need to work on the means I’m using to get there lol
Prefer not to answer. Sorry for being vague.
Yes there are some indexes. The date column I have to utilize for prior month’s activity is not currently indexed. If I use another date column that is similar, it cuts the cost of the explain plan in half. Perhaps I should try adding my date column as an index?
It really depends on what you're trying to do. I really like Azure SQL. You can use SSMS and/or Azure Data Studio to interact with and then also explore APIs and features. You can get 12 months of a lot of it for free: https://azure.microsoft.com/en-gb/free/ Cheers.
I use putty terminals all day long. Black text on a grayish-brownish background. The color settings are Red=200, Green=200, Blue=190. Also I turn the monitor’s brightness and contrast down low. And give your eyes a rest every so often, like every 2 hours or less.
Formatting. Indentation. Comments. Use an editor that allows for collapsing/expanding clauses subqueries, etc. If your wall of code is formatted properly, you don't have to do as much eye work to navigate to the bits you care about. For example, I don't like this style: SELECT columnA, columnB, columnC, columnD, columnE, columnF, columnG FROM my_table You have to scan the whole line with SELECT to find a certain column. Instead do: SELECT columnA ,columnB ,columnC ....etc I have plugins in my editor that allow me to collapse the "tall" list of columns, and when it's open, it's much easier to scan for what I'm looking for.
[f.lux](https://justgetflux.com/) to take the blue out of the screen if I'm using SSMS, etc. VS Code in Solorized Light or Dark mode. The newer Azure Data Studio (a spin off of VS Code just for SQL Server) in Solorized Light or Dark mode. Both of the latter allow you to zoom the whole interface making text large.
Yes, you can use the [regular SP2 download.](https://www.microsoft.com/en-us/download/details.aspx?id=30437) Make sure you pick the right version for your system though (x64 vs x86).
Are you interested being the administrator of the database/OS and handling things like configuration and patching details? Or are you looking for a managed database, where you basically just connect to a SQL server that's already running?
&gt;VS Code in Solorized Light or Dark mode. I use the One Dark Pro theme. Love it.
linode.com, do.co, [online.net](https://online.net), vultr.com.
in the big cloud database services, you aren't root, so can't do everything you might want to do, such as compiling/installing extensions you can also just rent a server (virtual private host) and install your own db on it, if you need to be root
That could be a good option for me. Are there any hosting services you'd recommend?
I have DBVis set to a peach background
Ok, I have worked with two of them. If you’d prefer to message, I might be able to help :-) understand if not
1. Syntax highlighting. 2. White/light text on black backgrounds rather than the opposite. 3. Avoid upper case words; upper case keywords is 1970s syntax highlighting. It's not necessary in the 21st century. Lower case is easier to read. Mixed case is fine, too, but all upper case is bad. You don't want words to have a similar appearance. `FROM` and `JOIN` are both square blocks. `ORDER BY` and `GROUP BY` look a lot alike in shape. `from` and `join` are more distinct. `order by` and `group by` are as well. 4. Write your queries with one element per line as much as you can. One field name, one table, one join condition, one where condition, etc. This helps me. I don't know if it helps others. f.lux or whatever the hell is built in to Window 10 works well to cut the blue from the screen and lower the color temperature. Break software that tells you to get up and move around every hour has been important for me in the past, too. I think EyeLeo and Strechly are what I used to use. I'm in the habit now so I haven't reinstalled anything to do it on my new system.
linode has a good rep. or you can use AWS EC2. lowendbox.com if you want something cheap
I wear tinted glasses. Helps.
SSMS has a dark mode which is just disabled from a config file. It's not complete-- for instance, it's black text on dark gray for some of the menu navigation-- but if you're in SSMS every day, it's worth enabling to reduce the eye strain you mentioned. &amp;#x200B; I also pin and hide everything from my SSMS window save for the query editor and the top ribbons. The less busy the screen, the less distracted I tend to be. Additionally, like doctorzoom, I've found that using a more vertically aligned formatting standard helps by making scanning relatively effortless, thus reducing strain. Also, totally unrelated to formatting and eye-strain, but I like this little trick that I've found a lot of SSMS Developers don't know about: Shift+Alt+Up|Down arrows allows you to work on multiple lines at once. I mostly use it to temporarily comment out lines of code or input similar syntax at the same time, but it's a handy little time-saver.
This. I look at a lot of code, and I like dark mode for most things. But I still use f.lux. That "orange" when you turn it on quickly stops registering in your mind and then you don't have a terrible headache at 2PM.
 I have a pair of these: [GUNNAR glasses](https://gunnar.com/product/vayper/) And they work wonders. Slight magnification and the tint and the coating all work together. I got mine on Woot for around $30-40, and they pop up there frequently enough. Also, formatting and indenting is HUGE, not just for eye strain, but for readability of your code.
Use something that makes you feel relaxed, and doesn't make you spend too much time to find what you need when reading. * Good format. It's a holy war on how to format SQL queries, but the best practice is to use shorter, rather than longer lines, keep relevant statements and identifiers visible (not hidden in the middle of the text). It's also great to keep your row lengths to less than 120 columns. * Divide and conquer. If you can split your query into smaller parts, they're easier to read and understand. * Use a good font. I can't stress enough how important this is. I'd easily flip out 200$ for a premium font, but for now I do quite well with using `Hack`, after I've used `Source Code Pro` for a while (and wasted 1h on a bug that was basically confusing `i` with `1`). `FiraCode` is popular too. There are many monospace fonts. A good font is easy on the eyes, quick to read, easy to identify symbols. * Get a good screen. Is your screen too bright? Is it too dark? Too small maybe? Get something that you can read normally.
simple formats... select [colAlias] = t.[colName] , [colAlias2] = t2.[colName] from [dbo].[table] t join [otherSchema].[table2] t2 ON t2.FKID = t.ID AND t2.A = t.A aliases on left make them MUCH easier to find... schema-qualified table names, spacing to keep schema/tables roughly aligned, spacing to keep table aliases aligned. also, write succinct code... too much really stupid crap is difficult to follow.
If it's not something where latency is very important, look at scaleway.com (Paris, Amsterdam) - they have cheap but fast instances (the cheapest I've seen that are not complete garbage). Or you can look at DigitalOcean - a classic for North America, since it's cheap, and low latency for USA.
Windows 10 has a night light function built in
What RDBMS are you looking to run? That will have a bearing on which provider you use.
**SQL access** and **data access in a CRM** are two separate things. SQL Access will generally be one account that has read/write or windows logon accounts. For what has access to what data You should have a table in the DB that defines users and what they have access to. Or a report in the CRM itself that shows users and access.
Yes, one line per item selected and commas at the start of lines rather than the end of the previous makes SQL much easier to read
Assuming that the original developers weren't terrible people who unecessarily split things into dozens of similar but slightly different tables and used integers for table and column names, what you are asking for should be fairly easy to pull off, the problem is that it is going to be very dependent on how they structured things in the database when they were designing it. Ideally there would probably be a table there that has the information that defines who each of the individual users (sales people) are in the system, probably also with an ID number of some kind (which might not be visible from the front end). There's probably another table that connects each of the users to all of the different zip codes (again, probably through that ID number for the user and possibly an ID number for the zip code) which could be something as similar as a column for the user ID and a column for the zip code they have access to. If that's the case then finding what zip codes people are assigned would be just a matter of pulling the data from those tables. Adding or removing zip codes in bulk would be just adding or removing entries. Unfortunately, that's just one possible way that they could have implemented things though, and isn't necessarily the case with your system. If they were nice and made tables with useful and understandable names (and assuming that you are somewhat familiar with how the CRM works), you could probably figure out how things fit together with some select queries like "SELECT TOP 10 * FROM [tablename]" to see what kind of info the tables hold, picking table names that look related to the data you are searching for (starting with tables with User, Employee, SalesPerson, ZipCode, or similar things in the name) and branching out from there, looking for ID columns with the same or similar names between tables (such as a User table having an ID column but a UserZipCode table having a UserID column) and guessing the relationships from there. Bonus points if the designers actually implemented relationships in their system in which case you can just look at how those relationships are built.
This is helpful thank you. But unfortunately the structure of the database is all over the place. The table names are not even remotely understanble. When called support they said this software is so old we don't even know or have. Documentation to support it. I am in one of those situations were it's a no win.
That is unfortunate. If you have someone who is familiar with the CRM and also has a decent understanding of SQL they could probably figure out how things fit together anyways given enough time by brute forcing it and working through each table. Someone who doesn't know the CRM could do it too, it would just probably take longer as they would have less starting info to go on (not knowing what to expect from the data).
Smoke weed, drink juice
Also, creating VIEW's can help standardize common and complex queries. e.g. getting the max record thru a subquery for a user and the values from multiple tables.
Maybe obvious but when my eyes get tired I make the font size bigger. I also agree with people saying use a decent style/format.
Why are commas at the start of lines better than at the end?
In my experience it is a lot easier to not notice a missing comma at the end of a line. Can’t tell you how many times I’ve been writing a long query and get the vague “syntax error at or near char 72, position 13” only to spend 5 - 10 minutes poring over the code to finally find a missing comma. Since my team has switched to putting commas up front as the standard we have all noticed a marked decline in that issue.
For one... Alt+tab allows you to matrix select in smss so they are all in a single line... try it pretty sweet trick if you keep similar things with clean spacing Two... Makes it easy to see if you missed one
Another advantage of using comma at the beginning of line is when you're trying/testing, it makes easier to comment columns without running into stray commas here and there
You might consider deleting this post.
Thanks, this worker perfectly, so much simpler than I was thinking it would be!
I know this is extremely simplifying your problem, but : If allowed, you can also leverage regex in CLR for true regex powaaah. I added the following email addresses to the table &gt; ('loco.burrito.19490@gmail.com'), 'loco.burrito.18840@gmail.com') select e.email from emails e where -- email like two periods and four repeating numbers, but, not five repeating numbers ( e.email like '%.%.%' + replicate('[0-9]', 4) + '%' and e.email not like '%.%.%' + replicate('[0-9]', 5) + '%' ) or -- email like one period and a hypen e.email like '%.%[-]%'
It's definitely a start, I work for a large company and or data sets are huge. I believe im looking for something closer to &amp;#x200B; [e.email](https://e.email) \~ '\^\[a-z\]+\\\\.\[a-z\]+\\\\.\[0-9\]{3}@.\*$' I cant remember exactly how to follow this logic though. Just from looking at the logic I pasted this is how i would read it. \[a-z\] . \[a-z\] . \[0-9\]{3} 1 Letter a-z + . + 1 Letter a-z + . + 3 Numbers 0-9 @ anydomain &amp;#x200B; Am I reading that correctly? This is from a RS query.
If you have this functionality in the interface, click the save button while running sql server profiler, it's the easiest way to trace what goes where
I think this stuff used to bother my eyes. Now I can stare at a computer for 12 hours a day, no sweat. I guess I've got no additional suggestions besides "your brain will probably get used to it..."
I'd recommend playing around with &gt; [https://www.debuggex.com/](https://www.debuggex.com/) I believe what '\^\[a-z\]+\\\\.\[a-z\]+\\\\.\[0-9\]{3}@.\*$' is trying to accomplish is: string begins with \[a-z\] then literal .\[a-z\] then literal .\[0-9\]x3 then an immediate @ then any characters end of string I believe the correct regex is \^\[a-zA-Z\]+\\.\[a-zA-Z\]+\\.\[0-9\]{3}@\[a-zA-Z\]+\\.\[a-zA-Z\]{2,3}$ for your example. I'm by no means amazing with regex, but, hopefully this helps. :|
You might have better luck with /r/sqlserver, which is the subreddit for Microsoft SQL Server, than with /r/sql, the subreddit for the SQL language[*](https://en.wikipedia.org/wiki/RAS_syndrome)
Hmmm
\+1 for Gunnars.
Usually the application will connect to the database by passing the login information (either stored in the application or something that the user provides when logging in) or by passing the user's credentials directly (when using Active Directory). It might be worth checking to see if the application has a configuration file or option that stores the connection string and user login info so that you can change it there to match.
I tried a little bit but i cant tell if [debuggex.com](https://debuggex.com) is checking for a match with Java or SQL languages
f. lux gamer/office glasses indentation and formating vertical monitors and zoom in ctr-wheel works in ssms dark theme editors less staring by arguing over every line with the guy next to you
IIRC, regex is regex. It's not coupled to a programming language.
Linked tables commit changes from data source to access, and from access to the data source. It goes both ways
Linked tables modify the table they are linked to directly, it's just using access as a shitty interpreter. If possible you should avoid access at all cost and modify the program to write directly to SQL Server
&gt;Shift+Alt+Up|Down arrows allows you to work on multiple lines at once. Now this is why I like reading through the comments on programming subreddits.
I am a current CS student. My first database course we were taught to use aliases and = I took the course last fall. My professor was/is an 85yo man who knows his SQL but did not teach us JOINS So for the example given, I would write SELECT C.ID, C.Name, O.Date, O.Amount FROM Customer C, Orders O WHERE C.PrimaryKey = O.PrimaryKey How screwed am I in the real world?
While attempting to use the debuggex website i plugged in the following expression '\^\[a-z\]+\\\\.\[a-z\]+\\\\.\[0-9\]{3}@.\*$' I also used one of the emails that the query actually produced and debugged keeps on telling me its not a match, iv tried getting rid of the '\^ and doing only \[a-z\]+\\\\.\[a-z\]+\\\\.\[0-9\]{3}@.\*$ but still no luck...
&gt;Some people like the black screen w white text, green text.. Monochrome? That's masochistic. Even VIM and Emacs users use syntax highlighting. This tells me that you probably aren't treating SQL like any other language -- many programmers throw out common coding practices when it comes to SQL. &amp;#x200B; You would probably have the same amount of eye-strain if you were to write 600 lines of C or Java code without a formatter or syntax highlighting all the same. &amp;#x200B; Ditto if you put everything inside one function/method, and a lot of people do exactly this when they write giant SQL "queries of doom" unnecessarily. &amp;#x200B; Follow the D.R.Y (don't repeat yourself) principle and break your SQL into modular components like you would ***with any other language***. Use separate functions, procedures, and views unless its a one-time-use query. &amp;#x200B; Also, go get yourself a decent SQL IDE. Jetbrains IDE's are best for SQL -- DataGrip, PyCharm, IntelliJ, etc..
TVF or stored procedure. Declare and set the password in a parameter and pass it in when calling the TVF/SP. Reference the parameter in your DECRYPT BY PASSWORD. Don’t store the password in the same database as the data it decrypts.
You'll be fine. Using the JOIN clause is probably preferred in most cases these days, but in some situations, especially when querying Oracle DBs, using a WHERE is still seen fairly often for joins, at least in my experience. &amp;#x200B; That being said, adjusting to using the JOIN clause should be an easy transition.
20 20 20 rule https://www.medicalnewstoday.com/articles/321536.php
I use a windows store add-on called f-lux which adjusts the light of the screen to the time of day/ your liking
‘SQL For Mere Mortals’ is what you’re looking for
 FROM fact_table AS f INNER JOIN dimension_table AS d ON d.sport = f.sport AND d.competition IN ( '*' , f.competition )
I turned down the brightness and contrast on my screen by a significant portion. Helps out a lot.
What happens when I have things with different columns that I need to join and sum/count? The group by is always repeating some rows and screwing up with the sum...this may be unrelated but I couldn't figure out how to do properly with joins. Example: &amp;#x200B; `wallet_id in_value out_value` `1 200 250*` `2 200* 250` `3 250 250*` &amp;#x200B; I have a table "ins" and another "outs" that both have a n:1 relation with a "wallet" table. So a wallet has many ins and outs. Ins has wallet\_id and in\_value column, outs has wallet\_id and out\_value column. How can I show wallet\_id, sum(in\_value), sum(out\_value) using joins? &amp;#x200B; This is happening: even with ins not having "out\_value", out\_value is filled on every row with the value from the last "out\_value". For everything to work, it should be NULL like below. &amp;#x200B; `wallet_id in_value out_value` `1 200 null` `2 null 250` `3 250 null` &amp;#x200B; Is that a join problem or something else?
Thank you I think this solution will work
Good base salary. The future only goes up. Learn as much as you can, get into ETL, or modeling, learn a little Python. Try and get a promotion --&gt; i.e., a new title. Doesn't matter if it comes with a raise, just focus on a promotion &amp; new title. In two years, or three, then apply for a *senior* level position at another company. You'll get a 20-30% raise. Stay there for a few year, learn as much as you can, then rinse and repeat until you decide you're ready to go into management, *or* you find a magical company that you want to spend 5-10 years at making mid six figures.
It sounds like you're joining the tables themselves (i.e. not aggregated), instead of aggregating them first. You could use subqueries that aggregate (using `GROUP BY`) to get those sums and then join those ([see here](https://dba.stackexchange.com/questions/47861/how-to-join-two-result-sets-to-query-on-output-came-from-two-statements)). Note that, because you want nulls for ids not found on either side, you'll want a full outer join.
NegativeScreen and syntax highlighting
Thank you. I really enjoy my role as it is new and our database is huge and complex so if definitely challenges me. I appreciate the response.
https://www.postgresql.org/docs/9.4/sql.html That should cover every detail. (Except that some things differ between database vendors)
All I can say is a friend oh mine had a bachelor degree in Business Administration and Management and today he works as SQL Developer in Los Angeles and makes $110K a year. So don’t give up, just keep studying. And the good thing about Database (SQL...) it’s easy to find templates of most queries/scripts and this alone is a time saver
https://www.w3schools.com/sql/ https://www.dofactory.com/sql http://www.sqlservertutorial.net/sql-server-basics/sql-server-like/ https://www.tutorialspoint.com/sql/sql-like-clause.htm
Don’t look for that job in particular. Get a job somewhere that has that for advancement. Start low, get experience then you’ll be considered over someone with a CS Degree. If you touch real world software you’re already better than them interview wise. They’ve never touched real software outside of school.
I can totally understand the confusion. In simple words, when you install MySQL you are also configuring the machine as a server. Since you will be connecting locally, you can just enter "localhost" as the host name. Hope this helps.
Adjust monitor color to have less blue. And use sublime text 3 and get a package that lets you customize the delivered syntax colors / find highlighting
Use the windows authentication
I believe you are absolutely right! Thanks a lot for pointing me in the right direction!
Get on a help desk, start getting exposed
Only working on assignments will show you if you’re ready or not
I'm a fan of dynamic SQL, and build all kinds of stored procedures like the one you're talking about. But if its coming in through the web, and you know the column names in advance, why not just build your query there?
In this context, what is a helpdesk?
Damn that is not that much for LA's cost of living
very helpful
Lol that was for another post. Sorry
Only working on assignments will show you if you’re ready or not. There isn’t any other way. Sorry
Get a job on the help desk at company where they have this type of infrastructure - financial, local govt, something with a dataset. Help desk fields the support calls for this tech, you’ll be exposed in no time.
Agree 100%. There are places to use dynamic sql. This ain’t one of them.
I understand those things. What I’m saying is that in practice very few companies execute scrum by the book like that. The vast majority do what is being described above, and that is what my comment is based off of.
I made myself the tech-person for the jobs I was in, offering to help people with Tableau and Excel projects, all the way up to writing little python scripts to automate certain workflows, etc. I was able to leverage those skills/experiences to snag a Data Analyst position within the same company.
Your github should show a ton of projects
 if (Query != null) { sbCommand.Append(" and Code = @txcode or or CONTAINS(Name,'"'+@query+' \* "')" parameters.Add("@txcode", Query, DbType.String); } how can i write contains in dynamic sql am getting string errors
not separate columns for each account, accounts should all be in a single column CREATE TABLE whatever ( ... , account_name VARCHAR(37) , account_yesno CHAR(1) );
&gt; They cannot accept null wrong, foreign keys ~can~ be NULL this is only the first of several factual errors
I can help!
Tech support helpdesk. People have a technical problem, they call you, you fix it. Could give you a look behind the scenes at the tech stack and get some experience at the same time. Do you have a portfolio of personal projects you've been working on? Having good SQL skills and some coding background makes you a great candidate for being an analyst. I've never been interested in exactly which kind of degree someone has when I've been recruiting for all levels of data analysts. For junior/entry level posts it's always been more about showing some natural interest in the main skills through your own projects.
We know that Foreign Key is the referential relational key and must be a Primary key in the main table and a primary key can never be null. Then please can you tell me that How can a Foreign key column have a null value. It depends on the SQL developers, how are they going to make a relation between two tables. If someone allow null to foreign key column then really it would be a horrible mistake.
For security training, I would recommend the SEED labs: https://seedsecuritylabs.org/
You have to create a function to remove numeric and special character from a string. For more information- you can visit- [Remove numeric and special characters from a string](http://www.sql-datatools.com/2016/01/sql-remove-non-numeric-characters-from.html)
You can use a CASE statement with almost any DB as below - last query shows you the SO\_TYPE field and the result of the CASE statement: &amp;#x200B; \&gt;create table test(so\_type int); Query 1 Complete ---- 0:00.0 0:00.0 0:00.0 \&gt;insert into test values (0),(1); Query 2 2 Rows Inserted ---- 0:00.1 0:00.1 0:00.1 \&gt;select so\_type, case so\_type when 0 then 'Collection' when 1 then 'Booking Slip' else 'Unknown' end from test; SO\_TYPE|CASE SO\_TYPE 0|Collection 1|Booking Slip Query 3 2 rows ---- 0:00.0 0:00.0 0:00.0 \&gt; &amp;#x200B; Alternatively, you could create a lookup table (particularly if you need to use this a lot), and join on SO\_TYPE to the key in that table, projecting out the expansion. The test table above would be a suitable definition for the lookup table.
Try this [https://stackoverflow.com/a/1008566/3872659](https://stackoverflow.com/a/1008566/3872659) &amp;#x200B; `Create Function [dbo].[RemoveNonAlphaCharacters](@Temp VarChar(1000))` `Returns VarChar(1000)` `AS` `Begin` `Declare @KeepValues as varchar(50)` `Set @KeepValues = '%[^a-z]%'` `While PatIndex(@KeepValues, @Temp) &gt; 0` `Set @Temp = Stuff(@Temp, PatIndex(@KeepValues, @Temp), 1, '')` `Return @Temp` `End` `---- TEST IT ----` `create table #TempStringCheck (string varchar(50) NULL);` `Insert into #TempStringCheck values('Collection - 0'),('Booking Slip -1');` `select dbo.RemoveNonAlphaCharacters([string])` `from #TempStringCheck`
Are you trying to copy a subset from table1 to table2? Use where to filter table1 like in a normal query instead of a case statement.
no. to be very exact it's a cte. I query a few dmvs and the result should be written within a database. But, a field I querry is emtpy a lot of times. So I want only to insert my result of the DMV-Query into my table, if this field is not null. (Reason: This runs as an SQL-Agent Job, every 10 seconds. and 95% of times the job is failing, because this field is NULL, and NULL is not possible in the target table. So I only want to execute the insert into script, if this value is not null) I hope this is more understandable now :)
T-SQL Fundamentals by Itzik Ben Gan
Why not simply exclude those rows from the select: insert into table2(...) select ... from table1 where some_column is not null; This would insert those rows that do not violate the NOT NULL constraint. If you do not want to insert anything if at least one NULL value is found, you could use: insert into table2 (...) select ... from table1 where not exists (select * from table1 where some_column is null);
Yes, SQL is still relevant.
`CASE #table1.value is not null THEN` `Insert into table2 (values) select * from table1` `ELSE 'do nothing'` `END` &amp;#x200B; I'm confused as to why you don't use : `INSERT INTO TABLE2 (values) SELECT * FROM table1 WHERE NOT value ISNULL` Or `INSERT INTO TABLE2 (values) SELECT ISNULL(value,'') AS value FROM table1` &amp;#x200B; But `IF OBJECT_ID(N'table1') IS NOT NULL` `Insert into table2 (values) select * from table1` Might be what you're looking for
 select .. other columns ..., case so_type when 0 then 'Collection' when 1 then 'Booking Slip' else 'Unknown' end as book_type from ...;
Yes it is. I've been using this and Practical SQL from No Starch Press. I prefer the No Starch Press book. I'm not the biggest fan of HF's teaching style but the content's still relevant.
When you place the CASE condition make sure to add this syntax ISNULL(fieldname, ‘’) Or NULLIF(fieldname,’’) Sometime empty cells are not treat as NULL in sql and you need to redefine it
When you query Table 2 you can do something like "group by item having count(\*)=1". This will filter out any items with duplicate entries. (Though it's weird to me that you have two tables both with Price and Item, and also that you want to ignore the carrots completely.)
https://www.reddit.com/r/Database/comments/c92pyr/is_head_first_sql_still_relevant/essg4zz?utm_source=share&amp;utm_medium=web2x
&gt; If someone allow null to foreign key column then really it would be a horrible mistake. please... stop... you are **so wrong**
Thank you for your response! I do have a web dev portfolio. The projects I’ve got on it don’t really include SQL so I should probably work on that. As far as the job titles should I just search for database analyst or what do you suggest?
Your question inspired me to check out if I could apply dark theme to my SSMS in the office. I'm limited to 2012 (sigh) but found the following very useful and have used the exact examples: https://www.sentryone.com/blog/aaronbertrand/making-ssms-pretty-my-dark-theme
Try INTERSECT
Or use a CHOOSE statement in tsql, which is syntactic sugar for CASE.
As I said before, it's redundant and badly implemented https://www.reddit.com/r/Database/comments/c8laq4/-/esoyxeh
Thanks for the best practices advice. I will implement this.
T-SQL querying is the more advanced book.
Thanks for the link, I was going to make SSMS dark as mentioned but you provided a tutorial. Thank you.
Agreeing with r3pr0b8, also foreign key doesn't have to reference primary key, but any unique constraint (alternate key).
Not sure if dynamic SQL is the right thing for your situation. It could be done, but you would have to be extra careful and protect the code against SQL injection. Checking the column names against schema, wrapping them in brackets and perhaps check the procedure right or execute with impersonation.
I agree with all of this except two-three years = senior level position? :|
Does it matter which price is correct for the carrot?
It's like self advertisement, can and do well are two different beats
Yeah I make slightly less in New Zealand.
go through the mysql documentation it should have a sample db you can download and run their examples against.
Download the adventureWorks database. And watch the videos on YouTube. If you need lessons from Udemy. Send me your email I’ll send you the Udemy lessons packages for SQL it’s called mySQL for data analytics and Business Intelligence
I'm in a vaguely similar scenario. I hold no degree, but attended a full stack Boot camp, where I mostly wrote React apps with a node back end, both with Mongo and SQL dB's. For me, "help desk" was the level two support team for an application I gained expertise on as an end user (insurance company). I attended the coding boot camp, while on the help desk with the goal of landing a dev gig, but it can be difficult to sell yourself even with a portfolio. I've since moved two a job on the same company as a sysyems analyst, where I am essentially the "SQL guy" on the team. I've brokered an agreement where upon proving myself for several months, my team will take me on as a dev full time. Alternatively, I could easily migrate to another team in a more purely data driven role, but I've wanted to be a dev since highschool, back when I took some visual basic and c++ classes. All this to say, although it's possible to get a dev gig without a degree, unless you're a rock star or can network, you need to get there through a succession of positions. My company may have openings in Seattle, so feel free to pm for more details if needed.
It was his salary as a junior, he is a senior now. I don’t know his new salary
There has to be something related to your current job you could start using sql for. Figure out some report that would be useful for your boss- aged tickets in queue, repeat offends on unlocks, whatever. Start writing the report. If you need help, figure out who the admins for the DB are. Most people are happy to take a few minutes to show you the ropes. If someone is NOT happy to help, do not keep reaching out to them. As you write the report, when you run into errors or stuff you don't know, don't just google it and steal the code. Take the time to understand HOW AND WHY it works. I Google the syntax of functions daily because I can't recall the order of the arguements but knowing which functions can be used to produce the needed results is why I can usually get the job done. Never stop learning. If you have the free time, reach out to other departments that might need simple stuff. Learning different parts of the business (not to mention making connections) will help you understand the data so much more. My two cents anyway.
The Udemy course matt mentions is really good. Udemy and coursera has plenty of sql courses for beginners. There is a free sql bootcamp on youtube too by freecodecamp.org which is ad free. Learning sql is fairly easy compared to other programming languages. Good luck!
Also, don't knock the usual tools used in helpdesk. Windows task scheduler can trigger batch files which can kick off sql s scripts via sqlcmd. Powershell is great for queuing AD, file shares, remote machines, etc and can generate input into sql (ssid, import data from ssms, whatever) to be able to run more advanced queries against it.
This. MySQL has a few good, yet relatively small sample data sets. When you really get into the documentation, you’ll see they used specific data relationships to be used in combination of specific operations like joins and such to compare average query time, using with index/ no index, etc. Start going through that and you’ll have plenty of fun for a while
Thanks. I didn’t know about the sql boot camp thing. I’m gonna check it out
I'm doing the Ultimate MySQL Bootcamp right now and I'm loving it
I just started as well
I will do so. This old server was not set up for AD and had a very small collection of user accounts.
Initial thoughts from a quick glance... What's going on with the `pants_sizes` and `shirt_sizes` tables? Are they sort of dictionary/lookup/control data tables, and if so, what do they key to? I see a `sizes.size` column, but it's a string value. The `items` and `sizes` tables don't seem adequately normalized in their current state. If I'm reading this correctly, if you had a shirt "item" with a "size" S, M, L, and XL that would require 4 entries in the `items` table, each with a different `size_id` to key to the `sizes` table? That would result in an annoying update process if you ever wanted to change an item description, title, etc. Same question for if you wanted the same item with multiple colors. Are users able to leave multiple feedback entries on the same item? If no, I don't think the `feedback` table is necessary - you could just include those fields in `user_item_feedback`. Can an item belong to more than one category? Can subcategories exist?
I like [Sqlzoo](https://sqlzoo.net/) for learning the basics, and it even lets you practice right on the page.
Nice
DM me if you want to collaborate or whatever
It is the case when you are making a relationship with Unique constraint to another table then a FK may have only single null value. But it is not correct as per the business standards.
Hello. Firstly, thank you for your response. It has given the feedback and an overview I wouldn't have been able to reach by myself. &amp;#x200B; An archiving system would be a nice feature indeed so users might be able to refer back to old items for documentation. Subcategories do not exist, a long sleeve shirt will belong to the long sleeve category and a jacket to the outerwear category. &amp;#x200B; I'll go ahead and get rid of the feedback table, this is my first database and I kind of went crazy with the normalization. &amp;#x200B; For colors and sizes, the plan was the have 4 entries in the sizes table each being S, M, L, XL. And each item would point to a one of these entries. I did not think about the annoyance that would come from having the possibly update my database, so thank you for that, it was a good point. &amp;#x200B; The pants size and shirt size tables was a bit of an experiment, I wanted to see if I could possibly create a table similarly of what I had there, but based on clothing type category as there are different styles of sizing for bottoms and tops i.e. Waist size 30, 32, 34, 36... &amp; Shirt size 46, 48, 50 or S M L &amp;#x200B; I also wanted to make some for clothing that were from international countries as they have different sizing metric, but that seems to be out of scope for this project.
Does that give any benefits over a CASE statement that would work on far more databases? Possibly easier to read, just like a DECODE, although again I suspect CASE is slightly more widely supported than DECODE, and don't think OP provided details of the DB being used.
This is the right answer. It's something call the [Entity Attribute Value model](https://en.wikipedia.org/wiki/Entity%E2%80%93attribute%E2%80%93value_model). I call them EAV tables for short. 99(.999999)% of the time these kinds of attributes will *eventually* change. Using an EAV table this means no schema change. DB changes are a frequent source of development slowdown for teams, so it's important to avoid when *reasonably* possible.
Hey, sounds like you are on the right track. For me proficient is an excellent understanding of the core SQL syntax and enough knowledge to Google the rest. If you really want to benchmark yourself though maybe look into some of the certification requirements, I'm not saying you need to do the certs but they at least have a checklist of functionality
Thanks dude - good idea with the certification requirements. Much appreciated!
The simple answer is, your don't. &amp;#x200B; Please provide details of the issue you are trying to solve and someone will be able to help you.
SQL Server loops are done with Cursors or While loops: [https://docs.microsoft.com/en-us/sql/t-sql/language-elements/declare-cursor-transact-sql?view=sql-server-2017](https://docs.microsoft.com/en-us/sql/t-sql/language-elements/declare-cursor-transact-sql?view=sql-server-2017) [https://docs.microsoft.com/en-us/sql/t-sql/language-elements/while-transact-sql?view=sql-server-2017](https://docs.microsoft.com/en-us/sql/t-sql/language-elements/while-transact-sql?view=sql-server-2017) &amp;#x200B; It's worth noting that if you are dealing with anything more than a small amount of data, you should be attempting do write your sql logic as set-based operations. The reason for this is that loops, such as a cursor, process one row from the result set at a time. If you are dealing with hunderds, thousands or more rows, the execution time, resources consumed and potential for blocking/lockign of resources becomes too high. Based on your use case though, it sounds like you're dealing with minimal data and therefore a cursor should be fine.
Your question is unclear. Try giving some sample data of the tables you're looking at. In general, regular SQL doesn't have loops, but does run against every record in the referenced tables. This is sometimes called an implicit loop. I would expect that you need to join the main table to the table with your parameters, but I couldn't be sure without more information.
Your website says it is available for Linux now (e.g. [https://support.querypie.com/hc/en-us/articles/360025982654-The-QueryPie-Open-Beta-is-finally-here-](https://support.querypie.com/hc/en-us/articles/360025982654-The-QueryPie-Open-Beta-is-finally-here-) ), but your text above says it is not - which one is right? Also, is there a reason it could not support other databases via a generic ODBC/JDBC interface, rather than you having to do per-database work before people using a currently-unsupported database can even try it. I use Kognitio, so if there was a generic ODBC/JDBC option I would give it a try today.
A SELECT statement "iterates" over all rows in a table. What exactly do you mean with "each object"?
I may consider having a look when you support Postgres
You really need to be looking at joins. You can join one or more tables in a query to enable you to get data from more than one table in the query.
Welcome to hell... I'm glad we don't use them often (trigger). And we are on SQL Server where is less a pain.
Yeah this.
Looping select statements is generally a bad thing. I have it used depth but a lot of new database engines now provide better options for that. Understand the data set you need and select it. So if your trying to query for your area/controller/action then you may need three nested queries to select each one at a time or may need to utilize a pivot to unroll data into those categories. It is hard to say without knowing what you want to do and what you are working with.
If I recall, you can't update other rows in a table upon which the trigger is firing. Hell, just updating other tables that have FKs to the trigger-table will get errors. If you are updating rootparentid of the triggering-row, then you don't run an update, but do this ... :new.rootparentid := .... But yeah, updating other rows is a nope. Doing lots of things in triggers generally becomes a bad idea (IMO). How about determining rootparentid for the insert/update that is being used against the table, and include the column setting there?
Look into triggers. I'm a SQL server guy, so no guarantees it'll solve your solution. I'd suggest you to rethink your solution. It sounds like your upload table doesn't add value. Why does this table exist? Can this be data be shown in fewer tables and still keep the normal forms?
To better understand the problem i have, here some additional infos. The whole thing is written in PL/SQL, there are a lot of packages and each package has procedures and functions. I am trying to iterate through all objects and get the object name, parameters etc. so that i can concatenate and create all possible URLs. I am not talking about a table.
Triggers seem to be the right thing, I will look into them. And: I think I gave too little information. 'uploads' is a relationship table. Whenever a user uploads a file, the id of the file and the id of the user will be saved in uploads. Now, the reason i made the uploads table, instead of just storing the id of the user in the file, is because I will implement something like sharing. Example: fileID | userID 5 2 5 6
Going by job title alone is a minefield at all levels, not just for juniors. You could find very similar jobs going by "data analyst", "BI developer", "database analyst", "junior data scientist" depending on who was writing the job advert. Looking for some combination of those and really studying the job description itself is probably your best bet. And do add some data and/or database specific work to your portfolio. Always a good conversation starter when you get to interview stage.
seems like you are correct about this being impossible. i ended up doing this from the code side. was a lot easier that way. thanks for the reply anyway
This data is stored in a table. I recommend you search your preferred PL/SQL documentation for "Displaying Information About Schema Objects"
We have something similar set up, but it's a separate stored procedure that gets called. It takes several different parameters including the name of the stored procedure calling it, and a concatenated string of all of the parameters the original stored procedure was given in the format of "@[parametername] = 'parametervalue', @[parameter2name] = '@parameter2value', ... "
Are you going to use Linux or Windows? Lookup LAMP (Linux, Apache, MySQL, PHP) or WAMP/WIMP (Windows, \[Apache / IIS\], MySQL, PHP) tutorials &amp;#x200B; These will show you how to get started installing Apache or IIS (to host the website), MySQL as the database engine, and PHP as the server-side language.
im using mamp as a server hoster on the apache server
Do you have a generic solution to concatenate your strings, or do you manually create your concatenation?
I am able to retrieve the data for one package but i am not sure how to approach it when I want to automate it to iterate through all packages
Well it looks like you have everything you need to start writing the account system... it's not an easy task, you'll need to build your database schema, then write a sign-up page, login page, there's tons of tutorials and examples on doing this https://www.tutorialrepublic.com/php-tutorial/php-mysql-login-system.php
i have a signup and login page from html and css but idk how to get them to do anything
This should give you the columns from both tables while excluding any entries where the Item exists more than once in Table2. SELECT * FROM Table1 AS A INNER JOIN (SELECT B.Item, B.Price FROM Table2 AS B INNER JOIN (SELECT Item, COUNT(1) AS Rows FROM Table2 GROUP BY Item HAVING COUNT(1) = 1) AS C ON B.Item = C.Item ) AS X ON A.Item = X.Item
SSIS
I believe that it's always done manually, something like this: EXEC Loggingprocedure @Procname = 'thisprocname', @Parameterlist = '@parameter1 = ' + @parameter1 + ', @parameter2 = ' + @parameter2 I'm not sure how you would structure a snippet to do it all dynamically unless all of your procedures had the exact same parameter lists.
The form action needs to be a PHP page (`&lt;form action="login.php"&gt;`), which accepts the form input and checks it against the database. Sounds like you might need to start with some basic PHP forms / input and move onto something more complex once you become familiar with it
okay thanks ill have a look into it thanks for the help
Dynamic can be done by placing the global variable @@PROC in the stored procedure, which returns the object_id of the procedure it is running in. From there it's just a simple query to the sys.procedures and sys.parameters tables. It's generating the concatenated string from there that I'm struggling with.
Thank you, care to elaborate?
Interested, shoot me a PM!
can you just edit the python script that generated it? Pretty sure it was a python script.
Very simple and fast, with existing tools https://blog.sqlauthority.com/2008/02/06/sql-server-import-csv-file-into-sql-server-using-bulk-insert-load-comma-delimited-file-into-sql-server/
Your select is wrong. It should be `'SELECT ' + @Test + ''`.
My code there is a gross oversimplification of what I've been trying. Your suggestion would work if I were to add it manually, yes. What I want to do is have a generic piece of code I can paste in to any stored procedure to capture and output the parameters. I can query the parameters in any stored procedure out of the sys tables using @@PROC to get the stored procedure object_id. From there I can output a list of all the available parameters, but not their values. I don't know if what is like to achieve is possible, but I'm hoping it is!
So something like this then? DECLARE @SprocParameter1 = (SELECT FROM SYS) DECLARE @SprocParam1Value = 'SELECT ' + @SprocParameter1 + ' You'd need to have some kind of logic for handling sprocs with multiple parameters. [Here](https://www.reddit.com/r/SQL/comments/bnj60j/ms_sql_built_a_cool_little_tool_to_compare_tables/) is some Dynamic SQL that I've done which has some similar functions. I have another example I can give you which captures the values in a similar way you are describing if this isn't sufficient. It should be doable.
CSVKit is a nice and simple command line tool that can easily import CSVs into databases
If you want to be a DBA, maybe go with Oracle. If you want to do focus on analytics &amp; data science, then probably go with MS SQL.
I am currently learning R, so I am want to learn something that goes along with it. A lot of companies here are demanding more skills on top of just R like SQL, Tableau etc. So I want something that would supplement my R skills I guess.