Now, mobile is the true Satan. :D :D &lt;3
The field needs to be indexed. I don't know what DBMS you're using, but if it's SQL Server or Oracle (and many others), index the acct\_nbr field and the query you have written will use that index and improve tremendously. There's no need to create temporary tables or anything else.
The most complicated SQL script I have personally written came out to be a bit over 4,000 lines of SQL. It consisted of several temporary tables, and a lot of unions pulling in tons of data. It was used to calculate the average returns cost over time of every single product we sell on our website. We then use that data to come up with a "returns cost per sale amount" that gets withheld when we are internally calculating our net profit on sales. I felt really bad for the person that had to peer review the script before we deployed it to production. Outside of that, we have many other scripts that my team maintains or built that over over 1,000 lines of SQL. 
Thanks man, I’ll check it out 
Oh fo sho. Now tell me more of the SARGEability of computed columns. I must learn something!!!
You can either be a jerk off and watch videos, go to websites that give you questions, etc., or you can dive in and do something real. What are you interested in? Create a database. Find data, and there is plenty of it out there, but find it and import it into a database. Are you in a fantasy football league? Awesome. Figure out how to import that data, plus other data, and then write queries to produce something meaningful in a context that you actually understand. Like you might know what a concept like "Points Per Game" means in the context of Fantasy Football, but if you import a data set of every player, from every week of the season... can you tell me what their PPG is? Can you tell me what the distributions look week by week? What if I ask you to tell me what running back has the highest PPG year over year for the past 5 years, but only running backs that graduated from universities north of the Mason Dixon Line? What if a RB has only played 4 seasons? What if you need to normalize the data to account for a season with an injury? You can go so far down that statistical rabbit hole, but the point stands: Do something. Start a project and dig into it. You don't have to get into hardcore predictive modeling, or machine learning, but just digging in and doing simple calculations, joining tables, learning the use of sub-queries, etc., is going to be more beneficial than any of the websites. Then things like ETL, and query optimization are really fields unto themselves. 
It's certainly possible. Four years ago I was working in a call center and next week I start a new gig as a BI engineer. My stepping stone was a position in what was essentially data entry, which allowed me to start working with SQL. From there I taught myself on an as-needed basis. Whenever I had a problem outside my skill set I'd just run it through Google and adapt what I found to what I was trying to do. Something like [W3Schools](https://www.w3schools.com/sql/default.asp) is great to get you started. If you can't get your hands on real live production data to play with, look up the Northwind and/or Chinook databases for something approximating real world data. They're Microsoft-based, but the basic ideas should be transferable. Maybe someone else can chime in with an Oracle specific equivalent. Best of luck in your switch! Don't be afraid to ask more questions here. People are generally friendly and happy to help.
That wasn't the issue unfortunately
I honestly don't think formal classes are necessary unless the managers want you to have certain certifications for the position. I'm not sure if your salary will be enough, but an entry level position would be ideal as you grow your skills. There are many paths you can take with basic SQL and knowledge of how data is handled, but big data definitely isnt going anywhere.
If you need the data often id add a prefix column so you don't need to compute it every time. Maybe even a bit that flags it as an ABW account
I think that indexes would be a bit more advanced than say a primary key. Both are kind of similar in a storage and searching manner.
They’re as useful as any other query, you just get to reuse them in a simple way.
I mean... All of the above? It really depends on the company. At one company an analyst might not be expected/required to write their own queries, they just consume the data. At another the DBAs might leave that kinda thing to the DBEs, and instead focus on admin stuff. Like the other commenter mentioned, a sproc is just a reusable bit of code. Any developer should know how to set one up, but it's useful for anyone working in the space to know how to use.
Both companies that I have worked for (eBay and [Overstock.com](https://Overstock.com)) only allow the DBAs to create stored procedures. All of the business intelligence developers, ETL developers, business analysts, etc, who actually use the data warehouse, can't create stored procedures without first working with the DBA team, and having that team implement it.
It depends. It’s not as concrete as you believe. Many jobs could or could not utilize stored procs. For example, my company policy is that stored procs (and triggers for that matter), is for DBA tasks only. But yes, there are plenty of application uses for it, but our policy is that application are to handle all logic on their side. Just depends. Obviously DBA is gonna use it the most in general, but it’s always a good thing to know. 
Are you a fellow utahn? I raise my fry sauce to you. 
Thanks a lot for the explanation. Well I was going to dive into the rabbithole but then I said stop. Because firstly I want to do some exercices before I start to do my own project. I want to know what skills I got before getting my hands dirty with a project like for example the fantasy footballteam. I will surely do that. Agaim thanks for the motivation.
I am. Pass me the lime Jello!
As long as it's indexed, LIKE should also work fine for prefix checking.
LIKE on a prefix should be sargable. No need to create an extra table.
Can I consider the Polybase technology as to be like on prem DB links?
Is it not possible to use the DB on the same server as the website? Are you using a Web Host? They might have a database service available 
yeh im hosting on heroku. Would you recommend just using the postgres service they offer?
It won't work with your current set of data, cause 4th insert into table2 contains 'I0045', which is specified in row 2 of table1 - the row you say is the correct answer. Without this line: insert into #table2 (col1, col2) values ('e', 'I0045') the query below works &amp;#x200B; select \* from table1 as t1 where ( select top 1 1 from table2 as t2 where t1.column1 like '%' + t2.col2 + '%' ) is NULL
I’m not experienced in Heroku unfortunately. Depending on your backend language and how willing you are to change your code, Postgres would open the door to Digital Ocean as well as one of the new DBaaS providers.
Apologies. That was a typo. Will run your query later a get back to you. On mobile now. Thanks.
I will definitely have a look at that. Thanks.
if you are database developer or oracle apps technical consultant you will create stored procedure &amp; functions. while BI analyst and BI developer are dealing mainly with the data and queries.
Hi, I ran the code on my fiddle. It gives me syntax error. I removed the `insert into ... 45` line too.
This works. select * from table1 as t1 where ( select t2.col2 from table2 as t2 where t1.column1 like concat('%', t2.col2 ,'%') limit 1 ) is NULL;
I work as a BI analyst and write SPs every day, use them for Tableau workbooks. 
Kinda... SELECT 'Apple' FROM table WHERE Qty &gt;= 0.1 AND Qty &lt;=30 UNION ALL SELECT 'Orange' FROM table WHERE Qty &gt; 2000 AND Qty &lt;=20000 UNION ALL etc
Looks like you need them separate first, and then can put them together. I would try a table of some sort with 2 columns--1, the FK that will tie back to whatever a row in your "table" table represents. 2, FruitType--this is just where you put Apple, Pear etc for positive matches. Then, when you want to include all of the fruit into one row for a given row in "table", use the STUFF function to cram all of the many rows into a single row. That's how I would go about doing exactly what you described. If I were more fully reviewing, I would ask why you're trying to do that and possibly be able to pose a different problem.
Can I have a second helping of those funeral potatoes? 
Most companies where you're doing database work you should be writing stored procedures. My current title is BI Developer. The last place I was a programmer. The place before I was an ETL developer. The place before I was a database analyst(I think). Most places that are ran by competent managers, you should be writing stored procedures. It guarantees the same code is being ran ever time, and it allows the server to cache an execution plan so it's more efficient than executing SQL manually. I'm a database guy, I like working with them. Most places where you're deep into databases you'll be writing stored procedures. Don't get bogged down in job titles, read the description of jobs.
How do to versioning of a database schema... how do you track database object changes and perform continuous deployment of database objects? Are stored procedures maintained within the database, or are they managed/executed by an external application? Pros/Cons of each method? 
In my shop, no stored procedure (or any other database object for that matter) gets into the production environment without crossing a DBA's desk. We welcome developers writing stored procedures when they make sense over ad hoc queries, but they're going to get looked at by a DBA before they go live. We'd *really* like to be involved from the beginning to provide guidance, especially for newer developers or taking advantage of new database engine features, but that doesn't always happen.
Yeah for sure. If I don’t need classes I’ll try to save my money. My company does offer 3000/year in education funding, so I will definitely see what I can find and not come out of pocket. The job posting They have had up for a year now starting salary is basically what I make now so I think it would be lateral. In my current position I will “cap out” or hit my ceiling around 80k before taxes. I think the ceiling is higher in data? What I do know about the different commands to query a database is very rudimentary but I can insert data, and I know how to query for different things and combinations of things and run a little bit of code. But I do enjoy the very little i do know and I definitely enjoy how logical it seems. 
&gt; Is there a way to set up my query to return multiple results that meet the conditions I specify? sure... a `name_quantity` table CREATE TABLE name_quantity ( name VARCHAR(9) , lo_qty DECIMAL(9,2) NOT NULL , hi_qty DECIMAL(9,2) NOT NULL ); INSERT INTO name_quantity VALUES ( 'Apple' , 0.1 , 30 ) ,( 'Orange' , 2000 , 20000 ) ,( 'Pear' , 30 , 20000 ) ,( 'Kiwi' , 10000 , 300000 ) then use a LEFT JOIN and COALESCE to 'N/A' when the join fails 
Also, they mention writing SQL and PLSQL? What is the difference between the two and are their some specific resources for the latter? Sorry for all the questions. Thanks again for your time. I know it might be kind of strange for a mechanic to want to code but It’s very interesting to me 
&gt;prem DB links Forgive my inexperience with Oracle : ) But, yes it does appear that they are similar. I just looked into DB Links in Oracle and it seems that they function very similarly to PolyBase External Tables in SQL Server.
What error messages do you get when it fails? It's pretty hard to diagnose without you providing information.
im a programmer and i write PLSQL packages/procedures/sql all day :) 
Chess is a relatively easy game to pick up, but when you're playing skilled palers, you'll need to fully comprehend a deep understanding of it. This is just advice to anyone learning chess from scratch.
yep! :) 
https://www.virtual-dba.com/mysql-error-1114-table-is-full/
The first thing I would try it run the INNER JOIN with a TOP 5 and check the execution plan to see if there is any missing indexes
Thanks. I tried using the actual execution plan and it never returned. Tried the estimated plan but didn't see any warnings for missing indexes.
Look at your estimated execution plan for each and I bet they are different. I also bet you'll probably find your answer. Your INNER JOIN is probably building the enter data set, then trying to filter out the where clause. I'm guessing the LEFT JOIN is looking at each row only the left side and only matching on the right. 
Looks like broken stats to me. If the tables arent humongous, run update statistics &lt;table_name&gt; with fullscan 
Pastetheplan.com and show us both estimated
The ceiling is definitely higher depending on how far you go. You don't need to be a manger either to get that kind of salary - if you're good at working with the data you can do just as well. Again, SQL is just one of many tools, but an important one. Having the right tools and skills to get the job done is what matters. There are quite a few applications that do the same thing, but different companies of course won't want to buy another product so they generally list certain applications in job postings. Something else to consider is the type of business you support. I see a lot of need for developers who have background accounting/finance knowledge. But you can say the same for any business who has data (which is everyone).
Yeah I have seen a lot of different applications. I am glad that I know they use oracle so I can gear my learning towards that. Thanks for all your help and insight. You’ve been very informative and I really appreciate your time.
No problem about the questions. It's really good to be interested. So if you look up definitions, SQL is "structured query language" and PL-SQL is just "procedural language" SQL. You would learn SQL first which would mainly involve querying (returning data based on your specifications). Your DBA most likely will not allow you to have insert/update/delete access until you're a bit more seasoned. PL-SQL is basically an "addon" and is more complex than regular SQL. In my experience it is generally used for automation (data migration or maintenance tasks). If you want to dig a bit deeper, you can read up on ETL. It's good to understand, but most likely you will have enough to learn for the next few years with just SQL. &amp;#x200B; I feel the mindset of a mechanic would transfer over to SQL very well. You need to understand how something is working before you can fix it properly. SQL is no different. A lot of writing SQL (especially in the beginning for myself or people I've trained) is reading the error messages and fixing your syntax. What is the error message saying? What doesnt look right with my code? When you get further it's digging into code that might not be yours and trying to see why its not outputting what an end user expected. Your SELECT, FROM, WHERE should be pretty quick and easy to learn. Most get stuck for a bit understanding joins completely. You can try writing SQL on the fly at the w3schools link below. It's a great way to start translating your theoretical knowledge into practice. [https://www.w3schools.com/sql/func\_sqlserver\_convert.asp](https://www.w3schools.com/sql/func_sqlserver_convert.asp) &amp;#x200B;
In PSequel you can't even stop a query once you run it (really shitty if it's a query taking 2 minutes).
[Navicat](https://navicat.com/en/products)is my favorite multi-flavor SQL interface for Mac. They have software that can connect to MySQL, PostgreSQL, MongoDB, MariaDB, MS Sql Server, Oracle, and SQL Lite; and they also offer one that can connect to multiple at the same time. It's not free, but it works very well. &amp;#x200B;
So you actually paid $599 for that?
Well, my employers did. I first used the MySQL version at my previous job, but now I use Premium so I can connect to the MS SQL, MySQL, and Oracle servers here.
Without more info (which fairly you said is hard to provide) it's hard to give any insight other than just shotgunning out possibilities/former issues we've seen. &amp;#x200B; You say you've simplified the script... how much have you simplified it? Is it possible there is a mistake in your join logic? Try an inner join with just the top 5 or something. Also... is this running with parameters or as a sproc? If it is I'd try ruling out [parameter sniffing](https://www.brentozar.com/archive/2013/06/the-elephant-and-the-mouse-or-parameter-sniffing-in-sql-server/). &amp;#x200B; Ultimately - the answer is they're running different plans, and you're gaming the optimizer by nesting your select. Usually the optimizer is smart enough to know the 2nd and 3rd plans are identical, so you probably have some outdated stats. Without getting your actual execution plan and comparing the 2 you won't ever really know. Hope you figure it out, good luck.
DBeaver always gets recommended. It has a bunch of features and recently got customizable dashboards.
It is $120 per database type if purchased for personal use
When I say "simplified", I mean removed columns from the select list and only put one join criteria for the sake of brevity. But since the same code ran vastly differently with just a switch from INNER to LEFT, I don't think the number of columns or join criteria was what was causing my issue. Updating the statistics did seem to help the run time a bit, but the overall issue remains. In either case, the process is being phased out, so this problem is only temporary.
You could use a CTE or temp table to determine the total row count for each user. Divide that value by 100 and you have 1%. &amp;#x200B; WITH TotalRows AS ( SELECT User --Whatever key is used (UserID, etc.) , COUNT(Row_ID) AS TotalRecords --You could use MAX(), too FROM YourTable ) SELECT YT.User --Whatever other columns you want to include FROM YourTable YT JOIN TotalRows AS TR ON YT.User = TR.User --Whatever key is used (UserID, etc.) WHERE YT.Row_ID &lt;= CEILING(TR.TotalRecords / 100) --CEILING rounds up to the next whole number &amp;#x200B;
What's the benefit of writing a stored proc for a tableau workbook, over say a view?
after updating statistics, what plans you get for your #2 example and #3? these should parsed to identical trees. Maybe there's some other pieces (group by/distinct/etc) that you have not described? 
Nothing inherent to Tableau, just the same reasons you'd generally choose between an SP and a view.
SQL servers query optimizer doesn’t work on subqueries. Therefore, something is screwy with the optimizer, more than likely caused by statistics being out of date. This is why it runs fast as a sub query but slow otherwise. Your query optimizer is being silly. Update your stats and see if that works. A plan will tell you more than anyone here could though. Your query should be fast, you aren’t doing anything inefficient with that join. Should take a split second. 
you can also do it in many other GUI clients. You can also connect to it again and call `SELECT pg_terminate_backend(pid);` for the `pid` of the query.
assuming your running SQLServer, the top command can be specified as a % instead of # of rows https://docs.microsoft.com/en-us/sql/t-sql/queries/top-transact-sql?view=sql-server-2017 
If stats aren't the issue, I would suggent to run your query and in another window run sp\_who2 or sp\_whoisactive (if you have it). Maybe you have something holding a lock on a row that's getting hit on your inner join and not on your left join? Look for your running query SPID to be blocked.
Honestly, I tend to turn off autocomplete in whatever Database Management Tool I'm using as in larger databases with thousands of objects it tends to cause me more trouble when writing SQL then it's worth. 
Why are you going for it?
* Strong metadata search * Object Dependency * Profiler * Row sampling (with seeding) * Visual EXPLAIN * Summary statistics Also one thing I've always wanted is to save every query I've ever ran automatically in some cache and again make it searchable, show relevant queries when I select an object, etc. I'm an old man I need assistance!
Certs are often highly theory based. I got my CCNA as a precursor for an IT career. I don't use much of any of that thing. Practical knowledge is much more employable. However, I'd say study as much as you would for a semester of school it you are completely fresh to SQL. That's just what I've heard others say
I love clever stuff like this
I've never found one... Each parameter needs to be declared. 
I made a video for 70761, and talk about the different joins, execution plans. Also how sql will optimize a left join into an inner join. Inner joins are usually hash matches, while left joins are usually nested cursors/loops. This sounds like an index / physical layer issue. https://youtu.be/Hgbhdgf6Y3M
There are 3 main parts/functions to a good SQL editor. 1) An object explorer, preferably tree-like, that lets you explore and generate/modify SQL. 2) A result set. Typically something that lets you copy data out. Resizing columns and displaying non human data types in human readable format is a plus (binary, epoch, etc). Bonus points if you can sort or re-order columns after the fact. 3) The SQL editor. Normally this will color code and syntax check your SQL. Usually this will provide some auto complete options. Occasionally, this will reformat your SQL, but never will it give you any options to do it “right”, or to a style guide. And the biggest place editors fall down - EDITING. Emacs, Vim, Sublime - they are all made to do amazing and efficient things with text. Your SQL Editor won’t. Azure Data Studio is the closest I’ve found, and it is SQL Server only, and only has a strong editor - it is weak on the result set and object explorer. Some other features that are nice - simple and light weight (Dbeaver fails hard here). Inexpensive (Navicat fails hard here). And not Fugly (SQLWorkbench/J fails hard here). 
If it were me, it were a large / frequently used / consistently changing query, I would add a bit columm to your table indicating whether to use the row or not. That way you could simply say "WHERE ProcessCustomer = 1" If it's just a few that never change, just write the parameters out. 
Yes. you can create a CTE with all of the expressions you want to remove, and join it using the LIKE keyword: WITH BadWords(string) AS ( SELECT '%DO NOT%' UNION SELECT '%duplic%' UNION SELECT '%DONOT%' UNION SELECT '%NOT USE%' ) SELECT * FROM Customers c JOIN BadWords bw ON bw.string NOT LIKE c.[Cus Name] WHERE [Close Date] &gt;= '01/01/2018' AND ... This will filter what you want, while keeping all the nonsense from the WHERE clause.
Seems the issue was an upgrade to MySQL 5.7 which broke the "0000-00-00" value, and Phpmydirectory hasn't been updated. Anyone have an idea of a solution besides changing where the database is hosted to one on an older MySQL platform? Thanks.
Why would you not just flat out join on an indexed computed column? Am I missing something? XD
It will work, that's why I said, "definitely from left" as in, no leading %. I was just giving another option, if needed. =\\
I would definitely hope someone entry level would understand what an index is, was, etc. However, I would not expect an entry level person to be able to read an intermediate execution plan and understand. 
Sorry m8. :( What was the issue? 
Why? Because I didn't know I could index computed columns and use them as SARGable columns until you mentioned that way to solve the thing (I spent too much time on old versions of SQL Server). Now, I have a new trick to play with and a few queries that might benefit from it. 
First step is that you will need to unpivot your data set and make it skinny. So instead of a bunch of columns, you just need two: one for data and one for time. Then it should all fall into place. Do you know how to do this in T-SQL?
Is it actually faster?
Thanks! I'll take a look.
Oh... visual explain. Yes.
Just use crowsfoot notation to represent 0/1/many and don't worry about directional arrows.
Promotion
Time wise (like 4 months?) Or quantity wise ? 
Whats the expected result vs the result itself ? You know left John will include everybody on the left weather or not they meet the 'on' condición right?
Time wise likely. But I worked full time when I did it
Trick question. The answer is, "no," but that isn't quite accurate. Where does each column come from? The same table? You can absolutely improve the performance of conditions such as "NOT LIKE", but otherwise no.
Each data set has a number of observations that include a variable country\_year in it (time\_id). The farthest left data set has every possible country\_year (from 1800 to 2017, last year with data for all datasets), and all of the other ones to the left have far fewer years (like, 1990 to 2017). But when I left join them all together, it just seems to make up a bunch of rows. I'm not sure what kinds of new rows are being made--I'll check--but I know they shouldn't be there in the first place.
I imagine you would be a lot better off doing something like: CASE WHEN blah like '%blah%' THEN 1 WHEN blah like '%bluh%' THEN 1 WHEN blah like '%bloh%' THEN 1 ELSE 0 END AS isBlah Then just selecting `WHERE isBlah = 0` and either wrap it in a sub-query, CTE, or use a #table. You would ideally do this before joining, and either before your other WHERE conditions (which are hopefully indexed) or after them if they are necessary to produce [blah]. The trick is to isolate the use of the LIKE condition from other logic/joins. A simple hybrid approach would be to select all the ID's, and then generate [isBlah] in a CTE, or #table, etc., and then join on ID `WHERE [isBlah] = 0`
Yeah, it looks like the two larger datasets aren't merging correctly. There's lots of NULL values for some reason.
Ok do that, and try to give more info on that as it's not very clear what the problem is
Ok do that, and try to give more info on that as it's not very clear what the problem is
Still not sure :/
OMG! You know what, it was working fine and there was an error message in a different program that made me think the original had messed up, and MYSQL seemed off, *too*, so I thought it was off. Thank you for the advice!
Buy the reference guide and read it front to back. You might have to re-take. It's not particularly easy, but your basics will help quite a bit.
Haha well glad to hear ! Good luck with that one ! 
I really doubt it
You could try the condition: WHERE who_switched &lt;&gt; ‘Bot’ 
you might be able to adjust what you join on or going from inner/outer joins where applicable. Not sure if that would be faster than a bunch of where parameters.
I started using vs code for more complex queries. With the color brackets plugin (or something to that effect) and some other plugins it takes azure studio up a few notches. I usually have both open at the same time since simple stuff is still easier in ads (like select * from somewhere just so I can see what the data looks like) but for long complicated stuff, vs code all the way.
https://tableplus.io/
NEWID() will generate a UniqueIdentifier type value for each line. 
There might be a better way to get it done but this should work. First, you would have to create an employee table, and have them numbered sequentially (1 through X). Then make a distinct location query (one row for each location) into a sub query or temp table, with a new column: ,rand() as num which will create a random decimal for each row. You should add a seed if you want it to be as random as possible. ,row_number() over (order by num) as row_n Then do a row_number() over the random row outside the subquery to get an ordered list of the locations by the random value. You can then join this to the sequential list of employees (row_number = employee_number) and back to the starting query. If you have more locations than employees, you can modulo the row_number() column by X to get it down to X employees. 
ability to pin result set tabs
Underrated comment.
The Architecture that’s been implemented here, while I’m not familiar with the tech stack being used like Redash, appears to be an Independent Data Mart Architecture. As each “Lake” is simply pulling from a “Cloud”, and there’s really no atomic point in this data. This will become a nightmare to maintain as when a new business process comes online, if more than one “Lake” needs access to this piece of information, you’ll need to update the scripts for each “Lake” using it and not just adding it to the ETL process.
You want to read about normalizing and normal forms.
The NOT LIKE clauses are not sargable if they begin with a wildcard. You can't speed that up without pre-computing the data. You could add a derived column to your customers table with all the conditions of your where clause, create an index on that, and query for that. 
Hello? Anyone? If I should modify my post, let me know. 
Brush up on normalization. 3rd normal form should suffice 
Adding to these questions anyone know any sources that help a new user learn / practice this type of stuff?
The only thing I can think of is to make sure you have an index including all these columns, so you can at least get away with an index scan instead of a table scan. \`NOT LIKE '%xx%'\` is, like others have pointed out, not sargable.
everything is possible if you're brave enough
Here is something that works based on your basic requirements, however, there are still some questions to be answered. &amp;#x200B; 1. What happens when you have more stores than employees? this won't work as employees would then have to take more than one store. 2. Is it possible that some employees need to be excluded? For instance when they are off sick or on leave? If so, then you need an additional flag on the employee table to say whether or not they are available. &amp;#x200B; -- create test data with cte_invoice as ( select * from (values (11547, 'ASD1001', 3, 'Landscaping'), (14676, 'ASD1001', 3, 'HVAC'), (11547, 'ASD1002', 2, 'Door repair'), (11547, 'ASD1005', 1, 'Garage Repair') ) a(InvoiceNo, Store, TotalInvoices, Description) ), cte_employee as ( select * from (values (100, 'Joe Wilson'), (101, 'Hasim Imadanyen'), (102, 'Becky Connor'), (103, 'Victoria Barclay') ) a(SalespersonId, Salesperson) ), -- assign an id for every unique store cte_invoice_with_store as ( select *, DENSE_RANK() OVER(ORDER BY Store) StoreNum from cte_invoice ), -- assign an id for every employee cte_employee_with_id as ( select *, RANK() OVER(ORDER BY SalespersonId) SalespersonNum from cte_employee ) -- select data from both tables joining on the id's we created earlier select a.InvoiceNo, a.Store, a.TotalInvoices, a.Description, b.Salesperson from cte_invoice_with_store a left join cte_employee_with_id b on b.SalespersonNum = a.StoreNum &amp;#x200B;
I’m assuming the customer table is much smaller than the case table. Why not make a temp table of all the customers you want. Then inner join that table to the case table. It might help. 
Interesting read, I'm personally mostly familiar with Kimbal architecture, was there a discussion or reasons why not to go that way?
Thanks! I don't want to spam you, but if you want an invite to the webinar, PM me and I'll send you the link
create an employee table with employee id and randomly generate rows based on the below link.for each row assign the unique Store# id [https://stackoverflow.com/questions/580639/how-to-randomly-select-rows-in-sql](https://stackoverflow.com/questions/580639/how-to-randomly-select-rows-in-sql) &amp;#x200B; Please find a simple oracle sql [http://sqlfiddle.com/#!4/e24b5a/8](http://sqlfiddle.com/#!4/e24b5a/8) with st1 as (select rownum row\_num1 ,store\_id from (select distinct [a.store](https://a.store) store\_id from store a ORDER BY dbms\_random.value)rec) ,emp as (select rownum row\_num1,a.\* from employees a ORDER BY dbms\_random.value) SELECT a.\*,b.employee\_name FROM emp b,store a,st1 c where c.row\_num1=b.row\_num1 and c.store\_id=[a.store](https://a.store) &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B;
Thanks jgs84 1.)This will be an issue. The amount of stores varies from 500-2500+. I only have 5 employees. 2.) Yes, that might be an issue but I figure I could remove the absent employee off the list. Another possibility is to have just tell one of my employees to take "Jason" stores in addition to her assigned stores for the day
Looking for something like this? WITH marchBillCustomers AS ( SELECT DISTINCT customerId FROM `BillPayments` WHERE payment_status = 'SUCCESS' and extract(month from billDate) = 3 ), AnotherCTEHere ( ) SELECT Blah FROM marchBillCustomers A LEFT JOIN AnotherCTEHere B 
Can you throw that case statement for determining fiscal quarter in a sub query, then group by fiscal quarter in the parent query? 
What we do in our data warehouse is to have a date_d dimension table with one record per date. Some of the columns in the table are: Calendar Date, Calendar Year, Fiscal Year, Month Number, Month Name, Fiscal Year-Month, Calendar Year-Month, Day of Week, Name of Day, Week of Year, etc.... 
I am, and I actually tried this from my select statement; SELECT TOP 1 PERCENT * FROM xyz. That just populates 1% of records in XYZ, obviously. What I need is *1% of each users* records from the table. I wasn't able to wrap my head around a solution that would output those values using the TOP 1 PERCENT method. 
It would be way simpler to just do this: select datepart(quarter, cast(vrijedatum1 as datetime)) AS QuarterTime, count(naam) as Total from table group by datepart(quarter, cast(vrijedatum1 as datetime)) I don't quite understand what you mean by "current date", but it sounds like you need a subquery that joins on QuarterTime which counts someting else and then gives you another column next to total? Use a CTE.
You could SUBSTRING(), LEFT(), RIGHT(), and CHARINDEX() functions to do this very easily. Here is a good write up on this: [https://social.technet.microsoft.com/wiki/contents/articles/17948.t-sql-right-left-substring-and-charindex-functions.aspx](https://social.technet.microsoft.com/wiki/contents/articles/17948.t-sql-right-left-substring-and-charindex-functions.aspx) This is assuming you're using MS SQL Server.
How about Select SUBSTRING(SourceCol, 0, CHARINDEX(‘-‘, SourceCol)) AS OutCol1, SUBSTRING(SourceCol, CHARINDEX(‘-‘, SourceCol)+1, LEN(SourceCol)) AS OutCok2.
Easier to use a simple UNION? select LEFT(2, field) AS Col1, RIGHT(6, field) AS Col2 from table where left(field, 3) &lt;&gt; '-' union all select LEFT(3, field), RIGHT(5, field) from table where left(field, 3) = '-'
Or you could make it in excel/openoffice in about 30 seconds and import it. We use a date table in a lot of stuff too, love it!
Can some provide a bit of guidance on how to solve this problem? Or link me to a resource. Thank you in advance great soul! 
There's so many tutorials , books, and blogs that teach SQL. If you're looking specifically to learn normalization, then a good start would be databasestar.com or books written by Itzik Ben Gan. 
It kinda depends on your existing knowledge of the material. If you are already a DBA with a few years of SQL Server experience you might get by with reading the documents of the items the exam covers and reviewing some of the free sample questions that the dumps offer. If you don't have the experience you are going to have to spend a fair amount of time studying.
Apologies! It's an Analyst role.
This is a static solution and tbh, kinda clunky. Why not just use one query that would handle both situations? (no unions or where clause needed) MySQL: SELECT LEFT('xx-xxxxx',LOCATE('-','xx-xxxxx') - 1) , RIGHT('xx-xxxxx',LOCATE('-','xx-xxxxx') + 2) Replace the 'xx-xxxxx's with the column name and Bob's your uncle.
MySQL? **piece of cake** (ignore all the CHARINDEX replies you're getting) SELECT SUBSTRING_INDEX(code,'-',1) AS first_part , SUBSTRING_INDEX(code,'-',-1) AS second_part FROM ...
[SUBSTRING_INDEX](https://www.reddit.com/r/SQL/comments/avdvoo/help_with_a_query/eheck2h)
I don't understand your question. Your year is being pulled from your third table. This table is joined to your first table by code. 
Like you said, you'll need that extra JOIN predicate for year: SELECT c.code, name, region, e.year, fertility_rate, unemployment_rate FROM countries AS c INNER JOIN populations AS p ON C.code = p.country_code INNER JOIN economies AS e ON c.code = e.code AND p.year = e.year The reason why you're getting extra rows with non-corresponding data is that your query is returning all of the rows from countries/populations and tacking on the columns from the economies table. It doesn't know (since you have specified), that you want to see economies data for each year specifically.
Thanks for reading it! My question is, why does the output show AFG having a fertility rate of 4.653 in 2010 , and have 5.745 in 2015? These are the opposite of values that are shown in the table. 
You didn't join on year so it's taking every available year for the code AFG from your third table and populating it into your output. It is also taking every available fertility rate on your second table and populating it in your output. Your third table has 2010 &amp; 2015 and your second table has 4.653 &amp; 5.745. Since you didn't join on year and just on code it made every single match it could to the first table. 
Damn, that's nice. Microsoft isn't so kind to us SQL Server users.
Yes, that would indeed be a lot easier haha I requested this option as well. But if I could solve it by SQL that would be nice too hihi
Very jenky indeed 
Oh no! I do not know how to do that. I'm beginning to think I should spend some more time learning before jumping into this. I'll do some reading and try to figure it out! Thanks for pointing me in the right direction!
I have a natural aversion to using substring() and charindex(). I think they are difficult to read and intuitively understand, and they can be big hits to your performance. I'm not saying you're wrong, but I dislike seeing them unless absolutely necessary. If the execution time is negligible between that or a union, I would go with the union personally.
We have a couple clients that do weird shit like this, and a date table is really the only solution, I suppose.
Union-ing two queries and using LEFT() function in the WHERE condition on both hardly seems more performant than a single query with a couple of inline functions. But I get where you're coming from. &amp;#x200B; Different strokes for different folks, I guess.
There's a logic bug in your dates. You're doing WHEN CONVERT(datetime,vrijedatum1) BETWEEN CONVERT(datetime,'07-29-2018') AND CONVERT(datetime,'10-27-2018') THEN 'Q1FY2019' This is flawed because of your end date. The date option has it be midnight, so on your last day (10/27), ONLY transactions that happen at midnight get counted. Reformat it to the following and you'll avoid the logic bug: WHEN CONVERT(datetime,vrijedatum1) &gt;= CONVERT(datetime,'07-29-2018') AND CONVERT(datetime,vrijedatum1) &lt; CONVERT(datetime,'10-28-2018') THEN 'Q1FY2019' The first part is doing a &gt;= on the start day (so it includes midnight) and is doing a &lt; on the end date + 1. That encompasses the full time period you're interested in.
Ill try this, thanks!
Thank you for pointing that out! :)
Group by user and order by what ever you need. 
Filter by the minimum Entry\_Creation\_Date between two Bots with 24 hours gap and Current\_Switch\_Status =Off
Thanks for this input. I have a follow up question if you have the time. This solution worked great, and I can confirm that I'm able to pull 1% of each user's records. I did leave out 1 details though. I wanted the 1% of user records I return to return randomly. In my initial query, I was able to do this with ORDER BY USER, RAND(). I ended up creating a tempt table, and using your query to pull 1% of user records, but the records returned are the records with the most recent time stamp. Is it possible to pull the records randomly, rather than chronologically? I tried dumping my data into the temp table with ORDER BY USER, RAND(), but that random order is lost when i run the second query for the 1% records. 
 SELECT CA.CUST_ID , CA.CUST_ADDR_ID , CA.CUST_ADDR_EFF_DT , CA.AUDIT_USER_CREA_DTM , A.acct_id , A.acct_no , CN.CUST_NM AS Customer_Name , CA.CUST_ADDR_STR_1 || ' ' || COALESCE(CA.CUST_ADDR_STR_2, '') AS Street , CA.CUST_ADDR_CITY_NM AS City , S.STT_ABRV AS State , CA.CUST_ADDR_POST_CD AS Zipcode , CA.CUST_ADDR_TYP_CD FROM Account A INNER JOIN PCMP.CUSTOMER C ON C.CUST_ID = A.CUST_ID INNER JOIN ( SELECT CUST_ID , MAX(CUST_ADDR_DT) AS latest FROM PCMP.CUSTOMER_ADDRESS WHERE VOID_IND = 'n' GROUP BY CUST_ID ) AS max_addr ON max_addr.CUST_ID = C.CUST_ID INNER JOIN PCMP.CUSTOMER_ADDRESS CA ON CA.CUST_ID = max_addr.CUST_ID AND CA.CUST_ADDR_DT = max_addr.latest INNER JOIN PCMP.CUSTOMER_NAME CN ON CN.CUST_ID = C.CUST_ID INNER JOIN PCMP.STATE S ON S.STT_ID = CA.STT_ID WHERE A.Acct_No = 34835
What is group concat? 
Quick and dirty way could be to count the number of '-' and send the 1s to a column and send the 2s to the SSN column. 
Dbeaver by far Only thing it sucks at so far is backups and restores
I can be the old-school guy, MS Access connected to your SQL server. Every company I've worked with have an Access front end to SQL somewhere, good to know and pretty simple to set up.
Your subquery looks at more rows than your outer query so you might be picking up max(CUST_ADDR_DT) from records where VOID_IND is not 'N'. try changing your subquery to this: (SELECT MAX(CUST_ADDR_DT) FROM PCMP.CUSTOMER_ADDRESS CA2 WHERE CA2.CUST_ID = CA.CUST_ID and CA2.VOID_IND = 'n')
I second this, MS Access front-end to a SQL database is pretty common. It also supports VBA development so the customization of buttons, forms, ect. can be accomplished.
You could use a tool such as LibreOffice Base (very similar to Microsoft Access) which can connect to Postgres and allows you to build simple GUI interfaces. The other approach would be a very simple Python or PHP web app although this depends on your level of knowledge.
This is what I came here to say. If licensing isn't an issue, I would definitely recommend Access. Access is an okay back end, but it's really an *excellent* front end.
Hi. This didn’t work unfortunately...
I would put your main query in a CTE and then use row_number, partition it by the account number and order by the date it was added to the database descending: ;with your_Cte as ( select row_number() over (partition by a.acct_no order by CUST_ADDR_DT desc) as rn &lt;rest of the select/join&gt; ) select * from your_Cte where rn = 1 This should give you the max date.
would i use LEN or DATALENGTH to accomplish that?
Thanks for the help guys. I actually pulled it off soon after asking here but i will explore your suggestions to learn a bit more. The way i did it: UPDATE table SET NewColumn1 = SUBSTRING (OriginalColumn, 1, LOCATE('-', OriginalColumn) - 1), NewColumn2 = SUBSTRING (OriginalColumn, LOCATE('-', OriginalColumn) + 1);
 select distinct ?
Maybe something like this? (Sorry, on mobile) You can do that using replace and len. Count number of x characters in str: len(str) - len(replace(str, 'x', ''))
I highly recommend PostgreSQL so congrats for making the best decision to learn SQL IMO. Use psql command line to really learn. If you need a GUI, pgAdmin3/4 are the "go-to" front end client for PostgreSQL. I've installed Dbeaver on a Windows box before but didn't love it. Try them out though and see what works for you. &amp;#x200B; &amp;#x200B;
If you have trailing or beginning spaces I'd use datalength
okay thanks for the direction,. I will run with that for now. &amp;#x200B;
Off the top of my head, an easy way to do it would be dumping your data into a second temp table and creating your own row numbers based on a random order. &amp;#x200B; Since you're using SQL Server, you could use ROW\_NUMBER(): SELECT User , ROW_NUMBER() OVER (PARTITION BY User ORDER BY Rand()) AS RandomRowID INTO #TempTable2 FROM YourTable This will create new row numbers for each user in a random order. Then use the same code as above, but replace Row\_ID with the new RandomRowID in the WHERE clause.
GROUPBY the two columns you are selecting
did you try it with the join? (SELECT MAX(CUST_ADDR_DT) FROM PCMP.CUSTOMER_ADDRESS CA2 INNER JOIN PCMP.STATE S2 ON S2.STT_ID = CA2.STT_ID WHERE CA2.CUST_ID = CA.CUST_ID and CA2.VOID_IND = 'n') 
 \*\*\*\* Section Times in Video \*\*\*\* \#1: Open the Index Usage Blog Post [0:32](https://www.youtube.com/watch?v=ekOTr1nrLSA&amp;t=32s) \#2: Copy the Index Usage SQL Script [1:39](https://www.youtube.com/watch?v=ekOTr1nrLSA&amp;t=99s) \#3: Paste the SQL Script in SSMS [3:06](https://www.youtube.com/watch?v=ekOTr1nrLSA&amp;t=186s) \#4: Execute the SQL Script [3:21](https://www.youtube.com/watch?v=ekOTr1nrLSA&amp;t=201s) \#5: Modify the WHERE Clause [4:25](https://www.youtube.com/watch?v=ekOTr1nrLSA&amp;t=265s) \#6: Post any Questions! (I will reply) [7:43](https://www.youtube.com/watch?v=ekOTr1nrLSA&amp;t=463s) 
Most people in mid/large level companies use MS Access. It does exactly what most people need
I don't know what you're trying to accomplish. Based on your initial comment select distinct item, fee from table Will suffice. This will give you the distinct combinations of item and fee. Exact duplicates will be removed. Based on your edit "I'm looking to record them before removing them". What does this mean? using HAVING count(*) &gt; 1 Will omit any items from 'table' that only have 1 entry, like C01 in your example.
I want to make a separate report table that lists all invalid rows, with a column noting why those rows are reported such as duplicate rows. Then I'll remove invalid rows in the item, fee table.
I have no idea if this function is in oracle, but use something like SQL Server’s ROW_NUMBER() where you partition by the field that needs to be unique. Then just filter only 1s in the final result.
 select [name], [value], count(*) as total_entries, 'Duplicates' as note from testing_table group by [name], [value] having count(*) &gt; 1;
GROUP\_CONCAT is an amazing MySQL-exclusive function that can convert multiple rows from this: A B C To this: A,B,C
The group by method others have suggested will definitely get you to your final goal, but I don’t think there’s been a solution for pushing the duplicates to another table. You could of course do a “having count(*) &gt; 1, but if you wanted to capture each row instead of just getting a count of the duplicates, I’d do a row_number() over (partiton by (two columns in question) order by (whatever would make the most sense) as ord You can then select * where ord = 1 into one table, and * where ord &gt; 1 into another.
This is likely going to be a permissions issue; likely the secondary server does not have access to the drive/directory where the backups are placed. As @[alinroc](https://www.reddit.com/user/alinroc) mentioned, do you have error messages that you can paste in here?
&gt; Edit: I'm looking to record them first before removing them. I would just backup the table to another table... SELECT * INTO table_backup_20190227 FROM table; Truncate the original table (assuming the application isn't currently using it) TRUNCATE TABLE table; Then re-insert the unique records: INSERT INTO table (item, fee) SELECT distinct item, fee FROM table_backup_20190227; This assumes your table isn't more complex than your post indicated...
At my previous employer, calendars were so tricky and had so many exceptions and business rules. For example, the "month" for futures orders were between the 20th of the previous month and the 19th of the current. That is, a Futures order with a delivery date of April 23rd was considered a "May" delivery. However for Point of Sale information, the week and month were defined by "Friday". So if Friday was March 1st, the entire previous week (Feb 23rd forward) was considered the first week of March. Sales, however, also liked to use a 4/5/4 calendar, so the first 4 weeks are "month 1" of a season, the next 5 weeks are "month 2", etc... regardless of where the months start/end. The only solution we had that worked without fail for all the various calendars was to simply build a table at the granularity of a day and have a column for each kind of thing we needed "POS Week" ,"POS Month", "Fiscal Quarter", and so on. This allowed for a simple join to get this information. The downside to this method (over a formula) is that you have make sure it is always filled out in the future for any dates that appear in your system.
That's not SQL, it's VB.NET.
[Imgur album](https://imgur.com/a/XNfZzj5) with the same graphic for other programming languages - for comparison! | If you want more information on how these were created, it can be found [here](https://www.globalapptesting.com/blog/picking-apart-stackoverflow-what-bugs-developers-the-most)
Thanks again for all the help
Manually keying in the dates doesn't make your query dynamic. if you had a calendar set up you could run it without changing anything.
The query below will work for a string that is either spaces and/or zeroes. SELECT * FROM ( VALUES('000 00000 0 0'),('1'),('00000000000000'),('0000010000') ) test(col) WHERE LEN(REPLACE(REPLACE(col, ' ', ''), '0', '')) = 0 If you want it to be just zeroes, use this: SELECT * FROM ( VALUES('000 00000 0 0'),('1'),('00000000000000'),('0000010000') ) test(col) WHERE LEN(REPLACE(col, '0', '')) = 0 AND ISNUMERIC(col) = 1 
I'd highly recommend HeidiSQL for an easy to use work bench/UI Pretty much allows access to all SQL dialects (except SQL-lite) and is very easy to use.
Oh Snap. I didnt even know that. Jesus. You sure about this? Can you help 
It sounds like you are wanting to build a user interface that stores data in a database. There are lots of options from Delphi to Django to just about every programming language out there that has the capability to display and interact with a GUI. If you are on Windows you can build something fairly easily with the free version of Visual Studio. I say easily, but there's a ton of context to get past. If you are on another OS, or just want something free and easy take a look at Lazarus. It will let you build a cross-platform drag and drop database interface quickly and easily. If you want web-based something like Ruby on Rails or Django or even PHP could work for you.
PgAdmin. It’s a SQL workbench. Start there to actually learn the SQL, then start playing with Grafana and the likes. 
 Select abc.id, abc.type, def.date, def.code From Schema1.abc, Schema2.def Where def.code not in ('21318037') Where's your join context? How are you joining `abc` to `def`?
Thanks pal, but I don't understand your answer, could you explain? Thank you once again for your response...
##r/DataArt --------------------------------------------- ^(For mobile and non-RES users) ^| [^(More info)](https://np.reddit.com/r/botwatch/comments/6xrrvh/clickablelinkbot_info/) ^| ^(-1 to Remove) ^| [^(Ignore Sub)](https://np.reddit.com/r/ClickableLinkBot/comments/9wy10w/ignore_list/)
Exactly. I was able to resolve the issue. It's exactly as u guessed. Thank you 
Thank you for your help! I have already used a trim function and removed all middle spaces. Can you please explain your logic that you have used?
No joins (this query was given to me to try to optimize as currently it's pulling close to 500k records). It is SELECTing [Table].[Field] FROM [Schema].[Table] I have extremely little knowledge of SQL, so I'm not sure if having joins would help or not
This got me closer. I can now retrieve row numbers but I can't figure out how to remove them. I have them aliased as "rn"
Absolutely is, and OP should definitely use it. However, IMO the bigger problem is not using ANSI-92 joins. Op: highly recommend you switch to ansi-92 joins to get a better picture. Something like: SELECT abc.id, abc.type, Def.date, Def.code FROM INNER JOIN ON —missing join context here WHERE def.code NOT IN (‘21318037’); Once you do that, you can add the row_number column and use a sub query to get the first row Adding the Row_number column: SELECT abc.id, abc.type, Def.date, Def.code, ROW_NUMBER() OVER (PARTITION BY abc.id ORDER BY 1) as RownumColumn FROM INNER JOIN ON —missing join context here WHERE def.code NOT IN (‘21318037’); Note that you don’t have to order by 1. You can order by some other column if you wish, like createddate to get the newest/oldest one. Once you establish the rownumber, use it in a sub query. SELECT main.* FROM ( SELECT abc.id, abc.type, Def.date, Def.code, ROW_NUMBER() OVER (PARTITION BY abc.id ORDER BY 1) as RownumColumn FROM INNER JOIN ON —missing join context here WHERE def.code NOT IN (‘21318037’) ) main —just aliasing the entire query as main here WHERE main.RownumColumn =1;
Dbeaver by far has been my favorite. And I like telling people I use dbeaver.
&gt; From Schema1.abc, Schema2.def You're selecting from abc, def. Those are two tables. It's doing a join whether you explicitly use the JOIN keyword or not.
Thank you!
Have you checked out [Revere's](https://revere.ai/features) editor and visual builder? It's free, cloud-based and can write/save/run raw queries or build via graphical interface - and set it to run on a schedule. Has routing and alerting, might not need that now. (MySQL isn't supported yet). Check out [PopSQL](https://popsql.com) too, does visualizations but limited in what you can do with free version (Mac app).
Perfect! UpVote my response if you don't mind : )
&gt; What is group concat? da bomb
The answer kinda depends on the database you’re looking to connect to. Or are you hoping to host one on your machine for practice?
Just saw these guys at the mairadb user conf. Tool looks cool and even has an interface for Cassandra (paid version only) pretty interested to try it
You can easily install SQL server. Microsoft has both an oltp and dwh database available for practise (adventureworks)
SQLite is the easiest, as it's file-based and not a server PostgreSQL is my favourite
It's not MySQL exclusive since MS SQL 2017's STRING_AGG()
Read up on FULLTEXT INDEX. The MATCH AGAINST operator is designed to do just what you’re looking for 
I really like your style, would love to hear about more complex topics as well as things like SSIS or other integration / data pipe lining type tooling.
I wanted to thank you for your help. Although I didn't get the correct answer I was close enough to earn some points. I'll be starting early on the next assignment extra early so that I can reason/walk through things this time. 
What was the correct answer? 
Man.. my first instinct is to set fire to the servers and walk away. The only thing I've got off the top of my head would be something (probably two functions) that rip both strings apart into their respective words, put them into a temp table, then compare. Might not be the best way to do it, but I'll give you some matches..
Is it? I kinda despise Access. What an I missing?
I'm pretty high, and just casually OK at SQL, and use a different variant of SQL but hopefully this helps. WITH marchBillCustomers AS ( SELECT DISTINCT customerId FROM `BillPayments` WHERE payment_status = 'SUCCESS' and extract(month from billDate) = 3 ), marchLoanCustomers ( select distinct customerid from LoanPayments l --This join right here makes sure you only get loan payers who also are bill payers join marchBillCustomers m on l.customerid=m.customerid where payment_status = 'SUCCESS' and extract(year from billDate) = 2018 ) SELECT extract(month from billDate) AS Months, count(distinct BP.customerId) AS Number, count(distinct Mloan.customerID) FROM `BillPayments` as BP, marchBillCustomers as Mbill, left join marchLoanCustomers Mloan on mloan.customerid=bp.customerid where BP.customerId = Mbill.customerId group by Months order by Months
What do you despise about it? Access is really useful in two situations imo: 1. As an AIO database and reporting solution in situations where people either can't afford a relational database / dba or don't have access to them 2. As a front end to a back end database Contrary to what some elitists might say, Access *is* a relational database. With this, you get advantages like referential integrity, data validation, etc. that you'd expect from a relational database. In addition to this, Access has really strong form and reporting capabilities. So you can use forms to prevent users from getting access to the underlying tables and validate the data they're inputting. And that data can later be displayed on a report, analyzed in Excel, etc. Since it's part of Office, you can also combine it with VBA to get more powerful solutions. Due to the strong form capabilities I mentioned earlier, it was designed by Microsoft to be really intuitive. So you get a lot of flexibility in designing something really intuitive while all of the power is handled on the backend on a database server. The *worst* features in Access are backend related. The SQL editor, for example, is garbage and ANSI SQL is not supported. So there are no views, no window functions, etc. There are Access hacks that you can use to mitigate this. But it would just be nice if you could use the standard SQL that *literally every other major database supports.* Additionally, I think Access files are limited to a 4gb size, which could be a problem later on. You can upgrade Access to SQL Server, but then you have to purchase SQL Server, which isn't cheap. But by using an Access front end with a database server backend, though, you get the best of both worlds. So that's why the combination of the two are recommended.
One of my favorite features of dbeaver is that it had integrated ssh tunneling. The only other interface I found that had it was pgadmin III. But I think DBeavers was much cleaner. For everything else I tried using I needed to use Putty.
SQL Server Express is you want to just screw around 
If you normalize the brand/keyword table so only 1 keyword is associated with each brand, you could run dozens of queries against it in a short amount of time (assuming you have an index on the brand and keyword) Full text searching might be good fit here too... Might be worth looking into
And SQL server development is fully featured and free 
I guess my biggest gripe is that I didn't find it particularly intuitive at all, and, as you pointed out, I *hated* the SQL editor. I may have just been too quick to throw in the towel, but I found it easier to set something up in Excel to serve as a user access point. I did have some pre-existing VBA code to ~shamelessly steal~ modify, though, so that certainly contributed to the easier setup. If I'm ever in a similar situation I guess I'll have to give it a second look. Sounds like the issues were more mine than the software's.
Access is meant to fill *very specific* use cases, like the ones I mentioned earlier. It can't be too good at SQL (otherwise it would eat into Microsoft's SQL Server sales) but it can't be too bad either (or no one would use it as opposed to something like SQLite.) So it's *just good enough* for you to do what you need to do, even if it requires more effort than it should. I was probably equally frustrated when I first started learning Access. But as I was learning it, I realized that it's more powerful than I initially gave it credit for. My main reason for learning it was that I already knew SQL / relational databases and VBA in Excel. So Access seemed like a natural compliment. What really motivated me to learn it was learning that I could use it as a FE and use a different database as a backend. So I created my own linux server on a virtual machine that hosted an instance of postgres, and I connected to this instance using LinkedTables in Access and SSH tunneling in PuTTY. I like SQL a lot, but I completely refuse to use SQL in Access. That's how bad using SQL in Access is imo. I think I even struggled creating table aliases. The query designer isn't actually that bad once you get used to it. Access' form capabilities smoke Excel's UI capabilities much like Excel's analysis capabilities smoke Access's. It's not the right tool for every job, but Access solutions can be valuable.
Thanks for the education! I'll definitely have to give it a second look.
&gt;I really like your style, would love to hear about more complex topics as well as things like SSIS or other integration / data pipe lining type tooling. Thank you so much! I will definitely be adding more SQL and Business Intelligence videos to the channel. I will be sure to share each of them on here as well.
Before running this query create two column dateasd(d,7 ,date) 7daysprior in a temp table or cte.. Lets call it #base Then select sum(revenue) revenue, date, daysprior Into #base2 From #base Group by date, daysprior Select b.date, b2.date as lastweek, b.revenue as revenue, b2.revenue as last weeks revenue. From #base b left join #base b2 on b.date = b2.[7daysprior] Follow the same pattern to get two weeks ago. If there is a record for every date , you could probably do something like this ...lag(revenue,7) as 1week, lag(revenue,14) as 2weeks ...instead of two joins.
I actually did that already
I appreciate it!
&gt; I want the summary to self populate after the input is given have you thought about using a **view** -- no triggers required
&gt; If you normalize the brand/keyword table so only 1 keyword is associated with each brand, you forgot to say **1 keyword per row** the Disney brand in OP's example would have 4 rows in the normalized table 
Aye.
Hope this can help you [https://education.stratascratch.com/guides/getting-started-with-stratascratch/getting-started-with-stratascratch](https://education.stratascratch.com/guides/getting-started-with-stratascratch/getting-started-with-stratascratch)
In the where clause he's counting the length of the string after replacing all 0's in the string with a null character. If after this transformation the string has length 0, then you know the string contained only 0's, otherwise the length of the evaluated string would be &gt;= 1.
You could use the row\_number window function: SELECT * INTO tableduplicates FROM ( SELECT item, fee, row_number() over (partition by item, fee order by item) as duplicate FROM Table ) temp WHERE temp.duplicate &gt; 1 DELETE temp FROM ( SELECT item, fee, row_number() over (partition by item, fee order by item) as duplicate FROM Table ) temp WHERE temp.duplicate &gt; 1
the advantage of this method is you can expand the criteria greatly, I had to use this to remove duplicates from a table with 800 odd columns (reporting data warehouse, awful wide structure made by non-database people)
There are literally hundreds of possible tasks they may ask you to perform. The job description would probably give you a clue. Some possible tasks may be. &amp;#x200B; Debug a stored procedure that is returning wrong / duplicate data Performance tune a query, view stored procedure etc. Provide index recommendations Write a query based on some requirements (highly likely) Import some data into a table There will probably also be some technical questions, some areas I find that recruiters mostly ask about are: Indexes (talk about b-tree and column store, clustered and non clustered) set operators (union, union all, intersect, except) primary keys and foreign keys data warehouse design and data modelling Explain different types of logical joins (left, right, inner, full outer, cross). Also get bonus points for talking about physical joins (nested loop, merge, hash) &amp;#x200B; Good Luck.
A few other ideas you could use: substring(fed\_ids , 2, 1) = '-' --(assuming no leading spaces) substring(ltrim(fed\_ids), 2, 1) = '-' --(assuming there could be leading spaces) len(rtrim(ltrim(item))) = 10 &amp;#x200B;
For learning purposes [PostgreSQL](https://www.postgresql.org/), is a really good start as it is closer to the standard than most other products. Plus it is an extremely capable DBMS that can be used for more than just learning. SQL Server and MySQL ignore the SQL standard even for little things (e.g. `||` vs. `+` for string concatenation) and thus I would not recommend them for someone beginning to learn SQL as this adds a lot of confusion. 
I would assume it depends on the level you are interviewing for, but on my skills test are questions like: Explain the different types of joins and unions. Difference between having and where. I have a diagram of a badly designed (simple) database as ask them to point out a few problems. Differences between the index types. Differences between the different lock types. The URL to Stackoverflow. I also ask them to walk through their though processes on fixing a Function that is performing badly. (I want to hear them talk about reading the execution plans and how to do that.)
Learn PostgreSQL!
One of my mentor's favorites is list as many differences been delete and truncate as you can. Without doing it myself, I think there are about 7.
I'm not sure yet, but I'll try to post it when I get the assignment back. The professor said that I was close, but still off with this: `SELECT Name` `FROM Tb_Product P` `WHERE NOT EXISTS (` `SELECT *` `FROM Tb_Consumer C` `WHERE NOT EXISTS (` `SELECT * FROM Tb_Transactions T, Tb_Consumer` `WHERE T.Con_ID = Tb_Consumer.Con_ID` `AND T.Con_ID = C.Con_ID` `AND City = 'Stevens Point'))`
Will there be reporting involved? If so, being able to write complex queries incorporating calculations may be asked.. getting to know the built-in functions(sum, avg, min, max, etc) will help you with this.
Thank you, I was extremely tired last night and realized a couple easier ways to do this. Thank you!
indexes, table joins, sub-queries
I'm surprised join is not bigger.
That's how assholes write SQL.
Yes, what u/pineapple_catapult said. If you break it down using a string of '0000010000', here's how it works: REPLACE(col, ' ', '') String will remain as '0000010000' since there were no spaces in it. REPLACE([...], '0', '') The [...] is the string that remains after removing all spaces. Then, every '0' character will be removed. The string will now be ''. LEN([...]) This is the length function. Since the [...] is now equal to '', the length of the string is 0 after removing all spaces and zeroes from it. If there were any characters that are not either a space or a zero, the length would be &gt; 0.
Why isnt that working? What does it do?
The values are being taken as " " I mean, no values are being taken...only an empty cell
ooohhh @xulafu, thank you so much for the assistance(answer) am very grateful!
I had to adopt this a bit, but the last code you entered is more or less exactly what I needed, thank you!
Fantastic. Glad to hear.
Create a numbers table (https://www.red-gate.com/simple-talk/sql/database-administration/creative-solutions-by-using-a-number-table/) and your result is a join away. 
select item, SUM(qty_on_hand) from itemloc where whse &lt;&gt; 'L' and item = 'item1' GROUP BY item
This is the correct answer. Also FYI: if you add 4 spaces to the beginning of each line it will auto-format to be a block of code.
 select item, SUM(qty_on_hand) from itemloc where whse &lt;&gt; 'L' and item = 'item1' GROUP BY item
This guy formats.
% is a wildcard character in string matching you need to escape it 
Some bedtime reading there, thanks! Looks like it'll work grand for me.
udemy
substr in BO might be allowing only to pick up characters AFTER a certain position. try this =Substr([Product Remark Internal];Pos([Product Remark Internal];"%")-2;2)
Sorry. I just copied and pasted this as I was headed out the door this morning. If it’s the formatting then it’s my fault and I can edit it after work. If it’s not the appropriate way to query this information then I’m willing to listen and learn from other methods. 
Yeah, that's a bit to pricy for me. :/ 
Unfortunately you can't do that in PSequel, as once you run the query the program just lags until it's finished, our until it times-out (which is usually 30 seconds - 1 minute).
Microsoft Access is always an option.
I’ve built simple .NET applications that allowed end users to query and remove records from tables in the database. There are various things that .NET has for specifically dealing with SQL Server but the drawback is you have to tailor something to your own needs. I’m sure someone has put similar code out there that you could tweak to get working for your use case. 
It isn't wrong. And if something works, it isn't stupid.... but that is a horrible way to write the query for what you are trying to do, and there are many other ways to write that query more elegantly.
Thanks, I knew I was doing something stupid...
An access front end can be a great option. It also supports VBA for custom development. I’ve spun up a few access DBs like this so you can message me if you want more specifics 
I second the .Net idea. No matter what solution you implement I would recommend not deleting the record outright and do a soft delete instead. Inevitably an end user is going to delete a record that they did not intend to delete and that will create a whole new world of problems for you. Add a column to the table and call it “Record Deleted” with a yes/no, true/false value within it. Then only show the users the no/false values. Another method would be to insert the record into an archive table before deleting it from the primary table. Bonus points for capturing the person who deleted the record with a date/time. This way you can easily restore records without resorting to backups.
sqlzoo.net
Please read the sidebar regarding formatting your code. 
I’d say you’d have to pass your strong variable into a results set like a table. Then you’d likely have to do the same with your table with some kind of function which returns a table. Inner Join them and group the results. As long as one keyword matched you’d get a resulting row. Efficient. Not likely as a quick stab. But you could do all sorts with that SQL row already in the DB. Trigger to do the split on change to a new table, optimise table to be super fast. 
Could you just remove the spaces, then just sum(cast x as int) where it’s = 0
You’d need to split the string on the first space possibly (do some records say Coca Cola. In which case split on first [0-9]) then what’s to the left os the product, the right is the code, remove spaces, cast as int and where clause that expression.
You can start a new session
Theoretically this should not do anything and I'd recommend just not doing that. However there have been cases where my database, well specifically one table, ground to a halt. When this happened restarting the server this database sits on and dropping + re-adding the 'broken' table fixed the issue once each.
Not there. 
I quite like CTE's over nested joins and sub-queries. From a maintainability standpoint, they're easier to read and make changes when needed. There shouldn't, as far as I know, be a large difference between a CTE and what it's sub-query would be. 
This completely depends on your database platform. Postgres, for example, materializes CTEs. This can be beneficial if you're using a CTE multiple times within a query, but it can also be a burden depending on intersecting criteria which the optimizer may be able to cut out without materializing.
Thank you!!! I’ll spend more time fixing the database so the queries would be much easier and less complicated!
It's a bit tricky, but here's how I'd do it: 1. Attend your lectures and study groups 2. Read the assigned readings 3. Engage with your tutors for assistance 4. Answer the question with your new found knowledge. Also look into Order By and Top - the exact answer will depend a little on which platform you're using.
Just be careful of over-using CTEs so you don't needlessly create memory pressure on the server.
Might I suggest, that rather than have them DELETE the records, you allow them to MOVE the records to a backup table.....End Users...Nuff Said.
Postgres has string agg as well. 
You could group by a case statement Case when day between 1 and 7 then ‘week 1’ when day between 8 and 14 then ‘week 2’ etc. I’m sure there’s a more elegant solution but can’t think of it. This would work tho
Is there no date? If so, could you use a Datepart for Week? DATEPART ( week, date ) 
EDIT: nevermind guys i found out theres a date table in my working schema all i need to do is to join it with the days and group em by the week and divide the sum by 7 im dumb :)
 Will this work? With results as( Select top 1 first\_name, last\_name, count(\*) from table where description like '%java%' group by first\_name, last\_name order by count(\*) desc) Select first\_name, last\_name, course\_no, description from table T left join results R on t.first\_name = r.first\_name and t.last\_name = r.last-name where description like '%java%'
Are you looking for two numbers followed by a percent symbol? Would '45.67**89%**' be an acceptable match?
How about the following: &amp;#x200B; Create three tables Table1 - Players Field1 - PlayerID Field2 - Playername etc. Table2 - Badges Field1 - BadgeID Field2 - BadgeDescription etc. Table3 - PlayerBadges Field1 - PlayerID Field2 - BadgeID With tables in this way, you can expand them however you want and simple join to pull data.
Will you and the person have access to a whiteboard or is this telephonnyyyyy? &amp;#x200B; Either way, without knowing what level developer you are looking for I would ask &gt; \-What is the difference between reactive and proactive followed by, give an example of a situation which ... \-How would you solve a data truncation error? \-What are the differences between a query written in SSMS/app (or pass-through) versus calling a stored procedure? \-How would return all rows from table A which are NOT in table B? \-Explain how you would update/delete more than 1M records \-Which is more important, updated statistics or indexes? \-What is the difference between equality and inequality? \-Are you familiar with disk I/O and how they impact performance? \-What are DMVs? How have you leveraged them? \-Do you know any powershell, if so, examples, if not, why? &amp;#x200B; Loaded Questions &gt; \-Is CXPACKET good or bad? \-Are cursors good or bad? \-Can you Google or do I need to LMGTFY :P &amp;#x200B; Hypotheticals &gt; \-If you could write code which performed faster or was more maintainable, which would you choose? \-When you you are mapping out a new process, do you write it out or 'code as you go' (I am 100% guilty of this) ? &amp;#x200B; \+ what others have said about data modeling, snapshot isolation, indexes, execution plans. However, I think operator talk may be a little out of scope cough @jgs84 &lt;3 &amp;#x200B; Hope all works out. :) &amp;#x200B;
Completely dependent upon the RDBMS. For SQL Server, it's true. For Oracle &amp; Postgres, it's not - those two materialize CTEs into (basically) temp tables.
r/ssis/ would probably be more appropriate
There’s not enough people subscribed there though. 
SSIS? You mean, embedded execute SQL until the end of time? ;D
&gt; Add a column to the table and call it “Record Deleted” with a yes/no, true/false value within it. If you make it a `datetime` field, you can record *when* it was deleted too. Anything with a `NULL` values hasn't been deleted. &gt;Another method would be to insert the record into an archive table before deleting it from the primary table If you're using SQL Server 2016 or newer, you can have SQL Server do this for you with a temporal table.
Ahaha, I’m about to get fired on my first day. Shit!
Both people have given great answers, but, I would suggest pitting your CTEs against extrapolated versions of your CTEs to see which perform better. CTEs, be can very beneficial, but, at the same time they can't. Unless you are dealing with sub-second and microsecond thresholds, I would always learn toward maintainability. 
Sounds like ether A) a disgruntled DBA or B) X &amp;#x200B; This sounds like the RO is being hammered and needs updated statistics rather than an index rebuild, but, WTF do I know? :(
RIPPPPPPPPPP
All I can see is Excel and I'm going to throw up, BRB. XD
Without knowing how often these deletes happen, You could take this a step further by creating a trigger to insert the DELETED record(s) into a table, then, delete the record(s) from the primary. &amp;#x200B; Perhaps explore CDC? Who knows...:/
Sidebar? Did they not read what they linked? Lol. XD
Explain "advanced SQL". 
What you've done here is fantastic, but, it's a DBA's worst nightmare. So many questions....Where exactly are you gathering this "alert" data from? Why couldn't you create a parameterized SPROC? How is 'adhoc' code generated? :/
I am surprised I haven't seen a WINDOW'd function query written yet. However, that is what I would suggest. With that said, having count(\*) is cool. 
Rebuilding an AG should result in the same exact copy on the other side because what gets shipped isn’t SQL operations like classic replication, but actual LSNs via the transaction log. If the DBA wasn’t completely incompetent, you may find that this is a simple matter of them not wanting to incur the traffic that happens from index maintenance. So, they purposely broke the AG, performed the maintenance, and then rebuilt it. I do not recommend that approach, but I don’t know your environment. In my experience, usually rebuilding stats daily is a good enough way to skirt massive index maintenance. Also, the Ola Hallegren scripts with ONLINE=ON for rebuilds over a certain fragmentation level works well too. We have massive DBs in AGs and never break the connection for maintenance and it works fine. We also alert when the log send or redo queues get over 100 GB or so because that’s a sign of trouble in our environment. YMMV.
You're creating a game solely out out of SQL? Please tell me you have a git. 
Absolutely not. They aren't always the answer (and I can hear people 'overhead this', 'overhead that'), but, they can their purposes. Best of luck m8.
This is what I do. Expiration_date datetime NULL
Define Data Analyst?
I don’t think there’s enough information for this post. Could you please elaborate a little more?
You'll want to look for jr analyst or reporting analyst positions. Generally, you'll need SQL, excel, and a visual builder (ssrs, tableau, powerbi). If you want to stand out, learn python or r
Giving career guidance is extremely difficult. For instance, Just because you are used to collecting data has no bearing on how well you can understand data in general. Do you read summaries of studies? Do you follow the metrics? Are you a glorified data entry analyst? Etc. IMO tutorials are the career killers of people as they don't truly learn, but, learn how to regurgitate what they are shown. My best recommendation is to work with a proper recruiter, let them know your background, and apply for junior level roles. A proper recruiter will have gobs of metrics, profiles, and assessments to help you a thousand times over than this subreddit. My best advice is to dig inside your mind and find what makes your brain tick as when you do, the money will always come. If you are chasing dollar signs, you will be another causality. Best of luck Muncher. 
I thought the point of a cute is that when it's called, you cant join to it again. This saves on virtual memory. 
I would say to just ask it. I didn't know r/ssis existed. Or if it's a pretty short question, feel free to PM me. I've been writing SSIS since 2005.
Damn the downvote is real, better just post on /r/ssis/ ! XD 
\^ Agreed. SSIS IMO is heavily coupled with SQL. 
RedGate makes a SQL Server tool like this called [SQL Search](https://www.red-gate.com/products/sql-development/sql-search/index) that does this. Additionally, from the Data Warehouse perspective there is [Azure Data Catalog](https://docs.microsoft.com/en-us/azure/data-catalog/)
I believe his means, someone who understands and can interpret data. While this is a correct definition of a data analyst, it's not the same as WORKING with data. Is that something Muncher can't learn, no. The question is flat out over simplified. Financial, Analytical (Operational), Performance data, blah blah ... I feel you. 
select \* from sys.colu...fuuuu; this...strong this \^
Ooh. Will give them a go
Still definitely worth learning the layout of the sys (resource database) in SQL Server for some dynamic query magic at times :)
I think there's enough crossover that you could get an answer for your question here.
@the_box_muncher, could you confirm or deny?
Oracle can also treat CTEs as inline views without materialization.
Agreed. OP could look into extended properties for table/column descriptions. However, I don't believe that solves his bigger problem. :/
Yup. /u/RONSOAK is ultimately dealing with some messy issues: &gt; and even has two competing datamarts First, it sounds like the Data Warehouse environment is actually an Independent Data Mart Architecture, which isn't really an architecture but an anti-pattern (anti-architecture?). &gt; Because my data warehouse is so poorly documented &gt; people only know how to look for data because they already know. People know what they know and don't know what they don't know. Unfortunately, whomever implemented /u/RONSOAK's architecture didn't really know what they were doing. The success of a Data Warehouse is founded on its ease-of-access and ease-of-understanding to not the technical units of the company but the business, non-technical units. Documentation is key, and it really is quite the puzzle to me as to how people implement Data Warehouses without knowing their company's data and keeping a detailed record of it. Unfortunately /u/RONSOAK, usually when presented with this predicament in a Data Architecture - the best and least expensive resource way (in the end) is taking a Bottom-Up approach and refactoring the entire environment. Starting with documenting your company's data, understanding what the core data is and setting up some Data Governance. 
Sure fire away
I have mine on a long delay and I just trigger it manually once I've started typing a column name
And that my friend, /u/RONSOAK , is your introduction to technical debt. XD 
Add std_code to your view and use that to drive the update
Thanks for the suggestions all! I will try some of these this weekend ! 
Most likely Birthdate isn’t being stored in the database as an INT and it’s a date column. I don’t know what flavor of SQL you’re using but you can do DATEPART(YYYY, BirthDate) = 1963. If that doesn’t work then just research a date part function in whichever database you’re using. 
Can you revoke permissions from your tables?
Well, complex subqueries..I guess 
I've built a tool in SharePoint with custom lists that exports the content of an admin table, allowing (specific) end user to edit, and set up a ssis package to run at regular intervals to update the actual database table. If SharePoint is available to you, it's an easy solution.
Not sure what DMBS you're using, but you can try something like SELECT * FROM Employees WHERE YEAR(BirthDate) = 1963 or perhaps SELECT * FROM Employees WHERE DATEPART(YEAR, BirthDate) = 1963
LOL, no.
Let er rip! We got you!
I am practicing using SQL SMS and the first one worked I apparently used the wrong order of where i put the YEAR function by having it reversed with BirthDate as I assumed someone used YEAR as a variable name &amp;#x200B; Thank you for the help and do you know where I can get more info on this besides just TutorialsPoint or W3C?
The guy below figured it out for me as it was &amp;#x200B; SELECT \* FROM Employees WHERE YEAR(BirthDate) = 1963 &amp;#x200B; and I just had the location of the YEAR and BirthDate variable mixed up as I misunderstood YEAR being a function and not just a name for a variable &amp;#x200B; Thank you for the help I appreciate it 
There's also /r/SQLServer that might be a good middle ground between here and /r/ssis.
It's not stored in any particular format, it's stored as a datetime and by default it displays in some format. Use the appropriate format function and parameters for your flavor of SQL to get the desired display format.
You probably want to make that field an ENUM. [https://dev.mysql.com/doc/refman/5.7/en/enum.html](https://dev.mysql.com/doc/refman/5.7/en/enum.html) &amp;#x200B;
last *working* backup
Awesome thanks @noesqL - couple things that help: - you could set up some stored prodedures db-side, but can be limiting when trying to expose broader teams to setting up their own; plus: sometimes it’s nicer to have some GUI/platform that handles the nitty gritty for you (there’s fierce debate on either side, and surely a place/time for both) - Revere specifically is read-only to client dbs - and alert conditions/queries sit in Revere and run on schedules you set in Revere’s platform I’m assuming by adhoc you’re meaning alert queries, and those are user-controlled and configured. Hope that helps, but super curious to hear what you think 
I guess when someone is putting the information in I want a drop-down box that only allows for "certificate" or "associates." This is a field for degree type.
[http://www.studybyyourself.com/seminar/sql/course/?lang=eng](http://www.studybyyourself.com/seminar/sql/course/?lang=eng). There you can learn basics for querying a database using SQL. The course, along with examples, is quite easy to follow. You can submit exercises as well. An **advanced** course is also available.
[http://www.studybyyourself.com/seminar/sql/course/?lang=eng](http://www.studybyyourself.com/seminar/sql/course/?lang=eng). You can do exercise on this resource. Well designed for beginners. For free. 
What's the question though 
One of the best online course. For beginners and and advanced students. For free. With examples and online exercises. [http://www.studybyyourself.com/seminar/sql/course/?lang=eng](http://www.studybyyourself.com/seminar/sql/course/?lang=eng).
Finally! Thank you for helping me learn. 
It what context? Are you using multiple tables as the cursor, or hitting multiple tables on each of the cursors loop? The second will work, not sure about the first.
It's the first one, I had an exam two days ago and solved it like that, and my friend used two cursors and we was talking about it who is the right one. The question was to take some data from table A and insert it into table B, so I was thinking about it since then.
Without knowing the schema this is a guess. But I think you could count on the special table, group by aid where aid = A. Wrap that in a sub query and then join in your other table 
Using a YEAR or DATEPART function will make your query non-sargable meaning that indexes cannot be used, this may be expensive on large tables. You should do something like: &amp;#x200B; SELECT * FROM Employees WHERE BirthDate BETWEEN '1963-01-01' AND '1963-12-01' &amp;#x200B;
&gt; =Substr([Product Remark Internal];Pos([Product Remark Internal];"%")-2;2) Bless you!!!! Solution Verified
It helps to rephrase the problem. You want to find projects that have no employees that don't specialize in 'A': SELECT * FROM Works W WHERE NOT EXISTS ( SELECT * FROM Specializes S WHERE W.eid = S.eid AND S.aid &lt;&gt; 'A' ) 
It helps to rephrase the problem. You want to find projects that have no employees that don't specialize in 'A': SELECT * FROM Works W WHERE NOT EXISTS ( SELECT * FROM Specializes S WHERE W.eid = S.eid AND S.aid &lt;&gt; 'A' ) AND EXISTS ( SELECT * FROM Specializes S WHERE W.eid = S.eid AND S.aid = 'A' )
Thank you for your reply. The problem is that employees can specialize in more than one area. So with this code, the employees who do specialize in A, but also specialize in B, are being discounted, right? Do you know how to fix?
Yes, you're right. To get it to work using my method you need a projects table, an employee to project linking table, and an employee to specialization linking table. SELECT * FROM Projects P WHERE NOT EXISTS ( SELECT * FROM EmployeeProject EP WHERE EP.ProjectId = P.ProjectId AND NOT EXISTS ( SELECT * FROM Specialization S WHERE S.eid = EP.eid AND S.aid = 'A' ) ) In Enlish we are returning all projects where there are no employees who don't have A as one of their specializations.
Got it. Thanks a lot! &amp;#x200B;
Don't think so. Although if it were small enough you could possibly cast it from a varbinary.
You can store a base64 string in a textual/string column, but not a numeric one.
 I would create an Id set as int primary key and auto Inc, then a separate field (text or blob) for the base64. 
I'm not able to change anything on the table.
gobbledoc gave me a possible solution by converting the Base64 encode string that I have on C# into byte\[\]. Not completely sure but I guess if I want to get the original primary key number I would have to convert that byte\[\] value stored in the column to string which would be the original Base64 encode string to then uncode and get the original number.
Not completely sure but I think you gave me a solution. By encoding the number 5 for example in Base64 and then converting the resultant string into byte\[\] gives me something like "822096640" which allows me to store in the column as you said. What I don't know is if I will be able to get this "822096640" number later, convert it back into Base64 and decode to get the original number 5. But I guess so, thank you!
Even if you do get it to work, you'll hit the int limit pretty quickly I imagine. But hopefully it gets you going in the right direction. It sucks having limitations like this. Another idea could be to create a separate table with an autoincrementing int PK and store the base64 string in it. Then you could store the int in your field and lookup the base64 string when required. It's late on a Friday night, but hopefully you get the jist of what I mean.
Generally speaking, if you're going to be storing a lot of ints that large (9-10+ digits) and not using them as actual ints, you should be storing those as strings, not as ints. Not sure about all SQL servers, but at least with MS SQL, strings ((N)VARCHARs) get stored in a separate place outside of the pages, and the page simply has a pointer to where the string can be found on disk. Large ints like that can quickly fill your pages and make table scans much slower. For example, we store all phone numbers as just the numbers(10 digits), and format them when displayed. We store those as VARCHARs for this exact reason, even though they're actually large ints.
The definition I have in my head of what one is someone who retrieves and organizes data and uses it to make conclusions 
take this with a grain of salt as I am not working in IT, but I would think even a junior DBA would need more experience than that and it would likely be why you're not getting calls.
 SELECT w1.eid , COUNT(w2.eid) AS num FROM works AS w1 LEFT OUTER JOIN works AS w2 ON w2.pid = w1.pid AND w2.eid &lt;&gt; w1.eid GROUP BY w1.eid 
To be fair, no where on my resume does it say it was in a Access. Just says I developed a system. I also had a smaller side project in which i did use SQL server so I am familiar with interfacing with SSMS. I have created tables, queries, tinkered with permissions, and have a good base understanding of the concepts of indexes and page reads. I do genuinely appreciate your comment but I feel comfortable stepping into a jr level SQL server management position. I would certainly need a few topics explained but I am more than capable of going from there. 
Do you know people in that type of role ? Maybe a bit of networking could go a long way either in directly finding a job or by getting some targeted advice. Best of luck !
Do you feel naturally inclined to look at DBA roles because it's what you want or because it's what you think your skills most align with? Have you looked at/expanded your search criteria to include any of the following? Are you even interested in any of the following? * Etl developer * Analyst roles * Data engineer/SQL developer * data warehouse developer I ask because it seems like you have experience with some architecture. All of those roles leverage that skill set, just in different ways.
Your experience creating that invoicing tool better aligns with what an IT analyst or developer does. I very rarely interface with the DBAs at my company. They seem to deal more with scheduling of concurrent requests, optimization, installing updates and patches, partitioning of space to specific programs, aka the stuff I personally find tedious and boring.
Do you have a LinkedIn profile? Check out and apply to some jobs on there. The HR filters won’t catch your resume before a person can see it. Also, have you looked at applying to junior data analyst roles? You’d still be writing queries and reports in these roles and they pay well once you have experience. A lot of fortune 100 companies have programs to hire students fresh out of school like Humana and there are several in government (since you’re in DC). Good luck and don’t get discouraged!
dm me your resume (don't need any personal info) and send some job postings you applied for. 
If your last backup is bad, what's that say?
Look into BI roles
It took me 90 days and 140 resumes to land my first jr dev role. The first is the hardest- smooth sailing after that. Hang in there! 
 \*\*\*\* Section Times in Video \*\*\*\* Download SQL Server 2019 [0:30](https://www.youtube.com/watch?v=Aff9jwSPRTU&amp;t=30s) Install SQL Server 2019 [2:11](https://www.youtube.com/watch?v=Aff9jwSPRTU&amp;t=131s) Download SSMS [13:45](https://www.youtube.com/watch?v=Aff9jwSPRTU&amp;t=825s) Install SQL Server Management Studio [14:19](https://www.youtube.com/watch?v=Aff9jwSPRTU&amp;t=859s) Configure SQL Server 2019 (Best Practices) [14:52](https://www.youtube.com/watch?v=Aff9jwSPRTU&amp;t=892s) 
Feeling comfortable and having the experience employers want are two different things. Nothing of your experience would make me hire you as a DBA, even at a Jr. level. I doubt many DBAs are hired out of college. It's something you grow into over time. Your best bet is to get into a related field and work your way up. 
I have to agree with the post above that your experience aligns better with a developer role. I took a similar route (MIS major, now working as a DBA), and based on what I learned in college versus my internship (which was on a DBA team specifically), the college stuff generally was more geared towards roles in the range of business analyst to bi developer. On top of that, most DBAs start in a field in the dev side before moving into the admin role, so if you really are pushing for a DBA role that may the your clearest path. Furthermore, if you enjoyed building a system and the parts that go into that kind of work, I would really consider the developer path, as a lot of DBA work is maintenance/troubleshooting and not so much building (automation skills are useful, though). Some find it rewarding, but others find it extremely boring, as you don't always get recognition from the business side for things, as you don't "generate income", so to speak. High stress, relatively low reward type of work at points. In terms of specializing, that's generally something that's going to happen naturally based on what job(s) you spend time in over the years. I would focus on fundamental skills that transfer well like data modelling, SQL writing, indexing, etc, as though skills will allow you a greater target for landing a job. I'd say your experience so far is a good start, just don't limit yourself to only DBA roles to start out, as getting into those roles is tough to do at a young age, as DBAs have a huge amount of power and managers generally are resistant to hiring younger, inexperienced people for those roles.
Full disclosure - I took a few classes through Pluralsight, but most of this is me trying to figure it out. I'm sure it's a simple fix, but I keep hitting roadblocks. 
I'm not trying to be mean, but I think you need to adjust your understanding of what a "massive" project is. $5 million really isn't something to throw out as an impressive figure. I'd focus more on your actions and not that dollar figure. That's a modest amount (not insignificant, so don't take offense!) to most medium-sized companies or any government organization. I'm not saying not to mention the amount. I'm just saying that when you do, remember that you might be competing against people who worked on projects where that was the monthly budget for the project. You're in the DC area, after all. Also, getting your first job is a lot harder than everyone makes it out to be. I don't want to discourage you, but when I finished my masters in 2008, I had to apply to about 300 jobs over the course of 6 months. I applied for jobs like it was my job. Now, 2008 was a really bad year for the economy, and I wasn't working in IT. However, my experience isn't unique. Getting your foot in the door is difficult. Please don't get discouraged if you don't hear back after 20 or 30 applications. It's not you. That's normal now. I know people with law degrees who worked in restaurants for years while job searching. Remember that the HR person reading your resume first probably doesn't have a technical background. They're looking for key terms. Sometimes the first pass of your resume isn't done with human eyes. Sometimes a computer sifts through the resumes first to identify the ones with all the key words. If a key term is listed in the job posting, include it in your resume. Broaden your horizons on the types of jobs you apply for. I'm not saying you wouldn't cut it as a Jr. DBA. I'm saying that you might have to slide into your dream job at an angle and not head-on. Look for related jobs. Look for non-IT jobs that have a focus on SQL (analyst jobs in the government maybe). No one really gets their dream job at first, and it's okay. It really is. No need to freak out. Also, try Indeed and USAJobs. Good luck!
It's rare for your first full time job to be a DBA position. Most entry level DBAs start as system admins or developers. Get a system admin position with a company that is fairly large but doesn't have a formalized DBA team. You'll get the opportunity to manage a system from front to back. Read up and familiarize yourself with every concept you'd need, then implement it. One excellent piece of advice if you want to be known as the "sql guy" is to ask if you can hold some kind of a lunch and learn session where you speak about the way you've set up your sql environment. If you're in a decent sized company and known as the "sql guy" well then guess who looks like the one to turn to when it's inevitably time to make a formalized DBA team? Yep, that would be you. (If they don't end up going that route, your resume will show all the skills required to manage a database system and you should be ready for entry level if you keep reading up) Also, go for some microsoft certs!
Better hurry up and fix your backup processes before you need to restore from backup
My point exactly
Try: CAST(n as numeric(36,2))
Try not looking for a database administrator but instead a different position that also uses SQL. Something in analytics will be huge with that business degree. Marketing analyst, shit like that, all needs SQL and business -- which is super rare. People often have one and not the other.
This, your experience and degree leans more to a Business Intelligence role for now. Get in, get more experience with the development tools and see if you are really interested in a DBA role. Find out what training and certifications are needed by taking to the DBAs there. 
i also have to say, that this is part of a long formula...with lots of ifs, and and or. &amp;#x200B; Basically i need to take, for this part, when variable 1=variable 2 or variable1-variable 2 has a variation of -+2%
Just coming back to this - still don't know why the discrepancy, but putting a non-clustered index on the rowID guid table had it's run time down to mere minutes. So I guess I won that round?
Thanks, I will try that shortly. Just wondering, why (36,2)? 
I am loading data into a flat file. The data is huge so I want to know if I can set up the package in a way that will load 1g or 2 into one file and break into another file until all the data is load. 
I am loading data into a flat file. The data is huge so I want to know if I can set up the package in a way that will load 1g or 2 into one file and break into another file until all the data is load. 
I am loading data into a flat file. The data is huge so I want to know if I can set up the package in a way that will load 1g or 2 into one file and break into another file until all the data is load. 
I am loading data into a flat file. The data is huge so I want to know if I can set up the package in a way that will load 1g or 2 into one file and break into another file until all the data is load. 
Agree 100%. 
Python + SQL are the core languages for Machine Learning.
I’m not 100% sure, but I think this would still return NULL if as the num if there wasn’t a w2.eid. Might have to use coalesce around the count 
Why python in particular for machine learning? Why not C#? Thank you for the answer! I've been learning Python in my free time for fun. I'm a SQL person for work. I've had comments saying that learning python along with SQL is a really good idea.
Python has better libraries with better models.
Good question, what I have done is use an identity column from my table, then implemented a conditional split to create roughly the same file size for Excel spreadsheets. 
Python is considered easy to learn and it's pretty powerful due to there being plenty of libraries available. However, if you're in a Windows environment and running SQL Server, I'd argue PowerShell is going to be a much better language to learn.
Yes of course you can do that. Are you pulling the data from a database? The easiest way would be to make the target files static (file1.csv, file2.csv, etc), and use your source query to select out a specific subset. You could use the NTILE function to quartiles for example, and write to four files.
Good stuff.
1. It is hard to get a job as a junior employee. 2. Your project and the role of a DBA do not have a lot in common. 3. Some employers want experience in their specific version database technology and undervalue the importance of theory and core concepts. 4. 20-30 positions is not a lot if you're resume is not strong.
FYI, `COUNT` actually does return 0, as opposed to other aggregate functions which might return NULL, such as `SUM` you could, like, test it for yourself ;o)
What was your attempt that didn't work?
 WHERE variable1 &gt;= -0.98 * variable2 AND variable1 &lt;= 0.98 * variable2
Most recruiters have Python and SQL has their first keywords when searching for talent.
Tried this: SELECT CAST(AVG(CAST(datediff(DAY,c.created,c.updated) AS numeric (36,2))) AS numeric(36,2)) AverageTimeToOnboard, c.PROJECT_NAME AverageTimeToOnboard Got this: Column 'dbo.Data' is invalid in the select list because it is not contained in either an aggregate function or the GROUP BY clause. 
Burn!!!! My bad. You’re right. Was on my cellular device 
You need to look into GROUP BY. Because you're using an aggregate (average) you need to group the rest of your data.
&gt; Why not C#? To answer this question specifically, there's a couple reasons. C# used to be a Microsoft ecosystem language mostly due to .net framework. There is of course Mono, but it's an open source re-implementation with its own flaws. Now there's .net core too, so you can actually use C# on POSIX somehow responsibly, but at this stage it's just playing catch-up with others. Also, most of ML (and a lot others applications) is done in the cloud or on distributed farms, and Linux won the cloud war long, long time ago. There's lots and lots of libraries and other supporting tools created out there for other languages, Python especially. Also, Python is easier to pick up and start writing code. You have inline interpreters, you can write code without much cruft. It's generally very user-friendly. C# is a great language overall, but not as straightforward.
I have been researching, but can't seem to figure it out. I have a design walkthrough on Monday, hopefully I can get some clarification then. Thanks for your help on this!
AVG is an aggregate function so you need to group by any other field you are selecting.
Is this from a sql developer perspective, dba or both? I'm a dba and sadly never was a developer of any kind. Made the switch from Linux Systems Administrator to DBA and can manage the shit out of a database and query it but asking me to do something in Python is going to be a minute or two... Curious from what perspective this concept is expected from employers or potential hiring firms.
What was your logic in the conditional split? I don’t quite understand. 
You can use a proprietary file type that may be faster. Also, you can or should be able to use bulk export and import. Is it speed, processing power, or database usage that's the problem for just having a big file? A common ETL method is to only grab changed data between extracts. 
The files cannot be static because I don’t know off hand how many I’ll need. 
No, it’s not speed or processing power that’s an issue. The issue is the end users. Whoever is picking up the file needs to be able to open it on their machine. They were complaining that the files were too large. 
I'd go through the T-SQL documentation. [https://docs.microsoft.com/en-us/sql/t-sql/language-reference?view=sql-server-2017](https://docs.microsoft.com/en-us/sql/t-sql/language-reference?view=sql-server-2017) I learned about more syntax and functions through the documentation then I've ever learned from a book or website.
What are they trying to accomplish? What tools are they using? If you're using gigs of data, they should have the data loaded into a database system that has the processing power to do the work behind the scenes while they use the tool of their choice to connect to the database. Processing gigs of raw files without a database housing them is significantly less performant and a huge pain. 
Depends on the job your looking for, but in general python + R + SQL are all reasonably cheap and easy to learn and for the cornerstone of data science roles. After work experience you'll probably gravitate to one and get more focused experience that will drive career development. I'd argue putting tableau or powerbi in that bucket, as data visualization experience. Also cheap and easy to learn. As much as I hate it data analytics, data science, and data visualization are becoming huge buzzwords (at least in my area). So demand is high...understanding is low in my experience Source: my career path
I am actually using this WHERE clause. How can i put two of these one for SSN and one for FED ID fed\_id like '\_\_-\_\_\_\_\_\_\_';
PowerShell is a commandline/scripting language. Python is a programming language. The two are not even comparable.
i tried that, but Business objects is telling that there is an error on "="...why?
Sure, so I had a column in my table, named [ID] and so I used that column in my conditional split. If my [ID] column was less than 1,000,000, then the record would into my first spreadsheet, between 1,000,001 and 2,000,000 then second spreadsheet. And so on and so on. I had roughly 10 million records. Then end result was that I created spreadsheet roughly the same size. Make sense? 
\&gt; Why python in particular for machine learning? Why not C#? The simplest answer is that no one creates machine learning models in C#. Because no one uses C# for machine learning, there are not packages ready to use in C#. No one would ever pay an employee to hand-code a neural network in C# vs. using a ready-made Python library. &amp;#x200B; \&gt; with SQL have over learning any other OO Language Also remember that Python isn't really an OO lanague. It can do OO, but it's not that. Python is heavily used not in just machine learning but all sorts of data analysis. One advantage that it has is that it is interactive. In that regard, it works a lot like SQL does.
So I've been stalking all your replies for the last hour learning a ton and this comment really jumped out to me. I've recently stepped into a sort of hybrid role of DBA/Analyst/Data Engineer in a major manufacturing company and I'm currently struggling with rationalizing this issue on a daily basis. Do you have any thoughts on what a good reporting environment should look like?
Others have answered, but I just wanted to chime in as someone that uses both. The simple answer is that if I'm working with data, I have to write more/more complex code in C# if I wanted to accomplish the same task as Python. C# is a great language, like Java, for enterprise or app dev, but Python is better suited to data wrangling. Way less code, typically.
I'm personally a SQL Developer at the moment. Looking to be a DBA in the long term. I'm 23 and I was riding the SQL support desk for 1.5 years. Spent the last 6 months as a developer. I've been learning Python for fun in my personal time and I've mentioned it to other programmer friends and recruiters and they've all been very encouraging 
You didn't happen to aggregate all those resources did you?
C# wasn't designed around data handling. It does that in a clunky way. Plus you need to compile it to run it, unlike Python, where statements can run one after the other with no problem. Python makes it stupidly easy, so you can focus on your problem. And Python is really fast with bulk operations. Also performance. A python algorithm that uses a lot of loops and some floating point arithmetic is around 6 times slower than Go. Go in the same setting is around 2 times slower than C. D is slightly faster. Rust is the fastest. But as soon as you express your arithmetic and loops as vectorized operations with numpy, and wrap it all with `@numba.jit(nopython=True, cache=True)`, you get extremely hard to write and understand code, that is as fast as Go. With that, you can actually write performance-critical parts in such wrappers, so you can still run Python, gain from all of its glory and ease, and still have the performance of the fastest programs....
perhaps "da manual" might help... did you understand what i meant?
Depends on what you define as "huge view", but I haven't seen this used yet. Maybe not good enough to justify effort put into creation and maintenance of such documentation. Usually you can get and idea about objects just from reading names, fkey constraints and a bit of existing code.
Yes, that makes a lot of sense. Thank you so much. 
No problem, I hope it helps. 
"There's an old saying... 'Python is the second-best language for everything'" &amp;#x200B;
You can use a CASE statement, depending on your actual conditions... having a table1 / table2 of: &amp;#x200B; |col1| |:-| |a| |b| |c| |d| &amp;#x200B; |col2| |:-| |a| |b| |c| |f| || &amp;#x200B; You can do this: SELECT * FROM Table1 t1 INNER JOIN Table2 t2 ON CASE WHEN t1.col1 = t2.col2 THEN 1 WHEN t1.col1 = 'd' AND t2.col2 = 'f' THEN 1 ELSE 0 END &amp;#x200B;
JOIN ON (A = B AND B &lt;&gt; 5 OR B = 5 AND A = 4)
Sure, just convert Base64 to decimal.
&gt; C# is a great language overall, but not as straightforward. Yup. I know Python and am currently finishing up a C# book. I like C# *way more* than Python. But it was significantly harder to learn.
PowerShell is also on Linux now
You'd need an " = 1" or " = 0" after the END clause of your CASE statement, so that a join evaluation occurs.
C# is my favorite language by far, and I'm a Linux fan and user. I shrieked like a little girl when they announced .net core.
Seems like you need a View. You can bring in the columns you need from the 2 tables, and then make a few computed columns on that view. Super easy.
I'd like to add that with 2017s added R, Python, and better JSON support... and 2019s huge polybase improvement... the demand is just going to grow.
You could try something simple like: Select T1.ColA ,T1.ColB ,T1.ColC ,T2.ColA ,T2.ColD ,T2.ColE -- update T1 set T1.ColB = T2.ColD, T1.ColC = T2.ColE From Table1 T1 join Table2 T2 on T1.ColA = T2.ColA Where Clause here run the SELECT when you see the results you want updated, then run from the UPDATE
What does -- do?
just join the tables and create a new table or a view. create view testView as select p.colA, p.colB, p.colC, s.colD, s.colE from primary p join secondary s on p.colA=s.colA &amp;#x200B;
Can I replace "view" with "table"?
It's mine too. I think Anders Hejlsberg is a great language designer. I'm not surprised that typescript is being embraced more and more every year considering that he helped develop it. The biggest criticism of C# was that you were stuck on the Microsoft platform. I'm glad that criticism is going away and that they're embracing open standards.
Any one of these SHOULD work: Where field1 between field2 and field3 Where field1 &gt;= field2 and field1 &lt;= field3 Where field1 &gt; field2 and field1 &lt; field3 Where field1 &gt; field2 and field1 &lt; field3 Are you sure the fields are numbers? is it possible that one or both fields may not be numbers? &amp;#x200B;
I think its used for huge companies data analytics department. A lot easier to edit views than digging through hundreds of sql code
My main expertise is SQL and databases are my medium of choice when working with data. But when data doesn't live in your working database, Python can be your best friend. Need to pull a CSV from somewhere, clean it up, and load it into your database? Maybe you need to take data from a NoSQL database and groom it to be loaded into your SQL-based data warehouse. Or perhaps you want to automate the creation and delivery of a report you create from SQL query results. All of those scenarios are easily handled by Python and a few powerful libraries like like pandas and SQLAlchemy. 
So what I would recommend you do is try to follow the agile / scrum development cycle. What this means is to start off with the bare-bones of what you need to get started and build it, then "release" it and improve on it after. So... build a simple database of Pokemon and their names: &amp;#x200B; Table: Pokemon |ID|Name| |:-|:-| |1|Bulbasaur| |2|Ivysaur| |3|Venusaur| |4|Charmander| &amp;#x200B; Then once you're done with that... figure out what to add to it next, then work on adding it. &amp;#x200B; I would recommend against an iOS app... partly because I'm not familiar with them, but it might be more hassle than it's worth due to iOS developer fees, copyright... unless there's some way to sideload an app like you can on Android? Otherwise, it would be easier to host a simple web app on AWS or some other cloud platform.
Whenever I play Pokemon I need 3 save spots, one for my Charmander, one for my Squirtle, and one for my second Charmander.
Your username is completely apt in this case. I intentionally went to #4 in the table :)
Not to hijack this, but do you have links for material to go the other way? Best data science intro for SQL developers/admins? &amp;#x200B;
For data science, you can pull from your databases with SQL and manipulate and visualize the data with Python. 
Ahh, good point. I only thought of making it an iOS app because she has an iPhone *shudder*. She has a tablet that runs an older version of Android as well as Windows 10, so maybe I'll take that route. I really appreciate the advice. It helped me wrap my head around what I want to do!
You could split the file in half and generate a key that lets you join the halves together. This is not my wheel house, so I don't have any expert advice to give you.
SQL Training on Simplilearn may be... 
Thanks! The problem is that I have to go ahead and create an SSIS package for a couple hundred such files. I could probably go about splitting it if there were only a few.
I'm not an expert in SSIS but you may be able to "split" them in SSIS. Best of luck. I'm curious to see how this issue is solved.
Is the file comma delimited?
would the powershell command **Write-SqlTableData** offer a solution ? It can even create the base table for you with the right headers, but everything will be in nvarchar(255), you can later alter this to the proper length and datatypes. [https://docs.microsoft.com/en-us/powershell/module/sqlserver/write-sqltabledata?view=sqlserver-ps](https://docs.microsoft.com/en-us/powershell/module/sqlserver/write-sqltabledata?view=sqlserver-ps) &amp;#x200B; i have had trouble reading all sort of files with ssis and Powershell just offers a quick solution with that command.
So you need to something like this: select hourname, sum(minutesinstatus) as totminutesinstatus from ( select datepart(hour, fieldname) as 'hourname', datediff(mi, field1, field2) as 'minutesinstatus' from table ) x group by hourname
Thank you! The only issue though with this is with a status that lasts multiple hours. If I had a start time of 1200 and and end time of 1430 I would want to return these results: &amp;#x200B; Hour,MinutesInStatus 12,60 13,60 14,30
I agree completely with all that you said. I'm head of a dev-ops department and would recommend you look into a jr. developer or jr. technical BA. The job of a DBA is quite different than what you did and focuses more on the security, maintenance, performance, setup, DR, etc. If you go into a small company, it might be one position for both, but I doubt they'd be looking for a junior level person to fill that position. Some DBA knowledge is definitely helpful for query optimization and best practices if you need to install or troubleshoot an issue with an instance of SQL. If you can gain some DBA knowledge, it will be super helpful in your career. I'd recommend doing some research for companies around your area and check out their websites for jobs. LinkedIn is fine, but I've found you may have better luck at least getting a response if apply directly.
Python's a nice scripting language that can work as a nice glue for data processing pipelines. Let's say you want to produce a set of PDF reports every day based on data in your database. You can set up stored procedures and views to prepare the data, then you can use Python to take that data and generate the PDFs with charts, graphs, analysis, etc., and put them in a web folder for distribution.
So take the datepart(hour, field) and make it the end time, not the start time?
But then that wouldn't return the '13' in the example above :(
If you asks about /u/Yablonsky answer, it's a one line comment. That syntax allows you to run a select, and when it's ok you just highlight the "update" without the "--" and it will run the update on the same rows returned by the select.
If we wanna use the cursor, how it will be?
SO you need some case logic like CASE WHEN datepart(hh, fieldname) &lt; datepart(hh, fieldname2) then fieldname2, else fieldname end as blah
the -- comments out the line. If you highlight the script from after the -- through to the end of the script, it will only run what you have highlighted
Like... the _number_ of columns is capped at 255? or the _length_ of each column? There isn't really a column limit in SSIS... so that's weird... there was one in Excel 97/2003 ... but if you're using a flat file source it shouldn't have a limit
What do you mean the "record count is zero" -- you mean to say that no record appears, because no records appear for which you can calculate? For example, if you sold bananas, pears, apples, and cherries, and wanted to run a calculation across them, but you didn't sell any pears in November... it isn't that you are multiplying by zero and getting a zero result, you are literally not getting cherries in your calculation. If you wanted to include them you would need to do a LEFT JOIN and use something like ISNULL() to force the 0 to appear.
I still don't think that works, that would just return a 14. I am wanting to return the table above( with '12','13','14' )in it from this table. SELECT '12:00' StartTime ,'14:30' EndTime INTO #Temp &amp;#x200B;
So if something was created at noon, and lasted 6 hours, which "hour" would you want to appear? Or do you want every minute from 12 to appear in 12, every minute from 13 to appear in 13, etc.?
Yup, you got it! So if I had a start time of 1200 and an end time of 1800 I would want these results: StatusHour,Minutes 12,60 13,60 14,60 15,60 17,60 18,60
Im saying that when you times anything by zero you get zero aka 1 x 0 = 0. So on the list of records I am using a few of them have a discount of zero which means that if I do my formula above Quantity X UnitPrice X Discount then the ones that appear like so 1(QTY) X 2(UnitPrice) X 0(Discount) = 0 
Ok, so that is a very different, and much more interesting question. Let's say we have the following timestamps: | Start | End | | :--- | :--- | | 2019-01-01 12:17:00 | 2019-01-01 18:43:00 | You would want to see: | Hour | Minutes | | :--- | :--- | | 12 | 43 | | 1 | 60 | | 2 | 60 | | 3 | 60 | | 4 | 60 | | 5 | 60 | | 6 | 43 | Correct?
Oh, so something like CASE WHEN Discount = 0 then &lt;calculation&gt; else &lt;calculation&gt; * discount. Or just do something like Total - (Total * Discount).
Yes, exactly! I was thinking of having each hour as a separate column and counting the minutes for each column this way then just doing a pivot at the end. I would also need to include a 'WHEN' for partial hours. &amp;#x200B; ,CASE WHEN DATEPART(HOUR,StartTime)&lt;&gt;DATEPART(Hour,EndTime) AND 13 BETWEEN DATEPART(HOUR,STartTime) AND DATEPART(HOUR,EndTime) THEN 60 ELSE 0 END AS '13'
Ultimately, you need 1 row of data for each full hour between the two timestamps. I can think of how to do this with a LOOP but am trying to think of a way to do it without one, and I'm thinking there is a way.
/u/AbstractSQLEngineer, can this be done without a loop?
Ok, so this is what I have and I think something like this should work! It will return the hours as columns which I can then pivot. I have hours '13' and '14' as examples: &amp;#x200B; ,CASE WHEN DATEPART(HOUR,STartTime)&lt;&gt;DATEPART(Hour,EndTime) AND 13 BETWEEN DATEPART(HOUR,STartTime) AND DATEPART(HOUR,EndTime) AND 13&lt;&gt;DATEPART(HOUR,STartTime) AND 13&lt;&gt;DATEPART(Hour,EndTime) THEN 60 \-----------accounts for inbetween full hours---------------------\^ WHEN DATEPART(Hour,StartTime)=DATEPART(HOUR,EndTime) THEN DATEDIFF(MINUTE,STartTime,EndTime) \-----------accounts for times within the same hour------\^ WHEN DATEPART(HOUR,StartTime)&lt;&gt;DATEPART(HOUR,EndTime) AND DATEPART(HOUR,StartTime)=13 THEN (60-DATEPART(MINUTE,StartTime)) \--------accounts for start time minutes when start and end time are different hours--------\^ WHEN DATEPART(HOUR,StartTime)&lt;&gt;DATEPART(HOUR,EndTime) AND DATEPART(HOUR,EndTime)=13 THEN (DATEPART(MINUTE,EndTime)) ELSE 0 END AS '13' &amp;#x200B; ,CASE WHEN DATEPART(HOUR,STartTime)&lt;&gt;DATEPART(Hour,EndTime) AND 14 BETWEEN DATEPART(HOUR,STartTime) AND DATEPART(HOUR,EndTime) AND 14&lt;&gt;DATEPART(HOUR,STartTime) AND 14&lt;&gt;DATEPART(Hour,EndTime) THEN 60 \-----------accounts for inbetween full hours---------------------\^ WHEN DATEPART(Hour,StartTime)=DATEPART(HOUR,EndTime) THEN DATEDIFF(MINUTE,STartTime,EndTime) \-----------accounts for times within the same hour------\^ WHEN DATEPART(HOUR,StartTime)&lt;&gt;DATEPART(HOUR,EndTime) AND DATEPART(HOUR,StartTime)=14 THEN (60-DATEPART(MINUTE,StartTime)) \--------accounts for start time minutes when start and end time are different hours--------\^ WHEN DATEPART(HOUR,StartTime)&lt;&gt;DATEPART(HOUR,EndTime) AND DATEPART(HOUR,EndTime)=14 THEN (DATEPART(MINUTE,EndTime)) ELSE 0 END AS '14'
I asked someone here to chime in, at the moment I'm wondering how you might do this with a cross join. Something like datepart(hh, start), datepart(hh, end), datediff(mi, start, end) and then joining on the difference between those values in start &amp; end, and then dividing by the total datediff(mi) by the portion of the hour. So if hour 1 was .45 minutes, it would be 3/4 of an hour, or 60*.75. For whole hours it's always an easy 60. Then you would just group by and sum. I think you can do it without a LOOP but it's end of day on a Friday and I'm about to go on a 2000 mile road trip in the morning so I'm not going to be able to do much more for you. Interesting question.
If it works it isn't dumb. :)
I appreciate the help, it got me thinking! And I think I solved it, I am just not sure it is optimal! I have a wedding I am going to in 45 minutes so can't be super responsive. 
Ha! See my other comment. I asked someone from this forum to chime in that might have a more elegant way to approach this. I think a cross join is the first step without using a loop or a crazy case statement.
Yeah, I saw that, and I am sure you are right!! I know enough to do the work, now it is just a fun problem for me lol
If you add &gt;GROUP BY c.ISSUE\_KEY, c.PROJECT\_NAME at the end of your 2nd query, it should work. An aggregate function needs to know how to group the data you want to run it over. Here, as your key is ISSUE\_KEY you need to group on that and PROJECT\_NAME to include it in the output. &amp;#x200B; Basically, once you use an aggregation function, you need to use GROUP BY to tell it how to aggregate your data (not 100% true, as you can get around this with [windowed functions](https://stackoverflow.com/questions/2404565/sql-server-difference-between-partition-by-and-group-by), but let keeps this simple-ish). Here, the GROUP BY should tell the AVG() function over which group of rows the average should be computed.
ALTER TABLE &lt;tablename&gt; ADD CONSTRAINT &lt;constraintname&gt; CHECK (type = 'student' or type = 'faculty' and email like '%@umn.edu');
You could do a temp table with the 24 hours of a day and join to your data with if part of hour in the time range as part of the join. In the select you’d use datediff to get the no of minutes in your hr, prob with a case statement
I'm still a bit confused. Not sure what to do after std_code is added to the view. 
Sent you a PM, OP.
Thanks! I’ll give it a try!
If you can set up a view vw with: * std_code * avg_gr_t1 You can do: UPDATE grade g SET gr_t1 = (SELECT avg_gr_t1 FROM vw WHERE g.std_code = vw.std_code) WHERE g.std_code in (SELECT std_code FROM vw) ;
This may work, Thank you :)
Yea. Its called ROLLUP. If I'm reading this correctly
Yo. u/notasqlstar summoned me. Make a @table with records 0-23 (unless you have a time dimension) Join @hours hours on hours.value between datepart(hour startdate) and datepart(hour enddate) make sure your in 24 hour format. Then you do Case when hours.value = datepart(hour, start) Then datepart(minutes) --maybe some math -60 or whatever When 'same thing but end' Else 60 End Faster in a cursor. But it can be done outside of one. Faster if you have your time split out prior. Faster if you do a ton of things. 
Is excluding records which do not qualify for a discount an option? 
This.
How about: SELECT Distinct OrderID, UnitPrice * Quantity * case when isnull(Discount,0) = 0 then 1 else Discount end FROM Order_Details; &amp;#x200B;
Ran the query but it did not update to the average of gr_t1 for their major. It seems (I could be wrong ) that the above query does not reference the average for each major. create view vw as (select avg(gr_t1) avg_gr_t1 , std_code from student natural join grade group by maj_code, std_code) UPDATE grade g SET gr_t1 = ( SELECT avg_gr_t1 FROM vw WHERE g.std_code = vw.std_code) WHERE g.std_code in ( SELECT std_code FROM vw)
Thank you so much! I am at a wedding right now but will check this out tomorrow!!
Your view is wrong, you're taking the average of gr_t1 grouped by std_code and maj_code, which will just give you the grade per student. You need to self join grade to expose the relationship between std_code and average(gr_t1). Or use a windowed average.
I ran into a similar problem with an export from our hosted expense vendor that has 375 columns. The issue wasn't with the number of columns, but the size of the table I was trying to import to exceeded the maximum allowed by SQL when designing the table. My work around was I identified columns in the sport that would always have no data or data I wouldn't ever need and set those columns to be a single character or ignored (some columns were yes/no or 1/0 so it was helpful to have the first character). In the SSIS package I allowed truncation on those columns so it wouldn't cause the package to fail when it ran into data (you could also create a substring to just grab the first character). We're in the process of building out web services to get around this, but this has worked for about 3 months with no issues. 
Without having the data you have, perhaps looking into a MERGE would work as well. 
Where are the answers for coursera? 
I would probably do something like your suggestion - because mathematically I don't need to think too much about math, but your second option looks far more elegant. 
here you could work on a practical example :P [https://www.reddit.com/r/SQL/comments/awdmp4/recursive\_cte\_for\_bill\_of\_materials/](https://www.reddit.com/r/SQL/comments/awdmp4/recursive_cte_for_bill_of_materials/)
I haven't messed around with recursive CTEs much recently, but it looks like you're getting close. &amp;#x200B; I always like to think of the first portion of the query being the base case which generates the initial set of rows returned. After that, the next portion of the query attempts to join from the initial set of rows until nothing is left. &amp;#x200B; Here's a quick example I whipped up: drop table if exists #test create table #test ( SurrogateKey bigint, ForeignKey bigint ) insert into #test values ( 1, 2 ), ( 2, 3 ), ( 3, 4 ), ( 4, 5 ), ( 5, 6 ), ( 6, 7 ) ;With traversal as ( SELECT #test.SurrogateKey OriginKey, ForeignKey FROM #test WHERE SurrogateKey = 1 -- this first portion of the query generates the beginning set of records. UNION ALL SELECT traversal.OriginKey, #test.ForeignKey FROM #test INNER JOIN traversal ON #test.SurrogateKey = traversal.ForeignKey -- we join back to the result set generated in the previous iteration of the recursion until no more nodes to travel to ) select * from traversal Your query looks very similar and I think you just need to take a closer look at join relationship here: " INNER JOIN BOM ON BOM.BillNo = FG.ComponentItemCode" and make sure it's going in the right direction. I think you actually want to go from FG.BillNo = BOM.ComonentItemCode(but maybe I"m misunderstanding the relationship in your table/columns). The other thing you need to do is rework the select list in the second part of the union. UNION ALL SELECT BOM.BillNo , BOM.ComponentItemCode , BOM.QuantityPerBill , BOM.UnitOfMeasure FROM dbo.BillOfMat FG INNER JOIN BOM ON BOM.BillNo = FG.ComponentItemCode You continue to select from the preceding result set when you access the BOM table. Notice my example where I select from the #test table, rather than from traversal. This is because you need to continue iterating through the results, right now you'll just keep returning the same thing and not get anywhere. I'm guessing you might be hitting you max recursion depth since it will never terminate. &amp;#x200B; &amp;#x200B; 
Hey, I don't know how big your tables are, but if they're small, I've always found the SQL Server 2017 Import and Export data(or whatever version you're on) application to be quick and easy for one off tasks. It's easy to create flat files and it also options for other types of exports. It might even be installed already, based on how you installed SQL server. &amp;#x200B; If you will be continually updating the tables then looking into a more robust ETL solution might be better though.
Python is a scripting language.
Python let's you do stuff with the data once you've obtained what you need.
You should be able to use [Azure Data Studio](https://docs.microsoft.com/en-us/sql/azure-data-studio/download?view=sql-server-2017). Which is free and cross platform. It doesn't have nearly as many features as SSMS though. I also hear good things about [DB Beave](https://dbeaver.io/)
&gt; It doesn't have nearly as many features as SSMS though. For someone just getting started though, it should be plenty. Most of what's missing is features that DBAs need, and the team is pushing releases out monthly. Also, Microsoft _has_ released command line tooling. https://docs.microsoft.com/en-us/sql/linux/sql-server-linux-setup-tools?view=sql-server-2017
Great stuff! Though I'd say that most courses I've seen that say "Master this topic" are actually geared towards beginners. I'd like to see more good advanced SQL courses.
I've always had decent success by simply searching using apt. If you want something like SSMS, "Azure Data Studio" runs on Linux (maybe not all distros) and macos as well as Windows but it's in early development. The "Azure" part of the name is just marketing, it will connect to any Microsoft SQL Server. ADS is about as far away from a computer line client as it gets, though. Microsoft provides [this](https://docs.microsoft.com/en-us/sql/linux/sql-server-linux-setup-tools?view=sql-server-2017) I haven't fiddled with that, so I can't do anything other than suggest the link. There used to be a thing called freetds that would let you connect Linux programs to SQL Server. It's just a library. I don't think it went away, but there might be something better by now. If I were in your shoes, I would be looking at python or powershell. Bash seems antiquated (I wouldn't use the Windows DOS shell for anything) and the workflow for c++ (edit, compile, test, repeat) doesn't seem great for trying to figure out how databases work.
From experience, don't go too overboard with the SQL. Just use enough to get your data. If you need to do more advanced data munging use python or r. Some things are hard to do in SQL and take tons of code. Where in python it might take two lines of code. 
I have addressed your first question in my comment
As others have mentioned, Azure Data Studio works fine for querying using UI. If you really want to go command line, sqlcmd works with Windows. You can try SQL Server and the command line tools tools in [Docker](https://hub.docker.com/_/microsoft-mssql-server) first without needing to setup anything on your PC. Apart from the address and port, you'll also need credentials to connect.
You could setup an odbc driver on linux and use python for your queries. There is also a tool called sqlcmd, but I haven't tried this myself.
Thanks for the suggestions! It appears that flat files allow SSIS (or the SQL Import Wizard) to import more than 255 columns. The limit is for Excel files. The reason my flat file was limiting the number of columns in SSIS seems to be the fact that it was originally an Excel file saved as a flat file, and not a flat file originally. I'm unsure about the exact logic behind it, but it seems to have worked for now.
These are solid intros. The Duke SQL class on Coursera is even more extensive then that 1st one at UC Davis. And then id do the Udemy one that is based around PostgreSQL. Right now im learning about windows which is far beyond and more important anything iv learned in any other class.
This is very useful information. I am currently a DB Developer with almost 4 years of experience. I was looking into something else I could learn and pair with SQL. So far, I've worked with C# and I am.not a big fan. If I were to look for another job, would you say SQL + Python would do? Or should I add something else into the mix before I start looking?
 SELECT CASE WHEN AP.MODEL IN ('CAT1','CAT2','CAT3') THEN 'VENDOR1' WHEN AP.MODEL IN ('CAT4') THEN 'VENDOR2' END AS VENDOR, Count(Table1.Devices), Server_STATUS FROM Table1 LEFT JOIN Table2 ON Table2.key = Table1.key GROUP BY VENDOR, Server_STATUS; 
I'm assuming you're using MSSQL. Since you're using a CASE statement to determine the vendor, you could put it in a sub-query: select main.vendor ,main.server_status ,count(main.devices) as total from ( select Table1.Devices ,server_status case when ap.model in ('Cat1', 'Cat2', 'Cat3') then 'vendor 1' when ap.model in ('cat4') then 'vendor 2' end as [vendor] from table1 left join table2 on table2.key = table1.key ) as main group by main.vendor ,main.server_status 
the problem with that Vendor1 61 Vendor1 711 Vendor1 59760 Vendor2 32737 I need server status, but I need the Server\_STATUS as well, I need the Healthy, Down and Lost combined together?
Try his 2nd query. You should not be seeing multiple rows for Vendor1, if you are grouping only by Vendor
This make sense let me try it
fixed few bugs in the code i shared in my comment
This looks right. The other answers incorrectly didn’t either 1) Use a nested statement so you didn’t need to put the case statement in the group by 2) Put the case statement in the group by... garbage form as you’re doing the calculation twice. 
It omits a column the OP said was needed.
Not really as it requires me to have it and the next part I need to add the sum of all the purchases 
Something along these lines should be doable, no? Avoids the @table. I don't have time to play with the cross join but I'm fairly confident it would work out... I think. | id | act_start | act_end | | :--- | :--- | :--- | | xyz | 12:15:00 | 18:45:00 | with cte as ( select id , start , end , datepart(hh, act_start) as hr_start , datepart(hh, act_end) as hr_end , datediff(mi, act_start, act_end) as tot_time from table where id = xyz ), | id | hr_start | hr_end | tot_time | min1 | min2 | | :--- | :--- | :--- | :--- | :--- | :--- | | xyz | 12 | 18 | 390 | 15 | 45 | cartesian as ( select * from cte cross join n? on hr_start = hr_end + 1 ), | id | act_start | act_end | hr_start | hr_end | tot_time | min1 | min2 | rn_asc | rn_desc | | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | | xyz | 12:15:00 | 18:45:00 | 12 | 13 | 390 | 15 | 45 | 1 | 7 | | xyz | 12:15:00 | 18:45:00 | 13 | 14 | 390 | 15 | 45 | 2 | 6 | | xyz | 12:15:00 | 18:45:00 | 14 | 15 | 390 | 15 | 45 | 3 | 5 | | xyz | 12:15:00 | 18:45:00 | 15 | 16 | 390 | 15 | 45 | 4 | 4 | | xyz | 12:15:00 | 18:45:00 | 16 | 17 | 390 | 15 | 45 | 5 | 3 | | xyz | 12:15:00 | 18:45:00 | 17 | 18 | 390 | 15 | 45 | 6 | 2 | | xyz | 12:15:00 | 18:45:00 | 18 | 18 | 390 | 15 | 45 | 7 | 1 | select x.hr_start, sum(time_bucket) from ( select x.hr_start, 60 as time_bucket from cartesian where rn_asc &lt;&gt; 1 and rn_desc &lt;&gt; 1 union all select x.hr_start, min1 from cartesian where rn_asc = 1 union all select x.hr_start, min2 from cartesian z wherern_desc = 1 ) n group by x.hr_start 
https://www.reddit.com/r/SQL/comments/aw9pvl/query_help_optimal_way_to_do_this/ehn67l8/
It's telling you explictly what is happening, " MySql.Data.MySqlClient.MySqlException (0x80004005): Unable to connect to any of the specified MySQL hosts. " It can't connect to the sql db. Verify that the server is running and that your connection string is correct. 
What’s the difference between this and using `sp_whoisactive`? I’ve pretty much never needed a query like this again after using that.
Wow thanks. I'll try it in the next day or so!
Sp_who2 and sp_who3 has all I need to really know if I'm having issues
&gt; I need server status, but I need the Server_STATUS as well, I need the Healthy, Down and Lost combined together? So you want the Healthy, Down, and Lost values summed together as a single status, and all other status values will be summed individually?
OP's explanation of what he needs is lacking.
remove the GROUP BY PEINTRE At the moment you're asking it to give you the count of rows for each painter. 
You need to have the field PEINTRE listed alongside the count?
That may not work if he wants to display the PIENTRE in the details of the results though. In that case, he’d need to remove the group by and add to the COUNT(*) OVER(PARTITION BY PEINTRE) AS “COUNT_PEINTRE”
.... Fee * Qty - ((Fee * Qty * (1+Dsc) - (Fee * Qty)) 20 * 10 - ((20 * 10 * (1+.10) - (20 * 10)) =
Is this a typo and you meant to use SELECT? I don't even know what you meant by this query.
That seems to be a bug in HeidiSQL, you'll probably get a better answer asking on their forum
It's not a typo. Moreover, the query works correctly in cpanel. I'm just trying to understand if HeidiSQL requires different formatting.
I hadn't considered that. I just assumed it was a formatting quirk. Thanks for the suggestion.
OK, I've read MySQL docs. This appears to be MyISAM weird syntax. AFAIK no other major RDBMS uses this, so you will have to look up standard syntax and rewrite it.
Nope, gives the same result :(
No, I’m asking do you need to display both the count of the painters and the painter ID?
Why would you expect it to return 2 | 9? I think I must be missing something...
Oh whoops replied to the wrong person. &amp;#x200B; I translated my request wrong (French --&gt; English). PEINTRE is meant to say PAINTER. I edited the post.
That’s what i thought. 
That’s what i thought.
I translated my request wrong. PEINTRE is meant to be PAINTER. I edited my post.
Oh right. No I only need the number of painters. Not their name or ID.
Oh, okay, cool. Then check out doing what the other comment said and take the GROUP BY out. That’s what’s throwing it off.
Are you serious. I was convinced I tried that before. Wow do I feel dumb. It worked. &amp;#x200B; Thank you so much though. Very appreciated!
Yeah, you’re not concerned about grouping by painter ID. You just want to know the count of all painters. I know it seems counter-intuitive, but only use group by when you know you want to “break the results up” by the thing you put in the group by.
It worked! Thank you so much. Now, I know I need to do a self join. I will try to solve it without a view and just use joins. The fundamental part of the solution is to know I have to use a self join. I *think* that was the missing piece.
I don't use this query often, so I'll likely just stick with phpmyadmin when I use it--since it works fine there. I was just wondering if it was something simple--like HeidiSQL requiring " instead of ' or something basic that I just didn't know. In that regard, I appreciate the context. Thanks for following up. &amp;#x200B;
Yeah, it's my first time dealing with VIEWs and my first reflex is to put a GROUP BY whenever I'm using an aggregation function. Thank you for your help.
Look up ...OVER(PARTITION BY... also. It may help you right better analytical queries.
It looks like line number, starting from 2 (maybe 1 = header), so 2 = first row.
No, you would have to do some heavy rewrite to that query. MySQL apparently supports multiple DELETE even in InnoDB tables, but that is still not standard and other RDBMS won't allow it. You would need to use 3 CTE to delete from 3 tables in one query.
Hmm yeah, that would make sense. I thought it was supposed to be an id or something, but now I look at the query it only selects one field
Let’s talk through this. I take that the grid you’ve provide is your example data. Can you mock up an example of how you want the data to be shown?
Could you give an example of the raw data that you have now, and what you want the raw data to look like when you query it?
Did this get you what you need?
Windowed functions can be an even easier way to do this, but it would depend which DBMS and version you're using
Oracle - sql plus and sql cl I wish I had 🌟 star to give to you. 
Then, what you want is avg(gr_t1) over (partition by maj_code) - won't need any self joins, etc
Professor prohibits us from using anything else. Must use self joins , but I will definitively keep it in my personal knowledge base. 
It's not the field name, it's the group by. At the moment you're asking for the count of rows *for each painter*. If you want the total count of rows, you will have to remove that Group By.
I will keep this list updated.. 
I see. Is there a reason why the discount field is left NULL instead of zero? Perhaps the addition of a constraint on discount of 0 may work as well. 
I would suggest you check out the access rights of the user you are using for Heidi. Typically phpmyadmin will act in a mostly root capacity where your remote user for Heidi may not have delete rights
Just remove the entire group by, or else it will return the counts grouped like you are currently seeing. 
Thanks for the reply. u/distraughthoughts helped me out. It's solved.
Thanks for the reply. u/distraughthoughts helped me out. It's solved.
Could you show an example of what your query is doing and what you want it to do?
I'm getting the error 3504: Selected non-aggreate values must be part of the associated group, what I would like to start with is just an output at a individual row level by boking (TP\_ID) of all the select variables 
No, I mean show me what you want to do. You’re doing a correlated subquery and it may just be better to handle it another way.
It’s probably in your subquery bc theres no group by or aggregate functions in your main query. Every time you use GROUP BY every single column has to be either listed on the GROUP BY clause at the end OR be part of an aggregate function (MAX(), CONCAT(), etc). In your subquery make sure that each column is either listed at the end or encompassed by an aggregate function. Or just get rid of the GROUP BY clause at the end if you’re not actually aggregating anything. 
ah right, this is where I have an issue - i previously had a more basic query written which a friend altered for me as i was getting a spool error - so they added in the subquery so I don't quite understand how that works. &amp;#x200B; Orginally I had just written a sel from the BKG table then inner joined all of them. All i want is a table created with all those variables from linked tables. &amp;#x200B; This was my original one: **select** b.TP\_ID,b.BKG\_CREA\_UTC\_DATE , r.REMARK\_TYPE\_CODE, a.SME\_ID, M.DOC\_FARE\_AMT, S.MKTG\_FLT\_NO **from** PQMF.QMU0101\_BKG\_REF B **JOIN** PQMF.QMU0107\_TP\_REMARK R **ON** B.TP\_ID = R.TP\_ID **JOIN** PQMF.QMU0110\_TP\_PAX A **on** R.TP\_ID = A.TP\_ID **JOIN** PQMF.QMU0121\_TP\_AIR\_SEG F **ON** A.TP\_ID = F.TP\_ID **JOIN** PQMF.QMU0174\_DOC\_FARE M **ON** F.TP\_ID = M.TP\_ID **JOIN** PQMF.QMU0120\_dated\_air\_Seg S **ON** F.DATED\_AIR\_SEG\_ID = S.DATED\_AIR\_SEG\_ID **where** b.BKG\_CREA\_UTC\_DATE BETWEEN '2019/01/01' **and** '2019/01/31' **and** r.REMARK\_TYPE\_CODE = 'RX' **and** S.MKTG\_FLT\_NO &gt;'9090' **AND** a.SME\_ID &gt; '1' &amp;#x200B; Since then I've had to add two variables from the remarks table: REMARK\_ID, REMARK\_TEXT
I mean, if you could draw the output that you’re getting now versus what you want. In, like, CSV format.
ok, that works by getting rid of the group instruction - however I've run out of spool space then - is there a way to simplify the query so it will fit within my spool space? (We do have a large limit - it's the case of the remarks table being HUGE!)
I can't get an output with the error though.. 
Hmmm it shouldn’t be that bad, but you should get rid of the the subquery too. Just join directly to the table in the subquery and use the stuff in the WHERE clause in the ON clause instead
Still saying no spool, I've updated it to: **select** b.TP\_ID,b.BKG\_CREA\_UTC\_DATE , r.REMARK\_TYPE\_CODE, a.SME\_ID, r.REMARK\_ID, r.REMARK\_TEXT, F.OND\_DEST\_CITY\_CODE, M.DOC\_FARE\_AMT, S.MKTG\_FLT\_NO **from** PQMF.QMU0101\_BKG\_REF B **JOIN** PQMF.QMU0107\_TP\_REMARK R **ON** B.TP\_ID = R.TP\_ID **JOIN** PQMF.QMU0110\_TP\_PAX A **on** R.TP\_ID = A.TP\_ID **JOIN** PQMF.QMU0121\_TP\_AIR\_SEG F **ON** A.TP\_ID = F.TP\_ID **JOIN** PQMF.QMU0174\_DOC\_FARE M **ON** F.TP\_ID = M.TP\_ID **JOIN** PQMF.QMU0120\_dated\_air\_Seg S **ON** F.DATED\_AIR\_SEG\_ID = S.DATED\_AIR\_SEG\_ID **where** b.BKG\_CREA\_UTC\_DATE BETWEEN '2019/01/01' **and** '2019/01/31' **and** r.REMARK\_TYPE\_CODE = 'RM' **and** S.MKTG\_FLT\_NO **IN** ('9092','9094') **AND** a.SME\_ID &gt; '1' &amp;#x200B;
He is getting no output. The query does not work, it errors. 
It's because you dont have all of the columns in the sub query listed in the group by in that subquery. You are selecting 3 columns, type code, I'd, and text, but are only grouping by type code. 
What’s the exact error? Doesn’t sound like an issue with the query you have but with what you’re running it on/your database settings. I don’t have an answer for this one might be worth submitting a new post
Okay
Just an fyi, adding all of the fields to that group by will make the aggregation unnecessary. You'll be grouping by all of the selected fields for no reason, you can just delete the group by statement entirely. Do you know what they were trying to do with that? Why is it 1,2?
You cannot be certain that the start - end will have all hours of the day. There is no ON in the CROSS JOIN Should just be simple Select Id , hrs.Hour , IIF(DATEPART(HOUR, act_start) = hrs.Hour , 60 - DATEPART(MINUTE, act_start , IIF(DATEPART(HOUR, act_end) = hrs.Hour , DATEPART(MINUTE, act_end) , 60 ) ) FROM OriginalTable OriginalTable OUTER APPLY ( SELECT Hour FROM @Hours WHERE Hour BETWEEN DATEPART(HOUR,OriginalTable.act_start) AND DATEPART(HOUR,OriginalTable.act_end) ) hrs 
Not a criticism of this post in the slightest This is just something I'd like to see more of in general. Specifically, I'd love to see more tutorials about implementing statistics and machine learning algorithms with SQL (using procedural languages, Python UDFs, or even just with Pure SQL). I've found some blogs on this, but not really any course content coverage. Some examples: https://periscopedata.com/blog/multi-dimensional-clustering-using-k-means-postgres http://www.silota.com/docs/recipes/sql-n-tile.html https://periscopedata.com/blog/understanding-outliers-with-skew-and-kurtosis https://periscopedata.com/blog/finding-nearest-neighbors-in-sql PAMDAS takes 5 to 10x the RAM as the size of the actual dataset. So...when I'm working huge data sets it's a real non-starter. I typically opt to run a Python UDF inside Postgres/SQLite and then do the last-mile presentation in Python with Jupyter. All in all, there's a lack in SQL-Centric data science content out there, yet this is much of what DS jobs might actually look like in the wild in large organizations running on huge datasets. 
You can address the issue of absent employees by having the temporary table of 4 employees and use the temporary table instead of employees and use 4 instead of 5 in the SQL I have given. 
Oh, I just wanted to do it without using a case to see if I could without resorting to a loop. I don't think the OP wants non-business hours in the final set, but maybe he does. Was just fun to think through.
I'd be tempted to say that all chats are group chats with n participants, your messages table just has a chat id and a participant id. If you want to record who was in the chat even if they didn't participate perhaps change the messages table to a chat events table and record joining, leaving, inviting, etc. along with the messages. You could always separate that chat participation into a separate table if that over complicates things. One thing either approach would allow would be to allow showing historical transcripts of chats to participants for only the period of time in which they were part of the chat. This also allows the conversion of a 1-to-1 chat into a group chat by inviting the group. 
My guess is that this site steals content from other blogs, reposts them, then posts &amp; spams them here. The account has only ever posted two comments and they were both "thanks for your suggestion". Every link they've posted points to this same domain.
All chats are group chats. Like above user said. But you store an array of userid and you always order then lowest to highest by user I’d. When someone opens a chat. You order all participants userid and make a query. If the chat exists. Get that chat. If it doesn’t. Create it (I’ve implemented something like this by using google fire base for my messenger function but sql for everything else. This way messaging has that real time update but I don’t need sockets for anything else on the site. And I just use the user info from my sql tables ) A better way to do it is to have a chat room table. And a look up table that matches users to chat rooms. Then, you just add user 3 to chat room 6 and user 34 to chat room 6. This makes it easier to look up for sql and easier to give user functionality to add and drop participants. 
Hey, thanks for the answer. With a naive implementation of this I will have always produce work-arounds for nearly every other action than the groups. E.g. Groups can share resources and comments, so users would see that he is in a group with someone just because they chatted. &amp;#x200B; Additionally I would create a massive amount of "Groups" with atleast n\*(n-1) groups for n users, PLUS the ones I actually would like. 
Even if there was something wrong with the query, that message means the app crashed, which is not normal.
No argument there. I come here often for some more challenging questions. Just because I'm weird like that. 
I studied in [sqlbolt](https://sqlbolt.com) I find out about it in the sub wiki 
Azure SQL database I think you can get a 1 year free subscription. Cloud based so you don’t need to install anything other than SSMS. 
Thanks but that was not my question 
Just posted a video that may answer your question. Comes with downloadable scripts. I cover Descendant, Ancestor and Cyclical dependency recursion via CTEs. &amp;#x200B; I noticed that in your code you are only calling data from the recursive cte (in the union) and not the original table. You should take a look. (Check description to jump ahead to the Recursive CTE portion) &amp;#x200B; [https://www.youtube.com/watch?v=YVMgJlAiyNs](https://www.youtube.com/watch?v=YVMgJlAiyNs)
Heya... Super advanced t-SQL engineer here... Im curious to know what you mean by advanced. I would love to start a series gear toward high level topics. 
hey, self plug but try https://pgexercises.com
[Leetcode](https://leetcode.com/problemset/database/) has a few database / query exercises you can go through. After answering them on Leetcode, it shoudn't be too hard to think of variations of the question to strain your knowledge and practice solving those variations on your postgresql server. And given Leetcode's datasets are small and just used for checking your query, you could use larger datasets and practice query optimization and performance tuning. A bit less project-focused, but I've also found it helpful to read through SQL [documentation](https://www.postgresql.org/docs/11/index.html) just to get a familiarity with various keywords. There's rarely an immediate benefit, but it makes it easier to write more succinct queries. Good luck!
Dont do so much in one query... &amp;#x200B; Lets say i have 2 tables tblProduct (unique list of Products) tblTransaction (non unique products with quantities) You can: SELECT ProductId, Quantity\_SUM FROM tblProduct p OUTER APPLY ( SELECT SUM(Quantity) Quantity\_SUM FROM tblTransaction t WHERE t.ProductId = p.ProductId ) &amp;#x200B; Notice i didnt need to use a group by. Getting too much information at once is not a good practice. Split everything up, deal with smaller chunks of data.
Leetcode isn’t free, is it? I’m looking for an open tutorial
Leetcode has a freemium model. 
will check it out, thanks!
Hackerrank has better and more free SQL exercises than LC. 
Awesome series so far!
I like the Unix/Linux way of thinking about users and groups all users are groups not all groups are are users. So you have a group membership table that shows when a user becomes part of a group. Then you have a messages table that is tracked by group id. Optionally you have a group-settings table where groups can be given a name or various rules can be flipped like “who lets who into the group” 
This is amazing! This subreddit should to work together to have AbstractSqlEngineer nominated for SQL Server MVP away for 2019
You'll want to use a subquery here. Using in the where clause: where zipcode not in (select zipcode from trans4cust where item ='xbox360') or along those lines. Apologies for being on mobile. 
I think what you’re confused about is an INNER JOIN versus a LEFT OUTER JOIN. INNER JOIN: The records must match in both tables. LEFT/RIGHT OUTER JOIN: The records don’t have to match. If they don’t match, then you’ll get a NULL, unless the records are already NULL in the LEFT/RIGHT OUTER JOIN table.
Thank you for the kind words! 
I don't even know how to respond to this, lol
Well hello, If we are talking MSSQL, I'd love to see some T-SQL courses for machine learning/Data Science/statistics in particular now that in-database R and Python is supported. I could see these features alleviating a lot of performance and deployment pain points (especially with R) though I've yet to see them utilized "in the wild". Any other "Integration" story topics are greatas well as this seems to be Microsoft's new M.O. The new Polybase stuff too. Really though... Any new 2019 features. On a side note, Microsoft and the SQL Server community is easily the most vibrant and helpful community. I've already seen a decent number of posts on these topics. Ibwork for a consulting company so we use everything, but the majority of our customers are SQL Server shops. 
&gt; I thought with select you are only able to select columns that exist with in the table but I guess when you include left join it's a different result. no, your initial idea is correct -- you can SELECT any column *in any of the tables mentioned in the FROM clause*
So, more advanced data techniques. What about advanced data modeling, physical modeling. Feature rich models, scalable, complete..
Do you have another table with a list of all possible zip codes?
Hi. I see. it's the FROM clause that allows you access to all the columns of all the tables despite writing out code that looks like it's for only Table 1. Thank you for taking the time to explain friend. 
It’s just a column in the current table, there isn’t a separate table for them!
&gt; despite writing out code that looks like it's for only Table 1 obvs you gotta change what you think it looks like remember -- the entire FROM clause
Gotcha. I have to think about the entire code not just the initial part because that will change as well. Thank you.
So you need the zip code of everyone who has purchased a product but where no one in that same zip code has purchased an Xbox? Just clarifying because a table of transactions will not contain every possible zip code in the United States.
Yes that’s correct! Sorry for not being clear in my original post. 
No problem! On mobile so forgive my pseueocode but basically this: SELECT DISTINCT Zipcode FROM Transactions WHERE Zipcode NOT IN ( SELECT zipcode From transactions Where product = 'XBOX')
Ahh this totally makes sense thank you !!
 SELECT t4c.ZipCode FROM trans4cust t4c GROUP BY y4c.ZipCode HAVING SUM(t4c.XBOX360) = 0 ORDER BY t4c.ZipCode ASC
Ahh I see what you're saying about calling from the other table in the union. That's obvious. I rewrote it a bit trying to follow your example and logic (as i understand your logic) and unfortunately got a "max recursion error" . I wonder if I am trying to make my query go in the wrong direction? As the table is laid out, effectively, I think the grandparents are hte Y's (tank), which flow to a Z (packout unit) to a product (starts with number). I initially tried to start at the end, because I'm looking for the components that made up the Y's in their final iteration... so with that in mind, I tried this - which is where i'm getting the max recursion error WITH BOM AS ( SELECT BillNo , ComponentItemCode , QuantityPerBill , UnitOfMeasure FROM sqlmas.dbo.BillOfMat BOM WHERE billno LIKE 'Y%' --start at the "grand parent" (tank) UNION ALL SELECT FG.BillNo , BOM.ComponentItemCode , BOM.QuantityPerBill , BOM.UnitOfMeasure FROM dbo.BillOfMat FG INNER JOIN BOM ON FG.BillNo = BOM.ComponentItemCode ) SELECT * FROM BOM This one actually makes the most sense to me as far as trying to start at the small end, but it still gives me the max recursion error &amp;#x200B; WITH FG AS ( SELECT BillNo , ComponentItemCode , QuantityPerBill , UnitOfMeasure FROM sqlmas.dbo.BillOfMat FG WHERE billno LIKE '[0-9]%' --starts with numeric UNION ALL SELECT FG.BillNo , BOM.ComponentItemCode , BOM.QuantityPerBill , BOM.UnitOfMeasure FROM dbo.BillOfMat BOM INNER JOIN FG ON FG.ComponentItemCode = BOM.ComponentItemCode ) SELECT * FROM FG ORDER BY BillNo Converted my table above to an insert statement DECLARE @BillOfMat TABLE ( [BillNo] [varchar](30) NULL, [ComponentItemCode] [varchar](30) NULL, [QuantityPerBill] [float] NULL) INSERT INTO @BillOfMat VALUES ('1234-01', 'Z1234-1', 1 ), ('1234-16', 'Z1234-1', .0625), ('Z12341', 'Y1234-1000', .001 ), ('Y1234A-1000', 'Component_1', 5 ), ('Y1234-1000', 'Component_2', 3 ), ('Y1234-1000', 'Component_3', 6 ), ('Y1234-1000', 'Y4321-1000', .464 ), ('Y4321-1000', 'Component_1', 2 ), ('Y4321-1000', 'Component_69', 96 ), ('Y4321-1000', 'Z1999-1', 3 ) This small table is small enough to give the wrong output (product -&gt; Z) without getting recursion errors... sometimes. My goal is ultimately to have: 1234-01, Component1, .035 1234-01, Component 2, .00067 etc (last column made up on the fly i didnt do the math out)
Thanks - I think this worked! 
Can your vendor get you started with a test database or two? Then you can play around without fear of screwing your records. 
Yes! That was the first thing I asked for. So I technically have two DBs, the real one and a sandbox ( so they called it)
Oh for sure. In general, there should be more advanced design-centric courses, but I've done a good bit of database design work in my career thus far and the topics I mentioned were discounting those I already have decent experience with. Essentially, I was sharing my **wish list**. To give some background... I've done mostly data infrastructure and OLAP stuff as much of my previous experience was as a data warehouse developer, but I also have a solid few years of Python experience and I've been dipping my toes into machine learning stuff recently. I'm considering going that route vs. purely infrastructure oriented roles in the future. As such I'm looking for advanced SQL courses that bridge that gap into the world of machine learning and statistics. Because I have a foot in the Python as well, the marriage of SQL skills and data science tools in Python is something that appeals to me greatly. So...naturally, topics like Python/R in T-SQL (or pl/python and pl/r in Postgres) tickle my fancy.
https://docs.microsoft.com/en-us/sql/ssms/tutorials/tutorial-sql-server-management-studio?view=sql-server-2017 You can get started here . SSMS is intuitive and user friendly . If you are good with your SQL queries , you have nothing to worry . Just get your query reviewed by some one who is good at SQL. 
I'm a sysadmin that got pushed into SQL a little over a year ago, self taught. I suck but I've done some really cool stuff for the company and so will you. I'm guessing we need the same things to do our jobs, pull data into something useful, *not* being a full-on database admin. * Start by learning to backup the production database and restore it into the sandbox from SSMS. * Find the tables you want data from. Some are nearly useless for humans (they're just a temporary place to put data while SQL operates on it) and some you'll get to know by heart. Does your software tell you what table a given window is pulling from? MS Dynamics does it but I don't know about other software. Ask your vendor. Worst you can do is just to start right-clicking tables and select everything to see what's in there. * Start with some simple [SELECT](https://www.w3schools.com/sql/sql_select.asp) statements. With SELECT you're just looking, not changing. Click your sandbox on the left and try "SELECT * FROM TABLEIWANTTOSEE". Case doesn't matter BTW. * If the data you want is in multiple tables and you need to pull it together then JOINs are your friend. You'll want [this](https://i.imgur.com/v23nUwQ.png) or [this](https://i.stack.imgur.com/UI25E.jpg) later. * Hopefully your software vendor shares your screen and helps out. Take notes! I've seen them do things like, "I did not know you could do that..." Now I'm better than tech support in many ways. * Once you have a script written that does something, *anything*, useful, save it to the company file share. You're going to come back to it, I promise. Later you can discard super simple stuff as you start hammering out queries from memory. Have you met our Lord and Savior, PowerShell? Don't want to overwhelm you right off but PS is an animal for automating SQL tasks. /r/SQL and /r/PowerShell are the most helpful people I know of on reddit ATM. I've automated things like: * For every pay code is there an associated worker's comp code where applicable? Email me and the payroll manager every Friday if not. * These tax boxes should always be checked. Check everyday and check 'em if not. * Get the number of people logged in and email if we're close to the limit so I can kick old logins. * Kick old logins script. I could go on but I want to put the PS bug in your ear. I will always answer any questions you have if I can. Very daunting to get started but it's like learning a new language, it builds on itself. 
Have to disagree on this one. &amp;#x200B; I've written a ton of Python and SQL for data munging. Python is not better for data munging, and lines of code is a very poor indicator of how hard something is to code for. Of course, it will seem better to those who can't think as clearly with set-based logic as they can with procedural logic (I used to be one of these people). I too love Python and R, but ***better*** is about far more than just terse code and syntactic sugar. &amp;#x200B; For one...Python doesn't really handle large quantities of data well. PANDAS specifically does not fit this bill. &amp;#x200B; Per the author of PANDAS himself (Wes McKinney) "**my rule of thumb for pandas is that you should have 5 to 10 times as much RAM as the size of your dataset".** &amp;#x200B; Source: [http://wesmckinney.com/blog/apache-arrow-pandas-internals/](http://wesmckinney.com/blog/apache-arrow-pandas-internals/) &amp;#x200B; Pandas was built for analysis, not for data munging. It is not optimized for large data, and is grossly inefficient. It's flexibility is deceptive because it works so beautifully with small datasets. &amp;#x200B; Get a CSV of just few gigs and you've just wrecked your VM or server, and forget about doing it on commodity hardware. &amp;#x200B; Last time I tried to data munge a couple of 3GB CSV files in Python I maxed out my RAM. Tried all the Pandas tricks to help with RAM like using generator expressions, filtered w/ chunk sizes when reading from disk, and did a "import gc; gc.collect()". Didn't really matter, and never has at any other time I've worked with large data. &amp;#x200B; Yes... You can use PySpark, but again this isn't a Python solution. It's Scala. There's also Dask, but still its just jumping through hoops to avoid using the right tool 95% of the time - which is SQL. &amp;#x200B; TL;DR -- All that to say. Don't use Python when you can use SQL of any dialect. IMO - SQLite is a better alternative (faster, more portable, more RAM efficient) than Pandas by a long shot. &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B;
Wow that's awesome, thanks for all the info. I will definitely look into PowerShell. I'm excited to get into this, kinda scared, but i did offer my services to the VP of Finance (my boss) that I would give this a shot. Sometimes I feel like this may be a better route down the line than finance, for me at least. 
I agree to some extent. At my work we can't write procedural SQL due to security. I just use pyspark if the data is larger their data frame api is sweet and similar to pandas. The servers I work on have 196gb of ram so I rarely hit memory errors working with millions if records. 
Adding columns to a table is a simple operation. `alter table add column NEWCOLUMNNAME column type null` &gt;appending some columns not to the end Column position in the table should not mean *anything* if your code is written properly. Don't worry about where the columns live; if you need a specific ordering, do that in the `SELECT`. *However*, your SDK should not assume that it's running with the rights required to alter the schema. Ship your SDK with a change state-based change script (detect if the field(s) exist and create if necessary) that the person installing the new SDK can run.
&gt; Start by learning to backup the production database and restore it into the sandbox from SSMS. A financial analyst shouldn't have the rights do to this, nor should they have a need to do it. &gt;Case doesn't matter BTW. It does if the database is set up with a case-sensitive collation.
&gt; SSMS is intuitive and user friendly I live in SSMS 40+ hours/week and I think this statement is a bit of a stretch.
I watched through the video and definitely seems to be the right tool for the job, but i can't make it work. My last reply to soccerfreak has a lot more details. 
K.. couple things. Union all Select bom.billno , Fg.component ,fg.whatever From fg inner join hom On BOM.Component = fg.billno Join recursive child to original parent. Select recursive parent and original child. 
&gt; A financial analyst shouldn't have the rights do to this True, but here we are. Probably a small company like my own, many people wear many hats. Let's teach.
 alter table TableName add ColumnName type null;
Thanks, up too late and not paying close enough attention
What happens when there is no match but table B has a constraint that doesn't allow NULLs?
You have about 8,000 valid excuses for having your mind elsewhere. I only have a couple. ;)
Hey, can you try this, I rewrote it a bit: WITH FG AS ( SELECT BillNo , ComponentItemCode , QuantityPerBill , UnitOfMeasure FROM sqlmas.dbo.BillOfMat WHERE billno LIKE '[0-9]%' --starts with numeric UNION ALL SELECT FG.BillNo , BOM.ComponentItemCode , BOM.QuantityPerBill , BOM.UnitOfMeasure FROM dbo.BillOfMat BOM INNER JOIN FG ON FG.ComponentItemCode = BOM.BillNo -- Changed BOM.ComponentItemCode to BOM.BillNO ) SELECT * FROM FG ORDER BY BillNo I think that should get you what you want. &amp;#x200B; Notice that in the recursive portion of the query your goal is to get the the children listed for the parents (where the parent Id is held in BillNo and the children are in ComponentItemCode). When you join, you need to remember that FG.ComponentItemCode is the BillNo of the child you're trying to find. So when you join from the CTE to the BOM table you actually have the list of BillNos based on the parents.
I believe... not sure, but I believe the 255 limit is not really an SSIS limitation, per se, but that "Excel itself" has a limit of 256 columns. But wait, what's that, you say you can have more than 256 columns in an Excel workbook? Well, that is true, but from what I understand Excel is doing something behind the scenes there and cobbling things together. So while your workbook opens and looks like it has 257, the extra 1 column is part of another "file" that Excel is "joining" the first file. SSIS doesn't know what the fuck it's doing, so you get 256, because Excel has a limit of 256. https://superuser.com/questions/366468/what-is-the-maximum-allowed-rows-in-a-microsoft-excel-xls-or-xlsx Now... the real question is why the SSIS team and Excel team don't seem to talk to one another. There may be some additional plugin or something you can get to import larger XFD files. I tried Googling and found nothing.
Not sure if this is causing the problem, but it looks like you're trying to do a join without defining the relationship. And I think it's a better habit to do ansi joins. From table1 Join table2 on table1.column=table2.column
For shits and giggles. Which one would execute faster. Using a case/outer apply, or the method I was playing with? More genuinely curiously. I don't really understand what an "outer apply" is, but have used cross apply before.
You will need to join the ordered_items, and price_history tables. Something along the lines of Select whatever FROM ordered_items ordit --alias table JOIN price_history pH on ordit.item_id=pH.item_id I am guessing on item_id but it should a key used in both tables. 
i want result be like this : &amp;#x200B; |id| active\_hours | |:-|:-| |10|5:30| |20|6:10| |30|4:00| &amp;#x200B; If we can find inactive ... , total\_hours ... to be be additional two more column will be nice
Look into Azure Data Studio from MS: https://github.com/Microsoft/azuredatastudio
Hmm well having pure or read only SQL to work with changes things. PySpark is definitely a viable option. Having a beefy server helps too (I've been limited to 32gb shared Dev VMs recently). I've used PySpark/Data Bricks a bit too and have had a very positive experience. Spark Dataframes bring some top notch integration to the game and offer folks the ability to "pick their poison" so to speak. The ability to use SQL and Python on the same object without context switching is a real game changer for me at least. The myth that I try to dispel is that Pandas (specifically) can replace SQL. Its a real damaging perspective that I've seen pitched by non-data folk to newcomers repeatedly in some local Python meetups I've been to. I get a bit irritated when I see influential mentors misleading Junior developers in such a way. (Great power...great responsibility etc). 
Sololearn's SQL course is pretty good and it's free ( [https://www.sololearn.com/Course/SQL/](https://www.sololearn.com/Course/SQL/) ). Don't be intimidated. SQL and SSMS is pretty easy to use to do productive things. Have your vendor give you a read only SQL login as well as one with edit capabilities. Use the read only one to do most of your queries and experimenting. That way you can't screw up your data. More importantly, demand documentation from your vendor such as a database schema will help you locate where specific data resides in your database. 
This is a common difficulty. Select your project, employee, manager combinations, then add "not exists (subquery identifying cases where the same employee has a different manager)" to your where clause. 
Thanks for yur reply. I had been trying to do that, but I'm not sure how to encode it :/ do you have any suggestions?
I would search on /r/datascience, this is a common question. In general, different companies have different expectations but generally what you wrote is not wrong. Some companies hire "data scientists" but the role is data analyst with a different title. The titles are young enough that they're not defined completely yet. 
well, firstly: get rid of the "natural" join. Anywhere beyond academia, having columns with the same name does not mean that join needs to be done by that. secondly: One good way of writing queries is to recognize granularity of your desired output - in this case, it would be by employee. Group by in your outer query should match that. Right now you're asking for granularity of employee and manager.
I definitely agree with you, pandas is not a big data solution. Just in my positions as a data scientist I have experienced that pandas answeres 95% of questions if given a big enough resource. My organization also has a dedicated EDW/Engineering team of 30 FTEs. So they do the vast majority of data munging for me. Anything I do in pandas is just to get things in the right shape for modeling. The EDW team definitely uses database tools for munging all of the data. Usually what kills my memory usage is when I am doing stochastic/linear programming, as getting the data in the right format creates a bunch of massive intermediate probability matrices. Then I drop down to spark to do that kind of stuff. Otherwise pandas with pretty much clean data works perfect for my use case the vast majority of the time.
Check the description of this video. The HAVING with self join INNER vs EXISTS (minute marker) will have that code. The .sql can be downloaded also. https://youtu.be/XJnaRG59Jss
1. Thanks for the tip! So just use an inner join as specify the joining condition instead? 2. I have group by both because it told me e2.eid must appear in the GROUP BY clause or be used in an aggregate function haha. As you can probably tell I'm super new to sql.. &amp;#x200B;
I assume that for Machine Learning, the parts where the data is being manipulated in Python are a tiny part of the time that the model needs to run, so Python's speed is not a factor. Is that right? 
no I am fine posting it here, as it is kind of a home ground :P From what I have seen is their skills are shelved, with their current work being ETL/analyst work. makes me wonder sometimes is it better to be good at something (ETL) or spend my time on other skills. the data scientist as a "other skill" is completely different scenario as it is kind of a college degree. Personally now, for "other skills", I like to spend more time on programming, with full stack in mind.
Yes, mostly. You still need to process that data beforehand, clean it, etc. But yeah, the learning part is much bigger, and it's in C.
Am an analyst, and my department has several analyst positions open, so I'm tangentially involved in interviewing candidates. I wouldn't say we're looking for MBAs, but I think I'd lean towards MS before an MBA, since they would tend to have a more technical background, although, as you mentioned, they would need to have or strong willingness to learn the business. I would think it would be easier to teach MS the business instead of an MBA the technical/coding/analysis skills needed. Just my two cents.
DISCLAIMER: IN MY HUMBLE OPINION I think you will find the need for data professionals will greatly outweigh the need for programmers in the next decade. Learning how to create RESTful APIs for your middle tier is good to know, maybe some angular/mvc for front. But the demand for data professionals will keep going up. Companies will want specific data roles and specific OOP design pattern roles. The more abstract, the fewer lines of code, fewer bugs, more agile, quicker release cycles. Now that IT is reaching maturity with newer generations being brought up in technology, specialists will be in high demand. You already see it with these newer titles and demands for specific software and combinations of. Learning how to create full feature data management systems (not models based on need) for application and analytic consumption (master data), learning how to move data and analyze it, learning how to make decisions based on your data (ml,predictive,"ai"), and getting some r and python in there will make you a force to be reckoned with. Not to mention SQLs PolyBase being able to access other tech... its gonna get real fun. I'm not saying dont learn a programming language. I'm saying mastering a craft, in my opinion, should be the top priority for someone in a data career. 
makes sense and that is also happening as most of the personnel in project are technical people even int the business end :) i am assuming teaching the business part to a technical hire would be project specific. when a new project/client comes, the person may feel fish out of water sometimes when client comes with different business needs (reason i am asking it is getting a new client is one of the most important part) ?
I think with your background you'd be able to get an analyst gig without much trouble. A lot of it is soft people skills as well. The analyst is often sitting between business intelligence/DBA folks, as well as working directly with sales/management etc. So it's not quite as technical as what you're doing but can still heavily involve sql/tableau etc
\*\*\*\* Section Times in Video \*\*\*\* \#1: SQL Server Profiler Use Cases [0:32](https://www.youtube.com/watch?v=5fLsrRAtTJA&amp;t=32s) \#2: Download AventureWorks2017 OLTP Database [2:07](https://www.youtube.com/watch?v=5fLsrRAtTJA&amp;t=127s) \#3: Open Expensive Query Script in SSMS [3:35](https://www.youtube.com/watch?v=5fLsrRAtTJA&amp;t=215s) \#4: Open SQL Server Profiler [4:37](https://www.youtube.com/watch?v=5fLsrRAtTJA&amp;t=277s) \#5: Create New SQL Server Profiler Trace [5:00](https://www.youtube.com/watch?v=5fLsrRAtTJA&amp;t=300s) \#6: Run the SQL Server Profiler Trace [9:14](https://www.youtube.com/watch?v=5fLsrRAtTJA&amp;t=554s) \#7: Execute Expensive Query Script [9:30](https://www.youtube.com/watch?v=5fLsrRAtTJA&amp;t=570s) \#8: Analyze SQL Server Profiler Trace File [10:42](https://www.youtube.com/watch?v=5fLsrRAtTJA&amp;t=642s) \#9: Next Steps to Identify Solution [13:43](https://www.youtube.com/watch?v=5fLsrRAtTJA&amp;t=823s)
I did clear an interview that was for an analyst role but took another one with more technical requirements as I wasn't sure about the work (pay was similar for both of them) . The interview was more technical and was hardly about the analyst role. maybe it would have worked out fine but i wasn't sure. thanks for clearing up though :)
what a nice note, i am not sure i would be able to grasp the entirety so i am saving this one :) apis - true. 70% of my sources include rest/soap apis. it will be good to know how to make them. design wise also true - i also noticed a pattern when i started off and now is to keep things simple and not simpler. so that the pattern is easy to understand, implement, debug but not compromising on robustness. coding- why i was thinking programming is sometimes its good to know how things work if you want to save cost instead of using some costly tools. maybe this part can be shadowed by polybase. ml/ai/predictive - its all over the job market now but currently its still facts which has a huge priority than predictions. but its good to know if we can show some benefit using it to the customers.
1. yes 2. Well, you tried the first half ("must appear") and it's not giving you the result you need, time to try the second one ("be used in an aggregate"), maybe?
As an ETL/DW developer, your next progression or specialization should be to become a Big Data specialist and become a "Data Engineer". Most of the work that goes into data science is about ETL and engineering anyway. AI and ML doesn't make this magically go away. It just provides a good set of algorithms and models to consume the data. But the hard, ugly, messy, complicated bits are all still the same. Ingesting data, transforming it, QCing it, pivoting it, splitting and merging it, de-duplication, error handling, audit trail, etc. Similarly, the hard bit is to make the algorithms and models run at scale, at true real-world data volumes, run in parallel and not sequentially, host it in production, think of tenant and environment management. You'll be doing all that as a Data Engineer - which is mostly what you're currently doing too, but on a different tech stack. But you really need to upgrade your skills to become more knowledgeable and experienced on the Big Data stack - Hadoop and Map/Reduce, Hive, Yarn, Spark etc. Especially Spark - I can't emphasize how important it is to learn and understand Spark in depth. It is the most relevant platform for doing large scale distributed ETL and data processing. It has a steep learning curve but it is well worth the effort. It will most likely dominate ETL a few years from now.
thanks! you right about big data. i do have some experience, not on hadoop but on azure lake. getting files and writing usql scripts and pushing the refined data to sql db, not much though.
Can you expound a little more on what you would define the major features of a full data management system are/look like? I'm pretty early on in my career (2 years) and thinking I'm interested in focusing the rest of my career in data. I currently have a sort of hodge podge role in a manufacturing company's R&amp;D department where I'm wearing a lot of hats. Some of the major projects I've started have been in the realm of developing a data model of our manufacturing process and implementing a data warehouse containing all of our process data, devleoping some rudimentary data collection pipelines (mostly just python scripts and SSIS packages at the moment) and fleshing out our "real time" (we're a slow industry so this means like 15 minute refresh rates for us) reporting capabilities. I'm hitting the point now where I'd like to start investing some additional time in increasing my technical understanding of the challenges we have and figuring out how to leverage newer technologies and I've recently been made the technical lead (in a sort of council/team) of a project that is basically the implementation of data governance policy and creation of a data management system for the corporation.
Keep learning. and get certified. Azure is rapidly becoming very relevant too. But like i said, Spark will be the steepest learning curve. Play around with HDInsight
How do you identify when you are in a situation where moving from traditional ETL/storage methods to a big data stack like that are required? I'm in a similar role doing SQL Server ETL and management at my company and would love to start working with newer data engineering tools, but I struggle to understand what the use case/value added is with them.
A lot of the "value" is BS, to be honest. There's very little "true" Big Data. But the main reasons would be future-proofing your architecture, ability to handle social media and semi-structured and unstructured data, ability to handle streaming data, have a "schema on read" strategy for your data lake instead of a "schema on write" which an RDBMS or a DW forces upon you. Still, it is not a clear-cut answer. True answer is that you win some, you lose some. You lose the enterprise class stability and reliability and performance guarantees and strong SLAs. But you gain all the innovations being done in this super fast evolving space. You gain access to a ton of open source libraries and Apache projects. You get better integration with the ML world. You get to leverage the serverless and prebuilt services from your cloud provider. In some cases, you also get significantly lower cost. You don't have to pay expensive license fees anymore, especially if you're running a cluster and you don't want to pay for per-CPU licenses.
I have a series where I create a master data management system from an empty instance. Albeit, not as good as I want it to be. The MDM system contains the following features: Abstract subjects - 4 table shapes to make then entire system Multi tenancy - multiple customers / templates in the same table History archive and snapshot capabilities- not using temporal tables Hypernormalized objects - databse isolation down to the files (64 databases) 99% of the tables consists of the same 20 columns that allow EVERY RECORD to: Have toggleable rowguids, have paramterized security, have parameterized content, have time sensitive activity, have code specific references (keyname), have the ability to reroute (cleanse) a record to a new one, have a tenant specific sequence, have normalized physical modeling. The system operates in a way where the data is moved to many locations when altered, instead of the common bulk movement. Ex: a new people record will exists in the operating database and be recorded in history, atomic (scd type 1) and the warehouse and reports databases for immediate access. No reloads necessary. The design patterns allow for the creation of all taxonomy functions,views, change managment procedures to force data in a specific direction (governance and change management). The creation of these objects are done by the system. The design allows you to move specific high access data to different drives and servers... because the application will only talk to one database (interface). The design is a master data management system that works with any business, because it's a data managment system... not a business need system. I probably only mentioned half of the functionality. This is a full featured system. Since everything is data driven, you can turn on an off features. But they are always there. No matter where I work. It's the same model.
Do you have any expierence with how these applications are deployed over scientific data applications? I work for a major materials manufacturing company in R&amp;D and am responsible for I guess our data management, visualization, and software driven innovations. I think the reason why I struggle with most "big data" applications is because most of the press about them is from, for I guess obvious reasons, the challenges faced with web facing applications and scaleability in that domain and it's not really applicable to us. In a given plant in my company I may be tracking at most a hundred or so different work centers (something generating data) that has a few dozen signals generating time series data 24/7 at refresh rates either on the grain of seconds or more infrequently miliseconds. We currently only deal with event based aggregated data and there are a decent number of vendors out there who specalize in developing systems that interface machine PLCs with servers to collect and store the time series data in proprietary compressed formats which we then extract later to create control charts, OEE reports, etc... I'm not really sure where in this current model newer technologies should/would be inserted. I'm guessing the ability to consume that time series data in real time is the potential value in looking at these newer tech stacks? Sorry a bit of stream of consciousness there, but I'd love to hear your thoughts on it.
Oh yes, absolutely. Your use cases can easily fit the Big Data use case. Time series data is a very standard thing. &gt; which we then extract later to create control charts, OEE reports, etc... This is a classic ETL or Big Data ELT pipeline. Ingestion, transformation, storage, analytics, reporting. Big Data or not (you can also look at Vertica which is a champ on timeseries data, also look at Snowflake like i said), you should absolutely be ingesting all the data you possibly can. That means all the pre-aggregated timeseries data. That's also the underlying philosophy behind a data lake. Even if you don't know what to do with data (like your high volume timeseries data for example), first figure out a way to establish a batch based ingestion or a streaming data channel and dump it in inexpensive but reliable storage like S3 or Azure Blob storage, catalog the files and its schema (optional) and metadata in a Data Catalog service. This is the "schema on read" philosophy. When you or someone in your company figures out how to consume the data, that is when you need to actually look at the data in-depth, figure out a more formal schema etc so you can run a data science algorithm or more complex analytics ETL or complex queries on it. And because storage is decoupled from compute, unlike SQL Server (to some extent), you can store terabytes and petabytes of data and still incur very little costs. 
To clarify here so I'm understanding since a lot of this terminology is pretty new to me. The idea here is to capture the timeseries data and just store it in an unstructured (probably compressed) form on a per-sensor level on a cloud service. Batch based ingestion would look like regularly scheduled loads of the time-series data into the data lake. Where as streaming the data happens in some form of real time? Is there a benefit to streaming the data real time into something like S3 vs. just using jobs? How does this process map to the tech stack Hadoop and Map/Reduce, Hive, Yarn, Spark etc.
I think they meant something like: CREATE TABLE entity_attribute_value ( id SERIAL PRIMARY KEY, entity_id INT REFERENCES entity(id), entity_attribute_id INT REFERENCES entity_attribute(id), entity_value_id INT REFERENCES entity_value(id), ); Second one I believe is suggestion to create enum types for each attribute/value. Then those will take only 4 bytes regardless of text length (so the same as int4), tho I'm not sure if it is any better than having text type in dictionary tables (as far as performance goes). It certainly is annoying to modify enum types.
Depends on the relationship. If it is n:m you need the additional table anyway. If it's anything else you probably want to reduce null values. So if (almost) every user has exactly one company he belongs to, putting company_id directly in the user table makes sense. If you have many users, but many of them don't have associated companies you probably want an extra table for those that have.
It all depends on your use cases. Will a user every be related to multiple companies? How will your company-user data be used? Do you need to know every company a user is linked to, or do you really only care about their "primary" company?
It depends on what you want to do with this timeseries data and how quickly you want to do analytics and alerts on your data. But yes, if most of your analytics happens out of band and much later, then you can just have a separate strategy for data capture and dump, and a separate strategy to process that data and do analytics on it. Main principle is separation of storage from compute. Storage will be in the form of compressed open source formats - like [ORC](https://orc.apache.org/) or Parquet. These are also formats that other elements in your tech stack will also understand. But you should double check - in some cases, only support for 1 of the 2 formats will be there. Hadoop or HDFS is a distributed file system and might or might not be relevant to you. It becomes relevant if you want to load this data into a compute cluster Hive is like SQL Server - somewhat. Yarn is a job scheduler and manages task allocation across cluster nodes when you have a job running on multiple computers Spark is a distributed in-memory data processing engine. It actually doesn't require a lot of the traditional Big Data stack. Which is why I would suggest you to do a POC on Spark first and see what it takes and what it supports natively. Spark has traditionally supported Parquet but recently started supporting ORC as well. By separation of compute from storage, I mean your files can live in storage, and you could potentially (optionally) maintain your data catalog in a [custom Hive metastore](https://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-use-external-metadata-stores). This is a fancy way of saying "store the SQL create table definitions in a metadata store so when your cluster spins up, it can access your files in your data lake via a SQL interface". And you can have truly dynamic compute clusters. Say you want to run an intensive and complex ETL pipeline. Spin up a large sized cluster with tens of compute nodes, have your cluster read timeseries data from the files, do its processing in-memory using Spark, write the output and logs and errors to other files, and shut itself down.
Makes sense. Could be useful to have a user associated with multiple companies and be able to use that data in which case that table makes sense. I'll have to revisit my plans around the final result of the project and decide if that is going to be useful. Thanks! 
Hmm makes sense, every user will have a company. Just need to decide then if I will accommodate for users being associated to multiple companies or not
3rd form 
I used to be pissed that Data Developer is not more in use. (And often replaced with DBA.) But now I prefer to use Data Engineer instead.
I created a 17th normal form where every character of every table is stored in a table of itself in the cloud
Not sure if your being sarcastic but that would be an interesting setup to see work just to be able to see it, not for production
I was very lucky to have someone explain this really well when I first started programming. He told me The best way to thing about normalization is to imagine the velocity, mutability and scale of your data. Values that change or are edited frequently are good targets for normalization. Values that describe relations ships that might be leveraged at scale are also good targets for normalization. Values that will change infrequently or remain constant and are access in context that aren't too complex are good targets for unnormalized lookups . So in the example you provide the important questions to ask are: "is there an expectation that the company associated with a user would change?" it sounds silly but its entirely possible that if you provide a platform to a company they may want a "user" to represent a unique account for a person who works for them so in that context the company might never change after the first time you set it. "if there is an expectation a company would change would you also want to be able to see previous company records associated with that user?" In this case a join table that includes a datetime would allow you to track a users company history over time. &amp;#x200B; "can a user be attached to more than one company at a time?" if this is ever a possibility a single reference field won't work &amp;#x200B; "will you ever need to aggregate and report on data at a company level?" If you are building something for scale its important to know that even with well designed indexes if a table is gigantic containing many rows and many column it will be that much bigger on disk and will take longer to read from so its quite possible that: SELECT user\_id, company\_id FROM user\_x\_company WHERE company\_id=10 requesting data from a table with only two column that are both integers would end up much faster than SELECT user\_id, company\_id FROM profile WHERE company\_id=10 a table with 30 columns full of all kinds of stuff. &amp;#x200B; &amp;#x200B;
thanks! funny thing is I also have a hdfs setup that i did on my 2 old laptops and a vm few months back. master and slaves. if file goes missing it would pick up from the other one. such a nice thing . when i get time i dwell on other things but time is such a weird thing.
yup searched few jobs most are using that :D lol
How about doing a poc on a generalized data pipeline in the project and then comparing the estimated monthly costs with existing implementation. its easier to say things like these when it is a cloud based system. not sure on the infra setup. would that be a good idea?
While this is possible it really is not a good thing to be doing on your DB instance. More specifically this creates pretty bad security hole if your production database is regularly pushing data across the firewall on its own. Much better to use the oracle client on an application to connect to the DB and write the file as you have been doing. 
This is an excellent brake down and easy to follow the logic when you put the questions forward as you did. Then you very much for writing that out
That’s a great perspective to have and I didn’t think about that being a security concern. Out of my own curiosity, what security issue does that open up for an outbound stream of data from the Prod DB?
That’s a great perspective to have and I didn’t think about that being a security concern. Out of my own curiosity, what security issue does that open up for an outbound stream of data from the Prod DB?
As far as you can, and then some. Always push yourself I operate in a hyper normalized environment. I dont want to say 6+NF.. because there is no NF that explains what I have done. From the Database to the File, the Table to the user defined data types, Views, procedures.. all normalized, all created by the system. &amp;#x200B; Why do you need a company table? isnt that just a group? Is there really a difference between a company and a template? Just an abstract grouping of data. Other data can belong to many groups. You are right to create a new table to store a relationship between a group and a user... but... that is a table that represents Relationships, not just one RelationshipType. =) &amp;#x200B; always push yourself.
Another thing to consider is this... are the users in your table treated as individual entities or as appendages to the company. If you sell a service to companies, and those companies have users, each with a user account in your system... but then Bob from Company A goes to work for Company B, you wouldn't design a system where the user ends up with the same account which is now associated with a different company. Bob would get a new User record, because the users in that case, even if they represent the same person, should never be connected between Companies.
Interesting approach, do you ever run into issues with performance? Are your queries messy with too many joins when you need to generate a report for instance? With the company information are you saying you would go as far as having a table for company names that associate to an id and then a table for the company addresses that would associate to the id and so on instead of having company info in the one table? 
Your question has absolutely nothing to do with normalization. 
Can you give a data sample as well as a "changed" sample right now it's not really clear what you mean.
Cartesian joins! The first mistake most make when they're learning. Myself included.
You'll need to use a window function to look at date proceeding 1. Then the difference between these should be what you need.
For this project the users are associated to the company so if Bob were to go to another company then a new account would be created against that new company and the existing one marked inactive
I've edited my original question with a small sample. That database has over over 95,000 records. 
As it has to do with splitting data into different tables and then subsequently connecting those tables together I thought it fell under normalization
self join each row to the row with the preceding date, and then you can use DATEDIFF SELECT prevrow.startdate AS prev_date , thisrow.startdate AS this_date , DATEDIFF(DAYS,prevrow.startdate,thisrow.startdate) AS days_diff FROM yourtable AS thisrow LEFT OUTER JOIN yourtable AS prevrow ON prevrow.startdate = ( SELECT MAX(startdate) FROM yourtable WHERE startdate &lt; thisrow.startdate ) i'm sure there's gotta be an easier way to do this with window functions, though
This is new to me so please bare with me. Will this work if there are multiple rows with multiple dates for each primary key? 
Cool it's the easy way then, one thing I'd suggest if you can is to flip the column to date time if at all possible and if the software that updates the data allows it. Anyway You'll want to look at using partitioning https://docs.microsoft.com/en-us/sql/t-sql/queries/select-over-clause-transact-sql?view=sql-server-2017 https://blog.jooq.org/2016/10/31/a-little-known-sql-feature-use-logical-windowing-to-aggregate-sliding-ranges/
Thank you, this is super helpful. 
Make sure the design corresponds to current requirements, and if not too hard - for any obvious future extension of those requirements. Don't spend more time and energy on over-engineering it. Keep it simple. Features can be added, design can be changed. It's more costly than doing it "right" from the beginning, but nobody knows what that "right" will be. Normalization as such is not really a thing in most databases. We still use surrogate keys everywhere, and take many shortcuts, and often resort to de-normalization for performance and simplicity. There is no universally "right" method. It depends on your requirements.
I appreciate all of the help on this! What I wound up doing is: &amp;#x200B; 1.) Created a function which takes an integer (representing the hour), a start date, and a start time as input and outputs how many seconds of that time frame took place in the hour entered as the variable. 2.) Created a table which had each column representing a different hour from 7 a.m. to 6 p.m. 3.) I did an unpivot of this table to organize the data better &amp;#x200B; Throughout all of it, I used temp tables so that later I could select from the temp tables to find the data at any point. &amp;#x200B; /u/amckny had a solution much cleaner than mine! I may adapt the code to the suggestions of them as well as /u/abstractsqlengineer and /u/notasqlstar. I appreciate all of your help!! &amp;#x200B; &amp;#x200B; \--------------------------------------------count how many minutes they were in a status for each hour (0-60)-------------------------- SELECT \* ,CASE WHEN StatusKey LIKE 'Available' THEN 1 ELSE 0 END AS Available ,dbo.udf\_ReturnMinutesInHour(7,StartTime,EndTime) AS '7' ,dbo.udf\_ReturnMinutesInHour(8,StartTime,EndTime) AS '8' ,dbo.udf\_ReturnMinutesInHour(9,StartTime,EndTime) AS '9' ,dbo.udf\_ReturnMinutesInHour(10,StartTime,EndTime) AS '10' ,dbo.udf\_ReturnMinutesInHour(11,StartTime,EndTime) AS '11' ,dbo.udf\_ReturnMinutesInHour(12,StartTime,EndTime) AS '12' ,dbo.udf\_ReturnMinutesInHour(13,StartTime,EndTime) AS '13' ,dbo.udf\_ReturnMinutesInHour(14,StartTime,EndTime) AS '14' ,dbo.udf\_ReturnMinutesInHour(15,StartTime,EndTime) AS '15' ,dbo.udf\_ReturnMinutesInHour(16,StartTime,EndTime) AS '16' ,dbo.udf\_ReturnMinutesInHour(17,StartTime,EndTime) AS '17' ,dbo.udf\_ReturnMinutesInHour(18,StartTime,EndTime) AS '18' ,DATEDIFF(MINUTE,StartTime,EndTime) TotalMinutes INTO #Temp2 FROM #Temp &amp;#x200B; \--------------------------------------------pivot all of the hour columns into a column called "hour" to make it cleaner------------------------------ select UserID ,EmployeeID ,StatusKey ,Available ,\[Hour\] ,\[Minutes\] INTO #Temp3 FROM (SELECT UserID,EmployeeID, StatusKey,Available,\[7\],\[8\],\[9\],\[10\],\[11\],\[12\],\[13\],\[14\],\[15\],\[16\],\[17\],\[18\] FROM #Temp2) P UNPIVOT (\[Minutes\] FOR \[Hour\] IN (\[7\],\[8\],\[9\],\[10\],\[11\],\[12\],\[13\],\[14\],\[15\],\[16\],\[17\],\[18\]) ) AS Unpvt &amp;#x200B; &amp;#x200B; &amp;#x200B; \-------------------------------------Return final results------------------------------------------------ SELECT #Temp3.UserID ,#Temp3.EmployeeID ,#Temp3.\[Hour\] ,SUM(Minutes) \[AvailableMinutes\] ,ProductiveMinutes INTO #Temp4 FROM #Temp3 JOIN (SELECT EmployeeID, \[HOUR\], SUM(Minutes) \[ProductiveMinutes\] FROM #Temp3 GROUP BY EmployeeID,\[Hour\]) Productive ON #Temp3.EmployeeID=Productive.EmployeeID AND #Temp3.Hour=Productive.Hour GROUP BY #Temp3.UserID ,#Temp3.EmployeeID ,#Temp3.\[Hour\] ,ProductiveMinutes &amp;#x200B; SELECT \* FROM #Temp4 &amp;#x200B; DROP TABLE #Temp DROP TABLE #Temp2 DROP TABLE #Temp3 DROP TABLE #Temp4 &amp;#x200B; \--and here is the udf &amp;#x200B; CREATE FUNCTION udf\_ReturnSecondsInHour &amp;#x200B; (@Hour INT ,@StartTime DATETIME ,@EndTime DATETIME) &amp;#x200B; RETURNS INT AS BEGIN RETURN CASE WHEN DATEPART(HOUR,@StartTime)&lt;&gt;DATEPART(Hour,@EndTime) AND @Hour BETWEEN DATEPART(HOUR,@StartTime) AND DATEPART(HOUR,@EndTime) AND @Hour&lt;&gt;DATEPART(HOUR,@StartTime) AND @Hour&lt;&gt;DATEPART(Hour,@EndTime) THEN 60 \-----------accounts for inbetween full hours---------------------\^ WHEN DATEPART(Hour,@StartTime)=DATEPART(HOUR,@EndTime) AND DATEPART(HOUR,@EndTime)=@Hour THEN DATEDIFF(SECOND,@StartTime,@EndTime) \-----------accounts for times within the same hour------\^ WHEN DATEPART(HOUR,@StartTime)&lt;&gt;DATEPART(HOUR,@EndTime) AND DATEPART(HOUR,@StartTime)=@Hour THEN (60-DATEPART(SECOND,@StartTime)) \--------accounts for @Start time SECONDs when @Start and @End time are different hours--------\^ WHEN DATEPART(HOUR,@StartTime)&lt;&gt;DATEPART(HOUR,@EndTime) AND DATEPART(HOUR,@EndTime)=@Hour THEN (DATEPART(SECOND,@EndTime)) ELSE 0 End &amp;#x200B; End &amp;#x200B;
Then putting company_id directly in users would be viable because that user can have only the one permanent company. It's still not wrong to make it a separate table, it's just possibly not necessary. 
In your case, I would create the 3 tables. Company table User table Company_User table
SSMS is good, but get in the habit of writing SQL statements to do what you need rather than just using the wizards and right-click options. If you rely too much on tooling you've not really learned how to do something, you've learned the tool. Why is this important? Transferable skills. Sure there are syntax differences between database engines, but if you become a SQL ninja on any RDBMS you'll be able to crack the others no bother. 
I have a group table that has a grouptype of company thats under a groupfamily of organization that's under a groupClass of external I have a groupaddress table that has a fk to group but also an fk to addresstype of home that's under an addressfamily of general that's under an address class of company that's under an addressrealm of group Because I have a peopleaddress table that fks to... with an addressrealm of people I do this with name, contact, flag, numeric, id number, event also. These are called shared attributes. This allows me to store Pablo picassos full name, because the people table doesnt have a name field for an actual name. Everything I'd vertical. I also have a Relationship table with a type of Member under a family of direct (or descendant, or ancestor) under a class of Tree under a realm of Group to People (and people to group)... one relationship table for all domain data. Never have to run recursive ctes unless there is an update. No performance issues. 10s of millions in some tables, 100 of millions in relationship. Sub second calls, 8gig ram 4 core, cursors only.
so lets say that you are running this in a stored procedure. An attacker who gains the right privileges can edit that sp to dump any data and ftp to anywhere. Most DB servers are generally setup in such a way that the only way they can be communicated with is through the specific ports the DB listens on this way any other traffic that might pop up going to or from the DB is a big red flag for network monitoring. 
you havent specified a field in the where clause and your joins are incomplete (unless youre using something that doesnt follow standard SQL syntax) inner join tablea.XYZ = tableb.XYZ eg select * from orderdetails inner join orders on orderdetails.ordernumber = orders.ordernumber Joins are also sequential, so any join is compounded onto the result of previous joins, no need to specify a set of matching tables, its just Select * from A Join B on a.123=b.123 Join C on a.123=c.123 Join D on b.456=d.456 im guessing your where clause is the ordernumber so that would be a)where ordernumber = '10425' b)where ordernumber like '%10425%' you dont really want to use IN for a single criteria where clause
This makes sense for your use cases then. Pandas is more ideal if you're predominantly on the analytics end (as are R dataframes). I'm a Data Engineer.. so I'm mostly one of those guys that does the data munging and ETL work with SQL/Spark before it ever gets to the Data Scientists. So.. for me Pandas would be the wrong tool the vast majority of time. However, I'm trying to go the route of Data Scientist... so I have actually been using Pandas more frequently as of late. Mostly for the last-mile analytics and getting my datasets "just so" for plotting and models etc. 
yes just include a match on equal PKs in the ON condition
Excellent thank you for the help!
Might be off, but it looks like you're missing an IP in your httpd.conf. even something like localhost or 0.0.0.0
Sure!
Great answer right here. Honestly for me, I just take a step back from diagrams and labels and such for a little, and just think about it in the real world. I try to relate it to a situation in life I’ve been through or could easily imagine and start to design it from there. Obviously once the rough design is down, I’ll go over it from a tech perspective and make things proper. Good luck 
Are forbidden from Google? What have you tried? Have you created the course and professor tables yet?
Hi, I have tried to google solutions lol but none have worked. I have created the professor and course tables. I have tried: CREATE TABLE Teach ( TeachID CHAR(10) PRIMARY KEY, FOREIGN KEY (CourseID) REFERENCES Course, FOREIGN KEY (ProfessorID) REFERENCES Professor;
Post your code and post the errors you're getting. 
Basically you want to do a GROUP BY query. Grouping on the booking ID. Then use an expression like Max(case when Action = 'crew signed' then 'yes' else 'no' end) as crew_signed
BTW another word for what you are doing is "pivot". You can Google for that. Some DBs have a pivot command, but not many. 
BTW another word for what you are doing is "pivot". You can Google for that. Some DBs have a pivot command, but not many. 
Did you try LAG?
I can’t figure out how to post the code sorry but the error says: ERROR 1072 (42000): Key Column ‘CourseID’ doesn’t exist in table. Which I don’t understand because I know when I created the course table I included the course id field.
CourseID int, Foreign key (courseID) references course (id) For example
It's not referring to the course table in that error. It's referring to your new table. The foreign key constraint is defined separately from the column itself.
Did you click "install as a service"? Do you have windows firewall on? Have you tried disabling it then start and see if it's the firewall preventing it from listening to that port? If while shut off you type at the cmd line Netstat -a Is anything running on port 4040 or 4443?
Thank you so much, that worked! Have an awesome night!
That is exactly what I wanted, not sure why I didn't think of it as a Max before, I will do a little bit of research on Pivoting, it seems like the version of SQL Server we are on supports it &amp;#x200B; Thanks!
It might also be helpful to confirm which specific library you're using to connect to the database. Is this using the node-mysql library? Assuming that this is node-mysql, what you have looks fine to me. You are using placeholders to represent variables (the question mark), which is good, since it ensures that the library will automatically escape your query variables to prevent SQL injection. I don't see anything here that I'd consider concatenation. I'm a bit frustrated on your behalf that you got this feedback in this manner, since it just seems incorrect unless I'm missing something. Is it possible that the person that gave you feedback doesn't normally deal with JavaScript? Some languages implement placeholders/bindings a little bit differently, so maybe he misinterpreted this code since he's not familiar with reading JavaScript, or maybe he was referring to a different code sample?
 CREATE TABLE teaches (teachID char(10), courseID char(10), CONSTRAINT PRIMARY KEY teachID, CONSTRAINT FOREIGN KEY courseID REFERENCES courses(courseID));
He is using placeholders to represent the entire where clause. I dont understand this framework at all, but that looks suspicious to me. i would expect to see something like column = ?
some version of this is probably the answer. A) I would shut everything down then right click on the icon and run the xampp console as an administrator. B) Click on services and make sure that both apache and mysql are setup to run as services making sure that any windows firewall prompts that appear you click yes to. &amp;#x200B; Your configs look fine I don't think its the IP issue since your config is bound to localhost
Thanks for your help!
I've rarely used node-mysql myself, but it looks like the library recognizes an object as field/value pairs and it automatically builds the where clause and escapes the provided values. I usually see arrays used as placeholders in docs I've found, but I can see using an object in this way might be a useful feature. I just put together some sample code to print the SQL that gets generated using this syntax and to confirm if it properly escapes an input that has single quotes in it: &amp;#x200B; `connection.connect();` `var userInput = \`O'Reilly\`;` `var query = connection.query('SELECT id, name FROM test WHERE ?', { name: userInput }, function (error, results, fields) {` `console.log('Name: ' + results[0].name);` `});` `console.log(query.sql);` &amp;#x200B; This outputs: &amp;#x200B; &gt;SELECT id, name FROM test WHERE \`name\` = 'O\\'Reilly' &gt; &gt;Name: O'Reilly &amp;#x200B; This looks OK to me. The SQL that gets built properly escapes the value to prevent SQL injection. The following test code that uses concatenation fails with a user input that includes single quotes: &amp;#x200B; `var userInput = \`O'Reilly\`;` `var query = connection.query("SELECT id, name FROM test WHERE name = '" + userInput + "'", function (error, results, fields) {` `console.log('Name: ' + results[0].name);` `});` &amp;#x200B; outputs: &gt;SELECT id, name FROM test WHERE name = 'O'Reilly' &gt; &gt;/path/to/Parser.js:80 &gt; &gt;throw err; // Rethrow non-MySQL errors &amp;#x200B; It looks like the library is doing what it's supposed to as opposed to a concatenation method that wouldn't escape the value.
*Wooo* It's your **5th Cakeday** ichp! ^(hug)
Thanks for your help!
Is your question how you can enter the specific time stamp into your code and have oracle treat is as a time stamp? If so then the to_date formula is a good choice and you could write it as TO_DATE('2019/01/01 13:00:00', 'YYYY/MM/DD HH24:MI:SS'). I use to_date to input dates into my query’s (for example when I want to limit results to a narrow time window) and I would use to_char to change dates in my data into a more readable format. 
UPDATE: I managed to get the results I want by following this process (but there may be a much better way to do it?) Created the Hour array above: with hours as (SELECT date_trunc('hour', hour::timestamp) as date FROM generate_series(current_timestamp - interval '14 days' , current_timestamp , interval '1 hour') hour), Then Created a List of All items sold in the window under analysis sales as (SELECT distinct item FROM sales WHERE sale_date &gt; (extract(epoch from now()) - (30 * 25 * 60 * 60))), Then created an empty cartesian product array of the two because these are ALL the rows I want... empt as (select * FROM hours CROSS JOIN sales), From there I just joined my hourly aggregated sales data on item and date. Anything I could improve?
W3Schools will probably give you the meat one the bones you need after that, you should be able to blag any interview. Once in the role and you know what task need doing Google is your friend. I dunno what else to tell you, everyone asks for 3 years of exp and none of them know what 3 years exp looks like. 
This is the best answer, but it's often helpful to give an example. In this case you might link to an article showing the lag function in action (like the below): [https://www.mssqltips.com/sqlservertip/3468/sql-servers-lag-and-lead-functions-to-help-identify-date-differences/](https://www.mssqltips.com/sqlservertip/3468/sql-servers-lag-and-lead-functions-to-help-identify-date-differences/)
Thx you for the response kind sir, I’ll get into your recommendation for today. Thanks!
Select one row per pid and group concat the sid, use that as your from table to group concat the pid for each sid set, grouping by the sid set. 
Shivers down my spine. Who in their right mind would shoot themselves in the foot so badly by reducing where conditions so much and name things so unclear?
&gt; Just need to decide then if I will accommodate for users being associated to multiple companies or not This should be coming from the business. At least in an email if not in a spec document, because if you make assumptions they can bite you in the ass. 
Why
True and create a whole lot more work in the future
I think the problem is that you are replacing the whole where clause with a variable. meaning this variable can come for example as "1=1" this will just give you the whole data without filtering - a classic SQL injection problem.
Do your own hw lmao at least try lazy 
Its either ETL Developer, BI Developer, or Data Engineer for this kind of role.
Despite my own opinions on the javascript model, and being more on the DBA side (pure SQL), apis that hide functionality but do its own thing are the only way to prevent (more like mitigate) developers from adding sql injections. &amp;#x200B; I would agree with you, however, that implicit column matching is unclear (you give away expressiveness to type less), specially in this case, where you haven't got ridden completely of SQL. &amp;#x200B; In fact, by looking at node.js documentation for [a random mysql connector](https://github.com/mysqljs/mysql#escaping-query-values) (apologies if that is the wrong one), I am not seeing support for op's syntax, so I am unsure if it would work (I only see literal and id escaping, not full where clauses. &amp;#x200B;
The node-mysql library will automatically add quotes around each value defined in the object and will escape single quotes in the values. If the value of msku was set to 1=1 via some user input in this example then then: `... WHERE ?` would get replaced with: `... WHERE \`SellerSKU\` = '1=1'` This would literally search for a value of "1=1" since it's wrapped in quotes by the library, so it should be safe.
What naming are you referring to specifically?
How about with persons(personid) as ( select distinct personid from expertise) select p1.personid, p2.personid from persons as p1 inner join persons as p2 on p1.personid &lt;&gt; p2.personid where not exists (select skillid from expertise where personid = p2.personid except select skillid from expertise where personid = p1.personid);
TO_DATE is used, as the name suggests, to convert other types (normally strings) to the date datatype. TO_CHAR is used to convert various other datatypes to character types. If you are trying to convert a date type to a particular format, you use to_char(date, format mask) e.g. to_char(sysdate, 'DD-MON-YYYY HH24:MI:SS'); In the example you provided: select to_char(sysdate, 'Dy, MM DD YYYY HH:MI:SS am') from dual; The output is: TO_CHAR(SYSDATE,'DY,MMDDYYYYHH:MI:SSAM') Tue, 03 05 2019 12:14:28 PM
Normalization through BCNF is determined through functional dependencies. It's deterministic; everyone doing normalization correctly should end up with the same set of answers. (In the real world, there might be more than one decomposition in 5NF.) And you're not "splitting" anything, as far as I can tell. 
Typo (mobile phone). Corrected
Normalization can be thought of as the removal of repetition. Where do you see repetitive fields across the tables? Can you pull those fields into their own table and add a foreign key? I see at least two opportunities to make new tables to capture repetitive fields. 
Simply for practical and sanity reasons, I would combine all the 'people' tables into one table. You then just add boolean fields for admin,staff,student and driver. &amp;#x200B; &amp;#x200B;
My rate is $150/hr.
After my first reply, I did find that this syntax is intended to be used with UPDATE/SET and it may be a bit more useful in that situations. You could do something like: var person = { name: 'captaincoherent', number: '867-5309'}; var query = connection.query('UPDATE people SET ? WHERE id=1', person, function(error, results, fields) { // stuff }); and the library will translate it to something like: UPDATE people SET `name` = 'captaincoherent', `number` = '867-5309' WHERE id=1 I can see something like that being useful in situations where you want to build dynamic SQL in different scenarios. For example, maybe the person object in this case might represent only changed that should be updated, and the fields may not always be the same. Instead of having to programmatically construct the UPDATE statement for different situations, the library can do the heavy lifting for you. Other ORMs/query builder libraries do provide similar functionality for WHERE clauses where it can also be ostensibly useful. For example, it might be helpful to build queries based on dynamic search parameters (i.e., WHERE conditions) specified in some sort of UI.
Because you can probably pass this: 'SELECT id, name FROM test WHERE ? AND ?', { name: userInput, location: [1, 2, 3] } To cause the provider to eventually create this: SELECT id, name FROM test WHERE `name` = 'userInput' AND `location` IN (1,2,3) It's perfectly fine to concatenate strings to build SQL *as long as you're not concatenating user input to do it*. [You can use something like this](https://stackoverflow.com/a/337792/696808) to build IN clauses with C#, for example. 
I restarted and it runs with no errors in xampp. So xampp is perfectly fine now. However, when I try to open phpMyAdmin I see [this](https://imgur.com/a/Mh0ryxw). I did click install as service and both are "started" in the Windows Services menu. I don't have anymore port errors now. The new issue is with connecting phpMyAdmin to xampp. If anyone can explain that error and it's solution I'd be much obliged. &amp;#x200B;
&gt; You then just add boolean fields for admin,staff,student and driver. This could get hectic pretty fast for a production system and you’d either end up with a gigantic table of isStaff, isAdmin, etc. or you’d refactor. Easier to be safe with a PersonType control table and just reference the key, that way it’s scalable. I usually try and reserve booleans for system level properties like isArchived or isActive. Just a suggestion though, everyone has their own style :) 
Fair enough. :) Although I don't think it matters much for modern databases systems unless you have millions of records. Also, PostgreSQL table inheritance would be perfect for this, although I haven't used it myself. Also depends on how expensive joins are. &amp;#x200B;
You could index off the Persons table Key and PersonType without it being expensive. The only time you’d need to join to the control table is if you wanted to grab something like the Description. I have to stay away from Postgres or I get all obsessed over it’s simple enum typing and end up using custom enum types for many control tables :D 
I don't think the worry is performance of the booleans, but rather that new roles would the require you to keep adding columns rather than just adding a row to your PersonType table. &amp;#x200B; In this case though the direction also depends on whether a single person can have multiple roles. As an instructor in college I was both staff and student and I was able to give feedback to other students. If that is also the case in this setup then your booleans actually can capture that without having to build a bridge table for the many to many relation and can thus easily enable you to allow people with isStaff=1 or isDriver=1 to create feedback in the application.
I'll do it for $149/hr :D The minimum isn't going to matter in this case though. How complex of an HIS (Hospital Information System) are we talking here? Most come with the complexities of Patient Accounting - and healthcare finance is getting into a whole other arena in complexity if it's in the U.S. For a decent, generically scaling data model I'll say 120 hours minimum ($17,880) which is pretty low for a HIS since these are usually considered enterprise-level software. Though, that's the minimum - I suspect many challenges and much extra cash along the way ;) ======================= Now that you know the value of this skill OP, perhaps you could pick up a book and study it yourself for your homework. It's quite a nice skill to have for your future. 
I agree although admin is a bit different since it has no name/age/id field. I guess you could create the fields with nulls allowed and then write a constraint to make sure it is non null for the other three types. You might want to similarly add a constraint that license\_no must be non null for drivers. &amp;#x200B; Regarding names I also agree with the exception that id's can make sense to hold the table names so they are easier to discern later in queries.
[user/mattmc3](https://www.reddit.com/user/mattmc3) is right! To give some more info, it looks like your model is already in 2NF (Second Normal Form). To get it into 3NF, you need to remove any attributes in a table that don't depend on the primary key. So how I think of it is that every attribute in a table must express a fact about the key. Here is one example of how your model violates 3NF: It may seem like Student\_Phone\_No depends on the primary key of Student\_TP, but it actually does not. It is a fact about the Phone Number *associated* with the Student. So to make this 3NF compliant, I would create a Phone Number table, and place a FK in the Student table that references a record in the Phone Number table. HINT: You can also put more than just Student Phone Numbers in this table. There are several other scenarios in your model that are similar to phone numbers as well. So normalize them too.
i got it to work using IIF. Here is the code. &amp;#x200B; IIF (fed\_id LIKE '%-%-%', fed\_id, NULL) AS \[Social Security No.\], IIF (fed\_id LIKE '%-%-%', NULL, fed\_id) AS \[Fed Tax Id\],
While there are good reasons in real life scenarios to do this, the assignment for this student was to normalize the data model above, and this is effectively the opposite. By combining the different people "types" you are denormalizing this model even further. I fear if he follows this approach it will result in a bad grade...
This is already normalized through 2NF (second normal form). Unless you're supposed to go through the other forms, you can stop here. This is sufficiently normalized. 
Performance isn't the problem. Changing a schema in production can result in enormous downstream impact. Imagine a database which has 2,000+ reports being generated by analysts from 50+ different groups across a company depending on the schema remaining static. You would only change it if absolutely necessary. If business needs change and a new person type is added.. you are fucked
Is 255 the actual maximum number of columns a table can hold, or is that just a hypothetical number? Thanks!
wups yeah that was a typo on that Z. I ended up getting this to work around 1AM sunday night and have basically been going full time into the rest of the project, sorry I haven't updated the thread. I ended up getting this to work using this: [https://www.experts-exchange.com/articles/2440/MS-SQL-2005-T-SQL-Techniques-Logical-BOM-Explosion.html](https://www.experts-exchange.com/articles/2440/MS-SQL-2005-T-SQL-Techniques-Logical-BOM-Explosion.html) and changing titles as necessary. I'm updating the top level post to reflect the final code i used. I appreciate the help - definitely helped me to get an understanding of what the query is doing as I tweaked the other one to do my bidding. 
I like [www.stratascratch.com](https://www.stratascratch.com). They have lecture notes and guides you can follow in addition to their platform where you can practice on a live database. Most of the questions come from interviews at companies and some come from university classes.
Yeah, it would make sense for update statements. The select statements however have to be close enough to SQL so that you could copy the query and execute it for debugging purposes with minimal manual modification.
I practiced using strata scratch ([www.stratascratch.com](https://www.stratascratch.com)). They have datasets pre-loaded with questions and answers you can practice with. They source their questions from technical interviews from companies so I found it helpful to use for interview practice. They also have a subreddit r/StrataScratch where you can ask questions if you need help. &amp;#x200B; Otherwise, I found datacamp useful too but it's more expensive.
Can someone explain why they downvoted this comment? Downvoting it without explaining why the information is not helpful within the conversation does little to help. 
Nope, the maximum is 1024 for a "non-wide" table and 30k for a wide table https://docs.microsoft.com/en-us/sql/sql-server/maximum-capacity-specifications-for-sql-server?view=sql-server-2017 The 255 limit is usually the character limit of that data within a column.. basically the data type for the column gets set to `varchar(255)`... this is a fairly common thing and you just need to fix the data type within SSIS.
I'd like to know as well. Did they disagree with how I defined normalization? Did they not like my solution? Did they think I didn't explain it well enough or not well enough?
Are there resources you used to learn Python for these tasks? Or did you learn everything on the job? Any suggestions from where I can learn these will be highly appreciated!
Hi, were there any resources you used to learn DBA tasks? Or did you learn everything on the job? Thanks!
Try ‘%’ + @title + ‘%’
Honestly, when I got started with Python it was completely out of necessity. I taught myself how to do some basic things like CSV parsing and loops with the help of Stack Overflow and some free learning sites. As the challenges grew at work I learned incrementally to fit the task at hand. Recently, I've been in a role where we heavily use Pandas so I was able to learn a lot faster on the job with the help of someone who knew a lot about the library. I think that the incremental approach to learning was more beneficial trying to learn the entire language up front.
I have no idea, but be aware that some sort of automated correction mechanism used to be in place on Reddit - and maybe still the case. 
Why do you use the max function? I’m fairly new to sql but I’ve always just used case for something like this 
I have downvoted your comment initially - then removed the downvote. As to why I downvoted - OP's model does NOT violate 3NF, 'association' means a relation and since you have a PK, a functional dependency and, furthermore, 3NF (BCNF) theoretical definition that your attributes are the same. Why I removed the downvote - it's just a common sense (one hopes) to move demographic data to a common table - and the DB design is about common sense too. That aside, the diagram as it stands currently has 2 things that bug me: a 1:1 relationship (bus &lt;-&gt; driver) and feedback entity that seem to require relationships to staff/driver/student at the same time. 
Why would you want to do that? At each iteration it would move to the next record anyway... To amswer you question, I would use a foreach loop to wrap the object insert and subsequent foreach loop. Could you offer more details on what you're trying to achieve?
I'm not an expert by any means. But I think it really depends on how you are getting the data. I use software that utilizes OCR to extract data from documents. So I will usually start with all VARCHAR fields. Then later I will attempt to validate the data and move it to data type fields that are applicable for the datas type, dates, amounts. Etc.
I appreciate your criticism and discussion. I suppose if you consider Phone Number the determinant and it has no other attributes dependent on it in that table you are correct. I am just used to the actual phone number being an attribute in a table with a surrogate key that contains other attributes such as PhoneType (Home, Cell, Business). We do this since in our business there is a many to many relationship between people and phone numbers. You are right, I might have jumped the gun a tad, however my colleagues and I would still not consider the above normalized due to these issues. 
If you want to do something with the data such as date manipulation or summing etc the field need to be the right data type for it. Even if you don’t want to now you might in the future so it’s best to get the data types set correctly from the start The basic data types are usually fairly obvious based on the data you are putting in to it, ie date, integer (no decimals), numeric (eg amounts) 
So if there's a field that have monetary amount, for example $3.50, I should remove the $ and make it into a numeric value. 
Create your own user defined type. I use about 40 of them. You asked about $3.75, create a udt called udt_Currency from decimal(20,2). When you open a table and see udt_Currency you'll know exactly what it is. Other udts I use: _PrimaryKey, ForeignKey, Name, GenericDate, GenericNumeric, JSON, DynamicSQL etc.
I would definitely 
Yes. If you need to put the currency in a separate field. So one field is decimal 3.00 another field could be string "GBP"
try using `ISNULL(Memo, '') AS Woohoo` and `GROUP BY ... Woohoo`
You either need to group by ISNULL(Memo,'') or do as /u/r3pr0b8 said and give your ISNULL(Memo,'') a unique name and then group by that unique name.
Doing that returns an error saying that Woohoo is not a column. 
If I give Memo an alias, I cannot select by that alias name. It tells me there is no column Woohoo, but I do see what you're on about. Adding in a "And Memo = '' " returns none of the items I've revalued. 
&gt; ISNULL(Memo,'' hrm, just tested and you're right seems like the unique name only works in the order by so you'll have to group by ISNULL(Memo,'') as I mentioned in my other reply.
Oh wait I see what you're saying. Grouping by the IsNull seems to be working. 
Yeah I just answered your response to r3pr0b8, my first suggestion will work though.
And it did, thank you!
You're welcome aaaaaand you really shouldn't delete your question, how are others going to learn from it if it's gone?
Yeah sorry, it felt foolish but then after I thought about it again I realized it wasn't so foolish and then deleting it felt foolish.
I wouldn't say 100% of the time but mostly if you're running with a database it would be rare to want to loop just about any process. You'd need to get into a pretty weird situation where you should think "maybe looping would be good."
Each data type was created for a specific purpose and may even have limitations (i.e. text) based on that purpose. A firm understanding of types and their use, size and features is going to require some study. It will really be useful in the long run.
No, if you make a joining table for the types then you haven't denormalized further. You have a user entity and a user type entity. You can add user types, which means this model can be expanded on easily. What if you need a "mechanic" entity for the busses? In the current model, you need to add another table and modify all of your relationships. With a user and type table, you add a new type.
I’m looping through a text file. 
This is what I would do `with alpha as ( select userid from usertable where campaignid = 'Alpha' ) , bravo as ( select userid from usertable where campaignid = 'Bravo' ) , charlie as ( select userid from usertable where campaignid = 'Charlie' ) select count(distinct userid) as n_users from usertable u left join alpha a on a.userid = u.userid left join bravo b on b.userid = u.userid left join charlie c on c.userid = u.userid where (a.userid is not null and c.userid is not null) or (b.userid is not null and c.userid is not null)`
P_Code is the primary key in the Patient table. D_Code is the primary key in the Doctor table. P_Code and D_Code are both foreign keys in the Operation and Is_Examined_by table. To get the answer, use a union and a couple joins to get your dataset.
 select clientid , firstname , lastname , status , status_start_date , lead(status_start_date) over (parition by clientid order by status_start_date) as status_end_date , assigned_staff , changes_made_by , date_diff('hour', status_start_date, lead(status_start_date) over (parition by clientid order by status_start_date)) as hours_in_status from tablename
I didn't downvote you, but phone numbers are attributes, not entity classes. So they don't need their own table.
Here's the way to do it, I think HIve supports both CTEs and correlated subqueries so you should be fine: ;WITH userCampaign AS ( SELECT ut.userid, ut.campaign FROM usertable ut WHERE ut.date &gt;= 20190101 AND ut.date &lt;= 20190131 AND ut.campaignid IN ('Alpha', 'Bravo', 'Charlie') ) SELECT COUNT(DISTINCT uc.userid) FROM userCampaign uc WHERE uc.campaignid in ('Alpha', 'Bravo') AND EXISTS (SELECT 1 FROM userCampaign sub WHERE sub.userid = uc.userid AND uc = 'Charlie')
Because the OP wanted to reduce the number of results to one row per booking. CASE by itself is not an aggregate function. To meet the requirements, you need to apply an aggregate function (MIN, MAX, AVG, SUM, COUNT, etc.) to everything but the booking ID. Could have used MIN maybe but MAX made more sense given the values yes/no.
Use INTERSECT? Select userid From table Where alpha Intersect Select userid From table Where bravo Intersect Select userid From table Where charlie
I think you want UNNEST (the function). I use cross join unnest to flatten arrays in bq. Also, you may want to cross post in r/bigquery. 
Interesting. Does the optimizer reduce it to it's base type and then use it? 
Here is another approach Select count(*) from ( Select Userid , Max(case when campaign = 'alpha' then 1 else 0 end) alpha , Max(case when campaign = 'bravo' then 1 else 0 end) bravo , Max(case when campaign = 'charlie' then 1 else 0 end) Charlie From usertable Where ... Group by Userid ) t Where t.charlie = 1 And t.alpha + t.bravo &gt;= 1
What exactly are you doing? Maybe there is a better way to do it.
Be adaptable the field is continuously changing so hard to say I would have done anything different. Stay current on versions and technology and be the person that the company looks to for what will be the next way to improve the data they have. Data is power and knowledge knowing how to make it work for you is half the battle. 
Omg, thank you so much but it’s a long process with several procs involve. I have two files, one is a text file and the other is an excel file. The text file has one record (date) and the other ID’s. I’m loading the ID’s into a staging table. Loading the date file record (date) into an object type variable and then passing it to a stored proc with the prefix of the file name of the excel. The proc generates a report based on those inputs. I’m also passing the prefix to another stored proc to load records related to that prefix into a staging table. I’m then loading that report into a csv file location. 
If you can do SQL well, you should be okay switching platforms (SQL Server Oracle etc). I'd suggest getting a copy of Adventure Works, the free Microsoft test db, spun up on your local machine. It's small enough that you should be fine with the express (free) version. Try some basic reporting stored procedures. You pass in a date and it spits out all of the sales for that day. Next give it an order number and have it spit out an invoice. 
Just from my pov, stored procedures themselves aren’t complicated. They can be if they’re doing complicated things, but the “stored procedure itself” isn’t. You could use them as a view with parameters for instance. I wouldn’t worry about the flavor of sql you’re learning. Each has its own tricks but the logic of joins and the order with which clauses execute is what’s important. I’m not a sql first guy so take this with a grain of salt. 
im a beginner in SQL, my class just got into it a week ago and i missed 2 classes, so i completely missed the basics, i made a database for this but idk how to make a diagram with relations like that
This may help: https://dataedo.com/kb/tools/ssms/create-database-diagram 
i think it's required that i know how to code it, not make one by dragging, i think
Hmmm, that I haven’t done. =[
it saids it will create a table with relationships and primary, that's what im having trouble with :D, i forgot how to set relationships between tables
Try these: https://www.w3schools.com/sql/sql_foreignkey.asp https://www.quackit.com/sql_server/sql_server_2017/tutorial/create_a_relationship_in_sql_server_2017.cfm 
I'm not following. 
For your grid, is it representative of the original example? I'm not sure how you're coming up with some of these hours if it is. 
What are" Types of Join" for $200 Dollars 
What kind of text file?
You use the most reactive type possible that can still hold the data. With some exceptions, such as Zip codes are not numbers, phone numbers are not numbers, that kind of stuff. If you are storing a date then use a date type. If your storing a single char like gender then make it a char(1). Just be mindful of future changes so you don't need to do a lot of rework. The better you type your data the better integrity your data will have. 
&gt;I aspect result to be for id 10 active\_hours : (11:30 - 8:00)+(15:00-13:00) &gt; &gt;for id 20 active\_hours : (11:40 - 7:30)+(14:00-12:00) &gt; &gt;for id 30 active\_hours : (14:00 - 11:00)+(08:00-07:00) &amp;#x200B;
Using appropriate data types can have huge implications if this is used in a large corporate database - if it is just a small database to help in a personal project of course the implications won't be as big. But a few of the benefits of using correct datatypes: &amp;#x200B; * It saves space. A datetime2(0) takes 6 bytes and the same date stored in a varchar(19) would take 19 bytes. This might seem small but in large databases it can mean that you can store your table in ram in advanced queries or if you will spill to tempdb which will slow your query a ton. If you have very large rows (lots of columns with long varchars) you also get limited on which indexes can be created (max 1700 bytes per row in non clustered index) and of course both the table itself and the indexes will take more space on your disks. I once cut a legacy table from 1200mb on disk to 120mb just by going over the datatypes. This has a lot of performance implications. * It gives functionality. If you have '2018-01-01 00:00:00' as a varchar and you want to see if it is a sunday or monday you have to create your own function or cast it to date or datetime to use the built in date functions. But if you are gonna cast it to date for all operations why not just store it as date. For a datetime you can use for example: `SELECT FORMAT(@date, 'dddd', 'en-US') AS 'en-US'` to get the english name for the date. You can also utilize functions like AT TIME ZONE to see what the local time would be elsewhere, DATEADD and DATEDIFF to manipulate the date and find durations and many other functions as well. * It gives useful hints to SQL Server which creates better plans and gives better performance. If you have the varchar '2018-12-31 23:59:59' and SQL Server needs to estimate how many strings are between that and the varchar '2019-01-01 00:00:00' it has a much harder time than if those were declared as dates. If you know they are dates the gap is very small - if you think they are random strings the gap is very large. * It makes intentions clear. When another developer takes over your database down the line they'll appreciate any meta data hints on what is stored in any given field. It will significantly reduce the time they need to get a feel for the tables. There is probably a lot of stuff I missed in this quick overview but there really is tons of good reasons to declare the right datatypes. This doesn't mean you should spend years worrying if you need a varchar(8) or a varchar(10) but at least get the types right and use date, datetime2, bit, int and numeric where appropriate. If you then want to go further you can look into smallint, tinyint, char vs varchar, varchar vs nvarchar, datetimeoffset, money etc.
I like Strata Scratch ([www.stratascratch.com](https://www.stratascratch.com)) the best for practicing SQL. The questions come from technical interviews taken from companies so all the questions are relevant to working as a beginner. You can also post for SQL help on their subreddit r/stratascratch. I found it really helpful. I also like sqlzoo, datacamp, hackerrank, and leetcode. Tried them all and out of those 4, I think leetcode was the most helpful because they also have a discussion board to get help from others. Datacamp was cool for very specific niches.
I practiced using strata scratch ([www.stratascratch.com](https://www.stratascratch.com)). They have datasets pre-loaded with questions and answers you can practice with. They source their questions from technical interviews from companies so I found it helpful for advance level. They also have a subreddit r/StrataScratch where you can ask questions if you need help with SQL. Otherwise, I found datacamp useful too but it's more expensive.
It is still highly unlikely that a loop will create the best throughput for your operation. Databases are very well designed for multiple simultaneous interaction. Looping is adding large layers of cost on top of a potentially simple process by adding the highest cost layer on repeatedly, table qualification.
I think I'm more confused now based on where you're determining which hours are the correct ones to choose for each ID. Could you explain further in detail how you determined why you chose each of the times for each of the IDs?
Google "foreign keys" Also, don't get used to that diagram. The feature has been removed from the latest SSMS.
Yes. The optimizer uses the system type the user defined type is based upon.
Thx? Idk
It’s a comma delimited file. 
Either/or: 1) write a loop in whatever flavor sql you're working in. Or create an external piece of code (python, etc.) with a loop 2) modify your statement in such a way that you can supply several date points or a date range and produce a result set equivalent of running your statement multiple times with varying parameters
&gt; Also, don't get used to that diagram. The feature has been removed from the latest SSMS. Haha.
I send myself a graph on Slack every day that is based on running an R script that wraps around a SQL query. In R (and I’m guessing other languages) there is a command that you can manipulate to get the date when the script runs and dates relative to it (sys.Date()). Then you’d pass that date into a stored procedure or a paramaterized query and boom. We have a super robust task scheduler here but I prototyped this just using windows task manager. My script runs at like 427 every morning. 
I'm only generally following a lot of this thread, but are you saying that in this case the OP was right and the potential employer didn't know what they were talking about?
SQL Agent + stored sproc with a date parameter. You can use msdb.dbo.sp\_send\_dbmail and pass the output of your query to the @body parameter. &amp;#x200B;
This is what I was thinking too 
I would use an agent, but maybe create a data driven solution with that. Basically, you want to run a procedure, but with varying paramters. The simplest explanation I could give would be a few tables. One to hold the procedureId, one to relate a procedureid to parameter, one to take the procparam key and relate it to a variable table, and lastly a variable table to a type. The variabletype table would function as a way to run a function with an output. One variabletype could be fncFirstOfThisMonth(), another fncFirstOfThisMonth_NextYear(), another one could be fncCurrentCompanyForSession() Then you would have a neat,reusable,datadriven solution that can pass variables to procedures. And the relationship table would tell you the sequence to build a dynamic sql statement.
this.
You can't. SQL isn't made for pivoting. Postgresql in particular is really picky on that part, as every result has to be of a defined row type. You can only get dynamic output if you dynamically create that type and then run a query that fills that data container. Pivot tables is for OLAP cubes and reporting tools.
Pivot is relatively specific operation in SQL (converting a number of records to a number of columns). I think (looking at your first picture) that what you're looking for is 'group by' clause and some formula to create your buckets ( trunc(clippedtext * 10) seems to be able to get you 75% there)
Postgres is not great for pivoting. If it's possible if you have a fixed and known number of columns in your desired output. If your source table has say people's names, genders, favorite meal of the day, and how much they spend on that meal on average. Maybe you wanna find out per gender, how much in total was spent on breakfast lunch and dinner. You can do: Select gender, sum(case when meal = 'Breakfast' then amount else 0 end) as b_total, sum(case when meal = 'Lunch' then amount else 0 end) as l_total, sum(case when meal = 'Dinner' then amount else 0 end) as d_total From source Group by 1
Grafana! fast to install, really easy and pretty.
Are you trying to automate the results one time or is this something you want to run each month? Your title and body are a bit contradictory... For building the history you can write a while loop and change your date parameters programmatically using the dateadd function. . For an ongoing extract use a scheduled job 
What if you create an ETL package and the push the package to SQL server for automation via a sql agent job? Set a start month and year in SSIS as a variable and an end month and year as well. 
I'm building back the history but will need to do this every month going forward as well
Not the prettiest, but I would do: Select a.customer, a.spend_usd, spend_usd * ( select usdxfc from [exchange_rate_table] where currency = EUR)..... From [customer_spend_table] And the same for the other currencies. Obviously not ideal If you have many currencies though
could you use CROSS JOIN
You can PIVOT the currency table and then join it to the first table.
If you can avoid using a trigger avoid using it. The reason behind this is that as the codebase grows your trigger will be forgotten, then will become black magic making unexpected things happen. 
If there are a fixed number of currencies you could throw them in variables in the beginning to clean it up. DECLARE @curX_EUR AS FLOAT(5) DECLARE @curX_CAN AS FLOAT(5) SELECT curX_EUR = USDFXC FROM tbl_CurrencyX WHERE Currency = 'EUR' SELECT curX_CAN = USDFXC FROM tbl_CurrencyX WHERE Currency = 'CAN' SELECT Customer ,Spend_US ,(Spend_US * @curX_EUR) AS [Spend_EU] ,(Spend_US * @curX_CAN) AS [Spend_CND] FROM tbl_Customer This would give you exactly the output you describe above, but you have to know each currency ahead of time and update if you add more. ------------------- You could also just return multiple lines dynamically, and then pivot or whatever you're most comfortable with to get single lines (if you need them). SELECT tbl_Customer.Customer ,tbl_Customer.Spend_US ,(tbl_Customer.Spend_US * tbl_CurrencyX.USDFXC) AS [Spend_Other] ,tbl_CurrencyX.Currency FROM tbl_Customer ,tbl_CurrencyX This will give you something like: | Customer | Spend_US | Spend_Other | Currency | | :--- | :--- | :--- | :--- | | ABC | 100.32 | 116.67 | EUR | | ABC | 100.32 | 88.27 | CND | Then PIVOT to single lines is you want or do whatever you need. This will automatically shrink and grow as currencies are added/removed.
Hah, wait till you have to take care of dates. As in, what was the USDEUR exchange rate on 2016-01-05 when the customer made the purchase?
I would advise against implicit joins like this. (Column delimited froms) Outer apply, cross join, cross apply would produce the same result. Neither of the aforementioned joins support ON statements and produce the cartesian an implicit join does.
&gt; 9 joins seems ugly. That's because you're trying to denormalize your data with your query. Remember that first normal form says "no repeating groups"? That's exactly what your spend columns are. The intended way to do it is to just do this: SELECT c.Customer ,c.Spend_US ,s.Currency ,c.Spend_US * x.USDXFC AS "Spend_Converted" FROM CustomerSet c CROSS JOIN ConverstionSet x ORDER BY c.Customer ,c.Spend_US ,s.Currency And then your application needs to handle putting the data in the appropriate location for display. If you absolutely want to do it all with SQL, you'll either need a shitload of JOINs or you'll need to use the moderately arcane and not particularly flexible PIVOT syntax. If you absolutely need it to be maximally flexible, you'll need to use dynamic SQL to create the PIVOT query, probably in stored procedure, which is a *real* pain in the ass. What you're trying to do is make the database denormalize your data for you. It doesn't want to do that. The language of SQL is written so that doing that is hard because -- with the exception simplifying data display for reporting -- it's a bad idea to do it. The take away here is: **Don't break first normal form** and expect things to go smoothly. 
Ezpz. A unit of measure taxonomy would allow you to convert USD to millimole/deciliter if you had a conversion factor. As currency fluctuates you wind up reusing the conversions (typeid). But... It was a nightmare just thinking about it before I learned how to do it.
I had success by pivoting the second table, then joining to the first: &amp;#x200B; `-- SQLServer` `--CTE to build sample table` `with Customer as (` `select *` `from ( values ('ABC', 100.32)) v (Customer, Spend_US)` `)` `, Currency as (` `select *` `from ( values ('EUR', 1.16), ('CND', .8673661)) v (Currency, USDXFC)` `)` &amp;#x200B; `select *` `from Customer c` `join` `(` `--pivot the values in Currency table` `select *` `from Currency` `pivot` `(` `max(USDXFC)` `for Currency in ([EUR], [CND]) --list 'column' values` `) p` `) cPivot` `on 1=1` `where 1=1`
I had success by pivoting the second table and joining to the first, either with a cross join or a an inner on 1=1. &amp;#x200B; \-- SQLServer \--CTE to build sample table with Customer as ( select \* from ( values ('ABC', 100.32)) v (Customer, Spend\_US) ) , Currency as ( select \* from ( values ('EUR', 1.16), ('CND', .8673661)) v (Currency, USDXFC) ) &amp;#x200B; select \* from Customer c join ( \--pivot the values in Currency table select \* from Currency pivot ( max(USDXFC) for Currency in (\[EUR\], \[CND\]) --list 'column' values ) p ) cPivot on 1=1 where 1=1
you could use a subquery or a cte to rename the columns and then join on the new column names to find the conversion factor.
 select us.customer ,us.spend_us ,sum(case when xfc.currency = 'EUR' then us.spend_us*xfc.usdxfc else 0 end) as Spend_EU ,sum(case when xfc.currency = 'CND' then us.spend_us*xfc.usdxfc else 0 end) as Spend_CND from table_with_cust_and_spend_us us cross join table_with_currency_and_USDXFC xfc group by customer ,spend_us
You'll need to do some aggregates and a group by most likely. Do you have any sample data we can look at?
https://imgur.com/a/IJp8SyE https://imgur.com/a/uPm75cv
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/zaAxw4t.png** **https://i.imgur.com/4KainXA.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) 
With the fields you've listed so far in your question, you don't have enough data to create "percentage of population in capital". Can you list all the tables and fields available do you? Also, this smells like a homework question, so make sure you are "getting it" instead of just copying an answer if you do get one.
 BEGIN select artist, title from items where title LIKE '%' + @title + '%' return END &amp;#x200B;
Hi, you are right, that's why in my post i said any help is appreciated, pointers or tips on what i'm missing. Having a hard time figuring out how to join data values. https://imgur.com/a/5sylX5c That is the entire dataset.
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/Q1gv6CT.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20ehxxrsx) 
pivot first, join later or join first, pivot later - that's pretty much your choices (see /u/doctorzoom comment, for example). Oh, the keyword PIVOT is just a syntax sugar (a helpful shortcut, in other words). You can do pivot (the operation) in any flavor of sql that allows CASE (decode) and Group by.
Cool. Here is the answer, starting with some hints if you want to try to work through it yourself: Hint 1: You'll need to join two tables together and use a bit of each in your select statement. Hint 2: Join the tables country and city Hint 3: join them using country.capital = city.name Hint 4: in your select, you can do city.population/country.population to get percentage of population in the capital. Full SQL: select country.name ,country.capital ,city.population/country.population as capital_percentage from country join city on country.capital = city.name
&gt; How do I join the two tables and to translate everything across the column run a simple join yes, that will give you multiple rows per item alternatively, run two separate queries, one for the item, then another for its transactions the important point is that you **assemble the data into presentation format in the front end application** this is something that should be taught in every database 101 course
I automate queries using a Python script to connect/execute, then create a scheduled Jenkins job to run the Python script as needed.
In my experience, people with a lot of excel experience have a harder time initially learning to write good SQL than people without - mainly because writing good SQL requires a decent grasp of set theory. Spreadsheet gurus have pivot tables and vlookups running through their brains, but relational databases use grouping and joins to solve these same problems. &amp;#x200B; I could be wrong, but your starting point here looks like it might not be the real starting point. When you're solving this sort of problem, start with your source data, then ask yourself these questions: \- What are the dimensions? What are the facts? \- What is the grain of this source data? (What does one row represent?) \- What is the grain of the result I'm trying to achieve? (How will each row be unique?) &amp;#x200B; Once you've answered these questions, you'll be able to start constructing the queries that get you from your starting point to your end point. If it requires a pivot in excel, the solution will probably require a GROUP BY and some aggregate functions (at minimum), and may require some nested subqueries, joins, or even a windowed aggregate. If you don't know what some of these basic terms here mean, you've got a little bit of reading to do before you'll be able to approach the problem in a meaningful way. Hope that helps, and good luck.
Hi, thanks so much for the tips, i had to multiply the division by 100 to get the percentage also, i did an sql check and it seems to be working :)
And that's why you always have a date table. (And some sort of transaction tracking / historical review process. Like change tracking, temporal tables, slowly changing dimensions type 2, etc.)
If you don't know the currency list ahead of time, you won't be able to put together this line: `for Currency in ([EUR], [CND])` In those cases I'd resort to dynamic SQL: -- SQLServer DROP TABLE IF EXISTS #Customer; DROP TABLE IF EXISTS #Currency; SELECT * INTO #Customer FROM (VALUES ('ABC', 100.32)) v (Customer, Spend_US); SELECT * INTO #Currency FROM (VALUES ('EUR', 1.16), ('CND', .8673661)) v (Currency, USDXFC); DECLARE @cCur AS NVARCHAR(MAX), @cProd AS NVARCHAR(MAX); SELECT -- for pivot IN clause @cCur = STUFF(( SELECT DISTINCT ', [' + C.Currency + ']' FROM #Currency C FOR XML PATH(''), TYPE ).value('.', 'NVARCHAR(MAX)'), 1, 2, ' '), -- for SELECT clause @cProd = STUFF(( SELECT DISTINCT ', cPivot.[' + C.Currency + '] * c.[Spend_US] AS [Spend_' + C.Currency + ']' FROM #Currency C FOR XML PATH(''), TYPE ).value('.', 'NVARCHAR(MAX)'), 1, 2, ' '); -- Build dynamic SQL DECLARE @cSQL NVARCHAR(MAX) = N' SELECT c.*, ' + @cProd + N' FROM #Customer c INNER JOIN ( SELECT * FROM #Currency PIVOT ( MAX(USDXFC) FOR Currency IN (' + @cCur + N') ) p ) cPivot ON 1=1 WHERE 1=1'; EXEC sys.sp_executesql @cSQL; &amp;#x200B; &amp;#x200B;
Search Amazon for Itzik Ben-Gan and see if any of his books interest you.
A reason for normalization is to remove anomalies. Take a look at BCNF or 3rd nf. You are basically looking at a Triangle relationship. Retailer to product Retailer to crumb Crumb to product This allows you to start with any one subject and move to another or all. Having a table for crumbs, retailers, and products that are unique and creating relationships between them in a separate entity is probably what you are looking for You can pivot data post join. You can connect some or all of those 3 subjects. Depending on you pivot, a duplicated product (due to a join) will flatten out. The more atomic (nornalized) your data becomes, the easier it is to include and exclude properties.
This needs to be done in SQL. The presentation layer is exploding because of how large the data set is.
For this it's not relevant and already being done with the most up to date exchange rates.
Is DBM at an online college?
You're looking for pivot
Cross join
stored procedures and functions would be good place to focus. In real time scenarios , you might need to create these more often rather than single SQL statement . you can pick a project yourself , define requirement and starting writing procedures. Also learning some ETL tool like SSIS or Informatica would add value . 
What database system are you using? There's quirks to everyone's SQL. It might be a matter of just dropping the `()` after `moveReleaseDate` ... that doesn't make any sense.
It's Microsoft SQL Server 2017, I should've put it in the question.
When I was learning, I made it a point to move as much of my data from excel spreadsheets into a database as possible. Then I used SQL for as much of the searching and data manipulation as possible, only using excel as a medium for graphical representations of the data or for sending reports out. Its incredible how much you learn when you force yourself to learn how to manipulate the data yourself instead of relying on excel. Joins came first because I no longer had vlookup, then stored procedures, functions, and views, triggers, etc. One of the most difficult things about learning more advanced functionality is knowing how to ask questions. SQL makes this easy in my opinion. The first time I ever needed to get data from two different tables based, I googled "SQL joining two tables" - I had never even heard of a join at the time. I'm not entirely sure exactly what your skillset is. But for me at least, the way I learned more is by forcing myself to use it as much as possible. I'm also not the type of person to sit down and read a book on a language, so take it for what you will.
Double quotes don’t belong in T-SQL. Always use single quotes for varchar values.
Replace @d with your DATE. &gt;select datename(month, @d) + ', ' + datename(day, @d) + ' ' + datename(year, @d) &amp;#x200B;
If it's any help, the dates look like Friday, October 14, 2019, in the output table she gave in the assignment
I second the comment about itzik ben-gan. Particularly the exam ref for Microsoft 70-761. It reads like a book, easy to utilize, and will teach you every trick. Its like a $1000 giftcard to home depot
SELECT FORMAT(MovieReleaseDate, 'D', 'en-US' ) 
Thanks so much! That worked awesome. 
I'd never touch this again. PowerShell script to run the query and email formatted results, Task Scheduler to run monthly. Whether it's a one-time thing or will continue, this is the answer. Put a variable in the PS script to ask for the date to get those previous months. Tell me more, on PM if you want. I do this all day. 
That's essentially due to how an alias works. You could replicate the thread CASE statement throughout the query to see it 'work'. You'll need either A) CTE, B) Temp table(s), or C) a sub-query. Perhaps there is another column in the tblPhonebook and tblChatters tables which link through to tblMsgs outside of thread? 
I'll look into getting it.
This is great advice, I will save this and look into it.
Yeah, I *just* fall at an intermediate skill level of the language and I want to advance myself more. I feel like I'm just at a roadblock with what I know and I want to challenge myself more with more advanced and bigger queries. &amp;#x200B; I know how to use Joins, Functions, subqueries, and about all clauses. It's just I have a hard time taking multiple different routes to manipulating data. Where you Do a SELECT statement, a few subqueries and maybe a few conditions. &amp;#x200B; But then when you want me to go past that and add more then that, like data type conversions or a throwing in an expression with a CASE statement. I get a bit lost in where to apply it or what syntax will fit where. I just also need more data to work with too. With me being in School I find it hard to have time to make my own data or mess with my Database at home.
The problem is that I have to determine on the fly in the query whether the "msgSender" or "msgRecipient" is the appropriate field. If I am Person A, I want to be able to look through tblMsgs and find all the "threads" or "conversations" that I have with other people like Person B or Person C. These 3 people share a message table, with varying msgSender and msgRecipient depending on who sends and receives what. That is why I believe I have to use the CASE.
I hear you - it's difficult to learn by doing, when you have nothing to do. Take a look at your own lifestyle for inspiration. One of the first applications I made was a simple expense tracker. Made a database with a few tables, and whenever I spent money I recorded it in there. It started out as simply balancing my account with some basic math, but as I learned it expanded to tracking reoccurring / monthly expenses and subscriptions, projecting my portfolio growth, generating monthly budgets based off of defined costs etc. I used to be absolutely terrible with money (completely trashed my credit when I was between 18 and 21 years old). So not only did I learn some great SQL skills, I learned how to write a simple app in .NET for data entry and reporting, and it actually helped me get my finances in order.
This worked perfectly! Appreciate the response and insight here! Learned my something new for the day :) 
Yeah, that’s my takeaway. It does look there was an unrelated issue with the code sample, but I don’t think the feedback from the potential employer had any merit.
Are you wanting to manually export and than import the resulting files into R? If so, MariaDB would work. Off hand I'm not sure if R can natively connect to it. SQL Server can run with R if you want to set it up to be seamless.
I downloaded mysql and installed the mysql workbench. For some reason I can't connect to it. Could it be that I installed a XAMP package for a sql tutorial ? Can't sql export manipulated data in a cab file format? 
Microsoft has some free sample databases available for download - World Wide Importers is the most recent one I think. 
A couple things to keep in mind: the SELECT clause is for columns (which fields you want the query to produce) and the WHERE clause is for rows (what records you want the query to output). These are (somewhat confusingly) referred to as Projection and Selection, respectively. The other thing is that when you use the IN operator with a subquery, the query must return only one column, and that column’s type should probably be the same as the field being IN’d
I was thinking of just importing a very neat and "clean" csv file to a SQL server then mess with it, i.e. inner join, outer join, sort, blah blah. Then export it as a csv file. &amp;#x200B; I mainly want to get experience with SQL but also learn the ins and outs of mySQL as well.
In that case, grab the open source MariaDB. It is practically identical to MySQL from a usage perspective (not 100% sure on DBA side) and I've yet to find or see a difference in SQL syntax between the two, although my usage is relatively limited in the grand scheme of things.
This is from my cheat sheet: DATES Ran on 28 JAN 2016: SELECT CONVERT(VARCHAR(20),GETDATE(),1) 01/28/16 SELECT CONVERT(VARCHAR(20),GETDATE(),2) 16.01.28 SELECT CONVERT(VARCHAR(20),GETDATE(),3) 28/01/16 SELECT CONVERT(VARCHAR(20),GETDATE(),4) 28.01.16 SELECT CONVERT(VARCHAR(20),GETDATE(),5) 28-01-16 SELECT CONVERT(VARCHAR(20),GETDATE(),6) 28 Jan 16 SELECT CONVERT(VARCHAR(20),GETDATE(),7) Jan 28, 16 SELECT CONVERT(VARCHAR(20),GETDATE(),8) 14:32:36 SELECT CONVERT(VARCHAR(20),GETDATE(),9) Jan 28 2016 2:32:36 SELECT CONVERT(VARCHAR(20),GETDATE(),10) 01-28-16 SELECT CONVERT(VARCHAR(20),GETDATE(),11) 16/01/28 SELECT CONVERT(VARCHAR(20),GETDATE(),12) 160128 SELECT CONVERT(VARCHAR(20),GETDATE(),13) 28 Jan 2016 14:32:36 SELECT CONVERT(VARCHAR(20),GETDATE(),14) 14:32:36:247 SELECT CONVERT(VARCHAR(20),GETDATE(),20) 2016-01-28 14:32:36 SELECT CONVERT(VARCHAR(20),GETDATE(),21) 2016-01-28 14:32:36. SELECT CONVERT(VARCHAR(20),GETDATE(),101) 01/28/2016 SELECT CONVERT(VARCHAR(20),GETDATE(),102) 2016.01.28 SELECT CONVERT(VARCHAR(20),GETDATE(),103) 28/01/2016 SELECT CONVERT(VARCHAR(20),GETDATE(),104) 28.01.2016 SELECT CONVERT(VARCHAR(20),GETDATE(),105) 28-01-2016 SELECT CONVERT(VARCHAR(20),GETDATE(),106) 28 Jan 2016 SELECT CONVERT(VARCHAR(20),GETDATE(),107) Jan 28, 2016 SELECT CONVERT(VARCHAR(20),GETDATE(),108) 14:32:36 SELECT CONVERT(VARCHAR(20),GETDATE(),109) Jan 28 2016 2:32:36 SELECT CONVERT(VARCHAR(20),GETDATE(),110) 01-28-2016 SELECT CONVERT(VARCHAR(20),GETDATE(),111) 2016/01/28 SELECT CONVERT(VARCHAR(20),GETDATE(),112) 20160128 SELECT CONVERT(VARCHAR(20),GETDATE(),113) 28 Jan 2016 14:32:36 SELECT CONVERT(VARCHAR(20),GETDATE(),114) 14:32:36:247 SELECT CONVERT(VARCHAR(20),GETDATE(),126) 2016-01-28T14:32:36. SELECT CONVERT(VARCHAR(20),GETDATE(),127) 2016-01-28T14:32:36. 
Despite having an octagon for a name October actually is the 10th month
Already said in the very first sentence that a simple join is not what I am looking for. Nor is this an experiment in being lazy with the data and to fix it on the front end programmatically. &amp;#x200B; Also not sure how a DB101 class end all be all answer is to resolve it outside of a DB environment and to do so in an front end environment. Ichp answer was much more useful.
Thanks will look into this solution early tomorrow. Will be interesting outside of classes I never really had to deal with a pivoting my tables. Guessing I been lucky so far thus well structure data modeling and warehouses :)
Is there a software that makes mariadb easy to use like mysql workbench? 
For SQLScript it would be TO_DATE(FieldName,"yyyyMMdd")
I know this is basic but is it saved as a .CSV file. CSV - Comma Separated Values Comma delimited text files not saved as a CSV is more common than I like to admit.
I did not know that one could put a 'then' in a case when statement. 
For example id 10 : 08:00 \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_11:00 09:00\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_12:00 09:30\_\_\_\_11:30 13:00\_\_\_\_\_\_\_\_\_\_\_\_15:00 because we have overlapping range i need max EndTime 12:00 - min StartTime 08:00 = 4:00H + (15:00-13:00) because this range is not overlapping then i get 4h+2h = 6h &amp;#x200B;
Much appreciated. Thank you!
On mobile so I'm not going to format this well. A quick Google search would be much easier for you anyways but... For the first question, your where clause would look something like this. WHERE DateField &lt; GETDATE()-3 Your second question doesn't make sense based on the example you gave, so I'll just answer the example. SELECT COUNT(*) FROM SomeTable WHERE MONTH(BirthdayColumn) = 3
What did you try already? Check the DATEADD function.
What time zone would you use for that date table? I kid, but accurate currency conversion over time is a hard problem.
Two suggestions, first is a book, [The art of SQL](https://www.amazon.com/Art-SQL-Stephane-Faroult/dp/0596008945/ref=sr_1_1?keywords=the+art+of+sql&amp;qid=1551942493&amp;s=gateway&amp;sr=8-1) it's a wonderful book on how to think about sql problems, a primer on strategy rather than tactics. Second, is looking at SQL questions on stack exchange and figuring out how you would answer them. Stack exchange is a study in real world problems, most are easy and but there are a few hard ones spread throughout. Often the hardest part of answering a question is figuring out what the requirements are, because the asker doesn't have the technical background/understanding to properly phrase the question. However this part is good practice as well because coming up with a workable answer with crap requirements and lots of unknowns is a frequent occurrence for devs.
I think this is a much more readable alternative to sum of case ``` SELECT gender, sum(amount) FILTER (WHERE meal='Breakfast') as breakfast, sum(amount) FILTER (WHERE meal='Lunch') as lunch, .... ``` 
Depends on your system. I'd just use a Jenkins job runner if I have lots of these.
I practiced using strata scratch ([www.stratascratch.com](https://www.stratascratch.com)). They have datasets pre-loaded with questions and answers you can practice with. They source their questions from technical interviews from companies so I found it helpful to use for interview practice. They also have a subreddit r/StrataScratch where you can write more advanced and bigger queries. Otherwise, I found datacamp useful too but it's more expensive.
I'll suggest you Strata Scratch ([www.stratascratch.com](https://www.stratascratch.com)) the best for practicing SQL. The questions come from technical interviews taken from companies so all the questions are relevant to working on a job. You can also post for SQL help on their subreddit r/stratascratch. I found it really helpful when I was practicing for my SQL interviews. I also like sqlzoo, datacamp, hackerrank, and leetcode. Tried them all and out of those 4, I think leetcode was the most helpful because they also have a discussion board to get help from others. Datacamp was cool for very specific niches.
I practiced using strata scratch ([www.stratascratch.com](https://www.stratascratch.com)). They have datasets pre-loaded with questions and answers you can practice with. They source their questions from technical interviews from companies so I found it helpful to use for SQL and Python practice. They also have a subreddit r/StrataScratch where you can ask questions if you need help with SQL or Python. Otherwise, I found datacamp useful too but it's more expensive.
Will suggest you Strata Scratch ([www.stratascratch.com](https://www.stratascratch.com)) the best for practicing SQL. The questions come from technical interviews taken from companies so all the questions are relevant to working on a job. You can also post for SQL help on their subreddit r/stratascratch. I found it really helpful when I was practicing for my SQL interviews. Otherwise, I found datacamp useful too but it's more expensive. &amp;#x200B;
I practiced using strata scratch ([www.stratascratch.com](https://www.stratascratch.com)). They have datasets pre-loaded with questions and answers you can practice with. They source their questions from technical interviews from companies so I found it helpful to use for interview practice. They also have a subreddit r/StrataScratch where you can ask questions if you need help with SQL. Otherwise, I found datacamp useful too but it's more expensive.
I like Strata Scratch ([www.stratascratch.com](https://www.stratascratch.com)) the best for practicing SQL. The questions come from technical interviews taken from companies so all the questions are relevant to working on a job. You can also post for SQL help on their subreddit r/stratascratch. I found it really helpful when I was practicing for my SQL interviews. I also like sqlzoo, datacamp, hackerrank, and leetcode. Tried them all and out of those 4, I think leetcode was the most helpful because they also have a discussion board to get help from others. Datacamp was cool for very specific niches.
This really should have went through Google before ending up here. There's barely a coherent question. I suggest starting at w3school.com and then graduating to sqlservercentral.com to actually acquire the skills before giving up and coming here. This is the place of last resort. 
I practiced using strata scratch ([www.stratascratch.com](https://www.stratascratch.com)). They have datasets pre-loaded with questions and answers you can practice with. They source their questions from technical interviews from companies so I found it helpful to use for interview practice. They also have a subreddit r/StrataScratch where you can ask questions if you need help with SQL.
Find the questions come from technical interviews taken from companies at Strata Scratch ([www.stratascratch.com](https://www.stratascratch.com)). All the questions are relevant to working on a job. You can also post for SQL help on their subreddit r/stratascratch. I found it really helpful when I was practicing for my SQL interviews.
Lol wut