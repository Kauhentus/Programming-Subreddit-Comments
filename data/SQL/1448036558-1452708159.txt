In my experience, needing lots of JOINs is typically the result of poor design. I do a lot of: Poll a table for a job (SELECT COUNT), grab/parse job (SELECT), perform work, post result (INSERT).
Probably unrelated but why is the same country there multiple times? Does mySQL have the capability to do temp tables and stored procedures, if it does then I'd try making one like this. EDIT: I just checked, it can do both. ID (whatever ID your other table uses as a unique identifier) COOPart Then do a query that would part out all of your COO columns and put them in that temp table so you would have. ID COOPart 1 China 1 China 1 China 1 Vietnam 1 China 1 China 1 Germany 1 China 1 China 1 1 1 China 1 Germany 1 UK 1 Sweden 1 Sweden 1 China 2 Sweden 2 UK 2 Bob 2 Roger This is for MS SQL but it should give you an idea of how to do it http://stackoverflow.com/questions/14811316/separate-comma-separated-values-and-store-in-table-in-sql-server Then select Disctinct from that temp table and make the whole process into a stored procedure Have your document call that procedure and loop through the result set to put them into one field. --- Alternatively, have something that runs this on a regular basis to clean up the data so you don't have to deal with this much crap. Basically what you need can't be done with just a simple query done at run time.
Triggers are *not* the way to do this. As already mentioned, something like creating a stored procedure and a few variables to keep track of the metrics, then use some logic statements to check/execute sp_send_dbmail if they are met. Example: *** DECLARE @rowCntBefore INT = (SELECT count(*) FROM yourFillTable); DECLARE @rowCntAfter INT = 0; BEGIN TRY EXEC yourFillStoredProcedure; END TRY BEGIN CATCH EXEC msdb.dbo.sp_send_dbmail @profile_name = 'YourEmailProfile', @recipients = 'your@email.com', @subject = 'Fill SP failed to execute!', @query = N'SELECT ERROR_NUMBER() AS ErrorNumber ,ERROR_SEVERITY() AS ErrorSeverity ,ERROR_STATE() AS ErrorState ,ERROR_PROCEDURE() AS ErrorProcedure ,ERROR_LINE() AS ErrorLine ,ERROR_MESSAGE() AS ErrorMessage;', @attach_query_result_as_file = 1, @query_attachment_filename = 'ErrorInfo.txt' END CATCH SET @rowCntAfter = (SELECT count(*) FROM yourFillTable); IF (@rowCntAfter = 0 OR @rowCntAfter &lt; @rowCntBefore) BEGIN EXEC msdb.dbo.sp_send_dbmail @profile_name = 'YourEmailProfile', @recipients = 'your@email.com', @subject = 'Fill SP has ' + (SELECT CASE WHEN @rowCntAfter =0 THEN 'No' ELSE 'Less' END) + ' results!' @body = 'Row Count Before: ' + CAST(@rowCntBefore AS VARCHAR(12)) &lt;/br&gt; 'Row Count After: ' + CAST(@rowCntAfter AS VARCHAR(12)); @body_format = 'HTML' ; END *** As a side note, this is how almost all stored procedures utilized by jobs should be handled. Doubly so for the failures, as they can be rolled back with the TRY..CATCH. If you wanted to get a little bit more fancy, you could transaction block everything and also have them roll back if either of your failure conditions are met. 
&gt; Basically what you need can't be done with just a simple query done at run time. yes it can ;o) no temp table needed, either, although a (permanent) numbers table is used 
and that will work until someone enters one more country than what OP was expecting. A temp table with a split function will work forever and so will a regular cleanup operation.
&gt; until someone enters one more country than what OP was expecting. i tested with 17 countries in row 1, and a numbers table that has 19 values -- and it worked just fine it's easy to set up a numbers table with a much larger number technically you are right, if the numbers table has 187 numbers and someone submits a row with 188 countries... ... but i really don't thing a temp table is worth the inefficiency
Simple enough. Assumed that was the case. Thanks.
thanks! that solution works great
Do not touch the MDF &amp; LDF files. Those **are** your database - the main data files, and the transaction log. Before anyone suggests anything concrete, let's take stock of your current setup. What is your recovery model set to? Right-click the database in SSMS, Properties, Options. Ideally, it should be Full. If you're in Full mode, your LDF &amp; MDF files should be on different drives (for a variety of reasons). How are your backups scheduled, and how often do they run? Is there any process which cleans up old backups?
Assuming given the description that it probably running Full Recovery Mode, check using: *** SELECT name, recovery_model_desc FROM sys.databases *** If that is the case, you will want to setup transaction log backups that rotate out, at the earliest after you last full backup. Also consider changing your backup location to be on a network storage drive instead of the local OS; that is extremely bad practice as a storage failure also takes out your only recovery method. The TL'DR checklist: * Check recovery model * Move backup location to something that isn't the main server drive * Take transaction log backups regularly (this will prevent LDF 'growth') Also be aware, that without setting up a transaction log backup, your recovery point objective will be up to a week.
It usually depends on what function you are using. If you are summing, then I think both would work fine in this case. I typically use 'THEN 1 ELSE 0' when summing because it makes more sense to me. If you were doing a count, you would want to use 'THEN 1 ELSE NULL' because a count would increment for both 1s and 0s. If you were doing an average to find the rate you would want to use 'THEN 1 ELSE 0' because NULLs would be ignored in an average.
Filtering should be used to correctly account values for aggregation , not setting them to NULL. 
&gt; eventually be summed in a pivot table in Excel Unless you have an intermediate need for the un-pivoted data, do the pivot in your query (and I'd suggest 0 to answer your question either way) https://technet.microsoft.com/en-us/library/ms177410(v=sql.105).aspx
This. And when you're reading data and you don't care whether it's 0 or unknown, use coalesce.
He said in another comment that he'd ideally be moving towards a DBA position, so he'd need to know more than the average database developer
Use a zero for summing excel pivot tables.
This is not a question of join because there is no common key. This is just adding new columns with unrelated data.
If this is Oracle, SELECT f1, f2, f3, f4 FROM (SELECT t1.f1 f1, t1.f2 f2, t1.ROWNUM rn1, t2.f1 f3, t2.f2 f4, t2.ROWNUM rn2 INNER JOIN t2 ON t1.ROWNUM = t2.ROWNUM) I'm sure other flavors of SQL have ROWNUM equivalents.
You should probably just create a table to insert into and utilize indexes if they're matching on a key?
Yeah, I'd like to see the query plan here. This sounds like there are no indexes on any of the join columns.
I do a lot of statistical reporting and get asked to do wacky stuff like this all the time. I think you're worrying too much about the quantities - these probably don't matter. Whether they bought 2 apples 1 banana and 3 oranges is less important than the fact that they chose to get apples, bananas and oranges together. Eliminate the quantities and just figuring out the product groupings should make it a lot easier.
The identity values will always be sequential, but not incremental. Newer rows will always have higher values than older rows, but not necessarily in incremental order, due to deletions and other operations that can increment the identity.
Here's a idea. Design your DB in third normal form, not delimited data in a single column - then you wouldn't have this problem. 
That'll provide a cartesian product -- every row in table1 matched with every row in table2
Don't use the @temp tables unless you expect them to hold 1k or less records - which based on the above they will hold 50k. This is because the optimizer cannot read how many rows they have. I would convert everything to #temp tables and if you need all 20 - like others suggested - load a staging/reporting table instead of hiding all the data in these temp tables to select from at the end.
Interestingly enough, each of the @tables do store roughly 1-2000 records. Sometimes just a couple hundred. This is why I went with @tables Though you do raise an interesting point; the goal of this report is to improve our outreach. It stands to reason that as we onboard more support staff and improve outreach processes, the volume of records in the @tables will grow. Plus, it stands to reason that the base number of customers will grow. This is a good suggestion. Thank you. *** Mostly I just wanted to start a conversation on this board. There are, in my opinion, too many "hey r/SQL/ how do I do this" threads and not enough "hey r/SQL/ guess what I learned today" threads. I see a lot more of the conversational threads in r/database/, r/businessintelligence/, r/tableau/, etc. Just trying to see if we can have similar conversations on this board. Judging by the downvote and lack of upvotes I'm guessing no. Oh well.
Circus freak is fine with me! :) I'll take some DBA humor any day.
I'm pretty sure that won't work because Sql is going to think it's the same database when you attach it. Replication needs some starting point to know when to start replicating transactions. That's either through snapshot or backup There is an option to do an ftp snapshot where it takes a snapshot but that's not quote what you're talking about 
This process may be better-suited to a data warehouse/cube/BI sort of setup than an ad-hoc query out of an OLTP database. Not to take anything away from what you've done - it just may be the kind of thing that needs a different approach so as to not blow up your server :) You may want to hop over to /r/sqlserver or /r/mssql for SQL Server-specific conversations.
 WITH t1withid AS ( SELECT ROW_NUMBER() OVER (ORDER BY t1.a, t1.b) AS id , t1.a , t1.b FROM table1 t1 ) , t2withid AS ( SELECT ROW_NUMBER() OVER (ORDER BY t2.a, t2.b) AS id , t2.a , t2.b FROM table2 t2 ) SELECT t1.a AS t1a , t1.b AS t1b , t2.a AS t2a , t2.b AS t2b FROM t2withid t1 JOIN t2withid t2 ON ( t1.id=t2.id ) ORDER BY t1.id I haven't tested this, but it should work on Postgres. This is a super hacky example and while it works with this data set, it probably won't work in your actual application. Like everyone else is saying: you need to add foreign keys so that the two tables have something mutual to join on. 
So, here is the scenario I have encountered... we did the above scenario. Did the first few steps of replication, set up pubs and subs. and at the point where we would normally do a backup restore to secondary server, I did a LUN copy on our SAN, detached the stub database, attached the LUN copies database, and finished up the setup. All seemed well, replication was working. About 48 hours later, we started getting tons of errors that checkpoints were failing. Replication itself seemed fine, but my SQL error log was 14gb and I couldnt run a checkpoint. Restarted SQL Service, the replication database went into recovery, eventually came out, and things were fine again. fast forward another 48 hours, and getting errors again, checkpoints were failing again, lots of error log spam about it. Ultimately, I am trying to figure out if I can still do the LUN copy to see the replicated database, as it is quite large and that would save me the 2 hours it takes to backup/restore, since I can lun copy, mount, and attach in about 5 minutes. tried doing some googling, and found a couple references to not using a LUN copy on your SAN to seed a read only copy in always on, something about checksum issues, but couldnt find much.
Here is a [post](https://www.reddit.com/r/SQL/comments/33ixsi/question_about_how_a_query_executes_with_multiple/) I made awhile back which is somewhat similar... except I did blow up the server. 
If you're using MS SQL, checkout the PIVOT statement [here](https://technet.microsoft.com/en-us/library/ms177410%28v=sql.105%29.aspx?f=255&amp;MSPPError=-2147217396) If you're not using MS SQL, there might be an equivalent in your DB. There are also variations on this technique that will get you the same results without PIVOT. You might also have luck looking up "crosstab" queries.
I thought about Union but then how do you 'label' the rows? If I do something like: SELECT sum(quantity) as 'Total Quantity' FROM [OrderDetails] join products on orderdetails.productid = products.productid where OrderID=10248 union SELECT sum(price) as 'Total Price' FROM [OrderDetails] join products on orderdetails.productid = products.productid where OrderID=10248 I get the numbers in two rows, but there are no row labels and the column header states 'Total Quantity' which does not relate to both rows.
SELECT 'Total Quantity' as Label ,SUM(quantity) as Total Union SELECT 'Total Price' ,SUM(price) Obviously adding your group clauses, filters etc to each statement. This will return two columns, Label and Total with your headers and values. Edit, you don't need to add a group level.to.the derived field Label. Double edit, not that you need it in this instance!
I'm guessing that the data types for sum(quantity) and sum(price) are returning different data types, and whatever sql engine you're using isnt implicitly converting them. 
I wasn't using @tables, those were indexed tables. I solved it by making 4 left joins, one for each of the or conditions then unioning *those* results together. Basically what you're describing as a fuck up in your post was the solution to my very real problem of creating a 1.2T set from "small tables." Don't listen to the others here. A 500k row table is small. A 18M row table is medium. Even a 18M row table with 40 columns is pretty speedy if indexed properly. I use @tables with fake indexes all the time for 50k-1M summary tables that are used in on demand reports which multiple users can access. I use #tables with real indexes for stored procedures that build summary tables. The latter SP's can be 1200 lines +.
Thanks!
Think of a union as an append Select Statement 1 union Select Statement 2 This takes the output from the 2nd select statement and appends it to the output from the 1st select statement, just like they were one statement. So if I did something like this against a table named *people* which contains three rows: select FirstName, LastName from people union select City, State from people I might get output like this: Fred Smith Waylon Smithers Betty Grable Atlanta GA Detroit MI Los Angeles CA 
Not too familiar with Oracle syntax, but could you do something like this? LENGTH(Column) - INSTR(Column,'.') 
There are other ways of achieving what you want without regular expressions, but if you definitely want to use regex and assuming you are using Oracle 11gR2 or above. This will count characters after the first '.', it returns null if there aren't any '.'s. with qry (TEST) as ( select 'thisis.atestnow' from dual union all select 'thisisatestwithoutdot' from dual union all select 'this.is.atest.with.multiple.dots' from dual ) select TEST, regexp_count(regexp_substr(TEST, '([^.]*\.)(.*)', 1, 1, 'i', 2), '.') ACOUNT from qry TEST| ACOUNT ------|------------------------ thisis.atestnow | 8 thisisatestwithoutdot | - this.is.atest.with.multiple.dots | 27 3 rows selected. The regexp_substr section just copies everything after the '.' and the regexp_count just counts any character from that result.
I'm making no assumptions about his table, whereas you are, I'm just using distinct like OP had in his original query.
Sir, Thank you. I really appreciate the help. This works! Just so you know I am very new to SQL. I am using this regex to check decimal values for dollar value attributes. I noticed that decimal datatypes remove leading 0's. I.E '126.10' = 126.1 I was using this regex in a function to count after the '.' then insert a 0, and concatenate a '$' after output. This is done after all calculations are complete. Do you know of a better way of storing decimal values with precision 2 / keep the leading 0's?
Hmm I created an anonymous app to test. I keep getting "####" for the output. What am I missing? SET SERVEROUTPUT ON; DECLARE TARGET NUMBER; BEGIN TARGET := 100; DBMS_OUTPUT.PUT_LINE('$'||to_char(TARGET, '0.00')); END;
Change to '000.00'
Jesus, dude, you need to chill. You are assuming the table is normalised, one record per unique person. You don't know that for a fact. Ok, what of his data isn't normalised and has address information and someone has moved? Your approach wouldn't work. Never assume anything. So stop trying to be the big I am. You're not. 
&gt; I forgot you know the table structure of every db in the world. normally i would just let a blowhard like you have the last word, but i couldn't let that statement go by without remarking that your sarcasm skillz are no better than a 4-year-old's 
Hmm, interesting. Thank you! 
Be REALLY careful with MERGE, especially on older versions of SQL Server. https://www.mssqltips.com/sqlservertip/3074/use-caution-with-sql-servers-merge-statement/
This worked! Thanks again. Your a final-project saver :)
You're more than welcome. Glad I could help.
 Thanks everyone. I really appreciate the help. I couldn't figure out how to list "Name, Title, and HireDate" from the DimEmployee table without using a JOIN to the FactSalesQuota table. I was finally produce the 16 rows requested by the instructor by removing the DISTINCT, which does in fact result in several duplicate lines. This totally didn't make sense to me, however upon closer investigation, it appears that the table lists sales by quarter. The duplicate lines indicate that several employees have exceeded the average sales quota multiple times in a year. This was really throwing me off. Again, you all had some valid points and I really appreciate your help. Here's what I ultimately came up with: SELECT (e.LastName + ', ' + e.FirstName) AS Name, e.Title, e.HireDate FROM DimEmployee e INNER JOIN FactSalesQuota s ON e.EmployeeKey = s.EmployeeKey WHERE SalesAmountQuota &gt; (SELECT AVG(SalesAmountQuota) FROM FactSalesQuota WHERE CalendarYear = 2008) AND CalendarYear = 2008 ORDER BY Name; 
Does it explain in your textbook?
We haven't been given a textbook. We have a few powerpoint slides but it's not explained too greatly.
https://en.m.wikipedia.org/wiki/Relation_schema Also you should tell the school your teacher doesn't provide a textbook.
Yes, basic tutorials. question: how will you get the information into a database to query?
You could take a look at this: http://holowczak.com/converting-e-r-models-to-relational-models/
Funny thing is that this is what I meant. Dont know why I called it triggers but this solution is perfect. Thanks! 
At a guess, your insert statement is specifying a partition and one of your record's REPORT_TIME value is outside of the bounds of that partition. e.g. Partition P_20150911 allows dates less than 2015-09-12 00:00:00 so if you specify 2015-09-12 00:00:00 or more recent it'll err.
What I'm hearing here is your company is willing to pay for a Tableau license, but hasn't invested in actual database software. These are some fucked up priorities. I think probably the biggest advantage of SQL would be automating a lot of those load tasks - lots of DBA's I know pick up one other computer language to write/edit code for their ETL processes. Perl is good for this, as is Python. When I started out, I bought a "SQL for Dummies" book and still occasionally refer to it. It starts off assuming you're working with limited tools like Access and Excel for querying, and then goes more advanced. 
SQL FIGHT!!!!
Yeah, but a tableau license is expensive as fuck. DB software is a bargain, comparatively. 
I was in the same position you were in where I work on numerous brands across the board with weekly reporting deliverables. Each deliverable required about 10-15 sources of raw data. Personally, I just went to a few python meetup groups to learn the pandas package. Best thing I ever did, instead of spending like 30 hours a week scrubbing data in excel, it's now automated to about 10 minutes with QA. I use the rest of my spare time now to learn more python and making amateur attempts at building out api's to the data sources. Maybe you can do the something similar.
Definitely! So you've got some DBA's in Data Products, which is somewhere inside of your company. Currently, Data Products is delivering data to you in a certain format, and you're manually combining the data delivered from an outside vendor and the data delivered by Data Products to create a bigass table. What I would do is find a friend or colleague or sometime who has some degree of fondness for you who also knows some people in data products. Get some introductions made, buy the DBA they introduce you to lunch or drinks, and then show them the table you're building. Odds are, they can add a couple lines to the script they run to pull your data to do some automatic calculations for you. Additionally, they should be able to take in and load the data from an outside vendor to the same table and run the same reports off of it. If they like you, they may get you on their server and give you select permissions. edit: I re-read this and realized it came off as dismissive of your desire to learn SQL - if you want to learn it, you totally should. I just think that you should be able to get this handled no-muss/no-fuss if you get one of your in-house DBA's to catch a glimpse of the issue, as it's probably something they deal with regularly. 
Interesting. Can you give me more details about this? For my purposes, do you recommend Python over SQL? And if so, what software should I be using for Python?
I'm glad you recapped and understand what I'm saying! I do have some friends who are very familiar with SQL and I am reaching out to them for help and understanding. However, within the company I'd have to ask my group's VP for direct access to any of our enterprise software. Is there at least something I can use to practice on? My friend recommended I download MySQL Workbench, but I am completely clueless on how to use it. It's not like Microsoft Excel where I can just randomly create a raw data table and then play with formulas on my own to learn. 
Sure I'll PM you the information
Thank you!
But it will go to this partition PARTITION "P_99999999_999999" VALUES LESS THAN (MAXVALUE)
No you are missing my point, I'm guessing your insert specifies a partition and the value is outside that partition. e.g. SQL&gt; create table t (i integer) partition by range (i) (partition i1 values less than (5), partition i2 values less than (10)) / Table created. SQL&gt; insert into t partition (i1) select 1 from dual / 1 row created. SQL&gt; insert into t partition (i2) select 1 from dual / Error at line 17 ORA-14401: inserted partition key is outside specified partition
I can't believe you've gotten this far without it! You definitely should :)
You should absolutely be using SQL (Microsoft's SQL Express is free, but your company might be able to set you up on an existing instance), and you should be using SSIS to pull the data into the database. There's no excuse for you to be copy and pasting data on a regular basis. 
[https://msdn.microsoft.com/en-CA/library/ms345415.aspx](https://msdn.microsoft.com/en-CA/library/ms345415.aspx)
You can always just hire me. JKing, sort of. 
Anybody using SCOM for this purpose?
so everything i can find for TinyTDS makes it sound like it is only able to connect to MS Sequel.....i have a PSQL db on an amazon server. is this still a valid option?
sure. What I described was an idea. Since youre using a sql implementation on Amazon, youll need to find an interface for that implementation. I cant offer you specifics, the first step youll need to figure out is how to fetch and retrieve records in Ruby from your data store. It should be very straightforward. After you figure that out, get back to me.
What version of SQL are you using? 
12.0.4213
Do you always have to include all the items that were purchased? What if this was your transaction universe: 1. Apple, Orange, Magazine 2. Apple, Orange, Soda 3. Apple, Orange, Candy Bar 4. Apple, Orange, Chicken 5. Apple, Orange, Grapes 6. Apple, Orange, Beef 7. Bread, Eggs, Milk 8. Bread, Eggs, Milk What is the most popular combination by your definition: Apple+Orange, or Bread+Eggs+Milk
Ha! I've been caught out by the same thing before. Glad you fixed it.
Sweet, a new challenge. I'll see what I can come up with.
Thank you, that works perfectly. 
Bummer, I just got done making my test dataset. CREATE TABLE KR_TestTable (strField varchar(max)) DECLARE @Gibberish varchar(100) DECLARE @intI int DECLARE @charC char SET @intI = 1 SET @Gibberish = '' SET @charC = CHAR(13) WHILE @intI &lt;= 1000 BEGIN DECLARE @randy int SELECT @randy = ((CAST(RAND()*1000 AS Decimal(7,0)) % (50-15) + 10)) WHILE @Randy &gt; 0 BEGIN SET @charC = CHAR(((CAST(RAND()*1000 AS Decimal(4,0)) % (126-45)) + 45)) SET @Gibberish = CONCAT(@Gibberish,@charC) SET @Randy -= 1 END INSERT INTO KR_TestTable VALUES ('&lt;HTML&gt;&lt;HEAD&gt;&lt;/HEAD&gt;&lt;BODY&gt; href="' + @Gibberish + '" &lt;/BODY&gt;&lt;/HTML&gt;') SET @intI += 1 SET @Gibberish='' END SELECT * FROM KR_TestTable DROP TABLE KR_TestTable 
Just an FYI, wild carding the front of a string in a LIKE statement in MSSQL will result in it ignoring any indexes on the column being searched. FullText indexes and searching where built for this sort of thing, though they may be overkill in your situation, as with all database stuff it depends on your use case. 
Bread+Eggs+Milk; Solving this problem with GREATEST() and LEAST() has been possible for a single pair, but for more than three items I can only imagine that I would have to CONCAT() all three values and do a group by on them.
Manually change the date range? Jesus man! You gotta get to learn postgres a little better :D My advice: have a worker that saves the results of these queries to csv files. You can even have it mail out the file to your coworkers or whatever 
the reason i need to manually update date ranges is because the reports can be ad hoc as well. like i need 3 days form last week kind of deal, for me it would be a lot easier to just type in at the top the intervals needed, then run it. i played with the date functions but found i had to change them so much, jsut inputting them was easier.....
I would definitely go straight back to the pg gem and generate CSV's for each of the reports.
those joins are going to be the hard part... Access handles joins like complete crap. [They go something like this](http://stackoverflow.com/questions/20929332/multiple-inner-join-sql-access) select * from (((tableA as A join tableB as B on A.Col1 = B.Col1) join tableC as C on B.Col1 = C.Col1) join tableD as D on c.Col1 = D.Col1) edit: Additionally, that Decode() would probably convert to an ~~IIF()~~ case statement in Access and the TO_DATE() might be Format() in Access. I haven't used Access in a long time so take the above with a grain of salt, but good luck.
Your user table should just have the ID of everyone. This can be your 'Person' table. Your friend table would then have a relationship like this between you (1) and your 10 friends: User | Friend ----|------ 1 | 2 1 | 3 1 | 4 1 | 5 1 | 6 1 | 7 1 | 8 1 | 9 1 | 10 1 | 11 
So I take it that my SQL schema is fine.
Thanks! What exactly does Zero-fill mean, and why is there a separate Binary option? Wouldn't whether the data is binary be determined by the data type?
You can either do that or highlight just the code you want to run.
oh and will parsing change the data in the table, or do i need to execute it to make the changes
Anything you write in the query window has to be sent to the database engine where it is parsed for syntax, an execution plan prepared and then it is run against the database. The change in data can't occur until that final step. If you just parse the query (there's a button for it on the tool bar in SSMS) it only does the first step.
so, last i saw sage300 uses pervasive (now actian) as it's database. I'm having a hard time finding recent docs, so this will have to do in the interim... https://www.novell.com/documentation/nw6p/pdfdoc/sqlref.pdf That said, off the top of my head, if you are concatenating strings, it'd be something like `x.Value + ' ' + x.OtherValue AS MyColumnName`
Because you are using one of the columns you didn't change as a unique key. The cleanest way to do it would be to have another table called authors and make the key be autherid and bookid/isbn+edition and join the results. 
While may be technically possible (see /u/sqlburn's post), it's not advisable. Executing arbitrary SQL that's (potentially) been stored to a table by a random user is a **huge** security vulnerability. If you want to "store" queries for execution, use stored procedures.
Or you can change your primary key to include author id.
this doesn't really help much...i tried switching up the tables didnt help
What server are you using? Assuming you are in postgres land: I do it in the Schemaverse using a trigger (https://github.com/Abstrct/Schemaverse/blob/master/schema/deploy/trigger-fleet_script_update.sql) that builds a function automatically out of the SQL but you could also do with with a code block (http://www.postgresql.org/docs/9.4/static/sql-do.html) Edit: People are saying that this is a horrible idea aren't necessarily wrong but there are use cases where it can happen, and there is ways to do it securely. Haters gonna hate and such. 
It sounds like you want stored procedures.
aaaaagh now that helped :D ty , still a beginner and i know little compared to basicly everyone :P 
I'm fairly ignorant of the exact nomenclature of sql server, sorry. You want to have a primary key that is an auto increment id probably and use a unique tuple of (isbn, author_id). http://sqlfiddle.com/#!6/a0c58/2
You can, but just to echo everyone else, you probably don't want to. I'm assuming this is all self-contained in an Access database? Let's say you have a table named tbQuery that contains all your queries, and an ID on each row. Via VBA, you can have the form fetch the row you want and pass the cell containing the query to be executed as a string to RunSQL to execute. https://msdn.microsoft.com/en-us/library/office/ff194626.aspx In the link I provided, they construct the SQL string inside VBA. For your purposes, you would set SQL to the string inside your tbQuery table. I do this quite often, except my Access databases compose the query inside VBA itself (like in the example). 
I read the post like three times looking for mention of the environment, and I had access to it in the title the entire time. 
I have in the past done something like this in order to create a view of SQL data for someone I didn't want to eat up a recurring license fee. To do this I created a view in SQL containing the data I wanted to use in Microsoft access. Then I created an ODBC in Microsoft Access. If this sounds like what you want to do let me know and I can go more in depth. 
Case statement? 
I believe you'd want something like this: Select all ID's that are in the whitelist then make that a subselect in your WHERE clause and put NOT IN (whitelist IDs) SELECT item_ids FROM table WHERE item_ids NOT IN ( SELECT item_ids FROM table WHERE type_ids IN whitelist_types ) Your subselect will select all undesirable item_ids since they have the whitelist value in them and then your outer query will eliminate those item_ids from your result set.
It's not the items themselves that are white-listed. It's the types (categories). And there can be multiple types assigned to a single Item record.
Yes.... ahhh, I just comprehended your code. Yes that will probably work.
On this line: INNER JOIN PREMIUM ON employee.empID = PREMIUM.preID you're referring to the wrong column. It should be: INNER JOIN PREMIUM ON employee.empID = PREMIUM.empID See this SQLFiddle: http://sqlfiddle.com/#!6/bdd9b/2
Too late for me to test but I think you'd want something like replacing: AND XREF.type_id NOT IN( 839,675,674,668,672 ) with AND (select count(*) from items_xref_types where items_xref_types.item_id = I.item_id and items_xref_types.type_id NOT IN( 839,675,674,668,672 )) =1
Perhaps try FROM PREMIUM and then inner join the employee and benefit tables. That way you get each record from the Premium table and just substitute in the first/last names instead of empID and substitute the benefit name instead of the benID. SELECT empFIRST, empLAST, benNAME FROM PREMIUM INNER JOIN employee ON employee.empID = PREMIUM.empID INNER JOIN BENEFIT ON BENEFIT.benID = PREMIUM.benID If I took your information correctly, the results from my query should give: &gt;empFIRST....empLAST....BenNAME &gt;James.......King.......Basic Life &gt;James.......King.......Dental &gt;James.......King.......Family Health &gt;Jill........Parsons....Vision &gt;Jill........Parsons....Dental &gt;Kelly.......Bentz......Family Health 
/u/DyslexicExNinja is correct. Your idea is right, but you're executing it wrong. by joining employeeID to premiumID you are mixing up different IDs. ExNinja is pointing out that you need to reference the same item for both tables (which is empID). I'm finding it hard to explain more, but if you use that SQLFiddle link he attached, you can change the query on the right to what you originally have and compare it to what he has entered. 
Can't you just use the application layer to do this logic?
The below errors show up after roughly 48 hours, and spam the hell outta the error log. After about 12 hours of this, the error log file was 14gb Msg 5901, Level 16, State 1, Line 360 One or more recovery units belonging to database 'xxxx' failed to generate a checkpoint. This is typically caused by lack of system resources such as disk or memory, or in some cases due to database corruption. Examine previous entries in the error log for more detailed information on this failure. Msg 3013, Level 16, State 1, Line 360 BACKUP DATABASE is terminating abnormally. Msg 9003, Level 20, State 15, Line 360 The log scan number (6425620:442906:1) passed to log scan in database 'xxxx' is not valid. This error may indicate data corruption or that the log file (.ldf) does not match the data file (.mdf). If this error occurred during replication, re-create the publication. Otherwise, restore from backup if the problem results in a failure during startup.
fwiw, we have since blown away our replication Database, and redone the same process using a backup/restore instead of using the SAN snapshot to seed, and the errors have not returned. We are well beyond the 48 hour window now, so it definitely seems to be related to using the SAN Snapshots to seed. 
Also, maybe it is an update query... UPDATE table1 SET colour="red" WHERE fruit="%apple%";
Try CASE WHEN
If you use a set statement then you will overwrite the current value of the field. You need some sort of append clause there intead.
Then why do companies ask you to be proficient in SQL as well as require you to know Crystal Report? Does that mean you could do a decent job with a poor knowledge of SQL (I am not to good at making SQL queries. I tend to make an inner join on the foreign key with the primary key in like 100% of the queries I make) and knowledge of CR?
Two reasons: * They want people that can be DBAish and have a good understanding of the schema behind the data. * (More common) They copy-pasted the above's requirements without know a lick about what they mean.
Crystal reports allows you to design a nice presentation to simple tabular SQL query results. It allows you to put data into different sections, headers, footers, format data according to rules you can embed into the query, etc. It is a very powerful (albeit finnicky and, in my experience, buggy) tool for presenting data. But the task of retrieving the proper data and including the data points necessary to plug into the formulae you write in crystal needs to be done in SQL. The job could fall on two different people - one to write the SQL to get the data set and then another to do the Crystal work to present it in a nice pretty package but since the two tasks are very tightly linked (to make something work in Crystal you might need to tweak the SQL) it is much smoother to have a single developer able to work on both the back end SQL and the front end Crystal. 
I think you can change this slightly to rid the need of tracking. If in your stock table you have a physical quantity and an allocated quantity, each order has an ordered quantity, an allocated quantity and a despatched quantity, you don't need tracking at all. Warehouse holds 20x ProdA. An order (Ord1) is placed for 10x ProdA. Warehouse shows 20 physical, 10 allocated, thus 10 available. Order shows 10 allocated. If the order line is removed, allocated stock on the line is set to 0, allocated stock in warehouse subtracts 10, thus available is now 20. If order line is dispatched, order line allocated quantity is set to 0, order line despatched quantity is set to 10, Warehouse allocated quantity subtracts 10, warehouse physical quantity subtracts 10, leaving 10 physical, 0 allocated, thus 10 available. 
It is probably easier to learn more SQL than to try to get by knowing nothing but inner joins. 
If your only tool is a hammer, every problem will look like a nail. There is more to SQL than inner joins. There are outer joins, cross joins, group by/having clauses with aggregate functions, subqueries and CTEs, unions and intersections. But more importantly it is about knowing how to solve problems by thinking in sets. Could you squeak by knowing very little? I suppose. It sounds like you want to apply for jobs that require SQL knowledge and you want me to say that you will be OK with what you already know. I am not able to deduce that - that is up to you and the companies you are applying to. In my own experience, developers who come to my organization to do Crystal reports whose SQL skills are lacking don't last very long. 
The only thing we use it for is to make nice reports where you can click through to drill down into a more granular level of detail for cost accounting variances. Takes a lot of work to keep the data clean though.
Sounds like you had a very easy SQL class. Did it not cover outer joins? In the real world questions similarly complicated to your example are plenty common. 
Maybe, it was because it was a summer class.
Yeah. I agree completely. I should have said "at least as complicated but Likely much more so". Source: Am Financial Software Developer
I'd agree with posts above. if you learned just inner joins in SQL then it's like only breaking through the crust on the SQL pie. There's a whole lot of other good stuff in there. There is so much that can be done with SQL and being able to speak to its possibilities based on experiences has helped me tremendously. CR developers are seen in my area as not having the street cred that SQL people have, even though some of them can pump out great stuff.
First of all Crystal is the shit. Been using it for years and it still amazes me how complex and versatile it is. Personally, I have built some awesome crystal reports that have now become essential to the function of our business. That's an awesome feeling to make something with such a large impact. Crystal is a report builder. A way to spruce up information, to make it look pretty and presentable for specific applications and needs. It can be used to make invoices, YTD/MTD/rolling reports of all kinds, and anything really. 
Funny reply +1
Crystal Reports is business report writing software owned by SAP. It competes with Cognos, SQL Server Reporting Services (SSRS), and Oracle Reports. An open source equivalent (with the usual open source caveats with business software) would be JasperReports. It provides a fast way to author reports that aren't provided by the application vendor. You can use it to build a library of reports that are formatted consistently, have branding or other requirements of your business, and so on. Report authoring software outside the scope of the application is one of the major factors that led to the popularity of the general purpose RDBMS, since suddenly businesses were no longer beholden to the application vendor to provide custom reports. It also usually provides a consistent interface and usually provides a directory of reports that can be run. What's more, the software usually provides a method to prompt the user for parameters. Want to generate a custom monthly report? Create a prompt that asks the end user for what month they want the report for. Now your users can run reports they need *on their own*. They don't need to put in a request to have the report generated for them, and don't need to modify the report and change embedded values. People here keep repeating that it's used by people who don't know how to write programs or don't know SQL. That's *complete bullshit*. Everyplace I've worked (hospitals, engineering firms, school districts) the reports -- whichever system is used -- are written by analysts, DBAs, and programmers that have significant experience with the database, significant experience with the application, and enough experience with the business to know what the business rules are. They use this report writing software because these reports are faster to write easier to maintain overall, and easier for users to use. It's the same reason you use Word instead of LaTeX. It does the job you're actually trying to accomplish more efficiently. I have *never* seen any business grant access to author a Crystal Reports report to an end user because it's too easy to screw up and only analysts know how to verify that all the data are right. That includes Cognos and SSRS, which allow you to build a fully complete data model that is fully aware of the human names for database fields and how all the joins are typically made. The reports are usually generated as an HTML frame or PDF, and most report software today allows you to generate an Excel report as well (although formulas are usually a bitch). At the base level, it's very similar to the report writing features in MS Access. You specify a query or set of queries, and the report software allows you to populate a report with the data users need, often aggregated or collated in ways they want but can't directly do from the application. In my experience, Crystal Reports is a pain in the ass compared to SSRS and Cognos (I've not used Oracle Reports myself, and only have passing experience with JasperReports although I've heard little good). Crystal Reports is better than the shithouse that is ActiveReports 2.0 which you'll still find in many older enterprise applications for internal reports, but it's not much better. It's very flaky, and processes the report in order and you can't refer to a record or field that the report generator hasn't gotten to yet. The query writer is so flaky that it's usually better to just create a view for the report with your logic embedded in that, and then based the report off that view. Cognos is fantastic with more features than I've seen in other products (but I haven't seen them all and much of what I've seen is years old) but the problem with Cognos is that it's IBM software. That means it's expensive, obtuse to configure, and has IBM's legendary documentation. SSRS, from what I've seen, is fairly good. I've only had about 6-8 hours to work with it, though, so my experience is *very* limited. 
I think that you would like a free SQL course at [**Khan Academy**](https://www.khanacademy.org/computing/computer-programming/sql). You will find an interactive console there where you can play tutorials: a lecturer speaks about SQL and you see all SQL statements being written in real time. You can stop, go back or forward, or you can write the code by yourself. From my own practice I know that the best way to learn SQL is to complete at least 2 different courses, so when you finish Khan Academy you can enroll free [**Vertabelo Academy**](https://academy.vertabelo.com/) that covers SQL queries as well (by the way, the course name is [**"SQL Queries"**](https://academy.vertabelo.com/course/sql-queries) so you can be sure that this is what you've been looking for). There are a lot of interactive exercises to complete, and the last one is a really tough nut :-)
Right. I understand that. I was just wondering if there was a way to re-order the existing records so they are sequential or if it mattered.
Just take a dump of your database and run it locally... Also, you don't need to actually run a query to see what kind of performance hit it'll give. If you prepend EXPLAIN in front of your query it'll give you a step by step breakdown of the costs associated with running it. This is in Postgres, but I'm pretty sure it's universal.
Often it is better to pass Crystal Reports (CR) just the data it needs rather than have it parse all of the data. Your database server should be better at processing data than you CR server. You need to understand the structures that CR handles and be able to format your data best to mesh with those formats. If you are familiar with inner joins but are still learning SQL, I suggest writing your inner join query then making sure you can write the queries that identify your opposite records. You should be able to sum up the record counts of your queries and get back to the count on the full data set.
How's this? http://sqlfiddle.com/#!9/97a894/1/0 A couple of things to think about that might require changing the structure, or how you manage the application's inserts; You could get a scenario where a queen could be in two houses at the same time. Add this to the schema to see what I mean. INSERT INTO queen VALUES (2, '2015-12-18' ,2); And can a house be empty? How would that be shown in the data? 
You likely won't get much of a response because all of these can be pretty easily Googled and sound like homework/test questions
Holy f*** didnt even think anyone could understand my question. :) I need a little time to read into this query but it sure looks awesome. I send you reddit gold the least I can do! :) queen_id is actually the PK of the table, so the queen cannot be in two houses. But thanks for the input! Yes a house can be empty and it is no problem because the house_id is only a FK from the house table and a house doesnt need a queen. Also you can set a queen_id inactive where I set a inactive date in the same table, to manually remove the queen from the house. Just if you curious: The logic is for a beekeeping database. House -&gt; beehive, in each beehive there is one queen. With this code I'm going to break down some statistics for each queen. Example: Queen 1 was from XX to XX in Hive 1 and harvestes X amount of honey then came Queen 2 and was from XX to XX in Hive 2 and harvestes X amount of honey. Cheers thanks again 
Wow, thank you for that fast response! Yes, that worked perfectly, thank you so much! I understand it now that you've written that out, I was just going around in circles trying to figure out how to do it! I definitely have a long way to go learning the syntax.
That's quite a query.
You're close. Here's what you want. The ordrer by clause needs to be the final select and you won't need the row number function. SELECT EmpID, Manager, Region, Count (OfficeNumber) FROM Employees e JOIN Offices o ON e.Office_Number = o.OfficeNumber ORDER BY Count(OfficeNumber) DESC Edit: you'll need a group by clause as well, likely on EmpID and OfficeNumber.
It definitely needed a beating :D I was thinking if say an employee had equal count number of office locations. Would I have to throw in a case statement ? or how would I get around something like that? For example: EmployeeID | Office Number -----|------ 15| 20 15 | 20 15| 50 15|50
They are on the same volume, so that wouldnt be an issue. As I mentioned, replication itself seems to function, the only errors are running Checkpoints, and it seems to be good for the first 48 hours or so after restarting the SQL service.
Thank you. Now I know this is called implicit `JOIN`. :)
 INSERT INTO TED_SCHEMA.PRODUCTS SELECT *, (SELECT oracle_user_name FROM my_users;) FROM (SELECT oracle_user_name FROM my_users;).PRODUCTS; might work. Why not just create a userID and use that as an index for users instead of thier usernames? Add the id column to the user name table and products table and you're done. 
I can't change these tables or how they are populated unfortunately.
 begin for recUsers in (select ORACLE_USER_NAME from MY_USERS) loop execute immediate 'insert into TED_SCHEMA.PRODUCTS select p.*, '''||recUsers.ORACLE_USER_NAME||''' from '||recUsers.ORACLE_USER_NAME||'.PRODUCTS p'; end loop; commit; end;
thx jrob. shame no solution w/ the san replication - I really like taking advantage of these types of low level features. We are about to ramp up our AG usage in a huge way and i'll keep this in mind/report back if we encounter similar issues. cheers!
This worked - except I realized that some of the users returned from this query: select ORACLE_USER_NAME from MY_USERS don't have the `PRODUCTS` table - so the block fails. How can I ignore these users? Wrap the whole execute immediate in a IF statement that checks for the existence of the table?
I don't see why this needs to be recursive. If you don't mind posting the definitions of your tables, I can give you a more complete solution, but I believe this should give you what you're looking for, just replace customer_table with whatever table it is you're using to store the [account ended] value. SELECT s.customer, SUM(s.sales) FROM sales_table s INNER JOIN customer_table c ON s.customer = c.customer WHERE s.[month] &gt;= DATEADD(m, -12, c.[account ended]) GROUP BY s.customer
Thanks mate. That was a (bad) example of what I'm doing, here's a better example I have daily sales tables: sales_01012015, sales_02012015, sales_03012015....sales_DDMMYYYY Each one has a row for each customer sale. So if a customer makes 10 sales, he'll have 10 rows on there. I need to bundle every row for a customer into one table, here's what I'm doing... SELECT substring(table_name,...just the date part...) as [B] ,row_number() over (order by the date parts) as [A] INTO temp1 FROM myBD.information_schema.tables WHERE table_name like 'sales_%' DECLARE @a int SET @a=1 DECLARE @b varchar SET @b=(select [B] from temp1 where [A]=@a) EXEC('SELECT * INTO temp2 FROM sales_'+@b+' WHERE customer=mycustomerid') WHILE @a&lt;1000 BEGIN SET @a=@a+1 SET @b=(select [B] from temp1 where [A]=@a) EXEC('SELECT * INTO temp2 FROM sales_'+@b+' WHERE customer=mycustomerid') SELECT * FROM temp2
ER diagrams use lines to connect entities in **one-to-many** relationships the foreign key is declared in the "many" table, and it refers to a unique/primary key in the "one" table the syntax for this is in da manual
So, from looking at a section of the diagram [here](http://puu.sh/lEgwn/c1d80770d5.png) for example, I would create the foreign key in 'child bookings' table and then reference the child table primary key?
sample data -- CREATE TABLE relations ( col1 VARCHAR(9) , col2 VARCHAR(9) ); INSERT INTO relations VALUES ( 'A' , 'B' ) ,( 'A' , 'C' ) ,( 'B' , 'D' ) ,( 'C' , 'F' ) ; SELECT curly.col1 , curly.col2 , larry.col2 , moe.col2 FROM relations AS curly LEFT OUTER JOIN relations AS larry ON larry.col1 = curly.col2 LEFT OUTER JOIN relations AS moe ON moe.col1 = larry.col2 this produces -- col1 col2 col2 col2 A B D NULL A C F NULL B D NULL NULL C F NULL NULL so now "number of second degree relations" doesn't actually need the 3rd join, but i'm leaving it in anyway, and i hope you can figure out why what we are looking for is rows where the larry.col2 is not null but moe.col2 is -- SELECT COUNT(*) AS second_degree_relations FROM relations AS curly LEFT OUTER JOIN relations AS larry ON larry.col1 = curly.col2 LEFT OUTER JOIN relations AS moe ON moe.col1 = larry.col2 WHERE larry.col2 IS NOT NULL AND moe.col2 IS NULL second_degree_relations 2 
Like what MisterSQL said, check the execution plan for a definitive answer. But since the queries are exactly the same I'm wondering if we're missing some context. In other words, it may be useful to see the surrounding code. 
try this it should be the fastest if( select count(*) from localtable A left outer join linkedservertable B on B.ID = A.ID where B.ID is null ) &gt; 0 It would be faster because you push the work of query optimizing down to the DB engine (At least SQL server). Also in SQL server every table has statistics saved after multiple queries. If the account used in the linked server does not have sufficient permission it will not get access to this and might download all data before applying the where clause. 
Well it could be anything couldn't it. Daily sales, yearly page impressions, global monthly library borrowings - I don't have company permission to discuss their business online from their own PC, so I'm being vague and generalising. They genuinely have that many rows though so I'm not amalgamating them. Is dynamic sql the only solution to "for each"ing variable table names? What if the table names have an orderly variance (ie t1, t2, t3)
If your goal is to minimize execution speed, a cursor sounds like your best bet. In fact, go down to goodwill and buy a used eMachine. Ship the box to LA and make sure your ISDN line is working between your SQL box and the new eMachine "reporting" box. Then, make sure you've only allocated 1 GB ram to your SQL Server and make sure you've set MAXDOP = 1. (Oh god I could go on for awhile!)
Exists is always academically faster than IF Count(*)&gt;0 because Exists bombs out the first time it does confirm existence, so the only time that they ever would be equal in execution time (hypothetically) is when the condition evaluates to FALSE (meaning both did table scans).
If your DB platform supports recursive common table expressions then you could approach it that way. Unfiltered (to see the query in action) : with qryData (Col1, Col2) as ( select 'A', 'B' union all select 'A', 'C' union all select 'B', 'D' union all select 'C', 'F' ), qryAncestry (Col1, Col2, Degree) as ( select Col1, Col2, 1 from qryData union all select d.Col1, d.Col2, a.Degree + 1 from qryData d join qryAncestry a on a.Col2 = d.Col1 ) select * from qryAncestry COL1| COL2| DEGREE ---|---|---- A| B| 1 A| C| 1 B| D| 1 C| F| 1 B| D| 2 C| F| 2 Notice C,F appears twice, because it's in the anchor row, but it does get resolved afterward into the correct degree. Changing the final section to see count of degree of 2... ... select count(*) Degree2_Count from qryAncestry where Degree = 2 Degree2_Count| ---------- 2| This approach is extensible too, so could form the basis of checking even more degrees of ancestry.
As far as I know, dynamic SQL is the only way to do that. I'm assuming you work for a credit card processing company or something? 10's of millions of rows *per day*? Your bottleneck isn't the recursion. Look at [this section](https://en.wikipedia.org/wiki/Amdahl%27s_law#Speedup_in_a_serial_program) of the wikipedia article on Amdahl's law. You're currently trying to optimize part B, when part A is the fact that you're looking at literally over 3.5 *billion* records in different tables. But I've never worked with data in that quantity, so I can't really advise you on how to optimize part A. Maybe look at changing data types, change INT to SMALLINT or TINYINT where you can, change DATETIME to DATE where time doesn't matter, etc. 
&gt;Is dynamic sql the only solution to "for each"ing variable table names? I'll preface my comment with the statement, you probably didn't decide to put the data in separate tables. [Someone](http://25.media.tumblr.com/tumblr_m30m15D4ly1r37gzvo1_r1_500.gif) else probably did. There are a couple solutions just as equally bad. It comes down to the fact that you should never ever ever ever ever ever ever ever ever store data that is the same type in separate tables. It's like buying 3 additional homes to put your 1 of your 3 kids in a bedroom in each house. Checking on the kids isn't as easy as going down the hallway and checking their rooms. You have to put on shoes, write a list down of the addresses of the homes where you kept your kids, bear the cold, drive your car, walk to each door, unlock each front door, go to each hallway and go to each room and find check on your kids. SQL is all about set based logic. The fact your name is "recursion12312" and your posting in /r/sql makes me cringe :P . You're attempting to program in SQL using recursive logic like an OO programmer using c#, php, or JS. SQL itself is recursive, most SQL engines use a ton of statistics and indexes to determine what is the fastest way to read from the disk and disperse the computations needed over the processing threads. When you write recursive sql and break down the transaction to small serial transactions and create loops that hit the dataset more times than needed, you are going to hit a crawl. Rather than harnessing the power of the SQL optimizer, you are writing code that you understood... not what the SQL engine understands. It's like having an engineering genius on your hands and your feeding him/her step by step instructions on how to make 1 million peanut butter &amp; jelly sandwiches 1 sandwich at a time, step by step and verifying each step has been completed. With SQL it's more like defining your end product and expectations and the SQL optimizer decides the fastest way to produce the end product. Your approach is wrong because your data is stored wrong. It's not just your code but the fact that it's the only code that works over that data set. I'm not exaggerating. I've seen data analysts go down this road. It's a dark road of performance road blocks. $10USD I would bet that those date stamped tables don't have clustered indexes and are heaps. Denormalization of your data and partitioning it in tables causes extremely frustrating day to day routines that have very poor performance because this is not how you create time stamped tables. When a long process fails, you (the analyst) are forced to rerun and watch the process till it completes. Late nights and frustration, you quit and someone else inherits the mess. This is why I'm saying ,"You probably didn't create this mess, you inherited it". Steps to fix it 1. Find out how data is getting into these tables. 2. Create single table that can accommodate all the columns in the date stamped tables, include a column for the "date". Give it "id" column primary key with an integer that increments by 1. Make sure the primary key is the clustered index. The primary key and clustered index will help the data load faster into the table without fragmentation. Create and index of the date field. 3. Temporary pause the process that loads data into date stamped tables. 4. Change the process that to loads data into date stamped tables to load into your single table. Use the date that is normally used to create the date stamped tables and load that date as the field in the "date" column. 5. Create a recursive query to loop through all the date stamped tables and INSERT/SELECT from the date stamped tables to the single table that will be holding all the data from those tables. Make sure you start with the oldest tables first working your way through to the new ones, this will help create a correlation between the primary key and the date field. 6. Turn on the process that loads data (previously into date stamped tables) into your single table. 7. Validate all data has been moved from the date stamped tables. 8. Delete the old date stamped tables. Never use this method again. Going back to the whole PB&amp;J analogy. Having your data across tons of tables is like having the Bread, Peanut Butter and Jelly in individual packages. However, with a little less consistency. Maybe the bread has 3 slices per bag, maybe there is enough PB in a package for 1 sandwich or maybe there is enough PB in a package for .75 sandwiches. You need to consolidate your sources. This is called normalization. There should be one source for your jelly, one source for your PB and one source for your bread. If you had a 100 gallon vat of Jelly that was 3/4 full you could say ,"I have 75 gallons of Jelly". If you said ,"I have 7 million packages of jelly of various sizes" the only way you could know how much jelly you have is to open up each container, measure the jelly and repackage. THINK OF THE TIME THAT TAKES! When you consolidate your sources you can query and use aggregates against the dataset without having to create recursive queries, without having to open packages as you go along. Disks read faster when they access data on the drive in sequential order. Disks read slower when they are forced to jump from one point in the hard drive to another (random access). Disks are slow to respond (with the exception of SSD) and have latency. If you have 15 million rows and you were recursively moving through each one and each time you accessed the disk you have 5ms latency... 5ms * 15,000,000 = 75,000,000ms 75,000,000ms / 1000 = 75000s (75,000s / 60s) / 60m = 20 hours 20 hours to loop through a dataset of 15 million rows. This is only counting latency, this is not counting the latency of reading the disk. Granted SQL would try to do certain things such as cache page files and attempt to reduce the disk cost. If the data was read sequentially in a single table, 15 million rows should have a single moment of latency followed by a stream of data. If it's 1gb of data and the disk can read at 120 megabytes a second it should take just under 9 seconds to read the data from the disk. Consultants come into these situations all the time. Usually the companies have two attitudes in this situation (1) Find out if your data analyst is an idiot and let us know if he should be fired or (2) Help our data analyst, teach him better ways! 3 out of 5 situations the data analyst doesn't get it. We fix their denormalization issues, and then 10 minutes later the data analyst can't comprehend normalization and we find them loading data from the new normalized tables into #temp or new denormalized tables. This usually ends up in the firing of the Data Analyst. It's way to high of numbers for a failure rate. I don't get why people don't understand normalization. Don't be that guy, normalize your data. Normalize your data well. If you have questions about primary keys, clustered vs non-clustered indexes we can help. You just are no where near that point. You are in the position where you are stating "the company car has no power, the engine runs, has plenty of gas and just had a checkup last week saying it was in working condition" and we show up and see [this](http://www.scottclarkstoyota.com/blogs/207/wp-content/uploads/2012/12/car-upside-down.jpg). Sure you could probably remove the fenders, put 48 inch tires on it and the car would level out, you may even be able to drive it sitting up side down but if you hit debris in the road, you'll be decapitated. It is outrageous and absolutely crazy to continue developing a solution for a system that is fundamentally broken. 
You should NEVER suppress all exceptions with "when OTHERS". If the error was a fatal disk error, a network error, a tablespace error etc... you'd never know. Unexpected exceptions should be allowed to propagate (in scheduled job's case, exceptions are logged). Hopefully it was just an example on your behalf anyway. The lock should be inside the begin. A better approach is to skip the truncate if an exclusive table lock isn't possible, it'll run on the next schedule. CREATE OR REPLACE PROCEDURE example_name AS resource_busy EXCEPTION; PRAGMA EXCEPTION_INIT(resource_busy, -54); BEGIN lock table schema.tablename in exclusive mode nowait; EXECUTE IMMEDIATE 'truncate table schema.tablename'; EXCEPTION when resource_busy then null; END; 
oh you're right...crappy paste job by me. and yes, when others was just an example
Easily done. Yeah, thought the when OTHERS was just an example.
This is what Cube / Rollup and Grouping Sets were designed for. 
Alright. Lots to think about. Thank you for the time, great explanation 
I also work as a SQL Programmer. Interviewers usually ask some simple questions first to weed out people then do things like Whiteboard or have you create a database on the fly in the office. like el_chief said Although I've never heard of Table Inheritance, relational division and I understand but couldn't write out a recursive CTE statement =) If you're curious as to what employers in your area are looking for, you can try to apply at job placements companies like Robert Half technologies , Tek Systems or Phoenix staff (or whatever agencies are in your area) and ask them what they are looking for. 
Have you tried to cast your new string date to a date time type? Have you tried building the date string in ISO format (yyyy-mm-dd) and then casting or converting it to a date time type? &gt;However, it can't do that method to tag the correct year on the end. What is the best way to handle this? Are you getting an error? 
This fixed it for me. http://dannytsang.co.uk/mysql-workbench-error-code-7/
Your syntax works fine for me. How are you checking if the changes were applied? Object Explorer? If so, try refreshing or try... sp_help 'dbo.Events'
This is a typo: DF_dbo.Events_CreatedDate But also, you might want to wrap the code to be executed for your IF block in BEGIN and END or else the two ALTER TABLE statements will always execute. IF @var0 IS NOT NULL BEGIN EXECUTE('ALTER TABLE [dbo].[Events] DROP CONSTRAINT [' + @var0 + ']'); ALTER TABLE [dbo].[Events] ALTER COLUMN [CreatedDate] [datetime] NULL; ALTER TABLE [dbo].[Events] ADD CONSTRAINT [DF_Events_CreatedDate] DEFAULT GETDATE() FOR [CreatedDate]; END
&gt;[we have hit the 1024 limit of columns in a single table.](https://media.giphy.com/media/P7sbeJws0E9O0/giphy.gif) Wide tables only allow 30000 if they are sparse columns; the maximum number of non-sparse columns in a wide table is still 1024.
Find a way to put those fields into another table. Long-term, look for a CRM solution which doesn't abuse the database like this. Just because you *can* have 1024 columns on a table doesn't mean that you *should*. 
&gt; Find a way to put those fields into another table. I've been trying to figure out how to override the CRUD statements so that I could move columns into a custom table, using triggers maybe. But then that becomes an issue when the application does a join between primary data and the custom fields data since I would've moved columns to another table that it has no idea about.
You should build a separate table containing all future dates (couple years) which includes additional information such as leading/lagging business days, YYMMQ#, and fiscal year informaton. This way you can join against this date table and filter your data accordingly. I work at a financial institution where grouping data by quarter/fiscal year was daily work. This table will save you a lot of trouble if you ever have to calculate business days between two dates as well. Bank holidays and leap years were notorious for breaking code because every developer wrote their own approach at resolving it within procedures; Best to code it once and populate it in a table for future use. 
Why have you got that many columns in a table? It sounds like it hasn't been normalised. Either normalize or Create new table based on half the columns with 1 to 1 pk, using SELECT INTO. Drop those columns from first table. Reconfigure your process to work off both tables. 
Rename the original table to something else Create a view with the same name as the original table. The view takes care of joining the renamed table onto the new table, so its transparent to the app, and you don't have to amend all your select queries Then create INSTEAD OF insert, delete and update triggers on the view, which splits the op across the two base tables. All of this means you don't need to change any queries, or source code 
Somehow this application can tell that a table doesn't exist with the name of CONTACT2. I used SQL Profiler to see what queries it is issuing and at no point in time does it ever query for CONTACT2 during the start-up of the application and yet it complains "CONTACT2 does not exist" and closes. I'm working on figuring out how to get the two indexes on the view as well, I have a feeling it's basing it on that for some reason.
Is it possible it is querying INFORMATION_SCHEMA to fetch a full list of tables, and then noticing CONTACT2 doesn't exist as a true table? Or if its old software it could be hitting the system tables directly (in msdb). Most CRM software I've come across are pretty crappy once you look under the hood. 'Goldmine' rings a bell, theres a chance I looked at it once Have a look for any stored procedures in its database too, maybe its just calling those which is why a profiler filter for CONTACT2 is not showing anything. Come to think of it, I recall a 'feature' in SQL Server 2000 where a stored proc could fail if you changed the table/view it was referencing without recompiling the stored proc. you may need to recompile all the stored procedures. This might help.. https://www.mssqltips.com/sqlservertip/1260/script-to-recompile-all-sql-server-database-objects/ 
&gt; It is a set of instructions given to the database to combine data columns from two or more tables Off to a confusingly vague start. SQL joins *return* data joined from multiple tables, they don't join the columns in-place. SELECT Student_Details .student_id, Student_Details. firstName, Student_Details.lastName , [...] The stray spaces mean the code samples are invalid syntax. I think this piece needs to go back for some revision before it should be used as a guide or reference for learners.
&gt; Sounds like a badly designed datamodel to begin with. This is what happens when you let programmers design the data architecture. Are there programmers who are savvy with data modelling? Of course, but to me this reeks of a programmer heavy team who are probably great at what they do but consider the data model and afterthought to keep things "simple".
I kinda just make em up an hour before the interview and don't have any saved. 
Perform a backup and restore it under a different name to your new environment. Alternatively, ODBC connect and pull the information over. 
I would recommend looking at Red-Gate, specifically SQL Compare and SQL Data Compare. SQL Compare allows you to compare/sync schema, and Data compare allows you to do the same with data. If you're trying to copy/merge an old DB into a new one with the same or similar schema, it's a pretty straight forward tool. The only requirement is that your tables will all need a primary key (though you can manually make keys with RedGate if needed). http://www.red-gate.com/products/ 
What about just using a long COALESCE statement in the WHERE clause and check whether it is null. AND together a couple of them if there are too many columns.
Combing through the syscolumns and then using dynamic sql is an option. You could also do some sort of funky isnull, where you assign each column a numerical value, lets say 1, and if the sum of isnull(cola,1)...isnull(colz,1) = 26 then flag to delete.
It is SQL Server, not sure what year. Possibly dumb question: AND c.id_identity = 1 There's no id_identity field in the columns table. There is the is_identity field, but I don't get why we would want to limit this to identity fields, it seems like you would rather leave those out since they'll always be populated anyway. I'm assuming this is supposed to be is_identity and it should be 0 instead of 1. Past that, if the query worked (it doesn't, it wants to concatenate the 1s and 0s together instead of adding them as numbers), it would find any record where all of the non-identity fields (assuming we switched that bit around) were null. I need the reverse of that, any field that's null across all records (but the record can have other fields set). I appreciate the help, though.
&gt; AND c.id_identity = 1 This is a habit I have to identify auto-populated columns, it should say: &gt; AND c.id_identity &lt;&gt; 1 For the actual count, you could easily enough do something like: *** DECLARE @cntr INT = (SELECT COUNT(*) FROM sys.columns AS c INNER JOIN sys.tables AS t ON c.object_id = t.object_id WHERE t.name = 'YourTableName' AND c.id_identity &lt;&gt; 1) .... .... WHERE (' + @cmd + N') = ' + CAST(@cntr AS NVARCHAR(4)) 
But wouldn't that still be getting the count of null fields per record instead of the count of null records per field?
This is one I've used: http://blog.xojo.com/format-sql https://github.com/sqlparser/sql-pretty-printer/wiki/SQL-FaaS-API-manual
I'm trying to figure out a way to do this that doesn't involve going row by row and then checking the end results, but it's not coming to me. Could you elaborate a bit?
www.Poorsql.com
Table mockup: Table | Col1 | Col2 | Col3 | Col4 | Col5 -----|----|----|----|----|---- Row1 | | | | | Row2 | Value | | Value | Value | Row3 | Value | | Value | Value | Row4 | Value | | Value | Value | Row5 | | | | | What I'm looking to find out isn't that Row1 and Row5 don't have any values, I'm looking to find out that Col2 and Col5 don't have any values.
You could use unpivot. This query could be built using the data dictionary. Different data to yours, but result is correct. with ATable(Col1, Col2, Col3, Col4) as ( select 1, cast(null as int), cast(null as VARCHAR(MAX)), 'val1' union all select 2, null, null, 'val2' union all select 3, 2, null, 'val3' union all select 4, 3, null, 'val4' union all select 5, 4, null, 'val5' ) select COL as NULL_COL from (select count(Col1) as Col1, count(Col2) as Col2, count(Col3) as Col3, count(Col4) as Col4 from ATable) a unpivot ( VAL for COL in (Col1, Col2, Col3, Col4) ) b where VAL = 0 NULL_COL| ------------ Col3| 
Mine is massively more efficient than r3pr0b8's solution - mine does one table scan in worse case scenario (where no indexes on columns), his will do loads - one for each column. The query is easily built with sys.columns table on any table.
Turns out you need SQL Prompt installed for this to work. 
Data modeling is almost an art. I have seen multi-million making setups programmed by the best, but the data models were without exception a total disaster. Every self-respecting programming team should hire a data modeling specialist.
&gt; It is SQL Server, not sure what year. SELECT @@VERSION
ApexSql Complete is free unlike the RedGate SQL prompt and it has the ability to expand wildcards easily.
Probably a difference in the collected statics between the two instances. Sounds like apples and oranges since op is saying there's a smaller set of data in the dev environment but if you could disregard that, there would probably be little to no performance difference since it's going to be a scan anyway. 
Or stand on folder icon "columns" and use &lt;ctrl&gt;c &lt;ctrl&gt;v 
The missing ones are probably null or empty strings.
Read about networking.
very insightful..... 
You mean an ERD? I generally find ERDs too unwieldy; once you get beyond a database of trivial size, they're just too large to navigate in PDF or printed form. And no one I've worked for has invested in software that gives you a dynamic, navigable map (if such a thing exists). If you're using SQL Server, you can have SSMS create a Database Diagram for you, but you have to have the appropriate permissions.
If you query all data, Scans are the obvious choice. However, if a scan on a small index is sufficient, it will still be faster than a scan on a larger index. Usually the clustered index is the largest one, hence the slowest to scan.
In this case it's a full table join, presumably on the primary key of the person table so it's going to have scan every entry in whatever index it uses. I'm guessing the date of birth index must be 1:1 unique to person rows since it picked that one to scan. I'm not an expert on how query plans get picked though, so if I'm wrong here or you have any insight I'd be happy to hear it. 
Thanks, all. This is sort of what I imagined. 
ha. so what do you need help with? 
&gt; so it's going to have scan every entry in whatever index it uses. Yes. Yet it makes a difference to scan 1M rows that occupy 1GB on disk or 1M rows that occupy only 100M on disk. &gt; I'm guessing the date of birth index must be 1:1 unique to person rows since it picked that one to scan. No, why? Even duplicate index entries are stored multiple times in the index. &gt; I'm not an expert on how query plans get picked though, so if I'm wrong here or you have any insight I'd be happy to hear it. Well, I wrote a book about indexing. I can still be wrong, of course :) http://use-the-index-luke.com/ 
&gt; And nobody is doing rogue inserts. Hahaha. Er sorry, auto-correct and stuff. What I meant was, the only thing you can trust is the database, and even then, that's iffy.
Wow, good to know, thanks
1) are the statistics on the indexes up to date (i.e. no fragmentation)? 2) are there any structural difference between the tables between dev ad test? 3) have you tried using query hints to force the PK index? 9/10 times, the optimizer gets it right. Different scales of data can make it behave different, but usually it wont change unless there is a structural difference between the databases (columns in the tables, indexes on the tables, data types, etc).
Thanks for the pdf. After trying all types of concats and converts the Sage300 DB I was working with would not accept CONCAT or CONVERT or even CAST. I did find out I did not need the time stamp for my WHERE clauses so it became stupid simple at the end.
You're returning a Date. What is the type of the variable itself? It's probably taking the date and trying to store it into the variable which you have defined as a string in the variable list.
SQL Server will make that string to date conversion. SSIS is less flexible. &gt;When a string is cast to a DT_DATE, or vice versa, the locale of the transformation is used. However, the **date is in the ISO format of YYYY-MM-DD**, regardless of whether the locale preference uses the ISO format. Convert your string to YYYY-MM-DD
I'm leaning towards learning how to pass variables as parameters into a stored procedure now, as that seems like the more flexible option, but just out of curiosity... Are you saying that if I rearrange my string to a YYYY-MM-DD format, that SSIS's (DT_DATE) Cast function will be able to figure it out?
&gt;The type of the variable is "String." That's kind of my point. Your result is a date and you are trying to implicitly store it into a string variable. This is what is causing your error. You either need to change the variable to a datetime type or convert your date result explicitly to a string. http://i.imgur.com/mOI7lol.png &gt;Doesn't it require a year to imply a datetime format though? By putting (DT_DATE) you are not implying it is a date, you are straight up telling it that what comes next is a date and that it needs to try to convert it to such. By convention yeah it likes to have ISO format (YYYY-MM-DD) but there are other undocumented standard formats that it will take like the example you are using.
Thanks. Sounds like the subquery is the way to go; the column is indexed but that version does not use language specific to SQL Server (the TOP keyword) so it's more flexible too.
I believe what you are suggesting should give you the correct results. I made a [SQLFiddle](http://sqlfiddle.com/#!9/efac4/7) of your scenario.
Holy crap, the sql _does_ work! My actual case wasn't working because my ptm table was stuffed full of absolute bullshit, because I completely arsed up the script which populated it. That's four hours barking up totally the wrong tree. The perils of doing this baked out of your tree... Much obliged for your rubber duckery!
Local host means "on this computer". So if you want to connect from another computer you need to figure out what the pi is called on the network.
Came here to say "well just call up and ask them out directly then!" Was not disappointed to see you beat me to it. :)
Don't do it in SSIS. Put it as it is in a staging table in the database and then run a stored procedure on it that handles it. for example: select convert(date,'1 January, 2015')
Fantastic comment, I love your analogies and writing style. Absolutely spot-on.
Always upvote Futurama. Never under estimate the client.
I'm interested to know what he would be referring to, [HIPAA regulations](http://www.hhs.gov/ocr/privacy/hipaa/administrative/securityrule/adminsafeguards.pdf) stipulate that: &gt; enhance safeguards as needed to limit unnecessary or inappropriate access to and disclosure of protected health information I'm guessing your solution(s) probably don't violate this. &gt; Control and monitor equipment storing said information I don't see how this would require a temp table be required. &gt; Ensuring that the data within your systems has not been changed or erased in an unauthorized manner This could *maybe* be a sticking point, but your company should have revision processes in place to review your changes. The DBA saying 'No it's required.' isn't a valid answer. &gt; Implement procedures to regularly review records of information system activity, such as audit logs, access reports, and security incident tracking reports. Maybe there is some specific auditing logic that they have added to tempdb for this purpose? There are dozens of angles I could see him coming from, or maybe has an incorrect interpretation of what is required ([another PDF on the topic](http://csrc.nist.gov/publications/nistpubs/800-111/SP800-111.pdf)). I don't think any of them would be insurmountable or even difficult to meet the requirements there-of. It maybe as simple as adding logic to your revised report procedure that addresses some of the concerns. If everything is staying within the primary information database, I'm unable to see why it wouldn't be allowable.
&gt; they're just local to the same SQL server that the data itself was on Not only that, unless you make them global temp tables (prefixed with `##` vs. `#`), they're scoped only to the connection/session which created them. So other users wouldn't even be able to see the data contained in those temp tables. Edit: Rereading, I would have expected the opposite to be true - that the DBA would say (again, maybe incorrectly) that *having* a temp table would make it less HIPAA-compliant.
I'm not going to reiterate what others here have said, but [here's a good article](http://solutioncenter.apexsql.com/hipaa-compliance-for-sql-server-dbas/) that could help you see things from the DBA's perspective.
Fixed.
Correct. Essentially Tableau (as well as Qlik, etc.) are competitors that importing data from the multidimensional model, not connecting to it natively. Obviously Excel is the client of choice currently; What you might find more helpful is looking for DataViz controls that connect to an MDX source, rather than a full application. I think Telerik has some in their bundle. There's this - https://visualstudiogallery.msdn.microsoft.com/A72B95EF-7B51-4AF9-822F-F536E3ABE7F8 as well. 
I know, right? Woman are frustrating at times.
**RTFM u nerd ( ͡° ͜ʖ ͡°)**
&gt; connecting onto localhost which the raspberry pi is connected to I dont think you grasped the concept of localhost. 
[no u](https://upload.wikimedia.org/wikipedia/en/9/9b/Frohawk_Dodo.png)
This is probably the most possible explanation there could be and it is simultaneously the most thinnest argument one could possibly make. The DBA probably doesn't have a clue about server performance and this is the only way hes seen it done or could do himself. Either someone didn't explain hipaa compliance to him well or he isn't explaining the issue to OP very well.
Temp tables may be your IT department's way of making it easier to police what data is returned by a SQL statement that includes sensitive data. As a compliance auditor, would you rather verify a stored procedure whose last line is: * A 70-line SELECT involving 10-table JOINs w/ column aliases, where it would be easy to accidentally or maliciously "hide" sensitive column data in the result. * Or, a SELECT FROM an 8-column temp table populated with a query. Sure, in both, the main query could be equally complicated with opportunities to grab sensitive data. But, in the "raw" SELECT all it would take is adding an accidental * in the column list or adding "HasAids AS LikesKittens" to violate HIPAA. Instead, using a temp table at least enforces and documents a fixed set of columns that the SPROC is supposed to return so including sensitive data by accident or on purpose would be a little harder.
hehe my writing style has been called verbose and redundant and many times I don't disagree. :)
You can think of any index as a giant tree at the start of every branch there is a sign post with what records are down that branch getting more and more specific as you get closer to the leaves (the records themselves). The biggest difference between nonclustered and Clustered indexes is that a clustered index will always store all of the data for a record once the search gets to the "leaves". There are some intricacies but a non clustered index will need to either include the data it wants specifically, or direct the searcher back to the clustered index, so a nonclustered index may or may not require an extra step in the search. [this](https://www.simple-talk.com/sql/learn-sql-server/effective-clustered-indexes/) is a great article on the subject. 
this response made me smile. Also, been there so many times... just got to laugh it off and keep trucking!
Don't use Access, it's not real SQL. It's similar, but it has its own way of doing things. SQL Server Express Edition is free and will do everything you need, better than Access will.
I'm a SQL Dev but just finishing up a Database Management class where I have to do the same. I feel your pain. 
Yeah, my class is Database Management too. It really sucks being independent study too.
That's the wrong way to write a Join. Look up the proper ansi join 
I'm currently taking DBM in class and Java I online. I wish I had done the opposite. I've worked with databases for a few years now, so I should have done Java in class (because I'm struggling) and DBM online. 
that's because your rep_num can be found in two tbales, so you need to say which one: select rep.rep_num etc.
[Database Diagrams](https://www.mssqltips.com/sqlservertip/1816/getting-started-with-sql-server-database-diagrams/).
in trying to obfuscate, i left out details and confused myself further, and ran into errors.. I will try again with more accurate and necessary info. * Clients: ClientID, ClientName, ClientStatusID * Productions: ProdID, ProdName * ProductionClients: ProdClientID, ProdID, ClientID, HireDate, TermDate * ProductionJobs: ProdId, ProdLocationId, ProdContractDate expected results to be similar to : the first and last production contracted date for each client bolded. &gt; CLIENTID, CLIENT NAME, CLIENTSTATUS, PRODID, PRODNAME, 1stCONTRACTDATE, LASTCONTRACTDATE &gt; 1, Client1, 1, 420, Production420, **04-20-2001**, 04-25-2005 &gt; 1, Client1, 3, 487, Production487, 04-30-2015, **12-03-2015** &gt; 2, Client2, 2, 111, Production111, **05-01-2001**, 07-04-2001 &gt; 2, Client2, 2, 999, Production999, 02-14-2006, **09-22-2011** &gt; 3, Client3, 1, 187, Production187, **05-06-2007**, **06-08-2008** ..the first and last production contracted date for each client bolded. note client 3 only ever had one production, whereas clients 1 &amp; 2 have multiple. I only care about the first and last sorry to give wrong info, and THANK YOU for the help! my boss will be grateful ;) 
&gt; in trying to obfuscate, i left out details and confused myself further now i'm gonna wait a couple hours before replying, just to see if the specs change again 
which employee had the first date worked per company, and what was that date which employee had the last date worked per company, and what was that date 
MSSQL does too via extended properties. 
This is great, thank you. 
Type out your requirements from start to finish and reply to me, or see my example above @ the pastebin link on how to solve your problem.
I use extended descriptions as well, this can then be used to create your data dictionary via data source in excel. 
I spent 9 years in that world and the number of people that had no fucking clue what the actual HIPAA rules are vastly outweigh those who do. I would be included on emails from decently high ranking government employees full of PHI who would then flip out about said PHI being included when some one else replied to the email chain. 
About the only benefit I can see is that it makes it "easy" to use configurations to call the same package to perform the same operations on different sets of data. Not that that seems like a particularly *common* thing to need to do, but I suppose it's a possible benefit.
The two queries you posted are not 100% equivalent (e.g. filter on raw.repDate` missing; `order by` missing)—even thought that doesn't explain the difference on the first sight. Why don't you post the two execution plans you get? http://use-the-index-luke.com/sql/explain-plan/sql-server/getting-an-execution-plan Also: have you tried using a non-with Subquery. It seems that the only think you try to accomplish is to not repeat the expressions in the GROUP BY clause. That can be done that way to (not sure if it changes anything, but give it a try and also post the execution plan). SELECT * FROM (SELECT right(Source_name,3) as split ... ) raw GROUP BY .. ORDER BY
Update a set newcol = ( select min (s.subid) from s where s.fkid = a.id)
where column1 like '%string%'
 select locname from SAD_Location where locname like '%port%'; In oracle, the % is a wild character, meaning anything before port and after port is okay. It just has to contain 'port', in that alphabetical case, somewhere in the text field. 
The % is a SQL Standard wildcard, not just Oracle
Why not run queries in multiple windows? There's probably some kind of safeguard preventing one query from pegging the CPU on its own. Or you could download Prime95 and run that, the benchmark will peg your CPU to 100%
&gt; never use a CTE when you can be using a temp table This is probably one of the single worst pieces of advise I have ever seen on this sub-reddit. CTEs are logical constructs and take full implicit advantage of all the resources and optimization tools available to it (indexes, statistics, etc) from the underlying tables. Temp tables are a physical construct which are an isolated entity from the system that populated it, meaning the above has to be either be added or calculated when it is created; additional storage/resources have to be duplicated. Both have their advantages, but a blanket statement like that is just demonstrably incorrect.
I give this advice because I see a lot of developers start mindlessly using CTEs because they're easier and quicker to handle. CTEs are good for handling small amounts of data. I've seen tables grow exponentially and a CTE that was allowing a query to run in five seconds blows up to two minutes over the span of a year. I work with an Electronic Health Record database with terrabytes of data. It has MASSIVE tables (MS SQL). Using a CTE in this context is almost never an option and will get you shunned during code review. Our queries run off of the production DB, so they have to be lightning fast or they affect patient care. Temp tables give an extreme amount of re-use and performance capabilities by the way of compartmentalization. Please see: [A CTE should NEVER BE USED FOR PERFORMANCE](http://dba.stackexchange.com/questions/13112/whats-the-difference-between-a-cte-and-a-temp-table) Also: [With a CTE, the execution plan of the main query becomes intertwined with the CTE, leaving more room for the optimizer to get confused. By contrast, when a temp table divides two queries, the optimizer is not intertwined with that which created the temp table and the execution plans stay simple and decoupled from each other.](http://sqlserverplanet.com/design/ctes-vs-temp-tables-an-opinion) I could link you a ton of these. You won't find anyone saying performance of a CTE is better than a temp table. You will only find them saying that they're simpler to use, cleaner to look at, and for recursion. 
http://stackoverflow.com/questions/45535/get-month-and-year-from-a-datetime-in-sql-server-2005 I think this is what you're looking for?
Multiple sessions. I like it.
Your method to keep just the date works but keeping hh:mm does not. It gives me decimals. Anything else you can think of? I dont like how i extract my hh:mm at all but it ia the only way i could muster. 
No, you don't have too many conversions, the error is saying you can't convert the nvarchar value to an integer because of the /. Replace GETDATE() with whatever column or calculation produces the date you want: SELECT DATENAME(m, GETDATE()) + ' ' + DATENAME(yy, GETDATE())
&gt; I am needing a column that has the Month and Year. Why do you need a column when you can format the date through the SQL query that your report feeds from like /u/killit provided or through the report itself? 
like /u/notasqlstar you are getting min and max dates per employee, which is not what was asked
It gives you decimals? Mine is formatted like this: 14:09:00.0000000 Which is functionally the same as 14:09 It's possible to convert it to a varchar value and select the first five characters, but I don't recommend that because SQL handles DATE and TIME more efficiently than varchar. 
Thats what i meant yeah, i need it to not show the decimals due to a report requirement. 
"My question for r/sql is: Can I get rid of the top part of that error code so it only says "Error"? " That may be your question, and I honestly don't know the answer, but let me ask you this. Is the goal of this trigger to apply some sort of logic to prevent something from being inserted that is not an acceptable value, or is not unique? If No, What are you trying to accomplish because there is almost definitely a better way! If Yes, There is a better way of accomplishing this called constraints, where you can define a field in a table (or multiple fields) as having to obey certain rules. You can specify certain fields as needing to be unique, or needing to exist within a certain range, or even not having trailing whitespace. So between the trigger and the constraint the biggest advantage to using a constraint is that it will explicitly tell you which constraint is violated. So if a user tries to insert a record that isn't unique, instead of just getting back 'Error', it can say instead "Violation of constraint 'Table_Field_Unique'". Also don't be afraid to tell us what DB system you are using, helps us know what toolset is available, the response provided is from a MSSQL perspective.
you have a composite key that includes `pname` that's not the same thing as having a key over just `pname` for this purpose also, pro tip, primary keys are how the tables are physically organized. you don't want them to be composite, and you don't want them to be on a non-integer type, for performance reasons. much better to have that composite key be a secondary key.
You should try to extract the first five characters on the report side then. SSRS (I'm assuming that's what you're using?) can do it a lot faster than SQL
Couple quick things. 1) Shouldn't have told us it was a lab, because now we aren't going to hand you the answer &gt;:D 2) Give us more detail about what your tables look like, and if so what are you trying. Let us take a look at your query and see where you may be going wrong. If you don't know where to even start W3Schools has a great overview of standard SQL terminology, complete with examples. http://www.w3schools.com/sql/
I'm sorry, I'm still not understanding. Could you show me what it's supposed to look like and then explain it? I'm not able to create more than one primary key for each table? What's the difference between a composite key and a primary key?
you already did. it's composite if it's over more than one column. key foo(col) % not composite key foo(col, col2) % composite 
okay, so back to when you said to make pitcher.pname a key, you meant delete the other two tuples from the primary key?
temp tables for the win! thanks everyone, Ive learned quite a bit from all examples, but in the end, temp tables worked best for me. I also appreciate the low down on the concept, it helped grasp what was needed more effectively for me too.
Isn't clear to me from your post where you are actually having the problem.
i dont need anything additional, but Im also not going to stop you from showing me anything new or different ;) 
You may find this information useful: http://www.sommarskog.se/error_handling/Part3.html#Triggers "If you roll back the transaction in a trigger, SQL Server raises error 3609, The transaction ended in the trigger. The batch has been aborted when the trigger exits. "
Thanks for responding. I think see what you're doing, but I can't seem to make it work. In test 456, I should only be selecting those who clicked on test 123, and that's where one of my biggest hang-ups is. Unless this is what you mean by "conditions"...the only condition I inserted was "Clicked?" = "Yes" which wasn't right...
I corrected my approach to reflect the proper method.
The 3 comments above said everything I wanted to, I started my DBA experience at a healthcare laboratory. It was against our company policy for the report writers to have access to tempdb at all because of the potential problems it presented. I don't even see using tempdb for encryption purposes as viable, though it would certainly explain your increase in performance once removed. More context is needed for certain. 
So you want to took a nested value and write it into the table? Which language are you using? (or is there a sql command which I'm not aware of?) 
Which DB? 
MSSQL 2016 adds support for a native JSON format, but beyond that I think it'd all be client logic/regex fancy tricks. On other platforms I can't help I'm afraid.
Sounds like this is exactly what you're running into: https://social.msdn.microsoft.com/Forums/sqlserver/en-US/e6b95e8e-a29c-4b35-a78a-c43429632398/fatal-error-cannot-create-rtempdir?forum=SQLServer2016Preview
Yes! You're a genius! (And I really need to learn SQL...) I made some slight modifications and got the following query to work for me: select t.location, printf("%.3f", sum(st.456Clicks))/printf("%.3f", sum(st.123Clicks)) as percentage from ( select email, count(*) as 123Clicks, 0 as 456Clicks from table where test = "123" group by email union all select email, 0 as 123Clicks, count(*) as 456Clicks from table where test = "456" and "Clicked?" = "Yes" group by email ) st join table t on t.email = st.email where t."Clicked?" = "Yes" and t.test = "123" group by t.location order by percentage DESC Thank you so much!
I'm using scala. and yes, I'm trying to take a nested value so it can be accessed as a column when I write my query
&gt; Im trying to show the following information grouped by quarter you need -- GROUP BY Quarter
http://sqlzoo.net/ http://use-the-index-luke.com/ You should install and run a local database. Note that different backends have slightly different dialects; `select * from foo as bar` in mysql vs `select * from foo bar` in postgres, and other small changes like that. Probably install MySQL or Postgres.
Are you dead set on the structure of table A? I wouldn't force a column for each item like that. If you are, you need to make your queries more complicated by building a separate insert for each item column. You could conceivably do this with computed column names, but I wouldn't recommend it. Instead, I would recommend creating an orders table first like table B and then OrderItems, which looks a lot like your table C but the order ID comes from orders. Look at the Northwind database, I think it has something like this. Don't start with table A. Don't reinvent the wheel on something as simple as order processing.
Can you use something like MSSQL's Row_number? With player as ( SELECT *, row_number() over (PARTITION BY player ORDER BY date DESC) rn from playertable) SELECT * FROM player WHERE rn &lt; 11
I really like this idea! So basically I'd be adding a column that indicates how many times a particular player has already shown up on the table?
Could you select a distinct list of players then use a correlated top 10 cross apply to the statistics table?
http://www.w3schools.com/sql/
Dependant on the number of columns the simplest (not necessarily the best) way to unpivot data is to union the data together with its self. SELECT TempID AS TransID, 1 AS ItemID , Item1Count AS Quanitity FROM TableA UNION SELECT TempID AS TransID, 2 AS ItemID , Item2Count AS Quanitity FROM TableA UNION SELECT TempID AS TransID, 3 AS ItemID , Item3Count AS Quanitity FROM TableA Once you have the data in the format for TableC, you can easily calculate the data in TableB. Once thats done you can delete all the rows from TableA that Have ID's that are in TableB. Some RMDBs have features to do this other ways (namely MSSQL has UNPIVOT)
http://philip.greenspun.com/sql/
Use PostgreSQL, it's free like MySQL, but more compliant and far more comprehensive a DB.
I tried downloading PostgreSQL, but I just couldn't figure out how to get it started. I'm using pgAdminIII, but I can't connect to a server to then upload my relevant tables. Honestly I'm not sure why I need to connect to anything since all I want to do is work with a large excel table on my own hard drive... Do you know of any tutorials to get me started?
You da man
Missing the COLUMN keyword on the alter statement ALTER TABLE NONAPPLIANCE ADD COLUMN ON_HAND_VALUE CURRENCY UPDATE NONAPPLIANCE SET ON_HAND_VALUE = ON_HAND * PRICE;
Put a ; after the alter command
I'm not sure what that means. I'm pretty green when it comes to SQL. I saw a little anecdote in the book that mentioned ALTER TABLE commands not working too well in Access. I added a note to my instructor about it when I submitted the assignment. I just submitted that command as is with the note.
Excuse the condescending feel of this, I don't mean it, but are you sure the table "NONAPPLIANCE" exist? And are you doing this in a sub routine? This [link](http://stackoverflow.com/questions/10475021/alter-table-syntax-error) may be helpful
Hey that tutorial was really helpful - I was able to get everything set up. Now I'm in the process of importing my tables from excel into my database. I was able to get one of my tables in because it was only two columns with no numbers. The table I'm currently trying to import is my main statistics table with 24 columns. There are character, number, and date values, but the import feature won't accept them no matter how much I tweak. I'm currently saving the excel file as a .csv file and then saving it as a .txt file (that's what worked for my two-column table that I successfully imported). Here is the error message it's giving me: ERROR: invalid input syntax for integer: "OPP" CONTEXT: COPY Stats, line 1, column FG: "OPP" It's just confusing to me because the "OPP" column is clearly a character(80) data type, and I have no clue why the "FG" column would be referencing the "OPP" column in that second line. Now that I type this I'm wondering if the excel columns need to be in the exact order as my database table columns... Is there a way to rearrange the order of columns without creating an entirely new table? Sorry for the barrage of questions, but I'm at a loss right now and I can't find anything online (nothing for PostgreSQL, there's resources on the topic for mySQL). Thanks!
I feel sorry for the people you're working for on this side project.
Dude, I need the COPY syntax and csv data. I can't check anything with screenshots.
 SELECT OrderNumber ,ID ,OrderText FROM xxxx ORDERBY OrderNumber ASC ?
[Dropbox File](https://www.dropbox.com/s/oj5xo0n75rs8whk/Dropbox.xlsx?dl=0) Syntax: COPY "Stats" FROM '/Users/cn/Basketball/Season Player Stats.csv' USING DELIMITERS ',' Where "Stats" is the table created in the database.
Can you save file as csv, I don't have Excel
First off - thanks for the reply. However, I don't want to include the "ID" column in the results. In the generic example I provided, the ID column ranges from 1-6. I need to generate a column that doesn't actually exist. I need to basically "group by" the OrderNumber field (A, B, C) and have an ID # generated for each record therein. A,1, A,2, A,3, B,1, B,2, C,1
Shoot sorry, thanks for hanging with me. [Here](https://www.dropbox.com/s/uj3l7sk2do2ymsl/Dropbox.csv?dl=0) is the file saved as a .csv file. It still automatically opens in Excel for me, so let me know if you need me the csv in a .txt file.
gotcha, think the below will work or atleast set ya on the right path SELECT OrderNumber ROW_NUMBER() OVER (PARTITION BY OrderNumber ORDER BY OrderNumber) AS 'SubCategoryID' ,OrderText FROM xxxx ORDER BY OrderNumber ASC
Not sure if the first answer is correct. This statement looks like it would return any actor that's been in any movie that has a genre. Add this limit: Mg.genre IN (select distinct genre from tblMovieGenre) That should do it.
I honestly don't know. What I DO know is that the following query results in 18484 rows, which is what the instructor is looking for, but the query in my original post results in only 18400. SELECT * FROM DimCustomer c; I'm sure the answer is obvious to anyone who's been doing this for a while, but I'm really new to writing SQL queries. I have an assignment due at midnight, and was about to submit as is (knowing it's wrong) but then thought I'd ask here for help first.
To be specific, here are the exact instructions: "List all customers and the most recent date they placed an order. Do not show the time with the order date. First find the number of unique customers to determine that your results includes the correct number of customers. Then determine which fields are needed to create accurate information about the customer. (18484 Rows)." My query only produces 18400.
Hey I ended up figuring it out, thanks a bunch for helping me!
The first thing you should ask yourself is "does every customer have an order?" How would you figure that out?
Does this return any results? SELECT * FROM DimCustomer Where CustomerKey Not In (Select CustomerKey From FactInternetSales) 
Wouldn't surprise me if you have some customers with the same first and last name but which are actually different records. Try grouping by customerkey instead instead of customer name. You can add the names to the group by as well to allow selecting them, but of course they'll be redundant logically since the key is the most unique field. That's fine, that's how it works.
No worries. Lol I forget this type of thing in code all the time. Good thing I have good testers! They keep me honest.
thank you!
I find this is only true for the first month or so that I'm working in a database. After a while you do get a feel for what data you need &amp; where to find it. The built in system objects which tell you about the schema are **immensely** helpful, especially if you start tracing foreign keys (and if you're using MS SQL Server, `sp_help` is your best friend). The trouble with books &amp; classes is that everything is done with a very small scope, and closed-ended questions. Once you hit reality and it's "boom, here's 800 tables, no reasonable naming convention, and no one *really* knows what story they want the data to tell or even understands how the data truly works", things get crazy.
If you can't figure this out you havent been paying attention in class, this is like literally the basics. I'll give you the answer on the condition you drop this class or start giving a fuck. SELECT NAME, number_grade, ROUND(fraction_completed * 100) AS [percent_completed] FROM student_grades; 
So do you need help with selecting the 3 columns or multiplying and rounding column? I am not doing your homework for you. I will say you should look for information on inline scalar functions, casting, and arithmetic.
Why... Why did you give him the answer.
What I am understanding here is that you have *less* rows in your output than the instructor is looking for. I believe this has to do with the possibility that you have customers in your database that have *never* placed an order which, in this case, are being excluded by your query. You should ask yourself how you can join the tables so that if a row is missing from the FactInternetSales table it will not cause the customer record to be excluded. 
Sounds like a LEFT OUTER JOIN will work for you. This would produce a complete set of records from DimCustomer with matching records from FactInternetSales (where available). If there is not match in FactInternetSales, a null would be returned. Try replacing "INNER JOIN" with "LEFT OUTER JOIN".
Not sure but you might need to create the Trainer table before the horse table since you have a constraint referencing to the Trainer table before it even exists. Same thing for Owner table
What version and edition are you running? 
10.50.6000 SQL Server 2008 R2 Service Pack 3 (SP3) - Enterprise (licensed correctly) 
to show him how stupid he is being for not paying attention and how easy this was. Also, this isnt 100% the right answer. ;)
In my field we leave it at 5 when our databases are in Restricted and change it to 50 when they're live. 
Use [this](https://support.microsoft.com/en-us/kb/2806535) to set your MAXDOP (I think, based on that article, you've got it correct). IIRC from one or more of Brent Ozar's videos (or maybe it was Grant Fritchey, I don't recall) , Cost Threshold of 5 is too low for modern systems and you should be looking at 50 as a good starting point.
1) What are the two tables? You may have to join on something such as employee ID or it could be as simple as a select from table. 2) What are you entering into the front end? Choose what the front end is enabling you to input and insert (such as first_name, last_name, date, etc). Then use a SQL SELECT to query the information you just input in the front end; i.e. if the front end application asks for first_name, last_name input, and you do Jane Doe, verify using a SQL SELECT that it's inserted correctly into the corresponding tables. 
Is this server used for oltp or for BI? Oltp tends to receive more small dB requests, while BI tends to create fewer but heavy requests. I would increase the cost threshold for parallelism to about 50, AFTER ensuring things are not caused by one or two runaway queries that run havoc. *on mobile so formulated briefly
Create a database using baseball or basketball; use these statistics to analyze player depth/growth or the "success" of the player at that position. For example, batting statistics. Data science is about utilizing the data you've given and analyzing it. Just know that connecting R to certain platforms (SQL Server/MySQL/PostGRES) can be a complete hit or miss from what I'm told. 
Yeah, it was a good session. Did you make it on Saturday?
Sorry, yes MS SQL. Thanks!
R has pretty good ODBC support, so that's how I have typically connected it to Sybase, Postgres, etc. ODBC can be tricky but it typically does the job...
I think it is close to a correlated sub query.
No. It's an 8 hour drive each way, so the only way I could justify the trip is if I was picked to speak, and I wasn't.
First you're going to want to find the average selling price per order using this query: SELECT OrderID, AVG(SellingPrice) AS AvgSellingPrice FROM Transactions GROUP BY OrderID We're going to drop that query into the "FROM" clause of another query in order to create a derived table from it: SELECT t.TransactionID, t.OrderID, t.ProductID, t.SellingPrice, dt.AvgSellingPrice FROM Transactions t INNER JOIN ( SELECT OrderID, AVG(SellingPrice) AS AvgSellingPrice FROM Transactions GROUP BY OrderID ) dt ON t.OrderID = dt.OrderID AND t.SellingPrice &gt; dt.AvgSellingPrice And that should get you the data you need 
You need to use a left outer join instead of an inner join so that customers who have never placed an order still show up in the results.
Where exactly have you built this semantic-layer Business Objects Web Intelligence knockoff? VBA like MS Office VBA, Excel?
But the ID links to a certain horse name in the same table, I don't get how you would do this?
 select h.name, sire.name, mare.name, etc etc from horses h left join horses as sire on (h.sireid = sire.id) left join horses as mare on (h.mareid = mare.id) 
You got me curious, so I created a test environment to see the results. Both versions performed identically against 300k records, and had the exact same execution plans when the table was properly indexed. In this case, I'd stick with INNER JOIN since it's more familiar to the majority of developers
~~You can remove DateofAttest from the group by and wrap it in an aggregate function (MAX, MIN, COUNT) in the select clause in order to remove the duplicates.~~ ~~Here is what your query would look like (I tried to preserve your formatting)~~ select MP.Name as 'Provider Name' , MP.Code , MAX(Cast(WLS.DateofAttest as Date)) as MostRecentDateofAttest , MIN(Cast(WLS.DateofAttest as Date)) AS EarliestDateOfAttest , COUNT(Cast(WLS.DateofAttest as Date)) AS CountDateOfAttest ,Cast(WLC.DateAdded AS Date) AS 'Consumer Wait List Entry', SUBSTRING(convert(nvarchar(30), WLS.DateofAttest, 113), 4, 8) as 'MonthYear' from Membership.Providers MP Left Outer Join WL.WaitListStatus WLS on WLS.contractorcode = MP.contractorcode Left Outer Join WL.Consumer WLC on WLC.contractorcode = MP.contractorcode where MP.IsActive = 1 and WLS.DateofAttest &gt;= @datestart and WLS.DateofAttest &lt;= @dateend and 'All' in (@ProviderCodeMulti) or MP.Code in (@ProviderCodeMulti) and MP.ContractorCode = @ContractorCode and ('All' in (@ServiceDeliveryCounty)) and ('All' in (@ProgramCode)) GROUP BY MP.Name, MP.Code, CAST(WLC.DateAdded AS DATE) Order by MP.Name ~~Note that I also cast DateAdded as DATE in the group by, because if DateAdded has a time value it will cause the grouping to no longer work properly (12/8/2015 1:45:14 and 12/8/2015 2:27:47 would both get their own groups, despite them both being displayed as 12/8/2015). I also removed the distinct because it shouldn't be necessary.~~ Correct query below. I'm basically doing the same thing, but instead of aggregating DateofAttest I'm aggregating DateAdded. SELECT DISTINCT MP.Name AS [ProviderName], MP.Code, CAST(WLS.DateofAttest AS DATE) AS [DateofAttest], SUBSTRING(CONVERT(VARCHAR, CAST(WLS.DateofAttest AS DATE), 113), 4, 8) AS [MonthYear], MAX(CAST(WLC.DateAdded AS DATE)) AS MostRecentWaitListEntryDate, MIN(CAST(WLC.DateAdded AS DATE)) AS EarliestWaitListEntryDate, COUNT(CAST(WLC.DateAdded AS DATE)) AS NumberOfWaitListEntryDates FROM Membership.Providers MP LEFT JOIN WL.WaitListStatus WLS ON MP.contractorcode = WLS.contractorcode LEFT JOIN WL.Consumer WLC ON WLC.contractorcode = MP.contractorcode WHERE MP.IsActive = 1 AND CAST(WLS.DateofAttest AS DATE) BETWEEN @datestart AND @dateend AND ( @ProviderCodeMulti = 'All' OR @ProviderCodeMulti = MP.Code ) AND @ContractorCode = MP.ContractorCode AND @ServiceDeliveryCounty = 'All' AND @ProgramCode = 'All' GROUP BY MP.Name, MP.Code, CAST(WLS.DateOfAttest AS DATE) ORDER BY MP.Name
Thanks, but there is no specific mare.name attribute? only Horse.Horse_Name?
Here's another option if you want more control over what the replacement value is: DECLARE @prefix TABLE ( srch VARCHAR(255) ,rplc VARCHAR(255) ); INSERT INTO @prefix VALUES ( 'te ', 'te' ), ( 'de ', 'de' ), ( 'van ', 'van' ), ( 'dela ', 'dela' ), ( 'san ', 'san' ), ( 'o ', 'o' ), ( 'mc ', 'mc' ), ( 'los ', 'los' ); WHILE ( 1 = 1 ) BEGIN UPDATE n SET n.name = REPLACE(n.name, p.srch, p.rplc) FROM names n ,@prefix p WHERE ( n.name LIKE p.srch + '%' ) OR ( n.name LIKE '% ' + p.srch + '%' ); IF @@rowcount = 0 BREAK; END;
See the "as" word after the join type. Each table joined (even if itself) can be given an alias here, and then referenced using this name. Extra useful in self joins as it references *that* specific query/set of data in that table/self.
What do the steps of the job do? Is there a linked server? 
There will be S x P update executions where S is the search string. The number of updated rows could be N x P if that's what you meant?
He probably set the SQL Agent Service and/or SQL Server Service to run as his account which would start to cause failure. You may want to check this first. Start&gt;Run&gt;Services.msc What users are the SQL Server and SQL Agent services running as?
Your account will have to be added under the linked credentials of your boss. You can find them under Server Objects -&gt; Linked Servers.
What is step 2? Stored procedures? SSIS package? 
I expand Linked Servers and get: * Providers * Server 1 * Server 2 * Server 3 What would I do from there?
SSIS package. That's why when I run the script as a query it works, I believe.
SQL Server Agent is running as his account. SQL Server is running as "Network Service". Would it be okay to put my login instead of his?
You need to open the package (if you have access to the project still), or download it from the server. Check the connection managers contained in the package. See what servers they connect to and how they authenticate. If they use your bosses account then you'll need to give your account permissions on those servers, then update the connection managers to use your account. Then redeploy the package, and try running the job. 
Find the server that it's attempted to connect to and open it up. In the top box it should have a list of accounts and the credential usernames it uses to authenticate remotely. Check what there, looking specifically for your bosses account. 
Sorry, I don't quite know what you mean? Im still new to SQL and this field. I expand the Linked Servers folder, and have the 4 options I stated above. When you mean "Open it up", do you mean expand or go into properties of each server? In Properties of each server, I do not see any list of accounts. When I expand into the server, there is 1 folder, Catalogs. Expanding again, it's all databases. Each database has 2 folder, tables and Views. There's a Systems Catalog folder too, but same thing: Only databases with tables and views.
Okay, thanks! The security table is empty on all 3 of the servers I have. 2 of them also have the option of "Be made using the login's current security context" selected. The last has the remote login, but I believe that was from our corporate office - not my boss'.
I have the package open, (.sln extension in Microsoft Visual Studio) but am having a hard time finding the connections manager.
What tasks are in the package? Execute SQL? Data Flows? 
We are trying that too. IT can't seem to know how to un-archive his account to change.
Yep. Uses access as a back end for metadata, userforms and excel as the front end and then odbc connections out to the data sources
Here's some screenshots of it I threw together. I hadn't opened it in several years so I don't quite remember everything about what it can do. http://imgur.com/a/qJfjl 
Free for reddit users: If you would like to try the advanced lessons, just send me a message and I will set you up with a free account. I created the SQL playground for people to practice what they learn on sqooled.com. Once you know the basics of SQL, it just takes practice to really dive into all of the things that can be done.
Cool stuff :)
As far as software you'll need, SQL pro for running queries, and either sublime text or atom for creating and editing queries. For practice. I suggest you buy a book on amazon called Sam's teach yourself SQL in ten minutes. It covers every topic, but most importantly walks you through creating a test database with information so you can practice your queries. If your company is footing the bill, you can take a udemy course or something, SQL for newbs is pretty good.
They should have a QA database that you can practice on then you move your successful and efficient queries to go against the UAT and then Prod database. Each enterprise Database has a corresponding SQL tool for viewing the schema and writing queries. Also, if you want to automate your Excel work, take a look at [BIRT](http://www.eclipse.org/birt/). There is a [commercial version](http://developer.actuate.com/) too, but the Open Source version will probably meet all your basic needs. You want the commercial version with the Server for when you start wanting to automate report distribution or you have a lot of people running the reports on demand. 
Wow! thanks for looking into that and confirming! I didn't think they would be the same in the execution plan but I figured it would be close.
What version? Any service packs installed?
MS Server 2008 Service Pack 2 SQL Server 2005: Microsoft SQL Server Management Studio 9.00.5000.00, Microsoft Analysis Services Client Tools 2005.090.5000.00, Microsoft Data Access Components (MDAC) 6.0.6002.18005 (lh_sp2rtm.090410-1830), Microsoft MSXML 3.0 5.0 6.0, Microsoft Internet Explorer 8.0.6001.19553, Microsoft .NET Framework 2.0.50727.4252, Operating System 5.1.2600
Upvote for Sam's teach yourself SQL in ten minutes - that book saved my life more times than I can count working with SQL. If you need to practice right now, check out SQL Fiddle: http://sqlfiddle.com/#viewSample
&gt; Logins having accounts with Status Y Than this statement isn't exactly true. You want to see logins that only have accounts with the 'y' status is that right? 
How's this? SELECT l.Login ,at.Account ,at.Status FROM Login l INNER JOIN AccountTable at ON l.Account = at.ACcount LEFT JOIN ( SELECT Login ,Status FROM AccountTable at INNER JOIN Login l ON l.Account = at.Account WHERE Status = 'N' ) n ON l.Login = n.Login WHERE n.Login IS NULL; 
I believe [this is what you are loooking for](http://stackoverflow.com/questions/1285686/excel-use-a-cell-value-as-a-parameter-for-a-sql-query). You basically treat an excel sheet as a separate table and query the sheet for the value and then pass it to the procedure on the database. You could also do this using powershell by pulling the values out of excel and then performing your query and pushing the output to an excel workbook/sheet. 
Try sqlite with sqlite studio. It supports most common SQL syntax you might use but can be locally installed with no server, no port binding required, only a few MB.
Bloody hell. &gt;I turned my gaze to Github, looking for open source projects that MongoDB might have leveraged. &gt;The conference Wifi was flaky, so I had to tether to my phone while I looked through dozens of repositories that mentioned both PostgreSQL and MongoDB &gt; Who thought writing like this was a good idea?
SQL Express is free from Microsoft. It has RAM and CPU limits is all. Then again Sql 2014 Developer edition is also free or is only like $50. Then get SQL Management Studio to get into the databases, create them and try queries. I've been a SQL Programmer for decades. All I've used was SQL Management studio and the internet. =)
This is a reality of SQL or any other programming. You rarely are creating a database from scratch. You are inheriting a database that was built 10 years ago on a 15 year old system and that guy had creative ideas on how to make things work. then was modified here and there by 4 other employees over the years and now it's your baby.... your crazy inbred beat up Frankenstein looking baby... and your job is to make it look like the prom queen and dunk basketballs. 
I've liked SQL Express and MySQL workbench.
Can't wait to try this!! Thank you 
I second this. There's nothing like being able to practice with the data you want to work with. It's so much more exciting than going through hypothetical examples. Whoever your DBA is, your goal should be to make them your new best friend.
If you're working with MS SQL Server, SSMS is great. There are a couple of free add-ins that make it even better: [SSMSBoost](http://www.ssmsboost.com/) [ApexSQL](http://www.apexsql.com) (they have 3 free add-ins for SSMS, plus several other good products)
ApexSQL tools - WWW.apexsql.com SqlCop SsmsBoost SsmsTools 
Huh, maybe it will be worth checking out now.
Many Thanks dear! works PerfecT!!!
You should really look at installing a few CLR functions to expose .Net regular expressions to SQL - it works AWESOME for stuff like this. Having Regex available in SQL will change the way you do a lot of things. https://www.simple-talk.com/sql/t-sql-programming/clr-assembly-regex-functions-for-sql-server-by-example/ 
So im on my admin account right now but its saying the server principle for Domain\NewNameIcreated already exists???? I Don't see it anywhere. Also Domain\User is listed as my DBO (my original windows account prior to creating a new local windows username account). So right now SQL server 2012 is enabled with windows authentication. I'm logged in with my admin account aka "Domain\user" (that's the actual name). BUT, when I start up management studio, it's showing the old local windows account login aka "Domain\NameICreated" (windows authentication login, so its greyed out). So if it's in windows authentication mode. and im back on my main windows ADMIN account, why is windows authentication not showing DOMAIN\USER, but showing Domain\NameICreated (OLD WINDOWS LOCAL ACCT THAT WAS DELETED) So since sql server sees me as user Domain\NameICreated (thats been deleted, aka not my admin windows account login), I have no rights to anything because sql server needs to see me as DOMAIN\USER under windows authentication. 
You didn't realize how many great things she does. Oh, how complicated her things are. Every new task is a reason to say that you wished you knew how to do what she does effortlessly. L2KissAss. I used to hate my DBA as well. But magically once I turned kissass he was an unbelievable help. Even left his kid's soccer game to recover a production table I dropped.
Obviously, you would want to keep the history of the bidding as well - in its own table. That would make any bid an INSERT instead of an UPDATE. 
Hmn, I understand what's happened now but I'm at a loss for what exactly has caused it. If you open cmd and use the command "whoami" who does it show you logged in as? Or can you log in with SQL authentication and manually override the database permissions?
I am doing that. But I want to update the auction instance's highest bidding as well (which refers to one of those auction_bid instances, so I can retrieve the bidder's ID, bidding time and so on). However, don't take that way too literally, it was just an example, I just want to know if there's any feature that lets me update a row based on some condition.
I had heard about them but never used them before. Thanks for the link, I'll check it out.
Maybe I didn't understand very well but isn't this just [composite keys](https://en.wikipedia.org/wiki/Compound_key)? Also, database theory has been around forever so if you found something "new" then chances are either you're doing something stupid, or it isn't really new.
This smells bad to me. Loading a container is stored in a manifest, which should not care about max containers. The ship seems to be missing a calculated column that should call a function to provide you the current number of containers. You can add a constraint to the manifest to only allow a container to a ship where the current number of containers is less then the maximum number of containers. Just my opinion.
First, make sure that you can log into the database using that user information. If not, your sql server role does not allow remote connections, which is sometimes a default. 
Did someone call a wizard!? What's up?
You don't have a BRANCH_ADDRESS column in the STAFF table. This line: FOREIGN KEY (BRANCH_ADDRESS) References BRANCH (BRANCH_ADDRESS) Is telling Oracle that local column BRANCH_ADDRESS references BRANCH table's column BRANCH_ADDRESS, but no such local column exists. 
Hey, thanks for the input! Do you have links regarding fn_ParseArray? I can't seem to find any info on that. It looks like it's expecting @InvoiceNo to alerady have the single quotes and it's basically adding a comma in between each @invoiceNo? 
You're welcome :-) I've worked as a BI Specialist for quite some time. 
Just curious why you didn't use FROM and WHERE
 Sub Download_Standard_BOM() Dim cnn As New ADODB.Connection Dim rst As New ADODB.Recordset Dim ConnectionString As String Dim StrQuery As String ConnectionString = "Provider=SQLOLEDB.1;Persist Security Info=False;Data Source=192.168.1.155\FTEAHIST;User Id=sa;Password=ourpw" cnn.Open ConnectionString cnn.CommandTimeout = 900 StrQuery = "USE FTEAHIST SELECT TOP 10 * FROM dbo.members" rst.Open StrQuery, cnn, adOpenStatic, adLockBatchOptimistic, adCmdText rst.Open If rst.State = 0 Then MsgBox "It's closed" MsgBox rst.RecordCount 'USE FTAEHIST End Sub It allows the connection now but won't open the rst object - I get an error on the MsgBox rst.RecordCount (trying to test the record collection) because it says that the object is closed...?
I'd handle this within SSRS. Change the details row to a group then add more rows below inside the group.
Something like one of the following will do what you want. select 1 from your_table where Username = 'user_name' and Verification_Code = 'some_code' select case when exists ( select 1 from your_table where Username = 'user_name' and Verification_Code = 'some_code' ) then 1 else 0 end But I would recommend returning records if it exists. select Username, Verification_Code from your_table where user_name = 'user_name' That way you can give a better error message ("Unknown username, please try again"; "Invalid Verification code")
I like asking open ended question. Like, how would you implement pagination on a SQL query? 
Be careful with that Q11 duplicate delete. Deleting all but the max row id isn't always the best option. If employee records are version controlled, you might just blow away your whole lineage. 
 WITH data (started, finished) AS ( SELECT convert(datetime, '2015-06-09 17:43:48', 120), convert(datetime, '2015-06-10 11:20:00', 120) UNION ALL SELECT convert(datetime, '2015-10-03 06:16:37', 120), convert(datetime, '2015-10-03 23:15:39', 120) ), cte (num, started, finished, currdate) AS ( SELECT 1, started, finished, started FROM data UNION ALL SELECT num + 1, started, finished, case when dateadd(hour, 2, currdate) &gt; finished then finished else dateadd(hour, 1, DATEADD(hour, DATEDIFF(hour, 0, currdate), 0)) end FROM cte WHERE dateadd(hour, 1, currdate) &lt; finished ), PreResults as ( SELECT currdate, datepart(hh, currdate) hours_, datepart(mi, currdate) mins_, datepart(ss, currdate) secs_ FROM cte ) SELECT currdate, hours_, CASE WHEN mins_ = 0 AND secs_ = 0 THEN 3600 ELSE mins_ * 60 + secs_ END AS secs_ FROM PreResults ORDER BY currdate | currdate | hours_ | secs_ | |---------------------------|--------|-------| | June, 09 2015 17:43:48 | 17 | 2628 | | June, 09 2015 18:00:00 | 18 | 3600 | | June, 09 2015 19:00:00 | 19 | 3600 | | June, 09 2015 20:00:00 | 20 | 3600 | | June, 09 2015 21:00:00 | 21 | 3600 | | June, 09 2015 22:00:00 | 22 | 3600 | | June, 09 2015 23:00:00 | 23 | 3600 | | June, 10 2015 00:00:00 | 0 | 3600 | | June, 10 2015 01:00:00 | 1 | 3600 | | June, 10 2015 02:00:00 | 2 | 3600 | | June, 10 2015 03:00:00 | 3 | 3600 | | June, 10 2015 04:00:00 | 4 | 3600 | | June, 10 2015 05:00:00 | 5 | 3600 | | June, 10 2015 06:00:00 | 6 | 3600 | | June, 10 2015 07:00:00 | 7 | 3600 | | June, 10 2015 08:00:00 | 8 | 3600 | | June, 10 2015 09:00:00 | 9 | 3600 | | June, 10 2015 10:00:00 | 10 | 3600 | | June, 10 2015 11:20:00 | 11 | 1200 | | October, 03 2015 06:16:37 | 6 | 997 | | October, 03 2015 07:00:00 | 7 | 3600 | | October, 03 2015 08:00:00 | 8 | 3600 | | October, 03 2015 09:00:00 | 9 | 3600 | | October, 03 2015 10:00:00 | 10 | 3600 | | October, 03 2015 11:00:00 | 11 | 3600 | | October, 03 2015 12:00:00 | 12 | 3600 | | October, 03 2015 13:00:00 | 13 | 3600 | | October, 03 2015 14:00:00 | 14 | 3600 | | October, 03 2015 15:00:00 | 15 | 3600 | | October, 03 2015 16:00:00 | 16 | 3600 | | October, 03 2015 17:00:00 | 17 | 3600 | | October, 03 2015 18:00:00 | 18 | 3600 | | October, 03 2015 19:00:00 | 19 | 3600 | | October, 03 2015 20:00:00 | 20 | 3600 | | October, 03 2015 21:00:00 | 21 | 3600 | | October, 03 2015 22:00:00 | 22 | 3600 | | October, 03 2015 23:15:39 | 23 | 939 | Note, your seconds value for 6/9/15 5:43:48 PM (973) is wrong (43 mins * 60) + 48 secs = 2628
I will make sure to do that. Great point. Thanks for letting me in on the obvious... new to everything and just trying to get things working without breaking anything
I noticed this when I started my job, all of the SQL was embedded in the report generator files. Just made that a stored proc call - and allowed me to maintain the querys without requesting the files back and forth from the end user. Since this is my first SQL job I was pretty stoked with myself I thought of that one. Also saved myself a lot of time.
I hate these stupid questions. Literally every single day I have a discussion with one or more people about the best way to approach some problem. It's the essence of working on a team. Sometimes there are differing ideas and we discuss them to reach a consensus on what approach we, as a team, are going to take. If you ask me for a specific instance I'm just going to spout you some bullshit story because when something is part of the fabric of your daily work, specific instances don't really stand out. If there's an objectively better way to do something, we do it that way, but most of the time if warrants discussion it's because there isn't a one and its just a matter of agreeing on the approach.
Do you mean like this? Select p.* from posts p where p.author in (Select author from posts where section in (3,5,9) having count(*)&gt; N) You need the aggregate function **independent** from the select so that you're free to choose your columns in the select list. The above query may work on some DBMSes, but is not ANSI compliant IIRC. 
pag-what? So, do I get the job? 
www.codeacademy.com has some good tutorials that walk you through the basics. If your company pays for employee training and development, there are several corporate training companies that offer instructor-led training as well. I'd recommend www.protechtraining.com, but I'm biased. ;)
I always liked www.codeacademy.com. They have tutorials to walk you through each lesson and then practice modules as well. I found it pretty useful.
This sounds like your going down the dynamic SQL path. Look up sp_exectesql, it is your friend with this sort of thing.
Why not just add it to the query itself if it needs to use it? It might look a little bit more clunky, but otherwise you are going to be stuck doing more work with SP_ExecuteSQL.
First its easiest to do what u/mikeyd85 is saying. Second, for an exercise purposes, you could later try selecting into a temp table, then a union with the second query grouped by provider with the SUM of a column. Avoid the second because it can be a PITA to update a year from now.
I'll give the same advise I received a while ago in regards to this: Set it to 50 and leave it there unless you find a specific reason to change it. 
I get what you're after, it's a personality question. I just hate being asked for specific examples like that. I also hate being the on the asking side of those questions because all you get is bullshit answers that make the interviewee out to be the world's greatest arbitrator. I think it works a lot better to propose a scenario, ideally a real one and not some made up garbage, and actually work through it with the person. 
Submitted the form. I look forward to listening to it. I have never listened to a programming book so I am interested to see how it flows. Will leave a review when done listening sometime next week. Thanks!
90% of your site has no content or broken links. How do you expect anyone here to look at any of your "tips" as real. Get your garbage spam shit out of here.
I'm not in front of a computer, but it's just a table valued UDF that parses a string into a table using a delimiter. There should be a number of them at a quick Google search that would probably suit your needs. 
As it seems, this is only for US and UK people. Edit: I filled the form without choosing either US or UK and my form was accepted. So no problems on that one. Hopefully I will get the book. I like audiobooks and I was looking for some SQL action. Thanks
All the young kids come in this industry with what they think are epiphanies but are really a lack of understanding of history.
First, don't self-reference within the table itself. Create another table to model the logical relation (ParentComment, ChildComment). ~~Second, the MAXRECURSION option is available with recursive CTEs.~~ EDIT: MAXRECURSION does return an error with the result set, fixed in reply below.
I'm actually dealing with a Comment table. Why is it bad to self references comments? I understand if it was a part assembly where you need versioning of assembly, but comment table? Why? EDIT MAX RECURSION is exactly what i was looking for, didn't know such option existed. **EDIT** Looks like I cannot use MAX RECURSION because I actually need the data to return and not error. Looks like i'm gonna have to keep track of level manually.
Sequel. I attended a talk where Joe Celko chastised those of us that pronounce it like that but idgaf.
I had an interview where the candidate said "squeal", weirdly enough. 
S-Q-L in all cases except microsoft, which is "squeal server"
It may also depend on the speaker's language. In france most people say S.Q.L ( "esse ku elle"), and I only had met one personne saying "sequel".
School? 
This is the only correct pronunciation 
SQL was originally **S**tructured **E**nglish **QUE**ry **L**anguage. So the pronunciation of "sequel" is true to these roots and "S-Q-L", while acceptable, doesn't roll off the tongue quite as easily.
Joe Celko would chastise himself if he could.
S Q L, sequel seems silly to me, despite that it apparently was originally called that.
Same here Although I do say My S.Q.L SQL used to be called SEQUEL very early on too
The problem is adding the column. Data will look like this : 25-Oct-2013 8:00 Date first then time.
Squeal 
Sea-quale. 
There's nothing AT ALL wrong with self referencing tables, it's a common DB design construct. In fact making child / parent tables for comments in your case is de-normalising. Stick with one table, self referencing. You can place a restriction on level by making your recursive CTE have a level field. with cte_comments(parent_id, id, comment, level) as ( select parent_id, id, comment, 1 -- anchor, this is top level parent from t_comments c where parent_id is null union all select c.parent_id, c.id, c.comment, t.level + 1 -- recursive section from t_comments c join cte_comments t on t.id = c.parent_id ) select * from cte_comments where level &lt; 4
&gt; NULLable fields are the devil and should be avoided when possible. What a **absolute** load of rubbish
 SELECT d2.* FROM (select date, id, count(*) as visits from #temp where type = 'B' group by date, id having count(*) &gt; 1) D2 join (SELECT ID, MAX(VISITS) Max_V FROM (select date, id, count(*) as visits from #temp where type = 'B' group by date, id having count(*) &gt; 1) D1 GROUP BY ID) d3 on D2.id = d3.id and d2.visits = d3.Max_V There's probably a much more elegant solution, but I'm tired and not close to an optimal Ballmer peak. 
Data script if anyone else wants it: create table #temp ( date date ,id int ,type char(1) ) --initial insert into #temp select '1/1/2015', 1, 'A' union select '2/1/2015', 2, 'B' union select '3/1/2015', 3, 'C' --make data! insert into #temp SELECT distinct T1.DATE ,T2.ID ,T3.type from #temp t1 cross join #temp t2 cross join #temp t3 --create imbalances insert into #temp SELECT distinct T1.DATE ,T2.ID ,T3.type from #temp t1 cross join #temp t2 cross join #temp t3 where t3.type = 'b' --create more imbalances insert into #temp select '1/1/2015', 1, 'B' union select '2/1/2015', 2, 'B' union select '3/1/2015', 3, 'B' 
&gt; In fact making child / parent tables for comments in your case is de-normalising. Pray tell, what normal form does that violate?
Fixed conversion or varying by time?
No idea what language you're working in (or using a database at all), but you just need tuples of currencies and their conversion rate, such as: ('USD', 'GBP', 0.66) Now you may not want to answer this question this late in the game (Monday due date?), but you should think how: ('GBP','USD',1.52) differs from the first example, if at all.
Thank you very much for your interest! We hope you enjoy the book. Please let me know if you need anything else. Thanks :)
OP, this ^^ is the correct answer. You should build a common table expression including a level indicator on which you filter in your query, exactly as ziptime demonstrated. A few comments here are misleading IMO. Maxrecursion does something different (from MSDN): "When the specified or default number for MAXRECURSION limit is reached during query execution, the query is ended and an error is returned."
Ask Claire edit: you'll probably want a link table between them anyway
if you want time first, ask for the time first select to_char(sysdate, 'HH:MI:SS DD-MON-YY') from dual; if you want the time only, just ask for the time select to_char(sysdate, 'HH:MI:SS') from dual; Either way, a DATE in oracle ALWAYS stores a point in time, which us humans generally think of as a day, month, year, hour, minute, second...
what results?
You want to know which members won what competitions, what event they were attending, their place (1st, 2nd etc) and what prizes they get also how do you know which judge judged the event/competition?
i just looked at the title, are you using MySQL for your assignment?
Probably because the frontend is either an Access form or an Excel Sheet... Those are really the only 2 things I can think of where Visual Basic would be recommended. Their basic form (likely) has 3 fields for toppings which they have mapped to fields in Access (maybe via an OnChanged event or something)... I imagine populating a Key-Value-Pair in this particular instance would be more work and/or KVP's haven't been introduced. But yeah, if you have to delete orders, /u/Robotron6000 is correct. I'd still simply mark the order itself as InActive/Deleted/Completed via another field and then filter results that way, but that might be too much depending on the actual frontend.
looks good, might give it a try at work if it's easy enough to set a safe environment
I agree. Impressive. Point it at one of your synced SQL boxes and encourage everyone to become an analyst! 
Does this help? http://stackoverflow.com/questions/10249802/syntax-error-missing-operator-in-ms-access-query
Hey thanks for the reply, but sadly it doesn't. In the example you sent the problem was a triple join causing missing parentheses. In my problem this isn't the case. Still I tried different parentheses placings, just to be sure, but still the same error.
I still get the same error
&gt; `SELECT e.name, m.name FROM Employee e, Employee m WHERE e.mgr_id = m.emp_id;` Nope, nope, nope. 
I'm not in school; I need this for my job. I'm completely self-taught at this and my attempt at this got me no where near an answer which is why I came here for help. I normally just take the time to suss out an answer or google it (which didn't help here since I didn't even really know what to ask). Since I'm at home now, I can only describe my best attempt. It was with a correlated subquery inside, something to the tune of: SELECT account, status, update_dte FROM table1 t1 INNER JOIN (SELECT account FROM table1 WHERE (status = 'A' AND update_dte IS NOT null)) t2 ON t1.account_no = t2.account_no WHERE status = 'P' GROUP BY account HAVING COUNT(account) &gt; 2 edit: removed some snark
You were on the right track with your self join, but you were trying to treat it like a condition rather than a second data set. Self joins are a pretty common [solution](http://sqlfiddle.com/#!6/29b2b/3) for this kind of problem, as long as you pay careful attention to the join itself and the resulting data set as messing up the join itself could result in [replicating data](http://sqlfiddle.com/#!6/29b2b/17). However, there is usually more than one way to approach a problem. Technically, what you have in your example isn't a [correlated subquery](https://en.wikipedia.org/wiki/Correlated_subquery), just a subquery. However, we can solve the same problem as proposed [using a correlated subquery](http://sqlfiddle.com/#!6/29b2b/9). Forgive me for assuming that this is homework, but typically when people ask for very specific answers with a simple data set without any additional information, that is the case. I hope one of the solutions was helpful.
I'll try plugging those in tomorrow and seeing what I can come up with. Thanks for the suggestions. I'll post back with a success story hopefully tomorrow morning. Unfortunately our schema is absurdly convoluted so my only way of asking this in any semblance of sensible way was to simplify the data set like this. To give an example of how nuts we are... if you find accounts where the customer bought in a specific state you need 4 joins. I don't actually know if that's normal, but it feels not normal to me in my limited experience.
It would depend on how much your database was normalized, your schema, etc. Where I work, if you want to find out what client is being processed, it would require 3 joins so 4 isn't too extreme. If you run into another issue, I'll be happy to help to the best of my ability ;)
If I'm reading your question correctly you just want duplicated account numbers? You could use a CTE like: WITH cte AS ( SELECT account, row_number() OVER(PARTITION BY account ORDER BY account) AS [rn] FROM TABLENAME SELECT cte WHERE [rn] &gt; 1
Parenthesis are necessary: LAG(Sales) OVER (PARTITION BY CustomerId ORDER BY Date) If that doesn't work, it's a possibility there is a compatibility mode setting disabling that functionality. Try this instead: MAX(Sales) OVER (PARTITION BY CustomerId ORDER BY Date ROWS BETWEEN 1 PRECEDING AND 1 PRECEDING) Which is functionally equivalent and supported back to (I believe) version 8 or 9.
&gt; If that doesn't work, it's a possibility there is a compatibility mode setting disabling that functionality. Try this instead: &gt; &gt; &gt; &gt; MAX(Sales) OVER (PARTITION BY CustomerId ORDER BY Date ROWS BETWEEN 1 PRECEDING AND 1 PRECEDING) &gt; &gt; &gt; &gt; Which is functionally equivalent and supported back to (I believe) version 8 or 9. Awesome that works. :) The LAG function continued to not work tho, with the following error: &gt;Error:[IBM][CLI Driver][DB2] SQL0440N No authorized routine named "LAG" of type "" having compatible arguments was found. SQLSTATE=42884
Doesn't look like it. Also, why? The value of a sequence is irrelevant, it just needs to be unique and ordered if you want to sort by when a sequence request was made. 
Anyone think it's worth it? I'm a huge fan of PyCharm but my time with the EAP for 0xDBE I wasn't toooo impressed to warrant the cost
I had a lot of issues connecting to MSSQL databases and getting table layouts to load properly etc; might just stick to SSMS with the free add-ons for now. Would definitely use DataGrip if I was still stuck in a db2 world.
I believe it's worth it: It's not that expensive, and I have used DBVis' support system a couple of times. An example of a nice feature in the paid version: I believe that it's rather nice to be able to edit data in a grid view. In my last three jobs, I've gone through the bureaucracy of having the employer buy a license for me. That should tell you how much I appreciate the product.
Keen to try it out seeing how much I like PhpStorm. Also, for anyone who wants a free program that supports multiple databases, http://dbeaver.jkiss.org/ is pretty good. HeidiSQL for Windows also support MySQL and PostgreSQL now.
Thanks, I've turned on the pro version, and I've been enjoying it a lot. I feel a little overwhelmed with the gui, but I suppose it will just take me a little bit of time get familiar with everything. I might just try to write it off on my taxes if my request gets denied. This seems like a really powerful, amazing product.
No one is going to assist you, per se. You need to formulate an idea and then ask questions that are more specific. Imagine a schema as a roadmap. You have all this data and you want to do *something* for you. Simply taking Excel spreadsheets and making a database of them is a trivial problem and the word "schema" is completely irrelevant. What purpose will the database serve? I'm assuming that the users of the spreadsheets "do" something with them... generate graphs, or use them to look up information for customers, etc. So you need to figure out how a database is going to do those things, and the schema is the map that allows this to happen efficiently for lots of data (millions and millions of records that would be too large to ever live in Excel in the first place.) There is no "correct" way to set up a database. You just create a database and import some data. It seems you're more asking a question about data formatting. For example `DeliveryDate` is probably `datetime`, and `Serial` is probably `varchar`, etc. A final question you're going to need to figure out is how this database is going to be updated and what the intervals are for this update vis-a-viv how the database is being used. If it needs to be updated real time... then things become much more complicated than if it only needs to be updated once a year. Will this update be automatic, or will an intern always have to do it by hand? How much data is being inserted each interval? 
That's actually pretty badass
I know nothing about SQLite. This sounds like fun.
I would definitely love to hear what you think...I think the main challenge for anyone who knows SQL well is then figuring out WTF everything in the actual database means. The SQL here is fairly basic, with nothing more complicated than nested queries and then joining against those subtables. The WSJ investigation isn't mine, but the content of the exam -- warts and all -- are mine. I know the WSJ people on a non-personal basis so I haven't asked if my queries match the reality of what they did (for all we know, they didn't use SQL and every fact stated is something they found via other means, i.e. interviewing, documents...but I seriously doubt that). I wanted to make my students read the investigative work, and as a secondary objective, test if they actually knew SQL and to help them see how it applies to something that was actually published as a story. For myself, this was a great way to get acquainted with a very confusing but potentially interesting dataset...I've actually run through this as an exercise with Python and pandas, with the goal being a semi-automated detection tool in future releases of the databases...in the sense that many of the heuristics can be defined fairly easily in code: - find all medical procedures - then find all doctors who were top earners in each procedure - then filter that list for all doctors whose payment receipts in their respective procedure were 2 times or more than the second-highest earner per procedure I'm sure you could manage that kind of loop and functionality in MySQL or Postgresq...(SQLite lacks stored procedures, I believe)...but I literally don't know how to do anything in SQL beyond its declarative querying and table administration...i.e. how to do variables and define functions. I'm sure I could learn but I always defer to my other scripting skills and use those to glue SQL operations together.
If you know SQL, you probably know most of SQLite...it's a significantly simpler version of other variants such as MySQL...for example, there are only 4 or so actual data types (floats, integers, text, blobs, mainly)...and SQLite is fairly forgiving in that you could just have every column be "TEXT", and SQLite will sometimes do arithmetic if operated values contain numbers...though this is a bit inconsistent and is obviously not a good habit to get into. I use SQLite because it is so much easier than having students figure out how to get a SQL server (Postgresql, MySQL, whatever) running on their own computer, and then a separate client to boot. SQLite just writes to file, the end. The disadvantages are that SQLite, being rather chill about datatypes, also lacks useful types such as `DATETIME` (though it has some conventions for dealing with times in specific string formats)...and virtually all the available GUIs for SQLite, even the paid ones, are not up to par with what you can find for MySQL (Sequel Pro, Navicat, etc.) Note: SQLite may be simple, but it is still very powerful and robust. I believe every modern smartphone with Android or iOS uses SQLite to manage data internally. Firefox and Chrome both use SQLite internally too. So it's definitely not a "kid's" tool...just simple in all the right ways for most learners and programmers.
&gt;I have done endless hours of practice but still find myself not knowing where to begin a code I don't understand this. What do you mean by this? How can you do "hours of practice" and still not know how to perform a basic query?
No joins necessary. Look into the `GROUP BY` clause and the `AVG` function.
You'll need to add a rank field where you partition by ID and order by version DESC. "rank() over(partition by ID (ORDER BY VERSION desc)) RANKING"
I think you have it backwards you DROP TABLE IF EXISTS and CREATE TABLE IF NOT EXISTS
Who wrote the query? It's not something I've seen before.
This, or it's part of a larger statement that's dropping an existing 'child' table if the parent doesn't exist. I really wish `T-SQL` would get Oracle's `CREATE OR REPLACE` syntax for views and SP's at least.
If you try and drop a table that doesn't exist it will throw an error , which may cause the rest of the statements that follow not to execute, whereas the drop table if exists will not throw an error if the table doesn't exist, and thus the rest of the script can run. [](/GNU Terry Pratchett)
I don't think you suggestion also limits the output of otherdata column to the one of the max version. This is why I love when I'm given sample outputs as op gave. Sample output raises and answers many questions. While he didnt word it, it was shown in the sample.
It makes more sense in an automated workflow, such as a hands-free setup and deployment of a database. For example, when I'm writing a script that I'm sharing with someone else that is meant to bootstrap a database, let's say there is meant to be a table named `people` with several columns and several indexes, plus 100 rows of records. I would write the script like this (pseudocode): - If there is a table named "people", DROP it - Create a table named "people" with columns x,y,z - Create indexes on the "people" table - INSERT INTO "people" these rows of data, etc. So If this person runs this script on an entirely new database, or one without a table named `people`, it won't crash at the DROP TABLE line. However, if this person runs this script on a database that has its own version of the `people` table, this script will delete that table and recreate it as specified. At the end of the script, this person can be confident (well, as confident as you can be with a quick and dirty script against a self-contained database) that they have the `people` table exactly as I intend it to be.
Very slick. I'm actually working on an /r/OpenNPPES implementation (apache2 license). The Medicare providers zip referenced in the OP's article is a subset of the NPPES database. NPPES is the National Providers &amp; Producers Enumeration System, or the list of every entity in the US that does medical stuff. **The Problem** Almost anyone that is financially tied to the medical industry needs to get data from this database. If you are a HMO or insurer, then you pull down NPPES updates from the CMS.gov website on a weekly basis (update files published weekly, monthly updates available as well, and then there is the full 7GB DB). The Federal site allows you to search on a very limited number of columns, and not in bulk, neither does the main public version of the site (http://nppes.net). If you want to update the billing / physical information for any of the providers in your network, then you have to either roll out your own NPPES implementation internally, and keep it up to date with CMS's published weekly updates, or you have to search one at a time on nppes.net and hope that they are up to date. I'd like to make an open source (Apache 2.0) NPPES system that has an API to query from any other system so that you can either host it yourself internally, or query the OpenNPPES system with an API key. This project also serves as a learning platform (I am a jr dev). If this sounds interesting to you, then drop me a line or comment on the /r/OpenNPPES and request to join the [GitHub](https://github.com/OpenNPPES) project. It is currently in the scope planning and investigation phase. I'm investigating if a simple command line utility can be used (plus a chron job) to grab updates and keep a DB up to date with changes published to the CMS.gov site.
Sweet! I only vaguely know of NPPES, mostly through how much people complain about it. I suspect there's a lot of low hanging fruit in making the data more accessible to those in the industry (judging by how abysmal IT often is in medicine)
Exactly my thoughts. If companies want someone else to host it, then I can see there being value in paid API access so that OpenNPPES can handle all the IT stuff and they can just query against the system and get updated NPPES data on demand. Interested? I've got an architect level dev "in the know" for the project who runs a consulting company, but current time constraints rule out him getting involved yet. He has a related project idea, so if current projects slow down, then he may be able to restart his investigation into the related idea, which might lead to kicking OPenNPPES into active status as a parallel project.
Is there a meaningful performance difference, or is the planner going to see more or less the same thing? I've got 6 of these tables I have to join. Damn query is going to be a monster. 
I mean ... at least it isn't Access.
Ahhhhh fantastic links. Thanks a bunch!
What are some resources or guides I should read to help along the way?
I didn't know you could so that. I've been running IF (SELECT OBJECT_ID(tempd..#table)) IS NOT NULL...
Yes. Just a random question, any difference if I run it like this? IF OBJECT_ID('tempdb..#temp') IS NOT NULL BEGIN DROP TABLE #TEMP END
It's not homework... It's me just working on a poorly designed system. My brain just isn't working today well enough to figure out the solution and I figured I'd toss it out to Reddit to see if I'm being really dumb or if this is really as impractical as I'm making it out to be... The poor design came from only having 1 Item field in the Orders table originally. Eventually someone decided multiple items needed to be added and instead of properly breaking it out, they just added extra fields. Of course this isn't really Orders/Items.. it's just a generic example I made up to illustrate the point. 
If you read what I wrote, this is the way I receive the data. I didn't opt in for this. I think this is idiotic too; but that's the way it is when your company's ERP is horrid. So enough with the smartness and try to be nice.
It's because the ERP is horrible. Not my choice.
Interesting..... the nursing home link doesn't seem to have a published data page. am i missing that?
Whatever you do, don't write all that code by hand. Regular expressions or Excel formulas are your friend.
Lol if you think that's bad you should see how medical claims are typically stored. Here's a fun example: PRIMARY_DIAG_CODE | DIAG_CODE_2 | DIAG_CODE_3 | DIAG_CODE_4 | PROC_CODE_1 | PROC_CODE_2 -----------------|-----------|-----------|-----------|-----------|----------- It might be more normalized on the back end, but this is the view I have access to. Also there are 21 more diagnosis slots. I think it's actually pretty typical of the healthcare industry. And if you think that's bad, you should see an EDI file.
It's certainly working out well.
If you dont include begin...end it will only execute the one single statement following it, which works perfectly fine for dropping a table.
The database is is postgresql, running on windows (in dev) and linux (in production.) 
Ah cool. Well I personally would go the CTE with `row_number` route depending on performance, but it sounds like you have a good working solution.
With List as ( Select * from Contacts WHERE behavior = 'nice' ) Select * from List WHERE behavior = 'nice' GOTO town
--Dashing through the code --with a one line open tag Select top 10 * from WeGo WHERE destination = 'Over the hills' 
With SQL Server 2016, you can finally use `DROP` &lt;objecttype&gt; `IF EXISTS` &lt;objectname&gt;. So the above will become `DROP TABLE IF EXISTS #temp` http://blogs.msdn.com/b/sqlserverstorageengine/archive/2015/11/03/drop-if-exists-new-thing-in-sql-server-2016.aspx And the peasants rejoiced...
Here's one place for it: https://data.medicare.gov/data/archives/nursing-home-compare (btw data.medicare.gov has a lot of other things on it too, but like most data portals, you have to efficiently wade through the list of useless datasets) Here's one example of how that data was used in a public facing news app): http://projects.propublica.org/nursing-homes/ (disclosure: I worked at ProPublica but not on this app) 
Kickass. A political group I work with is very interested in accountability reporting. Is love to help in this regard.
[Common table expression](http://www.postgresql.org/docs/9.1/static/queries-with.html)
If anybody gets this working with the Teradata JDBC driver please let me know if there are any tricks to getting it working. I haven't tried with the 1.0 release, but I had trouble getting 0xDBE working.
Run it in Oracle: SELECT DECODE(SIGN(FLOOR(maxwidth/2)-ROWNUM), 1, LPAD( ' ', FLOOR(maxwidth/2)-(ROWNUM-1)) || RPAD( '*', 2*(ROWNUM-1)+1, ' *'), LPAD( '* * *', FLOOR(maxwidth/2)+3)) "CHRISTMAS TREE" FROM all_objects, (SELECT 40 AS maxwidth FROM DUAL) WHERE ROWNUM &lt; FLOOR(maxwidth/2) + 5; 
I've been learning SQL for over a year now through my job in the healthcare industry, and I see this daily. I've always thought it was a little clunky... But I don't know what the better alternative would be. What's a better way to associate an arbitrarily long set of ordered 'codes' to a single record/account? The only other possibility I can think of would be to have a delimited list in one field as a string... But then you would have to parse the string each time you want to access a specific code in that field. That also seems clunky, albeit in a different way...
Honestly SQL has no way to deal with this gracefully. You would need to use R, Python, or some other more fully featured language. Pandas (Python for data analysis) has pivoting and stacking built in to handle this type of problem. Edit: I'm not actually sure those exact functions would work, because I don't really resort to it as much as I should :)
Thanks for the response. Understood on the performance trade-off. I agree, we don't consider this to be a normal issue. Our ESXi cluster meets ALL best practices that would be relevant. I'll have to do some further research before making a decision, and some testing on the features you've recommended. We try to vMotion this paticular system as little as possible, but maintenance on underlying infrastructure leaves us with no other options (except the almighty downtime, which we cant consider). the issue is intermittent. Any N+1 cluster requires every VM in the cluster to be migrated at least once for a full maintenance across every host.
You need to find a good relational design/modeling book. Are you familiar with foreign keys at all? Here's a page that will help get you started: http://www.tomjewett.com/dbdesign/dbdesign.php?page=manymany.php. Here's a quick example. You have three tables: products, colors, product_colors. Products and colors are pretty self-explanatory. The products-colors table is a join table that defines all of the product-color combinations you sell. **Products** id | name ---|--- 1 | Super duper hone case 2 | Yoga pants **Colors** id | name ---|--- 1 | Cardinal 2 | Gray **Product-Colors** product_id | color_id ---|--- 1 | 1 1 | 2 2 | 1
Awesome, thanks! I will check that out.
 SELECT make,fuel, AVG(odometer) FROM T GROUP BY make,fuel
Hi, I'm the one who wrote the post. Thanks for the feedback. @11ry it would always be cool to invent something new. However, it looks too simple to be new, which means that someone probably has written about it. If someone has written about it, it might tell me that I'm "doing something stupid," which I want to find out before that stupid explodes in my face, because I'm using this technique. Even if it's not inherently risky, I can learn from anything someone has written if I can find that writing. Also, please note that usually composite keys are common in primary keys (detail tables) but not as extensions from other unique keys. @alinroc MAX_CONTAINERS does "break normalization" (2NF) if you view it as an attribute on MANIFEST, but I'm sort of treating it as part of the identity of the SHIP. I don't know if that's a valid perspective and that's why I ask for feedback. @valdest I think you're expressing the same 'breaks normalization' view as @alinroc. Again, thanks for the feedback I really want to get any and all critiques of this technique before I apply it more widely.
You have to put firstname and last name in the group by clause
Thank you for the detailed response- this helps fix the NULL issue, but I'm still confused as the IN clause successfully deleted all rows in the table when run as a stored procedure, but it failed to delete any rows when run from the query window. Is there a difference in how the IN clause would be executed in a SP vs. the query window?
There shouldn't be. Both cases should be skipping the nulls. Did you add some code to capture the @@rowcount?
I am very surprised to see this in a journalism program. How are the students handling it? I would have expected journalism majors to be put off by technical subjects. That said, I would like to see statistics as a required class for journalism majors - I see so many blunders in the press when writing about anything involving stats.
here you go -- CREATE OR REPLACE VIEW custtotal_VW AS SELECT cust.custid , cust.firstname , cust.lastname , SUM(custorders.ordertotal) AS OrderTotal FROM cust INNER JOIN custorders ON custorders.custid = cust.custid GROUP BY cust.custid , cust.firstname , cust.lastname
Love the arguments about how inefficient the select would be. 
I don't know the internals, but try casting current_date to int. If that works it *may* result in something you can compare with. I say this because in Teradata you can get an integer representation of the current date with the statement `cast(current_date as int) + 19000000` (results in 20151218; the explanation of why is boring).
http://stackoverflow.com/questions/24777471/to-date-with-as400 I wish I was near a computer at the moment, I've spent the past 3 years of my life doing SQL on an I series. Dates can be tricky because in my experience they are some odd non date field thrown together when the RPG programmers had to patch applications for y2k. If you can wait till Monday I can send you a code snippet that does exactly what you need. 
Really enjoying this blog. Props to Markus for another really good resource.
Thanks. Well, ironically, this was a code update from FI through the chassis and hosts, so we may be in a better position unknowingly! I'll look into the situation a little further down this path.
This joke came up when I ran the following query. SELECT TOP 1 * FROM tblJokes WHERE fldCringeRate &gt; 9000
Thanks for the response. I did some reading on SQL Server service accounts. It looks like the SQL Server Agent account on the dev machine needs to be given permission to access the prod server backup directory. Am I interpreting this correctly?
Contact your hosting company and they may have solutions set up for this. They may open an SFTP to your backup folder. They may have software that basically gives you access to the server via a web page and you can download load anything on the VM via http without being able to change anything. Hosting companies should have a solution for you. 
I'll find out Monday if these are options available to me. Thank you.
Define incorrect? I think you need to remove the GROUP BY as well, if the totals are not what you're expecting then likely something in your SELECT needs to be changed. 
This might have something to do with the fact that you're doing some weird stuff regarding correlated subqueries. where you're checking the state, you likely shouldn't do the following: AND "tbl_lineitem"."invid" = "tbl_invoice"."invid" as tbl_invoice is already matched down below when you do FROM "tbl_invoice" INNER JOIN "tbl_lineitem" ON "tbl_invoice"."invid" = "tbl_lineitem"."invid" The better method to do this, might be to check if the invid in tbl_postage exists in the tbl_lineitem.invid where the state is = or &lt;&gt; 'AZ' in SQLServer this would go something like SELECT "postagefee" FROM "tbl_postage" WHERE "tbl_lineitem"."invid" = "tbl_postage"."invid" and exists (select 1 from tbl_lineitem as t2 where tbl_postage.invid = t2.invid and t2.invstate = 'AZ') If that doesn't work, populating a [SQLFiddle](http://sqlfiddle.com/) with some sample data might be the best bet to figuring it out without sharing unnecessary data. Edit: The reason why the group by works is because and order's state can only be either or, but as soon as you remove the group by and it tries to do everything, some data likely gets dropped because an order's state can't be both AZ and not AZ. Just a theory though. This is likely where that 14.50 is going...
I've always preferred temp tables for multi-step queries like this due to the performance, the difficulty in visually separating the different CTEs (although it's not THAT bad), and that you're constrained to always use a select statement at the end of the chain. I guess for simplicity and conciseness, CTE chains are a bit better.
Thank you! I appreciate your time! I have been messing with it for quite a while. I will spend some time on it tonight, and am sure to learn new things from your example.
&gt; How can this best be done when I don't know how many words that are entered in the search field? this is not a mysql issue -- you would do this with php or whatever language is handling your form submission
I guess that I could split the string and send the words in separately with a foreach and end up with something like in the second example above. It just didn't seem very safe or convenient and I thought that maybe I could handle it directly with SQL wildcards.
Recursive CTE approach... with t(col1, col2, col3, col4) AS ( select 'A', 0, 1, 5 from dual union all select 'B', 0, 4, 0 from dual union all select 'C', 2, 0, 0 from dual union all select 'D', 0, 0, 0 from dual union all select 'E', 3, 5, 0 from dual union all select 'F', 0, 3, 0 from dual union all select 'G', 0, 3, 1 from dual union all select 'H', 0, 1, 5 from dual union all select 'I', 3, 5, 0 from dual ), t2 (col1, col2, col3, col4) as ( select col1, nullif(col2, 0), nullif(col3, 0), nullif(col4, 0) from t where col1 = 'A' union all select t.col1, nvl(nullif(t.col2, 0), t2.col2), nvl(nullif(t.col3, 0), t2.col3), nvl(nullif(t.col4, 0), t2.col4) from t join t2 on t.col1 = chr(ascii(t2.col1) + 1) ) select * from t2 | COL1 | COL2 | COL3 | COL4 | |------|--------|------|------| | A | (null) | 1 | 5 | | B | (null) | 4 | 5 | | C | 2 | 4 | 5 | | D | 2 | 4 | 5 | | E | 3 | 5 | 5 | | F | 3 | 3 | 5 | | G | 3 | 3 | 1 | | H | 3 | 1 | 5 | | I | 3 | 5 | 5 | 9 rows selected.
This goes beyond the capability of wildcards. You should check out regular expressions, which are used for more complicated pattern matching.
FULLTEXT search is an option here, if slightly clunky... but regex would probably be a better solution.
If you can get each individual clause in your first half to work, you can just string them together by using parenthesis in your where clause. For example: WHERE (ABC = 123 AND DEF = 456) OR (ABC = 456 AND DEF = 789) I'm having trouble following the second half of your question. Could you provide some data with examples?
I don't understand the question. Do you mean if you had columns A,B,C,D,E that searchTerm could be in any of them? If so, your query would look something like Select startDate, endDate, A, B, C, D, E from table where startDate &gt; 'enterDateHere' and endDate &lt; 'enterDateHere' and (A like "%searchTerm%" or B like "%searchTerm%" or C like "%searchTerm%" or D like "%searchTerm%" or E like "%searchTerm%")
You can use a case statement in the order by clause, check this page out for some examples: http://stackoverflow.com/questions/16936891/sql-order-by-using-case-when-for-different-ascending-descending-and-cust If you can't get that to work with your SQL server, another option I can think of is to run the query twice with your criteria in the where / order clause and perform a UNION ALL between them. There are probably some more experienced people here with better options, though!
TIL Thanks! Not sure yet where I will use this but cool! 
Let us know if you get it to work! I've never had a case to actually use this in practice.
If you don't mind that would be awesome. I don't work with SQL enough to really understand some of the other comments and I haven't been able to find a solution. Thank you.
Would need an example of the data and the desired output.
I was able to figure it out, thanks for the help!
Search for websites that will allow you to practice executing SQL statements. Syntax may vary slightly from sql version to sql version, so keep that in mind (especially if the Microsoft exam wants you to be on the cutting edge of what's being added to SQL server) but a quick Google search got me [here](http://www.sqlcourse.com/) which appears to allow you to write SQL interactively. Probably much better websites out there than this one, but if you have access to a SQL server you might also try the AdventureWorks database, known to be a universal practice dataset.
Yeah looks like I added an extra paren in the where statement. So you got it working? 
What's the error from the package? This is probably best left to a script task that accepts directories and file filters as inputs. The default FTP object likely won't get you far with SFTP if memory serves.
I didn't have it working until you mentioned the extra paren lol. I just got it to work. Thank you very much for your help. I really appreciate it. 
The error is: Error: 0xC0029151 at Execute Process Task RateGain, Execute Process Task: In Executing "\\fbnecl3\inzf\Pricing and Yield\WinSCP\WinSCP.exe" "-script="\\fbnecl3\inzf\Pricing and Yield\WinSCP\RateGain.txt"" at "\\fbnecl3\inzf\Pricing and Yield\WinSCP\", The process exit code was "1" while the expected was "0". Task failed: Execute Process Task RateGain I'm using an Execute Process Task rather than the inbuilt FTP Task. I'm trying to follow this process: https://winscp.net/eng/docs/guide_ssis
Here's a similar example I did a little bit back, except instead of 0's I have nulls: http://sqlfiddle.com/#!6/7f288/1/0
Kick winscp to the curb. Find a .net sftp library and use a script task to do what you do in the winscp script task. You can run, monitor and log everything that occurs in the process. 
&gt; "\fbnecl3\inzf\Pricing and Yield\WinSCP\WinSCP.exe" Is this the location of your winscp executable? I always test my winscp script in a cmd window first... then you can see if it works. Also, there is a winscp.com that comes with... see if that works. I have had some success using that instead... can't recall what the reason. 
The only .Net library I could find was a winscp library.. actually works pretty well. I wrote a couple of clr functions to download ftp files through a stored procedure
Maybe use Cygwin? https://www.cygwin.com/
I'm having the same issue as this person : https://social.msdn.microsoft.com/Forums/sqlserver/en-US/1605c595-5cf2-4052-9abb-b9aacd57eab0/merge-replication-subscriber-can-only-download-but-not-upload?forum=sqlreplication But no solution is given.
It's not free, but Cozyroc's SSIS add-in makes SFTP (and a bunch of other stuff) stupid easy. http://cozyroc.com/ 
You are: 1) truncating the current date to the year (1/1/2015) 2) then subtracting 3 days (12/29/2014) 3) then truncating to the year (1/1/2014) You might like: select add_months(sysdate,-36) from dual 
oh, this is awesome, thanks man. I was doing this now: add_months(trunc(sysdate, 'YY'), -36) but I really like this interval thing. Didn't know about it
I actually figured out that all I needed to do was separate each value by parenthesis. I was trying 'Part_No1', 'Part_no2' and it needed to be (Part_No1),(Part_No2)
 Select A.ID ,B.Text_Field FROM Table_A as A LEFT OUTER JOIN Table_B as B ON Charindex('&lt;jobdepid&gt;' + A.ID + '&lt;/jobdepid&gt;',B.Text_Field) &gt; 0 WHERE B.Text_Field is not null There you go. **Edit:** if you remove the where clause it will bring all fields but the ones that don't have a match will have null in Text_Field **Edit 2:** Other Things to consider, Assuming the ID in Table_A can be matched to other parts of that text field you could set up the '&lt;jobdepid&gt;' and '&lt;/jobdepid&gt;' parts of that charindex as two variables and save this as a stored procedure that you would then call with what &lt;&gt; &lt;/&gt; you want. Declare @StartTag Varchar(Max) Declare @EndTag Varchar(Max) Set @StartTag = '&lt;jobdepid&gt;' Set @EndTag = '&lt;/jobdepid&gt;' Select A.ID ,B.Text_Field FROM Table_A as A LEFT OUTER JOIN Table_B as B ON Charindex(@StartTag + A.ID + @EndTag,B.Text_Field) &gt; 0 WHERE B.Text_Field is not null
this is something i do for one-off requests all the time with regex in notepad++ you can find/replace directly into insert statements --add return carriages in place of commas find: , replace: \r\n --take fields and generate inserts (be sure to trim whitespace) find: ^\(.+\) replace: insert into foo select '\1' modify as you need but i use regex all the time for these excel requests and crap i get quite a bit
If I'm with you: SELECT 1, NULL, * FROM tb1 WHERE x = 1 SELECT NULL, 2, * FROM tb1 WHERE x = 2
 SELECT 1 AS one , 2 AS two , tb1.* FROM tb1 WHERE whatever
So I can assume this is going to work. Could there be a more efficient way to do it? Table B is almost 300k rows and Table A is 800 rows. I'm going to let it run for a bit, but it's looong. Maybe no good way around it though.
See: https://msdn.microsoft.com/en-us/library/ms152758.aspx Specifically the section on considerations for data validation, especially the first one. If any and I mean any changes occur on the subscriber during the data validation portion of the sync, it will fail because the snapshot values will not match the current values.
you cannot put "selects" into a column, unless it selects only one column please do tell us the column names hiding behind the asterisk
Any time you can avoid reading XML the better. It's stored as BLOB data which is kept on separate page files and BLOB data is so to be accessed. You can Create the view WITH SCHEMABINDING which locks down the source table from changes and you can index over these columns. It's really not the best situation. If you can get the data out of XML and into it's own table, it would be much faster.
If you're using a GROUP BY clause you need to include every field that is selected but is not contained in an aggregate function (in this case, all of your fields except MonthMargin): GROUP BY Dim.Employee.ID, Dim.Employee.EmployeeKey, Dim.Employee.FirstName, Dim.Employee.LastName, Dim.Employee.JobDescription, Dim.Employee.DateHired, LEFT(Fact.Sales.Final_StartDate,6) 
If that course is free, why not. It doesn't look particularly detailed though, and I'm hoping there are better free resources out there for you. I do think that hands-on experience is going to be best if we're taking what /u/duke442games said seriously (I have never heard of the test and therefore am not familiar with its level of difficulty). I always recommend [W3 Schools](http://www.w3schools.com/sql/) as a great primer to get you up and running, but maybe seek out a resource that walks you through the AdventureWorks database as it teaches the various principles...
Thank you! That fixed it.
That all being said, while this is a terrible construct and should never ever be used for anything ever, here's a possible solution to your problem. CREATE TABLE Table1 ([Name] varchar(16), [Type] varchar(6)) ; INSERT INTO Table1 ([Name], [Type]) VALUES ('Granny Smith', 'Apple'), ('Macintosh', 'Apple'), ('Golden Delicious', 'Apple'), ('Florida', 'Orange'), ('California Navel', 'Orange') ; SELECT MAX(Apple) AS 'Apple' , MAX(Orange) AS 'Orange' FROM (SELECT Name AS 'Apple' , NULL AS 'Orange' , ROW_NUMBER() OVER (ORDER BY Name) AS 'RowNum' FROM Table1 WHERE Type = 'Apple' UNION ALL SELECT NULL AS 'Apple' , Name AS 'Orange' , ROW_NUMBER() OVER (ORDER BY Name) AS 'RowNum' FROM Table1 WHERE Type = 'Orange') Fruits GROUP BY RowNum
it's really difficult to determine what you are truly trying to accomplish but this can't be done in a simple sql statement. Looks like you might want to look into some scripting (bash, ruby , python) or a stored procedure) 
I have been writing SQL for another noSQL solution and sometimes I want to strangle the guy who normalizes the objects on the sql side. Why do you have that many joins on non-id columns? If they have different objects for name, address, ect.... they should all be joining to a.id on the tables. So lets say A is "person" and each person has an ID if there are one to one and one to many relationship objects off "person" they should all contain "person_id" that points to the parent object. Sorry typing on my phone.
If you're sure to append and prepend the string with "," then this should work; Like '*,xxx,*' Otherwise: Declare @table table (value varchar(255)) Insert @table (value) values ('1,14,16,19') Select * from @table where value like '%,14,%' -- anywhere or value like '14,%' -- at the beginning or value like '%,14' -- at the end or value = '14' -- only id available
One other idea. I'm used to optimizer engines smart enough to change this on the back end. Try reversing your joins as well. select a.* from d Inner join c on c.obj_id = d.party_id Inner join b on c.att1 = b.att1 and b.obj_id = d.acct_id Inner join a on a.ref_id = c.site_id Where d.status = 'Y' Real SQL engines use statistics to determine that d.status may be best to run first and then work there way backwards. SQL overlays like hiveql are less sofisticated.
I was going to suggest using regex to find a string but then realised you're using Access. Edit: Apparently this guy solved the problem: http://bricestacey.com/2010/07/09/Regular-Expressions-in-MS-Access.htm Regular expression: /14/g http://www.regexpal.com
One thing I noticed. The CONVERT function in your where clause makes b.censusdate non-SARGable, so you're forcing a full table scan of BedCount. Is there a special reason that you're casting numbers to character fields in your WHERE clause? SQL is possibly doing an implicit conversion of the field (BedCount, IsActive, etc) to a character, then doing an alpha comparison instead of a numeric comparison. 
I'm not sure I'm understanding. Is there a problem with just joining on the first 10 characters of the claim number? Or maybe first 11 characters, assuming the claim number length &gt;= 12?
Yeah it's strange that this isn't covered by the documentation, feel free to submit your feedback to Oracle, I've found a few posts on their forums where people have come across these issues before. Sad that the Roman Numeral format isn't supported, I doubt anyone needs to use it very often, but there are [solutions out there](https://livesql.oracle.com/apex/livesql/file/content_CESOH7H2D4O88XLW60330Q3L9.html). As far as I can tell TM is redundant and would be equivalent to EEEE in TO_NUMBER but you'll have to wrap it in a function to detect if the string is normal or scientific notation first. And I suppose you can just calculate V yourself :)
That is a good doc. I'm not sure if they post a doc like that because their optimizer always chooses one type of join and you have to specify a merge join when two tables are already sorted physically on the disk in the same manner **OR** it's because they are just being informational. I'm still very skeptical of your joins. Particularly c.attl = b.attl. Even if c and b are different objects the fact someone would name a column so alike that would lead me to believe they aren't unique and maybe it shouldn't be in the join but in the where. This would prevent a Cartesian . Ultimately, you probably have a Cartesian product occurring and that is why your query takes so long to execute. 
Yeah general consensus is that a join table was the way to go with some added logic. Tbh I've built far more impressive and larger systems, but this particular approach I've never needed before. Most other little systems are either simple joins for a one-to-many relationship, or just separate tables. In terms of 'how far' down Dev - I've only written the connections and parameters and built the schema. Could redo under the new approach quite easily - and though I get the logic of the new approach, if something went wrong or I had to pivot, it'd be an area of uncertainty for me as to how different behaviours might be exhibited - and its only 'allowed' to take me 2 weeks - plus I already said 'no' (don't wanna look like a dick :) ), and the team I'm doing it for are ungrateful, untechnical data entry people who think I'm a sick because I ask them to read one of the four emails I've already sent on how to allow macros, rather than answer 15 lazy people's direct emails for assistance. Next project though I may try this out. Appreciate the help everybody :)
Thank you ! This is really helpful. 
Ok, I see. I don't think there are "subtypes" as a function of SQL (I could be wrong though), so I think your teacher wants you to build the structure in such a way that you can easily query for tv shows vs movie. Is there any scenario where a tv show could also be a movie? Or would those have two separate entries for each in your "images" table? 
As previously mentioned, you can use a LIKE to get the data you want. However, you run the risk of your database ignoring your indexes for your search, and so it could be a very slow running query.
I think I found something that works to find the newest version of a claim after it has been adjusted to run back through the system for testing. The only remaining thing I need is to update the status for each claim in my large temp table. One thing I may have left off is that each claim that has been reversed &amp; adjusted stores what claim it came from. So if the system takes claimID = 1234567890A1 then the claim is reversed, &amp; adjusted and now the most recent version of that claim is "123456789A1" (NOT the R1 version). select top 1000 c.orgclaimid as 'Reversed Claim', c.claimID as 'NewClaimID', c.createdate as 'NewClaimCreateDate' FROM load_test as lt (NOLOCK) LEFT JOIN claim as c (NOLOCK) on LEFT(lt.claimID, 13) = LEFT (c.claimid, 13) AND c.createdate &gt; lt.createdate AND c.claimid = ( select top 1 claimid FROM plandata_parallel.dbo.claim as c1 (NOLOCK) WHERE ( c1.orgclaimid = c.orgclaimid OR c.claimid = c1.orgclaimid ) ORDER BY createdate desc ) The table 'load_test' stores the list of all 50k claims that will be used for the test. 'claim' is the master table of all claims in the system. So I am taking all claims in the test (load_test) and left Joining to the master claim table. The join conditions are that * the first 13 characters in the claim ID are the same (the remaining 2 chars are the "R1", "A1", "R2","A2" etc. * the claim's createDate must be &gt; the date that the claim was stored in the temporary load_test table * And that the master claim.claimID is the newest claimID returned when comparing the load test claimID to the c.orgClaimID or the current claimID (since some claims may not have been adjusted prior to my test, which means the OrgClaimID column is empty. Hopefully if anyone else in the healthcare industry stumbles upon this similar situation, they can have at least part of the solution. HMOs are always fiddling with claims and reversing them, paying different amounts based on convoluted decision trees, checking doctor's billing dept's work, etc. If you are worried about performance, the `load_test` table shouldn't only ever be more than 50k claims, but the master `claim` table contains 93million records. the `claim.claimID` column is a primary key, and `claim.OrgClaimID` and `claim.CreateDate` are non-unique non-clustered indexes as well.
&gt; Is there any scenario where a tv show could also be a movie? star trek 
Don't think there'll ever be more than a few thousand rows in any one table at any time (archived groups table notwithstanding), so whilst it may feel dirty (it does), I'm satisfied with the solution. Like using spit instead of lube or foreplay :p
Just to addon to your advice. In order to be specific you: SELECT LEN(MAX(b.fund)) FROM TABLE Which will tell you the maximum length of the value. Unless he is joining on this value and needs an index, then simply casting it as varchar(max) shouldn't do anything to the execution time because all it's doing is saying it *can* be max length, and then filling the field with the true length of each value. Not positive though.
I might be misunderstanding your intentions, but it looks like your ORs should be ANDs. It's saying to check for the codes which are different (which you want) or just bring back *all* rows where both codes simply exist (probably don't want). I don't even think you need to close these statements in parentheses either. Let me know if that helps at all.
One method is to us a subquery to provide a list of Transaction IDs update Transactions set Flag = 1 where TransactionID in (select TransactionID from Transactions where Product = 'Banana') Or you can use a join and do something like this (may need a minor tweak or two) update T set Flag = 1 from Transactions T inner join Transactions Tsub on T.TransactionID = Tsub.TransactionID where Tsub.Product = 'Banana' 
Ahhh, brilliant. Thank you so much.
I see. Do products ever end up with a third UPC? It looks like, in a perfect world, you would maybe have a PRODUCTS table, a UPC table, and a PRODUCTS_UPC table to map them together.
UPCCode can change whenever a vendor sends us a new file, Original Remains as far as I know. In a perfect world There would a be Products table with a single UPC and if that UPC changes whoever is submitting a UPC Change has to explain why the UPC is changing else it's added as a brand new product. Then there would be a vendor and season table that would say "I Sell this product, for this season, for this price during this time period." Right now it's basically free for all. Product 1234 can be; * A Red Chair Sold as a Normal Item for $12 That has UPCCode 1234 * A Red Chair Sold as a Christmas item(because it's red and red means christmas) for $11 that has UPCCode 1234 * A Red Chair sold as a Valentines Item(Because it's red and fuck making sense) Sold for $15 that has UPCCode 1234. * A cigarette Lighter sold as a normal Item for $0.50 that **had** UPCCode 1234 but now has UPC Code 1245 * A stuffed Reindeer sold as a Christmas Item for $5 that **had** the UPCCode 7593 but now has the UPCCode 1234 And the last one to be updated by someone is the one that scans at the cash... Every time something scans wrong I get a call because I am the path of least resistance in the Automated Phone System (Option 1 -&gt; Option 1) and I'm the one most of the stores like anyway since I keep their shit running smoothly most of the time. So you can imagine how violently I want our system to be revamped. Step 1 is going to be getting rid of the whole more than one UPC bullshit Step 2 is going to be clearing out the stuff that hasn't been active for years Step 3 is clearing out everything that's in more than one event Step 4 is getting rid of all the crap that isn't the same Item. And while we're doing all that Design a proper DB structure. that we can shift the data to and rebuild our Software around it.
I goofed. I was thinking about something different. I'm sorry. :(
Sort of my job. I'm not a full-blown developer - I just do a lot in Excel/Access/Oracle/MySQL/FTP type low to mid-level solutions. At my best I'm an average Dev. At my worst....
In sql sever look at datepart function. So datepart(year,your date) = 2015. To get the months to show jan, feb... Use a case statement. 
&gt; To get the months to show jan, feb... Use a case statement. To get the months to show Jan, Feb, etc, (in SQL Server) use the DATENAME function ;)
While I was actually asking more for code to allow me to update fields, this has really helped, thanks. :) I currently have this thanks to what you put: UPDATE dimDate SET Year = datepart(year, [dbo].[dimDate].[SupplierDate]) UPDATE dimDate SET MonthName = datename(month, [dbo].[dimDate].[SupplierDate]) UPDATE dimDate SET Day = datepart(day, [dbo].[dimDate].[SupplierDate]) Which works perfectly to fill up fields once I manually create them in SQL Server and leave them blank, but it's the ORDER BY I am struggling on now, is there any way to make the months ORDER BY when it is an UPDATE statement rather than SELECT? I just got errors when I tried to implement that.
You mention that space on your server is an issue. Storage space (disk) or memory space (RAM)? If memory is not an issue, I would consider inline views to simplify your join strategy on the slow remote machine. Test performance with this: SELECT * FROM OPENQUERY([MASTER],' SELECT A.[1...20] , B.[21-40] , C.[41-50] , D.[51-55] , E.[56-60] , F.[61] FROM SCHEMA.TABLE1 A LEFT JOIN SCHEMA.TABLE B ON A.1 = B.21 AND A.2 = B.62 LEFT JOIN SCHEMA.TABLE F ON A.1 = F.69 WHERE A.70 &gt; 2008 ') M1 LEFT JOIN OPENQUERY([MASTER],' SELECT C.[41-50] , D.[51-55] , E.[56-60] FROM SCHEMA.TABLE C LEFT JOIN SCHEMA.TABLE D ON C.64 = D.64 LEFT JOIN SCHEMA.TABLE E ON D.51 = E.67 AND D.68 = 0 ') M2 ON M1.1 = M2.41 AND M1.2 = M2.63 AND M2.65 = M1.66 WHERE HOST.SERVERFUNCTION1 = HOST.SERVERFUNCTION(A.5,A.6,A.7) OR HOST.SERVERFUNCTION2 = HOST.SERVERFUNCTION(A.5,A.6,A.7) OR HOST.SERVERFUNCTION3 = HOST.SERVERFUNCTION(A.5,A.6,A.7) OR HOST.SERVERFUNCTION4 = HOST.SERVERFUNCTION(A.5,A.6,A.7) OR (HOST.SERVERFUNCTION5 = HOST.SERVERFUNCTION(A.5,A.6,A.7) AND HOST.SERVERFUNCTION6 = HOST.SERVERFUNCTION(A.5,A.6,A.7)) OR (HOST.SERVERFUNCTION7 = HOST.SERVERFUNCTION(A.5,A.6,A.7) AND HOST.SERVERFUNCTION8 = HOST.SERVERFUNCTION(A.5,A.6,A.7)) OR (HOST.SERVERFUNCTION9 = HOST.SERVERFUNCTION(A.5,A.6,A.7) AND HOST.SERVERFUNCTION10 = HOST.SERVERFUNCTION(A.5,A.6,A.7)) This way you have two chunks of data being returned to your speedy local server for the completion of the join and execution of the local SERVERFUNCTION. I may have broken something by moving the join condition C.65 = A.66 between tables C and D into the inline view join condition. This may not match field results from table D, but perhaps that can be addressed by removing the condition and using CASE statements in your topmost select clause. HTH.
Storage. It isn't practical for us to pull all of these tables down just to run this query to get our results. These source tables would take up a huge amount of the space we have. Interesting approach though. I'll be playing when I get back to work after the holiday.
Inline views normally don't impact storage. They are temporary and built into memory until the query is complete. Then the memory is cleaned out. If the inline view is too large to fit fully into memory, it gets swapped to disk in the space *already* allocated to the swap file. Your query is already bringing back all the records from table A greater than 2008. Unless table C &gt; table A, then my version won't be bringing back any more data.
Our clients use sftp, pgp and all kinds of secure mumbo jumbo. We ended up getting nsoftware ssis task additions to finish our etl. https://www.nsoftware.com/adapters/ssis/ 
2008R2. The example query was not specific. I will update the post from the office with the nuances.
Cool thanks! I think my next big question is how big the dataset is.
Fairly big. The return before the where is applied is ~4M rows with maybe a total of 30-40 columns. I don't think the where does much to reduce the data, just treats it.
http://imgur.com/pPxfPbT
Thank you so much, I'm sorry for the late reply but I greatly appreciate your response and guidance! :) 
Thank you!! Seriously thank you so much, sorry for the late response
Thanks!!
Thank you!
Thank you!! 
Sho 'nuff. Happy holidays.
Without where ... in and joins:: select transactionId, product, max(case when product = 'banana' then 1 else 0 end) over (partition by transactionId) as has_banana from ...
EXISTS is usually faster than IN.
AFAIK, Oracle and MS SQL query engines generate the same execution plans for both syntax.
Hopefully they would, yes. But do you want to hope the query optimizer fixes the plan, or know that it will take the best path by writing it the proper way?
CodeSchool.com is where I learned. They have an intro course for free but then you have to pay monthly to get the rest of their stuff. Would still highly recommend watching the intro course, as it lets you practice after each lesson. 
Codecademy and Khan Academy both have great interactive courses for free that start at the beginner level.
Hi, if you know absolutely nothing but you want to start writing sql to pull some data, check out sqlzoo.net. It covers the very basic simple concepts that you would probably using to pull data from your database in the real world. The nice thing about that website is that you type SQL and submit your code and get results in real time. It will show you your result vs the expected result. 
Looks like you should be able to JOIN the table against itself on the match ID which would give you a similar result as a CROSS JOIN. When you self join remember to give each table a different alias. On my phone so I can't type it out well but it would look like Select * From table1 a Join table1 b on b.matchid = a.matchid There doesn't seem to be a way in your table though to differentiate which team which hero is on meaning you're going to get results for everyone against everyone. If you had a column for the team ID you could say WHERE the team ids don't match.
hero 1-5 is team 1 and hero 6-10 is team 2. I could probably have explained that easier in the OP.
thank you i will try my best
Well if you can order the original column by when they are picked e.g. Team 1: Hero 1 Team 2: Hero 2 Team 1: Hero 2 Team 2: Hero 2 Then you could just make a row_number() column. Then you could do something like case when row_number() %2 = 0 /*Checks if row number is even. Team 2 will always have even row*/ Then insert into temporary table column 2 ELSE Insert into temporary table column 1 Just wrap the CASE in a WHILE statement. You could use IF statements to if you're not comfortable with CASES or feel they are inferior :p That's how I'd handle it. However, I'm still very much a beginner at SQL. I'd try what /u/Trollfailbot recommended first.
yeah the rating is mmr adjustment :) I can add a team column if needed
Sounds overkill to use oracle for such a simple app? Why not a more lightweight alternative such as postgres or sqlite?
Is the first one cartesian??
I do not know the alternatives you mention. I have around 10 million rows in some of the tables so I thought I'd pick something that can handle advanced stuff aswell :) I will check out postgres and sqlite
Why would anyone be doing themselves a disservice to learn something? Oracle is still a major, major player in the SQL world...
Here is a tip. DDL = DAC = drop, alter, create
Thank you. I figured that out already, but instead of DAC, I remembered it as CAD. Reminds me of an Autocad image file lol.
Maybe you could use a timestamp with your logged in indicator and then use a MySQL Event Scheduler to check if s certain amount of time has elapsed since the last timestamp refresh. For example: if the date difference of timestamp and now() is greater than 20 minutes, then set the indicator to 0. There's probably more elegant ways out there, but this could be a quick and easy method. 
lol I guess I am old as hell. I might adopt yours. I passed the test and onto the next one.
The default constraint won't be used if anything is inserted in that column, be it a space or null. Skipping the column works, but not in the syntax you have above. You'll need to specifically omit the column on the insert like so: INSERT INTO Address (ID, Address1, City, PostalCode) VALUES (1, '181 NE 11th Ave', 'Portland', 97212) But I would also question the need for having 'N/A' in that column, it's logically representing null anyway.
The purpose of using N/A was just a test. I could have used anything but just threw the first thing in mind. I would never use that real world.
Gotcha.
I haven't seen a database with multiple log files for over a decade but, IIRC, you should just be able to run DBCC SHRINKFILE (file_name | file_id, EMPTYFILE), maybe wait a little while and then remove the extra log file at your leisure using ALTER DATABASE foo REMOVE FILE extra_logfile_name. The default log file usually (always?) has a fileid of 2, so I'd try to remove the other one. I'm not sure that SQL will even let you remove fileid=2. 
Alright thanks, but regarding my main concern, would doing this delete some transactions, or does SQL Server wait until the first log file is filled before adding to the second?
This solved everything! Thank you so much. I can now see every combination of counters and combos. Simply amazing :) 
If you remove the extra log file using the tsql syntax that Microsoft provides, you shouldn't lose data (tlog records, in this case). Iirc, emptyfile basically says "don't put any more data in this file". I would make sure that a tlog backup happens after I run the shrink file command and before I run the remove file command. That should ensure that any active log records are safe. If there are un-backed-up log records in the file when you try to delete the log, SQL should disallow dropping of the file. As i mentioned, i haven't used the EMPTYFILE option on a log file since maybe even SQL Server 2000. If I'm nervous about this sort of thing, or I'm worried that SQL will let me do something disastrous, I always practice once or twice on a test database. One test result is worth a dozen " Well, maybe".
I'm pretty mediocre at math and I've been working with MS SQL for years. I'd say that it probably depends on what you're using it for, but I've never had to use any math in SQL or programming aside from very rudimentary stuff. It (SQL) wasn't super difficult to learn, I just had to make sure I used it in a meaningful way whenever I could, just so I could get the practice. I also made sure that whenever I started a project that was complex or more complex than anything I'd done before, I'd try to figure out if maybe there were some tools I was missing and could learn about (functions, stored procedures, views, pivots, benchmarking, optimizing, etc.) One thing I really wish I'd learned sooner was how to take extra precautions before executing major updates. My first year working as a DB Admin snapshots and backups saved my life more times than I'm comfortable admitting.
I have learned several languages to a comfortable level and SQL is very easy and not particularly mathematical. It is of course an extension of set theory but it is mostly logic. That being said if you struggle with math you would most likely, though not always, struggle with logic. The idea that someone would have "potential" with SQL and Oracle, without having ever produced any work on these systems, is just silly. Your teacher is merely being polite to you. If you want to learn raw SQL there are a million free high quality resources. Learning Oracle outside of working at a large company where they have implemented it, is extremely difficult. If you want a test then run a database on your computer that sends you an email whenever a table has an entry added to it. This could be done at zero cost and for someone new to SQL this should take you about 7-10 days effort. Good luck!
My math skills are hovering solidly at a one. I work in Oracle SQL (and several other database versions of SQL) every day. The money is good. However, note that being an SQL developer, or a someone who produces output from a database using SQL, is far different from being an Oracle DBA. An Oracle DBA is a completely different animal. They have to understand how to install, design and customize an Oracle installation. And how to keep it running. And permissions. And a bunch of other things. So, Oracle SQL is a starting point. Being an Oracle DBA is whole different kettle of fish.
Could you just use the data directly from the REST API? I'm not sure why you have to store it before using it. But if you need to, your database schema would just have to mirror the data format of the REST API response data. Then you would just insert the resulting data into your table. But, again, unless you are caching the data so you don't have to make multiple requests, I'm not sure when you'd want to store the data VS visualizing it right from the API request. Also, make sure you read the t and c of the service to ensure you are allowed to store the data.
I'm not sure. What I really need to do is somewhat like this: Pull data from the API (REST presented as JSON I think?) and then do some calculations and estimations and charting off of. I dont think I can do that right from the software that can do the presentation, but I will look now.
What kind of database are you looking to import the data into? Can you retrieve the data in XML or text formats from the API?
That's awesome. Thanks. Yeah, I guess I didn't need a Case when it's in Where clause.
Like so: WHERE (@ISSUED = 'Plan Review or Issued' AND B1PERMIT.B1_APPL_STATUS IN ('Accepted', 'APPLICATION SUBMITTED', 'Approved', 'Approved w/conditions', 'Approved with Conditions', 'BOARD APPROVED', 'Corrections Required', 'HOLD', 'PENDING', 'Pending Inspection', 'Plan Review', 'Plans Approved', 'Revision Required', 'COA Issued','Issued')) OR (@ISSUED = 'Plan Review' AND B1PERMIT.B1_APPL_STATUS IN ('Accepted', 'APPLICATION SUBMITTED', 'Approved', 'Approved w/conditions', 'Approved with Conditions', 'BOARD APPROVED', 'Corrections Required', 'HOLD', 'PENDING', 'Pending Inspection', 'Plan Review', 'Plans Approved', 'Revision Required',)) OR (@ISSUED = 'Issued' AND B1PERMIT.B1_APPL_STATUS IN ('COA Issued','Issued')) OR (@ISSUED = 'Finaled or Closed' AND B1PERMIT.B1_APPL_STATUS IN ('Finaled','Closed','CLOSED/EXPIRED','Closed/No Final','CO','CO N/A')) Or, if you insist on using a CASE expression (but beware that this will likely not perform as well): WHERE CASE WHEN @ISSUED = 'Plan Review or Issued' AND B1PERMIT.B1_APPL_STATUS IN ('Accepted', 'APPLICATION SUBMITTED', 'Approved', 'Approved w/conditions', 'Approved with Conditions', 'BOARD APPROVED', 'Corrections Required', 'HOLD', 'PENDING', 'Pending Inspection', 'Plan Review', 'Plans Approved', 'Revision Required', 'COA Issued','Issued') THEN 'True' WHEN @ISSUED = 'Plan Review' AND B1PERMIT.B1_APPL_STATUS IN ('Accepted', 'APPLICATION SUBMITTED', 'Approved', 'Approved w/conditions', 'Approved with Conditions', 'BOARD APPROVED', 'Corrections Required', 'HOLD', 'PENDING', 'Pending Inspection', 'Plan Review', 'Plans Approved', 'Revision Required',) THEN 'True' WHEN @ISSUED = 'Issued' AND B1PERMIT.B1_APPL_STATUS IN ('COA Issued','Issued') THEN 'True' WHEN @ISSUED = 'Finaled or Closed' AND B1PERMIT.B1_APPL_STATUS IN ('Finaled','Closed','CLOSED/EXPIRED','Closed/No Final','CO','CO N/A') THEN 'True' ELSE 'False' END = 'True' 
Thanks, I'll take a look :)
Google EXECUTE IMMEDIATE or REF CURSORS.
A downside to this approach is that the optimizer has no chance to help because it doesn't know the query until runtime. So there will be a performance penalty. Also a potential security risk if an arbitrary string can be added to the SQL string Consider as an alternative a SELECT CASE like statement where the variable is passed as one of "X" choices, then a different SQL statement run depending on the value passed. Then the system can see all the possible queries in advance, and you've delinked the variable from actually being in the SQL. This obviously falls down if you have hundreds of tables you are choosing from dynamically... but then you've got other problems that will dwarf this issue.
That's a good point. There's actually only about 5 different tables to choose from -- so I can code the store proceduret to accept a parameter "A" ,"B", etc -- then the script will choose a tablename based on that parameter. That would limit the possible entries the user can pass in.
&gt; EXECUTE IMMEDIATE That looks like that will work. I'll come back if I have issues, but I'll run with that for now and give it a shot. Thank you!
Ah that makes sense, I moved this over to sysadmin and sqlserver, thanks for the input!
&gt;That is, if the physician address has a lat of 50.1234 that I can/should limit the list of centers to those with a lat of 50 You *could*, but there are two issues with this: 1. If a physician's latitude is very close to the next integer, how will you search that? I suppose you could take the bounding integers and use that as a search range, but that can become a wide area. 2. At different latitudes, the distance between meridians (longitude) differs. IOW, at the Equator, one degree of longitude is more miles than at the Arctic Circle. Unfortunately (for you), geospatial features were added into SQL Server with 2008. Since 2005 isn't really supported anymore (and EOL is only a few months away), it would be good to upgrade to a version which is supported and supports the features you need - [`STDistance()`](https://msdn.microsoft.com/en-us/library/bb933808.aspx) would be a **huge** help for you here, as you would be able to calculate the distances between the physicians' addresses &amp; the centers, then rank the results &amp; return the top 3. Without moving up to a release which supports geospatial data natively, you're left doing a lot of potentially complex math yourself if you want to get it right.
I work a hospital system and I was asked to do just this last week. You really should upgrade your SQL Server to run a query like this. We used a SSIS tool called Melissa Data to generate the log and lat from address. First thing you need to get your geo code on both tables UPDATE YOURTABLE SET GeoCode = geography::STPointFromText('POINT(' + CAST([longitude] AS VARCHAR(20)) + ' ' + CAST([Latitude] AS VARCHAR(20)) + ')', 4326) Then use a cross apply to find the nearest location. SELECT * FROM YourTable CROSS APPLY ( SELECT TOP 1 LocationName FROM Location Loc WHERE Loc.GeoLocation.STDistance(YourTable.GeoCode) IS NOT NULL ORDER BY Loc.GeoLocation.STDistance(YourTable.GeoCode) ASC ) fn 
you don't need STDistance() to generate distance between two lat/long. This will work: ROUND(ACOS(ROUND(SIN(3.1415926536 * Input.source_latitude/180.0) * SIN(3.1415926536 * output.latitude/180.0) + COS(3.1415926536 * Input.source_latitude/180.0) * COS(3.1415926536 * output.latitude/180.0) * COS(3.1415926536 * output.longitude/180.0-3.1415926536 * Input.source_longitude/180.0),13)) * 6371,3) source_to_match_distance_km 
`STDistance()` will also be more accurate. IIRC, the above math is valid for a spherical model of the Earth, but Earth isn't a sphere. Over short spans, it will probably be OK but as distance increases so does the error.
Sql server uses an elipsoid model of earth for geography stuff. If one wants to implement math functions one should compile it and import the assembly for speed.
well yeah, but the point being he upgrading to SQL Server 201x isn't a requirement.
I don't think some of you are giving yourself enough credit when it comes to math skills and sql. Using set based logic and the type of thinking you need to do complex sql does in some way involve math. Now, do you need to be a genius in math to do complex sql? Absolutely not, but you are in some way using math when doing complex sql. 
I'm on mobile so can't be too detailed sorry, but my rough idea would be to add a row number column to the select that is over a partition of Conductor and group by Conductor. This will reset the row numbers for each group. Then have a CASE statement for each of the three columns where row number = 1, 2, or 3.
&gt; Whatever the stddistance function does must be using about the same trigonometry, or even more complex (due to elipsoid) But it's also natively compiled code.
this would need some way to make the conductors unique as I'm betting he has more than one of each conductor with assorted phase-states
If the five tables are essentially filtered versions of a master table, I'd also consider using views instead of actual tables. I don't know specifically how Oracle implements them, but an indexed view can be as fast as a base table, and can even be persisted on disk. Then the updating etc is handled automagically. No SP even needed. Whether that's a good option really depends on your exact implementation.
You are using it correctly. Try replacing your Union with Union All. They behave differently if one of the sets has no results.
Yes, that is pretty much the same. An indexed view is just a materialized view -- which is pretty much what I am making. These smaller "sub table" have additional fields and indexes that will allow reports go faster. Having a stored proc simply allows us to control how often this sub table is populated. But I will look into a the materialized view as an option, thanks.
Looks like you're missing the closing paren right before the UNION. 
excel is the bane of my existence.
I'm not sure about the last transaction, but current ones are shown in DBCC opentran. You can also see the open\active VLFs of a log file with DBCC loginfo, any active VLF is held until the transaction is complete. 
If you're in simple mode, I think you're SOL as to what the transaction was. Once it has completed, the log data isn't really kept any further. I hate when people say this to me, so I don't like being the one to say it, but is a 20GB log file that isn't getting any bigger really a big deal? 20GB of storage is nothing, really, and if you're running that tight on space you might have other concerns than the process itself. In terms of analysis, you're looking for a single transaction that is pretty hefty to get a 20GB log in simple mode. It has to be in a single overall transaction, so that gives you a place to start investigating from. Do you have any data cleanup jobs running that truncate or delete from tables? Microsoft's own SSISDB that came out with 2012 is a little notorious for it's own cleanup job being a log destroyer- the deletes it runs depend on cascading triggers to clean up child tables, and the end result is one massive transaction that can end up with a log twice the size of the data file. I know that from personal experience. **TL;DR**: Look for long-running queries or jobs. Logs growing to that size in simple mode imply either one large transaction or many large-ish transactions running simultaneously. 
&gt;In our Oracle database we have a table for conductor, and a related table for cables, which is obviously a one-to-many relationship. Um...since each conductor can be related to more than one cable, and each cable can be related to more than one conductor, wouldn't that be a many-to-many relationship? I'm not familiar with the Oracle platform, but in Access, this would be represented by an intermediary table (a join table) that contains fields for the conductor IDs and cable IDs. If a conductor is three-phase, it would have three rows in this table, its ID being associated with each of the cable IDs. With this, it would then be a simple matter to build a query like you're describing, using the join table to pull the pertinent information from the other two.
Thanks for testing, thats curious. Maybe the elipsoid function is a lot more hairy than the spherical trigonometry?
&gt; Maybe the elipsoid function is a lot more hairy than the spherical trigonometry? Oh, it definitely is. I saw an example of it many years ago but have no idea where. So instead, a little light reading for you: * https://en.wikipedia.org/wiki/Geographical_distance * https://en.wikipedia.org/wiki/Vincenty%27s_formulae * http://forums.groundspeak.com/GC/index.php?showtopic=146917 * http://williams.best.vwh.net/gccalc.htm (you may be able to pull the algorithm out of the JavaScript here)
Please tell me that was someone who got their SQL terminology crossed and went overboard with their naming for columns and aliases. Otherwise I'm not sure what I'm looking at, but besides verbage also not the worst code ever depending on what they are trying to build....well except the SELECT * but that tends to be my query plan/output control/reporting side that knows that could break something in some situations.
try this one http://crunchify.com/create-very-simple-jersey-rest-service-and-send-json-data-from-java-client/ 
That's because you think logically as programming calls for. People do not. 
Therein lies the never-ending IT challenge -- getting people what they *actually* need versus what they *think* they need. It's easy to ask someone what they need, and give it to them. The problem is that they don't really understand what they need. This is where good analysts come in. Instead of asking what the users need, they ask what problem they're trying to solve or what challenges they face, and then work with them to figure out what they need. Quick "at a glance" reporting is key, especially in large companies where folks need to have insight into a lot of things. They don't typically need the detail, they need a useful and accurate summary.
&gt; need thats a great point and much easier said then done. Even when you probe and try to find the actual problem, you'll find its not what they really need. Its something that definately takes time to understand and some people in IT never get this point. 
You may find /r/learnsql a more appropriate subreddit. This community is more advanced.
So true, my case in health care--just because something can be reported, doesn't mean it merits the effort, there is no essential value, and thus the importance of business requirements analysis. In my case, senior managers are often promoted nurses, clinical types, they do not know or could not be expected to know, what key data elements are needed for the different levels of reporting (business, operational, clinical), that is where a good CIO with data smarts guides the process and delivers a reporting system matched to the client's needs. The first step is to stop all the unnecessary reporting that is consuming valuable staff time and go back to basics to determine what is actually needed for your business objectives, the timing, and the system to deliver those reports--in short, develop a report management system based on defined needs.
no downvote from me... i thought this was hilarious (maybe because it was New Year's Eve and i was feeling no pain)
Be careful though. A guy at my work turned off all the reports that were only viewed once a year. He managed to turn off many of the year-end reports. He is no longer with the company. 
My situation now and very common. Not too many organizations value, understand, or can afford a dedicated BA. I do both roles now also and try to provide the best advice I can to my 'clients' and rationalize reporting. Having some luck with SSRS in SharePoint, when I can get some traction.
https://www.sqooled.com
Storage wise 20gb is nothing, true. However, I restored a copy of the bak for this db, and after shrinking the LDF some specific queries ran much faster (from 40 seconds down to 1-2 seconds).
For what it's worth, I did take a look at the default trace and at least found when the autogrowths occurred, which was insightful. 
OK - Thanks. :)
Provide data.
I'm not sure what data I can really provide that would help past what's causing the error. Some original values coming from the subview could be: 06/08/2015 07/12/2015 08/23/2015 And the resulting dates in the view after conversion would be: 2015-06-08 00:00:00.000 2015-07-12 00:00:00.000 2015-08-23 00:00:00.000 
Write it like this CONVERT(datetime, '20151001', 101)
I've tried this, ends up with the same error.
&gt;Where ReceiptDate &gt;= '20151001' . I also tried this with no luck CASE WHEN TRY_CONVERT(datetime, ReceiptDate ) IS NULL THEN 'Cast failed' ELSE 'Cast succeeded' END AS Result; Every single one comes back as "Cast succeeded". 
&gt; There's no real way to make what we do with our software work by setting that column as a datetime data type considering it also has to store other things. You are incorrect and that is the actual source of your problem. Ask instead the question: "How do I properly handle answers with possibly different datatypes?" I'm sure you'll eventually stumble upon a kludge that allows you to do this, but don't be surprised when things break on a semi-regular basis and the code you use to keep it running balloons into some unmanageable mess.
Thank you for your critique, but it's not feasible at this point in time to rewrite that part of the software for this one feature. I appreciate you trying to help me without actually helping.
Sorry to lead with more questions but why are you using datetime? It seems you don't really care about time. Why are you not using datetime2? This likely won't solve a thing, but it's good practice now days. Lastly, if TRY_Convert returned "Cast succeeded" then why not just return the convert? As in... select try_convert(datetime, ReceiptDate, 101) as NewDates 
&gt; Sorry to lead with more questions but why are you using datetime? It seems you don't really care about time. I don't, but date type doesn't work either. &gt;Lastly, if TRY_Convert returned "Cast succeeded" then why not just return the convert? As in... Because that still doesn't work for some odd reason. 
While your response was rude, it was also incorrect. [Here](http://sqlfiddle.com/#!3/4a3ef7/1) is the more appropriate version.
Could you explain why this is a better solution? Is it keeping all the users in one table vs three? I was always taught to keep like data together and separate the non like data, but I've never run into a situation where an ID was used for more then one item. 
I'm confused by your statement "employee ids range from 1000-1999" well what is going in the 2k and 3k range? I think keeping the data in the same table is appropriate and honestly if each row in your employee table has job title you need another table for titles and have a foreign key in the employee table (job_id). 
Right now, the best solution I could figure out was the following: * I can access the primary keys of Players programatically, and write the schema by hard-coding the primary keys of these newly added rows in table Players. * Create a temporary table to store these primary keys of table Players. * Insert a row in table Game * use MySQL's LAST_INSERT_ID() to get the ID of the last row inserted into Game * insert into ArePlaying by selecting from the temp table. What are your comments regarding this approach?
Sell the results, not the methodology. From your manager's perspective, "I have an employee who knows R" doesn't add value in any immediate sense. "I have a breakdown of the drivers of X" or "I have a means of predicting Y with reasonable accuracy" does.
There are a lot of resources out there. Beginners may check the following one : http://www.studybyyourself.com/seminar/sql/course/?lang=eng (course and online exercices). http://www.w3schools.com/sql (course and quiz).
Imo, most reporting and BI does not really need much advanced statistics or analytics. It is a growing trend to do more of that though but if I where you I'd rather focus on doing BI and reporting well first. That should be an easier sell to your boss as well I would think. Improve on SQL and the tools you have already. If you are set on it though, you need to think a bit more about it. What does your stack look like? Many of them do come with obvious extensions for BI and analytics. See what you already have licenses for (e.g. newer editions of Excel, SQL server and share point provide just about all you need, if you are on MS stack). SAS is a whole other can of worms you should stay away from unless you already have access to it (it is a big decision to get a SAS installation). R is easy to get your hands on but probably way too much for what you need it for (regressions, simple data mining). It can also be hard to integrate in your work flow , depending how your stack looks like.
The problem I ran into was that too few people understood the value of any statistic more complicated than "average", and didn't care when you told them. They didn't even care about the limitations of using only "average". Well, screw it. I"m still getting paid. 
Thanks, I get a lot of pushback when trying to introduce something new. Most people on my team the type who just want to be content and not push any boundaries. Obviously not the type of personalities I like to work with. 
thanks, we're mostly using SSRS, SSIS, Tableau. We do most things in Toad but some SSMS. My boss despises excel mostly because we work in IT and too many people build applications in excel and access that we eventually have to clean up if they ever leave the company or if they transfer. 
In my case, it was the people that were getting/using the reports. My team is fine with whatever as long as there's a procedures doc they can follow, a few of us are the developers, the rest just run what we build. We use SAS for a few processes that involve really big files, and that's pretty easy to teach people to run stuff in. Never tried it with R. I started to learn R, but could never figure out what to use it for, so I lost interest. I really did reach a point of "whatever, I'm getting paid". I work for a major bank, part of what my team does is produce reports that can end up in the hands of major corporate clients. You would think we would have access to some actual report generation software and maybe some contacts with a department that understands graphic design/report layout... that sort of thing. Nope. It's just us, Excel and PowerPoint. So fuck it. If it doesn't matter to them, why should it matter to me? I do my job the best I can and don't worry about all the rest anymore.
Okay, so you're not running SQL 2012 or higher - that rules out TRY_CONVERT which does work. Let me do some experimenting to see if we can replicate the same functionality in a CASE statement (unless someone has already). I think the reason why you're seeing different results has to do with how/when the functions are called within SQL - querying the field directly from the view invokes the function. However, if you query the field indirectly the function doesn't appear to be called - instead the engine seems to be basing everything off the datatype of the field. That would explain why it passes those tests without being able to return data. 
I've tried that before too, it ends up converting all the records to what seems to be dates and you can query it with a select statement, but as soon as you add in the query of: ReceiptDate &gt; '2015-01-01' ReceiptDate &gt; convert(datetime, '2015-01-01') convert(datetime, ReceiptDate) &gt; '2015-01-01' convert(datetime, ReceiptDate) &gt; convert(datetime, '2015-01-01') it returns Msg 8152, Level 16, State 10, Line 2 String or binary data would be truncated. I also tried out just to make sure: CASE WHEN ISDATE(CONVERT(datetime, ReceiptDate)) = 1 THEN 'Cast Succeeded' ELSE 'Cast Failed' END AS CastAttempt And every row comes back as 'Cast Succeeded'. I can even order the dates, but as soon as I try to select by one of those previous statements, error.
Well, short of using regular expressions (which are S-L-O-W), I'm out of ideas. Sorry dude.
Have you talked about many to many relationships yet? In 3rd normal form you would probably need 3 tables here. Attributes, Services, and a reltn table mapping them to each other which has an attribute and service id
Hey! Sorry for not providing any details, but yes the free lessons are structured for beginners. It starts with the very basic concepts of SQL. Once you know the basics, there is even an open playground environment where you can practice with a new set of data.
The guy above has the best example you are going to get with the amount of information given.
You need to define a few variables. 1. VarPath- path where the files are located. 2. VarFile- file name. 3. VarFilePath- full file path. Some tips: #1 should be the full path to the file and not a relative path. Ie: \\server\folder \ Not D:\folder\ #2 and #3 will be expressions that you build in expression builder. Use a loop container to cycle through all of the files. After importing a file move it into an archive folderich so you do not import it multiple times if the SSIS package runs multiple times in the same day. Load the file to a temp table first, then move the data into the destination table. Make sure that you truncate the temp table before and after you use it (better safe than sorry ). Build a table where you log the file names and record counts that you load. Use this to double check that a file has not already been loaded (optional safety feature).
ill have to edit it a bit as its quite sensitive.
1) He's created the alias "A" for "[SOME DB].[TABLE]". When he selects A.*, he's saying he only wants the fields from [SOME DB].[TABLE]. If he had just written "SELECT *", it would have selected fields from [SOME DB].[TABLE] and the fields from the joins. 2) The "cnt" in "where cnt &lt;= 8" is referring to an alias for count( [CUSTOMER_ID]) He assigns this alias when he writes count( [CUSTOMER_ID]) cnt This is equivalent to writing count( [CUSTOMER_ID]) AS cnt 3) The XXX is the alias he assigns to the sub-selection created in the parentheses (alias "a") filtered to "where cnt &lt;= 8". He can then refer to this entire subselection as "XXX", and then joins this subselection to [SalesReports].[dbo].[tbl_SalesReport_Pivot]. 
This is a classic top per group problem. There are many ways to solve, but I usually find a derived table of aggregates values using a CTE to be clean and quick. ;WITH CTE_MAX_PER_STATE AS ( SELECT [STATE] = D.[STATE] ,[MAXAMT] = MAX(D.[OCCURRENCES]) FROM EMAIL_DOMAINS D GROUP BY D.[STATE] ) SELECT [STATE] = D.[STATE] ,[DOMAIN] = D.[DOMAIN] ,[OCCURS] = D.[OCCURRENCES] FROM CTE_MAX_PER_STATE S INNER JOIN EMAIL_DOMAINS D ON S.[STATE] = D.[STATE] AND S.[MAXAMT] = D.[OCCURRENCES] Basically, we are building a CTE with the Maximum Occurrences per State (regardless of Domain) to find the highest count per state. Once we have that, we join that back to the Domains list itself by State and number of occurrences, getting the specific record for domains based on matching occurrences (which also provides the benefit of showing records tied for the maximum amount, as you mentioned). Thanks!
EDIT: I saw your SQLFiddle example and cracked it open. I'm almost certain my description below is where the confusion is stemming from. Here's an [updated SQLFiddle](http://sqlfiddle.com/#!3/a065b/2) that should get you on the right track. I've read through your initial query and the responses, and I feel like I'm missing something. I agree with a number of the solutions presented here, but you say you're still having issues. Can I recap to verify I've followed the process you're doing to run this query? * You have a base table that stores a column with NVARCHAR(MAX) data of mixed data types. Some of the data in that field will be dates, and we want to be able to query around the rows containing dates in that field. * You have a view (let's call it SUBVIEW) sitting over the base table. In that view, you are casting date only data as as strongly typed DATETIME column before returning it for query. That should look something like this. _ CREATE VIEW SubView AS SELECT [PK] = BT.[PKField] ,[ReceiptDate] = CASE WHEN ( ISDATE(BT.[DynamicField]) = 1 ) THEN CONVERT(DATETIME, BT.[DynamicField]) END FROM BaseTable BT * You attempt to query the SubView using the derived column [ReceiptDate] by treating it as a date, but receive errors about the conversion. As an example, the query should look something like this (over the view containing the case statement with ISDATE() as above). _ SELECT * FROM SUBVIEW S WHERE S.[ReceiptDate] &gt;= '20151001' I have a suspicion (perhaps unfounded) that you might be taking the CASE statement forward into your query in #3 above, when you've already attempted to convert the DateTime in #2 above (without a CASE statement). If that's true, the reason it might be erring is because it's trying to run the convert across ALL data, instead of using the CASE with ISDATE() to rule out non-date data first. I might be wrong though, which is why I wanted to recap. Thanks!
this is the classic "[groupwise max](http://jan.kneschke.de/projects/mysql/groupwise-max/)" problem
As pseudocode this works fine, but for it to actually run you have to put the main query in a CTE or derived table, like this SELECT state, domain, occurrences FROM ( SELECT state, domain, occurrences, RANK() OVER(PARTITION BY state ORDER BY occurrences) as [Rank] FROM table ) AS t WHERE [Rank] = 1 
agreed with the bad aliases! Given my limited knowledge surely when you declare an alias you would use "AS" rather than just declaring it after. for instance take line 2: FROM [SalesReports].[dbo].[tbl_SalesReport_Pivot] srp From what Ive learnt should be: FROM [SalesReports].[dbo].[tbl_SalesReport_Pivot] AS srp Or can the syntax vary based on what version we are using?
It depends. I know MS SQL (which I use at work) doesn't require the AS, so I've gotten used to reading it either way. I prefer it with the AS, because it helps me visually identify an Alias a lot faster.
I'm not sure why he put the WHERE clause outside of the "a" table and gave than an alias "XXX". It makes more sense hot distgenius wrote it, with HAVING instead of WHERE.
Not with pure SQL. With PL/SQL or T-SQL you can do something similar, e.g. using cursors. 
Glad you found an answer, unfortunately it is a wrong one
You could have a batch script that sets a variable to the day of the week. Then pass this to ssis. I have done this in the past.
Comes to ask an easy query question, you tell him to switch the database lol. What is this stack overflow
You should be banned from commenting with your clear lack of knowledge
Your edit explains your issue. Reasonably priced and Oracle aren't usually in the same sentence together. You might find some web-based classes but probably no hands-on classes.
I HAVE ~~PEOPLE~~ SQL SKILLS!
Not a bad idea. Although it would be nice to have something like a cert on the resume.
Analysis and corrections for pervasive-based conversion of insurance data.
I rewrote this a few times. I found a pretty simple way using a correlated subquery. I strongly recommend you do not turn this into some recursive method. Normally, you get these requests and we write rank() or row() aggregate queries but when you start ranking and putting order bys and then reducing those datasets, they can be very costly as many rank() and row() over commands can scan entire tables. If timestamp is indexed, the below query should run pretty fast. I'll always be the last to tell anyone to use correlated subqueries but if you want just 1 result, this is probably the fastest way to do this. SELECT O.Reason, (SELECT TimeStamp FROM Observations O1 WHERE O.TimeStamp &gt; O1.TimeStamp AND O.Reason &lt;&gt; O1.Reason ORDER BY O1.TimeStamp DESC LIMIT 1) as FirstTimeStamp, O.TimeStamp as LastTimeStamp FROM Observations O ORDER BY O.TimeStamp DESC LIMIT 1 **EDIT** This isn't right either. I need to fix the correlated subquery.
&gt; pervasive-based You mean like Pervasive Software? Or at least the company that used to be called that...I think they are Actian now.
Try MS MTA route for cheap certs and you can self-study with SQL Server Express. The tests are fairly cheap with MeasureUp preview with coupon codes. 
[removed]
To test, I write queries in pgAdmin, then in Tableau for data visualization. 
If you're good with scripting / c++, then go this route. It's been my experience for the bulk of DB people, scripting is not a strength. The opposite is true of software devs, great at code, not great at data. Think it has to do with minds that work better transactionally vs set based. In the file name expression, build the file name. If you want FILE_TUESDAY.TXT it will be something like this: "FILE_" + cast (sysdate (), mmmm) + "_.TXT" You will have to work with it.
Lets say I am writing an application like Reddit. I use SQL to select the subreddits the current user is subscribed to, then I use SQL to select the set of stories for the default subreddit and display them (ordered by popularity in descending order of course). When a user enters a comment and clicks save, I send an AJAX request containing the comment data to the back-end where I use SQL to insert the data into the comments table, then I user SQL to refresh the comments for the current story showing this users comment at the top. If the user chooses to edit the comment I use SQL to UPDATE the database with the new text. If the user deletes the comment I use SQL to either DELETE the comment record or more likely I UPDATE the comment record with a deleted_flag set to TRUE. Not only would I do this with a Reddit type app, I would do it with every web application I have ever developed (business apps, maintenance apps, ecommerce apps, CMS apps etc.). Every large business application has a back-end which Creates, Retrieves, Updates and Deletes data from a database, which is why you sometimes see the term CRUD. Now I have used all types of SQL servers (such as MySQL, MS SQL Server, Oracle, DB2, Informix, Postgres etc) and they all have different tools to allow you to dynamically create and test SQL statements before inserting it into my code. So for mysql I might use PHPMYADMIN or TOAD tools to access the MySQL server directly and look at my tables, build SELECT queries etc. and only when they work in there will I promote them to my code. In my code (currently PHP) I will use the PDO library to create the SQL query in the source and issue a PD command to send it to the MySQL server and get the results, using another PDO command to loop through the results and build some HTML output to show in the browser (or JSON output to send back in response to an AJAX request). Re visualization on the client (the browser in Javascript) I most often show the selected data in tables, though sometimes I will output a chart using a JS library called Highcharts.js (such as a bar chart or pie chart etc.). 
Much of my SQL is in support of a collection of .Net applications. However, I also do a lot of data manipulation with SQL.
### Query programs: * For SQL Server, I use SQL Server Management Studio * For MySQL, I use MySQL Query Browser * For Oracle, I use Oracle SQL Developer * For other, I use AQT * I basically use the tool made for the particular DBMS ### Do I export results into Excel? * Sometimes, for my job, to validate results * Moreso, at my prior job, to provide ad-hoc analyses * Moreso for personal projects ### What do I typically write SQL for, in my job? * Testing data warehouse logic and results * Embedding within SSRS reports * Embedding within SSIS ETL packages
Is this for auditing purposes? When you say you need to track the changes, how will you be keeping track of it? I.E. Will the table you're copying to look like an exact copy of the table, or will it be different in the sense that you've created a trigger to insert a row based on the changes made? So for instance, you updated an employee's address (but nothing else). The trigger will check updated, then insert into X table that address was changed (rather than doing a snapshot of before and after of the entire row). The reason I'm asking is because if you were to do a straight copy of a row, it's not necessarily the greatest idea. You would be tracking all changes, which meant even a simple fix such as updating city/state/zip/employee ID/employee name/whatever you have in the system will be triggered and written to the table. In addition, that means all tables where you need back ups will have the same exact table schema as the original table. Can you imagine the size of the table by that time, along with how hard it would be to find relevant information? On the other hand, crafting a trigger to only write out WHAT specifically changed...that may help your auditing purposes. I.E. If first name changes, it writes out a row specifying the before first name and after first name, along with a datetime stamp of what changed.
Yes. Edition should not matter in most cases.
Thanks for that - I've saved it for later perusing, though I'm not sure when we will upgrade to SQL Server 2016.
Thanks for your help!
Ah ha! The row I seek will be the row with the largest timestamp such that the reason of the prior row isn't equal to it. That makes sense. Thank you very much. EDIT: And the only exception to this, when the table only has one reason, I can handle separately.
I deal with the god damn database so the programmers don't have to.
Oracle SQL developer
umm, I think I did, its SQL developer? I stated it in the title
"SQL Developer" is also a job title. I read your post title as "I'm a SQL Developer, and I need a simple, fast answer". If you're referring to the *product*, call it Oracle SQL Developer. And then call Oracle and tell them to rebrand the product with a less confusing name.
I'm not familiar with Oracle but that looks like the results are set to 'return as text' instead of 'return as grid'. That would explain it for SSMS at least...
Maybe try LTRIM() or RTRIM()? I could be giving the wrong answer though as I'm not sure the question :)
Ah, left joining it did the trick; now it's nearly instant. Thank you!
Thanks, yes, left join worked. This data comes out of the system in access 2000 format, and is overwritten regularly -- I link the tables from the database I created, so I don't feel like I have a lot of control over the indexes. So I think for this purpose the left join will do the trick.
k gonna think next time, but im not sql developer by any means.. YET :D
And how do i change that.. cant find anything
The RETURNING clause does this in Postgres.
This pattern is very common in BI environments, then it would be called a slowly changing dimension, and in your case a type 2 SCD. Triggers are sort of the common classic way to implement your scenario, especially in existing oltp environments I've come across this. Please Google for type 2 SCD for examples. *on phone, so curtly expressed 
To your credit, SQL Developer is a lousy product name and a mediocre interface. I recommend Toad for Oracle and there's a free version I believe.
&gt;layers of caching between the DB and the users Would this primarily just be common search results? &gt;Its certainly possible to run a very large site off a single large SQL database. I am pretty uneducated on the topic, I am just operating off something I briefly read that for a high transaction volume you would use some intermediary process that would be faster than an actual SQL Server. But maybe this is a couple hardware/software version iterations old. Or maybe I have the wrong information/context. Either way, thanks for clarifying.
I'm a database developer not a data scientist. I always see data scientists job postings use ruby and python. What for?? What type of research requires those to be used vs sql?
Don't use Ruby, but use R, Python, your standard UNIX tools, and dabble in Java and C++ as well when necessary. I can't speak for everyone but in my area a large portion of the data I need to use is stored in various SQL databases. I write a lot of SQL code to manipulate and extract portions of that data for use in predictive modeling or machine learning applications. SQL is great for certain things but advanced statistical work is not one of them. I switch back and forth between R and Python depending on the needs of the project, who I'm working with, and existing code/infrastructure. R is traditionally a statistical language while Python is a true scripting language that has developed a into an excellent choice for machine learning work. Both language also often extensive data visualization libraries which is something that really can't be replicated in SQL. Data scientists often need to wear many hats and have to be able to adapt to constant change. R and Python end up being excellent choice as they are relatively easy to code in (can write a lot of code quickly), can be extended to work with virtually any language or data structure/system, and are generally fast enough to do the computations needed in a reasonably amount of time. 
Thanks for the gold! OUTPUT is incredibly useful. My favorite use has been for logging a few gigantic MERGE statements so I know what actually happened with the $action special field. Being able to turn essentially any DML query into a query that returns a result set or a second INSERT is incredibly powerful. Oracle and PostgreSQL have the RETURNING clause, but that only allows variables I think. OUTPUT is much more powerful. 
Not if it makes sense, and the way you describe your breakdown seems to make sense. The proper term for this kind of structuring is "normalization". Here's a good tutorial: http://www.phlonx.com/resources/nf3/ (There are additional normal forms but they deal with unusual edge cases. Interesting to learn but not vital.)
So the more tables you have doesnt necessarily mean your database will be bigger if you account for normalisation which is a standard practise in database arctiechture keep in mind that this is only true for normalising many to many table relationships.
As others say, it's not a bad idea and makes logical sense. But just be aware that joins across tables have a cost associated with them. There is a trade off between normalization to save space vs. performance in making those joins, etc... In this case it likely does not matter. But you do not need to go crazy with the normalization either. If you feel having a redundant key in one table suits your needs better, by all means go for it :) (of course keeping in mind growth, future scale-ability and so on)
Hi thanks for the response. What do you mean by "redundant key" in the table? Are you referring to extra column for having a foreign key?
Depends on how it's going to be used... proc can't be called from a SQL statement. Regarding returning a cursor, I meant a sys_refcusor. Something like this (although you would have to loop through the results). May have typos/syntax errors. Typed quickly. CREATE FUNCTION GetEmpDept RETURN sys_refcursor IS v_EmpDeptResults sys_refcursor; BEGIN OPEN v_EmpDeptResults FOR SELECT E.LastName, D.Name FROM Employees E INNER JOIN Departments D ON E.DepartmentID = D.ID; RETURN v_EmpDeptResults; END; For the sake of discussion (not argument), how would you do it different in a stored procedure to make that a better choice?
If it's always a one to one, and it makes any kind of sense that they be in the same table, I use one table. It makes for simpler queries and those that come after you don't have to hunt for that extra info. Performance wise, unless you have a LOT of data, or an underpowered server, it's probably not going to matter enough to worry about it either way. The basic idea behind normalization is that you only have a piece of info in the db one time. For Shop info, such as address and the type of merchant, that makes sense to have it one table to me. Others will disagree, I'm sure.
Just don't go crazy with the normalization. Third Normal Form is probably the highest you would want to go for most databases. Only some uncommon "big data" applications would want anything more than that. There's also something to be said for Second Normal Form for some applications as well.
Yea exactly. Sometimes instead of having to join just for a single piece of data, it might be better to just have that included. It's very situational and while it goes against normalizing standards sometimes performance is more important. 
Are the databases that experienced failures on a different server from where the query executes successfully? What is the compatibility level on the databases that experienced failures? 
FROM selects the table that you are extracting the data from. COUNT selects the count of all of the prices. WHERE is the clause that selects only those records where downloads is greater than 20000
Sounds like you should do some research on the logical processing order of operations for SQL. This is the order in which the SQL is read/run internally: FROM WHERE GROUP BY HAVING SELECT ORDER BY There are some more details to know, but that's the gist. 
You can trim your spacing either on the right or the left. SELECT LTRIM(AVG(invoice_total)) FROM invoices;
It was the compatability level. I have no idea how a few of these got set to compatibility level 80. Thank you!
I understand what you're saying, and in many programming languages when they have to query a DB they have the "from" statement first, as you thought it should be. However, this is just how SQL operates and I feel it's easier to quickly parse. That's just the order SQL needs to have the statements in in order for the syntax to be valid, I'm not sure of the actual reasoning created back in the day.
The COUNT() is a method. The capitalization I think is throwing you off. SELECT price, count(8) FROM fake_apps WHERE downloads &gt; 20000 GROUP BY price;
I don't have an answer for you, but this may help you with your debugging Try running the following query to view some configuration differences between database DECLARE @DB1 TABLE(name VARCHAR(MAX), minimum INT, maximum INT, config_value INT, run_value INT) DECLARE @DB2 TABLE(name VARCHAR(MAX), minimum INT, maximum INT, config_value INT, run_value INT) EXEC database1..sp_configure 'show advanced options', 1 RECONFIGURE WITH OVERRIDE EXEC database2..sp_configure 'show advanced options', 1 RECONFIGURE WITH OVERRIDE INSERT INTO @DB1 EXEC database1..sp_configure INSERT INTO @DB2 EXEC database2..sp_configure --Configuration options that exist in the first database that differ from the second database SELECT * FROM @DB1 EXCEPT SELECT * FROM @DB2 EXEC database1..sp_configure 'show advanced options', 0 RECONFIGURE WITH OVERRIDE EXEC database2..sp_configure 'show advanced options', 0 RECONFIGURE WITH OVERRIDE
No problem, glad you got it resolved
No. There's a name for following that specific flow of syntax when it comes to keywords. Looking for the specific term.
Order of Operations
Are you really using "group" as a column name in the **a** table? Using [Reserved Words](https://docs.oracle.com/cd/B19306_01/em.102/b40103/app_oracle_reserved_words.htm) as column names is a really really bad idea and it's likely to be a cause for your issues.
Also worth noting is that if I remove every aggregate column from the final select query, then it works fine (I don't get an invalid identifier error on aa.reporting_week_of_year or aa.group).
In the second query, why are you grouping by aa.asin_group instead of aa.group ?
Your reply, and the other guys reply showing 'stack' used by stackexchange were very interesting. &gt;user who initiates the write will see their screen updated immediately as if all the users see this but it is kinda faking it - the actual DB update may not occur for several seconds so other users would not see it immediately. This seems like a very interesting balancing act. Thanks!
Then why would you want an employer to be under the impression that you have a working knowledge of SQL?
SQL is like a bad of sand. Tell them that and they will know you know your stuff.
Do not claim to have skills that you do not possess, especially when it comes to data management as these assets are often critical to the day to day operations of a modern organization.
So if the job didn't request it why did you say you knew it?
A medium skill in SQL? I've been working with SQL in a professional setting on a daily basis for over 15 years and I'm approaching the lower end of medium with my skill set, though it is by no means my primary focus and merely a resource I access and work with on a regular basis. 
Understandable. You are correct - it's not a "cursor" in the same sense as something like SQL Server ... *headache*
Well the Order of operations don't flow like what the OP says, but idk what else he would be talking about. Query? Formatting? Idk, that's the only order the statement can be written in so I don't think there was ever a need for a different word
At 20,000 records a day and a table of 200,000,000 rows, this means you're holding a little over 27 years worth of data. One has to ask if all the data is currently relevant and if a good share of this can't be moved off into another table for archiving purposes or, better yet, to an external file that can still be accessed if needed. 
I use a numeric int for my "ID" in my dimension tables. I typical call it a Key and not a ID. So AddressKey and not AddressID in my DimAddress table. I have been using SQL server's GUID more lately 
Keyed-In Order, as opposed to Logical order used by the query optimizer. Reference - https://www.reddit.com/r/SQL/comments/3zqu35/noob_sql_question/cyohqb7
/u/Twankenstein, this is good advice right here. Ola's scripts may not be all things to all people, but for a *very* large portion of the population, they're immensely useful. Your goal right now is to get a handle on what's happening and tidy things up to keep performance at a consistent level (sluggish, but usable and consistent is better than today fast, tomorrow impossibly slow IMHO) - then move on to making it faster.
SQLite is very good. Especially if you are hooked on SQL but don't want to deal with DBA overhead.
You're right. A partitioning solution would be the best long term solution. This particular table contains all the grantors/grantees of each recorded property for Los Angeles county since 1983...so 32 years of data.
If you are using MSSQL, you can try looking into Lead/Lag - http://blog.sqlauthority.com/2011/11/15/sql-server-introduction-to-lead-and-lag-analytic-functions-introduced-in-sql-server-2012/ 
Not using MSSQL and interested in an ANSI-standard approach.
Thank you for the quick reply. I put the single quotes around the field names and aliases because without them I get an invalid column error. I'm not sure I understand the string comment (very new to this as I am a nurse being thrown into a position requiring sql) As for the &lt;&gt; you are correct I got confused by that. What is most confusing however is with the original query I posted I get records with either Bleep or Bop but the Type_of_Decision column is completely blank. 
Shouldn't matter on dimension tables unless they're getting quite large _and_ you're constantly inserting into them. But you probably aren't constantly adding dimensions, are you?
 SELECT Material FROM @InvTable AS t WHERE EXISTS ( SELECT NULL FROM @InvTable WHERE Material = t.Material AND Inventory = 'No Stock' ) 
rows in a relational database table do not have any sequence, so "above" makes no sense
This. You can determine duplicates using GROUP BY and COUNT. Tables are sets not linear lists. 
Our as400 group uses strings for their Ids.. it's infuriating.
What or the most popular "interfaces" for SQL or for python? Does this question even make sense? Google results do not seem to think so. :(
&gt; as400 You have my heartfelt condolences.
The best way to handle this fully depends on your data model, data volume, configuration and user activity among others. A simple start would be to figure out how often you think this function/procedure might be called and how big (no. of rows and memory size) the result set could get. Is this something that might be called every once in a while from the GUI or is it heavily used in some batch process? How often would you expect the data to change in those tables? Could the result from one query be shared among users or is it user-specific? IMO using a function is not at all "dumb" if it a) simple to design and implement b) easy to maintain and debug c) scalable with the number of connections/users and platforms it can be ported to d) gets the job done
Dude.... It is so fucking awful.... and we have to integrate it with out cloud based services....
The term you're looking for is IDE.
Awesome. Thanks 
And I bet IBM totes has a connector that enables cloud services to connect right up to old Betsy too. Don't get me wrong, the AS/400 is a rock solid platform...but it's ancient and dealing with IBM is...well...dealing with IBM.
Figured out a solution. See OP.
The worst part happened just before I left where the file import table had a incremental int id on it. However, they never had put in logic to handle what they did once they imported 32k files into the system. The server would assign an int id but the system would read it and floor it to -32768 and would log it as a failure with that file id. The software that processed the file used software we purchased and was no longer supported. I could have probably found the power builder tools to recompile it but the source we had for it had commits that we believe never were deployed. Whole thing was a mess.
If in a clustered index hopefully a sequential uid is being used.
Oh, snap, sorry about that. I interpreted the sidebar as "indicate if your question is about a particular RDBMS." Since I want a generic solution, I didn't so indicate. My mistake. I appreciate your effort and apologize that I wasted your time.
What will run this query exactly? is it from Management studio all the time or will it be a web app? If it's from Management studio you may have to escape all the special characters in those URL Also hoping all the URL's will be the same as it get more complex if other parts change (like the accounts payable part) If nothing changes then this is probably what you are looking for DECLARE @URLStart AS varchar(100) (I did not count the number of characters you may need more or less in each of these) DECLARE @URLMid AS varchar(30) DECLARE @URLEnd AS varchar(30) Set @URLStart = 'http://hostname/WebLink/search.aspx\?searchcommand=\%7B\%5BAccounts\+Payable\%5D\%3A\%5BSupplier\+Number\%5D\%3D\%22' SET @URLMid = '\%22\%2C\+\%5BInvoice\+Number\%5D\%3D\%22' SET @URLEnd = '\%22\%7D SELECT Field1 ,Field2 ... ,SupplierNo ,InvoiceNo ,@URLStart + SupplierNo + @URLMid + InvoiceNo + @URLEnd AS WebLinkURL FROM BLAH WHERE BLABEDYBLAHBLARGH If you are using a web app I would just construct the URL in app instead.
Thank you!! It will be running through a web app which I don't have much experience with. The all is called Prophix and it is used for budgeting and financial analysis. My understanding is that I can just drop this query into one of the modules and our users will be able to run the report. I will test it tomorrow and let you know!
IDs are the only thing in our database that's numeric... It's sad
Bloody Romans!
thanks everybody for the upvotes, I feel your pain too :) Next challenge will be to explain to my colleagues the reasons behind surrogate keys
That's alright, I can understand your line of thinking, and I definitely don't fault you for it. Besides, I enjoyed writing it out, I am just disappointed it wouldn't work for you.
Why would you store that value in tableA? It's already easily accessible in tableB. Unless the tables are massive and you're looking to optomize performance. It's generally a really bad idea to store the same data in 2 places, if you're going to use it often, consider creating a view with that value on it. SELECT * FROM TableA A INNER JOIN (SELECT MIN(Service_Date) as MinService, linked_id FROM TableB GROUP BY linked_id) B ON A.linked_id = b.linked_id
At least explain why
upvote for leading comma convention -- the pros and cons lean very heavily into the pro side
I do the same thing. Also useful to do a similar trick for complicated where clauses: "WHERE 1=1" then on the subsequent lines "AND X1=...Xn" Much easier to comment out quickly. 
Did not create, but I had to maintain (and to my shame, extend) a family of legacy data processing procedures. (Finance company.) Many of them were over 5k lines and had many sub procs at various points. (Largest was around 9.5k?) They were all called numerous times each night during end/start of day processing, luckily they were never ran during office hours. (Most of the time!) Some of the tables they touched were core system functionality and they were f'ing _massive_ so every extension was very hairy performance wise. Some of my greatest wins, and also my greatest loss, occurred deep in those trenches. Cold sweat even thinking about them now, jeez.
Nope. There is no such thing as a cross-database constraint, so I'm not sure what you're trying to see. Tables with the same or similar content may not even be named the same or visible to databases beyond the one the table is in. Through database connections you can look at the data dictionary from other databases, but there are no links between objects, unless you count (materialized) views that access foreign objects. 
&gt; yeah, i know, picky picky I take it just a little bit further: One "thing" per line, everything lined up (columns, table names, aliases, join conditions, etc.). Main difference from yours is getting the first column or table into it's own line rather than on the same line as SELECT, WHERE, etc. Easy to read, easy to debug, and worth the few extra seconds to type. SELECT tA.whatever AS aliasName , tA.anotherCol AS anotherAlias , tB.someCol AS someOtherCol , tC.something AS something FROM tableA tA INNER JOIN anotherTableB tB ON tA.joinColumn1 = tB.someColumn AND tA.joinColumn2 = tB.anotherColumn LEFT OUTER JOIN someTableC tC ON tC.someType = tB.someType WHERE tA.status = 'C' AND tB.someValue &gt; 2 ORDER BY tA.whatever ASC , tB.somethingElse ;
I like separating the tables (see select statement) and the keywords out, so *I* can clearly see what's going on. If a line is getting too long I can just put it on the next one, like with the select statement; select 5 columns then continue on the next line (I think 5 is nice). I also like CamelCasing in general, but I don't really use it for SQL, as it's a pain to type it out. This sql is completely made up, just to show the styling I like in my personal code. SELECT NUM.ONE, NUM.TWO, NUM.THREE, NUM.FOUR, NUM.FIVE, NUM.SIX, LTR.A, LTR.B, LTR.C FROM NUMBER NUM JOIN LETTER LTR ON LTR.LID = NUM.NID AND LTR.IE = NUM.IE WHERE LTR.ID &gt; 3 AND LTR.IE &lt; 3 AND LTR.COL IN ( 1, 2, 3, 4, 5 6, 7, 8, 9, 10 ) AND ( NUM.F &gt; 5 OR ( NUM.F = 0 AND NUM.G = 6 ) ) Columns are indented, keywords for the most part are to the left. If you're using a keyword like AND where it's a join, I like to indent it to show that it's part of the join.
Mine. SELECT [A].[Col1] , [A].[Col2] , CASE WHEN [A].[Stat] = 1 THEN 'Result1' WHEN [A].[Stat] = 2 THEN 'Result2' ELSE 'No Result' END [CaseStatement] , [B].[Col1] FROM [TableA] [A] INNER JOIN [TableB] [B] ON [A].[ID] = [B].[ID] LEFT OUTER JOIN ( SELECT [C].[1] , MAX([C].[2]) [2] FROM [TableC] [C] WHERE [C].[Bobbins] = 1 GROUP BY [C].[1] ) [C] WHERE [A].[ID] &gt;= @Param AND [C].[2] &gt;= 1 ORDER BY 1 DESC; edit: noticed a trailing bracket from Reddit formatting on the ORDER BY line. 
Back when I was a developer the most complicated logic I created was asset utilization and EPA-related compliance reporting. The former was complex due to the multiple places I had to gather data from on top of the various aggregating while also a lot of business logic, the latter was complex due to trying determine out of a total barrel of something, how much was EPA regulated material versus not on top of determining what portion of that was processed by either incineration, landfill, or reclaimation. I don't recall the exact lines of code for each but it was somewhere between 2000-4000 lines of code for either of them, maybe more. Nowadays as a DBA I don't have any really big monolithic pieces of business code to support or maintain (that's what developers are for :P) but I would say our index maintenance script is likely the biggest thing I activey work with; I'd be surprised if it was 1000 lines.
This. Plus if you're debugging someone else's coded that has shitty formatting, just use a code formatter to format it to your liking.
I have developed this as a standard, not the easiest to maintain at times, but I feel its pretty clean. SELECT a.col_1 AS some_name ,a.col_2 ,b.col_1 AS some_name_2 FROM table_1 a JOIN table_2 b ON a.col_1 = b.col_1 WHERE a.col_1 = 'some value' AND b.col_2 = 'some other value'; 
It's a glorious piece of work, too. I learned a ton and modified the shit out of it to expand it further. I'll post on Monday if I remember.
I like this. Put all schema in brackets and anyways define your table as well as column so that you know where they came from when you do multiple joins
Make sure to escape and parameterize so that it won't act strangely when you feed it garbage and it won't be vulnerable to SQL injection
There are certain elements of a query that will stop it from providing statistics, I'm not an expert, but perhaps make sure that your query is following best practice. Also, indexes are huge. Make sure they're made on the proper columns
Imagine normalization like a drop down menu. When you are filling out a form and you have to select what country you are from, it saves more space in the database to have a single complete list of all countries to choose from instead of typing in your country each time. That way, the country name is only stored once instead of every single time the form is filled out. The only new data to be stored is the countryID for that particular person. This also means if you want to rename a country, you change one row in the foreign table, not thousands in the primary. Look up database Normal Forms. In most cases, you want to normalize whenever possible. You can even split the parts of a 10 digit phone number into 3 different tables if you want to go to a very granular level. Or you can normalize email domains and split them at the @ symbol because lots of people use emails from gmail or other big personal email providers. Lots of tables actually save you space because you don't have to repeat information fully, you just reference it by ID. Normalization reduces redundancy and increases days integrity
yea i was reading about heidisql. ok i'll check that out. Yea i too don't really see much benefit form the gui. 
interesting. i'm trying to minimize learning new languages, and I understand python and perl are more or less equivalent for most people's needs. php i really don't know much about but if it's easier to learn then might be worth it. do you have a good resource?
I started with Microsoft Access, relational tables etc... then I did SQLplus which doesn't have a GUI and is command line only, then I used Linq in C#
Not created, but the most difficult bit of code I ever inherited was a query used in an old DTS package transferring data from Oracle to MSSQL. It was huge, way too over complicated with sub queries and joins going here, there and everywhere. Rather than redesign the code, new functionality had been bolted on over a number of years; put it this way at the end of one line was something like this: ))))))))))))))))))))))))) So suffice to say any new amendments were extremely difficult and the code took about 6 hours to run through a relatively small result set! It did teach me some fantastic lessons though, now as a DBA when I teach code writing I very much put an emphasis on simplicity! Most complex SP I have wrote, probably some integration piece transforming multiple large sets of data from dirty sources; no not pornhub, I mean text files :-)
Can you give me some of the pros? The comment out theory doesn't make sense to me. If you use trailing commas the only column you can't comment out without touching the rest of the query is the last column. With leading commas you can't comment out the first column. So I don't see much of a difference there.
&gt; Should I create foreign key relationships user.userid &lt;&gt; team.createuser, user.userid&lt;&gt; site.createuser, etc.? yes &gt; I know that you don't have to use foreign keys to execute JOIN statements - so are these keys really necessary? yes, they are -- they are the mechanism by which you will guarantee that there will ~never~ be a team created *by a user that doesn't exist*, etc.
it's not just commenting ;o) another advantage is [being able to spot missing commas](https://www.reddit.com/r/SQLServer/comments/1rkmrv/in_a_select_do_you_prefer_commas_before_or_after/cdo8067) more easily (here's [a second reddit thread on it](https://www.reddit.com/r/SQL/comments/2ww1oi/cant_get_a_case_and_join_combo_query_to_return/cov0thr)) another advantage is [editing in block/column mode](https://www.reddit.com/r/SQL/comments/30dn9k/formatting_sql_to_increase_productivity/cpsbgzt) 
Fwiw, I am still a novice but I remember reading something on the ultimate relationship database schema where you have a table of entities and another table of relationship types and then finally your many to many table that associates your relationships to the entities. Hoping I spark some Convo about this as I still had a bit of a time wrapping my head around it. Makes for some complex programming on the application side to display the data for the end user but was described as. the most efficient and dynamic way to have alot of relationships. 
Do your users have unique identifiers other than their username?
Agree with the 'yes you should create' In your example, if Bob (userId 2) is deleted - should his user record be deleted? Probably not. Maybe his status should be changed to 'inactive'. But also make a distinction between transactional data and audit (i.e. who updated) data. Audit data usually has different modelling rule.
Home Depot. I helped manage a web based pricing and shopping tool for bulk/pro purchases. Pallets of roof shingles, a truck load of drywall, etc. We had an issue where some vendors were pricing items "too high" or "too low". In some cases, they misunderstood unit conversions and were pricing individual roof shingles when they were supposed to price the bag (so, a bag of 100 shingles for $3). In other cases, they misunderstood board footage and priced the maximum length as the price per square foot (so, a 20' 4x8 red oak board for $50,000). In other cases, they juxtaposed product pricing based on what was on the screen in front of them (a pallet of 50 pieces of D4 vinyl siding, $22 .. a piece of vinyl siding J-channel, $1175). In other cases they were just being idiots. I had to hold a lot of meetings to define "what is bad" and "what is good" .. (versus, what is competitive or what is exploitative of market conditions). And I had to hold meetings because they said "when you compare the prices to common products you can tell they're off", so, "what is the definition of common" *** I created a stored procedure that: - obtained all Attributes for a product category (color, length, width, weight rating, etc) - isolated attributes with significantly fewer Attribute Values compared to other Attributes in that Category - this was my "assortment of like products" in a given product category - obtained the median price, and the standard deviation, based on unique vendor costs - isolated the outliers - generated a report to alert vendors that they'd potentially given an incorrect price to a product - used vendor feedback to update a table used to alert people that they were about to enter new "bad prices" and prompt the users to either adjust or accept these "bad" prices - store those results in the same table
https://www.udemy.com/sql-for-newbs/learn/#/ this one was great for me.
Yeah, you are right. Was just a quick thought of mine.
This will get you the top 5 for each team in MySQL. SELECT * FROM ( SELECT a.team, b.game_date, COUNT(*) AS row_number FROM teams_and_dates a JOIN teams_and_dates b ON a.team = b.team AND a.game_date &gt;= b.game_date GROUP BY a.team, b.game_date ORDER BY a.team, b.game_date DESC) teams_and_dates WHERE row_number &lt;= 5 http://sqlfiddle.com/#!9/913dd/19
The benefit lies in the fact that the majority of the time the first field will not be commented as it would be your unique identifying field for each row.
I started with MS Access way back in 1996! I'm a PostgreSQL guy now. With a bit of SQL Server I think good modelling is the best start. Read the popular books by Celko and Silverston. 
I got more involved with the use of it for dynamic website content management. Check out Atom CMS by Digicraft. Very good beginner stuff that also gives some practical application. Also read up on the manuals esp if you are using MySql. 
haha sounds good. Is data modeler in Toad same as in MySQL workbench? Yea I couldn't even export my table from modeler into the server.
I'm just starting. I tried out the [Khan Academy SQL course](https://www.khanacademy.org/computing/computer-programming/sql) just to see if I liked the language (And to fool around with the code without having to mess with setting up a server and whatnot). Then bought the [Learning SQL book](https://books.google.com/books?isbn=059655558X). Little by little. My most difficult hurdle is to find an actual use for SQL in my modern life, work or otherwise.
There are plenty of uses. In fact, you could argue everything can be structured and organized into databases. Just got to have a need and think of everything on a meta level. Things that can benefit from predictable data structures are best for SQL. So we can store, mine, and recall with accuracy all relevant details of things, processes, ideas, details, etc. The most difficult hurdle is if something is worth the time and effort to give it the structure. 
Best bot post evar
Thank you for the response! So, there is no problem listing that way..COOL! And, please read my comment, to the other person(/u/kfranken). Anyway, I ll quote here. refer, if in case you need what his reaponse was. Thank you again! :) &gt;Of course, I am not storing rows of 'not friends'. 'Cause it makes no sense at all. I am just picking up worst possible case - 'IF ALL ARE FRIENDS'. So, that gonna make so many(allot) rows. I was wonderig, if that't gonna make any difference to performance. And, I am using 'enum' data type. So, I guess there is no problem related to space(memory). ____________________________ &gt;And what about the second problem? If I have some 100s of 'status_check', like, 'friend_or_not_status', 'private_info_is_private_or_public_status', and so on... What if I create different 'status_check_columns',.. 100s of colums.. is that gonna effect performance?? _________________ &gt;And there is a little bit more about the first problem: Think that, I have 1000 users. Just to make it simple, none of 'em are interested in being friends with each other, except one user. H is friends with all other 999 users. So, my 'friends' table gonna be filled with those 999 friends statuses being 'active'. So, I was wondering, is there any method, so that I can just conclude all those friendships in one row? "This user have these many friends and these are his/her friends - 'some_unique_id' " to refer to his/her friends..
I've found that http://sqlzoo.net/ is a pretty good resource to learn
This isn't appropriate for /r/sql. It also shows you didn't read our sidebar.
Yea I looked in the SQL logs and it wasn't there. It logs the username in the .bak file if the job is manually ran within SQL. Never heard anyone be that picky about SQL before. But thanks. If THAT is your "Pro" tip for SQL, you've got a lot to learn.
That's really helpful. I'll check out Heidi and then toad. 
Can't really answer 1 without more information about your schema. 2 would require a trigger on each table you want to watch but it smells like a bad idea in general. Probably something that should be solved outside the database. 
IMO triggers are bad mojo. Does it have to be instant when a record is inserted/updated? Would a stored procedure that runs nightly suffice? No one will be able to answer the joining question without a lot of knowledge of your database. I'm not sure of your knowledge level, but you can join on things that aren't foreign keys, and that's likely what you'll end up doing.
&gt;By schema I mean what your tables look like. You need to find some way the data is related in some way. I can't really tell you any more than that with what I know about your data. Well, I know the data is related in some way. I can generate reports(using an external application) that can output not only the table I'm having trouble joining, but also the associated customer table. But for the life of me, I can't figure out how the report creates a relationship between the two tables. &gt;What are you trying to do in the trigger? When a stored procedure that contains an insert statement is executed, I want to check if that insert statement meets certain criteria. For instance, if customer A's information is entered in the database, I want SQL server to check if that information has been entered in the past. IF if comes back as having &gt; 1 entries, AND also meets specific conditions, THEN alter table information. Like I said in my OP, I can't just use a normal trigger as this involves more than one table. 
Thank you so much for response. Cleared all my doubts. :) &gt; boolean values That's exactly what I was thinking, I just wasn't sure. Thank you so much, again!!! :)
Like I said, we'd to know more about your data to offer any advice on the joins. For the second part, you'd be far better off building that logic into your stored procedure or perhaps some layer above that, like the code for the UI. A trigger isn't going to be an option. Another possibility might be to have a second procedure run periodically that looks for your condition and handles it appropriately. That might be the better choice if you have multiple procedures inputting the data. 
Is there a way to find how many postcodes start with SP and how many start with SH and then compare the results?
Having a referential-constraints-less schema is bad practice. Having to make-do with what you got by joining on any column you damn please is fine. And indeed, eliminating the almost inevitable Cartesian product will be your job :-) You may find additional information in information_schema.columns, or other information_schema views. 
it shows me the postcodes but dosn't tell me how many there are? Sorry to bother you, thankyou for your time
I cant recall that there is a way to have the query compare the result for you, but this should give you the result for both SP and SH in the same result with the most used first and then you can compare them by yourself. SELECT * FROM (SELECT POSTCODE, COUNT (*)SUM FROM (SELECT SUBSTR(POSTCODE, 1, 2)POSTCODE FROM CUSTOMERS) WHERE POSTCODE IN ('SP', 'SH') GROUP BY POSTCODE ORDER BY SUM DESC) 
Install the MySQL Community Edition, then use some GUI application (I use Sequel Pro on Mac) to load, retrieve, export the data.
I looked at the data, downloaded it. If you replace the column and line delimiters on the command line you can import the tables straight into Excel. sed "s/\*\@\@\*/\n/g" extract_charity_aoo.bcp | sed "s/\@\*\*\@/\t/g" &gt; extract_charity_aoo.bcp.txt Are you interested in this data only once? Tell me which table you are interested in and I will run the above script for you and post the csv somewhere. Are you to offload this data regularly and you are not knowledgeable of commandline tools you may need to ask someone to help you building an automated process. 
&gt;1) I work with a database that has 500+ tables but almost no foreign keys. As a result, all my joins have only used primary keys. Your joins should usually be on A primary key between most tables, it almost never on the primary keys of two tables unless it's a table be joined to itself. You use the term "Foreign keys" but the actual term is "foreign key constraints", they aren't indexes, or keys... they are security measure to avoid deleting and orphaning records in an RDBMS. So lets say I have a "people" table and there is a column in dbo.people that is eye color. This column is an int and it has a foreign key constraint to dbo.eyecolor. dbo.eyecolor has "key" int column and a "Name" nvarchar(30) column. The values in the table are (1, 'blue')(2, 'green')(3, 'brown') If I attempt to delete 1-'blue', I will get an error stating "The DELETE statement conflicted with the FOREIGN KEY constraint "FK_People_EyeColor". The conflict occurred in database "mydatabase", table "dbo.People", column 'eyecolor'." If I attempt to insert or update a value into the eyecolor column in dbo.people to an int that does not exist in dbo.eyecolor, I will get a similar error. Some people don't watch the warning signs and will drop foreign keys to delete/update records. Some people never create them in the first place because they are meant to prevent the developer from doing stupid things and there are stupid developers. &gt;Unfortunately, a table I need information from a table that has nothing in common with any other table. If there are no columns that can be matched upon there is probably another table in between the two data sets. if dbo.people is a database of all people, dbo.cars is the table of cars, dbo.peopleCars is a table of cars people own and dbo.manufacturers is a table of manufacturers and you want to find the number of people who drive cars that are based in the USA you have to make all those joins. dbo.people.id -&gt; dbo.peopleCars.peopleid dbo.peopleCars.carsid -&gt; dbo.cars.id dbo.cars.manufacturerid -&gt; dbo.manufacturerid SELECT COUNT(*) FROM dbo.people p INNER JOIN dbo.peopleCars pc on p.id = pc.peopleid INNER JOIN dbo.cars c on pc.carsid = c.id INNER JOIN dbo.manufacturer m on c.manufacturerid = m.id WHERE m.country = 'USA' Two tables, no direct relation but they do when joined in a relational database. As far as #2. Don't use triggers, don't go down that route. It's only pain and self loathing. Write an app that checks the table and manages it and executes stored procedures when data is found and it must do what must be done.
wow so far i have to say Heidi is really easy and awesome! Thanks again!
These guys are very good 
Are you trying to narrow your selection to no more than one claim per customer? The most recent claim? Do you have a CLAIM_DATE field or are the CLAIM_ID numbers in a sequence? If so, I'll show you how to select just the max(CLAIM_DATE) or max(CLAIM_ID).
Thank you for the idea! I'm going to give this a shot when I'm back at work tomorrow.
Any recommendations on a GUI (that supports bcp importing) for Windows? Especially one that is simple to use. I looked at MySQL Workbench and apparently that's extremely clunky and confusing.
I am not an SQL expert, but I do use it a lot. I do not think you can update without locking. When I need to do big updates like that, I run them at night or some other minimal utilization time like Sunday morning.
An hour and 12 minutes for 5 bullet points? /sigh This would be much more useful as a text article.
first of all, you're missing the GROUP BY clause, so whatever the query returns right now is already wrong furthermore, the fact that you're joining FallSemester and SpringSemester and summing across both suggests that the SUMs might be grossly inflated through cross join effects finally, you say you want to compare "each record" to the average, but you're not showing each record, you're showing SUMs the method to compare to averages is indeed to use a subquery, but i have a suspicion you're going to need a UNION to handle the fall and spring totals
I took a shot at loading these files into a local MS SQL Server database and ran into a number of relatively minor issues: they used *@@* as line delimiter and @**@ as column delimiter. Empty fields occur and what happens at the end of a line: *@@**@@*@**@ It becomes quite messy. I resolved this to first replace the column delimiters, then the line delimiters. Then it appeared that some fields actually contain LineFeed characters, which I had to account for. Next up was the occurrence of the ANSI Control Character (BEL), which really shouldn't be part of the data. Finally, during loading the fourth table some other field appears to be larger than specified and I called it quits. These issues are nothing out of the ordinary, however, they prevent anyone (me) from performing an import within, say 2 hours. It would take any database specialist at least one day, which is spent on debugging data issues (or understanding what they did). I do think it will be handy to load this data into any SQL database, so that you can combine the data easily. You may want to try Microsoft SQL Server Express for free which may be used for DBs up to around 10GB. 
Yeah, thats sounds very logical to run it during non work hours. What do you do when you have to update records during work hours when many people are using it? Would you go batch by batch? What would be your approach?
Yes, I used the table build scripts; they are fine for MS SQL Server. Then, like I said, I started to alter the BCP files to clean them from any unwanted characters and tried to load them. The first 3 tables loaded correctly. Note that when you use the import wizard, you will have to alter the string lengths of several fields as is standardizes on 50 characters while a few fields are longer. Don't bother with data types; integers and dates were all correctly 'implicitly converted'. 
It really depends on the amount of data, how time critical it is, etc. Sometimes I will write the update script to run in batches of 500 or 1000 in transactions that have a rollback on error. A lot of times if you write your where condition correctly and limit the number of rows that get updated each pass, running even during peek utilization is possible. If you tell me more about what exactly you are trying to accomplish, I may be able to give some examples to help.
What DBMS are you using?
Be careful with MERGE, if the generated plan goes parallel it can cause problems.
This is a suggestion based on 0 hours testing and no experience with a table that is simultaneously transactional and referential. As others have said, it's best to do this at night. I assume that the reason you need to update it while others are writing to it is because you're also reading from it. If this is the case then I might explore using replication instead. I don't know how much control you have over your architecture though. But I think that in theory this could allow you to update a table during activity hours when others typically write to it. The script will check to see if people are not currently, at that millisecond in time, using the table. If not, the script will execute an UPDATE. During the UPDATE, nobody will be able to write to the table. SQL should be smart enough to store the request and execute the write request after the UPDATE is finished, but if you have enough users and if the UPDATE takes too long, you could end up with a collision. And your users will experience some lag while waiting their turn. I don't know of any way to simultaneously write to and update a table. Even ETL plug-ins like Pragmatic Works organize their work into one-then-the-other. I don't know if it's possible to do both simultaneously. I don't think it is.
| Table | Customers | |:--------|:------------:| | PK | CustomerID | FK? | All WarrantyID's associated | String | Name . . . | Table | Warranties | |:--------|:------------:| | PK | WarrantyID | FK | CustomerID | Date | ReceivedDate of application | Date | ExpiryDate of Warranty | Status | Open/Closed So one item I just realized might be confusing for anyone trying to help me is that the application number for the warranty claim is a newly generated WarrantyID number and actually has nothing to do with their current WarrantyID. So for example. John's customerID is 12345 His first WarrantyID was 111, and expired June 1, 2015. His second WarrantyID was 223, and expires Feb 1, 2016 His second WarrantyID is due to expire soon so he applies for a new warranty. When he applies, he automatically is given WarrantyID 334 (which behaves as an application ticket # until it is approved). We received it January 5, 2016, before his second WarrantyID expired. Sarah's current WarrantyID 556 expired October 1, 2015 and we received her new application given WarrantyID 667 January 4, 2016. This is well beyond 30 days after expiry. I'm trying to run a proper query that gives me all the Sarahs. But what I'm getting on top of all of the Sarahs are all of the Johns since the query is telling me "We received his application January 5th, and John had a WarrantyID expire June 1, 2015, so he is well beyond 30 days after expiry and will be included in the results". It is not recognizing that he actually did have a valid WarrantyID at the time of receivedDate Important Note: The WarrantyID behaves as their application ticket number and also (assuming it is approved) the same number will be their warranty identifier. What I'm trying to make clear is that there is no separate primary key that indicates application number to distinguish it from the WarrantyID. The only way to distinguish that is if the status of the WarrantyID is 'open' which means that a decision has not been made yet on whether we will approve it or not. I did the best I could to make this problem more clear for all of you wonderful people willing to help. Thank you very much again everyone. Also to be clear, I am using Oracle for this so apparently some SQL doesn't work for them that would otherwise be good solutions.
Hey there. Would you mind taking a look at my comment that I just posted? I tried to make the structure and problem more clear... I hope it helps. Thank you for your time.
They're terrible variable names, but it's completely valid (at least with MS SQL Server) As to whether it's a "mistake" or not...that depends upon the requirements and implementation of the sproc. Or are you looking for different syntax to do the same job?
I'm not absolutely sure I understand the problem, but why don't you play with this and see if we're coming close to the answer (replace 'xxxx' with an actual customer ID): Select C.CustomerID, C.Name, W.WarrantyID, W.ExpiryDate From Customers C, Warranties W Where C.CustomerID = ‘xxxx’ And C.CustomerID = W.CustomerID And W.ExpiryDate = ( Select max(W1.ExpiryDate) from Warranties W1 Where W1.CustomerID = C.CustomerID )
Thank you! I will try this. My issue is that I am attempting to get every customer that falls into the same situation as Sarah (applying beyond 30 days after their most recent warranty expired) and make sure that I don't get anyone like John (who applied well within their expiry). Right now I'm getting all of both because the query is supposed to target only the most recent warrantyID of each customerID regarding the expiryDate. Right now it is looking at every expiryDate of every warrantyID for each customerID. And I'm just not competent enough with SQL to correct it without breaking the query. But I'm doing my best. I'll try your idea now.
Hey, thank you. I replaced the real variable names just in case. They do handle some terrible variable names anyway. I suppose I'd use the syntax that /u/Elfman72 pointed out but I was curious about it and a Google search didn't help. Do you have an example of a situation in which that notation would be required?
It's really a matter of preference at this point. From your original post, I wasn't clear on whether you were questioning the syntax or the naming convention. My personal preference is to declare variables at the point in the sproc/script where they'll be used. So if the first use of each of those variables was sprinkled throughout the code, I wouldn't declare them all up front like this. Exception: if it's a script I'm handing to someone to execute and those variables need to be set by the user, I'll declare those up front, in individual `declare` statements. Then the user can edit them easily.
Ah, sorry if I was not clear before. Yes I'm selecting a group. I only want to generate a report that brings up the people like Sarah. I do not want anyone like John. And that is *mostly* correct. I am comparing new applications (Open) against *only the most recent warrantyIDs belonging to those customers* (which will be closed).
Do you know how to use SQL Profiler? You might be able to find the sql that your reporting app uses by running profiler and then trying to generate the reports.
I've downloaded MS SQL Server. I'm trying to understand how to use it before importing the data. Thanks for the help! By the way, are you familiar with MS Access? Especially making a database in it? I might have thought of another way around this
It's hard to be sure without data to test against, but this may meet your needs: Select C.CustomerID, C.Name, W.WarrantyID, W.ExpiryDate From Customers C, Warranties W Where C.CustomerID = W.CustomerID And W.Status = ‘Open’ And exists ( Select 1 from Warranties W1 Where W1.CustomerID = C.CustomerID And W1.Status = ‘Closed’ And W1.ExpiryDate &lt; W.ReceivedDate – 30 And W1.ExpiryDate = ( Select max(W2.ExpiryDate) From Warranties W2 Where W2.CustomerID = W.CustomerID And W2.Status = ‘Closed’ ) ) We start by joining customers and warranties on the CustomerID column, just to pull in the customer name. We specify that status must be 'Open' to make sure we're getting only customers with applications that have not become warranties. We use the "exists" clause to say that we only want customers who have warranties (status = 'Closed') that expired more than 30 days before the application was received. Note that we search the Warranties table for expired warranties, but use the abbreviation "W1" to distinguish the results from our first search ("W") for current applications. But we're not satisfied with any expired warranty. The customer might have a mix of expired and unexpired warranties. So we also specify that we only want to consider an expired warranty if it has the maximum expiry date of all warranties (Status = 'Closed') that customer has (the "W2" selection).
I missed that this was for Access and went ahead and did it in SQL Server. Hope this helps anyways. set nocount on -- create 1st table Declare @table1 table ( Name varchar(10), Date_Time datetime, Thing varchar(10)) -- populate 1st table Insert @table1 (Name, Date_Time,Thing) Values ('Mike','1/9/16 4:15 PM','Thing 1') , ('Mike','1/9/2016 0:00','Thing 1') , ('Mike','1/11/2016 15:53','Thing 2') , ('John','1/10/2016 16:27','Thing 2') , ('John','1/11/2016 15:32','Thing 1') , ('Steve','1/11/2016 15:32','Thing 1') , ('Dan','1/8/2016 16:39','Thing 1') , ('Dan','1/10/2016 15:35','Thing 2') , ('Dan','1/11/2016 15:47','Thing 2') -- create 2nd table to remove time from date_time column Declare @table2 table ( Name varchar(10), DateOnly date, Thing varchar(10)) -- populate 2nd table from 1st, converting datetime to date only Insert @table2 Select t1.Name , DateOnly = Cast(Date_Time as date) , t1.Thing From @table1 t1 Group By t1.Name , Cast(t1.Date_Time as date) , t1.Thing -- pivot 2nd table Select Name , PivotOut.[Thing 1], PivotOut.[Thing 2] From @table2 pivot ( count(DateOnly) for Thing in ([Thing 1], [Thing 2]) ) PivotOut 
Preference, not mistake. I do it this way sometimes when testing to easily comment things out if I want. For example, if I want to test code for a sproc that will eventually have input parameters.
There are a lot of resources out there. The thing is to pick to right one. I recommend the following : http://www.studybyyourself.com/seminar/sql/course/?lang=eng. On that web site you can submit exercises and get a feedback right away. It is free and suitable for people willing to learn quickly.
Seems like this is the right question before we can start providing help.
By "using" the data do you mean other users need to be able to read the data or they want to be able to change it, too? This is relevant because an update statement should never block users who are trying to query the data. This of course depends on your DBMS (what are you using??). But the main ones in use out there will not lock readers out of a record that is changing (was updated but not committed). If users are trying to change the same row, however, there will be a wait or an error due to the lock. But in general users won't be locked out from accessing the data from the most recent commit. The next thing to consider is the amount of time the records will be locked. How long does it take you to update the 1000 rows? Less than one second? A minute? An hour? If you can minimize the amount of time this process is holding the lock then the impact of the lock becomes less critical. 
You need to add in a where not in clause.. Something like where custermerid not in (select customer id from orders where order date &lt; the date range you want) You create a list of custermerid that existed prior to your report period.
Hey, are you Channing Tatum?
No idea whether this applies since you didn't mention which database server you are using . . . MSSQL lock escalation can be somewhat aggressive at times. Basically, the server knows that locks consume resources, so it tries to limit the number of locks it's issuing. It starts out locking rows, if that gets too large it locks pages (multiple rows in each page), and if it gets really big it just locks the whole table. The server thinks, rather than issuing 10,000 locks for each of the 10,000 rows you're updating, it's easier to just lock the whole table and deal with 1 lock instead of 10,000 individual ones. You can *suggest* MSSQL to use a specific type of lock with a lock hint. For example, if you want to make sure the locks are never escalated above individual rows, you can use this: UPDATE mytable WITH(ROWLOCK) SET col1 = 'something' WHERE col2 = 'somethingelse' This *suggests* SQL to lock only the rows affected by the query, and never escalate locks to contain pages or the entire table. However, keep in mind this is only a suggestion - if server resources make it impossible to manage the number of rowlocks required, it will still escalate locks to pages or the entire table. 
Not sure, what if they have multiple orders on the same day, would they be a returning customer?
What I'm saying is - this query basically returns one record per customer, with a total order count and total order amount. I would think anyone with an order count &gt; 1 would be a returning customer, since they have placed more than one order. Anyone with only one order, would not be a returning customer.
I entirely follow, I'm just asking if their business logic is such that were a customer to place multiple orders on the same day, would they still be considered a repeat customer? Your assumption would dictate yes, but perhaps OP needs to clarify that? Also, this from that logic and OP's query, any customer that only places a single order in a given year would be a first time customer, regardless of previous years purchases. I understand I'm getting nit-picky, but I think these as points of ambiguity that should be validated.
From team QWOP.
Users Left Join Enrolments where Enrolments UserID is NULL.
http://imgur.com/J1fOBbX
Lynda.com essential SQL is pretty good and very cheap, best way is to just practice though. Als try knowledify sql app in app store.
A shitpost in /r/SQL? I never thought I would see that. Good work OP.
i'll walk you through how i would do #1 -- first, find the 'x' item and all of its tags -- SELECT x.item_id , xt.tag_id FROM items AS x INNER JOIN item_tags AS xt ON xt.item_id = x.item_id WHERE x.item_name = 'x' now find all other items that have the same tag -- SELECT x.item_id , xt.tag_id , ot.item_id FROM items AS x INNER JOIN item_tags AS xt ON xt.item_id = x.item_id INNER JOIN item_tags AS ot ON ot.tag_id = xt.tag_id AND ot.item_id &lt;&gt; xt.item_id WHERE x.item_name = 'x' now count the matching tags and restrict via HAVING -- SELECT x.item_id , ot.item_id , COUNT(*) AS matching_tags FROM items AS x INNER JOIN item_tags AS xt ON xt.item_id = x.item_id INNER JOIN item_tags AS ot ON ot.tag_id = xt.tag_id AND ot.item_id &lt;&gt; xt.item_id GROUP BY x.item_id , ot.item_id HAVING COUNT(*) &gt;= 4 WHERE x.item_name = 'x' simple, yes?
by the way, in many of the "tag" implementations i've seen, it is often *simpler* and *easier to use* if you do ~not~ use an integer id for tags, but instead, use the tag itself in the Items_Tags table Table: Items Columns: Item_ID, Item_Name Table: Tags Columns: Tag_Name Table: Items_Tags Columns: Item_ID, Tag_Name you would still set up referential integrity (FK reference) from Items_Tags to Tags, but using Tag_Name instead 