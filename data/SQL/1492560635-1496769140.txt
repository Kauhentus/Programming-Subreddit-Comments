It depends. 1. Yes, more data *usually* means more resources. But not necessarily. It's impossible to say from the information given here. 2. If your foreign keys aren't indexed, you're probably gonna have a bad time. It's a good practice to [index your foreign keys](http://sqlmag.com/blog/should-i-index-my-foreign-key-columns) to make joins more efficient. Depending upon your indexes and what you're querying for, your query may end up doing *less* work than you expect. 3. As others have noted, you should be using the ANSI92 JOIN syntax - `from Table 1 INNER JOIN Table2 on Table1.Field = Table2.Field` 4. As written, this definitely will slow you down as you're doing a `cross join` - unless you know that you need a `cross join`, there should be relationships tying all 4 tables together (1 to 2, 2 to 3, 3 to 4 for example). 
It would be perfect for WHERE NPA = 555 AND NXX = 555 and LN = 5555 But in the SQL Server world I'm accustomed to, if you tried WHERE CONCAT(NPA, NXX, LN) = '5555555555' It wouldn't use the index at all afaik. 
OP mentioned Oracle SQL, and I function-based indexes work fine with Oracle and PostgreSQL. SQL Server and MySQL don't support that feature however.
fa isn't a defined table, and fq isn't used. But you probably want something like this. select formid, count (CustomerName) as cntCustomer, sum(case formid is null then 0 else 1) as cntFilled, sum(case formid is null then 0 else 1) / count(CustomerName) as pctFilled from ... where ... group by formid 
Yeah, it's weird. The stored proc is designed to update a record in a table with all optional parameters, so for every column in the table, there's a parameter with a null default value. So it's pretty much this: CREATE PROCEDURE [dbo].[update_object] @id int, @description varchar(100) = null, @property_1_id int = null, @property_2_id tinyint = null, @memo varchar(max) = null, @summary varchar(200) = null AS BEGIN update object_table set description = isnull(@description, description), property_1_id = isnull(@property_1_id, property_1_id), property_2_id = isnull(@property_2_id, property_2_id), memo = isnull(@memo, memo), summary = isnull(@summary, summary) where id = @id END So if each parameter isn't supplied, its value is null and the corresponding column is set to its own current value. If it is supplied, the column is updated with the supplied value. [Similar to the answer to this StackOverflow question](http://stackoverflow.com/questions/3290622/using-update-in-stored-procedure-with-optional-parameters) that I found in the course of googling for this type of thing. Unless... (and it took almost 6 months for this case to result in a support ticket)... you're trying to update a column to have a value of `null`. In that case, the isnull() will be true, so the column is updated to its current value, as if the parameter wasn't supplied. So what I'm trying to do is something like `set description = isdefault(@description, description). 
I came across some code that I wrote right after graduation that used this syntax. I have no idea what I was thinking other than the JOIN keyword was scary because it required an ON. I write much better code now.
This is what I'd do. Just make the default a value you'll never really encounter.
Right. You can redefine the sp so that it doesn't automatically assign default values when nothing is entered in the parameter definition. So the app wouldn't change, just the sp. 
Probably going to go this route, using `'skip'` as the default.
I don't have a better solution for you, but can you explain why it must be vendor agnostic?
I have quite a collection of update procs that do this exact same thing. I can only get away with it because I can dictate that NULL is not allowed. I decided to Google this a bit again, and I laughed a bit when I saw the MS Connect post where Aaron fucking Bertrand asked in 2006 if we could get this functionality. Apparently not.
I too ran into this issue. I ultimately rethought what a NULL value represented, and preferred using the default value. NULL should represent an unknown or indeterminate value, not an "empty" or blank value. In the case of updating the value, you "know" the value and therefore not actually setting the value to NULL.
If due date and return date are both integers (weird excel like design), then just do return_date &gt; due_date +7
&gt; Who is teaching it... About a billion web sites. 
What does the execution plan tell you?
On the off chance that you are mistaken that they are integers (which is weird), then... Where DATEDIFF(day, DUEDATE , RETURNDATE) &gt; 7 Should do the job 
Because I have 3 separate database vendors involved (which is a large part of why we need these operations in the first place.)
Using the version of the proc in my above comment, it would be `exec update_object 237, @summary = 'My new summary'`
The function causes this solution to be non-sargeable, which in larger production environments could cause performance issues.
I hadn't considered that, as we as an organization have never used them before. I'll ask and see if that is something we might consider using. Thanks!
Holy crap, thanks! EndDate is currently just using a getdate() - which isn't optimal because now the report relies on the run date for the timeframe - and run date is based on the data driven subscription logic. So if it gets run on the 17th for some reason instead of the 16th - the report will show sales for 1-16th, instead of 1-15th... and since we pay commissions off of it, in theory someone could get paid twice for the same sale. I'm in wall to wall meetings today - so I wont get to test your code until tomorrow or Friday - but thank you a ton!
I'll double check indexes - but I'm pretty sure invoice date and salesperson are both indexed. It is a varchar field in the database, but is really a "code"... Like I4, H0, L9 etc... It is an assigned sales territory marker, not an actual person. I agree there is a ton of good advice here, I'll be testing out a lot of it. 
You should just be able to do (assuming your dates are MM/DD/YYYY) ORDER BY Date ASC Which would give you this: http://i.imgur.com/656cDXO.png
What is the query you're using to view your dataset? It sounds like you need to use an ORDER BY clause.
Apologies, I misread I think you can join on a a basic LEFT...so it would be like join LEFT(e.address,8) = LEFT(m.address,8) Makes it so you look at the first 8 characters (you can flex this number to whatever you please), which should be a unique identifier. Again, I stress caution with this. I dont know how to delimit, break apart, glue together the first two results, and join by that. Ill keep an eye on this thread though, and hopefully someone smarter than me looks at it. 
I'd go with something like: SELECT customer_id, product_id, rownumber() OVER (ORDER BY date ASC) AS order_number That's assuming you want a separate order_number column and no date column. By using the windowed function, I'm able to assign a row number (basically just an incrementally increasing INT) to each date value in ascending order. If you need to break up the groups and restart the count, just add a PARTITION BY on whatever column will be used to distinguish groups of orders.
Just ran into a similar problem joining on domains to email addresses. As others have pointed out, this code doesn't truly solve your problem because of potential issues with N. vs North vs. Nort vs. N, &amp;c., &amp;c. SELECT m.name, e.occupation FROM mytable m LEFT JOIN emails e ON substring(e.addresses from '#"% % #"%' for '#') = substring(m.addresses from '#"% % #"%' for '#') If this doesn't work well enough for you, you need regex and alias tables. If that's the case, I'm sorry.
&gt; BETWEEN '20091011' AND '20091012'; &gt; this approach is okay, **as long as you don't have any rows that fall on midnight at the upper bound** - which can be much more common if parts of your application strip time from date/time values. In that case, this query will include data from the next day. &gt; [otherwise]...you will return rows from October 12th at exactly midnight, but not at 1:00 AM, or 4:00 PM, or 11:59 PM. You can get around this by doing: trunc(FooDate) between '11-OCT-09' and '12-OCT-09' So between with dates is really a non-issue as long as you know the problem exists without truncating a datetime when comparing dates.
So I'm assuming your data looks something like this: public.table_a column1, ..., columnN, tablea_tableb_fk public.table_b tableb_pk, column1, ..., columnN, column_tablea_calc If that's the case, what you want to do is update the value of public.table_b.column_tablea_calc based on updates/inserts to public.table_a, correct? What you're talking about with "if an insert is attempted..." is an UPSERT, so an INSERT statement with an ON CONFLICT (tablea_tableb_fk) DO UPDATE SET column1 = EXCLUDED.column1, ..., columnN = EXCLUDED.columnN. Once you confirm my understanding is correct, I can probably write a trigger for you. Please just be aware that using triggers to write to your DB can cause unexpected behavior. In other words, thar be dragons.
&gt; as you know the problem exists without truncating a datetime when comparing dates. Exactly, as that's not an issue with `BETWEEN`, it's just called knowing how to handle dates in SQL
So just change @enddate to getdate() and done. I mean you could probably rewrite your process to make it sexier, but at the end of the day a simple drop table and rerunning the process at whatever interval you want would work. Let me know how long it takes to run the top and bottom of that query compared to how it runs today. I've seen SSRS reports which go from taking 20-30minutes that optimize down to 20seconds, 15 for the SQL sproc, and 5 for the actual report. General rule of thumb with SSRS is to do as little as possible in SSRS and as much as possible in SQL.
You could, not sure what the benefit of this would be. I would test the code I gave you and see how long it takes to run. If it takes a very short amount of time, what is the benefit of doing it like you suggest? If your environment is running a lot of processes and you're concerned about the timing, then yes, there are better ways to do it. 
Try this: select m.name, e.occupation from mytable m left join emails e on e.addresses = split_part(m.addresses, ' ', 1) || ' ' || split_part(m.addresses, ' ', 2); I think this should return '6390 Chimney'. I don't have a Postgres install handy (and if I did, I'd probably try to pull it out via substring). Also expect it to be slow. If it's a frequent need, consider creating a computed column on the first two words, so that you can do a simple join on the columns. Edited to show the split_part() in context of the query.
Personally I don't think you want that as a column in the table, since the purchase will "age out" of the 52 week limit, requiring constant maintenance on that column. You may be better off creating a view that displays only the items that are purchased within that date range. Just a suggestion.
Select * from emails E join addresses A on a.Address like e.address +'%' (fighting with my mobile so please excuse formatting) Like with a % as a wildcard works well from out experience and won't result in indexes being misused/ignored due to functions in joins.. Performs well on the data we use it for, but keeping an eye on this as new tricks/methods are always welcome! ** this is for t-sql... Not familiar with postgres! 
Performance will suffer using functions in your join as indexes will probably be ignored. Depending on size of datasets and indexes you may not notice. 
can you elaborate on what the heck is going on here '#"% % #"%' for '#' 
Just organize it like source code and use git/GitHub/Bit bucket. MSSQL for example has a "data project" template that will even allow you to validate your sql against the scheme without a db connection. Datagrip by jet brains has some similar features for multi platform sql
Are you using Microsoft Sql? If I understand you correctly, you could use the lead/lag function if you are using a fairly recent version. Otherwise, it sounds like you are going to need to iterate each line of the table. or put something in place at the front end to add that value at the time it gets inserted by using the max date for that customer ID and a date diff function. 
I think you want to union a few select statements together. SELECT ID, 'Apples' AS Product From CurrentData Where Apples &gt; 0 Union SELECT ID, 'Oranges' AS Product From CurrentData Where Oranges &gt; 0 Union SELECT ID, 'Melons' AS Product From CurrentData Where Melons &gt; 0 ect... Edit should be greater than 0, not 1. 
Without knowing anything about your schema or which platform you're using I couldn't really say for certain, but it would likely involve getting the previous sale date using LAG and comparing that with the current record's date. Something like SELECT customerID ,SaleDate ,CASE WHEN datediff(day, lag(saledate, 1) OVER (partition by customerID order by saledate desc), saledate) &lt; 365 THEN 'Yes' ELSE 'No' END as SaleInLastYear tbh I'm not sure that my syntax is quite right there and I wonder if you even need to put the LAG into a subquery before you can use it in a datediff but that's the sort of direction I'd be looking at. 
I'm new to this and don't know what I am doing but what is the answer to this and what is wrong with it. I have basic knowledge and can read what is going on but can't diagnose it. SELECT price + amount AS total FROM items
Ah, I can't believe that didn't click. I was trying to understand the query so much that I did not take the time to actually read it. 
Actually you'd do it like this where timestamp = max(timestamp) and cast(timestamp as date) between getdate()-7 and getdate() So you always get the most recent data in the table, and if you accidentally run it or if the subscription accidentally goes off before the SPROC then it will be empty. SSIS and other more elegant solutions abound, but you have to weight how much time it would take vs. what you gain. If the job only runs in a minute or two, and the table is going to be small... I'd probably take the current process and just do it as an SPROC running in SSRS. If you have concerns about those pieces, then put it in SSIS and have a column which starts as a 0 and changes to a 1 if the data was sent, and then just only send data where that value is 0.
&gt; assuming your dates are MM/DD/YYYY Why does that assumption have to be made? If the date is being stored properly as a `date` data type, the formatting has no impact on the sorting. If you're storing dates as strings, OTOH, that's going to present several problems over time.
Although your response here didn't correctly answer OP's question, have you looked into "computed columns"? They are virtual columns that are not persisted and can be used in the example you were thinking of. (no maintenance necessary). They're called generated columns in mysql if that's your flavor 
First of all, march to your teachers office and DEMAND that he cease with this ancient crappy join syntax that the real world stopped using a billion years ago and invite him to join the fast paced futuristic world of [1992](https://en.wikipedia.org/wiki/SQL-92) when proper joins were invented, and refuse to leave until you've witnessed him updating all of his teaching materials. Secondly, you want to look into ways of making the employerid a count of distinct ids. But that's not as important as the first thing. e: to expand my answer, if you were running your query without grouping or aggregation, imagining you had one agent with two employees, with 3 members each, you'd get something like AgentID|EmployerID|MembershipNo :--|:--|:-- 1|1|1 1|1|2 1|1|3 1|2|4 1|2|5 1|2|6 If you count how many values there are in the second and third columns, there are 6 in each column - but there are only two *distinct* values in employerID. If you can figure out how to change from counting the values and instead count the *distinct* values, you'll have it beat.
Sounds like you need to use a window function. I'm versed in TSQL, but this might help you get there. SELECT * FROM (SELECT CAST(recordDate AS DATE) , DATEPART(HOUR, recordDate) , ID , ROW_NUMBER() OVER (PARTITION BY CAST(recordDate AS DATE), DATEPART(HOUR, recordDate) ORDER BY recordDate DESC) orderedID FROM TableA GROUP BY CAST(recordDate AS DATE) , DATEPART(HOUR, recordDate)) a WHERE a.OrderedID &lt;= 30; edit: fixed the formatting. The example uses a window function in a derived query to get the 30 elements.
Thanks so much for the response. I get what you are saying but I must also count the EmployerID column. For this example, the output should look like: **AgentID, Employers Count, Members Total** *1,2,21* As in Agent 1 has 2 Employers under him (employerID 1 and 2) and their total members is 21. edit: formatting
It's just like I explained in the table above. Joining the result means that there is one row per member, with each employee appearing several times. If we just count how many times a employeeID appears without checking whether it's a duplicate we end up just counting the number of rows. 
Ohh ok... b.c. it is showing the employer number once for each member. So when I run it that way I would see employerID 3 becuse it is counting all 3 rows.
I'm sort of curious if the instructor would actually stop using the old join syntax. I'm skeptical that they would especially if they are dug into their old ways.
You're right it doesn't matter, but it looks like he wants to order by date, just from looking at the dates and the order he wants
You can do something like this. Please note that with this code a higher 'combined rank' is better, a rank of 1 means the row had the LOWEST a.Paid and a.Claims values. SELECT a.ItemID, ..., rownumber() OVER (ORDER BY (a.Paid + a.Claims) ASC, C.Paid DESC) as 'Combined Rank with Tiebreak' FROM TABLE a JOIN TABLE c ON &lt;some stuff&gt; 
Thanks for that suggestions, I'm going to try that. The plain English instruction I was given was, "Rank paid and claims individually, then add those ranks together. Rank all the items by that combined rank, using the 'paid' figure as a tie breaker."
group by ~~estimated_price,~~cast(estimated_price/sq_ft as numeric(36,2))
This is definitely something I could look into as a Plan B. Thanks @ DrumKeys.
Off the top of my head, I'd do something where you get the avg greyhounds and add in the col of origin. Group by origin and retrieve just the top average by origin. 
Probably going to need a subquery here too.
Yep, +1 for Registered Servers in SSMS. 1. Open SSMS 2. View &gt; Registered Servers 3. Right click Local Server Groups &gt; New Server Registration &gt; Enter server name &gt; Save 4. Repeat until all of your servers are there 5. Right click Local Server Groups &gt; New Query 6. Run the query and it'll auto-add a "Server Name" dynamic column, so you don't need to join in `sys.servers`
Sorry I reformatted my earlier response, I THINK i understand your need correctly. edit: apparently it's not available in mysql, [here](https://preilly.me/2011/11/11/mysql-row_number/) is someone emulating the implementation
&gt; First of all, march to your teachers office and DEMAND that he cease with this ancient crappy join syntax that the real world stopped using a billion years ago This is the second or third time this style of join has appeared this week. I'm wondering if there's a whole bunch of instructors stuck on this, or it's the same instructor's students who keep popping up here.
maybe change it to &gt; AVG(cast(estimated_price/sq_ft as numeric(36,2))) instead. I have to be honest, I don't really use AVG or postreSQL so there may be some subtle differences I'm not aware of. I use SUM(x)/COUNT(*) to calculate
I keep them in a Windows folder that I organize by type or job, and then back the entire structure up in a RAR file. I also use SSMS Tools to save every query I ever run, and keep that backed up as well. Mostly I use the search function in SSMS Tools and just search for a table name.
im using PHP/html I'm basically just trying to make it so that a user can just select the week from this drop down form, input the weight they are, hit submit and have weight be inserted under the corresponding column in phpmyadmin. What do you think a proper table structure would be?
It took me three tries to understand wtf the first question was trying to get me to do, and then when I completed it the page crashes and I can't even load that page in a new browser. Wow.
I like the SQL server advice. I would have used a union in access...
Might be bad formatting as on mobile apologies if so but does this look fine? SELECT Name FROM greyhounds GROUP BY country_of_origin HAVING wins &gt; AVG (wins); 
You could use a trigger to check to see if a tour is already booked by a guest, or disallow new registrations if a tour is full. 
If an order is placed change the quantity on hand. 
You're close, but everything in your SELECT clause (besides aggregated values) needs to also be in your GROUP BY clause. Since you're planning to group the values by name and country_of_origin, you need to have them both in SELECT and GROUP BY. Your HAVING clause is correct. Think about the last part of the question as well. It wants you to group by a specific order.
Excellent ideas! I'm going to try and figure out how to implement your second idea. Thank you very much!
For a homework assignment that is great and all, but never would do that in a real world environment. Business logic really belongs at only one level. Mixing it between database and application is a recipe for disaster when it comes time to do updates. Because of how complex it gets I usually leave it at the application level. The only trigger I've used in a production system was for prevented updating a field that would break the integrity of a downstream table. 
Thanks a lot!
its nice that datagrip does version control and has quick fixes, our devs already use intellij primarily... wish it wasn't $200 a year for datagrip ... since even our sales folk want to run queries. but dems the brakes
Why wouldn't you put this just in a normal SELECT query ?
Go to the Microsoft SQL server page and look at their training program. Start taking the certification exams to become a SQL Server administrator. It would be the fastest way to become useful. SQL Server is a relatively easy database to admin you are lucky.
What happens when price or amount is null
ive done weeks 1 and 2 and its nice and slow for someone like me to figure out. im going to do week 3 soon enough :)
That's kind of a rough entry, but it is the best education you'll get. I'd recommend /u/Trek7553 post, they posted Brent Ozar and I think going through those and using some Pluralsight then moving up to the MS SQL pages is a good progression. 
Its not
So, my mistake here. The bracket strings are not always the same length, nor are there always three of them
Reverse + substring with a character matching function to determine the start/stop locations. What db are you using?
Help? You don't know how lucky you are. Take the job, now on your resume you are a DBA. Work it for 1-2 years and learn all you can. Find a new job as a DBA and increase your salary greatly.
I have created the user using command line and I have checked and the new user is there in postgresql. Now how can I install copy of postresql sitting on my computer on his computer?
Do you want to copy the database and put the copy on his computer so that there are 2 databases? Or do you want to grant him remote access to your database on your computer?
&gt; a company merger means that my old role of planning assistant is going to go 1. How big is the merged company? &gt; saving my job is going to require me to become a member of an associated team currently developing and later maintaining a large company wide SQL server. 1. Is there already a team of DBAs in this associated team, so you'd have senior DBAs to lean on or are you going to be the first and only? 1. Is the flavour of SQL server you're going to be architecting and maintaining already determined? (It'd be good to directing actual training dollars toward the right DBMS)
&gt; you set the name of a constraint. Common practice for name is Type That helped, thanks.
You want two select statements using a union between them. The first select will be your buffer row and the second is your actual dataset. On mobile so I can't really write a query but let me know if you have questions. 
 DECLARE @Ordnum as varchar(20) SET @Ordnum = 'POPF579566' SELECT 'START' as [NAME], 0 as [JTC-SB-SSP], 0 as [JTC-SB-IQF], 0 as [JTC-SB-WCG] union all SELECT [NAME],[JTC-SB-SSP],[JTC-SB-IQF],[JTC-SB-WCG] FROM (SELECT name, sku, qty FROM dbo.FundraiserIndividuals AS fi INNER JOIN orders AS o ON o.id = fi.orderfk WHERE ordernumber=@Ordnum) AS SourceTable PIVOT ( SUM(qty) FOR sku IN ([JTC-SB-SSP],[JTC-SB-IQF],[JTC-SB-WCG]) ) AS PivotTable THANK YOU!! I tried the above which worked but had to take out the Order By or it moved START down in the list I'm trying to recreate a spreadsheet we use to import orders to our system that has a very specific layout. It uses the first row with the SKU's [JTC-SB-xxx] to find the products and the very next row under it must have "START" in the first column, with the 3rd row then kicking off the list of peoples names and qty of products ordered. Without the START in Col 1 Row 2 the import does not work. Thank again!! 
Your group by clause should be a where clause. You only need to group by the seller_id
Nope, that is invalid Oracle syntax. /u/SloppyPuppy's MERGE example below would work but if OP insists on using an UPDATE for some homework reason, this would do also: UPDATE customers c SET c.credit_limit = 1000 + (SELECT MAX(o.amount) FROM ORDERS o WHERE c.cust_num = o.cust) WHERE EXISTS (SELECT 0 FROM orders o WHERE o.cust = c.cust_num AND o.amount &gt; c.credit_limit)
Agree, the reason I do not like this update is because it forces Oracle to: * run over the "order" table twice instead of a single time with "merge" * do one of the joins (because in the "update" case there are 2) as "nested loop" instead of "hash" if you do this kind of update to a big population itll take a lot of time.
Hi! I tried this code: SELECT seller_id, COUNT(*) FROM items GROUP BY seller_id HAVING COUNT(*) &gt; (SELECT COUNT(*) FROM items) / (SELECT COUNT(*) FROM items GROUP BY seller_id) But still having the same error. 
In SQL syntax, is it not possible to just use the function AVG() for the &gt; SELECT COUNT(*) FROM items GROUP BY seller_id Like enclose that line with the AVG()?
Is "num" a premade syntax in SQL?
The group by doesn't need to be in the brackets, it can be its own line.
Insert into ( Colname, Colname2, Etc. The select is right, but the first column is identity, which you are not inserting.
Not sure if the same syntax holds true for MySQL but with MSSQL you can use OPENROWSET. Research that function and you'll see some examples. 
http://stackoverflow.com/questions/3635166/how-to-import-csv-file-to-mysql-table
This will give you what you're looking for, just drop it in Excel and look at variances, or code it in SQL and use WHERE's to look at specific items / maxes, etc. with a1 as ( select make , model , year , region , auction , avg(auc_price) as avg_auc_price , avg(ret_price) as avg_retail_price from table group by make , model , year , region , auction ), a2 as ( select make , model , year , region , avg(auc_price) as avg_auc_price , avg(ret_price) as avg_retail_price from table group by make , model , year , region ), a3 as ( select make , model , year , avg(auc_price) as avg_auc_price , avg(ret_price) as avg_retail_price from table group by make , model , year ), a4 as ( select make , model , avg(auc_price) as avg_auc_price , avg(ret_price) as avg_retail_price from table group by make , model ), a5 as ( select make , avg(auc_price) as avg_auc_price , avg(ret_price) as avg_retail_price from table group by make ) select a.make , a.model , a.year , a.region , a.auction , a.auc_price , a.ret_price , a1.avg_auc_price , a1.avg_retail_price , a2.avg_auc_price , a2.avg_retail_price , a3.avg_auc_price , a3.avg_retail_price , a4.avg_auc_price , a4.avg_retail_price , a5.avg_auc_price , a5.avg_retail_price from table a left join a1 on a1.make = a.make on a1.model = a.model on a1.year = a.year on a1.region = a.region on a1.auction = a.auction left join a2 on a2.make = a.make on a2.model = a.model on a2.year = a.year on a2.region = a.region left join a3 on a3.make = a.make on a3.model = a.model on a3.year = a.year left join a4 on a4.make = a.make on a4.model = a.model left join a5 on a5.make = a.make 
Are you aliasing the tables in the SELECT clause consistently with your FROM clause? e.g., SELECT Products.productID From Products p join Orders o on o.ProductID=p.ProductID Wouldn't work but SELECT p.productID From Products p join Orders o on o.ProductID=p.ProductID e: Your example "solution" join Orders o on o.OrderID=p.ProductID Doesn't look like it would do what you want, unless the orderID is just a different name for the productID which doesn't sound likely.
a) I don't think this is what OP was asking and b) you could achieve the same thing so much more easily using [WITH ROLLUP](https://technet.microsoft.com/en-us/library/bb522495\(v=sql.105\).aspx) or a grouping option. 
That is exactly he he'll need to look at the variances on a per model basis to see what % exists between the average on multiple conditions. I'd probably go a step further and do some indexing here to establish a baseline. 
Not too difficult in SQL: SELECT 100 - avg(avgDiffPerct) from (SELECT make ,model ,year ,region ,avg([auction price]) / avg([retail price]) avgDiffPerct FROM table) x That would give you a rough idea of what to do with your SQL. You might like to tweak exactly what you're looking for: this is the average % the auction is lower than the retail price, not sure if that's how you would want the difference expressed. If you're looking for something more raw: SELECT make ,model ,year ,region ,avg([auction price]) - avg([retail price]) DollarDifference FROM table Might do. Slight caveat here: I'm assuming auction price or retail price will be null when they don't apply. 
I don't think he was after multiple conditions, just the difference in otherwise identical sales. Even so, doing it with WITH ROLLUP in the group by clause saves you from doing all of those subqueries.
It basically does exactly what you did with a bunch of subqueries. If you have GROUP BY car, model, year WITH ROLLUP, it will produce results that are basically a union of the same statement grouped by car, model, year, and car and model, and just car, all in the same statement. 
I don't think that was the question: &gt;i want to find the average difference a **vehicle of the same make/model/year and in the same region** sells for between the two different channels. And even if it was I don't see how having 18 columns for two data items helps.
That could be done easier with a two column table by selecting the relevant rows, no?
I think at that point you need to discuss what the purpose is for. If it's going to be connected to a front end like SSRS or Tableau, then you might do it one way, which might not be a best practice as far as a DBA is concerned, but it will have a substantially better execution time for front end users. Personally I try to avoid CTE's for views or sprocs that are connected to these types of tools, and would use a series of #tables in a sproc to manage it and provide all of the variances / indexes in a neat little summary table.
Nice, good luck!
You need a Table-Valued Parameter.
I haven't worked in telephony in a few years, but this should account for most formatting types of numbers: DECLARE @PhoneNumber nvarchar(15) = '+14805551212'; SELECT STATE_CODE FROM {YourTableHere} WHERE NPA = CASE LEFT(REPLACE(@PhoneNumber, '+', ''), 1) WHEN 1 THEN SUBSTRING(REPLACE(@PhoneNumber, '+', ''), 2, 3) ELSE SUBSTRING(REPLACE(@PhoneNumber, '+', ''), 1, 3) END ; You want to wrap this inside of a stored procedure, with the PhoneNumber variable being a required parameter for your proc. Keep in mind as well if there are multiple rows that share the same NPA (likely), it will return all of them and not just a single record. This may be problematic for your vendor.
`USE` is not the keyword you're looking for in the first place. `USE` [switches database contexts](https://docs.microsoft.com/en-us/sql/t-sql/language-elements/use-transact-sql). /u/Murbarron is correct, your best option is a TVP in this case.
CREATE TABLE Logins (Email varchar(50), FirstName varchar(50), LastName varchar(50)) GO --create the procedure CREATE PROC insertFromTXT @fileLocation varchar(256) AS DECLARE @DynamicSQL varchar(256) SET @DynamicSQL = 'BULK INSERT Logins FROM ''' + @fileLocation + ''' WITH ( FIELDTERMINATOR = '','')' EXEC (@DynamicSQL) GO EXEC insertFromTXT @fileLocation = 'C:\Users\Jacob\Desktop\FinalsProject\names.txt'
I use Oracle much more than SQL Server so I'm going by memory but you should be able to use the information schema. Select table_name From information_schema.tables
Couldn't you just open the file in Excel and delimit on spaces, tabs, or commas (depending on the structure)?
If you're using MSSQL you can do something like select name from sys.tables.
The structure in the notepad document isn't the problem (I think). I actually imported the data into the Microsoft SQL Server Management Studio as a database. Now I am trying to learn the language to use in order to manipulate the data to create the correct tables that the assignment is asking for. I am using youtube but I can't figure out how to manipulate the data I have stored. Any tips are greatly appreciated 
Were you instructed to use SQL? If not, stop over complicating it. Pull the data in to Excel like previously suggested and perform your calculations there. If you were instructed to use SQL and are expected to figure all of this out on your own, forget it. The bit of extra credit probably isn't worth the effort and the teacher is ignorant for expecting anyone to put that kind of time into it. 
Can you provide an example of what you want? Do you want to display the table name in the columns of the query results? If so, there is no way to automagically do that. You'll need to use an alias. 
Yes, table name as a column - just the way database name appears on using current_database(). 
I am using PostgreSql. From what I've seen so far, it is very similar to T-SQL.
Yeah I was instructed to use SQL. I have figured out 1 of the 3 questions but the others look impossible to me. 
Were you able to import the data from the text file into a table yet? 
Yes I was. I also found out about the group function. So I was able to complete the second part of the assignment as well with out while loops. Is there a way to add a third column that will state "Yes" and "No" if column A is the number 5 or 6? Here is my code: USE DatabaseW Go Select A, sum(B) as TotalCost From Table4 Group By A Thank you!
Go for a case statement 
Yeah use a decode statement or case which ever synatx you choose.
The case statement worked! Thank you!!
Yeah, jOOQ is close to just typesafe SQL, which is awesome.
Thanks, if this means that he can access the database I am working on then this is ideal. I want to give access so he can do his own data manipulation. The database is saved on my computer.
You have to use join query here because we need to retrieve data from two tables. Join is nothing but making a primary key- foreign key relationship between two tables. Your query is wrong because first, you haven't used join and second, you are using unknown field names. Type the correct field names that you want to retrieve from the DB and not the field names given in the question. So the first answer should be 1) select city, state_province,country_name from countries,locations where region_id=&lt;id of asia&gt; and countries.country_id= locations.country_id; On second one what do you exactly need ? Country name? Or location id? If it's location id then the query would be Select location_id from locations where state_province is null; If you want the country name then the query would be Select country_name from countries ,locations where state_province is null and countries.country_id = locations.country_id; Hope it helps even though it's 4 days late. 
A big thanks to both DeathMetalDave and abbbbbba for the sample code. I was able to get a working SP based off this info and have submitted it to our vendor. Hopefully it fulfills their request otherwise I may be back for more guidance. Thanks again!
Update: I tried this at work today and it gives me all the table names present in the database. I want only the table name that I'm currently using. But thanks for sharing this piece of information. I did not know something like this existed. Oh, and you use the same query for PostgreSql too. 
Thanks for sharing! 
Do you have a server? You should run the SQL instance there. The server is running 24/7 and you only need to maintain one copy of the database. You and your associates would connect to the SQL instance remotely. You would install SQL like you did on your workstation, but install it on the server do you have an IT team that might be able to assist? It's like your email, you can access it from anywhere as long as you have an Internet connection because the emails aren't stored on your workstation but rather is stored on a server 
It's the Enterprise version. The reason why I mentioned clustering is because because it had an option for installing nodes of a cluster when I installed it yesterday. I am not a DBA, so I assumed with clustering you get mirroring. Maybe that assumption is not correct. Maybe my focus needs to be mirroring. All I know 2 catastrophic SQL failures in 5 years is having a toll on me. I am too old to staying up 20+ hours trying to get fix a machine in hopes of saving 1 days worth of data. So maybe my question is how do I do mirroring? Is it build in MS SQL, or is there a middleware product that needs to be purchased?
A cluster is great for failover redundancy. You may have a minor delay in uptime, always on is a better solution for this but is also more complicated and more of a pain. Mirroring is a good idea, but probably over kill because you need the same server X 2. With a cluster, you need the same server X 2 but the disks can be shared and failover from one node to the other. For clustering, you must configure server clustering. Once the servers are clustered ( this is all outside of SQL Server, this is sys admin stuff at the server os level) you can begin the SQL Server portion of clustering which I think is easier. Once clustered, let's say you have the two servers, we will call them node 1 and node 2. If the DB and disks reside on node 1 and node 1 is patched and reboots, the db and disks are allocated over to node 2. Node 2 is now running the db and has the disks. There can be a delay here which can hurt uptime. Our delay is ~20 seconds. If node 2 goes down, it would then move the db and disks over to node 1. With mirroring, if it goes down, you may be able to access a read only version of the DB if you re-point the connectors to it. Or if you have a listener setup, you point to the listener so you're always looking at what's active. https://www.brentozar.com/archive/2012/02/introduction-sql-server-clusters/
The keywords you are looking for are "ON DELETE CASCADE": ALTER TABLE `campaignlabel` ADD CONSTRAINT `campaignlabel_fk1` FOREIGN KEY (`campaignid`) REFERENCES `gamelabel`(`id`) ON DELETE CASCADE;
Performance will take a hit if the images are fairly large. If the files are small or there is a security risk in saving the images to a directory, saving images to a database is possible (just not recommended by many). I created a project where images can be retrieved from an SQL database for security reasons using .NET. This code project was used as my [reference](https://www.codeproject.com/Articles/33310/C-Save-and-Load-Image-from-Database). Load times were relatively slow switching from record to record but it works. The underlying SQL command: Insert into [table] (indexvalue, pic1) Values (1, Image)
hello sorry for bothering you again. why is it that some of the selected instances have nil ID? if I remove the Left Join part such thing doesn't happen. also I added a where ID in () because I want to chain with previous query
&gt;consider using dynamic SQL No... never consider dynamic SQL unless there is absolutely no other option. It has terrible performance and can easily cause a security hole. Its the number one 'Do not do if at all possible'. OP: The following post has a bit of code you can re-purpose to parse and split a CSV string into a table: http://stackoverflow.com/questions/697519/split-function-equivalent-in-t-sql 1000x better than dynamic SQL and no need to worry about injections.
I do not have access to the server. The database is saved on my local machine. I am the only Data guy in this small business.
It is for a course I am taking. Building a small pilot application in visual studio in c# that will display my datatable of 16 car parts. I just need to add images (they were provided to me and are small) to each part in the database.
This is the best tl;dr I could make, [original](http://tech.marksblogg.com/billion-nyc-taxi-rides-aws-ec2-p2-8xlarge-mapd.html) reduced by 97%. (I'm a bot) ***** &gt; In this benchmark I&amp;#039;ll see how well 8 Tesla K80 cards spread across two EC2 instances perform when querying 1.1 billion taxi trips. &gt; Conf $ cd ~/prod/mapd-storage/1 $ initdb data $ mapd server -config mapd. &gt; I&amp;#039;ll create an environment variable with my credentials for MapD. $ read MAPD PASSWORD $ export MAPD PASSWORD. I&amp;#039;ll then execute the above SQL script through the aggregator so that the table is created on both leaf nodes. ***** [**Extended Summary**](http://np.reddit.com/r/autotldr/comments/67ohij/11_billion_taxi_rides_benchmarked_on_gpupowered/) | [FAQ](http://np.reddit.com/r/autotldr/comments/31b9fm/faq_autotldr_bot/ "Version 1.65, ~110309 tl;drs so far.") | [Theory](http://np.reddit.com/r/autotldr/comments/31bfht/theory_autotldr_concept/) | [Feedback](http://np.reddit.com/message/compose?to=%23autotldr "PM's and comments are monitored, constructive feedback is welcome.") | *Top* *keywords*: **MapD**^#1 **I&amp;#039;ll**^#2 **instance**^#3 **node**^#4 **data**^#5
You can do something like this to insert an image binary into a table: -- Create a table to hold your images CREATE TABLE ImageTable (ID INT IDENTITY(1,1) NOT NULL, Image VARBINARY(MAX) NOT NULL) -- Insert the binary of the image into the table INSERT INTO ImageTable (Image) SELECT BulkColumn FROM Openrowset (Bulk 'C:\Path\To\File.jpg', Single_Blob) as image
I received the following error when I tried to execute. "You do not have permission to use the bulk load statement."
&gt; "You do not have permission to use the bulk load statement." Bulk load is a server level permission. You'll need to grant the login that you are using the ability to use bulk load. GRANT ADMINISTER BULK OPERATIONS TO [YourServerLogin]
Is this a local instance of SQL Server? You will need to login as a sysadmin in order to grant that permission. Or you could just load the table while you're logged in as sysadmin.
I concur. The main reason for my situation was for compliance with file security. 
Start with `Tools -&gt; Import and Export Settings` I know it's not simple like a registry key or INI file, but it should get the important stuff. If they use Registered Servers, do this too: https://docs.microsoft.com/en-us/sql/tools/sql-server-management-studio/export-registered-server-information-sql-server-management-studio
Thank you for all your help. That is exactly what I decided to do since it has been too much of a headache to get the images in the actual database
ahh I see, I have a hunch that might be the case . I really need to learn sql these days. thanks a lot!
How does one DO?
Its not too late to redesign. I originally had all 73 companies in one table. Each company has 4 columns with approx 65-90 data points. So it is alot of data. 
BTW I completely agree with you. I am just new to SQL and after reading about Primary Keys I went and created 73 tables in my database. Id prefer to have only 2 tables. First table with Company ID, Stage, Company Symbols. Second Table with all Companies and the Data. Are you able to give me some pointers to do this?
If you've already created the tables, refer to my first comment about how to create a view that sits across all of them and acts as if it were a single table.
I think I understand. SO now I would have an extram column where it would just say Company ID, and 1 (for the first company) would be in the first column 73 times, then 2 (second company) 73 times, Short example Company ID / X / Y / Z 1 / Value / Value/ Value 1 / Value / Value / Value 1 / Value / Value / Value 2 / Value / Value/ Value 2 / Value / Value / Value 2 / Value / Value / Value Correct?
Im starting to understand. I actually started learning SQL today and it makes sense when you put it like that.
While you're redesigning, think about changing that stage column. If I interpret your intentions correctly, those are Roman numerals and therefore numbers and you should consider storing them that way. Numbers are numbers are numbers no matter how they're printed. You may paint yourself into a corner later storing them your way. I have found that is if often best to store data for what it is, not what it looks like. 
First the good news: the most important SQL features will be covered in the book if it's up to the 2008 R2 version. For Windows 10 install [SQL Server 2016 Developer Edition](https://www.microsoft.com/en-us/sql-server/application-development) (it's free), all features of 2008 R2 are also included in this version. You will also need to install the SQL Server Management Studio, which you will later use to get the database up and running. From the Errata on the [books webpage](http://www.informit.com/store/sql-in-24-hours-sams-teach-yourself-9780672337598) I got this info: &gt; Microsoft SQL Server &gt; &gt; First, you need to download the appropriate script file containing the DDL to create the tables and data used in this book. To do this, go to www.informit.com/register, follow the instructions to create an account, and register your copy of this book. Once you are able to access the bonus content for this book, download the TYSQL6e_MS-SQL-Code.zip file to your computer. Open the file to access CanaryAirlineScript.sql. &gt; To execute this script file, open SQL Server Management Studio (SSMS) on your computer. Click File, then Open, then File. Navigate to where you downloaded the file CanaryAirlineScript.sql. Choose Open. The cript should now appear in a query window. Click on Execute to run the script. Upon execution of this script, the sample tables are created and the corresponding data is loaded into the tables for use in subsequent chapters. You should be able to see and query your tables from within SSMS. 
Holy crap. This makes perfect sense. There was no mention on getting the source from that website. I will be trying this when I get off work at 3pm. Seems easy enough. Any special details I need to know during install/config of Sql server 2016? Or just default and click next where I can. Thanks a bunch for the swift reply! Side note, will this cover, in your opinion, what I need to pass the basic MS Sql cert exam. I will report back when I am up and running
If your goal is to learn enough to pass the exam, get a book tailored to the exam.
I was surprised at how much stuff WASN'T transferred when I used that tool. I couldn't notice that anything had changed when I used it on my own account. It didn't even bring over my preference for displaying the line numbers. It was a disappointment. I didn't even think about the registered servers though! Good catch, thanks!
Thanks again! You're super helpful!
This is good advice as MSFT likes to test on the new features released in that edition. Also, beware that the "querying" exams now have you write actual code, not just multiple choice. That caught me by surprise last time I tested as all the practice tests had just been multiple choice questions.
without knowing the data too well, its can be problematic to pull distincts on a left join.
 SELECT u.user_id, SUM(ih.invoice_amount) AS InvoiceTotal FROM erp_user AS u INNER JOIN order_number AS o ON u.user_id = o.user_id INNER JOIN invoice_header AS ih ON ih.order_id = o.order_id GROUP BY u.user_id Use left joins instead of inner joins if you also want to return users who have no orders/invoices.
You're going to have to be more descriptive. Compliant with what ISO standard. For what purpose?
I'm not sure? I've been told that enabling openquery to a production database so that I can validate data on a dev server is not ISO compliant. I wasn't given any specific source to this claim, and I don't believe it, but I thought I'd come here first.
There's no reason why SQL Server 2008 can't be used on Windows 10. While it's not officially supported, it should still work just fine (source: I have used it on Windows 10).
Just make sure you install Management Studio (SSMS) as well as the database engine - https://goo.gl/images/d7xfnT
Maybe they meant SOX or PCI and not ISO. Linking dev to prod would be a compliance violation for either of those governing bodies.
Probably violates A.12.1.4, in ISO 27001/27002 &gt;A.12.1.4 – Separation of development, testing and operational environments &gt;Control: Development, testing, and operational environments shall be separated to reduce the risks of unauthorized access or changes to the operational environment.
And this would only matter if there was PII, etc., types of data, correct?
SOX doesn't care much about PII specifically. PCI spells out what kind of data it's concerned about and access controls. 
[A Simple Guide to Five Normal Forms in Relational Database Theory](http://www.bkent.net/Doc/simple5.htm) 
My understanding was that he didn't want to count every occurrence of a status within an hour. But I did leave off the entire part about order! Editing it now. 
I think [this training plan](https://www.brentozar.com/needs/) is a good start. It's specific to SQL Server but the general concepts apply to all platforms especially if you are new in the field. For example, it's really important to learn how to perform and verify backups of your existing databases before searching for opportunities to improve their performance. Performance tuning is not worth anything if your system suddenly got corrupted and you can't restore it.
Know your backups. Schedule, location, recovery point and recovery time objectives. Test your backups regularly. This is job 0, the most important "best practice" for a DBA - you have to protect the data. If you don't have backups that you know can be restored properly, nothing else matters.
Thanks for the replies! I will see if I can tweak this to my needs. I only care about the 10 status and what I'm looking for is a point-in-time count. For instance, at the 11 hour on 4/25, two orders were in the 10. Thanks again for the help! 
Awesome suggestion! Thank you! Do you have any suggestions on finding non-profit work? I've searched through Craigslist and Google for non-profit organizations seeking SQL work but haven't had much luck.
This is very useful, thank you!
Networking is probably the best way. Idealist.org can give you a good idea of the problems that NPOs are facing, if you can turn their problems into SQL solutions (calling data from a 3rd party app, ingesting in SQL and pushing to salesforce) then you can start making pitches and handing out cards at social events. 
&gt; Sixth, monthly analysis. I have scripts that I run to verify active users, look at table space growth, look for invalid packages, and about 100 other things. I run it once a month and look for the delta between that and the last run. Remember, it's a database. The information is there. You have to figure out how to pull it and massage it into something that you can use. I feel like you could dump those parms daily into your own analysis database and do some nice charting to see how and when things are growing day-by-day, week-by-week, month-by-month. 
Brentozar.com, sqlskills.com, thescarydba.com. They give back so much to the community it's crazy. 
**Bad habits :** - Running things with unnecessarily privileged accounts or granting excessive permissions to standard user accounts. - Not testing backups regularly. - Running updates / deletes without using explicit transactions. - Running big updates/deletes without taking a backup (or checking a current one is available). - Running services or scheduled processes using standard user credentials. - Not performing housekeeping of user accounts periodically. - Not running regular DB maintenance processes (consistency checks, index optimisation, etc) - Having failure notifications switched off for scheduled jobs. - Weak (or blank!) sysadmin/root passwords. - Not having ETL processes and other interfaces properly documented (or no business contact/owner recorded). - Not monitoring DB growth and reviewing storage requirements periodically. - Going with the default configuration options without any thought. - Leaving scheduled jobs running months or years after they're needed. - Not having dedicated / separate test servers (and therefore testing on production) - Storing backups in insecure locations. - Being inconsistent in configuration options (between instances) for no good (or documented) reason. - Inconsistent naming of db objects. - Allowing db access to really wide user groups (e.g. all Domain users) because it's easier that way. - Not commenting stored procedures with at least a header. - Gratuitous use of SELECT *, particularly in production queries. - Not documenting system changes because you're in a rush. - Not reviewing system and application logs for months on end. - Not auditing schema changes (if possible). - Leaving dozens/hundreds/thousands of temporary objects cluttering your schema up. - Not keeping your DB software and OS up to date. More positively, if you're mainly working with a single database I'd suggest getting to understand it well. Learn how people connect with it, what client connections look like, what overnight processing occurs, where the bulk of the system data sits, how it's structured, how much data changes on a daily basis, who are the key system users, what time do people use the system etc, etc. It really does make things easier in the long run and it's a lot less stressful (when things go wrong) if you understand who you need to speak to and what the impact might be. 
&gt; test your backups YES! This above anything else. I have seen failed restores more times than I care to think about. Ideally, test them by restoring to the QA instance. If you don't have a QA instance that matches Production, then you are sub-optimal.
If the question you're trying to answer is "How many orders have Status 10 on '2017-04-02' at 14:00" then you could really simplify this down even more, but I'm trying to provide you with a complete view of what's happening here. But if you're always providing the date, time and status inputs and just expecting a number... select count(*) OrdersWithThatStatusAtThatTime from ( select distinct Order from #Time where Convert(EnteredDate, date) = @Date and DatePart(hour, EnteredDate) = @Hour and Status = @Status ) t;
Thank you a ton for the feedback, my main area of confusion comes from the "Create 4 records idno and the quarterly amount" What does this mean broken down?
Good question. It will only be a problem if they are on three different servers and don't allow remote querying. It's not really clear.
Thank you. I cannot get ms Sql 2016 to install. I originally downloaded ms 2014. Uninstalled it. Then dl 2016. There are still signs of 2014 all over the place on hard drive. I think it it raising conflicts. 
Welcome. Also try Highcharts. [highcharts](www.highcharts.com) That may be a bit more of what you're looking for as far as displaying dashboards goes.
Nice! Good to hear!
So how would I create these new records from the data I'm receiving in my output? So far I have: SET SERVEROUTPUT ON SET VERIFY OFF DECLARE v_idno donorbackup.idno%TYPE :=&amp;input_idno; v_yrgoal donorbackup.yrgoal%TYPE; v_newgoal donorbackup.yrgoal%TYPE; BEGIN SELECT yrgoal INTO v_yrgoal FROM donorbackup WHERE idno = v_idno; IF v_yrgoal &gt; 500 THEN v_newgoal := v_yrgoal * 2 ; ELSE v_newgoal := v_yrgoal * 1.5; END IF; UPDATE amttopay SET amt = v_newgoal WHERE idno = v_idno; END; / SET SERVEROUTPUT OFF SET VERIFY ON 
 SELECT u.[User_ID] as "ID", u.User_FIRST_NAME as "LastName", u.User_LAST_NAME as "FirstName", COURSE.COURSE_CRN as "CRN", COURSE.COURSE_SUBJ_CODE as "SUBJ", COURSE.COURSE_CRSE_NUMB as "CRSE", UCOURSE.UCOURSE_TERM_CODE as "Term" FROM PERSONAL.COURSE COURSE LEFT JOIN ( PERSONAL.UCOURSE UCOURSE left join PERSONAL.[User] u ON UCOURSE.UCOURSE_PIDM = u.User_PIDM ) ON COURSE.COURSE_CRN = UCOURSE.UCOURSE_CRN WHERE UCOURSE.UCOURSE_GRDE_CODE IN ('W','NP','F') AND UCOURSE.UCOURSE_TERM_CODE = 201603 AND CONVERT(VARCHAR(255),u.User_ID) + CONVERT(VARCHAR(255),COURSE.COURSE_CRN) IN ( SELECT DISTINCT MyDistinct = CONVERT(VARCHAR(255),u.[User_ID]) + CONVERT(VARCHAR(255),COURSE.COURSE_CRN) FROM PERSONAL.COURSE COURSE LEFT JOIN ( PERSONAL.UCOURSE UCOURSE left join PERSONAL.[User] u ON UCOURSE.UCOURSE_PIDM = u.User_PIDM ) ON COURSE.COURSE_CRN = UCOURSE.UCOURSE_CRN WHERE UCOURSE.UCOURSE_GRDE_CODE IN ('W','NP','F') AND UCOURSE.UCOURSE_TERM_CODE = 201603 ) 
Wow, thank you so much /u/krankie. The donor table is already created and populated. The amttopay is empty so I'm taking that info, checking to see if it's greater than 500, run an execution and the output gets placed into amttopay broken up into 4 pieces. 
Oh no I mean't, since the yrgoal is split into quarters, that gets printed out 4 times in 4 separate lines. Like this: IDNO NAME STADR CITY ST ZIP DATEFST YRGOAL ----- --------------- --------------- ---------- -- ----- --------- ---------- CONTACT ------------ 12121 Jennifer Ames 24 Benefit St Orwell KY 09075 24-MAY-97 400 Susan Jones The user inputs an idno into the prompt, and it searches the donor table, which then picks out the matching line and checks to see if the yrgoal is greater than 500. Does its execution which always results in being divided by 4 or (quarters). So yeah its definitely worded poorly, and I apologize if I translated it to an equally poor degree. I'm extremely grateful for your help. 
Because I can have greater control in how new values are cached instead of relying on the old default settings of identity. Don't get me wrong, I set identity(1,1) for a lot of tables but the ones where most of the action happen in an application I want to make sure it has everything it needs to work without hiccups.
Yup, bigint and UNIQUE constraint was the actual correct answer. All their questions are horribly phased.
dude, i've been interviewing people for senior SQL/BI position... the number of basic questions that can't be asked (what types of indexes, what type of constraints, diff between decimal and float, describe a transaction, etc) is astonishing. many of these people already have a similar title.
A view that looks for trips planned `WHERE trip_date BETWEEN GETDATE() AND DATEADD(day, 30, GETDATE())` sounds like it would work.
&gt;When the client runs the report, isn't something going to have to be stored on the database that records the date that the report was run? No. Your query is reporting on all events which are scheduled to transpire between **right now** and **right now plus 30 days**. You don't have to record anything here. &gt;Or is this done via the reporting software? Your reporting software (which is not a concern here, per the italicized note) will probably record the parameters that are used to execute the report, but that's out of scope here. &gt; Or should I just be worrying about creating a stored procedure that adds 30 days to the input date variable? From the description, you're permitted to create whatever you need to return the required data. Taken at face value, you don't even need an input parameter. &gt;Since we aren't using reporting software and our assignments use a standalone database with no front-end, this question is confusing. The assignment is just asking you to create a query and any other database objects necessary to produce a result set that meets the criteria. Reporting software or a front-end (other than SSMS) would just make that result set look nicer.
You could always write a script to automate it with Python and sqlalchemy (or whatever). Hard to give better advice without more information. 
 Check this out and see if this is what you are talking about. http://blog.bubble.ro/how-to-make-multiple-updates-using-a-single-query-in-mysql/
&gt; Could I just add fields (i.e. GUIDE_ID, GUEST_ID, etc.) to make my life easier? yes... the ones in the SELECT clause
Exactly. When you aggregate one field in the select statement, you need to either aggregate or group by all the rest. 
If it were me, I'd use a subquery. I don't quite understand your schema, so I can't say exactly how it would come out; but I think you'd use a group by on trip_id in the tours_reservation table, then join that result to the other tables to get the other fields.
In my experience, if you care about the value of the unique identifier much beyond the value that the field starts at, you shouldn't use IDENTITY or SEQUENCE at all. Requirements like, "Every number must be assigned sequentially," don't work well with IDENTITY or SEQUENCE. That's just not what they're for. I've used systems that required sequential identifiers, and for those they used custom rolled identifier creation instead of an IDENTITY or NO CACHE SEQUENCE. 
 SELECT A.AcctNum, A.Amt, A.PostingDate, A.TrxType FROM A INNER JOIN B ON A.AcctNum = B.AcctNum WHERE B.PostingDate &lt;&gt; A.PostingDate Use the INNER JOIN to get the accounts and then the WHERE clause to filter instead of within the JOIN
&gt; ELECT a.* &gt; FROM @tablea a &gt; JOIN @tableb b &gt; on a.acctnum = b.acctnum &gt; and a.postingdate &lt;&gt; b.postingdate The amount doesn't have to be the same. As long as the adjustment has the same accountnumber and the postingdate of the adjustment is not the same date as the charge, then that accountnumber can go to the desired output table. 
this is the exact query that i used. unfortunately, this query would pull in both accountnumber. 456 should not be in the output table because it has an adjustment on 3/3/17 which matches the postingdate of the 456 charge in table A.
What about a subquery to find all the account numbers that would be valid?
Have an upvote, good answer, very similar to mine
[Here it is in SQL Fiddle](http://sqlfiddle.com/#!9/50ab3/4/0) 
Edited my original post.
Been a while, and I'm sure you could do it with a hash, but couldn't you just say ... SELECT Item, Number, COUNT(asterix) FROM Employee GROUP BY Item, Number HAVING COUNT(asterix) &gt; 1 Not the fastest way to do it, but ... I think that would work. Edit: I guess the asterix is used in markup ... so replace the word asterix above with an actual asterix. 
thanks! this works. 
What's the My $.... oh I get it... lol I understand all that except how do I make it itterate through the whole new database? Or does adding the ON statement make it all work? What if there's a WHERE statement like: WHERE crm_Deleted IS NULL AND crm_status = 'complete'
Thanks for the response! The ultimate question I'm looking to answer is how many orders are in status 10 at the intervals noted above for any given day. The data point that I need to count does not exist. What I have is start points (when the status 10 begins) and end points (when status 30 begins. From those start and end points I need to come up with a way to count in between those times (on the hour). Also, any given order can switch back and forth between 10 and 30 multiple times. Basically, I need a point-in-time audit where one does not exist. I've used what you've provided thus far to, alas, no avail - but I'll keep at it. Thanks again for taking a look! 
 IF OBJECT_ID('Customers','U') IS NOT NULL DROP TABLE Customers CREATE TABLE Customers ( CustomerID INT NOT NULL IDENTITY(1,1) PRIMARY KEY, CustomerName VARCHAR(100), ContactName VARCHAR(100), Address VARCHAR(100), City VARCHAR(100), PostalCode VARCHAR(11), Country VARCHAR(20) ) INSERT Customers (CustomerName, ContactName, Address, City, PostalCode, Country) VALUES ('Cardinal', 'Tom B. Erichsen', 'Skagen 21', 'Stavanger', '4006', 'Norway'), ('Poppy', 'Faceless Erichsen', 'Pool 21', 'Scavenger', '6969', 'Picnic'), ('Acer', 'Britney Erichsen', 'Fool 21', 'School', '1227', 'Canada') UPDATE a SET ContactName = b.ContactName, City = b.City FROM Customers a INNER JOIN ( SELECT CustomerID = 1, ContactName = 'Alfred Schmidt', City = 'Frankfurt' UNION SELECT CustomerID = 2, ContactName = 'Facepalm Schmidt', City = 'America' UNION SELECT CustomerID = 3, ContactName = 'Oreos Schmidt', City = 'Germany' ) b ON a.CustomerID = b.CustomerID INSERT Customers (CustomerName, ContactName, Address, City, PostalCode, Country) VALUES ('Cardinal', 'Tom B. Erichsen', 'Skagen 21', 'Stavanger', '4006', 'Norway'), ('Poppy', 'Faceless Erichsen', 'Pool 21', 'Scavenger', '6969', 'Picnic'), ('Acer', 'Britney Erichsen', 'Fool 21', 'School', '1227', 'Canada') SELECT * FROM Customers a 
SQL statements will affect everything in scope. JOIN defaults to an INNER JOIN, so the JOIN .... ON means that the query will include/affect only records where the joined field is the same in both tables. If you're looking to change all records where there is a matching ID across both tables you don't need a where clause at all. WHERE ND_Labor is null will prevent it overwriting anything that already has a value How to find the current precision depends on the platform you're using which you haven't told us. Don't worry too much about this though - just define your precision so it's good enough to cover the source data, and provided the destination precision isn't less than that it won't error. Do be aware though if any of the varchar data can't be converted to a number it will cause an error.
Looks to me like this is just teaching you natural keys. Keep going.
alright this is a stretch, but would clearing out system cache (ccleaner or something) and a restart maybe be worth a shot?
Open it up in XML view and search for the old file name there?
What database are you using? Oracle? MySQL? Postgres? By "batch" do you mean they need to be in the same transaction? (As in, they need to **commit** at the same time?) You probably mean this. If so, commit at the end instead of after every statement. If you use a database that auto-commits then change the setting (can be statement level or session level or database level). You might be using a program to connect to the database that auto-commits very unhelpfully, so check for that, too. Or by "batch" did you mean you want to do it all in one statement? If so, you can use a block. In Oracle for example: BEGIN insert...; insert...; ... END; Don't forget to commit at the end. The only thing this accomplishes is it allows the entire block to be sent as one command which is sometimes necessary with some applications or programs. It has no effect on the transaction whether these are submitted one-by-one outside a block or one-by-one inside a block. Either way it's all about the timing of the commit. Or by batch did you mean you need to run it batch-mode like in a background job? Somehow I doubt you meant this.
Oh, I think I get it now... maybe... For every "Status 10" event, is there a following "Status 30" event that indicates the "Status 10" is no longer active? If so, we can number all of the Status events, then join them together, providing a time range in a single record per occurrence of "Status 10" event. I don't know how you're specifying a datetime range to check against, so this is as far as I can take it: with Status10 ([Order], EnteredDate, RowNum) as (select [Order], EnteredDate, row_number() over (partition by [Order] order by EnteredDate) from #Time where Status = 10), Status30 (Order, EnteredDate, RowNum) as (select Order, EnteredDate, row_number() over (partition by [Order] order by EnteredDate) from #Time where Status = 30), MatchedStatus ([Order], EnteredDate, EndDate) as (select a.[Order], a.EnteredDate, b.EnteredDate EndDate from Status10 a left outer join Status30 b on a.[Order] = b.[Order] and a.RowNum = b.RowNum) select [Order], EnteredDate, EndDate from MatchedStatus m order by [Order], EnteredDate; If my assumption about matching events is not correct, this would make a huge, useless mess.
What platform? I'd imagine the solution would involve querying system tables to create dynamic SQL, which would be highly platform specific. My first thought is that said dynamic SQL would loop through all tables, create a "master" table that would have user_ID, table_name, number_of_occurrences, and then query that to get a match count. Question needs to be clearer around whether a user_ID would appear in each table more than once, and whether the total count would be a count of tables it occurs in or the total number of occurrences across all tables.
You'll want to use names that mean something, mainly for two reasons: 1) ambiguous columns. You'll have to give ambiguous columns an alias, and it will be convenient to know which specific table it is. Personally I like to alias all of my columns anytime I have a join or sub query. 2) execution plan. The Execution plan will usually give you a result set for each point, and having a good alias will allow you to easily diagnose issues. Let's say you have 3 tables: customers, orders, and customerorders. If your order table has customerID in it, then you'd simply be able to use CUSTOMER AS C INNER JOIN ORDER AS O ON C.ID = O.CustomerID If you need the CustomerOrders as a relationship/linking table, then you'd have something like: CUSTOMER AS C INNER JOIN CustomerOrder AS CO ON c.ID = CO.CustomerID INNER JOIN ORDER AS O ON CO.OrderID = O.ID Salesforce uses link tables (objects) a lot, so that's why I referenced that situation. Hope that helps a bit. At least the first letter of the table helps a ton. If two tables have the same first letter, then I usually just use the whole table name as an alias as long as it's not too long. If the table is made out of two or more words (CustomerOrder) then I just use the first letter of each word.
foo, bar, baz, wux in that order. If there are more than four I use descriptive names like a good person.
You'll want to use the [CHARINDEX()](https://docs.microsoft.com/en-us/sql/t-sql/functions/charindex-transact-sql) function. That coupled with LEFT or SUBSTRING or a few other string functions will get you your goal.
Delete the conditional split. Delete the three data flows. Save the SSIS package, close down Visual Studio/TFS, and for good measure restart your computer. Open it back up and add back the split and data flows (but add them from scratch, don't copy/paste from somewhere you saved them in the GUI.) SSIS has a frustrating habit of "remembering" metadata. And I've even had packages that didn't behave properly yet looked fine -- so saving/closing VS and re-opening it somehow sorted things out. Good luck!
If you don't have company standards, go with what works for you. In my case, I'd likely alias tables like this: EMPLOYEE AS E POSITION_ASSIGNMENT AS PA QUALITY_STANDARDS AS QS ...etc. That way I can preface all of my columns with the alias, to avoid ambiguity and increase clarity. For temp tables, I usually name them something intuitive. So if I'm creating a temp table around Employee data, I might call it #tmpEmp. If Accounts Receivable, #tmpAR. And I don't know if your platform allows for common table expressions, but I always preface those with cte...so a CTE around Employees I'd name cteEmp or something. When you're deep into a 300-line stored procedure, it's really helpful if the aliases and temp tables have some sort of meaning. I'd get completely confused if I had #tmp1, #tmp2, etc. You asked about commenting -- I comment almost everything. I personally don't think you can over-comment, but I know some developers who think good code is self-commenting. I find comments extremely helpful, even for code I've written when I have to go back later and do maintenance.
one of two conventions: - the capital letters from CamlCasing of source table/view - don't alias seriously, I sometimes get flak from coworkers for my use of [fully].[qualified].[object].[names]... but dammit if it ain't clear what's going on.
This code assumes you only have one underscore, and if no underscore is in the string you'll get two blank columns: DECLARE @string VARCHAR(30) = 'clientbills_reference843'; SELECT REPLACE(LEFT(@string, CHARINDEX('_', @string)), '_', ''), RIGHT(@string, CHARINDEX('_', @string)); 
Did you parameterize the connection manager? Check the expressions to make sure that you haven't set the path to pull from a parameter or variable that you have set to the old value. You could also take the shortcut and just check all of your parameters and variables to make sure that they're not referencing the wrong path.
I am new to SQL, so there might be mistakes here, but here is how I would try: Use CHARINDEX('_') which will give you the place of the underscore. LEN(col) - CHARINDEX('_') + 1 should return the length of the string from the underscore to the end, this is to find out how long it should be after the underscore. SUBSTRING(col, CHARINDEX() + 1, LEN(col) - CHARINDEX() + 1) I believe this will create a substring that starts after the underscore, and capture the rest of the string. I think this could also work RIGHT(col, LEN(col) - CHARINDEX() + 1) Let me know if it works because I am also learning SQL right now!
Oh yeah! Will be testing for awhile! Thanks again! 
Platform is currently sqlite3 (data is injected using python sqlite3 module) but could be rewritten for mysql or postgres. Each table contains no duplicates of user_IDs. The total number of occurrences in all tables = total matches for all tables. I'll certainly dig further into forming dynamic sql for either platform. 
I certainly hope you're only doing that during data exploration and not with queries that might actually make it into production...
I always use either two or three letter abbreviations for table names, and I tend to lean towards three usually. Some reasons for this are: 1) It allows my team to standardize our use of aliases across most/all of our reports and this saves us tons of time and heartache trying to discover what the purpose of a query is and what it is doing. 2) Using three letters not only ensures that someone reading my code can easily understand what the query says, but it also makes the query much easier to scan through to find specific columns, errors, etc. because you're looking for a string of letters rather than a single one (which I find very easy to lose in a long query). 3) Three-letter aliases cover both single and multiple word tables, for single word tables I use the first three letters, for two-word tables I use the first two letters of the first word and the first letter of the second, for three or more word tables I'll take the first letter of the first three words.
I am not sure why you would want to do this in the query rather than whatever tool/application is consuming the data, however: Nest whatever your existing query is in a subquery, then use the following for those fields. CASE WHEN &lt;Insert Column Name for P/R&gt; = 'Rate' THEN NULL ELSE &lt;Insert Column Name for Rank&gt; END AS Rank, CASE WHEN &lt;Insert Column Name for P/R&gt; = 'Rate' THEN NULL ELSE &lt;Insert Column Name for Group&gt; END AS Group, CASE WHEN &lt;Insert Column Name for P/R&gt; = 'Rate' THEN NULL ELSE &lt;Insert Column Name for Product&gt; END AS Product,
I guess I still don't understand the need - I mean the data being there on all rows might be duplication, but it definitely doesn't leave room for confusion, quite the opposite. Its absolutely clear what data belongs where. In our organization, we would use something like SSRS reports or Tableau for data exposure, and any formatting would be handled there.
 👍
&gt;real names CUSTOMER AS CelineDion INNER JOIN CustomerOrder AS ChrisODonnell ON CelineDion.ID = ChrisODonnell.CustomerID INNER JOIN ORDER AS OJSimpson ON ChrisODonnell.OrderID = OJSimpson.ID ?
There are a couple of things going on here. The Senior wants to run these reports quarterly, so she wants a query that just about any of our analysts can run. I suggested we build out an SSRS report that could do it, and offered to do that. The response I received was that there are people who work here whose job it is already to do that, and so I shouldn't be building SSRS reports. When I gently pressed as to why *they* weren't putting this together, the most coherent answer I received was basically that the Senior didn't want to give up control of the project to another department. So that's why I'm here at quarter to five dicking around with a less-than-efficient method for this project, rather than doing it better, or handing it off to someone who gets paid to do it better.
Thanks!
thank you all for the suggestions. i figured out what was causing the old file path to fail my package and it wasn't even an issue with the SSIS package itself. the "unrelated" destination object i mentioned was connected to a different accdb that had a linked table that referenced the old file path. what's weird was that my data flow doesn't even export to anything related to that broken linked table. after fixing the linked table in that file, the problem was solved. hope this might help someone in the future.
Just post the problem, someone will do it free. Possibly even me.
Yeah dude, you don't have to pay for this. Just looking at it I am thinking you are going to join the tables on dc_number and then select whatever columns you want from the offenders table and add in a count of each receipt date from the dc_incarhists table doing the group by and order by Basically (on mobile so expect typos and crap formatting): Select A.dc_number, A.first_name, A.last_name, count(B.receiptdate) as num_incar From offenders as A Join dc_incarhists as B On A.dc_number = B.dc_number Group by A.dc_number, A.first_name, A.last_name Order by count(B.receiptdate) descending 
 SELECT off.dc_number, off.last_name, off.first_name, count(distinct inc.receiptdate) num_incar FROM offenders off INNER JOIN dc_incarhists inc on inc.dc_number = off.dc_number GROUP BY off.dc_number, off.last_name, off.first_name ORDER BY count(distinct inc.receiptdate) desc
&gt; Select A.dc_number, A.first_name, A.last_name, count(B.receiptdate) as num_incar &gt; From offenders as A Join dc_incarhists as B On A.dc_number = B.dc_number &gt; Group by A.dc_number, A.first_name, A.last_name &gt; Order by count(B.receiptdate) descending Just tried some of the code you guys wrote and tried to get it to execute and I ended up getting this. What needs to be touched up in the code after my changes. I probably messed something up when filling out the names. http://imgur.com/RMc0SDR
just tried some of the code you guys wrote and tried to get it to execute and I ended up getting this. What needs to be touched up in the code after my changes. I probably messed something up when filling out the names. http://imgur.com/RMc0SDR 
It's running though? Just the inflated incarceration numbers is the problem l? Try changing count(B.receiptdate) to count(distinct B.receiptdate)
That's cool just make sure you include a reference to this thread in the bibliography of your assignment in a manner consistent with the academic honesty policy of your education institution.
We’re using listener in connecting to our databases and based on what we know, whenever the primary replica is up and running, all requests will be redirected to it. But since the batch job that creates the snapshot ran on the primary replica, the said snapshot was not created or replicated on the secondary replica. So whenever failover to secondary replica happens, all components that are using/referencing the snapshot will encounter an error. This wasn’t a concern up until we implemented High Availability set-up in our databases. 
You should GROUP BY everything in the SELECT list except the count, and your filter for 50 should be a HAVING
This is what I've got so far but all it's giving me back is the first &amp; last names of the directors without the number of films they've directed. SELECT DISTINCT firstname, lastname FROM dbo.movie_directors INNER JOIN directors ON directors.directorID = movie_directors.directorID GROUP BY directors.firstname, directors.lastname HAVING COUNT(movie_directors.directorID) &gt;= 50;
Add your count to the select clause.
and delete the one at the end or keep it? This is what I've got now, and I'm getting an error. SELECT DISTINCT firstname, lastname COUNT(*) AS NumMovies FROM dbo.movie_directors INNER JOIN directors ON directors.directorID = movie_directors.directorID GROUP BY directors.firstname, directors.lastname HAVING COUNT(movie_directors.directorID) &gt;= 50; 
You're missing a comma before count. Keep the one at the end. 
Thanks so much for your help. I was able to easily fix the SYSDATE issue by just doing this: alter session set nls_date_format = 'dd-mm-yyyy hh24:mi:ss'; I was also able to use the link you provided for Auditing with Triggers and get something to work. CREATE OR REPLACE PACKAGE Auditpackage AS Reason VARCHAR2(10); PROCEDURE Set_reason(Reason VARCHAR2); END; / CREATE TABLE HS_MINIONS ( ID NUMBER GENERATED BY DEFAULT ON NULL AS IDENTITY, MINION_NAME VARCHAR (30) NOT NULL, MINION_RARITY VARCHAR (10) NOT NULL, MINION_COST NUMBER (1) NOT NULL, MINION_CLASS VARCHAR (10) NOT NULL, MINION_TRIBE VARCHAR (10), CARD_SET VARCHAR (20) NOT NULL, STANDARD VARCHAR2(1) CONSTRAINT CARD_STANDARD CHECK (STANDARD IN ('Y', 'N')), PRIMARY KEY (ID)); CREATE TABLE AUDIT_HS_MINIONS ( OLD_NAME VARCHAR (30), OLD_RARITY VARCHAR (10), OLD_COST NUMBER (1), OLD_CLASS VARCHAR (10), OLD_TRIBE VARCHAR (10), OLD_SET VARCHAR (20), Systemdate DATE); CREATE OR REPLACE TRIGGER AUDIT_MINIONS AFTER INSERT OR DELETE OR UPDATE ON HS_MINIONS FOR EACH ROW BEGIN INSERT INTO AUDIT_HS_MINIONS VALUES (:old.MINION_NAME, :old.MINION_RARITY, :old.MINION_COST, :old.MINION_CLASS, :old.MINION_TRIBE, :old.CARD_SET, SYSDATE ); END; I'm happy this works, but I'm wondering if there's a way to make it even better. For example, if I delete something from HS_MINIONS, then what I deleted shows up in AUDIT_HS_MINIONS. That's great. But if I insert something into HS_MINIONS, then the audit table just has a bunch of null values for everything by the system date. If i were to change everything to .new in the trigger, then the opposite would happen. Is there any way to fix that, such as with an IF/THEN statement? Also, if there's an update statement, then will show the old or new values, but it isn't very helpful in actually showing what was changed, just what it used to be or what it is now. Is there a way to be more granular in this regard so a user could see exactly what the change was? In the larger scope of things, I don't know if these are even relevant questions. I'm learning this because I might want to get a more SQL focused role at some point, so maybe what I already have would work fine for most businesses. I don't know if there's a standard way to handle audit tables, so any advice in that regard would also be great. :)
This solution worked for me: SELECT name, number_grade, ROUND(100 * fraction_completed, 0) AS percent_completed FROM student_grades;
I was going to apply solely with my reddit username ;) Thanks, I'll look for that. Do you know if tech companies care that much about experience? I come from a marketing background and am just switching over to the IT side of things.
&gt; SELECT name, number_grade, ROUND(100 * fraction_completed, 0) AS percent_completed FROM student_grades; Thats what i've done and it's not working 
There are a lot of entry level analyst positions. Mostly grunt work, but the pay is pretty good. Obviously companies look for experience in most facets, but when you have a technical skillset you have an advantage.
Call some it recruiters​. They get paid to place you. Robert half is a big nation wide firm. Be honest about your experience. Hiring seems to go in cycles. You may get placed in a few weeks to a few months. Be flexible with what your looking for at first and be willing to take some short contracts. Re your experience as long as you know your stuff there'll be work for you. Getting the first gig is the hardest. I've got no it degree and about half my co workers also don't have it degrees. I'm not interested in taking more college classes, but I'm very much still learning. It's now from my coworkers and online resources and books. 
I'd say you probably don't need an audit record for an insert, or the `:new` version of an insert, because it would have the exact same data as the actual table. I'd do something like this: 1. Create `AUDIT_HS_MINIONS` with the exact same field names and data types/lengths as `HS_MINONS`. Add `CHANGE_DATE date` and `CHANGE_TYPE varchar2(6)` to the beginning of the field list. 2. On `insert into HS_MINIONS`, do nothing to `AUDIT_HS_MINIONS`. Or, if you want, just set `CHANGE_DATE = sysdate, CHANGE_TYPE = 'INSERT', ID = :new.ID` and leave everything else blank. 3. On `delete from HS_MINIONS`, insert a record to `AUDIT_HS_MINONS`, with `CHANGE_DATE = sysdate, CHANGE_TYPE = 'DELETE'`, and all other fields coming from `:old`. 4. On `update HS_MINONS`, first check if there is actually a change. If so, do the exact same insert :old you did with `delete`: . if ( nvl(:old.MINION_NAME, 'qqq') != nvl(:new.MINION_NAME, 'qqq') or nvl(:old.MINION_RARITY,'qqq') != nvl(:new.MINION_RARITY, 'qqq') or .... --check each field ) then ... --else do nothing, because nothing changed end if; What you end up with is the current state of the record in `HS_MINIONS` (of course), and all previous states in `AUDIT_HS_MINIONS`. You could see the change over time by unioning and sorting by CHANGE_DATE: select * from ( select * from AUDIT_HS_MINIONS where MINION_NAME = 'Deathwing' union all select null, '&lt;&lt;CURRENT&gt;&gt;', HS_MINIONS.* from HS_MINIONS where MINION_NAME = 'Deathwing' ) order by CHANGE_DATE 
You mentioned you're switching to the IT side of things- so I'm not sure how long you were previously in marketing, but if you have knowledge of any specific system front-ends, they can help open the door for a lot of analyst positions. I started my IT career in education, but not with SQL at first. I got a lot of front-end experience working with student info systems and then moved into a job focusing on SQL. It's opened the door to a lot of other similar jobs in education, or potentially a move to development/consulting for the companies that make the systems. Either way, the front-end knowledge has been an incredibly useful tool while working with the database. So all that said, you may have an advantage with marketing companies, or other companies using similar systems. Otherwise, starting somewhere that uses systems that can help you get other jobs in the future (peoplesoft, workforce, etc) is always a good idea.
I have held this position and hired for it in the past. I can't speak to others, but if you have certifications and can clearly answer technical questions about how to query a database and seem on top of things I couldn't care less about experience. If you can show that you have a grasp of the core concepts of querying a database, that you can learn our business and reporting needs, and will give at least 85% effort, I'd hire you for an entry level position regardless of your previous experience. SQL doesn't care if you've been flipping burgers for the last decade, just that your syntax is right.
In your pastebin, you had some extra selects, maybe they are interfering. Try this: https://pastebin.com/SCxQxNX7
To select for only 2010, just add another clause under where: AND movie.movieYear = '2010'
Hi, I see you are postgreSQL DBA. Do you know how I can give access to the postgreSQL sitting on my local machine to someone else so he or she can maintain it as well?
If there is only one row per date, then you can do this: SELECT TOP 2 * FROM dbo.datatable ORDER BY date DESC; If there are several rows per date, and you want all of them for the two latest dates, this fetch them: SELECT * FROM datatable AS d WHERE d.date IN (SELECT DISTINCT TOP 2 date FROM datatable ORDER BY date DESC);
you have about an hour left. I'd get off reddit and take a xanex. But only if you've taken xanex before and know the right amount. Don't over do it.
"just in time" learning is probably not in time
Never taken a xanax before. But I can try to get off reddit. Thank you though.
Not sure it's more interesting but most financial institutions use SQL in some fashion or another, I spent 12 years supporting buy side trading at a major investment bank, and 90 percent of our data was in Oracle so I used SQL on a daily basis.
I've got a number of really good ideas for you but they all involve time travel.
Please go ahead. I have a secret DeLorean.
Programing... all BI is mostly the same in all businesses. 
"Business intelligence" in the title is also common. 
How are you doing SQL in codepad? It doesn't appear to allow you to run SQL queries there.
This. Another thing you can do is add a check constraint to make sure it's Truncated.
I really like https://academy.vertabelo.com/ for this, but it's not completely free. Personally I also review date and time related SQL documentation since it can get extremely confusing and easy to mess up if the data set includes different local timestamps. 
I can't run it. I have to type the query down for a scenario and the interviewer can watch what I type on the screen realtime. 
Thank you. Happy to pay for high quality lessons. The price seems steep, do the individual exercises get very advanced? 
You're welcome. Note that you can also download SQL server Developer from Docs.microsoft.com, and stackoverflow's database is available Free via BrentOzar.com
I made use of stackoverflow.com as my learning resource. It is a knowledge sharing forum where tech questions faced on a daily basis are posted and answered by the users.This is kind of like a bottom-up approach. Instead of studying the concepts and learning them with examples, you search for real problems and see how people found solutions. This will be an advanced level of practical understanding for known concepts. I wouldn't suggest this approach if you are not familiar with a concept itself. 
I used a slightly different version of Vertabelo Academy that was made for my workplace's specific tables and needs, but just taking a look over these 2: https://academy.vertabelo.com/course/sql-queries and https://academy.vertabelo.com/course/standard-sql-functions , it looks like almost the same thing I used. The custom version I used was awesome. I'm using the skills learned on a near daily basis at work with long, sometimes 200 line queries. If you are a beginner in SQL, this is a fantastic learning tool. If you're intermediate, it's a good refresher. If you're advanced, this is kinda easy. The good thing is they have a try before you buy pricing model. 
That's funny, [we had almost this exact same question a few hours ago](https://www.reddit.com/r/SQL/comments/692z65/need_help_with_problem/) and OP deleted it after he got an answer. I guess it's college assessment time somewhere? Join your directors and movies tables, group by director and HAVING COUNT(distinct genre) &gt; 1
Thank you for the resources. I'm mostly looking at Oracle and MySQL due to wanting to work at Amazon, would you still recommend studying SQL Server?
Thank you, I hadn't considered using StackOverflow to pull real life examples. There are some great responses in here.
The difference between Intermediate and Advanced seems blurry. The difficulty seems to increase exponentially. Looking at some of the later exercises in Vertabelo it seems to cover things I haven't found elsewhere. I'm giving the trial a trial. Thank you.
sheesh, you guys automatically assuming TOP is gonna work in OP's database
Perfect. Good luck. 
Try focusing more on redshift instead of SQL server then. SQL server is fairly easy to pick up if you know Oracle.
Thank you!
Create a user for the person with a password - Done Grant them appropriate permissions, ideally using groups - Done Edit pg_hba.conf to allow that user to connect to the database (md5 is probably the access type you want) - All four Types in the `pg_hba.conf` method are md5. Edit postgresql.conf to make sure Postgres is listening on the right port (5432) for the IP where the person will be accessing (you can have it listen on * for all IPs) - listen_addresses = '*' Reload the configuration using pg_reload_conf() or restart the postgres service - `select pg_reload_conf();` returned t. Then I copied the 9.6 and psqlODBC folders into the user's folder I created and downloaded pgAdmin 3 on his computer and opened it and entered the password assigned to him and returns `Error connecting to the server: FATAL: password authentication failed for user ....`. Any assistance would be appreciated here Erudition303.
You can do that way. SELECT Date FROM datatable GROUP BY Date ORDER BY Date DESC LIMIT 0,2 I assume you are on MySQL, so `LIMIT` is a built-in. Otherwise there are similar function on other RDBMS.
If you don't mind me asking, what did you do exactly for them? What were you responsible for? I primarily worked more in ETL so it was more focused on dumping in new data and cleaning it up but I'm interested in learning more about what else is out there. 
Eh... honestly this is shitty advice... Working with shitty developers will teach you bad habits and you wont know it and likely spend the rest of your career in bad shops. Beyond that shitty shops are shitty for a reason, they pay less because if they paid well, the good candidates would stick around. You probably won't have any kind of mentor/team environment and won't really know how bad your QA is until you fail to land your next job because you didnt know what a unit test was. 
I couldn't really say. All of the SQL devs/DBAs I've come across never bothered with certs or anything, unless their employers were paying for it, as they just kind of fell into it, and I was the same way. My first real job out of college was as a Production DBA (re: backups, high-availability, disk/hardware performance, updating/patching) and never really got to delve into the deep query writing, since that was the other half of our team's responsibilities. This was at a huge 30,000+ employee company, so I only ever got to work on my specific area while I was there. My next job was again as a DBA, but at a much smaller company (&lt;100 people). Since our team was only 4 people (2 DBAs, 2 BI/Analysts), I was able to get my hands into a lot more than purely Production DBA work. I was able to get with our BI guys and learn about SSAS/DWH/Tableau/SSRS, and I learned what I could from them while I was there. We did lunch-and-learns for knowledge sharing, SQL PASS events for work trips, and just tried to help out the over-worked BI guys any chance I could get. This is where I was able to learn the bulk of my SQL knowledge. I would have various departments asking me to run/build reports for them, so I was able to use more of the analytical functions within SQL like LAG/LEAD, OVER, CTEs, and so on within years of sales/customer data. Funnily enough, I'm in an odd spot now. I do full-stack dev as a contractor making 3x what I was making as a DBA, but I do sometimes miss writing just SQL day in and day out.
 with RANKED as ( select T.*, row_number() over (order by START_DATE) RNK from YOUR_TABLE T ) select A.START_DATE START1, A.END_DATE END1, B.START_DATE START2, B.END_DATE END2 from RANKED A join RANKED B on A.RNK + 1 = B.RNK where trunc(A.END_DATE, 'MM') != trunc(B.END_DATE, 'MM') * Uses `row_number()` to create an ordered ranking on START_DATE and sticks it in a CTE called `RANKED` * Joins `RANKED` to itself where the A is the record immediately before B. * Filters to records that aren't in the same month. You can modify the `row_number` to include a `partition by`, and do the same for the join, if you need to do this per-CUSTOMER, per-JOB, per-whatever. 
That's amazing! How much time do you estimate you spent practicing on your own time?
At least an hour a day. I read blog articles every day as well. Still try learn something new every day 
You have to group by the columns that are not aggregated. Try for example this: SELECT c.cus_code, i.inv_number, i.inv_date, p.p_descript, SUM(l.line_units * l.line_price) AS Subtotal FROM dbo.customers AS c JOIN dbo.invoices AS i ON c.cus_code = i.cus_code JOIN dbo.lines AS l ON i.inv_number = l.inv_number JOIN dbo.products AS p ON l.p_code = p.p_code GROUP BY c.cus_code, i.inv_number, i.inv_date, p.p_descript ORDER BY cus_code ASC;
That does not look like MS SQL code. Try asking in /r/MSAccess/.
love your indentation, dampmom now if only we could get you to adopt the **leading comma convention** SELECT c.cus_code , i.inv_number , i.inv_date , p.p_descript , SUM(l.line_units * l.line_price) AS Subtotal FROM dbo.customers AS c JOIN dbo.invoices AS i ON i.cus_code = JOIN dbo.lines AS l ON l.inv_number = i.inv_number JOIN dbo.products AS p ON p.p_code = l.p_code GROUP BY c.cus_code , i.inv_number , i.inv_date , p.p_descript ORDER BY cus_code ASC;
thank you, I don't know why I didn't think of that honestly. I really appreciate it! 
Using lead which accesses the next row: with dates_data_vw as ( select to_date('20170101','yyyymmdd') as start_date , to_date('20170131','yyyymmdd') as end_date from dual union select to_date('20170201','yyyymmdd'), to_date('20170215','yyyymmdd') from dual union select to_date('20170216','yyyymmdd'), to_date('20170331','yyyymmdd') from dual union select to_date('20170401','yyyymmdd'), to_date('20170429','yyyymmdd') from dual union select to_date('20170501','yyyymmdd'), to_date('20171231','yyyymmdd') from dual ) , find_date_gap as ( select lead (start_date) over (order by start_date) as next_value , end_date + 1 as end_date_plus_one from dates_data_vw ) select * from find_date_gap where next_value != end_date_plus_one and next_value is not null ;
Why do you always delete your posts after you get an answer? And are you sure the information you need to solve the assignments is not in the textbook? 
This will give you the average purchase amount per customer. Expand on that if you need more detail. SELECT c.cus_code , SUM(line_units * Line_price) / COUNT(*) AS AveragePurchaseAmount FROM customers AS c JOIN invoices AS i ON c.cus_code = i.cus_code JOIN lines AS l ON i.inv_number = l.inv_number GROUP BY c.cus_code HAVING COUNT(l.line_number) &gt;= 1 ORDER BY cus_code ASC;
To give you an idea: SELECT q1.*, q2.answer FROM Question_Answer q1 INNER JOIN Question_Answer q2 ON q1.questionaireID = q2.QuestionaireID and Q2.questionID = 1 This should work but it's likely inefficient as that is a query that uses your existing view as a subquery, joined to itself- so if you wanted to make my statement into a view, it would have a view within a view, probably completely unnecessarily. You should provide the SQL of your current view, it's probably smarter to alter your existing view rather than build another view on top of it.
You can go to object explorer, which is usually on the left side of ssms. Expand Databases, expand your database, expand Tables, find your table, right click on your table, drill down on Script Tables as..., and click new query window. Repeat for each table.
Wow ! Your code actually worked ! Ok then. I will try to paste my SQL for the view in here. It may take a bit of time, so hang in there. Thank you :D Edit: Ok, here goes. Lets hope the formatting is correct. Edit 2 : I changed the german words so that its easier to understand. *The XMLCONTENT is a template stored as a text file in the DB wich contains the wording of the questions, wich i had to extract with the CASE statement. *QUESTIONID is the ID of the question. *QUESTIONTYPE is the type of the question (In this case we have MEMO, SCALESINGLELINE and SCALEMULTILINE) *QUESTIONAIREGUID is the ID for the questionaire. *ANSWER is the answer. *QUESTION_TEXT is the question asked. *KEYWORD is the name of the template used for the questionaire ( the questions and formating of the questionaire displayed once published online ) *INSERTTIMESTAMP is the date and time of the completion of the questionaire. *QUESTION_1 trough _3 are just the XMLCONTENT containing the template of the questionaire, split up in 3 parts as to not exceed the maximum of 65.535 characters. Hope this helps. SELECT SUBSTRING(dbo.GWQMASTERCOPY.XMLCONTENT, 1, 65000) AS QUESTION_1, SUBSTRING(dbo.GWQMASTERCOPY.XMLCONTENT, 6400, 65000) AS QUESTION_2, SUBSTRING(dbo.GWQMASTERCOPY.XMLCONTENT, 128000, 65000) AS QUESTION_3, dbo.GWQMASTERCOPY.KEYWORD, dbo.GWQUESTIONAIRE0.INSERTTIMESTAMP ,dbo.ITDANSWERS.QUESTIONTYPE, dbo.ITDANSWERS.ANSWER, dbo.ITDANSWERS.QUESTIONID, dbo.ITDANSWERS.QUESTIONAIREGUID, CASE WHEN dbo.ITDANSWERS.QUESTIONTYPE = 'MEMO' THEN LEFT ( REPLACE ( REPLACE ( REPLACE ( SUBSTRING(SUBSTRING(dbo.GWQMASTERCOPY.XMLCONTENT ,CHARINDEX('&lt;Text&gt;&lt;![CDATA[', dbo.GWQMASTERCOPY.XMLCONTENT,CHARINDEX('FieldType="MEMO" ID="' + dbo.ITDANSWERS.QUESTIONID, dbo.GWQMASTERCOPY.XMLCONTENT,1))+15 ,1000 ),1 , CHARINDEX(']]&gt;&lt;/Text&gt;',SUBSTRING(dbo.GWQMASTERCOPY.XMLCONTENT ,CHARINDEX('&lt;Text&gt;&lt;![CDATA[', dbo.GWQMASTERCOPY.XMLCONTENT,CHARINDEX('FieldType="MEMO" ID="' + dbo.ITDANSWERS.QUESTIONID, dbo.GWQMASTERCOPY.XMLCONTENT,1))+15 ,1000 ),1)) , '&amp;#252;' , 'ü' ) , '&amp;#246;' , 'ö' ) , '&amp;#228;' , 'ä' ) , LEN ( REPLACE ( REPLACE ( REPLACE ( SUBSTRING(SUBSTRING(dbo.GWQMASTERCOPY.XMLCONTENT ,CHARINDEX('&lt;Text&gt;&lt;![CDATA[', dbo.GWQMASTERCOPY.XMLCONTENT,CHARINDEX('FieldType="MEMO" ID="' + dbo.ITDANSWERS.QUESTIONID, dbo.GWQMASTERCOPY.XMLCONTENT,1))+15 ,1000 ),1 , CHARINDEX(']]&gt;&lt;/Text&gt;',SUBSTRING(dbo.GWQMASTERCOPY.XMLCONTENT ,CHARINDEX('&lt;Text&gt;&lt;![CDATA[', dbo.GWQMASTERCOPY.XMLCONTENT,CHARINDEX('FieldType="MEMO" ID="' + dbo.ITDANSWERS.QUESTIONID, dbo.GWQMASTERCOPY.XMLCONTENT,1))+15 ,1000 ),1)) , '&amp;#252;' , 'ü' ) , '&amp;#246;' , 'ö' ) , '&amp;#228;' , 'ä' ) ) -1 ) WHEN dbo.ITDANSWERS.QUESTIONTYPE = 'SCALESINGLELINE' THEN LEFT ( REPLACE ( REPLACE ( REPLACE ( SUBSTRING(SUBSTRING(dbo.GWQMASTERCOPY.XMLCONTENT,CHARINDEX('&lt;Text&gt;&lt;![CDATA[',dbo.GWQMASTERCOPY.XMLCONTENT,CHARINDEX('FieldType="SCALESINGLELINE" ID="',dbo.GWQMASTERCOPY.XMLCONTENT,1))+15,200),1,CHARINDEX(']]&gt;&lt;/Text&gt;',SUBSTRING(dbo.GWQMASTERCOPY.XMLCONTENT,CHARINDEX('&lt;Text&gt;&lt;![CDATA[',dbo.GWQMASTERCOPY.XMLCONTENT,CHARINDEX('FieldType="SCALESINGLELINE" ID="',dbo.GWQMASTERCOPY.XMLCONTENT,1))+15,200),1)) , '&amp;#252;' , 'ü' ) , '&amp;#246;' , 'ö' ) , '&amp;#228;' , 'ä' ) , LEN ( REPLACE ( REPLACE ( REPLACE ( SUBSTRING(SUBSTRING(dbo.GWQMASTERCOPY.XMLCONTENT,CHARINDEX('&lt;Text&gt;&lt;![CDATA[',dbo.GWQMASTERCOPY.XMLCONTENT,CHARINDEX('FieldType="SCALESINGLELINE" ID="',dbo.GWQMASTERCOPY.XMLCONTENT,1))+15,200),1,CHARINDEX(']]&gt;&lt;/Text&gt;',SUBSTRING(dbo.GWQMASTERCOPY.XMLCONTENT,CHARINDEX('&lt;Text&gt;&lt;![CDATA[',dbo.GWQMASTERCOPY.XMLCONTENT,CHARINDEX('FieldType="SCALESINGLELINE" ID="',dbo.GWQMASTERCOPY.XMLCONTENT,1))+15,200),1)) , '&amp;#252;' , 'ü' ) , '&amp;#246;' , 'ö' ) , '&amp;#228;' , 'ä' ) ) -1 ) WHEN dbo.ITDANSWERS.QUESTIONTYPE = 'SCALEMULTILINE' THEN LEFT ( REPLACE ( REPLACE ( REPLACE ( SUBSTRING(SUBSTRING(dbo.GWQMASTERCOPY.XMLCONTENT,CHARINDEX('ID$'+ dbo.ITDANSWERS.QUESTIONID,dbo.GWQMASTERCOPY.XMLCONTENT,1)+63,500),1,CHARINDEX('&lt;/Text&gt;',SUBSTRING(dbo.GWQMASTERCOPY.XMLCONTENT,CHARINDEX('ID$'+ dbo.ITDANSWERS.QUESTIONID,dbo.GWQMASTERCOPY.XMLCONTENT,1)+63,500),1)) , '&amp;#252;' , 'ü' ) , '&amp;#246;' , 'ö' ) , '&amp;#228;' , 'ä' ) , LEN ( REPLACE ( REPLACE ( REPLACE ( SUBSTRING(SUBSTRING(dbo.GWQMASTERCOPY.XMLCONTENT,CHARINDEX('ID$'+ dbo.ITDANSWERS.QUESTIONID,dbo.GWQMASTERCOPY.XMLCONTENT,1)+63,500),1,CHARINDEX('&lt;/Text&gt;',SUBSTRING(dbo.GWQMASTERCOPY.XMLCONTENT,CHARINDEX('ID$'+ dbo.ITDANSWERS.QUESTIONID,dbo.GWQMASTERCOPY.XMLCONTENT,1)+63,500),1)) , '&amp;#252;' , 'ü' ) , '&amp;#246;' , 'ö' ) , '&amp;#228;' , 'ä' ) ) -1 ) END AS QUESTION_TEXT FROM dbo.GWQMASTERCOPY INNER JOIN dbo.ITDANSWERS ON dbo.GWQMASTERCOPY.GGUID = dbo.ITDANSWERS.QUESTIONAIREMASTERCOPYGUID INNER JOIN dbo.GWQUESTIONAIRE0 ON dbo.ITDANSWERS.QUESTIONAIREGUID = dbo.GWQUESTIONAIRE0.GGUID WHERE KEYWORD = 'MAU (OT) Fragebongenvorlage / Mastercopy 1'
Thanks for the detailed reply! I wish our company did any sort of trainings like that... I probably would've reconsidered leaving. Funny, that's also I got into my current job. I switched departments and learned from my peers. I maybe read one SQL "book" and did a training but that was it. The rest was on the job learning. I think my strategy is to just try and find a company/industry that I find interesting and try and find any SQL job and see if I can get my foot in the door to learn all I can. Thanks again!
I did something similar a few years ago and wrote it up, here: https://icculus.org/~chunky/stuff/sqlite3_example/sqlite_to_pgsql.html It was much easier than I had expected EDIT: I think you may have galvanised me into doing this again, with a much larger and more complex project I have. Thanks :-)
Why? What is the trade-off?
Thank you for your reply, I think I understand what you are getting at, let me play around with it and see what I come up with! 
Here you go... http://blog.sqlizer.io/posts/sql-43/ Here is an article that talks about how SQL is 43 years old but it is STILL one of the top databases for a number of reasons. It's goes into the ease of use and it's strong user communities as some of the reasons for its popularity. It goes into how it is one of the most common database languages, one of the most common programming languages, and one of the least dreaded programming languages. There was even a little part where it stated "SQL - so easy, marketers can learn it." It also shows that most of the jobs and money back this DB type with some differing flavors. But right smack dab in the middle of all that is a perfect example of the query you need to write with data to match. The only thing I can help you further is with how to map out your queries from information given. The first thing you need to do is chart out a couple of things; 1) each information field the query is calling 2) the table where those fields live 3) the order of operations 4) limiters of the information 5) modifiers to the presentation of info I say this because it is easy to get lost in that order of operations part until you are a little bit more well versed in either SQL or the data you are digging in. In the first query directive, there was the term "or" listed but there term "and" was coded by you in your answer. 
If compression is not enabled, I think you will get the slack space included in the backup file. Easy enough to test, but I'm short on time this afternoon.
Thanks for you help! I am almost done with them. 
You can STUFF the distinct Year values in to a comma separated list, then use dynamic SQL to generate the pivot. See below: http://stackoverflow.com/questions/14797691/dynamic-pivot-columns-in-sql-server
The output would be the same but the execution plan would probably be different. If you used union it would attempt a dedupe unecessarily I think.
use MAX... and you don't need DISTINCT SELECT menu_item.menuitemposref AS 'item number' , menu_item.menuitemname1master AS 'item name' , menu_item.majorgroupnamemaster AS 'major group' , menu_item.familygroupnamemaster AS 'family group' , MAX(menu_item_price.price) AS 'price' FROM location_activity_db.dbo.menu_item INNER JOIN location_activity_db.dbo.menu_item_price ON location_activity_db.dbo.menu_item_price.menuitemid = location_activity_db.dbo.menu_item.menuitemid INNER JOIN location_activity_db.dbo.guest_check_line_item_hist ON location_activity_db.dbo.guest_check_line_item_hist.postransref = location_activity_db.dbo.menu_item.menuitemposref AND location_activity_db.dbo.guest_check_line_item_hist.businessdate between '3/01/2017' and '4/30/2017' WHERE location_activity_db.dbo.menu_item.majorgroupnamemaster = 'beverages' GROUP BY menu_item.menuitemposref , menu_item.menuitemname1master , menu_item.majorgroupnamemaster , menu_item.familygroupnamemaster ORDER BY 'item number' , 'major group' , 'family group'
Create a view and connect to the view from Excel. Then pivot tables or powerpivot. Its way better at dynamic tables.
Pretty much what we doing, although can be slow.
i thought it would be annoying for other users to see a bunch of similar posts in a row. and referring to your other question, we only have lecture slides on powerpoint, and the schedule for my class has been kind of weird since the midterm, few classes cancelled for various reasons so this last assignment has been more difficult without the help of learning straight from the professor. also, our professor encouraged us to actually reach out to forums for help on our final project/hw assignment which i thought was pretty neat. it sort of makes sense as in the real working world you have resources like reddit for help whenever you're stuck on something! 
I'm glad! Keep it up. It will pay off. 
What I found solve the problem was that the attributes (clientid &amp; staffid) had to be defined within the appointment table before they became foreign keys. I basically tried a few things before (the various commented lines) and after a bit of mix and matching and help from a friend it was fixed 
Pretty much anything supporting trading, as far as SQL nothing much beyond joins, inserts, deletes, but we had developers for the more complex stuff, example creating reports, issues with data, that is probably more along the lines of what you would be doing? So I would think you could definitely find work in finance development / support. Edit: One thing you could do is google the top 20 asset managers, and check individual websites for development positions, I used to work for SSgA.
Is this also how SELECT DISTINCT would work?
It's likely people just can't be bothered to update all their joins. E.g. I'm reading Chris Date's _SQL and Relational Theory_ right now, it's full of the old syntax. So far he's used `NATURAL LEFT OUTER JOIN` once to advocate against outer joins.
Well, strictly speaking, SQL is a rather poor implementation of a relational language. In fact, it's not actually a relational language. But the broad cross-platform and cross-vendor support it enjoys is unbeatable, so we all learn to hack it and make it behave as much as possible 😊
https://www.codewars.com/?language=sql
yes
So Java is just a really good and really widespread general purpose programming language. It does many many things, one of them is interfacing with a database through like you said JDBC. To make Java and a database work together, you don't need much. You need a database somewhere which you know the URL of (this can just be localhost on your own computer btw) and you have permissions to access. From there you create a Java program which imports some library package like JDBC. This package contains other Java classes for you to use to connect to the database. You plug in the URL, some login credentials, some other setup data into a connection object class and voila you can connect. From here you can create Strings which are valid SQL statements and pass them into other statement objects to execute the SQL. If the SQL was a query you can return the result set of the query which you can then iterate through and retrieve the values by column into Java primitives or objects. If it was an update statement you might just get back success/failure or the number of records updated. If it fails an exception will be thrown that you can then catch and inspect.
The general philosophy is put derived columns in views.
You can use a COMPUTED COLUMN in SQL to make such columns. There is a property of a computed column, which is PERSISTED. A persisted column will store the data in the database, whereas a non-persisted column will be calculated at run time. You may index a persisted column, you cannot index a non-persisted column, so if in your example you were querying where bitrate = @bitrate, then a persisted column would make sense.
Aha, I haven't heard of this yet, thanks. My application uses a lot of multi-level orderby's, which I can achieve here with a join: CREATE VIEW video_metadata AS SELECT id, width * height AS area FROM videos; SELECT * FROM videos INNER JOIN video_metadata ON video_metadata.id = videos.id ORDER BY videos.width DESC, video_metadata.area DESC; Does this seem like a good idea? I have a feeling that just keeping these metadata as regular columns will be faster as long as I'm willing to waste the space. I'll keep looking into this and see if I can improve my design.
Looks like SQLite doesn't support computed columns, unfortunately: http://stackoverflow.com/questions/1124695/can-i-create-computed-columns-in-sqlite sqlite&gt; create table vids (width INT, height INT, area AS width * height); Error: near "AS": syntax error That sucks, I think that would have been perfect. Thanks though.
Ahhh gutted. Sorry I didn't see that it was SQLite.
One thing you can do is put all the `videos` columns in `video_metadata` too and avoid the join later. Basically make `video_metadata` a 'super-view' that gives you all the video data.
I've tried that. 
Maybe this will help http://stackoverflow.com/questions/10517046/sharepoint-lists-linked-to-sql-server
Look into external content types. Back in the day implemented Sharepoint lists as a crude system for CRUD operations against SQL server tables.
Like at the SQL prompt? I ran... select invent.idno, itemname, price, onhand, transaccode, transaction FROM invent, transac WHERE invent.idno = transac.idno; (all the variables I'm using) but nothing came out. I also tried running it without the WHERE, same output. No idea what that means since I have data in my inventory table... but have nothing in my transaction table, but that shouldn't matter should it? I'm only trying to insert into them. 
It's probably sending the windows user credentials to the database for permissions. Look at the sql server and give yourself the same permissions he has and you should be golden. 
The excel document has something called a "data source connection". The data source is configured to use a specified login/password combination which likely belonged to the previous user. You simply need to reconfigure the data source to use your credentials. This can be done through excel and does not need to be handled via the SQL Server. 
I bet you don't have access to the DB that excel is trying to access. See what DB you're trying to connect to and chat with your DBA, or whoever grants access.
**Here's a sneak peek of [/r/excel](https://np.reddit.com/r/excel) using the [top posts](https://np.reddit.com/r/excel/top/?sort=top&amp;t=year) of the year!** \#1: [This seems like the one subreddit where the harder your problem is, the more likely people will try to help](https://np.reddit.com/r/excel/comments/5nk0nw/this_seems_like_the_one_subreddit_where_the/) \#2: [I wrote Excel tutorials full of gif, and I'm looking for feedback!](https://np.reddit.com/r/excel/comments/5dgx0p/i_wrote_excel_tutorials_full_of_gif_and_im/) \#3: [What Excel best practice do you personally recommend?](https://np.reddit.com/r/excel/comments/5q2opq/what_excel_best_practice_do_you_personally/) ---- ^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| [^^Contact ^^me](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| [^^Info](https://np.reddit.com/r/sneakpeekbot/) ^^| [^^Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/5lveo6/blacklist/)
First, doing inserts in a loop is wrong in 99.9% of the time. So let's just forget about that option and find a better one. How many rows are you trying to insert? Are there already important data in one of the tables? Right now you're doing INSERT to two tables and then UPDATE to them as well. You likely don't need the UPDATEs at all. It is possible to insert millions of rows with one single INSERT (without any loop) if you base the INSERT on the result of a SELECT. But I don't understand your use case enough to give more pointers. People are more willing to help if you also post the table structure as executable CREATE statements. 
The thing is it's my assignment problem. I have to use a cursor here to insert. Unless I don't have to loop to use a cursor?
I tried this and sure enough it threw an error. I logged in with his old account and then I realized that initially I connected to and looked at the wrong server name/database. I chose the correct one and matched the permissions and it is all good! Thanks for the help!
I stand by my previous statement that insert in a loop is wrong in 99.9% of the time. That said, perhaps your teacher is having some pedagogical reason to require it here. Try to break the problem down. Do you understand why it doesn't insert anything when the SELECT that the cursor is based on return no rows? It's difficult to give advice when we don't know the problem statement and given table structure. On the other hand, if you write it down here your teacher will be able to google it. 
Windows authentication used their Windows login. Your Windows login obviously does not meet their Windows login. You will need to contact the admin of the database and tell them you need connection to it and explain the situation. Try to see if they can simply "Grant the same access that the previous user had." Basically they need to add your user name as an approved connection.
Ah OK. I misunderstood the issue. So on SQL Server, the user needs to have access to the specific databases hosting the tables which the query is pulling. You can do this under security-&gt;logins-&gt;your windows ad-&gt;user mappings. Be sure to enable db_datareader
Consider doing this in a two step approach using dynamic SQL, using one query to create the list of websites to be used as a fields in a pivot. Here's something similar-ish I've done before with MS SQL to transform a list of ids (responserow), questions and their response into a table with the questions as fields: DECLARE @survey NVARCHAR(50) DECLARE @columns NVARCHAR(max) DECLARE @query as NVARCHAR(max) set @survey = 'This is a filter clause that probably isn't relevant to what you're doing' SELECT @columns = COALESCE(@columns + ',[' + cast(Question as nvarchar(128)) + ']' ,'[' + cast(Question as nvarchar(128))+ ']') FROM QuestionBank qb WHERE survey = @survey set @query = 'SELECT survey, responserow,' +@columns+'FROM ( SELECT r.survey, responserow, question, response FROM responses r inner join questionbank qb on qb.questionid = r.questionid WHERE r.survey ='''+@survey+''') x PIVOT ( max(response) FOR question in ('+@columns+') ) as pvt order by responserow' execute(@query) It'll be rather different in PSQL I'd imagine (don't know whether you can concatenate a field recursively like that) but that might work as an approach.
They're useful for date range joins. Say you had a table with price history of an item, with a start and finish datetime that price applied. you could then get the price at the time of a sale by something like: FROM sales s INNER JOIN PriceHistory ph on s.ItemID = ph.ItemID and s.saledate &lt; ph.PriceExpired and s.saledate &gt; ph.PriceStarted I find it's easier to conceptualise these when you bear in mind that a join clause is essentially just a WHERE condition in a different location.
Thanks, but I can't get this to work in the postgresql. I'm still stumped but I appreciate all that time that you took!
Thank you. I guess I'm not understanding why it isn't essentially doing a semi-cross join for the rows that match the operators. Your example makes more sense than the way I was using it though. 
Put both of them `into #table` and compare the results to see if they are equal. I would do this even if someone told me they were. If they are, then they are, and you don't need someone to tell you what you already know. :)
Ok, now I'm trying to review this piece of code without a lazy habit "I see no x, then do x". And from the very begining I see something I don't really understand, so please confirm if I'm guessing this right. You're trying to use cursor to actually insert **new** product and it's transaction information to tables - the one that aren't on the tables yet, right? If yes, then how do you want to actually do that if there's no *invent.idno* matching the prompted value? Also when this id exist, then fetching overwrites values you've prompted at the very begining and you'll only multiplying the same records. Also the query in cursor is producing a cartesian because there are no condition to join two tables. Please put DMLs of both tables to look if there are any foreign keys. You definitely overlooked this: v_itemname invent.itemname%TYPE := '&amp;v_itemname'; SET invent.idno = v_idno, itemname = 'v_itemname', This will literally put the text you've set between apostrophes and not the actual value (this answers why the update query is not working). Try to use more variables to get fresh values from tables and do some "debugging" with *dbms_output.put_line()* function before and after each step (you need to use *set serveroutput on* to actually view output).
That left join inside the inner join (right in the middle) should certainly be an inner join IMO. This is how it is in WooCommerce, and I did not want to change the SQL sub-select queries without understanding more exactly what they do. The where-clause on that sub-query forces it to act as an inner join anyway. The first left join cannot be an inner join, because it is specifically looking for non-joins, i.e. I'm looking for all wp_posts that do NOT join this sub-select, and so an inner join would not work - that would give me the opposite. Using exists and no exists is an interesting one. I'll give that a quick try and see how it fairs for performance. I had just assumed it would be running those sub-queries many, many times, just like IN, except that it does not need to actually return any data - it just needs to know that there is *at least some* data. 
and I apologize if my questions is as clear as mud - this is one of the first queries I've ever written. 
&gt; I think what has happened is the group by is causing the multiple lines to be added together giving me much higher numbers than it should. without digging into it, i would say it's probably your joins that are bringing back multiple rows FROM hbm_client INNER JOIN blt_billp ON blt_billp.payr_client_uno = hbm_client.client_uno INNER JOIN tbh_client_summ ON tbh_client_summ.client_uno = hbm_client.client_uno INNER JOIN blt_bill ON blt_bill.tran_uno = blt_billp.bill_tran_uno INNER JOIN hbm_matter ON hbm_matter.client_uno = hbm_client.client_uno WHERE blt_billp.ar_status='o' 
Just tried `EXISTS` and it took the same amount of time, maybe a few seconds less, but *minutes* to execute. Worth a try though, thanks for the suggestion :-)
Ok, I'll start there. I did just comment out one of the joins and it did lower my totals. Looks like I need to get back to the drawing board on this. Thanks for the help
Essentially what I'm trying to do is pull an Accounts Receivable report by client number. I'm trying to see how much money each client still owes our company and the agings. The parent table is HBM_CLIENT and I want to join it with TBH_CLIENT_SUMM on Client_uno. From there, I need to join TBH_CLIENT_SUMM to another table BLT_BILLP on PAYR_CLIENT_UNO. Does this make sense?
I'd run it against the entire table and look at all possible cases, not just specific test cases.
Personally, I'd suggest a date table to join on.
I would just left join to the date table and implement an isnull or coalesce or case when statement when your date range is null then return the other date. You can order by the date value afterwards.
This is what I imagined. The aggregations wouldn't return NULL, they would return 0. You can do a case when 0 to change to NULL if you like. SELECT 'ONB' AS SLA , Client , ISNULL(DATEADD(m, DATEDIFF(m, 0, ORIG_ACTV_DT), 0),DT.DateStamp) AS DateRange , Country , Region , [Labor_Type2] , SUM(Compliance) AS IDCompliant , COUNT(DISTINCT ORD_ID) AS IDCount , SUM(Compliance) * 1.0 / COUNT(DISTINCT ORD_ID) AS ComplianceRate FROM #NULL LEFT JOIN DateTable DT ON ORIG_ACTV_DT = DT.DateStamp WHERE Client = '' GROUP BY Client , ISNULL(DATEADD(m, DATEDIFF(m, 0, ORIG_ACTV_DT), 0),DT.DateStamp) , Country , Region , [Labor_Type2] ORDER BY ISNULL(DATEADD(m, DATEDIFF(m, 0, ORIG_ACTV_DT), 0),DT.DateStamp) ASC , Country , Region , [Labor_Type2]
Hmmm.. I'll try to run it. Currently have a sub-process built which pings date and the parent table to find all distinct values I'm looking for then full outer joins them. Thanks.
But wait, how is this going to work for things which don't exist in #NULL? They'll join fine, but there won't be any nulls in #null.
ok thanks. Yeah I'm still trying to wrap my brain around it. 
Are they actually nested or are they sequential with an unnecessary amount of brackets? I can't work it out.
Did you remove the grouping and the sums?
Hahaha! Well, you're not wrong. :)
I just did. I think that may have been the ticket. Just need to do some validation on this, but that may have been it. A combo of SELECT DISTINCT and removing the SUM and GROUP BY Thanks
I think you're the third person (and this about the fifth question) relating to movie/actor/director databases. I wonder if you're all at the same university. To be fair though this is fairly tricky compared to the others. I don't want to give you the answer but here's the approach I would take: Create a subquery that outputs distinct directors with &gt; 25000 ratings. Inner join that subquery to movies. Inner join the actor table to movies. From that lot of data, group by actor, director, and count the distinct movieIDs. Apply order by clause.
this was my first thought too. but dont you need a subselect for the 25000 num rating? (and director in... sry am on mobile device) you also want to count the movies &lt; 25000 rating as long as the director has had a movie that made him famous. im not a mysql expert tho
If the market is closed, won't the `pc` value be zero? I.e., market closure means no price change, therefore price change is zero. So just filter out all the rows where `pc = 0`. This has the nice property that it also works if there are changes later on in the file.
One thing - I meant staging table rather than temp. If you make a temp table (#) in step 1 then it won't exist for step 2. Poor choice of words on my part. 
I'm not a DBA, jut a guy that uses writes some SQL, so you may find better advice from someone else, but if you're just talking about fundamentals then... A course based on T-SQL is the way to go. Sybase and MS SQL are very similar. Oracle and Postgres are different from the later. So if you can find a class / course on Sybase, go to it, if you find MS SQL, it's good too (if we're talking about query fundamentals). It's not exactly the same, but it's close. This link may help: http://stackoverflow.com/questions/1043265/what-is-the-difference-between-sql-pl-sql-and-t-sql
Sorry to be pedantic; but isn't this a derived table? It's in the from clause whereas a subquery is in an in clause and selects only one column. Or at least that is my understanding. 
~~No, a [subquery](https://technet.microsoft.com/en-us/library/ms189575(v=sql.105\).aspx) is any query inside another query.~~ This is the second wrongest I've ever been in my life.
Yeah, that's what I was thinking... It's hard to find a Sybase course.. but MS SQL might be the next best thing - That being said... what would the recommendations be for this in the Washington DC Metro/Baltimore area?
Thanks. I've read this through many times at this point, done a bit of reading. 1. If I understand correctly, I should consider this a prep query. And after it runs, I would just separately import whatever is present in the staging table, if anything, into the production table using bulk insert or similar, right? 2. In the data we're pretending to import, I would need to reference the 3 most recent rows from the production table and then reference the current CSV values somehow in the 4th row, right? 3. I think that's it. The end of your example is the formal end of the insert command, right (and the query would end/exit)? I just hope I haven't embarrassed myself too badly asking stupid questions. And if it's too much trouble to explain basics, I promise I won't take it personally. ty again &amp; cheers 
There's probably a format applied to it, the date format is "medium date"
Not sure... is `NUMBER` a data type? Try `DECIMAL` ?
Tried decimal, it highlights the whole word. 
That awkward moment when you learn you've been misusing terminology for the best part of a decade.... If you'll excuse me I've got several years of shame to catch up on.
Everything about Access is fucky and dumb and yuck. I've spent a lot of time and energy on trying to repress my memories of my Access days so I'm not 100% sure of the exact problem, but I do seem to recall that data types get weird between different versions of Access and when you're using Jet or ODBC (to e.g., connect your Access to a real grown up RDBMS). Try using decimal or double as the data type, and with/without precision settings maybe? If that doesn't work have a look in Options &gt; Object Designers for an option about ANSI 92 compatibility and set it, close and reopen the DB and try the query again. Note that this may or may not fuck up other things, including wildcards (ANSI uses % instead of *) Better yet just throw the whole thing away and never use Access ever again.
Im not sure about what means and the correct the way of going about that part
I think the error is because you are introducing column names without identifying the data type, length etc (not to mention duplicating column names which is not allowed) I can't figure out what you are trying to do on line 11. I am no guru, and have only worked in MS SQL, but that whole line makes no sense. At the top, you are creating a table and laying out what each column is called, what type of data is allowed, the number of characters and whether the data can be null or not (if it is required). If line 8 (Phone_number) is the last column, then you need to close out the parentheses (opened on line 1) and get rid of the last comma. If Phone_number is NOT the last column, and it looks like you might be trying to add more as the terms 'References' and 'Owner' only appear in line 11, then why the sudden switch in format?
So here answer A is correct? Edit: And the left side is the "many" side?
Yes. 
That's the only thing I can think of. I fucking hated questions like this in college, ugh. I think I might have PTSD just from reading this post lol.
I'm a mssql dev, and I know trying to set the size of an int in mssql will throw an error. Not sure if that is allowed in mysql.
I might add that ROW_NUMBER () OVER (PARTITION BY customer ORDER BY date **DESC**) will give you the latest date, whereas **ASC** will give you the earliest date. 
I think you have an extra ')' at the end of your sql. &gt; CREATE TABLE individual( &gt; username VARCHAR(50) NOT NULL, &gt; last_name VARCHAR(50) NOT NULL, &gt; given_names VARCHAR(100) NOT NULL, &gt; registred_period_start VARCHAR(10) NOT NULL, &gt; registred_period_end VARCHAR(10) NOT NULL, &gt; postal_address VARCHAR(20), &gt; phone_number VARCHAR(15) -- we use like this here idk if it's the best choice for you &gt; ); 
Probably will have to edit the final select in order to get the final results you want but in a nut shell. ;WITH CI AS ( SELECT EmployeeID, EventDateTime CIDateTime FROM TABLE WHERE eventype = 'CI' ) , CO AS ( SELECT EmployeeID, EventDateTime CODateTime FROM TABLE WHERE eventype = 'CO' ) SELECT EmployeeID, CIDateTime, CoDateTime FROM CI JOIN CO ON CI.EmployeeID = CO.EmployeeID *EDIT* --IF these are clock in out times and you'd want to correspond better clockout times to clockin I would do something like and CoDateTime &gt; CIDateTime and CoDateTime &lt;= DATEADD(hour, 18, CIDateTime) -- Assuming you don't have anyone who works more than an 18hour shift, might also want to make it a left join if you have clock ins with no clock outs.
I edited my original query with which should resolve this issue.
Another good solution would be to use an outer apply to get the closest clockout record available, but that only works if you are guarenteed to have a clockout for EVERY clock in, it would look something like this: ;WITH CI AS ( SELECT EmployeeID, EventDateTime CIDateTime FROM TABLE WHERE eventype = 'CO' ) , CO AS ( SELECT EmployeeID, EventDateTime CODateTime FROM TABLE WHERE eventype = 'CI' ) SELECT EmployeeID, CIDateTime, CoDateTime FROM CI OUTER APPLY ( SELECT TOP 1 CODateTime FROM co WHERE CO.EmployeeID = CI.EmployeeID AND CoDateTime &gt; CIDateTime ORDER BY CoDatetime) c
I should also point out that if this is for learning or purely reporting purposes, no problem. But if real employees are getting paid real money based on your code, I would argue that you should consider how to defend against things like missing clock in/outs, take into consideration what happens when a legit clock in/out happens across dates (clock in at 11pm, out at 3am) because some methods you see don't handle all these cases well and most will make one possibly flawed assumption or another. :)
Thanks. This eventually will be to export data from one payroll system into another to run a parallel test payroll. Real people will not be getting paid (or not paid) real money as a result of my novice TSQL skills!
okay same syntax in oracle, only problem is it still returns all instances of date.
"Owner" is a reserved word in MySQL. Pick a different name for your column.
I think owner isn't a reserved word for MySQL. 
Actually you're right. It is a keyword, but the keyword is not marked (R) for reserved. https://dev.mysql.com/doc/refman/5.5/en/keywords.html It still looks like the error message is choking on the term "owner" however. Maybe the tool is producing the error?
Ya can be. but the main see the main error? "Cannot Add Foreign Key constraint". That's why i recommended to remove the quotes in constraint name and see if everything gets resolved in my reply to OP.
Me neither, but I figured this would be where I'd start and then fix it as I figure out where it's going wrong.
Use distinct. 
If you want to learn more, google crow's foot notation. Here's [an example](http://www.vertabelo.com/blog/technical-articles/crow-s-foot-notation).
1. use `snake_case` for all identifiers (for compatibility, and easy typing) 1. don't use keywords (like owner) for identifiers. Protip: most keywords are singular, so use plural versions if they make sense and you should be okay, else try `owner_id` 
You don't need the quotations around the name of your constraint. I'm on mobile so the syntax won't look right but Constraint nameofconstraint foreignkey (yourforeignkey) references tablename(foreignkey names). 
Are you sure that latitude and longitude are of type varchar? If they are Int in the the Location table, then they will need to be Int in your Reader table also, or else your foreign key statement will fail.
To be precise, `=&gt;` is not exactly a foreign key relationship. Nor is it called a 'crow's foot'. It's called exactly what the exam question itself calls it, a functional dependency. A functional dependency `x =&gt; y` (you can read this as 'x arrow y') means that for every unique value of `x`, there is exactly and only one value of `y`, and if you know `x` you can automatically know `y`. This sounds like a foreign key, but it's not exactly the same because something like a calculated column in a view can also have a functional dependency on some column in a base table, e.g. if you have a formula for discount level to apply based on order total. 
Yeah, I'm guessing it is a type mismatch between the two tables for the lat/long columns.
The comma after the last column definition, before the constraint statement actually does belong there. That's not the error.
How big is 'very big'? What data type is X? Is X indexed in all of these tables? Is X unique in each table?
I'm hesitant to answer this question in the negative, but I don't think there is a lot to be done by you as a query user. [While it is possible to force queries to use indexes in particular ways](https://docs.microsoft.com/en-us/sql/t-sql/queries/hints-transact-sql-table), provided the statistics are up to date on the table the optimiser is almost always going to take the most efficient approach anyway. I would suggest that if you don't actually need to include unmatched rows, you're better off doing inner joins and then letting the optimiser work out the order of applying these indexes - I would expect it to work a little better than a left join as each successive index match would have fewer distinct values.
Sorry i was using reddit on my phone
I think that may be it because the table that is being referenced, i made that table a few days ago and haven't looked at it so ill take a look and then correct that if its wrong 
Alright i didnt know that some words were reserved so i guess ill change it 
Lol its not an exam, its actually a practical but the tutor wasnt being that helpful, so I decided to ask reddit for help.
Our SQL server has 64 GB of RAM and it took 5:56 to run the query as written without the additional tables being joined. I don't want to be flagged for a heavy query but it's almost inescapable with the rows returns originally anyway.
This sounds suspiciously like the table has no PK with a clustered index. Yes?
This is the classic problem solved by transactions. You do the check and reserve all inside a single transaction at the `SERIALIZABLE` isolation level so that multiple users requesting items won't get inconsistent results. One thing that could be improved here though is the table scheme, you might want to have it as: id, name, qty_available So that a query to find the available quantity of an item only has to look up a single row of the table (given a primary key). Of course, I don't actually know if this is totally appropriate for your scenario. It probably is if you can agree that your scenario is pretty much the same as a library and book lending scenario.
 I already give an explanation as to why i used a picture instead of a screen capture, so stop badgering. If you are not going to give advice or help, please dont answer as you are wasting my time.
I understand that you are the moderator but when you get the same comment more than twice it gets annoying and repetitive. Next time ill make sure its a screen capture so its clear for everyone to see. And i will be deleting this post. 
Alright, next time I won't use screen captures. 
This is SQL &gt;Finally, you do realize that your left join is equal to the following query, right? SELECT X from Atable; &gt;Why even do the join? You are doing a left outer join and only selecting columns from the left table. You might as well not even include Btable because it isn't even needed here. X exists in tables A and B. A and B share X values but have different ones too. I'm trying to return only X values in table A if X does not exist in B.
&gt; This is SQL SQL is a language. I assume you mean SQL Server. &gt; X exists in tables A and B. A and B share X values but have different ones too. I'm trying to return only X values in table A if X does not exist in B. That's very different from your original post. You want something like this: SELECT x FROM Atable a WHERE NOT EXISTS (SELECT null FROM Btable b WHERE a.x = b.x) You did not post your table definitions, but the above query will work better if x is indexed, especially in table b.
Do the Miami users have the connections available/installed? It should be located in **C:\Users\theirUsername\Documents\My Data Sources**
Yes but it's empty
A left join returns all the records of the left table (first table listed). When there is no match in the second table, it shows nulls for any columns selected from the right table. Look at the [Venn diagram here](https://www.w3schools.com/sql/sql_join_left.asp) which helps to explain it. What I posted is called a correlated subquery. For every record in table A it looks to see if there is a match in B. If there is a match, it excludes the row from A, otherwise it includes the row from A.
I'm using a left outer join though. Does my query and your query accomplish the same thing? And, if so, is yours more efficient?
Some time ago (right around when "the cloud" &amp; its Linux-powered servers became a thing), Microsoft said SQL Native Client (ODBC) was the way forward. Looking now, oh damn. [OLEDB has been obsoleted](https://docs.microsoft.com/en-us/sql/relational-databases/native-client/sql-server-native-client) and was last available in the drivers for SQL Server 2012. You should switch your provider / connection string over to use the Native Client sooner than later. Functionally, I don't think you'll see much of a difference.
Left join and left outer join are the same thing. The term "outer" is optional. http://stackoverflow.com/questions/406294/left-join-vs-left-outer-join-in-sql-server Our two queries are not the same at all. Your query returns x for all rows from table A. My query returns x from all rows in table A only if there is no match for x in table B.
 SELECT ABS(NULL) + ABS(1) + ABS(1) --Output: NULL So you're going to have to check your numerator for nulls as well: SELECT CAST((Case when d.[1-2] is null then 0 Else ABS(d.[1-2]) end + Case when d.[2-3] is null then 0 else ABS(d.[2-3]) end + Case when d.[3-4] is null then 0 else ABS(d.[3-4]) end) / (Case when d.[1-2] is null then 0 Else 1 end + Case when d.[2-3] is null then 0 else 1 end + Case when d.[3-4] is null then 0 else 1 end) as decimal(10,2)) as 'Var' Edit: Using `COALESCE`: SELECT CAST(( ABS(COALESCE(d.[1-2],0)) + ABS(COALESCE(d.[2-3],0)) + ABS(COALESCE(d.[3-4],0)) ) / ( Case when d.[1-2] is null then 0 Else 1 end + Case when d.[2-3] is null then 0 else 1 end + Case when d.[3-4] is null then 0 else 1 end ) as decimal(10,2)) as 'Var'
I'm relatively new to SQL and joins was definitely a big step , especially as the db I have access to in work is massively normalised so I end up having to join 5 or more tables for a relatively simple query(Just the bloody timezone tables to get correct times use up 3 joins !) Then for me the next step was some grouping . I had a query where I had help and ended up making a sub query to get the result I wanted . Then along came an expert and rewrote it using grouping and it ran about 3 times faster :) I still consider myself very much a beginner though as I'm still not proficient with the above. It would be interesting to know what would be considered the next few suggested steps in my learning.
Thanks. That's just what I was after mysekf.
No? Ops query returns only unmatched records. Note the where clause. 
That works but using COALESCE (d. [1-2], 0) in place of the case whens is more elegant.
You can avoid the join using EXISTS: SELECT A.x from Atable A WHERE NOT EXISTS (SELECT * FROM btable b where A.x = b.x) This *may* give a slight gain in performance doing it this way if a.x or b.x is nullable, in that the query would no longer need to distinguish between b.x being null because the record doesn't exist or because the field is null in a record. This would be a very tiny improvement compared to indexing a.x and b.x though, if they're not already indexed.
I'd recommend reading Chris Date's _SQL and Relational Theory._ He very carefully uses relational theory to explain how to use and understand SQL properly and at a very deep level.
I used to think of myself as an advanced expert, but a few really sharp guys around r/SQL taught me a lot of good things in the past 6 months, and have humbled my self image :) Specifically, you should really know how to use (and abuse) aggregate functions. SUMs of CASE statements, sequencing time-gaps, that kind of stuff. That's where I was for years, but Reddit later taught me how to incorporate those concepts with windowing functions, leading to several "aha!" moments that have really helped me cut down on query complexity &amp; execution time. IMO, another important attribute of a SQL expert is someone who really knows the ins and outs of at least 2 different systems. I have a strong preference for SQL Server and a rather strong dislike of Oracle, but each platform has strengths &amp; weaknesses. There are a few problems in particular that Oracle would smash to bits, that SQL Server doesn't have much of a solution for. Learning at least one other major RDBMS can teach you a lot about *all* of them. SQL Server, Oracle, DB2, Postgres... They all have free editions available for you to learn from. Master recursive CTEs and know how to use them to process hierarchies and how to procedurally generate sequences of data. ("Get me the date of every Saturday from now until the end of next year", "find all of the finished goods that have *this* item in the Bill of Material" ) Finally, play around with weird data types like arrays, XML &amp; JSON. This will be very platform specific, but will force you to think outside of SQL's "rows and columns" mentality.
D'oh... I did think about using `COALESCE` but it errored with a single argument, completely blanked about adding `0` as the second argument... good call.
Ah thanks, that was so helpful! I've got some googling to do. I'm quite advanced in sql at my office but was starting to feel like I'm just a small fish in an even smaller pond and that if I go to an interview and tell everyone how great I am at sql, it will likely backfire. Hence wanting to ask some experts where they'd place skills! It's clear from reading all these replies that while I do have one or two advanced skills, I'm still lacking in some of the intermediate stuff. Off to google! 
Sound great thanks . I'm someone who likes to understand the underlying theory so this will be ideal.
Oh man subqueries really confused me at first, I would make all these temp tables in order to get around doing subqueries until I figured out the logic. And ugh I get you on lots of joins, I use so many comments to make sure I can keep track of what I have joined. I'm working on my biggest query yet with what feels like a thousand joins (it isn't lol). I feel like for me at least, there was a big plateau after learning the basics of joins, case statements, and aggregates. So many of the online tutorials teach that stuff super well and at least at my job, I don't need to do much else. I've been doing while loops for a week or two now and it's definitely been interesting/relieving figuring that out- took my 5800 line query down to 1200! I had to pull five years worth of cohorts so I was doing a lot of copying and pasting code. 
&gt; 1200! 1200! ≈ 6.35 * 10^3175 /r/unexpectedfactorial
Is there a way for me to determine this with Viewer access? EDIT: X, the PK, is non-clustered.
It's a 20TB database, not sure if that matters. How do I check the execution plan?
&gt; fully qualified DNS entry 
I probably should have mentioned this but if I know Btable is bigger than Atable, wouldn't the left join be more efficient than your variation? Also FYI I verified the columns I intend to use are indexed, non-unique, and non-clustered across A, B, C, etc. tables. X is used to join as PK across all tables.
That should work, assuming that `billed_fees` and `paid_fees` are numeric data types and you're spelling `distinct` properly when you attempt to run it. Are either of those fields `null`able? When you say "only about 6 have values", what do the other records have?
Domain
Just bigger? In other words, do you already know that everything in Atable has a matching record in Btable? If *either* side can be unmatched, it probably doesn't matter which table is bigger. e: also is the data actually non-unique? If the data items actually are unique but the field property is non-unique the optimiser is doing a lot more matching than it actually needs to: if it knows both sides are unique, once it finds a match it can stop checking for more matches and move on to the next record. Might be worth asking your DBA whether setting the field property to unique would cause any problems.
Ah I see. Because the where clause is applied after the join, rows are filtered after the left join. If the where clause were part of the join condition then it would have been different (would have been as I described).
Actually it makes some difference. Using A as the driving table, checking against B using "not exists" will be faster than doing it the other way around. It will basically do this: For each row in A do an index lookup by value (stopping at the first one found). If B is much bigger than A then it would have to do more work using B as the driving table.
In SQL server management studio it's Query &gt; Include Actual Execution Plan and then run the query. However if it's just a really really big table there may not be anything in particular about the plan that will be useful.
For future reference, you could have googled that and gotten a much more complete answer in much less time than waiting for me to respond. ;) Google is your friend, use it! That being said, right click on your query and pick Display Estimated Execution Plan. You'll probably want to google how to read that and what everything means, but the primary thing is that at the top of the query plan window it may say something about a missing index. If it does, try creating the index it suggests. This is assuming you don't have a more experienced SQL dev or DBA in your company that is in charge of this stuff (because you'd have been asking them, not us, right?). If you get nothing there, you can also right click over the query and choose Display Actual Execution Plan and then run the query. When it's finished running you'll get a similar window but this one will have more concrete details. Again you should be looking for a missing index. But I cannot stress enough that your left outer join where you remove lines that had no values is worse than doing an inner join. The DB optimizer is very good and you should almost never have to think about goofy workarounds like that. It's not a programming language, it's a query language, which means you don't tell it HOW to do what you want, you tell it WHAT you want, and it decides how to best get that data. But also I saw someone suggested using an EXISTS clause, and that may be even faster than a join if you don't actually need any of the data from table B. In all cases you can use the execution plans to see if you're missing an index.
I would consider a working understanding of joins to be basic: if you don't know joins you can't really say you know SQL.
That's correct, but OP was asking compared to a left join method: I think using exists will beat left joining on non-unique fields no matter which side you use as base.
Thanks, this is awesome! And totally revealing of holes in my knowledge. Time to learn! 
This is gold
What happens when you run this query now? Since you have a `where` clause, it should be giving you at least a subset of all rows?
As a self taught developer, I know the feeling. 
You need a subquery that is like: (Select custid, count(*) from (select distinct custody, order from ordertable)) Edit: this answer is low effort, but do work on subqueries...that is the only way in many cases to get what you need in one query. You could create tables that act as subqueries-- and it helps to honk if subqueries in this manner, as tables. Then you can just join those tables/subqueries up after you create them. 
What is NULL-100? One will never know.
Correct
There is a write up I had bookmarked from a few years back, which uses the DamerauLevenshtein​ distance within UDF's. I've never tried it but it might be worth a read [here](http://www.sqlservercentral.com/articles/Fuzzy+Match/92822/)
You added table t without using a join condition which creates a Cartesian product between your results and table t. Add the following to your where clause: and vl.type_id = t.id
No problem. This error becomes more apparent if you consistently use this syntax which keeps join conditions out of the where clause: select vl.id as id, vl.vline_id, l.id as lineid, l.name as name, vl.position, t.id as typeid, t.part_number as partnumber from vline_lines as vl inner join `lines` as l on vl.line_id = l.id inner join types as t on vl.type_id = t.id where vl.vline_id = 5 group by l.id order by position asc edit: sorry for the crappy formatting. Too sleepy to use pastebin
Oh. I was suuuper wrong trying to create a INNER JOIN syntax :/ INNER JOIN kinda looks more complicated. I'll use mine above with your correction :) Your query in [pastebin](https://pastebin.com/DC2egkTh) and of course it works. :) Thanks man! 
&gt; Master recursive CTEs yes, ish... often I can use an APPLY on the data, and I find them far easier to track throughout the query, since they're effectively joined the same way as tables... the other tables and their columns are still just as accessible (CTEs only expose selected columns, especially when building one after another)... adding a column to chained CTEs means going back to the first place needed, adding to the output table, and adding the data, then repeating for each additional CTE in the chain... compared to just adding "CROSS APPLY ( SELECT * FROM tbl WHERE join condition ) tblAlias" and referencing the columns. unless the performance is notably better in the CTE, the maintenance efforts of a complex CTE will often lead to me rewriting the query.
Inner join syntax is much better because you can clearly define which clauses go with which joins. It's much more organized and definitely the standard so you will need to understand it. Your way is not wrong, it'll work, but too easy to make mistakes and maintain. You won't find anybody writing joins like that anymore. Read a tutorial, it's really simple.
CROSS / INNER APPLY and recursive CTEs are nowhere near the same thing. There are many problems in SQL that are *only* solvable via rCTE and if you were to rewrite the query you would almost certainly be breaking it (or applying arbitrary limits where an rCTE could recursively process for as many iterations as you'd like or need.)
Hi - I was actually able to get this to work. It's part of a much larger query and it turns out the issue was elsewhere in the query. I essentially am looking at about 6 different tables and have about 7 columns - only the last one having a calculation value. Client Name/Matter Name/Bill Number/Outstanding Accounts Receivable (Calculated field) The above is 4 columns - but should suffice. The client name, matter name, and bill number will always be there. Whether there is any data or not in the calculated field will depend if there is still money due on the bill. So if the bill has been paid, it will return a 0 in the last field. If there is money due, there will be a non-zero value. If there is a 0 value, it doesn't matter to me, so I just wanted to remove it. Does that make sense? I apologize for being the absolute worst at explaining things.
Every day, you should do one of two things. Take 30 minutes throughout your work day to accomplish this. Read three random blogs about technology you find interesting. SQL Central and Pinal Dave are my favorites, Brent Ozar has been hit or miss for me. The next thing you should do, is pick a topic you need to learn more about. Perhaps it's why you use temp tables over variable tables, this will be the fourth thing you read that day. This won't accomplish a lot for you today, or tomorrow, or 6 months from now. I've been in the job for 6 years now, so if you count 45 weeks of the year at 5 days a week for 6 years, that's 1350 opportunities I've had to read on the topic. So at 30 min a day, that's 40,500 minutes or 675 hours of reading high level content. It just adds up over time.
Python also goes well with SQL Server which I'd say is equal to Oracle in opportunity. MySQL also plays well with Python and is one of the most used RDBM's out there. Java is also really great to learn too though. I think learning the basics of programming and concepts and getting decent at either to start is more key though. Syntax can follow and be re-taught. Java has a more steep curve than Python imo and Python could be learned quickly afterwards.
Download MySQL, use Python to scrape and import web data. Then use MySQL to administrate and play with the data. Think of problems to solve, or look at stack overflow and find questions by people and try to replicate them. There are also mock DB's out there for you to import and play with. 
The INNER JOIN is executed before the WHERE. If you filter earlier in the query the result set will cost less cpu and memory. Please start writing (ANSI) JOINS the next time.
As far as I know the `where` clause is logically before the `join` clause, because you want to filter out as many of the source table rows as possible before you do the join--which has potentially quadratic characteristics.
Or you could open the table and type them in manually​.
not sure i understand (you haven't explained which table is parent and which is child) but it sounds like you want a LEFT OUTER JOIN
There should be no difference between using `OPENQUERY` and `EXEC AT`... they're both being executed on the remote (Oracle) server and the results are being sent over the VPN. I don't think there is any room for efficiency of a query when you're sending the entire table across the VPN... if you can limit the rows being selected or do a backup ==&gt; restore, that would be more efficient, but if you need all those rows, this is the only way to do it. The only suggestions I can make are to try different drivers out / investigate Oracle-specific options (driver and DBMS), try creating it as an SSIS package, or look into a WAN accelerator / optimizer for the traffic. I've used WAN optimizers in the past to improve SQL Server replication and it reduced the amount of data replicated by over 90%.
SO this?: select * from IDENTITY_GROUP left join TRADING_USER ON TRADING_USER.GROUP_ID = IDENTITY_GROUP.ID where IDENTITY_GROUP.BUSINESS_KEY = :key and IDENTITY_GROUP.ACTIVE = true and TRADING_USER.ACTIVE = true This was literally the first solution I came up with and it still didn't work....
no... the child condition stays in the ON clause SELECT * FROM identity_group LEFT JOIN trading_user ON trading_user.group_id = identity_group.id AND trading_user.active = TRUE WHERE identity_group.business_key = :key AND identity_group.active = TRUE 
I mean i definitely tried this with no luck, i'm starting to think it will never work. 
I am an infra DBA. There is a difference between a developer and a DBA (IMHO). I have managed to get through the last few years not having to know more than basic SQL topics. Knowing how to backup, restore, configure HA, setup replication etc is not necessarily knowing SQL. Being able to find those badly performing queries is also not necessarily knowing SQL. Being able to tune, to create a good relational model, when to add what type of index on certain types of data is part of SQL knowledge, and one that I am lacking. Subquery factoring, usage of CASE, why bitmap indexes are not always a good thing are some recent topics that I've become involved in, but the good foundation of SQL fundamentals that I don't have means understanding stuff like this takes longer (plus I'm old). Good luck. 
It doesn't matter. They're the same thing regardless of whether you join a to a, or a to b. The order of the on is irrelevant.
Thank you. I'm glad you all have confirmed that I know what I'm talking about :)
I walked away and had a cup of coffee and ended up with pretty much the same solution. I had to add a Count(acct) to the HAVING clause to filter out ones with only 1 row. I just finished validating the logic. THank you so much for responding!
this. You are trying to query over network which is notoriously slow. I would create an ssis project and port one or both of the tables to the same environment 
I use 3.8 base edition daily at work, and I've recommended it to power users who I think could benefit from it. The Query builder is a really attractive feature for business users that were used to Hyperion or Access. I thought TIC was okay, but I didn't like its security model, and felt like it was a little inadequate for a server deployed solution. Other than that, it's a great product for the price point.
Download mysql work bench. Don't kill yourself working in the terminal.
Can you post the table schemas? I.e. the lists of fields and their data types in the various relevant tables.
Thanks! I couldn't think of the right term to label what I was trying to do and queue was it. I've looked into it and apparently doing that in MySQL is pretty bad (see my edit above) so I'm going to look into using RabbitMQ or another message broker.
Common table expressions, they can do anything! 
Please use civil language on /r/sql - thank you.
I know that UPPER() gets me all hot and bothered. It's great for when Shachou wants his numbers all in uppercase. 
Coalesce, baby. Took me a long time to get it, but now I just wanna use it everywhere.
&gt; DDL vs. CML. Don't you mean "DML" instead of CML?
isnull, just saves so much typing
https://www.postgresql.org/docs/9.6/static/queries-with.html https://technet.microsoft.com/en-us/library/ms190766(v=sql.105).aspx http://www.morganslibrary.com/reference/with.html
CTE's and Window functions definitely make taking a query from "Basic Extract" to "Advanced Analytics" much easier and more powerful.
I don't have favorites. They are all my friends.
Finally! Shame the NHS seems to run on 2008 R2 :(
Presumably because it is quicker than a CASE statement.
All the window functions. Chopping up a table and updating it. Navigating to the next or previous rows without a cursor (lead and lag) is a great way to compare between dates in different records in an event log. Max(cast(newid() as nvarchar(36))) over (partition by name, address, city, zip) - will get you a rowid that you can use instead of select distinct. Very helpful when you have a very large denormalized table with tons of repeating rows. Or when you have to join a bunch of tables, then chop them back up with different rules, but have to keep a key to the original tables. Row_NUMBER() OVER ( PARTITION BY name, city order by date, uid) to get sub tables inside larger tables. I use this to sort duplicates by certain criteria so I can salvage the best record and delete the lesser dupes. It also makes it easy to build a mapping table from salvaged record to discarded duplicate records. Not a function, but OUTER APPLY/CROSS APPLY is awesome for joining tables with much more flexibility such as joining a one-to-many join as a one-to-one join. There are three ways to flatten many rows into one row - lead/lag, join a table as xml and strip the markup, and pivot/unpivot.
in reality I'm expecting it to return stuff as objects as I'm using it in code with hibernste etc.
Thanks, I didn't know about these! I have always just created views or temp tables for such complex queries. Is there any difference in performance? 
Yes. If you string a few of these together they get very slow. Temp tables work much better, are reusable, and indexable
I'm aware of CTEs, but aside from recursion what do they do special that a subquery or temp table can't do?
I second this. Row_number has changed my life.
I agree. Row number/ rank _with_ CTEs over subqueries any day. 
Numbers in uppercase. Right..
Personally, I don't think switching to coalesce would hurt because you can't always be consistent in terms of using the same functions and features to achieve your objectives. As long as you're consistent with your naming conventions and standards, you should be good.
Faster than temp tables in most cases since data doesn't need to be copied. Cleaner code compared to a subquery for large complex queries.
You said everything I was going to say and then some (hadn't seen the newid trick before). 
&gt;It is not faster than temp tables. I've done tests and I've read others that have done the same. Any minute extra time spent "copying data" is still less time than it takes the engine to calculate how to handle the subqueries, especially if you are chaining multiple CTEs together. This entirely depends on the RDBMS. If you are using an MPP database with slow write fast read, very often they are faster than temp tables for some use cases. &gt;As just one anecdotal example, I wrote a big long set of queries as chained CTEs, there were like 10 of them, each building on the last. I do agree that CTEs are much easier to read than subqueries, by the way. The performance was pretty bad. I rewrote those CTEs as temp tables and the results came back instantaneously. And that is not the only time I've tested that. And I have had the exact opposite occur. &gt;People are afraid of temp tables because they think they will be slow, but inserting data into a table is one of the core functions of databases and must be extremely fast. That fear is totally unfounded and if you rewrite your more complicated CTE queries using temp tables you will see a noticeable increase in speed. Or, perhaps, you know, some RDBMS's are different than others, and different applications have different requirements. Also, not every application can make use of temp tables. Even if your solution would perform better utilizing temp tables, if it is a one off query, it might not make sense at all to take the time to build it that way. Some targets wont even support using multiple statement transactions (not every RDBMS is MSSQL). For instance, if you are writing a report that is to be consumed via SSRS, you simply cannot use any kind of multi-statement logic such as temp tables. You have some legitimate points about temp table performance being in some cases superior, but you seem to be wrapping it up in your predetermined decision that CTE's are otherwise useless or not of much value aside from being "easier to read". You acknowledge here that they are easier to read, yet in your original post you asked how they were better than sub-queries. It seems more like you have encountered people using them in situations where they could do better and perhaps that has colored your view of them. Just like anything else with SQL, CTE's are one method of operations that has some really great uses and applications, and just like anything else it can be used or misused for specific use cases. Just like window functions are great but they can be resource intensive and i have seen them over-used when other simpler options would accomplish the same with a lower performance hit (and vice versa). 
I still love my ISNULL(), but COALESCE() is more fun to say. That and I like how it was explained to me "Extracted from the Æther" 
&gt; If you string a few of these together they get very slow. Like everything in SQL, the truthful answer is 'it depends'. Most of the time when I find CTE's are that excessively slow, it's because the person who wrote the SQL doesn't quite understand how the query engine is going to process it.
&gt; People are afraid of temp tables because they think they will be slow, but inserting data into a table is one of the core functions of databases and must be extremely fast. That fear is totally unfounded and if you rewrite your more complicated CTE queries using temp tables you will see a noticeable increase in speed. Writing ANYTHING to disk is always going to be slower - whether you notice it or not. There are use cases going both ways, and if the query is something critical, then it should be fully evaluated using each solution and optimized for maximum performance - which, just as /u/DiabolicallyRandom stated below, can be entirely dependent on which RDBMS is being used.
I don't know about your organization, but typically the people developing SSRS reports aren't people who can just write and deploy stored procedures.
You'll want to use row_number()over(partition by custid order by date desc) then with that result pull all where that equals 1. Like this Select * From ( select *, row_number()over(partition by custid order by date desc) rownum from custdata) a Where a.rownum=1
&gt;am I doing myself a disservice to use SQL Server Absolutely not. They are all mostly the same. 99% of what you know will translate to other RDBMS systems. The little stuff (like semicolons, using [these brackets] instead of "these quotes", getdate(), @variableNames, and so on) is truly the little stuff. You would adapt to the small differences in a matter of days, not weeks.
It looks like this worked perfectly. Sales data joined together and did not duplicate any rows like it was for me earlier. Thanks!
That's the beauty of it, you can have it return anything you want. At least in MSSQL you can. Doesn't matter what the data type is, as long is matches the rest of the rows. You can have it pull back a string 'No value available' or use it to prevent dividing by 0, etc. 
Eh.. they're a little different [https://docs.microsoft.com/en-us/sql/t-sql/language-elements/coalesce-transact-sql](https://docs.microsoft.com/en-us/sql/t-sql/language-elements/coalesce-transact-sql)
Oh I agree on that, but consistent is currently better, in the our new environment in doing it my way ;)
I mean... yea. ISNULL() is basically a neutered form of coalesce. I can't really personally think of a use case for it that coalesce can't already cover - and I prefer writing portable code whenever prudent to do so.
Take a look at http://postgis.net
try_convert Saved me days of debugging
How many rows in `course_main` and in `users`? where cm.course_id like '%SP2-2017%' ... aa.data like '%/webapps/collab-ultra/tool/collabultra%' or aa.data like '%"resource/x-bb-toollink","title":"Blackboard Collaborate%' or aa.data like '%/webapps/bb-collaborate-bb_bb60/list%' or aa.data like '%/webapps/bb-collaborate-bb_bb60/list4mashup%' https://www.brentozar.com/archive/2010/06/sargable-why-string-is-slow/ Basically you're going to have to go through every single one of those rows because of the wildcard character at both sides of the search terms... so if there's a few million, it's going to take some time as it can't use an index to do so. If you can remove the beginning wildcard from the `cm.course_id` it could speed it up a lot (assuming there's an index on that column)... otherwise you may have to further limit your data some other way.
I've been very reliant on the WITH clause and have been struggling with how you cannot use it in a Delete...
I've been programming in SQL for about 4 years and I feel like most of the proficiency comes from understanding the tables and how they work together. Once you know how they interact the only thing holding you back is learning syntax and clauses.
&gt; SQL (which I've been only learning for a week) Ask us again in a couple months. Regardless of how much experience you have in other languages, SQL is a different animal. You'll get there... don't worry about your coworker. At one week in, I think I was just happy I could finally keep the different types of joins straight.
&gt; Regardless of how much experience you have in other languages, SQL is a different animal. You'll get there... don't worry about your coworker. This. SQL is an entirely different way of looking at things and until you mentally make the jump from row by row thinking to set thinking, you'll struggle. A week isn't very long to learn SQL.
I can fix the merge process, I have it set where it won't bring back basketball, Basketball. So if I can order it on the merge that'd be awesome! Is there a recommendation you have? The only way in 2008 I know is to add the 2 columns with the plus sign.
The other commenters in this thread have provided sage advice. The only thing that gave me pause...what are you trying to DELETE? INSERT INTO? OK, sure. UPDATE? You better have a WHERE clause and a good reason. DELETE? You better have a WHERE clause, a backup, and a damn good reason. (We don't talk about TRUNCATE or DROP.) Slightly less pause... the SELECT *. I know it's tempting. But try to resist the temptation. Know what data you want to extract and from which tables.
I used SQL as read-only for years. The idea of running anything to make changes to a live table with only a week's proficiency makes my skin crawl a bit. Be VERY VERY careful! SQL doesn't really have too many limitations once you understand it, but it does have a lot of quirks that can come up to cause unexpected behavior. Often when a query you thought was vetted out doesn't do something correctly, it may just as likely be something to do with unexpected data in your source as a problem in your logic. As mentioned already, post full examples of your query syntax and we can probably help. Side note, are you sure you have permissions to run a Delete statement? I noticed you didn't mention what errors you are getting back.
Thanks guys! I'll see which works out for me XD
Man that's good to hear bc I'm at my first SQL job - and first job in IT for that matter - and I feel a lot like OP. Boss says hey we need all patients who are this condition and have had this test, or else have been excluded from needing that test done, etc. I fee like the code can't be that hard, but I can't visualize where all the data is (are?) and how to combine it (...them). 
Can you elaborate on the difference between "row by row" vs "set" thinking. I think this is a great topic. I have an understanding of what you're talking about, but I would love to hear your explanation. 
You're correct it's being used as a table in most contexts. Thanks! 
It's how it has *always* been done.
Yeah that's pretty much what I do. The issue comes up when inner joins/WITH is involved or when i need to reference another select.
Yeah as someone who has programmed with Java, which writing sub queries is the closest I feel to objection orientation, it seems with sql it's a different thinking process.
Sqlcourse.com was quite good if I remember correctly - you are given tasks and can execute the code right in browser to see the results.
Check that http://www.studybyyourself.com/seminar/sql/course/?lang=eng. It is kinda what you re looking for. Free, for beginners, well structured, with online exercises, tuned for people who want to learn at one's pace. If you want to run SQL queries locally on your machine you can for example install [MySQL workbench](https://www.mysql.com/fr/products/workbench/) or [XAMPP](https://www.apachefriends.org/fr/index.html) (or [MAMP](https://www.mamp.info/en/) on MacOS). Of course you need to create a database first, but you can create a database with only one table at a first step.
Select * from w3schools Where subject like 'SQL%' Order by 1 desc 
Khan Academy has a SQL course that is solid also
So are you going to tell us the problem?
Yep, stored in memory as a heap and retrieved generally in the order inserted or ordered by the primary key (if at all), unless the WHERE clause allows you to take advantage of another index, but none of that will still guarantee a specific order. However ORDER BY clauses can also severely impact performance if used on complicated queries with lots of JOINs or aggregate functions, like MIN/MAX, unless they're strictly necessary for the return set and a TOP clause. 
&gt; I know they are next to useless No, they're quite useful. They're just *unnecessary* and harmful to performance in many of the instances where they're used because they're stupidly looping through a single table in a row-by-row fashion when a single set-based operation would work far better. I use cursors daily in my work because I have to iterate the same code over a few dozen/hundred/thousand databases and it's literally the only way to do it. Like any tool, cursors *do* have value and purpose. The important part is knowing *when* they're appropriate to use. A blanket statement of "never use cursors" or "they're pretty much useless" is not appropriate. I've got a 24" Crescent wrench sitting out in my garage that may *seem* "next to useless" but when I need it, I'm damn glad I've got it in my toolbox. ^(I can't believe I just wrote 3 paragraphs defending cursors)
 &gt;^(I can't believe I just wrote 3 paragraphs defending cursors) There there. Itzak has done the same.
A post can be a link or a self post, not both. Reddit's UI doesn't make that clear. 
Does SELECT length/1000 as Length not work?
I may be being ignorant here, but wouldn't SSIS or a While loop with some dynamic SQL also work? I've never used a cursor, and I'm not 100% sure of their reach in terms of functionality.
Oh my gosh! Thank you so much. As soon as I saw that I knew that was going to work. Thank you! 
Sorry, I didn't mean to downplay the usefulness of cursors. I knew they exist for a good reason and I could have worded it a bit better. I like the wrench analogy, btw.
To answer the general question about feeling proficient, I have to say it's not actually about _proficiency_ so much as it is about _comfort._ By that I mean you don't need to know how to do everything; you just have to know what's possible, build a rough-and-ready mental model of how it works, and get comfortable with the idea of figuring stuff out.
I believe average is a reserved word. Try wrapping it in [] like this: " as [Average]"
SOLVED. Thanks to /u/fauxmosexual
Well now I feel bad because I figured out a solution and added it to my original post message a few hours ago. I thought I wasn't going to hear back from anyone else. I really, really appreciate it.
I just stole yours from the edited post and changed it up a little, no worries.
The query works, I would just like to add that if you want to avoid scanning the table from disk twice you might want to consider a query like this leveraging window functions in sql server: --One scan per table query SELECT p.* , p.ProductStandardPrice - AVG(p.ProductStandardPrice) OVER () AS [DifferenceFromAverage] , AVG(p.ProductStandardPrice) OVER () AS [Average] FROM Product AS p For larger tables you might run into some trouble with overhead if you scan the entire table multiple times. We get the same results but with different query plans. Check your executionplans (ctrl + m) together with the query results to get a feel for how your query is performing. http://i.imgur.com/mwlYapi.png
Hi there, If the data is static (I assume this is just for a university project, and it will never get updated or used in any other way), it may be easier to export the DB and have everybody just import it into their local drive.
Yeah that's what I was thinking. At this point I have another question. When I wanted to export the DB with the Wizard, I chose a flatfile as destination. But somehow I couldn't manage to export the whole database but only one file for one table. Is there a better way to do this? Thanks again!
No problem, and for that, I would just backup your entire database, and give your group the database file to restore in their local drives. It's been a while since I worked with MS SQL Server, but through SQL Server Management Studio, there should be an easy graphical way to backup/restore entire databases. Probably right-click on the database you want to export, and look for options to backup.
That sounds promising! I will try it out later in the day. Thanks!
Checkout [SoloLearn](https://www.sololearn.com/?cc=77) You will learn all the basics in few days.
A window function would do the job as well (just for the sake of mentioning my favorite feature!) SELECT *, ProductStandardPrice - AVG(ProductStandardPrice) OVER() AS DifferenceFromAverage, AVG(ProductStandardPrice) OVER() AS Average FROM Product
&gt; As with all things database, "it depends." Precisely, the right tool for the job. 
Somehow the group by l.id doesn't work anymore on the productive system. When I removed it the result is the same and it works. 
Triplex/suplex/duplex should be data items, not columns. I'd replace those three columns with BatteryType, and that field could contain Triplex/suplex/duplex. What are the three watt columns for? I would think you'd only need one, and it would have a number type (int, I guess?). If the wattage can be any number rather than a few fixed values you shouldn't have a lookup, just store it as a number for each bike. You need a bike table - enginetypes aren't a property of the renter. The bike table should have BikeID as a pk, and a enginetypeID, batteryTypeID, and either a wattID if you're keeping the lookup field, or just watts if you're not. &gt;Choice of engine x Battery defines the price It's not clear how this is calculated but you might need a table with engineID, batteryID and price, joined to the bike field on both engineID and batterID. After you've done the above you'd have met the needs of the question, but from a real-world perspective it would be super weird to have just bikeID in the renter table. You would normally also have a rentals table with customerID, bikeID, and the start/finish datetimes of the rental. 
You should consider posting which flavor of SQL you're using (MS SQL Server, MySQL, Oracle, etc...) they're all _pretty_ similar and for something like this will probably be identical, but sometimes there's something very simple in one flavor of SQL that's complicated in another. For this question... Is it doable? Sure. Is it practical to do in SQL? Probably not. How are you going to _use_ the data if it's spanned across hundreds or thousands of columns? The "proper" way to do it would be to group it by customer_id, device_used, so you'll get multiple rows of data: Assuming MySQL... http://sqlfiddle.com/#!9/62f00/5 DDL: CREATE TABLE items (`customer_id` int, `device_used` varchar(9)) ; INSERT INTO items (`customer_id`, `device_used`) VALUES (1, 'Nokia'), (1, 'Nokia'), (1, 'iPhone'), (1, 'iPad'), (2, 'Samsung'), (2, 'Blackbird'), (3, 'Nokia') ; SQL Query: SELECT customer_id, device_used, count(device_used) as times_used FROM items GROUP BY customer_id, device_used ORDER BY customer_id, times_used DESC, device_used Output: customer_id|device_used|times_used |:--|:--|:-- 1|Nokia|2 1|iPad|1 1|iPhone|1 2|Blackbird|1 2|Samsung|1 3|Nokia|1
Thanks, will look more close at this tomorrow morning (past midnight here). Naturally I should have said that I am using PostGreSQL right now. An alternative solution (just occurred to me) would be to list the preferred device and second most preferred device for each customer. Something like Customer_id device#1 device#2 1 Nokia Ipad That might be more SQL like, also might be more practical as I guess you can skip knowing what possible devices exist. Any input on how to do this? 
I don't completely understand the context of the question, I take it these are electric bikes that need recharging? I think your current schema is pretty good: You've got a table for bikes, one record per bike, with the appropriate lookups. You've got one table for customers, with one record per customer. And you've got junctiontable (which I would call something like Hires) that has a record for each time a customer hires a bike. I wouldn't have another table for dates, that would sit within the Hires table. Your price problem is interesting: you would need to join that to the bike table on the combination of bike attributes that determines the price. But prices can change, and you would want to be able to determine the price paid by any particular hire. Either you would want to store the price at the time of the hire in the Hire table, or (if you wanted to make your life difficult to impress your examiners) have the entire history of prices in the price table - each price having a start and finish datetime that it applies for.
Yes, these bikes need recharging. It's a part of the text, so I assume they want me to somehow keep track of how much battery is used? *edit*: or perhaps throw in an IF() in the html (if duration rent &gt; battery capacity, echo error) Secondly, do you believe this table to be better or way redundant? http://imgur.com/a/JGBme I'm thinking if the wattage for an engine has to be changed, or an engine name, it allows for just editing the first table which will then alter all corresponding data. I might just throw date/price into the junction table, as it has no look-up (it's pure input data per line). Lastly: I should use *one* to *many*, if *one* person can rent *multiple* bikes, but one bike can only be rented by one person at a time? 
I might have misunderstood the question: I took wattage to mean something like horsepower, which is basically static. If wattage is something to do with the amount of charge a bike has then it does make more sense to take the approach you have, which is without a lookup field. I think the date table is unnecessary, the start date, end date, duration and time are all specific to the hire - so they should go into the hire table. In terms of the join, it is still technically a many to many in that one bike can have many hirers and one hirer can have hired many bikes. In the real world you might put a constraint on the table to make sure that each hirer can only have one hire at a time, but that doesn't change the nature of the join.
~~Change ON i.resercode=s.resercode to ON r.resercode=s.resercode~~ Actually maybe not quite so simple: the problem is you're grouping invoiceline in a way that returns three rows per resercode, and then joining that back to itself on invoiceline, which is why you're tripling up. Do you need invoiceline in the outer query at all? i.e., SELECT r.resercode, m.memname, sum(subtotal) AS total FROM bnb.reservation r JOIN bnb.member m ON m.memid=r.memguestid JOIN (SELECT resercode, SUM(unitcost*nounit) AS subtotal FROM bnb.invoiceline GROUP BY resercode, feetype) s ON r.resercode=s.resercode GROUP BY r.resercode, m.memname ORDER BY r.resercode; As to more efficient, you probably don't need to do this with an inline view: SELECT r.resercode, m.memname, sum(i.unitcost*i.nounit) as Total FROM FROM bnb.reservation r JOIN bnb.member m ON m.memid=r.memguestid JOIN bnb.invoiceline i on i.resercode = r.resercode GROUP BY r.resercode, m.memname
Did you see my edit?
Second try works like a charm! Thanks so much. Will have to keep this in mind for the final. (We got the schema early to play around with, and I'm trying to figure out what sorts of queries they might ask for.)
Which schema do ypu like the most, take 3 or the on in my previous comment (with independent battery and engine lookup)
Thanks for the reply! The ERP we use is Made 2 Manage which the front end of it is terrible enough now I'm dealing with the database part it seems even worse. None of the tables or fields are named anything remotely comprehensible and their documentation is virtually non existant. All the string fields are defined as "Text" as opposed to "varchar(max)" which admittedly I don't understand 100% but it seems like best practice is to no longer use Text ever. I'll play around with the SQL based on what you said here and see what I can come up with. Thanks again!
&gt; None of the tables or fields are named anything remotely comprehensible and their documentation is virtually non existant. LOL. That's so you can hire some high priced system consultants to help you figure it all out.
&gt; Do I make fact tables that are so finely grained that each row basically represent one row of an entity as it would be in the operational database? Yes, that is the usual design pattern.You can always aggregate from the lowest grain. You can also create aggregated fact tables as well (materialized views are good for this) - many BI reporting tools are "aggregate aware" and will use the appropriate tables in their queries. http://www.kimballgroup.com/data-warehouse-business-intelligence-resources/
I agree with the above. I should also point out that some RDBMS systems have the ability to export data as XML, so you would have the best of both worlds.
I think what you're asking for is how to implement a tool like SSRS or Tableau so that your users can drill down?
Check this out then: http://www.sqlservercentral.com/articles/MERGE/103127/ https://www.mssqltips.com/sqlservertip/3074/use-caution-with-sql-servers-merge-statement/ http://michaeljswart.com/2012/08/be-careful-with-the-merge-statement/ I still use Merge, but I'll probably opt to primarily use IFTT logic with insert / update. Make sure to use the right tool for the job, but you may receive benefits from that minor code adjustment.
The third one, but I don't understand why date is a separate table.
&gt; I hope this makes sense in any way. It does not. I am not clear on your use of the word formatting, since, to me, that means font, text style (bold/italic/etc) and visual only qualities, so losing data during the formatting doesn't make sense to me. The only formatting I can see which can lose data when your view is doing a data conversion. eg. a date time being converted to character data could lose information or even get converted to just a date and lose its time. Give a longer explanation and give a better idea (even a shortened example of what you mean) of what the data you're looking at contains. 
Not sure if your specs allow it, but what if you only update projects that have no uncompleted items?
The simplest approach would be to mark it as complete if it doesn't have any incomplete actions.
The sharepoint list that the Ac view pulls from performs look-ups based on other lists to collaborate all the information into one full list. Formatting in this case means that the data it should show like name or role is instead replaced by the ID of that row in the column. So if the name should say Shmooel, and Shmooel is row 42(and thus ID 42), it will return the number 42 in the name column. Same thing with multiple names, it returns multiple row IDs. This is happening in the Ac view, which is the most important one. The Ac has columns title Fa, At, Ad, AUI, AP, and AL. These all return values in their cells referencing the row ID from the source view. The Fa column(in the Ac view) should output a name, for example, but instead outputs a number which references the row ID for the Fa column in the Te view. While the At, Ad, AUI, AP, and AL columns are all also dropping their data and all output a number which corresponds to the row ID on view Ag. I want to have one master view that replaces the row IDs in view AC with the values of those columns found in the Ag and Te views, instead of just the ID #s. I hope this helps you help me. Also, thank you!
I'm using EmEditor to view the file, and it's formatted. I want to be able to view it with all the columns and stuff organised. For example, in SQLite it looks as though it does this (from google images) but nothing appears when it says Import Complete. It says MySQL dump at the top, not sure if this helps. --edit-- I don't have a server or anything to upload it to. I was hoping to be able to view it in Windows 7.
Also not a heavy hitter on SQL Server, but your method sounds fine to me, and it doesn't appear to scale any worse than theirs. I would offer a potential alternative though. Instead of trying to do all of the load in gigantic batches, would it make sense to try to break it up into smaller chunks that perform quicker. I know that could produce issues like back-pressure, but it may be worth a look.
I think there is a free version of SQL server. 2014 or 16. Includes SQL server management studio. If you need help querying the data I'm sure we can help
And how could I check, if all items are uncompleted? Something like: "IF (SELECT done FROM action)=1" comes to my mind, but is that how you would check all of them? Can you even compare a whole table with 1? Or ist there a for_each function to do so? :x Sorry for asking so stupid questions..
It's readable, and coloured etc. 
Yeah, probably, but how can I check that? That is kinda my problem.. Something like: IF(SELECT done FROM action)=1 comes to my mind, but I don't know how I can compare a complete column with something or how to compare them one after another :/
So something like: UPDATE project SET completed=1 WHERE (SELECT COUNT(t1.done) FROM [...]) =(SELECT SUM(t1.done) FROM [...]); ? SUM counts how many of the actions are finished and compare that to the COUNT of them. I will try it, thanks!
In that case, I see nothing wrong with your approach.
 UPDATE project SET completed=1 WHERE pid not in (SELECT pid from project_action pa inner join action a on a.aid = pa.aid where a.done &lt;&gt; 1)
Thank you very much! I will test it. But basicly all it does: it makes a list of all pid's, where there is an unfinished action (a &lt;&gt; 1) and sets all projects to 1, where the own pid is not in the previous made list, right? That's kinda easy to understand. Thank you very much!
That's the gist of it. Hopefully having projects with no actions at all being set as complete isn't a problem.
Ah! I've got the file importing into MySQL, but it's taking a while. I'm not sure if that's because I havent' done it correctly and it will just stay like that, or because it's so big. I've managed to get the columns to appear in the Schema menu. I'm hoping it will eventually import and I can view the table data.
What exactly is it? A .sql file? A .bak? Regardless, it may be easiest to just download the developer edition of SQL Server (assuming MSSQL) and run/restore it. E: Read the thread. There may be tools out there which will do what you want, but personally I'd just prep a local environment and run the script.
I've somehow managed to view it organised into columns in MySQL, but it doesn't appear to have listed all the entries. The information I'm searching for doesn't show up, yet when I search for it in the text editor it is there. I set no limit to the number of rows. I'm trying to export it as a CSV so hopefully it will have all the info there. --edit-- actually maybe I didn't import it properly and had to let it load longer (it's 13gb) which is why it's not showing all the data.
&gt;Their answer? Scalability. &gt;Basically **they believe** that over time as the data grows (its not very large and doesn't grow terribly much per week) that my process will eventually take longer than their approach. As the data grows, so does the amount of compare time using the MERGE. They can **believe** anything they want, unless they or you have hard proof a way is better it's just assumptions. Fake some data on a test server, and measure the time it takes. &gt;*"Remember kids, the only difference between science and screwing around is writing it down."* It's not only the time it takes, but your way also does some other stuff. If you delete ALL the data and rebuild it, statistics and indexes are also rebuild/refreshed. If their merge doesn't at least concern [20% of the data](https://www.sqlskills.com/blogs/erin/understanding-when-statistics-will-automatically-update/) statistics will not be updated.
SQL Server 2016 no longer includes Management Studio. It's separate (free) download. But SQL Server Express Edition (the free version) has a limit of 10GB per database so _if_ this is a SQL Server database, it probably won't work. 
Where did you get it? Find out what type of database file it is. It sounds like a dump of a PostgreSQL database.
a .sql file is simply a .txt file. There is absolutely nothing special about it other than it opens by default in a database manager application. I bet it's a create database script that sets up and populates a database. Once you've ran it successfully and made and populated the tables, you can export a table to excel.
Good thought but it didn't help. Thanks for your reply! 
There's a few tools, you'll need to google for them, but they help map out a dependency chart. The next best thing I can recommend for you to do, is to find an off hour, perform tasks while running a SQL profiler trace. Between seeing if they have any foreign key or constraint links and tracing specific tasks you execute, you can piece together your own documentation. It's easier said than done, I know. That's where I'd start though if they offer nothing and you had to figure out what things do. Also capturing events in the DMV's with sp_whoisactive, that can possibly help.
it sounds like you have an actual database, not a script. There is no chance a script could be that big. Try googling a gnu file to look at it.
Sure it can. Its probably a script to creat the database and insert all the data in the database. It's one of the more common ways to backup small MySQL databases. That said it's a dumb way to back up a 13gb database. 
If the script (I'm guessing that's what this 13gb file is...a series of commands to recreate a database and all of the data in it) failed at some point midway through the process it may not have fully created all of the tables/columns/rows. It's going to be hard to explain how to rectify this in reddit comments.
How wide is the table? That with the number of records and no indexes would cause issues. Regardless of if it is in a where statement or a join. Try an IN with a few values to see what happens. I would think that you will have issues no matter what though.
This is likely the correct response here. Using IN will probably have worse performance than a JOIN. The join just needs to be optimized by having proper indexes. You'll want an index on both the material code table and the input table (if the input table is large enough).
That's what I needed! it makes sense now. Thank you for your help! 
Return the ABS result (any any other calcs in question) as a column so you can sense check it. Might be that your data types are misaligned somewhere. That's made me feel quite foolish at least once.
D_W_Hunter, thank you so much for getting back to me. I've been trying to figure out a way to join the 3 views using something like an INDEX(MATCH) method and I just can't seem to find somewhere that helps me identify my problem. I'm not at work right now, but I've tried Join statements to connect the views based on a singular Unique Identifier but that just causes a shit storm; since my Ac view goes from 46 rows(as it should be right now) to 6k+ regardless of inner, left, or right. If there's a direction you can point me in and I can try tomorrow and see how my results are? I'll take any helpful advice you can give me. Thank you again for getting back to me.
That isn't the way I was using the IN clause. If memory serves, Access does not allow subqueries in an IN clause (which is very unfortunate). Will double-check in the morning. My original query looked like this: SELECT * FROM yourtable WHERE mycol IN ('matcod1','matcod2','matcod3');
Is this what you're trying to do? Your code makes me wince... WITH E AS ( SELECT Email, Name, COUNT(*) OVER(PARTITION BY Name) Count FROM EmailName ) SELECT P.*, E.Email FROM People P LEFT JOIN E ON E.Name = P.Name AND E.Count = 1
Because it would only update resources that aren't available yet. With the "AND r.DATE_TO &lt;= NOW()" constraint I filter out those resources not available yet, as their time of use is beyond NOW(). Let's say you booked a chair from today 1pm to 5 pm. With the statement of yours the query would make it available at 2pm while you are still using it. Other than that, it's not the problem I face, I believe. As phpmyadmin says that I got a syntax error in my first declaration 
I think the point that GenConsensus was making is why do this with a cursor at all?
Because me was stoopid at that moment 
You can search all the agent jobs for a specific filename (Replace `FILE_NAME` with your file name): USE [msdb] GO SELECT j.job_id, s.srvname, j.name, js.step_id, js.command, j.enabled FROM dbo.sysjobs j JOIN dbo.sysjobsteps js ON js.job_id = j.job_id JOIN master.dbo.sysservers s ON s.srvid = j.originating_server_id WHERE js.command LIKE N'%FILE_NAME%' GO There's also a couple more places I can think of to look... stored procedures, functions, and backup devices: SELECT * --Stored Procedures FROM sys.procedures WHERE (OBJECT_DEFINITION(object_id) LIKE '%FILE_NAME%'); SELECT * -- Functions FROM INFORMATION_SCHEMA.ROUTINES WHERE (ROUTINE_DEFINITION LIKE '%FILE_NAME%'); SELECT * -- Backup Devices FROM [sys].[backup_devices] WHERE ([physical_name] LIKE '%FILE_NAME%') **These need to be run against _each database_ so you'll have to change the database name at the top of SSMS and re-run it**
How is item inventory related to each store in your database?
When you say it gets updated daily, what do you mean? Is the last modified date on the file constantly or periodically changing or do you see it somewhere else? Since it seems like the file is constantly in use can you see if there is anything running for very long time that is currently running? SELECT sqltext.TEXT, req.session_id, req.status, req.command, req.cpu_time, req.total_elapsed_time FROM sys.dm_exec_requests req CROSS APPLY sys.dm_exec_sql_text(sql_handle) AS sqltext It could be some sort of job outside of SQL that is running a manual backup job constantly, one thing you can check is the query history for anything on that backup file, so this: SELECT TOP 10 SUBSTRING(qt.TEXT, (qs.statement_start_offset/2)+1, ((CASE qs.statement_end_offset WHEN -1 THEN DATALENGTH(qt.TEXT) ELSE qs.statement_end_offset END - qs.statement_start_offset)/2)+1), qs.execution_count, qs.total_logical_reads, qs.last_logical_reads, qs.total_logical_writes, qs.last_logical_writes, qs.total_worker_time, qs.last_worker_time, qs.total_elapsed_time/1000000 total_elapsed_time_in_S, qs.last_elapsed_time/1000000 last_elapsed_time_in_S, qs.last_execution_time, qp.query_plan FROM sys.dm_exec_query_stats qs CROSS APPLY sys.dm_exec_sql_text(qs.sql_handle) qt CROSS APPLY sys.dm_exec_query_plan(qs.plan_handle) qp where SUBSTRING(qt.TEXT, (qs.statement_start_offset/2)+1, ((CASE qs.statement_end_offset WHEN -1 THEN DATALENGTH(qt.TEXT) ELSE qs.statement_end_offset END - qs.statement_start_offset)/2)+1) like '%BACKUP_FILE_NAME%'
For the date modified time, that's coming directly off the file properties (right click &gt; properties)... The time stamp is ~2:50am every day. I only see it changing the one time daily. Neither of those returned back anything... For the first script the only return was a row that contained the script itself... I tried it against all our db's as well as the MS default ones.
 SELECT Name, COUNT(*) FROM People group by Name HAVING COUNT(*) &gt; 1 This would be people who have the same name in the people table. SELECT Name, COUNT(*) FROM EMailName group by Name HAVING COUNT(*) &gt; 1 This would be the people who have the same name in the EMailName table. If either of these return rows, then joining on NAME gives ZERO confidence that you're attaching the right email address to the right person. &gt; yes I am aware. I would rather exclude the record then create a duplicate. Because I don't know if alex smith is the same person or different people I don't want to tie either to a certification. Is there any chance that someone could get added to the People table without getting added to the EMailName table or vice versa? Because if they can, then you're still not guaranteed that the Alex smith you're joining on is the same Alex Smith in both tables, even if there's exactly one row of that name in both tables. If that's NOT the case... SELECT People.Name , EMailName.EMail FROM People INNER JOIN EMailName on People.Name = EMailName.Name WHERE ( SELECT COUNT(*) FROM EMailName ECount WHERE People.Name = ECount.Name ) = 1 Is about the most straightforward way to get it
I made some images to help:) http://i.imgur.com/T2sG1x4.png So this would be or original database, (actual data from the database, but only INV_STORE 1-3 fit in the screenshot.) I want to select the BARCODE,STYLE,COLOR,SIZE, and REORDER combination for the closest LAST_REC DATE as given by this next screenshot. http://imgur.com/waZM17M and then, if all values aren't zero, give a 1 under REORDER/DESIRED, and if all values for REORDER/DESIRED are 0, then have it as a zero without the store number given, as shown in the last screenshot. http://imgur.com/uaNe1s5 
Maybe look at the backup history for that database? The query is searching on the resulting file name so maybe that'll work. You could also search on database name to see all backups happening on it. It may not ID the job calling it, but if you see when its running perhaps that'll give a clue in case there is a 3rd party backup app or something... SELECT physical_device_name, name, backup_start_date, backup_finish_date, backup_size, database_name FROM msdb.dbo.backupmediafamily fam INNER JOIN msdb.dbo.backupset bSet ON fam.media_set_id = bSet.media_set_id WHERE bSet.type = 'D' AND physical_device_name LIKE '%myFile%' ORDER BY backup_start_date 
Definitely 
that's it! thanks a ton
yeah, i was an idiot i didnt realize you could do case within a sum function
Hi! So indeed, I placed my filename in your script, and I got one line of output (the start and end dates correspond to 1-15-2017). the physical device name matches my UNC path, the name of the backup matches, and the database name matches. 
You are awesome, thanks for the help and I will try it out 
Beautiful, thank you.
Ur a sql star
Loop up CONSTRAINTS, particularly Foreign Key constraints and / or Check constraints.
Optimization is a weird beast. It's a combination of research, playing with queries and stats and seeing how that affects the exe plans, and research. Every time someone better than me manages to better optimize a query, I try and figure out why it is that that particular solution worked. Understanding the in-memory structure of a heap and how an index is stored and affects queries can really help, too. [Redgate](https://www.red-gate.com/library/inside-the-sql-server-query-optimizer) and [SimpleTalk](https://www.simple-talk.com/sql/) have some particularly good PDFs and articles about it. You'll hear the term "Accidental DBA" (See [1](https://www.sqlskills.com/help/accidental-dba/), [2](https://www.red-gate.com/library/troubleshooting-sql-server-a-guide-for-accidental-dbas), [3](https://www.simple-talk.com/books/sql-books/troubleshooting-sql-server-a-guide-for-the-accidental-dba/)) thrown around a lot. Quite a few people have fallen into that role through corporate shakeups or by virtue of just being halfway decent with a database, so there's a lot of good stuff out there for people who just haven't learned yet. [SQL Server Central](http://www.sqlservercentral.com/stairway/) has stairway article series and practice questions of the day on just about everything you could want, though maybe not some of the more intimate DBA stuff. After all that, just doing a bit on [Set Theory](https://www.math.ku.edu/~roitman/SetTheory.pdf), reading MSDN (or the associated internal structure of your DB of choice), and finding white papers is a good route. Finally, I'd suggest looking up some of the job requirements and trying to learn what you don't already know (mirroring, replication, how the agent functions and schedules job tasks, backups, automating tasks you hate doing, working with the system tables, server installs, 3am phone calls because production is down...). Most of what's in there is MS SQL since it's what I'm familiar with, but I've worked a little with others and they're fairly similar past the syntax and special features, until you get really deep into the engine. Anyhow, that's how I've been training myself. I'm hoping it pays off one of these days, haha. I know there's a lot of info there, but you don't have to learn it all at once and you seem to have a pretty decent head start. Good luck to ya!
Omg... when I was starting out... like watching an episode of pimp my ride. "Yo dawg, I heard you like views, so I put a view in your view, so you can view your views"
This is fantastic- thank you!!!
What you said. (And good thing OP wasn't asking about the question at the top of the page because I've no idea what it even means. Never heard of such "levels".)
See dnoeth's post in this thread: (http://community.teradata.com/t5/Database/Regarding-LEAD-analytical-function-function-in-TERADATA/td-p/28079) dnoeth's views are much respected in the TD community. LAG will be a case of adjusting the logic to suit. HTH
Sure thing! If you want any more targeted articles or to find stuff like the difference between OO, SQL, and NoSql; figure out less common syntax like tally tables, windowing functions, and dynamic SQL; why Optimization A works in one place but not another... feel free to PM me. I've probably already tried it or read a good article for it. If not, it gives me a chance to learn, too! :)
You could export the data to a CSV file and query it with this tool: https://github.com/dinedal/textql Or you could do what I prefer - run [DB Browser for SQLite](http://sqlitebrowser.org/), import the data into a sqlite database, and query it there. You don't have to install SQL Server at all.
Bunch of unions going on but from a quick glance the Order table is in the last query you union'd. You would add [Order].Referencenumber in the select. Since it's unioned, the other queries above this may need a matching column. For the sake of this example just add a select for the other queries for ' ' as [ReferenceNumber]. Kinda like this: SELECT ' ' AS [ReferenceNumber] from table1 union SELECT ' ' AS [ReferenceNumber] from table2 union SELECT [Order].[ReferenceNumber from [Order] something like that.
You want to avoid storing superfluous data, so don't store each person with every jean ~~selection~~ record. You only need to store the records where a jean selection was made. Assuming you have the following data structures and data sets: CREATE TABLE [dbo].[Person] ( [PersonID] INT NOT NULL IDENTITY(1,1) PRIMARY KEY CLUSTERED, [Name] VARCHAR(20) NOT NULL ); INSERT [dbo].[Person] ([Name]) VALUES ('John'); INSERT [dbo].[Person] ([Name]) VALUES ('Alex'); INSERT [dbo].[Person] ([Name]) VALUES ('Bowie'); CREATE TABLE [dbo].[Jean] ( [JeanID] TINYINT NOT NULL IDENTITY(1,1) PRIMARY KEY, [Color] VARCHAR(10) NOT NULL ); INSERT [dbo].[Jean] ([Color]) VALUES ('Blue'); INSERT [dbo].[Jean] ([Color]) VALUES ('Green'); INSERT [dbo].[Jean] ([Color]) VALUES ('Red'); INSERT [dbo].[Jean] ([Color]) VALUES ('Yellow'); INSERT [dbo].[Jean] ([Color]) VALUES ('Black'); INSERT [dbo].[Jean] ([Color]) VALUES ('White'); You can create a reference table utilizing the primary keys of `[dbo].[Person]` and `[dbo].[Jean]` without needing to store a bit for each selection: CREATE TABLE [dbo].[PersonJean] ( [PersonID] INT NOT NULL FOREIGN KEY REFERENCES [dbo].[Person]([PersonID]), [JeanID] TINYINT NOT NULL FOREIGN KEY REFERENCES [dbo].[Jean]([JeanID]) ); Now, loading that table with data: INSERT [dbo].[PersonJean] ([PersonID],[JeanID]) VALUES (1,2); INSERT [dbo].[PersonJean] ([PersonID],[JeanID]) VALUES (1,3); INSERT [dbo].[PersonJean] ([PersonID],[JeanID]) VALUES (2,6); INSERT [dbo].[PersonJean] ([PersonID],[JeanID]) VALUES (2,1); INSERT [dbo].[PersonJean] ([PersonID],[JeanID]) VALUES (2,2); INSERT [dbo].[PersonJean] ([PersonID],[JeanID]) VALUES (3,4); INSERT [dbo].[PersonJean] ([PersonID],[JeanID]) VALUES (3,5); INSERT [dbo].[PersonJean] ([PersonID],[JeanID]) VALUES (3,6); You can use the following query from your application to set the check box values. SELECT [P].[PersonID], [P].[Name], [J].[JeanID], [J].[Color], CASE WHEN [PJ].[JeanID] IS NOT NULL THEN CAST(1 AS BIT) ELSE CAST(0 AS BIT) END AS [JeanSelection] FROM [dbo].[Person] [P] CROSS JOIN [dbo].[Jean] [J] LEFT JOIN [dbo].[PersonJean] [PJ] ON [PJ].[PersonID] = [P].[PersonID] AND [PJ].[JeanID] = [J].[JeanID]; Which outputs the following dataset: PersonID Name JeanID Color JeanSelection ----------- -------------------- ------ ---------- ------------- 1 John 1 Blue 0 1 John 2 Green 1 1 John 3 Red 1 1 John 4 Yellow 0 1 John 5 Black 0 1 John 6 White 0 2 Alex 1 Blue 1 2 Alex 2 Green 1 2 Alex 3 Red 0 2 Alex 4 Yellow 0 2 Alex 5 Black 0 2 Alex 6 White 1 3 Bowie 1 Blue 0 3 Bowie 2 Green 0 3 Bowie 3 Red 0 3 Bowie 4 Yellow 1 3 Bowie 5 Black 1 3 Bowie 6 White 1
Sorry, but I'm not sure what you're asking for. Do you need help fitting the output of the query to the objects in your Windows Form or getting the checkbox selections loaded into the table?
http://imgur.com/a/msB90 I made 2 pictures of really terrible example in Paint. I hope you understand. I can't get the Jeans table to appear in DropDownComboBox and to get those checkboxes
Yep. Just add it to the select in each part of the union, and if it doesn't exist (for whatever reason) then add `NULL AS 'ReferenceNumber'` to that bit of the query. Or something like: SELECT * , CASE WHEN ReferenceNumber IS NOT NULL THEN RefereneNumber ELSE 'None' END AS 'ReferenceNumber'
You and I read the requirements totally differently; I was assuming that OP wanted each record to say whether or not that was the first time that that user had purchased that item. ie, final output would be {user,item,date,is_first_purchase} as opposed to yours, which is {user,item,user_has_purchased_multiple_times}. *shrug*. As you say, OP needs better specificity :-)
Basically, you'll use OLEDB, ODBC, or if Java, the JDBC driver. Here's an example page to get you started: https://www.mssqltips.com/sqlservertip/4709/connecting-a-java-program-to-sql-server/
Just something I've gotten used to. I find it's easier to write out the first statement and then just copy and paste the rest.
Thanks but I think my post was misleading. I should have mentioned also that I plan on using a more popular SQL implementation for web, say MySQL, Postgres or SQLite
The way I access it is below. Everything is done solely within Excel. Head to the Data tab. Under the "Get External Data" section of the tab, click "From Other Sources," which opens a drop down menu. One option is "From Microsoft Query." Clicking it opens a menu box titled "Choose Data Source" with three tabs. Under the Database tab, one of the options is "Excel Files." Selecting it, opens a directory menu, which lets you select the Excel file you have the data in. It can be in the same spreadsheet you're working from. Once you choose the file, choosing ok opens the Query Wizard. Go to the Options button, which opens the Table Options menu. You'll need to make sure "System Tables" and "Tables" are selected. Once they are, hit ok, to return to the Query Wizard, which should show each Sheet within the Spreadsheet as a table. Use the arrow buttons to choose the sheets or columns with the data you want to use SQL on. Clicking the Next button will allow you to choose from a few more features to transform your data. When you've got everything selected, hit Cancel. As counterintuitive as that sounds, a pop up warning will appear saying, "Do you want to continue editing this query in Microsoft Query?" Choose Yes, which opens Microsoft Query with all the data you previously selected ready for SQL. You can write your SQL code by selecting the "SQL" button near the menu bar. Once you've written your query, hit ok to close the SQL menu, then go under "File" and choose "Return Data to Excel." That will drop the results of your query into whichever Sheet and Cell you choose in the final pop up box. Googling "Microsoft Query Excel" should provide more thorough guides too.
As /u/sHORTYWZ stated, that'll still get you going. My fault, I didn't realize I was replying in the general r/SQL sub lol. I work on the MSSQL stack 99.99% of the time so thats where my brain went. For MySQL, you can use the java.sql.* library. Effectively the exact same process as the MS SQL link above, just different classes and methods. 
You're going to have to figure out exactly what it is you're trying to achieve first. Those database engines are not the same as MSSQL that you're talking about in your description, and SQLite is quite different to the server-based database engines too. You mention JavaScript. As /u/yawaramin said, are you really talking about Node.js, or are you talking about JavaScript in the browser? They're different things. In most cases, you're likely to be needing something server-side (PHP, Java, etc...) to do the DB interfacing; you will not be exposing your database server to the Internet directly. Figure out what language and environment you're going to be working in first, and then go from there. 
Unique only exists in Oracle and Access afaik. In Oracle it's exactly the same, not sure about Access.
&gt; What is the difference between distinct and unique in terms of SQL? the only difference is where the DISTINCT and UNIQUE keywords are used otherwise, they mean exactly the same
You can give a try to something like this. WITH res AS ( SELECT * , ROW_NUMBER() OVER (PARTITION BY MMSI, date_trunc('hour', timestamp_insert) ORDER BY timestamp_insert ASC) as rn FROM tbl ) SELECT * FROM res WHERE rn = 1
Thank you! That's so simple and appears to work perfectly! Obviously the table is pretty huge, so it takes forever to run, but that's okay. I'm only planning on running it once a week or so in an automatically scheduled task. Note to self: Learn more about partitioning! It is awesome
You can do 2 things to improve the performance 1. indexes 2. materialize the result in a table and run the query only for the new rows which are not yet materialized. BTW you do not need to materialize all the columns in the table. All you need to materialize are 2 columns MMSI and timestamp_insert from the query above. Than you join the 2 tables when you need the data. with proper indexing that should be fast even in big tables 
I usually do something like: sqlite3 databasename &lt; script.sql Then sqlite3 databasename SQLite version 3.8.5 2014-08-15 22:37:57 Enter ".help" for usage hints. sqlite&gt; .schema CREATE TABLE foo ( ... 
Alternatively, using `first_value` to get the first row of every hour should also work: select id, ais_class, ... from ( select id, ais_class, ..., first_value(id) over ( partition by mmsi, date_trunc('hour', timestamp_insert) order by timestamp_insert ) as hourly_id from tbl ) as sq where id = hourly_id I haven't tested it, but there should be two speed advantages: 1. It's using a subquery instead of a CTE 2. It's using the `id` primary key in the filter criterion which should use the PK's index
Clever technique, I think we can reduce the materialised column requirements to just the `id` column, since it's the PK. That way we automatically take advantage of the PK index.
Does it need to be installed on the SQL server, or can it be installed on my local machine? I've had issues figuring that out. 
The SSRS we have installed is on a separate server than the SQL server I am using for my data. I'm thinking I'll have to install it on that server?
Something like this should work. SELECT DISTINCT ON (mmsi, date_trunc('hour', timestamp_insert)) * FROM tbl ORDER BY mmsi, date_trunc('hour', timestamp_insert), timestamp_insert
And I did think it odd, but I changed my Query Style to match what the company liked. What with extensibility being a bear at other companies, knowing this first hand.....
It sounds to me like you went for a junior role and they give you an awkward test on tricky syntax. If so that's BS. You can learn that syntax, it's not important. What you need to understand is the thinking behind the syntax and a test like this isn't going to reveal that.
Only one SSRS instance is needed and it does not have to be on the same server as any of its data sources.
Moving up to what? What was the job called?
Standard ANSI joins are easier to understand and conceptualise the relationships between tables.
I missed what he was doing on the where, thanks.
EXCELENT! Thank you very much for your response. I hate when they say only a little with what went wrong. BTW [Here is the Page 1 part of the test. didnt link as an album :(](http://imgur.com/Nj8N46J) And yea I guess I need to re-visit the Join problems.
As someone who is just starting with SQL, how advanced are these questions and how important is it that the commands are memorized to 100% perfection? Wouldn't anything that is wrong just throw a syntax error and allow you to fix? Obviously it would be better to know them 100%, but is it that debilitating to get something wrong when you're just doing lookups?
This was for an Entry Software Integration Developer type role.
Was that meant as a reply to me? I described it as intentionally bad SQL not because of the non-ANSI joins, but because there was no join condition in the where clause - bad SQL in the sense that it does not do what it appears to be trying to do.
Ah I see. The answer to the first question is actually ~~27~~ e: 30, whoops. 
I'm not sure I would describe it as a gotcha question exactly, because there are two subparts to that question and the only difference between each is one has the join in WHERE and the other doesn't - it's trying to make you look at the two and explain the difference. 
Yea I see everything that your saying now. Damn,
Just curious, in your first answer, how did you come up with 6? In previous experience, interviewers also look at the chicken scratch you put on papers during an exam. It helps them evaluate your thought process and your ability to brainstorm. You will get them next time. 
I use `*` sometimes in production code when I'm incrementally adding something within a chain of sub-queries: select z.item1 , z.item2 , z.item3 , z.item4 , z.item5 , z.item6 , z.item7 , z.item8 , z.item9 , z.item10 , z.ex1 , z.ex2 , z.ex3 , z.ex4 , case when z1.ex5 as ex5 from ( select y.* , case when y1.ex4 as ex4 from ( select x.* , case when x1.ex3 as ex3 from ( select w.* , case when w1.ex2 as ex2 from ( select a.item1 , a.item2 , a.item3 , a.item4 , a.item5 , a.item6 , a.item7 , a.item8 , a.item9 , a.item10 , case when b.ex1 as ex1 from table a join table2 b ) w join table2 w1 ) x join table2 x1 ) y join table2 y1 ) z join table2 z1 I don't particularly feel the need to specify the list more than twice within a sub-process so long as its clear at the root and stem. Do the same thing for `#tables`. Feels easier to read and edit / modify.
Thanks again for your insight! Quick question regarding indexing... I read that you should be careful with indexing tables that have frequent updates or inserts. As it happens, this particular table gets inserted into relatively frequently... once every few seconds. Do you imagine this might be a problem? 
Thanks!
What is &gt;Division_Name 
Sorry, ignore the greater than signs, I was trying to do it all as code, but apparently isn't correct. I shouldn't touch computers apparently. 
Yeah I thought at first he was talking about shortening the names to e and t. I was like shit I don't know anything
I think this is a poor attitude for an aptitude test on SQL. I know the OP said this is the convention used by the company but I wonder if it is their current practice in production. It's actually a good method to determine if someone can understand potentially legacy code that hasn't been updated. I agree with the sentiment people shouldn't write JOINs in this fashion, but code still exists in this structure and unfortunately people still write it this way.
Sqlzoo.net Use This, then know you're ready before the interview
To format as code, indent each line by four spaces. You may not be able to edit the main post text, but you can post a comment with formatted code. I usually use my favourite text editor to write out the code, quickly indent by four spaces, then paste here.
It's easier to understand the query if the joins are laid out along with the join conditions: select * from employee e, department d where e.department_id = d.id ... vs select * from employee e inner join department d on e.department_id = d.id ... The main reason is that if your query is complex, the join conditions remain with join statement. In the first example if there were 15 other joined tables and the `where` clause had 30 other criteria in it and it'd be difficult to work out what's going on.
Granted, I'm a complete amateur at this, but learning... I didn't even know the first block was possible. I guess I'm being taught the right way. 
Possibly, unless they wanted to test and see if OP had knowledge of other join syntax. It can be helpful to know how to do and some places may have a little legacy code left over that needs conversion or may need to be troubleshot. If they primarily write like that in their shop, then they are probably past help.
It's old syntax for joins. For 10 years, I'm surprised you've never seen this. Were you primarily SQL Server? If you started around 2005, most people had adapted to the new syntax by then. I know it was popular with the older Oracle / DB2 / Sybase crowd.
Yeah, the project I work on consistently started around SQL 2000/2005. I wager if I show that to my managers they will laugh, they have another 10 years on me and did a lot of work with Sybase and DB2. (And were the ones that started this project before I came on board.)
You can kill two birds with one stone https://www.reddit.com/r/SQL/comments/6cn946/comment/dhw7w15?st=J31NXTBU&amp;sh=b00d70b4
The link you gave me was awesome. I have a few meetings to get to before I can come back to this, but I wanted to say that I really appreciate your help through this. Can't be easy dealing with newbs. Also, there isn't anything after your colon above.
Personally I write my code like that, it's ANSI 86 which is what real oldschool hackers conform to. It's easier to read which tables are involved as they're all on one line. Snowflakes started syntax patrolling SQL and came up with the newer sytnax which they claim is to avoid accidentally missing a join....like just don't forget to do your joins...take your ADHD meds or whatever....but kids today need handholding and have to make things as lame as possible. 
Sorry mate, [Here is the other page](http://imgur.com/Nj8N46J)
In PostgreSQL you have a built-in `age` function: # select age('1988-03-12T00:00:00'::timestamp); age ------------------------- 29 years 2 mons 11 days
My explanation is a bit sloppy. I meant that if two different rows contain the same values in all their attributes (e.g. id1 = id2, name1 = name2) only one of these two rows is outputted. Here is an example for the use of DISTINCT with one or multiple columns. http://www.postgresqltutorial.com/postgresql-select-distinct/ Hope that helps. :)
 select DATEDIFF(YEAR, '5-6-85', GETDATE()) as Years , ((DATEDIFF(DAY, '5-6-85', GETDATE())/365.25)-DATEDIFF(YEAR, '5-6-85', GETDATE()))*365.25 as Days Gets years and days, how specific we trying to go?
lol, my university studies (in 2013) taught me to add more tables in the from clause. i started using it in the real world cus im usually only joining two tables and its less typing. Tech Support Jockey Here.
Because someone wrote it and put it on the apple store.
Sorry about that. https://pastebin.com/43CCmnWS
Something like: Select CP.idNumber, STD.firstname, STD.Lastname, Count(CP.PunchTime) AS Punch_Time From ClockPunches CP RIGHT JOIN Students STD ON STD.IDnumber = CP.IDnumber Where (CP.punchTime &gt;= (DATE_ADD(CURDATE(),INTERVAL -10 DAY)) OR CP.punchtime is null) ANd STD.Isactive =1 And CP.usertype = 1 Group by STD.Lastname Having Count(coalesce(Punch_Time,0)) &lt; 2 Order By STD.Firstname The right join means it will include all students even if there's no clock punch record, and the change to the where and having clause allows them to include nulls. 
Just did, yea same result. Do you think it could be an error in how the DB is setup and it may never return these results? I tried to change it to STD.isactive = 0 which would be students who graduated and received no returns, where they definitely would not have clocked in, in this time frame.
If they do not clock in the system does not leave a blank or null. I tried to do it off attendance but unfortunately the teachers do not stick to attendance well.
Won't be a problem in the setup. The nulls, in an outer join, capture unmatched records as well as records with a null in the field. I think I spotted my mistake in the earlier one: Select CP.idNumber, STD.firstname, STD.Lastname, Count(CP.PunchTime) AS Punch_Time From ClockPunches CP RIGHT JOIN Students STD ON STD.IDnumber = CP.IDnumber Where (CP.punchTime &gt;= (DATE_ADD(CURDATE(),INTERVAL -10 DAY)) OR CP.punchtime is null) ANd STD.Isactive =1 And (CP.usertype = 1 or CP.usertype is null) Group by STD.Lastname Having Count(coalesce(Punch_Time,0)) &lt; 2 Order By STD.Firstname e: I'm assuming the punchTime and usertype aren't ever genuinely null
&gt; Do you think it could be an error in how the DB is setu no, it's because in an OUTER JOIN you should never have WHERE conditions on the row that might not exist SELECT STD.idNumber , STD.firstname , STD.Lastname FROM Students STD LEFT OUTER JOIN ClockPunches CP ON CP.IDnumber = STD.IDnumber AND CP.usertype = 1 AND CP.punchTime &gt;= CURRENT_DATE - INTERVAL 10 DAY WHERE STD.Isactive = 1 AND CP.IDnumber IS NULL ORDER BY STD.Lastname , STD.firstname 
The first thing I'd say is try to replicate the problem on your local machine if you haven't already. The next thing I'd say is then enable tracing. Usually, I find a typo in my code somewhere along the way. 
&gt;but now when I try and change it back it simply reverts to what it currently is What is the it, the data or the transaction? If it's the data not changing after your procedure runs, is it possible that the DB has implicit transactions set to on? If so, you need to explicitly COMMIT after your statement.
I've done this type of query to show where employees have missed work. How I've solved the problem was to select all users who logged in in the last 10 days. Join that to all active users. Those results where last 10 days are null. 
As far as I know, unprotected names can be set at any case, depending on the tool. If you want to prevent your tool from changing the case to its preference, you need to protect the name with double-quotes.
maybe this will help you understand -- [The FROM clause](https://www.sitepoint.com/simply-sql-the-from-clause/) it has very simple examples and shows clearly how the joins work
1. Correct, no two rows have the same item + att1 + att2. 2. By top I mean the items with the highest difference value, so difference sorted desc. 3. By last 2 I mean the 2 most recent entries. The top most recent entry will have the difference field populated programmatically by checking the current top's price and calculating it against the new price being inserted. 4. Yes, I am using MySQL. By chance do you know if MariaDB supports windowing functions? If push comes to shove I'd use that and just have to find a new host that provides it. Edit: I guess what could also work is if I could select just the newest unique items (item + att1 + att2) by date and then sort by date + difference.
Depends on your definition of simple, but R's Shiny (with R Markdown and ggplot) is good Or Jasper Reports
ReportServer CE is free and very quick to build basic reports.
It's an alias for the table. You can do n.id instead of name.id to refer to the table/columns inside it. They're great especially if you get longer name objects.
Thank you so much. I was having a hard time finding it out without knowing the name. I was just able to find it when I came back here and saw your answer. I really appreciate your help. Do you know why I would get the following error Syntax error (missing operator) in query expression for this statement? select case when s.Active = 0 then 'INACTIVE' else 'ACTIVE' end as "STATUS", I am new (again, used in 15 years ago) to sql and using a script that I received from somebody else for a data transfer from one program to another. 
What's the full query and error? Typically the error message will direct you to where the issue is. What RDBMS?
So quick question, why are you left joining versus inner join? Is it necessary to have records appear when there is no corresponding value on the right side?
(I have some free time for the next 30 mins so I will see if i can give you some comments in your query) (without knowing the schema at all, it'll be hard to tell if i'm even in the ballpark) Fair enough. Because it is hard to tell what the actual problem is and where to fix it these are the steps I myself usually take to find out where to start: 1. break apart each join first into their own query. Verify that the results are close to what you expect. 2. Review the first query and change the left join to an inner join to see if the row count is the same. if not, then review both sets to determine if you need one of them over the other. 3. do this for each separate union'd query. 4. once you do that, at least for me, I run a transaction plan for each of the queries to see where the longest time takes and determine if things can be adjusted for speed or simplicity. 5. once that is done, determine if a union is in fact needed or if the whole thing can be rewritten into something totally different. I know the above doesn't really help you much, it's just my own thought process as to how i break down lengthy queries like this to better understand what exactly the resulting data should be. Is it possible for you to share each table in rows of like 10 or so? I can probably re-write this in a way I better understand it so you can figure out how you need to change it. If not, no problem, in a general sense, the left joins are going to slow down your process a lot esp if the tables are fairly large and there is no indexing on any of the tables, and other things too... And to answer your question, if you are putting *' ' as columnName* on a integer field, it may not work. Try making it say *0 as whatever* instead. 
I will try that in the morning. Thank you!!!
I've also looked into a k nearest neighbor algorithm, but I'm not sure that I grasp that entirely.
Plain boring .xls files have a 32,000 row limit to them. XLSX will allow essentially unlimited row numbers but exporting still needs a bit of tweaking. Export as a CSV file (Tab separated or Commas, whatever suits best), open Excel and import your csv.
I'd recommend spooling to a csv or some other flat file and import into Excel. You can also do this easy in nearly all the major IDEs (Toad, SQL Developer, RazorSQL, Data Grip) as well so perhaps it would just be easier to run the statement in one of those and simply export the result to the desired format. 
I guess this also makes me question why you need the entire result sets of more than 32k rows in Excel? Is there some manipulation you could just perform via SQL or PL/SQL instead?
Someone in another department wants the data. 
the only real advantage is when writing stuff down it saves space - when i hand write queries short hand on a whiteboard i still sometimes write like this - but if I ever saw code like this in code review I would question whether the person belonged in a code writing environment.
I gotta admit, this is pretty awesome in some ways, but I think it could be a nightmare if not done properly (just like anything, I guess). I also feel like maybe Postgres' Composite data type is a more cost-effective solution to this problem. /2c Edit: I don't think you can do one-to-many with the composite type, but I'm sure there's a way to accomplish it in PG. Maybe via a custom function and a json column.
I think I got it. I used INNER JOIN and found a missing period. Thanks again for your help!! Dan
Tried it and that didn't work. Still getting the same error. I need to keep investigating. Thanks for all the help.
I'd be very interested to see the equivalent in PG.
I don't agree with what you're saying at the top of your comment but thanks for the input. I mean you seem to be telling me that I need to select the data from the view into a table in order to use an index. Kind of not sure why we have them in views at all. 
I'm not sure I follow the exact scenario, but DISTINCT and GROUP BY are generally equivalent. You definitely can get some mileage out grouping sub queries, then joining on unique criteria. The DB engine can take advantage of guaranteed one to many joins, vs many to many joins with a subsequent group by.
Views are analogous to saved sql statements. Views add a layer of abstraction from your underlying tables to make coding easier and simpler. Of course views can do a bit more than queries (for example you can grant privileges on views). But these saved sql statements read from regular tables. Views use indexes on tables, too. But views do not have their own indexes. There are such things as a "materialized view" in Oracle and an "Indexed View" in SQL Server. These are almost more like tables than views, albeit they are tables with some additional features. You didn't tell us what database you are using, but chances are good that you need to check your index strategy on your tables and/or tune the sql in your queries (including the select statements in the view definitions). Start there, and then after you have done that you can decide whether you need some kind staging table or materialized view of some kind.
Thanks that is helpful. Our strategy is to index the tables the views sit atop but no one has written a process like this, yet, which joins all of the views together. 
I have a fairly trivial process which is currently taking +90minutes to execute because I suspect that the indexing is not appropriate.
Is there a performance loss if you index the table and then point a select * from view versus selecting from the table directly if there involves multiple joins.
OK. If this isn't one single 90 minute query, have you analysed which step is causing the biggest wait? Have you looked at your execution plans for the expensive queries?
I don't know if I can get it down to one-to-many - is there an advantage to few-to-many over lots-to-many? &amp;nbsp; Each child belongs to multiple parents, and sometimes the child appears multiple times in the same parent. Each item&amp;project combination only appears once, but maybe 15% of the items are associated with multiple projects. &amp;nbsp; The only thing guaranteed to be unique in any of this is the RowID in the parent/child table. :P 
I'm new here, but couldn't you just do datediff(birthday, now())/365 and then round it down?
There are two main efficiencies gained by one to many joins. First, many to many merge joins have IO overhead because it temporarily stores records in case they are needed for the next record or two. This isn't necessary for one to many. Second, one to many joins can guarantee the record set won't grow, so there's less guesswork in record estimates. If I understand your scenario correctly, I think you can first create a delimited list of child projects per parent project, then join to project item. SELECT pi.ItemID, pl.ChildProjectList FROM (SELECT pi.ItemID, pi.ProjectID FROM dbo.ProjectItem AS pi GROUP BY pi.ItemID, pi.ProjectID) AS pi INNER JOIN (SELECT p.ProjectID, STUFF((SELECT ',' + CONVERT(varchar(10),cp.ChildProjectID) AS [text()] FROM dbo.ChildProject AS cp WHERE p.ProjectID = cp.ParentProjectID GROUP BY cp.ChildProjectID FOR XML PATH('')),1,1,'') AS ChildProjectList FROM dbo.Project AS p GROUP BY p.ProjectID) AS pl ON pl.ProjectID = pi.ProjectID;
&gt; the data is in a view and not a table I hope by now it is clear from the other comments how this is not really true since views do not hold data.
There is zero performance difference between using a table or a `select * from table` view. They are exactly the same in every way. Suggest good indexes on the underlying tables, and your queries will get the full benefit.
So in theory can you index a view? Or is the question itself asinine? I see the option in the object explorer which suggests to me that I could add them if I wanted to, but I've never tried or had a reason. In practical terms what would be the difference in indexing a view vs. a table?
This is exactly what I was trying to learn here. Thanks. Fuck. I need to use #tables and rewrite everything to fix this. 
Either you're being locked down by some really annoying security/access policies at your org you haven't really explained, or you're doing it wrong. Also be aware that creating indexes on temp tables is in itself something that has a cost, it may very well not work out any faster - to create the index the db has to do a lot of sorting and matching anyway. 
&gt; Either you're being locked down by some really annoying security/access policies at your org you haven't really explained, or you're doing it wrong. I'd bet on the former but what do I know. I have my answer though.
I really don't care about the io if I get better performance when my process runs. So you're saying that indexing the views directly is a possibility?
This is my preferred way to do effective date joins. CREATE UNIQUE CLUSTERED INDEX IX ON #reference (ReferenceID, EffectiveToDate DESC); SELECT * FROM dbo.MainTable AS mt CROSS APPLY (SELECT TOP(1) r.ReferenceValue FROM #reference AS r WHERE mt.ReferenceID = r.ReferenceID AND mt.CreationDate &lt;= r.EffectiveToDate ORDER BY r.EffectiveToDate DESC) AS r;
Potentially, at the cost of more io during DUI operations. [overview](https://www.brentozar.com/archive/2013/11/what-you-can-and-cant-do-with-indexed-views/) you can find more including example use cases on Google. Unfortunately there are caveats with anything so it may or may not be an option for you.
So I have a chain of procedures that all start off with A: which is a cross apply that calculates all of the possible permutations across all of the views for specific high level data points (columns), then each sub-process joins to this #set (on more than just date), but then each of these sub-processes are views (non-indexed) which are joined together, which are *'s pointed at tables which may or may not have the fields indexed that I need. And I'm developing in a /dev/ environment so I have to write my code as it will execute in production, but have no idea what performance will be like in production only that I'm spiraling into execution time hell at the moment.
http://i.imgur.com/jCZ24KK.png
I'd need an actual execution plan to identify specific opportunities for improvement. General advice has already been thoroughly covered in this thread.
Cheers for this. Just needed a push towards ROWS UNBOUNDED PRECEDING function. CASE WHEN LAG(DATA) OVER (PARTITION BY emp ORDER BY START_DATE) &lt;&gt; HR OR LAG(HR) OVER (PARTITION BY emp ORDER BY START_DATE) IS NULL THEN 1 ELSE 0 END as GRP_INC Placed the query into a sub query then ran SUM(GRP_INC) OVER (PARTITION BY emp ORDER BY START_DATE ROWS UNBOUNDED PRECEDING) which got the correct grouping and allowed me to run the min/max functions over the new grouping. Thanks again for the help.
Dumb question, but what is the benefit of having multiple LDFs?
Nothing at all, log files are written to sequentially so it has no impact on performance. The only reason we used multiple ones is because our drive filled up. Source: https://www.sqlskills.com/blogs/paul/multiple-log-files-and-why-theyre-bad/
Yeah, but that's only if you still have the log files. I'm talking about only having the MDFs/NDFs with no log files. It's possible to attach with a single log file, but not multiple ones
If I detach my test db and then reattach it with only the first 3 lines of the script it attaches without error. What kind of error are you getting?
*File activation failure* *The log was not rebuilt because there is more than one log file.* The error is pretty telling, I'm just confused as to why it can rebuild one, but not multiple ones if it's the same kind of file. Are the log files still there though? Before you attach, delete the log files. You'll get an error and won't be able to restore. If you try it again with a database with a single log file, delete that log file and then attach just the MDF and it'll work. 
&gt; both asc/desc I wouldn't recommend this indiscriminately. Indexes speed up reads and slow down writes (in general. I've seen indexes that slow down reads, but it's not very common). Like any kind of performance optimisation, whatever you do to speed up a particular query is going to have flow on effects to other queries. If you're about implement an optimisation that is *outside* the query itself, you really need to consider the whole environment. If you have a query that could be sped up 10x, but it only executes once a month, or out of operational hours, etc. - is it really worth implementing? What if it causes a query that runs 1m+ times a day to run 10% slower? The reason every DBA's most used phrases is "it depends", is because you always have a lot of factors to consider. Query optimisation is no different.
I got stuck designing a series of reports for Acctg. I haven't done this stuff in years! After I've determined the report packages, I'm going to write requirements to move the data and the reporting into Cognos. My solution is short term. How would I write the dynamic sql in there? Do you have an example?
Thank you!
&gt; the report is just running that 1 select statement Controversial opinion time: doesn't matter. Put it in a stored procedure anyway. Your DBA will thank you and your future self will thank you when the report requirements change from this "simple" query to a 300-line behemoth. &gt; it's only accessible to company employees Are you certain of that? Networks get compromised daily. &gt; i'm not worried about SQL injection. You should **always** be worried about SQL injection. I screwed up my formatting (it's fixed now) which may have made it hard to read. I don't have access to the code I wrote to do this anymore (changed jobs) but these links should get you going: https://www.codeproject.com/Tips/584680/Using-comma-separated-value-parameter-strings-in-S https://social.technet.microsoft.com/Forums/sqlserver/en-US/0ead7ceb-3fdd-4625-aa82-1d4195f984b1/passing-multivalue-parameter-in-stored-procedure-ssrs?forum=sqlreportingservices https://www.mssqltips.com/sqlservertip/3479/how-to-use-a-multi-valued-comma-delimited-input-parameter-for-an-ssrs-report/ https://stackoverflow.com/questions/17244656/using-ssrs-report-to-pass-in-a-param-value-the-is-a-csv-list-to-sql-dataset
Fair point, I should have mentioned the DUI impact (I did in the other thread about 10minutes prior /facepalm).
Yes, I agree. I bit the bullet and rewrote the process that way and it takes seconds now. I was really hoping I wouldn't have to do that, but deep down in my soul I knew it would need to happen at some point.
nice ! thanks for the info.
That math will come up one day short for every February 29th between the two dates.
I think the case statements might be fine, it's all those isnull statements I thought could be the problem.
Disclaimer: I was never actually taught SQL so I don't really know what I'm doing, and I don't have write permission on the database. I have not tried to look at an execution plan before. Do I just turn on "include actual execution plan" before I run the query?
It seems to be the case statements. If I get rid of those then it gets further along. I think that I need to get a SQL person here to help me out. Thank you so much for all the help you have provided me. 
There is no indexing at on the item/project table right now, and there is a single indexed "id" column in the child/parent table that I don't use at all. I'll bring up indexing to the db admin again - I know we tried to index something a while back that was causing a big slowdown every time new rows were inserted. It sounds like it's worth revisiting.
Oh, and [here](https://www.mssqltips.com/sqlservertip/2866/sql-server-reporting-services-using-multivalue-parameters/)'s a little tutorial from 4+ years ago (nothing has changed, it looks like!) for this exact use case, I think.
TIL I've been a pirate my whole work life. LOL
Access supports the IIF() function which essentially a short handed variant of a case statement. So you can likely convert the existing case statements to nested IIF() I can't recall the other limitations but it's access so there will be some ;) Alternatively SSIS should be able to perform inserts​ into Access in 32bit mode so you could setup an etl process. BCP could be used to dump .CSV files for importas a final resort.
I'm certainly no expert, but I might generate a field that combines those values and converts that combo into a date. Then compare those dates to [Now - 12 months]. Something like: SELECT CDate([month1] &amp; "/1" &amp; "/" &amp; [year1]) AS DateCombo, Store, PartNum FROM Table1 WHERE DATEDIFF('m', CDate([month1] &amp; "/1" &amp; "/" &amp; [year1]) , DATE()) &lt;= 12 ; I just tested it and it seems to work... good luck!
Just a follow-up to include sales. If you want to sum across all sales in this period, skip the DateCombo in the SELECT portion, as that will return individual records. Instead, add a SUM(sales): SELECT Store, PartNum, SUM(sales) FROM Table1 WHERE DATEDIFF('m', CDate([month1] &amp; "/1" &amp; "/" &amp; [year1]) , DATE()) &lt;= 12 GROUP BY Store, Partnum ;
Thank you for the response. I'm going to give it a shot now. So for future reference, is oracle case-sensitive depending on how the table or view name is written? EDITED: So that definitely solved that issue. Would I add extra views to the query statement by using the operator in this format if I didn't have the option of using dba_views? Select view-name from user_views, all_views where view_name like '%DBA\_%' 
In reply to your edit, no. Just use all_views if you cannot see dba_views. The only reason to use user_views instead of all_views is if you only want to see views that your account owns, meaning views that were created in your schema.
&gt; I've tried setting foreign keys but keep getting errors with my syntax, but my main problem is I have no idea how I should separate the | from each category and split them up. I don't really understand what you mean with setting foreign keys in that context. When you say you are using JDBC, I suppose you can do the splitting in the Java application? If so, just use the method String.split to separate the different categories and add each one of them individually. String categories = "Action|Drama"; String[] parts = categories.split("\|");
Apologies, I should have specified. It's part of a JDBC project but that's largely irrelevant to the question as I just need to know what the query should be in SQL developer. I guess with the foreign key thing is, I know that my MovieId and CategoryId will be foreign keys for my MovieCategories table so I thought I had to do something like set those in order for it to sync? I'm not entirely sure. I thought about splitting up the movie categories in Java prior to uploading them to the server but that proved to overcomplicate things as some movies have one genre, whereas others may have two, three, or four. 
I strongly suggest to do the splitting inside the Java application. In my opinion it is much easier than doing this with SQL where you most likely need a stored procedure for that. The number of categories does not really matter, just use a for-loop and do a single INSERT command for each category of the movie. String categories = "Action|Drama"; String[] parts = categories.split("\|"); for( int i = 0; i &lt; parts.length; i++ ) { // SQL-Insert for each category } &gt; I guess with the foreign key thing is, I know that my MovieId and CategoryId will be foreign keys for my MovieCategories table so I thought I had to do something like set those in order for it to sync? I'm not entirely sure. Yes, that's right. The thing is, you only have the name of the category at that point so you will have to do a SELECT to find the corresponding ID for it. Assuming $movieID and $categoryName are the variables with the values in your Java application the INSERT could look something like this: INSERT INTO MovieCategories VALUES( $movieID, SELECT id FROM category WHERE name = '$categoryName' );
Can you explain your schema a bit better? Does every agent ID have exactly 1 alias? How do you know jsmith in the network table is jsmit1 in the other table? If you can write a rule around that, you can join on it, but you need some way of relating them; ideally a proper foreign key identifier.
Maybe i'm confused but the Categories are already listed in the CSV file. So when I use a BufferedReader and place it into the database they are already there, not split. I'm not inserting the categories into the DB. 
&gt;I'm wondering how to make my own GUI to make it easier to access stored data Going to need more information. Which stored data? And how are you wanting to interact with it? What you've said in literal terms could apply to thousands of business applications out there right now. Do you want to write your own SQL client? It's quite easy to do a very basic "runs the statements you type" sort of thing but doing it well is another matter. If you want to play with some method of visualising data then that's do-able to. Plenty of existing tools, but it can be fun to build something yourself (if your primary goal is learning). 
&gt; I'm not inserting the categories into the DB. Why not? I thought you wanted a table in your DB that holds the categories. &gt; So when I use a BufferedReader and place it into the database they are already there, not split. Ok, so you are reading a line from the CSV file with your BufferedReader. What exactly are you doing when you say you "place it into the database"? Do you write the whole line into the database? Or do you split it up into the separate attributes (movieID, movieTitle, category)? What SQL-commands are you using exactly? If you already do splitting for the attributes, splitting of the categories is almost the same. If that's not it, I seem to be misunderstanding your problem.
&gt; ... access stored data. Maybe you want Microsoft Access? You can use it to make data-driven graphical apps. But only on Windows. If you want to go cross-platform your best bet is a web application framework like Django or Ruby on Rails.
Okay, thanks. How do you know that jsmith in one table relates to jsmit1 in the other? Can you write a rule, in SQL, that describes how you know that? For example, maybe all of them share the first five letters, and you can join on left(a.col, 5) = left(b.col, 5). (Note that the performance on this will be horrible, so only do this for very simple queries on very small tables.) The *real* solution is to set an indexable ID in both tables that relates them well. This is part of what's called normalization - defining relationships where necessary and appropriate so you and the database engine don't have to make guesses. So if the network table is called network_logon then the alias (jsmit1) record might have a network_logon_id corresponding to jsmith's primary key on the network table.
Thanks to your help asshelmet, I was able to finish my first homework/project for my online class. The finishing code ended up like this: select view_name from dba_views where view_name like 'DBA\_%' escape '\' or view_name like 'USER\_%' escape '\' or view_name like 'ALL\_%' escape '\'; I kept trying to use AND operator but was getting no results, and then I forgot that and would be looking for all three of those conditions together, so I switched to OR. 
This, ideally you have a new table with both id's per person so you can join on that between both to give it a common value. If there is some kind of method to the naming then you can do a number of string functions to compensate which is not efficient, but "works" Case statements in your joins, substring, charindexes, etc. These all work, but are dependant on naming coventions of the values you want to join on.
^(Hi, I'm a bot for linking direct images of albums with only 1 image) https://i.imgur.com/zwfWY1Y.png ^[Source](https://github.com/AUTplayed/imguralbumbot) ^| ^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^| ^[Creator](https://np.reddit.com/user/AUTplayed/) ^| ^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^| ^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20di4vl83) 
Gotcha, thank you both. Unfortunately I have no ability whatsoever to add to/modify the tables in questions and they already include 1000's of employees as well. It seems like the only option I have at this point is to match the first seven characters and hope that doesn't eventually become and issue (it will, just a matter of time before two employees have the same first seven letters in their network IDs). 
Ruby on Rails might be a good place to start. 
Sr. DBA here. There are actually a few categories of DBA. The primary ones seem to be Operational DBA and Development/Application DBA. There are of course hybrids, and everyone's role varies depending on the need of their department. Operational DBA's usually handle the System side of things: Security, backups, replication, etc. Development DBA's usually handle the code base itself, with some interaction with Security, Backups, replication, Code Repository, and development/tuning. This is the category I fall in (and the one I assume you're interested in as you seem like you want to code). I'd suggest checking out [BrentOzar's](www.brentozar.com) site, which has cool blogs and the Office Hours Podcast (which is awesome). My favorite DBA book so far: Database Administration: The Complete Guide to DBA Practices and Procedures (2nd Edition) https://www.amazon.com/dp/0321822943/ [Microsoft Docs](docs.microsoft.com/sql) : This page is an excellent resource as well. You can download SQL server developer (AKA Express) and have your personal server on your machine. You can use Brent Ozar's guide to install it, then download the Stack Overflow database from his site. to get some data.
reddit
We call them logical DBAs and physical DBAs where I work. The LDBAs we call on to update tables, etc. 
Thumb ===&gt; Ass
One of our system DBAs is also our senior IT, she spends a lot of time fighting fires on both fronts because the company refuses to spend money to upgrade and be stable. 
Please read through phptherightway.com Do not use any mysql_ functions. Use PDO. 
Generally you're going to want to store your data in Unicode, then accept that a handful of hyper-obscure languages aren't in Unicode. Other solutions tend to be silly. However, if you actually need a silly solution, you can just store binary and have the client interpret it in whatever arbitrary way makes sense in context.
Check out OutSystems. A RAD visual programming language. 
That's what we're agreeing with too. What's the purpose or benefit in adding a delete column? Is this a straightforward answer or does it pertain to her specific data set for her assignment? Edit: We're saying it can allow you to modify data at a later point in time. Is this correct?
With [LIKE](https://technet.microsoft.com/en-us/library/ms187489.aspx) keyword
Also remember if you're looking for strings regardless of case, convert both strings to UPPER or LOWER
1. You should only include columns that are NOT being aggregated 2. Wild cards should be used as little as possible, unless doing something like SELECT * FROM in a view, and even then qualifying the names is nice.. 3. Yes, it sorts based on order of the columns as well as asc/desc. 4. It wont make much difference, but full name qualifications solves 4B, and makes the code more readable ;) 4B. Don't use *, qualify the column name. This would be a valid solution SELECT movie.title, count(casting.actorid) actors FROM movie INNER JOIN casting ON movie.id = casting.movieid WHERE movie.yr = 1978 GROUP BY movie.title ORDER BY count(casting.actorid) desc, movie.title asc edit: sigh, our glorious postee runs away 
this is valid -- SELECT foo FROM .. GROUP BY foo this is also quite valid -- SELECT foo FROM .. GROUP BY id , foo assuming that `id` is a primary key, these two GROUP BY clauses might not deliver the same results, depending on whether there are duplicate `foo` values this, however, is not valid -- SELECT id , foo FROM .. GROUP BY id note that MySQL will still run it. despite it not being valid
Only an issue on SQL Server if you're using case sensitive collation (which is pretty rare in my experience).
He didn't say he was using SQL Server, Oracle, or such or even if the server is set for case sensitivity specifically, so it's wise to assume the worse case here and just do a conversion.
Without getting into software, yet, it sounds like you're describing a hardware solution where there are physical servers around the globe which contain localized data, and that these servers will connect to a central server to sync/update the network. Correct?
Thank you very much for your detailed answers. As for your question concerning 4a where you ask for examples that lead to different result: I did not manage to reproduce it, so I guess I have messed up something.
Hi, thanks very much, will look into those. Wonder if AWS have something coming
Agreed, I've used it to solve some tricky problems that would have otherwise required a decent amount of development time. Even paying for an [Azure SQL Database](https://azure.microsoft.com/en-ca/pricing/details/sql-database/) can be really affordable for small databases like this. Being able to access them for anywhere with internet can make small tasks less tedious. Basic: * $60/year * 2GB max size * 4GB backup storage included * Backups kept for 7 days default Standard * $175/year * 250GB max size * 500GB backup storage included * Backups kept for 35 days default 
Well, two things (note: not an MS SQL expert, but I'm looking at this from a higher-level perspective). One: why can't you do sharding? Do you have a lot of write-mostly tables where *all* the users need access to *all* the rows? Second: Have you actually tested load times of the application? I'm guessing it's a 2-layer application where the whole page gets rendered each time, after doing a bunch of SQL queries. Unless the page itself is very heavy, the extra 100ms RTT should not bring you out of your latency SLA. If it it, did you (or they) evaluate SPAs w. API access?
The answer to this question is going to depend on what type of data you are working with and what type of modeling you are trying to achieve. I believe you are correct about not needing SSRS for what you are trying to accomplish. I like to use SSRS when their is a scheduling component to the reporting (ie run at this time every day and email to abc@xyz.com).
TYVM. I have looked at PowerBI extensively. I was not able to clearly find a way to import those charting objects into a VB application. MS is also trying very hard to keep it (PowerBI) all cloud-based or, at the very least, requiring a license for enterprise (local) production reports. I had some success w/ PowerBI Desktop, but wanted to see if anyone had input to this end. I couldn't find any way to get them (PowerBI, Visual Studio) to talk to eachother aside from literally automating screenshots of the PowerBI report and including those images in the VS application, which I am not above trying. Going that route, it may be easier to use PowerBI and/or SRSS and just grabbing the images I need. That's largely the purpose for this post in general. Just looking for the path of least resistance, least amount of buggy application-layer automation, and it's been brutal, so far. ty. 
Well I wanted to use a Vis Studio application as the final frontend. That's the compatibility I was having trouble tracking down (b/t PowerBi Desktop &amp; Vis Studio application)
I wanted to continue to use the same front end for versatility down the road. Since this is clearly exploding past anything I was remotely anticipating, I am certainly open to other ideas. I was originally looking at *only* using PowerBI as a replacement but then found Report Builder, SRSS, and all the countless variations of similar historic versions of these reporting tools. Can you point me to any related resources? tyvm
SSRS is the platform for delivering reports, and you want SQL Server Data Tools (which is basically visual studio) for report authoring. https://docs.microsoft.com/en-us/sql/ssdt/download-sql-server-data-tools-ssdt. Get the latest version of that, it's backwards compatible as far as SQL 2008 iirc. Those should be all you need.
Thanks, I already have everything installed. I was wondering if you could direct me to any of those canned reports for reference. thx again
Not sure what you mean by canned report. Try having a play: open up your SQL Server Data Tools and create a new project - if you're familiar with the Visual Studio environment it shouldn't be too hard to muddle your way through. Once you've got it linked to your Reporting Services db you can publish/deploy your reports straight to there, and manage subscriptions through the reporting services web portal thing.
if you are doing extremely write and read heavy applications without calculations Cassandra might be worth looking at. I should settle all your latency requirements and you can globally distribute it. Its important to note that it is not anything like any RDBMS. but it scales and is quite fast. Reddit uses it to power their backend. 
An alternative approach, if needed, would be to use Kafka to distribute the transactions to the various data centers. It's very fast, so you should be able to replicate the data quickly. It can act as a log of transactions, so you can use it to replay transactions if you store enough of the data. Anyway, Kafka has the ability to replicate to different data centers. This may be faster than any replication that a database provides. I'm not sure how you would implement this assuming writes can come from any data center, but I'm sure there's a way. If a pure database solution proves to be too slow, this may be worth looking into.
nuodb.com same theory as Cockroach DB but far more mature. Used with a couple of POCs I did and loved it.
Yes one end of each of the arrows definitely looks like crow's feet notation. But the other end is always just an arrow head, which gives no information about the cardinality or participation of the table it points to. With regards to the asterisk, the second article you linked is actually of the UML notation and not of crow's feet. In UML, they put explicit mins and maxes for each participating relation, with an asterisk meaning no upper bound. I'm just going to assume the weird notation I saw is using asterisk to mean 0 like you said.
The learning curve can be steep, but it doesn't have to be. Which Cognos product specifically? I can tell you about BI and Framework Manager.
&gt; Hi y'all, I'm currently working on constructing a sales report for a store, but I'm struggling with a problem when multiple items have been sold at the same location. I'll draw it out to see if I can explain this a little better. &gt; So here would be the sales for this year for an item with style number 21 and stores 3 and 10 &gt; 21 L 3 $25 21 M 3 $125 21 L 10 $100 &gt; when i run the query, i get something like this for money made. &gt; 21 L 3 $25 21 M 3 $150 21 L 10 $100 &gt; So whats happening is when multiple items are sold at the same store, one column will have the total sold for this store, and one will have the actual info for the size. This holds true for thousands of items, but what I want to do is select the larger value when the stores are the same. &gt; If someone could help me out that would be wonderful. You have to provide the structure or else we are guessing. Probably a derived table could help you achieve your goal. 
Yes, 2 layer app. The whole page doesn't get rendered each time, just the data. We're looking to test a centralised database, perhaps in Singapore, with web front-ends in local region and VPN's through to the databases. It's all in AWS so hoping the AWS backbone between regions will drive some lower RTT's. Sorry, can you extpand on 'SPA's with API access'? 
ok I will try that. Thanks a lot
https://docs.microsoft.com/en-us/sql/relational-databases/replication/sql-server-replication Takes a couple of seconds for transactions to replicate on our setup. Might fit your requirements.
I don't think so? I have no idea what I need to do &gt;_&lt;
Correct, by using UPPER or LOWER, the resulting query will be slower when working on a large table that has millions of rows. Choices related to the best option for the given situation should always be considered, never what is simply convenient. You should notice that the OP didn't provide many details related to what he is looking for specifically. He didn't mention he is doing a comparison with a large table or small table. He also didn't request the fastest performing string comparison. He didn't say he was concerned about index usage. It is up to the OP to decide what information will be the best solution for his situation and problem because we don't know anything about his/her needs.
You can create a table value function to split the input string. Example using the name SplitCSV CREATE FUNCTION [dbo].[SplitCSV](@input AS Varchar(4000) ) RETURNS @Result TABLE(Value varchar(255)) AS BEGIN DECLARE @str VARCHAR(255) DECLARE @ind Int IF(@input is not null) BEGIN SET @ind = CharIndex(',',@input) WHILE @ind &gt; 0 BEGIN SET @str = SUBSTRING(@input,1,@ind-1) SET @input = SUBSTRING(@input,@ind+1,LEN(@input)-@ind) INSERT INTO @Result values (LTRIM(RTRIM(@str))) SET @ind = CharIndex(',',@input) END SET @str = @input INSERT INTO @Result values (LTRIM(RTRIM(@str))) END RETURN END Then you can use the query: select distinct ''''+CONVERT(varchar(8), ord_hdrnumber)+''',', ref_number from referencenumber where ref_numbeR in (select value from dbo.SplitCSV(@RMA))
I'll write you something after I get home from work.
Hard to tell exactly what your goal is but I would do this: Back up the databases being used. Uninstall SQL Server 2016 Eval Install SQL Server 2016 Whatever license you purchased Restore the database you backed up. If the above is going to affect your setup, just install the licensed version over the eval version. (You may need to either change the instance name or use a new one to avoid conflicting instances on the same machine)
Use PL/SQL
Thanks a lot! I was able to import that but the adventureWorks file was .bak I am still unable to import any .sql file and I need to do that. I tried the MS NorthWind database but that also gives an error when I execute the sql query. Can you please guide me towards a working .sql database? And secondly, why can't I run the queries? I am not changing anything, I just download the file and run the query. Version incompatibility maybe? 
 First issue is you're mixing up database engines. The link to the first sample you're trying to import is for MySQL not MSSQL so there will be issues running it (syntax is different between MSSQL and MySQL) The second one, you need to run the Model first to create the tables, then the Data script to populate them. As said elsewhere, for SQL Server the best bet is to grab AdventureWorks from MS and restore the backup. I find that restoring from scripts instead of a full backup, can be finicky ... in MSSQL, a .sql file is just a script, not an actual database, you run a .sql script against an existing database. 
Since you mentioned looping... here's a basic way of doing it. It would be a poor choice for accomplishing this. Script is posted for demonstrating a loop only (may have errors - not tested). BEGIN FOR CD IN ( SELECT Unique_ID, Country_Code FROM Country_Details WHERE Question_Num = 4 ) LOOP UPDATE Country_Details SET Total_Num = ( SELECT COUNT(*) FROM Accounts WHERE Nationality = CD.Country_Code AND CType = 'Convenience_Store' ), Total_Value = ( SELECT SUM(Balance) FROM Accounts WHERE Nationality = CD.Country_Code AND CType = 'Convenience_Store' ) WHERE Unique_ID = CD.Unique_ID; END LOOP; END; / A simpler way of updating the same thing is like this: UPDATE Country_Details SET Total_Num = ( SELECT COUNT(*) FROM Accounts WHERE Nationality = Country_Details.Country_Code AND CType = 'Convenience_Store' ), Total_Value = ( SELECT SUM(Balance) FROM Accounts WHERE Nationality = Country_Details.Country_Code AND CType = 'Convenience_Store' ); I'm sorry if I have misunderstood your question entirely.
Thanks! I'll try this approach and see where I land. 
thank you for the sources
this is an awesome site. Thanks
Most welcome
I'm not sure that the sub-query in the WHERE clause is necessary. If I understand the requirement correctly then it should only hit rows where question_num = 4 regardless if there's a correspondent in ACCOUNTS. The correlation to nationality is made in the first sub-query so you're already covered for that.
Thanks for the suggestions! I'll give these a try.
This sounds applicable to all IBM products in general.
Thank you, this is extremely helpful!
Lol downvoted for suggesting PL/SQL 
I know you didn't mention this specifically, but do not use dynamic memory allocation or thin provisioning. 
Great info, thanks a lot. Mostly reinforces what I was reading and put gave me some more recent paths. After sitting with the Sharepoint admin, turned out to be nothing to do with Named Pipes anyway (she was seeing an error that all reports pointed to named pipes but it wasn't) so back to the drawing board.
Yeah, because it's the wrong approach.
I tried this after I posted. The amount of data I'm pulling appears to be too big for our db to handle a correlated update. Looping might be the only way to hammer this through. Either that or creating a script that generates 3k update statements that an analyst has to paste and run.
Probably downvoted because the PL/SQL approach would be row by row (slow by slow).
Yep, I use Oracle mainly as well, but I came from SQL server, which yells at you and takes a dump on itself if you DARE to use an Alias. It basically treats it like an alias in a group by.
I usually get around this by making a control/fact table. The table in this case would have all students' names or IDs. Then I would do a full outer join to that table, so that the student names are ALL always returned as a record, regardless of whether they have punched or not.
dude, did you actually test any of /u/SwitchbladeAli's suggestions?
It seems I misread his post. That's on me. Thanks for calling me out to look over it again.
This worked perfectly. Thanks so much for the assistance. Also, apologies for not taking a closer look and testing out first.
A couple years ago, my company decided to switch software systems in order to go to something more "common" across the various arms of the company (nice thought, except it totally didn't work out that way). As part of it, we switched from being able to run SQL directly against a local server to having the server moved offsite (consolidation!), only a couple people having any access directly to the database whatsoever (security!), and the rest being forced into some level of COGNOS access (convenience!). I've been working with only access to Query Studio, and I have nothing good to say about it, though I'm sure part of that is just how they've chosen to set it up for our company. If you want to just pull some simple data with only basic criteria, then it can usually get the job done. However, anything even moderately complicated is completely out of the question. Everything is in separate sections (they refer to them as "namespaces," if I recall correctly), and you absolutely cannot cross between them when selecting fields. Want data from both? Run two queries, and stitch them together in Excel or something. They also made the confusing choice of allowing basic LIKE operations in one side, but not in another where such queries would be more common. It's just so bad that I had my colleague, who does have db access, do a dump of all the table and field info for the new system, and I'll end up blindly writing queries in Notepad++, hoping I'm choosing the right fields when I come across something I'm not familiar with, and send them off to be run. It's incredibly inefficient, makes you a burden to others, and is time-consuming and more difficult than being able to do some simple trial and error on your own, but for many things, it's simply the only way. Also, I'm not a db admin or anything, so I don't know the specifics, but on a weekly basis they send out alerts that some database in COGNOS failed to update as expected, and everything's delayed. Usually doesn't impact me, but it doesn't give me confidence. So yeah, I'm a bit salty on the subject. I long for the days of being able to run SQL directly. We've got a saying at my company for anyone who has to deal with this: "COGNOS Is Crap."
Typically you'll self-join to form a hierarchy... or at least that's where I most commonly see it. Basically you have: employeeID, employeeName, managerID Where managerID matches an employeeID and you want to find the name of an employee's manager.
Ugh, I hate when people do that. 9 times out of 10 it's some awful internal Java app. You end up with a non-normalized database and an environment that doesn't scale well (most of the time). I like to refer to such designs as incestual. 
I frequently join a table to itself to help identify date overlaps or duplicates. i.e. if I expect a table to be unique based on a set of three columns but that isn't enforced by the schema and someone decided it'd be clever to insert bad data directly through the backend and now I have to identify every case and clean it up. With SQL I think sticking to the every-day techniques (INSERT, SELECT, UPDATE, DELETE) tends to be better for maintainability and performance than doing things in a clever way. So for me, some of the more important "more than basic but still basic" ideas aren't really code related. I use MS SQL Server at my work place, so if I was looking for intermediate techniques I'd consider [Included Columns for Indexes](https://docs.microsoft.com/en-us/sql/relational-databases/indexes/create-indexes-with-included-columns) or [Making sure my queries are Sargable](https://www.brentozar.com/archive/2010/06/sargable-why-string-is-slow/) SQL was built to be simple, so when you start trying to make it more complicated by throwing around MERGE statements or CTEs or other complex tools it tends to be more trouble than it's worth. Although, the occasional OUTPUT clause never hurt anybody.
Yeah Window functions were the first thing that popped into my head for this question. Handy little buggers, but easy to miss if you are new to SQL.
Popular RDBMSs like MySQL [still not supporting CTEs and window functions](https://veekaybee.github.io/mysql-window-functions/), unlike PostgreSQL and virtually all commercial databases, probably contributes to this. (Only MariaDB 10.2, [which got a stable release just 10 days ago](https://mariadb.com/kb/en/mariadb/mariadb-1026-release-notes/), finally added both of those features, which aren't an option if you're still coding for compatibility with older versions or MySQL.)
Just to add to the above - I would suggest either using GETDATE() or GETUTCDATE(), depending on your database / timezone since daylight savings time might cause fun problems that don't show up in testing depending on the time of year...
Seriously? That blows.
Depending on why DB your running there could be different ways to do this Oracle: sysdate SQL Server: CURRENT_TIMESTAMP MySQL: current_date() Should note that implicit conversions are rarely done with dates so both your provided variable and current date will either both needed to be in a date format or both in a string format of some sorts. 
use **CURRENT_DATE** -- it's standard sql, supported by all database systems that support standard sql 
I guess the one I use most frequently is UNPIVOT. We have some order entry systems that break recurring and non recurring charges into separate records and others that have both on the same record. UNPIVOT lets me blow out the records with multiple charges into multiple records so I can normalize my tables that combine records from all systems.
That's what we do. 
One more try: SELECT Q1.MovieTitle , Q1.Year , Q1.CatDescription FROM ( SELECT DISTINCT Movies.MovieTitle , Movies.Year , CategoryDescription.CatDescription FROM Movies LEFT JOIN MovieCategories ON Movies.MovieId = MovieCategories.MovieId LEFT JOIN CategoryDescription ON CategoryDescription.CategoryId = MovieCategories.CategoryId ) Q1 INNER JOIN ( SELECT MovieId FROM Ratings LEFT JOIN Users ON Ratings.UserId = Users.UserId LEFT JOIN Occupation ON Users.OccupationId = Occupation.OccupationId WHERE Rating &gt; 4 AND Occupation.OccDescription = 'Programmer' ) Q2 ON Q1.MovieId = Q2.MovieID; What does this query yield? SELECT MovieId FROM Ratings LEFT JOIN Users ON Ratings.UserId = Users.UserId LEFT JOIN Occupation ON Users.OccupationId = Occupation.OccupationId WHERE Rating &gt; 4 AND Occupation.OccDescription = 'Programmer' I believe my query is doing the exact same thing as your query. You are using an older join method, I declare the joins to make it more evident. I added the movies prefix to the MovieID you had missing. For that fix alone, see this: SELECT DISTINCT Movies.MovieTitle , Movies.Year , CategoryDescription.CatDescription FROM Movies , CategoryDescription , MovieCategories WHERE CategoryDescription.CategoryId = MovieCategories.CategoryId AND Movies.MovieId = MovieCategories.MovieId AND Movies.MovieId IN ( SELECT MovieId FROM Ratings , Users WHERE Ratings.UserId = Users.UserId AND Rating &gt; 4 AND Users.OccupationId IN ( SELECT OccupationId FROM Occupation WHERE OccDescription = 'Programmer' ) );
Errr.......wait, I just realized a very stupid mistake. Let me review this a little and I'll get back to you if I need more help. Thanks a bunch. 
[Here if you need me.](http://imgur.com/gallery/5lsY0)
Hahah thanks.
This should work based on a sample set of data I created: WITH RN AS ( SELECT * FROM ( SELECT ROW_NUMBER() OVER(PARTITION BY ID ORDER BY ID, Date) AS RN , * FROM [Workspc].[dbo].[sample] ) X ), RN_ON AS ( SELECT * FROM RN WHERE Status = 'On' --AND RN &lt;&gt; 1 ) SELECT * FROM ( SELECT * FROM RN_ON A UNION ALL SELECT B.* FROM RN_ON A LEFT JOIN RN B ON B.ID = A.ID AND B.Status = 'Off' AND B.RN = A.RN-1 --UNION ALL --SELECT * --FROM RN --WHERE --Status = 'On' --AND RN = 1 ) X ORDER BY ID, Date edit: Could probably rewrite this without an RN_ON section at all. 
select reservationid, startdate from reservation where startdate &lt; CAST(GETDATE() AS DATE)
Just to make sure, is the user added to the Database and part of the "Login Users". If you expand "Security &gt; Logins" is that user there? 
Might want to add `WHERE Status = 'On' AND RN &lt;&gt; 1` to the RN_ON section in order to meet your needs better. General framework that should get you where you need to go. edit: Basically if the first row is set to ON you want to ignore it because you only are interested in cases where it is set to OFF and then when it is turned back on. If you wanted to datediff() this for averages you could use a JOIN instead of a UNION and the total count would be the total row count, so you could group by ID and count(*) to see the total activity. Not adding this could cause an error but if you want to include these cases you could add a third union section where RN = 1 and Status = 'On'.
You could give it to the DBAs to run on the server and create a view for you to bump against?
An easy way to keep it straight is that you are literally GROUPING the results BY the columns you are using. Take this for example: DROP TABLE IF EXISTS dbo.markers GO -- I'm organizing my markers by position, brand, and color (because I'm a crazy person) CREATE TABLE dbo.markers ( ID int ,brand varchar(10) ,color varchar(15) ) [My Markers](http://i.imgur.com/NNUazhb.jpg) -- I'm putting my markers into my table INSERT INTO dbo.markers ( ID ,brand ,color ) Values (1,'Expo','Light Green') ,(2,'Expo','Dark Green') ,(3,'Expo','Orange') ,(4,'Expo','Light Blue') ,(5,'Expo','Dark Blue') [My Markers in my table](http://i.imgur.com/XUJ0jKN.jpg) --How many Green-ish markers do I have? SELECT count(color) FROM dbo.markers WHERE color like '%green%' [Two... duh](http://i.imgur.com/kLnJs1m.jpg) --List the positions the green-ish markers are in SELECT ID ,count(color) FROM dbo.markers WHERE color like '%green%' GROUP BY ID [Position 1 and Position 2](http://i.imgur.com/QeikiW2.jpg) --List the positions and shade the green-ish markers are in SELECT ID ,color ,count(color) FROM dbo.markers WHERE color like '%green%' GROUP BY ID ,color [Lightish Green in one and Dark Green in two](http://i.imgur.com/x6v0Mt9.jpg) --List the position, brand, and shade the green-ish markers are in SELECT ID ,Brand ,color ,count(color) FROM dbo.markers WHERE color like '%green%' GROUP BY ID ,Brand ,color [Light Green Expo in position 1 and Dark Green Expo in position 2](http://i.imgur.com/PauSjZv.jpg) 
Okay I solved all of that issue and have just one more little thing before I'm all done. My Timestamps field in the Ratings table is in epoch time. I want to run an update after my table is loaded (via a CSV file) that converts it to YYYY-MM-DD format. The query listed below does it correctly, but when I try to update the table I get a ORA-01427: single-row subquery returns more than one row error. Here's my code: UPDATE Ratings SET Timestamps = ( SELECT to_char(to_date('1970-01-01','YYYY-MM-DD') + numtodsinterval(timestamps,'SECOND'),'YYYY-MM-DD') FROM ratings); Any ideas? Edit: NVM got it. I'm an idiot. 
What are you wanting the query/view to return? Each unique STATEMENTDATE and whether there is a record for that date that meets the criteria `POSNEG='P' AND instr(MEDIUM,'tv') &gt; 0 OR MEDIUM = 'Televisie')` ? Essentially, yes. I've got a dataset with multiple media statements (on different mediums) and I'm trying to create a view that for any given day shows if any statements were made, on what medium and if they're postive or not (which is what POSNEG is for).
I did not know this. I don't know what I'd do if I had to get used to no CTEs again. Will stick with TSQL.
Sounds like a bunch of great ideas, but it would never get traction where I work. Gigantic multi-national mired in bureaucracy, the Cognos group is woefully understaffed, and even if they weren't, there's resistance to pretty much any request. "Oh, that pre-made report is ignoring user selections and is overriding with a hard-coded filter that has no reason to be there? That's too bad." Though, that's pretty much par for the course on any sort of software support. Actually, it's not just software. The whole company is run that way. I have no idea how we manage to stay in business, let alone have been in business for 100 years. People in charge making decisions based on uninformed gut feelings that simply don't care about the impact of their decisions on the people on the ground that have to implement them. The latest was a push for a new web-based metrics suite. I was asked to validate it for my site, so I dutifully documented the mistakes, shortcomings, and fundamental flaws. 14 pages later, they decided to delay implementation while they sorted things out (though, that didn't stop the executives from decreeing that we were to start using this system immediately, data integrity be damned!).
Be aware that using a function I the where clause can cause performance issues in t-sql... Eg SELECT X, Y WHERE transaction_date = GETDATE() Ideally you declare it prior to use DECLARE today_date as date = GETDATE() SELECT X, Y WHERE transaction_date = today_date Excuse horrible formatting, on mobile and its being very clunky. 
I've been searching online for the solution but I have no luck. Perhaps the people of reddit will catch my mistakes? I really need help on this.
Ooh btw I'm a first year Software Engineering student. I'm on my first semester and Database is one of my core subjects!
I'm not really familiar with mySQL but, your updateLanguage is defined to take two parameters, and you are calling it with only one? 
ooh what I'm trying to do is taking in the parameter cat_id just so I can I can query only the cat_id = cat01. Is there a way I can do this without calling it in?
You have a concurrency issue. The first stored procedure isn't finished when the second fires, MySQL tries to tear down the sp and subsequently poops itself. I'm on mobile but there a way to block so they run in series, see if this link does anything for you... https://stackoverflow.com/questions/27429608/can-a-heavy-stored-procedure-wait-another-sp-to-finish#27432828 
I would think you'd want to put an end; under Drop procedure statement line. I'm more of an mssql guy but I can't remember.
I don't think you want to group by user_response.ans_id, you're getting a separate total for each response instead of each person. Also, why are you summing the ans_IDs? 
ooh this is a database for a personality test and the user are given the choice of 1 to 5 (the ans_id). I'm suppose to categorise the questions to it's category and get the sum. I hope you understand what I'm trying to say haha. English is not my first language :)
In that case you might like to think about putting user_response.catID in the SELECT and GROUP BY clauses and remove it from the WHERE clause - then you will get one row per person per category with the sum of the ans_ID. That does clarify your original results a bit - user 1 looked to be working because they must have given the same answer to all questions in the category. You were adding up only the responses where the answers were the same: I guess user 1 had 7 answers of 5, but user 2 had 1 answer of 1, 2 answers of 2, 1 answer of 4, and 3 answers of 5.
http://imgur.com/a/4mani I removed user_response.ans_id from the "group by" and it worked. Thank you so much for helping. I really appreciate it.
and yes the user did gave the same answer for that category! Nice catch hahah.
I guess I'm slightly confused by your post. I want to do literally nothing for this set up. I want the end user to be able to 100% do this on their own, so they will either need to paste a copy of an XLSX (or CSV) file into a directory which will be consumed by a process that I set to run every 30 minutes or so... and then have the Tableau visualization being powered by a query which is set to a live connection and bumps up against this table of ID's to include/exclude before running. The other option would be a web interface where the user just pastes a list of IDs. I may clone the Tableau workbook and give each user their own version, as well as giving each user their own table (or ID) within the exclusion/inclusion table. &gt;You can also create an access frontend form and use a file dsn stored on the same network share and then set up a linked table to the server and database. You have to give user permissions to the database and schema. Strongly recommend a separate schema so you don't have to give users access to dbo. I thought about this but a lot of these users are not terribly technical. I thought about a URL where they could go and copy/paste into a text box or something, which would do some rudimentary validation to prevent invalid IDs, etc. -- one of the reasons I'm hesitating to use SSIS is the inevitability of them creating the files in weird formats and them being rejected / useless when it comes to actually using the process. What I envision is to rewrite my current process to `select * into #table from source where conditions` and then point the visualization at the #table. So it would seem trivial to add a join during this pre-step which bumps against a list of ID's which are to be included/excluded. For example they might want to include an ID which doesn't meet the where conditions that are necessary, or they might want to exclude ID's which do meet the conditions. They wouldn't be able to modify source records at all, but for the purposes of the Tableau suite it would "correct" the data for purposes of executive summary review. In reality they should be reaching out to the people on the ground and getting them to change their records accordingly, and then those changes should be imported once done and the next successive data load runs, but when they're preparing for a QBR and discover a bunch of user errors which need to be excluded/included they can't wait a week plus for it to be addressed properly. Similarly it isn't a good look for them to present data which we know is wrong... so historically the solution has been to email me. What I'm trying to find is the easiest simplest most foolproof method ever to avoid user error and just let them use the dashboard suite as a full service analytic tool. I'm likely going to try and offload the ownership of the URL or SSIS package to a group in IT and just let them deal with it. 
You can either add in a store results statement ( forget the exact nomenclature... sorry ) or you could *try* throwing a sleep (1); between each one. Found it: mysql_store_result()
I'm on mobile so I can't format properly, but I would suggest EXISTS as a subselect. SELECT * FROM table_b where b_id in (SELECT b_id from table_b where exists (1 from table_b where b.key = 'F#')). Hopefully you get the idea. 
Surrogate key
Yeah. Then you have a lot to learn, good luck with your studies!
and/or they actually store the 'join queries' in a specific table
&gt; then the best way to ace the interview is to ask questions to the questions they ask. That's been my method and it feels like it works really well. Usually after intros they will say, do you have any questions before we start? I'll mention I have questions for them and they may be better suited for after the interview, but they are just questions about the job and environment. They'll generally say, sure, let's hear the questions now. Then I just ask questions about questions I would assume they would ask me. Like, how is there schema configured, is it a highly normalized environment with strict foreign key enforcement for data integrity or is that something they need me to help come in and analyze / resolve? Do they scope their expressions in SSIS or is it just a singular scope? What type of tools or scripts do they run that evaluates extended events or dmv's for performance? Are they doing regular index maintenance and are things tuned to where excess indices are removed and others are set to the proper fill ratios? The interviews tend to become more just conversations about SQL and how it is in their environment. It shows interest in the field and also experience / education. It also allows them to see what I would highlight as important in order. (Typically lead off with data integrity and maintenance / care questions and move downward. See Ozar's hierarchy of needs.) 
Cheers - I think I can work in the pk over fk into that - as the fk in this case is a one-to-many (up to 6 items share the same fk - there are multiple fk's in this table). I'll give it a whirl!
Use a subquery: UPDATE my_table SET override = CASE WHEN (SELECT SUM(override) FROM my_table WHERE my_fk = 1) &lt;&gt; 0 THEN 0 ELSE 1 END WHERE my_fk = 1 ; Even better, you don't need to sum *all* records, you just need to stop when you hit a `1`. In theory, this will be faster (I'm not familiar with MySQL's optimizer, so grain of salt): UPDATE my_table SET override = CASE WHEN EXISTS (SELECT 'X' FROM my_table WHERE my_fk = 1 AND override != 0) THEN 0 ELSE 1 END WHERE my_fk = 1 ;
Not OP, I like the 2nd solution. The only thought I had because I can't see the data model, what if there are negative values? Theoretically it would be possible to sum -1 and 1 then have 0 be the outcome. The exists criteria would be a false positive then. Assuming there are data constraints so it's impossible to be negative, I'd think the 2nd is better performing. For a bonus, I think a IF structure tree and losing the case when could provide the best performance when combined with your 2nd query. I'm also not familiar with the MySQL optimizer though.
Thanks for the visual example! I figured it was something like that, but I was just imagining all the joins but then why would "everyone fail that test"? Not sure if he said that to make sure I knew what I was doing or because their schemas really ARE hard to understand.
Everyone has their own style. You may want to look at how Salesforce does their objects. They have relationship tables/objects called "Junction" tables/objects. This may be what they're referring to.
I didn't even think to ask what they use for CRM (it was only a 25 min pre-screen so i'm lucky to get what info I did get) but either way thanks. Junction tables makes sense to me. I'm hoping it ends up being what has been mentioned although I specifically remember hearing there would be a PK_Customer table or something (specifically that PK was in the name of it which is weird to me) and I did not hear anything about a linked CustomerOrders type table, perhaps it was just because that convo was cut short. I'll try to sneak in some more questions before that test I guess, or just hope I'm logical enough to see what's going on when I get in the db.
 SET @X int = 2006; SELECT * FROM Table WHERE @X = YEAR([DateColumn]) But what you really probably want to do is `SET @X int = YEAR(getdate())-11` so that you won't ever have to change the code. When a new year comes it will auto-update. edit: Syntax 
I see, but why would the timeout be higher from the old server to the Oracle than new server to the Oracle? Processing the statement, etc, should be the same, as I see it, but I am curious to the deeper functionalities, as this is not my main field. Also I have just checked the settings on the linked server objects on both servers, Query Timeout = 0 and Connection Timeout = 0 on both.
I feel like I would do this using a group by a modified version of the timestamp, use a case where if datepart min is between 0-14 then set the min to 15, 15-29 then min = 30, 30-44 = 45, 45-59 = 60, then add those values to a datetrunc of the hour. Hope that makes sense as I am omw out the door and can't write out a full example. 
You say you can't make changes to existing tables, but can you make a new table? If you make a table with your intervals start point, end point, and then a label, you can then do a "between" join and group on the label column. 
It's super weird that you're being asked to do this with a union. In the real world you'd do this with a single SELECT, not by adding two selects together. What a union does it take two separate statements and adds their output together. You're getting the extra values because the first subquery returns all penalties, and then the union just ADDS the people who meet both conditions. In other words, if you deleted the first three lines for your query you'd be getting exactly the data the question is asking for, but you wouldn't be using a UNION. Also, get out of the habit of using those old style joins: http://sqlblog.com/blogs/aaron_bertrand/archive/2009/10/08/bad-habits-to-kick-using-old-style-joins.aspx
So an index is like a Dewey Decimal system in a library. You could go into the library and look at each book to SELECT the records you want, or you could go to the index and get a list of where all the books from 2006 are, and then just go look there. Which one is faster? When you use a YEAR([Date]) function you are not going to take advantage of the index. So you could follow the advice the other commentor made, or make your @parameter a datetime and use a between statement. Actually now that I think about it you need to `DECLARE @X int = 2016`... I think? Not in front of a SQL box right now. @Parameter is just a parameter you're passing into your SQL code. 
It's hard to say because you haven't provided many details, but the first step to grouping data into intervals is often to build a table that defines those intervals. For example here is a table that defines all of the 15 minute intervals for today. You could then use unequal joins (inclusive on the interval_start and exclusive on the interval_end) to group the data from your table into these intervals. DECLARE @today smalldatetime = CAST(GETDATE() AS date); WITH tally AS ( SELECT ROW_NUMBER() OVER (ORDER BY (SELECT NULL)) - 1 AS n FROM (VALUES(0),(0),(0),(0),(0),(0),(0),(0),(0),(0)) a(n) CROSS JOIN (VALUES(0),(0),(0),(0),(0),(0),(0),(0),(0),(0)) b(n) ) SELECT DATEADD(mi,15*n,@today) AS interval_start ,DATEADD(mi,15*(n+1),@today) AS interval_end FROM tally WHERE n &lt; 96 Feel free to ask follow up questions if this sets you down the right path.
I've handled this a couple ways before depending on the scenario. The approach you take really depends on how large the value lists are that users are working with and whether or not they need to be able to re-use them. 1. For include/exclude sets that aren't massive (&lt;~500 values), you could prompt the user for an nvarchar(max) value that's just a comma separated list of their includes (and a separate for their excludes). You could then have a table-valued function in the DB that parses those values into a temp table that you would write into your main select statement and use a join to filter them in/out. If the list is small and indexes on the ID column are set up properly, a WHERE columnname IN is probably OK as well for simplicity. 2. For very large include/exclude value lists (e.g. &gt;1,000), give them a quick web UI to paste a list of values that would upload it to a table. It should also generate a "list name" based on their user ID and datetime. Store the list name and list items in a table and prompt the user while generating a report for an optional list name and include or exclude value, then write your SELECT to use the list they've stored. The last time I went this direction, I also needed to limit the lists prompt to show only the executing user's uploaded lists. Also had a cleanup job that would delete any lists created &gt;90 days ago. You can get as complex as you'd like with the web UI allowing them to add, edit, delete their lists. It would be very easy to implement this quickly in .NET or whatever web code you'd like. Hope that helps.
Oh cool. I didn't know you could join on a second select statement like that. I'll try it tomorrow. This is one of those times where I realize I don't know near enough about SQL yet. 
By using a pass-through query in Access you can use the syntax as is.
This is a poor use case for a `union`
I'm really not an Oracle guy but a bit of a google around suggests checking the log at $ORACLE_BASE/diag/rdbms/$ORACLE_SID/trace directory. Apparently this issue can be caused by insufficient disk space, if so the trace log should confirm. Maybe. According to some dudes on the internet I randomly googled.
Yeah. Totally working. Assuming all the previous code that's meant to guard doesn't interrupt, then it works perfectly fine. Even worse is when I comment out the NOTICE on line 47, the EXCEPTION on Line 66 doesn't even get raised. Edit Summary: Yes, Yes.
Should be pretty simple, just create a field that represents each 15 (or 30) minute block from the existing date-time field. Assuming the field name is DTField, this will turn something like this: "2015-03-04 08:32:15.527" into this "2015-03-04-08-2" which is year-month-day-hour-15 minute index (0 is 0-14 minutes, 1 = 15-29 minutes, 2 = 30-44, and 3 = 45-59) CAST(CAST(DTField as date) as varchar(10)) + '-' + RIGHT('00' + CAST(DATEPART(HOUR, DTField ) as varchar(2)), 2) + '-' + CAST(DATEPART(MINUTE, DTField ) / 15 as varchar(1)) Then of course, if you are doing any aggregates, include it with other columns in the GROUP BY clause for something like: SELECT CAST(CAST(DTField as date) as varchar(10)) + '-' + RIGHT('00' + CAST(DATEPART(HOUR, DTField ) as varchar(2)), 2) + '-' + CAST(DATEPART(MINUTE, DTField ) / 15 as varchar(1)) as Interval, COUNT(CALLS) as CallCount FROM CALL_TABLE GROUP BY CAST(CAST(DTField as date) as varchar(10)) + '-' + RIGHT('00' + CAST(DATEPART(HOUR, DTField ) as varchar(2)), 2) + '-' + CAST(DATEPART(MINUTE, DTField ) / 15 as varchar(1)) (edited to make a little more readable in the code block)
Thanks, but unfortunately that is not the case.
The best explanation I could think of, is if this is the end of the introduction to the union function just to demonstrate that you can achieve the same results with various methods and to also illustrate that just because you can, doesn't mean you should. If they didn't intend that as part of the lesson, then I think it's a typo or the teacher is stupid.
Completely my thoughts. I haven't taught SQL (yet) but I used to teach English and at the conclusion of a lesson I would always teach students how to break all the rules I just taught them, but then remind the that they shouldn't. It very clearly tells you who understood the material and who didn't. 
&gt; Here was the assignment &gt; * *Using a union, get the player number of each player who has incurred at least one penalty and lives in Stratford* I can think of two ways to go about getting the player number of every Stratford player that's had at least one penalty, neither of which use a union. Both assume that if you have a row for that player in penalties, they have had at least one penalty. -- Basic select version SELECT players.playerno FROM players INNER JOIN penalties ON penalties.playerno = players.playerno WHERE players.town = 'Stratford' -- Exists Version SELECT player.playerno FROM players WHERE players.town = 'Stratford' AND EXISTS ( SELECT 1 FROM penalties WHERE penalties.playerno = player.playerno ) There is no version of this where a union makes sense. Did you cut and paste the objective from your assignment or is there any possibility you missed part of the instructions? 
It's giving me and undefined expression error for SUBSTR. Does this go after SELECT? I tried putting it after select and just at the end of the query...not sure why it's not working. It's a very short query with just SELECT, FROM, ORDER BY. 
Yes, it should probably be in your SELECT clause, depending on what you want to do. SELECT whatever, SUBSTR(country, -3, 3) AS last_three_chars FROM your_table ORDER BY whatever
Possibly, I'd consider sargability a huge factor in performance tuning, but probably on the intermediate / advanced side. When you learn joins and where clauses very well and they become second nature, I'd suggest to remember this term and then dive into it. It's a very quick way to impress people you work with and you'll appear as a wizard.
Thanks, I figured it out. 
 INSERT INTO tblService SELECT **DISTINCT** id , service_cd , service_desc FROM tblPatient WHERE service_cd IS NOT NULL Should do the trick. However, if there can be multiple service_desc's associated with the distinct id, service_cd records, you may still run into issues.
Oh, yes, I forgot to mention that is one of the things I tried. And indeed the problem is that the descriptions can be different. And ARE...that is actually the problem that needs fixing. They started showing up with two different descriptions for each service type.
&gt; SELECT **DISTINCT** id , service_cd , service_desc FROM tblPatient WHERE service_cd IS NOT NULL no problem man /// INSERT INTO tblService SELECT distinct_id , service_cd , service_desc FROM ( SELECT row_number() over(partition by distinct_id order by service_cd, service_desc) as row_num ,distinct_id , service_cd , service_desc FROM tblPatient WHERE service_cd IS NOT NULL ) drv where row_num = 1
Oh man, that row number business would be useful in a few places I can think of. This totally did the trick, thanks so much! You are my hero.