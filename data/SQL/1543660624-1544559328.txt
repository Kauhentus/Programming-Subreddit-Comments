We have our data in SAP. The work I do draws on that data to provide insights into how the business is performing, and I work a lot with forward looking data - forecasts. When I took on the role everything was incredibly manual. Manual data extracts from SAP, and manual comparisons in excel to previous extracts. I honestly can’t see how power query is going to do half the job of my database, but I also see that my boss is concerned if I leave the business and no one can maintain it after I’m gone. Personally I think we need to hire people with the right skills for the tools we need, rather than building tools to match the skills of the people we have. 
Look into sequences.
Ah, thanks, that helps. In truth most of our reports currently don’t need to store data, but there’s one big one which does, and I’ve already brought that one into SQL. But there’s so much more we can be doing with our data than we currently are. I’m supposed to be a data analyst, and I’m expected to provide insights such as a data analyst would. It frustrates me that I’m being limited to the function of a reporting analyst.
If your website is hosted on anything, it might already be using a website to run. This is what websites literally do behind the scenes, take an input from a user, and modify data on the server (usually in the form of a database), or just display it out in the form of an SQL query being mapped to some HTML. Regardless: First you would need to extract all your data that comes out of it into something more standardised, such as a CSV file. To get there however, is where a programming language needs to come in. Some program/script will need to be written to become the thing that will allow an input (PDF) and do some processing (programming language such as python or java), which gets output (CSV). Just putting it into a CSV alone won't cut it, the data has to be normalised (https://en.wikipedia.org/wiki/Database_normalization) before the database be able will use if efficiently. 
**Database normalization** Database normalization is the process of restructuring a relational database in accordance with a series of so-called normal forms in order to reduce data redundancy and improve data integrity. It was first proposed by Edgar F. Codd as an integral part of his relational model. Normalization entails organizing the columns (attributes) and tables (relations) of a database to ensure that their dependencies are properly enforced by database integrity constraints. It is accomplished by applying some formal rules either by a process of synthesis (creating a new database design) or decomposition (improving an existing database design). *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/SQL/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
Database normalization is the process of restructuring a relational database in accordance with a series of so-called normal forms in order to reduce data redundancy and improve data integrity. It was first proposed by Edgar F. Codd as an integral part of his relational model.
Yeah I totally agree. Why the hell are they teaching this crap syntax anymore? Nothing is worse then trying to read a query with 10+ tables joined like this.
Unfortunately this is the nature of academia... my best professors were always the adjuncts that teach on the side and actually do this stuff for a living, not the tenured professors. 
Unfortunately, manually is the most effective for a lot of these. There are some Python packages that can help (https://github.com/WZBSocialScienceCenter) if the formatting is generally consistent. The basic process is that you'll need to OCR or utilize the underlying text i threre is any (sometimes PDFs are literally just images). 
Demonstrate the value of doing it in the database. Do a demo of the same process, one in Excel and one the right way. Show how it's faster, more accurate, produces better-looking results with less manual effort, eliminates duplication of data &amp; potential data breaches (where do those extracts live? On your laptop? What happens if the laptop is stolen?), etc. Maybe pull the data into PowerBI Desktop. If that means your co-workers need to learn new skills, then they learn new skills. If both they and your manager refuse, you know it's time to move on to a place that appreciates someone with your skillset.
You may need to wrap your select statement in parenthesis. Or better yet move it out of the insert statement all together and assign the result to a variable you pass to the insert statement.
Keep in mind that this is a system that people are currently using and they feel comfortable with. If you build something custom and arcane to most of your coworkers, then they are entirely reliant on you maintaining and assisting with that for a business process that they are currently handling by themselves in Word. It might be more efficient than you realize. 
I don't know what the purpose of this statement is for, but if the appointment table is empty, then MAX(ApptId) + 1 may also return null. 
Not very comprehensive in my opinion and little content except the plug for your product. I would compare Squirrel to other tools in the same space; that would be useful as there are many alternatives and it's hard to choose. That would make for an interesting article. 
Piggy backing off of /u/lostburner 's comment as I think it's ultimately the most appropriate answer. What you're asking for OP is an addition to the current technology architecture of the company. Prior to adding any architectural changes to your company, you should identify if a technological solution is really the best solution at all - and if so, what you can implement that would have the least amount of impact on the business and the current technology while giving the most value. How big is this list that they're maintaining? How long does it take them to update each month? If it's only one person spending a couple of hours a month to update this then it's probably not in the business' best interest to pay for developers/IT to implement this as an automated solution. Additionally, depending on what your company's tech stack is - it would cost additional licensing fees for the database itself along with whatever OS platform it's going to run on. Personally, I would find out how long it's taking to update this list each month, if the department has any complaints about the current process to begin with and depending on that answer if there's anything that could be done to improve the process without completely reinventing it for them. (Such as seeing if this data is actually being already held elsewhere and automating the data pull for them to where they can then just copy it into the Word document while still utilizing the same process they already are). 
You might look into building a Content Management System (CMS). They're essentially user-structurable data systems with access control and other features baked in, and there are open-source ones available.
Help us help you. What's your question? All you've done is throw code in a post.
Hello sorry! &amp;#x200B; So the case is I have a purchase table, a customers table and a rewards tier. A purchase is made by a customer who in his table has an earned points attribute. Each customer also belongs to a specific tier. When a row is inserted into Purchases, reward points need to be calculated and added to the earned points attribute in customers. Customers also earn bonus points depending on the tier they belong to. &amp;#x200B; So I managed to clean up that trigger and sorta make it more simple and it goes to the following: &amp;#x200B; Create or Replace trigger JavierRewards after insert on Purchases for each row Begin Update customers set customers.earned\_points = customers.earned\_points + Round(:New.purchase\_amount \* 1.5) where customers.cust\_id = :new.cust\_id; end; &amp;#x200B; &amp;#x200B; Im missing the part of where I take the extra points attribute from the Rewards\_Tier table but I cant seem to get it. 
So this actually compiles but Im still missing the calculation of the extra points &amp;#x200B; REATE TABLE REWARDS\_TIER( tier\_id char(2) constraint rewards\_tier\_pk PRIMARY KEY, tier\_name varchar2(20), extra\_points number(3,2) -- Number with decimal, e.g., 0.10 -&gt; 10%. ); &amp;#x200B; CREATE TABLE CUSTOMERS ( cust\_id char(6) constraint customers\_pk PRIMARY KEY, first\_name varchar2(100), last\_name varchar2(100), credit\_line number(10,2), curr\_balance number(10,2), earned\_points number(5), tier\_id char(2), constraint tier\_fk foreign key (tier\_id) references REWARDS\_TIER, constraint balance\_chk check (curr\_balance &lt;= credit\_line) ); &amp;#x200B; CREATE TABLE PURCHASES( purchase\_id char(7) constraint purchase\_pk PRIMARY KEY, cust\_id char(6), purchase\_date date, purchase\_amount number(10,2), pending\_flag number(1), -- a value of 1 means it is pending constraint cust\_pur\_fk foreign key (cust\_id) references CUSTOMERS ); &amp;#x200B; These are the tables
I've been using the beaver one tho
Their site looks like its from the mid 90s, and they host their code on sourceforge. The app requires java too. I dont want to discredit anyone, but a little facelift and upgrading would help.
Which DBMS?
I've no idea what this question is asking. What do you mean by a 'constant sql table'? What are you planning to hardcode?
Nothing wrong with lookup tables. Quite the opposite, in fact - it improves data quality and future changes (adding a new plan, changing a plan's name, etc.) are much easier.
havent taken the 462, however i took the 768 (developing sql data models), i was lucky in that i was familiar with the material tangentially just because id worked extensively with non Microsoft OLAP, but my process was just to use the purchasable practice exam software, then: 1) Take Practice exam 2) Write list of weak areas, i.e general subjects, with specific sticking points 3) Study plan, focusing on nailing broad areas before finer details 4) Repeat 1-3 until comfortable with the material and consistently passing Took me about 2 weeks to get 3 passing scores in a row, but i was lucky that my employer needed another certificate for partner status so i had a lot of working hours to devote to the cert.
Thank you anything helps!
Any tools in particular you'd recommend comparing it to? We wrote a HeidiSQL review as well that we could compare it to, but do you have any suggestions?
Cool, we'll add that to the list to review &amp; compare.
Maybe consider another line of work? There is no way someone who has tried at all in even a basic SQL class doesn't know what a primary key is.
Diiiiick
Sound like maybe a union? I assume you have 3 types of data, that you want back in 3 rows? sql1 to get 1st row UNION sql2 to get 1st row Union sql3 to get 1st row 
Actually they are simply numbes. I am sure there is a simple solution I am not seeing for splitting it up. Basically I need 3 rows, each with some simple but different arithmetic based of the columns of the underlying table, in a “result” column. Each Row should then have a different label in the “type” column Thanks
A primary key is a unique identifier in the table. If using access, it will create one for you. If using something else, you have to define it. Google "primary key sql"
I freaking love Dbeaver. Been using it for about a year now. Connect to teradata, vertica, ms sql, Oracle. All in one object explorer. Love the organization. Especially the folders to organize those connections. I must have 30 different Oracle schemas. 4 teradata servers with thousands of schemas each. 10 plus ms Sql servers. Etc etc.
Unless I'm mistaken, you don't mention what DBMS you're using, but for oracle you could use REGEXP_SUBSTR for this. The regex would be something like 'SN#\d\d\d\d' or 'SN#\d{4}'
Haha that's next level dude I just use it for my postgresql server 😅
I see, and you are trying to avoid selecting 3x times for performance.
The schema filter is the tits I tell you. Take teradata for instance. You connect to one server and all of the schemas populate. I can filter by wildcard just what I want to see. And select specific schemas to add to it. It is so amazeballs. 
So I think you want 6 values? 3x rows of 2 Cols? How about 6 values in one row, with inline maths? Is that an option? Eg if second line is just 10 percent higher you can have So val1, val2, val1_variant1=val1 x 1.1, val2_variant1=var1 x 1.1, val1_variant2 etc... If your maths is too complex then this may be cumbersome. If you want to use that structure, how about using a table viewed function? It's a function that will return data as a table, you can join on it etc.. In the tvf, create a temp table, first get the values you want, insert into temp. Then... Do the maths for the second set of values, insert, then the 3rd, then return the table. You might pay a performance hit for this if you try to do one record at a time (can can pass in Params, eg a primary key col) vs the whole table at once.
oh maybe I wasn’t clear I know what it is and I’ve added it I’m just starting out and learning it’s been like a week so mean ): anyways when I add data to a table it says primary key not found but i check and it’s there idk what else I’m suppose to do 
Looking for a solution in bigquery or microsoft sql server
In SQL Server, there is probably some function that tells you the character position at which a substring occurs in a longer string. So for each of the rows in your example, find the position where "SN#" occurs, and take the seven characters from there. Or if the length of the SN# part is variable (for example, some serial numbers are 4 digits in length but some are five digits in length), look for the first ' ' after the location where the SN# starts, and take the string from SN# up to that first ' '. 
Select Substring(author_number_subject, charindex(‘SN#’, author_number_subject),7) From Table Essentially, the char index is to find what position the string ‘sn#’ occurs in the columns. Using that position you use the substring function to take 7 characters to the right. Hope this makes sense
Assuming SQL / SSMS select books ,substring([author_number_subject],charindex('SN#',author_number_subject,1),4) as [number] or something along those lines. substring takes a portion of a field from the start and end poisitions you indicate, charindex looks for the position of a string within a string to use as your start position. If the SN# value isnt always 4 digits I would use another charindex and look for ' novel' and then subtract 1 from it as the stopping position so it runs until the digits end
I have been using Squirrel SQL for a couple years now. I think it is great. I connect to a Progress OpenEdge database. For my SQL Server databases I still use the Microsoft tools.
For things like this I create a lookup table with just 1 column. That being in your case the event as both the data and primary key. This way you don't need to link to the event table to get the event. Also if you hard code in source I can almost guarantee they will ask you to add a new event in about a year or so.
 select type, sum( case when type = "TypeX" then x when type = "TypeY" then y when type = "TypeZ" then z end) as "Sum of stuff" from myTable;
 select 'Type X' as "Type", sum(x) from myTable group by type union all select 'Type Y' as "Type", sum(y) from myTable group by type union all select 'Type Z' as "Type", sum(z) from myTable group by type;
This is all just personal preference. Typically you don't want to write everything on one line. Some queries can be thousands of lines long and this will obviously not work. Using indents is my preferred style, but not the way you showed. select thing , 2thing , 3thing , 4thing from table a inner join thing b on a.id = b.id and a.id2 = b.id2 where a.thing = 2 and b.thing = 4 and (b.thing = 1 or b.thing = 3) 
I have only used sql at uni so am very much a beginner, however there are two resources I have used for formatting after I was marked down for using the second type that you showed. These two resources are: https://sqlformat.org/ and https://www.sqlstyle.guide/
Apparently MS put out a style guide way back when and the DBEs/DBAs in my company adopted it. It's wonderfully easy to read compared to stuff I've seen from some of our outside contractors. SELECT Col1 ,Col2 FROM Table1 INNER JOIN Table2 ON T1.A = T2.A AND T1.B = T2.B WHERE T1.C = 'value' 
I use a style similar to @notasqlstar. The main difference is that I put reserve words in caps.
It's a team choice. I rely like [http://poorsql.com](http://poorsql.com) If you code in SQL Server, there's a plugin four Management Studio and Visual Studio. 
I actually put everything in caps such as SELECT, FROM, DATEPART, JOIN, etc., but with column names I use the correct case, such as ColumnName. For things like the number of weeks or days in the datepart I put those in lower case.
The manual for the RDBMS you're using.
[The Basics of Good T-SQL Coding Style](https://www.red-gate.com/simple-talk/sql/t-sql-programming/basics-good-t-sql-coding-style/)
Are you on SSMS? How do you right align the reserve words SELECT FROM etc or are you just using lots of manual spaces?
@InelegantQuip Any chance you have a link to the MS style guide you are referencing?
Joe Celko's [SQL Programming Style](https://www.oreilly.com/library/view/joe-celkos-sql/9780120887972/) is *the* right way. Many of the "rules" are backed up with really good explanations. Some even have scientific reasons for the recommendations. I also recommend using a tool such as Red Gate's [SQL Prompt](https://www.red-gate.com/products/sql-development/sql-prompt/index) (if you're a SQL Server user) to help you keep your style consistent. It has a keyboard shortcut that automatically re-formats the code according to the style rules that you can configure.
IN bigquery you could use REGEXP_EXTRACT with regex being "SN#\d+"
Afraid not. I've just picked it up from working there. I thought it was just our house style until recently and one of the DBEs mentioned the MS connection. If it helps to find it, he mentioned that the aligned whitespace was called the "trough" in the guide.
Yes SSMS and yes manual spaces, I'm afraid. It was a pain at first, but at this point I just do it habitually.
Yup, mine is similar. 
Exactly. So right now I am using 3 selects (and in each specifying type) and then union all to get the data into 3 rows
It largely depends on your business. Most standards are caps for queries and lowercase + underscores for files. Tabs are generally used for nested queries. The most important part is to make your code easy to understand because in a lot of cases you're not the only one working on the project.
Here's a real world example of a query with my style that I think represents a good amount of the things you would style in query to begin with. This is the style I use for ad-hoc queries or report writing. The styles I use for PL/SQL or T-SQL vary differently based on the added programming concepts. WITH inti_clients AS (SELECT c.cust_id ,c.cust_name ,inti.aux_data AS inti ,ehr.aux_data AS ehr ,o.order_num ,o.receive_date ,oml.aux_data AS oml_seq FROM orders o JOIN profiles p ON o.order_num = p.order_num JOIN customers c ON p.cust_id = c.cust_id AND substr(c.flags, 1, 1) IN ('M', 'V', 'N') LEFT OUTER JOIN aux_data inti ON c.cust_seq = inti.aux_data_id AND inti.aux_data_type = 'C' AND inti.aux_data_format = 'INTI' LEFT OUTER JOIN aux_data oml ON o.order_num = oml.aux_data_id AND oml.aux_data_type = 'S' AND oml.aux_data_format = 'OML#' LEFT OUTER JOIN aux_data ehr ON c.cust_seq = ehr.aux_data_id AND oml.aux_data_type = 'C' AND ehr.aux_data_format = 'EHR' WHERE TRUNC(s.receive_date) BETWEEN '01-JAN-18' AND SYSDATE) SELECT ic.cust_id ,ic.cust_name ,ic.active ,ic.ehr ,get_cust_ct(ic.cust_id) AS CT ,get_cust_sale(ic.cust_id) AS "Sale Manager" ,get_cust_area_sale(ic.cust_id) AS "Area Sale Manager" ,CASE WHEN ic.inti = 'F' THEN 'Ordering + Returning" WHEN ic.inti = 'E' THEN 'Ordering' WHEN ic.inti = 'R' THEN 'Returning' WHEN ic.inti = 'C' THEN 'App System' ELSE 'None' END AS "INTI Type" ,SUM(CASE WHEN ic.oml_seq IS NOT NULL THEN 1 ELSE 0 END) AS "Electronic Orders" ,SUM(CASE WHEN ic.oml_seq IS NULL THEN 1 ELSE 0 END) AS "Manual Orders" FROM inti_clients ic GROUP BY ic.cust_id ,ic.cust_name ,ic.active ,ic.ehr ,get_cust_ct(ic.cust_id) ,get_cust_sale(ic.cust_id) ,get_cust_rsm(ic.cust_id) ,CASE WHEN ic.inti = 'F' THEN 'Ordering + Returning' WHEN ic.inti = 'E' THEN 'Ordering' WHEN ic.inti = 'R' THEN 'Returning' WHEN ic.inti = 'C' THEN 'App System' ELSE 'None' END; A few points: * I prefer to leave the starting phases of logical query processing unindented and then indent from there. I find this easiest to read in a larger query when you need to jump from phase to phase. * I keep the ON in JOINs on the same line as the JOIN because usually this is a write once and forget, while ANDs in the JOIN you may come back and look at multiple times. It's easier for the ON to get out of the way I think when this is done. * I use commas at the start of each line in the list because it's easier to comment out the entire line when building complex queries, and much easier to identify missing commas that could cause inappropriate aliases and combine two columns.
If you're writing lots of sql writing keywords in all caps gets tedious really fast. I just use lowercase and let the syntax highlighting show me the keywords. 
/r/SuddenlyGay
When dealing with short queries like that, it doesn't really matter in my opinion. You are still able to read it easily. Later on, when you move to longer and more complex stuff, you need to write on multiple lines and use proper indentation. So, from your examples, the third one would be the best one. Also, there is no right or wrong. It comes down to personal preference. Just read through this post: https://stackoverflow.com/questions/272210/sql-statement-indentation-good-practice. As you can see, every single person does it differently, but there is a general consensus on which style is the best in terms of readability and error-finding. And lastly, SQL is generally spoken of as a programming language, however it is not really a programming language. It is rather a communication language used to manage databases. 
Omg this is the best thing ever
Try with this query: UPDATE ( SELECT (column1, COUNT(column1) as "column_1_count") FROM table1 GROUP BY column1 ) AS a, destinationTable AS b SET b.column_1_count=a.column_1_count WHERE b.column1=a.column1 &amp;#x200B;
Sadly I get "SQL Error: ORA-00907: missing right parenthesis 00907. 00000 - "missing right parenthesis" Similar to all my other attempts. 
Can I see your query? 
UPDATE ( SELECT (rprojid, COUNT(rprojid) as "column_1_count") FROM pcase GROUP BY rprojid ) AS a, rproject AS b SET b.proj_participant_count=a.column_1_count WHERE b.rprojid=a.rprojid;
https://www.sqlstyle.guide/
I've created such a document in the past for our team, as the team was using various styles. Eventually we all agreed upon right align key words and commas on the left of the attribute or metric. Key words in capital letters. SELECT cus.customer_id ,cus.first_name ,cus.last_name ,add.address_id ,adr.address_line_1 ,adr.country_code FROM sales.customer cus JOIN sales.address add ON cus.address_id = adr.address_id WHERE adr.country_code = 'uk' ;
I'm amazed that someone could write a 200-page book on the subject. The table of contents makes it worth looking into
I suspect there are simpler ways, but you can calculate the distinct dates in a subquery. Then join user, team and dates with a group by id. (select id, distinct dates from team group by id) as dates
Interesting. I would have expected two rows of data, both with 2 as max(id). 
You can install SQL Server on Linux, and it's really easy to do so. On my Mac I have SQL Server For Linux running in a Docker container and it takes about 3 minutes to setup: [Here](https://medium.com/@reverentgeek/sql-server-running-on-a-mac-3efafda48861)
That worked! Cheers!
Thx, I did that already. Up and running. 
I used to be a real stickler for formatting where any code that wasn't lined up and spaces correctly line by line was like nails on a chalkboard, but the formatting tools built into Dev and text editing tools work so well that they really set my default format. The main thing is documenting your code and using good naming conventions for objects and alias's. We have documented naming standards for all of our code so our developers are all speaking the same language 
You can pass the two MS exams for your area of MCSE and be considered proficient in SQL. Simple, right?
Sounds it! Can you take these exams online?
Sounds it! Can you take these exams online?
It depends on where you work. 1 shop I was at had a very specific formatting procedure. Most places as long as it's readable it's fine. I'm in a team with 3 other developers, each one of us uses a slight variation. Thus you can tell who wrote what part of the code. There is no right or wrong, unless your boss/team/ teacher says there is.
SELECT \* FROM TABLE
Honestly, it's archaic but pretty straight forward. It's a 100 page pdf that change monthly, but the changes and amount of work and time put into is dependent on the extent of changes. I was just recently assigned this, so I'm the maintainer. End users will just view it and/or search terms if they need.
I call lower case code, "angry SQL," and have trained my boss to understand that when I show him code that's in lower case that it isn't "finished." When we are completely happy with everything I will go over things and make appropriate words CAPS in order to conform to my general style. As a team I think this has helped us because anytime we crack open a view or sproc and see it written in lower case we realize it's something that isn't golden. If we have a problem with something that is written in a specific style we tend to think the problem is more on the ETL side of things. Pretty simple to do a CTRL+H and replace specific words with CAPS. As many of us know sometimes you will have to go back and edit code you wrote a year or two ago, and you have no real memory of writing it. When I crack open some of my old code that's in a specific style I know I finished it, and I trust my past self. When I crack open some shoddy ass "angry SQL" then I know I need to question myself.
I spent a minute reformatting your example into my style. It just makes more sense to me after years of working with COBOL and RPG, but your example is perfectly readable. I also alias things differently than you do. I use a schema like: `FROM Table A` and `INNER JOIN Table B` instead of giving the alias a more meaningful name. WITH inti_clients AS ( SELECT c.cust_id , c.cust_name , c.active , inti.aux_data AS inti , ehr.aux_data AS ehr , o.order_num , o.receive_date , oml.aux_data AS oml_seq FROM orders o JOIN profiles p ON o.order_num = p.order_num JOIN customers c ON p.cust_id = c.cust_id AND substr(c.flags, 1, 1) IN ('M', 'V', 'N') AND (c.cust_group &lt;&gt; 'QC' OR c.cust_group IS NULL) AND c.cust_id NOT IN (SELECT DISTINCT ac.app_value FROM app_config ac WHERE ac.format = 'NOCUST' AND ac.config_type = 'EXCLUDE') LEFT OUTER JOIN aux_data inti ON c.cust_seq = inti.aux_data_id AND inti.aux_data_type = 'C' AND inti.aux_data_format = 'INTI' LEFT OUTER JOIN aux_data oml ON o.order_num = oml.aux_data_id AND oml.aux_data_type = 'S' AND oml.aux_data_format = 'OML#' LEFT OUTER JOIN aux_data ehr ON c.cust_seq = ehr.aux_data_id AND oml.aux_data_type = 'C' AND ehr.aux_data_format = 'EHR' WHERE TRUNC(o.receive_date) BETWEEN '01-JAN-18' AND SYSDATE ) SELECT ic.cust_id , ic.cust_name , ic.active , ic.ehr , get_cust_ct(ic.cust_id) AS CT , get_cust_sale(ic.cust_id) AS "Sale Manager" , get_cust_area_sale(ic.cust_id) AS "Area Sale Manager" , CASE WHEN ic.inti = 'F' THEN 'Ordering + Returning" WHEN ic.inti = 'E' THEN 'Ordering' WHEN ic.inti = 'R' THEN 'Returning' WHEN ic.inti = 'C' THEN 'App System' ELSE 'None' END AS "INTI Type" , SUM(CASE WHEN ic.oml_seq IS NOT NULL THEN 1 ELSE 0 END) AS "Electronic Orders" , SUM(CASE WHEN ic.oml_seq IS NULL THEN 1 ELSE 0 END) AS "Manual Orders" FROM inti_clients ic GROUP BY ic.cust_id , ic.cust_name , ic.active , ic.ehr , get_cust_ct(ic.cust_id) , get_cust_sale(ic.cust_id) , get_cust_rsm(ic.cust_id) , CASE WHEN ic.inti = 'F' THEN 'Ordering + Returning" WHEN ic.inti = 'E' THEN 'Ordering' WHEN ic.inti = 'R' THEN 'Returning' WHEN ic.inti = 'C' THEN 'App System' ELSE 'None' END;
SQL. If you learn a different thing then you will be proficient in that instead
Cheers man. Super helpful!
No.
Confidence that you are proficient in SQL.
Very interesting read; I really enjoyed it. I come from a Python and SQL background, so the comparisons to R were very cool to me. Thanks for writing/posting!
Imagine this guy working as a DBA on Google/Facebook... dude do something else, it's not immoral you just won't be good doing whatever you are doing. Programming is hard and not for everybody. Btw, you could solve this with 1 simple query. 
By databases do you mean two completely different SQL servers?
Idk if that was necessary... However, if it is so simple, can you please include it in a reply. I would LOVE to see that. Genuinely curious, I actually align more with OP. I read through r/sql just so I can learn more and BE more proficient at my current job. Again, not condescending, just extremely curious what your response to that question is. If you would PM me it, that would be cool. Also, not OP. Also, not going to help OP for 100 bucks. Also, fuck reddit, I am never going to get that answer.
Thanks, glad you liked it! The code is here and runnable on Ubuntu if you want to play with it: https://github.com/oilshell/blog-code/tree/master/data-frames I will probably write a followup post about using sqlite from shell scripts. That turned out to be a pretty convenient idiom! I learned some stuff about Common Table Expressions too through other comments: https://lobste.rs/s/hnfc6a/what_is_data_frame_python_r_sql 
I'm sorry as when I read that, it sounds ambiguous. They are on two separate db instances and are on different servers, yes. But I thought you could import from a separate datasource in Management studio as long as your are connected to both. 
In interviews I'll usually ask a windowing function question. Someone that knows windowing functions probably has a good understanding of SQL. I personally do not like to see certificates on resumes but YMMV.
That's it? I'm self taught, consider myself to be able to get stuff done in a way that nay be barbaric, and window functions seem to be one of the easier things to do (compared to nested subqueries with temp tables with weird joins, etc)
There is probably a better answer out there (I hope?) But the quick and dirty way I've solved this is to wrap all you have there in a (is it a subquery or a derived table? Not totally sure of the difference but I'm leaning toward calling it a derived table) so like SELECT name FROM ( SELECT c.name, avg(s.salary) as salary FROM company as c inner join salary as s ON c.id=s.company_id inner join employee as e ON e.id=s.employee_id GROUP BY c.name HAVING salary&gt;=40000 ) derived_tbl
Hmmm, it looks like you've gotten stuck with a field that's two different data elements in it and I'm assuming that the qualifier \[salary\] is also based off the varchar field \[name\]. I'd suggest taking \[name\] and breaking it into two fields for data quality, not at real time, but if you really need this to work in the format above, below assumes the last space (' ') in the varchar string is the delimiter between the \[name\] and the \[salary\]/\[other\_value\]. **MSSQL** ***(My Closest Interpretation Based on Your Table Joins)*****:** &gt;SELECT REVERSE(RIGHT(REVERSE(c.\[NAME\]),CHARINDEX(' ',c.\[NAME\]))) AS "NAME", &gt; &gt;avg(s.salary) as salary, &gt; &gt;\--I'M NOT SURE IF THIS DATA IS RELEVANT, BUT HERE'S THE OTHER VALUE IN THE NAME FIELD &gt; &gt;\--(CAST(REPLACE(RIGHT(c.\[NAME\],(CHARINDEX(' ',REVERSE(c.\[NAME\]))-1)),'K','') AS NUMERIC)\*1000) AS "SOME\_OTHER\_VALUE" &gt; &gt;FROM company as c &gt; &gt;inner join salary as s &gt; &gt;ON c.id=s.company\_id &gt; &gt;\--YOU MAY WANT TO CONFIRM IF THIS JOIN IS REQUIRED &gt; &gt;inner join employee as e &gt; &gt;ON e.id=s.employee\_id &gt; &gt;GROUP BY [c.name](https://c.name) &gt; &gt;HAVING s.salary&gt;=40000; **MSSQL** ***(What You Really Might Be Trying to Do)*****:** &gt;SELECT REVERSE(RIGHT(REVERSE(c.\[NAME\]),CHARINDEX(' ',c.\[NAME\]))) AS "NAME", &gt; &gt;AVG(CAST(REPLACE(RIGHT(c.\[NAME\],(CHARINDEX(' ',REVERSE(c.\[NAME\]))-1)),'K','') AS NUMERIC)\*1000) AS "SALARY" &gt; &gt;FROM \[company\] c &gt; &gt;GROUP BY REVERSE(RIGHT(REVERSE(c.\[NAME\]),CHARINDEX(' ',c.\[NAME\]))) &gt; &gt;HAVING AVG(CAST(REPLACE(RIGHT(c.\[NAME\],(CHARINDEX(' ',REVERSE(c.\[NAME\]))-1)),'K','') AS NUMERIC)\*1000) &gt;=40000; &amp;#x200B;
I'm conflicted about whether I like the space after the comma or not. On one hand, I get it - it makes spotting a missing comma even easier - and on the other it looks foreign to me. I'm not sure what you mean by the aliasing - as in being more descriptive with the aliasing? Unlike what I did with the `FROM orders o`? If so, then that's understandable. Generally certain tables for certain databases in our environment, that are queried a lot - like the `CUSTOMERS` and `ORDERS` table will always and only be aliased as `c` or `o` so you get used to it - but I can see where it can be a bit obfuscated. 
What would be a good answer to this question?
I'm also having a little trouble with this. I'm trying to figure out how to come up with the entities and attributes. Any other tips that you can give? All I see is an overflow of information and am having a hard time trying to pick out what the entities are supposed to be.
&gt; Also, not OP. Suuuure.
If all you are returning is just the Company Name then only select the company name and then do the limiting in the actual Where/Having clause. &amp;#x200B; The employee join in this example is an un-necessary join because you aren't pulling any information from the employee table. My personal preference is to also do the left outer join since it should perform quicker than an inner and will do the exact same thing in this instance since you are putting a limiter on salary. &amp;#x200B; SELECT c.name FROM company as c LEFT OUTER JOIN salary as s ON c.id = s.company_id GROUP BY c.name HAVING avg(s.salary)&gt;=40000
This makes a lot of sense. THANK YOU! 
Also quite new to SQL. Thanks for your input! 
Yeah, I figured that'd be the case. can I come back after his due date for the assignment and get my answer?
I recently had a guy with an MS in CS fail to solve this in an interview. He did solve it in scala. That would have been ok for someone out of college but this guy had a decade of experience.
A good answer is a correct solution. This is my go to weeder question for SQL. In this case, the problem is to to use a list of train arrival times to find the average time between stations for all trains. Using LEAD() or LAG() is the most obvious way to solve it. Using a CROSS JOIN is ok, but more expensive. If someone says they know MySQL then I expect them to use temp variable (good answer) then describe GROUP_CONCAT() as an alternate solution (excellent answer, shows you know some of the esoteric shit in MySQL).
Most important concepts are table, record, primary key, association, foreign key, filtering, aggregate functions, join, grouping, having, subquery, derived table, sorting, limit. Go for a practice oriented tutorial. The following resource may help you [http://www.studybyyourself.com/seminar/sql/course/?lang=eng](http://www.studybyyourself.com/seminar/sql/course/?lang=eng).
Most important concepts are table, record, primary key, association, foreign key, filtering, aggregate functions, join, grouping, having, subquery, derived table, sorting, limit. Go for a practice oriented tutorial. The following resource may help you [http://www.studybyyourself.com/seminar/sql/course/?lang=eng](http://www.studybyyourself.com/seminar/sql/course/?lang=eng).
Most important concepts are table, record, primary key, association, foreign key, filtering, aggregate functions, join, grouping, having, subquery, derived table, sorting, limit. Go for a practice oriented tutorial. The following resource may help you [http://www.studybyyourself.com/seminar/sql/course/?lang=eng](http://www.studybyyourself.com/seminar/sql/course/?lang=eng).
Most important concepts are table, record, primary key, association, foreign key, filtering, aggregate functions, join, grouping, having, subquery, derived table, sorting, limit. Go for a practice oriented tutorial. The following resource, for free, may help you [http://www.studybyyourself.com/seminar/sql/course/?lang=eng](http://www.studybyyourself.com/seminar/sql/course/?lang=eng).
"RedHat" Linux. That statement makes me think you actually need to do the reading. If you hadn't of heard of redhat, that is. 
Programming isn't hard. You just have to actually try. The perk is to be a dba you generally have to prove it. And if you dont cut it, you get cut. No one fucks around with their data estate on amateurs unless they plan on training them. And by training them I mean letting them train themselves in your test environment.
 No need to learn any language, you can master the use of some tools and algorithms; 1. Learn visualization tools, such as Tableau or [FineReport](http://www.finereport.com/en/), which are very convenient and powerful for visualization and will be very helpful for your future analysis of business and presentation ; 2. Master the algorithm of AI Machine Learning and use tools (such as Python/R) for modeling.
If you actually mean databases, then yes. Just use Database.Schema.Table in your query. If you mean servers, then no. In that case I would use data connections in Excel to extract the data you need and then combine the result sets using PowerQuery. It might take some time and thought to get it set up initially, but once that's finished all you'd need to do to update the results is refresh the data in the workbook. If there are more use cases than just this one report that require data from both servers, it may be worth looking into setting up an SSIS package to transfer data between the two servers. That sounds like it may be a bit beyond your current skillset, though.
Angry sql. That's a funny mental picture :-). I guess that would work in some organizations. We have a different system. Nothing gets into production databases unless it's tested by QA and has been approved by the Quality Gate. And those people don't care about casing. Also, we have started using u-sql (used in Azure Data Lake) and the language itself is case sensitive. I.e you can't write angry sql. It won't compile. So you can't trust the casing anyway.
Check out the Import/Export wizard. It's relatively simple to do, but you'll have to do it each time you want to run that report. It's not automatic.
 **Use data analysis as a capability to develop** Most of the work now requires you to have the logic analysis ability, especially the analysis and understanding of the data. Today, with the deepening of the concept of data operation, large Internet companies such as BAT emphasize that all employees participate in data operations, and data analysis as a capability in training is also a future trend. The skills and knowledge required by data analysts (4 steps) The four steps of data analysis: data acquisition, data processing, data analysis, data presentation. **1, data acquisition** Data acquisition seems simple, but it needs to grasp the business understanding of the problem, and transform it into a data problem to solve. To be straightforward, what kind of data is needed, from which angles to analyze, and after defining the problem, then collect data. This link requires data analysts to have structured logical thinking. Recommended books: "Pyramid Principles", McKinsey Trilogy: McKinsey awareness, tools, methods; Recommended tools: mind mapping tools (Xmind Baidu brain map, etc.); **2, data processing** The processing of data requires an efficient tool: **Excel and high-end skills**: Everyday work is common, easy to master, and it is easy to process 100,000-level data. [FineReport](http://www.finereport.com/en/): Professional reporting tool, a daily report design can be used as a template, as long as you can write SQL to get started. Compared with excel reporting, the development of technical requirements is less, can quickly develop regular reports, dynamic reports, and can be placed on the mobile and large screen viewing. Oracle and SQL sever: The most commonly used tens of millions of databases in the enterprise, proficient in the SQL language. Maintain continuous technical learning, such as learning a new and popular distributed database such as Hadoop to enhance personal abilities and help with job search. **3. Analyze the data** Analytical data often requires various statistical analysis models, such as association rules, clustering, classification, prediction models, and so on. Therefore, mastering some statistical analysis tools is inevitable: **SPSS series**: old statistical analysis software **SAS**: Classic mining software that requires programming. **R**: Open source software, new and popular, more efficient for unstructured data processing, requiring programming. **Various BI tools:** [Tableau](https://www.tableau.com/): the originator of the visualization tool, freely visual analysis of the processed data, the chart effect is amazing [FineBI](http://www.finereport.com/en/business-intelligence): Similar to Tableau, it can perform arbitrary dimension analysis on the front end; data can be processed at the front end (computation, filter and filter, etc.), and can be connected to a big data platform such as Hadoop, and the data processing performance is better. **4,** [data visualization](https://www.finereport.com/en/product-features/data-visualization.html) Many data analysis tools already cover the data visualization part, and only need to effectively present and report the data results, which can be displayed by wordPPTH5. It's a long road, but you have to master the tools and skills and build up and you'll find out how good you are. Hopefully!
Coupon code has expired.
&gt; do the left outer join since it should perform quicker than an inner wut???
&gt; A non-correlated subquery, I think, is where the results of the outer query depend on the inner I'm assuming thats just a typo rather than a misunderstanding. It should read correlated (because the inner query is correlated in some way to the outer). An inline view is where you select from a resultset which is also a select. ie.. select a.id from (select username, id from users) a
Do you have any recommendations for books or websites that a newish person can read that will help build the knowledge up to be able to tackle the question you asked above? 
&gt; A ~~non-~~correlated subquery, I think, is where the results of the outer query depend on the inner, and the inner query is evaluated for every row in the outer dataset You have this backwards. The correlated sub-query is when the **inner** query depends on the **outer**. So, using your example, it would look like this: select * from a group by x having x &gt; (select average(y) from b where b.col1 = x.col1); so your inner query is now constrained by some value of the outer (this can be done in many ways, the point is that a value from the outer is being referenced in the inner) https://en.wikipedia.org/wiki/Correlated_subquery 
It looks like your inner query has 2 columns with the same name. The outer query can't distinguish them
An in-line view is a subquery you treat as a table For example: SELECT outside.col1 FROM ( SELECT inside.col1 FROM example_inside inside ) outside A correlated subquery is based off the outside table's results: SELECT outside.out_col_a, ( SELECT inside.col1 FROM example_inside inside WHERE inside.COL2 = outside.out_col_b -- Pay attention to the where statement -- The value selected from inside is dependant on the value passed from outside. ) FROM EXAMPLE_OUTSIDE outside Thus, an non correlated subquery is a subquery that has no reference to the outside query. SELECT outside.out_col_a, ( SELECT inside.col1 FROM example_inside inside -- No reference to outside in here. ) FROM EXAMPLE_OUTSIDE outside
How could I fix this? Haven't encountered it before. 
make one "as" as well
Perfect - thanks!
**Correlated:** SELECT 1, (SELECT 2 FROM b WHERE a.x = b.y) FROM a; Notice, [a] is used inside the sub-select, causing the engine to execute the sub-query for each record returned by [a]. **Non-Correlated:** SELECT 1, (SELECT 2 FROM b) FROM a; Since this is uncorrelated, the sub-select is independent of the outer query. This means the engine can (and likely, will) only run this query once. **Derived Table / Inline View:** SELECT 1 FROM (SELECT 1 FROM a); These can be correlated as well. They offer a chance to project and filter in advance of further query operations.
This isn't a simple assignment, regardless of what the OP says. Based on the information provided you'd need to have an understanding of RHEL, a web application framework such as (NodeJS + ExpressJS), JavaScript so that you can properly setup the web application framework, HTTP Get and Posts, A database to store the information needed, SQL, etc. This, to me, seems like a final project and something that OP probably would have known about in advance. Also, the $100 price tag is laughable considering that this would be most comparable to a Full Stack Development assignment. 
Oh, we QA things obviously, but that doesn't mean we go back and format the code. This is especially true in iterative data sources where we expect to have to spend a few weeks of making incremental changes before it is completely finished. Nothing is "finished" for us until it is consumed by the application layer and accepted by the business. The SQL is an important piece in that process, obviously, but we rarely will modify code to style until we are sure it is done.
Correct, I would never call `orders o` or any other shortened name. It would be A, B, C, or X, Y, Z, or B1, or X2, etc. I set the aliasing up more like an equation so that in my where conditions / joins I can diagram it more easily in my head... this is A, it joins to B, and then C joins to B... this is X1, it joins to X2, they both become C, and C joins to B, etc. I've been told by some people this practice is very annoying to not use shortened aliases such as O, or SH, or DR, etc. but it was how I learned and it's what makes me comfortable.
Yes you can but we'd need more information first. The SQL part of the problem appears to be easy with the data that was given. He will have an array of sites and will query them against a database and change the passwords of those sites only. This can be as easy as a simple UPDATE query or a lot harder depending on how the DB was structured. The "harder" part would be to develop a tool (maybe a simple curl could solve it) that would issue requests to that server and get the site data to be inserted into above query. As SQLSavant wrote there are some different areas regarding the problem as a whole but not much depth into any of them (simple http get request, store in an array, simple update query(ies)). Depending on the language that would amount to as low as 10 lines of code. But to really answer how I'd do it I'd need more information: How is the DB structured, if I have access to the server (to get the website routes or better yet the user table(s)), to see how exactly the metadata has user login/passwords, if I can access the users, etc... 
I only partly agree... I agree that if you try and are curious enough you can definitely learn programming. But not everybody likes it, which makes it really hard for some people (to even start/keep practicing), so only those that really are into it will succeed on the field. OP doesn't seem like he will, there are lots and lots of hard problems that require the time and honestly can't be outsourced. 
Google DISTINCT or GROUP BY.
You can continue with your way by adding a group by and having clause: SELECT CustomerID FROM PRODUCT WHERE ProductID in ('A', 'B') GROUP BY CustomerID HAVING COUNT(DISTINCT ProductID ) = 2 But group by clauses are usually less than optimal...you can also do this as a join or a subquery--I'm fairly certain the SQL engine will re-write the subquery as a join anyway, so let's do it that way: SELECT c.CustomerName FROM Products p1 JOIN Products p2 on p1.CustomerID = p2.CustomerID JOIN Customers c on p1.CustomerID = c.CustomerID WHERE p1.Product = 'A' and p2.Product = 'B' http://sqlfiddle.com/#!9/7dab97/4
Distinct I get, but it doesn't really help. If I throw distinct in there, it will give me each row that is distinct: so CUSTOMER with Product A is distinct from that same exact customer with Product B so that'll make two lines. Times that by thousands I'm still matching up customers manually to find out which have both Product A AND Product B. I did try that, but thank you for the idea. As for GROUP BY, I'll dig into that. Thank you. 
Here's what seems like another good start: https://www.udemy.com/the-complete-sql-bootcamp/ I recommend leetcode as you learn. Start with simple SQL questions. Download MySQL so you can play with a local copy of it on your machine. Then progress to the advanced SQL questions.
Oh, I understand. I didn't read your question closely enough. Check out what ihaxr showed in his example with a join. Another way you can mentally visualize this would be using a CTE such as: with cte as ( select customer from table where product = 'a' ), cte2 as ( select customer from table where product = 'b' ) select a.customer from cte a inner join cte2 b on b.customer = a.customer You can accomplish something like this in multiple ways, may need to use DISTINCT or GROUP BY if someone purchases Product A more than once, etc. You can google WHERE EXISTS as another example.
Thank you so much, I will try those. 
powershell if you can wait a day, i'll share what i have that does mostly that. 
That would be a huge help, thanks
When you say a "query in excel", you have something in excel that runs and then you want to save it as CSV? or have something ran within excel and then exported? 
I use Excel purely as a means to view and manipulate the data, I essentially just want to export the results of a query to a CSV file somehow, without the use of Excel
could you do something like this: https://stackoverflow.com/questions/3710263/how-do-i-create-a-csv-file-from-database-in-python and then use windows scheduler to run it on the schedule that you need? admittedly i jump between .net and python, but i'd go straight to .net for this because i have some similar things already made.
You could mail it, attatching the query results as csv using sp_send_db_mail
The data is in SQL, or Oracle or ??? and i was assuming you had Windows, correct?
I set this up with Python + AWS lambda/cloudwatch with a CRON. Guessing others will have more efficient approaches
SSIS 
You've been given some good specific solutions, but I have some general advice. Think of it this way: Get all Customers, where they are in: * List of customers buying A * And list of customers buying B There are any number of ways to accomplish this. The most intuitive to me is something like this: SELECT * FROM Customers WHERE CustomerID IN (SELECT CustomerID FROM Sales WHERE ProductID = 'A') AND CustomerID IN (SELECT CustomerID FROM Sales WHERE ProductID = 'B') Note: I'm assuming your ProductID column doesn't live in the "Customers" table, since that wouldn't really make sense. I made up the "Sales" table. 
If your company allows and you have permission, you could create a linked server
sql agent job - BULK EXPORT or bcp.exe or SSIS package file (dtsx)
No, all this information I need is in the same table, that's what's making it so difficult. I've been trying for several hours now with no luck. I managed to get hold of someone who's pretty good at SQL however and he supplied me with a query that works. Basically: Select Customer ID, Product ID FROM Onlytable a where Product ID = 'a' and exists (select 1 from Only table where a.customerID = customerID AND Product ID = 'b') 
That's another way to do it. Either of my queries would do the same thing, just replace "Sales" with "Customers". I assumed it must be separate because obviously a customer can have multiple sales. You don't want to duplicate all the customer information every time you make a sale. In all of these you're doing the same thing - starting with a base set of customers and applying two separate filters to it.
I would use python for this. You can use pandas read_sql_query function to read your query into a dataframe, and then pandas to_csv function to dump the dataframe to your local disk.
I greatly appreciate the help, thank you. 
Actually, there is a minor problem with your query. What are you hoping to see in the ProductID column? Right now, it will just show "A" for everyone because of how you're filtering. Maybe that's ok, just thought I would mention it.
I essentially need to have a customer listed once who has both product A and product B. So technically in the product ID column it doesn't matter if it shows only product A or product b so long as the customer it pulls up has both. So I think it's ok. I only need one to SHOW as long as it's pulling both behind the scenes.
Perfect. It will be all "A" so you might as well just remove it from your SELECT statement. One last thing - you will get multiple rows for the same customer if they purchased product A more than once. Add DISTINCT to fix that.
I second this, Plenty of good vids on YouTube to get you started also! 
Cheers ill check it out! Do you know if there are any good online tests to quickly gauge your skill level? 
Use a windows batch file to run sqlcmd to run the command and export to csv. Examples here https://stackoverflow.com/questions/425379/how-to-export-data-as-csv-format-from-sql-server-using-sqlcmd Then just add it to task scheduler in windows.
You could automate your current excel file to open daily using a simple bat file, then have your refresh and export to happen on open in vba 
When you say reporting and SQL, that typically means a reporting product like Jasper, Business Objects, Tableau, etc. There's a lot of vendors that run a lot of different products specifically tailored to making pretty SQL reports. Every different flavor of SQL has it's own vendors too. I think what you were really asking about, though, is pulling and formatting your data for reports. In that case, I think the following are good places to start: * Joins and Unions * Indexes * Group By * Case Statements * Exists 
I didn't this a lot.
I’m looking for resources (books, Udemy, etc) that would be useful for a person like myself (just now getting out of beginner to intermediate) to learn reporting in MySQL. It it helps to know, I’m using SQL for work pulling basic purchasing info and I just want to be better equipped to be a resource at my job when it comes to getting info out of the system. Thanks in advance! Edit: I’m specifically speaking about SSRS reports. 
Thanks for feedback, but I’m specially speaking of learning of creating reports in SSRS. 
[removed]
Out of curiosity, what was the problem you gave for the interviewee to solve?
This was also my approach, after spending two weeks wrestling with the various minutiae of configuring a development environment for Azure and then troubleshooting the errors. I pine for the days when we controlled our own boxes...
Thanks! Have you tried any others? 
I'd be curious about who's using the CSV for what and if they can connect to the database instead. 
Given a table with train_id, station_id, arrival_datetime, find the average transit time between each adjacent station.
My only thoughts are (SYSDATE - (240/(24\*60))) could be converted to (SYSDATE - INTERVAL '4' HOUR) for easier readability. I'm also not sure if it's necessary for the FOR/LOOP to contain an extra check against the last active time for the same datetime qualifiers as the logon time. To me, it just seems easier to put that qualifier in the c\_login cursor and cancel out the need for the extra IF/END IF statement. I could be completely missing something, though.
SSRS would be related to MS SQL, not MySQL 
Create a query.sql file SELECT metrics FROM table WHERE foo = 'bar' INTO OUTFILE '/var/lib/mysql-files/output.csv' FIELDS TERMINATED BY ',' ENCLOSED BY '"' LINES TERMINATED BY '\n'; Add this to cron 0 0 0 * * * mysql -u USERNAME -pPASSWORD -h HOSTNAMEORIP DATABASENAME &lt; query.sql &gt; /dev/null 2&gt;/logs/query.log With some inspiration I might put that mysql command in a bash file, capture date parameters, then change the files to output_YYYYMMDD.csv and query_YYYYMMDD.csv. 
I have also used Database Browser Portable but I didn't care for it much. It did the job I needed it for but I would not use it again. Also I have used MS Access as a front end for MS SQL Server databases. 
[Wise Owl Tuutorial](https://www.youtube.com/watch?v=LYroSet0asQ&amp;list=PLNIs-AWhQzcnjESI1CcsLI6jHqIi_TVTZ)
What database are you using?
Thanks, I called db.schema.table and fumbled through the rest and it worked smoothly. I'm interested in learning more about the innards of SSIS packages. I just deploy the updates to the packages, update data set xmls, and run full loads. That's about the extent of messing with them. I'm not privy to the function of developing the packages, just privy to bitching and moaning on those update days. Great responses, maybe I'll post here more often. Thanks, again.
Give this a shot. I think it \*might\* be what you're looking for. There may be some extra details I'm missing. &gt;SELECT &gt; &gt;products.CategoryId, &gt; &gt; products.ProductName &gt; &gt;FROM &gt; &gt;categories &gt; &gt;INNER JOIN &gt; &gt;products ON categories.CategoryID = products.CategoryID &gt; &gt;INNER JOIN &gt; &gt;order\_details ON products.ProductNumber = order\_details.ProductNumber &gt; &gt;GROUP BY products.CategoryId, products.ProductName &gt; &gt;HAVING SUM(order\_details.QuotedPrice \* order\_details.QuantityOrdered) &gt; AVG(order\_details.QuotedPrice \* order\_details.QuantityOrdered);
Unfortunately I'm not familiar with Python, only VBA and JS
It's the comparing *product* sum to *category* average that I'm stuck on. I think what you've written is still comparing product sum to product category (because each row is by product).
Unfortunately I'm not familiar with Python, only VBA and JS
Unfortunately I'm not familiar with Python, only VBA and JS
This is what I currently do but it's not an optimal solution
I like this solution actually, I'll look into this
Yes, normally this would probably be the best approach but due to work related restrictions it's not possible
SQL and yeah windows. 
This was the kind of thing I was looking for, will give it a go.
It's for our web server and warehouse database but unfortunately connecting the two directly isn't possible right now so I have to set up a scheduled export and import of data for the time being.
Glad it worked out for you. From the sounds of things, you may be further along than I'd assumed. If you want to get hands on, try to get exposure to the folks in your org that develop the packages. In my experience, if you express interest and the ability to learn then people are happy to lighten their workload by dumping some of it onto you. Good way to get your foot in the door. As for posting here I'm relatively new here myself, but most folks seem knowledgeable and helpful. Don't be shy.
Unfortunately INTO OUTFILE isn't available to us and also due to work restrictions we can't access the cron.
I see... It's not very pretty and not how I'd write it, but give this a shot. &gt;SELECT products.ProductName &gt; &gt;FROM &gt; &gt; ( &gt; &gt; SELECT products.CategoryId, &gt; &gt; AVG(order\_details.QuotedPrice \* order\_details.QuantityOrdered) AS "AVG\_FOR\_CATEGORY" &gt; &gt; FROM categories &gt; &gt; INNER JOIN &gt; &gt; products ON categories.CategoryID = products.CategoryID &gt; &gt; INNER JOIN &gt; &gt; order\_details ON products.ProductNumber = order\_details.ProductNumber &gt; &gt; GROUP BY products.CategoryId &gt; &gt; ) derivedTable &gt; &gt;INNER JOIN &gt; &gt; products ON derivedTable.CategoryID = products.CategoryID &gt; &gt;INNER JOIN &gt; &gt; order\_details ON products.ProductNumber = order\_details.ProductNumber &gt; &gt;GROUP BY products.ProductName, derivedTable.AVG\_FOR\_CATEGORY &gt; &gt;HAVING SUM(order\_details.QuotedPrice \* order\_details.QuantityOrdered) &gt; derivedTable.AVG\_CATEGORY; &amp;#x200B;
 I see... It's not very pretty and not how I'd write it given the option, but as it needs to have GROUP BY/HAVING give this a shot. &gt;SELECT products.ProductName FROM ( SELECT products.CategoryId, AVG(order\_details.QuotedPrice \* order\_details.QuantityOrdered) AS "AVG\_FOR\_CATEGORY" FROM categories INNER JOIN products ON categories.CategoryID = products.CategoryID INNER JOIN order\_details ON products.ProductNumber = order\_details.ProductNumber GROUP BY products.CategoryId ) derivedTable INNER JOIN products ON derivedTable.CategoryID = products.CategoryID INNER JOIN order\_details ON products.ProductNumber = order\_details.ProductNumber GROUP BY products.ProductName, derivedTable.AVG\_FOR\_CATEGORY HAVING SUM(order\_details.QuotedPrice \* order\_details.QuantityOrdered) &gt; derivedTable.AVG\_FOR\_CATEGORY; Edit: Reddit really messed up and added a lot of special characters. I think I fixed it all.
Cool, I have lots of reports running like this.
What data type is that field? Also why the need for a distinct? 
my first hunch as well. A text field would explain this. &amp;#x200B; and no reason for the distinct, imho.
Why not try using the tcp\_keepalive parameters that can be set in sysctl.conf? &amp;#x200B;
Yep, in a text field a max on numbers will bring back the highest first character. In this it would be a nine.
Yes it is text, I have to store big numbers and normal number-types werent enough. Any ideas how to fix this?
&gt; I have to store big numbers how big?
do a cast SELECT MAX(CAST(wert AS int)) FROM normal 
around 800 digits or more
Create a temporary bigint field, select wert into with convert(), drop the wert field, and then rename the temporary field? 
wut??!! could you give an example of one of your larger numbers? also, what's the use case? what do these numbers represent? in any case, VARCHAR(937) is your only choice make sure you right-adjust every value and fill with zeroes on the left 
What’s the biggest possible number you will use? Create a new column with the correct data type then cast or convert the text value to update that field, or start again if feasible with the correct data type. BIGINT can potentially store up to 18446744073709551615
Ouch. You'll have to left-pad your strings with zeroes, then `max` would work, but I would consider changing to a database which supports numbers with that many digits. PostgreSQL supports up to 131072 digits for the decimal type.
200545053750562232848736693522597838706935447352370269716215218452260069676138954927830051489443504374415475356984845992537431390474677567532448373286887480303177292341315969391700016452324013896820990227300381913750929771100272951855357973201457068801652939176205099961110727203875386429450576000003617477845553969570500175501640559834921729095811144410464907218759285325151862977083174447542544484379373913391111732042041428633156752013628125672567026190180351361035073223722088794847497131848482831926833235777568346552957683053241204849421865866697006021408626555074816814771122862815129389981029216232699588067978063669414281467214452694190020078856434947572698550492313862510947821932687770866893704038336298888589238375910478528591008364902985637047189043206933361899210602106660427527230914754210018744756763420541679581751589 is an example. I am working on a math software that stores number sequences on a server so its an one-time calculation for every n of f(n). The number above is from the Fibonacci-sequenz. 
Note: I solved it by first finding the biggest value that I get from MAX(value) and then selecting all values that are bigger than MAX(value), sorting it backwards and limiting it to 1. &gt; SELECT * FROM normal WHERE wert &gt; 99991 ORDER BY wert DESC LIMIT 1; Works well in my code.
Select top 1 wert from normal order by len(wert) desc, wert desc
Drop the line of code stating the foreign key relationship. The other table object its referencing doesn't exist. 
Please refer to the Format Your Code section of the sidebar. What version of MySQL are you running? Invisible indexes were added in MySQL 8. Prior editions will not understand what the VISIBLE keyword means. 
i am using 8.0.12 brother
I have done something similar, we would record gaps in the primary key as it would increment. Sometimes SQL Server will skip a generated number, most commonly when a transaction is rolled back and the seed isn't reseeded or when the server is restarted. The application was not smart enough to recover from these gaps, so we needed something to track it. The process to determine skips was easy, create a CTE or temp table, grab the max key and the next couple digits under it. Once it skipped, the application would begin to hault, so it would stop generating new keys. So for the mail portion, I had it log each time it ran this check. The table it logged to also incorporated the notification feature. The key piece of this is that I wrapped my table logging statement with my email notification statement, which would be your stored procedure, in a single transaction using begin and rollback transaction. If an error occurs during the send email / logging notification, it would jump to the error handling piece that still inserts and notifies it errored out. If it does not, it would log that it has sent an email notification and send an email. A part of that statement that logs and sends the email is structured to do a check based on configuration settings in either another table or that procedure. It would use parameters to determine the maximum number of times to notify within a hour, the threshold of skips to look for, etc. This was the logic check that happens first and then proceeds onto the mail notification if it hits the right filters. &amp;#x200B;
That shouldn't result in a *syntax* error. It should be "table does not exist" or "column does not exist in specified table" or something similar. This is an error at the command parser level. 
Make sure you get some keys on those fields so you can safely use select top 1 in a loop and not block inserts from whatever process is adding rows. FIFO. 
Code templates? I have one for: SELECT * FROM WHERE ROWNUM &lt;=1000 The cursor lands right after the NUM and I just enter the table name when I call the template with CTRL+ Space.
&gt;what's left of the NoSQL movement? Did it really waste half a decade I would say no. Whatever NoSQL ultimately turns into, it needed to go through those growing pains. That's pretty common with new technologies.
This *might* work. I've commented the sections that you might need to play with. The syntax is strange to me so someone else might be able to comment and give you more assistance. Songs.ID IN ( SELECT Songs.ID FROM Songs , ( SELECT DISTINCT GenresSongs.IDGenre AS IDGenre FROM GenresSongs ) AS G1 , ArtistsSongs --here , Artists --here WHERE Songs.ID IN ( SELECT Songs.ID FROM Songs , GenresSongs WHERE Songs.ID = GenresSongs.IDSong AND GenresSongs.IDGenre = G1.IDGenre ORDER BY Random() LIMIT 500 ) ORDER BY ArtistsSongs LIMIT 5 --here ) AND &lt;Genre&gt; IN ('Alternative', 'Asian Music', 'Dance', 'Electro', 'Folk', 'Metal', 'Pop', 'Punk', 'Rap/Hip Hop', 'Reggae', 'Rock')
Thanks for the hint. I'll look into how to implement these.
I'm not familiar with Magic Nodes, but do you know if it would be possible to add a: ROW_NUMBER() OVER(PARTITION BY ARTIST ORDER BY [Qualifer of your choice]) AS "RN" ...then limit your results where RN &lt; 6? 
Marklogic is acid Meh
 $basepath = "c:\folder\folder\" $timestamp = date (get-date) -format yyyy-MM-dd $cn= New-Object System.Data.SqlClient.SqlConnection("Server=servername;database=databasename;trusted_connection=yes;") $q = "exec dbo.ProcName" $da = New-Object System.Data.SqlClient.SqlDataAdapter($q, $cn) $da.SelectCommand.CommandTimeout = 1500 $ds = New-Object System.Data.DataSet $da.Fill($ds) &gt;$null| Out-Null $cn.Close() if($ds.Tables[0].Rows.Count -eq 0){ Write-Host "No Data found") return } $file = $basepath + "filename_" + $timestamp +".csv" $ds.Tables[0] | export-csv $file -notypeinformation 
VIEW &gt; Toad Options &gt; editor &gt; Code Templates Add new. To use: CTRL+ Space and start typing the name of your template. It will automatically populate the query into your editor when you select it and press enter.
what happens if the mail fails or is otherwise delayed? 
Oops! Forgot the distinct :) Thank you again. Will do.
Thanks. Usage is a bit more than SSMS right click, but still a big improvement. Thanks.
Yep. I work with both all day and this is the closes my I’ve gotten.
That won't scale correctly when your numbers start being a factor 100 apart. Nathan\_nuggets' suggestion makes more sense to me (i changed the sql server dialect to smt mariadb-compatible): `select wert1 from normal order by length(wert1) desc, wert1 desc`
The sidebar's wiki for this subreddit has a lot of good info! I'd recommend w3schools 
that's insane yeah, VARCHAR (or CHAR) is your only hope by the way, did i tell you my latest Fibonacci joke? it's just as funny as my previous two Fibonacci jokes combined!! 
This use case is a great way to get a nice introduction to Python.
[Codewars](https://www.codewars.com) is the same sort of thing, and has a SQL variant. 
Hahaha great one, I hear a lot of math jokes but I didnt know this one yet
You seem to be right, thanks!
You mean besides people realizing it was a stupid name for non-relational databases?
SQL is a pain in the ass when working with applications but it's amazing for querying and live reporting. Having a "schema-less/migration-less" database is awesome when you know you are working with large databases where downtime needs to be kept to a minimal and ingress is more important that immediate data continuity. If you go the NoSQL route you are then accepting that your data will be loosely maintained. You may have to go through and historically update tables to conform to the new schema and doing that after the fact, maybe ok with you. If you store your objects multiple properties and children, your tables will be quite large and this may make reporting quite slow in comparison to if this was in a normalized RDBMS. I strongly believe in both tools for different jobs. NoSQL is stronger than ever, it's just that it has been found that it cannot cure cancer and it doesn't solve the issue of world hunger.
Im not familiar. Tell me more please.
This comes close https://www.sqlservercentral.com/Forums/Testing-Center/Question-of-the-Day-QOD
Google tcp_keepalive + oracle, your bound to get much better explanations than I can give you. IMHO far better than triggers. 
Man im going to sound stupid, but could you dumb all this down, just a little bit? &amp;#x200B; I understand where are you going with this, I dont really understand the table logging with your email notification statement. So you have an extra table logging errors in case the notification fails?
So imagine you have two tables, one is your configuration and the other is a table that logs the actions. &amp;#x200B; So if MaxRun = 1 and MaxRunTime = 15:00, it means that the notification will be sent only once and after 15:00. &amp;#x200B; Your configuration table contains those parameters, but how does your SQL Statement know the query has already ran one time where the query lasted past 15 minutes? What's going to keep it from running again five minutes later and reporting the same long running query? This is where that logging table comes into play. You would probably want to insert the MessageID, current date / time, and if an email was sent. Let's use your maxrun and maxruntime as an example. First time your procedure kicks off, it would analyze your config table and see X query is in the threshold to notify users. It would do an insert into the logging table with the messageid, current date/time, and a flag to indicate that it is going to send an email. Now if your procedure sees that there is one entry for the messageid in the logging table, it can go ahead and sent that email. If the procedure ran a 2nd time, it would see there are two entries for the messageid, indicating it has already notified which would surpass the max run of 1. The point I'm trying to make regarding the max run filter, is that you need a way to capture the number of times the query has already ran within your filters. (The life of the query, perhaps the span of a hour, maybe a day.) If you can't figure out how many times that SQL has ran historically, you won't be able to know the number of times it has ran. This table also serves a second purpose, review and troubleshooting. There have been times where people asked me, why didn't the alert trigger? Well... if it runs an insert on that logging table, you're inserting a snapshot of what was happening. You can now dive in and report back, it didn't alert because it alerted one time previously.
&gt; it's amazing for querying and live reporting Can you explain this? I thought something similar to SQL VIEWs was basically non-existent, or at least harder with a lot of nosql. I use heaps of layered viewed these days in SQL, and it reduces a huge amount of what used to be crappy application level code. How is nosql better than what SQL offers with views, triggers etc? Especially taking into consideration how much you can do with JSON in postgres.
NoSQL thought ACID and relationships weren't necessary. Until people wanted reporting. Now NoSQL is implementing ACID and relationships. As was foretold by the prophecies (and database admins who knew better to begin with).
Are you asking my why SQL is amazing for querying and live reporting? SQL beats NoSQL hands down on querying performance and with proper normalization is really efficient with space. It's just that the big SQL engines really haven't kept up with scalability and distribution. NoSQL isn't better than what SQL offers for views. NoSQL is great for being a document repository of "loosely defined" objects. I wrote &gt;SQL is a pain in the ass when working with applications but it's amazing for querying and live reporting. If you've ever written application code that works with object orientated code you usually end up using a orm, writing your own orm, or you delicately have a bunch of string builders putting together your sql code. The results are great if your database is properly normalized. However, SQL seems to give a lot of devs trouble. This is where NoSQL shines. They can serialize and object to it's primatives and literally slap an API with raw serialized data they'll consume it. Where data is loosely defined and constraints aren't a huge concern, it's basically licensed murder. Over the time the data isn't that reportable. The "NoSQL SQL" engines they have do not run as well as their SQL counterparts but are good enough for simple extracts and very simple reporting. Right now I'm pushing hard core to split our application into a SQL and NoSQL solution as my company wants to start pushing terabytes of blob data to us from various sources. Thats where I'm pushing NoSQL. The data schema isn't that important, I just need to extrapolate the most important data from it and build sound dimensional models, and that is where SQL really shines. 
As someone who has had developers use dbmail in the past to send emails.... don't do it. Just don't. Write a .net application or powershell or python or anything and have it do the work. The logging for troubleshooting issues with dbmail is almost non existent. It's basically an OS call to a little mail forwarding executable that ships with sql server and is probably 32 bit single threaded. It's a black hole, it does not log to the event log and returns little back to sql server. Find something else and use your sp to feed it the data to process.
&gt; However, SQL seems to give a lot of devs trouble. I agree 100%, although to be clear this is not because of some problem with SQL. It sounds like you may be working on an optimal solution. Very intriguing. 
Isn't answering n00bz SQL homework assignments on Reddit good enough? You get karma?
Whoops, sorry. I misread your comment as you saying "nosql is amazing for querying and live reporting" ... which is the opposite of what you actually said. But I'm interested in your points generally. &gt; The results are great if your database is properly normalized. However, SQL seems to give a lot of devs trouble. This is where NoSQL shines. They can serialize and object to it's primatives and literally slap an API with raw serialized data they'll consume it. Where data is loosely defined and constraints aren't a huge concern, it's basically licensed murder. Does this seem like a bad thing though? Isn't it basically saying "devs lacking experience in SQL find it hard, so we'll skip it and just let them do everything in application code" ... isn't this kind of making project technical decisions based on the lowest common denominator in your team? i.e. the people who aren't experienced with databases in general. I know that I was one of these devs... I'd say for my first maybe even 15 years or so as a webdev, I was doing all sorts of crap in app code that I later realized is so much better done in SQL. And most of it only took a few days of learning. &gt; Right now I'm pushing hard core to split our application into a SQL and NoSQL solution as my company wants to start pushing terabytes of blob data to us from various sources. Thats where I'm pushing NoSQL. The data schema isn't that important, I just need to extrapolate the most important data from it and build sound dimensional models, and that is where SQL really shines. Well that sounds like it makes sense. I guess most blowback against nosql is when people think it's all that's needed, rather than an extra piece of a specific non-whole portion of your data. But it feels like nosql is very rarely how a project should be started. If you get to limitations where somebody who knows SQL well thinks some nosql would be suited for some stuff... then that's probably the right decision. Just seems crazy for 95%+ of typical webdev projects though. &gt; SQL is a pain in the ass Are you just talking about for people that don't know it? I know it well, and I don't see how it's a pain in the ass aside from the fact that we need to go out an find out own migration systems... it would be nice if this was standardised so that it was a mainstream included feature in each sql db. I've been working on something myself over the last few months and it's going really well. It uses typeorm for the actual automated migrations, but I can simply define the schema in code and it actually handles foreign keys and stuff like that (including matching the type and automatically naming the FK columns by default etc), including generating and updating the ORM model classes. So yeah, this is an area where it's a pity this stuff is more standardised. So my schemas are very easy to define, and there's no redundancy needed with the foreign keys or anything like that. But it seems to me the whole schemaless thing is just going to create a huge mess that you need to debug in the future, which is going to be much more work than defining a schema to begin with. I'm right now dealing with organisation millions of JSON files I've been collecting over the last 5 years or so, mostly with the same formats, but sometimes they vary etc... it's a fuckload of work going back and figuring out the different schemas etc, especially all their minor changes over time. So much that I'm having to also write some automated system to keep a database of detected unique schemas based on property names etc. Obviously no option to have constraints here seeing they're just .json files in tarballs etc. But if I could have put constraints on them, the whole job to actually use the data right now would be trivial compared to needing to reverse engineer all this shit. Even just determining the context of the files from the filepaths got to become such a pain that I ended up writing code to just detect the purpose of the file based on its content (JSON property names etc).
Access is nice for small- to medium-sized datasets, but I know the feeling of it getting horrendously slow. It also may not be the easiest thing to transition, based on how users currently interface with the Access database(s). Perhaps branch out a small portion of the existing logic into MSSQL, giving it a beta run in parallel with Access, and see if there are more pros than cons for your company. That way it's easy to fall back to the existing technology if you absolutely have to. That being said, I can't see and haven't seen an Access database get worse after it was transitioned to MSSQL.
For the first one, I'm not familiar with the accept argument, I can really only help with the SELECT, but this might be a push in the right direction. &gt;accept vPName CHAR FORMAT 'A4000' - &gt; &gt;prompt 'Enter employee lastname: ' ; &gt; &gt;select p.\* &gt; &gt;from Product p &gt; &gt;where UPPER(PName) = UPPER('&amp;vPName'); For the second one, again I'm not familiar with the accept argument, I can really only try to help with the SELECT, but this might be a push in the right direction. &gt;accept vQtyOrder NUMBER FORMAT '999.99' DEFAULT '000.0' - &gt; &gt;prompt 'Please Enter the Quantity Desired: '; &gt; &gt;select 'Amount Ordered: '||(p.UnitPrice \* o.QtyOrder) AS "AMT\_ORDERED", TotalCost &gt; &gt;from Product p, Orders o &gt; &gt;where p.UnitPrice = o.UnitPrice &gt; &gt;and QtyOrder = '&amp;vQtyOrder';
SQL is a language that is standardized by ISO. Each database vendor has their own unique "flavor" of SQL. https://en.wikipedia.org/wiki/SQL:2016 is the latest but no vendor has caught up to that yet So, you need to pick a database vendor, then the choice of version is done for you PostgreSQL and SQLite are pretty easy choices these days. PostgreSQL is a database server, and SQLite is designed to work locally on one machine
**SQL:2016** SQL:2016 or ISO/IEC 9075:2016 (under the general title "Information technology – Database languages – SQL") is the eighth revision of the ISO (1987) and ANSI (1986) standard for the SQL database query language. It was formally adopted in December 2016. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/SQL/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
Watching a few youtubers has shown me both MySQL is an option also server management editor?
MySQL is incredibly popular, in terms of use. You'll find excellent documentation for it, as well as community support. Just keep in mind MySQL is from Oracle, who tend to be the baddies like M$ when it comes to certain sections of the community.
Yea the manual is pretty weak/confusing... However it's the only documentation I can find. I've only had experience with MySQL and it seems far more easy to work with. Thank you for the help thus far. I'll see if I can figure it out over the next couple days. If I can I'll send you that $5 :) I think there is a good chance it'll work. 
I'm running into a similar issue at work, but SQL Server isn't an option. Would leveraging Power Query/Pivot and/or Sharepoint help me at all?
If they don't have an interface to manage the data or developers to make one, you may want to look at a CRM system. Most popular is Salesforce but there are other options depending on needs.
Check out dbeaver. I couldn't prop it up more. I hit teradata, Oracle, ms sql, and vertica. You can right click on a table and click read data in query window like ssms, and it builds the query for you. You can also have many different environments opened in one object explorer, and you can highly customize how those objects are organized. Just try it, it's free.
I don't want your money, mate. Thanks though.
SharePoint wouldn't be my first suggestion... some things it can do well, but if volume limits are what you're bumping up against, SP only gets worse without a lot of special effort. PQ/PP is just for visualization and reporting. SQL is for storage... Excel/PBI for reporting you might be able to consider either Azure SQL Service (pay-by-the-use), or perhaps something like Postgres on-prem
Pivot is expensive memory wise. Power query just adds another layer. You'd be better off setting up macros that have view disabled.
I'm not sure that connecting to an Access DB from Management Studio is even possible, and even if it is it's not going to solve your performance problem.
No, I want to avoid Access altogether and was thinking that the limitations of Access are making queries perform worse (I also hate Access in other ways). What I've done before was a linked table to the ODBC. I'm not really trying to do this for anyone else, since some people find Access hard enough, I just want to see if I can pull data quicker.
Time is valuable. Thank you for yours!
It helps me learn to solve problems, so thank you, too.
As you said, Salesforce is a CRM system and is not suitable for data reporting or warehousing, especially for transactional data.
If SQL Server is not an option because of licensing cost, consider PostGres. It is free and open source and a very good relational database system. https://wiki.postgresql.org/wiki/Microsoft_Access_to_PostgreSQL_Conversion 
You can install SQL Server Express Edition for free or Developer edition for cheap, create a database, import your data, and test it out. I don't know how big your data set is but it can probably fit within the limitations of Express if Access can hold it. 
Thanks for the reply. I totally agree with what you're saying. As an analyst I'm not really in a position to change the setup for our data and want to avoid stepping on toes. I'm mainly curious if I will see a performance increase on my queries if I pull using a different platform vs. Access or if it's sort of negligible.
Management of a SQL server adds a layer of complexity on top of things - you're going to need suitable hardware and a person to manage this installation as well. Switching to MSSQL itself isn't a cure-all, but if you provide it with appropriate hardware and management, it'll be a significant improvement.
I'm unclear on what you've got an what you're proposing. Is your data currently in an Access DB, and are you planning to connect to that data via ODBC? If so you're not likely to see any real difference in performance. 
It's often used as a pop-up IT system that fulfills virtually any role you could want including robust reporting, the cost however is the prohibitive part for most people so if you need to house more than a couple million records the costs can start adding up. There are cheaper and open source options like Suite CRM that aren't as robust in terms of reporting. If it's purely transactional data something like quickbooks may be fine... But I don't know that I'd recommend using a full database that's unattached to any interface without a developer of some kind to set up easy access to it. Otherwise you're limited making everyone that can't write SQL helpless.
SSMS is just a front end for MS SQL Server. It doesn't actually do any data crunching. If you've outgrown your Access database, you need to either re-engineer it or move to a more powerful database. Your best free options are MS SQL Server Express and PostgresSQL. I would go with SQL Server Express. Judging by the nature of this post though, you really need to talk with somebody who knows a lot more about servers and DB's than you do. You're going to need help getting things setup and ported.
1a) accept vPName prompt 'Please Enter Product Search Keyword: '; select * from Product where (PName like 'p%' or PName like 'P%') and PName like '&amp;vPName'; 1b) accept vPName prompt 'Please Enter Product Search Keyword: '; select * from Product where PName in ('p%','P%') and PName like '&amp;vPName';
To answer this is going to require more information, such as what the back-end database software is, and what the server specs are. There are a *lot* of things that can cause performance bottlenecks. Access is certainly not an ideal SQL IDE (it has it's purposes - those just aren't being a performant IDE) but SSMS might not be the best solution either - as an example, if the RDBMS is Teradata, using something lightweight like their SQL Assistant might be better. Likewise if the server(s) is old and outdated, you might be capped out on RAM, IO, CPU, or any other potential laundry list of concerns. Many IDEs have performance tools to identify these bottlenecks. I would assume your DB admins would also likely be aware of the existing issues and if a benefit can be had to moving IDEs, or if the issue is more tied to hardware or other issues beyond what you can easily control.
Mysql is fine. I've used it before. Postgresql is superior in my opinion. In particular, it's more standards compliant 
Sometimes even reddit gold!
Sorry, some of this is out of my understanding. We have a system that's kind of like an IBM ISeries AS/400 and I believe we connect to that through an ODBC with Access.
Ah right, so the database is very likely to be DB2. Yes, you are very likely to get a big performance difference querying it directly, but ideally you'd want to avoid connecting via ODBC at all. Something that supports DB2 out of the box, like TOAD, would be a better option than MS SQL Management studio. 
What really is there to manage. Install it and update it. 
You're right, all of those DBAs out there just sit there and pick their noses all day. Managing a MSSQL instance is a bit more complicated than an Access DB.
I must have mis read your comment. I was reading it as you need someone to manage the hardware and the actual insulation portion. Not the data. I was assuming op was essentially their dba. 
I learned from MySql. Its free and easy to get up and running. Most of the big take aways will carry over to other database platforms.
If I understand you correctly: A &amp; B are a composite primary key C is an attribute related to the primary key (A&amp;B) C has a direct correlation to D D does not have a direct correlation to A &amp; B
Yup!
I may be completely wrong, but I think R(C) needs to serve as a foreign key where in another table C would become the primary key, serving Attribute D. 
Thank you ! So you mean that does not satisfy 2nd normal form?
No, I think if it need to be 2NF, then it would need to be broken down into two tables R(A,B,C) and OTHER_TABLE(C,D) with foreign key constraint. But, database normalization never was my strong point. That's just the best I remember. 
Isn't access file size limited to 2GB? We have legacy apps we constantly have to repair / restore access files. Much depends on the front end and usage. oP should provide more info. Personally, I like sqlite. But as others have suggested, a real SQL server, Ms express, postgres or even MySQL would be an improvement.
Check and see if this fixes it: &gt;CREATE TABLE flight ( &gt; &gt;FLIGHT\_NUMBER int, &gt; &gt;airline varchar(255), &gt; &gt;weekdays varchar(7), &gt; &gt;PRIMARY KEY (FLIGHT\_NUMBER) &gt; &gt;); &gt; &gt; &gt; &gt;CREATE TABLE flight\_leg ( &gt; &gt;FLIGHT\_NUMBER int, &gt; &gt;LEG\_NUMBER int, &gt; &gt;departure\_airport\_code char(3), &gt; &gt;scheduled\_departure\_time time NOT NULL, &gt; &gt;arrival\_airport\_code char(3), &gt; &gt;scheduled\_arrival\_time time NOT NULL, &gt; &gt;PRIMARY KEY (FLIGHT\_NUMBER, LEG\_NUMBER), &gt; &gt;FOREIGN KEY (departure\_airport\_code) REFERENCES airport(AIRPORT\_CODE), &gt; &gt;FOREIGN KEY (arrival\_airport\_code) REFERENCES airport(AIRPORT\_CODE), &gt; &gt;FOREIGN KEY (FLIGHT\_NUMBER) REFERENCES flight(FLIGHT\_NUMBER) &gt; &gt;); &gt; &gt; &gt; &gt;CREATE TABLE leg\_instance ( &gt; &gt;FLIGHT\_NUMBER int, &gt; &gt;LEG\_NUMBER int, &gt; &gt;LEG\_DATE date, &gt; &gt;airplane\_id char(5) NOT NULL, &gt; &gt;number\_of\_available\_seats int NOT NULL check (number\_of\_available\_seats &gt;= 0), &gt; &gt;departure\_airport\_code char(3), &gt; &gt;departure\_time time, &gt; &gt;arrival\_airport\_code char(3), &gt; &gt;arrival\_time time, &gt; &gt;PRIMARY KEY (FLIGHT\_NUMBER, LEG\_NUMBER, LEG\_DATE), &gt; &gt;FOREIGN KEY (departure\_airport\_code) REFERENCES airport(airport\_code), &gt; &gt;FOREIGN KEY (arrival\_airport\_code) REFERENCES airport(airport\_code), &gt; &gt;FOREIGN KEY (FLIGHT\_NUMBER, LEG\_NUMBER) REFERENCES flight\_leg(FLIGHT\_NUMBER, LEG\_NUMBER) &gt; &gt;); &amp;#x200B;
Just to be crystal clear, your ODBC connections are to Access based backend file(s) correct? Why not partition your data by moving some tables to other backend files you only call when needed? How often do you reference transactions more than 3 years ago? There's a button in Access 2016, may not be in older editions, that is designed specifically to assist in moving an Access database schema over to MSSQL, I haven't gotten to push it yet, but it might be something for you to look into. It would allow you to keep your Access front ends in place with a more capable back end, in a win/win until you decide you need to build a new front end app. 
The GUI you use to make a connection and execute queries does not matter. It will not make them faster.
Second to PostgreSQL. There is so much more there than MySQL will ever provide you.
Check out SQLiteStudio. For a personal project it has a great feature set—IDE-like interface, GUI views of tables and queries, query autoformat, etc.
&gt; The results are great if your database is properly normalized. However, SQL seems to give a lot of devs trouble. This is where NoSQL shines. They can serialize and object to it's primatives and literally slap an API with raw serialized data they'll consume it. Where data is loosely defined and constraints aren't a huge concern, it's basically licensed murder. &gt; &gt; Does this seem like a bad thing though? Isn't it basically saying "devs lacking experience in SQL find it hard, so we'll skip it and just let them do everything in application code" ... isn't this kind of making project technical decisions based on the lowest common denominator in your team? i.e. the people who aren't experienced with databases in general. It isn't about dev's lacking experience or being low acuity, it's about speed. People these days want fast app and microservice development. If you haven't tried a NoSQL (like Mongo or Couchbase) solution using a plugin for Node or C#, do it. It's awesome and it's far less complex than something like Microsoft's Entity Framework. You can define a loosely formed schema on the database and also on the client. Most of the plugins do validation on the object. The database allows updates via an API it creates per the object schema. Changes in the schema do not require you to migrate old data. You can do this as your leisure, or don't. &gt;But it feels like nosql is very rarely how a project should be started. If you get to limitations where somebody who knows SQL well thinks some nosql would be suited for some stuff... then that's probably the right decision. Just seems crazy for 95%+ of typical webdev projects though. Most webpages have reduced their postbacks by switching to js to make calls and update objects on the page without reloading. Most of those calls use json to communicate to and from the servers api. Json in js acts as a object. JSON can contain multiple arrays within arrays. You can break down the object into it's many parts and flatten them or you can send the entire json object to the API and store it in it's original form. The newer NoSQL solutions are really attempt to keep things "lite" and some are very forward that they are "document" databases that contain "json documents". NoSQL Solutions really shine with small projects because how quickly you go from front end to back end with such little development. The large projects shine because of scalability, distribution and the lack of need to perform migrations during schema changes, but that doesn't mean they suit all projects. &gt;I know it well, and I don't see how it's a pain in the ass aside from the fact that we need to go out an find out own migration systems... it would be nice if this was standardised so that it was a mainstream included feature in each sql db. I've hit that point where I'm just tired of writing migrations. I agree that if SQL needs to finds a common language to perform migrations better. &gt;I've been working on something myself over the last few months and it's going really well. It uses typeorm for the actual automated migrations, but I can simply define the schema in code and it actually handles foreign keys and stuff like that (including matching the type and automatically naming the FK columns by default etc), including generating and updating the ORM model classes. So yeah, this is an area where it's a pity this stuff is more standardised. ORM's require you to compromise a lot. Even Microsoft's "Code First" EntityFramework ran me in circles. [Maybe we just need a better Standard/ORM?](https://xkcd.com/927/). &gt;But it seems to me the whole schemaless thing is just going to create a huge mess that you need to debug in the future, which is going to be much more work than defining a schema to begin with. I'm right now dealing with organisation millions of JSON files I've been collecting over the last 5 years or so, mostly with the same formats, but sometimes they vary etc... it's a fuckload of work going back and figuring out the different schemas etc, especially all their minor changes over time. I wouldn't call what you are doing exactly the same thing but it is pretty close. I don't say it's the same thing because in a situation where your schema is changing over time and data isn't updated, there will be a consistent divergence in schema. It wont be all over the place. You can go back and update old data to fix the schema. However, that is a data change request, not a migration script made at the time of update. The json documents are easily manipulated, they don't require to be rekeyed or constrained to foreign key constraints. The objects in the arrays of the json document are simple and easily manipulated. Are you loading these json files directly into SQL? or are you using a mapping document to translate the json files into a common json schema, then insert into SQL?
Thank you - although all the replies were excellent explanations, this one seemed to really strike a chord, likely because it's formatted well and has comments exactly where I should be paying attention. Really clean answer, thanks for this.
Access will allow pass through queries too a different database. Migrate tables to SQL server express, change the Access front end to connect to those tables using a pass through query. That way they can continue to use the queries and reports they've already developed.
&gt;$basepath = "c:\\folder\\folder\\" $timestamp = date (get-date) -format yyyy-MM-dd $cn= New-Object System.Data.SqlClient.SqlConnection("Server=servername;database=databasename;trusted\_connection=yes;") $q = "exec dbo.ProcName" $da = New-Object System.Data.SqlClient.SqlDataAdapter($q, $cn) $da.SelectCommand.CommandTimeout = 1500 $ds = New-Object System.Data.DataSet $da.Fill($ds) &gt;$null| Out-Null $cn.Close() if($ds.Tables\[0\].Rows.Count -eq 0){ Write-Host "No Data found") return } $file = $basepath + "filename\_" + $timestamp +".csv" $ds.Tables\[0\] | export-csv $file -notypeinformation Great thanks, need to set some other stuff up for this to work how I want but looks to be good
Thank you very much!
I appreciate the reply, however I was more trying to get recommendations of what has worked for others considering my skill level. 
Thanks, I’ll look into it!
Have you confirmed that all the values in the COLUMN1 field are actual dates? This might help: &gt;SELECT &gt; &gt;CASE WHEN ISDATE(Column1) = 1 THEN CAST(Column1 AS DATE) ELSE NULL END AS "COLUMN1FORMATTED", &gt; &gt;Column1 &gt; &gt;FROM Table1; &amp;#x200B;
 DECLARE @datevar VARCHAR(250) = '~LIT~(2018-12-04 12:00:00.00),~LIT~(2018-12-04 12:00:00.00),,,~LIT~(C)' SELECT CONVERT(DATE, SUBSTRING(REPLACE(REPLACE(REPLACE(REPLACE(@datevar,'~LIT~',''),')',''),'(',''),'-',''),1,8)) ,CONVERT(DATE, SUBSTRING(REPLACE(REPLACE(REPLACE(REPLACE(@datevar,'~LIT~',''),')',''),'(',''),'-','.'),1,10),102) Both worked fine for me, perhaps there's an outlier in the table that's not formatted the same and is throwing that error?
I have actually. I've done this two ways, using a single GUID in the WHERE for a row with proper dates and also by using WHERE SUBSTRING(Column1,7,4) = '2018'. Front end users are using a calendar type drop down that's not modifiable, so they cannot edit, the worst they can do is run it wide open with no dates.
Just glancing at it, it looks like more of a data issue than a programmatic issue to me. Maybe just try to use ISDATE() to filter if there is anything crazy that managed to make it into to the table.
\*I'm not arguing I'm trying to learn something disclaimer\* Based on some other comments, it sounds like OP is connecting to a DB2 database through ODBC, so essentially using Microsoft Access as a front end. What hardware requirements would there be? That db2 must be sitting on something (a server) already. Am I being naive? His main complaint is that his queries are slow - so wouldn't the optimal solution be that he just changes his "client" for querying. Someone mentioned TOAD, he'd use that to query the DB directly and get better results no? I'd go ahead assume that they are using Access for data entry as well - so it would have to stick around in that role.... &amp;#x200B; Am I ignorant of the complexities? (I'm genuinely asking!) 
Thanks!
While this is not an explicit answer to your question, just more of a stylistic choice, you may have some luck with the TRANSLATE function, which operates like multiple REPLACE functions, this may make your code easier to read. &amp;#x200B; [https://docs.microsoft.com/en-us/sql/t-sql/functions/translate-transact-sql?view=sql-server-2017](https://docs.microsoft.com/en-us/sql/t-sql/functions/translate-transact-sql?view=sql-server-2017)
Well... Nothing works but I think we are damn close! This causes it to only produce 5 results in the end, but it does parse properly. Though it takes wayyy longer than it should. Especially since I reduced down the tracks to only 40 total from each genre: &amp;#x200B; [`Songs.ID`](https://Songs.ID) `IN (SELECT` [`Songs.ID`](https://Songs.ID) `FROM Songs, (SELECT DISTINCT GenresSongs.IDGenre AS IDGenre FROM GenresSongs) AS G1, Artists WHERE` [`Songs.ID`](https://Songs.ID) `IN (SELECT` [`Songs.ID`](https://Songs.ID) `FROM Songs, GenresSongs WHERE` [`Songs.ID`](https://Songs.ID) `= GenresSongs.IDSong AND GenresSongs.IDGenre = G1.IDGenre ORDER BY Random() LIMIT 40) ORDER BY` [`Artists.ID`](https://Artists.ID) `LIMIT 5) AND &lt;Genre&gt; IN ('Alternative', 'Asian Music', 'Dance', 'Electro', 'Folk', 'Metal', 'Pop', 'Punk', 'Rap/Hip Hop', 'Reggae', 'Rock')` &amp;#x200B; If I add in AritstsSongs the program will hang forever &amp; never parse the SQL. It also states automatically that the mask is invalid if using "order by ArtistsSongs" as you suspected may be the case. 
converting string to date and vice versa is fun in MSSQL (the kind of fun one can only expect by having one's toenails pulled out while under salty lemon juice). So, the first thing you should check, is whether you are actually using the correct *language*! add *set language english* right at the start of your script. Not sure it will solve your particular problem, but it might well do it.
SQLBolt.com was my favorite in browser tool to learn SQL. For joins there's always the Venn Diagram approach and the Don't Use Venn Diagram approach. I found both useful.
Hey, i'm trying the same and i don't know where to start. Would you please share with me anything that you found, ill do the same, please.
I would suggest to start with some online courses, like these ones: [https://www.coursera.org/learn/sql-for-data-science/](https://www.coursera.org/learn/sql-for-data-science/) And w3schools have a lot of excersizes: [https://www.w3resource.com/sql/tutorials.php](https://www.w3resource.com/sql/tutorials.php) &amp;#x200B;
Is that the end of the code? There is nothing that comes after the second IF() statement?
the first IF checks to make sure @ProcessID has a non-zero value the second IF does nothing -- looks like something's supposed to come after it 
Just a few that come to mind: Data Analytics Analyst Business Intelligence Analyst Potentially Data Scientist
It's badly written if @processorId is null
Look up the Northwind and/or Chinook databases and follow the instructions to set up a local instance on your machine. It's fairly simple - you'll run a script or set of scripts that create the DBs, tables and populates them with data. Then have fun.
I'll try to look at this after work. Have you tried to email the developer and ask them to look at it?
There's a lot of code before and after. I just wasn't sure what the statement IF (0 = did. I got the answer from r3pr0b8. He said it's just checking to make sure the parameter @processid has a non-zero value.
Thank you very much. 
Strong recommendation. I'll check it out. Thanks!
I'm leaving that (emailing the developer) as a last resort. He states if you need help to post on the Media Monkey forums, which I did. However, they aren't very active... More than likely I'll get a response eventually but only time will tell. Thanks for taking a look at it :) 
"You seem to be right" from OP, but no upvote? I have done my part to correct this injustice.
thanks will do !
BTW, where is this position? 
ranking the results and picking the top one per UserId would get you what you're looking for, I think: ;with parcels_ranked as ( select *, ranked = rank() over(partition by UserId order by Date desc, Time desc) from parcels ) select * from parcels_ranked where ranked = 1 &amp;#x200B;
This is some remote position for a company based in ohio
It checks if @ProcessID has a non-zero *non-null* value. It sets null @ProcessID values to 0, *then* checks that there are no 0 values.
Northwind is an older database structure, you would want to check out Adventure works which is the updated version provided by Microsoft.
Nice! I was hoping someone would post a more elegant answer. The data warehouse my team uses only supports a subset of SQL-99, so subqueries is the answer to everything. 
I second SQL Bolt, though on definitely not an expert. I've heard good things for Wise Owl on YouTube. 
Wideworldimporters is another one
Another option is to download the StackOverflow database. It's much larger and can give you a better feel for querying large tables.
ugh. I don't understand the notation used so I'm confused. &amp;#x200B; If the phrase 'primary key A,B' means you intend lists to work as tuples (i.e. "a,b" -&gt; "(a,b)") then you cannot have a candidate key (A,B,C) since (A,B) is a subset. If you intend to use lists as individual elements ( i.e. the phrase "candidate keys A,B,C" means that you have 3 candidate keys) then your primary key cannot be (A,B) (again, since there exists a subset that is a key). &amp;#x200B; &amp;#x200B; That aside, if you have a relation (a,b,c,d) with a key (a,b), c-&gt;d does NOT break 2nf (c is not a part of a key). &amp;#x200B; Dependency a-&gt;d would break 2nf, for example (2nf is dependency on the whole key) &amp;#x200B;
My bad. Just checked reference, it won't work either for Windows and InnoDB
Thanks, I'll spend some time on there tonight!
After you've worked in this field for a while you will start to understand that you don't need code that's too complex. I get away most times with writing small chained queries that create a temp table use that table further, join it to other temp tables and at the end spit up a digestible dataset. Before starting with my current employer I have worked as a data analyst for several years and I never felt as there is a level where you can say your SQL is good enough. You always get a requirement that you need to think long and hard about.
Depending on your SQL software, you could use CTEs (Common Table Expressions) to define your subquery and then refer to it by name. If you can't do that, then the easiest solution would be to repeat your subquery instead of referring to it by the alias- just copy and paste it.
Thanks for the reply. The Upper(p.Pname) syntax got me going where I needed. Much appreciated. 
Thanks, I'll look it up! Do you also happen to know *why* I cannot reference t from the subquery? Also, if I had defined t in my subquery, I couldn't have referenced it above from the outside. I tried to look it up but I couldn't find what "scope" aliases have.
Cool. 
Cool. 
I'm not sure I see the intent with getting the result of 11/07 - 11/21 and skipping the 11/14 record. Could you explain a little more?
Why not a simple essbase cube on Oracle cloud? Unless your attempting to unifi this data with another ERP dataset.... 
Aliases are just shorthand or alternate way of referring to something else. The subquery alias doesn't refer to the subquery itself, but the table that is a result of the subquery. That distinction is important. For your second query, if you aliased a real table in the FROM clause, you couldn't use the alias like in the second example, either. You can't write this: SELECT t.col1 ,t.col2 FROM Table1 t WHERE t.col1 = (SELECT MAX(Col2) FROM t) A CTE generates something you can refer to directly like a table or view, anywhere you want inside single query. You can nest them, make self-referential CTEs, etc., but that's because the they're handled as an object that is "insertable" in the query to build the "real" query. As far as scope, could say: SELECT * FROM ( SELECT someval as T, SomeVal2 FROM SomeTable WHERE someval &lt;&gt; Someval2 ) as A WHERE A.T = 1 AND A.SomeVal2 = 2 because you are aliasing the first column as "T", and the subquery itself as "A". They're just name aliases, they don't have any functionality beyond that. 
We’re trying to set up a study for a rehab program. We want to see each members rehab “episodes.” We are looking at claims data for this and a claim is billed for each day in rehab. Since the member can come and go as they please, it is hard to define what a rehab episode is. Right now our solution is that if they do not have a rehab claim for 30 days, that is the end of a rehab episode. So in my example this member would have 2 separate episodes; 1 that spanned from 11/07-11/21 and one that spanned from 2/13-2/16. I understand this is a bit confusing but thank you for taking the time to even read it haha.
Good luck! You'll do great!
You might want to make another post with your updated code in format. I think someone here will be able to answer your question but I'll check it out if I can.
HackerRank’s SQL exercises are helpful too. 
Select * From table t Inner join (select max(Val) as val from table t) t2 on t.val = t2. val 
Where at in Ohio? 
That, and go the next step and set the DATEFORMAT as well. [https://docs.microsoft.com/en-us/sql/t-sql/statements/set-dateformat-transact-sql?view=sql-server-2017](https://docs.microsoft.com/en-us/sql/t-sql/statements/set-dateformat-transact-sql?view=sql-server-2017) &gt;Syntax &gt; &gt;SET DATEFORMAT { format | @format\_var } &gt; &gt;Arguments &gt; &gt;*format* | **@***format\_var* Is the order of the date parts. Valid parameters are **mdy**, **dmy**, **ymd**, **ydm**, **myd**, and **dym**. Can be either Unicode or double-byte character sets (DBCS) converted to Unicode. The U.S. English default is **mdy**. &amp;#x200B;
Will the end date in your claim table (tableA) ever be a different date than the start date?
Just checked yes it can. Sorry I did not realize this until now. 
Another way (and my favorite) is to use the row\_number() function to assign a rank based on your criteria. select id , value , row_number()over(order by value desc) as rnk from testTable limit 1; &amp;#x200B; I either use this in a join ans specify "AND rnk=1" or use a "LIMIT 1" to get the 1st value only.
I think this is doable. Give me a couple hours and I'll put something together. 
I needed it for an assignment, as long as it worked they didn't care. I'd definitely like to learn a more elegant solution too, however. Thanks a lot though my man, I really appreciate your time and help with this!
[https://docs.microsoft.com/en-us/sql/samples/wide-world-importers-what-is?view=sql-server-2017](https://docs.microsoft.com/en-us/sql/samples/wide-world-importers-what-is?view=sql-server-2017)
Software Dev is full of shit.
If the test is something like a coder pad, where your interviewer is watching as you write SQL, they will most likely be looking for good process (along side the basics). I tend to look for folks who are proactive in understanding the structure and meaning of their data set, and then show a clear understanding of how the data needs to be transformed into a final output (either by describing their thought process up front or while writing queries).
Data Architect
Really appreciate the effort you’re putting into this! In tableA there are over 1 million lines of data. The query is pulling around 150k lines of data from tableA
I don't really understand his question, or what the "dev" said. There is a lot that isn't clear. I mean, my work laptop is an i5-7300U @ 2.70Ghz, and I wouldn't call that high end. I guess I could have a bunch of data on my localdb but I absolutely couldn't effectively run code, etc. -- also, why would you want all that data floating around on a local machine? Depending on the software I guess it's possible that client machines need to be an i5. I'd want to know what software it is, and why it's being used / what for. Why not just throw your database on a random i5 and give everyone access? 
The only explanation is that the software has been made incredibly shittily. I'm loving the FAQ tho: http://www.chiro8000.com/ &gt;Is Chiro8000 the most advanced medical software on the market? &gt;No. &gt;Is this bad? &gt;No. &gt;Are we the smartest people in the world? &gt;No. 
the answer is: you don't let's start here -- Customers who shopped in weeks 1, 2, and 3 SELECT x.Customer FROM ( SELECT Customer , COUNT(*) AS transactions FROM tbl WHERE Weeks = 1 GROUP BY Customer ) AS x INNER JOIN ( SELECT Customer , COUNT(*) AS transactions FROM tbl WHERE Weeks = 2 GROUP BY Customer ) AS y ON y.Customer = x.Customer INNER JOIN ( SELECT Customer , COUNT(*) AS transactions FROM tbl WHERE Weeks = 3 GROUP BY Customer ) AS z ON z.Customer = x.Customer see? no loop 
I mean, it might not be the best software in the world but all it requires to run is a Xeon E3 or Better OR Any ‘i5/i7’ @ 2.7 GHZ or Better with 8GB of RAM. This is medical billing software. I have no idea how robust it is, or what the full capabilities are but that doesn't sound like a high end machine by any stretch of the imagination, and this issue has nothing to do with SQL at all.
As a SQL newbie, a good rule of thumb is if you think you need to use a loop, you're probably wrong. There's a few approaches you could take, e.g., SELECT distinct s1.week, s1.customer, case when s2.customer is not null and s3.customer is not null then 'Returned' else 'Did not return' end ReturnedNextTwoWeeks FROM sourcedata s1 LEFT JOIN sourcedata s2 on s1.customer = s2.customer and s1.week + 1 = s2.week LEFT JOIN sourcedata s3 on s1.customer = s3.customer and s1.week + 2 = s3.week That might be an idea - but note that this will count a customer in all weeks. e.g., a customer who shopped on week 1,2,3 would appear on week 1 as returned, and week 2 and week 3 as not returning.
This is exactly what I do. So much easier to work on later, too when you break stuff up logically and neatly like that.
I like using `APPLY` to return the top N objects per group, but mostly for readability rather than performance. I've found lateral joins and windowing functions usually have similar performance, but you can try something like this to see: SELECT a.CaseNumber ,diag.Diagnosis FROM casestable a OUTER APPLY (SELECT TOP 1 d.Diagnosis FROM diagnosistable d WHERE d.casenumber = a.casenumber AND d.Diagnosis_Desc = 'Primary' ORDER BY d.Create_Dt_UTC DESC) AS diag
There's nothing wrong with the performance of your approach but for readability a FIRST_VALUE might be an improvement: SELECT DISTINCT a.CaseNumber ,FIRST_VALUE(diag.diagnosis) over (partition by casenumber order by Create_Dt_UTC desc) FROM casetable a LEFT JOIN diagnosistable diag ON a.casenumber = diag.casenumber AND Disgnosis_Desc = 'Primary'
From the rest of the ad, it sounds like it's a small-scale thing serving a single cube, much more of a data mart than a data warehouse. 
Brush up on your pivot tables in Excel...they compliment the data from the SQL and are a basic part of any Analyst position.
I'm not really sure if this is what you're aiming for, but here's a nightmare query. Give it a shot and see if it works. You'll need to change table and field names, of course. &gt;WITH memberData AS &gt; &gt; ( &gt; &gt; SELECT DISTINCT d.MEMBER\_ID, &gt; &gt; d.TRG\_BEGIN\_DATE, &gt; &gt; d.TRG\_END\_DATE, &gt; &gt; MAX(d.CALC\_BEGIN\_DATE) &gt; &gt; OVER(PARTITION BY d.MEMBER\_ID, d.TRG\_BEGIN\_DATE) AS "BEGIN\_DATE", &gt; &gt; MAX(d.CALC\_END\_DATE) &gt; &gt; OVER(PARTITION BY d.MEMBER\_ID, d.TRG\_END\_DATE) AS "END\_DATE" &gt; &gt; FROM &gt; &gt; ( &gt; &gt; SELECT c.MEMBER\_ID, &gt; &gt; CASE &gt; &gt;WHEN c.TRG\_BEGIN\_DATE = 0 &gt; &gt;THEN 1 &gt; &gt;ELSE c.TRG\_BEGIN\_DATE &gt; &gt; END AS "TRG\_BEGIN\_DATE", &gt; &gt; CASE &gt; &gt;WHEN c.TRG\_END\_DATE = 0 &gt; &gt;THEN 1 &gt; &gt;ELSE c.TRG\_END\_DATE &gt; &gt; END AS "TRG\_END\_DATE", &gt; &gt; c.CALC\_BEGIN\_DATE, &gt; &gt; c.CALC\_END\_DATE &gt; &gt; FROM &gt; &gt; ( &gt; &gt; SELECT b.MEMBER\_ID, &gt; &gt;b.CALC\_BEGIN\_DATE, &gt; &gt;b.CALC\_END\_DATE, &gt; &gt;SUM(CASE WHEN b.CALC\_BEGIN\_DATE IS NULL THEN 0 ELSE 1 END) OVER (PARTITION BY MEMBER\_ID ORDER BY CALC\_BEGIN\_DATE ASC) AS "TRG\_BEGIN\_DATE", &gt; &gt;SUM(CASE WHEN b.CALC\_END\_DATE IS NULL THEN 0 ELSE 1 END) OVER (PARTITION BY MEMBER\_ID ORDER BY CALC\_END\_DATE ASC) AS "TRG\_END\_DATE" &gt; &gt; FROM &gt; &gt;( &gt; &gt;SELECT a.MEMBER\_ID, &gt; &gt;CASE &gt; &gt;WHEN &gt; &gt;( &gt; &gt;a.LAG\_1\_START\_DATE IS NULL &gt; &gt;AND a.LAG\_1\_END\_DATE IS NULL &gt; &gt;AND a.LAG\_2\_START\_DATE IS NULL &gt; &gt;AND a.LAG\_2\_END\_DATE IS NULL &gt; &gt;) &gt; &gt;THEN a.START\_DATE &gt; &gt;WHEN DATEDIFF(DAY,a.LAG\_2\_START\_DATE,a.LAG\_1\_END\_DATE) &gt; 28 &gt; &gt;THEN a.LAG\_1\_START\_DATE &gt; &gt;ELSE NULL &gt; &gt;END AS "CALC\_BEGIN\_DATE", &gt; &gt;CASE &gt; &gt;WHEN DATEDIFF(DAY,LAG\_1\_START\_DATE,END\_DATE) &gt; 28 &gt; &gt;THEN a.LAG\_1\_END\_DATE &gt; &gt;WHEN a.END\_DATE = a.LAST\_END\_DATE &gt; &gt;THEN a.LAST\_END\_DATE &gt; &gt;ELSE NULL &gt; &gt;END AS "CALC\_END\_DATE", &gt; &gt;a.LAG\_2\_START\_DATE, &gt; &gt;a.LAG\_2\_END\_DATE, &gt; &gt;a.LAG\_1\_START\_DATE, &gt; &gt;a.LAG\_1\_END\_DATE, &gt; &gt;a.START\_DATE, &gt; &gt;a.END\_DATE, &gt; &gt;a.LAST\_END\_DATE &gt; &gt;FROM &gt; &gt;( &gt; &gt;SELECT MEMBER\_ID, &gt; &gt;LAG(START\_DATE,1) &gt; &gt;OVER(PARTITION BY MEMBER\_ID ORDER BY START\_DATE ASC, END\_DATE ASC) AS "LAG\_1\_START\_DATE", &gt; &gt;LAG(END\_DATE,1) &gt; &gt;OVER(PARTITION BY MEMBER\_ID ORDER BY START\_DATE ASC, END\_DATE ASC) AS "LAG\_1\_END\_DATE", &gt; &gt;LAG(START\_DATE,2) &gt; &gt;OVER(PARTITION BY MEMBER\_ID ORDER BY START\_DATE ASC, END\_DATE ASC) AS "LAG\_2\_START\_DATE", &gt; &gt;LAG(END\_DATE,2) &gt; &gt;OVER(PARTITION BY MEMBER\_ID ORDER BY START\_DATE ASC, END\_DATE ASC) AS "LAG\_2\_END\_DATE", &gt; &gt;MAX(END\_DATE) &gt; &gt;OVER(PARTITION BY MEMBER\_ID) AS "LAST\_END\_DATE", &gt; &gt;START\_DATE, &gt; &gt;END\_DATE &gt; &gt;FROM \[TEST\_REDDIT\].\[dbo\].\[JANUS1012\] &gt; &gt;) a &gt; &gt;) b &gt; &gt; WHERE b.CALC\_BEGIN\_DATE IS NOT NULL &gt; &gt;OR b.CALC\_END\_DATE IS NOT NULL &gt; &gt; ) c &gt; &gt; ) d &gt; &gt; ) &gt; &gt;SELECT g.MEMBER\_ID, &gt; &gt; g.BEGIN\_DATE, &gt; &gt; g.END\_DATE &gt; &gt;FROM &gt; &gt; ( &gt; &gt; SELECT f.\*, &gt; &gt; ROW\_NUMBER() &gt; &gt; OVER(PARTITION BY f.MEMBER\_ID, f.END\_DATE ORDER BY f.BEGIN\_DATE ASC, f.END\_DATE ASC) AS "RN" &gt; &gt; FROM &gt; &gt; ( &gt; &gt; SELECT mdBegin.MEMBER\_ID, &gt; &gt; mdBegin.BEGIN\_DATE, &gt; &gt; mdEnd.END\_DATE &gt; &gt; FROM &gt; &gt; ( &gt; &gt; SELECT DISTINCT MEMBER\_ID, &gt; &gt;TRG\_BEGIN\_DATE, &gt; &gt;BEGIN\_DATE &gt; &gt; FROM memberData &gt; &gt; ) mdBegin &gt; &gt; INNER JOIN &gt; &gt; ( &gt; &gt; SELECT DISTINCT MEMBER\_ID, &gt; &gt;TRG\_END\_DATE, &gt; &gt;END\_DATE &gt; &gt; FROM memberData &gt; &gt; ) mdEnd &gt; &gt; ON mdBegin.MEMBER\_ID = mdEnd.MEMBER\_ID &gt; &gt;AND mdBegin.TRG\_BEGIN\_DATE = mdEnd.TRG\_END\_DATE &gt; &gt; UNION ALL &gt; &gt; SELECT e.MEMBER\_ID, &gt; &gt; e.START\_DATE, &gt; &gt; e.END\_DATE &gt; &gt; FROM &gt; &gt; ( &gt; &gt; SELECT ROW\_NUMBER() &gt; &gt;OVER(PARTITION BY MEMBER\_ID ORDER BY START\_DATE DESC, END\_DATE DESC) AS "RN", &gt; &gt;MEMBER\_ID, &gt; &gt;START\_DATE, &gt; &gt;END\_DATE &gt; &gt; FROM \[TEST\_REDDIT\].\[dbo\].\[JANUS1012\] &gt; &gt; ) e &gt; &gt; WHERE e.RN = 1 &gt; &gt; ) f &gt; &gt; ) g &gt; &gt;WHERE g.RN = 1 &gt; &gt;ORDER BY g.MEMBER\_ID ASC, g.BEGIN\_DATE ASC, g.END\_DATE ASC; &amp;#x200B;
it's possible they're using some non-standard field-based DB encryption. i've seen it in legal and they must have it in medical, too. instead of doing any filtering or joining using the DB to operate like a relational DB on indexed data, they've 'encrypted' everything they put in there so that it gets the performance of a flat file. Then they pull down and 'decrypt' what they need to load everything they can in-memory for filtering and joining and such client-side.
Thanks so much. Going to try it tomorrow morning
There is no reason it should require those specs. Maybe they are mining CryptoCurrency when the app is running and want you to do your part?
That's terrible performance wise, compared to OP's solution. Like, not even in the same universe of performant queries. [Screenshot for a test case with ~5 mil diagnosis rows for 100 cases](https://i.imgur.com/FYs2TxU.png). 
I've never had to do a 'whiteboard' interview like this, but I would be impressed by someone who comments their sql code and can at least explain their thought process to someone who doesnt know sql. Oh and subqueries - I use subqueries and the RANK function all the time in my databases (of course this will not be the case for everyone and RANK will probably not be on a test they give you during an interview).
If you don't have an index, like `` then what you have is as good as it's going to get. With an index, something like that given your query: CREATE INDEX ix ON diagnosistable ( CaseNumber ASC, Create_Dt_UTC DESC ) INCLUDE (Diagnosis) WHERE Diagnosis_Desc = 'Primary' like that it will improve, though /u/r_kive's query will work much better, resulting in an index seek (but it will perform far worse when there's no index to use). What you can also do, if you're running into performance issues, is to dump the diag subquery results with where dx_Rank = 1 into a temp table, set up a unique clustered index on casenumber and join that. 
I called their tech line yesterday. I was told the information on the website regarding hardware requirements 'needed to be updated' and that all machines needed to be quad core 2.7ghz or higher, including the workstation/clients. Understanding tier 1 doesn't always understand everything, I emailed asking whether they meant physical or virtual cores (meaning a faster HT dual core would be ok work for the clients) and whether the clock speed was 'absolute' or whether turbo core speeds above 2.7ghz would render them 'passable'. They emailed back after my first post here with nearly the same response, but even more interesting: "We will only support or install on a system with an i5 2.7Ghz or above quadcore processor for server and workstation. The SQL Server 2014 Express database engine is limited to the lesser of 1 Socket or 4 cores". I'm not sure they even understand the difference between minimum hardware requirements and software imposed limitations (MS won't let you run express version on a muli-socketed many core enterprise class server, for that you have to license the full version). I'm emailing back to see if they genuinely don't understand this or are confused as they appear to me. It is possible their coding is so horrible it needs a high spec machine. The office staff has been complaining that the previous software version runs 'slow' on the clients. When I looked at it the hardware was basically sitting idle while you waited for things to happen. The network utilization wasn't high enough to be a limiting factor either.
So any computer running their 3rd party proprietary software needs to run a 2.7ghz system. That has nothing to do with SQL. Not trying to be a dick, but here are the things I know about medical billing software: 1. They are expensive. 2. They tend to integrate with other systems, and the cost of changing to a new system can be substantial. 3. They are complex as shit. Whether your client should or should not switch to new billing software is not a SQL question. It might be that the client could upgrade their hardware, to spec, and only spend a few grand versus spending twenty, thirty, or fifty grand to switch to a new vendors software. It very well may be the case that this software sucks balls, and is poorly written, and doesn't need that much hardware to run efficiently -- but, again this has nothing to do with SQL in the first place, and an i5 isn't that hardcore or robust of a machine in the first place. So it's just kind of a meh situation. &gt;I'm not sure they even understand the difference between minimum hardware requirements and software imposed limitations (MS won't let you run express version on a muli-socketed many core enterprise class server. For that you have to license the full version). They may or they may not, or their software may touch on some packages/modules/features that are native to specific software/operating systems and therefore they have decided to set the minimum requirements at a certain level in order to set a standard and mitigate technical support for clients who have an older infrastructure. Again. They are only requiring an i5. I have an i5 that qualifies as my work laptop and I still require a much more powerful server to do the heavy lifting on the database side. Without knowing more about this software, what the database components is, etc., then I just don't think I (or anyone else) can give you an educated opinion here. Have you priced out competitors software and looked into the effort it would take to integrate the clients business with them? If there is a readily available solution then go with it, if there isn't then you have to eat the cost and there isn't shit you can do about it. Welcome to the world of proprietary software. 
Also a newer one!
Learn regular expressions in and out of SQL. You’ll be able to search for patterns in strings that some developers would call impossible to find. Not for the interview, just a tip for success. For the interview I’d touch up on REPLACE(), IIF(), TRY_CONVERT(), common table expressions, temp tables, and table variables. 
&gt; They are complex as shit. Especially Medical Billing. Clinical applications are one thing - sure they're complex, but they're not anymore complex than any other data entry style application. You do not want to switch billing systems, even for a smaller healthcare company, if you can help it. That shit is an absolute nightmare. 
I worked for BCBS for awhile. It's a huge nightmare. Not trying to say that this software is or isn't good, just that this has nothing to do with SQL and randomly deciding to go with new software could have massive consequences.
They need a Data Architect still. So that the Data Architect can go in there and tell them how much of a bad idea an Independent Data Mart Architecture actually is. 
I've not found a more efficient way and have also found that people who come along behind me and read the code understand this pretty well with minimal comments. Also, please comment why just in case you forget why two years later. I've learned that the hard way. :) 
Brilliant. I've always used the pattern from the OPs query. Never even though to use OUTER APPLY. Beauty
Edit: Just some wording. 
It ain't stupid if it works. Code is more manageable if written in small manageable pieces. I run into problems all the time when I do code review on queries written as selects from online subqueries that select from other online subqueries or joins where column names are changed from one to the other. The stuff of nightmares.
Yes! or joins to multiple subqueries that also have joins, and then a giant select field list with no prefixes
Yes, that would be possible. You would lose any data stored in the MSDB though.
Your table is expecting 9 Fields, your CSV is providing 8
That's a very nice solution, thanks! However, if `t` was a subquery, I'd be in touble again because SELECT id, val FROM (…) AS t1 JOIN (SELECT max(val) as val FROM t1) AS t2 ON t1.val = t2.val does not work.
Even if that's the case, there are so many better ways to do that. One solution that comes to mind is memory mapped files, where all content that gets stuffed into the memory mapped region is encrypted, and a working set is kept in memory. Anything is better than using a database as a flat file because it's just not that.
Have you tried to do this? The solution to this is something you would learn in the first 30 minutes of learning sql. Its very easy.
What's wrong with the original query? Or rather, what are you trying to do? Your second query kind of makes no sense at all, so it's rather hard to grasp your goal. I guess this should work if you just want to rewrite the query to produce the same results but in a different manner: SELECT country , (SELECT COUNT(*) FROM city subCity WHERE subCity.CountryId = topCountry.CountryId) AS NumberOfCities FROM country topCountry ORDER BY NumberOfCities DESC, country; 
It checks for null *OR 0*, both being invalid. And it's a perfectly fine way to do it.
My school taught me little on SQL, but just trying to get data I have alter view finalproduct as select products.productname, components.COMPONENTNAME from products left join components on products.productid=components.productid; There are productid's that are the same yet I still get no data
Your INSERT INTO isn't right. You can either supply every column - e.g. assuming a 3 column table called foo: INSERT INTO foo VALUES ('a','b','c') Or you can explicitly state which columns you want to insert into : INSERT INTO foo (col1) VALUES ('a') You're specifying only one column (first name) but you've not told the SQL engine which column you're trying to insert. Generally speaking it's better to be explicit in case the table structure changes later on. So : INSERT INTO foo (col1,col2,col3) VALUES ('a','b','c')
On the phone and email they're telling me *quad core with a base clock no lower than 2.7ghz* on the client/workstations. That's not mid end i5 laptops or even most quad core i7 mobile CPUs. This kicks her off the C5 tablet, even a brand new one (they top out with an i7 5600U @ over $3k) doesn't meet that This effectively means putting a fairly high spec desktop or high end laptop in every patient room just to access patient records during a consult or session...behind a desktop/laptop monitor, not facing her clients, less eye contact (reduced quality of interaction) or reverting back to the stone age of written notes on a clipboard that have to be deciphered/entered after hours or by staff. This whole quality of interaction component, like with a shrink or sales, is very important otherwise patients next time may be more likely to 'wander off' to the earliest available appointment-- which may be elsewhere. I tend to believe my client when she talks about the importance of all this: just a few years ago there were three competing chiro practices in town. The other two are gone, she's the only one left. None of this has anything to do with SQL anymore. It is clear that the issue requiring unusually high spec client stations has to do with Forte's integration + EMR software that runs on top of SQL. Whether being CPU intensive is by necessity or spaghetti mess can't be answered here. The only thing I can really conclude is that either their basic support doesn't understand their own software, or the dev(s) are failing to facilitate a 'modern doctor-patient experience' with regard to my client. &amp;#x200B; Thank you all for your thoughts
I can kind of understand it sounding stupid on "defining itself by what it isn't"... But in practically, thinking about it now, it's actually a very useful specific unique single word (helpful as a search keyword) that covers something both specific and broad. So even if it sounds silly, it's actually very useful for searching and discussion on "sql vs these newer non-sql databases". Plus you don't even need to include the word "database" in your search, that one-and-only word will give you exactly what you're looking for (and nothing else).
&gt; it's about speed. People these days want fast app and microservice development. Right, but purely on that point... yeah you save a little time dealing with schema migrations, but now you have to write fucktons of manual code for constraint checking, and doing basic simple data processing tasks that could easily have been done with a few JOINs/VIEWs. Not to mention all the time you're going to be spending in the future fixing your broken inconsistent data (assuming you ever discover the problems to begin with). No overall it seems to me that for most datasets, nosql is more work, not less taking everything into account. The schemas/migration thing is trivial compared to the rest. &gt; Most webpages have reduced their postbacks by switching to js to make calls and update objects on the page without reloading. Most of those calls use json to communicate to and from the servers api. Json in js acts as a object. JSON can contain multiple arrays within arrays. You can break down the object into it's many parts and flatten them or you can send the entire json object to the API and store it in it's original form. Not really sure how this fitted in. But postgres + mariadb both support JSON. And postgres supports recusrive CTEs, so you can do a lot with JSON. &gt; I've hit that point where I'm just tired of writing migrations. Well you're doing it the hard way in that case, so fair enough being sick of it. There's much better ways to do this, especially for smaller projects that aren't like to have schema conflicts between devs. I've never gone near anything that needs you to write the migrations manually, that seems like a pain in the ass. I either use stuff that is declarative and syncs the schema to the declarations, or something that generates the migration files automatically by diffing the dev database with the previous version. &gt; ORM's require you to compromise a lot. Not sure what you mean here. Likely we have different definitions of what an "ORM" is. To me it's any code that is used to do database queries and populate application code objects. Doesn't matter if you wrote the code yourself or not. I'm not sure what the alternative is aside from horrendous copy &amp; pasted procedural code that is running every DB queries manually, and likely needing all the escaping to be done manually too.
True, I was tired and miscalculated. Still I rather write separate conditions because of SARGability (not in this case, just as a habit).
ZIPs and PHONEs aren't INTs. 
Does it not work because you need aliases in your select?
I would recommend you try to make a query that returns the result you want first. When you run this query, do you get any errors? One thing I noticed in your OP is that you spelled the foreign key column two ways "Product ID" and "productid". Look at your join again to confirm you have spelled the column names correctly.
I formatted the code &amp; posted like you advised. Hopefully someone will have some fresh ideas. Thanks again! [https://www.reddit.com/r/SQL/comments/a3ns66/need\_help\_with\_fairly\_simple\_sql\_query/](https://www.reddit.com/r/SQL/comments/a3ns66/need_help_with_fairly_simple_sql_query/)
I figured that would be an issue but was having trouble finding information on what the appropriate type would be for numbers. Would it be var char or something like that?
No, it just says `ERROR: relation "t1" does not exist` pointing to t1 in the subquery (right after `FROM` in the last line).
Cool, let me know how it turns out. 
Yes, varchar works. US ZIP codes can start with 0, which you'd lose if converted to an INT. Canada uses letters and numbers. Phones with area codes would overflow the maximum INT value, and often have formatting like '(321) 123-1234'
Don’t use t1 alias in the subquery, just select from the table. 
Int if it's an integer. Numeric(x,y) if it's not an int. Y is the measure of precision to the right of the decimal, x is the total length. The most common one I see is 10,2 .
I don't think you understand what my issues is, but that's ok because CTEs (`WITH`\-clauses), as u/[distgenius](https://www.reddit.com/user/distgenius) mentioned, are just what I've been looking for :-)
That’s fair. 
I'm not sure if MediaMonkey can handle "full" SQL or if it's more of a SQL-lite software, but give this a shot. Maybe it's a push in the right direction. &gt;SELECT b.SONG\_ID &gt; &gt;FROM &gt; &gt; ( &gt; &gt; SELECT a.SONG\_ID, &gt; &gt; ROW\_NUMBER() &gt; &gt; OVER(PARTITION BY [Artists.ID](https://Artists.ID) ORDER BY a.SONG\_ID ASC) AS "CNT\_ARTIST" &gt; &gt; FROM &gt; &gt; ( &gt; &gt; SELECT [Songs.ID](https://Songs.ID) AS "SONG\_ID", &gt; &gt; ROW\_NUMBER() &gt; &gt;OVER(PARTITION BY Genres.IDGenre ORDER BY [Songs.ID](https://Songs.ID) ASC) AS "CNT\_GENRE" &gt; &gt; FROM Songs &gt; &gt; INNER JOIN GenreSongs &gt; &gt; ON [Songs.ID](https://Songs.ID) = GenreSongs.IDSong &gt; &gt; INNER JOIN Genres &gt; &gt; ON GenreSongs.IDGenre = Genres.IDGenre &gt; &gt; WHERE Genres.Genre IN &gt; &gt; ( &gt; &gt; 'Blues', &gt; &gt; 'Rock' &gt; &gt; ) &gt; &gt; ) a &gt; &gt; INNER JOIN ArtistsSongs &gt; &gt; ON a.SONG\_ID = ArtistsSongs.IDSong &gt; &gt; INNER JOIN Artists &gt; &gt; ON ArtistsSongs.IDArtist = [Artists.ID](https://Artists.ID) &gt; &gt; WHERE a.CNT\_GENRE &lt;= 500 &gt; &gt; ) b &gt; &gt;WHERE b.CNT\_ARTIST &lt;= 5;
It's hard to understand what you want, but I'll try. So, out of the left join what you're getting is the countries that have no cities. This gives you countries with cities: select country, count(city.countryid) -- Though I'd use count(city.cityid) from country inner join city using (countryid) group by country So you could then union that with select country, 0 from country where not exists (select countryid from city where city.countryid=country.countryid) If that's what you want, but the left join is just cleaner in my opinion. 
Perhaps you could take a look at the concept of CTE (Common Table Expressions - [https://www.sqlite.org/lang\_with.html](https://www.sqlite.org/lang_with.html)). Using this format you could select all songs from the selected genres, then filter the result down further by limiting each artist's song lists. ;with LimitedGenreSongs as ( &lt;insert genre script here&gt; ) select * from LimitedGenreSongs where SongId in ( &lt;insert artist song limit script here&gt; ) &amp;#x200B;
Writing the query as a union statement rather than a left outer join can sometimes improve performance. The goal is to write one statement where you find all matches (using an inner join) and union **all** another statement where no matches are found. See pythor's example queries. &amp;#x200B;
Try googling "SQL UNION".. I think you will get on the right track!
&gt;That's not mid end i5 laptops or even most quad core i7 mobile CPUs. This kicks her off the C5 tablet, even a brand new one (they top out with an i7 5600U @ over $3k) doesn't meet that This effectively means putting a fairly high spec desktop or high end laptop in every patient room just to access patient records during a consult or session Right, that kind of sucks, but nevertheless my Latitutde 7480 will run that software and [here](https://www.newegg.com/Product/Product.aspx?Item=9SIAD0X6HM0270&amp;ignorebbr=1&amp;nm_mc=KNC-GoogleMKP-PC&amp;cm_mmc=KNC-GoogleMKP-PC-_-pla-TechDeals-_-Notebooks-_-9SIAD0X6HM0270&amp;gclid=Cj0KCQiArqPgBRCRARIsAPwlHoW0NQKvha8JWFtRIM9_u5-W02pcdFqQWClNEdU8G0_B3DzmUhsippkaAjKiEALw_wcB&amp;gclsrc=aw.ds) is one with an i7 and 16GB of RAM for $1300. While this software smells like it was poorly written, in fairness that isn't an expensive high end laptop and I would seriously question how many would need to be purchased for this clinic. You could also set up (1) or (2) robust machines with this software and then use tablets to remote onto those machines to view patient records. I would question how many people are accessing them concurrently and how many machines would be necessary for the business to function properly. Both of those options, without more knowledge, would seem to be cheaper for the client than changing software. &gt;behind a desktop/laptop monitor, not facing her clients, less eye contact (reduced quality of interaction) None of that means shit to a software developer that is only asking you have a computer that costs about $1000 to run their application. Apparently your client doesn't want to spend a grand per laptop to give the patient a better experience. So if it isn't worth a couple of grand to the client, you can imagine it is worth even less to the developers. &gt;None of this has anything to do with SQL anymore. That is my larger point. Not to be an asshole, but I wouldn't listen to any advice you get from a SQL forum about a problem like this. Prior to my current life as a SQL developer I co-founded an ran an online healthcare company for two years. I wouldn't advise you listen to me, either, but I'm just trying to point to some areas where you might be more focused on the software being the problem and not the client. &gt;The only thing I can really conclude is that either their basic support doesn't understand their own software, or the dev(s) are failing to facilitate a 'modern doctor-patient experience' with regard to my client. Another option is that the software is supported and written by teams outside of the country, and the support people don't know. You could test it on some less robust machines, but again I gave you an example of a $1300 laptop that will run your software. Generally speaking it is a bad idea for a database to be on a local machine carrying patient records. In order to be HIPPA compliant for something like that you'd need to have a variety of security/encryption and other types of software integrations -- which may explain the requirements they're giving you. For all you know the software company is planning for the future and those requirements listed are on a roadmap to get them through the next 5-10 years of development. Therefore they do not support slower machines and do not recommend that their clients purchase machines below those requirements. It could be that the software works today on a slower machine, and then it updates and you wake up the next morning and it won't run. You mentioned earlier that you're the IT guy and you told the client that they'd be fine with the technology they had for awhile... but did you ever consult with this software company before you made that statement? Again I'll point out that it could be hugely expensive for your client to change software. It might not be, you'd have to figure that out. If it isn't expensive then make a recommendation to change. If it is expensive then start shopping on NewEgg. I like the idea of setting the software up on a desktop and using devices to connect to it. There may be some additional software you could get to temporarily "send" the screen to a tablet, or have (1) version of the program running and accessing multiple records at the same time thereby eliminating the need for having multiple machines running the program. If the software is only being used to access records and not being used for the other types of features it is supposed to have then why not simply find an application to tap into the database directly and have people use some that application to access them on tablets? 
Are you sure that's the right think ? Because UNION require two requests... I don't want to put the results of my requests one after the other, but all in one line
u/themistik doesmn't want to put record 1 below record 2, that is already the results. Instead, the user wants to put record 1 beside record 2.
What is the primary key that is relative to both Lasagna and Cookie?
The primary key for the commentary of the ETA ? For ETA, it's for both results 1. For Commentary, well, it's 1 for Lasagna and 2 for Cookie
No, I mean \*why\* do you want Cookie and Lasagna to be next to each other. What field considers them similar to each other?
The thing is, I explained, the commentary are not similar. It's the ETA related to them that is similar. Like you have a service and you have to make commentaries. For each commentary, you make a new row in your database But for the service 1, you will link X numbers of commentaries. Like, for the service 1, there is commentary 2, 3 and 4. And for service 2, there is commentary 1, 5, and 6. I want commentaries to be on the same line, in order to read them more easly.
&gt; Would it technically be possible for me to install SQL Server of the same version on a throw-away server temporarily for the sake of running a database backup on that temp server and use that backup to restore the production server? Take a backup first of what you have on your server, so if things go south now, at least you can get back to where you are. It is technically possible to do an install on another server, you could even use the developer edition. &gt;Another thing to note is that you can restore msdb and model databases across **editions** such as Express, Developer, Standard and Enterprise. You only need to be concerned that the **versions** are the same. &gt; &gt;[Source](https://www.mssqltips.com/sqlservertip/2571/restoring-sql-server-system-databases-msdb-and-model/) You can also fix your MSDB via these steps and instructions, they are basically the same. One is from Microsoft and has lots of extra non-applicable information and the other is a user experience guided tutorial. [https://www.mssqltips.com/sqlservertip/3658/how-to-fix-a-corrupt-msdb-sql-server-database/](https://www.mssqltips.com/sqlservertip/3658/how-to-fix-a-corrupt-msdb-sql-server-database/) [https://docs.microsoft.com/en-us/sql/relational-databases/databases/rebuild-system-databases?view=sql-server-2017](https://docs.microsoft.com/en-us/sql/relational-databases/databases/rebuild-system-databases?view=sql-server-2017) What sucks is that you are probably going to lose all of your MSDB data, like schedules, jobs, etc. A long time ago I found this site and it has a powershell script that I ran. What it does is connect to SQL Server and then extracts the exact text necessary to rebuild SQL Agent jobs. You may want to look at this and see if it's worth running to create some backups of your jobs (if even possible) before switching out MSDB. [https://rahmanagoro.wordpress.com/2010/08/26/script-out-sql-agent-jobs-from-powershell/](https://rahmanagoro.wordpress.com/2010/08/26/script-out-sql-agent-jobs-from-powershell/) I will post the code below as I have altered it slightly, again, all credit goes to the above and below link. Powershell Script: # Source: https://rahmanagoro.wordpress.com/2010/08/26/script-out-sql-agent-jobs-from-powershell/ # Load SMO extension [System.Reflection.Assembly]::LoadWithPartialName("Microsoft.SqlServer.Smo") | Out-Null; # Get List of sql servers to check ########## START VARIABLES THAT YOU MUST SET ########## # Declare where the Home is $HomeFolderLoc = "E:\Backups\"; # Declare where the Scripts are $ScriptFolderLoc = $HomeFolderLoc + "Jobs\"; # Declare where the archive files are $ArchiveFolderLoc = $ScriptFolderLoc + "Archive\"; # Declare name and format of file showing list of servers $Servers = $HomeFolderLoc + "Servers.txt"; # Declare format and names of files to move to archive $SqlFiles = $ScriptFolderLoc + "*.sql"; ########## END VARIABLES THAT YOU MUST SET ########## # Grabs SQL Servers from the list $sqlservers = Get-Content $Servers; # Move files to the archive so fresh scripts populate the folder #Move-item $SqlFiles $ArchiveFolderLoc -Force; # Set the age limit of files to exist $limit = (Get-Date).AddDays(-15) # Delete files older than the $limit. Get-ChildItem -Path $ArchiveFolderLoc -Recurse -Force | Where-Object { !$_.PSIsContainer -and $_.CreationTime -lt $limit } | Remove-Item -Force # Create a datestamp for the sql filenames $DateStamp = get-date -uformat "%Y-%m-%d"; # Loop through each sql server from sqlservers.txt foreach($sqlserver in $sqlservers) { # Create an SMO Server object $srv = New-Object "Microsoft.SqlServer.Management.Smo.Server" $sqlserver; # Jobs counts $totalJobCount = $srv.JobServer.Jobs.Count; $failedCount = 0; $successCount = 0; # For each jobs on the server foreach($job in $srv.JobServer.Jobs) { # Default write colour $colour = "Green"; $jobName = $job.Name; $jobEnabled = $job.IsEnabled; $jobLastRunOutcome = $job.LastRunOutcome; $jobNameFile = $ScriptFolderLoc + $jobName + $DateStamp +".sql" Write-Host $job.Name Write-Host "The location of the file is called " $jobNameFile $job | foreach {$_.Script()} | out-file $jobNameFile # Set write text to red for Failed jobs if($jobLastRunOutcome -eq "Failed") { $colour = "Red"; $failedCount += 1; } elseif ($jobLastRunOutcome -eq "Succeeded") { $successCount += 1; } Write-Host -ForegroundColor $colour "SERVER = $sqlserver JOB = $jobName ENABLED = $jobEnabled LASTRUN = $jobLastRunOutcome"; } # Writes a summary for each SQL server Write-Host -ForegroundColor red "========================================================================================="; Write-Host -ForegroundColor red "$sqlserver total jobs = $totalJobCOunt, success count $successCount, failed jobs = $failedCount."; Write-Host -ForegroundColor red "========================================================================================="; } .bat file that kicks off the powershell from SQL: powershell.exe -file E:\Backups\SQLJobBackup.ps1 Contents of the server file: server\server &amp;#x200B;
Gotcha, I understand now. Based on SOUS\_ETA is what you need linked together.
What do you mean ?
Ah yeah, I remembered UNION wrong. You have to make subqueries then: select a,b,c,(select d,e,f from table b where ...) from table_a where ...
Give this a shot, I'm not sure if it'll work as I've never done it in MySQL, only Oracle and SQL Server 2017. From the looks of the internet, this is how the MySQL syntax is. Maybe it's at least a push in the right direction. SELECT SOUS\_LIBELLE, SOUS\_DEB, SOUS\_D, SOUS\_ETA, COMM\_TEXT, GROUP\_CONCAT(COMM\_TEXT) OVER(PARTITION BY SOUS\_ETA ORDER BY COMM\_TEXT ASC) AS "CONCAT\_COMM\_TEXT" FROM T\_SOUS as sou INNER JOIN T\_ETA\_has\_T\_SOUS as aso ON sou.ID\_SOUS = aso.T\_SOUS\_ID\_SOUS INNER JOIN T\_ETA as eta ON aso.T\_ETA\_ID\_ETA = eta.ID\_ETA INNER JOIN T\_COMMEN\_has\_T\_SOUS as asos ON asos.T\_SOUS\_ID\_SOUS = sou.ID\_SOUS INNER JOIN T\_COMMEN as com ON com.ID\_COMMEN = asos.T\_COMMEN\_ID\_COMMEN WHERE ID\_ETA = 1
... oh my. I'll check this thanks
Turns out you can't use GROUP\_CONCAT() in a windowed function in MySQL, weird. I think this might get you a little closer, but you'll have to add the rest of the fields back in. &gt;SELECT &gt; &gt;SOUS\_ETA, &gt; &gt;COMM\_TEXT, &gt; &gt;GROUP\_CONCAT(COMM\_TEXT ORDER BY COMM\_TEXT ASC SEPARATOR ', ') AS "CONCAT\_COMM\_TEXT" &gt; &gt;FROM T\_SOUS as sou &gt; &gt;INNER JOIN T\_ETA\_has\_T\_SOUS as aso &gt; &gt;ON sou.ID\_SOUS = aso.T\_SOUS\_ID\_SOUS &gt; &gt;INNER JOIN T\_ETA as eta &gt; &gt;ON aso.T\_ETA\_ID\_ETA = eta.ID\_ETA &gt; &gt;INNER JOIN T\_COMMEN\_has\_T\_SOUS as asos &gt; &gt;ON asos.T\_SOUS\_ID\_SOUS = sou.ID\_SOUS &gt; &gt;INNER JOIN T\_COMMEN as com &gt; &gt;ON com.ID\_COMMEN = asos.T\_COMMEN\_ID\_COMMEN &gt; &gt;WHERE ID\_ETA = 1 &gt; &gt;GROUP BY SOUS\_ETA; &amp;#x200B;
honestly dude after hours of calling people who applied, this makes the most sense. If your interested please let me know. 
 I thoroughly tested this out. It seems the format does not work. Unfortunately it does not give me any real error info... all the app returns if the SQL does not work is: "The error probably resides in the Filter or SQL filter qualifiers" &amp;#x200B; However, I did edit my OP and linked to the manual... it is a good bit outdated though. 
Unfortunately it does not work... I updated my OP with the only manual I can find about the languages capabilities. I am going to email the developer and see if he has any insights. 
That's unfortunate. Best of luck.
Thanks :) 
I'm always down to randomly critique others online
And with SP_WHOISACTIVE he sees you when you’re sleeping and knows when you’re awake. 
improving a query can fall into two categories -- clarity, readability, maintainability on the one hand, and performance on the other hand you can only go so far in the first category, for which this sub is well suited, as those concerns transcend any specific platform for performance, you pretty well have to identify your database platform, as well as other information such as indexes, etc.
He also be checking you all's bad coding deeds :) SELECT ao.owner FROM all_objects ao JOIN user_source us ON ao.object_name = us.name AND ao.object_type = us.type WHERE instr(UPPER(text), UPPER(bad_deeds_list)) &lt;&gt; 0;
I see you haven't gotten an answer yet, so I'll give it a go. This sounds like a case for what I call a "query builder query", where the output of one query generates a number of other queries. Hopefully that makes sense...here's how I'd do it for your question: select sqlstr = 'Drop View A' + convert(varchar(2), primarykey) + char(13) + char(10) + 'Create View A' + convert(varchar(2), primarykey) + ' as select * from Alphabet where primarykey = ' + convert(varchar(2), primarykey) from Alphabet Note: this was written/tested in MS SQL, but I assume it would be possible in MySQL.
&gt;is out. It seems the format does not work. Unfortunately it does not give me any real error info... all the app returns if the SQL does not work is: Could you post the full sql that you tested?
Post some code here! We'll tell you what's up...
I'm curious to see how this goes, because I started to wrap my head around it and was running into issues getting the logic into a single query.
It works! Trying to wrap my head around it to get a little better you are way more advanced then I am. Say I wanted to add a where statement where a column from my original table (tableA) is equal to a value. Is this possible from the above query? Basically I want to tack on a: Where tableA.code = ‘1’ Is tableA called out in your a-g A simple NO is fine you’ve done more then enough haha
Thank you! I understand, build strings for each row and then copypasta it into the query.
I am using the following regular expression, which I took from another report (that was not built by me): First one changes the column red if the value is less than or equal to 0, second one changes the column to black and the third one (which I have attempted to hijack from the other one, should change the text to blue if the column is empty, though I am not very clear on the regex for empty, I have tried NULL etc. `=iif(Fields!feesOfficeCurr.Value &lt;= 0, IIF(Fields!CATEGORY.Value=left(MonthName(Month(now)), 3), IIF(Fields!CATEGORY.Value &lt;= 0 , "Red","Black","Blue"), "Black")`
Post the code. Lots of lurkers here to provide feedback.
This subreddit hardly has a high volume of posts so I'd say contributions are welcome. If your queries aren't too complex then putting together something in SQLFiddle or similar can be really helpful to understand the context. 
Great! I'll be back when I need to then. Thanks
Mainly it's things like would grouping work better than a sub query or how do I get xyz by xyz... That kind of thing.
Why not use something like sql fiddle? 
That's great! Glad to have helped. * **If you want to add the additional qualifier**, you'll need to add it between existing lines 77-78. * ***If it does not work*** ***and aggregates incorrectly***, you'll need to change line 77 to a subquery SELECT \* FROM tableA WHERE code = '1'. Hopefully that makes sense. **Note:** I would highly recommend adding a WITH (NOLOCK) to your table references to prevent locking, just read uncommitted, unless this query just needs to read only committed transactions. **I'll explain how it works so you can have a better understanding, although I'm not the best at explaining how my code works:** 1. We start with your existing table, but we don't know what date transaction comes another date transaction. To find this out, we use LAG() OVER(PARTITION BY...). This will lag the begin and end dates by one and two records, dependent upon the lag field. You can just run this subquery to see what it's doing. 2. Subquery \[a\]: Then, we need to calculate the begin and end dates. We calculate them by using some logic in CASE statements. If all the lag days are NULL, then we know it was the first day for that partition, setting the CALC\_BEGIN\_DATE to START\_DATE. If the date difference between LAG\_START\_DATE\_1 and LAG\_START\_DATE\_2 are &gt; 28 days *(what I consider a month, you can change this if you need to)*, then we consider it a different transaction, setting the BEGIN\_DATE to LAG\_1\_START\_DATE. We perform a similar calculation with end date checking to see if the date difference between LAG\_1\_START\_DATE and END\_DATE &gt; 28, then it's a new calculated end date. If it is the last date in the partition, or MAX(END\_DATE), then we know it's the very last day of that partition. You can just run this subquery to see what it's doing. 3. Subquery \[b\]: This is where things get fun. We already have the dates calculated, but they're all over the place in this nonsense-filled table full of NULLs where nothing is lining up. You can see the data coming together here, but it's nowhere near what you want. This is where we use SUM(CASE WHEN...) to on-demand increment each of the partition, essentially assigning 11/07 to the same partition as 11/21 for Member A. We refer to these fields as TRG\_\*\_DATE Downside, the data still looks like a huge mess even after doing this, but it's coming together! 4. Subquery \[c\]: This is a really simple step, for anywhere there is a TRG\_\*\_DATE field, I set the 0 to a 1. Looking back on it, this code shouldn't be necessary anymore. I wrote it for something that I was trying but didn't work out. You could try taking it out and see if it's still okay. 5. Subquery \[d\]/WITH \[memberData\]: This calculates the MAX(CALC\_\*\_DATE) OVER(PARTITION BY...) to get the largest one for the partition, mainly to fill in the remaining NULL values that have been left behind. 6. Below the WITH: We join the WITH on the WITH, back onto itself, where the MEMBER\_ID = MEMBER\_ID and the TRG\_BEGIN\_DATE = TRG\_END\_DATE. The member ID needs to be the same pulling from both sources and both the partitions *(yes, those are still hanging around)* need to be the same to line the BEGIN\_DATE = END\_DATE. There's one issue, because we lagged the records *all the way back* in first part of the query, we're never going to get the END\_DATE record. Why? Because the future doesn't exist. We can't lag into the future. (Actually, in Oracle, doing a -1 you can, but in MSSQL, you can't, so... lol.) To fix this, we UNION ALL memberData *(we don't union the whole table, only the very last record of each partition, that's where the e.RN = 1 comes into play)* back onto the two INNER JOINs so we can find that missing END\_DATE. 7. Then we need to de-dupe now that we just did that UNION ALL and remove any unnecessary duplicate END\_DATE records, so we do one last ROW\_NUMBER() OVER(PARTITION BY...) to find where there are duplicates in subquery \[f\], using ORDER BY to find the valid ones. And in subquery \[g\] we only select the values that returned g.RN = 1. Anything where g.RN = 2 was an unintentional duplicate END\_DATE.
***Tl;dr:*** It worked. [https://www.reddit.com/r/SQL/comments/a3ewyq/mssql\_need\_help\_rolling\_up\_dates\_into\_one\_row/eb83gp1](https://www.reddit.com/r/SQL/comments/a3ewyq/mssql_need_help_rolling_up_dates_into_one_row/eb83gp1) [https://www.reddit.com/r/SQL/comments/a3ewyq/mssql\_need\_help\_rolling\_up\_dates\_into\_one\_row/eb874hm](https://www.reddit.com/r/SQL/comments/a3ewyq/mssql_need_help_rolling_up_dates_into_one_row/eb874hm)
I don't think you can use an IN() statement *(in the way you currently have it formatted)* because the subquery is returning two values but the parent query is only requesting one value. Your query also isn't getting the most spent by a user. I didn't run this, but try: SELECT a.USER\_ID FROM ( SELECT dt.USER\_ID, dt.SUM\_AMT ROW\_NUMBER() OVER(PARTITION BY dt.USER\_ID ORDER BY dt.SUM\_AMT DESC) AS "MAX\_SUM" FROM ( SELECT USER\_ID, SUM(Amount) OVER(PARTITION BY USER\_ID) AS "SUM\_AMT" FROM User u INNER JOIN purchase p ON u.USER\_ID = p.USER\_ID ) dt ) a WHERE a.MAX\_SUM = 1;
Can you just put the artitst restriction inside the genre restriction? &amp;#x200B; [Songs.ID](https://Songs.ID) IN ( SELECT [Songs.ID](https://Songs.ID) FROM Songs , ( SELECT DISTINCT IDGenre FROM GenresSongs ) AS T1 WHERE [Songs.ID](https://Songs.ID) IN ( SELECT [Songs.ID](https://Songs.ID) FROM Songs INNER JOIN GenresSongs ON [Songs.ID](https://Songs.ID) = GenresSongs.IDSong INNER JOIN Genres ON GenresSongs.IDGenre = Genres.IDGenre WHERE GenresSongs.IDGenre = T1.IDGenre AND [Songs.ID](https://Songs.ID) IN ( SELECT [Songs.ID](https://Songs.ID) FROM Songs , ( SELECT DISTINCT IDArtist FROM ArtistsSongs WHERE IfNull(PersonType, 1) = 1) AS T1 WHERE [Songs.ID](https://Songs.ID) IN ( SELECT [Songs.ID](https://Songs.ID) FROM Songs INNER JOIN ArtistsSongs ON [Songs.ID](https://Songs.ID) = ArtistsSongs.IDSong AND IfNull(ArtistsSongs.PersonType, 1) = 1 INNER JOIN Artists ON ArtistsSongs.IDArtist = [Artists.ID](https://Artists.ID)WHERE ArtistsSongs.IDArtist = T1.IDArtist ORDER BY Random() LIMIT &lt;Number|Caption:Number of files per Artist|Value:5|MinValue:0|MaxValue:100|ID:1&gt; )) ORDER BY Random() LIMIT &lt;Number|Caption:Number of files per Genre|Value:20|MinValue:0|MaxValue:100|ID:1&gt; )) AND &lt;Genre&gt; IN ('Blues', 'Rock')
So, an account has a parent account that it can be tied back to. This is the part that I'm not sure I understand. The sales team wants to use a checkbox on the Parent Account to say "Key Account" and have that flow down to all child records. 
Was not aware of its existence but it looks useful, thanks.
Exactly this. Accounts are all the same record type, and have a look up field to a "Parent Account" Multiple Accounts can be tied to any "Parent Account" The Sales team wants check/uncheck a checkbox on the Parent Account, so they can view the sales/opportunities for an entire hierarchy. Today, we ask them to check every account by hand that they want to view. It's tedious, and they no longer want to do this.
`FOREIGN KEY (ArticleID) REFERENCES Author(AuthorID)` -- really? so an author can write only one article? also, PK/FK pairs must have the same datatype, yours don't
Run this for me: SELECT * FROM author WHERE AuthorID IN ('9253','Elon Musk');
it doesn't have to be so hard!! SELECT User_ID , SUM(Amount) AS total FROM purchase GROUP BY User_ID ORDER BY total DESC LIMIT 1
&gt;Right, but purely on that point... yeah you save a little time dealing with schema migrations, but now you have to write fucktons of manual code for constraint checking, and doing basic simple data processing tasks that could easily have been done with a few JOINs/VIEWs. You aren't constraining your opinion to different use cases. Your argument is that it doesn't apply to what you do. What I'm pushing NoSQL for is to handle large transaction data. 2018 Sensor Data { "Object":"10", "Time":"05:15:00 10/01/2018", "Lat": 1245.15, "Long": 1255.167 } 2019 Sensor Data { "Object":"10", "Time":"05:15:00 10/01/2018", "Lat": 1245.15, "Long": 1255.167, "Alt": 152.11, "Operator":"TA112224" } I really don't have to maintain consistency between the data. If I write a view or choose to index Operator on the sensor data it just treats the lack of the key on the root position as a null. I also really don't care for the old data as it's been pulled from my NoSQL solution to my star schema on SQL. In my use case for NoSQL, there isn't a "fuck-ton" for uses for constraints. JSON and XML schema's have versions included in their headers. If you have sweeping changes in your JSON or XML content, you should be versioning your data to a schema version. If the data is old, you should be handling using older mapping documents. In my use case though, I'm purposefully avoiding this because I don't have any specifications I absolutely must meet and I'm using NoSQL because of it's distribution features and loose schema. If I needed tighter constraints, I'd probably go with another technology of version my schemas and upgrade data as needed. &gt;Not really sure how this fitted in. But postgres + mariadb both support JSON. And postgres supports recusrive CTEs, so you can do a lot with JSON. Check out postgraphile and postgrest too if you haven't before. They create entire APIs for you without even writing any code. Going back to rapid app development. A recordset isn't an object that front end developers work with these days, or at least would want to. Being able to parse JSON and being able to consume JSON as an API is a big difference. Maybe postgresql or mariadb consume these now but historically they didn't. NoSQL was booming like 6 years ago when many of these SQL databases didn't offer these services. &gt;Not sure what you mean here. Likely we have different definitions of what an "ORM" is. To me it's any code that is used to do database queries and populate application code objects. I've worked with ORM's that read dirty form data and rather than update an object once, it updated an object multiple times per each column having a change. This drove me nuts as if you didn't pass in a set list of dirty properties and just wanted to reconcile some changes on a list of objects, it would update the entire object that way. We had tables with 50 columns and it would need to update 1500 rows so the ORM would then perform 75,000 transactions. Everyone has had an ORM that has given them the option to rewrite, write their own, or just deal with crappy ORM logic. &gt;Not to mention all the time you're going to be spending in the future fixing your broken inconsistent data (assuming you ever discover the problems to begin with). You know, I am not going to continue to have a conversation if you are going to make statements like I would have poor judgement to not maintain my code because it's not the way you would do it. I feel and have stated that SQL and NoSQL have their uses, you apparently disagree and think what you are doing is flawless. If that is so, great! Have a wonderful day. I hope you are very successful. 
And it looks like the other table is also referencing the same field as a foreign key, but on one is inserting a number on the other an actual name lol. 
Sounds like they want a summary. If so, this could much easier be achieved with a view than a stored procedure. How do you know if an account has a parent account? Is there another field or another table denoting the parent account?
True. That is the easiest way straight out of the gate. 
They are error codes. You can google them for answers to the issues you are facing.
Are they indexed anywhere so that I can look at the list of errors. Apparently MS VS does this as well.
yes, this is the right subreddit post your table designs and your query, we'll have a go
Again, you can more than likely find this list online. Theyre generally well documented.
Thank you.
The server hardware was intended to be fine for awhile, a xeon 1243v3, one of the faster clocked quad core xeons, sits near the top end of what sql server express will use (instance limited to use of '1 socket or 4 cores' and 1GB RAM use). As far as the client/workstation machines, we were expecting some modest upgrades there soon, I initially proposed upgrading the lower end i3 lap (it's a sandy bridge G2 socket, 1333mhz DDR that'll accept up to 45w quad core CPUs, and a newer used C5. The C5 she's using is a few year old 1.5-2.5 ghz i7 U680. I can't believe they'd roll out client software and insist on specs a newer/current hardware build C5/F5, with an i7 5600U (pc mark score around 4400) wouldn't be adequate just to access EMR that it's used for. Or that a quad core mobile 2820 can't handle. They've decided to get sticky on the base clock and 'quad core CPU'. Accounting/billing/report activities aren't done on the mobile devices. That's all done by running the client software on the server itself. I've watched resource use on the server-- doesn't even begin to tap into what that hardware is capable of. Throwing that level of hardware at the clients doesn't make any sense. There have been issues since the last software version with 'slow access' to records. Whatever issue is at heart, with system resources on the server (running the client software) sitting near idle while their software appears to hang for 3-5-10 seconds at a time, I don't get the impression faster hardware will solve this. If I convince her to spend a ton of money on high end hardware (and these issues remain, which I peg as 'likely'), then I look bad.
Wow, your code is massive! Is this the average length and complexity for SQL queries you write professionally? Thanks!
Apex Refactor does nearly the same thing as SQL Prompt with re-formatting. It's also a free add-on for SSMS
It depends. I'd say the normal length of a query is probably between 100-400 lines, however the project I am working on is massive and essentially its own database. It currently is coming in around 9000 lines of code.
To find an empty cell in SSRS, I've had luck with: CStr(Fields!FieldNameHere.Value) = ""
You know what they say about the road to hell and good intentions, right? &gt;, sits near the top end of what sql server express will use But that isn't what they're using. &gt;I can't believe they'd roll out client software and insist on specs a newer/current hardware build C5/F5, with an i7 5600U (pc mark score around 4400) wouldn't be adequate just to access EMR that it's used for. Or that a quad core mobile 2820 can't handle. They've decided to get sticky on the base clock and 'quad core CPU'. I mean, I don't know shit about this, but I know that a $1300 laptop will solve the problem. The software company might suck, but fact here is that you didn't check to see what their requirements were going to be in the future before you made your recommendation. You don't have to tell the client that, but you could have done better with your initial assessment to their hardware needs. It doesn't matter if the software sucks and requires more hardware than it should if the client needs that software to run their business. &gt;Accounting/billing/report activities aren't done on the mobile devices. That's all done by running the client software on the server itself. So develop a software solution that lets them get patient records on tablets without using the software you're talking about. If you aren't a developer, and just an IT guy, then find the software, or put your client in touch with someone who can provide this solution without having to upgrade the hardware. The cost for this service may exceed the cost of upgrading the hardware. &gt;If I convince her to spend a ton of money on high end hardware (and these issues remain, which I peg as 'likely'), then I look bad. I mean, I gave you a link to a $1300 laptop that will run that software. How many of these does the client need? If you buy them, and they run the software, but the software sucks... how is that on you? Are you the hardware guy or the software guy? 
Exactly.
Like, as an example, I'd be interested in developing a solution for your client to access records on tablets providing they exist in a database which is accessible. Whether this solution is compatible with HIPPA or not is not something I'm willing to guarantee, and if I were to guarantee it then the cost would jump greatly simply due to the sheer complexity of the regulations relative to compliance. At the end of the day if I gave you a rock bottom price of $50/hr then after ~20 hrs of work you could have (1) laptop that runs the software you want. You seem to be complaining this software isn't designed for tablets, or patient experience, and maybe you're right... so start your own company that produces software that does and try to sell it. If you don't want to do that, and you can't find another vendor to provide the same solution... then you gotta buy some new laptops. Like, forget the why... you don't have any other choices but the ones I just outlined, correct? So move down that path.
boooo \^\^\^(good job)
Holy cow! Why are your queries this long? Are you building tables using a bunch of INSERT INTO statements? Every resource I looked into studying has no-where near as much code. I would like to come closer to your level. Are there any learning resources that you can recommend that implements such large codes? Thanks for the reply!
They aren't single queries, they're chains of queries and stored procedures. There are multiple tables being built. &gt; I would like to come closer to your level. Are there any learning resources that you can recommend that implements such large codes? Thanks for the reply! I'm just an amateur.
Check out the LAG function for SQL Server. &amp;#x200B; [https://www.mssqltips.com/sqlservertip/3468/sql-servers-lag-and-lead-functions-to-help-identify-date-differences/](https://www.mssqltips.com/sqlservertip/3468/sql-servers-lag-and-lead-functions-to-help-identify-date-differences/)
This is exactly what I need. Thanks! 
LAG is a good approach. An alternate approach would be to use a common table expression. This should work: ;WITH CTE_Dates AS ( SELECT DISTINCT ImportDate ,ROW_NUMBER() OVER (ORDER BY ImportDate) RowNum FROM ImportedFiles ) SELECT A.ImportDate ,DATEDIFF(day,B.ImportDate, A.ImportDate) DaysBetween FROM CTE_Dates A INNER JOIN CTE_Dates B ON A.RowNum = B.RowNum + 1
You are a huge help. I did some more troubleshooting last night and discovered that the database installed on the viewpoint server was SQL server 2016 RTM, which Microsoft no longer hosts a download for, and I haven't been able to find a download for it elsewhere. Ultimately, I wasn't able to install a similar version on a different server, as the same version is no longer available. I'll most definitely take a look at the links and script you posted. Thank you so much for the reply!
this is a clever solution. I think with an ORDER BY clause in the CTE it would work perfectly. thanks!
lol. Nice.
Thanks! I forgot to ORDER BY in the outer query -- I'll add that. You shouldn't need an ORDER BY in the CTE, though, because it's already included via the ROW_NUMBER function, which handles the assignment to RowNum.
ah ok, good to know! 
Based on your explaination, I don't fully understand what you are trying to accomplish, but perhaps a Switch Statement will do you better than an IIF. =Switch(Fields!feesOfficeCurr.Value &lt;= 0, "Red", Fields!CATEGORY.Value=left(MonthName(Month(now)), 3), "Black", Fields!CATEGORY.Value &lt;= 0, "Blue") Note: To add an 'ELSE' statement to the Switch, the final line could look something like this 1=1,&lt;Else Do This Thing&gt;) Otherwise, it seems your syntax on the IIF is off. The IIF statement only takes three parameters, and the code you pasted for this piece IIF(Fields!CATEGORY.Value &lt;= 0 , "Red","Black","Blue") has too many parameters 
Do you need single quotes around the fromDate and toDate?
Not CONTAINS. Just use WHERE date = dateVariable AND date2 = date2Variable. 
Definitely check out the links, they go into detail about how to repair that database. 
I've spoken at one SQL Saturday, volunteered at three, attended at least ten, and ran two after parties. &amp;#x200B; SQL Saturdays are fantastic networking events. Bring resumes, business cards, ways to take notes, and make sure you're attending things that are interesting. Make a point to introduce and chat to someone at least once per session, check out twitter and see if anyone is tweeting about the event (may be a good lunch buddy), and remember that everyone attending this event are just people like yourself. Some of us are working a job outside business hours, traveling thousands of miles, and we're trying to put on a great event. Some of us are volunteers and are doing this out of our own pocket to help contribute back to the community. And a lot of us are just folks taking time out of our own schedule to meet other like minded career individuals and better ourselves. I have yet to have a bad experience or meet someone awful at these events, but I'm sure there are some bad eggs. &amp;#x200B; Personally, I think the networking piece is the biggest and most important part. The second most important part is getting good information from the presentations and attending things that will benefit you. Most presenters will make their slide decks available online, so don't worry about writing down their slides verbatim. Write the notes and things that would help you piece together their presentation when they aren't there presenting. Presentations can vary wildly, I've seen some where they had an intro and end slide, some were 100% participation based, some were pure demo based, and everything in between. &amp;#x200B; Some people like the vendor booths, some people don't. I would recommend selling your soul to them. You cut up your "entry" tickets which contains your information so they can try and sell things to you. As compensation, you are entered in a raffle. I've won two $100 Amazon gift cards and an Echo from all the SQL Saturdays I've attended, so the prizes are actually worth something. (I'm sad I never got the Redgate Toolbelt license though...) &amp;#x200B; Can you attend the pre-cons? I attended the Extended Events pre-con by Jason Brimhall and it was outstanding. You will get so much more out of those pre-cons and workshops then the SQL Saturday event itself. The event is free however, (maybe just the cost of lunch) and the pre-cons are not. &amp;#x200B; I've worn everything from a suit to shorts / sandals / graphic t-shirts to these events. I'd recommend to dress business casual though if you are potentially looking for a job. &amp;#x200B; After parties also vary wildly based on who's hosting it. I've seen the after parties where they will pay for 1 drink per person, the parties I hosted the company gave me the credit card and said keep the tab under X. Just be gracious and enjoy yourself while making great connections. &amp;#x200B; Are there any specific questions you have? &amp;#x200B; &amp;#x200B;
That's good to know. I guess I had no idea it would have that effect. I will try removing the null values.
OK thanks for those examples. Is col1,col2 recognized by SQL or are those just placeholders you are using for the example? Is foo actual Syntax or is that a placeholder? I haven't heard about that before. I would have thought that where you have foo I would need to define the table. I updated my tables to not say "Not Null" and edited the script to define each column. However, now It's saying: https://imgur.com/YlihNFz Incorrect syntax near ';' I can't tell what it is referring to since there are no ; in the script let alone at that line. Thank you, 
With sp_Blitzcache he knows if you've been bad or good.
I'll start by asking are you sure you want to structure your table like this in its final form? Essentially all the days are being pivoted out, which could lead to an infinite amount of columns. This could end up becoming a truly horrific thing haha. On the contrary, if you have a dataset that is of limited date ranges, I'd understand.
As someone who has also spoken at a SQL Saturday, attended 7, and organized 5 of them, I'm going to say that everything /u/FoCo_SQL said is spot-on. Re: vendor booths specifically, **please** go talk to them, enter their raffles, check out their offerings. Without those vendors, SQL Saturday doesn't happen and they need to see a return on their dollars spent in sponsoring the event and sending people to staff the booth. Thank them for being there. What to wear: whatever's comfortable. This isn't a job fair. Mind if I ask which one you're going to? The only one in the US between now and January is Washington, DC and if I wasn't working Saturday, I'd probably be jumping in the car tomorrow morning to drive down for it.
Is this post deleted? 
&gt; You aren't constraining your opinion to different use cases. I agreed earlier that nosql makes sense for some data, but not most of it by default, or as the **only** db in most projects. The stuff I wrote just there was specifically responding to you talking about "it's about speed. People these days want fast app and microservice development. " ... so I thought you were speaking generally there. &gt; Your argument is that it doesn't apply to what you do. Yeah I think we got confused here. Those points were specifically towards nosql-for-all-data on common projects. So maybe I misunderstood you points there. Seems we already agreed earlier on the some-vs-all data thing. &gt; pulled from my NoSQL solution to my star schema on SQL. Sounds like we're on the same page. &gt; A recordset isn't an object that front end developers work with these days, or at least would want to. Being able to parse JSON and being able to consume JSON as an API is a big difference. Have you used graphql? I'm pretty new to it, but it's looking like it's going to make webdev as a whole much easier, especially removing a lot of the need for lots of communication/documentation between frontend + backend devs. Graphql documents itself. Check out postgraphile too, it's pretty amazing. &gt; Maybe postgresql or mariadb consume these now but historically they didn't. NoSQL was booming like 6 years ago when many of these SQL databases didn't offer these services. Yeah fair enough. I've stopped using mysql/maria myself, but postgres is pretty good as JSON stuff, you can index on JSON properties and lots of other features I haven't used yet. &gt; I've worked with ORM's that read dirty form data Well those issues sound annoying, but those are issues with whatever ORM you use, not ORMs in general. But on my point of the definition of an "ORM"... everyone is using one in some form (aside from really crappy n00b devs manually writing putting strings together of SQL commands and manually escaping etc. &gt;&gt; Not to mention all the time you're going to be spending in the future fixing your broken inconsistent data (assuming you ever discover the problems to begin with). &gt; You know, I am not going to continue to have a conversation if you are going to make statements like I would have poor judgement to not maintain my code I didn't mean you specifically, that was aimed at any project in general that stores ALL of their data in nosql. So I think we got confused here again on which bits I was aiming at. Again, agree with your use case that you mentioned. I've also considered nosql many times for interim storage of JSON data and stuff like that. Thanks for taking the time to share your views. Like almost every debate it seems, I think we accidentally ended up debating two different topics (apples vs oranges), so no wonder we didn't seem to be agreeing. :)
JK I got it. I'm a dummy. Once I wrote it out as: Import-CSV C:\NewClientData.csv | ForEach-Object {Invoke-Sqlcmd ` -Database $database -ServerInstance $server ` -Query "insert into $table (first_name,last_name,samAccount,city,county,zip,phone1,phone2) VALUES ('$($_.first_name)','$($_.last_name)','$($_.samAccount)','$($_.city)','$($_.county)','$($_.zip)','$($_.phone1)','$($_.phone2)')" } All you folks are a life saver. it works beautifully now.
Awesome thanks
Yes, i could definitely see this being problematic but this query is just to help me diagnose a problem that started sometime between 11-1 and today, so it's a single use type of thing. 
K, let's go with that as it won't be so hellish. SQL Server, Oracle, or something else?
I'm using snowflake but I can probably translate from mysql if that's simpler.
K. I'll write it as best I can. I'm not familiar with MySQL, mostly MSSQL and Oracle.
Thank you so much! I am not completely useless with sql and can usually modify example queries from other sql flavors to fit my needs but I'll be grateful for any help just to have a starting point. I noticed snowflake has a pivot function so I'm playing around with that now. 
Glad to hear it's working. My example wasn't very clear - "foo" is just an example table name which has three columns called col1, col2, col3. Next time I'll give a full CREATE TABLE statement for clarity.
The only way I know how to do it (without investing too much time into a pivot function is: WITH issueData AS ( SELECT event\_type, date, COUNT(event\_id) OVER(PARTITION BY date) AS "CNT\_BY\_DATE" FROM \[TEST\_REDDIT\].\[dbo\].\[SPRINGCLEANMYLIFE\] ) SELECT event\_type, CASE WHEN date = '2018-11-01' THEN CNT\_BY\_DATE ELSE NULL END AS "20181101\_DATA", CASE WHEN date = '2018-12-06' THEN CNT\_BY\_DATE ELSE NULL END AS "20181206\_DATA" FROM issueData;
It's the DC one. I have some stuff to take care of the morning so I probably won't make it there until close to noon. I was planning to spend all day there anyway so I'll definitely look into Matt's session, thanks!
I don't have any specific questions - your response covered everything and beyond of what I had in mind, thank you!
&gt;( SELECT event\\\_type, date, COUNT(event\\\_id) OVER(PARTITION BY date) AS "CNT\\\_BY\\\_DATE" FROM \\\[TEST\\\_REDDIT\\\].\\\[dbo\\\].\\\[SPRINGCLEANMYLIFE\\\] ) Thanks a lot. I don't think the portion I quoted above is doing what you intended: \`CNT\_BY\_DATE\` is the total number of ALL events on that date. So the subquery's result set looks like this: EVENT\_TYPE EVENT\_DATE CNT\_BY\_DATE sign\_in 2018-11-01 3 sign\_in 2018-11-01 3 check\_out 2018-11-01 3 cart\_abandon 2018-11-02 2 check\_out 2018-11-02 2 sign\_in 2018-11-03 2 sign\_in 2018-11-03 2 &amp;#x200B; In reality on 11-01 there were 3 *total* events, but only 2 \`sign\_in\` and 1 \`check\_out\`. &amp;#x200B;
No, you did not get it all thanks to Reddits garbage copy and paste. Let me copy and paste it in MediaFire. 
Apologies, I forgot to mention. I have pre - existing obligations and registration apparently lasts from 7:45 AM - 8:20 A according to the schedule guide. If I arrive late, do you know if I'll be barred from entering? 
 No, you did not get it all thanks to Reddits garbage copy and paste. Let me copy and paste it in MediaFire. Here you go: https://www.mediafire.com/file/m95g6cnti2lnho0/SPRINGCLEANMYLIFE.sql/file 
You are fine. I have been trawling the net for days and yours was the first that made it click. Thanks again.
Might be fun to have a weekly thread, like critique Tuesday, where all the top level comments are people's queries, then people can respond to them
You should be fine. “Registration” is really for folks who didn’t sign up online. Do that, print your SpeedPASS and cut it up at home. Then you just have to check in at the table when you first walk in and you’ll be set. 
Nope. You should still be fine. Usually they are held in public spaces like universities or libraries or convention centers. You can even enter presentations late, but do try to not disturb the presenters and be as non intrusive as possible. There will likely be someone working the registration booth almost all the way up to the end. It's generally easy to find volunteers or people who have attended more than once who can help guide you around if you're confused. One pro tip, print and pre cut all of the raffles and your name tag. 
Sounds good, thank you
Sounds good, thank you
ah, bummer :( &amp;#x200B; Idk. The \`event\_type\` is actually a bunch of error states. Error rates increased dramatically and i know there's one specific error type that's increased drastically over the past month, but I wanted to be able to show hard data to the team responsible for that type of error, like "from 11-1 to 11-25 these were the errors with the highest counts, while from 11-26 onward this new error shot up to the number one spot". &amp;#x200B; I know this to be the case, but i don't have actual numbers to show them. &amp;#x200B; I can certainly get this all manually by running a series of queries, but i was hoping to be able to just give them a query to see for themselves because they can be difficult and resistant to admitting problems. &amp;#x200B; &amp;#x200B;
Yep. :( This is the only thing that comes to mind, something that helps predict trends it error counts by the day. https://www.mediafire.com/file/u1hd0p34rjhytto/SPRINGCLEANMYLIFE.sql/file 
The fun part of testing everything is I have to remove all break lines and spaces. It's very picky. I'll test it out after work and let you know. Thank you for the help! 
Will do once I get home. I just pasted both of the separate scripts in my OP in each section as described. Was I only supposed to put a certain section of each? 
You'll love it. Finding a community of people who all have some focus on a subject that feels so foreign to most people but is enriching to you is one of the most welcoming experiences I've felt. I've help organize the last four in my area, and I've delivered a few sessions. Without over quoting others, SQL Saturday is a great representation of what is going right in the data community. I hoped you'll find yourself as heartened by the community as I do every time!
you need better sample data to explain what you want is every row going to get the same 'u1' value? what's the point of that?
Thank you taking the time to organize the events and I hope so too!
I love SQL Saturday’s! I love PASS in general. Attend as many as you can. I’m in the Denver area and try to go every year to the Denver and Colorado Springs events. Colorado is pretty causal and even the speakers wear shorts. So wear what you want. Many of the Saturday speakers also attend/organize monthly after-work meetings and networking lunches. They also have virtual meetings. When I started attending the monthly meetings several years ago as a newbie, 90% of what I heard was over my head. Now only about half is. :) Our monthly meetings generally have a 30 minute beginner presentation and an hour presentation by an expert. You go every month, make friends, learn, and they let you practice your presentation skills in a friendly place. The experts can really explain complex topics in easy to understand ways. Before every meeting they ask if anyone is hiring or looking for work or if anyone needs help with a problem. I can’t recommend the organization enough. Message me if you want more info. 
Damn I didn't even know these existed. The one in my area is having an event soon but registration is already closed :( Are there any other national database or data analytics related conferences that travel that I should know about?
You can’t group by salary and return all fields with *. When you group, any field not appearing in the group by clause has to be aggregated, which you haven’t done. The correct answer selects and groups by only salary, so it works fine. We then just snag the relevant fields where the salary is contained in this list of salaries that are repeated. 
I'm getting a strong Futurama vibe here and it pleases me greatly.
I think I get it. This problem actually came from the subquery section and I had forgotten that very important bit from the grouping section. Can I ask you another question? In the first block of code, what does COUNT(*) refer to? Is this referring to every group? So far in the course, I've seen * in the SELECT statement but not anywhere else. &gt;GROUP BY salary HAVING COUNT(*) &gt; 1 
Sorting is idempotent. Why would you sort twice?
The last line of a SQL contract or the last SQL line of a contract?
Esta "001" anyway? A Sting? Cause autoincrement is not por strings and i believe 001 integer = 1
Wear comfortable shoes, no matter what you wear. If in doubt, nice jeans or chinos and a polo shirt would be totally fine and smart enough. Take a big bottle of water, work out if there is secondary toilets because in between sessions the main ones will be packed. Take snacks, enough energy bars and fruit to last the day, there may be catering and it may not be great. Don’t forget, have fun and try and learn something new! Talk to speakers later on after their session if they are hanging around, everyone will be super friendly and chill 👍
That's what I was thinking I'd find but if the sub isn't very active there's not really a need.
Registration is closed? They should have a waitlist, sign up anyway and if a spot opens up you’ll be alerted. 
 SELECT customer.* , s1.last_customer_login , s2.last_customer_product_view , ... FROM customer LEFT OUTER JOIN ( SELECT customer_id , MAX(login_date) AS last_customer_login FROM customer_login GROUP BY customer_id ) AS s1 ON s1.customer_id = customer.customer_id LEFT OUTER JOIN ( SELECT customer_id , MAX(login_date) AS last_customer_product_view FROM customer_product_view GROUP BY customer_id ) AS s2 ON s2.customer_id = customer.customer_id LEFT OUTER JOIN ( SELECT ...
Have a look at one of the reliable way to manually fix [SQL Server Page Level corruption](http://www.databasefilerecovery.com/blog/sql-page-level-corruption.html).
This. Just was helping a cowoker yesterday writing SQL in C#, he was getting an error *cannot coerce type of date to int*, because he hadn't wrapped the date string in single quotes. And yes, *contains* is a linq thing, not a SQL thing. Just remove the contains entirely.
It’s just counting the number of rows in each group. The * counts nulls as well, where as count([some field]) would not. In this scenario it probably doesn’t make a difference. Another example to just count the total number or rows in a table would be Select count(*) from table 
That a shame, They have these in Europe, but not in the UK it seems. I suppose I had better get used to that sort of thing...
Thanks! I was thinking more something about: SELECT customer.* , s1.* , ... FROM customer LEFT JOIN ( SELECT customer_product_view.customer_id, customer_product_view.product_id FROM customer_product_view ORDER BY customer_product_view.created desc GROUP BY customer_product_view.customer_id ) AS s1 ON s1.customer_id = customer.customer_id ... the difference is that I return more columns instead just last date and this query will be super slow due order by
This is the way I've handled those situations in the past. Start with a primary key that someone has told you is wrong, and also get them to give you the correct information for it. Add that primary key to your WHERE clause in the inner-most subquery, as you don't want to be refereshing the whole dataset to troubleshoot a problem if you can help it. Then, work it from the inside out through the sub-queries. Execute each of the sub-queries until one just doesn't look like it's supposed to look. If you're still stuck at that point, I'd start to wonder whose numbers are off, the stored procedure, or the external person calculating them.
It's totally doable, I'm not sure what benefit you get out of it. The only way I could think to do this, would be create a table with the following field: ID \----- 001 Then, create a function that will convert VARCHAR to NUMERIC/NUMBER *(depending on what platform you're on)*. In that function, increment the number by 1, effectively making it 2. Convert the 2 back to a VARCHAR(). Then LPAD() OR REPLICATE() *(depending on what platform you're on)*, add the 0's back into the mix.
There are quite a few SQL Saturdays in the UK every year. You also have SQLBits and SQLRelay.
I'll look int those, thanks. I must say I wish I'd checked out this subreddit sooner. You guys are very supportive.
You should come over to /r/sqlserver and check out the community Slack (get an invite at https://dbatools.io/slack/)
Great advice. I like to break long queries into common table expressions for this specific purpose. Makes it easier for me to troubleshoot. Unfortunately, the DW I mostly use does not support them.
Well that sucks. 
The best bit is when you prove your calculations are correct and they have been manually calculating it wrong for years in the previous system. Then they ask you to "Break" your code to calculate it the same until the business is ready to "re-model" their targets. Unfortunately there is a good chance your calc is wrong somewhere, but its more amusing when you're right.
One of my strategies is to do what an old computer science professor told me: Solve it on paper first, and if that checks out, then apply those mechanics to the program and build it based on how you solved it on paper. If you can't solve a test case on paper, you can't expect a computer to do it. You need plenty of sanity checks along the way. So often when designing a complex query I will start on paper. Once I get the overall design figured out, then I will break it down into modules - divide and conquer method. Each small sub-section, which may be an in-line view, a temp table, a procedure, a CTE, etc, is validated and put through all known scenarios. Then I give it the signoff and it becomes a building block that I can depend on. Wash, rinse, and repeat. As you create more building blocks that solve sub-problems, you bundle them together as the final product. It makes it easy to unbundle and see what each section of code is doing. If it's all buttoned up into some massive statement without building diagnostic abilities into it, it's super hard to figure out where calculations are going wrong when you've effectively built a black mystery box. Also don't be afraid to totally scrap it and start over. Sometimes a different approach can open new doors for things you hadn't thought of before. I recently did that. It was so damn crafty... I didn't want to let go of it. But performance-wise it was not optimal, and I identified some scenarios that probably wouldn't happen but *could* happen, and so I scrapped it, wrote about 3x the amount of code and now it's ugly, but the performance is great and it seems to be rock-solid. 
Or add a new col: measure_old, measure_correct and drop measure_old once everyone is ready. Then just for fun: CASE WHEN suser_sid = ‘manager demanding incorrect numbers’ THEN measure_old ELSE measure_new END AS measure, 
You may also want to refactor while going through each subquery. Make sure all columns and temp tables have descriptive names, and format things so your eyes don't glaze over the screen. You'll probably have to investigate this again or make some change in the future, so you don't want to be overwhelmed every time. 
Just checked, dbeaver is not free. The Enterprise Edition is about $150 per year. Much cheaper than toad, so I'm still going to check out the free trial. Just didn't want any commercial customers reading this to get the wrong impression. There is a free community edition for personal use.
&gt;Songs.ID IN ( SELECT Songs.ID FROM Songs , ( SELECT DISTINCT IDGenre FROM GenresSongs ) AS T1 WHERE Songs.ID IN ( SELECT Songs.ID FROM Songs INNER JOIN GenresSongs ON Songs.ID = GenresSongs.IDSong INNER JOIN Genres ON GenresSongs.IDGenre = Genres.IDGenre WHERE GenresSongs.IDGenre = T1.IDGenre AND Songs.ID IN ( SELECT Songs.ID FROM Songs , ( SELECT DISTINCT IDArtist FROM ArtistsSongs WHERE IfNull(PersonType, 1) = 1) AS T1 WHERE Songs.ID IN ( SELECT Songs.ID FROM Songs INNER JOIN ArtistsSongs ON Songs.ID = ArtistsSongs.IDSong AND IfNull(ArtistsSongs.PersonType, 1) = 1 INNER JOIN Artists ON ArtistsSongs.IDArtist = Artists.ID WHERE ArtistsSongs.IDArtist = T1.IDArtist ORDER BY Random() LIMIT &lt;Number|Caption:Number of files per Artist|Value:5|MinValue:0|MaxValue:100|ID:1&gt; )) ORDER BY Random() LIMIT &lt;Number|Caption:Number of files per Genre|Value:20|MinValue:0|MaxValue:100|ID:1&gt; )) AND &lt;Genre&gt; IN ('Blues', 'Rock') IT WORKED!!! Thank you so very much good sir. The developer got back to me and said he didn't have time to help (even if paid to,) so I really appreciate you helping me figure this damned thing out! &amp;#x200B; I had to remove all the break lines, and modify the min/max values of course, but it works fine, and FAST! Songs.ID IN (SELECT Songs.ID FROM Songs, (SELECT DISTINCT IDGenre FROM GenresSongs) AS T1 WHERE Songs.ID IN (SELECT Songs.ID FROM Songs INNER JOIN GenresSongs ON Songs.ID = GenresSongs.IDSong INNER JOIN Genres ON GenresSongs.IDGenre = Genres.IDGenre WHERE GenresSongs.IDGenre = T1.IDGenre AND Songs.ID IN (SELECT Songs.ID FROM Songs, (SELECT DISTINCT IDArtist FROM ArtistsSongs WHERE IfNull(PersonType, 1) = 1) AS T1 WHERE Songs.ID IN (SELECT Songs.ID FROM Songs INNER JOIN ArtistsSongs ON Songs.ID = ArtistsSongs.IDSong AND IfNull(ArtistsSongs.PersonType, 1) = 1 INNER JOIN Artists ON ArtistsSongs.IDArtist = Artists.ID WHERE ArtistsSongs.IDArtist = T1.IDArtist ORDER BY Random() LIMIT &lt;Number|Caption:Number of files per Artist|Value:5|MinValue:0|MaxValue:100|ID:1&gt;)) ORDER BY Random() LIMIT &lt;Number|Caption:Number of files per Genre|Value:500|MinValue:0|MaxValue:5000|ID:1&gt;)) AND &lt;Genre&gt; IN ('Blues', 'Rock') PLEASE PM me your PayPal or some other way I can send you some cash for your efforts :) 
Had to do this a few times over the last three years. We were rebuilding clusterfuck of linked Excel Spreadsheets, so that the same output could be generated through SSRS. After releasing the report, the business office tells us it looks great, but a few numbers are off. After a few days of investigation (drilling through 5-7 Excel spredsheets &amp; formulas), we finally see the problem... Someone in the business office didn't like the output of the formulas in one of the spreadsheets, so they just changed it to an '8'. Was awesome to walk into the office of the person who said my report was wrong and tell them that the report was an accurate representation of the data in the system. YOUR SPREADSHEET IS WRONG!!!! (and probably had been for months because these spreadsheets were just copied and renamed for the next month's data. Good Times!
Wonder if they were just testing something with the 8 and forgot to change it back. lol
[Since you aren't posting the code for us to help you have one of these instead](https://zippy.gfycat.com/ConstantMedicalAxisdeer.webm)
We thought of that. But it was obvious that someone was trying to get the numbers they wanted. It was after that discovery, that my boss posted [this comic](https://static.wixstatic.com/media/06aab9_3323c6dd35b04c9ebcef148e03586bb9~mv2_d_1650_1666_s_2.png/v1/fit/w_1650,h_1666,al_c,q_80/file.jpg) on my cube-wall.
Why not just use a normal auto-increment and then create a view that adds the 00 to values with LEN() = 1, and 0 to LEN() = 2, etc.
I use the free version and it does everything I need it to do. It does look like the enterprise edition has the query builder like you find in SSMS though. That is really nice to have admittedly. being able to drag the tables in and draw your joins. all of the following examples are from the free version, im sure there are many more options i havent even come across yet, but functionality in the free version is nice as it is. here is a view of my object explorer: https://imgur.com/fjGqU8d generate SQL statement: https://imgur.com/fdXCYJ9 catalog filters: oracle - https://imgur.com/3mprJzG teradata - https://imgur.com/diZmV1e good luck, hope this helps. 
Select Distinct
What is the problem with it? Is it running slow? Are you asking if it is clear?
&gt; Re: vendor booths specifically, **please** go talk to them, enter their raffles, check out their offerings. Without those vendors, SQL Saturday doesn't happen and they need to see a return on their dollars spent in sponsoring the event and sending people to staff the booth. Thank them for being there. &amp;#x200B; That's a really good point, I can't believe I forgot about that specifically. The vendors really do make it possible to have such a large gathering and it's good to support them and interact with them. If they don't see any kind of feedback from these events, there's no reason for them to help support it. 
I wish the Denver monthly meetups weren't so far but since the Boulder meetups seem to have disappeared, I may have to start making that trek once a month. 
`TOP` in a CTE, especially with a non-fixed number, is going to give the optimizer trouble. Take your CTE and turn it into a temp table. select top (@pageSize) E.Event_Id,count(O.OrdersId) as TotalOrdersd,MIN(es.start_date) into #mytable from Event E left join OrderTransaction O on O.Event_Id = E.Event_Id inner JOIN EventCategory EC on Ec.Category_id = E.Category_id inner JOIN eventShowings es on es.event_id = E.Event_Id inner JOIN Venue V on V.id = es.venueid WHERE @orig.STDistance(geography::Point(V.Latitude,V.Longitude,4326))&lt; @Meters AND E.Event_Id &gt; @pageNo AND E.Category_id = @categoryid group by E.Event_Id; select ce.EventId,E.Name,ce.start_date,E.ThumbnailPhoto,V.Address,E.tz,E.Category_id from #mytable ce inner join Event E on ce.EventId = E.Event_Id inner JOIN eventShowings es on es.event_id = E.Event_Id inner JOIN Venue V on V.id = es.venueid order by TotalOrdersd desc; This may not get you all the way there but it should help.
Like so? SELECT t.submitter_id, u.zd_user_id, DISTINCT(u.zd_email), t.ticket_id, t.rpm_account_id, t.platform_product, t.driver, ....... ....... .......
I solved the problem. The "001" was an INT value, this is what I did: I set the column to have a primary key, then I used this code. ALTER TABLE *table_name* CHANGE `column_name` `column_name` INT(3) ZEROFILL AUTO_INCREMENT 
What if you had two joins to the same table? Which one would your select statement select from?
What dbms/version? SQL Server has a LAG function that would work here, for example.
(•_•) ( •_•)&gt;⌐■-■ (⌐■_■) Woot! I'm just pleased it worked for you! As you said in the title, simple SQL. It's the beauty of the internet. I can't even imagine how many times a stack overflow answer has saved me days of my life for free 
&gt;By using table aliases, you can replace the fully qualified table name throughout the query with an alias name instead. An alias in this context is an alternative name or reference that you define within a T-SQL statement for use within that statement. SQL Server doesn't store the alias permanently. [Source: Kim Tripp on IT Pro Today](https://www.itprotoday.com/sql-server/aliases-t-sql)
SQL server, oracle, redshift have window functions like lead and lag that can compare one row to another.
Once stored as a table, data sets do not have a specific order, so there's no preceding value. If you have a field that links to the 'preceding' record then you can use it to join. If your platform supports lead/lag, then you can use these functions within a result set. I do also think that you should really clarify your ask - given your definition and your pseudo-code you won't get 72.25 in your output result set.
I would try &amp;#x200B; SELECT DISTINCT t.submitter\_id, u.zd\_user\_id, u.zd\_email, t.ticket\_id, t.rpm\_account\_id, t.platform\_product, t.driver, 
Because you are telling the system you want the table to be called C for the purpose of this search not Customer.
Another way of thinking about it is that you are referencing columns from record set and you aren't joining tables - you are joining record sets. The table name becomes the "default" name of the record set in the from or join clause. When you are writing "INNER JOIN customer c" what you're saying is "inner join to a record set C, which happens to have all the same records from the table customer"
Did it yell you how it didn't work? TRUNCATE is a DDL statement, while DELETE is DML, so if you have "delete from" but not "alter" permissions on the table, the truncate would fail but the delete would succeed.
That sample data represents a single user across multiple devices.
To append to the good info given from /u/YrPalBeefsquatch. Truncate de-allocates pages of space, it's a minimally logged function that performs quickly and cannot be executed against a table that has constraints. De-allocating the pages does not allow the database engine to do checks against those constraints, so it will error out. Delete is a fully logged function which can work with constraints and is significantly more taxing on resources. Something else to note is that TRUNCATE will reset the SEED IDENTITY of the IDENTITY column, you need to use the seed functions in conjunction with DELETE to achieve that same behavior. It is seen as best practice to use TRUNCATE over DELETE due to performance, but business needs should be taken into consideration. It's also note worth that you can use a WHERE clause with your DELETE, you cannot do this with TRUNCATE. It's all or nothing. 
Elaborating on this, although you may run into a problem with TRUNCATE, it's generally better to use it in stored procedures, etc. if you're going to be executing the code frequently. Oracle Example, however applies to SQL Server as well with some verbiage changed: DELETE writes to the undo logs in preparation for the actual commit statement, in the event something wrong happens and the transaction needs to roll back. TRUNCATE does not write to the undo logs, truncate just wipes the table. Also, take into consideration maybe you \*want\* the transaction to write to the undo logs, even though it's more process intensive. Tl;dr: If you're performing a DELETE on a table that has billions of records versus a TRUNCATE, you'll experience a large performance improve for the TRUNCATE, but you're sacrificing your ability to ROLLBACK mid-transaction if an error is to occur.
You can rollback truncate in SQL Server. BEGIN TRANSACTION TRUNCATE dbo.Table ROLLBACK TRANSACTION SELECT * FROM dbo.Table &amp;#x200B;
Learned something new today. Didn't think that was possible.
Only if your database is in full backup.
You can actually rollback a truncate in SQL Server in Simple mode, Bulk Mode, or Full mode.
There are also sneaky ways to see how many records were truncated. ;)
MySQL supports window functions. Take a look at those. They're exactly for cases like these. Last_value, with a window that orders by date.
Pretty handy.
I have seen many database folk transition to sales or director level positions. 
Seconding this, I've seen many take the next step of management or sales analytics.
These explain plans are exactly the same. I guess you are using different versions of sqldev. Don't pay attention to the XML part. Also, try to understand what an explain plan is, and how indexes work, or you won't gain any knowledge on why it 'speeds up your query' (and when it won't)
Sleep over it. Or take a break. Especially sleeping helps solve tough problems like nothing else.
This actually does make sense. Thanks.
&gt; They aren't single queries, they're chains of queries and stored procedures. There are multiple tables being built. &gt; &gt; I see. I am going to break those queries down to study them as part of my journey towards proficiency. Are there any resources you used to learn and get better? Thanks!
The most helpful thing for me was becoming semi-active in this sub and trying to answer other peoples questions / asking questions of my own. In the beginning I would read someones problem, try to recreate it, and then read the solution someone gave and try to understand it. Gradually I was able to start trying to solve their problems myself / posting answers and receiving feedback from other users who had better solutions, or who criticized certain things that I was doing which were unnecessary.
Your method of aliasing make it difficult to figure out from what tables your columns are being selected.
That's the complaint I've received in the past, but my response is that I always list the items being selected sequentially in order of the tables, so all the A.Fields, then all the B.Fields, etc. In my defense it makes more sense (to me) to give them these types of aliases so that I can think of them algebraically. Most of the code I write is for the purpose of analytics, modeling, or forecasting. Most often my `A` alias is a sub-query that calculates something, and it joins to `B` which is another sub-query that calculates something. And then `A` and `B` are wrapped in an outer-query that is given the alias `X` which then might join to `Y`, where Y has an `A1` and a `B1`. In this example `A` and `A1` are similar, and `B` and `B1` are similar. Some of these types of queries/calculations can be very long in terms of code, so I tend to use aliases as though they are mathematical expressions such as, `f(x) = n`
YES ! Identity columns are reset. No real recovery from logs.
I left SQL to be a dev. The upward tract for most people in IT depends on your age. If youre young, transitioning to development/data science/etc. etc. is probably easier. Most people within their 40+ start looking at management or director level stuff.
a single user? i see four of them -- User_ID 1, 2, 3, 4
Yep, that's the problem - one true user has multiple accounts across multiple devices, how do we efficiently map all those accounts back to the true user?
Where did you learn your advanced SQL schools? I hope to be near your level one day. Thanks!
I do that too, or sometimes temp tables (Temp tables significantly outperform CTE's in certain scenarios)
It said that a foreign key constraint failed, even though the table with the foreign keys had already been truncates. 
Pretty sure TRUNCATE won't trip any triggers. No idea if that's relevant here
Do your tables have foreign keys? Under certain circumstances truncate will refuse to clear your tables if you have foreign keys in order to preserve data integrity, whereas delete will simply remove those records.
The truncate refused because of a foreign key constraint, even though I had previously wiped the table the used said foreign key. 
Yea, I thought as much. Doesn’t matter if the table is wiped, as long as there’s a fk it won’t truncate. You need to either drop and recreate the foreign keys or delete and reseed your tables.
&gt; Also in SQL Server, you cannot TRUNCATE a temp table because there are no physical pages attributed to it. You mean you cannot TRUNCATE a table variable. Also, another thing is that TRUNCATE does not trip triggers. And, TRUNCATE requires ALTER TABLE permissions due to many of the aforementioned reasons.
Why not mix in some hands-on experience and go and play around (after you get the basic concepts from the videos) in SQLFiddle, or something like that. 
I hadn't heard of SQLFiddle. I'll give it a try!
It's pretty cool. 
nice. looks kind of confusing. I'll have to get used to it. 
This has worked for me EVERYTIME. You get so bogged down you can't think straight, the break is beyond necessary.
I do this to myself all the time. Way I fix it is, I resign and look for a new job. Or, I pick a single incorrect row and follow it through from start to end. Almost always, fixing the issue for a single row is the fix for the whole query.
Do you're trying to figure out the best ERD for the already existing data source that was only one table?
i see okay, alter the table to add the new column, then update the table assigning appropriate values to the new column based on the other two columns pretty much a manual operation, i guess... 
My primary difference, SQL server user here. It's that if ya gone and effed up you can being back a truncated table, delete is basically gone without calling people you don't want to have to call.
My hope is to find a query to make the update, as the data set is quite large. 
You only have one record come back when you have many vendor contacts with many vendor ratings?
I have 2 contacts. They both show up if I take out the AVG column.
&gt; You mean you cannot TRUNCATE a table variable. Good catch, I did mean that! &gt;Also, another thing is that TRUNCATE does not trip triggers. &gt; &gt;And, TRUNCATE requires ALTER TABLE permissions due to many of the aforementioned reasons. The other two statements are also true and are also things that should be kept in mind when deciding between DELETE and TRUNCATE. &amp;#x200B;
Regardless of if both tables are empty, you cannot truncate a table that has constraints. (The constraints were created with the foreign key relationship.)
What are you running it on? Without a GROUP BY, MSSQL isn't liking the AVG() functions.
So I should group by vendor_contacts? It seems to be trying to group by vendor as it is. It is MySQL 5.7.21.
That worked. Thanks bro
Nice. I'm not sure what version of MySQL you're on, but you may also want to investigate AVG() OVER(PARTITION BY...). This negates the purposes for the group by as the analytical aggregate will handle the grouping for you in the partition by. (Just move the GROUP BY field(s) to the PARTITION BY...
I edited the post to include the actual table details instead of the abbreviated ones. The reason I'm trying to look for a solution is because the table is about 38,000 records and it seems silly to repeat so many data fields, when it could relate to a single ID on the table we already have in place. 
Thanks
Yes, it seems *very* denormalized. Just glancing at it, this is along the lines of how I might try to normalize it some. Just my thoughts. https://www.mediafire.com/file/0wa9x9q4c2og7c1/BRHENDER.sql/file This will run in MSSQL, I don't know what you're running.
unfortunately, there isnt a school that will take you this far. Which is why I started this series. &amp;#x200B; I am starting from scratch, been working on the first video to explain what the whole system will look like. Ill have the coding videos, but they should be way shorter. Ill have an animation video with almost the same name that will explain more of what is happening, and why we need to do the things described in the coding version.
I'm going to interpret this as you want those values that can be evenly divided by 0.25. Here's the mysql version... SELECT val FROM table WHERE MOD(val*100/25) = 0
Try pgexercices .com it lets you try and solve a wide range of queries raising in difficulty (postgres)
&gt; pushing terabytes of blob data to us from various sources Random semi-relevant question... do you have any thoughts on "Protocol Buffers" vs JSON for this kind of mass storage? Ever used (or considered) them? I'm wondering if it might make sense to convert all my JSON to it for the schema features, and also more efficient permanent storage as a bonus.
This is amazing. I like to joke that “I’ll give you whatever numbers you need ... wink wink” and it’s one of about 10 reasons why [How to Lie With Statistics](https://www.goodreads.com/book/show/51291) is in a visible spot in my office shelf. 
so how would you decide which User_ID values should all be covered by a single Universal_ID?
sqlzoo.net is the best, by a long way. Won't take you to stored procs, DBA stuff etc but will get you good at SQL and make the rest easily Googled. 
Can you paste your query as text so we dont have to type it out all over again
 select SUBSTR(PARCEL_RECEIVED, 4, 10) AS "MONTH/YEAR", COUNT(PARCEL_ID) AS RECEIVED, (select count(PARCEL_ID) from parcel where PARCEL_STATUS like 'CLAIMED') as CLAIMED, (select count(PARCEL_ID) from parcel where PARCEL_STATUS like 'UNCLAIMED') as UNCLAIMED from parcel group by SUBSTR(PARCEL_RECEIVED, 4, 10); select SUBSTR(PARCEL_RECEIVED, 4, 10) AS "MONTH/YEAR", count(PARCEL_ID) as CLAIMED from parcel where PARCEL_STATUS like 'CLAIMED' group by SUBSTR(PARCEL_RECEIVED, 4, 10); select SUBSTR(PARCEL_RECEIVED, 4, 10) AS "MONTH/YEAR", count(PARCEL_ID) as UNCLAIMED from parcel where PARCEL_STATUS like 'UNCLAIMED' group by SUBSTR(PARCEL_RECEIVED, 4, 10); 
Try this select SUBSTR(PARCEL_RECEIVED, 4, 10) AS "MONTH/YEAR", COUNT(PARCEL_ID) AS RECEIVED, SUM(CASE WHEN PARCEL_STATUS = 'CLAIMED' THEN 1 ELSE 0 END) as CLAIMED, SUM(CASE WHEN PARCEL_STATUS = 'UNCLAIMED' THEN 1 ELSE 0 END) as UNCLAIMED, from parcel group by SUBSTR(PARCEL_RECEIVED, 4, 10); 
Oh, thank you soo much.
you have an error in your syntax hint: google **leading comma convention**
while you were posting the actual text (thanks), i was busy trying to upload the solution as a jpg so here you go -- SELECT SUBSTR(PARCEL_RECEIVED,4,10) AS "MONTH/YEAR" /* really? substring for a date part? */ , COUNT(*) AS RECEIVED , COUNT(CASE WHEN PARCEL_STATUS = 'CLAIMED' THEN 'humpty' ELSE NULL END) AS CLAIMED , COUNT(CASE WHEN PARCEL_STATUS = 'UNCLAIMED' THEN 'dumpty' ELSE NULL END) AS UNCLAIMED FROM parcel GROUP BY SUBSTR(PARCEL_RECEIVED,4,10) 
Yeah i've got it, it's a common mistake of mine too.
yeah im too lazy to set the date format, but thanks.
^^^i ^^^need ^^^to ^^^learn ^^^how ^^^to ^^^use ^^^case
you have, last semester. you just never used it after that and forget about it, and you ask people on the internet for easy solution.
^^oh, ^^sorry.. ^^me
Yeah, we don't have those either. It's a home-grown solution we are forced to use. I do pull some data into SQL Server and do that. 
I generally try to avoid loops and cursors when writing SQL. Many times this can be accomplished by using a tally table, or table of numbers. If you don't have a physical table you can use a CTE like I do here, but it's less efficient. DECLARE @SeedDate DATE = '2019-01-01'; WITH Number AS ( SELECT ROW_NUMBER() OVER (ORDER BY (SELECT NULL)) - 1 AS n FROM (VALUES(0),(0),(0),(0),(0),(0),(0),(0),(0),(0)) a(n) CROSS JOIN (VALUES(0),(0),(0),(0),(0),(0),(0),(0),(0),(0)) b(n) CROSS JOIN (VALUES(0),(0),(0),(0),(0),(0),(0),(0),(0),(0)) c(n) ) SELECT DATEADD(dd,n.n,@SeedDate) AS GoalDay ,FORMAT(mrg.RevenueGoal*ROW_NUMBER() OVER (PARTITION BY DATEPART(mm,DATEADD(dd,n.n,@SeedDate)) ORDER BY n.n) /COUNT(*) OVER (PARTITION BY DATEPART(mm,DATEADD(dd,n.n,@SeedDate))),'C','en-us') AS [DaysGoal] FROM Number n JOIN MonthRevenueGoals mrg ON DATEPART(mm,DATEADD(dd,n.n,@SeedDate)) = mrg.[Month] WHERE n &lt;= 365 AND DATEPART(yy,DATEADD(dd,n,@SeedDate)) = DATEPART(yy,@SeedDate) AND DATENAME(weekday,DATEADD(dd,n,@SeedDate)) NOT IN ('Sunday','Saturday') AND DATEADD(dd,n,@SeedDate) NOT IN (SELECT Holiday FROM HolidayDates) My output doesn't have the dates that don't count as business days in it, which I thought was cleaner, but it could be easily adjusted to keep them in if desired.
There are probably multiple products that have a date_offered greater than the employee start date.
Could you just post the whole query?
Interesting. Honestly, I'm less familiar with that particular approach (tally tables), but I like it and see a lot of uses for that. I may use that more often now. I generally also avoid cusors and loops as much as possible, but I've gotten into the habit when I need to join on a date range to just make a little loop to insert a bunch of days into a table to work with in memory. I find I'm not doing more that 1000 loops generally, so it's rarely a performance problem for me. But yeah, just tested your solution and that's quite a bit cleaner. Going to study a bit more and see how else I can use that.
Yup. As the key(s) from your product table to the select to test. 
Would you recommend to have a table of numbers in a database? The more I look at this, the more uses I see to just have a table of numbers. Do you have this in your DBs? And if so, how many numbers do you keep in it?
The query as you wrote it would work fine, so I’m guessing something else you have added isn’t being properly grouped and thus isn’t working. Post the whole code and we can help further. 
Yes, I generally have a physical table in each of my databases and I size it to meet my needs. In this case I might use a `smallint` and just load 1000 rows. Be sure to create a clustered index on `n` for maximum efficiency. 
I realized that keeping the non-business days in the output was easier said than done! Here's a different approach that uses running totals rather than ratios and keeps the non-business days in the output... DECLARE @SeedDate DATE = '2019-01-01'; WITH Number AS ( SELECT ROW_NUMBER() OVER (ORDER BY (SELECT NULL)) - 1 AS n FROM (VALUES(0),(0),(0),(0),(0),(0),(0),(0),(0),(0)) a(n) CROSS JOIN (VALUES(0),(0),(0),(0),(0),(0),(0),(0),(0),(0)) b(n) CROSS JOIN (VALUES(0),(0),(0),(0),(0),(0),(0),(0),(0),(0)) c(n) ) ,Dates AS ( SELECT DATEADD(dd,n,@SeedDate) AS [Day] FROM Number WHERE n &lt;= 365 AND DATEPART(yy,DATEADD(dd,n,@SeedDate)) = DATEPART(yy,@SeedDate) ) ,GoalDays AS ( SELECT d.[Day] ,mrg.RevenueGoal/COUNT(*) OVER (PARTITION BY DATEPART(mm,d.[Day])) AS DailyGoal FROM Dates d JOIN MonthRevenueGoals mrg ON DATEPART(mm,d.[Day]) = mrg.[Month] WHERE DATEPART(yy,d.[Day]) = DATEPART(yy,@SeedDate) AND DATENAME(weekday,d.[Day]) NOT IN ('Sunday','Saturday') AND d.[Day] NOT IN (SELECT Holiday FROM HolidayDates) ) SELECT d.[Day] ,FORMAT(ISNULL(SUM(DailyGoal) OVER (PARTITION BY DATEPART(mm,d.[Day]) ORDER BY d.[Day]),0),'C','en-us') AS Goal FROM Dates d LEFT JOIN GoalDays gd ON d.[Day] = gd.[Day] ORDER BY d.[Day]
Do a select * instead - the data causing the duplication is likely not one of the columns you're pulling in.
Amazing. I just did exactly that, and re-ran your latest query, and literally made a little sound when it ran in under 100ms. Jezuz, I'm definitely doing this in my databases at work on Monday.
Hey I have another quick question for you if you don't mind. I didn't notice it right away, and it's not that big of a deal, but for some reason when I run the query it returns mostly rock tracks and only a few from the blues genre. I assume that is because I have far more rock than blues music. However, the script is \*supposed\* to select 500 tracks from each genre, so that shouldn't matter in theory. I'm just curious as I'm trying to learn. It's no big deal if you don't know why. For now I'm just creating separate "masks" as they are called and only doing 1 genre at a time. Thanks again!!! &amp;#x200B;
Are you looking to do this on MSSQL? 
I mean the script looks ok on casual inspection. So I would do some tests with really small numbers in the limits for the genre. Like 1 then 2 then 5 and 10 etc working your way up. If the results look right at those small numbers where you should be able to tell what's happening but result still seem wrong at big numbers then if you are really keen you could do some stats to see if it's actually wrong or just seems wrong. And if it's actually wrong it may be a data problem. Maybe there aren't 200 songs labelled as Blues in the database when you only have 5 from each artist. 
yes, but I would like to port it into mysql server in the future.
You can use Select idItem FROM items WHERE nameitem where nameItem LIKE '%hammer%' and colorItem LIKE '%red%' &amp;#x200B; etc... 
For now, as you're trying to get it into MSSQL, try researching the BCP (Bulk Copy Protocol) command. You can use command line in SQL Server to reach out to the network share and pull in the files. 
If you have Office 365 , look into Microsoft Flow. Very neat automation engine
That looks good, is there a proper way to move data from one table to the other. And make sure that it stays associated with the correct data. Something like this &amp;#x200B; INSERT INTO ASSIGNED_APPLICATIONS SELECT dbo.APPLICATION_ASSETS.Application_ID, dbo.Physical_ASSETS.Physical_Asset_ID FROM APPLICATION_ASSETS JOIN PHYSICAL_ASSETS on dbo.PHYSICAL_ASSETS.Service_Tag=dbo.APPLICATION_ASSETS.Service_Tag Assuming that APPLICATION\_ASSETS is the table I have posted above as reportedData
I'd like to map all the instances of user_IDs that share device IDs together. 
So, I seriously love this solution. I played with it quite a bit and pulled it all apart so I have a pretty good understanding of how is all works. I tweaked a little even to suite my needs, and created a nice little (2D) cube that has MTD and YTD goals in it. I also expanded the base structure of the insert dates to go based on a year, and made it more flexible to easily use additional years' goals and holidays, and is forgiving to leap years. Seriously, thank you so much for showing me all of this. BEGIN TRAN IF OBJECT_ID('[dbo].[Numbers]') IS NOT NULL DROP TABLE [dbo].[Numbers] CREATE TABLE [dbo].[Numbers] (n SMALLINT NOT NULL) GO CREATE CLUSTERED INDEX IX_Numbers_n ON dbo.Numbers (n); GO INSERT INTO dbo.Numbers SELECT ROW_NUMBER() OVER (ORDER BY (SELECT NULL)) - 1 AS n FROM (VALUES(0),(0),(0),(0),(0),(0),(0),(0),(0),(0)) a(n) CROSS JOIN (VALUES(0),(0),(0),(0),(0),(0),(0),(0),(0),(0)) b(n) CROSS JOIN (VALUES(0),(0),(0),(0),(0),(0),(0),(0),(0),(0)) c(n) GO IF OBJECT_ID('[dbo].[HolidayDates]') IS NOT NULL DROP TABLE [dbo].[HolidayDates] CREATE TABLE [dbo].[HolidayDates] ([Holiday] DATE NOT NULL) INSERT INTO HolidayDates VALUES ('2019-01-01') ,('2019-05-27') ,('2019-07-04') ,('2019-09-02') ,('2019-11-28') ,('2019-11-29') ,('2019-12-24') ,('2019-12-25') ,('2019-12-31') IF OBJECT_ID('[dbo].[MonthRevenueGoals]') IS NOT NULL DROP TABLE [dbo].[MonthRevenueGoals] CREATE TABLE [dbo].[MonthRevenueGoals] ( [Month] INT NOT NULL ,[Year] INT NOT NULL ,[RevenueGoal] DECIMAL(12,2) NOT NULL ) INSERT INTO MonthRevenueGoals VALUES (1, 2019, 1000000.00) ,(2, 2019, 1100000.00) ,(3, 2019, 1200000.00) ,(4, 2019, 1300000.00) ,(5, 2019, 1400000.00) ,(6, 2019, 1500000.00) ,(7, 2019, 1600000.00) ,(8, 2019, 1700000.00) ,(9, 2019, 1800000.00) ,(10, 2019, 1900000.00) ,(11, 2019, 2000000.00) ,(12, 2019, 2100000.00) /********************************** ******* ******* ******* QUERY AREA ******* ******* ******* **********************************/ DECLARE @YearToInsert INT = 2019 DECLARE @SeedDate DATE = CONCAT(CAST(@YearToInsert AS VARCHAR(4)),'-01-01'); WITH Dates AS ( SELECT DATEADD(dd,n,@SeedDate) AS [Day] ,DATEPART(mm,DATEADD(dd,n,@SeedDate)) AS [Month] ,CASE WHEN DATENAME(weekday,DATEADD(dd,n,@SeedDate)) NOT IN ('Sunday','Saturday') AND DATEADD(dd,n,@SeedDate) NOT IN (SELECT Holiday FROM HolidayDates) THEN 1 END AS IsBusinessDay FROM Numbers WHERE n &lt;= 366 AND DATEPART(yy,DATEADD(dd,n,@SeedDate)) = @YearToInsert ) ,GoalDays AS ( SELECT d.[Day] ,d.[Month] ,IsBusinessDay * mrg.RevenueGoal/COUNT(IsBusinessDay) OVER (PARTITION BY d.[Month]) AS DailyGoal ,IsBusinessDay FROM Dates d JOIN MonthRevenueGoals mrg ON d.[Month] = mrg.[Month] AND mrg.[Year] = @YearToInsert ) SELECT [Day] ,ISNULL(SUM(IsBusinessDay) OVER (PARTITION BY [Month] ORDER BY [Day]),0) AS [BusinessDayMTD] ,FORMAT(ISNULL(SUM(DailyGoal) OVER (PARTITION BY [Month] ORDER BY [Day]),0),'C','en-us') AS [GoalMTD] ,ISNULL(SUM(IsBusinessDay) OVER (ORDER BY [Day]),0) AS [BusinessDayYTD] ,FORMAT(ISNULL(SUM(DailyGoal) OVER (ORDER BY [Day]),0),'C','en-us') AS [GoalYTD] FROM GoalDays ORDER BY [Day] ROLLBACK TRAN
That's great! I'm glad to have helped!
Consider these two tables. Employee: employee start_date e1 9 e2 10 e3 11 e4 21 Product: product date_offered p1 10 p2 10 p3 10 p4 20 p5 30 Now what is the result of this query? select e.employee , e.start_date , p.date_offered from employee e inner join products p on e.start_date &gt; p.date_offered 
As all these tables contain different attributes, I don't see why you'd need to move the records from one table to another. You might need to update one or more tables when a new application is assigned to a user, user information is update, a new physical asset is deployed, a location needs to be created or updated, etc, but you shouldn't actually need to move data from \[APPLICATION\_ASSETS\]/\[PHYSICAL\_ASSETS\] into \[ASSIGNED\_APPLICATIONS\], you'd just need to insert the foreign keys. Also, the join on \[APPLICATION\_ASSETS\] to \[PHYSICAL\_ASSETS\] ON \[Service\_Tag\] = \[Service\_Tag\] looks wrong as \[APPLICATION\_ASSETS\] does not have a service tag field. Let me explain my thought process on this ERD: * \[PHYSICAL\_ASSET\_TYPES\]: This contains the type of physical asset, like an HP Model A Windows 10 Service Pack 1. * I put this in a separate table as there's a good chance the same physical asset type is duplicated across multiple users. * \[APPLICATION\_ASSETS\]: This contains the actual application that may be deployed to individuals. * I put this in a separate table as there's a good chance the same application is deployed to multiple users. * \[LOCATION\]: This contains the location information. * I put this in a separate table as there's a good chance there are many users to one location. * \[USER\]: This contains user information. * I put this in a separate table as it's better practice to have this user information stored elsewhere, while the user just becomes an INT. * \[PHYSICAL\_ASSETS\]: This contains a listing of all the physical assets that are assigned to individuals. * As all the tables above have been transitioned from the main table to their own tables, now we're only referencing foreign keys in this table, with the exception of the \[SERVICE\_TAG\]. The physical service tag will be different for each physical machine, usually. * \[ASSIGNED\_APPLICATIONS\]: This contains a listing of all the applications assigned to a \[PHYSICAL\_ASSET\_ID\]. This uses two foreign keys to track this information, \[PHYSICAL\_ASSET\_ID\] and \[APPLICATION\_ASSET\_ID\].
Mine! But I taught myself it, then moved to incorporating it with VBA, then with python, now I’ve designed a Django web application to integrate with our SQL Server Database
I'd say so. I started out working in a completely different field, learned SQL, and realized it was \*definitely\* my strongpoint.
user_ids 1 and 2 share device a, so they might be universal_id 1 but user_ids 2 and 3 share device b, so they might be universal_id 2 so user_id 2 gets both universal_id 1 and 2 you see the problem?
Since you already know how to do it in Linux. Try turning on Windows subsystem for Linux and doing it that way https://docs.microsoft.com/en-us/windows/wsl/faq
Mine was. I was a business analyst to start out of college, but I was pretty good at SQL so they asked me to apply for a database developer position that was open after about 2 years. Now I'm on the technical side and I am so much happier and have been able to learn and be exposed to so much more than if I would have stayed a BA.
We actually are but it's on the drawing board still. I received a diagram for using Microsoft's Edge and IoT bus and I was like "Awesome, &lt;thumbs up&gt;. Can't wait to work on &lt;product of my division on this platform." and our other development department said ,"Oh no... that is just for &lt;new application&gt;. You will operate on SQLServer as you've done in the past." Yeah no thanks. They want to create this messaging bus and basically copy data directly into our applications reporting tables. I told them "K thx bye, I guess I should get another job then because I sure as shit aint supporting it." I've also come to find out that development team has no clue what they are doing on the systems side and this diagram is basically copied from a recommendation anyways so I have no idea if it's actually going to come to fruition.
More cash I assume?
Task Scheduler could help, it is build-in windows application. Use Task Scheduler to trigger a VBS script, and that VBS script could trigger the Excel file and the module inside
How?
I mean, I have a career because of SQL if that counts.
these are the steps: 1. Open "Task Scheduler" 2. Create a text file and write VBA code inside, which has the path of the Excel Workbook and use [Object.Op`en`](https://Object.Open) method 1. Afterward, you would need another method to trigger the module inside but sadly I forgot the exact name... It was the code written by a colleague but it was lost 2. use `Object.close` method to complete the opeartion 3. When you are done, remember to change the extension of the file name to .vbs instead of .txt 3. Add the WorkbookConnection.Refresh to the VBA module inside your workbook ([Here](https://docs.microsoft.com/en-us/office/vba/api/excel.workbookconnection.refresh) is the doc FYI) &amp;#x200B; &amp;#x200B;
Got my internship because I learnt SQL. Got into industry project because I was comfortable with it. Finally landed a Data Cleaning job because I was experienced with SQL from the above roles. My job uses Rmysql package to run queries into the database. I also use Data.table in R to query data. Sql understanding is required to be able to use Data.table effectively.
How comfortable are you with python? Could use the os module with xlrd, and pyodbc
you have one cuolumn at the left side of your IN operator, you should have one column at the right side. if you want to carry multiple columns from the inner query to the outer SELECT, a JOIN is usually an easier option.
&gt;you have one column at the left side of your IN operator, you should have one column at the right side. That was a very clear explanation. Thank you. 
Are you allowed to have multiple columns on the left side of IN? I tried the following but I got a syntax error: SELECT student_name, count(student_no) FROM students WHERE student_no, count(student_no) IN (SELECT student_no, count(student_no) FROM student_enrollment GROUP BY student_no ORDER BY count(student_no) desc LIMIT 1 )
Yes. Started in tech support, learned a little SQL there for troubleshooting. Next job was a mix of support and building procs for crystal reports. Current job years later is a mix of BI and Analysis. 
some databases allow using multiple columns in IN, postgres is one of them, afaik. Syntactically, you can do it by putting parentheses around those columns on the left side of your IN-operator. So your specific syntax-error will be gone if you write: &gt;SELECT student\_name, count(student\_no) FROM students WHERE (student\_no, count(student\_no)) IN (SELECT student\_no, count(student\_no) FROM student\_enrollment GROUP BY student\_no ORDER BY count(student\_no) desc LIMIT 1 ) However there is a more fundamental problem: COUNT(student\_no) doesn't make any sense in the WHERE clause of your outer query. What would COUNT(student\_no) mean at that point in that query?
&gt; SELECT student_name, count(student_no) FROM students WHERE (student_no, count(student_no)) IN &gt; (SELECT student_no, count(student_no) FROM student_enrollment GROUP BY student_no ORDER BY count(student_no) desc LIMIT 1 &gt; ) It will not count the number of times the student_no appears in the student_enrollment table? 
Your outer query is a query in the students table. COUNT(student\_no) doesn't have much meaning in it's WHERE clause. If you solve the inner query, you are left with: `SELECT student_name, count(student_no)` `FROM students` `WHERE (student_no, count(student_no)) IN (5, 4)` If you read it like that, do you feel it makes sense?
https://sqlzoo.net
 WHERE (student_no, count(student_no)) It seems like it makes sense because student_no is referencing 5, so why isn't count(student_no) referencing 4. 
It is not referencing anything, it's a condition. We check for records in the student table for which the value of student\_no in is 5 and for which the value of count(student\_no) is 4. The problem is that count(student\_no) doesn't have much of a meaning within the context of the table students at that moment.
I started as an intern in HR but got hired full time as a process analyst after my grad degree. I worked with a sharepoint designer at the time, but her knowledge was limited in programming outside of InfoPath+SP. I started working on some projects to support process improvement and picked up SQL through access (like so many others). Fast forward a few years and I am now a senior data anlayst/developer running a new MS SQL Server and web server, architecting data structures for end user self service dashboards and reports via tableau, and have automated jobs (task schedulers, SSIS packages) to migrate, massage, and distribute information to many areas of the company I work for. SQL was a game changer for me- it opened the door to a lot of new projects and learning opportunities. I knew JavaScript going into the role, but I know so much more now. 
&gt;We check for records in the **student table** for which the value of student_no is 5 and for which the value of count(student_no) is 4. Only the first part makes sense. We have no way to meet the condition that count(student_no) = 4 since that information is not present in the students table. 
Sql is a good skill to have. 
exactly
What do you think was fundamentally wrong with my previous understanding? Was it that I did not understand the difference between a reference and a condition? 
Yes, you could put it like that. IN is an operator used to write WHERE-conditions, rather than a device that passes items from one query to the other. Now as I said in the beginning, if you want to get multiple bits of information from your inner part into the end result, a subquery in a where-condition is not the best way to do that. If you want the name of the student and the course count, a JOIN would probb be the most straightforward way to go.
Thank you! I'm going to wait until we cover JOIN in class and then come back to this. 
Thanks for this. I write a bunch of functions in R to get data out and push results back in of various databases for work. I’m trying to brush up on my python by doing some things in Python and getting the library names will probably reduce the barriers enough for me to just dive in. Is pyodbc the most mainstream odbc package in python?
Similar story here. I pivoted to becoming a DBA because of the SQL Server community.
There are quite a few options for odbc. I use this one because I feel it is flexible, and I found there documentation, when I needed it, easy to follow. 
Things are definitely much faster because I can write queries. Past people in my position used to have to wait for a dba or reporter to get datasets to them. I just write the queries myself and interface with the db through R. I can very quickly get info and answer questions, set up models. 
Im not very clear about what yoy're trying to achieve here, but if you want to limit the count(student_no) = 4 you might want to consider using a HAVING clausd &lt;...&gt; HAVING count(student_no) = 4 GROUP BY (your groupping column) I'm not at my pc at the moment, if that gives you a syntax error, try moving the HAVING after group by... some RDBMS like that way
I've never seen a database that allows multiple columns to the left of IN, but I can't guarantee that none exist. That would work more like an exists statement though. Anyway, I didn't read the whole thread, but I think the only way to do what you're trying to do, and this is the way I would do it, is to do an inner join to your subquery on student_no. It will have the same effect as IN and allow you to select the column containing the count.
Python is good for anything automated
If it means working myself into a job that no one else can do on my team? Then yes, it has transformed my career into being the "go to guy" for all things data related. I'm still amazed at how many people/teams still just accept "copy paste in Excel" as a way of life.
Did... Wait. What?
Leading commas for life!
I'm interested in knowing this as well. Besides DBA jobs which are apparently harder to get these days, it seems like the vague job title of "Data Analyst" seems to often specify the ability to write SQL queries, among having basic Excel skills and applied stats knowledge. 
Sure, can I just funnel some of my work to you? I need it by Monday morning and id like to not have to do it Sunday night....
Yes you should. I always think i should attend the Colorado Springs meetings on Wednesday and then the Denver meeting on Thursday, but i can't do the drive. 
Not just python you know
My journey sounds pretty similar to yours. Any chance you could share some of the tools and things you used to make your web app in Django and how it connects to sql server? I'm looking to possibly do something similar in the future. 
Nice one! That looks perfect, thank you 
Dm me :) and I will when I wake up!
When you need to join to perform your calculation. Something like this maybe, in SQL Server: CREATE VIEW vDiscountedProducts WITH SCHEMABINDING AS SELECT P.Id , P.DefaultPrice * (1.0 - PD.DiscountPercent) AS CurrentPrice FROM Product P JOIN ProductDiscount PD ON PD.ProductId = P.Id AND PD.IsCurrent = 1 GO CREATE UNIQUE CLUSTERED INDEX uxc ON vDiscountedProducts (Id); GO CREATE INDEX ix_CurrentPrice ON vDiscountedProducts (CurrentPrice); GO SELECT * FROM vDiscountedProducts WITH (NOEXPAND) WHERE CurrentPrice &gt; ... 
Landed a 1st tier part time IT support job to help me work through college. The job was downsized right around the time I finished school (non-IT degree). Landed a full time IT support job at a small tech business. Learned SQL as a useful skill to help with support and ran with it. 6 years later, I am now the DBA for the same small business and work 100% remote from overseas.
Yeah, but python is exceptionally good because it's concise, simple, fast and versatile
Fast?
Why would you go for MySQL?
Portability reasons
Just use some scripting languages and schedule them using Jenkins
Care to elaborate? Mssql has a Linux version. It's just that MySQL is the worst widely used rdbms today, I get if you just have to use it because you have no choice, but to give up something good for it doesn't make any sense
You have to elaborate why mysql is the worst widely used db, not me :-)
Python today goes hand in hand with data. It has a ton of data processing libraries written in C, maintained by the community, optimized for speed and ease. The only thing that will be slow in that implementation, is just the 50ms startup time. The rest will be as fast as possible.
Thanks
I asked to elaborate on what portability reasons. The second point, that it's the worst, is easy to show: there are hundreds of reasons. From not having full outer joins, being the last to implement window functions, having the most ancient support for procedural language, having flimsy partitioning support, a crappy fragmented documentation and behavior that behaves differently from the documentation (UInt is not UInt for example), implicit type conversions with data loss, being slow, not being SQL in fact, etc. It is a ball of features added in a stupid manner, late in the game. Like a Frankenstein, but with the head of a pig and the eyes of a fly. Just choose something else https://grimoire.ca/mysql/choose-something-else
Thanks
I moved from a help desk role to a SQL role in a different country... ¯\_(ツ)_/¯
You dropped this \ *** ^^&amp;#32;To&amp;#32;prevent&amp;#32;anymore&amp;#32;lost&amp;#32;limbs&amp;#32;throughout&amp;#32;Reddit,&amp;#32;correctly&amp;#32;escape&amp;#32;the&amp;#32;arms&amp;#32;and&amp;#32;shoulders&amp;#32;by&amp;#32;typing&amp;#32;the&amp;#32;shrug&amp;#32;as&amp;#32;`¯\\\_(ツ)_/¯`&amp;#32;or&amp;#32;`¯\\\_(ツ)\_/¯` [^^Click&amp;#32;here&amp;#32;to&amp;#32;see&amp;#32;why&amp;#32;this&amp;#32;is&amp;#32;necessary](https://np.reddit.com/r/OutOfTheLoop/comments/3fbrg3/is_there_a_reason_why_the_arm_is_always_missing/ctn5gbf/)
Good bot
mine has
Hope you leveraged that into a higher salary.
Yes noticed that in my market as well. I think Data Analysts are expected to know how to pull from databases now to avoid having a huge DBA team. Also knowing statistical software and techniques. 
Thanks a lot! &amp;#x200B;
You need an API that the Android can access and send recurve data to. The API can be php or can be python or node.js or anything else for that matter as long as it has access to the database and you can set it to listen on a specific port for a get or post event.
What version of SQL Server are you running? 
If you're on MSSQL 2017 or Azure, then I believe the query below is what you're looking for. SELECT b.\[Band\_ID\] ,b.\[Band\_name\] ,STRING\_AGG(g.Genre\_name) WITHIN GROUP(ORDER BY g.\[Genre\_name\] ASC) OVER(PARTITION BY b.\[Band\_ID\]) AS "Genre\_name" \--,g.\[Genre\_name\] ,stg.\[Stage\_name\] ,stg.\[Stage\_tent\] ,sch.\[Time\_start\] ,sch.\[Time\_end\] FROM \[Schedule\] sch INNER JOIN \[Band\] b ON sch.\[Band\_ID\] = b.\[Band\_ID\] INNER JOIN \[Band\_Genre\] bg ON b.\[Band\_ID\] = bg.\[Band\_ID\] INNER JOIN \[Genre\] g ON bg.\[Genre\_ID\] = g.\[Genre\_ID\] INNER JOIN \[Stage\] stg ON stg.\[Stage\_ID\] = sch.\[Stage\_ID\];;
What error do you get?
 DECLARE @Nombre NVARCHAR(MAX); DECLARE curso CURSOR FAST\_FORWARD FOR Select Object\_name(object\_id) AS Nombre from sys.objects where type = 'U' &amp;#x200B; OPEN curso FETCH NEXT FROM curso INTO @Nombre &amp;#x200B; WHILE (@@FETCH\_STATUS &lt;&gt; -1) BEGIN IF (@@FETCH\_STATUS &lt;&gt; -2) BEGIN DECLARE @statement NVARCHAR(200); SET @statement= concat('DELETE FROM ' + @Nombre + ';'); prepare stmt from @statement; execute @statement; END FETCH NEXT FROM curso INTO @Nombre END CLOSE curso DEALLOCATE curso &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B; error: [https://gyazo.com/1b4fa5100534876fa92258a5de1e8e66](https://gyazo.com/1b4fa5100534876fa92258a5de1e8e66)
You're trying to do this on MySQL?
Im trying to create a file.sql with this code inside to be used in any database, so yeah, Mysql
I don't know how to write it in MySQL, but you could give this a shot and see if it gets you closer. --SWITCH DATABASES USE [TEST_REDDIT] GO --DECLARE VARIABLES DECLARE @tableName NVARCHAR(MAX); DECLARE @sqlExec NVARCHAR(MAX); --SET VARIABLES DECLARE cur_TableName CURSOR FAST_FORWARD FOR SELECT OBJECT_NAME(OBJECT_ID) AS "TABLE_NAME" FROM sys.objects WHERE TYPE = 'U'; --OPEN CURSOR OPEN cur_TableName --FETCH NEXT RECORD IN THE CURSOR FETCH NEXT FROM cur_TableName INTO @tableName; --LOOP WHIEL THE FETCH STATUS = WHILE(@@FETCH_STATUS = 0) --BEGIN WHILE LOOP BEGIN --SET THE SQL SCRIPT TO THE DELETE STATEMENT NEEDED SET @sqlExec = 'DELETE FROM ['+CAST(@tableName AS VARCHAR(MAX))+'];'; --PRINT THE DELETE STATEMENT PRINT 'Executing script: '+@sqlExec; --EXECUTE THE DELETE STATEMENT EXEC sp_executesql @sqlExec; --FETCH NEXT FROM THE LOOP FETCH NEXT FROM cur_TableName INTO @tableName; --END WHILE LOOP END; --CLOSE CURSOR CLOSE cur_TableName; --DEALLOCATE CURSOR DEALLOCATE cur_TableName;
Thanks a lot for taking the time!
I'm just curious, did it run on MySQL?
Thank you!!
This is one of those things where SQL and the theory diverge. In set theory, there is no such thing as a meaningful duplicate. In SQL, a set can quite happily have duplicates. 
Did it work?
You really need to add genre as part of the primary key to allow multiple rows per band instead of concatinating it.
No, it didnt, here is a version that works (took me a while :p): DROP procedure IF EXISTS borrador; DELIMITER // create procedure borrador() &amp;#x200B; BEGIN DECLARE tablas VARCHAR(255); DECLARE checker INT default 0 ; DECLARE borrar CURSOR for select table\_name from information\_schema.columns where table\_schema=database() group by table\_name ; DECLARE CONTINUE HANDLER FOR NOT FOUND SET checker = 1; OPEN borrar; repeat fetch borrar into tablas; set @pru1 = concat("delete from ",tablas,";"); PREPARE stmt FROM @pru1; execute stmt; until checker = 1 END REPEAT; close borrar; END; // DELIMITER ; SET SQL\_SAFE\_UPDATES = 0; call borrador(); SET SQL\_SAFE\_UPDATES = 1; &amp;#x200B; &amp;#x200B;
Got it working! here is the full script: &amp;#x200B; DROP procedure IF EXISTS borrador; DELIMITER // create procedure borrador() BEGIN DECLARE tablas VARCHAR(255); DECLARE checker INT default 0 ; DECLARE borrar CURSOR for select table\_name from information\_schema.columns where table\_schema=database() group by table\_name ; DECLARE CONTINUE HANDLER FOR NOT FOUND SET checker = 1; OPEN borrar; repeat fetch borrar into tablas; set @pru1 = concat("delete from ",tablas,";"); PREPARE stmt FROM @pru1; execute stmt; until checker = 1 END REPEAT; close borrar; END; // DELIMITER ; SET SQL\_SAFE\_UPDATES = 0; call borrador(); SET SQL\_SAFE\_UPDATES = 1;
Interesting. 
I wouldn't advise turning off FK constraints. Essentially, you're just telling the database to ignore your mistakes and do it anyways. If it errors on an FK constraints, then there's an issue with the parent record. (Probably doesn't exist.) Do you know which FK you're violating? 
It wont let me disable the FK anyways but I had read on google that it might be a fix. It doesnt say which FK I'm violating which is why I'm so lost. I ran all those commands and they all worked. The 2 rows that did work in scholarship_awards came up. 
Could you post the results of them, just the scholarship ID and/or the student ID?
Students ID First_name Last_name "1" "Stephen" "Bowers" "2" "Michael" "Eppenberger" "3" "Kirkland" "French" "4" "Jonathan" "Majors" "5" "Julian" "Moll" "6" "Alexander" "Valov" "7" "ChengLong" "Ruan" "8" "Meagan" "Schreiber" "9" "Lina" "Shapovalova" scholarships scholarship_id / scholarship_name / Amount_available "1" "Presidents Award" "$15,000" "2" "Chancellor Academic Award" "$25,000" "3" "Chevron Award" "$20,000" these are the results from running those two . 
You don't need an api. Android apps can talk directly to databases, even remote ones
I believe in offline-first, so I'd recommend a local saline database and a library to sync to remote mysql Such as http://docs.sqlite-sync.com
Well, just taking a guess at where the foreign keys would be, everything for the insert looks okay so far. Still, I can't say what you might be violating without knowing the foreign key. Run this and it'll tell you the foreign keys in the table: USE \[enteryourdatabasenamehere\] GO EXEC sp\_fkeys 'scholarship\_awards'
Yes is was also a nice raise.
&gt;PRAGMA foreign\_key\_list(scholarship\_awards); &amp;#x200B;
It came back as: FK Scholarship\_applications from student ID to null
What fields are in SCHOLARSHIP\_APPLICATIONS?
would it help if i sent you the guide they have me following? there is no sensitive info as it is a project. Im guessisng i have a PK or FK labeled wrong from what you have been having me test. It just confuses me because ive done basically the same thing with a different database. &amp;#x200B; The guide has the tables the way they want them set up and the rules defining the database so it may clear things up seeing it all.
ID, Student_ID, Scholarship_ID, Essay_Response
Probably not, but my best guess is that something is up with that FK. I'm not a SQLite expert, I'm more on the side of MSSQL and Oracle, and I think the foreign key shouldn't say FROM student\_id TO null. (It's almost like the database doesn't know what the other side of the foreign key is, so everything is a reject? Just my best guess, again.) Could you try dropping SCHOLARSHIP\_AWARDS and then recreating it with the same foreign keys? (I'm guessing only that one foreign key on student\_id = student\_id.)
Is this what you're looking for? SELECT a.\*, SUM(a.Duration) OVER(PARTITION BY a.ContactStartInterval) AS "SUM\_DURATION" FROM ( select ID, ContactStartTime , DATEADD(minute, (DATEDIFF( minute, 0, ContactStartTime) / 15) \* 15, 0) AS ContactStartInterval , Duration &amp;#x200B; FROM cteTestData
This is the best I can think of to \*kinda, sorta\* do what you're talking about. I don't think it's exactly what you want either. &amp;#x200B;
Setup another CTE table and breakout the 15 minute durations with start time, end time and ID. Might seem a little heavy at first but will ultimately save you bashing your head against the keyboard writing insane nested datediff's. On mobile so sorry for formatting... DECLARE @DURATION TABLE (START DATETIME , END DATETIME , ID INT IDENTITY(1,1)) INSERT INTO @DURATION(START,END) VALUES('2018-12-09 00:00:00.000','2018-12-09 00:14:50.999') ...And so on.
And then what? I am not sure i follow where you are going with this.
This is what the expected output will look like. Interval CallsAnswered TalkTime 7:00 4 1407 7:15 5 2720 7:30 1 533 7:45 0 900 8:00 0 900 8:15 0 145
I was (still am) a licensed attorney and I was hired by a legal department at an insurance company right out of law school. The hours were long, the work was boring, and working with other attorneys generally sucks. After spending a year or two learning the business I started working with the analytics team, got access to the tables and ~~worked my ass off 2+ hours every night learning sql~~ just happened to be good at it. Now, 2 years later, I'm in a BI role making more money, working less hours, and doing work that love doing. 
1. Generate 15-min intervals in a separate table/subquery/cte. 2. For every 15-min interval get all overlapping contact records (that's a join) 3. If a contact (from 3) starts within your interval, count it as 'contacts received' 4. Figure out the overlap length per contact (if the contact starts prior to the 15 min interval and ends after- it is whole 15 min, if it starts prior to the start of the interval but ends before the interval ends it is &lt;end of contact&gt; - &lt;start of the interval&gt;, etc.) 5... 6. Profit.
Which BI tool do you use?
https://imgur.com/a/bcDY2XS
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/ZZDyCuI.jpg** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20ebgzszo) 
Did you create it?
For those not on 2017, STUFF accomplishes something similar and you can specify a delimiter as well.
this is a screenshot of how the tables and columns are set up. i redid the scholarship award table and it is doing the same thing. i hate using sqlite but they require us to so all the students use the same editor i guess. I prefer MSSQL as well. Based on the picture i sent above, would you mind telling me how you would label the PK and FK for the tables? Im beginning to think i have them wrong and that is the issue. If its not then im stumped
I wouldn't leave a RDBMS engine open publicly to the entire Internet so that a mobile app could directly connect to it from anywhere. That's bad security practice.
In SCHOLARSHIP\_APPLICATIONS PK: ID FKs: Student\_ID (referencing student table), Scholarship\_ID (referencing scholarships table) In SCHOLARSHIP\_AWARDS PK: ID FKs: Scholarship\_ID (referencing scholarships table), Student\_ID (referencing students table) In SCHOLARSHIPS: PK: Scholarship\_ID FKs: None In STUDENTS PK: ID FKs: None
yes. i redid the entire table with everything the same and it did the exact same thing. I also went through each table and checked PK and FK and everything seems to line up in my head but i could very easily be wrong.
Ok so i guess i had tables referencing the wrong things and it was throwing up that error because it couldnt find where it was coming from . it loaded the data right in when i changed it to what you had. thanks so much!!!!
Sometimes when you're dev'ing new tables, it's easier to just blow them away and make sure you get the constraints right in the first place. But, did you get all the constraints added per the requirement? 
Couldn’t get it to work but that might be my lack of SQL knowledge
All STUFF does is replace a given number of characters starting from a given position with a string. You're thinking of the common FOR XML kludge, which uses STUFF to remove the leading delimiter.
Yeah I was trying to avoid totally starting over but I was about to that point. Yes I think the constraints were the issue because thats all I had to change other than one out of place PK. Again, thanks so much. I would have been banging my head on the wall all night. 
That's correct. Forgot the FOR XML bit.
So you can think of WHERE as a filter - since you don't have any filtering criteria in your requirements, you don't actually need it. What you DO need, however, is an aggregate function - you want to SUM up the total raised, right? That goes in your select statement. Some minor syntax errors with the group by - it should read: GROUP BY committee_name, election_cycle AND is only used in the WHERE statement, and in joins.
No worries. 
[Elo](https://en.wikipedia.org/wiki/Elo_rating_system) As for normalization, look at columns that are repeating to use as your primary/foreign keys. Does it make sense to have 'team_id' vs. 'opp_id'? Each team is playing against an opponent. I'd try to have that better defined (to avoid the _iscopy situation) where team1 is the home v/ away team, or just do team1/team2. Franchise/Team&gt;Game, with PK/FK's between them. The ELO seems team-specific to a season, but win_equiv/forecast are game-specific **Franchise/Team** fran_id/opp_fran [PK] team_id/opp_id (fran_id vs. team_id seems like nomenclature. Is there another field to track at the team level vs. specifc season?) league_id elo_i/opp_elo_i elo_n/opp_elo_n **Game** gameorder game_id [PK] league_id (FK against the Franchise?) year_id date_game seasongame isplayoffs team1: team_id pts team2: opp_id opp_pts game_location game_result win_equivalent forecast notes
SELECT first_name, department, salary, (SELECT avg(salary) FROM employees as e2 WHERE e1.department = e2.department) as dept_avg FROM employees as e1 WHERE salary &gt; (SELECT avg(salary) FROM employees as e2 WHERE e1.department = e2.department); Mobile, so haven't tested. But this would be my first attempt.
It's hard to say without seeing more of the data or knowing more about the NBA, but I'd for sure split off the teams/franchises into a separate table. The "elo" columns likely refer to the [Elo rating system](https://en.wikipedia.org/wiki/Elo_rating_system).
**Elo rating system** The Elo rating system is a method for calculating the relative skill levels of players in zero-sum games such as chess. It is named after its creator Arpad Elo, a Hungarian-American physics professor. The Elo system was originally invented as an improved chess rating system over the previously used Harkness system, but is also used as a rating system for multiplayer competition in a number of video games, association football, American football, basketball, Major League Baseball, table tennis, Scrabble, board games such as Diplomacy (game) and other games. The difference in the ratings between two players serves as a predictor of the outcome of a match. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/SQL/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
The Elo rating system is a method for calculating the relative skill levels of players in zero-sum games such as chess. It is named after its creator Arpad Elo, a Hungarian-American physics professor.
That worked! I don't quite understand why my attempts didn't work. 1: SELECT first_name, department, salary, (SELECT avg(salary) FROM employees WHERE e1.department = e2.department) as avg_salary_dept FROM employees as e1 WHERE salary &gt; (SELECT avg(salary) FROM employees as e2 WHERE e1.department = e2.department) 
I looked more carefully at your code. Was it really just not stating the e2 and avg_dept aliases? 
You didn't alias e2 in the subquery in your SELECT
I ran the code without the original correlated subquery and it still works. In the code below, have we simply moved the correlated subquery to the outer SELECT clause? SELECT employee_id, first_name, department, salary, (SELECT round(avg(salary)) FROM employees as e2 WHERE e1.department = e2.department) as avg_salary_dept FROM employees as e1
That seems like a not very good parameter if it has to be 8000 characters. Why is it so long?
If you don't mind, about how big of a difference was it?
actually it's a list of user name till 8000+
Pass it as a object, or put the list in a table and reference it from the SP?
I don't mean to sound unhelpful but have you first gone through some basic articles on normalization and "how to" blog posts? Is there something especially tricky about your problem that you're not able to figure out after reading the existing literature?
Put it in a temp table 
compute the same aggregate twice? No thanks
Postgresql has many types of JOIN statements. Notably, there is the CROSS JOIN, which basically makes a Cartesian product of rows from both sides. It acts as a "FROM t1, t2" too. In fact let me use that in the example. Which is ok if you only have 1 row on one side. ``` SELECT first_name, department, salary, avg_salary FROM employees as e1, ( SELECT avg(salary) as avg_salary FROM employees ) e2 WHERE salary &gt; avg_salary ```
Just wrap your existing statement in another IIF, something like: =iif(Fields!CATEGORY.Value = "","Blue",iif(Fields!feesOfficeCurr.Value &lt;= 0, IIF(Fields!CATEGORY.Value=left(MonthName(Month(now)), 3) ,"Red","Black"), "Black")) I'm not sure if the CATEGORY is exact field name you use, can't tell from the screenshot.
This worked PERFECTLY! - Thanks so much. I feel like an idiot sometimes, but its only because I haven't actually bothered to learn this regular expression stuff. I really need to! - I was totally over complicating this! Thanks again.
&gt; Which is ok if you only have 1 row on one side. huge props for mentioning this frequently underappreciated property of CROSS JOIN minus a few points for using unqualified column names (drives me up the wall when debugging someone else's queries) here, do it this way -- SELECT e1.first_name , e1.department , e1.salary , e2.avg_salary FROM employees AS e1 CROSS JOIN ( SELECT avg(salary) AS avg_salary FROM employees ) AS e2 WHERE e1.salary &gt; e2.avg_salary
You really want to start a holy way about comma positions? :)
 SELECT e1.first_name , e1.department , e1.salary , e2.avg_salary FROM employees AS e1 INNER JOIN ( SELECT department , AVG(salary) AS avg_salary FROM employees GROUP BY department ) AS e2 ON e2.department = e1.department AND e2.avg_salary &lt; e1.salary
no, about unqualified column names -- read my reply again, Mister Easily Offended
The database might have a different date format. If you are inserting into a date datetime field, try setting the dateformat to dry if that's how their dateformat is set. If it's a varchar, cast it as a datetime before insertion.
 convert(char(10), [yourDate], 126) What this will do is force your date to a string in yyyy-mm-dd hh:mm:ss.m format. Since you have char(10) you are forcing upon it, the string gets cut off and will just leaves you with yyyy-mm-dd. If you are needing to do math with the date, just convert back at any time. You could also use the format() syntax: format([yourValue], ‘yyyy-mm-dd’) That should give you the same result. I tend to use convert over the format syntax in these cases. If I am wrong with either example please critique.
I agree with you on the service tag will be unique and giving it its own table would be over normalization. In our case the user doesn't change, one machine has one assigned user. How do I go about inserting the foreign keys and making sure they relate to the correct data? I think that's the question I was asking the whole time but I didn't know how to ask it.
An alternative is to pivot on genre, aggregate by count and either leave as is or compute something like "if count &gt; 1 then Y else N"
You misunderstood my comment. Oh well...
Just to clarify your understanding, if it is a DATETIME data type then it’s not stored as any particular format. The format is how the client is rendering it based on conversion rules. 
Well if you do go learn "regular expressions" you'll be in for a BIG surprise because what you're showing is a logical expression.
**bows head in shame**
If you want to build something like this from scratch, sometimes a switch statement can be easier to understand.
If you need the date as a string and in a specific format, handle the conversion yourself. What database are you using? Is the column a date column or a date string?
Always *explicitly* convert values that you need to be represented in a certain format (dates and times typically). Don't rely on defaults.
It was about a 15% increase when I was offered the job, but I feel like I have also got better yearly raises due to increased growth opportunities in development.
 SELECT committee_name, election_cycle, sum(amount_raised) FROM tables.contributions_donors GROUP BY committee_name, election_cycle
if it's MSSQL/T-SQL then I already answered a very similar question a couple of days ago. You need to say how the date will be formed. set language english sets the date format to YYYY-MM-DD HH:MM:SS
you started it ;o)
alternative.me is a good resource for finding alternative software products. Here's what they list for QuickBooks https://alternative.me/quickbooks 
interesting, thanks
In general you might be worth setting up a date table in your db, we have one that we can just join in that contains date, day of week dayname,dayofmonth and so on, If it's an isolated use case, you could use datename(dw, Target date) in an if statement to set the input date accordingly, Declare @inputdate date; If datename(dw,@inputdate) =Monday then Set @inputdate = @inputdate -1 day End if Select stuff from tables where date between @inputdate-7 and @inputdate I haven't tested this but I think the approach should work if you sort out the syntax I'm too lazy to remember right now! 
Just off the top of my head, backup the database and restore it then cross database insert. Oh, and do the insert in chunks, not all in one big transaction :)
True, but I think he means the server is interpreting the supplied value as a YDM value, but sending it from client it gets inserted as YMD. Presumably because the server is using a different locale
Have you just considered doing a backup and restore? We don't know what your bottleneck is with the inserts (network pipe? disk? query problems?) so it's hard to give some useful advice here.
Back probably the quickest , if that not possible SQL can handle multiple transactions easily so spin up lots of insert queries with a where filter splitting the data etc or the export data function is pretty good. Tips: dropping constraints , indexes , FK's , PK's will speed up the process a lot just remember to put them back.
A data flow that is a single select with a balanced data distributor with 7 parallel inserts seems like a good idea. Don’t forget to up the packet size to 32768 to minimize network acknowledgement overhead. Up all buffers to their max values and it should be much faster.
After much thought and tinkering of what you said and what others have said, I got it to work! Thanks!!! 
Detach the database files and move the files with robocopy.
Not really, I understand the basics of normalization, I have my own normalized table, I was just wondering if I did it right.
Thank you!
The answer lies in the order in which a query is executed. See [this article](https://sqlbolt.com/lesson/select_queries_order_of_execution) for details. The short answer is the WHERE clause is processed before the GROUP BY, and the HAVING clause looks at the aggregated rows produced by the GROUP BY. Therefore, the rows that have been filtered out by the WHERE clause will not be included in the comparisons made by the HAVING clause
I work as a senior analyst at a large consulting firm, so I do not have a lot of "DBA knowledge". I can tell you though that disk space is not an issue, nor is any query problem. Our team's DBA has tried backing up and restoring a single table to the new server, but he said that process took 2.5 days (I do not have permissions to do this myself). I have heard about using this bcp utility - are you by any chance familiar with this?
I'm not really clear on what you're asking? All of these items sound like they are attributes of an individual flight, and each flight would have only one of each, so they would belong in a single flights table. Month/day of Month/day of week/departure time is not something you would normally store in separate fields. You would store the planned departure as a datetime, the actual departure as a datetime, and calculate the month/day/day of week as you needed it when querying the table. Carrier would be the PK of the carrier table. In a flight table it would be a foreign key. If you stored both the planned and actual departure and arrival time, you wouldn't need the departure/arrival delay. You'd still need the weather delay. Not sure if this answers your question?
[How to Get First and Last Day of a Week in SQL Server](http://zarez.net/?p=2543) 
What is the error you're getting? INSERT INTO INSTRUCTORXXX(InstructorID, LastName, FirstName, DateOfHire) VALUES ('INS001', 'Smith', 'Alex', date'2017-03-21'), ('INS001', 'Nagy', 'Matt', date'2018-01-01'); 
it does help. I was confused because I thought the data values were too much for one table. So you're saying I need two tables. Flight and Carrier. I am suppose to determine the number of flights that were delayed for each carrier. Do I do SELECT \* From Carrier Delayed 
If that is all you need you don't need a carrier table. The query would look something like SELECT carrier, count (*) FROM flights WHERE delay &gt; 0 GROUP BY carrier
This is the error I'm getting: Error at Command Line : 10 Column : 42 Error report - SQL Error: ORA-00933: SQL command not properly ended 00933. 00000 - "SQL command not properly ended" I'm not quite sure what you're saying about the dates, but I think the date format should be fine because I used 'date' as my data type for the DateOfHire column which by default is DD-MON-YYYY, if i'm not mistaken 
Thanks :3 life saver 
Have a look at the data loading performance guide for various methods of loading large amounts of data: https://docs.microsoft.com/en-us/previous-versions/sql/sql-server-2008/dd425070(v=sql.100)
&gt; "SQL command not properly ended" Usually you get that when you have an unmatched quote, parenthesis or something like that. 
Worked for me after I removed the DEFAULT for the DateOfHire column mysql&gt; create database test; Query OK, 1 row affected (0.01 sec) mysql&gt; use test; Database changed mysql&gt; CREATE TABLE InstructorXXX( -&gt; InstructorID varchar(10) NOT NULL, -&gt; LastName varchar(50) PRIMARY KEY, -&gt; FirstName varchar(40), -&gt; DateOfHire date DEFAULT '16-NOV-2018' -&gt; ); ERROR 1067 (42000): Invalid default value for 'DateOfHire' mysql&gt; CREATE TABLE InstructorXXX( InstructorID varchar(10) NOT NULL, LastName varchar(50) PRIMARY KEY, FirstName varchar(40), DateOfHire date DEFAULT date'16-NOV-2018' ); ERROR 1525 (HY000): Incorrect DATE value: '16-NOV-2018' mysql&gt; CREATE TABLE InstructorXXX( InstructorID varchar(10) NOT NULL, LastName varchar(50) PRIMARY KEY, FirstName varchar(40), DateOfHire date ); Query OK, 0 rows affected (0.02 sec) mysql&gt; INSERT INTO INSTRUCTORXXX(InstructorID, LastName, FirstName, DateOfHire) -&gt; VALUES -&gt; ('INS001', 'Smith', 'Alex', date'2017-03-21'), -&gt; ('INS001', 'Nagy', 'Matt', date'2018-01-01'); Query OK, 2 rows affected (0.00 sec) Records: 2 Duplicates: 0 Warnings: 0 mysql&gt; select * from INSTRUCTORXXX; +--------------+----------+-----------+------------+ | InstructorID | LastName | FirstName | DateOfHire | +--------------+----------+-----------+------------+ | INS001 | Nagy | Matt | 2018-01-01 | | INS001 | Smith | Alex | 2017-03-21 | +--------------+----------+-----------+------------+ 2 rows in set (0.00 sec)
For the life of me I can't find any simple mistake like that, I even copy pasted your exact code above and it's giving the same error. 
Thank you for taking the time! The only problem is the assignment requires a DEFAULT constraint somewhere on the table
[Works for me.](https://i.imgur.com/ZJOp0ce.jpg)
the answer to your question is that THEN and ELSE can be followed only by a value, not a comparison try this -- WHERE CASE WHEN DATENAME(dw,getdate())='Monday' AND ld.CallDate BETWEEN GETDATE() - 8 AND GETDATE() THEN TRUE WHEN DATENAME(dw,getdate()) &lt;&gt; 'Monday' AND ld.CDate BETWEEN DATEADD(wk, DATEDIFF(wk,0,GETDATE()), 0) AND GETDATE() THEN TRUE ELSE FALSE END altho i think `GETDATE() - 8` might be problematic
&gt; In a high level view, is it that the rows that are being filtered or excluded by the WHERE impact the aggregate data that HAVING uses? that's it, i think you got it! 
Sorry...didn't see you were using Oracle &gt;.&lt; I don't believe you can insert more than 1 row at a time in Oracle. [oracle@localhost][orcl18c][~]$ rlwrap sqlplus matt/ SQL*Plus: Release 18.0.0.0.0 - Production on Mon Dec 10 15:01:23 2018 Version 18.3.0.0.0 Copyright (c) 1982, 2018, Oracle. All rights reserved. Last Successful login time: Mon Dec 10 2018 14:57:08 -05:00 Connected to: Oracle Database 18c Enterprise Edition Release 18.0.0.0.0 - Production Version 18.3.0.0.0 MATT@orcl18c&gt;create table t(id int); Table created. Elapsed: 00:00:00.14 MATT@orcl18c&gt;insert into t(id) values (1); 1 row created. Elapsed: 00:00:00.04 MATT@orcl18c&gt;insert into t(id) values (2),(3); insert into t(id) values (2),(3) * ERROR at line 1: ORA-00933: SQL command not properly ended Elapsed: 00:00:00.01 MATT@orcl18c&gt;insert into t(id) values (2); 1 row created. Elapsed: 00:00:00.00 MATT@orcl18c&gt;insert into t(id) values (3); 1 row created. Elapsed: 00:00:00.00 MATT@orcl18c&gt;select * from t; ID ---------- 1 2 3 Elapsed: 00:00:00.02
Sorry...didn't see you were using Oracle &gt;.&lt; I don't believe you can insert more than 1 row at a time in Oracle.
I guess the issue must be that I'm on Oracle SQL Developer which must be different from what you're using? I'm getting a syntax error after typing in your exact code
If this is oracle: INSERT ALL INTO INSTRUCTORXXX(InstructorID,LastName,FirstName,DateOfHire) VALUES ("INS001", "Smith", "Alex", "21-MAR-2017") INTO INSTRUCTORXXX(InstructorID,LastName,FirstName,DateOfHire) VALUES ("INS001", "Nagy", "Matt", "01-JAN-2018") SELECT 1 FROM DUAL;
That's the way to do it!
Thanks! This gives me peace in my mind.
Thanks for the confirmation! :-)
Thank you! This is definitely close to being correct but I received an error message: Error at Command Line : 15 Column : 101 Error report - SQL Error: ORA-00904: "01-JAN-2018": invalid identifier 00904. 00000 - "%s: invalid identifier" *Cause: *Action:
Sure thing. There are rules you can apply. And these are standardized rules. For example, data should not repeat across columns. You've mentioned it as well. It is hard to see if you're doing it without looking at the actual data. But it looks like you're mostly normalized.
If the tables are in a database that can't be down for the detach/copy, you could create a side db on the same server, copy the needed data to it, then do the detach/copy on the side db. 
That is not a valid date format. Use yyyy-mm-dd
I got it! Thank you so much for your help
So I do a nightly restore of a 1.5 TB database and it takes less than 2 hours. Our hardware is decent, but nothing special. I guess this is where the confusion is. Unless somehow your 1.5 billion record tables are holding like 100 TB each, then you've got a bottleneck *somewhere*, because it shouldn't take 2.5 days to restore a table with 1.5 B records. So again, it's hard to say "this will fix the problem" because no one seems to know what the problem is. :) I'm familiar with bcp, you can try it, it might work. But I'd really suggest figuring out what's taking the restore 2.5 days because that seems like a problem that should be solved no matter what.
no, that's not right in either case (no pun intended), both 1s and 0s are counted, so the count is everybody if you're going to use COUNT, use ELSE NULL (which you can lerave out as it's optional) COUNT(CASE WHEN SEX = '2' THEN "whoa" END) many people use 1s and 0s, though, but you have to use SUM, not COUNT SUM(CASE WHEN SEX = '2' THEN 1 ELSE 0 END) 
CASE expressions are not usually used in WHERE clauses. It's better practice to use Boolean expressions. select aid, cdate, cd, st, SID into #ap from ld where status like 'T%' and ( (DATENAME(dw,getdate())='Monday' and ld.CallDate between GETDATE() - 8 and GETDATE()) or (DATENAME(dw,getdate())&lt;&gt;'Monday' and ld.CDate between DATEADD(wk, DATEDIFF(wk,0,GETDATE()), 0) and GETDATE()) ); Also be aware that `DATENAME(dw,)` is specific to your server's culture (aka, language). However, so is `DATEPART(dw,)`. 
Depends of your system, i guess. Your query would rather be used with a sum() than a count(), as count just count the existence of data, not if it's 1 or 2 (unless you group on that field). &amp;#x200B; Simply use it that way: select count(*) from table where sex='2'; &amp;#x200B;
Thanks! The SUM (CASE WHEN SEX = '2' THEN 1 ELSE 0 END) worked perfectly.
Depends if you only want to learn SQL .. Something I'd suggest is to learn some python web scraping and crawling .. collect some data you're passionate about and create a database out of that. You can design your database you want but you'll also learn a variety of other things too.. you'll be more interested if you're working with data you're passionate about. Only a suggestion. 
I’m doing this right now with housing data with the goal of training a model to predict houses we might like as they go on the market. 
This is pure bullshit. Look at the code used to insert into MySQL: &gt; * loop: &gt; * create prepared statement &gt; * execute statement &gt; * close statement &gt; * repeat loop Like, seriously? Someone who wrote this had an agenda or is a terrible, terrible programmer. I'm betting on both, judging by the rest of that code. Prepared statements are meant to be reused, batched processing should happen with auto commit turned off and statements should be batched. On my machine, creating 50k records with all random values takes: 41 seconds using their method and 3.5 "as it should be done". Kotlin code: fun slow() { connection.autoCommit = true repeat (count) { val stmt = connection.prepareStatement(INSERT_QUERY) fillStmt(stmt, MeterReader.create()) stmt.execute() stmt.close() } } fun fast(batchSize: Int = 10000) { val stmt = connection.prepareStatement(INSERT_QUERY) connection.autoCommit = false repeat (count) { fillStmt(stmt, MeterReader.create()) stmt.addBatch() if (it % batchSize == 0) { stmt.executeBatch() } } stmt.executeBatch() connection.commit() stmt.close() } So that's it for the "ingest" part. Onto the "extract", the SQL part we go! Hey, look at my table: CREATE TABLE METER_READS ( meter_id BIGINT NOT NULL, timestamp DATETIME, -- bunch of DOUBLE coulumns error_code_1 BIGINT, error_code_2 BIGINT, FOREIGN KEY (meter_id) REFERENCES METERS(id) ); Have you noticed something? yay, no indexes, apart from the implicit FK one. Totally expecting great performance from that one! Let's look at the queries: SELECT timestamp, SUM(usage_since_read) AS reg_sum FROM METER_READS WHERE MONTH(timestamp) &gt;= ? AND MONTH(timestamp) &lt;= ? AND YEAR(timestamp) &gt;= ? AND YEAR(timestamp) &lt;= ? AND meter_id = ? And another one: SELECT timestamp, SUM(usage_since_read) AS reg_sum FROM METER_READS WHERE MONTH(timestamp) = ? AND YEAR(timestamp) = ? AND meter_id = ? Who needs to define the aggregation function on that timestamp column, right? Ok, that one's on MySQL. Anyway, apart from the table having no fucking index whatsoever, function in a `WHERE`? [Sargable](https://web.archive.org/web/20170222053424/https://en.wikipedia.org/wiki/Sargable)? Who even knows what that means, right? Because surely this query cannot be rewritten as `WHERE timestamp &gt;= '2018-05-01' AND timestamp &lt; '2018-06-01'` for example? God forbid it uses an index on some relevant columns, it could accidentally prove our product is not as great as we wanted to "prove".
It would help to see your query. Are you using a group by clause?
It's hard not being able to see some sample data and not knowing the keys, but here's my best guess. I'm assuming the PRIZE table has a PK called Id and it is a foreign key on the PRIZE\_AWARDS table. If Name is the key then you can just switch out the PrizeId with Name. `SELECT` `PRIZE.Name` `,Awards.AvgValue` `FROM` `PRIZE` `INNER JOIN (SELECT AVG(PRIZE_AWARDS.Value) AvgValue` `FROM PRIZE_AWARDS` `GROUP BY PRIZE_AWARDS.PRIZEId) Awards` `ON` `PRIZE.PrizeId` `= PRIZE_AWARDS.PrizeId`
Ok so i totally mixed up the tables. the ones that need help with are scholarship name and scholarship awards. i will try and post what the tables look like. Ive got the rows there but no values. ///////////// ID/ Scholarship_name / Amount_available "1" "1" "Presidents Award" "$15,000" "2" "2" "Chancellor Academic Award" "$25,000" "3" "3" "Chevron Award" "$20,000" ///////// ID / Scholarship_ID / Student_ID / Amount_awarded "1" "1" "2" "9" "$1,500" "2" "2" "3" "8" "$1,500" "3" "3" "1" "7" "$2,000" "4" "4" "2" "6" "$1,500" "5" "5" "2" "5" "$3,000" "6" "6" "3" "4" "$5,000" "7" "7" "3" "3" "$2,500" "8" "8" "2" "2" "$2,500" "9" "9" "1" "1" "$1,500" ID is a PK in both. Im trying to get the query to pull up the name and average that each name was awarded. And when trying to find the sum would i just replace average with sum?
i posted the tables above and yes when i use the group by clause it pulls up the names but leaves the value of the average as 0.0
Download any free data set and do some form of analysis. Either the NY cab rides or NOAA weather stations measurements. You can Google for them and many more. ClickHouse has kinks to some on their website with instructions on how to run a benchmark with them. But whatever you do, for the love of all that's sacred, don't do web scraping. It's in the legal gray zone, unethical, overdone, many companies do it better, etc. There's no glory in that, nor is it complicated enough to warrant a praise. I veto any CV that mentions web scraping when choosing a hire.
I also forgot to mention that they share no PK or FK . I feel like this may be my issue
Highlight date column. Ctrl+h. Replace " *" (space asterisk) with nothing.
&gt;///////////// &gt; &gt;ID/ Scholarship\_name / Amount\_available &gt; &gt;"1" "1" "Presidents Award" "$15,000" &gt; &gt;"2" "2" "Chancellor Academic Award" "$25,000" &gt; &gt;"3" "3" "Chevron Award" "$20,000" &gt; &gt;///////// &gt; &gt;ID / Scholarship\_ID / Student\_ID / Amount\_awarded &gt; &gt;"1" "1" "2" "9" "$1,500" &gt; &gt;"2" "2" "3" "8" "$1,500" &gt; &gt;"3" "3" "1" "7" "$2,000" &gt; &gt;"4" "4" "2" "6" "$1,500" &gt; &gt;"5" "5" "2" "5" "$3,000" &gt; &gt;"6" "6" "3" "4" "$5,000" &gt; &gt;"7" "7" "3" "3" "$2,500" &gt; &gt;"8" "8" "2" "2" "$2,500" &gt; &gt;"9" "9" "1" "1" "$1,500" &gt; &gt;ID is a PK in both. Im trying to get the query to pull up the name and average that each name was awarded. And when trying to find the sum would i just replace average with sum? &amp;#x200B;
Just use BCP. 1.5 billion rows in 3 days is beyond horrible. The average speed for transferring wide (150 columns, with text) data rows as text (CSV) between servers at 1Gbps link should be around 100 million per hour, on Postgresql. Raw data copying (compressed database files) are at around 4x faster. BCP works at somewhere in the middle, but is a bit buggy and finicky, but it transfers in binary. You should be looking at around half the time, depending on the nature of the data. For 1.5 billion rows that's around 5 hours per table. What people usually do that is incredibly slow: 1. Use SSIS 2. Use ADO.NET 3. Inserting into tables that have indexes or constraints like fk, uniques. If you're doing that, it can explain why it's slow.
A lot of analytics software when connected to the db did exactly that to date fields, and there is nothing you can do about it except either some hacky stuff like a proxy jdbc driver that parses and rewrites queries, or index on expressions.
I know, I work with Tableau that does exactly that, though there are ways around it. These queries are hand-written though.
This is what i came up with but get the same result. &amp;#x200B; &amp;#x200B; SELECT Scholarships.SCHOLARSHIP\_NAME, AVG(Scholarship\_Awards.AMOUNT\_AWARDED) AS "Average Amount Awarded" FROM Scholarships JOIN Scholarship\_Awards ON Scholarship\_Awards.SCHOLARSHIP\_ID = [SCHOLARSHIPS.ID](https://SCHOLARSHIPS.ID) GROUP BY SCHOLARSHIP\_NAME
Omg! Tableau! Yes, how did you know? I hope you're not connecting that monstrosity with a live connection to MySQL?
No. Live connection to SQL Server, row counts in dozens of millions and can't do extracts due to row-level security. Huge pain in the ass to optimize for these cryptic motherfucking queries. 
Have a look at the data type on the table - it might be that it is an integer, resulting in the result being rounded to the nearest integer. Try avg(cast(prize as numeric)) or what ever is appropriate for your variant of SQL.
&gt;///////////// &gt; &gt;ID/ Scholarship\_name / Amount\_available &gt; &gt;"1" "1" "Presidents Award" "$15,000" &gt; &gt;"2" "2" "Chancellor Academic Award" "$25,000" &gt; &gt;"3" "3" "Chevron Award" "$20,000" &gt; &gt;///////// &gt; &gt;ID / Scholarship\_ID / Student\_ID / Amount\_awarded &gt; &gt;"1" "1" "2" "9" "$1,500" &gt; &gt;"2" "2" "3" "8" "$1,500" &gt; &gt;"3" "3" "1" "7" "$2,000" &gt; &gt;"4" "4" "2" "6" "$1,500" &gt; &gt;"5" "5" "2" "5" "$3,000" &gt; &gt;"6" "6" "3" "4" "$5,000" &gt; &gt;"7" "7" "3" "3" "$2,500" &gt; &gt;"8" "8" "2" "2" "$2,500" &gt; &gt;"9" "9" "1" "1" "$1,500" &amp;#x200B; &amp;#x200B; Well all the integers would be rounding in the 2500's i believe if that were the case. I tried what you posted and it didn't work. I have posted the query i used and failed with but will again here. I will also post the tables so maybe you can see where im going wrong. Neither table has a PK or FK that links up but i believe i have that figured out. &amp;#x200B; &amp;#x200B; SELECT Scholarships.SCHOLARSHIP\_NAME, AVG(Scholarship\_Awards.amount\_awarded) AS "Average Amount Awarded" FROM Scholarships JOIN Scholarship\_Awards ON Scholarship\_Awards.SCHOLARSHIP\_ID = [SCHOLARSHIPS.ID](https://SCHOLARSHIPS.ID) GROUP BY SCHOLARSHIP\_NAME &amp;#x200B; &amp;#x200B;
Replying to edit. &gt; So the example is absolutely correct. In the real world nobody would lock the entire table for the entire time the software is running. In the real world no one would save streaming data like that directly to a relational database. You can batch in memory, then insert. You can stream to a pipeline like kafka, then gather, upload to database, again in batches. No one in their right mind would open tens of thousands of transactions a second. &gt; As for the speed - seconds for generating a free thousand rows? In Postgresql I have to get tangled up in zeroes if I want to get above 1 second. Like 10000000 rows to something. Ok, maybe 1000000 if it's not just a couple of columns. From Java code? No. Sure you can `INSERT INTO tbl(i) SELECT n FROM generate_series(1, 1000000) s(n)` in a single second, but that's not the same. 500k records using my batched code took mariadb 45s vs pgsql 17s, but docker-proxy reported huge CPU usage during mariadb inserts for whatever reason, which I think could be slowing it down significantly. I don't really feel like installing mariadb directly on my PC atm, so let's just say it's just slower. 
&gt; SELECT Scholarships.SCHOLARSHIP_NAME, AVG(Scholarship_Awards.amount_awarded) AS "Average Amount Awarded" &gt; &gt; FROM Scholarships JOIN Scholarship_Awards &gt; &gt; ON Scholarship_Awards.SCHOLARSHIP_ID = SCHOLARSHIPS.ID &gt; &gt; GROUP BY SCHOLARSHIP_NAME That query looks fine. Are you saying it returns a list of scholarship names, with 0 as the average award?
yes that is correct. the averages should be between 1500 and 3000
Legal gray zone? If posted online in a public forum then why is it considered in the gray zone. There is no assumption of privacy unless you try to figure out who said something under a handle.
I'd start here; https://www.edx.org/learn/sql and read the course descriptions to find areas that you are not familiar with. Courses are free but you can pay if you want certifications of completion or college credit.
Run the query again, but ungrouped. That might give you a clue into what's going on. SELECT Scholarships.SCHOLARSHIP_NAME, Scholarship_Awards.amount_awarded FROM Scholarships JOIN Scholarship_Awards ON Scholarship_Awards.SCHOLARSHIP_ID = SCHOLARSHIPS.ID Assuming that the join is working (and if it were completely broken, you wouldn't even have the list of names), my feeling is still that the data type of amount_awarded is an issue. 
when i ungroup it and take off the AVG (...) I get the values put in with the names accordingly. So i know the JOIN is working. I just feel like im missing something to connect the values between the ID and the amounts on the scholarship_awards table. I could be totally wrong though. I did double chekc and it says that its an integer. I did change it around to different types of data and try it over again just for shits and giggles and didnt have any luck.
Oracle is facing very stiff challenge from multiple other database vendors. Don't lock yourself into a vendor specific language like pl/sql. Instead learn python. Learn about postgres and MySQL. Learn about etl tools. Learn about big data as a long term goal, especially spark.
If the ungrouped query is working, the problem won't be your join. What type of SQL (i.e., the platform, e.g., Microsoft SQL Server, MySQL) are you using? And just to confirm, SELECT Scholarships.SCHOLARSHIP_NAME, AVG(cast(Scholarship_Awards.amount_awarded as numeric)) AS "Average Amount Awarded" FROM Scholarships JOIN Scholarship_Awards ON Scholarship_Awards.SCHOLARSHIP_ID = SCHOLARSHIPS.ID GROUP BY SCHOLARSHIP_NAME Doesn't work?
The school makes us use SQLite3 in DBrowser. I just tried one more time and CAST didnt work
Sorry, I'm out of ideas. I don't see the problem.
I have tried edx for a few other things. Will give it another go. Any specific concepts you think I should know? Like if I applied to another job what advanced concepts would they consider critical? I was thinking stored procedures. I have not learned those yet but they seem to come up a lot.
I don't see the problem, it looks like that statement is fine. For shits and giggles, does it work as a single table statement grouped on ID? SELECT SCHOLARSHIPS.ID, AVG(cast(Scholarship_Awards.amount_awarded as numeric)) AS "Average Amount Awarded" FROM Scholarships GROUP BY SCHOLARSHIPS.ID
It didnt work
Can you put a TO_CHAR() on both and see if it'll work? 
Nulls are undateable because technically they are nothing...
It's a grey area because that isn't a legitimate data acquisition method, just because you can doesn't mean you should. Additionally many reputable companies will not deal with data scrapers even if the data they are selling is cheaper, partly because of perception and you only are getting a partial dataset. Also, sometimes is less about the data that is scraped and more about what you're doing to the companies servers that is hosting the data. 
Well put
I think data / business modeling would be beneficial. I myself have been delving into analytics more, and work on a data warehousing team as a Business Analyst. I’m learning more and more about how schemas work, how data is modeled to the end user, and more of what the back end is doing when you query a specific table of data. I’d recommend searching into database design, data modeling, or data warehousing. This is all quite technical - it is IT focused as well for the most part - but it is what you will need to understand if you’re trying to get at the stuff behind the SQL queries. Hope this helps!
It's taxing on servers, that companies part money for, to interact with users that bring them profit. Robots are usually competition, and are very taxing on the server, which results in unwanted costs, for no gain. It's like malware. Companies do a lot to fight this, and at the same time someone at the same companies would pay 3rd party companies for web scraping, which leads to an evolutionary battle where each party becomes more and more complex in their methods. Millions of dollars are just wasted into this black hole. For nothing. It's unethical and in some countries in Europe it's illegal.
Without details of what version you're using, look at this, wrt your first question: https://docs.microsoft.com/en-us/sql/relational-databases/tables/specify-default-values-for-columns?view=sql-server-2017 Under the 'Limitations...' section: &gt; To enter an object/function, enter the name of the object/function without quotation marks around it. 
Try udemy it is best.
I'm not sure what makes this "modern" other than it's "new". 1. "Shorter syntax with less keywords" - It just seems less readable to me. And "shorter" isn't better if you're just making more things implicit. 2. "Functions" - A few SQL dialects have functions already. I'm not seeing what makes this better? 3. "Automatic Joins" - There is a commonly-accepted way to join tables by shared keys - \`NATURAL JOIN\` - although I'm not sure making this built-in as a default is a meaningful improvement. In fact, I never use \`NATURAL JOIN\` because I'd rather be explicit in my code to maximize readability. This seems like a large change in syntax for no gain.
We are in the same evaluation process looking for the right tool for making Whitelabel Embedded reports for our multi tenant/partner web app. Curious about what tool/service you decided on?
Thanks for the constructive criticism. 1. I'm not actually making things implicit (except for the joins). I'm just replacing common keywords with symbols. I get that we like what we're used to, but this is common practice in every popular programming language, except for SQL and Pascal. 2. Can you provide a link, or show me how it's done? I'm not aware of this feature. 3. You're right, NATURAL JOIN provides something similar, although it relies on similar column names, while my approach is to declare the connection explicitly in the table. I don't agree that being explicit necessary maximizes readability, but I guess it's a matter of taste.
If you're doing the INSERT via a SELECT you could call a function within that SELECT. For example INSERT INTO dbo.table_A SELECT @Item, @Quantity, someFunction_f(@Quantity, @Item)
Through decades, a lot of software has appeared that touted itself as newer, better than SQL, that leaves SQL behind. Where are they now? Trying to look more LIKE SQL rather than different, trying to implement the SQL standard. And for good reason. SQL has proven itself to be very good at data manipulation. It's clear and concise. And with modern additions it can do pretty much everything, with very few exceptions. &gt;It's also missing plenty of important features Like what? &gt;how intimidating SQL can be to novices, and even experienced programmers. It is honestly the easiest language I've ever learned. Half of the people in the company I work at, who have nothing to do with programming, are able to write decent SQL. It's not difficult. It can't really get easier. It just has to be taught properly, and on proper databases, not MySQL, which the majority adopts at first. &gt;server functions (using a somewhat daunting syntax) I don't know what you're referring to. UDF? CTE? &gt;the most important construct - session functions Why do you think it's important? Data doesn't live only in a session. It transcends sessions. So do functions. Functions in regular procedural code don't exist for a single run. They continue to exist in the code. What would be a good use case for this not being so? &gt;WITH RECURSIVE Recursive queries should be used for parsing trees, not iterations. Recursive functions are always much slower than loops. And loops are available in various forms in the database world, either in procedural languages (T-SQL, plpgsql, etc) or as functions, like, in your case, `generate_series(1, 1000, 1)` in postgresql. &gt;Automatic Joins Trying to automate too much takes control away from the developer, makes systems far more complex, and don't allow any fringe scenarios. For example, A references B, B references C, C references A. With proper control over how to join stuff, you can easily make a query that would make use of these tables. With automatic joins, you're forced into a corner with this. That being said, if you really treasure this feature, you could always name your columns accordingly, and do `LEFT NATURAL JOIN` at least in postgresql. *In summary* I get that you have good intentions and enthusiasm, and for sure there are a lot of systems that need improvements or replacements. SQL is a language that has many flaws, for sure. But before trying to replace it, did you make sure that you know it well enough to understand why it's an industry standard? If not, then maybe you should, and your efforts and intentions can serve the community really well, in the sea of new and existing database engines and SQL or SQL-like languages that they support. Maybe you could add the features that you really think are missing right there?
What you mean by functions (with the `WITH`) are not functions, these are common table expressions. Procedural languages have regular functions, like say in SQL Server very simplistic and rather useless example: CREATE FUNCTION fn_GetSomethingByCountry ( @country NVARCHAR(50) ) RETURNS TABLE AS ( SELECT something FROM table WHERE country = @country ) These live on the server, and you can use them in any query (permissions allowing of course). Ok, on to my thoughts. So first of all - I like it. If not as a replacement for SQL, I really like it as a pet project, it looks cool and would definitely score you some points with employers if put on your github. Would I use it myself? I don't think so. Look at this for example: cnt(start, end, step=1) -&gt; x = [$start] + (cnt [x &lt; $end] -&gt; x + $step) I don't know, maybe for someone familiar with functional programming, this looks readable. But functional programming is the lesser known programming paradigm, and it looks absolutely bonkers to me: * `x = ` what does this apply to? `[$start]` or the whole result? * `[...]` denoted a condition in your previous examples, here we see `[$start]` which kind of looks like set initialization? I don't really know * `cnt [x &lt; $end]` here you're referring to cnt, but it's a function that takes arguments, and yet you're calling it as if it was a table, which I understand refers to recursive part, but it's completely impossible to understand from this form How would you implement window functions in this syntax: partitions, over clauses? I feel like this would add either more cryptic characters, or give additional meaning to existing ones, which would make them even more cryptic. What would HAVING look like? Onto joins. Why `country -&gt; Country.id`, not just `country -&gt; Country`? I don't see the relevance of the id column for this, unless you want to denote which country is the join made on, but what's the point of that? With my simplified syntax, you get composite keys for free. But still, there's one issue with both - how, without knowing the schema, will you be able to parse and compile a query using your join? You'd need to read information_schema on an existing database to even transpile queries to SQL. Don't take this the wrong way, I really like the idea itself. Just don't go into it thinking it would solve all problems. If you think SQL is hard to understand, I can't imagine what would people think about this. I feel programmers could like it, but there are so many more people using SQL, all kind of analysts, with no background in programming, who'd find this extremely cryptic. 
In Oracle you could use an insert trigger for this. Dunno what DMBS you're using?
I Think in your case you'd want triggers on the other columns. A default value function can't get the values of the other columns that I know of without selecting. With a trigger you have the INSERTED variable to work with. 
&gt; Through decades, a lot of software has appeared that touted itself as newer, better than SQL, that leaves SQL behind. Where are they now? I don't know, because I couldn't find any. Can you give me some examples of such languages? I think SQL is today's standard because of the databases themselves, which are based on the solid ideas of relational mapping and ACID. I don't see any reason to think that it's the SQL syntax that is prized above all others. &gt; Why do you think it's important? Data doesn't live only in a session No, but intention does. Allows session-functions will improve interactive use (through a REPL), and allow different clients to work with the same database without risking namespace collisions. Of course, server-side functions are also useful and important, but when designing the database, you can't always predict how others will want to query your data. &gt; Recursive queries should be used for parsing trees, not iterations Maybe. I was just using an official example, I didn't invent it. The point isn't that `generate_series` is a super important function, but that you might want to be able to create functions like it, and not rely on the existence a builtin implementation. &gt; Trying to automate too much takes control away from the developer As I mentioned, the idea isn't to prevent explicit joins, but to allow the user to omit them when they are obvious. It's a bit like if SQL required everyone to write "WHERE 1=1" when they didn't care about the condition. It's just verbose and doesn't add anything. &gt; your efforts and intentions can serve the community really well, in the sea of new and existing database engines If you have any concrete ideas, I'll be happy to hear them! Thanks for the detailed reply, this is the sort of interaction I was hoping for.
to_date(&lt;value&gt;) works for me
&gt; Procedural languages have regular functions There is value in functions that only last for the session. Imagine that you're using a shell client to explore the database (a REPL environment), and you want to re-use some queries or expressions. Creating a permanent server-side function will pollute the global space, and might cause collisions with other users. Similarly, if you're using a language like Python or Go. Nowadays, most developers either use an ORM, or have to resort to awkward string manipulation. &gt; So first of all - I like it Thanks! :) &gt; would definitely score you some points with employers I'm actually a very experienced software developer. I'm considering this because I think it can really help, and not for my CV. &gt; maybe for someone familiar with functional programming, this looks readable I agree the syntax there is a bit confusing, but imo less so than the SQL equivalent. Also, SQL *is* a functional language. But I'm taking your point, and I'll try to improve the syntax for this kind of use-case. &gt; How would you implement window functions in this syntax: partitions, over clauses? I'm not sure yet. At worst, I can fall-back to SQL syntax, if I can't find a better one. That's what I'm doing right now for "groupby", for example. &gt; What would HAVING look like? Just like "where". There's no real reason to have different keywords, because the language can detect what the condition is referring to. &gt; Why country -&gt; Country.id, not just country -&gt; Country? It's actually possible to omit the "id" column in this situation, but I didn't want to complicate things. What this signifies is that "Customer.country" links to "Country.id", unlike NATURAL JOIN which requires the same column name, and might create unintentional join conditions. &gt; without knowing the schema, will you be able to parse and compile a query using your join? The idea is that the schema will be there. Otherwise, you'l have to specify the joins explicitly, as you currently do in SQL. &gt; Just don't go into it thinking it would solve all problems. I'm hoping to solve 90% of the problems. I know this syntax is more programmer-oriented, but I also feel that there's a *reason* programmers prefer this style, and that data analysts will prefer it too after a short introduction to it. But I'm also keen on hearing if there's anything I can change to make it more easy to adopt, while maintaining terseness.
&gt;Can you give me some examples of such languages? * Cassandra appeared as one of the first large nosql databases. Their language is now called CQL, and they like to advertise that it's similar to SQL (it's not really) * Spark was initially an API for Java, then Python. Then quickly this appeared: [Spark SQL &amp; DataFrames \| Apache Spark](https://spark.apache.org/sql/). * Even MongoDB has SQL now * Have regular files scattered around? [HiveQL is almost SQL](https://www.tutorialspoint.com/hive/hiveql_select_where.htm) * Don't like Hive? How about [Presto](https://prestodb.io/docs/current/sql/select.html)? To this date, there isn't a single widely used tabular data engine that didn't become widely used thanks to them offering an SQL-like syntax. Because: * SQL has been around for ages * SQL is clear when reading, because it uses English. The same reason why Pascal was considered one of the easiest languages and C - a horror story back in the 80s, even if Pascal was harder to code in. * SQL is easy to write programmatically. There is a clear order of statements, and one could use a very basic state machine to parse the instruction tree. So generating and parsing SQL is as easy as generating and parsing, say, JSON. &gt;allow different clients to work with the same database without risking namespace collisions Well most of the times people won't, and shouldn't write functions, unless they're the maintainers of that DB. If that's not the case, offering a separate schema (namespace) for functions for every user would solve this fringe issue. Yes, it would be nice, but hardly any useful for 99.9% of the use cases. And this could be fixed simply by adding temporary functions (same as there are temporary views and temporary tables in postgresql). &gt;but that you might want to be able to create functions like it, and not rely on the existence a builtin implementation. SQL is pretty powerful itself. I've only met a single case that I couldn't write as SQL (doesn't mean it's not possible), and that was a question with 250 bonus points on StackOverflow. Nobody got the bonus. With `generate_series` you pretty much have an iterator, and you can integrate that in any SQL query without any need to declare any functions. The hardest thing to write (but still possible) in SQL is state machines. What's the hardest (if at all possible) is parallel iterators - when you go through 2 lists at once and iterate to next() on either one based on a specific state information. How often is this required? Very rarely. At this point the answer is simply to move logic to outside of the DB into proper code with version control and unit tests debugging and such. &gt;to omit them when they are obvious `NATURAL JOIN`? &gt;If you have any concrete ideas, I'll be happy to hear them! * Adding temporary functions to postgresql * Adding window functions to ClickHouse * Adding ways to explicitly select an entire partition in hash-partitioned tables in Postgresql (useful for joins when computing datasets in bulk) * Adding support for cached `@numba.jit` calls in `plpython3` language in postgresql * Making postgresql_hll extension functions more concise (like add an `hll_count_distinct(value)` function instead of `hll_cardinality(hll_union_agg(hll_add(hll_empty(), value)))`) * Making postgres_fdw extension Read+Write enabled (so that we could have a proper Postgresql Cluster, BIG DATA style) * Enabling partition row routing for INSERTs into partitioned tables whose partitions are foreign tables in postgresql ...and so on
Haven't used it myself but there is [https://www.sqlprostudio.com/](https://www.sqlprostudio.com/)
Okay well I created two tables as close as I could to your definitions above. I omitted the first column of values in both though because the table definitions don't match the values you are showing, and it looks like you just put the ID for both tables twice. After testing I can confirm that the below query gets what you are after: SELECT Scholarship\_name ,AVG(Amount\_awarded) AS Average\_Amount FROM Scholarship INNER JOIN Scholarship\_awards ON Scholarship.ID = Scholarship\_awards.Scholarship\_ID GROUP BY Scholarship\_name &amp;#x200B; I am using SQL Server, but it should get you going on what you are trying to do.
&gt; I agree the syntax there is a bit confusing, but imo less so than the SQL equivalent. Maybe looks like it when you're talking about the simplest queries like SELECT a, b FROM c WHERE d &gt; 5, but that's covering a very, very tiny use case. &gt; Also, SQL is a functional language. Well that's just not true. It's entirely declarative, you don't get to decide on how something works, just what you want to get out of it. Set-based language maybe, as it operates on sets, but definitely not functional. &gt; &gt; What would HAVING look like? &gt; &gt; Just like "where". There's no real reason to have different keywords, because the language can detect what the condition is referring to. Hm, I don't see it. Take this simple query: SELECT C.Name , YEAR(Inv.InvoiceDate) AS PeriodYear , SUM(Inv.SalesValue) AS SalesValue FROM Invoice Inv LEFT OUTER JOIN Customer C ON Inv.CustomerId = C.Id WHERE Inv.SalesValue &gt; 0 AND ( C.Id IS NULL OR EXISTS (SELECT * FROM CustomerSegments CS WHERE CS.CustomerId = C.Id AND CS.Segment = 'Private')) GROUP BY C.Name, YEAR(Inv.InvoiceDate) HAVING SUM(Inv.SalesValue) &gt; 100000 Has conditions on SalesValue in both `WHERE` and `HAVING`. I struggle to envision how would this look like in your syntax. For having, maybe `&lt;table&gt; [ $where_condition$ ] groupby &lt;$column_list&gt; [ &lt;$having_condition$&gt; ]`? So in total, and I'm making things up (`!` denotes a null something, `and` and `or` are used instead of `&amp;&amp;` and `||`, maybe `and` could simply be `,` for where clauses?) is_private_segment(id) = !$id or CustomerSegments [ Segment = 'Private' and id = $id ] -&gt; true -- customer is either NULL or belongs to private segment Invoice [ SalesValue &gt; 0 and is_private_segment(Customer.id) ] groupby Customer.Name, year(PeriodDate) [ sum(SalesValue) &gt; 100000 ] -- group by with having -&gt; Customer.Name, year(PeriodDate) AS year, SUM(SalesValue) AS TotalSales Doesn't look that bad, I guess, though I don't know how to denote a left join. I feel groupby and select columns could be somehow simplified, I don't like the fact I have to repeat them (just like in SQL). 
Azure Data Studio https://docs.microsoft.com/en-us/sql/azure-data-studio/what-is?view=sql-server-2017
I use [DataGrip](https://www.jetbrains.com/datagrip/), it’s not free but you can try it for 30 days
"A NoSQL (originally referring to "non SQL" or "non relational")[1] " https://en.wikipedia.org/wiki/NoSQL
**NoSQL** A NoSQL (originally referring to "non SQL" or "non relational") database provides a mechanism for storage and retrieval of data that is modeled in means other than the tabular relations used in relational databases. Such databases have existed since the late 1960s, but did not obtain the "NoSQL" moniker until a surge of popularity in the early twenty-first century, triggered by the needs of Web 2.0 companies. NoSQL databases are increasingly used in big data and real-time web applications. NoSQL systems are also sometimes called "Not only SQL" to emphasize that they may support SQL-like query languages, or sit alongside SQL database in a polyglot persistence architecture.Motivations for this approach include: simplicity of design, simpler "horizontal" scaling to clusters of machines (which is a problem for relational databases), and finer control over availability. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/SQL/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
You seem to get the gist of it! Also, the declaration of "is_private_segment" makes your query much more understandable for me. My point was, that if you wanted, you could do Invoice [ SalesValue &gt; 0, is_private_segment(Customer.id), sum(SalesValue) &gt; 100000 ] groupby Customer.Name, year(PeriodDate) Because the compiler can tell what is being aggregated according to groupby. Although sometimes you might want to separate them, just for the sake of clarity. Either way, I find it very encouraging that you picked up the correct intuition for my language so quickly! &gt; but that's covering a very, very tiny use case. I did test it on a 50 line query I once wrote, and it became much shorter and also clearer imo.
I can cot connect to the server, does it require any specific configuration? thank you!
Are you using the right Auth type? I use it and have not had any issues. 
&gt; You seem to get the gist of it! I said I like it! I would separate where and having because there's a [clear order of operations in SQL](https://www.periscopedata.com/blog/sql-query-order-of-operations) and having them in the same place could be confusing as to what's happening first. &gt; did test it on a 50 line query I once wrote, and it became much shorter and also clearer imo. [~~Smallest~~ (second smallest, just checked one more) of my projects](https://i.imgur.com/2NHljH5.png). Not trying to enter an online dick measuring contest here, just saying I have some perspective. 
Haha that's terrifying :x &gt; there's a clear order of operations in SQL That seems like another strong argument for putting the "select" at the end of the query.
Sounds like a good case for a computed column. https://docs.microsoft.com/en-us/sql/relational-databases/tables/specify-computed-columns-in-a-table?view=sql-server-2017
No, It does require that you have the correct instance name and authentication type and, if you're using SQL authentication, a valid login and password. "Cannot connect to the server" is to vague to provide meaningful help; the application should give you a more detailed message explaining what happened.
Start with GUI tools like MS Access or (LibreOffice Base)[https://www.libreoffice.org/discover/base/] With them you can make tables using GUI, fill them using GUI and also do queries using the visual GUI. Once you get the gist of everything you can start writing SQL. Be sure to follow the (handbook)[https://wiki.documentfoundation.org/images/e/e8/BH40-BaseHandbook.pdf] with LibreOffice if you choose it. For theory of relational databases and SQL it's best to read an introductory book. Try (this)[https://www.amazon.com/Introduction-Relational-Databases-Christopher-Allen/dp/0072229241] An interesting tool is (RelAx)[http://dbis-uibk.github.io/relax/] if you decide to dig a little deeper into this.
Haha, why terrifying? Truth be told, a lot of that code is boilerplate, enable logging, check this, that, generate messages, execution stats, etc. But there are some behemoths as well, procedures spanning hundreds of lines of real code with SQL blowing people's socks off when they first see it, code rewritten from Java into T-SQL, lowering batch execution time from 7 hours to 15 minutes. Stuff. Anyway, I'd be very interested in seeing you progress. If you have a repo I could star or anything of the sorts, let me know!
https://www.postgresql.org/docs/9.1/functions-datetime.html
Udemy is pretty good. I used that when I had to learn SharePoint.
You are right, the error message is &amp;#x200B; A network-related or instance-specific error occurred while establishing a connection to SQL Server. The server was not found or was not accessible. Verify that the instance name is correct and that SQL Server is configured to allow remote connections. (provider: TCP Provider, error: 40 - Could not open a connection to SQL Server
I can't imagine maintaining all this SQL code. Is it version controlled? But I agree, well-written SQL is very fast. I had a similar (though not as big) effect when I moved a bunch of my logic from Go to SQL. Although the number of lines of code did multiply :) I created a repo, I'll be happy if you star it: https://github.com/erezsh/Preql Once my design is a bit more stable, I will start pushing in code and docs.
LEFT JOIN on the customer table then do a CASE statement like `CASE WHEN leftjoinedtable.cust_id IS NULL THEN 'Y' ELSE 'N' END AS 'NewCustomer'`
I don't think any SQL engine is capable of reading PDF documents. You probably want to code this in Python and use some PDF reading library. Your python code can then dump the data from PDF into an intermediate table. You then probably want to run a stored proc on that table to clean up the data, detect errors etc. And then you want to move the data from the intermediate table into your mainline transactional table.
Something like this: Select top 1 datediff(dd,productiondate,servicedate) , * From table where productiondate &lt; servicedate Order by 1 asc 
Oops, &lt; should be &gt; instead :-)
Best tool so far, the most universal I've seen, and easy and fast to use, is DBeaver. Unlike DataGrip it organizes partitions under it's parent table, and supports a ton of database engines
Looks like you can't get to the server from your Mac, or you've got the address wrong.
I've done this a bunch and finally found this tool https://tabula.technology/ You can grab entire tables easily.
Would it work with the link I have? It looks like a table, but there are no boxes. It's like 2 tables side by side.
That was it, thank you!
Not with SQL. Use a programming language that as a library that can parse PDFs to extract the data, then insert into SQL.
What languages and libraries did you use?
Possibilities: * You have the name wrong * The instance isn't configured to accept connections via TCP/IP * The instance isn't configured to accept connections from your machine via TCP/IP * There's a firewall *somewhere* between you and the instance (including on both your Mac and the instance's host computer) * There's no network route at all between you and the instance But thus far, I'm inclined to say it's *not* Azure Data Studio at fault here.
How frequently are you planning on doing this? The best way is unfortunately, manually, at least if you want complete accuracy.
Many times.... There are other files in that domain website that I want in my database.
I use SharpPDF with C# to open, and then RegEx find my data. Your PDF must have text to scan though. If it's all images you will need some OCR.
I haven't had a need to do it. But I do know that SQL is the completely wrong tool for this job.
try the lag function.
Ouch. Yeah, there are some Python libraries that come to mind like PDFTabExtract: https://github.com/WZBSocialScienceCenter PDF is a horrible format to import from being intended exclusively for display. Is it possible your data source has CSV available if you ask? That might save a ton of time.
Nope. Just Pdf. Thanks Alabama...
Are you looking for the one row in the entire table where this holds true, or do you need to find the appropriate row for every group of records where the ID column is the same value?
I have been using SQLPRo for MSSQL. It is in the App Store.
Ya, it's called https://en.wikipedia.org/wiki/SQL:2016
If you're looking for the latter, this is what you would use: SELECT ID ,ProductionDate ,ServiceDate ,Nominee FROM ( SELECT ID ,ProductionDate ,ServiceDate ,Nominee ,ROW_NUMBER() OVER (PARTITION BY ID ORDER BY CASE WHEN ServiceDate &lt;= ProductionDate THEN 1 ELSE 2 END, DATEDIFF(day,ProductionDate, ServiceDate) DESC) RowNum FROM TableNameHere ) TMP WHERE TMP.RowNum = 1
Of course it's version controlled! I use SQL Server Data Tools, a set of tools for Visual Studio that allow to have a regular solution in VS, just like a C++ or C# solution, for T-SQL code. You write your `CREATE ...` everything statements. It builds a package that you can deploy to a live target database, it automatically handles migration from the existing state to the state defined in the package and does all sorts of magic. I love the name by the way!
1. Use a calculated column 2. Use a trigger to auto-fill the column when a row is inserted/updated
if "to reference column**s** [plural] defined in a subquery's SELECT clause" is accurate, then the only place that subquery could be is in the FROM clause therefore, the answer is A, C, and D 
How do I keep LAG within a partition? I'm having some success with lag and I'm using it to calculate the days from the prior transaction, but if it dips into another partition, the transaction date belongs to another customer and the calculation isn't right. 
Convert it to word or excel using acrobat reader Pro or any similar software. It's much easier to programmatically work with office files as opposed to pdf