sp_Blitz bitches!
\#2 sounds like you're defining a report in terms of what you don't want instead of what you want, which seems less clear to me. Can't say I've ever seen that done before (in many years) so even if you prefer this method, it's very non-standard. You didn't mention which database you're using, but method #2 also sounds like it would generate more transaction log data unnecessarily. Say the "base" table has rows 1,2,3,4 and 5. The desired report should return rows 2, 4 and 5. So with method #1 you just insert data from "base" into "temp" and the transaction log stores an insert of rows 2, 4 and 5 into "temp". With method #2 you first insert 1,2,3,4 and 5 into "temp", log those inserts, then subsequently delete rows 1 and 3. Log the deletions. In total, that's more "stuff" to log, which obviously takes up more disk space. Also, I bet #2 would perform worse as the "base" table gets larger. Imagine "base" had a million rows and you only wanted 200k of them. Using method #1: insert 200k rows into "temp" (log writes the 200k rows). Done. Using method #2: First you insert a million rows (log writes the million rows), then you delete 800k rows (log writes the 800k rows). Done. Anyway I'm just kind of thinking out loud, again the situation could vary depending on which database you're using. Some databases allow you to bypass transaction logging entirely so the above point may be irrelevant. As an aside, a "temporary table" has a specific meaning in some databases, for example in Oracle a temporary table is a table that actually exists forever, but its contents are local to a session or transaction (your choice). Most newbies (no offense :)) tend to define a "temporary table" as an ordinary table that you create as needed and drop later when you no longer want its contents. Actually I, by default, wouldn't even store query results in any table unless I had a specific need to do so. Maybe you do, but I just thought I'd mention this as a heads up. I would generally only store or generate on the fly a query in a script or full-fledged application then run it as needed. One final point is with method #2, how would you handle aggregate functions? If you need sum(col1) grouped by col2 you can't just do that first then delete unwanted rows - that doesn't make sense. Maybe you don't have a need for aggregates for your use case, but most SQL reporting I've done (probably at least 95%) involves them. So you'd be limited in the kinds of reports you can run. OK I've rambled enough. Hopefully at least some of this is helpful.
Table variables are not very performant.
If you're a beginner, check out [https://www.stratascratch.com/blog](https://www.stratascratch.com/blog). If you're trying to a job and interviewing, youtube is great to learn how to prepare for technical interviews. They have such video blogs too ([https://youtu.be/n6gM265zG68](https://youtu.be/n6gM265zG68))
For SQL Server, SSRS, SSIS, Azure and Talend - [http://www.sql-datatools.com](http://www.sql-datatools.com)
[https://github.com/protocolbuffers/protobuf](https://github.com/protocolbuffers/protobuf)
You could just use an OR? `where keyword1 in () or keyword2 in()`
I have like 15-30 keywords, I'm trying to avoid making a line for each but I can't seem to find a way to avoid it.
You could great a #table and join on the conditions.
The query does not make sense to me. What are you trying to aggregate from ‘table’? Is there a relationship between table and the JSON? On what column? I feel that what I “think” you want to accomplish is not hard but I need more info to help you.
there is a column that has multiple keywords in a json array, but I want to query against 15-30 keywords to see if they exist in that json list. If so, return the row.
So the column \[value\] from the JSON corresponds with they keywords to match? If so (assuming) couldn't you just use: &amp;#x200B; SELECT j.\[value\], COUNT(\*) AS \[count\] FROM (SELECT \[value\] FROM OPENJSON(json,'$.labels')) AS j WHERE j.\[value\] IN('keyword1', 'keyword2', 'keyword3') -- etcetera GROUP BY j.\[value\]
You should also be able to use CROSS APPLY. e.g: SELECT json.value AS Keyword, COUNT(*) AS Count FROM table t CROSS APPLY OPENJSON(t.json,'$.labels') json WHERE json.value IN ('keyword1', 'keyword2', ...etc.) GROUP BY json.value
If you want to add columns just use JOINS, if you want to add rows use UNION or UNION ALL to add up query results just make sure both result sets have same datatypes and order
I think that would work, thanks will give it a try.
If you dont have two columns to match how do you expect the database to know which columns go with which?
Doesn't sound possible to join both tables. As said above you can use a union to combine both data sets vertically.
Not sure if this is what you're after, but SQL Server has support for transaction log shipping. You might just want the restoration part (not the backup/copy).
two questions... does workbench have a query tab? where you could code the actual sql statement and get it to run? because then you could do what you need and maybe get a more revealing error message from mysql why in the world BIGINT(20)? how many quintillions of rows are you expecting&gt;
Hi, thanks for the promt reply, &amp;#x200B; 1. Yes it does have the ability to write query statements, I however am not so great with them when it comes to the advanced creating relationship stuff (I'm still learning) 2. I opted for BIGINT(20) as there are currently 200,000+ contacts in the database, all of which will have 1 or more records in the flags table (currently there are 500,000+) so it was just more a future proof now so in 10 years time we might have 800K contacts but linked to 1.3mil flags
Which column in the other tables are you trying to link the foreign key to ?
 ALTER TABLE tbl_contactflags ADD CONSTRAINT FK_ContactID FOREIGN KEY ( ContactID ) REFERENCES tbl_contacts ( ContactID ) 1.3 million fits easily into INTEGER, which is half the size of BIGINT and more efficient INTEGER can hold numbers up to 2 billion and change
If they're taking the backups with Ola Hallengren's maintenance scripts, you could try sp\_DatabaseRestore: &amp;#x200B; [https://www.brentozar.com/archive/2017/03/databaserestore-open-source-database-restore-stored-procedure/](https://www.brentozar.com/archive/2017/03/databaserestore-open-source-database-restore-stored-procedure/)
I've found a lot of answers and good explanations from the [SQL Authority](https://blog.sqlauthority.com/) (listed above). He seems to be consistently good at providing code examples that either solve or illustrate his topics.
Trying to link ContactID to ContactID in the following tables (all same field parameters etc) &amp;#x200B; tbl\_contacts.ContactID = PK tbl\_attendees.ContactID = FK tbl\_contactflags.ContactID = FK tbl\_consent.ContactID = FK tbl\_speakeronboarding.ContactID = FK tbl\_sponsoronboarding.ContactID = FK &amp;#x200B; I have also ensured I reduce the index name so for example 'tbl\_sponsoronboarding.ContactID' relationship would be indexed as 'conta\_sponb'
I might go and change them over to INT(11) just because of the size/efficiency however I have succesfully made a foreign key with another bigint (CompanyID) with no issue. &amp;#x200B; That SQL statement ran perfectly, and when I checked the table the foreign key had been applied. So I temproarily unticked it and reselected to see what happened and got the following error \[Error\]([https://i.imgur.com/ueS7Fpr.png](https://i.imgur.com/ueS7Fpr.png))
The tableau part will be fine, it is usually a matter of keeping in mind what data you have and what you are looking for. The ability to quickly sandbox and demo concepts makes it easy to pick up. ETL processes can be more complicated though. If i were you i would focus on informatica if that should be part of your responsibilities as i would imagine you'll be using informatica to feed tableau. Naturally, it will be very challenging to make accurate visualizations without an accurate data model to feed your vis software. As an aside the analytics tools usually abstract away a lot of what you have to actually do with raw sql.
A lot depends on how big/busy the source tables are. I have used both methods. When pulling data from large busy tables I pick the columns I need and if possible use a criteria supported by indexes on the large busy table. If the fields are not part of any index then method two works best. I have also learned to avoid NOT IN and if necessary using CTE's to reverse a NOT IN to an IN to speed things up. The only caveat is you need the disk space for the data and the log file entries. Once I have the data in the temp table I put an index on the tempt table to support my query.
Informatica will definitely be what we use. In fact, if I had to wager, I would say my teammate and I will be the main informatica experts (or thats the goal). I have a few months before I have to worry about Informatica coming down the pipeline luckily. Also, thanks! Reading stuff like this makes me a lot less nervous. I got into this field based on my philosophy and political science background oddly enough because my understanding of logic and using SPSS some in grad school. Its all been self-taught in a field of people who went to school for this, so impostor syndrome is strong.
Imposter syndrome is a funny thing, just about everyone has it. Just nobody talks about it. Something that helps is to make the technology yours with personal projects. Don't attempt to passively learn it. Actively investigate working with and how to go about solving problems. Software and analytics are not actually that difficult. What people often find challenging is that they are fields in which you have to think critically all the time. They are not fields in which you can passively go through the motions and collect a pay check. Recruiters always talk about passion for your job, you dont need to have a passion for your job. Have a passion for something real and important like your family, your wife, kids, etc. As long as you can actively think critically about what you are doing youll be fine.
Thanks for the reply it is not what i am after unfortunately. I just need a way to run a job that looks i n a folder location where a bak and a trn file are stored and for it to restore the backup file and then the tlog file without taking into consideration the names of the files as these will change daily.
Funnily enough, I was looking at the [https://www.brentozar.com/archive/2017/03/databaserestore-open-source-database-restore-stored-procedure/](https://www.brentozar.com/archive/2017/03/databaserestore-open-source-database-restore-stored-procedure/) when I received your message, unfortunately, they are not using Ola Hallengren's maintenance scripts.
A blog I follow recently blogged about the blogs he reads, there's like 5 bajillion blogs in the feed: https://bertwagner.com/2019/07/16/how-i-continuously-learn-about-sql-server/
Dang, how could I forget Bert? Great guy
So the import wizard basically creates a package, then executes it. You can save and reuse those packages. You found the trouble though. The fields are all automatically set up varchar 50 by default. Anything over 50 chars as you need to increase the field size in the connection properties. I would not recommend using the auto field sizing feature. If you're going to reuse the packages, you want to possibly have permanent tables (with correct field types) to match your import packages definition. This will help you to automate the process with minimal failures. And the failures that you do get will be useful (I.e bad data that you want to know about). This was really quickly done on phone, so if you have more specific questions, ask away!
The import/export tool is sort of a miniature version of SSIS. I'd use SSIS since you have it. Build your packages to handle your truncation and validation errors. It's not necessary to create one package per file, but you'll generally need at least one dataflow per file. SSIS can also help you convert the files to, txt before importing into SSMS, but this isn't often necessary unless you're having metadata issues. If you can schedule your packages as jobs, even better, but you'll need elevated permissions. Using PowerShell is often for invoking the SSIS packages via DTExec command. Importing directly from SharePoint might vary by version - when I last worked with SharePoint 2010 lists as a datasource, I usually brought the files into Excel and then into SSMS via SSIS, and found the entire process clunky. Hopefully someone can weigh in with a smarter recommendation.
If you've already automated the migration of the trn files to a local file location, what's stopping you from renaming the file to always have the same name? Run this process against this "working" folder, than have the original files (with the original filenames) stored off in an "archive" folder.
define "simply added", maybe? yeah but the "heavy machinery" here is the cross join - you get all possible combinations from the sql engine and it becomes your job to filter out what's not needed.
I did it with VBA in excel. Basically I set date parameters which correspond to the names of files and then just click a button every week to upload.
Powershell
ETL - which is what you are doing, is kind of a specialty for a reason. If it is getting to the point where you are having issues there, maybe it is time to bring on an ETL dev.
How non technical are these employees?
Have you tried out the newer "import flat file" wizard? [https://docs.microsoft.com/en-us/sql/relational-databases/import-export/import-flat-file-wizard?view=sql-server-2017](https://docs.microsoft.com/en-us/sql/relational-databases/import-export/import-flat-file-wizard?view=sql-server-2017) &amp;#x200B; I've found it's way easier than the older import/export wizard, especially for one-off imports.
That might work. There's a range, but I do know there are a chunk of people that have little Excel knowledge. One thing I was thinking of might be a webpage with a series of links of pre-created queries that when clicked on would pull the necessary data from the database and present it. Do you have any thoughts on that idea?
I don't understand why you put SSDT aside right away. Visual Studio is easy and your team is new to database versioning, so it's obvious they don't know source control. In our database department about 25 people of different skills use SSDT on both new and legacy and large database projects, it works great. We've migrated from RedGate source control like two years ago and back then people didn't have much clue what is source control and why we need it.
I use OPENDATASOURCE, and parameterize the insert and select statements, the 'what driver should I use', the options, and the file path. This requires having knowledge before hand and tables that hold dataset and datapoint information (your own information schema)... a design to hold options and locations... but, if you are repeating the same file it makes it super easy. Exec prc_ImportFile 'peopleinfo', '_20190618','xlsx' -- grabs standard peopleinfo.xlsx information but looks for file peopleinfo_20190618.xlsx' Exec prc_executeprocessingrules 'STANDARD', 'PEOPLE IMPORT' --funneling data is an art. BUT.... you could still use opendatasource without all the fancy features.
You can use either datepart or datename to get the year: SELECT CONCAT('B3_PARENT (',(SELECT left(datename(month, dateadd(month,-1,getdate())), 3)),CHAR(32),DATENAME(YEAR,dateadd(month,-1,getdate())),')') SELECT CONCAT('B3_PARENT (',(SELECT left(datename(month, dateadd(month,-1,getdate())), 3)),CHAR(32),DATEPART(YEAR,dateadd(month,-1,getdate())),')')
Of course it depends a bit on the other tools you use in your workflow, but try out [SeekWell](https://www.seekwell.io) (I'm a co-Founder). It's a clean, modern interface that lets you automate data flow to/from your database and Google Sheets (great for ad hoc dashboarding), set up auto alerts in Slack and share database connections and searchable code libraries with your team. It has been built by analysts, so has a host of productivity-enhancing shortcuts like autocomplete, slash commands (a la Notion or Slack) and preloaded snippets. If you try it out, I would love to hear what you think!
Its because they would need to learn how VS works as well and also because management thinks something that plugs into SSMS would be more straightforward to use. Don't get me wrong, I would much prefer to learn to use SSDT but that is not a path I can take right now. So the next best option is to use a plugin that does what SSDT does but is also reliable and easy to use.
&gt; I know that SSDT is free and powerful but I cannot recommend that to my team as they have zero experience with visual studio and source control. But they don't have experience with any of the other tools you're proposing either.
Unfortunately the decision is not entirely up to me - might be my post does not come across entirely accurate. The team already has SSMS experience so management wants to go down that route.
 something like below should work: substring( convert( varchar, dateadd(month,-1,getdate()), 106), 4, 20)
it's the equivalent of asking why some people use text editors to write webpages while others use programs like Dreamweaver
Use the builder to help you learn syntax, then start writing it by hand. Anyone worth their salt who works with data regularly should be strong in writing the code. Once you’ve got the hang of it it’ll be much faster than using any sort of GUI tool.
I actually ended up messing around with the code and getting the datename version you used above. Nonetheless, thanks for your help on this!
Thats fine. Not judging anyone except my coworker who relies on it and can't write actual code. I don't use it for anything and was just curious if i'm missing out is all.
Yah the tool seems a lot slower to use for sure.
YEAR(DATEADD(MONTH,-1,GETDATE()))
You are not. At all.
Surely there has to be a way to correlate the data between the two tables if you think there is value in "combining" them. Can you provide a sample of the two base tables and what you'd want the results to look like?
Thanks Cal1gula for the response. The data sources I use do frequently have ","s in them. Most of the data is populated by individuals using free text... ex. Some columns include things like "Descriptions" where the user can practically write a novel describing their services. Unfortunately every time I try to just use the import wizard using CSV, i always run into "Truncation" issues. I try changing the columns to varchar(Max) with still no success. The originating files though generally are consistent, having identical column headers and the data types remaining the same. The information however can change every hour... example would be Financial transactions where spend could occur and be logged at any moment. My goal being to DL this file every day, or week, and import into SSMS to query against and join.
Remember that your next step after source control is automation, so don’t forget to set yourself up for it now. SSDT database solutions are extremely powerful and full featured for development with the code base being committed to TFS/Git/Whatever. Your ability to build a DACPAC and automate deployments would be a huge improvement. Saying you’re fine with SSMS but not SSDT is like saying your company builds doors, but you won’t buy a tablesaw because your staff only knows how to use a handsaw. There’s a learning curve, but it’s very much worth it.
Their SSMS experience isn't going to give them much of a leg up vs. SSDT once you throw a source control plugin into the mix. They'll _still_ have to learn how to use those plugins! Really, SSDT is not that hard to learn if all you're doing is keeping a database schema in source control and building release packages, especially if you're using DACPACs. If you're looking at Red Gate's tools (and I am a customer), you're going to have to shell out quite a bit for licensing to get functionality similar to what you can get for free with SSDT. The question is, do you _need_ the extra stuff that those tools offer to the point that it's worth that cost?
They know how to use SSMS. You'd be surprised to know that it's built on top of Visual Studio. So there's a no real learning curve. Perhaps a very small one. It's going to be way more difficult to teach them how Git works than Visual Studio. 🙂
&gt; because management thinks something that plugs into SSMS would be more straightforward to use. And "management" has extensive experience upon which to base this opinion?
if you’re doing rank() over a partition then you can just filter on the value of the rank. You probably have to do a subquery first since you can’t directly filter on window functions for most sql (Teradata has a QUALIFY statement that lets you filter on window functions, not sure if anything else does). select * from (select columns, rank() over (partition by column order by column) as col_rank from table ) where col_rank = 2 Your choice if you want to subquery or do a CTE.
Im thinking you either assign an order rank and then select that order rank, or use an order by and offset/limit command. From my phone so I cant write out a query, but those are my thoughts if it helps.
I've never seen "CASE &lt;something&gt; WHEN" with the &lt;something&gt; before. What does that do? Which database is this by the way?
That's right, MySQL does not support INTERSECT.
Not at all :) but unfortunately that does not stop them from making a decision. Most recently, request to migrate to a PaaS sql database was denied because it's not secure enough apparently.
just checked, the below works just fine (one record returns 'yes', another - 'no'): select t, case t when '!hot' then 'yes' else 'no' end [!uhoh] from (values ('!hot'), ('!not')) tt(t ) what do you mean by 'every record matches'?
Maybe you should look into power bi, this might be what you're looking for.
I have a master table and a dozen sub-tables that have multiple matching items per sub-table. So far the syntax I posted above has worked well to flatten the rest of the data. Unfortunately it appears SQLite appears to be grabbing the '!' as a special character.
 SELECT item , SUM(Quantity) AS total_quantity , SUM(Amount) AS total_amount FROM ( /* put your UNION query here */ ) AS u GROUP BY item
I'd definitely attend the SQL 101 stream if you do it.
We use the Redgate toolbelt at work and have slowly been adding our databases into source control and creating build release pipelines too. I find the Redgate products easy to use, and will allow your team to stay within SSMS, although you will still need a GIT tool for any branching / merging etc. From our experience they have been reliable, and they frequently roll out updates with new features and bug fixes. I find their product website almost always has the answers I am looking for, and their support has always answered any additional questions for us. &amp;#x200B; I haven't tried any other alternatives as the Redgate products were already used at work before I joined.
Thanks for the note. My main point for research was feature updates and bug fixes. Sounds like that tool has it.
The syntax looks correct to me, and I don't see anything saying that ! would break string comparisons but I'm not familiar with SQLite3. Maybe try something like this? CASE REPLACE(temp_restricts.restrict_abbr,'!','') WHEN 'hot' THEN temp_restricts.restrict_abbr ELSE NULL END '!hot'
If you're getting a long query with inflated numbers that's usually a one-to-many join gone awry somewhere; for what you're doing there, maybe try a CTE as long as these aren't too memory intensive: WITH T1 AS (first query), T2 AS (second query) SELECT a.col1, a.col2, ..... b.col FROM T1 AS a LEFT JOIN T2 AS b ON a.dept = b.oprseq
Ok I looked up the sqlite docs and understand what that means now... CASE temp\_restricts.restrict\_abbr WHEN '!hot' is equivalent to CASE WHEN temp\_restricts.restrict\_abbr = '!hot' -- not sure why they decided to break with the SQL standard there, but anyway... The '!' causing a problem seems strange. I can't imagine why it would unless it means something special to sqllite (I haven't used it before), so maybe you can take the first step of proving whether or not it's the culprit. Could you create a dummy table with just say 5 rows, with the values of restrict\_abbr as 'hot' and 'cold' and see if the query works on that? If it works then try again but with '!hot' and '!cold'.
That worked perfectly and I learned something. Thank you so much!
SELECT item_id, SUM(score) FROM table GROUP BY 1
Do you have control over how the data is generated? Or is it given to you in Excel? You need to change both *source and target* column types in the import wizard. So the target is SQL, and you need to change the types there, which it sounds like you are doing. You ALSO need to do that at the flat file connection screen (the advanced tab). And for a flat file connection, you will probably want to use string (DT_STR) type. It would be helpful if you have a data dictionary, or at least some kind of mapping document for your source system, so you know what the exact field sizes should be. If you plan to reuse the package, you will need to consider the persistence of the data in your tables, which will depend on the requirements. You may want to `TRUNCATE` the table between each run, or you may want to keep the rows. The first step is getting a solid package configured. One that you are sure doesn't have data typing issues. Then, if/when the package fails, you get more useful errors and you can address actual data issues, instead of process ones.
CASE WHEN condition THEN SUM(field)
well now i'm wondering what it is. i opened up management studio and i tried to find design query in editor tool so i can see what it looks like, but i can't find it... where should i be looking?
It is definitely the ! causing trouble. I'm using the exact same syntax pulling data from 3 other tables and they work just fine.
I get '!hot' on every record.
When I get cocky, I just remember what Milton Friedman had to say about even a pencil for gods sakes. And when I get despondent, I think about what my boss's boss makes without knowing much at all.
open up a query window, then right click in it, the menu should have it, thats how I got there, i'm sure its in a menu up top some where.
What you've described is the function of BI. There's a ton of free desktop tools (PowerBI, Microstrategy Desktop, Tableau Desktop to name some big ones) that would allow you to get started down this path. With each of them, you can also set up a cloud or hosted solution that would allow some curation and sharing of reports and data.
Doing the intermediate and advanced courses on mode right now. I've done most of the examples on sql zoo so the intermediate course is a breeze. Where do you recommend going from there?
&gt; It's more reliable to use tab or pipe (|) as a delimiter. Those are found in regular text much less frequently. The *true* foundation of the tabs vs spaces debate...
&gt; ~~GROUP BY 1~~ GROUP BY item_id
 CASE temp_restricts.restrict_abbr WHEN '!hot' THEN temp_restricts.restrict_abbr WHEN '!cold' THEN temp_restricts.restrict_abbr ELSE NULL END
this is something I might need to revisit, if I can't solve the similar problems I'm having....
Yeah I've never seen the benefit of using the ordinal. So now you have to look at the select to parse it? It's not like the order by or group by are often difficult to read. Plus, that syntax doesn't work between all sql engines.
I'm still teaching my team Git. It's been a year. They've picked up C# and Python in the same time period.
How VS works. File&gt;New Project&gt;New Database Project Then start writing DDL queries...
Fair enough, I just use it for one off queries
SELECT DISTINCT
`Select [YearPreviousMonth] = Year(DateAdd(MM, -1, GetDate()))`
Sadly depending on the data this answer is most likely less efficient than the original.
I don’t want to sound too critical but if your team cannot take a weekend and learn something as easy as SSDT then I doubt they will make it with other tools out there.
WHY would you want to do this? What would be the point? If the tables are logically close enough to hold essentially the same data (albeit with some columns empty), why would you not just use a single table? Seems like bad design to me. If the data is distinct enough to warrant two separate tables, it should be in two separate tables.
Check the post edit for the scheduled live stream.
Thanks bunches. I coincidentally found a related post in New that had something close enough that I could adapt it. I knew it was pretty straightforward, but was trying to add it onto an already cobbled together query.
Sounds like you got that bit sorted. Try using square brackets instead of single quotes for your column alias, so [!Hot]
I think datacamp's mobile app has SQL. Can't vouch for if it's decent or not.
Look into SoloLearn. It's a good start to a few languages, including SQL.
Because you're requesting that the owner of the remote repository _pull_ your changes into their repository. Yes, one could argue that you're requesting that the owner of the remote repository _merge_ your changes into their repo, but as the action they're taking is to fetch those changes from you, pull is _slightly_ more descriptive.
Whether the program exists on the server it is installed on?
Union does not join tables it joins result sets.
Worked with a colleague who said she was an expert at SQL. She showed me her queries. They were all a nightmare to read. I raised my eyebrow and asked her to show me her thought processes on a specific query. She fired up the Query Designer and started dragging and dropping. "See, it's just like Access.." Sigh... it all makes sense now. An expert, she is not. No, you are not missing out.
Try Enki they have daily short lessons and quizzes.
I think that when you have more experience with SQL, writing your own code gives you more control.... especially when the code you're writing gets more complicated, with complex queries or elaborate stored procedures. I've always viewed the designer as a tool that a novice would use to do basic ad hoc work.
' that syntax doesn't work between all sql engines ' is a valid point, but the advantage of ordinal is when it is not just a column from the table but is an expression.
You would need to replace the commas with something else and then in your import script you replace it back.
I discovered them recently.....game changer for me.
I use the OVER() and OVER()
The exams I've taken have all been multiple choice. No queries were written, everything was just theoretical. I believe you were allowed pen and paper.
I use them for all sorts of things. One of my favourite uses is to order the row level details based on the number of times something happens. For example, show all injections delivered to patients, ordered by the patient with the most injections first. SELECT * FROM dbo.Injections ORDER BY COUNT(*) OVER (PARTITION BY PatientID) DESC
Use them a lot because of analytics reasons. For example, RANK() after SUM() turns out handy when some types of entries needed to be compared. Say you have logistics operations and need to see what is most profitable branch (logistics.type) per day. `SUM(l.profit) OVER (PARTITION BY TRUNC(l.delivereddate), l.type, c.clientcode) AS profits` Then, on an outer layer of the script, you can RANK them to see `RANK() OVER (PARTITION BY l.delivereddate ORDER BY income)` &amp;#x200B; Hope this example is not too vague :)
Not on SQL server, but on Postgresql (very similar but with extra features like custom functions) I use them for every reason one can, but some of the now weird cases: Measure and combine effects of some records in time (ex. how many visits each patient has had in the month before a specific visit), so a visit creates both an effect event and a counter-effect synthetic event one month after. Running sum over them gets you the answer in the form of time intervals where a certain value is in effect. Another interesting use is to aggregate daily (hourly) value records into intervals of constant value (ex. Price hasn't changed in a month, there's no reason to keep records of it for every day, so you transform them into time intervals). This is done by subtracting row number overall from row number over partition by value, then using that flag in a group by. They're useful to find, mark and process any data state changes. They enable one to write any processing algorithm you would normally use procedural SQL or other languages for. But it's not always worth doing so because of complexity.
You must already have an Identity column in your table. This command allows you to insert your values to the identity column (instead of the automatic ones). If you want to generate your sequence, use Window function ROW_NUMBER()
Not sure I understand completely, but you can make a automatic diagram of an existing database by expanding the database, right click 'Database Diagrams' and choose 'Create New Diagram', if that is what you mean.
This should work too: &amp;#x200B; SELECT CONCAT('B3\_PARENT (', FORMAT(DATEADD(MONTH, -1, GETDATE()), 'MMM yyyy'), ')')
That doesn't make it any more user friendly. Git is the epitome of what development without the end user in mind. It took 15 years to get a functioning GUI...
Not sure if you are still struggling but: SELECT 'abc' as account,mnth,Month\_num, SUM(p.payment) over (partition by p.account order by month\_num asc) as total\_paid FROM #Mnth m LEFT JOIN #payments P on p.postdate = m.mnth LEFT JOIN #AccountList a on a.StartDate = m.mnth ORDER BY mnth asc Inital tables used. As I mentioned, Main table is all dates and 1 - 42 (#Mnth) CREATE TABLE #Mnth (Month\_num int, Mnth date) CREATE TABLE #AccountList (Account Varchar(3), StartDate date, Balance int) CREATE TABLE #Payments (Account Varchar(3), PostDate date, Payment int) &amp;#x200B; I note you haven't used the inital balance of 500 anywhere and I've hard coded the account name for sake of speed when making the temp tables above. &amp;#x200B; Have a good day, Rozza
There is no IDE needed, all multiple choice questions, some case studies and “choose and arrange pieces of code in correct order” questions.
Hey Thank YoU! So would I set the row\_number or would I enter it as a select statement?
Two quick uses that come to mind: * computing running totals * designing tables that are supposed to have effective date rangers. Rather than create Effective and Expires columns, and rely on the application to enforce that there are no gaps, you can design your table to just have an effective date, and use the LEAD function to derive the Expiration date `--Running total example` `DECLARE @t1 TABLE (` `GroupingCol VARCHAR(10),` `LineNum INT,` `Val1 FLOAT` `)` &amp;#x200B; `INSERT INTO @t1 SELECT 'one', 1, 1000.33` `INSERT INTO @t1 SELECT 'one', 2, 200` `INSERT INTO @t1 SELECT 'two', 3, 19999` `INSERT INTO @t1 SELECT 'two', 4, 23.6` &amp;#x200B; `SELECT *, RunningTotal = SUM(Val1) OVER (PARTITION BY GroupingCol ORDER BY LineNum)` `FROM @t1` &amp;#x200B; `--Expiration date example` `DECLARE @t2 TABLE (` `EffectiveDate DATE,` `ValueToUse FLOAT` `)` `INSERT INTO @t2 SELECT '19000101', 55` `INSERT INTO @t2 SELECT '20140603', 75` `INSERT INTO @t2 SELECT '20180813', 100` &amp;#x200B; `SELECT *, ExpirationDate = ISNULL(DATEADD(day, -1, LEAD(EffectiveDate) OVER (ORDER BY EffectiveDate)), '20991231')` `FROM @t2`
I use them for determining first occurrence, second occurrence, etc within a group. Several of my queries involve tracking claim movement between adjusters. Window functions allow me to quickly determine first assignment, second assignment and so on grouped by adjuster, claim, office, however i want to group it. Extremely useful and abstracts away all the craziness you'd have to do if window functions didn't exist.
&gt; Git is the epitome of what development looks like without the end user in mind. Arguing semantics here, but anyone who uses a piece of software is an end user of that software. git was never designed for "regular people." But has _any_ version control system ever been designed for "regular people"? Certainly not rcs, cvs, Visual Source (un)Safe, TFS, Perforce, or svn. svn might have come closest, but even that wasn't great. Why? Because the whole idea of version control gets difficult and confusing in a hurry. Regardless of what product you're using. If you try to keep it simple (no branching or forking, everyone commits to the same tree), you can quickly find yourself in a logjam. Start using git and other distributed VCSs the way they're meant to be used, and you're having to explain [directed acyclic graphs](https://en.wikipedia.org/wiki/Directed_acyclic_graph) to people at which point they'll throw up their hands and say "screw it." But how do you get them to use something if they refuse to make the attempt to understand the workflow? Git wasn't even really designed to be used directly. Anyone who was around for the origins of git will agree that it wasn't designed for "end users." Linus Torvalds meant for people to layer other stuff on top of it to abstract the craziness - just like he released a kernel into the world and let everyone else build userspace around it. But nobody really bothered to do that for git because what he created was "good enough" for the initial users to get their stuff done.
&gt; Arguing semantics here, but anyone who uses a piece of software is an end user of that software. Yes, we're on the same page. Git is not designed for anyone but the original developer. This makes it extremely difficult to learn. Hence the example, my team has learned 2 programming languages (both Python and C# already running in production) they're still struggling with Git.
Saw your blog post earlier in the week from somewhere (can't recall). Really enjoyed it. Looking forward to Part 2!
Note: this only works "automatically" if your database has good foreign key constraints set up between tables. Not everyone is fortunate enough to have these
Sorry if there is bad formatting, on mobile. Instead of doing Case when column = value Then something When column = other value Then something else End You can do Case column When value then something When other value then something else End Basically saves you the trouble of writing "column =" multiple times. The downside is that you can only invoke one column, and you can only check to see if that column is equal to another value, no other comparisons (greater than, less than, etc) are allowed.
Thanks! This worked well! Is there a reason why to insert the "AS u" part?
That is the perfect response. :D
I agree! I wish I found them earlier
Thank you, I really like writing SQL posts because there is a practical use case. &amp;#x200B; I write over on [dev.to](https://dev.to), but looking forward to getting more involved here too
That does sound useful! That's a similar story to how I started using them, I needed to see the second occurrence of something and couldn't get around it any other way.
🥇poor man’s gold for you, sir.
Will do! Thanks for your help!
Sounds good, thanks for pointing me in the right direction!
Yeah everyone is not fortunate, but then you just get a visual list of your tables, next to each other. But that showing is still true, because your DB is, as it is.
Three months. For example, in first row we have period 2017-01-01 - 2017-03-01 (01, 02 and 03 months), so SumOfQuantity is 10+5+20=35.
Okay, I think I understand. You don't want to group by date, you want to group by month, i.e. 201701, 201702, and 201703.
yes, because if you leave it off, you will get an error message similar to MySQL's "Every derived table must have its own alias" not sure what the MS Access or MS SQLServer error message is
You can use LAG() to do this, in a weird way. Just lag once, then again on the same fields. The oldest lag would be your starting point, the newest lag would be the ending point. Add all of the lagged counts together, current month+lag 1 month+lag 2 month.
SELECT lagged.\[PatientID\],
\`SELECT lagged.\[PatientID\], lagged.LAG2\_TIME AS "START\_DT",\`
Wait. SQL server doesn't have custom functions?
You can't write custom aggregate functions https://stackoverflow.com/questions/4374709/custom-aggregate-function-concat-in-sql-server In Postgresql aggregate functions can also be used as window functions. You can define aggregate functions, or use extensions that add aggregate functions written in C++, and use them as window functions
Thanks again! In the table I actually was working on, just realized I had to join each of db1 and db2 to db3, so: SELECT [Item], SUM(Quantity) AS total_quantity, SUM(Amount) AS total_amount FROM (SELECT db1.[Item], CONVERT (decimal(18,2), SUM(db1.[Quantity])) AS 'Quantity', CONVERT (decimal(18, 2), SUM(db1.[Amount])) AS 'Amount' FROM db1 JOIN db3 AS db3 ON db3.[Item] = db1.[Item] GROUP BY [Item] UNION db3.[Item], CONVERT (decimal(18,2), SUM(db3.[Quantity]*-1)) AS 'Quantity', CONVERT (decimal(18, 2), SUM(db3.[Amount]*-1)) AS 'Amount' FROM db2 JOIN db3 AS db3 ON db3.[Item] = db2.[Item] GROUP BY [Item]) AS u GROUP BY [Item] will no longer work, how can I go around to still condense the lines that item is repetitive?
Replacing single quote with square bracket also gets me a column title of 'ot'. Other tests: '!!Hot' = Hot '.!Hot' = .!Hot !Hot = ot '\!Hot' = Unrecognized token
I'm using the newly released SSMS 18.1 and when I r-click on the Database Diagram folder under the database name, the only option is "New Diagram". I'm not seeing a "Create..." anywhere and that's what's been confusing me I think.
I don't see any reason that you couldn't accomplish that exact same thing in SQL with be actually, pretty similar syntax
Microsoft has two ways of licensing SQL: Server + CAL and Core-based licenses. When using Server+CAL, the number of cores on the server doesn't matter; the cost of the server license doesn't change. You're limited by the number of CALs you purchase with regards to now many users/devices can access the server. With a Core-based license the server license is more expensive, but you don't need CALs and can have any number of users/devices connecting to it. If you only need 20 CALs, then I suspect that a Server + CAL licensing model will work out to be cheaper than a Core-based license (please don't just take my word for that and get quotes for your own situation). Eventually, as your user base grows, the cost of additional CALs will reach a tipping point and a Core-based license will become the cheaper option.
in the SELECT and ON clauses in your `u` subquery, you refer to `db1.[Item]` and `db2.[Item]` and `db3.[Item]`, properly qualifying the ambiguous columns name by prefixing them with their table names you forgot to do this in the GROUP BY clauses
Does it have to be returned as !Hot or can you just return Hot and have the front end handle it? If !!Hot returns just Hot does doing !!!Hot or !!!!Hot change anything? I guess I'm out of ideas at this point other than using dyamic sql for the whole thing in order to create the column alias.
Thanks, found it with the right click. That does seem slower and even for a novice like me seems a lot harder.
I think what you're looking for is CONCAT().
I like to use dense_rank w/o a partition to group records. I work for a hospital dealing with claims and we need to track interment inpatient claims to final claim to make sure we're getting paid right. Inpatient claims sucks the most...
You be you but... I can't think of a more painful way to learn SQL than on a tablet. (OK, maybe using MS Access... maybe). You're going to be doing a lot looking at data and a lot of typing. So a lot of screen real estate and a real keyboard are going to be your friend.
It depends on what database you're using. Concatenating strings is done differently in different products.
Not sure what database this is but I’m trying to do this in something called clindex
so would i just insert it like this? rclm\_dem.AGE, rclm\_dem.SEX, CONCAT(rclm\_dem.RACE\_ASIAN,rclm\_dem.RACE\_NATIVE)? &amp;#x200B; I tried \^ the above and I just ended up getting an error
Also would Concatenating be the right command to be using in my situation? Wasn't sure and wanted to get some advice on whether this was the way to go or if there was also a better way to do this.
lag() and lead() are a lifesaver. Try finding a workaround for row-wise comparisons — maybe you can do it through a self-join, but in some cases there just is no workaround. I also use row_number() all the time to rank things within partitions.
Is this MSSQL?
I honestly couldn't say :\\. I was just given the opportunity to practice SQL at work with our database so I'm still relatively new to this. I can only assume it is?
How would you make a custom aggregate function in SQL server?
How is the information stored in each of those race columns? Y, N, Null? In that case, I don't think concat would help you as you'd end up with something like N N Y N.
Define custom and function? Do you mean an ACTUAL function, like a Table Valued Function, because the example was just a query with two aggregates
Agreed on the row_number () trick to find rankings. I use this all the time.
From the Data dictionary, All i see is the code 1-YES so It is safe to say that it is stored as Y,Null
I use them in many different ways, they're so useful. Once you learn how to use Window functions you'll never need to have a confusing Group By clause at the end of your query, and you'll be able to retrieve multiple aggregations from a single query. One example of how I've used them is this: I sometimes have to query against a price\_list table that has two rows for every product - LIST and NET. So, to get both List and Net price into a single row in the query results, I do this; &amp;#x200B; select DISTINCT p.ITEM\_NUMBER ,MAX(DECODE(price\_type\_code,'LIST', price,null)) over (partition by p.item\_id) list\_price ,MAX(DECODE(price\_type\_code,'NET', price,null)) over (partition by P.ISBN) net\_price from ITEMS p, PRICES pp where p.ITEM\_ID = pp.ITEM\_ID and p.ITEM\_NUMBER = 'X'
Like a monkey with a miniature cymbal?
My first thought is that when you start dealing with the aggregate data, you'll want a top-up/fill-up record to have a slight bit of information about the previous record, i.e. the mileage. You could either alter your table structure and application to calculate and store the Distance Traveled Since Last Refueling, or you could use the LAG() window function to gather and calculate this info for your fuel economy query. Either way, it then seems like km/l is the sum of Distance Traveled divided by the sum of Liters Purchased. This is just a conversation starter... we can chat about actual SQL once I know I'm on the right page.
You’ll just need to cast the height decimal field to a string, then you can concatenate the 2 together. Depending on what database you’re using there will be different functions you have to use. Just know that this means you can’t do numeric aggregation with it if you convert it to 1 string field, i.e. no averages, sums, etc.
If it’s always in this specific format, you can use a regex match/replace. I’m too lazy to figure out the exact code you’d need, but I’d start looking at that if I were you.
I think what you're looking for is a conditional statement. CASE WHEN rclm_dem.RACE_ASIAN = 1 THEN 'ASIAN' WHEN rclm_dem.RACE_NATIVE = 1 THEN 'NATIVE' WHEN rclm_dem.RACE_BLACK = 1 THEN 'BLACK' WHEN rclm_dem.RACE_WHITE = 1 THEN 'WHITE' END AS Race
I'm not familiar with Postgres but if it has windowed functions, one approach could be to just identify the first and last fillup through row numbers, as below. Then the math just becomes a matter of excluding that last fillup from the sum of liters purchased. SELECT a.id, a.license_plate, MAX(case when b.fill_no = 1 THEN odometer end) ODO_START, MAX(case when b.rev_fill_no = 1 THEN odometer end) ODO_END, SUM(case when b.rev_fill_no &gt; 1 THEN liters_purchased else 0 end) FUEL_USED, SUM(case when b.rev_fill_no &gt; 1 THEN liters_purchased else 0 end)/ NULLIF(MAX(case when b.rev_fill_no = 1 THEN odometer end)-MAX(case when b.fill_no = 1 THEN odometer end),0) KM_PER_LITER FROM vehicle a LEFT JOIN (SELECT vehicle_id, timestamp, liters_purchased, odometer, ROW_NUMBER() OVER(PARTITION BY vehicle_id ORDER BY timestamp) fill_no, ROW_NUMBER() OVER(PARTITION BY vehicle_id ORDER BY timestamp DESC) rev_fill_no FROM fuel_top_up WHERE timestamp BETWEEN 'date1' and 'date2') b ON a.id = b.vehicle_id GROUP BY a.id, a.license_plate;
Interesting take, I'll think about that one some more. The only issue I see is that the sum of liters purchased needs to exclude the amount purchased in the very last fill-up as it's only relevant for future km traveled.
&gt;CASE WHEN rclm\_dem.RACE\_ASIAN = 1 THEN 'ASIAN' WHEN rclm\_dem.RACE\_NATIVE = 1 THEN 'NATIVE' WHEN rclm\_dem.RACE\_BLACK = 1 THEN 'BLACK' WHEN rclm\_dem.RACE\_WHITE = 1 THEN 'WHITE' END AS Race I think thats one part of what I'm looking for! So currently my table looks like this &amp;#x200B; |subjid|race white|race asian |race black| |:-|:-|:-|:-| |001-001|Yes||| |001-002|||yes| |001-003||yes|| and I'm trying to get it to look like this to reduce the amount of columns I have. &amp;#x200B; &amp;#x200B; |Subjid|Race| |:-|:-| |001-001|White| |001-002|Black| |001-003|Asian|
You also need to accommodate partial fill-ups and fill-ups that didn't get recorded.
Custom aggregate function. It's pretty clear what it is. Imagine an aggregate that returns a concatenated string composed of all the distinct strings. Or an aggregate that returns the first non null value, or a fast distinct count using HyperLogLog or something. I linked that page just for the answer: mssql doesn't support custom aggregate functions.
Why not use the LIKE operator and use a single character wildcard for example: CASE temp_restricts.restrict_abbr WHEN LIKE '_hot' THEN temp_restricts.restrict_abbr ELSE NULL END AS '!hot' I'm not familiar with SQLite so not sure if the 'LIKE' is in the appropriate place for syntax. I'm also not sure if there is any characters before or after the "!hot" or "!cold".
Fill-ups that don't get recorded would be a data quality issue and I'll have ways of enforcing that from a management perspective. The partial fill-ups should still be accounted for properly as the odometer wouldn't have increased a great deal between that new fill-up and the previous one (and the number of liters can't exceed the capacity of the tank which remains constant throughout the vehicle lifespan)
I'm not sure about that... if I fill up, drive 200 miles, and then have to put 6 gallons in, those 6 gallons reflect how I drove for those 200 miles (apologies... I'm speaking 'murican). If I drive like a bat out of hell, I'll have to put more than six gallons in. The amount I fuel up with has very much to do with how I drove before that fillup. Am I missing something? The coffee \*has\* worn off...
It should work then. SELECT rclm_enroll.SUBJID AS 'Subject ID', CASE WHEN rclm_dem.RACE_ASIAN = 1 THEN 'ASIAN' WHEN rclm_dem.RACE_NATIVE = 1 THEN 'NATIVE' WHEN rclm_dem.RACE_BLACK = 1 THEN 'BLACK' WHEN rclm_dem.RACE_WHITE = 1 THEN 'WHITE' END AS Race FROM rclm_enroll JOIN rclm_dem ON rclm_enroll.SUBJID=rclm_dem.SUBJID
Yup, Postgres has window functions...Good stuff.
I'm winding down for the day so I'm probably feeling/thinking similarly to you right now :) The way I was looking at it was that when you buy fuel, you're buying it to power future trips, not to make up for previous ones. &amp;#x200B; As an example (let's leave the date out for the sake of this hypothetical): Fill-up 1: Gallons purchased = 5 Odometer = 10,000 miles &amp;#x200B; Fill-up 2: Gallons purchased = 2 Odometer = 10,150 miles &amp;#x200B; What we're really looking to figure out is how far those 5 gallons from Fill-up 1 took you. In this case, the odometer difference is 150 miles and we purchased 5 gallons to power that trip. The fuel economy between Fill-up 1 and 2 would then be 150/5 or 30 mpg. Note that we're not taking into account the gallons purchased in Fill-up 2 as those gallons will apply when calculating the fuel economy between Fill-up 2 and an eventual Fill-up 3.
This is where my lack of SQL knowledge really becomes evident! :) I'll read up on window functions tomorrow so that it makes more sense to me
This data model feels broken to me.
It worked!!! Thanks for the clarification on what I needed. I also had an additional question. So lets just say in the future I have some person who says that they are of multiple ethnicity &amp;#x200B; |SUBJID|White|Black| |:-|:-|:-| |001-001|Yes|| |001-002||Yes| |001-003|Yes|Yes| How would I get both of them to show in this instance? Because with this Case command it will only show the first one. Say I execute the case command, then my table would look something like this: &amp;#x200B; |SUBJID|Race| |:-|:-| |001-001|White| |001-002|Black| |001-003|White|
Select distinct replace(left([value],instr('&gt;',[value])),'&lt;','') where [value] like '%&lt;%'
It seems really weird to me as well, but this is all I got so far for real world-practice. I've been using this data base + info on w3schools.
At least make some effort.
I use this from time to time for a variety of purposes, but never to give a user the ability to run it. Will the historic data change historically? If not, why not expand your scope a bit and run the loop for all years, and all products, and export the data to a table. Then moving forward you would leverage your stored procedure once a year (or whatever) to get the most recent years information, and add it to the table. Then users will be pointed to that table and can get all the data they want without actually being able to send a LOOP. My concern here being that multiple users try to run it at the same time, which might have some impact on using an #table, or having it dropped/shared by multiple users to get bad data, overload the server, etc.
&gt; which might have some impact on using an #table, or having it dropped/shared by multiple users to get bad data, overload the server, etc. #temp is connection specific, ##temp is global. You can see it yourself by opening 2 different query windows in SSMS.
Yes, it is, but I don't know how PostGres works and in the past I've had some issues with loops and #/## tables and overlap between concurrent users. Come to think of it, might have only been with ## tables.
Thanks for the response. A table **does** make more sense but I've been reluctant to build one at my current organization -- they've had a slew of data analysts build bad reporting tables without any details about their scope, load date, update processes, etc. (Classic documentation problem). I've been trying to keep things a bit more self contained \[even if potentially slower\] until the organization can make a real effort to understand what has been produced vs what is being used. If I recall, all #temp tables are #temp+...id.... per individual session connections so there shouldn't be a dirty read scenario, but I'll confirm this. I hadn't considered it. I was using the #temp structure to appease the reporting structure, which wants the column name for its definitions. I've also looked into a recursive-CTE as an option, but can't get it to run for more than two years (the "Anchor member" and then the first "Recursive member"). I'll keep at it.
&gt; but I'll confirm this. I'm pretty sure it's correct for MS SQL, but don't know anything about PostGres which was why I mentioned it. In general though I have used similar types of loops, just always dump them into a table and have them saved as sprocs which run on some kind of scheduled basis. If your doing this on a yearly basis, and the data can't change for past years, then it just doesn't make sense to me to let a user run a loop when requesting the report, however, if it only takes a few seconds then it might make sense.
&gt;I'm pretty sure it's correct for MS SQL, but don't know anything about PostGres which was why I mentioned it. No worries, better safe than sorry\^\^ &gt; If your doing this on a yearly basis, and the data can't change for past years, then it just doesn't make sense to me to let a user run a loop when requesting the report I'm investigating at the moment if the data can change - I'm fairly certain it *cannot*, so a table might be the final outcome - because as you've said - why have a user loop the same data over and over and over and over instead of just read from the table.
Reformatted previous message to make it more readable and added the tests you suggested. It appears I will need to rework the data after query to add the ! or create a different naming convention.
Use a numbers table to make the last x years, then join and group on that. Something like below, but I don't know your tables. SELECT ProductDescription as [a_Product] ,d1.[a_FiscalYear] ,count([FactID]) as [a_CountedValue] ,count(distinct DIMID) as [a_UniqueCount] FROM [DB].[DBO].[FACT] F join [DB].[DBO].[DIM] P on P.ProductID = F.ProductID right join (select year(dateadd(yy, [number] * -1, getdate())) as a_fiscalYear from ( select distinct number from master.dbo.spt_values where number &gt;=0 and number &lt; @cnt) as d) as d1 on productfiscalyear = d1.[a_FiscalYear] ​ where ProductType = 'P' and ProductFiscalYear &lt;= year(dateadd(yy, @cnt, @maxfiscalyear)) and (ProductStatus = 'Active' or (ProductStatus in ('Canceled','Terminated' and TerminationDate &gt;= dateadd(yy, @cnt, @maxfiscalyear)))) ​ group by ProductDescription, d1.[a_FiscalYear]
Even if it can change then I would just truncate the table, and repopulate on a monthly basis. In the insurance industry past data can change sometimes because of how long it can take to settle a policy. Generally it would only do this for the last year. In this narrative you would run your loop for all years &amp; products, etc., then modify the code to put it into a stored procedure which only runs for the last year or two years or whatever, then have some logic to delete those rows before inserting the new ones. It sounds like your company needs to come up with some better schema choices. For example we might use a schema like this: 1. ssis.Tablename - Raw data coming from SSIS 2. finance.Tablename_Descriptor - The data coming in from SSIS is financial in nature, so it has a finance schema + the name of the Table, and the descriptor might be something like _PricingChild, or whatever. 3. tab.TableName_Descriptor - The data here is for Tableau consumption, and connected to a report. This is an over simplification and in reality we have a few other ones in between for staging, etc., but whenever we aren't sure if something is being used anymore we can login to Tableau server, see which reports are inactive, look at their datasources, and then generally we'll rename the tab.Tables and with an affix such as _DELETE. If 30 days goes by and no one complains, we will proceed to delete it, but back up the sprocs, views, etc., which were being used to drive the logic.
What is instr?
In order to consistently calculate fuel economy, a user will have to fill the tank at each fueling. This is the only way to capture how much fuel was burned.. the fuel tank capacity is a constant, so refilling it shows how much fuel was used. While it is true that the actual atoms of fuel you're putting in the tank will be used for future trips, the quantity that you put in reflects how much you used between the previous fill up and the current one. This is due to the aforementioned unchanging capacity of the fuel tank. In your example above, yes, with your first fill up you purchased 5 gallons, but only two gallons were used to power that trip. How do I know? Because the tank was full after you put 5 gallons in on the 12th, and you were able to make it full again with two gallons at your second fill up. Filling the tank at each refueling is the only way to figure out how much you've used. Unless you have a fuel flow meter, there is no other way to calculate consumption. &amp;#x200B; PS - you got a pretty amazing 75 miles per gallon.
Investigating. Produced different results for the count() columns, but the looping worked. Performance WAY better though. Might need some tweaking in the where clause - which also has to loop because the product termination dates are shifting per year.
How does this loop exit? I'm not seeing it... WHILE @cnt &lt; 0 **and @cnt &lt; 19** ?
I'd say 90% of the times I've used window functions it's using the ROW_NUMBER() function to either: (1) get the first/last instance of something (2) get rid of duplicates
&gt;In this narrative you would run your loop for all years &amp; products, etc., then modify the code to put it into a stored procedure which only runs for the last year or two years... Yeah no problem understanding this - I'd probably end up with an SSIS package or SQL job to take care of running it for me. &gt; better schema choices You mean .dbo. isn't the only one? /s Yeah seriously - we have .crm. (data from the crm) and .dbo. (literallyeverythingelse). On top of that their DB names don't make sense. It's easy for me to criticize, but I'd like to actually help them organize, and am currently leading this project. This is a great suggestion - I think we could use the .ssis. at the very least with very little effort. &gt; affix such as \_DELETE I initially suggested this when I started in April, but they told me that far too many things are run on an annual basis, so we'd need to wait a year to find out. Didn't seem worth pursing that point. &amp;#x200B; Appreciate the thoughts a lot.
Yep! Dad taught me this one growing up. Just write the current mileage on the receipt for this fill-up. Now you know your MPGs.
&gt; **and @cnt &lt; 19** Where'd you get 19? Maybe it was " @cnt parameter from 1-20 ?" \[I meant that as 1 'through' 20\]. Sorry for the confusion. &amp;#x200B; The looping occurs here: SET @cnt = @cnt +1 @cnt is provided by the report (as a parameter) \[and technically, it's a negative number "number of years back"). When I run the script in SSMS I declare it. DECLARE @cnt INT = -1 --or some number between -1 and -20 0 is simply meant to prevent them from going forward in time (e.g. FY2021).
What do they mean by "run" on an annual basis? You mean sprocs? You can write a script to look through those to see all the ones which touch a specific table.
Oops. Missed the tag. Instr finds a specific string in a field and returns the position if found. I'd imaging MS SQL has an equivalent.
If you provide me with a sample of each of the tables you will be using, I can show you some sample code
&gt; What do they mean by "run" Most of the reports are direct queries, rather than stored procedures, usually a combination of table queries and business logic. Some of these reports point to relatively specific tables (with sometimes unknown origin, either because it was loaded with an import/export tool, rather than SSIS, or because the SSIS package isn't scheduled). So appending \_DELETE to the tables of course fails the reports (as we'd want), but the perceived issue is no one is running the report except annually (if ever). I built some report usage queries, but they currently configured to only 60 days of history -- I need to get back to that process to commit the log every 30 days or so and then build out a full history, but got pulled away from it. Maybe there's a usage query (or dependency query) for table-based queries, rather than stored procs/views?
As a general rule I try to only make reports that are select * from view/table. Really makes maintaining and updating the code base a lot easier. &gt;but the perceived issue is no one is running the report except annually (if ever) Design a logging procedure and if you insist on keeping the logic in the report, then modify the report to insert a row of data whenever its run. This won't solve your problem today, but it will give you some answers in a year. Year long projects can be good for your resume. Show strategic thinking, etc.
&gt; if you insist on keeping the logic in the report Not me, previous dev's no longer with the org. &gt; Design a logging procedure I'll prioritize this again - I was about 50% of the way through a project encompassing this, reporting usage, and SSIS usage (plus our DW refresh details, ie number of packages, time it took to run, rows, etc). &amp;#x200B; A big part of my job is transitioning towards best practices. The WHILE loop seemed to be in contrast to that - so I wanted this sanity check that ehhh, maybe it's not the best solution (even if it works). &amp;#x200B; So much of what was previously developed was customized, undocumented, without requirement, and often outright wrong. ^(I am our janitor.)
In general a loop is not a best practice. Having said that, I do leverage them regularly, and for many things they are a necessity. I'm not going to UNION 10, 20, or 100 queries together. I'm going to write a loop.
I agree, that sounds very silly. I just don't see the value of recording what records weren't changed, and if you ever need to audit what this process is doing then identifying the changed records is going to be the first step in any case. The only value I can see is if there was no kind of logging on your original data, in which case having a snapshot of the state of the data afterwards might be useful to establish that a record existed at the time but wasn't altered. But obviously if that is the case that's a whole different problem that should be fixed.
You could always right out the rest of the conditionals with AND statements. SO I'd look like CASE WHEN rclm\_dem.RACE\_BLACK = 1 AND WHEN rclm\_dem.RACE\_WHITE = 1 THEN 'White, Black' Etc. &amp;#x200B; Or you could Unpivot [https://docs.microsoft.com/en-us/sql/t-sql/queries/from-using-pivot-and-unpivot?view=sql-server-2017](https://docs.microsoft.com/en-us/sql/t-sql/queries/from-using-pivot-and-unpivot?view=sql-server-2017) and then use a concat
Moving my production environment from 2008 R2 to 2016 in the next few weeks. Looking forward to cleaning so much unnecessary code once I do.
Thanks, I had replaced it with CHARINDEX too
What's the point of the window function in that case? You can easily rewrite it as: SELECT p.item_id, MAX(p.item_number) item_number , MAX(CASE WHEN pp.price_type_code = 'LIST' THEN price ELSE NULL END) as list_price , MAX(CASE WHEN pp.price_type_code = 'NET' THEN price ELSE NULL END) as net_price FROM items p JOIN prices pp ON p.item_id = pp.item_id GROUP BY pp.item_id (using normal joins and `CASE` instead of `DECODE` because standards) It will perform an order of magnitude better, needing one sort and no segments. If you use a DISTINCT with window functions, you're most likely doing what could be done more efficiently with a `GROUP BY`.
Did it solve your problem?
not really, i can't get the syntax right. It is saying that the 'a' column does not exist.
The Show Criteria Pane of what? What tool are you using?
Yikes! Sorry!
SSMS
[Here I made you a fiddle](https://www.db-fiddle.com/f/qdzAYGbaJZjMu4s5urxK1t/0)
Write out the query. Don’t screw around with query “designers” as they only work for trivial cases.
Can you run the nested section by itself?
That's great! I'd not thought of how to use them nested like that. Very useful :)
Yes, i can run that part.
This is a lot simpler than you (and many in this thread) are thinking. ([This guy](https://www.reddit.com/r/SQL/comments/cew91g/question_tough_sql_use_case_calculating_fuel/eu5p00o?utm_source=share&amp;utm_medium=web2x) articulates it pretty clearly clearly). &amp;#x200B; I simplified the sql fiddle that [this guy](https://www.reddit.com/r/SQL/comments/cew91g/question_tough_sql_use_case_calculating_fuel/eu5y8g5?utm_source=share&amp;utm_medium=web2x) provided here: [https://www.db-fiddle.com/f/ei9tEXCEV3V7xNhiUT9giH/0](https://www.db-fiddle.com/f/ei9tEXCEV3V7xNhiUT9giH/0) &amp;#x200B; (It should really accept StartDate and EndDate as input parameters, but I wasn't able to figure out how to do it in SQLFiddle.)
That would limit each dish to a single allergen...
Yes, a column can be both a primary key and have a foreign key on it (Protip: you can even span them across multiple columns)
Typically, you'd model this as an intersection table. &amp;#x200B; You have the Dish table with the list of dishes and a PK of DishId, an Allergen table with one row for each type of allergen, independent of what dish they're in (only one row for peanuts, for example) and a PK of AllergenId, and then the table showing the relationship between the two with a two-column PK of DishId, AllergenId. Each of the two columns is also defined as a FK to the reference tables. &amp;#x200B; There \*are\* some uses for a table with a single-column PK that comes from another entity, but those would be typically performance or space optimizations for circumstances where the main entity has elements that are only sometimes populated. For example, if you had a table for hospital staff, you may have a separate table for the extra details of staff who are doctors, where you capture their license number, school they went to, specialty, etc. You could model that with a single column that is both PK and FK to the staff table. That model can cause issues longer-term with maintenance and data structural change, so it typically makes sense to just put all that in the staff table and only populate it sometimes.
Correct. Expand your key to include both attributes and you are good to go. `PRIMARY KEY (DishID, Allergen)`
Howdy. Think of SQL as a command prompt from the DOS days. So doing a `SELECT * FROM Table` is just like doing `CD... TableName DIR`, and it will list all the records in the table. Think of a table as a tab in Excel. VLOOKUP's are JOINS, and JOINS are much more powerful. Anyway, you can do shit just like you did in Excel, for example `SELECT *, ColumnName + ColumnName2`. You can also do things like GROUP BY which is like doing a SUM of a column, or a COUNT. You can write small snippets of things together, and then JOIN those together to approximately do everything you can think of in Excel. It's very easy to learn, just Google, "How do &lt;thing&gt; in SQL from Excel" and there is generally a post that covers it. Download and install MS SQL Server, get a copy running on your local disk, import some data, and start playing with it. There are YT videos that cover free datasets and basic operations.
Thanks. I need a crash course.
Google Northwind database. Figure out how to get MS SQL Server on your computer, figure out how to import the data. That alone is going to be a real pain in the ass, but there are plenty of resources that cover those specific things. All of this is free. Once you have a database server on your computer, and a copy of SSMS installed so you can query it, and all the data imported to it... then things will become a lot simpler. Understand that you won't (necessarily) have to do things like set up a server, import data, etc., but you might one day, and you need to understand how those things "look" -- plus its necessary to get the data on your computer in a way that you can query. Then at that point, things will become a lot simpler.
SQL is the language of databases. It can do pretty much everything.... build, modify, and delete structure. Read, write, and change data. Different db's have different dialects, but they're incredibly similar and picking up a new one is pretty easy. It's also fairly easy to learn at a useful level. But with a great deal of depth if you want to keep learning. If you want to work with data, SQL is a basic skill.
https://i.imgur.com/YhnzbcC.png
Is there a way to grab that image on mobile? I get just a bigger blurred image. I’ll try desktop later.
I find it helpful to think of the set of "columns" of a "table" as forming a predicate. Let's make a table, and call it `person`. Cool. Now let's make 4 columns within our `person` table, `{id, first_name, last_name, dob}`. Awesome. One possible predicate for the above set of columns could be: &gt;A customer, uniquely identified by `id`, has a first name of `first_name`, a last name of `last_name`, and a date-of-birth of `dob`. This probably seems obvious to you. Good. You said you had Excel experience. Awesome. I assume you are familiar with deriving new values from existing values, yes? Good. ***Deriving new predicates from existing predicates is the chief reason for SQL's existence.*** With our simple and singular table, `person`, we can ask, and answer, at least the following predicates: 1. How many of our customers have the same name? 2. How many of our customers were born before the year 2000? After 2000? 3. Show me the number of customers born in each decade, starting from 1900 up to right now, whose last name isn't "Smith". And easily dozens more. As to your exact question, "what SQL actually is???"... "SQL" is a word. That's what it "is".
Can't believe noone has actually defined it, but SQL (Seek-well) is the Structured Query Language, a high level interpreted programming language for RDBMS (relational database management systems) processes. 'Data' here is like Excel sheets, rows &amp; columns, instead of 'cells' you have 'records' in 'tables' instead of 'sheets'. SQL has some great tools for how different tables talk to one another. Classic examples are student/class or client/billing or product/grouping. It allows you to connect relevant data together and do something with those results (usually feeding something else, like an application, an excel sheet, a visualization tool like PowerBI, Tableau, or Qlik), or just spit out in a text document.) It's what makes the data world go 'round.
At the very basic, SQL is declarative. You ask the engine for what you want, you get it back (if you declared it right.) SELECT * FROM foo where bar = lastActive
That's extremely helpful, thank you! &gt; a two-column PK of DishId, AllergenId. Each of the two columns is also defined as a FK to the reference tables. How would this look in SQL?
Sometimes a google search is better than a reddit post.
“The quest for knowledge should not be limited to one singular source” - me I did a google search but was hoping for some insight which many of the people here gave, and I thank them for it.
sqlbolt.com was a good starter for me. I think it's based on MySQL, so there will be some syntactical differences from MSSQL (which is what I use at work), but it's a great way to get your feet wet.
+1 for this - much easier for a crash course than trying to get a SQL server setup and managing all of that.
Teamtreehouse has a SQL tutorial that I found really helpful. I'm sure YouTube has plenty of stuff as well.
You have a lot of good information here. SQL is a database programming language. Like mentioned think of database tables like an Excel sheet. here are some examples of basic SQL tasks. You can SELECT data from a table based on what you are looking for: SELECT LastName, FirstName, Phone, Email FROM TableName WHERE LastName = ‘Smith’ This will return the data from the 4 columns (LastName, FirstName, Phone, Email) where the LastName is equal to Smith. You can join two tables by linking them on a column in each table that contains the same data: SELECT TableName.LastName , TableName.FirstName , TableName.Phone , TableName.Email , TableName2.JobTitle , TableName2.Manager FROM TableName JOIN TableName2 on TableName2.IDColumn = TableName.IDColumn WHERE TableName.LastName = ‘Smith’ This will return the data from the 6 columns (LastName, FirstName, Phone, Email, JobTitle, Manager) where the LastName is equal to Smith. The two tables have the same column IDColumn that contains the same data. You have to specify the table.column so that SQL knows where the columns reside. INSERT INTO TableName (LastName, FirstName, Phone, Email) VALUES (‘Flanders’, ‘Ned’, ‘374-294-7338’, ‘NFlanders@gmail.com’) This inserts a new row or record into the TableName table. UPDATE TableName SET LastName = ‘Smith’ WHERE LastName = ‘Flanders’ This will change or update the LastName data from Flanders to Smith for all records in the TableName table where the LastName column is Flanders. DELETE FROM TableName WHERE LastName = ‘Smith’ This will delete all rows or records from the TableName table where the LastName column is Smith. These are just a few simple examples. Things can get a lot more complex. Hope these help.
Such helpful people in this thread. It’s really nice to see.
That’s why I posted the question rather than just relying on a google search.
Everyone's pretty helpful here with explanations, but if you actually want to learn SQL, this is the course I used to give to the newbies I hired with zero experience: https://www.codecademy.com/learn/learn-sql It's super straightforward, free, and you can grind through it in a weekend if you want. If you're honest with your interviewer and tell them that you grinded though a codeacademy course and taught yourself the basics of SQL on your own, it would be a huge statement about your initiative and ability to pick up new skills quickly.
I haven’t seen anyone mention why you would want to use it. Why not just save your data to an excel file or a CSV file? This is where sql is great, or really any database system. It’s meant to handle large amounts of data (terabytes) and complex relationships among this data. There has been a lot of hoopla with software in the last few years of people trying to use what they call NoSQL databases but a lot of the time you really can’t beat it. It’s been doing this job quite well for 40 years if I remember correctly so it’s pretty widely used. My guess is that if your familiar with data like excel then you should be able to pick up the basics pretty quickly, but it can get quite complicated so don’t let that over whelm you.
Since you have a many to many relationship I would think that you need a linking table in between. So both the Dish ID and Allergen tables would have their own PK series, then you'd have a third table made up of two FK's for the two tables as a composite PK. To query you'd join on [dishid.pk](https://dishid.pk) = [relationship.dish](https://relationship.dish) and [allergen.pk](https://allergen.pk) = relationship.allergen where dishid.description = whichever dish
I found Socratica's YouTube series very helpful: https://www.youtube.com/playlist?list=PLi01XoE8jYojRqM4qGBF1U90Ee1Ecb5tt Khan Academy also has a course on SQL: https://www.khanacademy.org/computing/computer-programming/sql
Kahn academy has a great SQL intro. Maybe 3 hours and you'll have the basics. It doesn't take that long, but it's good to have some extra time to practice on your own outside the course. SQL is kind of like Excel formulas. If you can do Excel, you can do SQL.
In Data Analysis, SQL is mostly used for either pulling the data efficiently and providing summary tables and views. It's possible to do run of the mill analysis, although typically people work with a language like R for further analysis.
A lot of folks already explained what SQL is, but just putting my cents here, it would be great if you can look up some information about relational database design stuffs like what primary key, foreign key are, the different types of relationships among tables (one to many, one to one, many to many...), Knowing the SQL syntax would not be enough, because understanding the table structures will make querying much much easier. Search: entity relationship diagram, if you can draw one of this, you ar good to go! Good luck!
You learn faster if you go and use it somewhere online. Google. Practice SQL online. Its really simple language. To get you started... SELECT clause, is a list of columns you want to see FROM clause, is the table where the columns live in. WHERE clause, is the filter. Like dates..or customer name, etc
For me they did, but it seems that a lot of people don't like to use them for visualizing JOINs.
You could Google this 10x faster than writing here!
&gt; (Seek-well) Surely it's seek-will?
I think the venn diagram method is ultimately confusing. If I could offer a critique of your approach though, it'd just line up the matching values to make it very obvious visually what is going on, e.g., order_item | inventory_item :--|:-- Nail | Nail Hammer | Hammer Screws |Screws Pliers | Blow Torch | |Tool Cabinet | Nail Gun
W3 schools has pretty good beginner exercises
https://sqlbolt.com/ is a good place to start.
&gt; (Seek-well) &gt; &gt; Surely it's seek-will? I usually hear it more like seekwull, with the emphasis on see.
That still doesn't make sense. How can u know after fill up 1 that you used 5 gallons? In your little example you actually went 150 miles on 2 gallons, because you were at full, drove 150 miles, then put in 2 gallons to get back to full. To actually calculate MPG u have to do so after you fill up again from driving. &amp;#x200B; That's why the simplest way to record your MPG is to reset your trip odometer at a fill up, drive. then see how many gallons you put in the second time and divide to get miles driven per gallons used. if u just fill up and count your gallons and then drive an arbitrary distance before filling up again it doesn't actually reflect any useful kind of data. &amp;#x200B; Your looking at the data starting from 0. meaning 0 + 5 gallons = 150 miles (30mpg). when in reality gas starts at 100% full and you subtract to get your gallons used. 100% - 2 gallons = 150 miles (75mpg)
I'm with you OP. I'd probably use EXCEPT in this instance to capture the new / changed rows. SELECT * FROM dbo.Source EXCEPT SELECT * FROM dbo.Destination Of course, if you've got some column mismatches that'll need resolving by specifying the correct list of columns.
You know vlookup in Excel? Well it's horribly slow on larger data, as is the entire Excel. Let's say you have 10 million records with 100 columns. Excel will be hit hard by this, but for a database engine this is normal work. Instead of vlookup, you do a simple, compact JOIN statement, and from multiple tables you get exactly what you need the way you need it, extremely fast. You can't manage each cell individually like you do in Excel, but that's not necessary in databases, where each table has its own structure and data. Do, SQL will let you easily define what columns or expressions you want to see, from which tables, how they are joined (vlookup basically), what filters to make, and what columns to group by if you're aggregating data (sum of revenue grouped by day). You can even select from the results of other selects so you can see which days performed better than the average for example. It's the most powerful language by far, in data processing. Nothing is faster, simpler and with so many features as SQL is.
"Sequel" is how we say it in these here parts.
Obviously it's sea-quille
You are not the only one who thinks Venn diagrams are not suited for that: [https://blog.jooq.org/2016/07/05/say-no-to-venn-diagrams-when-explaining-joins/](https://blog.jooq.org/2016/07/05/say-no-to-venn-diagrams-when-explaining-joins/)
&gt;"SQL" is a word. That's what it "is". Actually "SQL" is an abbreviation ;)
&gt;Download and install MS SQL Server, get a copy running on your local disk, import some data, and start playing with it. Note that SQL Server ignores several very basic syntax rules from the SQL standard. I don't think it's a good choice for educational and learning purposes.
&gt;Thanks. I need a crash course. There are several interactive tutorials that don't require you to install a relational database on your computer. E.g. * [https://sqlzoo.net/wiki/SQL\_Tutorial](https://sqlzoo.net/wiki/SQL_Tutorial) * [https://pgexercises.com/](https://pgexercises.com/) * [http://www.sqlcourse.com](http://www.sqlcourse.com) &amp;#x200B; The site: [http://www.postgresqltutorial.com/](http://www.postgresqltutorial.com/) also has a good introduction to the SQL language (and as Postgres sticks very close to the SQL standard, it's also a good place to start learning the language)
I would recommend another database to be honest. PostgreSQL is open source, is used across platforms, and is very sought after at the moment due to licensing costs from the likes of Microsoft and Oracle. Although to learn SQL, you could probably just use SQLlite.
Which DBMS product are you using? In standard SQL you can use extract(year from current_date - interval '1' month)
ReportServer works quite well with Postgres: [https://reportserver.net/en/](https://reportserver.net/en/)
Are you looking for a UNION maybe? select * from (values (1,2),(3,4) ) as one(a,b) union all select * from (values (6,7),(8,9) ) as two(x,y) Returns: a | b --+-- 1 | 2 3 | 4 6 | 7 8 | 9 Or do you want something like this: a | b | x | y --+---+---+-- 1 | 2 | 6 | 7 3 | 4 | 8 | 9
No, MySQL neither supports INTERSECT nor EXCEPT (that is unrelated to the SQL client you are using)
While that is a good advice, it won't help because MySQL simply doesn't support that.
Thanks for the feedback, I'd like to improve the visual if it's not crystal clear, which join are you referring to that needs matching up?
Wow, that post is from 2016, I wonder why Venn diagrams are still so prevalent? &amp;#x200B; Thanks for the link, that was a good read :)
How much you wanna bet the job he's applying for uses it?
I'll accept your interpretation: I'm a New Zealander. We get a little weird with our vowels.
Nothing, because we don't know anything about the technologies used there - the question only refers to SQL, nothing else. At lest he/she should understand that "SQL" is not a specific database product. And that "learning SQL" means something different than "learning Microsoft SQL Server"
1. Do you have the database for it ? That looks interesting, I know what he does but I wanna make sure
I don't. It's a sample question to see if you can read SQL. The other questions have a test environment that I'm fine with because I can check and change things to make sure I'm correct. The verbal brief was it's for a hospital
I mean presenting the before-join state as that, rather than as order_item | inventory_item :--|:-- Nail | Nail Hammer | Hammer Screws |Screws Pliers | Tool Cabinet Blow Torch | Nail Gun
Ok... I'm still looking at it to find an explanation for it... It doesn't look that difficult tho lol I hate to be stuck on simple things like that smh
Bruh... Excellent analogies.
And it turns out your question started a good thread anyway, so my bah humbuging was wrong.
Sql is just a language for extracting specific information from a relational database. A relational database is something ~like a series of excel spreadsheets related to one another, called tables. Every table has a primary key that uniquely identifies each row. Often, the primary key in one table is a foreign key in another. Meaning that when values in one table based on attribute match that of another, we can retrieve these records based on this conditionality being met. This let’s us minimize redundant information storage and retrieve only the relevant data we want when we query the database, which is important. If we had one big mega table, we’d have to read everything into memory at once, which isn’t efficient on your computational resources. Everything else in sql and relational databases is related to the above concepts.
Australian and just as confused. These Americans are odd.
[codeacademy.com](https://codeacademy.com) [khanacademy.com](https://khanacademy.com) just go through their basic entry SQL classes and you should get the gist pretty quick. SQL basics might be the easiest to grasp of any language. You'll be fine.
Hello! In case no one posted, i have learned SQL from www.sqlbolt.com! Try it!
By "word" I mean symbol-sequence, of which an "abbreviation" is a proper-subset of. So, yes, from one level of abstraction, "SQL", is a word.
SQL is composed of DML, DDL, and DCL. DML is Data Manipulation Language. Querying a database is requires working with the DML. At the most basic level, SQL is simply information retrieval in a simple, quick, and repeatable manner. For basic data analysis, you are working likely only working with the DML. DDL is Data Definition Language. DDL defines the structures that hold your data, so tables, and the substructures that give your data meaning, integers or variable character columns or constraints. Indexes also fall into this, but indexes both hold data and provide meaning. DDL can simply be though as the rules of data. DCL is Data Control Language, permissions, but I would not worry about this when you're starting out. I rarely work with the DCL. In a larger organization, the DBA or some other person in charge of allocating access to resources will likely work with this and not the person doing analysis.
\*Acronym
? 1. What is this query designed to do? frustrate the shit out of someone who can't read an entire SQL statement on one line
you forgot to mention which SQL you're using MS Access?
I'm not using MS Access, but after downloading SQL (using the following video: [https://youtu.be/WuBcTJnIuzo](https://youtu.be/WuBcTJnIuzo)), I used the SQL workbench to create the database.
Oracle Apex is an easy way to do this. Online tool that gets rid of the hassle of creating databases and all that junk on SQL developer but still let's you import data and practice queries
Making my life easier, thanks
I find them very useful even for non trivial queries where there is a very long select list but I still don't want all the columns or there are several objects being joined. Not to use the joins produced by the designers but to have all table/column names correctly spelled in my query so I don't have to worry about spellings.
You have MySQL. In case someone asks again.
 Select Main.DirectorName, Main.FilmReleaseDate, Main.FilmName, Main.FilmBoxOfficeDollars, Main.TheFilmRank, Counter.Filmcount From MyFilmTable as Main Inner join (Select DirectorName, count(DirectorName) as Filmcount from MyFilmTable GROUP BY DirectorName HAVING Count(DirectorName) &gt; 1) AS Counter ON Counter.DirectorName = Main.DrectorName
&gt;is the Structured Query Language that's not what it stands for it's not even considered structured
It seemed to have worked! Thank you! My prior table with just my original *with* statement &amp; following *select* statement returned 252 rows and your statements returned 203. Nothing seemed to duplicate; I tweaked the with statement to order it by ThefilmRank and then to Select the top 100%. I'm having a hard time looking this up but what does Counter do (such as when you put Counter.Filmcount)? How is that different than using the Count?
I'm trying to setup a MariaDB connection with Dbeaver and it gives me this error when I try to establish the connection. I can access MariaDB from the command line, I was able to use it from DBeaver too before I rebooted, after which it said the connection was lost and needed to be setup again. I tried creating a user and grant it all the privileges and then create the connection with that username, but it still gives me the same error. My distro is : Archlinux
Yeh, he's probably going to be using MS SQL.
Awesome, thanks! Took the class a few semesters ago but lost it all. Need to start practicing cause I'm going into either Sytems Anaylst or Data!
I would Venn diagrams to visualize UNION ALL/ INTERSECT / EXCEPT but not for JOINs where one file is a header file and another file is a detail file such as Sales Order Header and Sale Order Lines. I would use a Venn diagram when the tables being joined were the same such as joining sales data from two companies where each company has an independent database and you are combining the results to get combined sales.
Cleveland, OH if anyone doesn't want to click to find out where the job is.
i added the &lt; 19 myself the existing logic makes more sense knowing that @cnt is a negative number. Or else the loop would never exit!
&gt; CREATE TABLE Allergen\_Relationship ( &gt; &gt;DishID VARCHAR(30), &gt; &gt;AllergenID VARCHAR(30), &gt; &gt;PRIMARY KEY (DishID, AllergenID), &gt; &gt;FOREIGN KEY (DishID) REFERENCES Dishes(DishID) &gt; &gt;FOREIGN KEY (ConcertID) REFERENCES Allergens(AllergenID) &gt; &gt;); Is this the correct format in which to do that?
Exactly, but I didn't think anyone would catch it. Was definitely just trying to keep the post simpler - it's always hard to balance what details are necessary to the question vs what is actually being executed :)
I like the personal insight
I love cleveland
true. but its hard to review the logic of a while loop with no enter condition given.
Thank you, I should have mentioned that. They’ve offered relocation in the past if that helps.
you forgot to mention which table you're trying to update **pro tip** test your queries directly in MySQL before embedding them in PHP code -- you'll get more meaningful error messages than "no result"
not every database engine supports comparisons in the SELECT clause **pro tip:** since so many different platforms use (slight variations of) the SQL language, it's a good idea always to mention which platform you're using
Microsoft SQL Sever Mgmt Studio 18 Sorry about that!
Thank worked. As usual i'm embarrassed
there are dangling commas in front of your SELECT and FROM keywords fix that and re-post p.s. consider using **leading comma convention** -- can you see them more easily now? INSERT INTO temp_mg ( pnum , agent , xxx , xxx , enrollingdate ) , SELECT p_bs_polnum as pnum , p_ag_agent as agent , xxx , xxx , b_gm_enrollingdate as enrollingdate , FROM p_base , p_agent , b_group_master , b_group_orgdata WHERE A=B AND A=C AND b_gm_enrollingdate ??????
My mistake. I didn't want to post the entire script as most of it doesn't pertain to my question.
what SQL engine are you using?
so did you fix your problem?
I would guess the error is the UPDATE with ORDER BY.
Sorry, im very new to SQL. Would you mind explain further?
Sorry about that, it's MariaDB 10.0.0.38
Where A=B AND A=C? SO 1=1 AND 1=2? I don't think that will work.
Here we go again with using Venn diagrams to demonstrate joins...
Just reposted with your suggested format and the rest of the script I left out.
Relational databases are build on set logic. And sets don't have a particular order. So you can't update with ORDER BY clause. You can SELECT with ORDER BY because you are presenting the result, but update changes data in the background, it doesn't make sense to order here.
I just used those in place of table names since it didn’t seem relevant to my question about the max function. . I fixed it in my repost so it isn’t as confusing. My bad.
W3schools.com. Free and interactable. You can breeze through that in a few hours I think and they even have a test at the end to test your knowledge.
I tried removing ORDER BY and am still having the issue
Sorry if this is a bit vague. It's the end of the work week and I'm a bit faded. Can you create a temp table or cte with an additional row_number() column and then from that, create another temp table or cte by joining the table or whatever additional tables you have created to itself? Sorry if that is a bad explanation. But I think row_number() and creative joins and ctes are the way to go.
Counter is just the alias I gave to the sub query for quick access. You could change it to bob or filmcounter or sldkhflksdjflksjf if you want as long as you use that alias in all the instances where it's needed. changing things to ) AS bob for your alias name bob.Filmcount in your field list and bob.DirectorName = Main.DrectorName in your join would make it do the same.
I've had a headache all day and can't think. Immediately thinking of going to the sys table to get the column names and then running some kind of loop, not sure if there is an easier way. Going to be a weekend project.
If it makes you feel any better, I'm pretty certain it's possible and not too difficult. I just remember I did a project where I was grabbing a shit load of stats from different reps and actions and states and making giant spreadsheets and inverting what was on each axis. I think the key is basically separating each column into its own table and then creating a new table that uses the stuff on the X axis as the base. Then join the columns appropriately. God speed and I hope you mean each table just holds different data but is formatted the same or else that might be a headache.
I actually tried it out and it's great. Thank you for the suggestion.
Unpivot the table.
But that won't pull the column names down as values will it? It will just unpivot and structure the data vertically?
MAX() is pretty straightforward once you get used to it. It will go in the SELECT clause, and you’ll need to update the query to have a GROUP BY clause at the end.
My first question would be, "why is the data stored like this…?"
yes, you can join on pretty much any condition. say your second_table has a range (from_id, to_id) and the "bucket" number. select f.ID, s.bucket from firsttable f join secondtable s on f.id between s.from_id and s.to_id
Nevermind, my idea is dumb, you will have to specify the column names for this to work, defeating the purpose.
I can do that dynamically... still not sure how I would insert the dynamic values though unless I was using a loop function.
You are awesome. Thank you!
Yeah, do SELECT column_name FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = N'YourTable' to get the column names. make a dynamic sql string with your unpivot code and look through that above query to populate the field names then execute it after it builds the statement
That's kind of where my head is at, was curious if I was over complicating matters or not.
third party app, its not up to me :-(
What you're trying to do doesn't care about NULL now, but it may change later, as you expand your schema.
Thanks for the response. Select max(b_gm_enrollingdate) as enrollingdate ^^^Like this? What do I type for the group by?
Python or R, read file into a dataframe, unpivot in the application, then push via SQL.
Looking over your query, you’d probably want to group by policy number. It can depend on what you want the max policyissue date to refer to.
Looking over your query, you’d probably want to group by policy number. It can depend on what you want the max policyissue date to refer to.
So the original architecture was to do this outside SQL, because it's more appropriate, but our IT group sucks balls and gave us a really high number for what that would cost, and said it would take 1-2 months to complete. Pretty sure I can bang this out in a few hours and just do it in SQL so that we can meet a deadline in 3 weeks.
 UPDATE license_seats INNER JOIN licenses ON licenses.id = license_seats.license_id INNER JOIN temp_asset2Phone ON temp_asset2Phone.phone_number = licenses.serial INNER JOIN assets ON assets.asset_tag = temp_asset2Phone.asset_tag SET license_seats.asset_id = assets.id
Wow. I never even heard of this version of SQL before, and if what you say is true...that’s...horrible. I’m so sorry.
It would be helpful to know what SQL Server version you're on. For the splitting... If 2016+, use [STRING_SPLIT](https://docs.microsoft.com/en-us/sql/t-sql/functions/string-split-transact-sql?view=sql-server-2017). If prior versions use Jeff Moden's [Tally OH! splitter](https://www.sqlservercentral.com/articles/tally-oh-an-improved-sql-8k-%E2%80%9Ccsv-splitter%E2%80%9D-function) table valued function. For the concatenation of the resulting table with one row per value that got split out... For 2017+ use [STRING_AGG](https://docs.microsoft.com/en-us/sql/t-sql/functions/string-agg-transact-sql?view=sql-server-2017). For earlier use [FOR XML concat](https://www-red--gate-com.cdn.ampproject.org/v/s/www.red-gate.com/simple-talk/sql/t-sql-programming/concatenating-row-values-in-transact-sql/amp/?amp_js_v=0.1&amp;usqp=mq331AQCKAE%3D#_Toc205129486) (go to the "black box XML section).
That worked wonderfully! Thank you very much for your help.
first of all, let's fix this -- FROM p_base , p_agent , b_group_master , b_group_orgdata WHERE p_bs_polnum = p_ag_polnum AND p_bs_polnum = p_go_polnum AND b_go_bcntrlnum = b_gm_bcntrlnum AND p_bs_status like 'A%' AND b_gm_distrib like 'AFGA' rewritten using explicit JOIN syntax -- FROM p_base INNER JOIN p_agent ON p_agent.p_ag_polnum = p_base.p_bs_polnum INNER JOIN b_group_orgdata ON b_group_orgdata.p_go_polnum = p_base.p_bs_polnum INNER JOIN b_group_master ON b_group_master.b_gm_bcntrlnum = b_group_orgdata.b_go_bcntrlnum AND b_group_master.b_gm_distrib like 'AFGA' WHERE p_base.p_bs_status like 'A%' note three things -- each newly joined table links to a previously-mentioned table, the only WHERE condition remaining is a "driving" condition which restricts the rows you start with in the first table (other conditions are filter conditions and therefore moved to ON clauses), and finally, it is **imperative** that you prefix all column names with the table names that they belong to -- the fact that someone has decided to embed a cutesy table abbreviation right into the column name actually helped me here but this is a terrible practice and needs to be stamped out everywhere, just like implicit comma joins instead of explicit JOIN syntax please plug that in, make sure it produces exactly the same results, and then we'll move on to implementing that max thingie
This is one of those few cases where dynamic SQL is appropriate (because you're working with changing table+column names). Basically check the sys.columms (or INFORMATION_SCHEMA.COLUMNS) (or whatever your DBMS's corresponding system tables are, those are SQL Server) to grab the column names and use them to build your CREATE and INSERT statements for each table you want to do this to. I can help ya through it if you want but it sounds like you may want to do it yourself (plus it's a good problem to work through for yourself at least once).
I'm pretty savvy with dynamic SQL and loops, I was more probing to see if there was an easy solution I was missing.
just use **column aliases** SELECT tableA.id , tableA.height AS height_tableA , tableB.id_1 , tableB.height AS height_tableB FROM tableA INNER JOIN tableB ON tableB.id_1 = tableA.id
I just had a really ugly thought... and please read this as a completely hack-y solution and there's so many other routes you should try before you get to this point. You said this was for a LOT of files? If you have the ability to change how they are importing, you could have it import them without treating the first row as a header row, making the first data row have your dates. You can then use it to do additional pivoting/unpivoting to your hearts content.
That is correct, I could choose to have the files imported such as Col1, Col2, etc. Each file should always have 13 columns (12 months + the name column)... but I'm struggling to see how that would be useful. I'd still need to do a loop, but it might get me out of having to use dynamic SQL?
There will likely be a question dealing with joins, and most probably something specific to using a LEFT OUTER JOIN. There will also likely be a question involving an aggregate function where the solution involves using the GROUP BY and HAVING clauses. You may see questions whose solutions deal with using a correlated subquery in order to limit the result set. Know how UNION, UNION ALL, INTERSECT, and MERGE work--I've seen questions related to those. Know about the DISTINCT clause, as well as ORDER BY. Know how to use an aliased SELECT clause effectively as another table.
I told you that I’m new at this. Calm down lol. Thanks for the response though. I’ll try it when I get back to work Monday. Maybe I’ll get lucky and find out how to do the max thing correctly until then.
Any chance for a remote position?
I just want the max enrolling date to be the most recent. The table includes an enrolling date every single year. Only needing those. Thanks for the response!
the max thing will involve joining to a subquery... i'd be happy to help once you've made those changes and confirm they work is `b_gm_distrib` going to be the same for all `b_gm_enrollingdate`s? because if so, it'll make the subquery simpler
Honest question: how else would you show them?
Ah I see. Thanks again. Didn’t know that would be needed. Unfortunately won’t be able to update it until Monday. First thing I’ll try is your explicit join. Normally I wouldn’t go to Reddit, but the actuary that’s out for a week is the expert. I’m not sure I understand your question. Can you dumb it down for me?
I thought I’d link [this post](https://www.reddit.com/r/webdev/comments/b46iyd/sql_joins/) about SQL joins that I saw the other day here on Reddit. It uses Venn diagrams to visualize SQL joins. There was [a reply](https://www.reddit.com/r/webdev/comments/b46iyd/comment/ej4io25) linking to a [blog post](https://blog.jooq.org/2016/07/05/say-no-to-venn-diagrams-when-explaining-joins/) about Join diagrams vs Venn diagrams. Another commenter in that thread shared [this image](https://i.pinimg.com/originals/52/20/c4/5220c492bc4e1a8b9175aba77ed7d091.png) of a Join diagram.
On live now
okay, presumably you want the max enrol date for each control number, right? so there are multiple rows for the same control number, each with a different date are all the `b_gm_distrib` values on those multiple dates for the same control number the same?
Ah I gotcha. Yes that’s correct. The end result is a list of policy numbers (pnum) with the most recent enrolling date as one of the values. There are multiple past enrolling dates for each specific policy and I only need the most recent for my purposes. My list of policies is substantial so getting an additional 20 rows keeps producing errors or aborting due to the additional enrolling dates. The b_gm_distrib value will be the same for all enrolling dates for a specific policy number. Its an identifier for the distributor. It only varies for different policies.
Venn diagrams work well for UNION, UNION ALL, INTERSECT, etc. I dont have a simple answer for how to demonstrate joins. But, what really helped me out was first understanding what is a cartesian product and how the different join types affect the product.
Thanks for clarifying!
Congratulations on getting to the interview stage. For the technical side, I really like the advice in [this video](https://www.youtube.com/watch?v=uAWWhEA57bE) that runs through the type of questions I've experienced (and asked) during Intermediate level interviews. It's also important to touch on the 'soft questions' that come up in interviews. I wrote up an [interview guide](http://www.helenanderson.co.nz/tech-interviews/) recently that tackles how best to answer questions around data cleansing, tooling and requirements gathering. Good luck with your preparation!
[This](https://blog.jooq.org/2016/07/05/say-no-to-venn-diagrams-when-explaining-joins/) might summarize your sentiment?
Hey, I don’t mind having a look if you want to PM me. Depends on how ‘advanced’ the question is as to whether I’d be a lot of help but I’ll give it a go!
There is a really good Slack server, not aware of any IRC/Discord ones https://SQLcommunity.Slack.com
Maybe post the question way after the screening etc. Something like that is always interesting.
Sure, give me a PM. But my reply might take long depending on time of day.
Service accounts can be created by your org. They’re typically used for things like automation. They can be created like your typical user account or with special user types. I do not know who at your org would be in charge of creating them, but suffice to say they are a common device
for beginners W3 schools is all you need, it contains everything you need and none of blahblah.
You can create whatever user account on the Oracle DB as you choose, whether the account is logically tied to a human user or an application doesn't matter.
Please proceed, governor.
Is it the MAX enrolling date overall or the MAX for each policy number? If it's the former, use a sub-query in the WHERE clause e.g. WHERE p\_base.enrollingdate = (SELECT MAX(enrollingdate) from p\_base) &amp;#x200B; If it's the latter you could use a sub-query and join the table with policy number and enrolling date to that e.g. SELECT .... &amp;#x200B; FROM p\_base a INNER JOIN ( SELECT p\_bs\_polnum , MAX (enrollingdate) as maxenroldt FROM p\_base GROUP BY p\_bs\_polnum) b &amp;#x200B; WHERE a.p\_bs\_polnum = b.p\_bs\_polnum AND a.enrollingdate = b.maxenroldt &amp;#x200B; &amp;#x200B; **(And use proper JOIN syntax as** /u/r3pr0b8 **says - for your own sake.)**
Why do you have 60.000 tables to update? I'd say revisit your relational database design or stick to a nosql database?
Select * from user_table U Left outer join photo_table P on U.photoid = P.id Left outer join name_address NA on U.id = NA.userid Left outer join address_table A on NA.AddressId = A.id where U.id = someuserid
If I do something like: select u.name, p.filename, a.address from user_table left join photo_table p on u.photo_id = p.id left join name_address n on u.id = n.user_id left join address_table a on a.id = n.address_id where u.id = some_id I get two rows like this: NAME | FILENAME | ADDRESS John | file.jpg | my home address John | file.jpg | my work address What I want is something like this: NAME | FILENAME | ADDRESS John | file.jpg | my home address, my work address
Sorry the format is so bad, I'm on my phone.
&gt; What I want is something like this: GROUP_CONCAT!!!
Oh wow, works perfectly! Thank you. Simply: select u.name, p.filename, GROUP_CONCAT(a.address) from user_table left join photo_table p on u.photo_id = p.id left join name_address n on u.id = n.user_id left join address_table a on a.id = n.address_id where u.id = some_id
okay, give this a try... maybe run the SELECT by itself to inspect the results before adding it to the INSERT statement INSERT INTO temp_mg ( pnum , distributor , agent , issuedate , enrollingdate , plan , status , mode , modeprem ) SELECT p_base.p_bs_polnum , group_subquery.b_gm_distrib , p_agent.p_ag_agent , p_base.p_bs_issuedate , group_subquery.latest_enroldate , p_base.p_bs_plan , p_base.p_bs_status , p_base.p_bs_mode , p_base.p_bs_modepx FROM p_base INNER JOIN p_agent ON p_agent.p_ag_polnum = p_base.p_bs_polnum INNER JOIN b_group_orgdata ON b_group_orgdata.p_go_polnum = p_base.p_bs_polnum INNER JOIN ( SELECT b_gm_bcntrlnum , b_gm_distrib , MAX(b_gm_enrollingdate) AS lastest_enroldate FROM b_group_master WHERE b_gm_distrib like 'AFGA' GROUP BY b_gm_bcntrlnum , b_gm_distrib ) AS group_subquery ON group_subquery.b_gm_bcntrlnum = b_group_orgdata.b_go_bcntrlnum WHERE p_base.p_bs_status like 'A%'
If you're using SQL Server: https://www.mssqltips.com/sqlservertip/2914/rolling-up-multiple-rows-into-a-single-row-and-column-for-sql-server-data/ If you're using MySQL, GROUP_CONCAT does the same thing.
you forgot the GROUP BY clause also, you forgot to alias `user_table` with `u` but i'm glad you got it working
It works without group by though?
I agree with /u/canuckathome that this is not a normal design. I think a lot of replies will be questioning that design so maybe it's worth an explanation, because if we can avoid that altogether with an alternate design that would likely be ideal. That said it sounds like you don't even know what the performance problem is. You should try to figure that out first before jumping into a solution. Offhand your guess about "multiple handshakes" seems to imply that you're opening a new connection per query...maybe? Maybe you could explain exactly what you're doing first.
I’ve not used postgreSQL but it looks like it’s basically doing the same thing as an inner join.
The first query gets all the columns in both table1 and table2, joining them on their respective "id" columns. This is the same as an inner join but with different syntax, as MyLastGamble said.
The problem is, i can do this manually but i want the computer to automatically rename the columns. I have hundreds of columns and this would take a long time to do.
Tiny tangent here: why are the commas on the left side instead of the right side?
Regarding SQL server, as of (I believe) SQL server 2008 you can use STRING_AGG. Feels eailsier and more intuitive to me.
Yeah, so it's selecting all records from table 1 and table 2 where the ID exists in both tables. Those statements are equivalent, HOWEVER if you had additional columns in table 2 that you wanted to bring in then your second SQL statement won't be able to do that (you'd use the first). BTW equivalently, and most commonly, you would do this: SELECT * FROM table1 INNER JOIN table2 ON table1.Id = table2.Id This means you now have access to all columns of table 1 + table 2 (if table 2 had more than just an ID column) and is equivalent to your first SQL statement.
I’ll definitely give this a shot Monday morning. I really appreciate it. Thank you!
Why 60,000 tables - we have another database which have a similar structure. We are handling the data from a process unit where each of these tables correspond to different sensors(tags) whose minute data we want to historize. If I am storing them in a single table there will be a lot of repetitive elements which might increase the size unnecessarily. We are using MYSQL connector in python to query and insert the data into each tables one by one. We did quick reading online regarding this but couldn't find much about inserting into multiple tables, there's a lot of material on inserting into multiple rows of same tables. We saw an article which said that each table has a file related to that, which essentially means writing into 30,000 tables means opening and writing into 30,000 files. So we were searching for dbs which can store all these into a single file. Yupp total noob in sql if you haven't guessed till now , if you can give me some related keywords and I'll read about them and update
Yes, and I would imagine each country has multiple customers/suppliers. If table 2 is supposed to represent a list of relevant countries, then you should ensure that list is unique by enforcing a primary key.
This was standard SQL not that long ago. Putting the two tables side by side is a cross join. The result is called a Cartesian Product. Then the where filters out the cases where the two columns don't have the same values. This is essentially an inner join. I say essentially because I'm not quite sure how duplicates and nulls behave.
The first one is the "legacy" syntax for an inner join between table1 and table 2. The preferred way to write that query is: select t1.* from table1 t1 inner join table2 t2 on t1.id = t2.id;
I don't know the best solution from this information but if you go the route of having 60,000 tables that are manually managed, that's probably the wrong way off the bat. :) Maybe you'll want to look into a time series database. Some info can be found here: [https://blog.timescale.com/what-the-heck-is-time-series-data-and-why-do-i-need-a-time-series-database-dcf3b1b18563/](https://blog.timescale.com/what-the-heck-is-time-series-data-and-why-do-i-need-a-time-series-database-dcf3b1b18563/) \- I'm mostly a relational/SQL guy but these kinds of databases are growing more popular for (what I think is) the kind of work that you're doing. As far as "each table has a file", Oracle (and probably SQL Server and others) gives you greater control over how your tables are stored. In Oracle you can define tablespaces which are linked to one or more data files and you could therefore have 30,000 tables all working with one data file. That actually doesn't sound like a "good thing" to me though, just pointing out that you can do it. :) Regarding "couldn't find much about inserting into multiple tables", Oracle also lets you insert into multiple tables at once (multi-table insert) but you do have to list all the tables and that will be one monster query that will probably hit a maximum character limit. I wouldn't recommend this either. I'm actually wondering if you might be able to take advantage of table partitioning. I've used it in Postgres and Oracle but not MySQL so I can't vouch for its quality in MySQL, but basically the idea is you have a logical table that is physically laid out by partitions according to lists or ranges of IDs/timestamps/whatever data you want... To explain a little better, here's an example I wrote about here on reddit a few years ago, partitioning on Postgres: [https://www.reddit.com/r/PostgreSQL/comments/3x5ggo/new\_to\_postgresql\_is\_it\_possible\_to\_create\_a\_view/cy1vyxx/](https://www.reddit.com/r/PostgreSQL/comments/3x5ggo/new_to_postgresql_is_it_possible_to_create_a_view/cy1vyxx/) \- I believe Postgres offers more automatic solutions to partitioning these days btw. The purpose of automatic partitioning would be so you don't have to manually create all the partitions as I did in that example. So using a partitioning solution, for your use case, you could have something like a table called sensor\_minute\_data with the underlying partitions that are something like sensor\_minute\_data\_1, sensor\_minute\_data\_2 etc... When you select \* from sensor\_minute\_data where sensor\_id=5, the database will automatically route you to sensor\_minute\_data\_5 and neither you nor the application has to actually worry about it. Likewise the application can insert as much data as it wants into sensor\_minute\_data and the data will go into the right partition automatically. Hope that helps get you started on researching a good solution, at the least!
returns only one row, though.... is that what you want?
Yes thanks. I just want one row. Everything in a single row.
yeah, that's not gonna happen maybe rethink how you assign column names across tables so there is no collision of names
but you have no control over which user is returned! think about it!
Okay that's awesome to know. I'm excited to start doing some testing.
&gt; This was standard SQL not that long ago On a scale from the birth of the universe until current day perhaps. In terms of programming languages, that standard is basically an ancient fossil at [27 years old](https://en.wikipedia.org/wiki/SQL-92)
Sorry, maybe I'm misunderstanding, but I do have a where clause like `where u.id = 1`.
okay, you're good
Cheers. Thanks for your help today.
makes it easier to comment out fields
This is called an implicit join. Some engines will adjust the exec plan accordingly... some developers like this code... some developers dont. I'm in the dont side just because the on statement is right there and tells you the relationship its joining on, while many implicit joins make for a weird where clause. To each their own.
Can’t you make it trigger based ? When a new record is added, you have a query that adds it to the DB. Have an event that triggers another query to refresh the board. Does it make sense ? Let me know what you think
Unfortunately I do not have write access to db. Ehr handles all dB writes.
Yes, thanks a lot, I'm now reading those links, will definitely update.
This is the original way that tables were joined in SQL prior to ANSI 92 (?). Rather than having a Join or Inner Join as part of the FROM clause, the relationships of the tables was specified in the WHERE clause.
It survived in the Oracle universe until much more recently compared to other SQL dialects.
Is it SQL Server? Why not schedule a job to run every 10 minutes?
I’m more worried about the query that runs every minute to get list of queued patients. Will that affect dB performance for the EHR users?
I know bugger all about SQLite, but my first guess would be that the user connecting to the DB doesn't have read access to the table.
Are you 100% completely sure your program is opening the file you *think* it is, or is it opening a different, empty one?
As i said with intellisense if i drill into the Local Db file everything is there. I did notice that if i try to save table mappings to a var, the var shows nothing but the intelisense show the Armor table.
Your program is very likely not opening the same database as you're looking at in other tools.
Okay would you have any idea how to get it to open the right database? Sorry i'm kinda at a loss here and very confused
I've never used SQLite with C#, but on Android if you change your SQLite schema you have to completely clean and rebuild the project from scratch. If you try to make incremental changes without resetting the build you get things like this.
for sure, its happened to me where im going about my business i can see paths and contents but when it comes to to accessing files i cant
I would use Path.Combine(), which will handle the slashes between path objects and filenames. &amp;#x200B; &gt;var databaseFilePath = Path.Combine(path,"DandD.db");
By using absolute paths, or if using relative paths, making sure you're looking at the same place in intellisense and in your program. Usually what ends up happening is the program's working directory is not what you think it is; especially when running a program from an IDE or other GUI tool.
It's blank on the new Reddit version. It's there on the old version in case anyone was looking.
Just looked it up: string_agg is 2017+ on SQL server. Had my hopes up there for a minute.
I never knew the correct pronunciation of PostgreSQL lol
Microsoft SQL Server has SSIS, which is an ETL tool. SQL Agent is your job process scheduling service. There are stored procedures available and, as of a few versions ago, you can do insert, update, and delete in one fancy command called Merge (to achieve incremental loading). Debateable whether one does the transformation in SSIS or after, once the data has been staged at the destination. Sometimes you want to get into the source system, get what you need, and close the connection quickly. Also, when doing operations locally on one SQL Server (for merge, cleanse, aggregation, etc.) the server takes advantage of the SQL optimizer to make better query plans for faster overall processing.
ODI is the built-in Oracle ETL tool, right?
Debugger. Set a breakpoint around where you open the DB, then step while inspecting.
What is it? lol
Ah you are right, sorry. Before that you are left with the FOR XML query then, unfortunately..
Fast inserts may result in slower selects. What performance is more important: the insertion into the tables or the selection from the tables. You say repetitiveness worries you. If you’ll be selecting from all 60k tables anyway, you may be better off inserting into ONE table. It may only cost you some more disk space. (Depends on size of repetitive columns). If you will ‘usually’ select on a ‘per period’ basis, you may want to create one table per day part or even per hour and insert into that. Lowering the level at which you perform the data operation is always good, I.e. SQL. I don’t think MySQL will be able to optimise 60k inserts into different tables to the extent to your requirements. It will, however, be relatively easy to do into one table.
Well, it depends on the time the query needs to be executed. But yes if it’s querying millions of rows, expect some slowness.
My point was to add some sort of event that triggers the refresh query whenever the EHR writes on the query. That’s not possible for you case ?
My experience at a larger company was that IT generally preferred to do ETL type work using tools like Informatica rather than stored procedures/packages, etc. I believe the main reason for this was that it was easier to integrate with job scheduling tools that could monitor processes for success/failure and make sure the subsequent jobs only get run if upstream jobs get successfully completed. This can also be handy when you have several database vendors (SAP, Oracle, Teradata, MS SQL, etc.) and need a tool that can easily work between them. The idea here is to standardize on an ETL tool and process so your database-specific people could focus on issues with their respective database systems. It also allowed for better compliance with data governance standards.
I think you can do it with a "recursive CTE", not sure how performant it would be https://www.sqlite.org/lang_with.html
From the [PostgreSQL FAQ](https://wiki.postgresql.org/wiki/FAQ) &gt;What is PostgreSQL? How is it pronounced? What is Postgres? &gt; &gt; PostgreSQL is pronounced Post-Gres-Q-L. (For those curious about how to say "PostgreSQL", an [audio file](http://www.postgresql.org/files/postgresql.mp3) is available.) &gt; &gt;... &gt; &gt;Postgres is a widely-used nickname for PostgreSQL. It was the original name of the project at Berkeley and is strongly preferred over other nicknames. If you find 'PostgreSQL' hard to pronounce, call it 'Postgres' instead.
We just ported a self-written ETL (or actually ELT) system with about 100 procedures (including packages) from Oracle's PL/SQL to Postgres' PL/pgSQL. Works just fine (but then Postgres is much more powerful than MySQL, so your mileage my vary)
Be really careful with MERGE. It's not one big atomic transaction like you would assume. Underneath the covers its doing regular insert, updates and deletes. So you have to think about locking behavior and interaction with triggers, etc.
u/'or'1'='1
y u/or'1'='1
Thank you again! I'm reading over what you said and I'm blown away by the idea of joining a table back to itself but in a way that is more filtered down in order to return what you want. I don't know if you do this for a living or are currently in school (unfortunately I am doing neither but hoping to do the former eventually)... But do you find using a join and subqueries works faster and costs less than using the WITH function?
It looks ok to me, I don't normally write my constraints at the end of the table but that is fine. Is there a reason that you are using varchars for ID fields instead of integers or Guids (unique identifiers).
well you can do it as you wish Ive been to companies that have proper ETL scheme using one of Informatica, DataStage, SSIS, ODI, etc. I've seen companies do it with PLSQL procedures/packges. I've seen materialized view used, I've seen python, linux, files, FTPS, RSynchs, netapps shared clusters, heck even excel. it all depends on the needs and how much money the company has (informatica and oracle arent cheap at all).
this is perfect thank you for not giving this table its own auto_increment surrogate PK
I wornder what were the considerations to whyu move from Oracle to PG. We do exactly the opposite.
When I was in school and they started teaching us SQL, we were taught to put the constraints at the end. Like you said, nothing wrong with it, I think it just might be easier in the learning process to see everything at the end like that
Just a couple quick things: 1.). If you have any control over the datatypes of those columns, make them as small as you possibly can given maximum, reasonable expectation growth. Varchar is very inefficient at storing numeric data and this has all sorts of cascading effects. IO is often a big bottleneck for a rdbms and increasing storage requirements means higher resource consumption to read, persist and write the data, which slows everything down at scale. 2.) Name your constraints explicitly. Any professional reason to do this will likely have multiple environments involved (development, test, production, disaster recovery, etc... ). If you don't name them, sql will and there is no guarantee they will be the same across environments unless you name them explicitly. At some point, if you are working across multiple environments, you will be bitten by differences between them, so best to just keep them in sync as much as possible.
possibly MSSQL specific... CREATE TABLE [DishAllergen] ( [DishAllergenID] int NOT NULL IDENTITY(1,1) [DishID] VARCHAR(30) NOT NULL , [AllergenID] VARCHAR(30) NOT NULL , [OtherColumn] int NOT NULL CONSTRAINT [DishAllergen_FK_OtherColumn] DEFAULT (1) , CONSTRAINT [DishAllergen_PK] UNIQUE NONCLUSTERED ( [DishAllergenID] ) , CONSTRAINT [DishAllergen_AK] PRIMARY KEY CLUSTERED (DishID, AllergenID) , INDEX [DishAllergen_IX_AllergenId] NONCLUSTERED ( [AllergenID] ) , CONSTRAINT [DishAllergen_FK_DishID] FOREIGN KEY ( [DishID] ) REFERENCES Dishes(DishID) , CONSTRAINT [DishAllergen_FK_AllergenID] FOREIGN KEY ( [AllergenID] ) REFERENCES Allergens(AllergenID) ); First, yes I added the unnecessary ID column... often it's just easier to include, when it comes to support/maintenance/etc... academic scenarios will bitch about it being unnecessary / confusing / etc... but if you need to make an update to OtherColumn, it's far easier to use the ID than the pair of varchars... especially if you need to use nvarchars... even more so if the PK has more columns... the ID just ends up being easier to keep around. Second, I specify that the table's disk layout is still DishID/AllergenID (clustered)... this ensures lookups are fast... I also add an index to quickly perform lookups by AllergenID... may end up being unnecessary, but useful 99% of the time. Third, I name my constraints... PK for the identity, AK for the Alternate Key (sometimes referred to as Business Key)... everything's name is prefixed with the table name (also renamed to reflect the FK pair)... I included Othercolumn just to indicate naming the default constraint as well.
In that particular instance the client moved away from Oracle because of license costs (and Oracle's attitude towards our client didn't help them to keep that customer either) Postgres 11 is a much better choice than Oracle's Standard Edition even without considering the license costs. There are so many things that simply work better in Postgres. And it turned out we could simplify a lot of the code because of Postgres' much better JSON and string support. And its efficient array support also allowed us to simplify and improve things in some areas. Full text search and PostGIS are also way ahead of what Oracle can offer. In my opinion, Oracle is only a serious alternative to Postgres if you consider the Enterprise Edition **with** the performance and tuning pack. Parallel query and partitioning are more efficient in Oracle EE than they are in Postgres 11 (not to forget parallel DML). But the main "selling point" would be AWR/ASH for me.
ohh yeah, when comparing enterprise vs standard absolutely. we are moving from PG to Oracle enterprise which is like a huge advantage. but yeah when Oracle came here for the sales pitch, when they talk about money they talk in multiplications of $100K.
WHERE bought\_additional\_products IS NULL
Thanks for the reply! But here is the problem I'm having with that. bought_additional_products is not the common attribute among the records I want. Let's say record #4 is, Record 4: Purchased product, paid by check or record #5 is, Record 5: Purchased product, smiled at cashier and so on. The problem is I want to get rid of people who only purchased product, and did nothing else.
i'm sorry, but my answer is as comprehensive as your description of the problem perhaps you could give an accurate table layout including column names datatypes, and a few rows of actual data
Edited the original post.
Oracle's JSON support (especially in 12.x, regardless if standard or EE) does not match the flexibility of Postgres' implementation. You can't e.g. create one index on the complete JSON document which then supports arbitrary queries on the JSON value. Oracle also has no equivalent to Postgres' @&gt; operator to look for arbitrary (complex) objects inside a complex JSON structure (e.g. `where json_col @&gt; '{"a": 1, "b": 2}'`) And with Oracle 12 can't even compare JSON "objects" - it only compares strings taking the order of the keys into account. The value `{"a": 1, "b": 2}` is the same as `{"b": 2, "a": 1}` but it's impossible in Oracle 12.2 to compare them without breaking them up into each key individually. Oracle finally 18c added `json_equal()` for that. Also Oracle has big problems with high rates of updates on CLOB (or BLOB) columns - we actually have one customer that seriously considers moving from Oracle to Postgres because of Oracle's CLOB implementation because the high rates of updates are causing serious contention on the LOB segments (and Oracle support isn't helpful). We did load tests on both platforms and Postgres was consistently faster in that scenario (with a small table with \~50 million rows and only 500 - 1000 updates per hour) I can not confirm that Oracle is more stable than Postgres. Postgres on Linux is closer to unbreakable than Oracle in my experience. But this is getting totally off-topic now.
 SELECT ID , Bought FROM tbl WHERE Paid = 'No' AND Smiled = 'No' AND Bag = 'No'
Huh, thought there would be a workaround for that. Ok, if that is the way it goes then, appreciate it.
Thank you, I've written some kind of behemoth using this, but it works. The performance depends on the data, if there is a lot of data that needs splitting (e.g. duration &gt; 1h -&gt; splitting into seconds), the performance will tank (500ms on a 8 year old CPU for 10k rows of data, split into 30k rows of and then grouped by and summed), but that shouldn't be much of a problem since only 1 user will be accessing the db at big intervals and I won't be splitting into more than 300-600 rows of data for seconds time span). But that's just speculated result using arbitrary data, I'll check the real performance in real world application once I finish it. If anyone is interested in this monstrosity or needs something similar, here is the code for split and group by seconds (the time span can be changed by modifying duration of the current time span and strftime string format) WITH splitted\_data AS ( SELECT session\_type\_id, begin\_timestamp, used\_duration, initial\_duration - used\_duration as duration\_left FROM (SELECT id, session\_type\_id, begin\_timestamp, duration as initial\_duration, MIN(CAST(strftime('%s', strftime('%Y-%m-%d %H:%M:%S', begin\_timestamp/1000 + 1, 'unixepoch')) as INTEGER) \* 1000 - begin\_timestamp, duration) as used\_duration FROM session\_data WHERE begin\_timestamp + duration &gt;= 1562940797000 AND -- weed out things that won't need splitting begin\_timestamp + duration &lt; 1562943599000) UNION ALL SELECT session\_type\_id, CAST(strftime('%s', strftime('%Y-%m-%d %H:%M:%S', begin\_timestamp/1000 + 1, 'unixepoch')) as INTEGER) \* 1000, MIN(1000, duration\_left), duration\_left - 1000 FROM splitted\_data WHERE duration\_left &gt; 0 AND begin\_timestamp &lt; 1562943599000 -- end timestamp ) &amp;#x200B; SELECT session\_type\_id, begin\_timestamp, SUM(used\_duration), strftime('%Y-%m-%d %H:%M:%S', begin\_timestamp/1000, 'unixepoch') as date FROM splitted\_data WHERE begin\_timestamp &gt;= 1562940797000 -- weed out things that were left by splitter after split GROUP BY date, session\_type\_id
I am curious if anyone else has a solution for this if nothing else and you have a really long list of attributes to be a little bit fast you could use dynamic queries selecting all your columns from information_scheme and depending on your server version use string_agg or 'for XML path' to output a string with all columns = 'NO' I am on my phone right now and it is really unhandy and need to go to sleep but if you need me to elaborate I can do so
If you have Alter Table rights, I would suggest adding a bit column to the table that is 1 if the customer only purchased and 0 otherwise. You could then use the new column to filter for what you want. This would require an update where you have to use all the columns in your where clause, but it would allow you to query the table with the single column going forward.
This is my recommendation as well. Unfortunately the table setup is not ideal. If you have several different setups you need to sort by regularly, set a flag for all of them too.
Yeah, I think a dynamic query or pre-processing with a scripting language is the way to go. The simplest algorithm I can think of would process all the column names appending the columns with a yes to a string. Then, the where clause can be where yes_columns = 'Bought' I haven't worked with XMl in SQL server, so maybe that has some cool tricks to make it simpler. Here's an example from Stack Overflow https://stackoverflow.com/questions/13372276/simple-way-to-transpose-columns-and-rows-in-sql
In the real world we deal with shitty table design all the time. Looks like this is one of those times.
I'm the *woefully underpaid* IT Swiss Army knife at my work; Net Ops, Tech Support, Onsite Support, DBA, Data Analysis, Developer, Server Admin... Hell if you poke me in the right spot I'm pretty sure I even make toast. Any Join will have a higher cost but it's usually negligible and a WITH is not always going to be enough to get all the data or constraints you want. Your's is one of those situations, because the data you wanted to filter on did not exist we hat to create it and because we don't want to store it because then we would have to constantly update it so instead we have to generate it on the fly. Could do Temp Tables, could do Multiple CTE's, could Self Join. Each one has a cost and each one has it's use and depending on complexity we pick the one we need.
Unpivot with Cross apply drop table if exists #BadDesign create table #BadDesign ( MetricName varchar(30) , [2018-01-01] int , [2018-02-01] int , [2018-03-01] int ) insert into #BadDesign (MetricName, [2018-01-01], [2018-02-01] , [2018-03-01] ) VALUES ( 'Safety', 0, 1, 0) , ( 'Phone Deployment', 19, 42, 16) , ( 'Errors', 0, 0, 0) select bd.MetricName , v.Date , v.MetricValue from #BadDesign bd CROSS APPLY ( Values (datefromparts(2018,1,1), [2018-01-01]) , (datefromparts(2018,2,1), [2018-02-01]) , (datefromparts(2018,3,1), [2018-03-01]) ) as v (date, MetricValue)
ETL can be done by anything from shell scripts to Python, to PL/SQL to Java to T-SQL to Java etc. You can also use ETL software like Talend or Dig Dash or Informatica. A lot of these tools fall under Business Intelligence-type softwares. &amp;#x200B; You don't need PL/SQL (or pg/sql or T-SQL or whatever) to do ETL. In fact, after nearly 20 years in databases, I would recommend using something that is not tied to any particular database. &amp;#x200B; The less code you have running business logic in your Oracle database, the easier it'll be to goto PostgreSQL and save your IT dept loads of money (we are getting rid of Oracle for anything and everything we can). &amp;#x200B; A side note : I'm no ETL expert but apparently the order has changed recently. Our BI people are doing ELT instead of ETL. Might have something to do with hardware advances (like SSDs). &amp;#x200B; Good luck.
Where is your unpivot coming in from? Actually planning on working on this later today. The only issue is that the values in the cross apply would need to be dynamic from the name of the columns and not necessarily known. Could be loading over 200 files in this format and normalizing the data.
Look up the SQL HAVING clause. That’s what you’re looking for. Unfortunately, I don’t have time to mock it up for you.
The cross apply with values, the first is a new value where I create a date same as column name, the second is column name itself, repeat for each column. You can do this dynamically from sys.columns and try to cast them to date data type.
If you have to do columns like this in the future, you could make a view that turns the Yes/No's into 1/0s. Then, have one column that sums up all of the attributes. For that one view, you can't get away with specifying every column name, so it won't save you any time for just one query. But once you have it, you can write your query by just doing: WHERE ( Bought &lt;&gt; 1 OR AttributeSum &lt;&gt; 1 ) To exclude people who purchased and only purchased. And you can do the same for any subset of columns.
Hmmm... That isn't exactly an unpivot is it? I understand cross apply... you're giving me an interesting idea though. I have ~200 files, and I could bring them in, and normalize the data sequentially because I know the files will always have 13 columns, with the first column always being a name. Then just cross apply the values, and later I can update the dates by simply using a date table and specifying the range from where 1 begins based on the name of the file.
Just to add some further context. I'm working with a part of the business that has very "complex" Excel files that can have up to 20 or 30 tabs in each file. The rules in these individual tabs can vary greatly from file to file, *but* each file has (4) tabs which are always in the same format, a pivot table that is populated by various vlookups, etc. So the idea is to write a Python script which strips out every tab from each individual file, and then to import the CSVs which have the name "import" in the file, which will all be in this same format, then to normalize that data and begin ETL'ing new files on a monthly basis as the business fills them out.
Yes it is. It's the ETL software that also has out of the box workflows for Oracle ERP/EBS data (financials/inventory/etc), which lets you move that sort of data into a Data warehouse without much customization.
Thanks for this in-depth answer! This was very helpful - I've definitely got a lot to learn. Just to be sure I understand: let's assume that table2 contains only the id column, and the ids are unique, but the same is not true of table1. Say the ids look like this: table 1 id = 3, 3, 3, 4, 4, 4, 5, 6, 7 table 2 id = 3, 4, 5, 6 The first query returns all columns of all but the last row of table1, and also includes the id column of table2. This column is necessarily the same as the id column of table1. The second query does the same, but doesn't include this duplicate column. But, this is only true because of the assumptions we made about table2's contents (namely that it had only one column, comprised only of unique entries). Were there duplicates in table2, we would have gotten a cartesian product from the first query.
Unpivot definition - convert columns to rows. I gave you the tools for your task with a working example - now you must try yourself and make it happen. If you have another concrete problem, let me know and I'll be glad to help.
Oh, I understand how to use the functions and whatnot. and solve this problem. I made this post looking for some more clever ways to achieve the same results which I think you've provided. Thanks.
The ID column really serves no purpose. You would still have to do a lookup on the other two columns (presumably) to find that ID
Keep logging bugs with the vendor until they fix their stupidity.
Surrogate keys exist for a reason. Since you feel otherwise, feel free to debate with the experts who define the term and justify their existence.
SQL is a standard query language. MySQL, PostgreSQL, SQLite, and MS SQL Server implement the language standard while adding their own feature sets. T-SQL is Microsoft's language extension that adds features outside of the ANSI standard. "SELECT column FROM table WHERE condition" will be 99% the same in all implementations. When you start adding cursors, casts, window functions, etc., you get into the differences.
Postgres/MySQL/SQLServer (MSSQL) are all different brands of SQL servers - they all (mostly) the same language (SQL), but have variations on how they do things on the backend, and some variation of supported features, as well as different programming languages. TSQL is the programming language for MSSQL.
Postgres, MySQL and SQL Server are all implementations of a database engine that supports SQL. From an administration standpoint, they're all very different. From a capabilities standpoint, they're all just databases, just like Oracle. They will all have slightly different features implemented. T-SQL is Microsoft's implementation of SQL in SQL Server. It contains Microsoft's extensions to standard SQL. As an example, in SQL Server, you use the ISNULL() function to replace a null value in a query result. The MySQL equivalent is IFNULL() and Oracle uses NVL() From a developer's standpoint, if you're strong with SQL on one database engine, you can transition relatively easily to another. From an administration standpoint, the transition from one to another is not as easy.
Leave it to Oracle to have the syntax for a simple function make no sense, heh.
I think their "logic" is that it stands for NullVaLue.
To expand on this, I would recommend to have 1 column to store bits in an ordered fashion. For example - given your table structure: &amp;#x200B; |ID|Bought|Paid|Smiled|Bag|Bits| |:-|:-|:-|:-|:-|:-| |1|Yes|Yes|Yes|No|1110| |2|Yes|No|Yes|Yes|1011| |3|Yes|No|No|No|1000| |4|No|No|No|No|0000| |5|Yes|No|Yes|No|1010| &amp;#x200B; This way simple equal to, greater than, less than operators can help you out for your purpose - in this case, particularly: where bits = 1000 Of course, for ease of maintenance and to reduce the chances of error and in the interest of "single version of truth" , I would recommend putting a view on top of the table where you set the "bits" column from the underlying real columns. The advantage is that you only need to add a new column to the view definition when a new column is added to the underlying table. But for selection/reporting, you would get a much more convenience.
Thankfully my shop only has a couple Oracle servers scattered about from a couple mergers back for some backend metadata kind of stuff. We're working to deprecate them as the licensing fees are just atrocious compared to anything else we use. Haven't personally had the pleasure of needing to write any code for them yet... hoping to keep it that way. With that said, we mostly use Greenplum, which has it's own share of quirks... mostly in that it's stuck on the Postgres 9.x codebase with a few backports of 10.x here are there.
With cte As ( Select id, colValue, colName From ( Select id, smiled, bag,... From table )x unpivot ( ColValue for ColumnName in (smiled, bag, etc) ) unpvt ) Select * From table t Where t.id In ( Select t.id From cte where Id not in ( Select Id From cte Where colname&lt;&gt; 'bought' And colValue = 'yes' ) ) This query will basically get you all things from original table where they have a Yes in something other than bought.
cross join doesnt need "handling" - it's simply every possible combination. I think you are looking for a join to a column that contains value of multiple attributes (concatenated). an example how to get your "B.[type] (provided Category = 'hardware')" value would be via subquery: (select b.[type] from table_b b where category = 'support') I would say that you either will need to provide more data examples or give more insight on how that string is created so better suggestions can be given.
Intermediate to advanced? Basic SQL skills that I encounter in the wild at work are typically people who can write just basic selects, and they tend to work with "queries" that are prewritten where they only need to change a few variables. Intermediate users are writing queries like that, nested subqueries, improving execution times, writing stored procedures, etc. Maybe some SSIS. Advanced users are really DBA's only. Maybe some dynamic SQL in here. Loops, triggers, cursors, designing error handling/logging procedures. Working in SSIS. If you're looking for an entry level job I'd say you are already above average for many of the candidates that are going to be applying. In the world of Data Analysts you're probably approaching "advanced" skills, but in the world of DBA's you're probably a beginner though.
Hopefully you are on SQL 2016+ You could you a similar technique, this is a very slow join... i would highly recommend a different format / storage of your value - value - value thing... after you get the return from the below statement, you will have to pivot the data... I just wanted to see if a join on string\_split worked.. and it does. &amp;#x200B; DECLARE @table TABLE (fullColumn NVARCHAR(100)) DECLARE @table2 TABLE (partialColumn NVARCHAR(100)) INSERT INTO @table (fullColumn) VALUES (N'test - c1 - c7 - a5'); &amp;#x200B; INSERT INTO @table2 (partialColumn) VALUES (N'c1') , (N'c7') , (N'c8') , (N'a5') , (N'a8') &amp;#x200B; SELECT \* FROM @table LEFT JOIN @table2 ON partialColumn IN (SELECT RTRIM(LTRIM(\[value\])) FROM STRING\_SPLIT(fullColumn,'-'))
All of them are RDBMS (Relational Database Management Systems). The important word here is "relational". It stands for a lot of stuff, but basically it's the following: * Data is organized into tables with a predefined schema (column names, their order, their data type and any constraints) * Data is stored tuple by tuple (either a part of a row or a full row is a tuple) * They usually are [ACID-compliant](https://en.wikipedia.org/wiki/ACID) and for that they employ different technologies when operating * They are usually operated with a standardized language called [SQL](https://en.wikipedia.org/wiki/SQL#Interoperability_and_standardization), which has different approved levels of feature standardization (later standards have more features). If that all seems obvious and you think that every system managing data should be like that, then let me present you alternatives to each point: * You don't want to have a predefined schema. The application decides what to write and what to read. You just need an engine to provide you methods to do this fast and easy. Especially with dynamic languages like JS, Python, PHP, this is handy. Such products are many, but [MongoDB](https://en.wikipedia.org/wiki/MongoDB) is an example. Or sometimes you have specialized data structures like graphs, in which case you would use [LemonGraph](https://github.com/NationalSecurityAgency/lemongraph) or [Neo4J](https://neo4j.com/) * If you have a ton of columns (there are cases with thousands, but hundreds are quite common), but you only want to read 2-3 of them, in a relational database you'd have to go row by row, and thus reading a huge ass data file, when all you want is just a tiny fraction of it. So there are [columnar store database engines](https://en.wikipedia.org/wiki/Column-oriented_DBMS) out there, that are NOT relational databases: Vertica, MonetDB, ClickHouse, Hyper, Exasol, Redshift, BigQuery, cstore_fdw for Postgresql, MySQL's columnar store data engine, etc. They store each column separately, so if you need just 2 or 3, it won't touch the rest when reading. Much faster in these cases. * ACID compliance is expensive to achieve. Performance is sacrificed for security. But sometimes you don't care about data security that much (when you just want to load some data for some computations for example, or it will be irrelevant in an hour or so). All of the above data engines are usually not ACID-compliant (unless I got this wrong). * SQL is hugely powerful, famous, well standardized and easy. That's why you'll find a ton of database engines try to implement their variant of it, even though it usually isn't standard SQL. Just because it has a "SELECT" doesn't make it SQL. That's why many have changed the names to, say, [CQL](https://cassandra.apache.org/doc/latest/cql/). But I've rarely seen SQL in non-relational databases. --- Ok, so we got that cleared. Microsoft SQL Server, MySQL, and Postgresql are all Relational Database Engines. They basically do the same thing, but in a different way. * They all support SQL (aside from MySQL but that's a rant for another day and if you don't want to get confused, ignore this remark) * They all support transactions (ugh... [MySQL..... is ... aaaagh](https://bugs.mysql.com/bug.php?id=11472)) * You connect to them differently though. Quite differently. Some drivers are standardized like ODBC, JDBC, ADO.NET, but in the back-end it's all different. * Their security and rights differ A LOT * Their configuration can't be more different * Microsoft SQL Server didn't even run on Linux until recently * Different licensing / pricing. Postgresql is as free as air. MySQL is Oracle, Microsoft SQL Server is the most closed one. * Different sizes and complexity. I used to leave the SQL Server installation over night when I still used HDDs. Postgresql is relatively small, MySQL is a bit bloated with all those data engines. * Different features. Aside from standard SQL. * Different structures of entities. Microsoft SQL Server can query across databases and servers. MySQL can query from many databases. Postgresql can't (unless you are willing to put your back into it). Postgresql has table inheritance. MySQL is comfortable with multiple data engines. * Slightly different syntax (how and when you quote column names) * Different treatment of text data and encodings. Collations, etc. Microsoft SQL Server and MySQL have case-insensitive collations (if you compare "Bob" and "bob" they will be equal), which are often the default. Postgresql doesn't. * Feature set. Postgresql can be extended to do and be anything you want (MSSQL too, but damn it's bad at this). Its builtin feature set however blows everything out of the water. * Their "extra"-SQL language. They all have it. SQL is [Declarative](https://en.wikipedia.org/wiki/Declarative_programming). All engines also have an [Imperative](https://en.wikipedia.org/wiki/Imperative_programming) language available. T-SQL is the friendliest (Microsoft SQL Server) because you can use it seamlessly and not even know it. Postgresql has the default pl/pgsql, but can be extended to also use Java, C, Python, Javascript (though you probably shouldn't), and it's quite separate from regular SQL. You can't just start writing pl/pgsql. MySQL is the worst at this simply because debugging any errors is just an impossible task if it's complex code. * Their speed. Few people compare them any more, but Microsoft SQL Server is probably the fastest RDBMS out there right now, with Postgresql not far behind, and I would even dare to challenge MSSQL developers, that I could make the same thing run on Postgresql as fast, or faster. It's just not as easy as in MSSQL. Right... Examples maybe? This is doing some Imperative coding in 2 of the engines (use as reference. The MSSQL example at least is probably wrong because I haven't worked with it in a while) MSSQL: ``` DECLARE @how_many INTEGER SELECT sum(whatever) FROM foo INTO @how_many IF @how_many &gt; 5 THEN BEGIN CREATE TABLE bar (whatevers INTEGER) END ``` Postgresql: ``` DO $$ DECLARE how_many INTEGER; BEGIN SELECT sum(whatever) FROM foo INTO how_many; IF how_many &gt; 5 THEN CREATE TABLE bar (whatever INTEGER); ENDIF; END; $$ LANGUAGE plpgsql; ``` But the SQL part is the same: ``` SELECT sum(whatever) FROM foo ... CREATE TABLE bar (whatever INTEGER); ``` --- &gt; SQLite in my university but we didn’t touch any the rest of softwares It's almost the same, except for how you connect to the DB and how you create a DB. &gt; And what is T-SQL for? When employers often use this? Yes, it's useful. Unless you're in web development or back-end engineer in a microservice architecture (most things are trivial in those cases, and nobody even writes SQL, relying on ORM instead). For manipulating data, T-SQL, plpgsql, pl/sql (Oracle), etc. are very nice tools to have that make things possible.
Oh sweet. Pleasant surprise! I know I need practice but I don’t graduate for another year, so I’m making sure I have an extra marketable skill (I’m really enthusiastic about getting into a data science position, so very obviously I need to learn SQL). Thanks for the response!
So for data science you might be more towards the beginner side. Depends. A lot of that is done outside of SQL, and going to more heavily draw on a statistics / business background. A lot of data science is more about getting the data in a certain format (using SQL) to then have it consumed by another tool such as R, SAS, SPSS, etc. but sometimes its in reverse. If you're going down that road and don't have some kind of MA, then you're probably going to start off somewhere as a generic analyst, and I would imagine your current skills are enough to suffice for a job like that, many of which won't even test your SQL skills at all.
right, so you get duplicate ROWS in both, but not duplicate columns here is your example (also, this is a useful tool for messing around when learning. light weight in browser): https://www.db-fiddle.com/f/ijfFEercRbUDS4Bq9ZnXgJ/0
I usually see tSQLt recommended for the job. Haven’t used it myself though.
This is an excellent overview. I'd like to add one thing to it regarding MySQL specifically: MariaDB is a fork of MySQL with its origins back when Oracle acquired Sun Microsystems in 2009. Being a FOSS product, a large number of the MySQL developers either weren't interested in working for Oracle or weren't offered jobs there and so formed a new company to continue maintaining the application the way they wanted to. Without overcomplicating things by pointing out the areas where MariaDB now differs from MySQL (because this is supposed to be a general overview), suffice to say, where you see MariaDB you should recognize its close relationship to MySQL and not think of it as an entirely different platform. (Heck, the executables even are still called mysql, mysqld, etc.)
Postgres is object relational from the ground up, making it inherently compatible with ORMs. Its lineage goes back to its fork off of Ingres during the days of VAX/VMS. MS SQL is a fork off of Sybase, both with a far simpler typed storage model. MySQL is a step above a Key-Value store, with pluggable ISAM engines. Postgres OTOH presents tables &amp; rows as inheritable types as well as enjoying an extensible engine. Some addit. examples... Postgres can write to Nvidia GPU's, or Sqlite &amp; ArrowDb DB's directly, with distrib. aggregates. PG can support columnar, graph, geospatial &amp; temporal data , supported by specialized indices. Imagine indexing an entire row: CREATE INDEX foo on bar ( ((bar)::bar)); And then finding the slightest changes in a row with just a simple row-level comparator: ;SELECT (bar).id FROM bar JOIN rab ON bar.id = rab.id WHERE (bar)::bar != (rab)::bar;
The problem with Oracle is that it predates the sql standard and in order to be backwards compatible they haven’t changed to match the standard. We were on Oracle for 15 years before moving to Postgres. The move took 1.5 years, but it was so worth it. Postgres is very standards compliant, has superior documentation, has more sane syntax, is so much simpler to do things like replication, and is just as performant.
As icing on the cake NVL isn't even efficient. Instead of just evaluating if the first variable is null it evaluates both variables. I always use COALESCE over it for more efficient queries. It's also part of the ANSI standard while NVL is not. It should be noted that NVL will implicitly convert datatypes while COALESCE will not. So I guess it has that going for it.
You should definitely debate /u/r3pr0b8 about this.
&gt;you use the ISNULL() function to replace a null value in a query result. The MySQL equivalent is IFNULL() and Oracle uses NVL() And the SQL standard defines the function `coalesce()` for that. Which - I think - is supported by all DBMS products as well.
I can understand why several things in Oracle actually violate the SQL standard. What I never understood why Microsoft (or Sybase back then) and MySQL chose to ignore even the most simply rules from the SQL standard e.g. `||` for string concatenation) or `"` for quoting identifiers even though those rules existed long before those products were created.
&gt;MySQL can query from many databases. Postgresql can't (unless you are willing to put your back into it). MySQL's "databases" are actually schemas.
&gt;pl/sql (Oracle), Note that PL/SQL in Oracle is only used to write stored procedures, functions and triggers. The query language is still "SQL". Unlike Microsoft SQL Server, the other database engines (Postgres, Oracle, DB2, Firebird, MySQL, ...) make a clear distinction between procedural code and SQL queries.
Well you should know how to code SQL, and not expect programs to make it for you. It’s a better practise. I used this app to make a reverse engineered diagram so that I can test to see if my own SQL works. And if all the keys are correctly linked Other than that I have no idea if it’s good or not. Seems to have a lot of features though.
I do know how to write SQL. Don't get me wrong. I simply use the GUI to sharpen my skills. I know that on the backend I will be writing pure SQL commands sans GUI. I am just disappointed in the GUI I currently have.
Oh right my bad. I mean I guess you could try to learn it? I only learnt it last month (I’m a student) but depends on what you need it for, if you really want to bother learning it. It’s not too difficult surprisingly.
Yeah, my plan is to do a couple of years where I’m currently interning to build up skills in multiple technologies and tools then do an MSc in data science. Again, thanks for the help!
You take that back. Oracle has no logic.
My example showed how to use plpgsql without functions, triggers, procedures.
Yes, but the part "for manipulating data" could be confusing to a newcomer. You do not use PL/SQL or PL/pgSQL to "manipulate data" in Postgres or Oracle. You use SQL - even inside a procedure or function. And that's the big difference between SQL Server and all the others - it does not make a distinction between procedural code and SQL. I just wanted to make that obvious.
MySQL Workbench is a pile o'crap. I use it on a Mac and it's utterly terrible. I assume it's just as useless on Windows. It has a couple of saving graces and does a few things that tools like Datagrip and DBeaver don't do, which is why it's still my default, but I resent it. I'm just lucky, I guess, that I don't do MySQL as the primary part of my job.
Thank you very much! Works perfectly as I wanted.
Missing months problem I have solved in temp table.
that's not how databases work
try HeidiSQL
 SELECT \* FROM user\_table U LEFT OUTER JOIN photo\_table P ON U.photoid = [P.id](https://P.id) LEFT OUTER JOIN name\_address NA ON [U.id](https://U.id) = NA.userid LEFT OUTER JOIN address\_table A ON NA.AddressId = [A.id](https://A.id) WHERE [U.id](https://U.id) = someuserid
I'm guessing *they* would call it "logic." I sure as heck wouldn't!
Agreed, but I would add the past two years have seen enough significant improvements to the product to consider it different enough (storage engines, improved replications, PL/SQL compatibility)...
That's up to the way the db was designed. Ask your DBA if the vendor table is audited, and if so, how far back the changes are stored.
The correct answer is as rand2012 states. However many commercial applications do have history tables specially for financial data. You will need to see if your application has one. Another avenue is looking at the data in the accounts payable tables. Many commercial applications are 'denormalized' and will capture this data there as well. If you are designing a database this is a consideration.
note that while I included the identity and named its constraint the "PK", I actually didn't assign it AS the PK... officially the PK is the pair, and the table is ordered by the pair... the identity exists solely to make certain maintenance updates easier.
Another alternative to those mentioned: https://www.dbvis.com/
Do you prefer using NVARCHAR over VARCHAR? Thanks!
Answer:. HeidiSQL
select max(Salary) ... group by rank, Salary
That returns the highest salary, not the second highest.
Case is required for an update like this. I would also consider case to be a basic/rudimentary function of sql.
Nothing obvious other than case without using some kind of temporary column/variable/table.
Yea case is your best bet here, and the application of it in this case would be really easy
Select top 1 salary From (select top 2 salary from table order by Salary desc) Order by Salary ASC But I've never cared for that flip flop trick as personal preference, but it works. On mobile and couldn't remember your table name, so I just used "table" because I'm too lazy. Or Select Top 1 Salary From Table Where Salary &lt;&gt; (Select max(Salary) from Table) Order by Salary Desc Both much easier than Window functions, though learning window functions is valuable so if that's the lesson, just ignore this.
There's always a chance. Management did mention they are looking for someone local tho.
This might be easier to just join the table on itself, as I don't think you can reference a windowed function result in an aggregate. SELECT a.Row, a.TestID, a.TestName, a.VariationID, a.Sales, a.Sales-COALESCE(b.Sales,0) AS [Sales Uplift] FROM Table a LEFT JOIN Table b ON a.TestID = b.TestID AND b.VariationID = 0
I'm not positive I understand, but I think you just want to calculate the difference between Sales between rows X and X-1? If that's true, check out the [LAG Function](https://docs.microsoft.com/en-us/sql/t-sql/functions/lag-transact-sql?view=sql-server-2017). If that's not true, correct me and I'll try again. Also, is this something you want to store in the actual table? Or is this a reporting question? Because if it is going to store persistently, you'd want some safe guards set up in case values change.
I haven't been in MySQL for a couple years, but I'd imagine this is just a case for the [RANK() Function](http://www.mysqltutorial.org/mysql-window-functions/mysql-rank-function/). I'm not positive about MySQL, but RANK will return an empty set if you add to the where clause WHERE rank = 2 (you'd need to nest your select to add your RANK field to the where, but easy enough). Also...I'm not sure I understand why you're bothering to do an online problem set when your reaction to a problem is to go online and ask "If you could provide two solutions that each utilize those functions, that would be great. " Is this homework or an interview question or something?
That can easily be solved using [window functions](https://www.postgresql.org/docs/current/tutorial-window.html) \- in this case the `lag()` function with three parameters: select testid, testname, variationid, sales, sales - lag(sales,1,sales) over (order by variationid) as "Sales Uplift" from the_table; lag() can be called with one or three parameters. With three parameters the second one defines the number of rows to go back and the third parameter defines a default value if that "previous" row does not exist. By using the current row's sales value, the uplift will be calculated as zero for the first row.
no, where rank = 2 works before group by I apologize for not providing full query select max(Salary) as SecondHighestSalary from (select Salary, row_number() over (order by Salary desc) as rank from Employee) temp where rank = 2 group by rank, Salary
Using NVARCHAR over VARCHAR is an entirely different set of considerations. NVARCHAR is essentially double the size of VARCHAR in order to support unicode characters for multilingual/global applications. My use of one over the other would be based on requirements. That said, neither NVARCHAR or VARCHAR typically make good keys because they are relatively very large compared to numbers. There are exceptions of course, but most of the time I see these used as key values is because of developer convenience not because of any real business requirement. I strongly prefer to design for scalability, efficiency and performance over developer convenience because I can always build tools/utilizes to make life easier but the database schema is often the foundation of a software stack/application/platform and it is very time consuming and expensive to change the farther along the life cycle it gets. That's because you typically have a cascade of dependencies (references, stored procedures, views, variables, temp tables, scripts, web services, interfaces, data objects, UIs, etc..) that may need to change to stay consistent.
It's been a minute since I've worked with PHP/MySQL, but I think the issue is with your backticks ( \` ). Try replacing them (escaping if necessary) with single quotes and see if that does any better.
I did both `UPDATE CustomersTable SET position = 'Computer Engineer' WHERE position = 'CE'` and `UPDATE CustomersTable SET position = \'Computer Engineer'\ WHERE position = \'CE'\` but still get the same error
To escape a character, the backslash needs to go in front of the character. So with escapes it'd be something like this: &amp;#x200B; UPDATE CustomersTable SET position = \'Computer Engineer\' WHERE position = \'CE\'
Sorry, I just wrote that very fast. I did it correctly in my code lol
Your SQL query makes no sense to me. Selecting then updating, and an `order by` in the update? Is this some kind of weird MySQL voodoo?
I didn't know you weren't allowed to do that but, you're right, after a Google search I learned that you aren't. Thanks. &amp;#x200B; But the following code still gives the same `#1054 - Unknown column 'CE' in 'where clause'` error. $sql = " SELECT * FROM CustomersTable UPDATE CustomersTable SET position = `Computer Engineer` WHERE position = `CE`; ";
I think you want the IN clause; AND a.Area IN ('0018','0015','0006','0022','0016')
Do you need the SELECT? Try starting your query with just the UPDATE command.
Apologies, I'm not as familiar with mySQL and php as others likely are. Why do you need to do the SQL statements together? Seems odd to do: SELECT stuff UPDATE two rows ORDER BY Instead of: UPDATE rows ; SELECT stuff FROM table WHERE condition ORDER BY name ASC ; as either a combined statement or two different ones. What are the results you're expecting to get out of the table, and does this work when you run it directly against the database? https://stackoverflow.com/questions/17583747/select-and-update-in-same-query PHP may not like you having two types of statements in the same query response.
Still getting the same error
That worked. I didn't know the IN clause existed
&gt;I have always read that Python and SQL go together very well This is mainly for ETL type stuff... pulling data from various sources (other DBs, flat files, CSV, Excel, etc...) and transforming and validating the data based on business rules before inserting them into SQL. So if there's a report you need that compares sales to returns, maybe your CRM is the only place the returns information lives and your sales data is on a separate ERM platform... so you can use Python to pull those two sources of data together to generate a single result set / report / dashboard.
Try: UPDATE CustomersTable SET position = \`Computer Engineer\` WHERE ID in (select ID from CustomersTable where position = 'CE'
That code still makes no sense. Why are you attempting to do it this way? Why are you selecting from and then updating the table in the same statement?. I don't think it'll work in the first place.
That's a great usage example.. do you think that's preferable to something like PowerBI tools in Excel?
I have a few problems with this: In practice I need to update more then one value (I thought giving a more simple example would keep things minimal, for you kind people to help me). Another thing is my table is getting uploaded \[every hour\] from the *web master*. I am trying to make a product page. &amp;#x200B; I am very new to SQL so I apologize if I didn't respond in a useful way.
&gt;But in practice I have a few hundred rows that need to be updated. &gt; &gt; &gt; &gt;Sorry if I didn't reply in a useful way i'm very new to SQL
Personally, I'd rather use Power BI, SSIS, SSRS, SQL Linked Servers, etc... but sometimes those aren't the best tools to connect to the sources and using C# or Python or some other actual programming language gives you more flexibility and control, especially when it comes to complex business rules and requirements. I've dealt with writing business rules into SQL Stored Procs and directly in SQL via lengthy CASE statements and... it's rough, programming languages are so much better for those scenarios.
Sorry, i'm very new to SQL. Are you suggesting I do this instead?: // SQL block 1 $sql = " SELECT * FROM CustomersTable "; // SQL block 2 $sql = " UPDATE CustomersTable SET position = `Computer Engineer` WHERE position = `CE`; ";
Yes, that is what you need to do.
Assuming that's what OP is trying to achieve. It's not clear.
I'm trying to make a product page with a database that gets updated by a third party. I can't change the values so I figure I can `UPDATE` them in SQL, to pretty them up for the end user. &amp;#x200B; Does this make sense? Again, please excuse me i'm very new and have low confidence in what i'm talking about lol
You're approaching this like someone with a background in like C or something. SQL is not like that. You update a column WHERE other column = VALUE. So instead, why not just update case when position = certain then position = computer engineer?
&gt;Yes, that is what you need to do. Assuming you just want to read the table, send it back to the user, then perform an update. It still didn't work. I got the same error.
So what if you have a few hundred rows?
Just to be clear is this what you're suggesting?: &amp;#x200B; ... $sql = " CASE WHEN position = CE THEN position = Computer Engineer ELSE Employee END; "; ... Sorry if I didn't reply in a useful way, i'm very new to SQL
Why are you using a variable?
Then you need to perform the update first, then in a second transaction perform the select.
Referring to: `(select ID from CustomersTable where position = 'CE')` There are many IDs and more get added every hour. I'm under the impression you're telling me to manually input the IDs. Is that right? &amp;#x200B; Sorry if i'm not replying in a useful way, i'm very new to SQL
If you literally just replaced this: $sql = " SELECT * FROM CustomersTable UPDATE CustomersTable SET position = `Computer Engineer` WHERE position = `CE`; "; with this: // SQL block 1 $sql = " SELECT * FROM CustomersTable "; // SQL block 2 $sql = " UPDATE CustomersTable SET position = `Computer Engineer` WHERE position = `CE`; "; ...then it's not going to work. You need to execute the two queries and fetch their results separately as well. And also I thought this was explained to you in another comment but first off lose the backticks. So for example you want: position = 'Computer Engineer' Depending on how PHP works you may or may not need to escape those single quotes. If it's smart it shouldn't be necessary, but I've never never touched PHP in my life so I don't really know. Additionally, be aware that you are simply wiping out the variable $sql by placing one assignment statement after another like this. You should either make them $sql1 and $sql2, OR do something like (in pseudo-code): set $sql for first query execute query fetch results and such set $sql again for second query etc. Another heads up, as someone who has used Java quite a bit (but again, not PHP) your database driver may require a separate function for select queries and update statements. So it might be `$conn-&gt;query($sql);` for a select and `$conn-&gt;update($sql);` (you'd have to look it up, of course, I am just guessing it's "update")
So, there may be a misunderstanding here. Regardless of how frequently or infrequently the database gets updated, your query will respond with the data based on the moment a user hits the page. Based on your comment [here](https://www.reddit.com/r/SQL/comments/cgg3j3/really_struggling_with_update_here/euh2dq9/), you're trying to make the user see "Computer Engineer" instead of "CE". You can do this with CASE statements, or with PHP code in the display. https://www.w3schools.com/sql/func_mysql_case.asp SELECT number, name, position, CASE WHEN position = 'CE' THEN 'Computer Engineer' WHEN position = 'WR' THEN 'Will Rodgers' END AS 'Position Description' FROM your table I don't know enough php to guide you in doing this with code but it should not be difficult.
Sorry, let me try again. &amp;#x200B; I have a table with a lot of abbreviations, I can't edit the table but I am displaying it's values on a web page. To make things look nicer for the end user I want to change the abbreviations to their full words. &amp;#x200B; Again, I don't have permission to edit the values of the table but I can query the values.
Hi, there are a lot of posts here but they seem really unhelpful by asking you questions rhetorically, which helps nobody. It really sounds like you should read up some more on SQL as you're not asking the correct question. However....... You do not need to do an UPDATE at all, this changes the underlying data in the database which you've said you cannot / should not do. Instead it sounds like you would like to display a "friendly" name to the user based on a column. You can do this with a CASE statement: SELECT CASE WHEN position = 'CE' THEN 'Computer Engineer' WHEN position = 'OT' THEN 'Other' ELSE position END as 'Position' FROM CustomersTable This will change the `position` column to 'Computer Engineer' when it's 'CE' and 'Other' when it's 'OT', otherwise it will return whatever the value of `position` is (unchanged). https://www.w3schools.com/sql/func_mysql_case.asp
So you want to change the output without changing the underlying tables? THis is SQL Server syntax but should be close select number , name , case when position = 'CE' then 'Computer Engineer' when position='WR' then 'Wide Reciever' else position end as position from CustomersTable order by position Alternatively (but this is a pretty bad idea) select * into #CustomersTemp from CustomersTable update #CustomersTemp SET position = 'Computer Engineer' WHERE position = 'CE'; select * from #CustomersTemp order by position
No that nested select statement should make it so that it will grab every ID where position equals 'CE' and replace it with 'Computer Engineer'
No, manual adjustments are never a best practice. Is the app where people enter this data placing this? If so maybe you need to convert it earlier in the pipeline
After reading the w3schools article on *MySQL CASE Function* I think you're right that this is closer to what I want. That said I get the No Results page and when putting it in phpMyAdmin SQL console it didn't give me any error but an alert saying "Unrecognized statement type. (near **CASE**)"
I don't have access to convert it earlier in the pipeline. I'm getting my table values from a 3rd party. I can only query the data
 sql = " SELECT * UPDATE stones SET stonetype = `Sapphire` WHERE ID in (SELECT ID FROM stones WHERE stonetype = 'SA') FROM stones; "; Didn't work
So I should change the queried value in another language?
Maybe I just haven't learned this yet, but I feel like an UPDATE statement shouldn't be used in a SELECT statement like this? What if you just ran the UPDATE stones SET stonetype = `Sapphire` WHERE ID in (SELECT ID FROM stones WHERE stonetype = 'SA')
you could use python Database drivers or ORM libraries (SQLAlchemy) to create some kind of CRUD app or some other app that leverages database entries
This one should be higher. LAG won't work here because every uplift is calculated against variationid = 0. The other solutions are comparing it to the previous variationid.
Then setup a view with logic that changes it.
This worked!!! But how do I `SELECT` other values after this. SELECT * FROM CustomersTable; SELECT CASE WHEN position = 'CE' THEN 'Computer Engineer' WHEN position = 'OT' THEN 'Other' ELSE position END as 'Position' FROM CustomersTable Doesn't seem to work
Woops I copied the actual values by mistake. &amp;#x200B; I'm starting to learn \[from this thread\] that I should use `CASE` function
Nope, you can't have more than 1 select keyword. What you'll want to do is just continue to add to the select list, it's a simple comma-separated list. So you can do this: SELECT *, CASE WHEN position = 'CE' THEN 'Computer Engineer' WHEN position = 'OT' THEN 'Other' ELSE position END as 'Position' FROM CustomersTable To select "everything", then your new calculated column. Or you can (and should) specify each field you want to select manually: Select Column1 ,Column2 ,Column3 ,CASE WHEN position = 'CE' THEN 'Computer Engineer' WHEN position = 'OT' THEN 'Other' ELSE position END as 'Position' ,Column4 FROM CustomersTable
Jesus man I've been working on that all week. Is there anything you need from me? lol I don't know how to repay you.
If the VariationID 0 is the problem, you can use OFFSET 1 ROW.
Just glad to help. :)
Okay, end of the month if I can afford to give you gold I will. I just put it as a reminder in my phone. I recognize that this might be your job and am genuinely thankful for your help. Other wise i'd probably be working on this for a month lol
I figured it out! I made an error in my group by clause. That’s what was causing my errors. It clicked when I fixed the original error and this popped up. I grouped by everything Max(b_gm_enrollingdate) as enrollingdate - - - - Group by *all columns not enrollingdate* and it gave me what I needed. It’s not the prettiest code, but it was functional. Thanks for all of the advice! I appreciate it and I learned something.
Which database/version? Offhand it sounds like a job for window functions.
 SELECT a.ID, a.FirstName, a.LastName, a.MailingAddr1, a.MailingAddr2, a.MailCity, a.MailZip5, CONCAT (a.CellAreaCode, a.CellTelephoneNum) [Cell], CONCAT (a.LandlineAreaCode, a.LandlineTelephoneNum) [Landline], a.Area [Area] FROM databse1 AS a JOIN database2 AS b ON a.ID= b.ID WHERE ( a.CountyName='TheCounty' AND b.Modeldatekey = '20190610' AND ( b.PositiveScore - b.NegativeScore) &gt;=0.4 AND a.Area = '0018' ) ( OR a.Area ='0015' OR a.Area ='0006' OR a.Area ='0022' OR a.Area ='0016' ) ORDER BY a.LastName Your parenthesis are incorrect.
Haha, don't even worry about it. I would highly suggest you go through the w3schools SQL tutorial. It's very basic, but will give you a really good understanding of the syntax!
I am using Microsoft SQL Server Standard Edition (64-bit) 10.50.6560.0
Thanks for your help. I was able to accomplish my goal with a CASE function
I’ll do it Sunday morning and night. I also have to get more familiar with Angular next month
Wow apparently that's SQL Server 2008. :) So assuming that one supports window functions (it probably does but not 100% sure), you should be able to do this. I don't have the time at the moment to work out a solution (I would like to later because I enjoy this kind of thing), but definitely read up window functions if you haven't before because they're most likely what you need.
&gt; BETWEEN is the same as writing &lt;= and &gt;, no BETWEEN specifically includes the endpoints, the a strict inequality like `&gt;` rules that out and anyhow, it should be `&gt;= and &lt;` for example, foo BETWEEN '2019-07-01' AND '2019-07-31' is problematic because is excludes datetime values in the 31st after midnight, whereas foo &gt;= '2019-07-01' AND foo &lt; '2019-08-01' captures those datetimes but not midnight on the 1st of the next month
Forget about the 1st of the month because you can get around that by adding one. Do both execute the same and produce the same plan / speeds, or is one clearly better than the other as it relates to leveraging an index?
I'd simply join the `VariationID = 0` row and calculate it, as proposed by /u/xeonotic. If you're dead set on window functions, or you don't know the lowest VariationID for each TestID, you can use `FIRST_VALUE` SELECT d.Sales - FIRST_VALUE(d.Sales) OVER (PARTITION BY d.TestID ORDER BY d.VariationID ASC) AS SalesUplift FROM yourData d Or some such.
my layman's understand of how SQL is executed is that BETWEEN is always converted into two comparisons that are ANDed together so execution use of an index should be identical and be careful about "get arounds" as my SQL mentor used to say, a query that gets the correct result, no matter how slowly, is ~way~ better than a query that gets an incorrect result blazingly fast
The best would probably be either the Oracle OCA for Database or an MS SQL Server cert of some sort. Don't know of any others (they probably exist, but won't be taken very seriously).
Whichever flavor of DBMS you choose will have a manual with a Sample database you can download to use for instance MS SQL has Northwind database.
Windowed functions are ideal, as mentioned before, but I think in a pinch something like the following could work; the gap with this method would be that it relies on the sequence being FAIL &gt; PASS, and not FAIL &gt; PASS &gt; PASS (in other words, if it failed at 1:22PM, and then passed at 1:59PM, and then passed again at 8:36PM, it wouldn't bring it in), though that may also be an issue with windowed functions if you're strictly looking at #1 and #2 in sequence. SELECT * FROM ( SELECT Serial_Text, COUNT(Date_Time) Records, MAX(Date_Time) LAST_RECORD, MAX(CASE WHEN Passed = 1 THEN Date_Time END) MAX_PASS, MAX(CASE WHEN Passed = 0 THEN Date_Time END) MAX_FAIL FROM Table GROUP BY Serial_Text) a WHERE a.Records &gt; 1 AND a.LAST_RECORD = a.MAX_PASS AND DATEDIFF(hh, a.MAX_FAIL, a.MAX_PASS) &lt;= 2
The only certs worth anything are the ones offered by the platform vendors themselves (Oracle &amp; Microsoft, for example). However, a cert is not necessary and does not prove that you actually *understand* the material. Many people (at least in the SQL Server community) put little to no stock in the papers themselves. Easy to understand when (as I found out this weekend) you can pay a company in China to take the whole exam for you. Not to mention the braindumps and week-long classes that will teach you the answers to the questions so you can pass just by rote memorization. They're multiple-choice exams. Managers may love seeing certs but they demonstrate literally **nothing** about your ability to do the job.
No. Use a proper /r/businessintelligence tool over random python scripts. Most paid BI tools can also do the ETL for you. But sometimes it just feels easier to have python parse your CSV and load it into your main Database.
Create an api for your data. Could become high profile in your company if you manage data that can be used by other LOBs and/or applications. Find open source information that can compliment your existing data. Use python to get the data, massage it and dump in to a table.
https://i.imgur.com/xioMcFe.jpg
2008/2008R2 had support for _some_ windowing functions but not the full array of them.
You need to change this. That version has gone EOL as of earlier this month and you/your organization should have migrated off it by now.
&gt; For instance MS SQL has Northwind database. Northwind was put out to pasture a decade ago. We have AdventureWorks and WideWorldImporters now. https://github.com/microsoft/sql-server-samples/tree/master/samples/databases
Weird, I still see Northwind references when I look up commands sometimes after forgetting specifics. Anyway regardless of the old man db reference, point still stands OP should look up his documentation and get whatever they use.
Stuff sticks around on the internet for a long, long time. I haven't seen anyone do "new" demos or examples with Northwinds in forever.
Thanks
This. If you don't understand the logic behind the ANDs and ORs, you're going to have issues with your SQL.
This is my understanding, BETWEEN isn't a function and doesn't prevent you taking advantage of an index. I always opt for using two comparisons, because I don't like how BETWEEN means having a special AND with a different meaning to the other ANDs in the WHERE clause, and because I can never remember which side of the BETWEEN is inclusive.
Build a web app. Giving non IT people access to a management console is a recipe for disaster.
You could try making a UDF https://aws.amazon.com/blogs/big-data/introduction-to-python-udfs-in-amazon-redshift/ or try Pandas on top of your DB to do some ML or more customized viz. Redshift example: https://blog.panoply.io/connecting-jupyter-notebook-with-postgresql-for-python-data-analysis
Awesome! I love your ideas
If you are doing any kind of data analysis, data engineering or data science, then SQL and Python will go very well together. I'm newish to Python and SQL. I have a ton of experience using R and SQL. In general you will want the drivers to your database, pandas, and a plotting library such as seaborn or matplotlib. From there you can add to your tool chest as your needs increase.
I concur. I think the best way to go forward would be to hire someone who can provide some management sql functions at a technical level.
How hard was R to use? Do you think it was an essential language to learn? How often do you use it?
Thanks! I will try this on tomorrow. The fail&gt;pass&gt;pass situation shouldn't happen as a unit shouldn't be tested again after it has passed. I can live with the .001% discrepancy when/if that does happen.
Yeap. I use PL/Python (and other PL languages) all the time inside of PostgreSQL. I've some in-database machine learning and can say that PostgreSQL is wonderful for this. PG also has linear regression functions and clustering algorithms (with PostGIS) built in. Most of the time I'll grab and load data from the web or elsewhere with Python and then do the heavy data tasks in PG and then again I'll use Python to graph query results to view ML error metrics etc in a Jupyter notebook. Python and Postgres make an excellent team.
I think R is good to have under your belt because sometimes it's a lot easier to do it R than in Python. I don't think it's essential. R is easier for some people and for some reason that I can't understand, giberish to others. I think that Python is wordy and everything feels like it was tacked on, because it was. But Python is the current people's champion, so until something better comes along, it's the programming language to learn. For engineering and some data science stuff, it's better than R. R is better for statistics, plotting and quick reports. I used R about once a week. I used it daily for a long time. These days I use Python daily and R when I need to calculate things for which I can't find anything using Python. For a long time my suggestions where SQL, a programming language, Excel, a plotting program such as tableau. These days the lines are so blurred that I would learn whatever you can to make your life easier.
Most BI folks that I know still make their fancier dashboard using Tableau and Power BI, because they are more polished as BI solutions go. Nothing gains business user trust better than fancy visuals. But I mean.. I personally use Python and Postgres hand in hand for the whole shebang. That being said, I'm fairly light on the front-end analytics, and more on the engineering side. So that means a lot more ETL, data maintenance task, and machine learning rather than reporting, data viz, and presentation. I grab and load data with Python and maybe do a bit of precleaning with Pandas. Then I do the bulk of the data work in SQL. PG if I have the option (you can use PG for machine learning stuff as well), and use Python again for data viz.
Thank you!
Thanks this is extremely helpful :)
Thank you! Very insightful
No problem
Is your reasoning for not wanting the additional AND simply aesthetic? No judgement, just curious! Also, both sides of the BETWEEN are inclusive.
Aesthetic and I think there's a slight readability/comprehension shortcoming to using the same word in the same clause as one commonly used to have a different meaning. It's never actually tripped me up that I can recall though.
AdventureWorks
https://datahub.io/search https://relational.fit.cvut.cz Working with data you are actually interested in can make learning alot more productive &amp; fun. For example I have been using some football stats tables to practice queries.
Put your query in CTE. Select from Employees and left join to your CTE.
I have installed DBeaver on a couple of workers computers to allow them to make changes to data. If any prompt comes up they will blindly accept it and mess things up. But if your permissions primary keys and foreign keys are in place you can limit the damage they can do. The main benefit is that it's similar enough to Excel.
[DVD Rental](http://www.postgresqltutorial.com/postgresql-sample-database/) for Postgres [AdventureWorks](https://github.com/lorint/AdventureWorks-for-Postgres) for Postgres
The central table is a revisioning containing just metadata like the ID, creation date and order/customer/contract ID's. EVERY table that is revisioned has two FK's to the revision table. To get simple informations, like the current products for a customer, you have to do over 12 joins. 8 of them to the revision table. Great work ... great work ...
R is fading away in favor of Python. R is more compact for the same things, but Python is much more rich in features and works better with a lot more data sources. I like R, but Python is the thing now.
Python and databases is a true story of love ! Connectors libraries are almost normalized. For example psycopg2 (for postgresql ) and the mydql-connector are quite the same in their function and usage !
Is it a web database ? I've always used phpmyadmin with mysql. More over if it isn't a web database, i've already used heidisql. It's a powerfull open source editor/client for many SGBDR such as MySQL.
I think everything but lag/lead.
HeidiSQL
Holy shit, who came up with that?
I would parse the data from the bundle_number column in TableA into separate columns that match tableB. Find out how you need to write your code to isolate each element from inside the string. You can use a combination of the string functions to do that. https://www.w3schools.com/sql/func_sqlserver_charindex.asp Then you create a view of that table with all your extra columns added. https://www.w3schools.com/sql/sql_view.asp
For general use but not web base the community edition of dbeaver is pretty good.
If you're wanting to be lazy and not type out all the column names you can query the schema SELECT CONCAT ('and ', + COLUMN_NAME, +'=No' FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = 'SalesTable') It'll build out most of the where statement for you. If you're just doing it as a 1-off. If you wanted to get really fancy you could write SQL where after it's all been concatenated it would be an executable statement. Then you can call the stored procedure and execute the code it returns.
I replied how to do it.
I would say this has "historical reasons". As the project started, there where only a few tables. Over the last years many new tables and usecases was introduced. And our company growed. Huge. From something the ten thousands we are now around 10 million entries.
Fixed, well not. MS are deprecated the passing of plain-text passwords for security reasons.
Hey, i just tested your query and it doesn't seem to work however the thought was correct. The thing is that concat only concats results from columns (horizontally), but we want to concat rows (vertically) therefore we need those 2 options mentioned above i used [https://rextester.com/l/sql\_server\_online\_compiler](https://rextester.com/l/sql_server_online_compiler) to test it online it is sql version 2014 though and doesn't include the string\_aggr function yet &amp;#x200B; /* i just concated columns of some random test table but you should be able to figure it our for your own use case */ // this is with string_agg function, i assume it should look like this SElECT String_Agg(A,' =''NO'', AND ') FROM ( select Table_Name A from information_schema.columns where Table_Name = 'Pilots' ) // this is with the for xml path hack, it was initially not intended for this task but is widely known, you can google for xml path and there are some neat explanations how this actually works SElECT cast('AND ' + A +' =''NO''' AS nvarchar) FROM ( select Column_name A from information_schema.columns where Table_Name = 'Pilots' ) B FOR XML PATH('') // for both queries you have to trim the result accordingly
Thank you. I just emailed my DBA about it .
Barring some weird constraint (like 3rd-party software that doesn't support anything newer, in which case you need to upgrade or ditch _that_ software), your DBA should have been all over an upgrade long ago.
You can certainly update a table with the correct value. If you need to use this automatically look up LAG() function or Window functions in general. Also, consider putting constraints in place that will ensure data integrity, so this mishap won't happen in the future.
I was being lazy because I can't format my code in reddit and was getting annoyed. I was going to write SQL that selects 3 columns from the schema The first part of the statement "and", the second part "COLUMN_NAME" and the third part "=No". Then nest it in a statement that Concatenates them.
Agreed. Unfortunately, my ccompany moves at a glacial pace.
I get what you're trying to do now, ignore my previous comment.