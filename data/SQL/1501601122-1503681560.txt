Oh, my bad, didn't know about those bylaws. It's a string compilation error, so it's probably a typo in the Parameters.Add or in the string query declaration. I'll keep that in mind next time, thanks for pointing it out and sorry on my part XD
You also mentioned adding tables and columns. Jut keep in mind it's a little trickier with sqlite to remove or modify columns. If you will need to make more complicated schema changes, and frequently, might be worth using something else. https://www.techonthenet.com/sqlite/tables/alter_table.php This also looks like good advice: https://stackoverflow.com/a/846665
They're not so much "bylaws" as guidelines to live by if you want to get valuable help from people who are volunteering to help you.
Right, but how are you going to *execute* all that dynamic SQL without a cursor?
I get this is not SQL specific, but it is still relevant. Here you go: https://medium.freecodecamp.org/the-100-correct-coding-style-guide-5b594a1655f0 
What we do in the Netherlands is add an abbreviation containing the province/stste/whatever so it'd be Portsmouth (TX) if there was one in Texas. It could also be that it just doesn't matter, as you'll have a second field with the state or zip so you know what Portsmouth you're talking about. If you're going to analyse later on you can use the additional data for this. edit: I just saw your other comments. If exact location is very important to you then just ask the zip code, next to this have a separate table containing all info related to the zip code (street,country,state,country,etc.etc.etc.) and the issue disappears. You'll effectively be using zip code as a foreign key (Would you find that as a good idea? at what point do you stop find normalization effective vs not effective?). There's also the performance aspect. If you have 5 million rows and every row has a column for city,state in a NVARCHAR(whatever) column whereas it could just be an INT column you will notice. Sure you will say that the joins will also affect performance but these will be efficient joins. Your UPDATE statements will be quicker (updating ints vs strings), you will have improved data integrity (you dont have CITY in two different tables such as customer data table and the salesorder table -- no redundant data), your indexes will be faster etc. etc. etc. Lastly the reason listed elsewhere in this topic, you can use your separate table to be loaded into a dropdown menu for easy selecting. You're right that there's no reason to put *every* piece of data in a separate table, normalization more often than not is the correct way to do things. Your google-fu might be 'top-notch', but I'd spend a bit more time working on your SQL-fu before ranting on how normalization is 'just bad' cause you don't like typing a few joins.
With the EXEC command. You execute a batch with multiple statements. Or Ctrl+C, Ctrl+V, F5. Also, registered servers is pretty sweet for running scripts on multiple DBs.
heres a imgur link to the screenshots if it helps http://imgur.com/a/vTB9m
It really depends on the situation. Please see my reply above.
&gt; With countries you already have a fixed number of countries beforehand That depends on your [definition of a country](https://en.wikipedia.org/wiki/List_of_sovereign_states), not to mention the changes that happen annually due to wars, unions, secessions, etc. It's more frequent than "every now and then" and definitely not "almost never".
Have you looked at the profiler? Are you using global temps? I see in your screenshot you have PID 442 blocking 420 with similar latches. Maybe you have a process blocking itself? Try running a filtered profiler on the temp db for awhile, gather some of the PIDs that blocked and cross reference.
Sure, you have a point. I guess you're just going to take a look at what the intended use for your database is. With regards to the countries, think of it this way: Are you going to let your users enter their countries in a freetext field or from a dropdown box with a list of countries? If it's the first you know beforehand the data will be close to useless, if it's the latter you already have a list of countries: add a key column and you have your foreign key table. It might need to be maintained every now and then, sure. If your app is so gigantic that it's used in every corner of the world and you cannot maintain it by hand anymore, just pay for a service that provides you with an updated list of [countries, cities and whatever hell else you want](http://www.geonames.org/export/) -- specific country example [here](http://download.geonames.org/export/dump/countryInfo.txt). This data is updated daily. I now see that you are a SQL Server DBA and all this info isn't new to you. My example of countries never changing came from my personal experience where our company only sells to a few multinationals in other large countries. Sure we have a complete country list but it basically never has to be updated as with the business we are it it will NEVER happen that a company in south Sudan buys one of our products (if that would be a contested country).
Differences in Joins? Nope. No one can. 
&gt; There's also the performance aspect. dis gone be gud &gt; If you have 5 million rows and every row has a column for city,state in a NVARCHAR(whatever) column whereas it could just be an INT column you will notice. yup... now describe a situation where someone will be searching for city 10634 in order to get that city's attribute data &gt; Sure you will say that the joins will also affect performance but these will be efficient joins. that's quite a bit of magical handwaving right there &gt; Your UPDATE statements will be quicker (updating ints vs strings), when was the last time you wanted to update city 10634 to 20731?? that just doesn't make any sense &gt; you will have improved data integrity (you dont have CITY in two different tables such as customer data table and the salesorder table -- no redundant data) oh, please... do say the same thing about first names, would you? &gt; Lastly the reason listed elsewhere in this topic, you can use your separate table to be loaded into a dropdown menu for easy selecting. this is going to come as a complete shock to you, but *you can do the same thing with just the city name by itself!* 
Alright Dis-gun-b-good kiddie &gt; yup... now describe a situation where someone will be searching for city 10634 in order to get that city's attribute data I don't understand how this would be an issue. If you need your attribute data along with information in the table containing your FK, just join. Heck, you're answering your own question: You're saying that you are maintaining attribute data on CITIES, in what other table are you going to maintain that? on every row of the CUSTOMERS table where you've stored the city name in plaintext? &gt;that's quite a bit of magical handwaving right there Please explain the handwaiving. Or better yet, as you're ignoring every other part of my post, explain to me how joining is worse than storing everything plaintext (and not having extra attribute info anywhere else?) Or will you have a CITIES table with the city name as the PK and attribute columns, on which you will join if you need extra info? Do explain. &gt;when was the last time you wanted to update city 10634 to 20731?? I honestly don't know what you're talking about. My users would see a dropdown box in which they would select their city. This value would be linked to 20731 which will be used to update from 10634 to 20731. The user doesn't need to see 20731, why would they? What part do you not understand that for you it doesn't make sense? &gt;oh, please... do say the same thing about first names, would you? If I have a user's info in multiple tables I'm not going to store the user's name over and over again am I? The user just has his identifier and you use that. If some attribute of the user changes it gets changes in the USERS table or whatever. I'm talking about redundant data between tables, not within one table, one of the things that normalization solves. &gt;this is going to come as a complete shock to you, but you can do the same thing with just the city name by itself! Either we mean exactly the same thing or we completely don't understand each other? Could you clarify? 
&gt; I honestly don't know what you're talking about. i'm not sure i have the time to explain it to you, or the desire CREATE TABLE cities ( id INTEGER NOT NULL PRIMARY KEY , cityname VARCHAR(99) NOT NULL ); INSERT INTO cities VALUES ( 10634 , 'Biloxie' ) ,( 10635 , 'Butte' ) ; UPDATE cities SET id = 20731 WHERE id = 10634 ; p.s. i'm not a kiddie... i started university in 1967 
No i haven't used profiler against TempDb yet...and i'm not sure about Global temps..i'll have to ask. Thank you for the suggestions! 
My question is, why would you have to do that? If a city stopped existing just don't use that identifier anymore? This is why i asked my final question, I have the idea that we are talking about two different things. The setup of your cities table is correct but the operation you're performing isn't. id is your primary key so you cannot change id's. If you meant to say update customers set city = 20731 where city = 10634 then why would you have to do that? The city stopped existing? This is something a user would do via your front-end interface.
Hey do you have any suggestions on what columns to use to filter JUST tempdb activity? i tried by db name "tempdb" and db id "2" but i dont seem to be getting what i expected...
Plenty of other jobs that use SQL that aren't DBA.
selet name, country from person a, address b, city c, country d where a.address_id=b.address_id and b.city_id = c.city_id and c.country_id = d.country_id; 
Cursors are great for utility scripts -- as in scripts that will only be used by developers/support staff to diagnose and fix common issues. No need to spend an extra 2 hours writing a script the "right" way just to save 30 seconds of execution time on a script that's going to be run once or twice a month.
 Using min on [Payment Date] is still bringing back multiple lines. Any tips on a sub query in the 'where' clause? select b.bill_num , c.client_code , c.client_name , convert(varchar, b.BILL_DATE, 101 ) [Bill Date] , min(convert(varchar, ba.TRAN_DATE, 101 )) [Payment Date] , cast(DATEDIFF(dd, b.bill_date,ba.tran_date) as decimal (8,2)) [Days Billed to First Pay] , sum(ba.sign*(ba.HARD_AMT+ba.SOFT_AMT+ba.FEES_AMT)) [AR Total] from blt_billp as bp join BLT_BILL b on bp.bill_tran_uno=b.tran_uno join hbm_client c on bp.payr_client_uno=c.client_uno join BLT_BILL_AMT ba on bp.bill_tran_uno=ba.bill_tran_uno and bp.payr_client_uno = ba.payr_client_uno join HBM_MATTER m on ba.matter_uno=m.MATTER_UNO join HBM_PERSNL p on m.BILL_EMPL_UNO=p.EMPL_UNO where c.CLIENT_CODE &lt;&gt; '13323' and p.employee_code &lt;&gt; '099' and p.TERMINATE_DATE is null and c.CLIENT_CODE = '08804' group by b.bill_num , c.client_code , c.client_name , b.BILL_DATE , ba.TRAN_DATE having sum(ba.sign*(ba.HARD_AMT+ba.SOFT_AMT+ba.FEES_AMT)) &lt; 0 order by 1,5 desc 
Did the disk usage report verify that the .ldf *itself* was 99% free space? If not, use DBCC SQLPERF(logspace) assuming MSSQL. If you're on full recovery mode, transaction log space won't be freed up until a differential or full backup is taken.
The transaction log space used Is showing 96% free as of right now. We take nightly backups. 
Ok, I wanted to give you an actual answer. I was mostly trying to be funny on my other. Do you know that your definition of 'Entry Level' is the same as the hiring manager? Is it the same as the operational manager? Is it the same to your potential peers? The answer is No. Your first step is to identify the landscape and scope what is required. Once you understand the position, you assess your CAPABILITIES. Many job applicants make the mistake of what they know versus what they can do. These aren't always the same thing. Yes, you have some knowledge of SQL syntax and fundamentals now, but are you capable of learning as you go? Can you solve problems you've never seen before given adequate resources? These are the questions you have to live by. We can only learn that which is unknown to us. So don't consider yourself or the job 'Entry Level'. Properly assess both and then qualify your capabilities against the requirements. It's your duty to convey this to the interviewer; theirs is to validate it compared to the position. If they ask you about something you know nothing about, tell them "I have yet to utilize that command/process. However, I once was faced with ______ and I used my resources to quickly come up with a solution. This is how I learned to use ______." The interview will either qualify you or disqualify you. That is not your fault, it just means you weren't a match. But now you have a lead on what to study up on next. TL:DR _"Entry Level" Title is BS. Figure out where the line is and compare it to where you're standing. Always take the chance and work out your bugs later. We can't wait for all bugs to be removed before release. Be the working version and improve while in Prod._
&gt; `WHERE DATE_1 like '%-JAN-16' ` It's never a good idea to treat a date like a string. Depending on the `NLS_PARAMETERS` of the session executing the query, the date format might be different, so you don't want to rely on implicit conversion. Instead, I would either explicitly convert to a string using `to_char`: where to_char(DATE_1, 'YYYYMM') = '201601' or do the ol' switcharoo and convert the string to a date, along with `TRUNCing` the date down to the first day of the month: where trunc(DATE_1, 'MM') --This makes MM the most granular date element, so DAY is set to 1 and the time becomes midnight. = date'2017-01-01' --the date'x' literal supports the format YYYY-MM-DD. You could also use TO_DATE('01-JAN-16', 'DD-MON-YY'), which lets you specify the format you want to input. &gt; (from your replies to other posters): `and ((extract (month from t1.date_1)) = (extract (month from t2.date_2))` Extract is neat, but it's not ideal because you have to check both MONTH and YEAR separately. I'd use `to_char` or `trunc` like above instead. I prefer trunc because I like dates to stay as dates instead of strings when possible. Try this: select distinct UNIQUE_ID from ( select UNIQUE_ID, trunc(DATE_1, 'MM') from TABLE1 intersect select UNIQUE_ID, trunc(DATE_2, 'MM') from TABLE2 ); 
&gt; c.CLIENT_CODE &lt;&gt; '13323' &gt; and p.employee_code &lt;&gt; '099' &gt; and p.TERMINATE_DATE is null &gt; and c.CLIENT_CODE = '08804' not a MS user here, but adding something like this would/should work, depending on what fields are on BLT_BILL_AMT: and ba.TRAN_DATE = ( select min(bb.TRAN_DATE) from BLT_BILL_AMT bb where &lt;bb.field1&gt; = &lt;ba.field1&gt; and &lt;bb.field2&gt; = &lt;bb.field2&gt; etc 
&gt;The database is in full recovery because we mirror the database. Has the mirror fallen behind/out of sync? An out of sync replica will prevent you from shrinking the logs.
Point taken on treating a date like a string. I'll add the TO_CHAR function to that portion of the SQL. I did take the previous advice already and removed the EXTRACT functions in favor of doing TRUNC(Date,'Month'), if only because it's less messy, but your point about leaving dates as dates when possible is also good advice.
Last night I checked and it was in sync. Since last night, it has failed over, and my mirror is the principal and vise versa. Its showing restoring right now. Not sure why it failed over... 
The PIVOT solution is correct, but in case your version of SQL doesn't support pivot, an alternative that should work would be: select companyid, max(case when job = 'CEO' then name else '' end) as CEO, max(case when job = 'CFO' then name else '' end) as CEO from table group by companyid
I've been trying something like this - but the logic of what's going on here is escaping me...and that's making it hard for me to write.
Unfortunately that's my DBA knowledge about exhausted! 
Imagine you and me are both on a db table called yourtable, with, I dunno, just transaction dates and a unique id. You wanted to select your minimum transaction date. You want to exclude me (and anyone else). It would be: select id, min(transaction_date) from table where id = &lt;you&gt; If you want both our minimum transaction dates, your subquery needs to do the min, and you bind the unique id's together: select a.id, a.transaction_date from yourtable a where a.transaction_date = (select min(aa.transaction_dt) from yourtable aa where aa.id = a.id) So the subquery is doing the min select the first query does, except you're binding a copy of the table (alias a) against another copy of itself (alias b). 
I hear ya. Ive read where I can change the mode to simple, shrink it and put it back to Full ... But I don't know what that will do to the mirroring.. 
Join the table to itself on the Teams value being different: SELECT One.Teams, Two.Teams FROM test One INNER JOIN test Two ON (One.Teams &lt;&gt; Two.Teams) 
One small thing... check the log_reuse_wait_desc value in sys.databases. May be a clue.
I've seen it happen that you need to run a transaction log backup, then the shrink, another transaction log backup and the shrink command again. Only then did the transaction log release the unused space.
https://www.google.com/search?q=running+sql+files+in+oracle&amp;ie=utf-8&amp;oe=utf-8&amp;client=firefox-b-ab Go the extra mile: analyse the sql files line by line, understand what they do and why.
True, I wrongly assumed we were talking about DBAs, sorry, didn't mean any offence, but the same stands for almost any SQL job. If you're trying to BS your way into a SQL job and the person interviewing you also works in a mainly SQL position, they will see right through it the moment it gets to technical questions. To any impressionable people who are still in the early days of their career: please don't try and blag a SQL job, you're wasting you're own time, the interviewers time, and the company's time; you're also taking an interview slot from someone who could actually do the job. If you get to the interview and realise it's too technical, be honest, and put a positive spin on it, they may be open to someone willing to learn with the right attitude; a good fit, and a willingness to learn can be just as valuable (to the right company) as a few years experience.
No offence taken, don't worry. I over sold my SQL skills for my current job, but what I need is pretty low level, mostly analysis, but fixing stored procedures and triggers is starting to come into play for me. The people doing the interview defnetly know what they are doing what it comes to SQL, it's just mine was a more junior position.
this returns A B A C B A B C C A C B yeah the problem with this is, that BC is after BA I need AB AC BC so that every team has played each other. Only then there is the remacht so A cant play B twice before B and C have played against each other 
 SELECT 1 AS season , one.team AS home , two.team AS away FROM teamz one CROSS JOIN teamz two WHERE one.team &lt; two.team UNION ALL SELECT 2 AS season , one.team AS home , two.team AS away FROM teamz one CROSS JOIN teamz two WHERE one.team &gt; two.team ORDER BY season , home , away 
Interesting .. I'll give that a shot! 
You can put `ApplicationIntent=ReadOnly` in your connection strings. [This documentation](https://docs.microsoft.com/en-us/sql/database-engine/availability-groups/windows/configure-read-only-routing-for-an-availability-group-sql-server) has more detail of how to configure everything. Obviously, if you try to submit an insert with intent read only, it will fail.
first off, do you have sys admin rights? when you say you ran " dbcc SHRINKFILE (database, 10)" is "database" the filename of your log file or the nam of you database? It needs to be the name of the log file. Also, that command (if it would work) would try to shrink your log file to 10 MB. If its 96% free and 12GB in size, you want at least 500MB on that (it wouldn't shrink past the in use size, but realize what you are telling it to do). Lastly, its always good to walk the log file down in size on a production system. you don't want to file changes to be running for long periods as it will be blocking. I'd cut to 8 GBs or so first based on timing, the down to 6 GBs and so on. edit: you'll get better traction on /r/sqlserver. Lots of SQL Server DBAs willing to help there. 
Remove ba.TRAN_DATE from your group by, and tweak your datediff to use MIN(ba.tran_date).
Wouldn't you instead insert a new city record with the new name ... then Update Address set cityid = &lt;new id&gt; where cityid = oldid? Or, just update city set name= newname where id=1342 ... one update and you've moved leningrad to st petersburg. oh look thats the whole point of this normalization thing.
Connection strings are (usually) configuration changes, not (compiled) code changes, which is why I suggested it. If your app is making changes, it should be connected to the primary instance, not switching back and forth depending on if it's running a select or not.
Try running the below query and see what the result is for your database. Select log_reuse_wait_desc, * from sys.databases This shows what the log file is waiting on to actually release the space. If it is showing LOG_BACKUP, I've had to take multiple log backups to be able to shrink it. 
You can also have your own instance (database) in the cloud, at DevGym.oracle.com It lets you create every object you'd need, and it's free. Just load the sql files into notepad++ and copy/paste into devgym's instance.
Issue a manual CHECKPOINT, then log backup, then shrink. 
The database should be a place where data comes to rest. SQL is great at reading/writing data, but it sucks at processing and calculations. The permutations and ordering should be handled in the application. If you’re going to go the sql route anyway, store a count of games played in the database so that it can be ordered by games played ascending, team1, team2
I can’t imagine any situation that a cursor should be placed in a trigger. I can justify cursors in a stored proc when the code is ancient and the world has been enshrined around the cursor and the Legacy it has blessed the enterprise with. I still have it in my targets if I can get the devs to work on rebuilding their app from scratch. looping through all columns of all rows updated seems really really bad. 
Trim function? Lookup ltrim and rtrim that should do it.
There's ambiguity in the question. There's really not enough info make a clear assessment. Is the value of the post column literally "I'm John, contact me at john@hotmail.com"? If the email address is an actual column in myTable, then you could do something static like this by using a static value to replace all email addresses and alias the column as emailAddress: select '*********@*********.com' as emailAddress from myTable If the email address is contained in a string as is assumed by the example you gave, then it becomes much, much more complicated. You'd need to isolate the email address from the string so that you could substitute the entire email address with a static replace or dynamically replace letters with asterisks characters. Both parts of this will require extensive use of functions and will get very hairy very quickly. You might not have any hair left on your head by the time you're done. *If you reply with a clarification, please make the same clarification in the OP.*
For me it looks like something that could be solved with regular expressions. Check if your database system offers this functionality. In Db2 regexp_replace would be the one I would look at.
SELECT post ,SUBSTRING(SUBSTRING(post, 1, charindex('@', post, 1) - 1), 1, LEN(SUBSTRING(post, 1, charindex('@', post, 1) - 1)) - CHARINDEX(' ', REVERSE(SUBSTRING(post, 1, charindex('@', post, 1) - 1)), 1)) + ' @ ' + SUBSTRING(REVERSE(SUBSTRING(reverse(post), 1, charindex('@', reverse(post), 1) - 1)), CHARINDEX(' ', REVERSE(SUBSTRING(reverse(post), 1, charindex('@', reverse(post), 1) - 1)), 1), LEN(REVERSE(SUBSTRING(reverse(post), 1, charindex('@', reverse(post), 1) - 1)))) I don't recommend it nor will it work if 2 @s are in the string but here you go. 
thanks for the advice! I've been browsing the gym, but I can't find the area where I can upload my own sql files. Could you please give me some pointers where to look?
You can use [DBCC Page](https://blogs.msdn.microsoft.com/sqlserverstorageengine/2006/06/10/how-to-use-dbcc-page/) to look into the page and see what's in there. Maybe it's a clustered index b-tree, maybe it's just a page everyone's waiting for to insert their 32 byte row in one at the time.
Good point. It does not have that option from my experience. However, this may be a good time to learn about object dependencies. I'll explain. You have a process to send people a greeting card. CardInfo (Table) GetCardRecipient (function) SendCard (procedure) The SendCard procedure uses CardInfo to get all the information about the card (Description, text, picture) and uses GetCardRecipient to determine who to send it to. Before you can use these objects to send the card, you'll need to deploy them. So you have all the code you need to deploy the objects in the database. So you try deploying SendCard first. Well SendCard uses the GetCardRecipient function as well as the CardInfo table, so those need to go first. Technically they don't have to if you use a FORCE keyword, but I don't like to do that as it leaves the object invalid and could be trouble later. By deploying the objects from the code in the sql files, you may actually learn a few things. The separations between objects should be marked with a "/" (slash), but pretty much anytime you see CREATE OR REPLACE you'll know it's a new object. 
Then you shouldn't use SQL it seems. Write a quick algorithm to determine that order in some other language (python would be great for this) and pull the data in using sql
At that point why not just alias or create a temp table out of the root data selection (the joins and where clause) then join two select statements, one like the one he has now, and one with the minimum date?
That worked! Its down from 6GB to 120MB. 
I think that's currently happening to my DR Citrix ... We had some major issues and the DB at the home office is the mirror, and its just stuck on restoring. I think because the DR database is not reachable any longer.... The log size on that is about 12GB. Is it possible to just stop replication to fix the log and start replication back up once I have the 2 DBs talking to each other? 
Good to hear. :)
Thank you everyone! /u/DamnedAdmin got me going. I had to backup, shrink, backup, shrink. That got me down to 120MB for the log size. Thank you all who submitted a post, I really appreciate you all. 
 The design of the database is the worst I've ever seen, but fixing it is a losing battle. The triggers are set to do verification and call stored procs that call other stored procs. I'm trying to do what I am allowed to do to fix the huge festering mess but even that is extremely limited. I probably won't even have the buy in to fix these small changes, but I have to try. I'm not even concerned about changing the triggers with cursors that call stored procs, I just want to fix the ones that do our logging. 
Just make sure to run a daily transaction log backup and that should keep the log filesize in check in the future. ;)
Just added a maintenance plan now :) Thanks again!
I ran into this a couple times and i had to take a full backup of the DB before shrinking the tlog, that worked in 1 go rather than 2.
Don't do it daily do it hourly or even more often. If you want to restore your database to a specific time, you can only restore to that point of time with transaction log backups. No log(s), no point in time. Shrinking and growing is bad. While your log is shrinking or growing SQL can't write to it and transaction will wait in a queue.
Yep - you'll want to go into the properties of the primary copy, go to mirroring, then select 'remove mirroring'. You can now shrink the log and then later re-create the mirror set. 
Thank you very much! I've not used DBCC Page before, I will read up on this!
True for many databases, but a Citrix database (not mentioned what exactly) should not really have too many changes daily. But yes, take as many backups per day as necessary. 
&gt; TRUNC(Date,'Month') The only catch with `TRUNC` is how weirdly the format models behave. `TRUNC(..., 'MONTH') = TRUNC(..., 'MM')`, but `TRUNC(..., 'DAY') != TRUNC(..., 'DD')`. `DD` truncates to the day (ie, sets time to 00:00:00), but `DAY` truncates to the first day of the week (normally sunday, but depends on NLS_PARAMS), which is what you'd expect `WW` to do, but instead that truncates to the day of the week that matches the first day of the year. `W` truncates to the day of week matching the first day of the month, and `WEEK` is not a thing at all. `YEAR/Y/YY/YYYY` all behave the same, as do all time components. Full list [here](https://docs.oracle.com/cd/B19306_01/server.102/b14200/functions230.htm). I guess the takeaway is to be careful and test new truncs with a `select .. from dual` when in doubt.
This is really good info to have. Thank you for the resource as well.
Thanks!
 SELECT ISNULL(UA.Department,'Corporate') AS Department , SUM(CASE WHEN UML.License = 'CONTOSO:DESKLESSPACK' THEN UA.Allocation ELSE 0 END) AS [CONTOSO:DESKLESSPACK] , SUM(CASE WHEN UML.License = 'CONTOSO:ENTERPRISEPACK' THEN UA.Allocation ELSE 0 END) AS [CONTOSO:ENTERPRISEPACK] , SUM(CASE WHEN UML.License = 'CONTOSO:STANDARDPACK' THEN UA.Allocation ELSE 0 END) AS [CONTOSO:STANDARDPACK] FROM tblUserAllocation AS UA INNER JOIN tblUserLicense AS UML ON UA.UPN = UML.UPN WHERE UML.UPN LIKE '%contoso.com' GROUP BY ISNULL(Department,'Corporate'); This doesn't account for users with a license, but without a department allocation.
This PIVOT should do it. You would only need to change the query to add new license types (the IN clause): http://sqlfiddle.com/#!6/71964/1/0 CREATE TABLE #tbluserallocation (username nvarchar(max), department nvarchar(100), allocation decimal(19,2)) INSERT INTO #tbluserallocation VALUES ('john.smith@contoso.com', 'Finance', .5) ,('john.smith@contoso.com', 'Engineering', .5) ,('sally.yates@contoso.com', 'Marketing', 1) ,('jessica.walters@contoso.com', 'Engineering', .5) ,('jessica.walters@contoso.com', 'Human Resources', .25) ,('jessica.walters@contoso.com', 'Quality', .25) CREATE TABLE #tbluserlicenses (username nvarchar(max), license nvarchar(max), reportingdate datetime) INSERT INTO #tbluserlicenses VALUES ('john.smith@contoso.com', 'CONTOSO:ENTERPRISEPACK','') ,('sally.yates@contoso.com', 'CONTOSO:STANDARDPACK','') ,('jessica.walters@contoso.com','CONTOSO:ENTERPRISEPACK','') And the query: SELECT * FROM (SELECT a.department, a.allocation, l.license FROM #tbluserallocation a JOIN #tbluserlicenses l ON l.username = a.username) sub PIVOT (SUM(allocation) FOR license IN ([CONTOSO:ENTERPRISEPACK],[CONTOSO:STANDARDPACK],[CONTOSO:DESKLESSPACK])) piv 
Just to note: Ordering is not guaranteed in SQL unless you specify order by. Try using `UNION ALL` instead.
Use the following to dynamically provide the licenses DECLARE @cols AS NVARCHAR(MAX), @query AS NVARCHAR(MAX) SELECT @cols = STUFF((SELECT ',' + QUOTENAME(license) FROM (SELECT distinct ul.license FROM tblUserLicenses ul ) newtab FOR XML PATH(''), TYPE ).value('.', 'NVARCHAR(MAX)') ,1,1,'') set @query = 'SELECT * FROM (SELECT ua.department AS division, ua.allocation, ul.license FROM tblUserAllocation ua INNER JOIN tblUserLicenses ul ON ua.username = ul.username ) x PIVOT ( SUM(allocation) FOR license IN (' + @cols + N') ) p ' exec sp_executesql @query;
basically the reverse is to find where the email address began. There is no way to use CHARINDEX from right to left otherwise.
The PIVOT method others have given should work just fine, but if you don't like that then this will work, too: SELECT COALESCE(a.Department,'Other') AS [Department] ,SUM(CASE l.License WHEN 'CONTOSO:ENTERPRISEPACK' THEN a.Allocation ELSE 0 END) AS [CONTOSO:ENTERPRISEPACK] ,SUM(CASE l.License WHEN 'CONTOSO:STANDARDPACK' THEN a.Allocation ELSE 0 END) AS [CONTOSO:STANDARDPACK] ,SUM(CASE l.License WHEN 'CONTOSO:DESKLESSPACK' THEN a.Allocation ELSE 0 END) AS [CONTOSO:DESKLESSPACK] FROM #tblUserAllocation a FULL JOIN #tblUserLicenses l ON l.UserName = a.UserName AND l.UPN LIKE '%contoso.com' GROUP BY a.Department ORDER BY a.Department Note that I see this mistake several times in your query: SELECT * FROM Table1 t1 LEFT JOIN Table2 t2 ON t2.Id = t1.Id WHERE t2.Field = 'Value' This is an implicit INNER JOIN. You're requiring `t2.Field` to have a specific value and not permitting it to be NULL, so you're filtering out all the records that would be present it `t2.Field` were NULL. The solution is to either put the condition in the JOIN condition to filter it before the join takes place: SELECT * FROM Table1 t1 LEFT JOIN Table2 t2 ON t2.Id = t1.Id AND t2.Field = 'Value' Or to allow the field to be NULL: SELECT * FROM Table1 t1 LEFT JOIN Table2 t2 ON t2.Id = t1.Id WHERE t2.Field = 'Value' OR t2.Field IS NULL These are almost but not quite the same query. If `t2.Field` is a nullable column, the queries will produce different results. Which one you use will depend on the results you want. 
it would be fairly easy especially if you always want it to return this. This is for MYSQL btw and you would just add some spaces. Select Concat('Hi my name is', MY.firstname,'contact me at *@*') From MyTable MY If you branch your db into different columns such as Mytable.Firstname, Mytable.Email You could also use a case statement and have different user levels or a defining characteristic for those who you would allow to see your email. CASE WHEN MY.UserID &gt; 1 THEN Concat('Hi my name is', MY.firstname,'contact me at', MY.emailaddress) ELSE Concat('Hi my name is', MY.firstname,'contact me at *@*') END AS 'Whatever'
Get all the match-ups by joining teams to itself where the team names are not the same. Then compare the values for the team names. If team 1 has the lower value than team 2, assign it to a value (e.g. 0). Otherwise assign a different value (e.g. 1). Then order by this calculation: with teams as ( select 'A' t from dual union all select 'B' t from dual union all select 'C' t from dual ) select t1.*, t2.*, case when t1.t &lt; t2.t then 0 else 1 end ord from teams t1 join teams t2 on t1.t &lt;&gt; t2.t order by ord, t1.t, t2.t; T T ORD - - ---------- A B 0 A C 0 B C 0 B A 1 C A 1 C B 1
You can go to w3schools or code academy. Microsoft has some test databases as well.
I always recommend downloading a dataset that interests you, and ask it questions until your curiosity is satisfied. I have local databases for weather, games, and sports I follow. Here are some sources for inspiration. https://www.kaggle.com/datasets https://aws.amazon.com/datasets https://cloud.google.com/bigquery/public-data
Removing white spaces is not the same as obfuscating data...
Yes correct. I wouldn't say convoluted just different
 WHERE Year(APPR_DT) &gt;= '2012' AND (ts.SRC_SYS_CD &lt;&gt; 'KSN' OR ts.ORD_ID &lt;&gt; '') When you negate something, you change ANDs to ORs.
The original code is keeping all records where appr_dt is &gt; = 2012, and removing any that have a src_sys_cd of KSN that also have a blank ORD_ID. Your code does something different and removes any records with src_sys_cd of KSN, regardless of their ord_id. It would also remove all records with blank ord_ids, regardless of sys_src_cd.
I wrote a solution but it was slow as hell... this suggestion seems good though: https://stackoverflow.com/questions/436351/how-do-i-find-a-value-anywhere-in-a-sql-server-database
Thank you. I knew the answer but it helps to have a post like this when and if someone wants to argue. Cheers.
Hey, there is this course on udemy perfect the kind skill that you are looking to acquire it is a complete microsoft business intelligence course on ssis,ssas and ssrs. covers all data flow and control flow managing and creating packages is ssis. it is priced at 10 dollors check tthis link out https://www.udemy.com/microsoftbusinessintelligence/?couponCode=MSBI-RDT-10 
Great thanks!
I would create dynamic sql that creates something like this `SELECT 'Table' AS TableName FROM Table WHERE StringColumn1 = 'string' OR StringColumn1 = 'string'` What type of database is it? Try this for MS SQL, and copy+paste the results in a new query. SELECT 'SELECT ''' + s.name + '.' + t.name + ''' AS TableName FROM ' + QUOTENAME(s.name) + '.' + QUOTENAME(t.name) + 'WHERE ' + c.filter FROM sys.schemas AS s INNER JOIN sys.tables AS t ON t.schema_id = s.schema_id CROSS APPLY (SELECT STUFF((SELECT 'OR ' + QUOTENAME(c.name) + ' = ''String''' AS [text()] FROM sys.columns AS c INNER JOIN sys.types AS t2 ON t2.user_type_id = c.user_type_id WHERE c.object_id = t.object_id AND t2.name LIKE '%char' FOR XML PATH('')),1,3,'') AS filter) AS c WHERE s.name &lt;&gt; 'sys' AND c.filter IS NOT NULL;
NOT(A and B) = NOT A or NOT B https://en.m.wikipedia.org/wiki/De_Morgan%27s_laws?wprov=sfla1
**De Morgan's laws** In propositional logic and boolean algebra, De Morgan's laws are a pair of transformation rules that are both valid rules of inference. They are named after Augustus De Morgan, a 19th-century British mathematician. The rules allow the expression of conjunctions and disjunctions purely in terms of each other via negation. The rules can be expressed in English as: the negation of a conjunction is the disjunction of the negations; and the negation of a disjunction is the conjunction of the negations; or the complement of the union of two sets is the same as the intersection of their complements; and the complement of the intersection of two sets is the same as the union of their complements. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/SQL/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.24
Non-Mobile link: https://en.wikipedia.org/wiki/De_Morgan%27s_laws?wprov=sfla1 *** ^HelperBot ^v1.1 ^/r/HelperBot_ ^I ^am ^a ^bot. ^Please ^message ^/u/swim1929 ^with ^any ^feedback ^and/or ^hate. ^Counter: ^97349
There are lots of website available and plenty of PDF/other stuff available for learning SQL best way is install free edition of SQL https://www.microsoft.com/en-in/sql-server/sql-server-editions-express and learn using the sample database
[Galaxql was how I got my start](http://sol.gfxile.net/g3/)
It is supposed to be NOT A AND NOT B, not NOT A OR NOT B. https://meta.wikimedia.org/wiki/Cunningham%27s_Law
edit: this is wrong. ~~Assuming no nulls, that is correct. However, `null = notnull` doesn't yield false, so it can't be negated.~~ **Chart1**: Does `NOT (ts.SRC_SYS_CD = 'KSN' and ts.ORD_ID = '')` return the record? | ORD_ID = '' | ORD_ID = 'X' | ORD_ID is null | :--- | :--- | :--- | :--- | SRC_SYS_CD = 'KSN' | NO | YES | NO | SRC_SYS_CD = 'X' | YES | YES | **~~NO~~ YES** | SRC_SYS_CD is null | NO | **~~NO~~ YES** | NO | **Chart2**: Does `(ts.SRC_SYS_CD &lt;&gt; 'KSN' OR ts.ORD_ID &lt;&gt; '')` return the record? | ORD_ID = '' | ORD_ID = 'X' | ORD_ID is null | :--- | :--- | :--- | :--- | SRC_SYS_CD = 'KSN' | NO | YES | NO | SRC_SYS_CD = 'X' | YES | YES | **YES** | SRC_SYS_CD is null | NO | **YES** | NO | (I'm an Oracle guy, so I'm open to the possibility that I'm wrong.) 
Mm ... maybe. The two pieces of code in OP are not equivalent; I'm not sure which one it's "supposed to be"
It is supposed to be an AND, but regardless it's written like an asshole.
Oh you're OP! Ha, sorry. Anyway, then yeah it should be NOT(A OR B) .. this style makes more sense when the thing in parentheses is very complicated conditional logic. Also a comment can go a long way here for the next guy ....
That's a good point and worth testing. I both queries will allow nulls, but will be right back to confirm. edit: Yep. Both queries allow nulls in either column. http://sqlfiddle.com/#!6/c5236e/3
This account is clearly just for advertising all over reddit.
Huh... Looks like I was wrong (about how Oracle behaves, too!) My mistake was taking the fact that `(null = notnull)` is unknown, and assuming that `(null = notnull and 'A' = 'B')` is unknown, too. But we *do* know that `(??? and FALSE) == FALSE`. Looks like I need to study up on my logic :) 
You should be able to do queries using ifnull() or isnull(), or use those in views to replace data. Basically you would create a new calculated column in the view for each column, using the function that checks for nulls. And yes you would have to take the data type into account as to whether to use "" or 0, etc. 
If the NULL should represent a sane default, then the column should be set to populate a default value. That is the best practice and there really is no "better" solution here. I would push back, if you can. ISNULL is your other option, but as you have said, using it everywhere (especially in WHERE clauses) will kill your index usage and your queries will not perform like you want. You can't push back and ask the db devs to add defaults to the columns? I can't think of a use case where NULL should ever represent valid data.
Its a bit more work, but null isn't always a bad thing. It can be annoying but don't discount it. Anyways what you are looking to do is WHERE .......... AND (ColA IS NULL OR ColA &lt;&gt; 'BadVal') AND (ColB IS NULL OR ColB = 'GoodVal') ........... While you can wrap the columns in ISNULL(), it can hurt performance. SQL Server has an internal nullable bit that it uses to quickly scan the data. If you wrap it in isull, it can't use the information about the column but has to evaluate every row.
What are you wanting to learn? *Using* SQL on an existing database or establishing and managing a database? 
PostgreSQL supports column security and row security You can basically say "let Frank select name and phone but not salary for all users" and "only let Shirley see her own row(s)"
Sql zoo and SQLite 
This really depends on what the primary job role is. Entry level DBA is different than entry level Data Analyst. The SQL required is totally different for both and the "Entry Level" portion of it is different. SQL is a huge topic - and it covers the management of Databases along with pulling useful data out of those databases. They don't separate well either... because you need to know how data goes in to best be able to get it back out. Here are my entry level questions I ask when I interview candidates: * What is the difference between a UNION and a JOIN. * Why is it important? * When would you use a UNION? * When would you use a JOIN? * In MS SQL How would you select data from a very large table if you needed to make sure that incoming transactions were not impacted? * In TD SQL what is SKEW and why is it an issue? How do you avoid Skew? * How do you reduce the records in your query to only 10 records in Oracle, TD SQL, and MS SQL? * How do you create a Temp or Volatile table? * What is an easy way to ensure your query has no duplicate values? * What additional (important) questions need to be answered when trying to remove duplicate values from a query? * What is a CURSOR, when would you use it, and should you avoid using it?
Kahn academy is amazing and free
It looks like you want to count the value +/- .25 without include that value itself. Something like the following should work. SELECT vals.val , sum(COALESCE(cnts.cnt,0)) variance_cnt FROM ( SELECT val FROM tbl GROUP BY val ) vals LEFT JOIN ( SELECT val , count(*) cnt FROM tbl GROUP BY val ) cnts ON vals.val &lt;&gt; cnts.val AND cnts.val BETWEEN vals.val - .25 AND vals.val + .25 GROUP BY vals.val ORDER BY vals.val;
Why does this need to be a recursive CTE? SELECT CASE WHEN dept = 'Edison' THEN name ELSE '' END AS Edison , CASE WHEN dept = 'Prof' THEN name ELSE '' END AS Prof , CASE WHEN dept = 'IT Masemarked' THEN name ELSE '' END AS "IT Masemarked" , CASE WHEN dept = 'Topdanmark' THEN name ELSE '' END AS Topdanmark FROM empldept ORDER BY CASE WHEN dept = 'Edison' THEN 1 WHEN dept = 'Prof' THEN 2 WHEN dept = 'IT Masemarked' THEN 3 WHEN dept = 'Topdanmark' THEN 4 END , name;
If you don't mind Microsoft stuff: Grab SQL Server Express [here](https://www.microsoft.com/en-us/sql-server/sql-server-editions-express) and the AdventureWorks Database [here](https://msdn.microsoft.com/en-us/library/ms124501.aspx) or [here](https://msftdbprodsamples.codeplex.com/) There are exercises and examples referencing AdventureWorks out there and once you're a bit more familiar with the database, you can try and apply your own scripts. You could also get the Developer Edition of SQL Server for free if you have a Visual Studio Developer Essentials account. I don't think you need anything to get the account. [Sign up here](https://www.visualstudio.com/dev-essentials/) Happy coding!
If this is MS SQL, I've used ISNULL() to get all records that are either Null or "0", with very little noticeable performance impact, even on 2 million+ row tables. This is in regular queries though where a few seconds don't matter as much. I've also used nightly jobs which scrub records with null appropriate values using UPDATE [table] SET [column] = &lt;value&gt; WHERE [column] IS NULL. I would not recommend doing this without knowing if the front end code is using null values. This may not be useful for what you're looking for now that I've re-read your post.
Not exactly what you asked for, but another interesting exercise is to group consequtive numbers with gaps &lt;= 0.25. DECLARE @tbl table (val decimal(9,2)); INSERT INTO @tbl VALUES (1),(1.25),(2),(4),(4.5),(4.75),(5); WITH cteData AS (SELECT t.val , CASE WHEN t.val - LAG(t.val) OVER (ORDER BY t.val) &lt;= 0.25 THEN 1 ELSE 0 END AS IsInSequence , ROW_NUMBER() OVER (ORDER BY t.val) AS Sequence FROM @tbl AS t) , cteSegment AS (SELECT d.val , d.Sequence - SUM(d.IsInSequence) OVER (ORDER BY d.val ROWS UNBOUNDED PRECEDING) AS Segment FROM cteData AS d) SELECT MIN(s.val) AS MinVal , MAX(s.val) AS MaxVal , COUNT(*) AS ValCount FROM cteSegment AS s GROUP BY s.Segment; MinVal|MaxVal|Count -:|-:|-: 1.00|1.25|2 2.00|2.00|1 4.00|4.00|1 4.50|5.00|3
Following!
I was using this exercise to learn how to write recursive queries. I was also told by a mentor that if a recursive query is used, we can do it dynamically without using the column names. My mistake. I forgot to mention this in the question. My SQL knowledge is noob level(simple queries and joins). Learning more SQL now. Is there a solution to write this as a recursive query to make it dynamic without using column names? Thanks for the help. 
The three resources I used when starting out were w3school, SQL zoo, and the Microsoft Adventure Works database. Microsoft had example questions and YouTube had videos of people solving problems and explaining how they figured out the answers. 
I agree. From what I read recursive CTE are more frequently used to query parent-children hierarchies. Any ideas on what a query without using column names would look like?
This account is not for advertising, This will be helpful for the SQL aspirants to quickly guide them in right direction to find the best resources. 
&gt;In most cases, what a null should represent is a sane default value, eg 0 for ints, "" for strings, etc. Null represents unknown not an empty string or 0. Null is what you know about my bank-account or real name. It's missing information: not 0 and an empty string, you can't assume that's my balance or name. Unfortunately if you care about performance /u/abbbbbba's way is the only correct way too type it all out. Just create a macro for it or use alt+drag and copy-paste. &gt;Perhaps wrap every table in a view that pre-resolves nulls to sensible defaults and then only use the views to generate reports? You could do that anyway for security reasons (different schema ect), write once with the ColumnA IS NULL ect and use it forever. You can even update via views if you want and use them as "tables" for a new version of your home-brewed ERP system.
Just to nitpick, I used (and would totally use again) `null` in place of the empty string in this solution (which is exactly the one that I came up with when I had a similar problem). 
It may be tempting to look for a 'recursive CTE' solution to this problem b/c we know there are some problems that are not solvable in a general manner using only classical SQL (eg arbitrary nesting) which are quite elegantly solvable using recursive CTEs. However one hallmark of this problem is that the content of the *data* interacts directly with the structure of the *database* itself (adding one *row* with *value* `'foo'` in the source means adding one *column* with *name* `"foo"` in the target); more often than not, this is regarded as wrong or at least undesirable. SQL provides AFAICS no functionality to deal with this situation, short of constructing a suitable DLL string yourself; in PostgreSQL, you can do that in a PL/SQL function.
I want to second the utility of using views. Use views, use lots of them, and preferably put them into a separate schema (I'd use one for experiments, and one for the proven useful ones). Using materialized views may be advantageous if performance due to many cascaded views should become a problem (but you'd have to refresh them when source data is changed, YMMV).
For even more gamification check out the Schemaverse.
The only posts you have made are ones advertising this website. Everyone can see your post history, you know.
Yes, this is essentially what I am doing now. I just have to remember to always do this for each condition or I end up getting burned. Realistically this is only an issue because the db developers should have had sensible defaults but didn't and now the software depends on nulls for specific functionality so I can't replace the nulls with default values safely. I suppose I just need to change my intuition to know that colA &lt;&gt; 'Val' will not include nulls, and negating a positive case also has the same effect, Ie: not colA = 'Val' will also exclude nulls.
Are you restoring the backup onto an existing instance of 2016? Or do you also need to install 2016 as well? If you just need to migrate the db from a 2012 to 2016 instance, backup and restore should work.
I don't mean null always should represent a sane default. I just mean in the case of this database, null should have been a sensible default because that's how the client treats a null. If an integer field is 0 or a string is blank, the user will often submit nothing and the database gladly accepts it as null instead of the sensible default. What I specifically don't like however is that saying null != 'Val' from a logical perspective makes sense, yet per the SQL spec, is surprisingly not true.
For safety, that's what I was thinking. Not having to account for nulls present where they should not be would be desirable. Even assuming the performance hit for this is bad, its still better to have correct reports that are slow versus fast reports with broken logic due to nulls. Still it would be nice if SQL would just intuitively include nulls when I say all records where colA != 'Val'.
Agreed it should be set that way, however I have no way to fully guarantee that it is safe from the perspective of the ERP. They already stopped paying for support for the app and we don't have source code so I am very reluctant to modify the data in anyway. Wrapping a view around the table seems safer.
Yeah that's fine, I do this in several places in the db, I even have a script to do it automatically based on a provided database schema. My fear however is if I do this in aggregated multi-table multi-column conditionals, that it will be a major performance killer.
Honestly SQL Server upgrades are one of the easiest things to do if you don't have to worry about the application code being compatible... you can backup/restore or copy the mdf/ldf files over and remount them (or run an in-place upgrade)... just be sure to change the compatibility level to the proper one after the db is on the server (right click the DB, properties, options, compatibility level).
You've got a new instance for 2016 to migrate to? 1. Go get http://dbatools.io/ 2. Shut down application 3. `start-sqlmigration -source sql2012 -destination sql2016 -backuprestore -networkshare \\commonserver\path -withreplace` 4. Shut down old instance 5. Start app server 6. Point app server at new instance 4. Test
None of the timestamps are identical in the sample data. But try using MAX or MIN(UPDATE_DATE) with GROUP BY ID. You might have to make a CTE or Subquery for that part then join the sub to the table to return the NAME since the NAME won't/can't be grouped and you can't use the aggregate without the grouping. edit: Got the grouping backwards
Ooooo, I have never heard about that one. Will check it out. 
There is a setting. "Play ribbit noise on startup" or something like that. Ribbit.
rowid 
View -&gt; Toad Options -&gt; Startup -&gt; Play Toad .wav file
Group by ID and use ~~FIRST~~ LAST with an aggregate function to select the appropriate values.
I can't remember exactly what it was, (I didn't deal with the issue, but overheard about it... Open planned office... Discussions happening everywhere etc) but I'm sure that we recently had a performance issue with a client upgrading from an older version to a newer version. I believe it was this or something similar: "Performance degradation when you upgrade from database compatibility level 120 to 130 in SQL Server 2016" - https://support.microsoft.com/en-us/help/3212023/performance-degradation-when-you-upgrade-from-database-compatibility-l
My hero.
 SELECT C.ID , MAX(C.NAME) KEEP (DENSE_RANK LAST ORDER BY C.UPDATE_DATE) AS NAME , MAX(C.UPDATE_DATE) FROM CUSTOMERS AS C GROUP BY C.ID;
OH! Sorry can't help any better than the others who already have, but, didn't know * could be limited to only part of a join like that. TIL, thanks for that. :D
Join the kit to sub items but make the quantity negative. Then you can aggregate to the item, and they'll cancel out the extras. Filter any results with zero quantity. Table - Qty 1 Paddle - Qty 2 Kit-Paddles&amp;Balls - Qty 1 &gt; Paddle - Qty -2 &gt; Balls - Qty -1 Paddle - Qty 2 Balls - Qty 1
One of the oldest organizations in this game and one of the best http://www.red-gate.com/. Been using them since 2003. Also, you can try Idera as well.
Can you get an execution plans for `ID = 'A'` and `ID IN ('A','B','C','D','E')`? Optimization is more about what the database is doing than how the code is written.
thanks I'll try them!
I'm not familiar with execution plans but after some Google and Youtube videos, it definitely looks like something that would shed some insight to my problem. Would you have any suggestions how to best learn how to get execution plans and understand them? I'd like to learn more, but I also have to work with the limitation that the DBA won't provide assistance unless absolutely necessary.
You might just check out Oracle's documentation on it. There's probably some youtube videos out there if that's too hard, a little googling should get you there. https://docs.oracle.com/cd/B10501_01/server.920/a96533/ex_plan.htm 
Highly recommend dbatools (as recommend by alinroc), the start-sqlmigration command is all you need! If you want to avoid PowerShell, performing a backup and restore will work fine. Personally I would avoid a SQL in place upgrade, performing a side by side migration gives you a fallback in case you run into any issues.
To aid mobile users, I'll link small subreddits not yet linked in the comments /r/SQLserver: Microsoft SQL Server Administration and T-SQL Programming including sql tutorials, training, MS SQL Server Certification, SQL Server Database Resources. --- ^I ^am ^a ^bot ^| [^Mail ^BotOwner](http://reddit.com/message/compose/?to=DarkMio&amp;subject=SmallSubBot%20Report) ^| ^To ^aid ^mobile ^users, ^I'll ^link ^small ^subreddits ^not ^yet ^linked ^in ^the ^comments ^| ^[Code](https://github.com/DarkMio/Massdrop-Reddit-Bot) ^| [^Ban](https://www.reddit.com/message/compose/?to=SmallSubBot&amp;subject=SmallSubBot%20Report&amp;message=ban%20/r/Subreddit) ^- [^Help](https://www.reddit.com/r/MassdropBot/wiki/index#wiki_banning_a_bot)
I've tried every combination with WHERE that I can think of, but I feel like I dont have the formatting right to pull up just "John"....
I've had too many heart attacks first thing in the morning because of this!
Right, it could be, and I reread and saw you mentioned that already. Maybe I don't understand what you would have meant by "wrapping the tables with views", but to my understanding that would mean using functions in the wrapping views, for each column. I'm not super experienced, and I don't see another way here...
As a database enthusiast who wishes he'll one day be good enough to work with oracle, I have to ask what is this Toad program that makes ribbits? Not exactly a sound I'd expect to hear from the back office!
Can you post the examples that you've tried, and why you think they haven't worked? Folks will be reluctant to just provide an answer since this is homework.
Can't believe I didn't think of that. Thanks! I'll test it out tomorrow.
 DECLARE @tbl TABLE (val DECIMAL (9,2)) INSERT INTO @tbl VALUES (1),(1.25),(2),(4),(4.5),(4.75),(5) -- Where the difference is exactly .25 SELECT t1.val, ( SELECT COUNT(1) FROM @tbl t2 WHERE ABS(t2.val - t1.val) = .25 ) cnt FROM @tbl t1 -- Where the difference is within .25 SELECT t1.val, ( SELECT COUNT(1) FROM @tbl t2 WHERE t2.val &lt;&gt; t1.val AND ABS(t2.val - t1.val) &lt;= .25 ) cnt FROM @tbl t1
This is one of my most common tasks at work. (Clients are on a buying spree and swallowing up competitors) and it is my job to merge their data. I wrote a custom script. I simply join information_schema.columns from both DBs, then use dynamic SQL to construct the insert statements and run each insert separately in it's own transaction. There are a lot of FK constraints, so I loop through the metadata to find the "root" tables, the ones with no foreign keys. Then reverse sort the tables by dependency and execute the inserts in that order. I get lots of data conflicts due to versioning and crappy DBAs, so whenever the column metadata does not match up perfectly I run a step to siphon off this data to a holding area. Any fields that dont have a destination gets poured in to name-value pairs, common format table for later review. It's pretty slick. We tried third party tools, but they are really hard to tweak for every oddity. Simply not enough control. I can't give away the code, unfortunately. But the methodology above works great. The secret is to break every task in to it's simplest process. Don't try to do more than one thing at a time (e.g. insert into and cast data type). Move data, audit, cast data, audit, insert data, audit. Then audit again. When I do a final audit, I usually pick a common data type like datetime and run all the data from both DBs against each other to make sure I have the same values, number of records,etc on both sides. Don't try to dedupe or filter on the front side. Do all that after your audit shows that ALL The data moved safely. Only then can you safely remap and remove the data. Make sure all your methods are data driven and do not require you to remember anything or do something manually. This is how it gets hosed. I also tag every table on the source with a uniqueidentifier on every row and audit to make sure every rowid made the move. Do this in a test environment. Audit audit test audit test. Remember that just because your DB constraints didn't squawk doesn't mean your application won't explode when it encounters duplicate data or older version content or any other craziness that arises from pouring garbage into the patient's open chest. Hope that helps.
 SELECT * FROM Person WHERE Person.FirstName = 'John'
It's a competitor/analog to PL/SQL Developer or Oracle SQL Developer. Apparently it has a startup sound, but instead of whipping the llama's ass, it just ribbits.
You can be absolutely awful and still get to work with critical oracle systems that help run multibillion dollar companies. Just become a consultant. 
Seriously, it's insane. "Alright, I'm settled in, over-ear headphones on, coffee by my side, no meetings for a few hours, time to buckle down a-" RIBBIT BITCH HOPE YOU'RE MOTHAFUCKIN READY 
Snapshot VM, in-place upgrade, test app connectivity, upgrade DB functional level, test app again? Not a bad workflow to rollback the snap if needs be, if you are quite sure of your application validation. Did this a couple times recently and it was just fine from 2012&gt;2014&gt;2016 and 2014&gt;2016.
I understand your yearning for a logic that works the way you think, but—come to think of it—SQL's logic is not without reason. Imagine you have a list of theaters with tonight's shows compiled from various sources. There are some theaters you didn't find tonight's show for; those rows have a `null`. You don't want to watch Star Wars, again, so you `select * from theaters where show != 'Star Wars'`. *If* your DBMS understood that query as meaning 'include nulls', you'd stand a risk of ending up in another Star Wars screening, again. Instead, SQL plays it safe and forces you to be explicit, as in `select * from theaters where show != 'Star Wars' or show is null`. You can also (in Postgres) say `select * from theaters where coalesce( show, 'anything' ) != 'Star Wars' or show is null`. You can even say `create view my_theaters as ( select coalesce( show, 'anything' ) from theaters );` so you don't have to repeat yourself. 
We can obviously not fix the data in your database, only you can. Your claim that `null != 'Val'` should be true from a 'logical perspective' is ... illogical. It sure does look logical given a certain set of assumptions, and, granted, that is a pretty prevalent set of assumptions that holds for many popular programming languages including Python, JS and so on. However, it is not unchallenged, and quite similar to dividing by zero. Many programming languages raise a DivisionByZero error on `1 / 0`; yet JavaScript for one gladly returns `Infinity` which satisfies some and irates others (it becomes *really* weird when you realize that acc to IEEE754 `Infinity == 2e308` does hold!). Excuse me, but I'm afraid when you say "I don't mean null always should represent a sane default. I just mean in the case of this database, null should have been a sensible default because that's how the client treats a null. If an integer field is 0 or a string is blank, the user will often submit nothing and the database gladly accepts it as null instead of the sensible default."—we're not discussing the merits and demerits of SQL `null` anymore, we're talking about you being less than happy with the particular records in your particular database. As you say, "null should have been a sensible default": just do it. just create that view on your data that has those sane defaults, and bingo!
Add an index.
I've been burned by this due to the storage used by the snapshot. On a smaller instance, it's not *too* bad if your snapshot storage is large and fast enough, but on a medium-sized database the diffs can start dragging performance down tremendously.
Why would anyone use Toad over SQL Developer anyway?
No idea, that's outside the scope of the engagement :P
The main purpose is disaster recovery, not performance. Reporting solutions are the most common case where both instances are actively used as separate processing and query servers. Consider pointing reporting modules at the read only instance. That could remove some bigger queries from your main server.
Yeah, that makes sense. I guess it depends on your storage fabric and how the environment is addressing it. Your maintenance window in the above may need to be rather large as well to allow time for the snap/rollback if there is an issue, rather than the export/import/flip DNS workflow.
TOAD still has more features than SQL Developer. And if your company already has licenses, then cost may not be a huge concern. But a lot of it is just familiarity. Some of us old timers have been using TOAD for years before SQL Developer was even a thing. And when SQL Dev finally came out, it wasn't very good. It kept getting better over time, though.
Wait, some of your sample code included a GROUP BY statement. That could make a huge difference. Are you sure you are comparing apples to apples with the IN statement vs the equals statement? (No grouping.) Also, are you sure you are comparing query times for the **last** row to come back? Meaning, total throughput? Or are you only looking for how long it takes for the first row in the result set to come back?
What kind of features are you missing from SQL Developer? I admit I kinda despise both SQL Dev and Toad when comparing them to SSMS. 
I think you want something more like this. select (extract(year from log.time), extract(month from log.time), extract(day from log.time)) as date, sum(case when log.status not like '200 OK' then 1 else 0 end), sum(case when log.status like '4__%' or log.status like '3__%' then 1 else 0 end), count(log.status) from log group by date It looks like you were cross joining your total and failed subqueries with your '200 OK' query, then counting the total cross joined records (32900).
Oh you're right! I'm still pretty new to SQL but that makes sense. Thank you! Will try in a sec
Can I draw you a picture? lmao
The apples-to-apples comparison would be based on the first two queries: SELECT tbl.* FROM [table] tbl WHERE tbl.ID = 'A' SELECT tbl.* FROM [table] tbl WHERE tbl.ID IN ('A', 'B', 'C', 'D', 'E') I wait for total throughput - query 1 (with the equal statement) takes less than a minute whereas query 2 (with the IN statement) continues to execute after 10 minutes. I was thinking for my purposes, I could just run query 1 (with the equal statement) x number of times for the x ID's I need to run it for, but was wondering if there was a more efficient way to write the code to pull everything all at once.
Then it's because of your data value distribution (or due to bad statistics if the optimizer is doing it wrong). For example, suppose the following is your data distribution by value: A = 1% B = 4% C = 15% D = 10% E = 20% X = 10% Y = 20% Z = 20% So for the value of A it uses an index, but for all 5 values it does a full table scan. And maybe there is no skew at all. Maybe there are just as many As as Bs. But when going for all 5 values, it reaches a threshold where the full table scan is considered better. If the full table scan takes longer than running all 5 queries individually, once for each value, then your stats are probably wrong. Even in that case you can direct the database to use the index. However, it is better to correct the root cause rather than introducing optimizer hints. 
You might be able to do the proof of concept in SQL server (at least), if not the full execution, for free with sql server developer (express) edition. Docs.microsoft.com/sql has all you need to get started. StackOverflow's database if available for free, and they have some damn good architecture over there. Worth a look.
&gt; sql server developer (express) edition Developer Edition &lt;&gt; Express Edition. Express is free for production use, Developer is not.
Before you start hacking away at this database, are you prepared to handle all the HIPAA-related requirements for this system?
It seems like there should be an off the shelf product that could handle this and pass HIPAA requirements.
Yeah, I would assume this is going to be a big HIPPA thing. OP, with whatever database language you pick, it will likely be easier to host it in a cloud service that is already HIPPA certified. I assume there is a bunch of other HIPPA requirements for the actual data you will be using and storing, but you're going to want ensure your server partner is already HIPPA certified. 
Hey just wanted to come back and say thanks so much! There were a few more requirements involved in the statement but you getting this initial problem sorted for me allowed me to move forward. Ended up getting it all finished up about an hour ago. :D
If the current datastore is MS Excel I doubt it's HIPAA compliant currently.
I work on a medical DB. The 3 of the 4 things you want to track won't be too difficult (imaging being the tough one) assuming you keep this simple and this is for study and not used as the system of record/ patient's official record keeping. Even so, this is not a small project. If it has Patient data in it, it must be HIPAA compliant in the USA. If you do a MS SQL Server on site and buy the license, you'll get SSRS free (I'm told). That will be a handy dashboarding tool.
That doesn't mean it shouldn't be a consideration in the new one.
Azure claims to be HIPAA compliant. Try it with powerBI. You may want to split out Tx, Rx, and Dx into separate tables.
 UPDATE mytable SET datecolumn1 = CASE WHEN datecolumn1 &gt;= '1900-01-01' AND datecolumn1 &lt; '2000-01-01' THEN NULL ELSE datecolumn1 END , datecolumn2 = CASE WHEN datecolumn2 &gt;= '1900-01-01' AND datecolumn2 &lt; '2000-01-01' THEN NULL ELSE datecolumn2 END , datecolumn3 = CASE WHEN datecolumn3 &gt;= '1900-01-01' AND datecolumn3 &lt; '2000-01-01' THEN NULL ELSE datecolumn3 END , ... 
your WHERE clauses have no operators.
Do yourself a huge favor and use an open data model. Look at the OMOP Common Data Model, or FHIR compliant data models.
... and non-zero values are evaluated as TRUE
you can't do it that way try this -- SELECT CMP.Campuscode AS Campus , 'Student' AS hours_type , ROUND(SUM(ATD.duration)) as hours FROM Students STD INNER JOIN Attendance ATD ON ATD.studentID = STD.StudentID INNER JOIN Campuses CMP ON CMP.Campuscode = STD.StudentCampus UNION ALL SELECT CMP.Campuscode , 'Teacher' , ROUND(SUM(TRA.duration)) FROM Teachers TCH INNER JOIN TeacherAttendance TRA ON TRA.teacherID = TCH.teacherID INNER JOIN Campuses CMP ON CMP.Campuscode = TCH.Campuscode note it's UNION ALL for efficiency
Works good, now I need to group by Campus code for each. If I add group by CMP.campuscode for both it runs for ever, if I add to the bottom it only groups by Teachers. There are 3 campuses and I need the student teacher hours for each then I have to calculate a ratio percentage. Usually I use ROUND(SUM(TRA.duration)/(ATD.duration)*100) This query has been destroying me. 
 Campus hours_type hours 34606 Student 329353 34601 Teacher 2558 34606 Teacher 4015 34652 Teacher 3035 Right now it returns this
UNION requires common columns. HOURS_S and HOURS_T are unique to their set, meaning there is no way to UNION them. Rather, have a "VALUE" and "LABEL" column where VALUE = the calculation for HOURS_S or HOURS_T and LABEL = 'HOURS_S' or 'HOURS_T'. 
It will still evaluate if the columns are ints or bools
The course has be postponed to 08/28/2017. There are still seats available.
/u/WeaselWeaz , it has never been my interest to spam, but I want to teach others how to use `SQL` and make more money at work. The course takes place at a not for profit location.
Calling the working query from r3p0b8's comment "good_union", do: select campus ,ROUND(SUM(DECODE(hours_type,'teacher',hours,0))/sum(decode(hours_type,'student',hours,0))) as teacher_to_student_hours from good_union group by campus decode() is a lot like a case statement, but more compact, and I like to use it in situations like this.
Wrong link.
 WHERE STD.ADMINID here there is no condition or operator specified .e.g. where name="John" etc.
I hope this wasn't an accounting database or anything...
&gt; I knew the answer I'm confused. The two pieces of code you posted in the OP are **not** equivalent. You're telling us that was intentional? If so, your post makes no sense. If not, you don't actually "know the answer".
Account has two posts, both are spam... :thinking:
Those aren't really reporting models.
I was asking what the specific code did, and then asked if the subsequent code was analogous -- I did this because someone was telling me that the original code was analogous to two AND's, and it wasn't, and they wouldn't believe me. So rather than spending time testing it and having a meeting to show them, etc., I just posted here and then linked them.
Yea I shouldnt have added that, we have to put Table.&lt;ADMINID&gt; for the server to run the script. I leave the &lt;&gt; off as workbench does not accept it. 
How many rows are we talking? Is this a live production instance that needs to be accessible while running the statement?
Thanks! Unfortunately I have aspirations of being competent so that might not be the best match. 
Couldn't you query the sys.tables and join the sys.columns columns to figure out the names of all the columns you need? Then you could use the stuff/for xml functions to build a query that would update all the columns you wanted. 
I'm (sort of) joking. Basically, it's a program that connects to a database and offers coding project management tools, script writing assistance, navigation and filtering, and a ton of other standard relational database functionality (updating and creating tables manually/systematically, connectivity with other applications, the works). It's honestly not as bad as what some people are going to claim, and it's been around for ages.
I've heard consultant horror stories, so I took that with a grain of salt. But thank you for that overview. 
The CDM sure is. Even fhirbase isn't bad. The point is using some model will allow you to use all sorts of tools that have already been built.
I don't know Teradata, but I do know SQL. You're going to need to know two things. First is a dataset that's SUM of Revenue by Product and SaleYear GROUP BY Product, Year. Second is a dataset that's SUM of Revenue by Product and SaleYearMonthDay GROUP BY Product, SaleYearMonthDay. SELECT d1.Product, d2.SaleYearMonthDay, (d2.AMT/d1.AMT) * 100 as AnnualRevenuePCT FROM Dataset1 join Dataset2 on d1.Product=d2.Product AND d1.SaleYear=datepart(yyyy,SaleYearMonthDay). 
Thank you!
be glad to help you out 1. when is this assignment due? 2. what are you having trouble with?
Where are you stuck? Do you just need some clarification on what something means? 
What was the program you wrote?
like 30 min lol, I answered the first one of the first problem, and completed the second problem set
Basically what /u/manojk92 suggested, used sys to get the names of the columns then put that in a DataTable in C#, went through the rows in the DataTable with a foreach running a query with a parameter for the column names.
&gt;like 30 min lol &gt;1 point an hour ago rip 2 ur grades
You can leverage [window functions](http://info.teradata.com/HTMLPubs/DB_TTU_15_10/index.html#page/SQL_Reference/B035_1145_151K/functions_WINDOW_AGGREGATE.html) in Teradata to process this without having to create derived tables. Something like the following should work: SELECT Product , SaleDate , SUM(Revenue) Revenue , CAST(Revenue AS DECIMAL(12,4)) * 100 / SUM(Revenue) OVER ( PARTITION BY Product ) Revenue_Pctg FROM TableA WHERE SaleDate BETWEEN '2016-01-01' AND '2016-12-31' GROUP BY 1,2 ORDER BY 1,2; 
SQLZoo.net is a little basic, but it gives you instant feedback which is nice. One thing that helps me is to physically write down what my data looks like and what I need it to look like. Once I have that picture it gets easier to write the code. Hang in there OP SQL knowledge levels out pretty quickly. Be grateful you are using T-SQL and not P/L SQL :)
[This tool might come in handy](https://kmtee.com/products/help-me-stack-overflow-youre-my-only-hope-coder-programmer-shirt-coffee-cups-mugs-16-6005?utm_medium=cpc&amp;utm_source=googlepla&amp;variant=34125900817&amp;gclid=Cj0KCQjwn6DMBRC0ARIsAHZtCeM__lHPU-3yKNab7E2lSOGbPgEoJUyizwzW1BEw2Z2EU1T4V7YTvQcaAuZfEALw_wcB)
Do exercises. Check that http://www.studybyyourself.com/seminar/sql/course/?lang=eng. Free, for beginners, well structured, keep things simple, with online exercises. Another thing you could do is to download some freely available data set, for example [wikipedia](https://dumps.wikimedia.org/) and start driving data analysis and data representation on it.
Try SELECT Whatever FROM tablea a LEFT JOIN (SELECT DISTINCT productno, etc FROM tableb) b ON b.productno = a.productno Switch a and b as needed 
Maybe try [Schemaverse ](https://schemaverse.com/) it's an online space sim that you play in sql. It's postgres but it's a good way to get some sql practice in outside of tutorials
I've done most of the SQL related courses on Pluralsight and done some training Brent Ozar's. But a lot of material is readily available online (sqlskills.com) and there's a whole lot to learn about MS SQL alone. But most important is to learn your own environment, what challenges is your business facing? Learn what's needed first.
Stanford has a free MOOC. 
That worked like a charm! Thank you SO much :)! I'm constantly surprised of the possibilities that SQL has :D!
This is called a subquery. You can also use them in the WHERE clause to remove or include results from the subquery. Edit: this is actually a Derived Table https://logicalread.com/when-to-apply-sql-server-derived-tables-mc03/ A Subquery is slightly different https://technet.microsoft.com/en-us/library/ms189575(v=sql.105).aspx
Hackerrank.com has some cool modules. Some are overkill, some are too basic, but I'm running through them right now. 
This is actually a derives table. Subqueries have a reference to an outer query in them. Select * FROM a WHERE field1= ( SELECT field7 FROM b WHERE field3 = a.field1 )
I've learned a lot from personal projects. I regularly build little datamarts and cubes about subjects I'm curious about. Some of the things I've learned are full text indexes, various CLR methods, recursive cte limitations and workarounds, cdc, sparse columns and column sets, mdx, many-many dimensions, and others.
Huh. TIL. Guess I'm not too hot on my definitions!
love this, figured it out ya'll thank you aha 
this is a great resource
Won't you have a ton of duplicated data? How many tables are you populating with all of the dates of the year? Why do you need all of the dates of the year in the first place? Even if there was a simple datetime column you can return the day in a query based on the date value. Sounds like you are trying to create blank records with no data and then populate them afterwards, which is usually (always?) unnecessary. Unless you are building some sort of calendar or custom fiscal year or something like that.
Yeah, you may be right actually, hadn't thought of that. I was going to turn the tables into forms anyway. The people I'm working with aren't so tech savvy so I'm trying to simplify the process as much as I can for them, thanks! 
Just use the GUI to bound and load the data. Try using a [Pop-up Calendar](https://www.techonthenet.com/access/forms/popup_cal2007.php)
I highly recommend you construct a table fo all dates from 1950 to 2050 and all numbers from 1 to 1 million. They will come very much in handy! There are a million ways to create a numbers table, although I never use access. Once you have a numbers table, you do : create table GFT_W_DATES as ( select DateAdd('d', '02-02-2017', number) as date , USD_Wires , Non-USD-Wires from GFT join NUMBERS on 1=1 where NUMBERS.number &lt;= 365 )
That's how I learned it. I can't find the exact class on my initial search, but I recommend trying an MOOC from somewhere like [coursera.](https://www.coursera.org/) 
What kind of database, and is this the entire query?
First, you should bring join on acf.DOCNUMBER, if that's the one you're bringing in from your pull: &gt; LEFT JOIN CUST_ACT act ON (act.DOCNUMBER=ant.DOCNUMBER AND act.dbktype=ant.dbktype AND act.bookyear=ant.bookyear AND act.accountgl=ant.accountgl) If you get the same error, you need to check the table for duplicate DOCNUMBERs. 
Have you got docnumber anywhere else in your query that doesn't begin with your table name before it? So ant.docnumber etc? 
Just to clarify that last point, you need to check for duplicate docnumber fields, as in, does docnumber field appear twice in the table. As opposed to having duplicate docnumbers in the data. I mean that's not great either, it might throw up incorrect results, but having duplicate docnumbers in your data wouldn't throw up an error. It sounds like something isn't aliased correctly. 
Awesome, thanks for the links :)!
I am learning PL/SQL is there any disadvantages in PL/SQL compared to TSQL.
If you are wanting to get better at writing queries then anything by Itzik Ben-Gan.
That's exactly what AlwaysOn is for. Less so for distributed regional service.
So, look at your SELECT portion as "What do I want to display", your FROM Clause as "what total data set am I looking at", and your WHERE clause as your "Filter what records I want to see". A Count(&lt;columnname&gt;) in your select statement will give you the TOTAL count of records that meet your criteria from the WHERE clause in your query. Remember that what you filter on, doesnt necessarily need to be displayed. pseudocode (because this is homework): select what you want to see (full name, count(&lt;column you want counted&gt;)) from (person.person table) where (firstname is John) if you post what you have tried, you're more likely to get help - as others have said.
also, hackerrank.com sign up, and go through their quizzes. It will grade you on how long it took you to finish up the query, and if it returned the proper results. https://www.hackerrank.com/ Use SQLFiddle http://sqlfiddle.com/ to mess around and/or share code with people. Try and answer questions on r/sql ... lots of people ask homework questions, so we try to not just give answers if we know it is for a class - rather we try to lead someone down the right path of thinking. Honestly, the critical thinking part is more important than syntax in the long run. Everybody uses a code corrector to see when they missed a comma or neglected a data point in the select clause that is in the group by....
Do you mean the Workbench? I don't think there's any specific reason why you couldn't just download 6.3.9 or whatever the latest version is. 
T-SQL: In the real world, I would always try and run a script that someone else generated or simply import a ready made table from Excel to do this. Here's a fun way with code (not something I would do in prod...probably): ;WITH CTE AS ( SELECT 1 as DayOfYear, CAST('2017-01-01' AS DATE) As FullDate UNION ALL SELECT DayOfYear + 1 as DayOfYear, DATEADD(DD, 1, FullDate) as FullDate FROM CTE WHERE DATEADD(DD, 1, FullDate) != '2018-01-01' ) SELECT * FROM CTE OPTION (MAXRECURSION 366) If you're new to SQL, this also gives you an opportunity to try out WHILE loops (or cursors, if you must.)
Maybe, I'll try
Have you tried putting Indexes on the tables that you are querying?
http://imgur.com/a/Jhx2B This is a picture of the page from the book. Workbench doesn't really look like what I should be doing, the next page tells me to call it from cmd, which fails. 
^(Hi, I'm a bot for linking direct images of albums with only 1 image) https://i.imgur.com/zylUxXk.jpg ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20dlcnh8b) 
Mysql workbench is a graphical program where you can see/interact with your tables and queries in a more intuitive way, you can type a query and see the results in a grid similar to excel. Your book seems to be starting from the command line. SQL can be run there, but it's not as easy to use as Mysql workbench. If you're completely new, I would recommend doing interactive tutorials to learn the syntax, technical books can be too thorough when it comes to SQL, which only requires a few commands to be competent, the rest is more nuanced/ can be referenced. When it comes to downloading mysql/workbench, I would just go with the latest versions
SSRS hates subqueries. Ideally what you want to do is just run that query and dump the data into a table then point SSRS right at the table and do a select * to build your report. If you do want / need / have to do it all in SSRS then I believe (IIRC) that you can use #tables to break it down so you aren't using sub-queries. Also you're probably going to shit with your parameter passing, etc. Really just dump the entire process into a table and have it run as an SPROC at n-interval then point your report at it. Way easier.
Check out [Using GROUP BY with ROLLUP, CUBE, and GROUPING SETS](https://technet.microsoft.com/en-us/library/bb522495.aspx) i think you'll want to do something like that to make it easier on yourself. Specifically, the _ROLLUP_ option will get you nicely rolled subsets by (their example) Year, then Month, the Day. You could do the same for Year, Month, Week. Then, you can just look at the Week/Month/Year subtotals and compare year to year. Added a Sample Query using the Rollup. You'll have to Change the names from Nmbr27&lt;TABLE_NAME&gt; to yours SELECT Sites.ID AS SiteID ,Sites.Descr AS SiteDesc ,SiteTypes.ID AS TypeID ,SiteTypes.Descr AS TypeDesc ,CountStats.CountYear ,CountStats.CountMonth ,CountStats.CountWeek ,CountStats.Qty FROM ( SELECT Count.SiteID ,DATEPART(yyyy,Count.DT) AS CountYear ,DATEPART(mm,Count.DT) AS CountMonth ,DATEPART(ISO_WEEK,Count.DT) AS CountWeek ,SUM(Count.Qty) AS Qty FROM Nmbr27Count AS Count GROUP BY Count.SiteID ,ROLLUP ( DATEPART(yyyy,Count.DT) ,DATEPART(mm,Count.DT) ,DATEPART(ISO_WEEK,Count.DT) ) ) AS CountStats LEFT JOIN Nmbr27Sites AS Sites ON Sites.ID = CountStats.SiteID LEFT JOIN Nmbr27SiteTypes AS SiteTypes ON SiteTypes.ID = Sites.SiteTypeID ORDER BY Sites.ID ,CountStats.CountMonth DESC ,CountStats.CountWeek DESC ,CountStats.CountYear DESC Output looks like this: SiteID |SiteDesc |TypeID |TypeDesc |CountYear |CountMonth |CountWeek |Qty ---|---|----|----|----|----|----|---- 7| Site7| 4| Non-Profit| 2017| 4| 17|11210 7| Site7| 4| Non-Profit| 2016| 4| 17|22779 7| Site7| 4| Non-Profit| 2017| 4| 14|14191 7| Site7| 4| Non-Profit| 2016| 4| 14|22571 7| Site7| 4| Non-Profit| 2017| 4| *NULL*|25401 7| Site7| 4| Non-Profit| 2016| 4| *NULL*|45350 7| Site7| 4| Non-Profit| 2017| 3| 11|8440 7| Site7| 4| Non-Profit| 2017| 3| *NULL*|8440 7| Site7| 4| Non-Profit| 2017| 2| 8|13176 7| Site7| 4| Non-Profit| 2017| 2| 5|12297 7| Site7| 4| Non-Profit| 2017| 2| *NULL*|25473 7| Site7| 4| Non-Profit| 2017| 1| 2|14210 7| Site7| 4| Non-Profit| 2017| 1| *NULL*|14210 7| Site7| 4| Non-Profit| 2017| *NULL*| *NULL*|115571 7| Site7| 4| Non-Profit| 2016| *NULL*| *NULL*|289799 7| Site7| 4| Non-Profit| *NULL*| *NULL*| *NULL*|405370 *Edit: Added a sample query*
Yes to this. In our shop we finally created a "Reporting" schema with big, wide, de-normalized and properly indexed tables. Its sole purpose is to feed SSRS reports, fast, and it works great. Also, instead of subqueries, try #temp tables as suggested above, or Common Table Expressions. 
Thank you. Should have mentioned I'm pretty new to this. Never worked with temp tables or stored procedures yet. Will have to give them a shot.
Do a Google search for "SSRS Parameter sniffing" and it will make your life easier to understand how it functions. In a nutshell what you want to do is take your parent query and point it at a table, and just do a select * inside of it to a #table or a permanent table of your choosing. Will make life much easier.
It might be worth refactoring the code into CTEs rather than sub-queries. They should behave the same in theory but might help; it's clearer to read anyway. I agree with the others regarding indexing the date. You might as well also remove the ORDER BY and do it in your report too. Let us know if you figure it out. I don't see why it would be so much slower running through SSRS - I love puzzles like this!
Can you get an execution plan for the query? edit: Also you can try `OPTION (OPTIMIZE FOR (@startdate = '20170101', @enddate = '20170131', @providerid = 20, @appttype = 55)` for some reasonable values that generate a good execution plan.
&gt; I can't find MySQL 6.0, this book is a few years old. What am I missing, did it get rolled back or something? Yes. MySQL 6.0 was never released. It was a version number assigned before Oracle bought MySQL (well, they bought Sun, which owned MySQL at that time). Oracle decided to continue the 5.x versioning for a while, releasing 5.6 and then 5.7. Just recently they found they don't like this scheme anymore so that the next MySQL major release will be 8.0. However, without knowing the book in question, I'm pretty sure you can use any recent MySQL release for your purposes.
Try converting 0 to NULL. If it makes you feel better, I can come up with every more messy ways to do it.
haha I'm sure there are much messier ways of doing it, but I am really trying to avoid converting the 0's to NULLS as when I do my final insert I will have to convert the NULLS back to 0 (it's a not null field), but might be the only way... I only need this for one column so wont be too bad I guess
My go to book is msdn. Are there any books that teach Merge Joins, Hash Joins, and Nested Loops? I continue to be surprised how many SQL professionals have never heard those terms.
What's up with like 75% of the posts on /r/SQL being spam? It's not like the traffic here is exceptionally good. How does this have 14 upvotes? OP's account is almost fully spam links to this datapine site. This all smells *SUPER* fishy.
&gt; I continue to be surprised how many SQL professionals have never heard those terms. possibly because SQL is not MS SQL Server i'm not saying that the things you mentioned aren't important... they are, but they're only important to a SQL Server DBA someone who wants to learn **SQL, the language**, is ~not~ going to be interested in those things once again, a reminder that /r/SQL tries to be **platform neutral** -- please see sideboard if ~any~ /r/SQL thread is going to want platform neutral replies, it's this one -- **"Books To Learn SQL"**
What? How are merge joins, hash joins, and nested loops not platform neutral? And isn't it pretty important for creating indexes, and optimizing queries?
Thanks for the help. That's what I've done with reports that need year/month/day, but I don't think it works properly with year/month/week. Week is not perfectly inside one month. Ex: In 2017, week 5 starts on 1/29 and ends on 2/4. So the Qty column you have doesn't always include the full week. Or am I missing something? Edit: Also, how do I get data just through yesterday? This year, 8/8 is in week 32. Last year, 8/8 was in week 33, so my sum for week 32 includes 7 days of data for last year. I need it to just go the 3rd day of that week.
SSRS does weird things when you run complex queries through it -- which goes double when you use parameters. The simplest and most straight forward way to solve the problem is like this: SELECT s.DateTime , s.ProviderID , s.AppointmentID , Status , c.ChgAmt , c.AppointmentID INTO #TABLE FROM SchAppointments s LEFT JOIN (SELECT s.AppointmentID, tik.ChgAmt FROM SchAppointments s LEFT JOIN PbrTicket t ON s.AprEncounterNumber = t.Number LEFT JOIN ( SELECT a.TicketID, SUM(p.Amount) [ChgAmt] FROM PbrAccountTransactionsAdd a LEFT JOIN PbrAccountTransactions p ON a.AccountID = p.AccountID AND a.TransactionID = p.TransactionID WHERE p.ProcedureChargeCatID in ('E&amp;M') AND p.Type = 'C' GROUP BY a.TicketID) tik ON t.TicketID = tik.TicketID WHERE --s.DateTime between @startdate and @enddate --AND s.AppointmentTypeID in (@appttype) --AND s.ProviderID in (@providerid) s.Status = 'ATTENDED' ) c ON c.AppointmentID=s.AppointmentID SELECT CONCAT(MONTH(s.DateTime),'/', YEAR(s.DateTime)) [Month/Year] , sum(case when Status = 'ATTENDED' then 1 else 0 end) [Attended] , sum(case when Status in ('CANCELLED','NO SHOW') then 1 else 0 end) [Cancelled/NoShow] , sum(case when Status = 'BOOKED' then 1 else 0 end) [Booked] , SUM(c.ChgAmt) [SumMonthCharges] , ROUND((SUM(c.ChgAmt)/COUNT(c.AppointmentID)),2) [MonthAvgCharge] FROM #TABLE S WHERE s.DateTime between @startdate and @enddate AND s.ProviderID in (@providerid) AND s. AppointmentTypeID in (@appttype) GROUP BY MONTH(s.DateTime), YEAR(s.DateTime) ORDER BY YEAR(s.DateTime) desc, MONTH(s.DateTime) desc In a perfect world you create the top part as a SPROC which dumps the data into a permanent table, and then you use the bottom part in SSRS. e: If the data is too large then you could create (2) #Table processes which each use the @parameters, and then join them at the end in a SELECT to feed SSRS. e2: A CTE may not save your ass from parameter hell.
The right-hand side spacer does clearly state: WHERE /r/SQL != Spam
I'm still trying to figure out CTEs (I'm fairly new to this). Removing the ORDER BY didn't make a significant difference. I added additional WHERE constraints on the dates on the inner queries and that made a huge difference in SSMS, but none in SSRS.
CTEs are just like local views: WITH x as (SELECT a from b) SELECT a from x It makes reading a query a lot more easier than sub queries. 
It might be worth also running a trace on the SQL server to see if indeed the same query is being run. 
Nothing by Celko or Date...wtf
Definitely an affiliate marketer trying to make some profits. 
you can download the stackOverflow database from their website and run it on SQL server developer edition.
please illustrate your question with sample data "a number of strings" is too vague otherwise, here is your syntax template -- INSERT INTO some_other_table ( column1 , column2 ) VALUES ( 1, 'moe' ) , ( 2, 'curly' ) , ( 3, 'larry' ) , ( 4, 'shemp' ) , ( 5, 'curly joe' ) , ( 6, 'joe derita' )
i'm in the same boat, spending most of my day practicing SQL. Here's a few training sites i use:https://community.modeanalytics.com/sql/tutorial/introduction-to-sql/ https://www.codecademy.com/learn 
look at the indexing on the 2 tables. probably don't have proper indexes to support this delete and on the others you do. 
In Pastetable and Raw, actually convert the Survey Date to a date data type (or create a new date field and populate it based on Survey Date) rather than doing it on the fly with a function in the delete, and index them both. If they're already both dates, remove the date value function.
Access? Compact database will update statistics and force query plan recompile. My guess is the query plan is optimized for the larger dataset which is why it runs quick. If your primary function is running this query you might also add an index to the date field which will boost performance but may hinder performance on things like appends/inserts.
Definitely agree parameter sniffing is the culprit. My recommendation is to wrap all ssrs queries in stored procs and define variables which can then be hooked to the parameters in report builder/visual studio. It does get complex when you need to run a list/IN clause through a variable but there are solutions depending on server version. Also for sure evaluate your table structures and add appropriate indexing if this is a reporting server. Preprocessing the data into tables would work but I don't think recreating data a bunch of different ways for each report request is very elegant and you have to give report users additional permissions.
You're looking for an upsert. There isn't a defined way in mssql. The method you choose will depend on usage patterns.
I've been directed toward researching a Merge statement to gather what I need but it seems like more work than it should have to be. I figured someone to do this would have already existed. upsert is what type of database? 
The merge statement is the equivalent of upsert. I've found that using SSIS and doing inserts and updates in separate steps is often more efficient. But if efficiency isn't the main concern, a merge statement will work fine.
I'm not a database guy. when you talk about efficiency I'm not really sure what take means. I have about 2.5 seconds to insert new data, delete non-existent data, and update existing data. The data consists of about 30,000 lines containing between 7-13 different values.
You're also deleting non-existent data? Is there a reason you can't completely replace the old with the new? BEGIN TRANSACTION; TRUNCATE TABLE dbo.Target; ALTER TABLE dbo.Source SWITCH TO dbo.Target; COMMIT TRANSACTION;
Merge is a good method in SQL server. I've you getva grasp on it building out so be easier. I found a big post the other day that went over 10 ways of doing it. Some good some bad maybe a I can find it tomorrow. One was a TRY UPDATE CATCH INSERT combo that while accomplished the goal, had its own cringey feel to it. 
that would be great. I'm looking for a lot of examples and basic information so i can determine whats best for me
I guess this is also an option. I didn't think of that. Thank you.
Codefights has an entire section in the arcade concerning SQL. Good stuff. I've learned a lot, some of the problems are extremely challenging.
Although I agree that SQL is not MS SQL Server (I'm one suffering from this confusion a lot) I also agree that join algorithms are not product specific. Up to a certain extend, SQL users can surely ignore those. Once they need to investigate performance, they'll need to understand how it works under the hoods. A lot of what happens is platform depending, but many issues are not — just because the algorithms used by the systems are essentially the same. Many databases support the three joins algorithms mentioned. Those which don't usually support a proper subset (looking at MySQL/MariaDB and SQLite). I even wrote a book about SQL performance that is platform neutral ([SQL Performance Explained](http://sql-performance-explained.com) — content freely available on http://use-the-index-luke.com). So, join algorithms and platform neutral doesn't seem like a contradiction to me (they are covered in my book: http://use-the-index-luke.com/sql/join). 
If you're aiming for a career solely working in MSSQL, yes. If you're aiming for a career involving Oracle, no.
If input would be something like "test' or ''= '" then the query would look like: SELECT uid from usr WHERE user = 'test' or ''= '' Because '' = '' are always true, you will be able to get all users id's.
If the variable `user` contains a valid fragment of SQL, the creator of the input can alter the query dramatically. Example: user = "';delete from usr;select 'anything"; query = "SELECT uid from usr WHERE user='" + user + "'" `query` is now `SELECT uid from usr WHERE user='';delete from usr;select 'anything'"`
Because you're taking raw user input and executing that in your query. http://bobby-tables.com
Ummm, create the dates with a spreadsheet and then paste them into a table? It's not sql, but this is kind of an esoteric task for access...
And that's bad, right?
I hope that's sarcasm, lol.
;-)
Just put a time stamp when for last accessed on your user/question table or whatever and maybe a last answered or something along those lines and use a subquery to determine the last question accessed or answered then return that in the main query.
There's only two XKCD #s I have memorized and that's one of them.
Like your name... It's not too bad if you like having to constantly troubleshoot why your queries are failing... Like if Sarah O'Connor tries to sign up for a membership. And it can be really exciting if your namesake signs up. But yeah, obviously in normal worlds, its' bad. I'm glad I heard about the term SQL injection before I ever got too deep into SQL, so when I posted my first line of code and someone came at me with a correction, I immediately understood what they meant and took steps to prevent it.
I don't even have it memorized, have to google every time. 
Just out of curiosity What SSRS program are you using?
How about if it was query = "SELECT...user='" + REPLACE(user,"'","''") + "'" ?
check out his username :)
Don't write your own input cleansing routines. You'll almost always miss an edge case. Or it'll be exploited. Or you'll mangle valid data. This is why parameterized queries (prepared statements) exist.
i can't read your version, so i reformatted it... SELECT CONCAT( '&lt;a href="admin_view_student.jsp?studentid=' , CAST(SDT.studentId AS CHAR) , '"&gt;' , CAST(SDT.firstName AS CHAR) , ' ' , CAST(SDT.lastName AS CHAR) , '&lt;/a&gt;' ) AS Name , PGM.programmeName AS 'Program' , FORMAT(SUM(ATD.duration),2) AS 'Total Hours' , PGM.MinClockHours AS 'Program Total' , ATD.attendancedate AS 'ATTDATE' , REG.Enddate AS 'EndDate' , CASE WHEN MAX(ATD.attendancedate) &gt;= REG.Enddate THEN SUM(ATD.duration) /* error */ BETWEEN MIN(ATD.attendancedate) AND REG.enddate ELSE NULL END AS 'Whatever' FROM Attendance ATD INNER JOIN Registrations REG ON REG.studentId = ATD.studentId AND REG.isActive = 1 AND REG.enrollmentSemesterId = 4000441 INNER JOIN Programmes PGM ON PGM.programmeId = REG.programmeId AND PGM.isActive = 1 AND PGM.programmename NOT IN ('Careers Pathway' ,'Instructor Training') INNER JOIN Students SDT ON SDT.studentId = REG.studentId AND SDT.isactive = 1 WHERE /* error */ REG.ADMINID GROUP BY SDT.Lastname ORDER BY SDT.firstName you have one syntax error (you cannot say `THEN SUM(ATD.duration) BETWEEN`) and one probable semantic error `WHERE REG.ADMINID` 
Not sure why you need the except. Where topping = 3 or topping = 2 already excludes values where topping = 1.
Just curious: Does anyone have an edge case or an exploit for this? I do agree with you about parameterized queries though.
Thanks, REG.adminID has to be there for our system. How can I get the BETWEEN to work is what I am asking.
Is this what you're looking for. SELECT o.Pizza , SUM(o.Amt) AS Amt FROM dbo.Orders AS o GROUP BY o.Pizza HAVING SUM(CASE WHEN o.Topping = 1 THEN 1 ELSE 0 END) = 0 AND SUM(CASE WHEN o.Topping IN (2,3) THEN 1 ELSE 0 END) &gt; 0;
I'm not really sure what you're trying to do there. `WHEN MIN(ATD.attendancedate) BETWEEN [some date] AND [another date] THEN [some expression] ELSE NULL END AS [column name]` is the correct usage. And `WHERE REG.ADMINID` is not valid either so you need a condition like `WHERE REG.ADMINID = 1`. Just saying it "has to be there" won't suddenly make it valid SQL. ;)
They are actually date time type already. The problem is I don't want the time :). Datevalue() in JET will keep the date and zero out the times. That said I didn't need the datevalue() conversion on the raw table to I removed that. Still taking an eon though.
I indexed survey date on both tables last night. Still running in the same amount of time (not sure how long- it's never actually finished. Even when left alone for 48 hours). I went ahead and did a compact/repair just now... still taking eons :(.
There's a few different fields that are indexed. The raw table has a primary key that's auto-number, pastetable doesn't since it's coming from an extract. Both survey date fields are indexed with duplicates.
Dang. One option would be dump the unique dates into a temp table and do your delete using an inner join to that small date table instead of subquery. SQL server I wouldn't normally recommend this option but I think it's ok for access. If you don't want to create another temp table you can try the subquery in the join clause instead of the where clause and see if that helps and you could try group by instead of distinct.
I think you're better of just building the Json rather that using the Xml converter. I don't see where the Xml converter handles arrays, and you can't start an xml element with a number, like "1YS1" or "1YS2". Try this instead: SELECT '{ "status": "success", "response": { "p_status_msg": { "AcademicPlan": { "1YS1": [' + STUFF((SELECT ', {' + CHAR(10) + ' "EVENT_ID": "' + s.EVENT_ID + '",' + CHAR(10) + ' "EVENT_SUB_TYPE": "' + s.EVENT_SUB_TYPE + '",' + CHAR(10) + ' "EVENT_NAME": "' + s.EVENT_NAME + '"' + CHAR(10) + ' }' AS [text()] FROM dbo.STDDEGREQEVENT AS s WHERE s.EVENT_CLASS = '1YS1' FOR XML PATH('')),1,2,'') + '], "1YS2": [' + STUFF((SELECT ', {' + CHAR(10) + ' "EVENT_ID": "' + s.EVENT_ID + '",' + CHAR(10) + ' "EVENT_SUB_TYPE": "' + s.EVENT_SUB_TYPE + '",' + CHAR(10) + ' "EVENT_NAME": "' + s.EVENT_NAME + '"' + CHAR(10) + ' }' AS [text()] FROM dbo.STDDEGREQEVENT AS s WHERE s.EVENT_CLASS = '1YS2' FOR XML PATH('')),1,2,'') + '] } } } }' AS Json;
select * from ( select distinct AcademicPlan = event_class ) x JOIN ( select eventdata from source join ) y on y.event_class = x.academicPlan for json auto fix it along, but having multiple data sources via JOIN is how you end up having hierarchical data in XML/JSON
&lt;ADMINID&gt; Admin Id of school. This is a required tag for all queries. Value will be automatically inserted. Like I said it has to be there, this requirement as added by our overseas developers, I have no control over it. I add it to the query as it is needed... I need to get all students who have gone past the registration date, and get the hours SUM(ATD.duration) from the registration date backwards. So if I did ATD.attendancedate BETWEEN [some date] AND [another date] THEN [some expression] ELSE NULL END AS [column name], it would pull every single students with dates between there not just students who have gone past. If I do Where ATD.attendancedate &gt; Reg.enddate then of course I only get hours past the end date.
I had been hoping to avoid hard coding the EVENT_CLASS, because there is up to a possible ~20 distinct values for that, but I'm starting to lean towards this. Otherwise, I would have to do some complicated pivoting, I think.
So if a student has passed the reg end date but his hours were 560 and to end the program you need 600, we charge him for those 40 hours against a surcharge amount based on the program. So hours @ Reg.enddate - PRG.minClockhours
Unfortunately, I don't think 2008 supports the FOR JSON functionality.
If you need to find 600 - 560 = 40 wouldn't you subtract instead of sum? Or subtract the two sums?
If the language supports alternate escape characters, you'd have to handle each one. For example if you only escape `'` something like this could be trouble. `FU\';DELETE FROM User;--` -&gt; `WHERE user='FU\'';DELETE FROM User;--'`
That's no problem. SELECT '{ "status": "success", "response": { "p_status_msg": { "AcademicPlan": {' + STUFF((SELECT ',' + CHAR(10) + ' "' + s1.EVENT_CLASS + '": [' + STUFF((SELECT ', {' + CHAR(10) + ' "EVENT_ID": "' + s2.EVENT_ID + '",' + CHAR(10) + ' "EVENT_SUB_TYPE": "' + s2.EVENT_SUB_TYPE + '",' + CHAR(10) + ' "EVENT_NAME": "' + s2.EVENT_NAME + '"' + CHAR(10) + ' }' AS [text()] FROM dbo.STDDEGREQEVENT AS s2 WHERE s2.EVENT_CLASS = s1.EVENT_CLASS FOR XML PATH('')),1,2,'') + ']' AS [text()] FROM dbo.STDDEGREQEVENT AS s1 GROUP BY s1.EVENT_CLASS FOR XML PATH('')),1,1,'') + ' } } } }' AS Json;
Our duration is per day so if they go to school on Mon Weds and Friday and have completed 6.08, 7 and 10 hours respectively for each day, I have to sum that to get the total hours. I have to get the total hours for the time frame stated based on students who have passed their Enddate but from enddate to first attendance date as you can see in my jacked Case, before I can do anything along those lines. I was just trying to explain what I am doing based on what you wrote here I'm not really sure what you're trying to do there. WHEN MIN(ATD.attendancedate) BETWEEN [some date] AND [another date] THEN [some expression] ELSE NULL END AS [column name] is the correct usage.
This is very close, but now all the EVENTS are grouped under 1YS1, while there are 1YS2, 2YS1, 2YS2, etc.
Dude you are awesome. 
True. Apologies for not noticing... though same concept for structuring applies to xml, if you have a way to convert xml to json
I don't know what DB engine you are using (i'm a SQL Server admin) but the only way to diagnose this in my world is to get the query plan and see where the IO/CPU is going. then just work it out. There is no way to diagnose this just by looking at the command. You have to start looking at how the engine is executing your query. You're basically just telling the DB what the end result should be, its going to decide how to get it done and only understanding that will let you make changes. 
Yeah it's still not really clear sorry... do students take the classes twice? Why wouldn't you be able to subtract the sum of their hours from the total hours in the program? A few lines of sample data might help a lot. Obfuscated of course.
It works using the same data you provided. http://sqlfiddle.com/#!6/2a3ba/2
Yea that is the goal but the student has had to pass to reg.enddate My problem is getting these students who have passed this date, then summing their hours from their 1st date of attendance to the end date. I am having problems getting that sum So When the Students Attendance date is greater than end date I need to get the sum of duration between 1st day of attendance and the end date. If I do where MAX(atd.attendancedate)&gt; reg.enddate I of course only get hours after the end date ect. I am sorry if I am all over the place just trying to explain it. I am an intermediate SQL dev and really trying to get through this internship with my head still on my shoulders.
learn to use parameters, man
Aahh, I hadn't added in the student ID to narrow it down, so it was selecting *all* 1YS1 events... but now, it appears to work beautifully. Thank you so much!
You mean the DBMS language, right? So my doubling up of single quotes might work with SQL Server but not with MySQL which needs the \' construct. I wonder about some unicode quotes that SQL Server (or any other DBMS) might "honor".
I think because pizza can have toppings 1, 2, or 3.
[This very relevant video](https://www.youtube.com/watch?v=ciNHn38EyRc) actually walks through a methodology that a malicious user might use to determine the RDBMS, uncover what tables are in the database and then output private information to the screen. Another example: user = getInput(); query = "SELECT uid from usr WHERE user = '" user="' UNION ALL SELECT TABLE_NAME FROM INFORMATION_SCHEMA.TABLES UNION select 'anything" + "'" Could potentially return a list of valid table names to the screen. Edit: used u/alinroc's "anything" to make the query valid.
I thought a parameter is the part of a function that specifies the input, i.e, the parameters, no? 
There are a couple options. SELECT p.ArtikelID , p.Datum , p.Preis FROM (SELECT p.ArtikelID , p.Datum , p.Preis , ROW_NUMBER() OVER (PARTITION BY p.ArtikelID ORDER BY p.Datum DESC) AS RowNumber FROM Productprices AS p) AS p WHERE p.RowNumber = 1; or SELECT p.ArtikelID , MAX(p.Datum) AS Datum , CONVERT(money,STUFF(MAX(CONVERT(char(23),p.Datum,121)+CONVERT(varchar(20),p.Preis)),1,23,'')) FROM Productprices AS p GROUP BY p.ArtikelID;
I actually just got my big-boy program (SSMS) but this is currently going through MS Access since I don't have rights to write data to the SQL server. I'm still very much new to the SQL Server world. Any tips for getting going? Our Admin advised he'd set up a sandbox for me to work with. 
This is essentially what I've wound up doing. I got the Min and Max dates from the imported file. Set them up in a new table that refreshes prior to the delete query. Then the delete query runs with a static boundry instead of comparing against another query... Ran in a couple of seconds. Will work okay so long as the data isn't intermittent updates. So if a single update contains data for multiple date ranges it might delete out data unintentionally.
/u/jc4hokies shows two very straight forward methods using window functions and aggregate functions to come to the same conclusion. I wanted you to see yet another approach to help with a more straightforward method. I merely present this to open conversation to describe how to use this presented technique. * Define The Table with a natural key of the productid and date * Create a Common table expression for the latest record for each product * inner join the CTE with the Table Variable on the natural key TSQL Code DECLARE @ProductPrices TABLE ( [ProductID] INT ,[Datum] DATETIME ,[Preis] MONEY ,PRIMARY KEY ([ProductID], [Datum]) ); INSERT INTO @ProductPrices ( [ProductID], [Datum], [Preis] ) VALUES (4711, '2017-03-05', 5) ,(4711, '2017-07-30', 4) ,(4712, '2017-06-15', 20) ,(4713, '2017-01-01', 10) ,(4713, '2017-05-01', 12) ;WITH Newest (ProductId, Datum) AS ( SELECT p.ProductID AS ProductID ,MAX(p.Datum) AS Datum FROM @ProductPrices AS P GROUP BY P.ProductID ) SELECT P.ProductID ,P.Datum ,P.Preis FROM @ProductPrices AS P INNER JOIN Newest ON P.Datum = Newest.Datum AND P.ProductID = Newest.ProductId 
I've always done it this way: select P1.ArtikleID, P1.Datum, P1.Preis from ProductPrices P1 where P1.Datum = (select max(P2.Datum) from ProductPrices P2 where P1.ArtikleID = P2.ArtikleID) But the window function method from /u/jc4hokies works too.
How about if it was query = "select * from items where id="+REPLACE(id,"'","''") Escaping can sometimes save you, if done exactly right, but only in a quoted context. If you're injecting a number, and you haven't cast it to a numeric type, then the attacker can do a lot of damage without ever needing single quotes. So the REPLACE in this case is useless.
Try this: DELETE AA.* FROM Raw AS AA WHERE datevalue(AA.[Survey Date]) IN (SELECT DISTINCT datevalue(PasteTable.[Survey Date]) from PasteTable) AND AA.[Survey Date] &gt;= (SELECT MIN(datevalue(PasteTable.[Survey Date])) from PasteTable) AND AA.[Survey Date] &lt;= (SELECT MAX(datevalue(PasteTable.[Survey Date])) from PasteTable);
Assuming one record per day only, otherwise should be datetime. Select ProductId, max(datm) Date, Max(preis) preis From product Group by productId 
This. If you have not gotten in to window functions, you are missing out on a whole new dimension of problem solving. The functions are critical to getting Max value from SQL.
 The +user+ is. It will let the end user do anything they like with your SQL data base. eg if user is "'; DROP TABLE usr; SELECT ';". It probably won't even produce an error
 Please do not ever do this. Data bastractions should not leak. This leaks and it also stinks.
 No don't do this. id is probably an int? So it should be input as an id rather than have the sql server do an explict converstion on it. Every single sql client lib thats worth anything has paramaters eg id=1 sql = SELECT * FROM item where id = ?; sql.execute(id); As for complex dynamic quries its not that hard either. args = []; sql = SELECT * FROM table; if (something ) { sql + = "WHERE name = ?; args[] = something; //Add paramater to array } sql.execute(args); 
My point was not about using parameterized prepared statements. These are an effective protection method and should always be used. My point was a criticism of escaping. You have an escaping technique (doubling of single quotes) that may be effective when the attacker is trying to break out of a quoted string. But if you notice, there was no quoting in my example. The attacker is in an integer context. So escaping quotes offers no protection.
This is great. Only complaint is using p for the alias for both queries makes it a little less readable.
I use the same alias for subqueries of a single table as a standard. It helps me recognize where a column comes from in one step instead of two.
In this case, where id is an int on the client side, whether implicitly or explicitly, it gets converted to a string, right? I'm just asking (not advocating): where's the exposure for injection there? In parameterized queries, the cleansing still happens, just at the library level. What does it do beyond any of this? 
No, it doesn't. The parameter is expected to be an integer (represented in string form), so you could call something like get_item.jsp?id=7 The resulting query would look like this: select * from items where id=7 So, while the query is a string, the id in the query would be treated as an integer at parsing time.
And in parameterized queries, as long as the only attacker-controlled values are in the parameters, there is no exposure for injection. My point was, in the scenario above, where the query is built without quotes, escaping provides very little protection against injection.
Further clarification on the scale.... each "digit" is going from 0, 1, ...9, A, B, ...X, Z
why?
If I ask my boss that he'll say "because that's how we do it" lol. I'm guessing his logic is that the 0-Z scale gives you more permutations without adding string length. And I'm sure there's a bunch of legacy code we have that's already expecting this format...
But who cares what the length of the ID is? Has he heard of bigint? Letting the database generate an integer ID has numerous advantages over this. Performance being a big one. You're trading like 1 or 2 extra numbers in an integer for a bunch of downsides.
So how are you going to implement this? Is it going to be a computed column? When two users add customers at the same time, how do you prevent collision of the id values? Lock the tables? More performance issues... 
OK his argument: "Integer will get too long. Customers are going to see this field and will need to know it/enter it elsewhere so it needs to be user friendly to the eye and not a hundred digits long" Still seems iffy to me.
I've seen something similar at my old company and the app code would make sure insert calls would only be executed one at a time. I don't know enough about app coding to give specifics though.
Sorry: By client I meant the jsp that's calling the DBMS. If, inside of get_item.jsp, you use a parseInt on request.getParameter("id"), then back as part of the command string, no escaping is necessary, right? I'm just trying to find out, specifically, how an injection could work in this scenario.
Ah, yes, this type casting is usually a good defense for ints. But, casting plus parameterized commands is even better. I am always reluctant to recommend anything as a replacement for parameterized prepared statements, because so many things have been bypassed over the last 15 years, whereas parameterized prepared statements always squish SQLi flat. Of course, there are those cases when you want to use input somewhere parameters are not allowed. Then you need other techniques, like letting the user specify an index that you use to look up a string in a list (for a table name or field name, for example). If at all possible though, no user-supplied string should ever be part of the prepared string.
i actually did this a few months ago... users wanted short strings instead of longer numbers... though in their case they'd also excluded letters which looked like numbers (i, o, etc)... net result was 31 digits for performance during bulk inserts, the table still used an identity(1,1)... it then included a persisted computed column (note: you want at least SQL2016, as persisted computed columns are recalculated during DBCC checks in prior versions - see [this](https://connect.microsoft.com/SQLServer/feedback/details/1026360/checkdb-flag-option-to-disable-recalculation-of-persisted-computed-columns)). The table is as follows: [intID] [int] IDENTITY(1,1) NOT NULL [txtID] AS substring('0123456789BCDFGHJKLMNPQRSTVWXYZ',([intID]/( 961))%(31)+(1),(1))) + substring('0123456789BCDFGHJKLMNPQRSTVWXYZ',([intID]/( 31))%(31)+(1),(1))) + substring('0123456789BCDFGHJKLMNPQRSTVWXYZ', [intID] %(31)+(1),(1))) with a PK on [txtID]. Technically it went further to support the full length, and it had some other handling as well, but you get the idea. it should be noted that this had a roughly 30% impact on performance (but was a ton faster than some of the other approaches being discussed).
I think you can use [REGEXP_REPLACE](https://docs.oracle.com/cd/B12037_01/server.101/b10759/functions115.htm#SQLRF06302) to strip the date(s) from the file name and then match on the result. Something like: SELECT * FROM FILE_INFO F INNER JOIN FILE_PARAMETERS P ON (REGEXP_REPLACE(P.FILE_NAME, '(-[0-9]{8})+', '') = F.FILE_NAME) If you're going to be doing a lot of working with matching parts of strings, it's probably worth your time to learn a bit about regular expressions. They're really helpful. [regex101](https://regex101.com/r/s5wgCQ/1) is a good place to test them; it explains what each element is doing.
This is a really cool way to do it--thanks! 
I'm sorry to bother you OP, but would you mind showing us the results of this(or just the general format, real data not needed)? I'm a little confused on why we needed the two Case statements in the HAVING clause (or perhaps I don't fully understand the query requirements). 
Sounds like a problem with the front end, you typically do not expose primary/foreign keys to the user. You are essentially convertings a number from base10 to base32 (thats what the other guys response was).
This type of information typically goes in a user's table. If you include it as part of the original table, that will also work, but you have duplication.
Why are you producing json from a SQL server rather than a web api? This would be pretty simple to do inside entity framework.
Quick question: Why do you need to join on P.ProductID = Newest.ProductID? Wouldn't the newest.datum join take care of that? Thanks!
The way I implemented in the OP, it starts from an arbitrary ID (not set as PK or FK) we hardcode and then my function, due to being default binding on the column, is called on every insert. Which I'm sure has it's own set of faults that I'm not aware of yet...
I assumed foreign key since your boss said that people will need to know it/enter it somewhere. When you have a default column that is not specified during inserts, you typically should also have a check constraint should someone decide to specify their own value. 
&gt; Update/Edit I ran the delete * with a specific range and it ran instantly. I ran my select distinct separately and it, too, ran instantly... I'm truly lost on what the issue might be here. I'm surprised no one here has mentioned this, but change that nonsense to a join. something like... SELECT DISTINCT datevalue(PasteTable.[Survey Date]) INTO #whatever from PasteTable DELETE AA FROM Raw AS AA INNER JOIN #whatever w on aa.[survey date] = w.[survey date] unless you can't use temp tables with your permissions/platform. Not sure...
For the example. Each Pizza has an Entry in the dbo.Orders, for each topping. Like the following. Pizza | Topping 1234 | 1 1234 | 2 1235 | 3 1235 | 2 1236 | 3 The request was to be able to identify Pizza Orders that had topping 2 and/or 3, but did not have topping 1. Based on this example data, the output of the query would be the following. Pizza 1235 1236
Oh, I understand now. Definitely did not understand that each pizza / topping had its own entry. Missed that! Thanks for taking the time. 
It would would give you July 30th for all products and fail to properly return the correct products price on the correct date record item.
Window functions!!!!! The first example for the rank function is basically the same solution. you would just filter where rank = 1. https://docs.microsoft.com/en-us/sql/t-sql/functions/rank-transact-sql 
Nested query Basically built a query that totals them all up by Department Name and Number, then query THAT table by only taking the rows where the total is max. That should get you 80-90% of the way there, I don't want to ruin all the fun. 
How frequently are products added or product criteria change?
It looks like you're on the right track, however, I do have some suggestions. Firstly, I don't quite understand the difference between your Customer and CustomerDemographic table. It just looks like The CustomerDemographic table is a more detailed version of the Customer table. If what I said is true, then there's no need for both. Remove the Customer table, and rename your CustomerDemographic table to Customer. Next, I was having a little difficulty understanding what you meant by joining Cartesian matrixes with tables since data modeling is ultimately set theory. Either way, here's tables I would use: 1. Customer (aka CustomerDemographic) 2. Criteria 3. Product 4. CustomerCriteria 5. ProductCriteria For Customer, it will be structured like you demonstrated in your CustomerDemographic table front earlier. For Product, this will list each unique product with a unique ID number like you already have. For Criteria, this can be a list if unique criteria that you want to check for that a customer may have associated with them (i.e. specific job roles, states they live in, etc.) For the CustomerCriteria table, this will store and criteria you want to relate to a customer. There for the ProductCriteria table, this will store any products that you want to associate any criteria with a product. Then you can easily inner join the Customer, CustomerCriteria, ProductCriteria, and Product tables together to give you a list of products for each customer, or vice versa, where they have matching criteria in their respective CustomerCriteria/ProductCriteria tables. 
Redgate SQL prompt does this I believe. Try the trial and see if you like it. I am sure there are other plugins that will do this as well.
There is ApexSQL Refactor for SSMS, and PL/SQL Developer has built-in beautifier. Both offer good deal of customization, but sadly these are all tools I know of.
As was mentioned earlier, redgate does this really well and has a ton of settings to get the code to what you want. There a are a bunch of free online ones I now use (current shop won't pay for redgate). I don't have a favorite, I just type SQL formatted into Google and run with it.
My preferred tool is SoftTree SQL Assistant. It works for a bunch of different SQL dialects. 
Ssmsboost is free and has code formatting as well as many other features. 
if you knew how many run-of-the-mill database developers wished they had a track to becoming data analysts...
Ironically, my go-to answer is the book *SQL for Smarties.* If you search carefully for the .pdf ...
If you don't mind me asking, what skills would I need to get the job title of Data Analyst?
Alright, maybe I'm thinking about this the wrong way, so out of curiosity is there any particular reason for that?
+1 for ssmsboost, i use the formatting tool at least a few times a day. For ssms 2016/17, download from the [beta channel](http://www.ssmsboost.com/social/posts/t5447-SSMSBoost-v3-0-Beta--for-SSMS-2008-2012-2014-2016--2017)
So, if I'm understanding this correctly..based on my experience.. Data Analyst is someone with read Access and some limited write abilities doing analysis and reporting work for more customer or business unit work. Database developer is the more IT told where they are building the data structures and datasets being used by the analysts. I have done both for financial companies (mortgage) and can tell you it is great to have both sides. I went analyst to dev because we needed to build our own sql box because IT sucked. It was great until IT audit came in and ate our lunch. Now, I advocate for being in the analyst role with the knowledge you learn from doing developer stuff. By that, take notice of query costs, efficiencies gained from correct indexing, and building a datamart well. Learn well the benefit of the query plan... You can get more exposure in the analyst role than the dev role. It's the Futurama quote for God, "When you do things right, people won't be sure you've done anything at all"
Because the money is in analyzing the data. Creating the data, sorting and storing it correctly is relatively easy after a few years, but making sense of it and communicating it in human language as actionable intel is what alot of people get paid good money for. This is atleast in my experience. Im in the same boat as you and was considering a move, but as an analyst now i always have people on the database dev side asking me how to get out
Depends what you're interested in exactly. To me, there's 3 identifiable data spaces to operate in these days. 1. Analysts who are tasked with understanding a business question, then using available data to answer it. Can involve building visualisations or notebook-type resources in business intelligence tools like Tableau, Qlik, Metabase, etc. Some overlap with Business Analysis skill set. 2. Database dev for the data mart/warehouse systems (Data Architect), were necessary data are integrated and organised in a way that is useful to the Analyst. The dev part is in building the ETL processes, automating and testing it. Can use tools like Business Objects, Mirth, etc. or treat it like a regular app and use Python, Java, etc. 3. Database dev for transactional systems, who work with the dev team to build a system that supports the application requirements. Don't see many of these, which is a shame since app devs can come up with data models or access patterns that are hard to work with. If you're looking at moving from 1 to 2, skills focus on SQL, database systems (Postgres, MSSQL, etc) warehousing patterns (Inmon, Kimball, etc), and general programming (mostly working with APIs to move data around, testing). So depending on your experience it could be complete career change, or a step in a different direction.
Makes sense- I'd already considered the potential salary cut, but maybe I should just look for jobs at another company to see if that environment would be enough to keep me interested in the analyst work.
It does, among its many other features. It's well worth the money if you use sql on a daily basis.
Apex Refactor!
That is a good idea. Especially in smaller or younger organizations it seems like the analysis will have the opportunity to work with the lower level data and tools supporting the analysis work. 
IMO I'd really suggest you go down the path of SSRS or SSIS. SSRS (SQL Server Reporting Service) is designed to do what you're doing. If you have a current SQL Server license, I'm told it's free. SSIS (Sql Server Integration Service) is an ETL tool that can spit out .csv files or if excel is installed on the server make .xls(x) files. This would be ran manually or as part of a server job (super reliable). Both those are very powerful tools that are in high demand. Once you master them, you will have very marketable skills. Faster way would be to use powershell. I do this at my current role. Not super happy about it as this is not an efficient way to do reporting. /r/powershell is fairly active and this is likely the quickest way to get your scripts to spit out excel files reliably. You'd have to run these manually or try to get Windows Scheduler to run them.
Not sure i get what you mean, @lukaseder. Do you support the standardization process? It's driven alot of recent improvements (eg temporal extensions). I think it's key to why SQL has survived so long. I went from years of SQLServer jobs to the current one that uses #Redshift and had very little trouble. Try doing that switching from Rails to Django. 
Don't they generally try and support the standard?
Yeah like window functions on steroids
Do you get online every 2 months? :) What I meant is that some SQL features are really crazy, yet that doesn't make them less useful. Pattern matching over a subset of rows in a table expression is both "crazy" and useful. This is just me saying that there are more every-day-useful features in the standard than this. But yeah. Why not standardise this, too.
Not an issue as long as the inserts are not dependent on data already in the table (for example, a LEFT JOIN back to the table to avoid duplicates). This may cause record locking issues. The entire table is never locked during inserts unless you use the table lock query hint. Most OLTP databases require multiple inserts into tables at the same time, at a high volume. This is something SQL Server supports quite well.
You'll only get ~~bad~~unexpected/incomplete data if you've got a dependency within the table itself and things run out of the expected ordering. However, you may see performance degradation due to contention for disk I/O - each of those insert queries will be updating indexes too, and you may get page splits depending on the data distribution and your clustered index. How much data are we talking about here?
You might misunderstand how the SQL standard works: there is a rather small set of features (called Core SQL) that is mandatory. Everything beyond that are optional features. Vendors are free to choose which they implement (if any). Generally, PostgreSQL aims to implement standard features if they make sense to them, there is need for them and there is somebody willing to implement it. However, just because there is a new optional feature in the standard, doesn't mean that it will be automatically implemented. But if you volunteer... ;)
To my knowledge, what you described would create a conflict which would result in whichever query arrives first getting a table level lock until it finishes its transaction. That would just make the queries run in series, but not cause bad data. If you have such dependencies, you might not get the same result every time depending on the order the queries run in, but that's not the same as getting corrupt data. Does MS SQL specifically handle this situation differently?
&gt;If you have such dependencies, you might not get the same result every time depending on the order the queries run in, but that's not the same as getting corrupt data. This is more what I was intending; I should have said "unexpected" instead of "bad."
Are you looking for TRUNC(MSGDATE, MM) by chance? TRUNC with MM would convert any date to the first day of that month. 2017-05-05 would be 2017-05-01 2017-11-30 would be 2017-11-01 Etc. https://www.techonthenet.com/oracle/functions/trunc_date.php
[removed]
Peer-to-peer has 1 writeable node and uses the transaction sequence number for what to update. https://docs.microsoft.com/en-us/sql/relational-databases/replication/transactional/peer-to-peer-transactional-replication https://docs.microsoft.com/en-us/dotnet/framework/data/adonet/sql/snapshot-isolation-in-sql-server
Log reader, it scans the tlog for undistributed commands, then it's put into the distribution db on the distributor. The ms repl commands table. It's all spelled out in ms documentation. 
you can have a look here for a list of ER tools http://www.databaseanswers.org/modelling_tools.htm not sure how updated the list is but as far as I see it has most of the tools in the market. From there you can check their respective websites. Not sure how you/your company is going to use the data after loading on the staging area but I would say don't try to diverse much from the source system while loading on staging.
quick update, i've seen that the GROUPING expression returns as tinyint, so i've matched both sides of the CASE statement with tinyint i.e. CASE WHEN GROUPING(&lt;table&gt;.&lt;column&gt;) = CAST(1 AS tinyint) but it is still remapping the output to bit: CASE WHEN CONVERT_IMPLICIT(tinyint,remapbits([Grp1077],(0)),0)=(1) THEN NULL ELSE [Expr1073] END 
What you are doing is basically what I'd do. For August 2016: MSG_DATE &gt;= TO_DATE ('2016-08-01 00:00:00', 'YYYY-MM-DD HH24:MI:SS') AND MSG_DATE &lt; TO_DATE ('2016-09-01 00:00:00', 'YYYY-MM-DD HH24:MI:SS') Using BETWEEN would include midnight of the next month, so you want to avoid that. You could use add_months if you want Oracle to figure out the next month for you, for example December. add_months(TO_DATE('2016-12-01 00:00:00', 'YYYY-MM-DD HH24:MI:SS'), 1) Would return January 1 2017, for use in the second comparison. Edit: forgot to_date in add_months function
thanks guys, ill need to check that out.
Does your function return a tinyint? Is the implicit conversion happening in the function?
It's not really clear what the problem is (at least to someone like me who never used ASPRunner.NET). How are you determining the LogID to insert into the archive to begin with? You should be able to store that in a variable and pass it as a parameter to your insert statement. Since it's in a variable, you can also use the LogID in a delete. As for the "format" specifically, you just need a single question mark for a parameter. Otherwise the format of the SQL statement seems ok. 
There is not enough information, what is the database you are using? Why don't you solve this issue with a after delete trigger on the main table? 
@Cal1gula The LogID is automatically created every time someone creates a new log entry. Basically someone goes into the app, they add a new log to the "main" table by entering all the required information (name, dept, phoneNumber, etc). The LogID is assigned to the entry. Yeah that's where I'm having trouble, how do I grab the logID for the entry I'm trying to delete? @manojk92 I'm not entirely sure, this app was started by someone else and they reassigned to me after she left. I have no previous experience using SQL or ASP runner. I've read something similar to what you mentioned and I thought about doing something like, string strDelete = INSERT INTO dbo.archive( x, y, z) SELECT x, y z FROM deleted Unfortunately, this did not work. 
Quick reply, sorry for brevity. You are close to devops, but not quite. I'd learn more about devops, servers, Deployment, Linux, Jenkins, build pipelines, build scripting, environment setup. Or go the backend developer route and learn more about writing service layer code on top of your DB, basically build APIs to access the code you are already writing. So learn about Java, .net, nginx, tomcat, data access layer, http response codes, web servers, web server config, unit testing, automated testing frameworks. I'm forgetting a bunch, but these two routes may get you started in the right direction. 
1. I would go with BI Developer. Even though your experience is a little bit light, it will get you plenty of looks from recruiters. 2. Be aware with the bullet points of new versions, but there's no need to immerse in one version vs another. They're almost entirely the same. 3. Learn about data warehousing and dimensional modeling. Be able to use Fact, Dimension, and Star Schema intelligently. SSIS and/or Tableau/Business Objects/Power BI/some slick visualization tool are next steps.
I think i finally managed to come up with a solution. string strDelete = "INSERT INTO dbo.archive( x, y, z) SELECT x, y z FROM dbo.main WHERE logID = " + keys["logID"].ToString(); Thank you!
 &gt; Aside from reading the Microsoft books and practice, is there anything that can improve my marketability? Learn ASP.Net and either C# or VB to become "FULL STACK". TBH once you are in the door you'll probably fall on one side or the other of the stack but HR is obsessed with this buzzword.
"deleted" is a special table that only works as part of a trigger so this would be done at the database level, not in the .NET code. It contains the set of records which were deleted as part of a DELETE statement. How do you know which LogID to delete? That's kind of a rhetorical question isn't it? If you have a user deleting a log entry then you must already have the ID available or else how would you delete it? What part of your code comes before this that returns the LogID? Just keep in mind that we're talking about two different methods here. The other method, which you have started, is to: INSERT INTO dbo.archive(LOGID, col1, col2...); DELETE FROM dbo.maintable WHERE LOGID = ? Using the trigger method it would be: `INSERT INTO dbo.archive SELECT * FROM deleted` (or something close to that) as part of your trigger. Once the trigger is configured on the table then you simply run the `DELETE FROM dbo.maintable WHERE LOGID = ?` from the app and the trigger handles the insert. Note that in both cases you need to have the ID available before running the query or else how will you know which record to delete and/or insert?
SQL server won't initiate a table lock for a general insert statement. When using the BULK INSERT command it can be advantages for locking the table, but usually bulk inserts imply massive data loads where you don't want resource contention to be an issue. Smaller inserts like you'd find in a production transaction system should never force lock a table.
1) Would highly recommend using parameters instead of a concatenation, especially if this is a public app you are open to receiving a visit from little [bobby tables](https://xkcd.com/327/) here https://blogs.msdn.microsoft.com/tom/2008/05/29/sql-injection-and-how-to-avoid-it/ 2) There's no delete in your query :)
Yes good point about not focusing on the SQL Server versions. The newer versions have a few different features. Nothing you can't live without or learn quickly though. I've never had an interview where anyone was like "Oh we're on 2016 and you only have used 2014 so I guess we can't hire you". They are like 99.9% the same thing. MS likes backwards compatibility so there are hardly any deprecated features in SQL Server.
Great opportunity to learn a whole host of tools there. I'd be a little concerned about that - seems like you'll have a lot of disparate reporting tools. I'm not mad keen on that. Still, if organised well, it's a great opportunity. Is Epic the electronic patient record software? As for questions, I'd imagine on a technical level, they'll want you to fully understand: Indexes, both clustered and non clustered. Sub queries and derived tables. Windowed functions. CTEs and recursive CTEs. Stored procedures. Functions, and the difference between each type of function (i.e. Scalar, Table-Valued). Data types, and their uses (i.e. Float vs Int). Beyond this, they may ask about ETL - how would you implement an incremental loading system, how would you build a solution, how do you pull data from multiple sources etc... (note: I'm not a massive ETL expert at all!). They'll probably have some specific questions about reporting as well - I could give you some examples for SSRS, but given the number of other products, I'd say a broad understanding of what each does and how it works would be a good place to start. 
https://sqlbolt.com if you like learning by doing
I understand, when I click on an open entry I already "know" the LogID to delete it. Unfortunately, I'm not able to see how I'm grabbing the LogID. I have predefined areas where I can place events, "Edit page: Before Process," "Before records updated," "After record updated," etc. The delete portion was done by the person working on the project previously. The code, dynamic tablea = globalVars.dal.Table(dbo. mail") tablea.Param["logID"] = keys["logID"]; tablea.Delete();
Interesting. Thanks for the information!
SSRS 2016 is functionally the same with paginated reports (chart visuals changed/added) the web portal received a huge face lift and added features like KPIs, Mobile reports (formerly datazen) and optional powerbi report server. Overall it looks and feels very slick. I agree with others that SQL version doesn't really matter the code is relatively the same I would just keep up with new features. You would be either a bi developer or reporting analyst.
Ah, that makes much more sense. Your post said you were having trouble creating a delete button so I think we all assumed that you needed the delete statement as well.
 #1 - If I were hiring you, you'd be a junior SQL/BI developer. #3 - Research Data Warehousing and Data Modelling. Most companies need it, but most companies rarely practice it. Fact Tables, Dimensionalizing data, Star Schemas, Third normal form, Slowly Changing dimensions and all that. Tabular Cubes and DAX are the new hotness. If you are doing any sort of aggregation reporting (counts, averages, YoY metrics, formulas, KPI's, etc.), tabular cubes will be a gift from heaven to your power users. It is so fast and compact that you will be amazed.
I basically have the same role and have been doing it for the same length of time. If you're not in an official Reporting &amp; Analytics group within your company then its unlikely you'll have a title that accurately identifies your role, which can make knowing what route to take for growth and advancement in that capacity somewhat difficult. I agree with what jcforhockies suggested about BI Developer being the role you currently fit into. So if you're looking for what to pursue next then I'd suggest BI Administrator and eventually BI Architect. Keep with mastering as much MSSQL technology as possible but also try to become profficient with any scripting or dev skills that are commonly used with ETL for data warehousing. Here's a link for career stats around BI Developer roles. http://www.payscale.com/research/US/Job=Business_Intelligence_(BI)_Developer/Salary
Awesome! Those are a lot of great things for me to research and make sure I'm familiar with. Yes, I believe Epic is the patient record system. I think it's pretty standard and most hospitals use it, as far as I know. Thank you so much for the write up. I'll be sure to thank you if I manage to get the job!! 
There's also SQLzoo.net
Holy smokes they use a ton of reporting products. Reading through the job description it seems as though SQL is an afterthought for the job description and more than likely they have a few jack of all trades master of none people working in the department. BOXI (business objects) and WEBI are functionally similar to SSRS but overall they are a pain to work with and they are primarily visual drag and drop. They are best used to build flat files which can be scheduled and dropped on an ftp but they can and do function as an SSRS like report that users can run adhoc. I have heard that epic is also painful to build reporting in the backend. Qlikview seems to be popular at hospitals. The only thing I know about it is that it is primarily used for high level dashboards probably being consumed by senior leaders (directors, VPs and executives). I agree with what the other poster recommended for SQL server. Since this is an IT team it is possible they could bring in the DBA for your interview. 
Yikes! I would prepare: * "The strengths and limitations of each toolset" in a one sentence answer per each including multi-dimensional databases (cubes). * Be able to use Data Warehousing concepts such as Fact, Dimension, and Star Schema intelligently. * Some visualization concepts such as when to use line/bar graphs, pie charts, tree maps, etc. * For ETL, know about data flows, transformations, and lookups are and do. * For SQL, maybe some performance related questions about indexes, and know that you look at execution plans to figure out how to optimize queries. Try to focus on the conceptual and hope that's what they're looking for. I don't like your chances if they're looking for technical knowledge in all that alphabet soup.
For SQL developer/query writer I usually ask the following * if you have a slow performing query, what steps do you take to speed up the query [Answer](#s "step one is to look for nested loops or queries that try to interpret the data on a record by record basis instead of as a set, step 2 is to look at profiler while the query is running to determine if there are other queries or processes holding it up, third is to look at what fields are being joined and if there are indexes or if the indexes need to be updated.") * what is normalization, which is faster and why [Answer](#s "normalization is the attempt at preventing data duplication. assume you have a table that holds employee names and positions. instead of having duplicate positions, you could have a title table and reference the table via an id. That way when systems administrator is renamed Windows Admin, all the employees who have that titleid also get updated. a completely flat table is 1st form normalization, no joins, it's the fastest. 5th form normalization is no duplicates in the data at all and invalid ids are not possible. 5NF as its called gives you adaptability at the cost of joins and speed.") * what is a must have sql add-on [Answer](#s "redgate toolkit, specially sql prompt. the free option of apex complete and apex refactor are also options") * talk about different types of joins [Answer](#s you have two tables A and B. If you want records that are in both A and B, you would use an inner join. If you wanted records from A regardless if they were in B, that would be a left join. The opposite of a left join is right join. full outer join is another join that returns results from A and B regardless if there was a match") * what is the different between delete and truncate [Answer](#s "delete is logged in the transaction log, truncate is not. truncate is faster") more advanced questions (i'd save these if someone said they were a 9 or 10 out of 10 on their sql skill) * difference between outer apply and cross apply [Answer](#s "say you have two tables and you want to top 2 results from table B based off a value in table A. you could use temp tables or better yet CTEs or you can use cross apply when you have a one to many relationship, however you want to limit the number of records returned from table B. cross apply is much faster than using a CTE to perform the same query. cross apply only returns results from table A that match to table B. outer apply will return records from table A regardless if there is a match in table b.") * difference between clustered and non-clustered indexes [Answer](#s "clustered index alters the way records are stored. it slows the writes to the table but the advantage is a faster lookups when doing certain types of where clauses. a non-clustered index is more like an index to a book. it doesnt alter the records but rather keeps a separate object to make finding those records faster. ") * what are temporal tables [Answer](#s "this is a new feature with sql 2016 that allows data to be stored at any point in time rather than only at the current moment. an example is having a table where you have employees and job titles. say on 1/1/2017 you add someone as SysAdmin and on 6/1/2017 they change to NetworkEngineer. The temporal table would allow a query to be written to find what the value was of the job title based on a specific date. temporal tables can be thought of as a history table of historical values. ") * how do you deal with concurrency [Answer](#s "there are some queries that need everyone to get out of the table while it performs an action. Often you will see a table lock where no one can access the table until you're done with your query. this is very intensive and a alternative is setting the transaction isolation level to serializable. it prevents any reads of records included in the query. if you need to allow people to continue to read the values while you perform a query, then you can allow snap_shot to let them read the last committed value")
[removed]
Thank you for the help! I will study up and hope for the best on Thursday!
It is a lower level position, so I think they're okay with me not being an expert in everything listed there. It'll be a learning process. But thanks for the ideas to study for and I'll hope for the best on Thursday!
Thank you for working through it and giving some advice. It is a fairly entry level position, so I don't think they're expecting me to be an expert in everything. I will have to learn a lot of stuff in the position and maybe be able to give some advice to phase out some less-than-useful technologies and replace them with better alternatives.
Let me tell you a (not very) secret. Companies write a Developer II/III job description and label it Developer I so they can pay a lower salary. That's not a Developer I job description. "Acts as the internal company expert..." Yeah. Not even close. Good luck.
1 - depends on what you want your next job to be: * If you want more of the same, then BI Analyst, as others have suggested, sounds like a good fit. * If you want to focus on architecting database structure and objects, Data Modeler is a good title. * If you want to get into application development, pick up a backend language (C#, Java, Python, etc.) and call yourself a backend developer. * If you want to support applications and infrastructure, pick up some experience in deployment tools and methodologies, then call yourself an Operations Engineer (also referred to as a "DevOps" Engineer). * If you want to be a utility data worker, I like [Data Engineer](https://medium.freecodecamp.org/the-rise-of-the-data-engineer-91be18f1e603). 2 - Only read that material if you will be using those technologies at your current job, or you have been hired by another company that uses those technologies. Unless you want to hyper-specialize, it probably makes sense to broaden you knowledge than it does to go deeper into what you already know. 3 - a few things: * Learn another language (Python is very approachable and is great for working with data). * Get involved in open source. It's a lot easier to get hired when people can see a portfolio of your work and measure your work product. Learning another language will help with that. * See if there is a [Code for America](http://brigade.codeforamerica.org/brigade/) brigade near you. It'll allow you to have a public work product and you can support your community. In my experience they tend to focus on data heavy applications, which is a sweet spot for SQL developers. * If you are solely interested in marketability, then the best thing you can do is network. Personally knowing people looking for workers will make you much more attractive than anything else. Look for tech [Meetups](https://www.meetup.com/) in your area. See if there's a tech slack for your community. Start going to conferences. You might even get work to pay for some of those. Hope that helps, and best of luck! *edit - added subtly snarky comment about the use of the term "DevOps" 
Let me know if you want me to answer those questions or if you want to try and I can give you feedback. 
Well, the only reason I'm not too scared is in most of the job descriptions I've applied to in the past it'd say: "Education - Required: Bachelor's. Preferred: Masters". In this one, it starts off with a required Associates and preferred Bachelor's. And for the minimum experience it says "2 years' experience in relevant area. Bachelor's degree considered in lieu of 2 years' experience." So I do think it's meant for a more entry-level position. And having been out of work for 8 months already I'm kind of just ready to take what I can get and make due with lower pay. I think when I was talking to the recruiter I mentioned a range or 48.5k-52.5k. When I had my San Francisco job I was only making 50k and being in North Carolina with such a low cost of living means that even on the lower end of my pay scale that I mentioned I'll probably be fine. You're definitely right, though, in that it's a lot to ask for in a Developer I position, but I'm up to the challenge and I want to expand my skillset and get started on my career again.
Is this for SSRS? Run a query to determine how often the report is being viewed if it's not often change it to the appropriate schedule off hours would be good a dedicated reporting server would be better. It sounds like this is a production database be careful not to block the prod tables. If it's not check for missing indexes and review query plans to see if you can optimize. Also if the table has a create/update time stamp you could forgo the truncate and just insert new/updated records.
It's not my intention to discourage you. My first thought was it might be a big team, and they dump everything in the job descriptions to satisfy with multiple hires. But no; there's only one IT opening listed. I re-read it more carefully, and the closer I read more shady it sounded. That being said, rag tag IT shops can be fruitful with the right temperament. There's opportunity to carve out advanced responsibility, and particularly with BI there can be significant exposure to executives. Not for advancement (you don't really want to advance at those companies), but seeing first hand what plays into executive decision making is a subtle and rare skill in BI. Also, becoming comfortable and confident in meetings is a quick way to get ahead (at the next company with better prospects). I wonder if the graduate degree is a West Coast thing. I can't recall a single BI job wanting a Masters. Pretty standard is Bachelors or equivalent experience.
Ooh, just the kind of stuff I was looking for. Awesome! Thanks.
I will be interviewing with the whole IT team when I go, so I will get a better understanding of what kind of role I'd fit into. I know the person who was previously in the position I'm interviewing for was promoted to a higher position in the company which does bode well for me I think. And great advice for being comfortable in meetings. I like to think that I'm comfortable talking with higher ups and learning from them, but it will be something that I have to pay attention to and stay on top of when I do get a job. About the degree thing - I've been applying to places all over the country. I have no real ties to any particular area so I'm open to moving, but I've come across it in maybe 65% of the companies I've been applying to where they'd prefer a Masters. Who knows, they may just put that in there to be hopeful that someone with more experience comes along. I've had a few interviews since I got laid-off and every time I've gotten some feedback it was: "We loved meeting with him, he interviewed well and had a great personality. We're just looking for someone with more technical experience." I just need a foot in the door somewhere to get the 2+ years of experience most places are looking for. If this job turns out to be that, I'll take it. It could also turn into a place that I stay for a longer period as well. The hospital gets pretty darn good reviews on Glassdoor (3.4/5) so I'm hoping it won't be too bad to work there. And thanks for talking things through with me. I appreciate the time. I am not discouraged at all; it's important to learn what to keep an eye out for and spot any potential red flags.
Thanks! I'll get in touch with any questions I have.
Doing personal projects is a way to build up technical experience during down time. Don't be shy to put relevant non-work related material on a resume. Projects are projects. All you need is for an interviewer to ask you about it, and you have the floor to explain various technical hurdles you overcame.
Assuming you're doing this in a stored proc, you could select the data from the slow view into a temp table first, then truncate, then read from the temp table to repopulate. It might make the whole process take a bit longer, but the moment where your rendered table is empty should be less than 15 seconds this way.
Do exercises. Check that http://www.studybyyourself.com/seminar/sql/course/?lang=eng. Free, for beginners, well structured, keep things simple, with online exercises.
This is a way to swap out data in &lt; 1 second with no chance of a black table being queried. It will still have to wait on reports already running, and block new reports until existing queries clear. TRUNCATE TABLE Staging_Table; INSERT INTO Staging_Table SELECT * FROM View; BEGIN TRANSACTION; ALTER TABLE Reporting_Table SWITCH TO Empty_Table; ALTER TABLE Staging_Table SWITCH TO Reporting_Table; COMMIT TRANSACTION; TRUCATE TABLE Empty_Table;
I think that would help. I'm using a sp so it's not difficult script. I just don't want to do a bad practice after a bad practice if it should never have been executed in this way to begin with.
Thanks. I am not familiar with the alter table command. Why not also use truncate and insert on the reporting table?
The point is that this code populates a staging table and not the table that is queried (think of it as being SP_rename and it becomes easier to understand). Once the staging table is ready, you just swap the tables. The fact this is built into a transaction means that: -The transaction, doing only a table rename will only maintain a lock a fraction of a second. -The data refresh will not impact users as they are not hitting the table being refreshed. -Using a transaction will lock out users trying to access the table while the rename is underway, introducing a minor delay that will be barely noticeable* *delay will only be noticeable only if you have a long reporting query (select) running on the reporting table, while the transaction is waiting to acquire the lock to perform the rename. During that time, new "select" statement will be locked out, until the rename has been done. 
The time inserting into the staging table doesn't impact queries. The `ALTER TABLE SWITCH` is a meta data operation (an operation on definitions not data, like `TRUNCATE`), so it takes a minimal amount of time other than blocking. This potentially saves you 15 seconds of availability compared to inserting into the reporting table.
Can you clarify what you mean by you can only perform one aggregate per query? You can process sum, min, max, etc multiple times in the same query even using different columns. IE: sum(cost), avg(price). Also, is there any particular reason you made a derived table for the basic aggregate and did the ranking separately? If you did it all in one, you could reduce the number of derived table if needing to filter on one of the OLAP functions.
this would probably be better in /r/SQLServer /r/SQL is about the language, not the Microsoft product
Thanks, always appreciate opportunities to brush up on SQL
Creating cubes is best way to update the reports each time you run. Instead of creating tables for report and truncating these table, query optimization and use of Indexes is better way to achieve periodic updates.
Try this from a cmd prompt `SQLCMD -L`
&gt; SQLCMD -L It just sits there. 
I'm guessing you don't have a login for that instance, if you don't have an SA account and password you can try instead of your own you're going to have to follow this: https://docs.microsoft.com/en-us/sql/database-engine/configure-windows/connect-to-sql-server-when-system-administrators-are-locked-out
Here is the error from the log: 2017-08-15 12:41:19.87 Logon Login failed for user '[local machine name]\Administrator'. Reason: Token-based server access validation failed with an infrastructure error. Check for previous errors. [CLIENT: &lt;local machine&gt;] There are no earlier errors that are relevant. 
That is a great guess, I don't. I will try the link.
You da man! In a nutshell for any that may follow: You need to get into SQL config manager and stop all of the services. add an ;-m to the end of your connection string in the start up paramaters (in my case, in later versions you can just add -m) Then you restart the service and log in via management console as an admin. 
Id love it if you posted the answers
BI is exploding in a lot of large companies beyond your basic DBA stuff. You're not looking after the core product, you're looking at business strategic data. As you're already a DBA having both sides of the coin can only be a good thing.
Experience wise, it sounds like you have a solid background. Yes companies will look at the certifications but don't put as much weight on it as experience. It will certainly help you considering you are lacking a formal degree. But at the current moment, it sounds like your resume should be enough to get you a Business Intelligence position seeing as you pretty much have done that work for awhile already. What does your resume look like? Where are you located? 
You could also use the awesome [dbatools cmdlet `Reset-DbaAdmin`](https://dbatools.io/functions/other/reset-dbaadmin/). Much less mess.
Thanks for this info. I will look into the items you mentioned and will plan on going down the BI Dev path.
&gt; Even though your experience is a little bit light Why would you consider my experience light? Is it the number of years or fact that I've focused more on in SSRS? I will keep the other items in mind. I am making a list of items to research and focus on to learn and improve my skill set.
I had to look up what Full Stack is. I guess I've started down that path. I've fabled in C# and was forced to learn how to program in SAS. What would you consider most useful ASP.Net or C#?
Thanks, I will make sure to keep up with the changes.
I'm in the Reporting &amp; Analytics group, but we are classified as programmer analysts. It's nice to know what I should be classifying myself as. I now there's a move towards data warehousing in my company, so I'll try to position myself in one of those teams to start learning that. Thanks for this info.
Wow, this is a lot of great information, thanks! I'll be looking into the different roles that you've identified to see which path I will want to follow. Data Modeler sounds more pleasing at the moment. I was able to find a couple of Code for America projects in my town, so that'll be good to check out. Thanks again!
Thanks!
I'd consider a solid BI Developer to have more SSIS and at least one other reporting tool. Pure SSRS is a little light.
I don't think that you can divorce the two in Visual Studio. That is every C# page opens a page designed to build the corresponding ~~(CSS and HTML)~~ ASP. See: https://en.wikipedia.org/wiki/ASP.NET#Code-behind_model It's not like the classic c.sh where your input is a textfile somewhere. This is designed from top to bottom to present its results on the web. You can write classes or whatnot in C# and not use the ASP side of things but the code is intrinsically connected to being presented somewhere in a nice format. I would start with something you know like SQL and learn to make calls to it from C#: This looks okay: https://www.youtube.com/watch?v=xUU9MdnohrI 
I think the biggest gaps in my resume are the lack of data warehouse and analysis experience. Nobody seems to be looking for someone who's just good at writing queries and reports, especially if it was outside the context of an OLAP environment. I'm in the northeast now but hoping to relocate to somewhere in Florida, or any warm place near an ocean honestly...
What I would do in your shoes is start a company on paper and list it on your resume in the capacity that you are a consultant. Your next step is to go out and fiend clients. And by clients I mean people who will let you work for free. Have a friend who does bookkeeping part time? Build some shit for them. Know someone who owns their own business? Build some shit for free. Meanwhile you start to assemble a portfolio &amp; references. Keep studying and maybe look into a certification (not essential, but helpful.) After a year or two you should have a nice solid amount of experience you can leverage. And in the meantime there is no harm in interviewing and trying to sell someone that you're a good candidate for a starting position as an analyst.
I meant fiendish clients who don't mind exploiting your free labor. Clearly not find.
Or that. Now, I have to migrate over all of my DBs that I thought were using 2008 to 2008 from 2005. Any suggestions? 
More dbatools goodness. If you want to migrate **everything** (logins, jobs, databases, alerts, dbmail config, etc.), check out `Start-SqlMigration`. If you just need to migrate the databases, `Copy-DbaDatabase`. Seriously, you **need** this Powershell module in your life as a DBA. I mean, yeah you *could* do a backup/restore manually of all your DBs, or detach/reattach, but when you can do it with a PowerShell one-liner, why do anything else? :) ^(yes, I know there are cases where it won't work out, but try this way first)
added the answers as spoilers to the original comment
Look for jobs with the Department of Education, school districts, etc. These kinds of organizations appreciate your educational background. Consider downloading MySQL or whatever free version you can get of Microsoft SQL Server. We all started with Access but it's considered a bit of a scourge to many database professionals, as it's just enough of a database to cause information silos in organizations. Put yourself out on LinkedIn with "SQL" in your skillset. Consider looking for Business Analyst positions that can lead to database work, these are often posted as "Technical B.A." positions. Above all, show drive, desire to learn, and a pleasant personality. Given what you posted, I'd hire you as a junior database analyst. I can teach SQL, I can't teach diligence, desire to learn, and team.spirit.
Thank you so much! Are you hiring? Lol. I will be looking into this ASAP because db work is just so much fun! I thought music theory was fun, but so many people just don't understand music. Db numbers are something anyone can understand as long as it is explained properly, if that makes sense. I love it!
So do you think the MIS degree is a waste?
Which SQL dialect are you using?
No, it's extremely in demand. I just think companies are being picky wanting people with 3-5 years of experience for an entry level job. 
Yea its stupid, I see some ad postings where they want a full stack with like 5 languages a bachelors but masters preferred and 5 years of experience and they call it entry level and pay 17 bucks an hour DOE. I am in my last 2 semesters, I did a Bank of America Internship and I work as an SQL dev fora different company, so hopefully I can branch into a good position without to much trouble. Like my job now, I make $10 an hour and they hired me as an 'intern' and told me its at my own pace no worries. Little did I know I would be the only developer, no one in the building knows how to write anything, I do all the coding and they start coming down if I have a small issue and the code takes longer than it should. 
I'm actually trying to do this through Excel, which I believe uses it's own dialect. MSSQL?
Excel isn't really SQL at all. You'd be much better suited to /r/excel. In mssql I'd use `CROSS APPLY`, but you can't really do that in excel, I don't think.
I came to answer your question but /u/opportunity_mocks stated it very well. Over the years I've seen a couple of BA's make the leap to database work. 
Most healthcare it/analysts positions are sql heavy
&gt; free version you can get of Microsoft SQL Server. For those wondering, MS Developer Edition is free to download and is the complete Enterprise Edition. A change MS made in the middle of last year.
You probably don't realize this, but your post is honestly a little offending. "Wild west db", really? Imagine how would you feel if someone posted a question about just having barely learned how to play piano, what are their career options in teaching music. I mean, sure it's great you have enthusiasm, but first step towards an sql career might be to actually realize that it's a serious field, that takes years of experience to master. Playing with access is nice, but nowhere near what it takes to be a database professional IMHO. Switch to some actual database server like postgresql and continue with learning. 
I just started my first BI role a few months ago, similar position to you in that I worked as a DBA for the last 6 years. I don't have any Microsoft qualifications, only my experience. I used to create SSRS reports and SSIS packages in my previous role which gave me a good grounding. What I did though was take some data sets home and started building SSAS cubes and learning dax/mdx in my spare time. I have also started learning R. Then there was a bit of embellishment on my CV to say this was part of my current role. You need to be careful with this though as when it comes to interview you need to make sure you have the knowledge and real world examples to answer any questions thrown at you. The company is now sending me to get the qualifications anyway. Although they care about the qualifications I won over someone with all the Microsoft stuff because I was able to put real world examples which the other guy wasn't, he just had the theory. I'm not saying they are bad, it's just the training for these qualifications is teaching to pass an exam. If you want a BI role you need to apply the technical knowledge to real world examples as well. Also on another note make sure when you say SSRS you are building them in Visual Studio not Report Builder as most larger organisations will tend to use Visual Studio for building reports.
Y'all rock!
I was noticing that as I was job searching last night. Thank you for the pointer!
I did not mean to offend. It was my husband's phrase actually and what he meant by it is that there is no set career path like there is in Music (practice music, pick an instrument, go to college, get a teaching certificate, get a job). There seem to several ways to come at this and that is why I am here. I am not here to dabble and mess about, I am here for pointers to help me along the path. My husband is a programmer (I do not wish to discuss what he does for privacy reasons), but he knows SQL out of necessity as a language, not a career field other than the lack of a set path, as with most computer fields. Also, "wild west" is not really an insult, the wild west is, historically speaking, where a lot of innovation and invention took place.
Use a simple update command while using the REPLACE function. Then I would put in a constaint to not allow row inserts where that column has spaces as a part of the parameter.
I realize it was not intentional, and I am not taking "wild west" as insult per se, it's just a nice summary of the overal feeling of your post... it all sounds very "carefree" (sorry for lack of better word, English is not my first language). The way you have worded it and based on your response, most likely the way you see it, is that literally anyone can come and "do sql" without too much of a formal training. While this might be true for a very small subset of very talented individuals (and I am not implying you might not be one of them), generally speaking most professionals still go the "career path" (start messing with computers as kids, start programming, get a CS or SW engineering degree, get to know the specific technology you will be working with, get certified for that technology, get a job). Tools like Access make it easy to believe that databases are easy. With all due respect, that's not true. I don't mean to discourage you at all - good luck in trying to get into the field. But for me personally it sounds like you are underestimating the complexity of the specialization and while your attitude might seem positive to some people, it might be off-putting for others (and therefore it might cause you problems during the search for appropriate position - that was the point that I tried to make, albeit probably not too clearly :)) Good luck!
Can you elaborate on &gt;assemble a portfolio Im currently an entry level analyst and do help moms and pops shops in spare time. How do i prove this? Do i host a piece of work online for everyone to view? I never had a portfolio.
I am just a bubbly person in general. It is who I am in life and in career. But don't make the mistake that bubbly means unintelligent, I am extremely smart and a self starter. Every recommendation from every professor and employer I ever had has said the same basic thing: "is not dissuaded from any endgame, no matter how hard the challenge." I have written research papers on topics that most would balk at the mere idea of and performed musical works that many cringe at the idea of even beginning. I am nothing if not tenacious and statements like yours simply drive me to prove you wrong.
"Google has showed how to do the query". Did you test it? What have you tested? This is some pretty simple SQL if you take like 30 seconds you should be able to figure it out. It probably took you longer to make a reddit post than if you tested on your own... edit: Downvote away but seriously it was the first result on google https://stackoverflow.com/questions/10432086/remove-all-spaces-from-a-string-in-sql-server
Well, it might be just a cultural difference, I guess. I am not living in a culture where writing about oneself "I am extremely smart and a self starter" would be seen positively under any circumstances (and again, I am not saying it's not true. It just doesn't sound right to me personally). Therefore my understanding of your post might be entirely wrong and out of cultural context. C'est la vie...
I think so. Here, we lay it all out boldly and telling someone to basically "take a chill pill" is not cool. In a case like this, where someone is laying all their cards on the table and asking for advice, then being shut down by someone is really not ok. It's just not civil.
I don't think it will help you using Excel, but I did a comparison of a few methods using MSSQL. CROSS APPLY SUBSTRING, and XML values work on the assumption that there will be 3 or less values in the CSV field. Method|Rows|Checksum|Duration :-|-:|-:|-: CLR function|1000000|-1915958208|00:00:02.412 CROSS APPLY SUBSTRING|1000000|-1915958208|00:00:05.601 XML values|1000000|-1915958208|00:00:13.608 XML nodes|1000000|-1915958208|00:00:18.609 DECLARE @starttime datetime2(3) = SYSDATETIME() , @zerotime time(3) = '00:00:00.000' , @rowcount int , @checksum int; --CLR function WITH cteResult AS (SELECT CONVERT(varchar(10),ss.String) AS Value FROM #csv AS c CROSS APPLY CLR.SplitString(c.Csv,',') AS ss) SELECT @rowcount = COUNT(*) , @checksum = CHECKSUM_AGG(CHECKSUM(*)) FROM cteResult AS r; SELECT @rowcount AS [rowcount] , @checksum AS [checksum] , DATEADD(MILLISECOND,DATEDIFF(MILLISECOND,@starttime,SYSDATETIME()),@zerotime) AS duration; SELECT @starttime = SYSDATETIME(); --CROSS APPLY SUBSTRING WITH cteResult AS (SELECT CONVERT(varchar(10),s.Value) AS Value FROM (SELECT Csv + ',' AS Csv FROM #csv) AS c OUTER APPLY (SELECT SUBSTRING(c.Csv,1,CHARINDEX(',',c.Csv) - 1) AS Value WHERE CHARINDEX(',',c.Csv) &gt; 0 UNION ALL SELECT SUBSTRING(c.Csv,CHARINDEX(',',c.Csv) + 1,-- CHARINDEX(',',c.Csv,CHARINDEX(',',c.Csv) + 1)-- - CHARINDEX(',',c.Csv) - 1) AS Value WHERE CHARINDEX(',',c.Csv) &gt; 0 AND CHARINDEX(',',c.Csv,CHARINDEX(',',c.Csv) + 1) &gt; 0 UNION ALL SELECT SUBSTRING(c.Csv,CHARINDEX(',',c.Csv,CHARINDEX(',',c.Csv) + 1) + 1,-- CHARINDEX(',',c.Csv,CHARINDEX(',',c.Csv,CHARINDEX(',',c.Csv) + 1) + 1)-- - CHARINDEX(',',c.Csv,CHARINDEX(',',c.Csv) + 1) - 1) AS Value WHERE CHARINDEX(',',c.Csv) &gt; 0 AND CHARINDEX(',',c.Csv,CHARINDEX(',',c.Csv) + 1) &gt; 0 AND CHARINDEX(',',c.Csv,CHARINDEX(',',c.Csv,CHARINDEX(',',c.Csv) + 1) + 1) &gt; 0) AS s) SELECT @rowcount = COUNT(*) , @checksum = CHECKSUM_AGG(CHECKSUM(*)) FROM cteResult AS r; SELECT @rowcount AS [rowcount] , @checksum AS [checksum] , DATEADD(MILLISECOND,DATEDIFF(MILLISECOND,@starttime,SYSDATETIME()),@zerotime) AS duration; SELECT @starttime = SYSDATETIME(); --XML values WITH cteResult AS (SELECT Value FROM (SELECT c.Csv.value('(R/V/text())[1]','varchar(10)') AS [1] , c.Csv.value('(R/V/text())[2]','varchar(10)') AS [2] , c.Csv.value('(R/V/text())[3]','varchar(10)') AS [3] FROM (SELECT CONVERT( xml, '&lt;R&gt;&lt;V&gt;' + REPLACE (Csv,',','&lt;/V&gt;&lt;V&gt;') + '&lt;/V&gt;&lt;/R&gt;') AS Csv FROM #csv) AS c) AS x-- UNPIVOT ( Value FOR RowNumber IN ([1],[2],[3]) ) AS u) SELECT @rowcount = COUNT(*) , @checksum = CHECKSUM_AGG(CHECKSUM(*)) FROM cteResult AS r; SELECT @rowcount AS [rowcount] , @checksum AS [checksum] , DATEADD(MILLISECOND,DATEDIFF(MILLISECOND,@starttime,SYSDATETIME()),@zerotime) AS duration; SELECT @starttime = SYSDATETIME(); --XML nodes WITH cteResult AS (SELECT v.v.value('(text())[1]','varchar(10)') AS Value FROM (SELECT CONVERT( xml, '&lt;R&gt;&lt;V&gt;' + REPLACE (Csv,',','&lt;/V&gt;&lt;V&gt;') + '&lt;/V&gt;&lt;/R&gt;') AS Csv FROM #csv) AS c CROSS APPLY c.Csv.nodes('R/V') v (v)) SELECT @rowcount = COUNT(*) , @checksum = CHECKSUM_AGG(CHECKSUM(*)) FROM cteResult AS r; SELECT @rowcount AS [rowcount] , @checksum AS [checksum] , DATEADD(MILLISECOND,DATEDIFF(MILLISECOND,@starttime,SYSDATETIME()),@zerotime) AS duration; SELECT @starttime = SYSDATETIME();
Sounds like a job for a [Materialized View](https://en.wikipedia.org/wiki/Materialized_view) if your RDBMS supports it.
No prob, I've worked in sql intensive jobs the past 11 yrs. mostly with healthcare.
You seriously rock. Is there any way I can send you my resume to make it look more IT/SQL happy and less hippy dippy/music teacher-ish? You obviously know the talking points they want and the boxes they want checked.
The point is you didnt read my post. I dont want to remove it from a query, I want to remove it from the table. What you posted would simply show the numbers correctly in a query however if you look at the database it would still have the spaces.
What you are saying makes no sense. Do you not know about UPDATE? There are multiple examples in that SO thread on how to update. Don't tell me *I* didn't read your post when you are struggling with SELECT vs UPDATE... this is like the most basic SQL concept. edit: Here https://stackoverflow.com/a/16745607/7948962 and here https://stackoverflow.com/a/38461657/7948962 from the same post
Right, but the usage of the REPLACE function in that StackOverflow page is applicable exactly as it is in an update statement. So at this point, all you have to do is write the basic update statement and use that REPLACE function in your set clause of the update statement.
 UPDATE mytable SET mycolumn = REPLACE(mycolumn,' ','') WHERE mycolumn LIKE '% %'; 
I can see where you're both coming from. On one hand, it is nice to see people excited about SQL, on the other hand, I have worked 12 years through tech support, IT, software consulting and finally to software development to get to the position I'm in. I could see where someone would be put off by the implication of "Hey my husband showed me SQL and it's great! Anyone have a job offer?". My response could be "well you could start as a tech support person and work your way up through 12 years of experience to get to a SQL dev"... Again, it's great to see people that like SQL. We like SQL too, or we probably wouldn't be here. However, I am also a realist, and if I saw someone apply with a music degree looking for any kind of position that was not the most entry level of IT positions I would be instantly wary. Just keep that in mind, unless you know someone who will fast track your career, you might have to do something with minimal SQL just to get your foot in the door. We started from the bottom, now we here.
I am fine working my way up, I am not asking to be fast tracked. However, because there is no set path to take, I came here for pointers on where to start looking at the bottom (typing IT jobs into Google returns * craziness and utter stupidity). Also know that if you saw my resume, you would see that I am more than just a musician, I have had to do tons of tech utilization in my career out of necessity and often lead the charge when it came to new tech in the classroom. Please don't write all musicians off as horn-tooting super nerds, our brains aren't that different from yours in that they are very specialized and the average human just doesn't get it.
I completely agree! I just have never seen, and am very curious about, the specifics of an SQL injection attack that is not thwarted by doubling up apostrophes (SQL Server) or casting ints. I have yet to see one.
Sure, date literals for example. You have to understand all your literals. And you would be surprised at the bad behavior I have seen. I have seen SQL queries built in JavaScript and sent to the server for execution. Where's your escaping and casting now? ;-)
The thing I tend to rely on, for people who adopt the "always remember to double up quotes on every parameter and cast all non-quoted values" strategy, is the failings of human imperfection. You only have to forget to do it or screw it up once. In a big enough application, with more than one well-intentioned person working on it, there is bound to be an error. It's far better to adopt a strategy of never, ever, ever, building query strings with user input, and always using parameterized prepared statements. It's a kind of mental hygiene, you keep the (nasty, tainted, dirty) data separated from the (beautiful, and completely not user-controlled) code. 
 SELECT MakeModelID FROM yourtable GROUP BY MakeModelID WHERE COUNT( CASE WHEN Number_of_Doors = 4 THEN 'Not these' END ) = 0
Ah I see. Would SAS count as another reporting tool? I have approximately 2 years experience coding out of SAS EG. I've never thought of adding that to my list as it's not SQL.
Thanks! I skimmed through that video and I'm afraid to admit that it looks fun. I'll be trying this during the weekend after I install SQL Server and SSRS on my personal machine.
They also require experience in a healthcare environment + knowledge of some healthcare-specific software 99% of the time :/ Source: Been looking through tons of SQL job listings recently. Tons of healthcare jobs in my area and I fit the bill for exactly zero of them since I've never worked in healthcare.
SAS is relevant. It's not as common in the industry (due to it's price), but it would give your resume more of an analytics vibe. SSRS/Crystal Reports are too much "how to alternate line colors" and not enough "delivering powerful analytics".
Nice. Okay I am good to go up to this point. I have attached the gui, or maybe you could help provide me what the script would be? Going from database instance *foo* to *bar* [The GUI](http://i.imgur.com/th2qpXR.png) The example script: Start-SqlMigration -Source sqlserver\instance -Destination sqlcluster -DetachAttach -Reattach -SetSourceReadOnly Do I just use *foo* for sqlserver\instance and *bar* for sqlcluster? Is it as simple as that? 
`Start-SqlMigration -Source OLDINSTANCE -Destination NEWINSTANCE -DetachAttach -Reattach -SetSourceReadOnly` Assuming you want to do a detach/reattach. I don't know (only because I've not used those options - I use `BackupRestore`) where the copies go to when you do this. Throw `-WhatIf` in there too, to review what it's going to do before you do it. Even better, do it on test instances first (you have a test environment, right?).
I made one on drop box. I had a few folders and inside those folders were PDF files, etc. It looked great on a phone and I could show people right in an interview what I had done, what it looked like in the end, and talk about how I got there. Then I could email them a copy. I put the URL right at the top of my resume and used tinyurl to give it an alias such as mylastname-portfolio.tinyurl.com. I had switched from an IT background and wanted to get into analytics. I got my first job as an analyst with very little SQL experience starting at 60k/year + full benefits. I just talked about how eager I was, and how I had been doing freelance work to build experience... and as soon as I learned how to interview for the position (and became comfortable speaking the language) it was pretty easy. I think I was hired on my third or fourth interview?
Anything like this for Microsoft SQL?
Nice. I assume on the portfolio you just included various SQL methods and queries you've written but no actual data/result? I wouldn't want them to go "so Anon i see you've been a consultant for company X and I see from the PDF that their revenue was down last quarter?". That kind of thing would not get me hired in the first place.
Had some sample data. By that time I had only done one "real" thing in SQL outside of some playing around, tests, etc. I had one folder for a website I built. One folder for graphic art work I had done. And one folder for a SQL project that inserted data into a database, spit out a PDF to be printed/emailed as a purchase order, and then showed how the database could be used (in pictures) to do analytics. Since I was originally trying to get into web analytics my pitch was that I had experience building websites and marketing them for years (which was true), and had 2 loose years of experience with analytics and some SQL. Now for my portfolio I just sent out pictures of SSRS &amp; Tableau reports that I've built which leveraged much more complex sprocs, models, etc. -- I don't really get SQL questions anymore, but sometimes there is a written test or something just to demonstrate the basic skills needed. 
Sorry to be a pain but on those Tableau/SSRS screens you blur out the actual figures/content right? The reason I'm asking is that in the current job I've created dozens of reports/dashboards using Excel and VBA. I create views and query the data from SQL server/Access into Excel using ADO/OLEDB and then do the analysis. I'm just worried of revealing sensitive company information in those screens in an interview as that would be a red flag. 
No I don't. I might blur out a company name or something like that. Most of the screens aren't terribly sensitive data. For example I built a predictive model at my last job which looked at incoming leads from paid search and predicts what their total revenue will be. I don't care if it shows the dollar amounts. I don't blur out the company name because it's the company I used to work for. Who cares if I show predicted revenue for Q1 of 2015 when it's 2017?
Well I don't care, you don't care but someone that does care could raise some eyebrows thinking if your going to publicise their internal figures later when you leave them as well. Either way i was thinking how would i prove all the sick projects I've done to my next employer once I leave this company and I guess the answer is to keep a detailed portfolio of all the major projects you've done. Otherwise people will be reluctant to just take your word for it and it could set me apart from all the other applicants. I never really thought of that. Thanks a lot for your help, this is genius. 
&gt;Well I don't care, you don't care but someone that does care could raise some eyebrows thinking if your going to publicise their internal figures later when you leave them as well. Never had a problem with it. Jobs are too plentiful and I get too many offers based on my portfolio. &gt;Either way i was thinking how would i prove all the sick projects I've done to my next employer once I leave this company and I guess the answer is to keep a detailed portfolio of all the major projects you've done. Yep. Works like a charm. I've been told I'm the only analyst who has one in many of the jobs I've applied for/gotten. Probably not the only one in the world obviously, but in my little corner it seems rare and it really makes the interview go easier. Someone asks me a question and its relevant? I direct them to my portfolio for an example. I don't get bogged down in details trying to impress or convince them... stay cool and calm and kind of change the conversation to ask them more questions about the new position and what it entails. It just makes it so much easier. I also always offer to email it, which is almost always accepted... which gives me their personal email address and lets me send a follow up thank you note. 
Oh yeah, I have 100 test environments. I will just ask one of my 20 personal assistants to check it out. ;) Translation: I am a one pony show and don't have a test environment. 
Smooth move, the thank you note is the cherry on top. But you're absolutely right. A good portfolio will change the entire tone of the conversation. You're a legend. 
If you look into modern interviewing techniques they basically tell you to be calm, collected, and not try to impress the company -- because they're already impressed (hence why they gave you an interview.) The interview is therefore your chance to see if they impress you, whether you really want the job, how interested you are in it, etc. It's your chance to ask questions and find out whether it will be a good fit, and doing this will assure them that you will be a good fit and that you actually want to work for them. As a result, and as a result of interviews being short, I tend to sort of 'ignore' technical questions when possible and just point to my portfolio. I try to preemptively use jargon words (like sub-query, or stored procedure, or SSIS package, etc.) when I'm summarizing my accomplishments. For me the hardest part was learning to interview for the job I wanted, and the first 3 or so were total bombs. I didn't know what to talk about first, and they kept having to bring things up and then I'd scramble for an answer. By interview 4 I was bringing up all those things preemptively, and then directing follow up questions to my portfolio, and then steering the conversation back towards questions I had for them. After I got that part down it was easy. On about 100% of the jobs I apply to I get an interview. Of those, for the ones that are good fits, I usually make it to the final round of candidates 70-80% of the time. And then of those I usually receive an offer 50% of the time.
Dynamic SQL seems like the way to go here. Something like: DECLARE @TableVar nvarchar(100) DECLARE @SQL nvarchar(max) SET @SQL = 'SELECT col1, col2... FROM ' + @TableVar + ' WHERE ...' EXECUTE sp_executesql @SQL Of course it gets more in depth, but this should give you the idea of how to build the query. More examples here: https://docs.microsoft.com/en-us/sql/relational-databases/system-stored-procedures/sp-executesql-transact-sql Also, I would highly recommend using the newer JOIN syntax as opposed to the implied syntax using commas. That is outdated by 25+ years now.
Ohhhhhh.... Thanks, that's a big help!
Select a.unit, b.Subunit,b.qty,c.$ From tab1 as a Join tab 2 as b On a.Unit=b.unit Left Join tab1 as c On b.subunit =c.Unit Where a.Type='X'; Hope it works :)
Here it is! [Count all rows of tables in a schema](http://www.dba-oracle.com/t_count_rows_all_tables_in_schema.htm)
I actually think we are mostly in agreement here. The lone disagreement we have is the default behaviour of NULL in SQL being logical or not and I do fully understand your viewpoint. I even think that the default behaviour is acceptable and has merits so long as you are aware of them. That all being said however I still think it violates the principle of least surprise. If I told you I had 3 records, one of which contained the value 'Hello' and I was returning all the records that did not have the value 'Hello' many people would expect 2 records to be sent. Yet a SQL master would have to expect 0-2 because the other records could be null and also removed. Seems like if I specifically wanted all values not equal to hello and not null, I would then be explicit and say colA != 'Hello' and colA is not null.
Thanks
Please don't link to Burleson. 
The "TAB" view is deprecated. Use ALL_TABLES instead. SELECT OWNER, TABLE_NAME, NUM_ROWS FROM ALL_TABLES ORDER BY OWNER, TABLE_NAME Keep in mind that the Num_Rows column is populated by statistics gathering. It may not always be accurate when compared with COUNT(*) from the table.
Where I work I am unable to do this because I'm running outdated versions of both, 9.3 and SharePoint 2007. There's an issue (for me) using standard commands since SAS is Unix based and SharePoint is windows. If you have the latest SAS version 9.4, you can use WebDAV protocol. If you can't use that but have the newest version of SharePoint, you can email the report from SAS to SharePoint and you just have to mess with some SharePoint settings. I can't link at the moment since I'm on mobile. Good luck!
 SELECT table2.column1 AS t2_column1 , table2.column2 AS t2_column2 , table2.column3 AS t2_column3 , table1.column1 AS t1_column1 , table1.column2 AS t1_column2 , CASE WHEN table1.column2 = 'VALUE' THEN 'MANAGEMENT' WHEN table1.column2 = 'VALUE' THEN 'DISSIDENT' ELSE 'OTHER' END AS slate , table1.column3 AS t1_column3 , table1.column4 AS t1_column4 , table1.column5 AS t1_column5 , table1.column6 AS t1_column6 FROM table1 INNER JOIN table2 ON table2.column5 = table1.column3 WHERE table1.column3 = 'value' ORDER BY table1.column4 DESC 
I think you have discovered the perfect formatting.
The new MS docs site is my go to. It's really well done. The old msdn pages were thick on text and overly complex examples. The new site is easier to read and the examples are solid. https://docs.microsoft.com/en-us/sql/t-sql/functions/functions
Leading commas, it's extremely readable :)
I'm a trailing comma guy, but I do have to say with the space after the comma, it'd be very easy to catch missing commas.
Myself and most others who write SQL full time have made themselves cheat sheets. Mine has a collection of string functions, the date conversions, stuffing many rows into a column, printing row counts, and pivot examples. In terms of making complex queries, it's all about experience. IMO a cheat sheet won't help much. For truly complex things, I try to break it down into steps and join many data sets together to get a final result.
It isn't so much a cheatsheet as encountering certain situations and learning the bits of SQL that will fix it for you. Deduping is a great example. Say you have a bunch of duplicate transaction records and you have this huge process and you need to join to it and you need it to be a 1:1 relationship based on certain criteria. At first this might be something you solve by doing a group by on a max(date), but then you encounter some scenarios where the date is the same... so all of a sudden you find out about row_number() and joining where = 1. Maybe you start needing to do dynamic parsing... then you learn about that. Most people learn by doing, not just reading about all sorts of random functions and then memorizing them to use in the real world. The best example here is that you can often come up with a solution to a problem using "low brow" techniques, and you'll arrive at the same solution as a "complex" technique. It's more about drawing on your knowledge, asking questions, learning how to handle certain scenarios, posting your approaches and letting other people critique them, etc. A great example here that I've talked about before is that I once had a really lengthy process that took 20 hours to run. It was doing interesting things and I personally didn't care how long it to took to run to get the data, because it had value and that's why we have servers. Upon looking at the process there was one piece of code that was taking up about 99% of the execution time, and it was a really simple piece of code like this: select a.* , b.key from table1 a left join table2 b on a.key1 = b.key or a.key2 = b.key or a.key3 = b.key or a.key4 = b.key or a.key5 = b.key Based on the size of the data sources it was just going nuts. So we sat down, we posted online, and we came up with a solution like this: select a.* , b.key as key1 , c.key as key2 , d.key as key3 , e.key as key4 , f.key as key5 into #table from table1 a left join table2 b on a.key1 = b.key left join table2 c on a.key2 = c.key left join table2 d on a.key3 = d.key left join table2 e on a.key4 = e.key left join table2 f on a.key5 = f.key select * , case when key1 is not null then key1 when key2 is not null then key2 ... end as key from #table Took a 20 hour process and reduced it to minutes, or seconds. Nothing complex going on at all, just a technique for a certain situation where your first approach was arguably as simple as possible... but the best solution required additional thinking. edit: My first manager used to encourage me to sometimes just start completely over. I used to hate it. Sometimes you write 500 lines of code and everything is going great and you're so close... but things start going nuts. You can try as long as you want to keep trying to tweak things to get what you want, but there is a great deal of value in starting over, giving it new eyes, making sure you know exactly what you want and then starting out from the beginning to get it. Sometimes you don't know what you want until you get 90% done because SQL is a discovery process. You feel like you're almost done but since you didn't start to get something specific all of your processes are basically shit and need to be rewritten. It sucks but it makes you better.
beautiful. Only "WHERE" should probably be "HAVING", since it comes after the "GROUP BY". 
so in my opinion you write your resume different for every job opportunity and try to incorporate your knowledge and job duties into your experience. But fashion your experience in such a way that incorporates what they are looking for. For Example, If they are looking for problem solving, then you can list out a few times you figured out problems You have all the talking points in the job description. You just have to focus your content of your resume on the job description.
Not always. There will always be entry level jobs that if you know your shit, they will take the time to teach the healthcare knowledge. I've worked in healthcare for 13 of my 14 yrs in the workforce, and I still am learning stuff about healthcare.
Awesome. Would you elaborate on/reiterate these points in the cover letter? Also, explain the music to computer jump (in my case) a bit?
IMO, cover letters are fodder that most places don't even read. If your resume is focused and not too lengthy, you'll be fine without it.
It looks like you've already got your answer... but You don't really want to do case statements unless its part of your final projection. It doesn't appear that there's any dependency on the case statement being in the subquery as you originally posted.
So he clearly doesn't understand UPDATE, which was completely obvious in his post. He said he looked up how to do it, and that he needed to have it change the table not return in the query. Your response was totally not relevant to what he described, and your link wasn't straight forward at all. Why not just give him a simple UPDATE syntax to see and send him on his way?
It really wasn't obvious from the post. And there were at least 6 answers on the stackoverflow link that would have helped if he did any research. Like, any amount of google would have solved the issue. "How to modify sql data" literally anything. I'm not a fan of doing other peoples work for them if they can't put any effort in.
&gt;Google has showed how to do this for a query however I need to make the change to the database, not a query.
&gt; And there were at least 6 answers on the stackoverflow link that would have helped if he did any research.
He came here to do it. If you're going to get involved and invest your time why not just answer his question? Otherwise why bother?
Because I believe that people need to practice on their own if they ever want to learn. Someone who made 0 attempts at practice does not need to be handed the answer. And why the fuck are you pointing me out? I gave more of an answer than the person who responded 3 hours before I did. Still OP has made 0 effort? Should I go to his job and just write it for him on his PC as well?
You didn't make much of an effort either. You didn't read his post, or if you did you ignored it. Then you were hostile. It doesn't promote a good environment for people who are looking to learn. People learn by asking questions. If you had actually linked him to something explaining how to do an UPDATE you might have a case, but you didn't because you put in the same lazy effort.
I tend to prefer option 2 for it's simplicity. You can always construct views to combine common report worthy values from multiple tables. It's curious you describe entity attribute value as storage efficient. I find if very inefficient to have values (the attribute column) describe other values (the value column). Having meta-data (column names, datatypes) describing values is significantly more efficient. EAV is flexible for sure, but not particularly storage efficient.
For Oracle, wouldn't EAV be more efficient if the 'value' column were VARCHAR2 or BLOB? Then your rows are only as big as they need to be and you don't end up with possible null columns. Whereas with options 1 and 3, you could end up with null columns wasting space in any given row. But I do see what you mean about views. It's not like the fields are going to change and if they do, there will be more modifications than just the DB.
May I know why? 
With EAV every single value has the overhead of the entity reference and attribute reference. Let's suppose we have a form with 5 VARCHAR2 fields, and consider two records, one with 5 and one with 1 non null value. EntityID|AttributeID|Value|Row Size -:|-:|:-|-: 1|1|Value1|4+4+6=14 1|2|Value2|4+4+6=14 1|3|Value3|4+4+6=14 1|4|Value4|4+4+6=14 1|5|Value5|4+4+6=14 2|1|Value6|4+4+6=14 EntityID|Field1|Field2|Field3|Field4|Field5|Row Size -:|:-|:-|:-|:-|:-|-: 1|Value1|Value2|Value3|Value4|Value5|4+6+6+6+6+6=34 2|Value6|NULL|NULL|NULL|NULL|4+6+1+1+1+1=14 So for record 1, EAV take up 70 bytes vs 34. For record two, they take up the same space at 14 bytes.
Ah. That makes sense. Thank you. 
You could also create a generic data store which is built entirely of flex fields and map different forms to the flex fields through a view. How is this better than just creating individual tables? Well I'm not totally sure, its possible to create more forms programatically, but you would then have to create a form_items table which contained how to display the field. Then that has its own bunny trail of configuration tables. ultimately you would still have to create a view to read the data in a clear way. https://livesql.oracle.com/apex/livesql/file/content_FFJHSV1I9TJLSLKWIS73HYO0U.html
Also easy to bulk add left commas with alt+shift in MS SQL
Wow you had to spare time to start over? We have so many processes still in place that worked first time and were put into production from before I started. No time to optimise no matter how many times I suggest the benefit.
Thank you this worked perfectly. As someone who is still getting a handle on things you have been extremely helpful asshelmet.
yes. It should work for Microsoft as well. More or less the same way...
oh. my. gawd. i've been writing SQL for **thirty years** (not an exaggeration, i started in 1987 with DB2 and QMF) my sincere apologies for the error
You are correct friend, I am still getting a handle on SQL. Asshelmet provided a perfect example on how to do this for me :)
[removed]
He is kind of infamous in the Oracle community for often perpetuating misinformation and for plagiarizing other people's work as his own.
Thanks for the info. It was one of my goto sites when I was working as PL/SQL developer. 
I would not listen to this person... your enthusiasm and attitude is EXACTLY what most employers want, well over knowledge (entry level). Some of the smartest people I knew in the tech field came from Music Theory Bachelors actually.... weird. 
Yeah, one thing he is good at is search engine optimization, so unfortunately his site is often at the top of a google search. It is better to stick to the real experts like Tom Kyte and Jonathan Lewis. For PL/SQL also Steven Feuerstein.
You do realize he copied it directly from the thread I linked you right? The one you said wouldn't help you? Right on page 1, fourth answer down. https://stackoverflow.com/a/16745607/7948962
I'm on my mobile so can't format this properly easily but here goes. SELECT Code, Description, COUNT(\*) as Count From Table Where Code IN (SELECT Code from table group by code HAVING COUNT(DISTINCT Description)&gt;1) GROUP BY Code, Description Order by code, COUNT(\*) desc 
Your goal table was somewhat different than your description of the problem. This should return only the most popular Description for each code, and only when there are multiple descriptions: ;WITH CTE AS ( SELECT Code, Description, COUNT(*) Count FROM table GROUP BY Code, Description HAVING COUNT(DISTINCT Description) &gt; 1 ), CTE2 AS ( SELECT *, ROW_NUMBER() OVER(PARTITION BY Code ORDER BY Count DESC) Row FROM CTE ) SELECT Code, Count, Description FROM CTE2 WHERE Row = 1 This should be sufficient if you want results like that in your Goal Table: SELECT Code, Description, COUNT(*) Count FROM table GROUP BY Code, Description HAVING COUNT(DISTINCT Description) &gt; 1
If you want OP to be notified of your comment, I would suggest replying to their comment directly. As I clarified further down - I don't have anything against enthusiasm, or claim that OP is not smart. But humility is a quality I happen to prefer over self-esteem, as I also stated below, this might be a view that is not common in the area where OP is from.
You're going to have to describe the problem in more detail. What is the sequence of events that causes problems? What should the sequence of events be?
 WITH cte AS ( SELECT Code , Description , COUNT(*) AS Popularity , COUNT(*) OVER (PARTITION BY Code) AS Duplicates FROM table GROUP BY Code , Description) SELECT * FROM cte WHERE Duplicates &gt; 1 ORDER BY Duplicates DESC , Code , Popularity DESC
The problem is that data is getting inserted into our employee table before it gets inserted into our department table. We have a process that is based off of the values in the department table, but there are some users where it runs before the insert into department. I'm thinking re-ordering the inserts in the SP is the best solution so anything that feeds off of the employee table gets inserted first. 
Thanks, the second snippet is what I was trying to accomplish. 
What CRM software? Sometimes these are very software specific on best practices and supported methods.
Thanks, guys. I've been having access issues the past couple of days so I haven't tested it yet. With my basic skills, it looks right. I'll confirm officially when I can get back into our machines.
How long does the process run? How often does it run? How many users are you inserting at a time? How long do the inserts take? Usually data needs to be inserted in a specific order. If that's not the case, insert the records that trigger the problematic process (I guess employee) last. Transactions can help, but I use them to roll back failures to a stable state. Using them to block another process feels wrong.
This kid can't understand the page you linked, let alone how to read to the fourth answer! I used to be him. It becomes daunting and you find all sorts of conflicting answers, some of which work for one type of database, some of which work for another. It is reassuring to *ask someone for help* before you commit to making changes to your datasource. Sure you can make the argument that this kid shouldn't be in a position to make changes to a database, but that isn't the point. If you're going to spend your time, if you're going to invest your energy... then be helpful, or why expend it at all?
DBEs, because its a lot of work, a lot of knowledge, and something that isnt easily managed offshore
In my local market I'd say my top two picks would be full-stack development, and business intelligence. Full stack devs seem to always be in demand, but my location (smaller country where the IT scene is more smaller orgs who need broad skillsets rather than specialists) might skew that, and those positions have SQL as one of many skills rather than the main focus so it might be outside of what you're looking for. Business intelligence is blowing up right now. The available tools are improving at breakneck pace, upper level executives are excited about it, and it takes a knack for analysis and understanding of business processes and people that doesn't always exist in people who have taken a more purely technical path to DBA/DB design. Interest in BI is pushing demand for ETL, data warehousing, report writing, and in my experience anyone who can do a little of each of those and has the soft skills to understand and work to business needs is getting snapped up right now. You don't even need to be particularly amazing at any of them. A nice bonus too is that BI very often puts you and your work in front of a very receptive crowd of important c-levels so internal opportunities are often pretty good.
What did you use to teach yourself? I know everything up to Joins and Sub queries but my lessons stopped there. I can perform those fast on my company database already.
Data Scientist because it's extremely math/statistics heavy. Usually requires at least a Master's degree, and you'll probably need to know another language like R.
True that studying data science is math/statistics heavy, but the practical application not so much. 80% of (supervised) machine learning is data manipulation and tweaking parameters to push results in the right direction.
HeidiSQL
It does sound like he needs to reorder the inserts, but that the inserts are only half the problem here. Queries are hitting tables that are currently on different versions of the truth. It might matter, maybe no one cares. Yeah, transactions are great for rolling back failures to a stable state, but they are also great for consistency. OP's specific situation could change this, but if for example, an employee always has to be in an existing department, and a department always has to have at least 1 employee to exist, both of these updates should occur in the same transaction otherwise any queries that run can return results that are inconsistent and can return data sets that should be impossible. What's more, if you do the departments first, and then there's an issue with the employees table, you need to rollback the employees, and the departments at the same time to be consistent again. That needs to be a single transaction. Otherwise you need to look at keeping track of when a last full update occurred, and have all your queries specifically exclude any data that was inserted after the last full update.
ModelRight, for no particular reason other than the fact that it was recommended by a friend.
My intuition is that wrapping related changes in transactions as a standard could lead to deadlocks.
Toad
Yeah, we've gotten complacent these days, with fancy things like syntax checking. Minor error aside, anybody who proposes this as the solution has clearly been around the block a few times.
Except that the application is all about these forms: processing, approving, copying, reporting, comparing, and transmitting. These aren't forms built for an app, it's an app built for forms. Storing the form definitions in XML or JSON makes them a pain to edit because you have to parse them out every time.
Ribbit. 
It gets better when people can't figure out how to turn it off and go crazy because of it.
I wound up with a work-around but I'll take a stab at this.
I remember I once had a conference call with a business analyst in the west of the country and I fired up Toad to check something during the call. I'm a French-Canadian and the slang term for us is "frogs". Laptop was close to the phone and the guy on the other end was wondering what that was. Obviously, I had to explain to him that the reason we're called frogs is because we're badly overrun in the summer. It's bad to the point that some sometime get inside but we can't find them (same as crickets), to the point that there's random rib bits from time to time. Good times.
Finally, an antonym for "cream of the crop." Alliteration - check, reference to farm - check, four words with two of them the same - check and check. Thank you for contributing to my vocabulary. 
I like Data Grip
The way data scientists perceive data is very different than relational databases. * Data is necessarily flat, always. * Distribution is more important than accuracy; erroneous data is preferred over skewed data. * Elegance and efficiency is time better spent iterating. * Hardware solves any problem. * Data isn't truth. It's witless ants being herded to exploitation.
Teradata SQL Assistant is what I use most often because it's there and flexible enough for the most part, second would be SSMS. I've tried to use other ones like DBeaver, Workbench/j and SQuirreL but found them lacking in areas.
I think the best way would be to definitely play around with database concepts on your own time by making your own database, or using sample databases like [AdventureWorks](https://msftdbprodsamples.codeplex.com/). Another resource I thought was kind of fun is [LeetCode](https://leetcode.com/problemset/database/). Although you won't use nearly the entirety of SQL, it's decent for putting your basic and intermediate skills to the test and see if you can find the most efficient answer and why that is the answer. Intermediate concepts I think are mostly querying like what would be basic concepts but to answer more complex questions; not getting into database administration but views do ride the line in this case. Advanced topics are much more database administration centric and are very concerned with access to data and performance of the database structure. I think the best way to get a handle on all of it is to look up all of these concepts (either on the web or a book), learn what they are and how to do them, and why use that method over others (ie why use a trigger vs stored procedure or a function). Intermediate Concepts: * JOINs, ANSI-89 and ANSI-92 syntax * UNION vs UNION ALL * NULL handling: COALESCE &amp; Native NULL handling * Subqueries: IN, EXISTS, and inline views * Subqueries: Correlated * WITH syntax: Subquery Factoring/CTE * Temporary Tables * Views * Branching Logic Advanced Topics: * Functions, Stored Procedures, Packages * Pivoting data: CASE &amp; PIVOT syntax * Hierarchical Queries * Cursors: Implicit and Explicit * Triggers * Dynamic SQL * Materialized Views * Query Optimization: Indexes * Query Optimization: Explain Plans * Query Optimization: Profiling * Data Modelling: Normal Forms, 1 through 3 * Data Modelling: Primary &amp; Foreign Keys * Data Modelling: Table Constraints * Data Modelling: Link/Corrollary Tables * Full Text Searching * XML * Isolation Levels * Entity Relationship Diagrams (ERDs), Logical and Physical * Transactions: COMMIT, ROLLBACK, Error Handling ---- Note: this is a list of what i was more or less rattling off the top of my head, if anyone can think of more feel free to add it. 
The key to success may be to embrace the fact that we have a kind of three-valued logic with SQL, going beyond the traditional two-valued one where there's only true and false. Instead, there's black and white and a unique color, call it 'transparent', that indicates a universal placeholder for a missing value, a 'hole' in the data. When you're asked to enumerate those records that do not have 'Hello', you'd write that as `col != 'Hello'`. Turns out one record has `col` set to `'Hello'`, one has `'Meh'`, but the third one has `null`. Now when we agree that `null` stands for 'pending', 'not yet known', or 'not applicable', do you or don't you include the `col is null` record? Another example: you have a drawer with three things. Someone asks you to hand them those items that can open the front door. Turns out there's one key that does open that door, so you give it to them, the other things being a key to the back door, and a banana... OK so far so good. Now someone asks you to give them the items that do NOT open the front door, so that would include the key to the back door, and definitely not the front door key, but do you hand out the banana? Yes and no. You should include it IF you take the request literally (in that case, either make sure the `opens` column includes a unique default value of your choosing, like you could record `'front'`, `'back'` for the keys, and `'other'` for the banana). Many people though would think you're not acting sane and claim that the very criterion, 'which door to open with this key', is just not *applicable* to a banana. Those people would rather have the DB contain values `'front'`, `'back'`, and `null`. Observe that for the right decision to be made with common sense, it is essential to know whether that third thing in your drawer is a banana or another key (the matching doors for which have yet to be determined). SQL's logic takes the point of view that since you're the one who said a given attribute was 'undetermined' / 'not applicable', then asking for a specific value (in the positive or the negative, to wit) certainly should *not* include imponderables! Seen this way, your problem is less one of SQL dealing with `null`, it is one of ownership, surprisingly. Turns out your gripes do start with the remark that you are "dealing with legacy databases", IOW you do not own the data, they're more or less set in stone. Just go ahead and take ownership, define a view that has `'front'`, `'back'`, and `'other'` instead of `'front'`, `'back'`, and `null` for that vexing column. In daily life, when faced with such a situation, it is not uncommon to ask back when and where in doubt: 'OK sure here's that key I'm certain of, but what about this banana? Have it or not? It *won't* open the front door for sure, but then again it's not for opening doors *at all*'. This is what SQL does, it forces you to be explicit when in doubt—either in the data that you record (have a default value within the bounds of a datatype (not always wise or possible), or else have an extra column with a flag, or else use `null`) or in the way that you structure your queries (where you have to use `x = a or x is [not] null` or `coalesce()` or similar means). 
It's because recently a lot of "Data Scientists" are just ppl that learned some SQL, some R, some Stats and they think they're changing the world. No technical knowledge, no methodology knowledge etc. It's sad that this senior expert role was depreciated into "kids with some toys". 
What RDBMS are you interested in? /u/Happyslapist made a good list if you're interested in SQL Server. I'd recommend MySQL or it's fork MariaDB. You can get XAMPP and get the full L/WAMPP stack (https://www.apachefriends.org/index.html).
Given answer is ANSI 89 and yours is ANSI 92 which is much more preferred. As to how Ansi89 works, it's a `CROSS JOIN` between the three tables and then filtered by the `WHERE`. 
Technically there is no difference. If you check the execution plan postgresql should give you exactly the same plan in both queries. But the query using "join" is more readable in my opinion 
In which areas do they lack can you specify that ?
what's the execution plan and how do I check it?
You *can* [query AD from SQL Server](https://www.mssqltips.com/sqlservertip/2580/querying-active-directory-data-from-sql-server/), but that's no guarantee that it'll work better than your current method. Before diving into a solution where SQL Server is doing the work (SQL Server licenses are expensive, don't spend that money on things cheap application servers can do), take a closer look at where you're "timing out" and how your script runs, and see if there's a better way to manage that. Is this really a "timeout" (script execution is killed after a fixed amount of time where no progress is made), or are you just killing it after a few hours? Or are you running out of memory? CSV is a poor way to store/manage large quantities of data. Can you throw your data into SQL Server table(s), and then operate on it in batches instead of trying to boil the whole ocean at once?
Markus Winand has a great site that's specifically developer oriented and features code snippets for multiple RDBMS engines: http://use-the-index-luke.com/
&gt; I'd recommend MySQL or it's fork MariaDB I suppose depending on your career goals... but i would never tolerate MyISAM in the org, and no one wants innodb's performance my OSS preference is 100% in favor of Postgres (though my day to day is MS SQL, and I hope to never need to touch Oracle)
"Complex" is in the eye of the beholder, and a lot of it boils down to just being exposed to different techniques for getting various tasks done. A "cheat sheet" may not help you as there's so much to the language - you can't put *everything* on there. And just throwing stuff on a sheet/in a doc doesn't help you with practical application of it, which is where understanding will come from. When I find a new technique, function, or piece of code that I want to stick in my "toolbox", I experiment with it and write a blog post about it. Once I've learned it well enough to explain it to someone else, I've internalized it and remember how it works at least in broad strokes. And then I've also got my blog post for reference to fill in the gaps.
I would have written it like so: SELECT DISTINCT sku FROM xsell x WHERE NOT EXISTS (SELECT null FROM product p WHERE x.sku = p.sku) There is no GROUP BY needed, and no subquery needed. But why is there no referential integrity between PRODUCT and XSELL? Ideally the PRODUCT table would have a column that indicates whether it is active or discontinued. Instead we have what appears to be orphaned records in XSELL. Also, why "clear out" records from XSELL at all? Is there a performance problem? With just 3 short columns and under 5 million records, it is a pretty small table.
Do you know some basic Python scripting? When I need to do something like this, I will nest my SQL statement inside a Python script with the term I am using to separate the tables as a variable. I will then save the unique values for the table in a Python array and iterate through them to create each table. I use Oracle and the Cx_Oracle python library, however I know there is a similar one for MS SQL. http://pymssql.org/en/stable/ Good luck!
Unfortunately I don't know any Python! I think I loosely understand what you're describing, but don't know how I could replicate this is in SQL. Thankyou anyway though! 
DBeaver is JDBC focused and I could not get my production MongoDB to connect. It was likely a combination of SSL and user error. When connecting to something like Teradata it would pull all user and table info even if you don't have permission to it, it could probably be solved in the options section. I liked DBeaver the most out of the ones I don't use. I used Workbench/j when I was using a Redshift DW, it seemed stripped down and didn't offer anything more than SQLA. SQuirreL is also stripped down and I think I was having connection issues on some dbs so I stopped using it.
If I was asked to do this for work, I'd use an external scripting language for it. There are a ton of languages that can do it. In particular I'd use C# and stream the CSV data directly into zip files to save space for that many rows. But if you're more advanced-level you can do it in pure SQL in a few ways... but none of which particularly elegant. A few things I'd recommend getting familiar with would be [BCP](https://docs.microsoft.com/en-us/sql/tools/bcp-utility), [Cursors](https://docs.microsoft.com/en-us/sql/t-sql/language-elements/declare-cursor-transact-sql) and [xp_cmdshell](https://docs.microsoft.com/en-us/sql/relational-databases/system-stored-procedures/xp-cmdshell-transact-sql) and ["Output to" syntax](https://www.google.com/search?q=sql+%22output+to%22).
Using BCP perhaps? https://docs.microsoft.com/en-us/sql/tools/bcp-utility Like so: bcp "SELECT * FROM yourdb.yourschema.yourtable WITH (NOLOCK)" queryout c:\sql\bcp.txt -c -T 
Though I have no direct experience with this specific usage(though I have used it to dump query results to files), this sounds like something SQL Server Integration Services could handle pretty easily(SSIS). Simply parameterize your queries from the source, and set the destination to your file. SSIS can even create the empty CSV files for you. If I am reading this correctly, this data currently exists in Access? Is there any reason why you aren't directly importing them to SQL?
That's actually a really good point. I didn't realize it was from Access. Why not just do SQL Import Wizard with Access as the source? Should be pretty straightforward.
This is how to do it. I use BCP for this kind of thing regularly.
Yeah, when doing some Googling earlier BCP kept popping up - I had a go at writing something similar to that out, but SQL didn't recognise 'bcp', so I'm thinking I might not have my SQL configured to use it? I should mention that I'm using SQL on a hosted external desktop designed to work with catastrophe modelling software, so I don't always have the permissions to add or change components. 
sql is a 4th generation language meaning you tell to the computer what you want but not how to do it. The rdbms in your case postgresql desides what to do. if you are interested to know what it does you can tell to postgres to show what is it doing. that you do by adding explain in fron of the query. you can read all about it here https://www.postgresql.org/docs/current/static/using-explain.html
I think you may have confused a few people when you put MS SQL in the title, and then said 'migrate out of Access' in the post - those are two very different things. BCP is a command line util that comes with MS SQL Server - not Access though. You'd execute it via a command prompt. Assuming your data is in MS access - then BCP also wouldn't be useful in getting the data out, so far as I know it's only meant to connect to SQL Server databases, and not MS Access files.
Seriously... Sure, the subquery is faster in this instance, but not due to it being the right tool for the job. I'm unfamiliar with CrateDB so it may not work this way, but if you're joining on a unique key (which they were, the sku), the "*15 billion row*" cartesian product should have never been materialized. However, since GROUP BY was used, the compiler may have been forced to materialize the cartesian product, which is most likely the source of the performance issue. The maximum number of materialized rows should have still been no greater than the number of rows in the child table (4.6 million in this case). Regardless, the *best* solution is like you suggested, a flag to indicate that the product has been deleted. The second best is cascading deletes. Referential integrity is a prerequisite to both.
It's a cmd line utility you need to run it from the cmd prompt. Also updated my other post since you need to include the db and schema in the query. Again though, if it's data from Access this might be way easier to do through the import/export utility...
Here's an example of an approach using pymssql and pandas, this assumes ALL records are included in the result with a column called 'Campaign' being used to split the results into separate files: import pandas as pd import pymssql import time conn = pymssql.connect(host=HOST, database=DATABASE, user=USERNAME, password=PASSWORD, autocommit=True, charset='utf8') with open(r'contacts.sql', 'r') as f: q = f.read() df = pd.read_sql(q, conn) campaigns = df.Campaign.unique().tolist() for campaign in campaigns: filename = 'ContactDetails-{}-{}'.format( campaign, time.strftime("%Y%m%d-%H%M%S")) df[df.Campaign == campaign].drop('Campaign', axis=1).to_excel('{}.xlsx'.format(filename), encoding='utf-8', index=False) It may be that if OP is only doing this one time that it's not worth the time to learn what that is actually doing, but knowledge of these kinds of things tends to be VERY useful when working with any sort of data.
&gt; and I hope to never need to touch Oracle Oracle the company is the devil. [Oracle Database](http://www.oracle.com/technetwork/database/enterprise-edition/overview/index.html) is top tier stuff.
What's most popular/in-demand in the industry right now? I've used a lot of different SQL DBs in the past. 
Great list, gives me a lot to Google and read up on, then try out in a DB. I haven't heeard of LeetCode, looks like a good place to test my skills and figure out what other areas/concepts I should focus on. 
First: What database are you using? MS SQL? MySQL? Oracle? I'm going to give you an MS SQL answer. Do you have an actual timestamp data type that you can order by reliably? If so you can do something like this. WITH ClockEvents AS ( SELECT Employee, TimeStamp, CAST(ROW_NUMBER() OVER (PARTITION BY Employee ORDER BY TimeStamp ASC) % 2 AS BIT) AS IsOdd FROM Table ) SELECT Employee, TimeStamp, IIF(IsOdd = 1, 'Clocked Out', 'Clocked In') FROM ClockEvents Or a more condensed - but less readable - version SELECT Employee, TimeStamp, IIF(ROW_NUMBER() OVER (PARTITION BY Employee ORDER BY TimeStamp ASC) % 2 = 1, 'Clocked Out', 'Clocked In') FROM Table 
Apologies - this is a process where the data *and* CSV export process previously existed within Access - but the data is now all in SQL, and I need to find a way to export it to CSV. Hope that clarifies it! 
Thankyou, this is helpful. The data is in SQL, not in Access, apologies if I was confusing - the whole process used to be within Access, but the data is now in SQL instead and I just need to get it into the CSVs. The VBA that the old Access process used was written by someone else and I haven't had much luck working out the equivalent way to do it using data in SQL. Hope that makes sense. 
`Queries` is a part of the alias. `Queries` is the name of the "table" and `Settings` is the name of the "column". As for your second question, I'm assuming your structure is something like: &lt;ParameterValue&gt; &lt;Name&gt;&lt;/Name&gt; &lt;Value&gt;&lt;/Value &gt; &lt;/ParameterValue&gt; I'm only moderately familiar with XML queries, so I may be somewhat wrong, but I'll give it my best guess. So in this snippet `ISNULL(Settings.value('(./*:Name/text())[1]', 'nvarchar(1024)'), 'Value')` the `./*:Name` bit is basically just navigating to the &lt;Name&gt; element. The `/text()` is retrieving the content of the element as text. The [1] is saying, if there's multiple &lt;Name&gt; elements, only use the first one - yes, it's not a 0 based array. The nvarchar(1024) is specifying the data type that SQL will convert the value to. The ISNULL is there in case there is no Name element, in which it defaults the name to `Value`. 
Hi, sorry - the data exists in SQL, not Access. I had a go at setting something up earlier using SSIS, but didn't have all of the options available to me to be able follow the instructions, so I was wondering whether I might not have the necessary permissions or functionality. I'll have another look into it though. 
&gt; What's most popular/in-demand in the industry right now? That is a area and field specific question, working on the enterprise level the big players are Oracle, MySQL, MS Server, and PostgreSQL. A useful link I found for explaining the diffrent DB Engines is [db-engines.com](https://db-engines.com/en/ranking/relational+dbms). You can also go to [PayScale](http://www.payscale.com/research/US/Job=Database_Administrator_\(DBA\)/Salary) and see what is big in your city. For example, I know alot about MS SQL and have many certifications for that engine because it's the most popular in my area. I also made an effort to know Oracle equally since it's the second most popular too and knowing both C# and Java I can work well in either environment. 
Ok, i think i get it... So the cross apply is basically taking the results from the first CTE for anything inside the &lt;ParameterValue&gt; XML tags taken from the ExtensionSettings Column from the Subscriptions table..then in the second query it parses the same XML data on 2 columns, 1 to show if it's " TO, CC or BCC" in the Name tags, and the other to return the list of addresses in the Value tags &lt;ParameterValues&gt; &lt;ParameterValue&gt; &lt;Name&gt;TO&lt;/Name&gt; &lt;Value&gt;e-mail@domain.com&lt;/Value&gt; &lt;/ParameterValue&gt; &lt;/ParameterValues&gt; This is a sample of 1 of the XML data...so ``ISNULL(Settings.value('(./*:Name/text())[1]', 'nvarchar(1024)')`` This part is parsing the XML data, returning the value between the Name tags as "TO" and the 1 below is returning the values in the Value tags as "e-mail@domain.com" ``Settings.value('(./*:Value/text())[1]', 'nvarchar(max)') AS SettingValue`` A quick follow up though...when the expressions starts with ``Settings.value`` in the second CTE, It appears twice for both expressions, but in 1 we get `'(./*:Name/text())[1]'` and the other is ``'(./*:Value/text())[1]'`` .... is Settings.**Value** taking the entire XML snippet then parsing it? I get that "Name" and "Value" inside the brackets is for the XML tag, but the first time Value is mention, i'm not too sure what that is. Oh, also, what does "Nodes" mean in this snippet? ``CROSS APPLY subscriptionXmL.ExtensionSettingsXML.nodes('//*:ParameterValue')`` Thanks again for your input, you explained it really well! /EDIT: IF anyone wants to know more: https://docs.microsoft.com/en-us/sql/t-sql/xml/value-method-xml-data-type 
&gt; the first time Value is mention, i'm not too sure what that is. **Disclaimer**: Most of what I'm about to say is based on my understanding of SQL Server's implementation of XQuery, which is built mostly on assumptions and comparisons to other programming languages, so take it with a grain of salt. `value` is a method offered by an implementation of another language called [XQuery](https://docs.microsoft.com/en-us/sql/xquery/xquery-language-reference-sql-server). It only applies to XML datatypes (maybe JSON in later editions of SQL Server? I haven't used it in that context, so I'm unsure), so my knowledge of it is limited. It is strange, because it's syntactically similar to [extension methods in C#](http://csharp.net-tutorials.com/csharp-3.0/extension-methods/), and to my knowledge there's nothing else like it in SQL Server - aside from other XQuery methods of course. Now that I'm done rambling, I'll try explaining a little better. `Settings.Value` is misleading. `Value` is not actually a database object like we're used to when we see` [object].[object]`. `Value` is actually more similar to a function that uses `Settings` as its first parameter. Its next two parameters are a "query" - in this instance, `'(./*:Name/text())[1]'` - and a datatype. So it's more accurate to view it as `[parameter].value([parameter], [parameter])`. Which is weird and confusing, especially if you don't have much experience with languages that make use of extension methods or something similar. It's also not named intuitively. If I were to name it, it would probably be called `GetValueOfXmlElement` because I like more verbose names. 
Ok yea, that's what i was going towards, just didn't know how to ask the question properly ( Hence the wall of text XD)... But that's perfect, all makes sense now. Thanks again!
Glad I could help!
This article could help you to integrate the BCP statement into stored procedures. https://www.red-gate.com/simple-talk/sql/database-administration/creating-csv-files-using-bcp-and-stored-procedures/ it is well explained and will walk you through the whole process with good examples. The only downside is they depend heavily on xp_cmdshell which may not be available on your server depending on your security levels.
You are very convincing. You have me convinced that this behavior is the best way to handle this, I do however still believe this should force a warning in the cases where nulls are excluded as a result of colA != 'Some value'. My chagrin with this comes with the fact that by assuming I didn't want the banana you didn't even make me aware it was an option. A warning would make this a non-issue. In your example someone asked for all items that do not open the front door, the banana will not open the front door so we can logically conclude it should be included in the request. Now if we frame the request give me all keys that will not open the front door we begin to see the logic in the response with respect to datatypes. Still, in conversation I would at least let the person know I was explicitly excluding the banana from the list of options as a heads up in case for some reason the banana was actually what they wanted all along. In contrived examples this always seems like a non-factor as in our simple example it would be very easy to spot that the banana was excluded from the result set. In larger queries formed by joining multiple tables with complex conditional logic, it is usually much more difficult to catch without thoroughly analyzing the raw data. If I got a warning I was losing records where I might not expect to lose them, I could then decide if that was what I intended and use the set or correct my logic to explicitly include/exclude the nulls. 
Then you may get somewhere with BCP, but you'll still have to split it. You could use something to generate the 170+ statements and put it in a batch file. Or you could use some PowerShell or python to split out one large file into a bunch of smaller ones. Or you could use a scripting language all the way - see my other post for an example.
I think the most popular ones are * MySQL/MariaDB (free) * MS SQL * Oracle/PeopleSoft
Given this, I also question the database in general, are we lacking indexes (fragmented maybe?) or updated statistics? Also, why group at all for what you need? That's expensive here, just use a DISTINCT to get the unique rows, you're not otherwise using any aggregate functions.
A database. SQL stands for Structured Query Language, so you need a database to run your query against to return data... Depending on the database you might be able to compile stored procedure and functions into the database as well, but they're database specific.
Nothing. A database to run them against and a client to issue the code from. They're not really comparable in this regard.
Please read our sidebar. This content is far too basic for /r/SQL.
&gt; WITH (NOLOCK) [Bad Habits: Putting NOLOCK Everywhere](https://blogs.sentryone.com/aaronbertrand/bad-habits-nolock-everywhere/) 
Amazing tool, literally cannot work without it
You need a database to connect to and query. The code isn't compiled by you or your program. It's all submitted to the database more or less as plain text. Indeed, if you look at the compiled binary for many applications that access an SQL database, you'll see the strings for the SQL queries plain as day. Remember that SQL is a *declarative* programming language like HTML is. You tell the database what you want, and the database figures out how to do it. The database engine does compile the query into an "execution plan" that it will often save and reuse for similar queries in the future. This is where it determines which algorthims to use, which indexes, which fields to return, how to execute functions, etc. The database server (a) parses the query, (b) determines and optimizes the execution plan, (c) fetches the data from disk, (d) returns the requested data to the client. Most RDBMSs have ways of viewing the query plan and manipulating the query planner, but you very rarely need to do that and you can easily direct the RDBMS to make bad choices. There's a more technical explanation for what the SQL Server database engine does [here](https://technet.microsoft.com/en-us/library/ms190623(v=sql.105\).aspx). Now, that just describes how the database engine physically decides how to execute a query. There's also the [logical execution order of the query](https://docs.microsoft.com/en-us/sql/t-sql/queries/select-transact-sql) (see the section labelled "Logical Processing Order of the SELECT statement"). That's important because you usually can't refer to something you describe in a clause you haven't processed yet. The biggest "gotcha" of logical processing order is aggregate functions. Notice that WHERE is after GROUP BY? That means the results of an aggregate function aren't determined until after the WHERE clause is processed. This makes sense, because you want to be able to filter results out of a SUM() before you start adding. However, that means you can't say `WHERE SUM(Amount) &gt; 100`. That's why the HAVING clause was created. It's a WHERE clause that's evaluated *after* aggregates have been done. Finally, the last two links are all specific to SQL Server. Other RDBMSs will operate similarly, but every SQL RDBMS is slightly different. That might be overwhelming, but for the most part, none of it is that important. You should focus on thinking in set-oriented ways, understanding how JOINs and aggregate functions work, and how three-valued logic works. 
This was pleasant to read. rock on!
&gt; just use a DISTINCT to get the unique rows That's just as expensive as a group by
Typically but not always however.
In this instance, it would be though
there you go.
I think either way the stress and headaches are unavoidable. Either deal with them now while learning a new skill or later at the job when you fuck all kinds of shit up. Most positions offer training anyway. People might have a higher chance at being hired if they're upfront and honest but demonstrate an ability to grasp new information quickly. I know X company that's hiring for a "I.T QA Analyst". They're starting the hiring process first in-house and 4 applied, all have no prior I.T experience but there's one who is likely gonna get it because she is pretty sharp and has over 10 years experience working on the floor with the systems. She was upfront and honest that she didn't know much when it comes to I.T but she showed she could pick things up pretty quick. Be honest 
You can use NOT IN. Select ids from table Y. Then select all rows from table Z where id NOT IN the result of first select and then you delete rows that are selected. Edit. You can also use NOT EXISTS or you can also use left join and then delete the ones that don't have joined rows.
Thanks but i'm totally beginner and need the code asap :( Can you give me a full code? Normal MySQL server witch runs on Xampp.
DELETE FROM table z WHERE z.value NOT IN (SELECT y.value FROM y.table)
Thanks works! DELETE FROM Z WHERE id NOT IN (SELECT id FROM Y);
Others are going to find it difficult to learn from your questions if you delete the question. 
Check out llvm. Postgres is starting to use it to compile parts of sql statements.
I never understood this logic. Employees should punch IN or punch OUT and the database should record if it is IN or OUT. There should be no guessing involved whether someone was coming or going.
Solid answer. Not sure why you picked up some downvotes. If someone does not understand the solution, please ask why. The use of the row_number analytic function with partitioning by employee seems right to me.
Docs.microsoft.com/sql It has info on how to download and implement SQL server (free) and documentation/tutorials. DevGym.oracle.com has a cloud instance of Oracle (also free) with tutorials/documentation as well.
Well said!
Hi You can write something like this : schoolName = ""; Get SQL results, sorted by school name. loop through SQL results { Get SQL Row If SQLROW.schoolName &lt;&gt; schoolName { close file if open; Open a new one with name SQLROW.schoolName; } write the data to the .CSV file; schoolName = SQLROW.schoolName; }
Analytic (windowing) functions.
PL/SQL is just Oracle's variant of SQL Learn ANSI SQL and it will help you, but learning PL directly is fine as well
Honestly, if you have any idea of how to navigate SQL, switching between PL/SQL or TSQL or whatever isn't that big of a deal. It's just a different flavor. If you want to be able to have code independent of database systems, learn ANSI
&gt;Windows Server 2008 instance where I will be creating and running PL/SQL scripts. Something doesn't add up for me here. * He wants you to be "in charge of" the server (implies Windows Server sysadmin), but you're going to be doing database work. * DBAs aren't *usually* sysadmins, and sysadmins don't typically get too far into the databases hosted on their servers. * I don't hear people refer to Windows Server installations as "instances" - but we *do* call SQL Server installations "instances". * While it's not impossible, people don't *usually* run Oracle (which is what PL/SQL is for) on Windows.
Get a good editor. I used SQL Navigator back in the day, but I think more people use [Toad](https://www.quest.com/products/toad-for-oracle/). I haven't done PL/SQL for several years now, but I actually miss it. We migrated all of our database over to SQL Server. And while I like SQL Server as well, there are things I miss in PL/SQL. Watch some quick tutorials on Youtube before you hit your class. While PL/SQL and TSQL are similar, language differences aren't the only thing you'll see. The structure is different as well (PL/SQL has a header and body, among other things). And if you get stuck, just search Google "SELECT TOP TSQL vs. Oracle", for example to see the differences. Now I'm missing my Oracle days. I feel so.. conflicted.
False. Oracle is a RDBMS. To query an Oracle DB, you use SQL, just like you do with almost all other RDBMS. Granted, each DB can have its slightly different syntax, but it's generally all the same. PL/SQL is a object-oriented procedural language for SQL and the Oracle database. This is similar (but much more powerful and feature rich) to SQL Server's T-SQL. It's modeled after the language Ada, which in turn is modeled after Pascal. Get familiar with the anonymous block and DBMS_OUTPUT.PUT_LINE. Toad is by far your best (although super pricey) IDE for PL/SQL and it's debugger is great. Make sure to pick up a copy of this book. It's seriously the only book that I've ever seen multiple copies of at every single job I've had. My current job actually calls it "Steve's Book" and it's often referred to as the PL/SQL Bible: [Oracle PL/SQL Programming](https://www.amazon.com/dp/1449324452/ref=cm_sw_r_cp_api_aHLMzbS0F388Q) 
I use it. My background is Python(which I used for purposes much different than working with databases) and Java(purely academic experience). It took only a few days of messing around with it to start becoming somewhat productive. As long as you have some experience with procedural programming and SQL as a whole, you're gonna be fine. Also, as taylorwmj has already pointed out PL/SQL is actually a procedural extension of SQL and is a language in its own right, although it's specifically designed for working with Oracle Database and the data that lives there.
You can do it in one go like this :) UPDATE CM SET filename = 'String' + filename FROM cm_steps CM WHERE ItemID IN (11667926, 11667979, 11667981, 11667997) 
well, that was a lot easier than I thought it was going to be! thank you! edit - Wait, sorry, will it pull the value from the filename column automatically if i do this, or do i need to specify it first?
It will grab the existing [FileName] value, append it to your 'String' and then update the value in the table. 
Perfect, thank you
Just looking for the syntax? insert into gapiaudit_stage2 select acreated , akey2 from gapiaudit where acreated &gt; 'gapiaudit_stage1.datereq' and acreated &lt; 'gapiaudit_stage1.datereq2' and akey2 = 'gapiaudit_stage1.clientinfo'
This is gonna sound preachy, but you need a test environment. Even if it's a couple instances running on an old desktop sitting in the corner. *Something* to do a run-through of your process before doing it for real.
Hey, I wanted to say thank you. I have everything moved over. The only think left to do is for me to remove a dependency of sql 2005 and another program. I did a little digging for that and I see that I have to do some registry changes. Haven't tackled that one yet, if you have a powershell script for that one, then you will officially be THE man. Would love to set up a test environment locally but not sure how to get the full version of sql 2008 so I am working with apples to apples without buying a copy. Is SQL Server Express Edition full compatible? I am guilty of not spending enough time looking at it. I use the backup and maintenance tools from https://ola.hallengren.com/ and it has made me lazy when it comes to keeping a close eye on my SQL server. 
man, that's needlessly cruel
 INSERT INTO gapiaudit_stage2 ( acreated , akey2 ) SELECT gapiaudit.acreated , gapiaudit.akey2 FROM gapiaudit INNER JOIN gapiaudit_stage1 ON gapiaudit_stage1.clientinfo = gapiaudit.akey2 AND gapiaudit_stage1.datereq &lt; gapiaudit.acreated AND gapiaudit_stage1.datereq2 &gt; gapiaudit.acreated 
But all I needed. It was indeed a syntax issue. I should have mentioned it before.
does your solution look anything like [this](https://www.reddit.com/r/SQL/comments/6v3kvb/ms_sql_help_with_a_query_3_tables_involved/dlxgfyb/)?
That is literally the same query as above except using a join instead of a where. I would personally use a join on the key, and the dates in the where. Guy was just asking about the syntax of inserting something... and I have no idea why you would insert, name the columns, then select the same columns instead of just using a simplified syntax. Either way, its the same thing. It's like the difference between a left join where null and an inner join.
Before trying to lear PL/SQL, it is necessary to learn basics of SQL like joins, select and you need to have good knowledge of RDBMS, as all SQL or database management is based on RDBS
Please try UPDATE CM SET filename = 'String' + filename FROM cm_steps CM WHERE ItemID IN (11667926, 11667979, 11667981, 11667997)
not quite, but nice try your syntax is **broken**... *even if* you remove the quotes from those strings and make them column references instead it *might* be almost equivalent if you actually mentioned the other table in your FROM clause tsk tsk
Since you were asking from a noob standpoint. If you are wanting to play around with sql and not install a database to do it. You might want to check this out: http://sqlfiddle.com/
I assumed he was using a shortened query as an example and trying to answer his actual question about inserting, not trying to get it to run. Which is what he said. I'm not going to rewrite something that someone isn't asking about. My point is that its the same concept structurally.
oh. my. hireafuckingnewbietowritesqlandthisiswhatyouget. god. the only asterisk i saw is here -- SELECT * FROM SPRTELE and that is perfectly okay
Maybe you need to check the column SFRSTCR_PIDM from which table it is and if it is existing.
How does this work? FROM SSBSECT INNER JOIN SCBCRSE ON SSBSECT_SUBJ_CODE = SCBCRSE_SUBJ_CODE AND SSBSECT_CRSE_NUMB = SCBCRSE_CRSE_NUMB INNER JOIN SFRSTCR ON SFRSTCR.SFRSTCR_TERM_CODE = SSBSECT.SSBSECT_TERM_CODE AND SFRSTCR.SFRSTCR_CRN = SSBSECT.SSBSECT_CRN INNER JOIN STVRSTS ON SFRSTCR.SFRSTCR_RSTS_CODE = STVRSTS.STVRSTS_CODE INNER JOIN SPRIDEN ON SFRSTCR.SFRSTCR_PIDM = SPRIDEN.SPRIDEN_PIDM INNER JOIN SGBSTDN ON SGBSTDN_PIDM = SFRSTCR.SFRSTCR_PIDM AND SGBSTDN_TERM_CODE_EFF &lt;= SSBSECT.SSBSECT_TERM_CODE INNER JOIN ( SELECT MAX(SGBSTDN_TERM_CODE_EFF) AS SGBSTDN_TERM_CODE_EFF , SGBSTDN_PIDM FROM SSBSECT INNER JOIN SFRSTCR ON SFRSTCR.SFRSTCR_TERM_CODE = SSBSECT.SSBSECT_TERM_CODE AND SFRSTCR.SFRSTCR_CRN = SSBSECT.SSBSECT_CRN INNER JOIN SGBSTDN ON SGBSTDN_PIDM = SFRSTCR.SFRSTCR_PIDM AND SGBSTDN_TERM_CODE_EFF &lt;= SSBSECT.SSBSECT_TERM_CODE WHERE SSBSECT.SSBSECT_TERM_CODE = '201750' AND SSBSECT.SSBSECT_SUBJ_CODE = 'PHY' GROUP BY SGBSTDN_PIDM ) xSGBSTDN ON xSGBSTDN.SGBSTDN_TERM_CODE_EFF = SGBSTDN.SGBSTDN_TERM_CODE_EFF AND xSGBSTDN.SGBSTDN_PIDM = SGBSTDN.SGBSTDN_PIDM INNER JOIN SPBPERS ON SPBPERS.SPBPERS_PIDM = SFRSTCR.SFRSTCR_PIDM LEFT JOIN STVETHN ON STVETHN.STVETHN_CODE = SPBPERS.SPBPERS_ETHN_CODE LEFT JOIN GORVISA ON GORVISA.GORVISA_PIDM = SFRSTCR.SFRSTCR_PIDM LEFT JOIN SPRADDR ON SPRADDR.SPRADDR_PIDM = SFRSTCR.SFRSTCR_PIDM AND SPRADDR.SPRADDR_ATYP_CODE = 'AA' AND SPRADDR.SPRADDR_TO_DATE IS NULL AND SPRADDR.SPRADDR_STATUS_IND IS NULL LEFT JOIN ( SELECT spraddr.SPRADDR_PIDM AS xspraddr_pidm , MAX(spraddr.SPRADDR_SEQNO) AS xspraddr_seqno FROM spraddr WHERE spraddr.SPRADDR_ATYP_CODE = 'AA' AND spraddr.SPRADDR_TO_DATE IS NULL AND spraddr.SPRADDR_STATUS_IND IS NULL GROUP BY spraddr.SPRADDR_PIDM ) xSPRADDR ON xSPRADDR.xspraddr_pidm = SPRADDR.SPRADDR_PIDM AND xSPRADDR.xspraddr_seqno = SPRADDR.SPRADDR_SEQNO LEFT JOIN ( SELECT SPRTELE.SPRTELE_PIDM , SPRTELE.SPRTELE_TELE_CODE , SPRTELE.SPRTELE_STATUS_IND FROM SPRTELE INNER JOIN ( SELECT MAX(SPRTELE.SPRTELE_SEQNO) AS SPRTELE_SEQNO , SPRTELE.SPRTELE_PIDM AS SPRTELE_PIDM FROM SPRTELE WHERE SPRTELE.SPRTELE_TELE_CODE IN ('CE', 'AA') AND ( SPRTELE.SPRTELE_STATUS_IND = 'A' OR SPRTELE.SPRTELE_STATUS_IND Is Null ) GROUP BY SPRTELE.SPRTELE_PIDM ) mSPRTELE ON mSPRTELE.SPRTELE_PIDM = SPRTELE.SPRTELE_PIDM AND mSPRTELE.SPRTELE_SEQNO = SPRTELE.SPRTELE_SEQNO ) xSPRTELE ON xSPRTELE.SPRTELE_PIDM = SFRSTCR_PIDM AND xSPRTELE.SPRTELE_TELE_CODE IN ('CE', 'AA') AND ( xSPRTELE.SPRTELE_STATUS_IND = 'A' OR xSPRTELE.SPRTELE_STATUS_IND Is Null ) I'm assuming this is going to take forever to run and would advise completely rewriting it.
SELEFT f.flight_number, f.to_airport, f.scheduled_arrival_time from flight where to_airport='SXF' scheduled_arrival_time=CURDATE() -- should work for the first solution
1. You have the Flight table, which contains your FROM_AIRPORT &amp; TO_AIRPORT columns. You have a reference table AIRPORT which you can join FROM_AIRPORT to in order to grab the country. The FLIGHT_INSTANCE has your actual arrival time but you use the context clue that there may not be a record in there if it hasn't arrived yet so you want to use an OUTER type join instead of the usual INNER join. And then you can use your ORDER BY clause to sort the data. 2. Like in the first example you need to use the TO &amp; FROM AIRPORT fields in joins to get the countries so you can filter them in a WHERE clause. Same idea with linking Flight to Flight_Instance as before. The new piece is Seat_Reservation - the join here to Flight_Instance will be on 2 fields instead of 1. 3. This one shouldn't be too much of a leap in complexity from the last query, but it does test knowledge of set operators (e.g. UNION, INTERSECT, etc.) You would want to work your way from the Customer table to the Flight table using the Relationship Diagram you posted. The end product would be 2 data sets (one of London, the other of Paris) that you need to find members belonging to both.
It looks like you have some smart quotes in there around `Opp Deck`, which are causing the syntax error. Barring any other errors, replacing the smart quotes with normal single-quotes should fix that.
Oh wow, good eye. No idea how that happened and I would've had no idea to look for it. Thanks!
Well, I guess that was only halfway there. Now I'm seeing: ERROR 1054 (42S22): Unknown column 'decks.id' in 'on clause' ...even though the decks.id on clause works fine in the 2nd query. The only difference is that I'm using AS.
Thanks for the reply. Could you elaborate on the Joins a bit more? Specifically which comes after ON keyword. I still dont quite get which key/common field you're using to make the joins?
[TechOnTheNet](https://www.techonthenet.com/mysql/joins.php) is a great resource you might want to check out. https://www.techonthenet.com/mysql/joins.php
The table "decks" appears twice in the newer query, so you need to replace all references to it with the matching alias. So decks.id should either be ud.id or od.id depending on which table reference you're trying to use. Same thing in the group by and order by clauses.
That makes perfect sense and worked perfectly. Thanks!
check out my blog: http://bi-solutions.gaussling.com/inner-joins-explained/ http://bi-solutions.gaussling.com/outer-joins-explained/
The ones with the F and P in your schema are the Foreign and Primary keys. Those will almost always be your joins.
There's not a lot of SQL going on in that article. Are you confusing SQL (a language) with SQL Server (a database management system)?
If you're joining 2 tables together, it's very common that the Primary Key (PK) of one table will be used. You then have to use the corresponding field(s) from the other table. I'll use 2 examples here: 1. Joining Flight to Flight_Instance. You notice Flight_Instance has a PK of Flight_Number &amp; Flight_Date. Well, Flight doesn't have a Flight_Date column on it since it's just the general Flight itinerary. So in this case your join may look like... SELECT * FROM Flight f INNER JOIN Flight_Instance fi ON f.Flight_Number = fi.Flight_Number; 2. Joining Flight to Airport. The Schema you posted indicates there's a relationship. You know that Airport has a PK of Airport_Code. Now you look for the related component from Flight - From &amp; To Airport are the only candidates for a join that make any sense so we use those. The below query would grab the details of a flight &amp; its airport it departs from. You can swap out TO_AIRPORT to get the destination airport details. SELECT * FROM Flight f INNER JOIN Airport dep ON f.From_Airport = dep.Airport_Code Hope this was able to help give some clarity on the general methodology.
Banner programmer?
 SELECT * FROM #Transaction ORDER BY InvoiceID ASC, PostingDate ASC, CASE WHEN Reversal = 0 THEN SequenceID END ASC, CASE WHEN Reversal = 1 THEN SequenceID END DESC Alternatively, `order by [Correct Sequence]`, duh!
SequenceID is the same for different reversals on the same day, which could cause this not to work. You could fix it by doing something like this: SELECT * FROM #Transaction ORDER BY InvoiceID ASC, PostingDate ASC, (SequenceID + CASE WHEN Reversal = 1 THEN 0.1 ELSE 0.0 END) That way if you end up with something like the top table SequenceID = 3 on the same day, it will order them after each other correctly. Edit - I should also add, doing the reverse ordering would probably be suited for a sub-query joined in.
&amp;nbsp; Unfortunatly, this query does not sequence the transactions correctly. &amp;nbsp; Clearly [Correct Sequence], is not in the dataset, duh! &amp;nbsp;
Oh, I see what you're saying, I just looked at the first data set, but I failed on the second data set. Corrected: SELECT * FROM #Transaction ORDER BY InvoiceID ASC, PostingDate ASC, Reversal ASC, CASE WHEN Reversal = 0 THEN SequenceID END ASC, CASE WHEN Reversal = 1 THEN SequenceID END DESC
I didn't realize I failed the second dataset, but yours failed the first dataset.
&amp;nbsp; Now it fails on the first dataset. I've added a few notes to the dataset to explain what is happening. &amp;nbsp; I'm preparing myself for the answer that this is not doable with the given data I have access to. &amp;nbsp; 
Ah, gotchu fam, I see what you're saying, I got this, 1 sec, but the query size is about to explode, should still be efficient and scalable
Is it fair to say that the only time there are multiple postings on the same day and invoice is when a reversal has occurred?
No, there can be multiple postings on the same day without any reversals.
 SELECT t.* FROM #Transaction t LEFT JOIN ( SELECT invoiceID, MAX(sequenceID) AS maxReversal, PostingDate FROM #Transaction WHERE Reversal = 1 GROUP BY InvoiceID, PostingDate) AS maxT ON maxT.InvoiceID = t.InvoiceID AND maxT.PostingDate = t.PostingDate AND maxt.maxReversal &gt;= t.SequenceID ORDER BY InvoiceID ASC, PostingDate ASC, CASE WHEN t.Reversal = 0 AND t.SequenceID &lt;= maxT.maxReversal THEN t.Reversal END DESC, CASE WHEN Reversal = 0 THEN SequenceID END ASC, CASE WHEN Reversal = 1 THEN SequenceID END DESC, CASE WHEN maxT.maxReversal IS null THEN SequenceID END ASC Cool, I'm pretty confident that matches your given output, but you may have a data scenario in the full DB where this doesn't match. If you do, post the exceptional data, and I'll see how I can hammer a rule to fix it.
Not sure how to get to in SAS specifically, but just using SQL you would get the list of items having count &gt; 283 and then do your deletes based on the item list. In T-SQL you would get something like this delete t from itemTable t where itemName in (select distinct itemName from itemTable group by itemName having count(itemName) &gt; 283)
proc sql; create table t1(drop=c) as select *, count(name) as c from t group by name having c &gt;4 ; quit;
Just use a # temp table in place of your CTE.
I think you mean &lt; 283 not &gt; 283, but otherwise it should work well.
select flight_number, country from table1 where actual_arrival_time &gt;= scheduled_arrival_time and actual_arrival_time is not null 
I assume the `WITH CTE` and `select values` syntax are the issue. Try this instead: select MODIFIEDDATETIME = (select max(modified) from ( select (dateadd(second, cij.MODIFIEDTIME, cij.MODIFIEDDATE)) as modified union all select (dateadd(second, cn1.MODIFIEDTIME, cn1.MODIFIEDDATE)) as modified union all select (dateadd(second, st1.MODIFIEDTIME, st1.MODIFIEDDATE)) as modified union all select (dateadd(second, csg.MODIFIEDTIME, csg.MODIFIEDDATE)) as modified union all select (dateadd(second, slo.MODIFIEDTIME, slo.MODIFIEDDATE)) as modified union all select (dateadd(second, dsc.MODIFIEDTIME, dsc.MODIFIEDDATE)) as modified union all select (dateadd(second, cms.MODIFIEDTIME, cms.MODIFIEDDATE)) as modified) AS x) from dbo.HeaderTable cij left join dbo.COUNTY cn1 on cn1.COUNTYID = cij.COUNTYID and cn1.COMPANYID = cij.COMPANYID and cn1.STATEID = cij.STATEID left join dbo.STATETABLE st1 on st1.STATEID = cij.STATEID and st1.COMPANYID = cij.COMPANYID and st1.COUNTRYID = cij.DLVCOUNTRY left join dbo.CUSTGROUPTABLE csg on csg.CUSTGROUP = cij.CUSTGROUP and csg.COMPANYID = cij.COMPANYID left join dbo.SALESORIGINTABLE slo on slo.ORIGINID = cij.SALESORIGINID and slo.COMPANYID = cij.COMPANYID left join dbo.DESTINATIONCODETABLE dsc on dsc.DESTINATIONCODEID = cij.DESTINATIONCODEID and dsc.COMPANYID = cij.COMPANYID left join dbo.COMMISSIONSALESGROUPTABLE cms on cms.GROUPID = cij.SALESGROUP and cms.COMPANYID = cij.COMPANYID;
This uses a Table valued constructor which isn't supported until after SQL Server 2000, but thanks for trying.
 SELECT t.InvoiceID , t.SequenceID , t.Reversal , t.PostingDate , t.TransactionAmount , t.SequenceID + CASE WHEN t.Reversal = 1 THEN -1 + ROW_NUMBER() OVER (PARTITION BY t.InvoiceID, t.PostingDate, t.Reversal ORDER BY -t.SequenceID) ELSE 0 END AS Segment FROM #Transaction AS t ORDER BY t.InvoiceID , t.PostingDate , Segment , t.Reversal , CASE WHEN t.Reversal = 1 THEN -t.SequenceID ELSE t.SequenceID END;
Having trouble understanding this.
&gt; delete **t** from itemTable **t** where itemName in Are your lowercase **t**s supposed to be asterixes? 
I guess you could CASE out all the modified dates to pick the max. I'm not going to type all that out though.
You're likely to get better responses if you take the time to format your query so that it is readable. I'm guessing that there is a column in Agent_Skill_Group_Interval that has the same name as another column in your selected values. Since you're selecting Agent_Skill_Group_Interval.* (after the UNION ALL), the default name for the selected value (the column name) is conflicting with the other column of the same name. The way to resolve this is to select only the columns that you need from Agent_Skill_Group_Interval and give each an alias.
It looks like you need to turn rows into columns. If that is the case, then [PIVOT](https://stackoverflow.com/questions/15931607/convert-rows-to-columns-using-pivot-in-sql-server) is your friend. 
Properly formatted below. The query you pasted is, as you said, only a part of the entire query. You didn't paste the entire query that is part of the subquery aliased as "ASGI". It would likely be best to paste the entire query. From what we have, nothing seems inherently wrong. FROM ( Select Agent_Skill_Group_Interval.* , SGPeripheralID = Skill_Group.PeripheralID , SGEnterpriseName = Skill_Group.EnterpriseName , SGSkillTargetID = Skill_Group.SkillTargetID , Media = Media_Routing_Domain.EnterpriseName , MediaID = Media_Routing_Domain.MRDomainID FROM Skill_Group(nolock) , Agent_Skill_Group_Interval(nolock) , Media_Routing_Domain(nolock) WHERE 1 = 1 AND Skill_Group.SkillTargetID = Agent_Skill_Group_Interval.SkillGroupSkillTargetID AND Skill_Group.MRDomainID = Media_Routing_Domain.MRDomainID AND ( Skill_Group.SkillTargetID NOT IN ( SELECT BaseSkillTargetID FROM Skill_Group (nolock) WHERE 1 = 1 AND Priority &gt; 0 AND Deleted &lt;&gt; 'Y' ) ) UNION ALL Select Agent_Skill_Group_Interval.* , SGPeripheralID = Skill_Group.PeripheralID , SGEnterpriseName = Precision_Queue.EnterpriseName , SGSkillTargetID = Skill_Group.SkillTargetID , Media = Media_Routing_Domain.EnterpriseName , MediaID = Media_Routing_Domain.MRDomainID FROM Skill_Group (nolock) , Agent_Skill_Group_Interval(nolock) , Media_Routing_Domain(nolock) , Precision_Queue(nolock) WHERE 1 = 1 AND Skill_Group.PrecisionQueueID = Agent_Skill_Group_Interval.PrecisionQueueID AND Skill_Group.PrecisionQueueID = Precision_Queue.PrecisionQueueID AND Skill_Group.MRDomainID = Media_Routing_Domain.MRDomainID ) A GROUP BY A.Media , A.MediaID , A.SkillTargetID , CONVERT(char(10), A.DateTime,101) , A.SGPeripheralID ) ASGI WHERE 1 = 1 AND Agent.SkillTargetID = ASGI.SkillTargetID AND Agent.PersonID = Person.PersonID AND Agent.SkillTargetID = AI.SkillTargetID AND Agent.PeripheralID = ASGI.SGPeripheralID AND CONVERT(char (10), ASGI.DateTime,101) = CONVERT(char(10), AI.DateTime,101) AND AI.MRDomainID = ASGI.MediaID
I thought about trying PIVOT, but I am not actually aggregating anything.
Just use an aggregating function that will return the input. See https://stackoverflow.com/questions/14618316/how-to-create-a-pivot-query-in-sql-server-without-aggregate-function for an explanation.
t is the table alias that he gave to the table "itemTable"
I can use MAX, but there are occasionally different dates for the code based on ID, so it will not work.
Thanks for the help and the responses. Below is the full query (it's lengthy that's why I originally didn't post everything) I called out where the error is occurring (line 163 about 3/4 of the way through) 
Part 2: UNION ALL Select Agent_Skill_Group_Interval.*, SGPeripheralID = Skill_Group.PeripheralID, SGEnterpriseName = Precision_Queue.EnterpriseName, SGSkillTargetID = Skill_Group.SkillTargetID, Media = Media_Routing_Domain.EnterpriseName, MediaID = Media_Routing_Domain.MRDomainID FROM Skill_Group (nolock), Agent_Skill_Group_Interval(nolock), Media_Routing_Domain(nolock), Precision_Queue(nolock) WHERE Skill_Group.PrecisionQueueID = Agent_Skill_Group_Interval.PrecisionQueueID AND Skill_Group.PrecisionQueueID = Precision_Queue.PrecisionQueueID AND Skill_Group.MRDomainID = Media_Routing_Domain.MRDomainID)A GROUP BY A.Media, A.MediaID, A.SkillTargetID, CONVERT(char(10),A.DateTime,101), --Error Occurs on line below A.SGPeripheralID) ASGI WHERE Agent.SkillTargetID = ASGI.SkillTargetID AND Agent.PersonID = Person.PersonID AND Agent.SkillTargetID = AI.SkillTargetID AND Agent.PeripheralID = ASGI.SGPeripheralID AND CONVERT(char (10),ASGI.DateTime,101) = CONVERT(char(10),AI.DateTime,101) AND AI.MRDomainID = ASGI.MediaID AND Agent.SkillTargetID in (5187,6004,5879,5739,5878,5377,5080,5659,5789,5642,5652,5653,5744,5915,5740,5553,5973,5738,5742,6003,5795,5877,6001,5790,5746,5188,5082,5089,5741,5091,5793,6002,5794,5947,5660,5379,5380,6129,5186,5792,5658,5708,5468,5378,5918,5381,5105,5791,5875) AND CONVERT(char(10),AI.DateTime,101) &gt;= '2017-08-10' AND CONVERT(char(10),AI.DateTime,101) &lt; '2017-08-12' AND CONVERT (varchar(5),AI.DateTime,108) between '07:00' and '20:00' GROUP BY Agent.SkillTargetID, ASGI.Media, Person.LastName, Person.FirstName, CONVERT(char(10),AI.DateTime,101) ORDER BY Person.LastName + ',' + Person.FirstName, ASGI.Media, Agent.SkillTargetID
 SET ARITHABORT OFF SET ANSI_WARNINGS OFF SET NOCOUNT ON SELECT Media = ASGI.Media, Date = CONVERT(char(10),AI.DateTime,101), FullName = Person.LastName + ',' + Person.FirstName, AgentSkillID = Agent.SkillTargetID, AgentLoggedOnTime = SUM(CAST( AI.LoggedOnTime as decimal (10,2))/86400), AgentAvailTime = SUM(CAST( AI.AvailTime as decimal (10,2))/86400), AgentNotReady = SUM(CAST( AI.NotReadyTime as decimal (10,2))/86400), AgentBusyOtherTime = SUM(CAST((AI.LoggedOnTime- AI.AvailTime) as decimal (10,2))/86400), --Need to 're-sum' in order to make SQL think these are aggregate functions CallsAnswered = sum(ASGI.CallsAnswered), CallsHandled = sum(ASGI.CallsHandled), AbandRingCalls = sum(ASGI.AbandRingCalls), AbandRingCallsTime = sum(ASGI.AbandRingCallsTime), RedirectCalls = sum(ASGI.RedirectCalls), RedirectCallsTime = sum(ASGI.RedirectCallsTime), AbandonHoldCalls = sum(ASGI.AbandonHoldCalls), TransferInCalls = sum(ASGI.TransferInCalls), TransferOutCalls = sum(ASGI.TransferOutCalls), ConsultativeCalls = sum(ASGI.ConsultativeCalls), ConferenceInCalls = sum(ASGI.ConferenceInCalls), ConferenceOutCalls = sum(ASGI.ConferenceOutCalls), OutExtnCalls = sum(ASGI.OutExtnCalls), AgentOutCallsOnHoldTime = SUM(CAST( ASGI.AgentOutCallsOnHoldTime as decimal (10,2))/86400), InCallsOnHold = sum(ASGI.InCallsOnHold), InCallsOnHoldTime = SUM(CAST( ASGI.InCallsOnHoldTime as decimal (10,2))/86400), IntCallsOnHold = sum(ASGI.IntCallsOnHold), IntCallsOnHoldTime = SUM(CAST( ASGI.IntCallsOnHoldTime as decimal (10,2))/86400), TalkTime = SUM(CAST( ASGI.TalkTime as decimal (10,2))/86400), HandledCallsTime = SUM(CAST( ASGI.HandledCallsTime as decimal (10,2))/86400), HoldTime = SUM(CAST( ASGI.HoldTime as decimal (10,2))/86400), -- AvailTime = sum(ASGI.AvailTime), --NotReadyTime = sum(ASGI.NotReadyTime), ReservedTime = SUM(CAST( ASGI.ReservedTime as decimal (10,2))/86400), WrapTime = SUM(CAST( ASGI.WrapTime as decimal (10,2))/86400), -- BusyOtherTime = sum(ASGI.BusyOtherTime), AnswerWaitTime = SUM(CAST( ASGI.AnswerWaitTime as decimal (10,2))/86400), AgentOutCallsTime= SUM(CAST( ASGI.AgentOutCallsTime as decimal (10,2))/86400), AgentOutCallsTalkTime = SUM(CAST( ASGI.AgentOutCallsTalkTime as decimal (10,2))/86400), AgentTerminatedCalls = sum(ASGI.AgentTerminatedCalls), ConsultativeCallsTime = sum(ASGI.ConsultativeCallsTime), ConferencedInCallsTime = sum(ASGI.ConferencedInCallsTime), ConferencedOutCallsTime = sum(ASGI.ConferencedOutCallsTime), HandledCallsTalkTime = SUM(CAST( ASGI.HandledCallsTalkTime as decimal (10,2))/86400), InternalCallsRcvd = sum(ASGI.InternalCallsRcvd), InternalCallsRcvdTime = SUM(CAST( ASGI.InternalCallsRcvdTime as decimal (10,2))/86400), InternalCalls = sum(ASGI.InternalCalls), InternalCallsTime = SUM(CAST( ASGI.InternalCallsTime as decimal (10,2))/86400), TransferredInCallsTime = SUM(CAST( ASGI.TransferredInCallsTime as decimal (10,2))/86400), TalkOtherTime = SUM(CAST( ASGI.TalkOtherTime as decimal (10,2))/86400), TalkOutTime = SUM(CAST( ASGI.TalkOutTime as decimal (10,2))/86400), InterruptedTime = SUM(CAST( ASGI.InterruptedTime as decimal (10,2))/86400), WorkNotReadyTime = SUM(CAST( ASGI.WorkNotReadyTime as decimal (10,2))/86400), WorkReadyTime = SUM(CAST( ASGI.WorkReadyTime as decimal (10,2))/86400), NetConsultativeCalls = sum(ASGI.NetConsultativeCalls), NetConsultativeCallsTime = sum(ASGI.NetConsultativeCallsTime), NetConferencedOutCalls = sum(ASGI.NetConferencedOutCalls), NetConfOutCallsTime = sum(ASGI.NetConfOutCallsTime), NetTransferredOutCalls = sum(ASGI.NetTransferredOutCalls), Assists = SUM(ASGI.Assists), TransOut = SUM(ASGI.TransOut), AHT = ISNULL(SUM(ASGI.HandledCallsTime) / SUM(ASGI.CallsHandled),0), AHoldT = ISNULL(SUM(ASGI.InCallsOnHoldTime) / SUM(ASGI.InCallsOnHold),0), perActiveTime = SUM(ASGI.TalkTime) * 1.0 / SUM(ISNULL(AI.LoggedOnTime, 0)), perHoldTime = sum(ASGI.HoldTime) * 1.0 / SUM(ISNULL(AI.LoggedOnTime, 0)), perNotActive = ISNULL(SUM(AI.AvailTime) * 1.0 / SUM(AI.LoggedOnTime),0), perNotReady = ISNULL(SUM(AI.NotReadyTime) * 1.0 / SUM(AI.LoggedOnTime),0), perReserved = sum(ASGI.ReservedTime) * 1.0 / SUM(ISNULL(AI.LoggedOnTime, 0)), perWrapup = sum(ASGI.WrapTime) * 1.0 / SUM(ISNULL(AI.LoggedOnTime, 0)), perBusyOther = ISNULL(SUM(AI.LoggedOnTime - AI.AvailTime) * 1.0 / SUM(AI.LoggedOnTime),0), AACW = ISNULL((SUM(ASGI.WrapTime) / SUM(ASGI.CallsHandled)),0) FROM Agent (nolock), Agent_Interval AI (nolock), Person (nolock), --This nested Select statement is necessary in order to make AgentLoggedOnTime, AgentAvailTime, and AgentNotReadyTime work correctly (Select Media = A.Media, MediaID = A.MediaID, CONVERT(char(10),A.DateTime,101), A.SkillTargetID, A.SGPeripheralID, CallsAnswered = SUM(ISNULL(A.CallsAnswered,0)), CallsHandled = SUM(ISNULL(A.CallsHandled,0)), AbandRingCalls = SUM(ISNULL(A.AbandonRingCalls,0)), AbandRingCallsTime = SUM(ISNULL(A.AbandonRingTime,0)), RedirectCalls = SUM(ISNULL(A.RedirectNoAnsCalls,0)), RedirectCallsTime = SUM(ISNULL(A.RedirectNoAnsCallsTime,0)), AbandonHoldCalls = SUM(ISNULL(A.AbandonHoldCalls,0)), TransferInCalls = SUM(ISNULL(A.TransferredInCalls,0)), TransferOutCalls = SUM(ISNULL(A.TransferredOutCalls,0)), ConsultativeCalls = SUM(ISNULL(A.ConsultativeCalls,0)), ConferenceInCalls = SUM(ISNULL(A.ConferencedInCalls,0)), ConferenceOutCalls = SUM(ISNULL(A.ConferencedOutCalls,0)), OutExtnCalls = SUM(ISNULL(A.AgentOutCalls,0)), ShortCalls = SUM(ISNULL(A.ShortCalls,0)), SupAssistCalls = SUM(ISNULL(A.SupervAssistCalls,0)), AgentOutCallsOnHoldTime = SUM(ISNULL(A.AgentOutCallsOnHoldTime,0)), InCallsOnHold = SUM(ISNULL(A.IncomingCallsOnHold,0)), InCallsOnHoldTime = SUM(ISNULL(A.IncomingCallsOnHoldTime,0)), IntCallsOnHold = SUM(ISNULL(A.InternalCallsOnHold,0)), IntCallsOnHoldTime = SUM(ISNULL(A.InternalCallsOnHoldTime,0)), TalkTime = sum(isnull(A.TalkInTime,0)) + sum(isnull(A.TalkOutTime,0)) + sum(isnull(A.TalkOtherTime,0)) + sum(isnull(A.TalkAutoOutTime,0)) + sum(isnull(A.TalkPreviewTime,0)) + sum(isnull(A.TalkReserveTime,0)), HandledCallsTime = SUM(ISNULL(A.HandledCallsTime,0)), HoldTime = SUM(ISNULL(A.HoldTime,0)), -- AvailTime = SUM(ISNULL(A.AvailTime,0)), -- NotReadyTime = SUM(ISNULL(A.NotReadyTime,0)), ReservedTime = SUM(ISNULL(A.ReservedStateTime,0)), WrapTime = SUM(ISNULL(A.WorkNotReadyTime + A.WorkReadyTime,0)), -- BusyOtherTime = SUM(ISNULL(A.BusyOtherTime,0)), AnswerWaitTime = SUM(ISNULL(A.AnswerWaitTime,0)), AgentOutCallsTime = SUM(ISNULL(A.AgentOutCallsTime,0)), AgentOutCallsTalkTime = SUM(ISNULL(A.AgentOutCallsTalkTime,0)), AgentTerminatedCalls = SUM(ISNULL(A.AgentTerminatedCalls,0)), ConsultativeCallsTime = SUM(ISNULL(A.ConsultativeCallsTime,0)), ConferencedInCallsTime = SUM(ISNULL(A.ConferencedInCallsTime,0)), ConferencedOutCallsTime = SUM(ISNULL(A.ConferencedOutCallsTime,0)), HandledCallsTalkTime = SUM(ISNULL(A.HandledCallsTalkTime,0)), InternalCallsRcvd = SUM(ISNULL(A.InternalCallsRcvd,0)), InternalCallsRcvdTime = SUM(ISNULL(A.InternalCallsRcvdTime,0)), InternalCalls = SUM(ISNULL(A.InternalCalls,0)), InternalCallsTime = SUM(ISNULL(A.InternalCallsTime,0)), TransferredInCallsTime = SUM(ISNULL(A.TransferredInCallsTime,0)), TalkOtherTime = SUM(ISNULL(A.TalkOtherTime,0)), TalkOutTime = SUM(ISNULL(A.TalkOutTime,0)), InterruptedTime = SUM(ISNULL(A.InterruptedTime,0)), WorkNotReadyTime = SUM(ISNULL(A.WorkNotReadyTime,0)), WorkReadyTime = SUM(ISNULL(A.WorkReadyTime,0)), NetConsultativeCalls = SUM(ISNULL(A.NetConsultativeCalls,0)), NetConsultativeCallsTime = SUM(ISNULL(A.NetConsultativeCallsTime,0)), NetConferencedOutCalls = SUM(ISNULL(A.NetConferencedOutCalls,0)), NetConfOutCallsTime = SUM(ISNULL(A.NetConfOutCallsTime,0)), NetTransferredOutCalls = SUM(ISNULL(A.NetTransferredOutCalls,0)), TransOut = SUM(ISNULL(A.TransferredOutCalls, 0) + ISNULL(A.NetTransferredOutCalls, 0)), Assists = SUM(ISNULL(A.EmergencyAssists, 0) + ISNULL(A.SupervAssistCallsTime, 0)) FROM (Select Agent_Skill_Group_Interval.*, SGPeripheralID = Skill_Group.PeripheralID, SGEnterpriseName = Skill_Group.EnterpriseName, SGSkillTargetID = Skill_Group.SkillTargetID, Media = Media_Routing_Domain.EnterpriseName, MediaID = Media_Routing_Domain.MRDomainID FROM Skill_Group(nolock), Agent_Skill_Group_Interval(nolock), Media_Routing_Domain(nolock) WHERE Skill_Group.SkillTargetID = Agent_Skill_Group_Interval.SkillGroupSkillTargetID AND Skill_Group.MRDomainID = Media_Routing_Domain.MRDomainID AND (Skill_Group.SkillTargetID NOT IN (SELECT BaseSkillTargetID FROM Skill_Group (nolock) WHERE (Priority &gt; 0) AND (Deleted &lt;&gt; 'Y')))
&amp;nbsp; This is great! I can't find anywhere where it is failing to sequence the transactions correctly. &amp;nbsp; 
Not sure if this makes a difference as well, but I'm working off MS SQL Server 2008 R2. 
Probably cause it's horribly written and not even functional. He's * Grouping by the value he's counting which will always result in a count of 1 * Referring to the alias instead of the column name in the HAVING clause, which to my knowledge won't compile * Selecting columns that aren't being aggregated or grouped by, which also won't compile * Using the wrong number (c &gt; 4) instead of (c &gt;= 283) * I've never seen that create table syntax/proc sql, but I'm not familiar with SAS
In the nested select you have `CONVERT(char(10),A.DateTime,101),` with no alias specified
Really?
/u/notasqlstar here's a great example of why I don't give people the answer when they don't do their research. :)
Here's your fix. Line 85 was not properly aliased after using the CONVERT function. It was then referenced in line 203, and didn't exist. https://pastebin.com/2w0ukMsj If this doesn't fix it, it's because the column DateTime from your Agent_Interval table doesn't exist. That's the only other reference to a column named 'DateTime'.
Shit, this is a fun question to answer.
I use Aqua Data a lot, 90% for Informix and I use semi colons with no problem. 
You're a genius. That worked! Follow up question, even though I have a date limit in the eventual where clause I'm getting a massive amount of data back. Am I missing something? 
http://sqlfiddle.com/#!6/412c9/22 I tried left self join and it seems to work. 
Yup. it's pretty gross, but this is the answer. select MODIFIEDDATETIME = case when dateadd(second, coalesce(chd.MODIFIEDTIME, ''), coalesce(chd.MODIFIEDDATE, '')) &gt; dateadd(second, coalesce(cms.MODIFIEDTIME, ''), coalesce(cms.MODIFIEDDATE, '')) and dateadd(second, coalesce(chd.MODIFIEDTIME, ''), coalesce(chd.MODIFIEDDATE, '')) &gt; dateadd(second, coalesce(cpm.MODIFIEDTIME, ''), coalesce(cpm.MODIFIEDDATE, '')) and dateadd(second, coalesce(chd.MODIFIEDTIME, ''), coalesce(chd.MODIFIEDDATE, '')) &gt; dateadd(second, coalesce(csg.MODIFIEDTIME, ''), coalesce(csg.MODIFIEDDATE, '')) and dateadd(second, coalesce(chd.MODIFIEDTIME, ''), coalesce(chd.MODIFIEDDATE, '')) &gt; dateadd(second, coalesce(cst.MODIFIEDTIME, ''), coalesce(cst.MODIFIEDDATE, '')) and dateadd(second, coalesce(chd.MODIFIEDTIME, ''), coalesce(chd.MODIFIEDDATE, '')) &gt; dateadd(second, coalesce(ctg.MODIFIEDTIME, ''), coalesce(ctg.MODIFIEDDATE, '')) and dateadd(second, coalesce(chd.MODIFIEDTIME, ''), coalesce(chd.MODIFIEDDATE, '')) &gt; dateadd(second, coalesce(dim.MODIFIEDTIME, ''), coalesce(dim.MODIFIEDDATE, '')) and dateadd(second, coalesce(chd.MODIFIEDTIME, ''), coalesce(chd.MODIFIEDDATE, '')) &gt; dateadd(second, coalesce(dlm.MODIFIEDTIME, ''), coalesce(dlm.MODIFIEDDATE, '')) and dateadd(second, coalesce(chd.MODIFIEDTIME, ''), coalesce(chd.MODIFIEDDATE, '')) &gt; dateadd(second, coalesce(dlt.MODIFIEDTIME, ''), coalesce(dlt.MODIFIEDDATE, '')) and dateadd(second, coalesce(chd.MODIFIEDTIME, ''), coalesce(chd.MODIFIEDDATE, '')) &gt; dateadd(second, coalesce(dnc.MODIFIEDTIME, ''), coalesce(dnc.MODIFIEDDATE, '')) and dateadd(second, coalesce(chd.MODIFIEDTIME, ''), coalesce(chd.MODIFIEDDATE, '')) &gt; dateadd(second, coalesce(itl.MODIFIEDTIME, ''), coalesce(itl.MODIFIEDDATE, '')) and dateadd(second, coalesce(chd.MODIFIEDTIME, ''), coalesce(chd.MODIFIEDDATE, '')) &gt; dateadd(second, coalesce(pdg.MODIFIEDTIME, ''), coalesce(pdg.MODIFIEDDATE, '')) and dateadd(second, coalesce(chd.MODIFIEDTIME, ''), coalesce(chd.MODIFIEDDATE, '')) &gt; dateadd(second, coalesce(pyt.MODIFIEDTIME, ''), coalesce(pyt.MODIFIEDDATE, '')) and dateadd(second, coalesce(chd.MODIFIEDTIME, ''), coalesce(chd.MODIFIEDDATE, '')) &gt; dateadd(second, coalesce(ssp.MODIFIEDTIME, ''), coalesce(ssp.MODIFIEDDATE, '')) and dateadd(second, coalesce(chd.MODIFIEDTIME, ''), coalesce(chd.MODIFIEDDATE, '')) &gt; dateadd(second, coalesce(tgh.MODIFIEDTIME, ''), coalesce(tgh.MODIFIEDDATE, '')) and dateadd(second, coalesce(chd.MODIFIEDTIME, ''), coalesce(chd.MODIFIEDDATE, '')) &gt; dateadd(second, coalesce(vig.MODIFIEDTIME, ''), coalesce(vig.MODIFIEDDATE, '')) then dateadd(second, coalesce(chd.MODIFIEDTIME, ''), coalesce(chd.MODIFIEDDATE, '')) else case when dateadd(second, coalesce(cms.MODIFIEDTIME, ''), coalesce(cms.MODIFIEDDATE, '')) &gt; dateadd(second, coalesce(chd.MODIFIEDTIME, ''), coalesce(chd.MODIFIEDDATE, '')) and dateadd(second, coalesce(cms.MODIFIEDTIME, ''), coalesce(cms.MODIFIEDDATE, '')) &gt; dateadd(second, coalesce(cpm.MODIFIEDTIME, ''), coalesce(cpm.MODIFIEDDATE, '')) and dateadd(second, coalesce(cms.MODIFIEDTIME, ''), coalesce(cms.MODIFIEDDATE, '')) &gt; dateadd(second, coalesce(csg.MODIFIEDTIME, ''), coalesce(csg.MODIFIEDDATE, '')) and dateadd(second, coalesce(cms.MODIFIEDTIME, ''), coalesce(cms.MODIFIEDDATE, '')) &gt; dateadd(second, coalesce(cst.MODIFIEDTIME, ''), coalesce(cst.MODIFIEDDATE, '')) and dateadd(second, coalesce(cms.MODIFIEDTIME, ''), coalesce(cms.MODIFIEDDATE, '')) &gt; dateadd(second, coalesce(ctg.MODIFIEDTIME, ''), coalesce(ctg.MODIFIEDDATE, '')) and dateadd(second, coalesce(cms.MODIFIEDTIME, ''), coalesce(cms.MODIFIEDDATE, '')) &gt; dateadd(second, coalesce(dim.MODIFIEDTIME, ''), coalesce(dim.MODIFIEDDATE, '')) and dateadd(second, coalesce(cms.MODIFIEDTIME, ''), coalesce(cms.MODIFIEDDATE, '')) &gt; dateadd(second, coalesce(dlm.MODIFIEDTIME, ''), coalesce(dlm.MODIFIEDDATE, '')) and dateadd(second, coalesce(cms.MODIFIEDTIME, ''), coalesce(cms.MODIFIEDDATE, '')) &gt; dateadd(second, coalesce(dlt.MODIFIEDTIME, ''), coalesce(dlt.MODIFIEDDATE, '')) and dateadd(second, coalesce(cms.MODIFIEDTIME, ''), coalesce(cms.MODIFIEDDATE, '')) &gt; dateadd(second, coalesce(dnc.MODIFIEDTIME, ''), coalesce(dnc.MODIFIEDDATE, '')) and dateadd(second, coalesce(cms.MODIFIEDTIME, ''), coalesce(cms.MODIFIEDDATE, '')) &gt; dateadd(second, coalesce(itl.MODIFIEDTIME, ''), coalesce(itl.MODIFIEDDATE, '')) and dateadd(second, coalesce(cms.MODIFIEDTIME, ''), coalesce(cms.MODIFIEDDATE, '')) &gt; dateadd(second, coalesce(pdg.MODIFIEDTIME, ''), coalesce(pdg.MODIFIEDDATE, '')) and dateadd(second, coalesce(cms.MODIFIEDTIME, ''), coalesce(cms.MODIFIEDDATE, '')) &gt; dateadd(second, coalesce(pyt.MODIFIEDTIME, ''), coalesce(pyt.MODIFIEDDATE, '')) and dateadd(second, coalesce(cms.MODIFIEDTIME, ''), coalesce(cms.MODIFIEDDATE, '')) &gt; dateadd(second, coalesce(ssp.MODIFIEDTIME, ''), coalesce(ssp.MODIFIEDDATE, '')) and dateadd(second, coalesce(cms.MODIFIEDTIME, ''), coalesce(cms.MODIFIEDDATE, '')) &gt; dateadd(second, coalesce(tgh.MODIFIEDTIME, ''), coalesce(tgh.MODIFIEDDATE, '')) and dateadd(second, coalesce(cms.MODIFIEDTIME, ''), coalesce(cms.MODIFIEDDATE, '')) &gt; dateadd(second, coalesce(vig.MODIFIEDTIME, ''), coalesce(vig.MODIFIEDDATE, '')) then dateadd(second, coalesce(cms.MODIFIEDTIME, ''), coalesce(cms.MODIFIEDDATE, '')) else case when dateadd(second, coalesce(cpm.MODIFIEDTIME, ''), coalesce(cpm.MODIFIEDDATE, '')) &gt; dateadd(second, coalesce(chd.MODIFIEDTIME, ''), coalesce(chd.MODIFIEDDATE, '')) and dateadd(second, coalesce(cpm.MODIFIEDTIME, ''), coalesce(cpm.MODIFIEDDATE, '')) &gt; dateadd(second, coalesce(cms.MODIFIEDTIME, ''), coalesce(cms.MODIFIEDDATE, '')) and dateadd(second, coalesce(cpm.MODIFIEDTIME, ''), coalesce(cpm.MODIFIEDDATE, '')) &gt; dateadd(second, coalesce(csg.MODIFIEDTIME, ''), coalesce(csg.MODIFIEDDATE, '')) and dateadd(second, coalesce(cpm.MODIFIEDTIME, ''), coalesce(cpm.MODIFIEDDATE, '')) &gt; dateadd(second, coalesce(cst.MODIFIEDTIME, ''), coalesce(cst.MODIFIEDDATE, '')) and dateadd(second, coalesce(cpm.MODIFIEDTIME, ''), coalesce(cpm.MODIFIEDDATE, '')) &gt; dateadd(second, coalesce(ctg.MODIFIEDTIME, ''), coalesce(ctg.MODIFIEDDATE, '')) and dateadd(second, coalesce(cpm.MODIFIEDTIME, ''), coalesce(cpm.MODIFIEDDATE, '')) &gt; dateadd(second, coalesce(dim.MODIFIEDTIME, ''), coalesce(dim.MODIFIEDDATE, '')) and dateadd(second, coalesce(cpm.MODIFIEDTIME, ''), coalesce(cpm.MODIFIEDDATE, '')) &gt; dateadd(second, coalesce(dlm.MODIFIEDTIME, ''), coalesce(dlm.MODIFIEDDATE, '')) and dateadd(second, coalesce(cpm.MODIFIEDTIME, ''), coalesce(cpm.MODIFIEDDATE, '')) &gt; dateadd(second, coalesce(dlt.MODIFIEDTIME, ''), coalesce(dlt.MODIFIEDDATE, '')) and dateadd(second, coalesce(cpm.MODIFIEDTIME, ''), coalesce(cpm.MODIFIEDDATE, '')) &gt; dateadd(second, coalesce(dnc.MODIFIEDTIME, ''), coalesce(dnc.MODIFIEDDATE, '')) and dateadd(second, coalesce(cpm.MODIFIEDTIME, ''), coalesce(cpm.MODIFIEDDATE, '')) &gt; dateadd(second, coalesce(itl.MODIFIEDTIME, ''), coalesce(itl.MODIFIEDDATE, '')) and dateadd(second, coalesce(cpm.MODIFIEDTIME, ''), coalesce(cpm.MODIFIEDDATE, '')) &gt; dateadd(second, coalesce(pdg.MODIFIEDTIME, ''), coalesce(pdg.MODIFIEDDATE, '')) and dateadd(second, coalesce(cpm.MODIFIEDTIME, ''), coalesce(cpm.MODIFIEDDATE, '')) &gt; dateadd(second, coalesce(pyt.MODIFIEDTIME, ''), coalesce(pyt.MODIFIEDDATE, '')) and dateadd(second, coalesce(cpm.MODIFIEDTIME, ''), coalesce(cpm.MODIFIEDDATE, '')) &gt; dateadd(second, coalesce(ssp.MODIFIEDTIME, ''), coalesce(ssp.MODIFIEDDATE, '')) and dateadd(second, coalesce(cpm.MODIFIEDTIME, ''), coalesce(cpm.MODIFIEDDATE, '')) &gt; dateadd(second, coalesce(tgh.MODIFIEDTIME, ''), coalesce(tgh.MODIFIEDDATE, '')) and dateadd(second, coalesce(cpm.MODIFIEDTIME, ''), coalesce(cpm.MODIFIEDDATE, '')) &gt; dateadd(second, coalesce(vig.MODIFIEDTIME, ''), coalesce(vig.MODIFIEDDATE, '')) then dateadd(second, coalesce(cpm.MODIFIEDTIME, ''), coalesce(cpm.MODIFIEDDATE, '')) 
I haven't used Access in a few years, so things might have changed. But I wouldn't populate "date_input" in advance and leave the other columns null. Instead, I'd build a form, and set the date in the form's OnCurrent event or in the control's Enter or GotFocus events. (I think. I'm struggling to recall the terminology.)
Dump the database to text, and run grep.
&amp;nbsp; This query was close, but it did fail in the following case: &amp;nbsp; &amp;nbsp; InvoiceID|SequenceID|Reversal|PostingDate|TransactionAmount|Segment|Calculated Sequence|Correct Sequence| -:|-:|-:|-:|-:|-:|-:|-:| 1|1|0|6/26/2015|5|1|1|1| 1|2|0|7/26/2015|7|2|2|2| 1|3|0|11/7/2015|10|3|3|3| 1|3|1|8/16/2016|-10|3|4|4| 1|2|1|8/16/2016|-7|3|5|5| 1|4|0|8/16/2016|17|4|6|6| 1|1|1|8/19/2016|-5|2|7|8| 1|4|1|8/19/2016|-17|4|8|7| 
https://i.imgur.com/2YeDA_d.webp?maxwidth=640&amp;shape=thumb&amp;fidelity=high
 SELECT t.InvoiceID , t.SequenceID , t.Reversal , t.PostingDate , t.TransactionAmount , CASE WHEN t.Reversal = 1 THEN MAX(t.SequenceID) OVER (PARTITION BY t.InvoiceID, t.PostingDate, t.Reversal) ELSE t.SequenceID END AS Segment FROM #Transaction AS t ORDER BY t.InvoiceID , t.PostingDate , Segment , t.Reversal , CASE WHEN t.Reversal = 1 THEN -t.SequenceID ELSE t.SequenceID END;
I'm on mobile now so it's hard for me to know. What I would do is remove the date filters from your WHERE clause and instead turn them into columns in your SELECT statement, exactly as they were in the WHERE. This will show you how the data looks and should give you hints or an explanation as to why your WHERE clause isn't filtering them out as you hoped. 
If you write SQL long enough, you write a [case statement of shame.](https://pastebin.com/WZvmrWjc)
Not familiar with Oracle, but apparently you can use `REGEX_SUBSTR` to accomplish this. https://docs.oracle.com/cd/B12037_01/server.101/b10759/functions116.htm Regex: `abc\d{4}` SELECT REGEXP_SUBSTR(your_column, 'abc[:digit:]{4}') "output_column" FROM table;
News to me that Toad can run natively in Linux. And no WINE does not count...
Proc Freq &amp; Proc Print Similar Question: https://communities.sas.com/t5/SAS-Procedures/Help-needed-with-excluding-low-frequency-variables-from-data-set/td-p/166852 There's also r/sas you can ask
Cool, this works - it now gives the same result as Intrexa's query. Thank you.
SSRS is free isn’t it? I don’t recall paying for it. 
Yes, you should be able to use SQL Server Developer edition which comes with SSRS. Check out [this link](https://blogs.technet.microsoft.com/dataplatforminsider/2016/03/31/microsoft-sql-server-developer-edition-is-now-free/) and the comment about SSRS.
SSMS and SSDT are free which will allow you to create reports. Publishing them will require a SSRS server target which will probably need licencing.
I use the Developer edition, it comes with everything the enterprise edition comes with, including SSRS. Obviously, it's not for use in production environments
As others have said you should be able to develop SSRS reports for free with SQL server developer edition. In all likelihood you will just be able to design and test reports and not deploy them. For development my preferred solution is Visual Studio (VS) with Business Intelligence Development Studio (BIDS), it lets you easily manage reports in a VS project and do things like manage shared data sources, sub reports and linked reports very easily. I have used Report Builder but it has always felt so limited compared to the options, tools and customise-ablity available in VS with BIDS.
Unless you are using it for development/testing, which is free under SQL Server Developer edition.
Unfortunately SAS has no method of deleting by frequency without using PROC SQL, but thanks.
So, thank you. This works. As I strive to learn...HOW THE HELL DOES THIS WORK SO SIMPLY
Thanks for the reply. But what is "foo" in your example? 
I am not sure what you mean by working simply. It is just left join using the same table. If you mean under the hood of SQL engine, then somebody smarter than me will have to answer that. There is a lot of magic happening there. A quick look at execution plan says that it scans the same table 4 times (x, a, b, c) and then sorts results. It might get into problems with the big amount of data, so some indexing or other optimization might be necessary. It would also get quite ugly if you wanted more than 3 values from column 'Code', imagine 26 joins if you used the whole alphabet. If performance is not an issue, self-join for the situation you described works. But if you need to do get that data fast, I am convinced there exists a better solution. 
Thank you so much, this worked! 
Great tip, you read right into what I'm actually doing which is pulling ATT ids, so that upper() was noted. Thanks again
Post the ENTIRE query.
lol! I'm quoting this when I turn in this code. Would there be a way to generate these case statements using dynamic SQL? Keep in mind I can't aggregate the result set. I will also be selecting everything from the dbo.HeaderTable table. So basically, I need this date generated for every individual row.
Try something like this. DECLARE @MaxOrder int = 3; SELECT CASE WHEN a.[Order] = 1 AND b.[Order] = 2 THEN 'CASE ' ELSE '' END + CASE WHEN b.[Order] = a.[Order] + 1 THEN 'WHEN ' WHEN a.[Order] = @MaxOrder THEN '' ELSE ' AND ' END + CASE WHEN a.[Order] &lt; @MaxOrder THEN 'DATEADD(SECOND,COALESCE(' + a.Alias + '.MODIFIEDTIME,''''),COALESCE(' + a.Alias + '.MODIFIEDDATE,'''')) &gt; DATEADD(SECOND,COALESCE(' + b.Alias + '.MODIFIEDTIME,''''), COALESCE(' + b.Alias + '.MODIFIEDDATE,''''))' ELSE '' END + CASE WHEN b.[Order] = @MaxOrder THEN ' THEN DATEADD(SECOND,COALESCE(' + a.Alias + '.MODIFIEDTIME,''''),COALESCE(' + a.Alias + '.MODIFIEDDATE,''''))' WHEN a.[Order] = @MaxOrder THEN 'ELSE DATEADD(SECOND,COALESCE(' + a.Alias + '.MODIFIEDTIME,''''),COALESCE(' + a.Alias + '.MODIFIEDDATE,'''')) END' ELSE '' END AS [SQL] FROM ( SELECT 'cn1' AS Alias , 1 AS [Order] UNION ALL SELECT 'st1' AS Alias , 2 AS [Order] UNION ALL SELECT 'csg' AS Alias , 3 AS [Order]) AS a LEFT OUTER JOIN ( SELECT 'cn1' AS Alias , 1 AS [Order] UNION ALL SELECT 'st1' AS Alias , 2 AS [Order] UNION ALL SELECT 'csg' AS Alias , 3 AS [Order]) AS b ON a.[Order] &lt; b.[Order] WHERE a.[Order] = @MaxOrder OR b.Alias IS NOT NULL;
Always put parentheses around `OR` logic.
That should fix it and (tran_type = 'cr' or tran_type = 'ra')
Most likely, one or more of your join conditions doesn't match table uniqueness. If you can get an execution plan, that might indicate which joins expand the data set.
That's it! Thanks!
That was it. I knew it was something simple. Thank you.
If you're going to use `SELECT *` like that, you need to alias your sub-sub-query columns (ex. MAX(SPRTELE.SPRTELE_SEQNO) AS **mSPRTELE_SEQNO**) so they don't collide with other column names. But really, avoid `SELECT *` if you don't literally want every column.
Re: the BI, it's like you work with me. I'm really really green with SQL and have started coding some out of necessity. The canned reporting packages that we have with some of our software is quite limited. The ability to use the basic ideas from those reports (the reports all the c-levels are used to seeing) and add multiple levels of customization, is absolutely mind blowing to them. In the first couple of weeks at my new firm I was able to create an aged accounts receivable report that the AR manager couldn't create in his 12 years here. It has been revolutionary...even the most simple reports have had immediate and tangible impact here. I work at a law firm as a financial analyst...and I have to know as much, or more of the "IT" side of things, as I do the financial aspects. My "job" was relatively non-existent about 10 years ago, but now we're in relatively high demand in my city, especially with an SQL background. You are spot-on though - BI is booming.
You are my hero!!!!
Aliasing those columns corrected the error, you're a genius! However, it's now taking forever to run, so I'll bring in the columns I need to get rid of the SELECT * and see if that helps. I was just being lazy and most of the time things run fast enough in this database. Thanks
No seriously, thank you so very very much!
change this -- and tran_type = 'cr' or tran_type = 'ra' to this -- and tran_type in ( 'cr' , 'ra' ) 
There's a few ways to do it. The easiest is probably to isolate the duplicates and non-duplicates into two separate queries and join them with a union. SELECT ID, DATE, CODE, 'Y' As [Duplicate] FROM TABLE GROUP BY ID, DATE, CODE HAVING COUNT(*)&gt;1 UNION SELECT ID, DATE, CODE, '' As [Duplicate] FROM TABLE GROUP BY ID, DATE, CODE HAVING COUNT(*)=1
What rdbms is it?
Can be done with a subquery. Might not be very efficient on a bigger table though. CREATE TABLE ##TestTable (ID VARCHAR(5), Date DATE, Code VARCHAR(5)) TRUNCATE TABLE ##TestTable INSERT INTO ##TestTable (ID,Date,Code) VALUES ('A123','2017-08-01','A1') INSERT INTO ##TestTable (ID,Date,Code) VALUES ('A123','2017-08-03','A2') INSERT INTO ##TestTable (ID,Date,Code) VALUES ('B234','2017-08-05','A1') INSERT INTO ##TestTable (ID,Date,Code) VALUES ('C456','2017-08-05','A3') INSERT INTO ##TestTable (ID,Date,Code) VALUES ('D789','2017-08-08','A1') INSERT INTO ##TestTable (ID,Date,Code) VALUES ('D789','2017-08-08','A2') INSERT INTO ##TestTable (ID,Date,Code) VALUES ('D789','2017-08-09','A3') SELECT tt.ID, tt.Date, tt.Code, CASE WHEN ID IN ( SELECT tt2.ID FROM ##TestTable AS tt2 GROUP BY tt2.ID HAVING COUNT(*) &gt; 1) THEN 'Y' ELSE '' END AS Duplicate FROM ##TestTable AS tt DROP TABLE ##TestTable
MSSQL
Sorry for my lack of knowledge, but what does truncate table team
https://docs.microsoft.com/en-us/sql/t-sql/statements/truncate-table-transact-sql You can ignore that I just had it in there because I accidentally ran the insert twice.
You can use a window function to get a count of a group of ids and if that count is greater than 1 it's duped. SELECT * , CASE WHEN (COUNT(1) OVER (PARTITION BY ID) &gt; 1) THEN 'Y' ELSE 'N' END AS [Duplicate] FROM YourTable
You want to return a row when @PERSON doesn't exist in adv_.person? I'm assuming that when you say the CASE returns a blank, you mean there aren't actually any rows in the result set? I'd probably SELECT @PERSON, then join to a subquery on _adv for that @PERSON and handle null results from _adv by using COALESCE or ISNULL. You could also try an IF...ELSE statement to check whether any rows exist in _adv for @PERSON, and return depending on that. Your WHEN [NOT] EXISTS won't help because whether it returns a row is still dependent upon @PERSON existing in adv_.person. The WHERE clause takes priority, so you'll only get a row when @PERSON = _adv.person.
This is a better solution than mine. Nice, never used a window over a count like this.
 &gt; I'm assuming that when you say the CASE returns a blank, Crap, you're right. I edited the post. Thank you so much for talking that out for me, I understand now. It seems so obvious! You're the best. 
No problem, it always seems really obvious in retrospect!
What SQL flavour is this? Assuming PL/SQL. The error is seemingly because you've bracketed IF incorrectly. If you're not working in an editor with paired bracket highlighting then Notepad++ can be used to pick up on bracket-related problems. https://www.techonthenet.com/oracle/loops/if_then.php You should probably use IF (EXTRACT(HOUR from Time key) =21 OR EXTRACT(HOUR from TIME_KEY)=22... However, you may find it easier to use a CASE statement https://www.techonthenet.com/oracle/functions/case.php and IF(EXTRACT(HOUR from TIME_KEY) IN (21,22,23...) E: At least some of my bracketing was wrong too, welp. E2: Or maybe not. I'm drunk. Please consult the appropriate documentation.
PL/SQL
It's in M$ interest to make everything freely available on dev licensing, so it is. You can play with (I think) all of the enterprise level SQL features by using a dev license.
Also, always put parentheses around OR logic.
 SELECT p.person, CASE WHEN ISNULL(a.datereviewed, '') = '' THEN 'NO' ELSE 'YES' END AS Status FROM (SELECT @PERSON AS person) p LEFT OUTER JOIN adv_ a ON p.person = a.person
still receiving right ) error
still receiving right ) error
Can you paste your full amended query? For reference, this returns fine in MSSQL. IF((1=1) OR (2=2)) SELECT 'Yes' ELSE SELECT 'No' AS Result But a CASE would probably be easier SELECT CASE WHEN (1=1) OR (2=2) THEN 'Yes' ELSE 'No' END AS Result
 SELECT TO_TIMESTAMP(TO_CHAR(TO_DATE('19700101000000', 'YYYYMMDDHH24MISS')+((ASH.DATETIMEASSIGNMENTSTART- 18000) /(60*60*24)),'YYYYMMDDHH24MISS'), 'YYYYMMDDHH24MISS') AS TIME_KEY, CASE WHEN (EXTRACT(HOUR from TIME_KEY) = 22 or EXTRACT(HOUR from TIME_KEY) = 23) and (EXTRACT(DAY_OF_WEEK from TIME_KEY) = 1 OR EXTRACT(DAY_OF_WEEK from TIME_KEY) = 2) THEN 'Shift A' ELSE 'no' END as SHIFT_TYPE FROM TECH_METRICS_UET.ASH_ASSIGNMENTHISTORY ASH 
Maybe it has do do with the Extract function or my Time_Key attribute?
Not the op there, but the point with wrapping or is to prevent accidentally pulling back more than you intend. For example, my company had an update ran in production that had a where clause that was something like Where 1=1 And (lots of stuff to filter the dat... And group . status = 'in progress') Or group. Status = 'live' By having the or outside the parentheses it pulled back literally every client we had instead of just live/in progress ones that also matched the rest of the logic. This update was to terminate their services, which caused a bunch of triggers that did further updates. Now, the person who did that should have turned the first group status into an in instead of adding another, and also not make a last minute untested tweak to a script going into prod.
Yup, and Adam Machanic has come up with some pretty wild/blazing fast shit too. My recommendations would be learning how to *really* read query plans, learn the difference between logical data flow and physical data flow, what the major operators do, how the different join operators work, etc. You'll soon gain a better understanding of why the query engine makes the decisions it does. SQL Sentry Plan Explorer is a great tool for this. When you first start looking at it, you'll likely be banging your head against your keyboard after giving it a 100% textbook perfect covering index that it just straight up ignores in favour of a CI scan. But slowly but surely, it will start to make sense and you'll start to gain an understanding, which may blossom into an appreciation for how it doesn't work to create "the best plan for this scenario" but rather "a good enough plan for every possible scenario as fast as possible." Once you cross this threshold, the power of the query optimiser is nothing short of mind-boggling. PROTIP: Never, ever, ever, under any circumstances, try to outsmart the query optimiser. It will not end well. What you're really trying to do is think like the DB engine does. You're trying to put yourself in it's shoes and reason out how you would accomplish the challenge (query) you've been given, in a manner that's "fast enough" today, and will still be "fast enough" a year from now when your data volume and/or distribution may have changed significantly. Brent Ozar has given tons of talks on this subject, my personal favourite is here: https://www.youtube.com/watch?v=uwGCPtga06U
Video linked by /u/sn0re_lacks: Title|Channel|Published|Duration|Likes|Total Views :----------:|:----------:|:----------:|:----------:|:----------:|:----------: [Brent Ozar_Watch Brent Tune Queries](https://youtube.com/watch?v=uwGCPtga06U)|SQLugSWE|2015-03-17|1:06:58|109+ (100%)|18,417 &gt; SQLRally Nordic recording from Brent Ozar’s presentation... --- [^Info](https://np.reddit.com/r/youtubot/wiki/index) ^| [^/u/sn0re_lacks ^can ^delete](https://np.reddit.com/message/compose/?to=_youtubot_&amp;subject=delete\%20comment&amp;message=dm1otj0\%0A\%0AReason\%3A\%20\%2A\%2Aplease+help+us+improve\%2A\%2A) ^| ^v1.1.3b
Nice! I've always used a subquery for this, even with window functions.
Frankly, your request is almost unreadable. A good technique to ask questions (and educate yourself BTW) is to try and come up with a so-called MWE, a minimal working example. Make it *fairly* minimal and try to really pinpoint the thing you want to ask about. I was first about to say that what you want looks like a `join`, but then I realize your tables do not seem to have any foreign keys pointing into another table? What?
1. When you write your data to a local file you know how it works and where it is. So let's look at what a database does. You can install a free database management system (DMBS) such as postgresql on your local computer and then you can write the data with sql to the database (postgresql). The data is now inside the database and you can retrieve it again using sql. The data is on your local computer and it doesn't cost anything. *How* it's stored is a different question though. The point with using SQL is that you shouldn't need to bother with the really difficult details of hos to store it. Storing some text in a file is not difficult. Storing really complex data that is related to other complex data can be a really tricky thing. Good thing then that many DBMS have already solved that problem. You just need to figure out how to use sql and all that stuff is taken care for you. There will of course be files created on your hard drive that postgresql uses, but don't bother trying to read those. Treat them as off limits and the responsibility of postgresql to handle. 2. If you install a free DMBS on your local computer it's entirely free. You don't even need internet. Just some small disk space. 3. Hibernate is an object-relational mapper. Don't bother with any of those tools yet. Learn how to do it the low level way first. 
I agree with /u/johnfrazer783. This is barely readable. But I'll try. This is what I gather: * You have one table Person, that contain information about persons. * You have one table House, that contain information about houses. If you also want persons to exist inside a house you would need an additional table that stores that information. Perhaps call it PersonHouse with the fields UID (from the Person table) and HouseID (from the House table) being foreign keys. This would allow the following scenarios: * An empty house can exist without any persons in it. * A person could be in two houses at the same time. * A person can exist without being in a house. 
So there is a storage on your hard drive for testing and learning purposes and eventually when you go into big league with it, or get a job that involves your manipulating SQL, it's actually stored online? Or it can still be and most likely is spread on variety of networks, local, and global.
"Stored online" just means that it's on another computer that can be access from the internet. :-) For testing and development you can absolutely use your own computer. But if you write something that should be accessible from the internet then it's a good idea that the computer containing the database is also connected to the internet. Your computer/laptop is likely not a good idea then because you probably turn it off at night. So a server somewhere would need to have the database instead. E.g. I work with databases that can be accessed from my company's intranet. They run on servers for that particular purpose on our local intranet. They are not accessible from the internet. But that's just because we have built it that way and it is what we want. Running dedicated servers absolutely has a cost to them. 
Alright, I have much wider picture now. Have a good day
You too, friend. 
Hi, check out http://online-trainings.gaussling.com They offer a wide range of database courses at affordable prices. I think the nosql courses are not online yet, but send an email. They might tell you when they are available.
To go into a bit more about the actual storage of SQL. When you install SQL, you specify where you put the server and the database files. These files have the extension ".MDB", ".LDB" and other types. In these files are pages of data, sort of like a page in a Word document. Just a blank white page with a bunch of data written on them. When you insert data into the database, it is written on these pages. When you query SQL, it goes to that .MDB file, and looks up the page of data and returns the data from the page. Sort of like how you look up data in a book by going to the index or table of contents and then find the page and read it. I simplified quite a bit for the sake of brevity--but in reality the process is very close to what I described. This series really helped me understand it: https://www.brentozar.com/training/think-like-sql-server-engine/ edit: The pages of data look like this http://i.brentozar.com/engine.pdf
This is not PL/SQL. It may be an Oracle database, but this is plain SQL, not PL/SQL.
Okay well I need the statement to work in an oracle environment and I got this syntax from oracle
Is the procedure using invoker's rights or definer's rights?
I understand that principle quite a lot for the short time I'm learning java. I think JavaFX and Scene builder helped me a lot with its Main class, Controller class and actual view. Little project I've done yesterday totaly unrelated to this. https://www.youtube.com/watch?v=lK1wlJ5AdyE Each thing has it's own purpose. 
I don't memorize the academic definition of normal forms, but donut_name, donut_description, and donut_price should be in a 4th table. Also, why donut instead of doughnut?
They are in a fourth table when I have to present four tables in 3NF. I'm instructed specifically to put the information in to three tables in 2NF. I went with "donut" instead of "doughnut" simply because the mock business I'm making the database for is called "Donuts-R-Us" :)
I agree you definitely need a link table as the person - house relationship is many to many (a person can have many houses, a house can have many people) From your question I can't quite work out whether you want to include Jon Snow Nights Watch, the Wall or only Jon Snow, Stark, Winterfell If you DON'T want to include the Nights Watch you'll need a where clause on Stark, if you do you need to look at a having clause. Happy hunting.
Donut_name, donut_description definitely shouldn't be on the Line table, unless it's an exercise in what not to do. Price is usually included on both Line and Item (Donut).
This appears to be an Excel question, you might want to ask over in /r/excel 
Right, and I completely agree. Which is why when I present the four tables in 3NF I have it exactly as you describe. The problem I'm having is how to present all the information in only three tables :(
Definer's.
&gt; edit: reddit doesn't like "*" used in text, but all the enclosed text in quotes has asterisks on both sides. \ is the escape character on Reddit Typing \\* gets you \*
wow posted by mistake, thanks.
What if you assume one order == one donut type? Customer, Order, Donut?
/u/Kllian, /u/mikeyd85, /u/Pseudoniceguy83, /u/jc4hokies I just wanted to let you guys know that I ended up getting the job!! Woohoo! The interview didn't end up getting as technical as I thought it would be, but I went in much more confident because of the help and ideas you guys had given me. I was able to study up and felt really great going in. Thanks again for the help!
Shit like this makes me really happy I learned SQL through my job and not through school. "Here's an example of how we shouldn't design a database. We won't accept the correct answer." And they do shit like use the old comma style joins. I see that bad syntax on every single test question that's posted here or on StackOverflow. Like wtf are teachers teaching people these days? I feel bad for you, hopefully when you get a job your boss doesn't make you design something the wrong way first before you do it the correct way. edit: I'm guessing your issue is that you have no way to distinguish the line numbers on your orders since you could have two donut "types" with the same "donut_id" on the same order. But the real answer is probably "whatever your teacher thinks is OK".
select top(1) datereviewed , (case when person = "@PERSON" and (datereviewed is null or datereviewed ='') then 'NO' else 'YES' END) as 'Status' from adv_ where person = @PERSON order by datereviewed desc
Your example is actually exactly how my v1 submission is structured, and that was kicked back as not being in 2NF. I kind of understand that rejection as the primary key would need to be a composite of (order_id, donut_id) and the special instructions and date would only be dependent on the order_id portion of the key.
A single order could have multiple different types of donuts, each with their own respective quantity. If the same donut_id was included more than once, wouldn't it just make sense to increase the quantity of one record rather than adding another record?
My v1 submission had a table dedicated to storing donuts and the order table used a composite key of (order_id, donut_id) to identify each row, but in that instance things like the date and special instructions would only be dependent on the subset key of order_id.
That's what I'm saying about the difference between whatever your teacher wants and a real world scenario. And the answer to your question should be determined by the front end POS system. Does the user modify the quantity? Then you modify the quantity on the order. Does the user add a new line? Then you add a new line to the order. I have no idea what the hell your teacher wants :) 
I hear ya, it's BS that the only feedback I'm getting is "it's not in 2NF"... Like, okay... HOW is it not in 2NF? Thanks anyways for your help!
I'm suggesting order_id is the PK with donut_id as an attribute. Basically require that an order is for a single donut_id of some quantity. If you want sprinkles and frosted, you make two orders.
[removed]
Ah, I see what you're saying! If I can't think of any other ways to structure the data I may just try that :) thanks
Imagine you have your phone catalog. It's a simple catalog. Two columns. One for name one for the phone number John, 123456789 Jane, 987654321 Michael, 56985555 ... Now you want to store them in a file so you create a file named phones.csv and store those numbers. Each person represents a row in the form `name, number`. Now you also want to add address to that person so you do John, 123456789, Johns home Jane, 987654321, Janes home Michael, 56985555, Michaels work ... Now you want to add relationships with them John, 123456789, Johns home, family Jane, 987654321, Janes home, family Michael, 56985555, Michaels work, friend ... This file is starting to get bigger. Imagine you have 1.000.000 such entries. Searching for them is going to be slow. What if you want to add relationships for them? Like what country is each from or their age or even avoid duplicates? This is going to be a lot of work. That's why we use special software called DMBS. DMBS is the kind of software and the specific implementations are many like PostgreSQL, MySQL etc. What they do is they manage the data more or less like the ones above and offer many conveniences. They don't save the data in plain coma separated fields offcourse. They use more sophisticated formats. Details vary for each implementation. The way they assist you to handle the data like searching for the data you want, changing specific data or even deleting the data is by using a special language. The language is SQL. It's not the software that is called SQL but the language the software offers to you for managing the data. That data should be saved somewhere. This somewhere is usually a computer hard disk. You can copy the files in a USB stick and use them in some other computer if you wish. This way you can share the phone catalog with your whole family. However this would be rather inconvenient. So you allow other computers over your home network connect to the DMBS software in your computer and they can query whatever data they need using the SQL language. Or maybe you want your cousin in Antarctica to have access to this catalog. In this case the most convenient way is allowing his computer to connect via the Internet with the DMBS software in your computer. You don't know when everyone may request the data and keeping you laptop available 24/7 for them to connect is inconvenient for you. So you put a dedicated computer to serve them whenever they wish. &gt; When I store data, is it free? Do I have to pay anyone anywhere, some kind of provider to have my SQL space on internet? If you use your home computer to serve the data from your home internet connection you don't have to pay anybody anything. Imagine you have to share that catalog with thousands of people. The internet connection and the computer should be fast enough to serve everyone anytime. Home internet connections have typically narrow upload bandwidth, unsuitable for this purpose. You usually rent bandwidth and storage space from some internet provider so he will deal with the computer maintenance and the bandwidth requirements. This leaves you time to focus in maintaining your catalog. The DMBS software now runs in providers computer. The data are located in there too. 
You're right, the source I had for informing me that it runs on Linux slightly mentioned that it runs via Wine. I've updated the post. Thanks for pointing it out!
Sorry I accidentally hit enter before completing my reply. That's why it seemed to make no sense. :)
Hmm... I thought that a grant that does not affect the procedure itself (or objects it depends on) would not cause a recompile (nor an invalidation). I am curious about the IDE you are using. Are you are running this in SQL*Plus or some other tool? Is it possible the tool is trying to do a describe on the procedure?
SQL is a language. It is used to get data from the majority of the relational databases that are around today. SQL Server, Oracle, Postgresql and MySQL are some relational databases that can be run on Linux or Windows or Unix operating systems. Even the non open source databases have a version that you can install on your own PC. The data in these databases are stored in files. The method of storing data differs between databases and the operating system. You can get free database space from some cloud vendors, some provide a database as a service type deal. These are very general details, there are some cases where SQL can be used for non relational data or data stored outside of what some consider a relational database. Hibernate is (AFAIK) a developer tool for Java. 
So, Oracle, MySql are same stuff, just a different vendor right? Like Eclipse, NetBeans but they are not IDE. Are the codes the same or MySql and Oracle have different codes? Whats the difference?
Thank you, I understand now.
This isn't what you want to hear, but get exposure to as many different technologies as possible, don't pidgeon hole yourself into one technology from the beginning
Actually Oracle and MySQL are owned by Oracle now. But yes, they are both relational database management systems which you can use SQL in (with caveats). The way that they do the managing of the data in liason with the operating system are very different yet lead to the same sort of functionality. The non relational databases are gaining in popularity with Big Data. Some of these use an SQL-like language to access data. Some use Javascript, some have API in lots of programming languages. 
Typically Oracle dba jobs have higher pay, but being hired for it as a rookie is also more difficult. Tech wise SAP, Oracle and SQL are all doing well - check the Gartner report. Regardless, DBA role is evolving - in 10 years it will exist but the tasks performed by the DBA will be completely different. I am assuming by SQL Career path you meant as a DBA otherwise SQL is a just a language with slight changes for each platform and learning which ever should not be an issue. 
The language is very similar (there's a language called "Standard SQL" but not all companies use it, and they all have unique functions that are specific to their own software). So for the most part, MS SQL, MySQL, Oracle, Postgres are interchangeable and if you know SQL you can write most queries for any of the different "flavors" if you know (or google) their unique differences.
Yep, I mean as a DBA. I'm looking for a change from front end work because I just don't enjoy it as much as being part of application logic, reporting/BI and planning. 
Totally agree. I've been freelancing and doing contract work for years using various flavors of SQL as well as front end and dev work. However, wouldn't being a sme on oracle or sql server have greater earning potential down the road if I focused on one or the other? 
On that note would you be able to recommend an Oracle learning path for someone like myself who is not a beginner, but looking to take seriously becoming a DBA? 
I think the easiest thing for you would be to approach the Development DBA route (AKA application DBA). We still write tons of code, but the operational DBAs handle the hardware/licensing/networking for the most part. 75-80% of my job is architecting/writing/deploying code as part of an application development team. But I am also in charge of backups, tuning, refreshing the DBs when they need it, and administer permissions/ensure the databases are online as well (among other things). Look into it; I think you'll do well coming from a development area. I began in SQL server and have transitioned to oracle. I love and hate both very dearly. Plug for my favorite team of DBAs, I think this is one of the best analogies for Dev vs Operational DBA: https://www.brentozar.com/sql/picking-a-dba-career-path/
Nice blog. Thanks for the link! 
Welcome!! They have a podcast/webcast called Office Hours that is a great listen. One of them (Richie Rump) is more on the .NET development side as well. I should have added that I come from a .NET development background, which is why I know the transition will be easiest for youZ
Wait, do you want to do database administration, database development, or BI? 
https://ozar.me/2016/04/should-you-be-a-generalist-or-a-specialist/
I disagree, as you could end up a jack of all trades but a master of none. While being familiar with multiple technologies may help when you're first starting out, but in the long run you'll want to pick a platform and master it. What we'll call "advanced database jobs" require a highly specialized skillset, and it's the advanced stuff that set the various database platforms apart. No one would ever fill a Senior Oracle DBA position with a guy who has been working on SQL Server for the last 10 years, but that same guy pretty much has his pick of SQL Server DBA positions. My recommendation would be to focus on SQL Server and Oracle, and then learn and master whichever one you land a job on. You'll probably find you have a harder time getting your foot in the door in an Oracle shop, but if you do, you're pretty much set for your career. If a company has enough money for Oracle, you'll never have to worry about your department budget. 
DBA..I guess I was just saying that my interest is more along the lines of data management, backup, storage, logic and analysis vs my current career which is in development. 
Just pick one to focus on, but be aware of the others
Roles which cover all of those are pretty rare - generally, the guy doing the backups and managing access/mirroring/performance etc etc is someone quite different to the person doing the analysis and business logic side of things. 
This and the Dev DBA route is your best bet.
Can you recommend an Oracle training path?
I'm actually more inclined to go the Oracle route because I prefer not to run Windows, but I do know that SQL Server can be run through a VM...I don't know if there are any limitations there.
SQL server can also be run via Linux if you're that against windows. Honestly, I've been working on getting some dev oracle databases spooled up on windows. Can I ask why you'd be so against them? Oracle, IMO has a bit of a steeper learning curve than SQL server. Transact SQL (T-SQL) is generally easier to grasp than PL/SQL is. It's part of the reason it pays a bit more (the other reason being it's a lot less common and generally more expensive to run oracle licensing). I learned T-SQL (sql server) first and used it 90% of the time at my last job. Now (for the last year or two) I've been 90% oracle. You can stand up free versions of both from these two links, and see which one you take to quicker. Http://Devgym.oracle.com Http://Docs.microsoft.com/sql Here to have my brain picked if you have any other questions at all.
Windows account or sql account? How are you connecting? SSMS or an application?
THENs in a CASE statement want to be the same datatype. Try adding 90 instead of string concatenation to keep everything in integers.
Can't thank you enough for taking the time to provide guidance. In terms of my aversion to Windows - it's actually my aversion to the hardware that runs Windows, not Windows itself. I had an x250 Thinkpad. It ran loud, bezel bigger than Chris Christie, trackpad was poop at best and horrid in sunlight. Maybe you can recommend a machine to bring me back? I'm fortunate to have a good amount of available study time each week and I'm willing to invest in training on top of self study. Will take a look at devgym now!
I agree with everything you said, up to the last sentence. No matter where you are, at times you will face budget issues. You will cut staff, you will lose through attrition and not be allowed to rehire, and you will have to fight for allocations for monitoring and productivity tools. Jobs will move to lower cost countries. I'm in a Fortune 500 and we have massive Oracle and SQL Server implementations. I fight for my department's budget fiercely at times because we are a business that's highly tied to the energy sector. As oil cycles, so do we. The last downturn hit us hard and an IT staff of 60 is now less than 30 and that's just my region in North America. Not only picking a technology, but knowing what industry you are going to be tied to is an important consideration when job hunting.
No, I'm a MSSQL guy right now AND the last 15 years. Learning Oracle in the fall though
I'm running PL/SQL Developer 10.
Lots of good advice. SQL knowledge is SQL regardless of syntax learn one real eek and others are easy to pick up. Knowing underlying db tech is more important e.g. rds vs columnar etc. In my experience, the most quickly growing demand is in hosted db technologies...particularly aws stuff like redshift and azure equivalents. The trend seems to be new companies building out on this technology and old companies migrating to it. Of course older db technologies can be hosted but I don't think their cost and tech is keeping up.
You need to set the user up in the config file. Send more details about what you are using and I'll try to find the documentation.
I'm using SSMS. Sql account :)
Are you attending a course or self learning?
When you launch SSMS, change the login from Windows auth to SQL. Then put in your user name and pass. That's pretty much it.
In SSMS you just type a user name and password. If it only works when you have the box checked that says something like "use my current login" then you're using Windows logins. If this is the case you can either log j to Windows as the other user, or press shift, right click on SSMS and choose run as other user, then type the username and password into a Windows prompt for the other user. At this point this SSMS is running as another user, so just check the box that says to use the current user info. Another possibility is that you are using SQL logins and SQL login isn't enabled. I'm on my phone and haven't done this enough to type instructions from memory, but you should be able to google how to enable SQL logins in SQL server. Also if you post your exact error it could help. Good luck. 
Courses at my school /r/wgu
Either this, or you could cast the hell out of everything. (At least in MSSQL)
&gt; Like wtf are teachers teaching people these days? The same thing(s) they were teaching 25 years ago.
Rather than building some case expression, convert it to a real date first. Then add two months. And, if necessary, map back to yyyymm format: select to_char ( add_months( to_date('201712', 'yyyymm'), 2 ) , 'yyyymm' ) dt from dual; DT 201802 
Started out in SQL as a DBA, ended up switching to more of a BI type role. Now expanding deep into R. Like many other people said, expand and get exposed to more technologies. 
Knowledge
It's been awhile since I did formal training but the line table looks wrong for 2NF, in fact I don't think it would float as a 1NF. If you have an update to a donut you'd have to update every record in the line table that has that donut. Version 1 is the closest, add line ID to order table, and a PK after that you should be good to go.
MySql stores data. So literally anything you can do with that data would be beneficial to a business.
Question is too vague.
The latest version I sent is the same as v1 but instead of using a composite key of (order_id, donut_id) as the primary it just uses order_id (more or less it's what you suggest, but using different names, no?). This means that only a single donut can be included on any given order line, but I believe that's inconsequential since it could be addressed on the application side. I'm pretty sure that's 2NF since no other attribute on the table could be determined by any other candidate key (that I can see). Your thoughts?
You can build an interactive website storing data in the MySQL database. 